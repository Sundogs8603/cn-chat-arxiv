<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22823;&#22810;&#25968;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#23545;&#29983;&#25104;&#30340;&#20882;&#29260;&#26679;&#26412;&#30340;&#29702;&#35299;&#36739;&#20026;&#32932;&#27973;&#65292;&#23384;&#22312;&#19977;&#31181;&#26126;&#26174;&#30340;&#22833;&#36133;&#27169;&#24335;&#65306;&#35823;&#23558;&#20882;&#29260;&#26679;&#26412;&#20998;&#31867;&#20026;&#27491;&#30830;&#12289;&#22312;&#25512;&#29702;&#20882;&#29260;&#26679;&#26412;&#30340;&#25191;&#34892;&#34892;&#20026;&#26102;&#34920;&#29616;&#26356;&#24046;&#12289;&#20462;&#22797;&#20882;&#29260;&#26679;&#26412;&#30340;&#25104;&#21151;&#29575;&#24448;&#24448;&#20302;&#20110;&#29983;&#25104;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.19475</link><description>&lt;p&gt;
&#20882;&#29260;&#38590;&#39064;&#65306;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#20854;&#19981;&#27491;&#30830;&#29983;&#25104;&#30340;&#24494;&#22937;&#20043;&#22788;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19475
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#23545;&#29983;&#25104;&#30340;&#20882;&#29260;&#26679;&#26412;&#30340;&#29702;&#35299;&#36739;&#20026;&#32932;&#27973;&#65292;&#23384;&#22312;&#19977;&#31181;&#26126;&#26174;&#30340;&#22833;&#36133;&#27169;&#24335;&#65306;&#35823;&#23558;&#20882;&#29260;&#26679;&#26412;&#20998;&#31867;&#20026;&#27491;&#30830;&#12289;&#22312;&#25512;&#29702;&#20882;&#29260;&#26679;&#26412;&#30340;&#25191;&#34892;&#34892;&#20026;&#26102;&#34920;&#29616;&#26356;&#24046;&#12289;&#20462;&#22797;&#20882;&#29260;&#26679;&#26412;&#30340;&#25104;&#21151;&#29575;&#24448;&#24448;&#20302;&#20110;&#29983;&#25104;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#29087;&#32451;&#65292;&#23427;&#20204;&#20173;&#28982;&#32463;&#24120;&#29983;&#25104;&#19981;&#27491;&#30830;&#30340;&#31243;&#24207;&#12290;&#35768;&#22810;&#36825;&#20123;&#31243;&#24207;&#26174;&#28982;&#26159;&#38169;&#35823;&#30340;&#65292;&#20294;&#20854;&#20182;&#19968;&#20123;&#21017;&#26356;&#20026;&#24494;&#22937;&#65292;&#21487;&#20197;&#36890;&#36807;&#26356;&#24369;&#30340;&#27491;&#30830;&#24615;&#26816;&#26597;&#65292;&#20363;&#22914;&#33021;&#22815;&#32534;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36825;&#20123;&#20266;&#36896;&#30340;&#26679;&#26412;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25277;&#26679;&#24471;&#21040;&#30340;&#31243;&#24207;&#65292;&#36825;&#20123;&#31243;&#24207;1&#65289;&#22312;&#36866;&#24230;&#28201;&#24230;&#19979;&#29983;&#25104;&#30340;&#23545;&#25968;&#27010;&#29575;&#36275;&#22815;&#39640;&#65292;2&#65289;&#36890;&#36807;&#24369;&#27491;&#30830;&#24615;&#26816;&#26597;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23545;&#20266;&#36896;&#21697;&#30340;&#29702;&#35299;&#38750;&#24120;&#32932;&#27973;&#65292;&#23384;&#22312;&#19977;&#31181;&#26126;&#26174;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#38169;&#35823;&#22320;&#23558;&#23427;&#20204;&#20998;&#31867;&#20026;&#27491;&#30830;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#22312;&#25512;&#29702;&#20266;&#36896;&#21697;&#30340;&#25191;&#34892;&#34892;&#20026;&#26041;&#38754;&#26356;&#24046;&#65292;&#36890;&#24120;&#23558;&#23427;&#20204;&#30340;&#25191;&#34892;&#32467;&#26524;&#39044;&#27979;&#20026;&#22914;&#26524;&#23427;&#20204;&#26159;&#27491;&#30830;&#30340;&#19968;&#26679;&#12290;&#31532;&#19977;&#65292;&#22312;&#35201;&#27714;&#27169;&#22411;&#20462;&#22797;&#20266;&#36896;&#21697;&#26102;&#65292;&#27169;&#22411;&#25104;&#21151;&#20462;&#22797;&#20266;&#36896;&#21697;&#30340;&#21487;&#33021;&#24615;&#24448;&#24448;&#29978;&#33267;&#20302;&#20110;&#25277;&#26679;&#29983;&#25104;&#20266;&#36896;&#21697;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19475v1 Announce Type: cross  Abstract: While language models are increasingly more proficient at code generation, they still frequently generate incorrect programs. Many of these programs are obviously wrong, but others are more subtle and pass weaker correctness checks such as being able to compile. In this work, we focus on these counterfeit samples: programs sampled from a language model that 1) have a high enough log-probability to be generated at a moderate temperature and 2) pass weak correctness checks. Overall, we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit is often even lower than that of samp
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65292;&#22312;Battleship&#28216;&#25103;&#20013;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#21305;&#37197;&#30340;&#25928;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27169;&#22411;&#22914;&#20309;&#25351;&#23548;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.19471</link><description>&lt;p&gt;
&#20005;&#26684;&#30340;LIPS&#27785;&#27809;&#33328;&#33337;&#65306;&#22312;Battleship&#20013;&#20351;&#29992;&#35821;&#35328;&#20449;&#24687;&#31243;&#24207;&#25277;&#26679;&#25552;&#20986;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19471
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65292;&#22312;Battleship&#28216;&#25103;&#20013;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#21305;&#37197;&#30340;&#25928;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27169;&#22411;&#22914;&#20309;&#25351;&#23548;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#32467;&#21512;&#20102;&#25105;&#20204;&#23545;&#35821;&#35328;&#30340;&#25484;&#25569;&#21644;&#25105;&#20204;&#23545;&#20110;&#22312;&#26377;&#38480;&#35748;&#30693;&#36164;&#28304;&#24773;&#20917;&#19979;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#20154;&#20204;&#22914;&#20309;&#22312;&#24040;&#22823;&#20551;&#35774;&#31354;&#38388;&#20013;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#22312;&#22522;&#20110;&#25112;&#33328;&#28216;&#25103;Battleship&#30340;&#32463;&#20856;&#25552;&#38382;&#20219;&#21153;&#20013;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#35821;&#35328;&#20449;&#24687;&#31243;&#24207;&#25277;&#26679;&#65288;LIPS&#65289;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#31526;&#21495;&#31243;&#24207;&#65292;&#24182;&#35780;&#20272;&#20854;&#39044;&#26399;&#20449;&#24687;&#22686;&#30410;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#36164;&#28304;&#39044;&#31639;&#19979;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#33945;&#29305;&#21345;&#32599;&#20248;&#21270;&#31574;&#30053;&#20063;&#33021;&#20135;&#29983;&#21453;&#26144;&#20154;&#31867;&#22312;&#21508;&#31181;Battleship&#26827;&#30424;&#22330;&#26223;&#20013;&#34920;&#29616;&#30340;&#20016;&#23500;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20165;&#20351;&#29992;LLM&#30340;&#22522;&#32447;&#22312;&#23558;&#38382;&#39064;&#19982;&#26827;&#30424;&#29366;&#24577;&#32852;&#31995;&#36215;&#26469;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65307;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-4V&#24182;&#27809;&#26377;&#27604;&#26080;&#35270;&#35273;&#22522;&#32447;&#25552;&#20379;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36125;&#21494;&#26031;&#25552;&#38382;&#27169;&#22411;&#22914;&#20309;&#21487;&#33021;&#27169;&#25311;&#21644;&#25351;&#23548;&#20154;&#31867;&#30340;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19471v1 Announce Type: cross  Abstract: Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can l
&lt;/p&gt;</description></item><item><title>TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19467</link><description>&lt;p&gt;
TV-TREES&#65306;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;
&lt;/p&gt;
&lt;p&gt;
TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19467
&lt;/p&gt;
&lt;p&gt;
TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#30005;&#35270;&#21098;&#36753;&#31561;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#24403;&#21069;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#25512;&#29702;&#65292;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TV-TREES&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#12290;TV-TREES&#20316;&#20026;&#19968;&#31181;&#20419;&#36827;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#20219;&#21153;&#26469;&#35780;&#20272;&#27492;&#31867;&#26041;&#27861;&#30340;&#25512;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#12289;&#20855;&#26377;&#26368;&#20808;&#36827;&#38646;-shot&#24615;&#33021;&#30340;&#23436;&#25972;&#35270;&#39057;&#21098;&#36753;&#65292;&#23637;&#31034;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#25581;&#31034;&#20102;&#26089;&#26399;&#39044;&#35757;&#32451;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#21508;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20174;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#20197;&#22686;&#24378;LLM&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.19465</link><description>&lt;p&gt;
&#36861;&#36394;&#21487;&#20449;&#24230;&#21160;&#24577;&#65306;&#37325;&#35775;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26399;
&lt;/p&gt;
&lt;p&gt;
Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#25581;&#31034;&#20102;&#26089;&#26399;&#39044;&#35757;&#32451;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#21508;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20174;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#20197;&#22686;&#24378;LLM&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20805;&#20998;&#39044;&#35757;&#32451;&#30340;LLMs&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25552;&#39640;LLMs&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#39044;&#35757;&#32451;&#30340;&#28508;&#21147;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;LLMs&#22312;&#27492;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#19987;&#27880;&#20110;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#21487;&#38752;&#24615;&#12289;&#38544;&#31169;&#12289;&#26377;&#23475;&#24230;&#12289;&#20844;&#24179;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;LLMs&#24212;&#29992;&#32447;&#24615;&#25506;&#27979;&#12290;&#39640;&#25506;&#27979;&#20934;&#30830;&#24230;&#34920;&#26126;&#65292;\textit{&#26089;&#26399;&#39044;&#35757;&#32451;&#30340;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#27599;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;}&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25581;&#31034;&#39044;&#35757;&#32451;&#30340;&#28508;&#22312;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#20174;LLM&#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#21487;&#20449;&#24230;&#12290;&#26368;&#21518;&#65292;&#21463;&#21040;~\citet{choi2023understanding} &#30340;&#21551;&#21457;&#65292;&#30456;&#20114;&#20449;&#24687;&#20272;&#35745;&#21463;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#24230;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#36824;&#29992;&#30456;&#20114;&#20449;&#24687;&#25506;&#27979;LLMs&#26469;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19465v1 Announce Type: cross  Abstract: Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to in
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#32418;&#38431;LLM&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#65292;&#20197;&#26368;&#22823;&#21270;&#24341;&#20986;&#30446;&#26631;LLM&#19981;&#33391;&#21709;&#24212;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;RL&#26041;&#27861;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#35206;&#30422;&#33539;&#22260;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19464</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22909;&#22855;&#39537;&#21160;&#30340;&#32418;&#38431;&#23545;&#25239;
&lt;/p&gt;
&lt;p&gt;
Curiosity-driven Red-teaming for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19464
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#32418;&#38431;LLM&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#65292;&#20197;&#26368;&#22823;&#21270;&#24341;&#20986;&#30446;&#26631;LLM&#19981;&#33391;&#21709;&#24212;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;RL&#26041;&#27861;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#35206;&#30422;&#33539;&#22260;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#29983;&#25104;&#19981;&#27491;&#30830;&#25110;&#26377;&#27602;&#20869;&#23481;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25506;&#31350;LLM&#20309;&#26102;&#29983;&#25104;&#19981;&#38656;&#35201;&#30340;&#20869;&#23481;&#65292;&#24403;&#21069;&#30340;&#33539;&#20363;&#26159;&#25307;&#21215;&#19968;&#20010;&#20154;&#31867;&#27979;&#35797;&#32773;\textit{&#32418;&#38431;}&#26469;&#35774;&#35745;&#36755;&#20837;&#25552;&#31034;&#65288;&#21363;&#27979;&#35797;&#26696;&#20363;&#65289;&#65292;&#36825;&#20123;&#25552;&#31034;&#21487;&#20197;&#24341;&#20986;LLMs&#30340;&#19981;&#33391;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#20154;&#31867;&#27979;&#35797;&#32773;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#32418;&#38431;LLM&#33258;&#21160;&#21270;&#32418;&#38431;&#23545;&#25239;&#65292;&#29983;&#25104;&#26368;&#22823;&#21270;&#24341;&#20986;&#30446;&#26631;LLMs&#19981;&#33391;&#21709;&#24212;&#30340;&#27979;&#35797;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;RL&#26041;&#27861;&#21482;&#33021;&#29983;&#25104;&#23569;&#37327;&#26377;&#25928;&#30340;&#27979;&#35797;&#26696;&#20363;&#65292;&#23548;&#33268;&#23545;&#24341;&#20986;&#30446;&#26631;LLMs&#19981;&#33391;&#21709;&#24212;&#25552;&#31034;&#33539;&#22260;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#23558;&#22686;&#21152;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#35206;&#30422;&#33539;&#22260;&#30340;&#38382;&#39064;&#19982;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19464v1 Announce Type: cross  Abstract: Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases an
&lt;/p&gt;</description></item><item><title>$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;</title><link>https://arxiv.org/abs/2402.19457</link><description>&lt;p&gt;
$\texttt{COSMIC}$: &#30456;&#20114;&#20449;&#24687;&#29992;&#20110;&#20219;&#21153;&#26080;&#20851;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19457
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24635;&#32467;&#36136;&#37327;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#26681;&#25454;&#24635;&#32467;&#22120;&#29983;&#25104;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#19988;&#20445;&#30041;&#20219;&#21153;&#32467;&#26524;&#30340;&#25688;&#35201;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#36825;&#20123;&#20219;&#21153;&#30340;&#32467;&#26524;&#38169;&#35823;&#27010;&#29575;&#19982;&#28304;&#25991;&#26412;&#21644;&#29983;&#25104;&#25688;&#35201;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;$\texttt{COSMIC}$&#20316;&#20026;&#36825;&#19968;&#24230;&#37327;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#23637;&#31034;&#20102;&#23427;&#19982;&#22522;&#20110;&#20154;&#31867;&#21028;&#26029;&#30340;&#24230;&#37327;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#24050;&#24314;&#31435;&#30340;&#24230;&#37327;&#22914;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#30340;&#27604;&#36739;&#20998;&#26512;&#20984;&#26174;&#20102;$\texttt{COSMIC}$&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19457v1 Announce Type: cross  Abstract: Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries. We introduce $\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$ highlight the competitive performance of $\texttt{COSMIC}$.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21151;&#33021;&#21464;&#20307;&#30340;&#22522;&#20934;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#21457;&#29616;&#38745;&#24577;&#22522;&#20934;&#21644;&#21151;&#33021;&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#25512;&#29702;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.19450</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#25512;&#29702;&#24615;&#33021;&#35780;&#20272;&#30340;&#21151;&#33021;&#22522;&#20934;&#21450;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19450
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21151;&#33021;&#21464;&#20307;&#30340;&#22522;&#20934;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#21457;&#29616;&#38745;&#24577;&#22522;&#20934;&#21644;&#21151;&#33021;&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20934;&#30340;&#21151;&#33021;&#21464;&#20307;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#40065;&#26834;&#35780;&#20272;&#12290;&#35299;&#20915;&#25512;&#29702;&#27979;&#35797;&#30340;&#27169;&#22411;&#22312;&#38745;&#24577;&#38382;&#39064;&#30340;&#34920;&#29616;&#19982;&#21151;&#33021;&#21464;&#20307;&#24555;&#29031;&#30456;&#27604;&#24212;&#35813;&#27809;&#26377;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;MATH&#22522;&#20934;&#30340;&#30456;&#20851;&#29255;&#27573;&#37325;&#20889;&#20026;&#20854;&#21151;&#33021;&#21464;&#20307;MATH()&#65292;&#24182;&#23558;&#20854;&#20182;&#22522;&#20934;&#30340;&#21151;&#33021;&#21270;&#38543;&#20043;&#32780;&#26469;&#12290;&#22312;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#22312;MATH()&#24555;&#29031;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#25512;&#29702;&#24046;&#36317;--&#38745;&#24577;&#20934;&#30830;&#24615;&#19982;&#21151;&#33021;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30334;&#20998;&#27604;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#34920;&#29616;&#33391;&#22909;&#30340;&#38745;&#24577;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#23553;&#38381;&#21644;&#24320;&#25918;&#26435;&#37325;&#27169;&#22411;&#20043;&#38388;&#30340;&#25512;&#29702;&#24046;&#36317;&#65292;&#20174;58.35%&#21040;80.31%&#65292;&#20294;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#24046;&#36317;&#21487;&#33021;&#22312;&#20351;&#29992;&#26356;&#22797;&#26434;&#25552;&#31034;&#31574;&#30053;&#26102;&#26356;&#23567;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#22312;&#30495;&#23454;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#29702;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19450v1 Announce Type: new  Abstract: We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;LLMs&#30340;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;</title><link>https://arxiv.org/abs/2402.19446</link><description>&lt;p&gt;
ArCHer: &#36890;&#36807;&#20998;&#23618;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;LLMs&#30340;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#26696;&#20363;&#26159;&#30446;&#26631;&#23548;&#21521;&#30340;&#20915;&#31574;&#20219;&#21153;&#65288;&#25110;&#8220;&#20195;&#29702;&#8221;&#20219;&#21153;&#65289;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;LLM&#19981;&#20165;&#38656;&#35201;&#20026;&#32473;&#23450;&#25552;&#31034;&#29983;&#25104;&#23436;&#25104;&#65292;&#32780;&#19988;&#38656;&#35201;&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#20570;&#20986;&#26234;&#33021;&#20915;&#31574;&#20197;&#23436;&#25104;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#19982;&#32593;&#32476;&#20132;&#20114;&#65292;&#20351;&#29992;&#24037;&#20855;&#25110;&#25552;&#20379;&#23458;&#25143;&#25903;&#25345;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;LLMs&#30340;&#22810;&#36718;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19446v1 Announce Type: cross  Abstract: A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or "agent" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#35758;&#65292;&#26088;&#22312;&#30830;&#23450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#22768;&#23398;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#30340;&#20301;&#32622;&#21644;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.19443</link><description>&lt;p&gt;
&#25506;&#31350;&#31070;&#32463;&#32593;&#32476;&#22768;&#23398;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Probing the Information Encoded in Neural-based Acoustic Models of Automatic Speech Recognition Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#35758;&#65292;&#26088;&#22312;&#30830;&#23450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#22768;&#23398;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#30340;&#20301;&#32622;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#22240;&#27492;&#21463;&#30410;&#20110;&#36825;&#20123;&#31185;&#23398;&#21644;&#25216;&#26415;&#36827;&#27493;&#65292;&#29305;&#21035;&#26159;&#22312;&#22768;&#23398;&#24314;&#27169;&#26041;&#38754;&#65292;&#29616;&#22312;&#24050;&#32463;&#25972;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24615;&#33021;&#22686;&#30410;&#24050;&#32463;&#36716;&#21270;&#20026;&#20851;&#20110;&#36890;&#36807;&#36825;&#20123;&#40657;&#21283;&#23376;&#26550;&#26500;&#23398;&#21040;&#21644;&#20256;&#36798;&#30340;&#20449;&#24687;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;&#22312;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20043;&#21518;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#30830;&#23450;ASR&#22768;&#23398;&#27169;&#22411;(AM)&#20013;&#20449;&#24687;&#20301;&#32622;&#30340;&#21327;&#35758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#65288;&#22312;&#36825;&#37324;&#65292;&#19981;&#21516;&#23618;&#32423;&#30340;&#20013;&#38388;&#34920;&#31034;&#65289;&#35780;&#20272;AM&#22312;&#30830;&#23450;&#20219;&#21153;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20851;&#20110;&#24615;&#33021;&#21464;&#21270;&#21644;&#30446;&#26631;&#20219;&#21153;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20986;&#20851;&#20110;&#21738;&#20123;&#20449;&#24687;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#26159;&#22686;&#24378;&#25110;&#25200;&#21160;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19443v1 Announce Type: cross  Abstract: Deep learning architectures have made significant progress in terms of performance in many research areas. The automatic speech recognition (ASR) field has thus benefited from these scientific and technological advances, particularly for acoustic modeling, now integrating deep neural network architectures. However, these performance gains have translated into increased complexity regarding the information learned and conveyed through these black-box architectures. Following many researches in neural networks interpretability, we propose in this article a protocol that aims to determine which and where information is located in an ASR acoustic model (AM). To do so, we propose to evaluate AM performance on a determined set of tasks using intermediate representations (here, at different layer levels). Regarding the performance variation and targeted tasks, we can emit hypothesis about which information is enhanced or perturbed at differen
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#26799;&#24230;&#27969;&#21160;&#20998;&#20026;&#28909;&#36523;&#12289;&#28044;&#29616;&#21644;&#25910;&#25947;&#19977;&#20010;&#38454;&#27573;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.19442</link><description>&lt;p&gt;
&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65306;&#28044;&#29616;&#12289;&#25910;&#25947;&#21644;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19442
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#26799;&#24230;&#27969;&#21160;&#20998;&#20026;&#28909;&#36523;&#12289;&#28044;&#29616;&#21644;&#25910;&#25947;&#19977;&#20010;&#38454;&#27573;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#32447;&#24615;&#22238;&#24402;&#30340;&#22810;&#22836;softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#36873;&#25321;&#19979;&#65292;&#26799;&#24230;&#27969;&#21160;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26799;&#24230;&#27969;&#21160;&#21160;&#21147;&#23398;&#20013;&#20986;&#29616;&#20102;&#26377;&#36259;&#30340;&#8220;&#20219;&#21153;&#20998;&#37197;&#8221;&#29616;&#35937;&#65292;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#37117;&#19987;&#27880;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#30340;&#21333;&#20010;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#21160;&#21160;&#21147;&#23398;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#8212;&#8212;&#28909;&#36523;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#25439;&#22833;&#20943;&#23569;&#36895;&#24230;&#36739;&#24930;&#65292;&#27880;&#24847;&#21147;&#22836;&#36880;&#28176;&#20542;&#21521;&#20110;&#21508;&#33258;&#30340;&#20219;&#21153;&#65307;&#28044;&#29616;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#65292;&#27599;&#20010;&#22836;&#36873;&#25321;&#19968;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#25439;&#22833;&#36805;&#36895;&#20943;&#23569;&#65307;&#21644;&#25910;&#25947;&#38454;&#27573;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#65292;&#27880;&#24847;&#21147;&#21442;&#25968;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#22312;&#23398;&#20064;&#26497;&#38480;&#27169;&#22411;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19442v1 Announce Type: cross  Abstract: We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression. We establish the global convergence of gradient flow under suitable choices of initialization. In addition, we prove that an interesting "task allocation" phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases -- a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#36827;&#34892;&#26368;&#22351;&#32452;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#20998;&#26512;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#39118;&#38505;&#25511;&#21046;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.19437</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#26368;&#22351;&#32452;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Worst-group Risk Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#36827;&#34892;&#26368;&#22351;&#32452;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#20998;&#26512;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#39118;&#38505;&#25511;&#21046;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;$(\epsilon, \delta)$-&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#19979;&#23545;&#26368;&#22351;&#32452;&#39118;&#38505;&#26368;&#23567;&#21270;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#21487;&#20197;&#22312;$p$&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#23376;&#20154;&#32676;&#65288;&#32452;&#65289;&#20013;&#36817;&#20284;&#26368;&#23567;&#21270;&#26368;&#22823;&#39118;&#38505;&#30340;&#31169;&#26377;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#30340;&#20998;&#24067;&#36890;&#36807;&#26679;&#26412;&#35775;&#38382;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20854;&#23454;&#29616;&#30340;&#26368;&#22351;&#32452;&#32676;&#20307;&#39118;&#38505;&#36229;&#20986;&#24230;&#20026;$\tilde{O}(\frac{p\sqrt{d}}{K\epsilon} + \sqrt{\frac{p}{K}})$&#65292;&#20854;&#20013;$K$&#26159;&#20174;&#25152;&#26377;&#32452;&#20013;&#25277;&#21462;&#30340;&#26679;&#26412;&#30340;&#24635;&#25968;&#65292;$d$&#26159;&#38382;&#39064;&#32500;&#24230;&#12290;&#24403;&#27599;&#20010;&#20998;&#24067;&#36890;&#36807;&#22823;&#23567;&#20026;$K/p$&#30340;&#22266;&#23450;&#22823;&#23567;&#25968;&#25454;&#38598;&#35266;&#23519;&#26102;&#65292;&#25105;&#20204;&#30340;&#36895;&#29575;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;$\Delta$-&#19968;&#33268;&#24615;&#21442;&#25968;&#31283;&#23450;&#24615;&#24847;&#21619;&#30528;&#30456;&#23545;&#20110;&#26368;&#22351;&#32452;&#39118;&#38505;&#30340;$\tilde{O}(\Delta + \frac{1}{\sqrt{n}})$&#27867;&#21270;&#35823;&#24046;&#65292;&#20854;&#20013;$n$&#26159;&#20010;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19437v1 Announce Type: cross  Abstract: We initiate a systematic study of worst-group risk minimization under $(\epsilon, \delta)$-differential privacy (DP). The goal is to privately find a model that approximately minimizes the maximal risk across $p$ sub-populations (groups) with different distributions, where each group distribution is accessed via a sample oracle. We first present a new algorithm that achieves excess worst-group population risk of $\tilde{O}(\frac{p\sqrt{d}}{K\epsilon} + \sqrt{\frac{p}{K}})$, where $K$ is the total number of samples drawn from all groups and $d$ is the problem dimension. Our rate is nearly optimal when each distribution is observed via a fixed-size dataset of size $K/p$. Our result is based on a new stability-based analysis for the generalization error. In particular, we show that $\Delta$-uniform argument stability implies $\tilde{O}(\Delta + \frac{1}{\sqrt{n}})$ generalization error w.r.t. the worst-group risk, where $n$ is the number 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CAPIR&#65288;Compositional API Recommendation&#65289;&#26469;&#20026;&#31895;&#31890;&#24230;&#38656;&#27714;&#25512;&#33616;API&#65292;&#24182;&#37319;&#29992;&#8220;&#20998;&#32780;&#27835;&#20043;&#8221;&#30340;&#31574;&#30053;&#23558;&#20219;&#21153;&#25551;&#36848;&#20998;&#35299;&#20026;&#35814;&#32454;&#30340;&#23376;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.19431</link><description>&lt;p&gt;
&#38754;&#21521;&#24211;&#23548;&#21521;&#20195;&#30721;&#29983;&#25104;&#30340;&#32452;&#21512;&#24335;API&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Compositional API Recommendation for Library-Oriented Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19431
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CAPIR&#65288;Compositional API Recommendation&#65289;&#26469;&#20026;&#31895;&#31890;&#24230;&#38656;&#27714;&#25512;&#33616;API&#65292;&#24182;&#37319;&#29992;&#8220;&#20998;&#32780;&#27835;&#20043;&#8221;&#30340;&#31574;&#30053;&#23558;&#20219;&#21153;&#25551;&#36848;&#20998;&#35299;&#20026;&#35814;&#32454;&#30340;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#38754;&#21521;&#24211;&#30340;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20173;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;LLM&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#24211;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;API&#25512;&#33616;&#25216;&#26415;&#24110;&#21161;LLMs&#20351;&#29992;&#24211;&#65306;&#23427;&#26816;&#32034;&#19982;&#29992;&#25143;&#38656;&#27714;&#30456;&#20851;&#30340;API&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#25552;&#31034;LLMs&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#38656;&#27714;&#21487;&#33021;&#26159;&#31895;&#31890;&#24230;&#30340;&#65292;&#38656;&#35201;&#32467;&#21512;&#22810;&#20010;&#32454;&#31890;&#24230;API&#12290;&#36825;&#31181;&#31890;&#24230;&#19981;&#19968;&#33268;&#20351;API&#25512;&#33616;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAPIR&#65288;&#32452;&#21512;&#24335;API&#25512;&#33616;&#65289;&#65292;&#23427;&#37319;&#29992;&#8220;&#20998;&#32780;&#27835;&#20043;&#8221;&#30340;&#31574;&#30053;&#20026;&#31895;&#31890;&#24230;&#35201;&#27714;&#25512;&#33616;API&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CAPIR&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#20998;&#35299;&#22120;&#23558;&#31895;&#31890;&#24230;&#20219;&#21153;&#25551;&#36848;&#20998;&#35299;&#20026;&#20960;&#20010;&#35814;&#32454;&#30340;&#23376;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;CAPIR&#24212;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19431v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task. To address this, we propose CAPIR (Compositional API Recommendation), which adopts a "divide-and-conquer" strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based
&lt;/p&gt;</description></item><item><title>&#20132;&#20114;&#24335;&#20998;&#21106;&#32467;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#21644;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#65292;&#36890;&#36807;&#21453;&#22797;&#35843;&#25972;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#21644;&#19987;&#23478;&#20462;&#35746;&#30340;&#27880;&#37322;&#26469;&#25345;&#32493;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#22914;&#20309;&#22312;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#20173;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.19423</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#21644;&#19987;&#23478;&#20462;&#35746;&#30340;&#27880;&#37322;&#22312;&#20132;&#20114;&#24335;&#20998;&#21106;&#20013;&#65306;&#25345;&#32493;&#35843;&#25972;&#36824;&#26159;&#23436;&#20840;&#35757;&#32451;&#65311;
&lt;/p&gt;
&lt;p&gt;
Leveraging AI Predicted and Expert Revised Annotations in Interactive Segmentation: Continual Tuning or Full Training?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19423
&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#20998;&#21106;&#32467;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#21644;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#65292;&#36890;&#36807;&#21453;&#22797;&#35843;&#25972;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#21644;&#19987;&#23478;&#20462;&#35746;&#30340;&#27880;&#37322;&#26469;&#25345;&#32493;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#22914;&#20309;&#22312;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#20173;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#20998;&#21106;&#26159;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#21644;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#31579;&#36873;&#22823;&#35268;&#27169;&#12289;&#35814;&#32454;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#20154;&#31867;&#19987;&#23478;&#20250;&#20462;&#35746;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#30340;&#27880;&#37322;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#21017;&#36890;&#36807;&#23398;&#20064;&#36825;&#20123;&#20462;&#35746;&#30340;&#27880;&#37322;&#26469;&#25913;&#21892;&#20854;&#39044;&#27979;&#12290;&#36825;&#31181;&#20132;&#20114;&#36807;&#31243;&#19981;&#26029;&#22686;&#24378;&#27880;&#37322;&#30340;&#36136;&#37327;&#65292;&#30452;&#21040;&#19981;&#20877;&#38656;&#35201;&#19987;&#23478;&#36827;&#34892;&#37325;&#22823;&#20462;&#35746;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#21644;&#19987;&#23478;&#20462;&#35746;&#30340;&#27880;&#37322;&#26469;&#36845;&#20195;&#22320;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#12290;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#39118;&#38505;--&#22914;&#26524;&#20165;&#20351;&#29992;&#19987;&#23478;&#20462;&#35746;&#30340;&#31867;&#21035;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#20154;&#24037;&#26234;&#33021;&#24448;&#24448;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290; &#65288;2&#65289;&#24403;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#21644;&#19987;&#23478;&#20462;&#35746;&#30340;&#27880;&#37322;&#37325;&#26032;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#26102;&#23384;&#22312;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65307;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#27880;&#37322;&#65292;&#26032;&#27880;&#37322;&#30340;&#36129;&#29486;&#21487;&#33021;&#20250;&#34987;&#20302;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19423v1 Announce Type: cross  Abstract: Interactive segmentation, an integration of AI algorithms and human expertise, premises to improve the accuracy and efficiency of curating large-scale, detailed-annotated datasets in healthcare. Human experts revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from these revised annotations. This interactive process continues to enhance the quality of annotations until no major revision is needed from experts. The key challenge is how to leverage AI predicted and expert revised annotations to iteratively improve the AI. Two problems arise: (1) The risk of catastrophic forgetting--the AI tends to forget the previously learned classes if it is only retrained using the expert revised classes. (2) Computational inefficiency when retraining the AI using both AI predicted and expert revised annotations; moreover, given the dominant AI predicted annotations in the dataset, the contribution of newly rev
&lt;/p&gt;</description></item><item><title>PEM&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#65292;&#36890;&#36807;&#24341;&#20837;&#21407;&#22411;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#36816;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.19422</link><description>&lt;p&gt;
PEM&#65306;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
PEM: Prototype-based Efficient MaskFormer for Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19422
&lt;/p&gt;
&lt;p&gt;
PEM&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#65292;&#36890;&#36807;&#24341;&#20837;&#21407;&#22411;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#22312;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#65292;&#23427;&#20204;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#33719;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22914;&#35821;&#20041;&#20998;&#21106;&#21644;&#20840;&#26223;&#20998;&#21106;&#65292;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#65288;PEM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#36816;&#34892;&#30340;&#39640;&#25928;transformer&#26550;&#26500;&#12290;PEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#21033;&#29992;&#35270;&#35273;&#29305;&#24449;&#30340;&#20887;&#20313;&#24615;&#26469;&#38480;&#21046;&#35745;&#31639;&#24182;&#25552;&#39640;&#25928;&#29575;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;PEM&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#33021;&#22815;&#25552;&#21462;&#20855;&#26377;&#39640;&#35821;&#20041;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19422v1 Announce Type: cross  Abstract: Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high seman
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#21019;&#36896;&#24615;&#26426;&#21046;&#65292;&#24182;&#35299;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36873;&#25321;&#20449;&#24687;&#28304;&#20197;&#29983;&#25104;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#24615;&#21644;&#21019;&#36896;&#24615;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.19421</link><description>&lt;p&gt;
&#30693;&#35782;&#22609;&#36896;&#65306;&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#21019;&#36896;&#24615;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19421
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#21019;&#36896;&#24615;&#26426;&#21046;&#65292;&#24182;&#35299;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36873;&#25321;&#20449;&#24687;&#28304;&#20197;&#29983;&#25104;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#24615;&#21644;&#21019;&#36896;&#24615;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20449;&#24687;&#20256;&#25773;&#39046;&#22495;&#65292;&#25628;&#32034;&#24341;&#25806;&#25198;&#28436;&#30528;&#20851;&#38190;&#30340;&#35282;&#33394;&#65292;&#36830;&#25509;&#20449;&#24687;&#23547;&#25214;&#32773;&#21644;&#20449;&#24687;&#25552;&#20379;&#32773;&#12290;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#20986;&#29616;&#65292;&#20363;&#22914;&#24517;&#24212;&#32842;&#22825;&#65292;&#26631;&#24535;&#30528;&#25628;&#32034;&#29983;&#24577;&#31995;&#32479;&#30340;&#36827;&#21270;&#39134;&#36291;&#12290;&#23427;&#20204;&#23637;&#31034;&#20102;&#20803;&#35748;&#30693;&#33021;&#21147;&#65292;&#33021;&#22815;&#35299;&#37322;&#32593;&#32476;&#20449;&#24687;&#24182;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#21019;&#36896;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#23427;&#20204;&#30340;&#8220;&#35748;&#30693;&#8221;&#36807;&#31243;&#21464;&#24471;&#19981;&#36879;&#26126;&#65292;&#29978;&#33267;&#25361;&#25112;&#20102;&#35774;&#35745;&#24072;&#23545;&#20854;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#21078;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#25628;&#32034;&#24341;&#25806;&#65288;&#20855;&#20307;&#20026;&#24517;&#24212;&#32842;&#22825;&#65289;&#36873;&#25321;&#20449;&#24687;&#28304;&#20197;&#20316;&#20026;&#20854;&#21709;&#24212;&#30340;&#26426;&#21046;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#19982;&#26032;&#29256;&#24517;&#24212;&#30340;&#20114;&#21160;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#35760;&#24405;&#20102;&#23427;&#24341;&#29992;&#30340;&#32593;&#31449;&#20197;&#21450;&#20256;&#32479;&#25628;&#32034;&#24341;&#25806;&#21015;&#20986;&#30340;&#32593;&#31449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19421v1 Announce Type: cross  Abstract: In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their "cognitive" processes opaque, challenging even their designers' understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional sea
&lt;/p&gt;</description></item><item><title>&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#65292;&#20294;&#20854;&#26377;&#25928;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#32771;&#34385;&#20445;&#25345;&#28216;&#25103;&#21487;&#22788;&#29702;&#24615;&#20197;&#21450;&#36991;&#20813;&#21508;&#31181;&#31639;&#27861;&#30340;&#38519;&#38449;&#12290;</title><link>https://arxiv.org/abs/2402.19420</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29702;&#35299;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19420
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#65292;&#20294;&#20854;&#26377;&#25928;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#32771;&#34385;&#20445;&#25345;&#28216;&#25103;&#21487;&#22788;&#29702;&#24615;&#20197;&#21450;&#36991;&#20813;&#21508;&#31181;&#31639;&#27861;&#30340;&#38519;&#38449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#22914;&#39057;&#35889;&#25293;&#21334;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#25293;&#21334;&#38590;&#20197;&#22312;&#29702;&#35770;&#19978;&#29702;&#35299;&#65292;&#20351;&#24471;&#31454;&#26631;&#32773;&#24456;&#38590;&#20915;&#23450;&#22914;&#20309;&#34892;&#21160;&#20197;&#21450;&#35774;&#35745;&#32773;&#24456;&#38590;&#20248;&#21270;&#25293;&#21334;&#35268;&#21017;&#20197;&#30830;&#20445;&#29702;&#24819;&#30340;&#32467;&#26524;&#65292;&#22914;&#39640;&#25910;&#20837;&#25110;&#31119;&#21033;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#33021;&#22815;&#29992;&#20110;&#29702;&#35299;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#65292;&#37492;&#20110;&#36825;&#20123;&#31639;&#27861;&#26368;&#36817;&#22312;&#20854;&#20182;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#23454;&#35777;&#25104;&#21151;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30830;&#23454;&#21487;&#20197;&#21463;&#30410;&#20110;&#25293;&#21334;&#20998;&#26512;&#65292;&#20294;&#26377;&#25928;&#37096;&#32626;&#24182;&#19981;&#23481;&#26131;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#20445;&#25345;&#32467;&#26524;&#28216;&#25103;&#21487;&#22788;&#29702;&#30340;&#24314;&#27169;&#20915;&#31574;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#35832;&#22914;&#20449;&#24687;&#19981;&#23436;&#20840;&#25110;&#31454;&#26631;&#32773;&#38388;&#19981;&#23545;&#31216;&#31561;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#36991;&#20813;&#21508;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#38519;&#38449;&#65292;&#22914;&#20309;&#20811;&#26381;&#25361;&#25112;&#20197;&#21450;&#22914;&#20309;&#24212;&#23545;&#21508;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19420v1 Announce Type: cross  Abstract: Iterative combinatorial auctions are widely used in high stakes settings such as spectrum auctions. Such auctions can be hard to understand analytically, making it difficult for bidders to determine how to behave and for designers to optimize auction rules to ensure desirable outcomes such as high revenue or welfare. In this paper, we investigate whether multi-agent reinforcement learning (MARL) algorithms can be used to understand iterative combinatorial auctions, given that these algorithms have recently shown empirical success in several other domains. We find that MARL can indeed benefit auction analysis, but that deploying it effectively is nontrivial. We begin by describing modelling decisions that keep the resulting game tractable without sacrificing important features such as imperfect information or asymmetry between bidders. We also discuss how to navigate pitfalls of various MARL algorithms, how to overcome challenges in ver
&lt;/p&gt;</description></item><item><title>&#22320;&#29702;&#30693;&#35782;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#65292;&#20294;&#26356;&#22823;&#30340;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.19406</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#22320;&#29702;&#34920;&#31034;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Scaling Laws of Geographical Representation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19406
&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#30693;&#35782;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#65292;&#20294;&#26356;&#22823;&#30340;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#34987;&#35777;&#26126;&#22312;&#20854;&#38544;&#34255;&#34920;&#31034;&#20013;&#23884;&#20837;&#20102;&#22320;&#29702;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#23558;&#36825;&#19968;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#35821;&#35328;&#27169;&#22411;&#35268;&#27169;&#25193;&#22823;&#26102;&#22320;&#29702;&#30693;&#35782;&#30340;&#28436;&#21270;&#65292;&#25552;&#20986;&#22635;&#34917;&#29616;&#26377;&#21644;&#26368;&#36817;&#25991;&#29486;&#20043;&#38388;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#24494;&#23567;&#27169;&#22411;&#65292;&#22320;&#29702;&#30693;&#35782;&#20063;&#26159;&#21487;&#35266;&#27979;&#30340;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19406v1 Announce Type: cross  Abstract: Language models have long been shown to embed geographical information in their hidden representations. This line of work has recently been revisited by extending this result to Large Language Models (LLMs). In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when scaling language models. We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size. Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Forchestra&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21508;&#31181;&#29289;&#21697;&#30340;&#26410;&#26469;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#35268;&#27169;&#21644;&#27867;&#21270;&#33021;&#21147;&#19978;&#22343;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.19402</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#36801;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#29992;&#20110;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Transferable Time Series Prediction Framework for Demand Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Forchestra&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21508;&#31181;&#29289;&#21697;&#30340;&#26410;&#26469;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#35268;&#27169;&#21644;&#27867;&#21270;&#33021;&#21147;&#19978;&#22343;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#35768;&#22810;&#19994;&#21153;&#38382;&#39064;&#20013;&#26368;&#22522;&#26412;&#19988;&#26368;&#26222;&#36941;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#21253;&#25324;&#38656;&#27714;&#39044;&#27979;&#21644;&#29289;&#27969;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#30001;&#20110;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38590;&#20197;&#25193;&#23637;&#20854;&#27169;&#22411;&#22823;&#23567;&#65292;&#23548;&#33268;&#20854;&#27169;&#22411;&#35268;&#27169;&#36739;&#23567;&#19988;&#34920;&#29616;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Forecasting orchestra (Forchestra)&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21508;&#31181;&#29289;&#21697;&#30340;&#26410;&#26469;&#38656;&#27714;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#21487;&#25193;&#23637;&#33267;&#39640;&#36798;0.8&#20159;&#20010;&#21442;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#38646;&#26679;&#26412;&#26041;&#24335;&#35780;&#20272;&#19979;&#28216;&#25968;&#25454;&#38598;&#26102;&#20063;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#25968;&#25454;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#30740;&#31350;&#65292;&#20197;&#20998;&#26512;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19402v1 Announce Type: cross  Abstract: Time series forecasting is one of the most essential and ubiquitous tasks in many business problems, including demand forecasting and logistics optimization. Traditional time series forecasting methods, however, have resulted in small models with limited expressive power because they have difficulty in scaling their model size up while maintaining high accuracy. In this paper, we propose Forecasting orchestra (Forchestra), a simple but powerful framework capable of accurately predicting future demand for a diverse range of items. We empirically demonstrate that the model size is scalable to up to 0.8 billion parameters. The proposed method not only outperforms existing forecasting models with a significant margin, but it could generalize well to unseen data points when evaluated in a zero-shot fashion on downstream datasets. Last but not least, we present extensive qualitative and quantitative studies to analyze how the proposed model 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>OpenMedLM &#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24179;&#21488;&#65292;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#33021;&#22815;&#36229;&#36234;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#21307;&#23398;&#22522;&#20934;&#19978;&#30340; SOTA &#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19371</link><description>&lt;p&gt;
OpenMedLM&#65306;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#65292;&#25552;&#31034;&#24037;&#31243;&#21487;&#20197;&#32988;&#36807;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19371
&lt;/p&gt;
&lt;p&gt;
OpenMedLM &#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24179;&#21488;&#65292;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#33021;&#22815;&#36229;&#36234;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#21307;&#23398;&#22522;&#20934;&#19978;&#30340; SOTA &#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs &#22312;&#23436;&#25104;&#19968;&#31995;&#21015;&#19987;&#38376;&#20219;&#21153;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#26469;&#25193;&#22823;&#23545;&#21307;&#23398;&#30693;&#35782;&#30340;&#20844;&#24179;&#35775;&#38382;&#12290;&#22823;&#22810;&#25968;&#21307;&#23398; LLMs &#37117;&#28041;&#21450;&#22823;&#37327;&#24494;&#35843;&#65292;&#21033;&#29992;&#19987;&#38376;&#30340;&#21307;&#23398;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#25104;&#26412;&#39640;&#26114;&#12290;&#35768;&#22810;&#34920;&#29616;&#21069;&#21015;&#30340; LLMs &#26159;&#19987;&#26377;&#30340;&#65292;&#20182;&#20204;&#30340;&#35775;&#38382;&#20165;&#38480;&#20110;&#23569;&#25968;&#30740;&#31350;&#22242;&#20307;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#65288;OS&#65289;&#27169;&#22411;&#20195;&#34920;&#20102;&#21307;&#23398; LLMs &#30340;&#19968;&#20010;&#37325;&#35201;&#22686;&#38271;&#39046;&#22495;&#65292;&#30001;&#20110;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#20197;&#21450;&#25552;&#20379;&#21355;&#29983;&#20445;&#20581;&#25152;&#38656;&#30340;&#36879;&#26126;&#24230;&#21644;&#21512;&#35268;&#24615;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; OpenMedLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25552;&#31034;&#24179;&#21488;&#65292;&#20026;&#21307;&#23398;&#22522;&#20934;&#19978;&#30340; OS LLMs &#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#21307;&#23398;&#22522;&#20934;&#65288;MedQA&#12289;MedMCQA&#12289;PubMedQA&#12289;MMLU &#21307;&#23398;&#23376;&#38598;&#65289;&#19978;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015; OS &#22522;&#30784; LLMs&#65288;7B-70B&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31995;&#21015;&#25552;&#31034;&#31574;&#30053;&#65292;&#21253;&#25324;&#38646;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19371v1 Announce Type: cross  Abstract: LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-s
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;&#20013;&#26377;&#26395;&#25552;&#21319;&#35843;&#26597;&#25928;&#29575;&#65292;&#25913;&#21892;&#21487;&#36861;&#28335;&#24615;&#65292;&#24182;&#32531;&#35299;&#25191;&#27861;&#26426;&#26500;&#38754;&#20020;&#30340;&#25216;&#26415;&#21644;&#21496;&#27861;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2402.19366</link><description>&lt;p&gt;
SoK: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#39640;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19366
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;&#20013;&#26377;&#26395;&#25552;&#21319;&#35843;&#26597;&#25928;&#29575;&#65292;&#25913;&#21892;&#21487;&#36861;&#28335;&#24615;&#65292;&#24182;&#32531;&#35299;&#25191;&#27861;&#26426;&#26500;&#38754;&#20020;&#30340;&#25216;&#26415;&#21644;&#21496;&#27861;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38656;&#35201;&#25968;&#23383;&#21462;&#35777;&#20998;&#26512;&#30340;&#26696;&#20214;&#25968;&#37327;&#22686;&#38271;&#65292;&#23545;&#25191;&#27861;&#26426;&#26500;&#21450;&#26102;&#36827;&#34892;&#35843;&#26597;&#30340;&#33021;&#21147;&#20135;&#29983;&#20102;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#36825;&#31687;&#31995;&#32479;&#21270;&#30693;&#35782;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;&#20013;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;&#23545;&#29616;&#26377;&#30340;&#25968;&#23383;&#21462;&#35777;&#27169;&#22411;&#12289;&#24037;&#20855;&#12289;LLMs&#12289;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#22312;&#35843;&#26597;&#20013;&#21033;&#29992;LLMs&#30340;&#20840;&#38754;&#25991;&#29486;&#32508;&#36848;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32508;&#36848;&#30830;&#23450;&#20102;&#29616;&#26377;&#25968;&#23383;&#21462;&#35777;&#27969;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#25972;&#21512;LLMs&#30340;&#38556;&#30861;&#21644;&#21487;&#33021;&#24615;&#12290;&#26368;&#32456;&#65292;&#30740;&#31350;&#26029;&#35328;&#65292;&#22312;&#36866;&#24403;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#25968;&#23383;&#21462;&#35777;&#20013;&#37319;&#29992;LLMs&#26377;&#26395;&#25552;&#21319;&#35843;&#26597;&#25928;&#29575;&#65292;&#25913;&#21892;&#21487;&#36861;&#28335;&#24615;&#65292;&#24182;&#32531;&#35299;&#25191;&#27861;&#26426;&#26500;&#38754;&#20020;&#30340;&#25216;&#26415;&#21644;&#21496;&#27861;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19366v1 Announce Type: cross  Abstract: The growing number of cases requiring digital forensic analysis raises concerns about law enforcement's ability to conduct investigations promptly. Consequently, this systemisation of knowledge paper delves into the potential and effectiveness of integrating Large Language Models (LLMs) into digital forensic investigation to address these challenges. A thorough literature review is undertaken, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the utilisation of LLMs in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and possibilities of incorporating LLMs. In conclusion, the study asserts that the adoption of LLMs in digital forensics, with appropriate constraints, holds the potential to enhance investigation efficiency, improve traceability, and alleviate technical and judicial barriers faced by law enforcement e
&lt;/p&gt;</description></item><item><title>LLM&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#23384;&#22312;&#27700;&#21360;&#31363;&#21462;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#19979;&#36890;&#36807;&#27450;&#39575;&#21644;&#25830;&#38500;&#25915;&#20987;&#30772;&#35299;&#20043;&#21069;&#35748;&#20026;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;</title><link>https://arxiv.org/abs/2402.19361</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27700;&#21360;&#31363;&#21462;
&lt;/p&gt;
&lt;p&gt;
Watermark Stealing in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19361
&lt;/p&gt;
&lt;p&gt;
LLM&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#23384;&#22312;&#27700;&#21360;&#31363;&#21462;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#19979;&#36890;&#36807;&#27450;&#39575;&#21644;&#25830;&#38500;&#25915;&#20987;&#30772;&#35299;&#20043;&#21069;&#35748;&#20026;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#27700;&#21360;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26816;&#27979;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#20105;&#36777;&#31216;&#24403;&#21069;&#26041;&#26696;&#21487;&#33021;&#24050;&#32463;&#21487;&#20197;&#37096;&#32626;&#65292;&#25105;&#20204;&#35748;&#20026;&#27700;&#21360;&#31363;&#21462;&#65288;WS&#65289;&#26159;&#36825;&#20123;&#26041;&#26696;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26597;&#35810;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#30340;API&#26469;&#36817;&#20284;&#36870;&#21521;&#27700;&#21360;&#65292;&#20174;&#32780;&#23454;&#29616;&#23454;&#29992;&#30340;&#27450;&#39575;&#25915;&#20987;&#65292;&#21516;&#26102;&#22823;&#24133;&#22686;&#21152;&#20102;&#20043;&#21069;&#26410;&#34987;&#27880;&#24847;&#21040;&#30340;&#25830;&#38500;&#25915;&#20987;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23558;&#20854;&#29992;&#20110;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#27450;&#39575;&#21644;&#25830;&#38500;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#38656;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#65292;&#25915;&#20987;&#32773;&#23601;&#33021;&#22815;&#27450;&#39575;&#24182;&#25830;&#38500;&#20043;&#21069;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#24179;&#22343;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#20851;&#20110;LLM&#27700;&#21360;&#25216;&#26415;&#30340;&#24120;&#35265;&#20449;&#24565;&#65292;&#24378;&#35843;&#20102;&#26356;&#21152;&#20581;&#22766;&#26041;&#26696;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19361v1 Announce Type: cross  Abstract: LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as suggested in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We mak
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22235;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#27169;&#24577;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.19348</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22478;&#24066;&#35745;&#31639;&#20013;&#30340;&#36328;&#22495;&#25968;&#25454;&#34701;&#21512;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19348
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22235;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#27169;&#24577;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22478;&#24066;&#30340;&#19981;&#26029;&#34028;&#21187;&#21457;&#23637;&#65292;&#22478;&#24066;&#35745;&#31639;&#20316;&#20026;&#19968;&#38376;&#20851;&#38190;&#23398;&#31185;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#65288;&#22914;&#22320;&#29702;&#12289;&#20132;&#36890;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#29615;&#22659;&#25968;&#25454;&#65289;&#21644;&#27169;&#24577;&#65288;&#22914;&#26102;&#31354;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65289;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#30340;&#21147;&#37327;&#65292;&#25104;&#20026;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#27491;&#22312;&#35265;&#35777;&#19968;&#31181;&#21033;&#29992;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20419;&#36827;&#26234;&#24935;&#22478;&#24066;&#20013;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#30340;&#36235;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20221;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#19987;&#38376;&#20026;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35843;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#30740;&#31350;&#25968;&#25454;&#35270;&#35282;&#65292;&#20197;&#29702;&#35299;&#27599;&#31181;&#27169;&#24577;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#20316;&#29992;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#35770;&#20998;&#31867;&#20026;&#22235;&#22823;&#20027;&#35201;&#31867;&#21035;&#65306;&#22522;&#20110;&#29305;&#24449;&#12289;&#22522;&#20110;&#23545;&#40784;&#12289;&#22522;&#20110;&#23545;&#27604;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#34701;&#21512;&#26041;&#27861;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#22810;&#27169;&#24577;&#22478;&#24066;&#24212;&#29992;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19348v1 Announce Type: cross  Abstract: As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applicatio
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#25991;&#21270;&#22270;&#20687;&#30340;&#20855;&#26377;&#23450;&#20301;&#24863;&#30693;&#30693;&#35782;&#22686;&#24378;&#39640;&#32423;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;</title><link>https://arxiv.org/abs/2402.19339</link><description>&lt;p&gt;
&#32541;&#21512;&#38388;&#38553;&#65306;&#23558;&#20855;&#26377;&#23450;&#20301;&#24863;&#30693;&#30693;&#35782;&#30340;&#19982;&#35270;&#35273;&#36716;&#25442;&#22120;&#34701;&#21512;&#20197;&#36827;&#34892;&#39640;&#32423;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19339
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#25991;&#21270;&#22270;&#20687;&#30340;&#20855;&#26377;&#23450;&#20301;&#24863;&#30693;&#30693;&#35782;&#22686;&#24378;&#39640;&#32423;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#21160;&#21270;&#39640;&#32423;&#22270;&#20687;&#29702;&#35299;&#30340;&#22686;&#38271;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#25277;&#35937;&#27010;&#24565;&#65288;AC&#65289;&#26041;&#38754;&#65292;&#24378;&#35843;&#20102;&#21019;&#26032;&#21644;&#26356;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23558;&#20256;&#32479;&#30340;&#28145;&#24230;&#35270;&#35273;&#26041;&#27861;&#19982;&#20154;&#31867;&#29992;&#20110;&#35299;&#37322;&#22270;&#20687;&#30340;&#24494;&#22937;&#12289;&#20381;&#36182;&#20110;&#29615;&#22659;&#30340;&#30693;&#35782;&#30456;&#21327;&#35843;&#65292;&#20197;&#20415;&#22312;&#22797;&#26434;&#30340;&#35821;&#20041;&#23618;&#38754;&#19978;&#35299;&#37322;&#22270;&#20687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25991;&#21270;&#22270;&#20687;&#30340;&#20855;&#26377;&#23450;&#20301;&#24863;&#30693;&#30693;&#35782;&#26469;&#22686;&#24378;AC&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#33258;&#21160;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#24863;&#30693;&#35821;&#20041;&#21333;&#20803;&#65292;&#28982;&#21518;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#21644;&#25972;&#21512;&#21040;ARTstract&#30693;&#35782;&#22270;&#65288;AKG&#65289;&#20013;&#12290;&#35813;&#36164;&#28304;&#25429;&#33719;&#20102;&#20174;14,000&#22810;&#20010;&#25991;&#21270;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#20855;&#26377;&#23450;&#20301;&#24863;&#30693;&#35821;&#20041;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#39640;&#32423;&#35821;&#35328;&#26694;&#26550;&#22686;&#24378;&#20102;AKG&#12290;&#25105;&#20204;&#35745;&#31639;&#30693;&#35782;&#22270;&#23884;&#20837;&#24182;&#23581;&#35797;&#30456;&#23545;&#34920;&#31034;&#21644;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19339v1 Announce Type: cross  Abstract: The increasing demand for automatic high-level image understanding, particularly in detecting abstract concepts (AC) within images, underscores the necessity for innovative and more interpretable approaches. These approaches need to harmonize traditional deep vision methods with the nuanced, context-dependent knowledge humans employ to interpret images at intricate semantic levels. In this work, we leverage situated perceptual knowledge of cultural images to enhance performance and interpretability in AC image classification. We automatically extract perceptual semantic units from images, which we then model and integrate into the ARTstract Knowledge Graph (AKG). This resource captures situated perceptual semantics gleaned from over 14,000 cultural images labeled with ACs. Additionally, we enhance the AKG with high-level linguistic frames. We compute KG embeddings and experiment with relative representations and hybrid approaches that 
&lt;/p&gt;</description></item><item><title>RL-GPT &#26159;&#19968;&#20010;&#20004;&#32423;&#20998;&#23618;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#24930;&#36895;&#20195;&#29702;&#21644;&#24555;&#36895;&#20195;&#29702;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#32534;&#30721;&#20219;&#21153;&#65292;&#22312;Minecraft&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.19299</link><description>&lt;p&gt;
RL-GPT: &#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#20195;&#30721;&#20316;&#20026;&#31574;&#30053;&#36827;&#34892;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
RL-GPT: Integrating Reinforcement Learning and Code-as-policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19299
&lt;/p&gt;
&lt;p&gt;
RL-GPT &#26159;&#19968;&#20010;&#20004;&#32423;&#20998;&#23618;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#24930;&#36895;&#20195;&#29702;&#21644;&#24555;&#36895;&#20195;&#29702;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#32534;&#30721;&#20219;&#21153;&#65292;&#22312;Minecraft&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#22312;&#21033;&#29992;&#32534;&#30721;&#26102;&#21508;&#31181;&#24037;&#20855;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#36923;&#36753;&#21644;&#31934;&#30830;&#25511;&#21046;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;&#39640;&#23618;&#35268;&#21010;&#36866;&#23452;&#20110;&#30452;&#25509;&#32534;&#30721;&#65292;&#32780;&#20302;&#23618;&#21160;&#20316;&#36890;&#24120;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#30340;&#32454;&#21270;&#65292;&#27604;&#22914;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#20026;&#20102;&#26080;&#32541;&#25972;&#21512;&#36825;&#20004;&#31181;&#27169;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#23618;&#26694;&#26550;RL-GPT&#65292;&#21253;&#25324;&#19968;&#20010;&#24930;&#36895;&#20195;&#29702;&#21644;&#19968;&#20010;&#24555;&#36895;&#20195;&#29702;&#12290;&#24930;&#36895;&#20195;&#29702;&#20998;&#26512;&#36866;&#21512;&#32534;&#30721;&#30340;&#21160;&#20316;&#65292;&#32780;&#24555;&#36895;&#20195;&#29702;&#25191;&#34892;&#32534;&#30721;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#35299;&#26377;&#25928;&#22320;&#20351;&#27599;&#20010;&#20195;&#29702;&#19987;&#27880;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#22312;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#20013;&#35777;&#26126;&#26159;&#38750;&#24120;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20256;&#32479;&#30340;RL&#26041;&#27861;&#21644;&#29616;&#26377;&#30340;GPT&#20195;&#29702;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;&#22312;Minecraft&#28216;&#25103;&#20013;&#65292;&#23427;&#22312;RTX3090&#19978;&#22312;&#19968;&#22825;&#20869;&#36805;&#36895;&#33719;&#24471;&#20102;&#38075;&#30707;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#25152;&#26377;&#35774;&#35745;&#26041;&#38754;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19299v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all desig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;UMAP&#25216;&#26415;&#30340;&#26032;&#22411;&#25925;&#38556;&#27169;&#24335;&#35786;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#31995;&#32479;&#20013;&#22810;&#31181;&#25925;&#38556;&#27169;&#24335;&#23548;&#33268;&#30340;&#19981;&#21516;&#38477;&#35299;&#36335;&#24452;&#65292;&#25552;&#39640;&#25925;&#38556;&#27169;&#24335;&#30340;&#20934;&#30830;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.19294</link><description>&lt;p&gt;
&#26410;&#30693;&#25925;&#38556;&#27169;&#24335;&#19979;&#30340;&#38477;&#35299;&#24314;&#27169;&#19982;&#39044;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;UMAP&#25216;&#26415;&#30340;&#26032;&#22411;&#25925;&#38556;&#27169;&#24335;&#35786;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#31995;&#32479;&#20013;&#22810;&#31181;&#25925;&#38556;&#27169;&#24335;&#23548;&#33268;&#30340;&#19981;&#21516;&#38477;&#35299;&#36335;&#24452;&#65292;&#25552;&#39640;&#25925;&#38556;&#27169;&#24335;&#30340;&#20934;&#30830;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25805;&#20316;&#21333;&#20803;&#32463;&#24120;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#32463;&#21382;&#21508;&#31181;&#25925;&#38556;&#27169;&#24335;&#65292;&#23548;&#33268;&#19981;&#21516;&#30340;&#38477;&#35299;&#36335;&#24452;&#12290;&#20381;&#36182;&#20110;&#22312;&#21333;&#19968;&#25925;&#38556;&#27169;&#24335;&#19978;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36328;&#22810;&#20010;&#25925;&#38556;&#27169;&#24335;&#30340;&#27867;&#21270;&#24615;&#33021;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#35782;&#21035;&#25925;&#38556;&#27169;&#24335;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#39044;&#27979;&#26041;&#27861;&#35201;&#20040;&#22312;&#38477;&#35299;&#36807;&#31243;&#20013;&#24573;&#30053;&#25925;&#38556;&#27169;&#24335;&#65292;&#35201;&#20040;&#20551;&#23450;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#26631;&#31614;&#65292;&#32780;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#36825;&#20123;&#26631;&#31614;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#20256;&#24863;&#22120;&#20449;&#21495;&#30340;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#20851;&#31995;&#20351;&#24471;&#20934;&#30830;&#35782;&#21035;&#25925;&#38556;&#27169;&#24335;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25925;&#38556;&#27169;&#24335;&#35786;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#31181;&#21517;&#20026;UMAP&#65288;Uniform Manifold Approximation and Projection&#65289;&#30340;&#38477;&#32500;&#25216;&#26415;&#23558;&#27599;&#20010;&#21333;&#20803;&#30340;&#38477;&#35299;&#36712;&#36857;&#25237;&#24433;&#21644;&#21487;&#35270;&#21270;&#21040;&#36739;&#20302;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#38477;&#35299;&#36712;&#36857;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19294v1 Announce Type: cross  Abstract: Operating units often experience various failure modes in complex systems, leading to distinct degradation paths. Relying on a prognostic model trained on a single failure mode may lead to poor generalization performance across multiple failure modes. Therefore, accurately identifying the failure mode is of critical importance. Current prognostic approaches either ignore failure modes during degradation or assume known failure mode labels, which can be challenging to acquire in practice. Moreover, the high dimensionality and complex relations of sensor signals make it challenging to identify the failure modes accurately. To address these issues, we propose a novel failure mode diagnosis method that leverages a dimension reduction technique called UMAP (Uniform Manifold Approximation and Projection) to project and visualize each unit's degradation trajectory into a lower dimension. Then, using these degradation trajectories, we develop 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.19267</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#25351;&#23548;&#65306;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#21487;&#20197;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#20934;&#30830;&#32763;&#35793;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#21477;&#23376;&#12290;&#33719;&#24471;&#21644;&#32763;&#35793;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#34429;&#28982;&#25104;&#26412;&#39640;&#26114;&#65292;&#20294;&#23545;&#20110;&#39640;&#36136;&#37327;&#32763;&#35793;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#25214;&#21040;&#26368;&#8220;&#26377;&#25928;&#8221;&#30340;&#25968;&#25454;&#25104;&#20026;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#30340;&#23454;&#29992;&#31574;&#30053;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#8220;&#36866;&#24403;&#22256;&#38590;&#30340;&#25968;&#25454;&#8221;&#26469;&#25214;&#21040;&#36825;&#20123;&#26377;&#25928;&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#25968;&#25454;&#19981;&#24212;&#36807;&#20110;&#22256;&#38590;&#25110;&#36807;&#20110;&#31616;&#21333;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24314;&#31435;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26631;&#20934;&#20173;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#8220;&#36866;&#24403;&#22256;&#38590;&#24230;&#8221;&#21487;&#33021;&#22240;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#39046;&#22495;&#32780;&#24322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#8216;Capturing Perplexing Named Entities&#8217;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19267v1 Announce Type: cross  Abstract: Employing extensive datasets enables the training of multilingual machine translation models; however, these models often fail to accurately translate sentences within specialized domains. Although obtaining and translating domain-specific data incurs high costs, it is inevitable for high-quality translations. Hence, finding the most 'effective' data with an unsupervised setting becomes a practical strategy for reducing labeling costs. Recent research indicates that this effective data could be found by selecting 'properly difficult data' based on its volume. This means the data should not be excessively challenging or overly simplistic, especially if the amount of data is limited. However, we found that establishing a criterion for unsupervised data selection remains challenging, as the 'proper difficulty' might vary based on the data domain being trained on. We introduce a novel unsupervised data selection method, 'Capturing Perplexi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#65292;&#20174;POMDP&#25191;&#34892;&#30165;&#36857;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#21551;&#21457;&#24335;&#65292;&#20197;&#25351;&#23548;&#25919;&#31574;&#36873;&#25321;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.19265</link><description>&lt;p&gt;
&#22312;POMDPs&#20013;&#23398;&#20064;&#36923;&#36753;&#35268;&#33539;&#20197;&#25351;&#23548;&#25919;&#31574;&#65306;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Logic Specifications for Policy Guidance in POMDPs: an Inductive Logic Programming Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19265
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#65292;&#20174;POMDP&#25191;&#34892;&#30165;&#36857;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#21551;&#21457;&#24335;&#65292;&#20197;&#25351;&#23548;&#25919;&#31574;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#35268;&#21010;&#26694;&#26550;&#65292;&#20801;&#35768;&#23558;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#20026;&#20449;&#24565;&#27010;&#29575;&#20998;&#24067;&#12290;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#30340;&#36817;&#20284;&#27714;&#35299;&#22120;&#26174;&#31034;&#20986;&#24456;&#22823;&#25104;&#21151;&#65292;&#20197;&#25918;&#23485;&#35745;&#31639;&#38656;&#27714;&#24182;&#25191;&#34892;&#22312;&#32447;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#25193;&#23637;&#21040;&#20855;&#26377;&#35768;&#22810;&#21160;&#20316;&#21644;&#38271;&#26399;&#35268;&#21010;&#35270;&#37326;&#30340;&#22797;&#26434;&#29616;&#23454;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#30340;&#20851;&#38190;&#28857;&#26159;&#36890;&#36807;&#23450;&#21046;&#29305;&#23450;&#24212;&#29992;&#22495;&#30340;&#39046;&#22495;&#30456;&#20851;&#31574;&#30053;&#21551;&#21457;&#26469;&#24341;&#23548;&#34892;&#21160;&#36873;&#25321;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20174;&#30001;&#20219;&#20309;&#27714;&#35299;&#22120;&#29983;&#25104;&#30340;POMDP&#25191;&#34892;&#30165;&#36857;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#21551;&#21457;&#24335;&#12290;&#25105;&#20204;&#23558;&#20449;&#24565;-&#21160;&#20316;&#23545;&#36716;&#25442;&#20026;&#36923;&#36753;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#25968;&#25454;&#21644;&#26102;&#38388;&#39640;&#25928;&#30340;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#20449;&#24565;&#30340;&#31574;&#30053;&#35268;&#33539;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#22312;&#32447;&#21551;&#21457;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19265v1 Announce Type: new  Abstract: Partially Observable Markov Decision Processes (POMDPs) are a powerful framework for planning under uncertainty. They allow to model state uncertainty as a belief probability distribution. Approximate solvers based on Monte Carlo sampling show great success to relax the computational demand and perform online planning. However, scaling to complex realistic domains with many actions and long planning horizons is still a major challenge, and a key point to achieve good performance is guiding the action-selection process with domain-dependent policy heuristics which are tailored for the specific application domain. We propose to learn high-quality heuristics from POMDP traces of executions generated by any solver. We convert the belief-action pairs to a logical semantics, and exploit data- and time-efficient Inductive Logic Programming (ILP) to generate interpretable belief-based policy specifications, which are then used as online heuristi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#39062;&#30340; SegPatch &#33258;&#21160;&#26001;&#22359;&#25552;&#21462;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#23545;&#33034;&#26609; X &#23556;&#32447;&#20013;&#39592;&#36184;&#30340;&#33258;&#21160;&#21270;&#26816;&#27979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040; 84.5\%&#65292;&#27604;&#22522;&#32447;&#26041;&#27861;&#39640;&#20986; 9.5%&#65292;&#26377;&#26395;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#21152;&#36895;&#39592;&#36184;&#35782;&#21035;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.19263</link><description>&lt;p&gt;
&#36890;&#36807;&#23545; X &#23556;&#32447;&#19978;&#30340;&#40092;&#26377;&#26631;&#27880;&#30340;&#24378;&#22823;&#26001;&#22359;&#25552;&#21462;&#26469;&#26816;&#27979;&#33034;&#26609;&#39592;&#36184;
&lt;/p&gt;
&lt;p&gt;
Spinal Osteophyte Detection via Robust Patch Extraction on minimally annotated X-rays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19263
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340; SegPatch &#33258;&#21160;&#26001;&#22359;&#25552;&#21462;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#23545;&#33034;&#26609; X &#23556;&#32447;&#20013;&#39592;&#36184;&#30340;&#33258;&#21160;&#21270;&#26816;&#27979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040; 84.5\%&#65292;&#27604;&#22522;&#32447;&#26041;&#27861;&#39640;&#20986; 9.5%&#65292;&#26377;&#26395;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#21152;&#36895;&#39592;&#36184;&#35782;&#21035;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#33410;&#28814;&#30340;&#21457;&#23637;&#21644;&#36827;&#23637;&#19982;&#39592;&#36184;&#23494;&#20999;&#30456;&#20851;&#65292;&#39592;&#36184;&#26159;&#23567;&#32780;&#38590;&#20197;&#23519;&#35273;&#30340;&#39592;&#22686;&#29983;&#29289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#33034;&#26609; X &#23556;&#32447;&#20013;&#33258;&#21160;&#26816;&#27979;&#33034;&#26609;&#39592;&#36184;&#30340;&#39318;&#27425;&#21162;&#21147;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#26001;&#22359;&#25552;&#21462;&#36807;&#31243;&#65292;&#31216;&#20026; SegPatch&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#26894;&#39592;&#20998;&#21106;&#21644;&#25513;&#27169;&#36718;&#24275;&#30340;&#25193;&#22823;&#12290;&#26368;&#32456;&#33719;&#24471;&#20102; 84.5\% &#30340;&#26368;&#32456;&#26001;&#22359;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#27604;&#22522;&#32447;&#29926;&#29255;&#21270;&#26001;&#22359;&#29983;&#25104;&#25216;&#26415;&#39640;&#20986; 9.5\%&#12290;&#36825;&#34920;&#26126;&#65292;&#21363;&#20351;&#26377;&#38480;&#30340;&#27880;&#37322;&#65292;SegPatch &#20063;&#21487;&#20197;&#20026;&#39592;&#36184;&#31561;&#24494;&#23567;&#32467;&#26500;&#30340;&#26816;&#27979;&#25552;&#20379;&#20248;&#36234;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#21152;&#24555;&#25163;&#21160;&#35782;&#21035;&#33034;&#26609; X &#23556;&#32447;&#20013;&#30340;&#39592;&#36184;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19263v1 Announce Type: cross  Abstract: The development and progression of arthritis is strongly associated with osteophytes, which are small and elusive bone growths. This paper presents one of the first efforts towards automated spinal osteophyte detection in spinal X-rays. A novel automated patch extraction process, called SegPatch, has been proposed based on deep learning-driven vertebrae segmentation and the enlargement of mask contours. A final patch classification accuracy of 84.5\% is secured, surpassing a baseline tiling-based patch generation technique by 9.5%. This demonstrates that even with limited annotations, SegPatch can deliver superior performance for detection of tiny structures such as osteophytes. The proposed approach has potential to assist clinicians in expediting the process of manually identifying osteophytes in spinal X-ray.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#20154;&#31867;&#21270;&#36712;&#36857;&#39044;&#27979;&#65288;HLTP&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#21463;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#30340;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#30340;&#36816;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.19251</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#22522;&#20110;&#35748;&#30693;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#20154;&#31867;&#21270;&#36712;&#36857;&#39044;&#27979;&#65288;HLTP&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#21463;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#30340;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#25216;&#26415;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#30340;&#36816;&#21160;&#23545;&#20110;&#30830;&#20445;&#23433;&#20840;&#21644;&#36816;&#34892;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#34701;&#20837;&#20154;&#31867;&#20915;&#31574;&#27934;&#23519;&#21147;&#20351;AV&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20854;&#20182;&#36710;&#36742;&#30340;&#28508;&#22312;&#21160;&#20316;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#31867;&#21270;&#36712;&#36857;&#39044;&#27979;&#65288;HLTP&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#21463;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#30340;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#12290;HLTP&#27169;&#22411;&#38598;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#12290;&#20855;&#26377;&#33258;&#36866;&#24212;&#35270;&#35273;&#25159;&#21306;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#27169;&#25311;&#20102;&#20154;&#33041;&#30340;&#35270;&#35273;&#22788;&#29702;&#65292;&#29305;&#21035;&#26159;&#26517;&#21494;&#21644;&#39070;&#21494;&#30340;&#21151;&#33021;&#12290; &#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#20851;&#27880;&#23454;&#26102;&#20132;&#20114;&#21644;&#20915;&#31574;&#21046;&#23450;&#65292;&#24182;&#19982;&#21069;&#39069;&#21494;&#21644;&#39030;&#21494;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19251v1 Announce Type: new  Abstract: In autonomous vehicle (AV) technology, the ability to accurately predict the movements of surrounding vehicles is paramount for ensuring safety and operational efficiency. Incorporating human decision-making insights enables AVs to more effectively anticipate the potential actions of other vehicles, significantly improving prediction accuracy and responsiveness in dynamic environments. This paper introduces the Human-Like Trajectory Prediction (HLTP) model, which adopts a teacher-student knowledge distillation framework inspired by human cognitive processes. The HLTP model incorporates a sophisticated teacher-student knowledge distillation framework. The "teacher" model, equipped with an adaptive visual sector, mimics the visual processing of the human brain, particularly the functions of the occipital and temporal lobes. The "student" model focuses on real-time interaction and decision-making, drawing parallels to prefrontal and parieta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;CIST-GCN&#65289;&#65292;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#65292;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#20102;GCN&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.19237</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Context-based Interpretable Spatio-Temporal Graph Convolutional Network for Human Motion Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19237
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;CIST-GCN&#65289;&#65292;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#65292;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#20102;GCN&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36816;&#21160;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#23545;&#33258;&#21160;&#39550;&#39542;&#21644;&#23433;&#20840;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;CIST-GCN&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;GCN&#30340;3D&#20154;&#20307;&#23039;&#21183;&#39044;&#27979;&#27169;&#22411;&#65292;&#20855;&#26377;&#29305;&#23450;&#23618;&#65292;&#36741;&#21161;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20379;&#22312;&#20998;&#26512;&#36816;&#21160;&#20998;&#24067;&#21644;&#36523;&#20307;&#34892;&#20026;&#26102;&#21487;&#33021;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20174;&#23039;&#21183;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#23558;&#20301;&#31227;&#21644;&#21152;&#36895;&#24230;&#32858;&#21512;&#21040;&#36755;&#20837;&#27169;&#22411;&#20013;&#65292;&#26368;&#32456;&#39044;&#27979;&#36755;&#20986;&#20301;&#31227;&#12290;&#22312;Human 3.6M&#12289;AMASS&#12289;3DPW&#21644;ExPI&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CIST-GCN&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19237v1 Announce Type: cross  Abstract: Human motion prediction is still an open problem extremely important for autonomous driving and safety applications. Due to the complex spatiotemporal relation of motion sequences, this remains a challenging problem not only for movement prediction but also to perform a preliminary interpretation of the joint connections. In this work, we present a Context-based Interpretable Spatio-Temporal Graph Convolutional Network (CIST-GCN), as an efficient 3D human pose forecasting model based on GCNs that encompasses specific layers, aiding model interpretability and providing information that might be useful when analyzing motion distribution and body behavior. Our architecture extracts meaningful information from pose sequences, aggregates displacements and accelerations into the input model, and finally predicts the output displacements. Extensive experiments on Human 3.6M, AMASS, 3DPW, and ExPI datasets demonstrate that CIST-GCN outperforms
&lt;/p&gt;</description></item><item><title>FSS&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#26469;&#25913;&#21892;&#32467;&#26524;&#65292;&#21516;&#26102;&#24341;&#20837;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.19197</link><description>&lt;p&gt;
&#32454;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;: &#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19197
&lt;/p&gt;
&lt;p&gt;
FSS&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#26469;&#25913;&#21892;&#32467;&#26524;&#65292;&#21516;&#26102;&#24341;&#20837;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#32032;&#23545;&#40784;&#30340;&#38544;&#24335;&#27169;&#22411;&#65292;&#22914;PIFu&#12289;PIFuHD&#21644;ICON&#65292;&#29992;&#20110;&#21333;&#35270;&#22270;&#30528;&#35013;&#20154;&#20307;&#37325;&#24314;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#35201;&#20040;&#26080;&#27861;&#25429;&#25417;&#34180;&#34920;&#38754;&#65288;&#22914;&#32819;&#26421;&#12289;&#25163;&#25351;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#37325;&#24314;&#32593;&#26684;&#20013;&#30340;&#22122;&#22768;&#20266;&#24433;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32454;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;&#65288;FSS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#35757;&#32451;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#12290;FSS&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#26469;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#19981;&#21516;&#65292;FSS&#26174;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#39640;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#65292;FSS&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#12290;&#36825;&#20351;&#24471;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#27861;&#32447;&#21464;&#24471;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19197v1 Announce Type: cross  Abstract: Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to int
&lt;/p&gt;</description></item><item><title>&#36127;&#37319;&#26679;&#26041;&#27861;&#23545;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#21508;&#31181;&#36127;&#37319;&#26679;&#26041;&#27861;&#21450;&#20854;&#23545;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#25104;&#21151;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.19195</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#36127;&#37319;&#26679;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Negative Sampling in Knowledge Graph Representation Learning: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19195
&lt;/p&gt;
&lt;p&gt;
&#36127;&#37319;&#26679;&#26041;&#27861;&#23545;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#21508;&#31181;&#36127;&#37319;&#26679;&#26041;&#27861;&#21450;&#20854;&#23545;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#25104;&#21151;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65288;KGRL&#65289;&#25110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#22312;&#30693;&#35782;&#26500;&#24314;&#21644;&#20449;&#24687;&#25506;&#32034;&#30340;AI&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#32534;&#30721;&#20026;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#12290;&#22312;KGE&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#27491;&#36127;&#26679;&#26412;&#23545;&#20110;&#21306;&#20998;&#30446;&#30340;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#21462;&#36127;&#26679;&#26412;&#38754;&#20020;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#26377;&#25928;&#29983;&#25104;&#25216;&#26415;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#20123;&#36127;&#26679;&#26412;&#30340;&#36136;&#37327;&#23545;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#20934;&#30830;&#24615;&#26377;&#30528;&#24456;&#22823;&#24433;&#21709;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#29983;&#25104;&#25104;&#20026;KGRL&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#26412;&#20840;&#38754;&#35843;&#30740;&#35770;&#25991;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#21508;&#31181;&#36127;&#37319;&#26679;&#65288;NS&#65289;&#26041;&#27861;&#21450;&#20854;&#23545;KGRL&#25104;&#21151;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;NS&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#27010;&#36848;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19195v1 Announce Type: new  Abstract: Knowledge graph representation learning (KGRL) or knowledge graph embedding (KGE) plays a crucial role in AI applications for knowledge construction and information exploration. These models aim to encode entities and relations present in a knowledge graph into a lower-dimensional vector space. During the training process of KGE models, using positive and negative samples becomes essential for discrimination purposes. However, obtaining negative samples directly from existing knowledge graphs poses a challenge, emphasizing the need for effective generation techniques. The quality of these negative samples greatly impacts the accuracy of the learned embeddings, making their generation a critical aspect of KGRL. This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL. Their respective advantages and disadvantages are outlined by categorizing existing NS me
&lt;/p&gt;</description></item><item><title>BigCode&#39033;&#30446;&#24341;&#20837;&#20102;StarCoder2&#21644;The Stack v2&#65292;&#22312;SWH&#23384;&#20648;&#24211;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#32508;&#21512;&#30340;Code LLM&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;StarCoder2-3B&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#20248;&#20110;StarCoderBase-15B&#12290;</title><link>https://arxiv.org/abs/2402.19173</link><description>&lt;p&gt;
StarCoder 2&#21644;The Stack v2: &#19979;&#19968;&#20195;
&lt;/p&gt;
&lt;p&gt;
StarCoder 2 and The Stack v2: The Next Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19173
&lt;/p&gt;
&lt;p&gt;
BigCode&#39033;&#30446;&#24341;&#20837;&#20102;StarCoder2&#21644;The Stack v2&#65292;&#22312;SWH&#23384;&#20648;&#24211;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#32508;&#21512;&#30340;Code LLM&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;StarCoder2-3B&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#20248;&#20110;StarCoderBase-15B&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BigCode&#39033;&#30446;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#23545;&#20195;&#30721;&#65288;&#20195;&#30721;LLMs&#65289;&#36827;&#34892;&#36127;&#36131;&#20219;&#24320;&#21457;&#30340;&#24320;&#25918;&#31185;&#23398;&#21512;&#20316;&#39033;&#30446;&#65292;&#24341;&#20837;&#20102;StarCoder2&#12290;&#25105;&#20204;&#19982;Software Heritage&#65288;SWH&#65289;&#21512;&#20316;&#65292;&#22312;&#20182;&#20204;&#30340;&#28304;&#20195;&#30721;&#23384;&#26723;&#30340;&#25968;&#23383;&#20844;&#20849;&#36164;&#28304;&#20043;&#19978;&#26500;&#24314;The Stack v2&#12290;&#38500;&#20102;&#28085;&#30422;619&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;SWH&#23384;&#20648;&#24211;&#22806;&#65292;&#25105;&#20204;&#36824;&#31934;&#24515;&#36873;&#25321;&#20854;&#20182;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#22914;GitHub&#25289;&#21462;&#35831;&#27714;&#12289;Kaggle&#31508;&#35760;&#26412;&#21644;&#20195;&#30721;&#25991;&#26723;&#12290;&#36825;&#23548;&#33268;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#27604;&#31532;&#19968;&#20010;StarCoder&#25968;&#25454;&#38598;&#22823;4&#20493;&#12290;&#25105;&#20204;&#20351;&#29992;3B&#12289;7B&#21644;15B&#21442;&#25968;&#30340;StarCoder2&#27169;&#22411;&#35757;&#32451;3.3&#33267;4.3&#19975;&#20159;&#20010;&#26631;&#35760;&#65292;&#24182;&#22312;&#19968;&#22871;&#20840;&#38754;&#30340;Code LLM&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#23567;&#22411;&#27169;&#22411;StarCoder2-3B&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;Code LLM&#65292;&#24182;&#19988;&#20063;&#20248;&#20110;StarCoderBase-15B&#12290;&#25105;&#20204;&#30340;&#22823;&#22411;&#27169;&#22411;StarCoder2-15B&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19173v1 Announce Type: cross  Abstract: The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#26469;&#26356;&#22909;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#65292;&#24182;&#22312;&#32599;&#39532;&#23612;&#20122;&#30340;4&#20010;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.19170</link><description>&lt;p&gt;
&#36890;&#36807;&#38271;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21319;&#32599;&#39532;&#23612;&#20122;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Legal Judgement Prediction in Romanian with Long Text Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#26469;&#26356;&#22909;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#65292;&#24182;&#22312;&#32599;&#39532;&#23612;&#20122;&#30340;4&#20010;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#26032;&#25104;&#26524;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#27861;&#24459;NLP&#39046;&#22495;&#20063;&#38543;&#20043;&#21457;&#23637;&#36805;&#29467;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#27169;&#22411;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#12290;&#30001;&#20110;&#20854;&#19987;&#19994;&#35789;&#27719;&#12289;&#38271;&#25991;&#26723;&#31561;&#29305;&#28857;&#65292;&#27861;&#24459;NLP&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#21644;&#36890;&#29992;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#27861;&#24459;&#26696;&#20363;&#30340;&#26368;&#32456;&#35009;&#20915;&#30340;&#26041;&#27861;&#65292;&#21363;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#22914;&#20309;&#25193;&#23637;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#12290;&#22312;&#26469;&#33258;&#20004;&#20010;&#26469;&#28304;&#12289;&#35268;&#27169;&#21644;&#25991;&#26723;&#38271;&#24230;&#26174;&#33879;&#19981;&#21516;&#26102;&#30340;4&#20010;&#32599;&#39532;&#23612;&#20122;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#19987;&#38376;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19170v1 Announce Type: cross  Abstract: In recent years,the entire field of Natural Language Processing (NLP) has enjoyed amazing novel results achieving almost human-like performance on a variety of tasks. Legal NLP domain has also been part of this process, as it has seen an impressive growth. However, general-purpose models are not readily applicable for legal domain. Due to the nature of the domain (e.g. specialized vocabulary, long documents) specific models and methods are often needed for Legal NLP. In this work we investigate both specialized and general models for predicting the final ruling of a legal case, task known as Legal Judgment Prediction (LJP). We particularly focus on methods to extend to sequence length of Transformer-based models to better understand the long documents present in legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating from 2 sources with significantly different sizes and document lengths, show that specialized mo
&lt;/p&gt;</description></item><item><title>MemoNav&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#30446;&#26631;&#23548;&#33322;&#30340;&#26032;&#22411;&#35760;&#24518;&#27169;&#22411;&#65292;&#36890;&#36807;&#19977;&#31181;&#23548;&#33322;&#35760;&#24518;&#31867;&#22411;&#21644;&#36951;&#24536;&#27169;&#22359;&#25552;&#39640;&#20102;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19161</link><description>&lt;p&gt;
MemoNav&#65306;&#35270;&#35273;&#23548;&#33322;&#30340;&#24037;&#20316;&#35760;&#24518;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MemoNav: Working Memory Model for Visual Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19161
&lt;/p&gt;
&lt;p&gt;
MemoNav&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#30446;&#26631;&#23548;&#33322;&#30340;&#26032;&#22411;&#35760;&#24518;&#27169;&#22411;&#65292;&#36890;&#36807;&#19977;&#31181;&#23548;&#33322;&#35760;&#24518;&#31867;&#22411;&#21644;&#36951;&#24536;&#27169;&#22359;&#25552;&#39640;&#20102;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#30446;&#26631;&#23548;&#33322;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#19968;&#20010;agent&#22312;&#38476;&#29983;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#30001;&#22270;&#20687;&#25351;&#31034;&#30340;&#30446;&#26631;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#30340;&#22330;&#26223;&#35760;&#24518;&#23384;&#22312;&#30528;&#25928;&#29575;&#20302;&#19979;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21033;&#29992;&#20102;&#25152;&#26377;&#21382;&#21490;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#20915;&#31574;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#37096;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MemoNav&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#20687;&#30446;&#26631;&#23548;&#33322;&#30340;&#35760;&#24518;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#31867;&#20284;&#24037;&#20316;&#35760;&#24518;&#30340;&#27969;&#31243;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#23548;&#33322;&#35760;&#24518;&#31867;&#22411;&#12290;&#22320;&#22270;&#19978;&#30340;&#33410;&#28857;&#29305;&#24449;&#23384;&#20648;&#22312;&#30701;&#26399;&#35760;&#24518;&#65288;STM&#65289;&#20013;&#65292;&#22240;&#20026;&#36825;&#20123;&#29305;&#24449;&#26159;&#21160;&#24577;&#26356;&#26032;&#30340;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#36951;&#24536;&#27169;&#22359;&#20445;&#30041;&#20449;&#24687;&#37327;&#22823;&#30340;STM&#37096;&#20998;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38271;&#26399;&#35760;&#24518;&#65288;LTM&#65289;&#26469;&#23398;&#20064;&#20840;&#23616;&#22330;&#26223;&#34920;&#31034;&#65292;&#36880;&#28176;&#32858;&#21512;STM&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#22270;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#37325;&#26032;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19161v1 Announce Type: cross  Abstract: Image-goal navigation is a challenging task that requires an agent to navigate to a goal indicated by an image in unfamiliar environments. Existing methods utilizing diverse scene memories suffer from inefficient exploration since they use all historical observations for decision-making without considering the goal-relevant fraction. To address this limitation, we present MemoNav, a novel memory model for image-goal navigation, which utilizes a working memory-inspired pipeline to improve navigation performance. Specifically, we employ three types of navigation memory. The node features on a map are stored in the short-term memory (STM), as these features are dynamically updated. A forgetting module then retains the informative STM fraction to increase efficiency. We also introduce long-term memory (LTM) to learn global scene representations by progressively aggregating STM features. Subsequently, a graph attention module encodes the re
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#27454;&#21517;&#20026;ClarifAI&#30340;&#33258;&#21160;&#21270;&#23459;&#20256;&#26816;&#27979;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#23459;&#20256;&#24182;&#25552;&#20379;&#20016;&#23500;&#35299;&#37322;&#65292;&#20197;&#28608;&#21457;&#26356;&#22810;&#25209;&#21028;&#24615;&#38405;&#35835;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24378;&#35843;&#20102;&#35299;&#37322;&#23545;&#20110;&#22521;&#20859;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2402.19135</link><description>&lt;p&gt;
&#24555;&#36895;&#24605;&#32771;&#65292;&#24930;&#36895;&#24605;&#32771;&#65292;&#25209;&#21028;&#24615;&#24605;&#32771;&#65306;&#35774;&#35745;&#19968;&#27454;&#33258;&#21160;&#21270;&#30123;&#24773;&#26816;&#27979;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19135
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#27454;&#21517;&#20026;ClarifAI&#30340;&#33258;&#21160;&#21270;&#23459;&#20256;&#26816;&#27979;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#23459;&#20256;&#24182;&#25552;&#20379;&#20016;&#23500;&#35299;&#37322;&#65292;&#20197;&#28608;&#21457;&#26356;&#22810;&#25209;&#21028;&#24615;&#38405;&#35835;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24378;&#35843;&#20102;&#35299;&#37322;&#23545;&#20110;&#22521;&#20859;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#65292;&#24555;&#36895;&#30340;&#26032;&#38395;&#28040;&#36153;&#21644;&#26085;&#30410;&#23545;&#23459;&#20256;&#30340;&#33030;&#24369;&#24615;&#25104;&#20026;&#29305;&#28857;&#65292;&#22521;&#20859;&#20844;&#27665;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#23545;&#20110;&#31283;&#23450;&#30340;&#27665;&#20027;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ClarifAI&#30340;&#35774;&#35745;&#65292;&#36825;&#26159;&#19968;&#27454;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#23459;&#20256;&#26816;&#27979;&#24037;&#20855;&#65292;&#26088;&#22312;&#36890;&#36807;&#28608;&#27963;&#20998;&#26512;&#24615;&#24605;&#32500;&#27169;&#24335;&#65292;&#36981;&#24490;&#24247;&#26364;&#30340;&#35748;&#30693;&#21452;&#31995;&#32479;&#29702;&#35770;&#65292;&#25512;&#21160;&#35835;&#32773;&#26356;&#21152;&#25209;&#21028;&#24615;&#22320;&#28040;&#36153;&#26032;&#38395;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ClarifAI&#21487;&#20197;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#23459;&#20256;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#32972;&#26223;&#35299;&#37322;&#65292;&#22686;&#24378;&#29992;&#25143;&#30340;&#29702;&#35299;&#21644;&#25209;&#21028;&#24615;&#24605;&#32500;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ClarifAI&#30340;&#35774;&#35745;&#65307;&#20854;&#27425;&#65292;&#22312;&#19968;&#39033;&#22312;&#32447;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#35774;&#35745;&#26377;&#25928;&#22320;&#40723;&#21169;&#26032;&#38395;&#35835;&#32773;&#26356;&#22810;&#22320;&#36827;&#34892;&#25209;&#21028;&#24615;&#38405;&#35835;&#65307;&#31532;&#19977;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#35299;&#37322;&#23545;&#20110;&#22521;&#20859;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#20215;&#20540;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26082;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#24037;&#20855;&#65292;&#21448;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#25903;&#25745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19135v1 Announce Type: cross  Abstract: In today's digital age, characterized by rapid news consumption and increasing vulnerability to propaganda, fostering citizens' critical thinking is crucial for stable democracies. This paper introduces the design of ClarifAI, a novel automated propaganda detection tool designed to nudge readers towards more critical news consumption by activating the analytical mode of thinking, following Kahneman's dual-system theory of cognition. Using Large Language Models, ClarifAI detects propaganda in news articles and provides context-rich explanations, enhancing users' understanding and critical thinking. Our contribution is threefold: first, we propose the design of ClarifAI; second, in an online experiment, we demonstrate that this design effectively encourages news readers to engage in more critical reading; and third, we emphasize the value of explanations for fostering critical thinking. The study thus offers both a practical tool and use
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65288;IECI&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#26631;&#27880;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#23637;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.19116</link><description>&lt;p&gt;
&#22914;&#20309;&#29702;&#35299;&#8220;&#25903;&#25345;&#8221;&#65311;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#29992;&#20110;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
How to Understand "Support"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19116
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65288;IECI&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#26631;&#27880;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#23637;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#65288;WPG&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#25512;&#26029;&#32454;&#31890;&#24230;&#30701;&#35821;-&#21306;&#22495;&#21305;&#37197;&#65292;&#20165;&#21033;&#29992;&#31895;&#31890;&#24230;&#30340;&#21477;&#23376;-&#22270;&#20687;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;WPG&#30340;&#30740;&#31350;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#38544;&#24335;&#30701;&#35821;-&#21306;&#22495;&#21305;&#37197;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#28145;&#23618;&#22810;&#27169;&#24577;&#35821;&#20041;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#65288;IECI&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#23545;&#24314;&#27169;&#38544;&#24335;&#20851;&#31995;&#21644;&#31361;&#20986;&#26174;&#24615;&#20851;&#31995;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#21033;&#29992;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25216;&#26415;&#26469;&#24212;&#23545;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36824;&#26631;&#27880;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#38544;&#24335;&#22686;&#24378;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;IECI&#65292;&#35814;&#32454;&#35780;&#20272;&#26174;&#31034;IECI&#30456;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#20248;&#21183;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19116v1 Announce Type: cross  Abstract: Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting findi
&lt;/p&gt;</description></item><item><title>CollaFuse&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22312;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26102;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.19105</link><description>&lt;p&gt;
CollaFuse&#65306;&#22312;&#21327;&#20316;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#23548;&#33322;&#26377;&#38480;&#36164;&#28304;&#21644;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19105
&lt;/p&gt;
&lt;p&gt;
CollaFuse&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22312;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26102;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#25193;&#25955;&#24335;&#27169;&#22411;&#22312;&#25968;&#25454;&#38656;&#27714;&#21644;&#38544;&#31169;&#26041;&#38754;&#32473;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#24102;&#26469;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#32852;&#37030;&#23398;&#20064;&#20998;&#21457;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#20250;&#32473;&#20010;&#21035;&#23458;&#25143;&#24102;&#26469;&#21387;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CollaFuse&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#26377;&#25928;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;CollaFuse&#23454;&#29616;&#20102;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#36825;&#36890;&#36807;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#26412;&#22320;&#20445;&#30041;&#25968;&#25454;&#21644;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;GPU&#36827;&#31243;&#65292;&#21516;&#26102;&#23558;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#30340;&#36827;&#31243;&#22806;&#21253;&#32473;&#20849;&#20139;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#12290;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#23637;&#31034;&#65292;CollaFuse&#36890;&#36807;&#22823;&#22823;&#20943;&#23569;&#23545;&#25935;&#24863;&#20449;&#24687;&#20849;&#20139;&#30340;&#38656;&#27714;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19105v1 Announce Type: cross  Abstract: In the landscape of generative artificial intelligence, diffusion-based models present challenges for socio-technical systems in data requirements and privacy. Traditional approaches like federated learning distribute the learning process but strain individual clients, especially with constrained resources (e.g., edge devices). In response to these challenges, we introduce CollaFuse, a novel framework inspired by split learning. Tailored for efficient and collaborative use of denoising diffusion probabilistic models, CollaFuse enables shared server training and inference, alleviating client computational burdens. This is achieved by retaining data and computationally inexpensive GPU processes locally at each client while outsourcing the computationally expensive processes to the shared server. Demonstrated in a healthcare context, CollaFuse enhances privacy by highly reducing the need for sensitive information sharing. These capabiliti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;FAITH&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.19103</link><description>&lt;p&gt;
&#38663;&#25788;&#22522;&#30784;&#30340;&#32454;&#35821;&#65306;&#20998;&#26512;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;FAITH&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#21463;&#21040;&#24187;&#35273;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#31867;&#22411;&#26159;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#65292;&#25105;&#20204;&#23450;&#20041;&#20026;&#24403;LLMs&#38754;&#23545;&#34394;&#20551;&#21069;&#25552;&#38382;&#39064;&#26102;&#29983;&#25104;&#24187;&#35273;&#25991;&#26412;&#30340;&#29616;&#35937;&#12290;&#26412;&#25991;&#23545;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#38416;&#26126;&#20102;&#20854;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#65306;&#19968;&#23567;&#37096;&#20998;&#27880;&#24847;&#21147;&#22836;(&#25105;&#20204;&#23558;&#20854;&#25351;&#23450;&#20026;&#34394;&#20551;&#21069;&#25552;&#22836;)&#25200;&#20081;&#20102;&#30693;&#35782;&#25552;&#21462;&#36807;&#31243;&#65292;&#23548;&#33268;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;FAITH&#8221;(&#34394;&#20551;&#21069;&#25552;&#27880;&#24847;&#21147;&#22836;&#32422;&#26463;&#20197;&#20943;&#36731;&#24187;&#35273;)&#36825;&#19968;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#34394;&#20551;&#21069;&#25552;&#24187;&#35273;&#12290;&#23427;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#32422;&#26463;&#34394;&#20551;&#21069;&#25552;&#27880;&#24847;&#21147;&#22836;&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19103v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \textbf{FAITH} (\textbf{F}alse premise \textbf{A}ttention head constra\textbf{I}ining for mi\textbf{T}igating \textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively,
&lt;/p&gt;</description></item><item><title>FlatNAS&#26159;&#25991;&#29486;&#20013;&#39318;&#20010;&#31995;&#32479;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#20002;&#22833;&#20989;&#25968;&#24179;&#22374;&#21306;&#22495;&#30340;NAS&#26041;&#27861;&#65292;&#21516;&#26102;&#20248;&#21270;&#20854;&#22312;&#20998;&#24067;&#25968;&#25454;&#21644;&#20998;&#24067;&#20043;&#22806;&#40065;&#26834;&#24615;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#32422;&#26463;&#20854;&#26550;&#26500;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.19102</link><description>&lt;p&gt;
FlatNAS&#65306;&#20248;&#21270;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#20013;&#30340;&#24179;&#22374;&#24615;&#20197;&#23454;&#29616;&#23545;&#20998;&#24067;&#20043;&#22806;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19102
&lt;/p&gt;
&lt;p&gt;
FlatNAS&#26159;&#25991;&#29486;&#20013;&#39318;&#20010;&#31995;&#32479;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#20002;&#22833;&#20989;&#25968;&#24179;&#22374;&#21306;&#22495;&#30340;NAS&#26041;&#27861;&#65292;&#21516;&#26102;&#20248;&#21270;&#20854;&#22312;&#20998;&#24067;&#25968;&#25454;&#21644;&#20998;&#24067;&#20043;&#22806;&#40065;&#26834;&#24615;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#32422;&#26463;&#20854;&#26550;&#26500;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20026;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26550;&#26500;&#30340;&#33258;&#21160;&#23450;&#20041;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#24182;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Flat Neural Architecture Search&#65288;FlatNAS&#65289;&#30340;&#26032;&#39062;NAS&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#25506;&#35752;&#20102;&#22522;&#20110;&#23545;&#26435;&#37325;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#21644;&#20855;&#26377;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#30340;&#21333;&#19968;NN&#20248;&#21270;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#19982;&#24403;&#21069;&#20027;&#35201;&#38598;&#20013;&#20110;OOD&#31639;&#27861;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;FlatNAS&#25104;&#21151;&#22320;&#35780;&#20272;&#20102;NN&#26550;&#26500;&#23545;OOD&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19102v1 Announce Type: cross  Abstract: Neural Architecture Search (NAS) paves the way for the automatic definition of Neural Network (NN) architectures, attracting increasing research attention and offering solutions in various scenarios. This study introduces a novel NAS solution, called Flat Neural Architecture Search (FlatNAS), which explores the interplay between a novel figure of merit based on robustness to weight perturbations and single NN optimization with Sharpness-Aware Minimization (SAM). FlatNAS is the first work in the literature to systematically explore flat regions in the loss landscape of NNs in a NAS procedure, while jointly optimizing their performance on in-distribution data, their out-of-distribution (OOD) robustness, and constraining the number of parameters in their architecture. Differently from current studies primarily concentrating on OOD algorithms, FlatNAS successfully evaluates the impact of NN architectures on OOD robustness, a crucial aspect
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;</title><link>https://arxiv.org/abs/2402.19088</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#21464;&#21270;&#29305;&#24449;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey in Characterization of Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19088
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#35821;&#35328;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#21560;&#32435;&#20154;&#31867;&#31038;&#20250;&#30340;&#25991;&#21270;&#21464;&#21270;&#12290;&#36825;&#31181;&#28436;&#21464;&#36890;&#36807;&#26032;&#35789;&#35821;&#65288;&#26032;&#21333;&#35789;&#65289;&#25110;&#21333;&#35789;&#30340;&#35821;&#20041;&#21464;&#21270;&#65288;&#36171;&#20104;&#24050;&#26377;&#21333;&#35789;&#26032;&#30340;&#21547;&#20041;&#65289;&#26469;&#20307;&#29616;&#12290;&#29702;&#35299;&#21333;&#35789;&#30340;&#21547;&#20041;&#23545;&#35299;&#37322;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#65288;&#22320;&#26041;&#29992;&#35821;&#25110;&#20442;&#35821;&#65289;&#12289;&#39046;&#22495;&#65288;&#20363;&#22914;&#25216;&#26415;&#26415;&#35821;&#65289;&#25110;&#26102;&#20195;&#30340;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#36825;&#20123;&#21333;&#35789;&#19982;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30456;&#20851;&#65292;&#20363;&#22914;&#32763;&#35793;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#31561;&#12290;&#35821;&#20041;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#21644;&#24418;&#24335;&#21270;&#34920;&#24449;&#36825;&#20123;&#21464;&#21270;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#30740;&#31350;&#36825;&#31181;&#24433;&#21709;&#26159;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#36817;&#26399;&#24341;&#36215;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20960;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#26816;&#27979;&#35821;&#20041;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#21162;&#21147;&#26469;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19088v1 Announce Type: cross  Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to charact
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.19085</link><description>&lt;p&gt;
&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65306;&#26397;&#30528;&#21487;&#25511;&#22810;&#30446;&#26631;&#23545;&#40784;&#26041;&#21521;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19085
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#40784;&#24037;&#20316;&#26088;&#22312;&#36861;&#27714;&#27169;&#22411;&#21709;&#24212;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#23545;&#40784;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#31526;&#21512;&#21508;&#31181;&#20559;&#22909;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19085v1 Announce Type: new  Abstract: Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the "alignment tax" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.19078</link><description>&lt;p&gt;
&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Smooth Tchebycheff Scalarization for Multi-Objective Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#37117;&#33021;&#25214;&#21040;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#32463;&#24120;&#30456;&#20114;&#20914;&#31361;&#65292;&#19981;&#33021;&#36890;&#36807;&#21333;&#20010;&#35299;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#25214;&#21040;&#24085;&#32047;&#25176;&#35299;&#65292;&#36825;&#20123;&#35299;&#20195;&#34920;&#20102;&#23545;&#20110;&#32473;&#23450;&#38382;&#39064;&#30340;&#19981;&#21516;&#26368;&#20339;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#21487;&#33021;&#19981;&#33021;&#20855;&#22791;&#35299;&#20915;&#19968;&#33324;&#21487;&#24494;&#20998;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#29702;&#35770;&#23646;&#24615;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#20809;&#28369;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#30340;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#23427;&#23545;&#20110;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#32467;&#26524;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19078v1 Announce Type: cross  Abstract: Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution. In the past few decades, numerous methods have been proposed to find Pareto solutions that represent different optimal trade-offs among the objectives for a given problem. However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem. In this work, by leveraging the smooth optimization technique, we propose a novel and lightweight smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization. It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods. Experimental results on variou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.19072</link><description>&lt;p&gt;
TimeXer&#65306;&#21033;&#29992;&#22806;&#29983;&#21464;&#37327;&#22686;&#24378;&#21464;&#21387;&#22120;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#24212;&#29992;&#30340;&#37096;&#20998;&#35266;&#27979;&#24615;&#36136;&#65292;&#20165;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#65292;&#20063;&#23601;&#26159;&#25152;&#35859;&#30340;&#20869;&#29983;&#21464;&#37327;&#65292;&#36890;&#24120;&#26159;&#19981;&#36275;&#20197;&#20445;&#35777;&#20934;&#30830;&#39044;&#27979;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#31995;&#32479;&#36890;&#24120;&#35760;&#24405;&#20026;&#22810;&#20010;&#21464;&#37327;&#65292;&#20854;&#20013;&#22806;&#29983;&#24207;&#21015;&#21487;&#20197;&#20026;&#20869;&#29983;&#21464;&#37327;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#22806;&#37096;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#19982;&#20808;&#21069;&#30830;&#31435;&#30340;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#19981;&#21516;&#65292;&#23427;&#20204;&#35201;&#20040;&#23558;&#25152;&#26377;&#21464;&#37327;&#31561;&#21516;&#23545;&#24453;&#65292;&#35201;&#20040;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#65292;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#31181;&#23454;&#38469;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#22806;&#29983;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#20869;&#29983;&#21464;&#37327;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#23884;&#20837;&#23618;&#65292;TimeXer&#20351;&#20256;&#32479;&#30340;Transformer&#26550;&#26500;&#20855;&#26377;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19072v1 Announce Type: cross  Abstract: Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RobWE&#30340;&#24378;&#22823;&#27700;&#21360;&#23884;&#20837;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#12290;</title><link>https://arxiv.org/abs/2402.19054</link><description>&lt;p&gt;
RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection
&lt;/p&gt;
&lt;p&gt;
RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RobWE&#30340;&#24378;&#22823;&#27700;&#21360;&#23884;&#20837;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#23558;&#27700;&#21360;&#23884;&#20837;&#21040;&#27169;&#22411;&#20013;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20197;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#20445;&#25252;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#20013;&#23458;&#25143;&#33719;&#21462;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#25152;&#26377;&#26435;&#26159;&#19981;&#36275;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RobWE&#30340;&#24378;&#22823;&#27700;&#21360;&#23884;&#20837;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;PFL&#20013;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#27700;&#21360;&#23884;&#20837;&#20998;&#20026;&#20004;&#37096;&#20998;&#65306;&#22836;&#37096;&#23618;&#23884;&#20837;&#21644;&#34920;&#31034;&#23618;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19054v1 Announce Type: cross  Abstract: Embedding watermarks into models has been widely used to protect model ownership in federated learning (FL). However, existing methods are inadequate for protecting the ownership of personalized models acquired by clients in personalized FL (PFL). This is due to the aggregation of the global model in PFL, resulting in conflicts over clients' private watermarks. Moreover, malicious clients may tamper with embedded watermarks to facilitate model leakage and evade accountability. This paper presents a robust watermark embedding scheme, named RobWE, to protect the ownership of personalized models in PFL. We first decouple the watermark embedding of personalized models into two parts: head layer embedding and representation layer embedding. The head layer belongs to clients' private part without participating in model aggregation, while the representation layer is the shared part for aggregation. For representation layer embedding, we emplo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20687;&#32032;&#37325;&#25490;&#21644;&#26102;&#38388;&#28369;&#21160;&#31383;&#21475;&#26377;&#25928;&#22320;&#23398;&#20064;&#26102;&#31354;&#20808;&#39564;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#21407;&#22987;&#36755;&#20837;&#24207;&#21015;&#25110;&#39044;&#22788;&#29702;&#24207;&#21015;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.19041</link><description>&lt;p&gt;
&#20855;&#26377;&#35270;&#39057;&#24207;&#21015;&#28145;&#24230;&#35270;&#35273;&#20808;&#39564;&#30340;&#22823;&#27668;&#28237;&#27969;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20687;&#32032;&#37325;&#25490;&#21644;&#26102;&#38388;&#28369;&#21160;&#31383;&#21475;&#26377;&#25928;&#22320;&#23398;&#20064;&#26102;&#31354;&#20808;&#39564;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#21407;&#22987;&#36755;&#20837;&#24207;&#21015;&#25110;&#39044;&#22788;&#29702;&#24207;&#21015;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27668;&#28237;&#27969;&#23545;&#35270;&#35273;&#22270;&#20687;&#30340;&#35299;&#37322;&#21644;&#24863;&#30693;&#36896;&#25104;&#25361;&#25112;&#65292;&#30001;&#20110;&#20854;&#22833;&#30495;&#25928;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#20540;&#65292;&#36890;&#36807;&#20687;&#32032;&#37325;&#25490;&#21644;&#26102;&#38388;&#28369;&#21160;&#31383;&#21475;&#26377;&#25928;&#22320;&#23398;&#20064;&#26102;&#31354;&#20808;&#39564;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#21407;&#22987;&#36755;&#20837;&#24207;&#21015;&#25110;&#39044;&#22788;&#29702;&#24207;&#21015;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19041v1 Announce Type: cross  Abstract: Atmospheric turbulence poses a challenge for the interpretation and visual perception of visual imagery due to its distortion effects. Model-based approaches have been used to address this, but such methods often suffer from artefacts associated with moving content. Conversely, deep learning based methods are dependent on large and diverse datasets that may not effectively represent any specific content. In this paper, we address these problems with a self-supervised learning method that does not require ground truth. The proposed method is not dependent on any dataset outside of the single data sequence being processed but is also able to improve the quality of any input raw sequences or pre-processed sequences. Specifically, our method is based on an accelerated Deep Image Prior (DIP), but integrates temporal information using pixel shuffling and a temporal sliding window. This efficiently learns spatio-temporal priors leading to a s
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#38382;&#39064;&#31354;&#38388;&#20869;&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#65292;&#23545;&#25239;&#38450;&#30149;&#27602;&#36719;&#20214;&#20013;&#30340;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.19027</link><description>&lt;p&gt;
&#22914;&#20309;&#35757;&#32451;&#24744;&#30340;&#38450;&#30149;&#27602;&#36719;&#20214;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#31354;&#38388;&#21152;&#22266;
&lt;/p&gt;
&lt;p&gt;
How to Train your Antivirus: RL-based Hardening through the Problem-Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19027
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#38382;&#39064;&#31354;&#38388;&#20869;&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#65292;&#23545;&#25239;&#38450;&#30149;&#27602;&#36719;&#20214;&#20013;&#30340;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#21152;&#22266;&#19968;&#23478;&#33879;&#21517;&#21830;&#19994;&#38450;&#30149;&#27602;&#20844;&#21496;&#27969;&#31243;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38450;&#24481;&#25216;&#26415;&#65292;&#20197;&#23545;&#25239;&#24694;&#24847;&#36719;&#20214;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#65292;&#36825;&#26159;&#23545;&#25239;&#36867;&#36991;&#25915;&#20987;&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19027v1 Announce Type: cross  Abstract: ML-based malware detection on dynamic analysis reports is vulnerable to both evasion and spurious correlations. In this work, we investigate a specific ML architecture employed in the pipeline of a widely-known commercial antivirus company, with the goal to harden it against adversarial malware. Adversarial training, the sole defensive technique that can confer empirical robustness, is not applicable out of the box in this domain, for the principal reason that gradient-based perturbations rarely map back to feasible problem-space programs. We introduce a novel Reinforcement Learning approach for constructing adversarial examples, a constituent part of adversarially training a model against evasion. Our approach comes with multiple advantages. It performs modifications that are feasible in the problem-space, and only those; thus it circumvents the inverse mapping problem. It also makes possible to provide theoretical guarantees on the r
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#23558;&#24369;&#23398;&#20064;&#32773;&#35299;&#37322;&#32452;&#21512;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#38543;&#26426;&#26862;&#26519;&#30340;&#35299;&#37322;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#23545;&#38598;&#25104;&#26041;&#27861;&#20013;&#35299;&#37322;&#36827;&#34892;&#21028;&#21035;&#24179;&#22343;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#23450;&#37327;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.19025</link><description>&lt;p&gt;
&#23558;&#24369;&#23398;&#20064;&#32773;&#35299;&#37322;&#30340;&#32452;&#21512;&#29992;&#20110;&#25913;&#36827;&#38543;&#26426;&#26862;&#26519;&#30340;&#35299;&#37322;&#24615;&#21644;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Combination of Weak Learners eXplanations to Improve Random Forest eXplicability Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19025
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#23558;&#24369;&#23398;&#20064;&#32773;&#35299;&#37322;&#32452;&#21512;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#38543;&#26426;&#26862;&#26519;&#30340;&#35299;&#37322;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#23545;&#38598;&#25104;&#26041;&#27861;&#20013;&#35299;&#37322;&#36827;&#34892;&#21028;&#21035;&#24179;&#22343;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#23450;&#37327;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
XAI&#20013;&#30340;&#31283;&#20581;&#24615;&#27010;&#24565;&#25351;&#30340;&#26159;&#35266;&#23519;&#21040;&#30340;&#20851;&#20110;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#35299;&#37322;&#22312;&#23545;&#23548;&#33268;&#35813;&#39044;&#27979;&#30340;&#36755;&#20837;&#21464;&#21270;&#26102;&#30340;&#21464;&#21270;&#12290;&#30452;&#35273;&#19978;&#65292;&#22914;&#26524;&#35201;&#35299;&#37322;&#30340;&#36755;&#20837;&#30053;&#24494;&#21464;&#21270;&#65292;&#20197;&#33267;&#20110;&#19981;&#20250;&#22826;&#22823;&#31243;&#24230;&#25913;&#21464;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#37027;&#20040;&#25105;&#20204;&#26399;&#26395;&#23545;&#20110;&#35813;&#26032;&#36755;&#20837;&#30340;&#35299;&#37322;&#20063;&#19981;&#20250;&#26377;&#22826;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36890;&#36807;&#23545;&#24369;&#23398;&#20064;&#32773;&#35299;&#37322;&#36827;&#34892;&#21028;&#21035;&#24179;&#22343;&#30340;&#32452;&#21512;&#21487;&#20197;&#25552;&#39640;&#38598;&#25104;&#26041;&#27861;&#20013;&#35299;&#37322;&#30340;&#31283;&#20581;&#24615;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#22312;&#21518;&#32493;SHAP&#26041;&#27861;&#21644;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;&#20013;&#24471;&#21040;&#23454;&#26045;&#21644;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#25152;&#33719;&#24471;&#30340;&#25913;&#36827;&#24050;&#32463;&#36890;&#36807;&#23450;&#37327;&#26041;&#24335;&#36827;&#34892;&#20102;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#38598;&#25104;&#26041;&#27861;&#20013;&#35299;&#37322;&#24615;&#31283;&#20581;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19025v1 Announce Type: cross  Abstract: The notion of robustness in XAI refers to the observed variations in the explanation of the prediction of a learned model with respect to changes in the input leading to that prediction. Intuitively, if the input being explained is modified slightly subtly enough so as to not change the prediction of the model too much, then we would expect that the explanation provided for that new input does not change much either. We argue that a combination through discriminative averaging of ensembles weak learners explanations can improve the robustness of explanations in ensemble methods.This approach has been implemented and tested with post-hoc SHAP method and Random Forest ensemble with successful results. The improvements obtained have been measured quantitatively and some insights into the explicability robustness in ensemble methods are presented.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#19978;&#26080;&#32541;&#25972;&#21512;&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#23478;&#26063;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19009</link><description>&lt;p&gt;
&#29983;&#25104;&#12289;&#37325;&#24314;&#21644;&#34920;&#31034;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#65306;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19009
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#19978;&#26080;&#32541;&#25972;&#21512;&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#23478;&#26063;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#22522;&#20110;&#19977;&#39033;&#26680;&#24515;&#33021;&#21147;--&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;--&#36328;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#31163;&#25955;&#25991;&#26412;/&#34507;&#30333;&#24207;&#21015;&#21644;&#36830;&#32493;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#23478;&#26063;&#65292;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#24120;&#22312;&#29305;&#23450;&#33021;&#21147;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#23427;&#26080;&#32541;&#22320;&#38598;&#25104;&#20102;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;DiLED&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#21270;&#32534;&#30721;-&#35299;&#30721;&#26469;&#23558;&#26631;&#20934;&#25193;&#25955;&#20013;&#30340;&#39640;&#26031;&#21152;&#22122;-&#21435;&#22122;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#20851;&#38190;&#26159;&#65292;DiLED&#19982;&#25104;&#29087;&#30340;&#25193;&#25955;&#27169;&#22411;&#30446;&#26631;&#21644;&#35757;&#32451;&#26041;&#27861;&#20860;&#23481;&#65292;&#21487;&#26377;&#25928;&#23398;&#20064;&#32534;&#30721;-&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19009v1 Announce Type: cross  Abstract: The vast applications of deep generative models are anchored in three core capabilities -- generating new instances, reconstructing inputs, and learning compact representations -- across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), autoregressive models, and diffusion models, generally excel in specific capabilities and data types but fall short in others. We introduce generalized diffusion with learnable encoder-decoder (DiLED), that seamlessly integrates the core capabilities for broad applicability and enhanced performance. DiLED generalizes the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding. Crucially, DiLED is compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20449;&#24687;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#20154;&#30446;&#26631;&#21306;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#23558;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#22312;&#20960;&#20010;&#30446;&#26631;&#21306;&#22495;&#20869;&#12290;</title><link>https://arxiv.org/abs/2402.19002</link><description>&lt;p&gt;
GoalNet: &#38754;&#21521;&#30446;&#26631;&#21306;&#22495;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19002
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20449;&#24687;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#20154;&#30446;&#26631;&#21306;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#23558;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#22312;&#20960;&#20010;&#30446;&#26631;&#21306;&#22495;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36947;&#36335;&#19978;&#34892;&#20154;&#26410;&#26469;&#30340;&#36712;&#36857;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#21463;&#22330;&#26223;&#36335;&#24452;&#12289;&#34892;&#20154;&#24847;&#22270;&#21644;&#20915;&#31574;&#24433;&#21709;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22823;&#22810;&#20351;&#29992;&#36807;&#21435;&#30340;&#36712;&#36857;&#26469;&#39044;&#27979;&#21508;&#31181;&#28508;&#22312;&#30340;&#26410;&#26469;&#36712;&#36857;&#20998;&#24067;&#65292;&#36825;&#24182;&#26410;&#32771;&#34385;&#22330;&#26223;&#32972;&#26223;&#21644;&#34892;&#20154;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30452;&#25509;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#21363;&#39318;&#20808;&#20351;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#26469;&#39044;&#27979;&#30446;&#26631;&#28857;&#65292;&#28982;&#21518;&#37325;&#22797;&#20351;&#29992;&#30446;&#26631;&#28857;&#26469;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#12290;&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#22312;&#20960;&#20010;&#30446;&#26631;&#21306;&#22495;&#20869;&#65292;&#36825;&#20123;&#21306;&#22495;&#20195;&#34920;&#20102;&#34892;&#20154;&#30340;&#8220;&#30446;&#26631;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GoalNet&#65292;&#19968;&#31181;&#22522;&#20110;&#34892;&#20154;&#30446;&#26631;&#21306;&#22495;&#30340;&#26032;&#36712;&#36857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19002v1 Announce Type: cross  Abstract: Predicting the future trajectories of pedestrians on the road is an important task for autonomous driving. The pedestrian trajectory prediction is affected by scene paths, pedestrian's intentions and decision-making, which is a multi-modal problem. Most recent studies use past trajectories to predict a variety of potential future trajectory distributions, which do not account for the scene context and pedestrian targets. Instead of predicting the future trajectory directly, we propose to use scene context and observed trajectory to predict the goal points first, and then reuse the goal points to predict the future trajectories. By leveraging the information from scene context and observed trajectory, the uncertainty can be limited to a few target areas, which represent the "goals" of the pedestrians. In this paper, we propose GoalNet, a new trajectory prediction neural network based on the goal areas of a pedestrian. Our network can pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#20108;&#39033;&#38543;&#26426;Gamma&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#36136;&#36807;&#24230;&#31163;&#25955;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#21152;&#24555;&#25512;&#26029;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.18995</link><description>&lt;p&gt;
&#29992;&#20110;&#24322;&#36136;&#36807;&#24230;&#31163;&#25955;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#36127;&#20108;&#39033;&#38543;&#26426;Gamma&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous Overdispersed Count Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#20108;&#39033;&#38543;&#26426;Gamma&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#36136;&#36807;&#24230;&#31163;&#25955;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#21152;&#24555;&#25512;&#26029;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35745;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#24314;&#27169;&#33258;&#28982;&#22320;&#22312;&#29289;&#29702;&#21644;&#31038;&#20250;&#39046;&#22495;&#20013;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;Poisson gamma&#21160;&#24577;&#31995;&#32479;&#65288;PGDSs&#65289;&#26159;&#26032;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#25429;&#25417;&#35745;&#25968;&#24207;&#21015;&#32972;&#21518;&#34920;&#29616;&#20986;&#30340;&#26126;&#26174;&#30340;&#28508;&#22312;&#36716;&#25442;&#32467;&#26500;&#21644;&#31361;&#21457;&#21160;&#24577;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#22522;&#20110;&#32463;&#20856;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;PGDSs&#22312;&#25968;&#25454;&#22635;&#20805;&#21644;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#20248;&#21183;&#65292;PGDS&#19981;&#33021;&#25429;&#25417;&#22522;&#30784;&#21160;&#24577;&#36807;&#31243;&#30340;&#24322;&#36136;&#36807;&#24230;&#31163;&#25955;&#34892;&#20026;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#20108;&#39033;&#38543;&#26426;Gamma&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#23427;&#19981;&#20165;&#26174;&#33879;&#25913;&#21892;&#20102;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#20419;&#36827;&#20102;&#25512;&#26029;&#31639;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20272;&#35745;&#22240;&#23376;&#32467;&#26500;&#21644;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18995v1 Announce Type: cross  Abstract: Modeling count-valued time series has been receiving increasing attention since count time series naturally arise in physical and social domains. Poisson gamma dynamical systems (PGDSs) are newly-developed methods, which can well capture the expressive latent transition structure and bursty dynamics behind count sequences. In particular, PGDSs demonstrate superior performance in terms of data imputation and prediction, compared with canonical linear dynamical system (LDS) based methods. Despite these advantages, PGDS cannot capture the heterogeneous overdispersed behaviours of the underlying dynamic processes. To mitigate this defect, we propose a negative-binomial-randomized gamma Markov process, which not only significantly improves the predictive performance of the proposed dynamical system, but also facilitates the fast convergence of the inference algorithm. Moreover, we develop methods to estimate both factor-structured and graph
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;Continuous OBB&#65288;COBB&#65289;&#30340;&#26032;&#22411;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#20013;&#30830;&#20445;&#36793;&#30028;&#26694;&#22238;&#24402;&#30340;&#36830;&#32493;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18975</link><description>&lt;p&gt;
&#29702;&#35770;&#19978;&#23454;&#29616;&#23450;&#21521;&#21253;&#22260;&#26694;&#30340;&#36830;&#32493;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Theoretically Achieving Continuous Representation of Oriented Bounding Boxes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;Continuous OBB&#65288;COBB&#65289;&#30340;&#26032;&#22411;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#20013;&#30830;&#20445;&#36793;&#30028;&#26694;&#22238;&#24402;&#30340;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#26377;&#20851;&#23450;&#21521;&#21253;&#22260;&#26694;&#65288;OBB&#65289;&#34920;&#31034;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Continuous OBB&#65288;COBB&#65289;&#30340;&#26032;&#22411;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#20445;&#36793;&#30028;&#26694;&#22238;&#24402;&#30340;&#36830;&#32493;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#20013;&#65292;&#20363;&#22914;Faster-RCNN&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18975v1 Announce Type: cross  Abstract: Considerable efforts have been devoted to Oriented Object Detection (OOD). However, one lasting issue regarding the discontinuity in Oriented Bounding Box (OBB) representation remains unresolved, which is an inherent bottleneck for extant OOD methods. This paper endeavors to completely solve this issue in a theoretically guaranteed manner and puts an end to the ad-hoc efforts in this direction. Prior studies typically can only address one of the two cases of discontinuity: rotation and aspect ratio, and often inadvertently introduce decoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding Ambiguity (DA) as discussed in literature. Specifically, we propose a novel representation method called Continuous OBB (COBB), which can be readily integrated into existing detectors e.g. Faster-RCNN as a plugin. It can theoretically ensure continuity in bounding box regression which to our best knowledge, has not been achieved in liter
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#20415;&#25658;&#24335;&#36229;&#22768;&#22270;&#20687;&#20013;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#25506;&#35752;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#65288;softmax&#12289;&#33021;&#37327;&#20998;&#25968;&#21644;&#28145;&#24230;&#38598;&#25104;&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;&#33021;&#37327;&#20998;&#25968;&#26041;&#27861;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;&#38598;&#25104;&#26041;&#27861;&#23545;&#20110;&#25152;&#26377;&#19977;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;OOD&#26679;&#26412;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.18960</link><description>&lt;p&gt;
&#22312;&#20415;&#25658;&#24335;&#36229;&#22768;&#25104;&#20687;&#20013;&#23454;&#29616;&#20083;&#33146;&#30284;&#20998;&#31867;&#30340;&#36234;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Out-of-Distribution Detection for breast cancer classification in Point-of-Care Ultrasound Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18960
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#20415;&#25658;&#24335;&#36229;&#22768;&#22270;&#20687;&#20013;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#25506;&#35752;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#65288;softmax&#12289;&#33021;&#37327;&#20998;&#25968;&#21644;&#28145;&#24230;&#38598;&#25104;&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;&#33021;&#37327;&#20998;&#25968;&#26041;&#27861;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;&#38598;&#25104;&#26041;&#27861;&#23545;&#20110;&#25152;&#26377;&#19977;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;OOD&#26679;&#26412;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#36825;&#26679;&#30340;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#25317;&#26377;&#21487;&#20197;&#21028;&#26029;&#20309;&#26102;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#35780;&#20272;&#30340;&#21487;&#20449;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26816;&#27979;&#36234;&#21306;&#65288;OOD&#65289;&#26679;&#26412;&#26159;&#26500;&#24314;&#23433;&#20840;&#20998;&#31867;&#22120;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#36319;&#38543;&#20043;&#21069;&#30340;&#19968;&#39033;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#20415;&#25658;&#24335;&#36229;&#22768;&#22270;&#20687;&#20013;&#21487;&#20197;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;OOD&#26816;&#27979;&#65306;softmax&#12289;&#33021;&#37327;&#20998;&#25968;&#21644;&#28145;&#24230;&#38598;&#25104;&#12290;&#25152;&#26377;&#26041;&#27861;&#37117;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;OOD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#33021;&#37327;&#20998;&#25968;&#26041;&#27861;&#20248;&#20110;softmax&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#38598;&#25104;&#26041;&#27861;&#26368;&#20026;&#31283;&#20581;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#26368;&#25797;&#38271;&#26816;&#27979;OOD&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18960v1 Announce Type: cross  Abstract: Deep learning has shown to have great potential in medical applications. In critical domains as such, it is of high interest to have trustworthy algorithms which are able to tell when reliable assessments cannot be guaranteed. Detecting out-of-distribution (OOD) samples is a crucial step towards building a safe classifier. Following a previous study, showing that it is possible to classify breast cancer in point-of-care ultrasound images, this study investigates OOD detection using three different methods: softmax, energy score and deep ensembles. All methods are tested on three different OOD data sets. The results show that the energy score method outperforms the softmax method, performing well on two of the data sets. The ensemble method is the most robust, performing the best at detecting OOD samples for all three OOD data sets.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.18945</link><description>&lt;p&gt;
Syntactic Ghost&#65306;&#19968;&#31181;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#26080;&#24863;&#30693;&#36890;&#29992;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18945
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#34987;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21487;&#20197;&#23558;&#28431;&#27934;&#36716;&#31227;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PLM&#21518;&#38376;&#25915;&#20987;&#37319;&#29992;&#26126;&#26174;&#30340;&#35302;&#21457;&#22120;&#65292;&#22312;&#25163;&#21160;&#23545;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#65292;&#22240;&#27492;&#22312;&#25928;&#26524;&#12289;&#38544;&#21311;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#26399;&#26395;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19981;&#21487;&#35265;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#65292;&#31216;&#20026;Syntactic Ghost&#65288;&#31616;&#31216;&#20026;synGhost&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#25932;&#24847;&#22320;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#39044;&#23450;&#20041;&#21477;&#27861;&#32467;&#26500;&#30340;&#27602;&#23475;&#26679;&#26412;&#20316;&#20026;&#38544;&#34109;&#35302;&#21457;&#22120;&#65292;&#28982;&#21518;&#23558;&#21518;&#38376;&#26893;&#20837;&#21040;&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#65292;&#32780;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#30693;&#35782;&#12290;&#27602;&#23475;&#26679;&#26412;&#30340;&#36755;&#20986;&#34920;&#31034;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23613;&#21487;&#33021;&#22343;&#21248;&#22320;&#20998;&#24067;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#24418;&#25104;&#24191;&#27867;&#30340;&#21518;&#38376;&#12290;&#27492;&#22806;&#65292;&#22312;&#20142;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18945v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors. Additionally, in light 
&lt;/p&gt;</description></item><item><title>SemEval-2024&#30340;&#20219;&#21153;10&#26088;&#22312;&#35782;&#21035;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#24182;&#25214;&#20986;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#21442;&#19982;&#32773;&#38656;&#33258;&#21160;&#25191;&#34892;&#24773;&#32490;&#35782;&#21035;&#21644;&#24773;&#32490;&#36716;&#21464;&#25512;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18944</link><description>&lt;p&gt;
SemEval 2024 -- &#20219;&#21153;10&#65306;&#24773;&#32490;&#21457;&#29616;&#21450;&#23545;&#35805;&#20013;&#24773;&#32490;&#36716;&#21464;&#30340;&#25512;&#29702;&#65288;EDiReF&#65289;
&lt;/p&gt;
&lt;p&gt;
SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18944
&lt;/p&gt;
&lt;p&gt;
SemEval-2024&#30340;&#20219;&#21153;10&#26088;&#22312;&#35782;&#21035;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#24182;&#25214;&#20986;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#21442;&#19982;&#32773;&#38656;&#33258;&#21160;&#25191;&#34892;&#24773;&#32490;&#35782;&#21035;&#21644;&#24773;&#32490;&#36716;&#21464;&#25512;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SemEval-2024&#20219;&#21153;10&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#22312;&#21333;&#35821;&#31181;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;-&#33521;&#35821;&#28151;&#21512;&#23545;&#35805;&#20013;&#35782;&#21035;&#24773;&#32490;&#24182;&#25214;&#20986;&#24773;&#32490;&#36716;&#21464;&#32972;&#21518;&#21407;&#22240;&#30340;&#20849;&#20139;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153; - &#29992;&#20110;&#28151;&#21512;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#12289;&#28151;&#21512;&#23545;&#35805;&#20013;&#24773;&#32490;&#36716;&#21464;&#25512;&#29702;&#12289;&#20197;&#21450;&#33521;&#25991;&#23545;&#35805;&#20013;&#24773;&#32490;&#36716;&#21464;&#25512;&#29702;&#12290;&#21442;&#19982;&#31995;&#32479;&#34987;&#35201;&#27714;&#33258;&#21160;&#25191;&#34892;&#19968;&#20010;&#25110;&#22810;&#20010;&#36825;&#20123;&#23376;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#25163;&#21160;&#27880;&#37322;&#30340;&#23545;&#35805;&#65292;&#37325;&#28857;&#25918;&#22312;&#24773;&#32490;&#21644;&#35302;&#21457;&#24773;&#32490;&#36716;&#21464;&#30340;&#21407;&#22240;&#19978;&#65288;&#20219;&#21153;&#25968;&#25454;&#21487;&#22312;https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git&#33719;&#21462;&#65289;&#12290;&#24635;&#20849;&#26377;84&#20010;&#21442;&#19982;&#32773;&#21442;&#19982;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#20854;&#20013;&#26368;&#25797;&#38271;&#30340;&#31995;&#32479;&#22312;&#21508;&#20010;&#23376;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;0.70&#12289;0.79&#21644;0.76&#30340;F1&#20998;&#25968;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#26469;&#33258;24&#20010;&#22242;&#38431;&#30340;&#32467;&#26524;&#21644;&#21457;&#29616;&#20197;&#21450;&#20182;&#20204;&#31995;&#32479;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18944v1 Announce Type: cross  Abstract: We present SemEval-2024 Task 10, a shared task centred on identifying emotions and finding the rationale behind their flips within monolingual English and Hindi-English code-mixed dialogues. This task comprises three distinct subtasks - emotion recognition in conversation for code-mixed dialogues, emotion flip reasoning for code-mixed dialogues, and emotion flip reasoning for English dialogues. Participating systems were tasked to automatically execute one or more of these subtasks. The datasets for these tasks comprise manually annotated conversations focusing on emotions and triggers for emotion shifts (The task data is available at https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git). A total of 84 participants engaged in this task, with the most adept systems attaining F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks. This paper summarises the results and findings from 24 teams alongside their system descriptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36229;&#36234;Dropout&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;Dropout&#24341;&#20837;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18929</link><description>&lt;p&gt;
&#36229;&#36234;Dropout&#65306;&#36890;&#21521;&#21487;&#25512;&#24191;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#24341;&#20154;&#27880;&#30446;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable Image Super Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36229;&#36234;Dropout&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;Dropout&#24341;&#20837;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36817;&#24180;&#26469;&#22312;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SISR&#65289;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20551;&#35774;&#20102;&#31616;&#21333;&#19988;&#22266;&#23450;&#30340;&#38477;&#32423;&#27169;&#22411;&#65288;&#27604;&#22914;&#21452;&#19977;&#27425;&#19979;&#37319;&#26679;&#65289;&#65292;&#20294;&#30450;SR&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26410;&#30693;&#38477;&#32423;&#25913;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;Kong&#31561;&#20154;&#39318;&#27425;&#25506;&#35752;&#20102;&#20351;&#29992;Dropout&#36827;&#34892;&#30450;SR&#26356;&#21512;&#36866;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#36807;&#25311;&#21512;&#30830;&#23454;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#30340;&#27867;&#21270;&#25913;&#36827;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;Dropout&#21516;&#26102;&#24341;&#20837;&#20102;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#25439;&#23475;&#20102;&#27169;&#22411;&#24544;&#23454;&#37325;&#26500;&#32454;&#33410;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#35770;&#25991;&#20013;&#23637;&#31034;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;&#65292;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#31616;&#21333;&#35843;&#33410;&#27169;&#22411;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18929v1 Announce Type: cross  Abstract: Deep learning has led to a dramatic leap on Single Image Super-Resolution (SISR) performances in recent years. %Despite the substantial advancement% While most existing work assumes a simple and fixed degradation model (e.g., bicubic downsampling), the research of Blind SR seeks to improve model generalization ability with unknown degradation. Recently, Kong et al pioneer the investigation of a more suitable training strategy for Blind SR using Dropout. Although such method indeed brings substantial generalization improvements via mitigating overfitting, we argue that Dropout simultaneously introduces undesirable side-effect that compromises model's capacity to faithfully reconstruct fine details. We show both the theoretical and experimental analyses in our paper, and furthermore, we present another easy yet effective training strategy that enhances the generalization ability of the model by simply modulating its first and second-orde
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#30340;&#26144;&#23556;&#65292;&#20197;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#65292;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#20934;&#30830;&#12289;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.18920</link><description>&lt;p&gt;
&#20809;&#35889;&#36935;&#35265;&#31354;&#38388;: &#21644;&#35856;3D&#24418;&#29366;&#21305;&#37197;&#21644;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#30340;&#26144;&#23556;&#65292;&#20197;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#65292;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#20934;&#30830;&#12289;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;3D&#24418;&#29366;&#21305;&#37197;&#21644;&#25554;&#20540;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#34987;&#20998;&#24320;&#30740;&#31350;&#24182;&#20381;&#27425;&#24212;&#29992;&#20110;&#20851;&#32852;&#19981;&#21516;&#30340;3D&#24418;&#29366;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#21151;&#33021;&#26144;&#23556;&#26694;&#26550;&#19982;&#32463;&#20856;&#34920;&#38754;&#21464;&#24418;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#20013;&#26144;&#23556;&#24418;&#29366;&#12290;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#25972;&#21512;&#31354;&#38388;&#26144;&#23556;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20808;&#21069;&#29992;&#20110;&#24418;&#29366;&#21305;&#37197;&#30340;&#21151;&#33021;&#26144;&#23556;&#26041;&#27861;&#33719;&#24471;&#26356;&#31934;&#30830;&#21644;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#24341;&#20837;&#20809;&#35889;&#26144;&#23556;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25670;&#33073;&#20102;&#36890;&#24120;&#20351;&#29992;&#20294;&#35745;&#31639;&#26114;&#36149;&#30340;&#20165;&#23545;&#36817;&#31561;&#36317;&#24418;&#29366;&#21464;&#24418;&#26377;&#25928;&#30340;&#27979;&#22320;&#36317;&#31163;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18920v1 Announce Type: cross  Abstract: Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#27861; $\texttt{AdaMergeX}$&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36866;&#37197;&#22120;&#34701;&#21512;&#26469;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#21644;&#35821;&#35328;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.18913</link><description>&lt;p&gt;
AdaMergeX: &#36328;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#36866;&#37197;&#22120;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#27861; $\texttt{AdaMergeX}$&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36866;&#37197;&#22120;&#34701;&#21512;&#26469;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#21644;&#35821;&#35328;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#22312;&#29305;&#23450;&#35821;&#35328;&#30340;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#30452;&#25509;&#24494;&#35843;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#36890;&#36807;&#22312;&#28304;&#35821;&#35328;&#19978;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#36873;&#25321;&#21478;&#19968;&#20010;&#20219;&#21153;&#26469;&#35299;&#32806;&#20102;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#20998;&#31163;&#20102;&#8220;&#20219;&#21153;&#33021;&#21147;&#8221;&#21644;&#8220;&#35821;&#35328;&#33021;&#21147;&#8221;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26410;&#33021;&#20805;&#20998;&#23558;&#20219;&#21153;&#33021;&#21147;&#19982;&#28304;&#35821;&#35328;&#25110;&#32773;&#35821;&#35328;&#33021;&#21147;&#19982;&#36873;&#25321;&#30340;&#20219;&#21153;&#23436;&#20840;&#20998;&#24320;&#12290;&#26412;&#25991;&#25215;&#35748;&#20219;&#21153;&#33021;&#21147;&#21644;&#35821;&#35328;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#30446;&#26631;&#35821;&#35328;&#21644;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#20219;&#21153;&#24046;&#36317;&#19978;&#12290;&#30001;&#20110;&#35813;&#24046;&#36317;&#28040;&#38500;&#20102;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20551;&#23450;&#23427;&#22312;&#21508;&#20219;&#21153;&#38388;&#20445;&#25345;&#19968;&#33268;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; $\texttt{AdaMergeX}$ &#30340;&#26032;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36866;&#37197;&#22120;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18913v1 Announce Type: cross  Abstract: As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by decoupling ''task ability'' and ''language ability'' by fine-tuning on the target task in the source language and another selected task in the target language, respectively. However, they fail to fully separate the task ability from the source language or the language ability from the chosen task. In this paper, we acknowledge the mutual reliance between task ability and language ability and direct our attention toward the gap between the target language and the source language on tasks. As the gap removes the impact of tasks, we assume that it remains consistent across tasks. Based on this assumption, we propose a new cross-lingual transfer method called $\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a reference task, we can determine that 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#21482;&#26377;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#22240;&#26524;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#27867;&#21270;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;DIGIC&#65292;&#21487;&#20197;&#20316;&#20026;&#38750;&#32467;&#26500;&#21270;&#20551;&#35774;&#19979;&#22522;&#20110;&#36328;&#39046;&#22495;&#21464;&#21270;&#26041;&#27861;&#30340;&#34917;&#20805;</title><link>https://arxiv.org/abs/2402.18910</link><description>&lt;p&gt;
DIGIC: &#36890;&#36807;&#22240;&#26524;&#21457;&#29616;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIGIC: Domain Generalizable Imitation Learning by Causal Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18910
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#21482;&#26377;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#22240;&#26524;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#27867;&#21270;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;DIGIC&#65292;&#21487;&#20197;&#20316;&#20026;&#38750;&#32467;&#26500;&#21270;&#20551;&#35774;&#19979;&#22522;&#20110;&#36328;&#39046;&#22495;&#21464;&#21270;&#26041;&#27861;&#30340;&#34917;&#20805;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#24615;&#24050;&#32463;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#20102;&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#24378;&#22823;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36825;&#31867;&#26041;&#27861;&#38656;&#35201;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#36890;&#36807;&#36328;&#39046;&#22495;&#21464;&#21270;&#26469;&#35782;&#21035;&#24341;&#36215;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#26114;&#36149;&#29978;&#33267;&#19981;&#21487;&#34892;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#35782;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#20998;&#24067;&#26469;&#21457;&#29616;&#39046;&#22495;&#27867;&#21270;&#31574;&#30053;&#30340;&#22240;&#26524;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23581;&#35797;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;DIGIC&#30340;&#26032;&#39046;&#22495;&#27867;&#21270;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22240;&#26524;&#21457;&#29616;&#20174;&#28436;&#31034;&#25968;&#25454;&#20998;&#24067;&#20013;&#25214;&#20986;&#19987;&#23478;&#21160;&#20316;&#30340;&#30452;&#25509;&#21407;&#22240;&#26469;&#35782;&#21035;&#22240;&#26524;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#21482;&#26377;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24182;&#22312;&#22522;&#30784;&#22240;&#26524;&#27169;&#22411;&#30340;&#38750;&#32467;&#26500;&#21270;&#20551;&#35774;&#19979;&#20316;&#20026;&#22522;&#20110;&#36328;&#39046;&#22495;&#21464;&#21270;&#26041;&#27861;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18910v1 Announce Type: cross  Abstract: Causality has been combined with machine learning to produce robust representations for domain generalization. Most existing methods of this type require massive data from multiple domains to identify causal features by cross-domain variations, which can be expensive or even infeasible and may lead to misidentification in some cases. In this work, we make a different attempt by leveraging the demonstration data distribution to discover the causal features for a domain generalizable policy. We design a novel framework, called DIGIC, to identify the causal features by finding the direct cause of the expert action from the demonstration data distribution via causal discovery. Our framework can achieve domain generalizable imitation learning with only single-domain data and serve as a complement for cross-domain variation-based methods under non-structural assumptions on the underlying causal models. Our empirical study in various control 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#65292;&#26088;&#22312;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20316;&#20026;&#30693;&#35782;&#26356;&#26032;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#21644;&#21709;&#24212;&#24615;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18909</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#20107;&#23454;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#65306;&#36808;&#21521;&#23454;&#29992;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#65292;&#26088;&#22312;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20316;&#20026;&#30693;&#35782;&#26356;&#26032;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#21644;&#21709;&#24212;&#24615;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#23558;&#30693;&#35782;&#26356;&#26032;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#20854;&#20445;&#25345;&#27491;&#30830;&#24615;&#21644;&#26368;&#26032;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#31574;&#30053;&#26126;&#26174;&#19981;&#20999;&#23454;&#38469;&#65306;&#23427;&#20204;&#20165;&#20351;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#65288;&#20027;&#39064;&#12289;&#20851;&#31995;&#21644;&#23545;&#35937;&#30340;&#19977;&#20803;&#32452;&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#32780;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#26356;&#26032;&#36890;&#24120;&#20986;&#29616;&#22312;&#26032;&#38395;&#25991;&#31456;&#31561;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#12290;&#23427;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#30452;&#25509;&#35780;&#20272;&#32534;&#36753;&#24615;&#33021;&#65292;&#31216;&#20026;&#38750;&#32467;&#26500;&#21270;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;UKE&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#21709;&#24212;&#36805;&#36895;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#25104;&#20026;&#19968;&#20010;&#26356;&#23454;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#22312;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;UKE&#23545;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#23427;&#20204;&#30340;&#20851;&#38190;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18909v1 Announce Type: cross  Abstract: Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark. We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#35268;&#27169;&#25928;&#24212;&#30340;&#35774;&#26045;&#36873;&#22336;&#28216;&#25103;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#27604;&#20363;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#27604;&#20363;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36817;&#20284;&#26426;&#21046;&#35774;&#35745;&#35774;&#32622;&#19979;&#20195;&#29702;&#21487;&#33021;&#19981;&#20877;&#21333;&#23792;&#20559;&#22909;&#30340;&#26465;&#20214;&#19982;&#25104;&#26412;&#36817;&#20284;&#27604;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18908</link><description>&lt;p&gt;
&#20855;&#26377;&#35268;&#27169;&#25928;&#24212;&#30340;&#35774;&#26045;&#36873;&#22336;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Facility Location Games with Scaling Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18908
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#35268;&#27169;&#25928;&#24212;&#30340;&#35774;&#26045;&#36873;&#22336;&#28216;&#25103;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#27604;&#20363;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#27604;&#20363;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36817;&#20284;&#26426;&#21046;&#35774;&#35745;&#35774;&#32622;&#19979;&#20195;&#29702;&#21487;&#33021;&#19981;&#20877;&#21333;&#23792;&#20559;&#22909;&#30340;&#26465;&#20214;&#19982;&#25104;&#26412;&#36817;&#20284;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#20854;&#20013;&#27599;&#20010;&#20195;&#29702;&#30340;&#20010;&#20154;&#25104;&#26412;&#20989;&#25968;&#31561;&#20110;&#20182;&#20204;&#36317;&#31163;&#35774;&#26045;&#30340;&#36317;&#31163;&#20056;&#20197;&#19968;&#20010;&#30001;&#35774;&#26045;&#20301;&#32622;&#30830;&#23450;&#30340;&#27604;&#20363;&#22240;&#23376;&#12290;&#38500;&#20102;&#19968;&#33324;&#31867;&#21035;&#30340;&#36830;&#32493;&#27604;&#20363;&#20989;&#25968;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#30340;&#27604;&#20363;&#20989;&#25968;&#30340;&#20998;&#27573;&#32447;&#24615;&#27604;&#20363;&#20989;&#25968;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20851;&#27880;&#24635;&#25104;&#26412;&#21644;&#26368;&#22823;&#25104;&#26412;&#30340;&#30446;&#26631;&#65292;&#24182;&#25551;&#36848;&#20102;&#26368;&#20248;&#35299;&#30340;&#35745;&#31639;&#12290;&#28982;&#21518;&#25105;&#20204;&#36716;&#21521;&#36817;&#20284;&#26426;&#21046;&#35774;&#35745;&#35774;&#32622;&#65292;&#35266;&#23519;&#21040;&#20195;&#29702;&#30340;&#20559;&#22909;&#21487;&#33021;&#19981;&#20877;&#26159;&#21333;&#23792;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#30830;&#20445;&#20195;&#29702;&#20855;&#26377;&#21333;&#23792;&#20559;&#22909;&#30340;&#27604;&#20363;&#20989;&#25968;&#26465;&#20214;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#33021;&#22815;&#36890;&#36807;strategyproof&#21644;anonymous me&#36798;&#21040;&#30340;&#24635;&#25104;&#26412;&#21644;&#26368;&#22823;&#25104;&#26412;&#36817;&#20284;&#27604;&#29575;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18908v1 Announce Type: cross  Abstract: We take the classic facility location problem and consider a variation, in which each agent's individual cost function is equal to their distance from the facility multiplied by a scaling factor which is determined by the facility placement. In addition to the general class of continuous scaling functions, we also provide results for piecewise linear scaling functions which can effectively approximate or model the scaling of many real world scenarios. We focus on the objectives of total and maximum cost, describing the computation of the optimal solution. We then move to the approximate mechanism design setting, observing that the agents' preferences may no longer be single-peaked. Consequently, we characterize the conditions on scaling functions which ensure that agents have single-peaked preferences. Under these conditions, we find results on the total and maximum cost approximation ratios achievable by strategyproof and anonymous me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25506;&#32034;&#20102;&#20174;&#32447;&#24615;&#25506;&#27979;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#30340;&#39034;&#24207;&#24494;&#35843;&#29616;&#35937;&#21450;&#20854;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.18905</link><description>&lt;p&gt;
&#35770;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#30340;&#25910;&#25947;&#24615;&#65306;&#24212;&#32447;&#24615;&#25506;&#27979;&#36824;&#26159;&#23436;&#20840;&#24494;&#35843;&#65311;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Differentially-Private Fine-tuning: To Linearly Probe or to Fully Fine-tune?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25506;&#32034;&#20102;&#20174;&#32447;&#24615;&#25506;&#27979;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#30340;&#39034;&#24207;&#24494;&#35843;&#29616;&#35937;&#21450;&#20854;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#38750;&#31169;&#26377;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;DP&#20248;&#21270;&#25216;&#26415;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;DP&#35774;&#32622;&#20013;&#65292;&#24050;&#32463;&#35266;&#23519;&#21040;&#23436;&#20840;&#24494;&#35843;&#26377;&#26102;&#20505;&#24182;&#19981;&#24635;&#26159;&#20135;&#29983;&#26368;&#20339;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#21363;&#20351;&#23545;&#20110;&#20998;&#24067;&#20869;&#25968;&#25454;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#25991;&#65288;1&#65289;&#20998;&#26512;&#20102;DP&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20197;&#21450;&#65288;2&#65289;&#25506;&#32034;&#20102;&#39034;&#24207;&#24494;&#35843;&#30340;&#29616;&#35937;&#65292;&#20174;&#32447;&#24615;&#25506;&#27979;&#24320;&#22987;&#65292;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#65292;&#20197;&#21450;&#23427;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;DP&#24494;&#35843;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#30830;&#23450;&#38544;&#31169;&#39044;&#31639;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#23436;&#20840;&#24494;&#35843;&#20043;&#38388;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#23545;&#21508;&#31181;&#22522;&#20934;&#21644;&#27169;&#22411;&#30340;&#32463;&#39564;&#35780;&#20272;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18905v1 Announce Type: cross  Abstract: Differentially private (DP) machine learning pipelines typically involve a two-phase process: non-private pre-training on a public dataset, followed by fine-tuning on private data using DP optimization techniques. In the DP setting, it has been observed that full fine-tuning may not always yield the best test accuracy, even for in-distribution data. This paper (1) analyzes the training dynamics of DP linear probing (LP) and full fine-tuning (FT), and (2) explores the phenomenon of sequential fine-tuning, starting with linear probing and transitioning to full fine-tuning (LP-FT), and its impact on test loss. We provide theoretical insights into the convergence of DP fine-tuning within an overparameterized neural network and establish a utility curve that determines the allocation of privacy budget between linear probing and full fine-tuning. The theoretical results are supported by empirical evaluations on various benchmarks and models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#24335;&#36830;&#25509;&#35843;&#26597;&#20102;&#36830;&#32493;&#24494;&#35843;&#20013;&#19981;&#21516;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18865</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#20943;&#23569;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18865
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#24335;&#36830;&#25509;&#35843;&#26597;&#20102;&#36830;&#32493;&#24494;&#35843;&#20013;&#19981;&#21516;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#26377;&#30740;&#31350;&#26174;&#31034;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;LLMs&#19981;&#26029;&#22312;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#29305;&#23450;&#39046;&#22495;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#23545;&#21382;&#21490;&#20219;&#21153;&#30340;&#25512;&#29702;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#65292;&#36825;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#38656;&#35201;&#22312;&#23398;&#20064;&#21487;&#22609;&#24615;&#21644;&#35760;&#24518;&#31283;&#23450;&#24615;&#20043;&#38388;&#20445;&#25345;&#26435;&#34913;&#12290;&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#35832;&#22914;&#35760;&#24518;&#37325;&#25918;&#12289;&#27491;&#21017;&#21270;&#21644;&#21442;&#25968;&#38548;&#31163;&#31561;&#31574;&#30053;&#65292;&#20294;&#22312;&#36830;&#32493;&#30340;LLMs&#24494;&#35843;&#22330;&#26223;&#20013;&#65292;&#23545;&#21508;&#20010;&#30456;&#37051;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#24335;&#36830;&#25509;&#30340;&#35270;&#35282;&#35843;&#26597;&#20102;&#19981;&#21516;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20960;&#20309;&#36830;&#25509;&#65292;&#36825;&#24847;&#21619;&#30528;&#19981;&#21516;&#26497;&#23567;&#20540;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#20302;&#25439;&#22833;&#30340;&#23665;&#35895;&#30456;&#36830;&#25509;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLMs&#24494;&#35843;&#20013;&#30340;&#27169;&#24335;&#36830;&#25509;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18865v1 Announce Type: cross  Abstract: Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs contin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#36807;Y-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#24182;&#35774;&#35745;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21253;&#25324;&#23398;&#20064;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#21518;&#39564;&#27010;&#29575;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#35299;&#20915;&#25918;&#26494;&#32422;&#26463;&#24341;&#36215;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.18853</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24102;&#26377;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#30340;&#22810;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Rethinking Multi-domain Generalization with A General Learning Objective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#36807;Y-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#24182;&#35774;&#35745;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21253;&#25324;&#23398;&#20064;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#21518;&#39564;&#27010;&#29575;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#35299;&#20915;&#25918;&#26494;&#32422;&#26463;&#24341;&#36215;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#27867;&#21270;&#65288;mDG&#65289;&#30340;&#26222;&#36941;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#22686;&#24378;&#36793;&#38469;&#21040;&#26631;&#31614;&#20998;&#24067;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;mDG&#25991;&#29486;&#32570;&#20047;&#19968;&#20010;&#36890;&#29992;&#30340;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#24120;&#23545;&#38745;&#24577;&#30446;&#26631;&#36793;&#38469;&#20998;&#24067;&#26045;&#21152;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#19968;&#20010;$Y$-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;mDG&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#26469;&#35299;&#37322;&#21644;&#20998;&#26512;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;mDG&#26234;&#24935;&#12290;&#36825;&#20010;&#36890;&#29992;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#21327;&#21516;&#30340;&#30446;&#26631;&#65306;&#23398;&#20064;&#19982;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#19968;&#20010;&#21518;&#39564;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#20010;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#36825;&#20123;&#39033;&#32467;&#21512;&#20102;&#20808;&#39564;&#20449;&#24687;&#24182;&#25233;&#21046;&#20102;&#26080;&#25928;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20943;&#36731;&#20102;&#25918;&#26494;&#32422;&#26463;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20026;&#22495;&#23545;&#40784;&#25552;&#20379;&#20102;&#19968;&#20010;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18853v1 Announce Type: cross  Abstract: Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22788;&#26041;&#32593;&#32476;&#65288;PNNs&#65289;&#36825;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#35757;&#32451;&#65292;&#32467;&#21512;&#21453;&#20107;&#23454;&#20272;&#35745;&#65292;&#22312;&#21307;&#30103;&#20915;&#31574;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#21487;&#20248;&#21270;&#27835;&#30103;&#31574;&#30053;&#65292;&#24182;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26356;&#22797;&#26434;&#30340;&#31574;&#30053;&#32534;&#30721;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18851</link><description>&lt;p&gt;
&#22312;&#22788;&#26041;&#21644;&#39044;&#27979;&#20013;&#24212;&#29992;0-1&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Applications of 0-1 Neural Networks in Prescription and Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18851
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22788;&#26041;&#32593;&#32476;&#65288;PNNs&#65289;&#36825;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#35757;&#32451;&#65292;&#32467;&#21512;&#21453;&#20107;&#23454;&#20272;&#35745;&#65292;&#22312;&#21307;&#30103;&#20915;&#31574;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#21487;&#20248;&#21270;&#27835;&#30103;&#31574;&#30053;&#65292;&#24182;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26356;&#22797;&#26434;&#30340;&#31574;&#30053;&#32534;&#30721;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20915;&#31574;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#26377;&#38480;&#30340;&#35266;&#23519;&#25968;&#25454;&#19979;&#23398;&#20064;&#38024;&#23545;&#24739;&#32773;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22788;&#26041;&#32593;&#32476;&#65288;PNNs&#65289;&#65292;&#36825;&#26159;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#35757;&#32451;&#30340;&#27973;&#23618;0-1&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#19982;&#21453;&#20107;&#23454;&#20272;&#35745;&#19968;&#36215;&#22312;&#20013;&#31561;&#25968;&#25454;&#24773;&#20917;&#19979;&#20248;&#21270;&#31574;&#30053;&#12290;&#36825;&#20123;&#27169;&#22411;&#27604;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#32534;&#30721;&#27604;&#24120;&#35265;&#27169;&#22411;&#65288;&#22914;&#20915;&#31574;&#26641;&#65289;&#26356;&#22797;&#26434;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PNNs&#22312;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#21644;&#20135;&#21518;&#39640;&#34880;&#21387;&#27835;&#30103;&#20998;&#37197;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;PNNs&#34987;&#35777;&#26126;&#33021;&#22815;&#20135;&#29983;&#21487;&#38477;&#20302;&#39640;&#34880;&#21387;&#23792;&#20540;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18851v1 Announce Type: cross  Abstract: A key challenge in medical decision making is learning treatment policies for patients with limited observational data. This challenge is particularly evident in personalized healthcare decision-making, where models need to take into account the intricate relationships between patient characteristics, treatment options, and health outcomes. To address this, we introduce prescriptive networks (PNNs), shallow 0-1 neural networks trained with mixed integer programming that can be used with counterfactual estimation to optimize policies in medium data settings. These models offer greater interpretability than deep neural networks and can encode more complex policies than common models such as decision trees. We show that PNNs can outperform existing methods in both synthetic data experiments and in a case study of assigning treatments for postpartum hypertension. In particular, PNNs are shown to produce policies that could reduce peak bloo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;NLP&#22823;&#22411;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;LSB-NLP&#28151;&#21512;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#38544;&#20889;&#25991;&#26412;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#20013;&#25991;&#23383;&#31526;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.18849</link><description>&lt;p&gt;
&#25552;&#21319;&#38544;&#20889;&#25991;&#26412;&#25552;&#21462;&#65306;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23545;&#20934;&#30830;&#24615;&#21644;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18849
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;NLP&#22823;&#22411;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;LSB-NLP&#28151;&#21512;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#38544;&#20889;&#25991;&#26412;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#20013;&#25991;&#23383;&#31526;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#38544;&#20889;&#26415;&#25216;&#26415;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22823;&#22411;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#25552;&#21462;&#38544;&#20889;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20256;&#32479;&#30340;&#26368;&#20302;&#26377;&#25928;&#20301;&#65288;LSB&#65289;&#38544;&#20889;&#26415;&#25216;&#26415;&#22312;&#22788;&#29702;&#22797;&#26434;&#23383;&#31526;&#32534;&#30721;&#65288;&#22914;&#20013;&#25991;&#23383;&#31526;&#65289;&#26102;&#22312;&#20449;&#24687;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LSB-NLP&#28151;&#21512;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#38598;&#25104;&#20102;NLP&#22823;&#22411;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#22914;&#38169;&#35823;&#26816;&#27979;&#12289;&#32416;&#27491;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#20998;&#26512;&#65292;&#20197;&#21450;&#20449;&#24687;&#37325;&#24314;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#38544;&#20889;&#25991;&#26412;&#25552;&#21462;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LSB-NLP&#28151;&#21512;&#26694;&#26550;&#22312;&#25552;&#39640;&#38544;&#20889;&#25991;&#26412;&#25552;&#21462;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20013;&#25991;&#23383;&#31526;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18849v1 Announce Type: cross  Abstract: This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#21644;&#34394;&#25311;&#27835;&#30103;&#25216;&#26415;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26356;&#24191;&#27867;&#30340;&#25509;&#35302;&#26426;&#20250;&#65292;&#20294;&#22312;&#23454;&#26045;&#20013;&#38656;&#35201;&#24179;&#34913;&#25928;&#29575;&#21644;&#21516;&#29702;&#24515;&#65292;&#20197;&#30830;&#20445;&#25216;&#26415;&#22987;&#32456;&#26159;&#30001;&#21307;&#25252;&#20154;&#21592;&#30340;&#26234;&#24935;&#25351;&#23548;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.18826</link><description>&lt;p&gt;
&#26426;&#22120;&#26080;&#27861;&#21462;&#20195;&#20154;&#31867;&#30340;&#24515;&#28789;
&lt;/p&gt;
&lt;p&gt;
The Machine Can't Replace the Human Heart
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18826
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#34394;&#25311;&#27835;&#30103;&#25216;&#26415;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26356;&#24191;&#27867;&#30340;&#25509;&#35302;&#26426;&#20250;&#65292;&#20294;&#22312;&#23454;&#26045;&#20013;&#38656;&#35201;&#24179;&#34913;&#25928;&#29575;&#21644;&#21516;&#29702;&#24515;&#65292;&#20197;&#30830;&#20445;&#25216;&#26415;&#22987;&#32456;&#26159;&#30001;&#21307;&#25252;&#20154;&#21592;&#30340;&#26234;&#24935;&#25351;&#23548;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18826v1 Announce Type: cross  Abstract: What is the true heart of mental healthcare -- innovation or humanity? Can virtual therapy ever replicate the profound human bonds where healing arises? As artificial intelligence and immersive technologies promise expanded access, safeguards must ensure technologies remain supplementary tools guided by providers' wisdom. Implementation requires nuance balancing efficiency and empathy. If conscious of ethical risks, perhaps AI could restore humanity by automating tasks, giving providers more time to listen. Yet no algorithm can replicate the seat of dignity within. We must ask ourselves: What future has people at its core? One where AI thoughtfully plays a collaborative role? Or where pursuit of progress leaves vulnerability behind? This commentary argues for a balanced approach thoughtfully integrating technology while retaining care's irreplaceable human essence, at the heart of this profoundly human profession. Ultimately, by nurtur
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.18815</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Handle Multilingualism?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18815
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#35821;&#35328;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;LLMs&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#36807;&#31243;&#65306;&#22312;&#21069;&#20960;&#23618;&#20013;&#65292;LLMs&#29702;&#35299;&#38382;&#39064;&#65292;&#23558;&#22810;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#33521;&#35821;&#20197;&#20415;&#20419;&#36827;&#20219;&#21153;&#35299;&#20915;&#38454;&#27573;&#12290;&#22312;&#20013;&#38388;&#23618;&#20013;&#65292;LLMs&#36890;&#36807;&#20197;&#33521;&#35821;&#24605;&#32771;&#24182;&#25972;&#21512;&#22810;&#35821;&#35328;&#30693;&#35782;&#26469;&#36827;&#34892;&#35299;&#20915;&#38382;&#39064;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32467;&#26500;&#65292;&#20998;&#21035;&#33719;&#21462;&#20107;&#23454;&#20869;&#23481;&#12290;&#22312;&#26368;&#21518;&#20960;&#23618;&#20013;&#65292;LLMs&#29983;&#25104;&#19982;&#26597;&#35810;&#30340;&#21407;&#22987;&#35821;&#35328;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#29305;&#23450;&#35821;&#35328;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#26816;&#27979;&#30001;&#36755;&#20837;&#35821;&#35328;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#65292;&#21363;&#20351;&#27809;&#26377;&#26631;&#31614;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#35821;&#35328;&#29305;&#23450;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35282;&#33394;&#25198;&#28436;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#26631;&#21644;&#25351;&#23548;&#20197;&#22686;&#24378;&#20854;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18807</link><description>&lt;p&gt;
&#35770;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35282;&#33394;&#25198;&#28436;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Decision-Making Abilities in Role-Playing using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35282;&#33394;&#25198;&#28436;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#26631;&#21644;&#25351;&#23548;&#20197;&#22686;&#24378;&#20854;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29616;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35282;&#33394;&#25198;&#28436;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#20223;&#29305;&#23450;&#39046;&#22495;&#19987;&#23478;&#26102;&#65292;&#20027;&#35201;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#20114;&#21160;&#26102;&#65292;&#35282;&#33394;&#30340;&#20915;&#31574;&#33021;&#21147;&#26174;&#33879;&#22320;&#22609;&#36896;&#20854;&#34892;&#20026;&#27169;&#24335;&#12290;&#26412;&#25991;&#38598;&#20013;&#35780;&#20272;LLMs&#22312;&#35282;&#33394;&#25198;&#28436;&#21518;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#20174;&#32780;&#39564;&#35777;&#35282;&#33394;&#25198;&#28436;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#22686;&#24378;LLMs&#22312;&#35282;&#33394;&#25198;&#28436;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#25552;&#20379;&#25351;&#26631;&#21644;&#25351;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;LLMs&#29983;&#25104;&#23545;&#24212;&#20110;&#36808;&#23572;&#26031;-&#24067;&#37324;&#26684;&#26031;&#31867;&#22411;&#25351;&#26631;&#65288;MBTI&#65289;&#30340;16&#31181;&#20154;&#26684;&#31867;&#22411;&#30340;&#34394;&#25311;&#35282;&#33394;&#25551;&#36848;&#65292;&#20195;&#34920;&#20154;&#21475;&#30340;&#32454;&#20998;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#20307;&#30340;&#23450;&#37327;&#25805;&#20316;&#65292;&#20174;&#36866;&#24212;&#24615;&#12289;&#25506;&#32034;&#24615;&#31561;&#22235;&#20010;&#26041;&#38754;&#35780;&#20272;LLMs&#22312;&#35282;&#33394;&#25198;&#28436;&#21518;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18807v1 Announce Type: cross  Abstract: Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts. When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks. Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\&amp;$ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-inspired and Self-based Artificial Intelligence&#65288;BriSe AI&#65289;&#30340;&#26032;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#25105;&#32452;&#32455;&#30340;&#26041;&#24335;&#21327;&#35843;&#21508;&#31181;&#35748;&#30693;&#21151;&#33021;&#21644;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26500;&#24314;&#20154;&#31867;&#27700;&#24179;&#30340;AI&#27169;&#22411;&#21644;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18784</link><description>&lt;p&gt;
&#22823;&#33041;&#21551;&#21457;&#21644;&#22522;&#20110;&#33258;&#25105;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired and Self-based Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-inspired and Self-based Artificial Intelligence&#65288;BriSe AI&#65289;&#30340;&#26032;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#25105;&#32452;&#32455;&#30340;&#26041;&#24335;&#21327;&#35843;&#21508;&#31181;&#35748;&#30693;&#21151;&#33021;&#21644;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26500;&#24314;&#20154;&#31867;&#27700;&#24179;&#30340;AI&#27169;&#22411;&#21644;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25361;&#25112;&#20102;&#24403;&#21069;AI&#30340;&#25903;&#25345;&#32773;&#25152;&#35859; "&#24605;&#32771;&#26426;&#22120;" &#30340;&#35266;&#24565;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#33258;&#25105;&#24847;&#35782;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-inspired and Self-based Artificial Intelligence&#65288;BriSe AI&#65289;&#30340;&#26032;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#25105;&#32452;&#32455;&#30340;&#26041;&#24335;&#21327;&#35843;&#21508;&#31181;&#35748;&#30693;&#21151;&#33021;&#21644;&#23398;&#20064;&#31574;&#30053;&#65292;&#26500;&#24314;&#20154;&#31867;&#27700;&#24179;&#30340;AI&#27169;&#22411;&#21644;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18784v1 Announce Type: new  Abstract: The question "Can machines think?" and the Turing Test to assess whether machines could achieve human-level intelligence is one of the roots of AI. With the philosophical argument "I think, therefore I am", this paper challenge the idea of a "thinking machine" supported by current AIs since there is no sense of self in them. Current artificial intelligence is only seemingly intelligent information processing and does not truly understand or be subjectively aware of oneself and perceive the world with the self as human intelligence does. In this paper, we introduce a Brain-inspired and Self-based Artificial Intelligence (BriSe AI) paradigm. This BriSe AI paradigm is dedicated to coordinating various cognitive functions and learning strategies in a self-organized manner to build human-level AI models and robotic applications. Specifically, BriSe AI emphasizes the crucial role of the Self in shaping the future AI, rooted with a practical hi
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18759</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#29366;&#24577;&#25277;&#35937;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Language-Guided State Abstractions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18759
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#35774;&#35745;&#29366;&#24577;&#25277;&#35937;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#22312;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#23454;&#29616;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#30340;&#20851;&#38190;&#22312;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#36825;&#21487;&#20197;&#23558;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#23637;&#29616;&#20986;&#26469;&#24182;&#38544;&#34255;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#29366;&#24577;&#34920;&#31034;&#36890;&#24120;&#26159;&#25163;&#21160;&#25351;&#23450;&#30340;&#65292;&#25110;&#32773;&#26159;&#20174;&#20854;&#20182;&#32321;&#37325;&#30340;&#26631;&#35760;&#36807;&#31243;&#20013;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;LGA&#65288;&#35821;&#35328;&#24341;&#23548;&#30340;&#25277;&#35937;&#65289;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#30693;&#35782;&#30340;&#32467;&#21512;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;LGA&#20013;&#65292;&#29992;&#25143;&#39318;&#20808;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#20379;&#30446;&#26631;&#20219;&#21153;&#30340;&#65288;&#21487;&#33021;&#26159;&#19981;&#23436;&#25972;&#30340;&#65289;&#25551;&#36848;&#65307;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#36825;&#20010;&#20219;&#21153;&#25551;&#36848;&#36716;&#21270;&#20026;&#25513;&#30422;&#19981;&#30456;&#20851;&#29305;&#24449;&#30340;&#29366;&#24577;&#25277;&#35937;&#20989;&#25968;&#65307;&#26368;&#21518;&#65292;&#20351;&#29992;&#23569;&#37327;&#28436;&#31034;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#27169;&#20223;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18759v1 Announce Type: cross  Abstract: We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones. These state representations are typically manually specified, or derived from other labor-intensive labeling procedures. Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks. In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demo
&lt;/p&gt;</description></item><item><title>&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#21644;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.18747</link><description>&lt;p&gt;
&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18747
&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#21644;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#28085;&#30422;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;11&#31181;&#35821;&#35328;&#23545;&#30340;&#24191;&#27867;&#30340;&#22810;&#32500;&#36136;&#37327;&#24230;&#37327;(MQM)&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#25506;&#31350;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#26102;&#65292;&#26159;&#21542;&#37027;&#20123;&#26681;&#25454;&#20154;&#24037;&#29983;&#25104;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#21028;&#26029;&#36827;&#34892;&#32454;&#35843;&#30340;MT&#24230;&#37327;&#26159;&#31283;&#20581;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26410;&#30693;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#32454;&#35843;&#30340;&#24230;&#37327;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#20197;&#21450;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18747v1 Announce Type: cross  Abstract: We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference. We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to metrics that rely on the surface form, as well as pre-trained metrics which are not fine-tuned on MT quality judgments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#25490;&#21517;&#21644;&#36807;&#28388;&#31995;&#32479;&#20943;&#23569;&#22810;&#26080;&#20154;&#26426;&#20219;&#21153;&#35268;&#21010;&#20013;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#35746;&#30340;&#22810;&#20934;&#21017;&#20915;&#31574;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18743</link><description>&lt;p&gt;
&#22810;&#26080;&#20154;&#26426;&#20219;&#21153;&#35268;&#21010;&#25903;&#25345;&#30340;&#22810;&#20934;&#21017;&#20915;&#31574;&#26041;&#27861;&#20462;&#35746;
&lt;/p&gt;
&lt;p&gt;
A revision on Multi-Criteria Decision Making methods for Multi-UAV Mission Planning Support
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#25490;&#21517;&#21644;&#36807;&#28388;&#31995;&#32479;&#20943;&#23569;&#22810;&#26080;&#20154;&#26426;&#20219;&#21153;&#35268;&#21010;&#20013;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#35746;&#30340;&#22810;&#20934;&#21017;&#20915;&#31574;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#30001;&#20110;&#20854;&#26131;&#31649;&#29702;&#24615;&#21644;&#39118;&#38505;&#35268;&#36991;&#33021;&#21147;&#65292;&#26080;&#20154;&#26426;&#22312;&#35768;&#22810;&#21830;&#19994;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22810;&#26080;&#20154;&#26426;&#30340;&#20219;&#21153;&#35268;&#21010;&#65292;&#38656;&#35201;&#25214;&#21040;&#19968;&#20010;&#28385;&#36275;&#38382;&#39064;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#38382;&#39064;&#26377;&#22810;&#20010;&#21516;&#26102;&#38656;&#35201;&#20248;&#21270;&#30340;&#21464;&#37327;&#65292;&#22914;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#12289;&#20219;&#21153;&#25104;&#26412;&#25110;&#39118;&#38505;&#31561;&#12290;&#22240;&#27492;&#65292;&#38382;&#39064;&#26377;&#24456;&#22810;&#21487;&#33021;&#30340;&#26368;&#20248;&#35299;&#65292;&#25805;&#20316;&#21592;&#24517;&#39035;&#22312;&#20854;&#20013;&#36873;&#25321;&#35201;&#25191;&#34892;&#30340;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#20943;&#23569;&#25805;&#20316;&#21592;&#22312;&#36825;&#20010;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24037;&#20316;&#37327;&#65292;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#21464;&#24471;&#24517;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#25490;&#21517;&#21644;&#36807;&#28388;&#31995;&#32479;&#32452;&#25104;&#30340;DSS&#65292;&#29992;&#20110;&#25490;&#24207;&#21644;&#20943;&#23569;&#26368;&#20248;&#35299;&#12290;&#23601;&#25490;&#21517;&#31995;&#32479;&#32780;&#35328;&#65292;&#28041;&#21450;&#24191;&#27867;&#30340;&#22810;&#20934;&#21017;&#20915;&#31574;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18743v1 Announce Type: new  Abstract: Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively used in many commercial applications due to their manageability and risk avoidance. One of the main problems considered is the Mission Planning for multiple UAVs, where a solution plan must be found satisfying the different constraints of the problem. This problem has multiple variables that must be optimized simultaneously, such as the makespan, the cost of the mission or the risk. Therefore, the problem has a lot of possible optimal solutions, and the operator must select the final solution to be executed among them. In order to reduce the workload of the operator in this decision process, a Decision Support System (DSS) becomes necessary. In this work, a DSS consisting of ranking and filtering systems, which order and reduce the optimal solutions, has been designed. With regard to the ranking system, a wide range of Multi-Criteria Decision Making (MCDM) method
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#29983;&#25104;AI&#26550;&#26500;GAIA&#65292;&#37319;&#29992;&#23618;&#27425;&#27169;&#22411;&#21644;&#21333;&#32431;&#22797;&#21512;&#20307;&#32452;&#32455;&#27169;&#22359;&#65292;&#23558;&#21442;&#25968;&#26356;&#26032;&#24314;&#27169;&#20026;&#21333;&#32431;&#38598;&#19978;&#30340;&#25552;&#21319;&#22270;&#34920;&#65292;&#24182;&#37319;&#29992;&#20313;&#20195;&#25968;&#30340;&#24418;&#24335;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18732</link><description>&lt;p&gt;
GAIA: &#29983;&#25104;AI&#30340;&#33539;&#30068;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
GAIA: Categorical Foundations of Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#29983;&#25104;AI&#26550;&#26500;GAIA&#65292;&#37319;&#29992;&#23618;&#27425;&#27169;&#22411;&#21644;&#21333;&#32431;&#22797;&#21512;&#20307;&#32452;&#32455;&#27169;&#22359;&#65292;&#23558;&#21442;&#25968;&#26356;&#26032;&#24314;&#27169;&#20026;&#21333;&#32431;&#38598;&#19978;&#30340;&#25552;&#21319;&#22270;&#34920;&#65292;&#24182;&#37319;&#29992;&#20313;&#20195;&#25968;&#30340;&#24418;&#24335;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GAIA&#65292;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#29983;&#25104;AI&#26550;&#26500;&#12290;GAIA&#22522;&#20110;&#19968;&#20010;&#23618;&#27425;&#27169;&#22411;&#65292;&#20854;&#20013;&#27169;&#22359;&#34987;&#32452;&#32455;&#20026;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#12290;&#27599;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#26681;&#25454;&#20174;&#20854;&#19978;&#32423;&#21333;&#32431;&#20307;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#26356;&#26032;&#20854;&#20869;&#37096;&#21442;&#25968;&#65292;&#24182;&#23558;&#26356;&#26032;&#20256;&#36882;&#32473;&#20854;&#19979;&#32423;&#23376;&#21333;&#32431;&#20307;&#12290;&#21442;&#25968;&#26356;&#26032;&#20197;&#21333;&#32431;&#38598;&#19978;&#30340;&#25552;&#21319;&#22270;&#34920;&#30340;&#24418;&#24335;&#36827;&#34892;&#65292;&#20854;&#20013;&#20869;&#37096;&#21644;&#22806;&#37096;&#35282;&#25193;&#23637;&#23545;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#21453;&#21521;&#20256;&#25773;&#34987;&#24314;&#27169;&#20026;&#21442;&#25968;&#33539;&#30068;&#19978;&#30340;&#19968;&#20010;&#33258;&#20989;&#23376;&#65292;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#20313;&#20195;&#25968;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18732v1 Announce Type: new  Abstract: In this paper, we propose GAIA, a generative AI architecture based on category theory. GAIA is based on a hierarchical model where modules are organized as a simplicial complex. Each simplicial complex updates its internal parameters biased on information it receives from its superior simplices and in turn relays updates to its subordinate sub-simplices. Parameter updates are formulated in terms of lifting diagrams over simplicial sets, where inner and outer horn extensions correspond to different types of learning problems. Backpropagation is modeled as an endofunctor over the category of parameters, leading to a coalgebraic formulation of deep learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#35760;&#24518;&#19982;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#24314;&#31435;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#35760;&#24518;&#21644;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.18726</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#31169;&#12289;&#35760;&#24518;&#21644;&#36755;&#20837;&#26354;&#29575;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Unveiling Privacy, Memorization, and Input Curvature Links
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#35760;&#24518;&#19982;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#24314;&#31435;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#35760;&#24518;&#21644;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#25104;&#20026;&#35299;&#20915;&#35768;&#22810;&#26032;&#20852;&#38382;&#39064;&#30340;&#26222;&#36941;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#36807;&#24230;&#25311;&#21512;&#21644;&#35760;&#24518;&#35757;&#32451;&#38598;&#12290;&#35760;&#24518;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#19982;&#35832;&#22810;&#27010;&#24565;&#22914;&#27867;&#21270;&#12289;&#26377;&#22122;&#23398;&#20064;&#21644;&#38544;&#31169;&#23494;&#20999;&#30456;&#20851;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#65288;&#36890;&#36807;&#25439;&#22833;Hessian&#30697;&#38453;&#23545;&#36755;&#20837;&#30340;&#36857;&#36827;&#34892;&#27979;&#37327;&#65289;&#19982;&#35760;&#24518;&#20043;&#38388;&#30340;&#32463;&#39564;&#24615;&#32852;&#31995;&#12290;&#23427;&#34987;&#35777;&#26126;&#27604;&#35745;&#31639;&#35760;&#24518;&#20998;&#25968;&#35201;&#39640;&#25928;&#32422;3&#20010;&#25968;&#37327;&#32423;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23558;&#35760;&#24518;&#19982;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#32852;&#31995;&#36215;&#26469;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#26412;&#25991;&#19981;&#20165;&#30740;&#31350;&#20102;&#36825;&#31181;&#32852;&#31995;&#65292;&#36824;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#35760;&#24518;&#21644;&#36755;&#20837;&#25439;&#22833;&#26354;&#29575;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18726v1 Announce Type: cross  Abstract: Deep Neural Nets (DNNs) have become a pervasive tool for solving many emerging problems. However, they tend to overfit to and memorize the training set. Memorization is of keen interest since it is closely related to several concepts such as generalization, noisy learning, and privacy. To study memorization, Feldman (2019) proposed a formal score, however its computational requirements limit its practical use. Recent research has shown empirical evidence linking input loss curvature (measured by the trace of the loss Hessian w.r.t inputs) and memorization. It was shown to be ~3 orders of magnitude more efficient than calculating the memorization score. However, there is a lack of theoretical understanding linking memorization with input loss curvature. In this paper, we not only investigate this connection but also extend our analysis to establish theoretical links between differential privacy, memorization, and input loss curvature. F
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20851;&#32852;&#35760;&#24518;&#27169;&#22359;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#25581;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#21160;&#24577;&#21644;&#35823;&#24046;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18724</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20851;&#32852;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Learning Associative Memories with Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20851;&#32852;&#35760;&#24518;&#27169;&#22359;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#25581;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#21160;&#24577;&#21644;&#35823;&#24046;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#23384;&#20648;&#26631;&#35760;&#23884;&#20837;&#30340;&#22806;&#31215;&#30340;&#19968;&#20010;&#20851;&#32852;&#35760;&#24518;&#27169;&#22359;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#30740;&#31350;&#19968;&#20010;&#31890;&#23376;&#31995;&#32479;&#65292;&#36825;&#20123;&#31890;&#23376;&#26681;&#25454;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#24615;&#20197;&#21450;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20132;&#20114;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#8220;&#20998;&#31867;&#36793;&#30028;&#8221;&#30340;&#23545;&#25968;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#26631;&#35760;&#39057;&#29575;&#30340;&#19981;&#24179;&#34913;&#21644;&#30001;&#30456;&#20851;&#23884;&#20837;&#23548;&#33268;&#30340;&#20869;&#23384;&#24178;&#25200;&#20250;&#23548;&#33268;&#25391;&#33633;&#30340;&#30636;&#24577;&#21306;&#22495;&#12290;&#25391;&#33633;&#22312;&#27493;&#38271;&#36739;&#22823;&#26102;&#26356;&#20026;&#26126;&#26174;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#33391;&#24615;&#25439;&#22833;&#23792;&#65292;&#23613;&#31649;&#36825;&#20123;&#23398;&#20064;&#29575;&#21152;&#36895;&#20102;&#21160;&#24577;&#24182;&#21152;&#36895;&#20102;&#28176;&#36817;&#25910;&#25947;&#12290;&#22312;&#27424;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#22914;&#20309;&#23548;&#33268;&#27425;&#20248;&#30340;&#35760;&#24518;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#22312;&#23567;&#35268;&#27169;Tr&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18724v1 Announce Type: cross  Abstract: This work focuses on the training dynamics of one associative memory module storing outer products of token embeddings. We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings. Through theory and experiments, we provide several insights. In overparameterized regimes, we obtain logarithmic growth of the ``classification margins.'' Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes. The oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence. In underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes. Finally, we assess the validity of our findings on small Tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#21253;&#21547;104&#20010;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#30340;&#38598;&#21512;&#65292;&#36825;&#20123;&#27169;&#24335;&#20195;&#34920;&#24120;&#35265;&#30340;&#21517;&#35789;&#65292;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24120;&#35782;&#30693;&#35782;&#20013;&#31574;&#21010;&#65292;&#32452;&#32455;&#25104;&#19968;&#20010;&#27880;&#37322;&#23436;&#25972;&#30340;&#27169;&#22359;&#21270;&#26412;&#20307;&#35774;&#35745;&#24211;&#65292;&#21487;&#19982;MOMo&#19968;&#36215;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18715</link><description>&lt;p&gt;
&#24120;&#35782;&#26412;&#20307;&#24494;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Commonsense Ontology Micropatterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#21253;&#21547;104&#20010;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#30340;&#38598;&#21512;&#65292;&#36825;&#20123;&#27169;&#24335;&#20195;&#34920;&#24120;&#35265;&#30340;&#21517;&#35789;&#65292;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24120;&#35782;&#30693;&#35782;&#20013;&#31574;&#21010;&#65292;&#32452;&#32455;&#25104;&#19968;&#20010;&#27880;&#37322;&#23436;&#25972;&#30340;&#27169;&#22359;&#21270;&#26412;&#20307;&#35774;&#35745;&#24211;&#65292;&#21487;&#19982;MOMo&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#24341;&#20837;&#30340;&#27169;&#22359;&#26412;&#20307;&#24314;&#27169;&#26041;&#27861;&#65288;MOMo&#65289;&#35797;&#22270;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#27169;&#24335;&#26469;&#25340;&#20945;&#26356;&#22797;&#26434;&#30340;&#27010;&#24565;&#65292;&#20197;&#27169;&#20223;&#20154;&#31867;&#31867;&#27604;&#36807;&#31243;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#28857;&#65292;MOMo&#23558;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#32452;&#32455;&#21040;&#35774;&#35745;&#24211;&#20013;&#65292;&#36825;&#20123;&#35774;&#35745;&#24211;&#21487;&#20197;&#36890;&#36807;&#31243;&#24207;&#26597;&#35810;&#65292;&#20197;&#25903;&#25345;&#21152;&#36895;&#26412;&#20307;&#24320;&#21457;&#65292;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;&#33258;&#21160;&#21270;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;MOMo&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#26159;&#65288;&#36804;&#20170;&#20026;&#27490;&#65289;&#20934;&#22791;&#23601;&#32490;&#30340;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#36805;&#36895;&#25104;&#20026;&#36890;&#35782;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21462;&#20195;&#25628;&#32034;&#24341;&#25806;&#22238;&#31572;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#21253;&#21547;104&#20010;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#30340;&#38598;&#21512;&#65292;&#36825;&#20123;&#27169;&#24335;&#20195;&#34920;&#24120;&#35265;&#30340;&#21517;&#35789;&#65292;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24120;&#35782;&#30693;&#35782;&#20013;&#31574;&#21010;&#65292;&#32452;&#32455;&#25104;&#19968;&#20010;&#27880;&#37322;&#23436;&#25972;&#30340;&#27169;&#22359;&#21270;&#26412;&#20307;&#35774;&#35745;&#24211;&#65292;&#21487;&#19982;MOMo&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18715v1 Announce Type: new  Abstract: The previously introduced Modular Ontology Modeling methodology (MOMo) attempts to mimic the human analogical process by using modular patterns to assemble more complex concepts. To support this, MOMo organizes organizes ontology design patterns into design libraries, which are programmatically queryable, to support accelerated ontology development, for both human and automated processes. However, a major bottleneck to large-scale deployment of MOMo is the (to-date) limited availability of ready-to-use ontology design patterns. At the same time, Large Language Models have quickly become a source of common knowledge and, in some cases, replacing search engines for questions. In this paper, we thus present a collection of 104 ontology design patterns representing often occurring nouns, curated from the common-sense knowledge available in LLMs, organized into a fully-annotated modular ontology design library ready for use with MOMo.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18700</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning to Compress Prompt in Natural Language Formats
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#22788;&#29702;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#38271;&#19978;&#19979;&#25991;&#12289;&#25512;&#29702;&#36895;&#24230;&#24930;&#20197;&#21450;&#35745;&#31639;&#32467;&#26524;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;&#37096;&#32626;&#20855;&#26377;&#31934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#19978;&#19979;&#25991;&#30340;LLMs&#26377;&#21161;&#20110;&#29992;&#25143;&#26356;&#26377;&#25928;&#21644;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#20316;&#21697;&#20381;&#36182;&#23558;&#38271;&#25552;&#31034;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#36719;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36719;&#25552;&#31034;&#21387;&#32553;&#22312;&#19981;&#21516;LLM&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;API&#30340;LLMs&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20197;LLM&#21487;&#36716;&#31227;&#24615;&#30340;&#24418;&#24335;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#12290;&#36825;&#24102;&#26469;&#20004;&#20010;&#25361;&#25112;&#65306;(i) &#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#25552;&#31034;&#19981;&#20860;&#23481;&#21453;&#21521;&#20256;&#25773;&#65292;(ii) NL&#25552;&#31034;&#22312;&#26045;&#21152;&#38271;&#24230;&#32422;&#26463;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18700v1 Announce Type: cross  Abstract: Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framewor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18679</link><description>&lt;p&gt;
&#25968;&#25454;&#35299;&#37322;&#22120;&#65306;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Interpreter: An LLM Agent For Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23454;&#26102;&#25968;&#25454;&#35843;&#25972;&#12289;&#20248;&#21270;&#19987;&#19994;&#30693;&#35782;&#20197;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#38388;&#22797;&#26434;&#20381;&#36182;&#24615;&#20197;&#21450;&#31934;&#30830;&#25512;&#29702;&#30340;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#30340;&#25968;&#25454;&#31185;&#23398;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#24378;&#35843;&#19977;&#31181;&#20851;&#38190;&#25216;&#26415;&#20197;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#26696;&#30340;&#20195;&#30721;&#65306;1&#65289;&#20855;&#26377;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#29992;&#20110;&#23454;&#26102;&#25968;&#25454;&#36866;&#24212;&#24615;&#65307;2&#65289;&#24037;&#20855;&#38598;&#25104;&#21160;&#24577;&#21270;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#29087;&#32451;&#24230;&#65292;&#20016;&#23500;&#24517;&#35201;&#30340;&#19987;&#19994;&#30693;&#35782;&#65307;3&#65289;&#22312;&#21453;&#39304;&#20013;&#35782;&#21035;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35760;&#24405;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#31185;&#23398;&#21644;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#19982;&#24320;&#28304;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#23637;&#29616;&#20102;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#25915;&#20987;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#32508;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23481;&#38169;&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;FT-NCBF&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.18677</link><description>&lt;p&gt;
&#38754;&#21521;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#25915;&#20987;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23481;&#38169;&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Fault Tolerant Neural Control Barrier Functions for Robotic Systems under Sensor Faults and Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#25915;&#20987;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#32508;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23481;&#38169;&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;FT-NCBF&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#26159;&#35768;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#30830;&#20445;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;CBF&#30340;&#36873;&#25321;&#12290;&#21463;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#21551;&#21457;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20542;&#21521;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;CBF&#65292;&#20174;&#32780;&#24341;&#20986;&#20102;&#31070;&#32463;CBF&#65288;NCBF&#65289;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;NCBF&#26159;&#22312;&#33391;&#24615;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#26426;&#22120;&#20154;&#31995;&#32479;&#32463;&#21382;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#26080;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#25915;&#20987;&#19979;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#32508;&#21512;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#21644;&#32508;&#21512;&#19968;&#31867;&#26032;&#30340;CBF&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#23481;&#38169;&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;FT-NCBF&#65289;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;FT-NCBF&#30830;&#20445;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18677v1 Announce Type: cross  Abstract: Safety is a fundamental requirement of many robotic systems. Control barrier function (CBF)-based approaches have been proposed to guarantee the safety of robotic systems. However, the effectiveness of these approaches highly relies on the choice of CBFs. Inspired by the universal approximation power of neural networks, there is a growing trend toward representing CBFs using neural networks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however, are trained and deployed in benign environments, making them ineffective for scenarios where robotic systems experience sensor faults and attacks. In this paper, we study safety-critical control synthesis for robotic systems under sensor faults and attacks. Our main contribution is the development and synthesis of a new class of CBFs that we term fault tolerant neural control barrier function (FT-NCBF). We derive the necessary and sufficient conditions for FT-NCBFs to guarantee s
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#27880;&#24847;&#21147;&#24314;&#27169;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#22270;&#20687;&#22788;&#29702;&#12289;&#35270;&#39057;&#22788;&#29702;&#12289;&#35270;&#35273;&#19982;&#35821;&#35328;&#24212;&#29992;&#21644;&#35821;&#35328;&#24314;&#27169;&#31561;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18673</link><description>&lt;p&gt;
&#20154;&#31867;&#27880;&#24847;&#21147;&#24314;&#27169;&#30340;&#36235;&#21183;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Trends, Applications, and Challenges in Human Attention Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18673
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27880;&#24847;&#21147;&#24314;&#27169;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#22270;&#20687;&#22788;&#29702;&#12289;&#35270;&#39057;&#22788;&#29702;&#12289;&#35270;&#35273;&#19982;&#35821;&#35328;&#24212;&#29992;&#21644;&#35821;&#35328;&#24314;&#27169;&#31561;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#31867;&#27880;&#24847;&#21147;&#24314;&#27169;&#19981;&#20165;&#22312;&#29702;&#35299;&#35270;&#35273;&#25506;&#32034;&#32972;&#21518;&#30340;&#35748;&#30693;&#36807;&#31243;&#26041;&#38754;&#29305;&#21035;&#26377;&#29992;&#65292;&#32780;&#19988;&#22312;&#20026;&#26088;&#22312;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#38382;&#39064;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#22788;&#29702;&#12289;&#35270;&#35273;&#19982;&#35821;&#35328;&#24212;&#29992;&#20197;&#21450;&#35821;&#35328;&#24314;&#27169;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#32508;&#36848;&#26368;&#26032;&#21162;&#21147;&#23558;&#20154;&#31867;&#27880;&#24847;&#26426;&#21046;&#25972;&#21512;&#21040;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#29702;&#30001;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;&#26377;&#20851;&#27491;&#22312;&#36827;&#34892;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#35831;&#21442;&#38405;&#25105;&#20204;&#22312; https://github.com/aimagelab/awesome-human-visual-attention &#19978;&#25552;&#20379;&#30340;&#19987;&#29992;&#23384;&#20648;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18673v1 Announce Type: cross  Abstract: Human attention modelling has proven, in recent years, to be particularly useful not only for understanding the cognitive processes underlying visual exploration, but also for providing support to artificial intelligence models that aim to solve problems in various domains, including image and video processing, vision-and-language applications, and language modelling. This survey offers a reasoned overview of recent efforts to integrate human attention mechanisms into contemporary deep learning models and discusses future research directions and challenges. For a comprehensive overview on the ongoing research refer to our dedicated repository available at https://github.com/aimagelab/awesome-human-visual-attention.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18659</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28216;&#25103;&#65306;&#35843;&#30740;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Games: A Survey and Roadmap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#24182;&#20276;&#38543;&#30528;&#20844;&#20247;&#23545;&#35813;&#20027;&#39064;&#30340;&#21442;&#19982;&#12290;&#23613;&#31649;&#36215;&#21021;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;LLMs&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#21253;&#25324;&#28216;&#25103;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21450;&#20026;&#28216;&#25103;&#25552;&#20379;&#25903;&#25345;&#30340;&#21508;&#31181;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#26126;&#30830;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21487;&#20197;&#25198;&#28436;&#30340;&#19981;&#21516;&#35282;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23578;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#21644;LLMs&#22312;&#28216;&#25103;&#20013;&#26410;&#26469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;LLMs&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#20316;&#20026;LLMs&#21644;&#28216;&#25103;&#20132;&#21449;&#39046;&#22495;&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#65292;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#25104;&#20026;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#26032;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18659v1 Announce Type: cross  Abstract: Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#30340;&#32452;&#21512;&#32467;&#26500;&#26469;&#37327;&#21270;&#20154;&#31867;&#23545;&#31038;&#20132;&#21644;&#23548;&#33322;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#19968;&#33268;&#30340;&#29305;&#24449;&#21644;&#29305;&#23450;&#39046;&#22495;&#30340;&#20542;&#21521;&#65292;&#20026;&#39640;&#25928;&#24314;&#27169;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18651</link><description>&lt;p&gt;
&#37327;&#21270;&#20154;&#31867;&#23545;&#31038;&#20132;&#21644;&#23548;&#33322;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Quantifying Human Priors over Social and Navigation Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#30340;&#32452;&#21512;&#32467;&#26500;&#26469;&#37327;&#21270;&#20154;&#31867;&#23545;&#31038;&#20132;&#21644;&#23548;&#33322;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#19968;&#33268;&#30340;&#29305;&#24449;&#21644;&#29305;&#23450;&#39046;&#22495;&#30340;&#20542;&#21521;&#65292;&#20026;&#39640;&#25928;&#24314;&#27169;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30693;&#35782;&#20027;&#35201;&#26159;&#38544;&#21547;&#30340;&#21644;&#20851;&#31995;&#22411;&#30340; &#8212;&#8212; &#25105;&#20204;&#26159;&#21542;&#26377;&#20849;&#21516;&#30340;&#26379;&#21451;&#65311;&#25105;&#33021;&#20174;&#36825;&#37324;&#36208;&#21040;&#37027;&#37324;&#21527;&#65311;&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#30340;&#32452;&#21512;&#32467;&#26500;&#26469;&#37327;&#21270;&#20154;&#31867;&#23545;&#36825;&#31181;&#20851;&#31995;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30528;&#37325;&#20110;&#20004;&#20010;&#22312;&#36827;&#21270;&#26102;&#38388;&#23610;&#24230;&#19978;&#25345;&#32493;&#30456;&#20851;&#30340;&#39046;&#22495;&#65306;&#31038;&#20132;&#20114;&#21160;&#21644;&#31354;&#38388;&#23548;&#33322;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#25512;&#26029;&#24471;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#29305;&#24449;&#38750;&#24120;&#19968;&#33268;&#65292;&#20363;&#22914;&#31232;&#30095;&#24615;&#20542;&#21521;&#38543;&#30528;&#22270;&#30340;&#22823;&#23567;&#21464;&#21270;&#12290;&#20854;&#20182;&#29305;&#24449;&#26159;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#65292;&#20363;&#22914;&#31038;&#20132;&#20114;&#21160;&#20013;&#30340;&#19977;&#20803;&#38381;&#21512;&#20542;&#21521;&#12290;&#26356;&#24191;&#27867;&#22320;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38388;&#25509;&#34892;&#20026;&#23454;&#39564;&#30340;&#38750;&#32463;&#20856;&#32479;&#35745;&#20998;&#26512;&#26469;&#39640;&#25928;&#24314;&#27169;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18651v1 Announce Type: cross  Abstract: Human knowledge is largely implicit and relational -- do we have a friend in common? can I walk from here to there? In this work, we leverage the combinatorial structure of graphs to quantify human priors over such relational data. Our experiments focus on two domains that have been continuously relevant over evolutionary timescales: social interaction and spatial navigation. We find that some features of the inferred priors are remarkably consistent, such as the tendency for sparsity as a function of graph size. Other features are domain-specific, such as the propensity for triadic closure in social interactions. More broadly, our work demonstrates how nonclassical statistical analysis of indirect behavioral experiments can be used to efficiently model latent biases in the data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#20998;&#26512;&#20102;LLM&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#24341;&#20837;&#20449;&#24687;&#27969;&#23545;&#40784;&#32422;&#26463;&#20197;&#25511;&#21046;LLM&#31995;&#32479;&#30340;&#25915;&#20987;&#38754;</title><link>https://arxiv.org/abs/2402.18649</link><description>&lt;p&gt;
LLM&#23433;&#20840;&#39046;&#22495;&#30340;&#26032;&#26102;&#20195;&#65306;&#25506;&#35752;&#29616;&#23454;&#19990;&#30028;LLM&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#20998;&#26512;&#20102;LLM&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#24341;&#20837;&#20449;&#24687;&#27969;&#23545;&#40784;&#32422;&#26463;&#20197;&#25511;&#21046;LLM&#31995;&#32479;&#30340;&#25915;&#20987;&#38754;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31995;&#32479;&#22312;&#26412;&#36136;&#19978;&#26159;&#32452;&#21512;&#30340;&#65292;&#21333;&#20010;LLM&#20316;&#20026;&#26680;&#24515;&#22522;&#30784;&#65292;&#24102;&#26377;&#25554;&#20214;&#12289;&#27801;&#30418;&#31561;&#38468;&#21152;&#23618;&#23545;&#35937;&#12290;&#38500;&#20102;&#24040;&#22823;&#28508;&#21147;&#22806;&#65292;&#20154;&#20204;&#23545;&#36825;&#31181;&#27010;&#29575;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#20063;&#26085;&#30410;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;LLM&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#22312;&#20010;&#21035;LLM&#19978;&#65292;&#32780;&#27809;&#26377;&#36890;&#36807;LLM&#31995;&#32479;&#19982;&#20854;&#20182;&#23545;&#35937;&#65288;&#20363;&#22914;&#21069;&#31471;&#12289;Web&#24037;&#20855;&#12289;&#27801;&#30418;&#31561;&#65289;&#30340;&#35270;&#35282;&#26469;&#26816;&#35270;&#29983;&#24577;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#20998;&#26512;&#20102;LLM&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#20010;&#21035;LLM&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#20449;&#24687;&#27969;&#26500;&#24314;&#65292;&#24182;&#23558;LLM&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#34920;&#36848;&#20026;LLM&#20869;&#37096;&#20197;&#21450;LLM&#19982;&#20854;&#20182;&#23545;&#35937;&#20043;&#38388;&#20449;&#24687;&#27969;&#23545;&#40784;&#30340;&#32422;&#26463;&#12290;&#22522;&#20110;&#36825;&#19968;&#26500;&#24314;&#20197;&#21450;LLM&#29420;&#29305;&#30340;&#27010;&#29575;&#24615;&#36136;&#65292;LLM&#31995;&#32479;&#30340;&#25915;&#20987;&#38754;&#21487;&#20197;&#34987;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18649v1 Announce Type: cross  Abstract: Large Language Model (LLM) systems are inherently compositional, with individual LLM serving as the core foundation with additional layers of objects such as plugins, sandbox, and so on. Along with the great potential, there are also increasing concerns over the security of such probabilistic intelligent systems. However, existing studies on LLM security often focus on individual LLM, but without examining the ecosystem through the lens of LLM systems with other objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we systematically analyze the security of LLM systems, instead of focusing on the individual LLMs. To do so, we build on top of the information flow and formulate the security of LLM systems as constraints on the alignment of the information flow within LLM and between LLM and other objects. Based on this construction and the unique probabilistic nature of LLM, the attack surface of the LLM system can be deco
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20272;&#35745;&#19981;&#21516;&#31034;&#33539;&#32773;&#21046;&#20316;&#30340;&#38646;&#21644;&#21338;&#24328;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#27599;&#26465;&#36712;&#36857;&#30340;&#34987;&#21033;&#29992;&#27700;&#24179;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#31163;&#32447;&#23398;&#20064;&#20197;&#26368;&#22823;&#21270;&#25903;&#37197;&#31574;&#30053;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18617</link><description>&lt;p&gt;
ELA&#65306;&#38646;&#21644;&#21338;&#24328;&#20013;&#34701;&#20837;&#21033;&#29992;&#27700;&#24179;&#30340;&#31163;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20272;&#35745;&#19981;&#21516;&#31034;&#33539;&#32773;&#21046;&#20316;&#30340;&#38646;&#21644;&#21338;&#24328;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#27599;&#26465;&#36712;&#36857;&#30340;&#34987;&#21033;&#29992;&#27700;&#24179;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#31163;&#32447;&#23398;&#20064;&#20197;&#26368;&#22823;&#21270;&#25903;&#37197;&#31574;&#30053;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23398;&#20064;&#30001;&#20110;&#33021;&#22815;&#20174;&#19987;&#23478;&#31034;&#33539;&#32773;&#25910;&#38598;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#26377;&#25928;&#31574;&#30053;&#32780;&#19981;&#38656;&#35201;&#30452;&#25509;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#19987;&#19994;&#27700;&#24179;&#25110;&#22810;&#20010;&#31034;&#33539;&#32773;&#65289;&#26469;&#22686;&#24378;&#31163;&#32447;&#23398;&#20064;&#25928;&#29575;&#30340;&#21508;&#31181;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#21644;&#21338;&#24328;&#30340;&#32972;&#26223;&#19979;&#65292;&#38656;&#35201;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#32467;&#26524;&#26681;&#25454;&#23545;&#25163;&#30340;&#31574;&#30053;&#32780;&#26174;&#33879;&#21464;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20272;&#35745;&#30001;&#19981;&#21516;&#31034;&#33539;&#32773;&#21046;&#20316;&#30340;&#38646;&#21644;&#21338;&#24328;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#27599;&#26465;&#36712;&#36857;&#30340;&#34987;&#21033;&#29992;&#27700;&#24179;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#20272;&#35745;&#30340;&#34987;&#21033;&#29992;&#27700;&#24179;&#32467;&#21512;&#21040;&#31163;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#25903;&#37197;&#31574;&#30053;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#31034;&#33539;&#32773;&#30340;&#38646;&#21644;&#21338;&#24328;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#21487;&#35299;&#37322;&#30340;&#34987;&#21033;&#29992;&#27700;&#24179;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18617v1 Announce Type: cross  Abstract: Offline learning has become widely used due to its ability to derive effective policies from offline datasets gathered by expert demonstrators without interacting with the environment directly. Recent research has explored various ways to enhance offline learning efficiency by considering the characteristics (e.g., expertise level or multiple demonstrators) of the dataset. However, a different approach is necessary in the context of zero-sum games, where outcomes vary significantly based on the strategy of the opponent. In this study, we introduce a novel approach that uses unsupervised learning techniques to estimate the exploited level of each trajectory from the offline dataset of zero-sum games made by diverse demonstrators. Subsequently, we incorporate the estimated exploited level into the offline learning to maximize the influence of the dominant strategy. Our method enables interpretable exploited level estimation in multiple z
&lt;/p&gt;</description></item><item><title>JCLEC-MO &#26159;&#19968;&#20010;Java&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#24037;&#31243;&#38382;&#39064;&#65292;&#20026;&#24037;&#31243;&#24072;&#25552;&#20379;&#20102;&#22312;&#24456;&#23569;&#32534;&#31243;&#24037;&#20316;&#24773;&#20917;&#19979;&#24212;&#29992;&#25110;&#35843;&#25972;&#22823;&#37327;&#22810;&#30446;&#26631;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18616</link><description>&lt;p&gt;
JCLEC-MO&#65306;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#24037;&#31243;&#38382;&#39064;&#30340;Java&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
JCLEC-MO: a Java suite for solving many-objective optimization engineering problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18616
&lt;/p&gt;
&lt;p&gt;
JCLEC-MO &#26159;&#19968;&#20010;Java&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#24037;&#31243;&#38382;&#39064;&#65292;&#20026;&#24037;&#31243;&#24072;&#25552;&#20379;&#20102;&#22312;&#24456;&#23569;&#32534;&#31243;&#24037;&#20316;&#24773;&#20917;&#19979;&#24212;&#29992;&#25110;&#35843;&#25972;&#22823;&#37327;&#22810;&#30446;&#26631;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#35299;&#20915;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#30340;&#39640;&#25928;&#25216;&#26415;&#65292;&#20294;&#23545;&#20110;&#27809;&#26377;&#32534;&#31243;&#25216;&#33021;&#30340;&#39046;&#22495;&#19987;&#23478;&#26469;&#35828;&#65292;&#20174;&#22836;&#24320;&#22987;&#23454;&#29616;&#23427;&#20204;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#26694;&#26550;&#26159;&#19968;&#20010;&#23454;&#38469;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#30001;&#23450;&#21046;&#20803;&#32032;&#32452;&#25104;&#30340;&#21508;&#31181;&#31639;&#27861;&#65292;&#20197;&#21450;&#23454;&#39564;&#25903;&#25345;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#24037;&#31243;&#38382;&#39064;&#38656;&#35201;&#20248;&#21270;&#22810;&#20010;&#29978;&#33267;&#22810;&#20010;&#30446;&#26631;&#65292;&#36825;&#22686;&#21152;&#20102;&#23545;&#36866;&#24403;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#26694;&#26550;&#30340;&#20852;&#36259;&#65292;&#36825;&#20123;&#31639;&#27861;&#21644;&#26694;&#26550;&#21487;&#33021;&#25972;&#21512;&#26032;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#26368;&#21021;&#26500;&#24819;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#21407;&#21017;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;JCLEC-MO&#65292;&#19968;&#20010;Java&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#23427;&#20351;&#24037;&#31243;&#24072;&#21487;&#20197;&#22312;&#24456;&#23569;&#32534;&#31243;&#24037;&#20316;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#25110;&#35843;&#25972;&#22823;&#37327;&#22810;&#30446;&#26631;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18616v1 Announce Type: cross  Abstract: Although metaheuristics have been widely recognized as efficient techniques to solve real-world optimization problems, implementing them from scratch remains difficult for domain-specific experts without programming skills. In this scenario, metaheuristic optimization frameworks are a practical alternative as they provide a variety of algorithms composed of customized elements, as well as experimental support. Recently, many engineering problems require to optimize multiple or even many objectives, increasing the interest in appropriate metaheuristic algorithms and frameworks that might integrate new specific requirements while maintaining the generality and reusability principles they were conceived for. Based on this idea, this paper introduces JCLEC-MO, a Java framework for both multi- and many-objective optimization that enables engineers to apply, or adapt, a great number of multi-objective algorithms with little coding effort. A 
&lt;/p&gt;</description></item><item><title>ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18609</link><description>&lt;p&gt;
ICE-SEARCH: &#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH: A Language Model-Driven Feature Selection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18609
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;In-Context Evolutionary Search (ICE-SEARCH)&#26041;&#27861;&#65292;&#36825;&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;(LMs)&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;(FS)&#20219;&#21153;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;(MPA)&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;ICE-SEARCH&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20132;&#21449;&#21644;&#31361;&#21464;&#33021;&#21147;&#65292;&#22312;&#19968;&#20010;&#36827;&#21270;&#26694;&#26550;&#20869;&#26174;&#30528;&#25913;&#36827;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#27169;&#22411;&#30340;&#20840;&#38754;&#19990;&#30028;&#30693;&#35782;&#21644;&#20854;&#36866;&#24212;&#21508;&#31181;&#35282;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;MPA&#20219;&#21153;&#65306;&#20013;&#39118;&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31958;&#23615;&#30149;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;ICE-SEARCH&#22312;&#30830;&#23450;&#21307;&#23398;&#24212;&#29992;&#30340;&#20851;&#38190;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;FS&#26041;&#27861;&#12290;ICE-SEARCH&#22312;&#20013;&#39118;&#39044;&#27979;&#21644;&#31958;&#23615;&#30149;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#27700;&#24179;&#65307;&#20915;&#31574;&#38543;&#26426;&#21270;ICE-SEARCH&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#20013;&#25490;&#21517;&#20026;&#39046;&#20808;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18609v1 Announce Type: cross  Abstract: This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.18607</link><description>&lt;p&gt;
&#22312;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#20013;&#25506;&#35752;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65306;&#19968;&#31181;&#23545;&#25239;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36817;&#24180;&#26469;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#20854;&#22312;&#37319;&#26679;&#36136;&#37327;&#21644;&#20998;&#24067;&#35206;&#30422;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#32452;&#32455;&#20998;&#20139;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#21033;&#29992;&#29575;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#30452;&#25509;&#20998;&#20139;&#31169;&#20154;&#25968;&#25454;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#31181;&#26041;&#27861;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#35843;&#26597;&#12290;&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#19982;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#26041;&#65288;&#20998;&#20139;&#32773;&#65289;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#24182;&#21521;&#21478;&#19968;&#26041;&#65288;&#25509;&#25910;&#32773;&#65289;&#25552;&#20379;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#20139;&#32773;&#21487;&#20197;&#23454;&#34892;&#30340;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18607v1 Announce Type: cross  Abstract: Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined.   In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can execut
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#20845;&#31181;&#25968;&#25454;&#20998;&#24067;&#26041;&#27861;&#30740;&#31350;&#20102;&#32593;&#32476;&#32467;&#26500;&#19982;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18606</link><description>&lt;p&gt;
&#32593;&#32476;&#25299;&#25169;&#23545;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of network topology on the performance of Decentralized Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18606
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#20845;&#31181;&#25968;&#25454;&#20998;&#24067;&#26041;&#27861;&#30740;&#31350;&#20102;&#32593;&#32476;&#32467;&#26500;&#19982;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#27491;&#22312;&#34028;&#21187;&#21457;&#23637;&#65292;&#29992;&#20110;&#22312;&#20114;&#32852;&#32593;&#36793;&#32536;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#35299;&#20915;&#22522;&#30784;&#35774;&#26045;&#25361;&#25112;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#21435;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#33410;&#28857;&#19978;&#65292;&#27599;&#20010;&#33410;&#28857;&#26681;&#25454;&#20854;&#21508;&#33258;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#12290;&#28982;&#21518;&#36825;&#20123;&#26412;&#22320;&#27169;&#22411;&#34987;&#20849;&#20139;&#21644;&#21512;&#24182;&#65292;&#24418;&#25104;&#33021;&#22815;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#30693;&#35782;&#20256;&#25773;&#65292;&#21363;&#33410;&#28857;&#22914;&#20309;&#21560;&#25910;&#26469;&#33258;&#32593;&#32476;&#19978;&#20854;&#20182;&#33410;&#28857;&#21487;&#29992;&#25968;&#25454;&#23398;&#20064;&#27169;&#24335;&#30340;&#27934;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19977;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#20845;&#31181;&#25968;&#25454;&#20998;&#24067;&#26041;&#27861;&#25506;&#35752;&#32593;&#32476;&#32467;&#26500;&#19982;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#26041;&#27861;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#39030;&#28857;&#23646;&#24615;&#65292;&#21253;&#25324;&#24230;&#20013;&#24515;&#24615;&#65292;&#20171;&#25968;&#20013;&#24515;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18606v1 Announce Type: cross  Abstract: Fully decentralized learning is gaining momentum for training AI models at the Internet's edge, addressing infrastructure challenges and privacy concerns. In a decentralized machine learning system, data is distributed across multiple nodes, with each node training a local model based on its respective dataset. The local models are then shared and combined to form a global model capable of making accurate predictions on new data. Our exploration focuses on how different types of network structures influence the spreading of knowledge - the process by which nodes incorporate insights gained from learning patterns in data available on other nodes across the network. Specifically, this study investigates the intricate interplay between network structure and learning performance using three network topologies and six data distribution methods. These methods consider different vertex properties, including degree centrality, betweenness cent
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35270;&#32593;&#33180;&#22270;&#20687;&#19982;&#34880;&#31649;&#29366;&#20917;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29992;&#20110;&#39640;&#36890;&#37327;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#26816;&#27979;&#65292;&#24182;&#20855;&#26377;&#28508;&#21147;&#35299;&#20915;&#31958;&#23615;&#30149;&#24739;&#32773;&#25972;&#20307;&#25252;&#29702;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.18600</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#31958;&#23615;&#30149;&#65306;&#36890;&#36807;&#35270;&#32593;&#33180;&#28145;&#20837;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18600
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35270;&#32593;&#33180;&#22270;&#20687;&#19982;&#34880;&#31649;&#29366;&#20917;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29992;&#20110;&#39640;&#36890;&#37327;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#26816;&#27979;&#65292;&#24182;&#20855;&#26377;&#28508;&#21147;&#35299;&#20915;&#31958;&#23615;&#30149;&#24739;&#32773;&#25972;&#20307;&#25252;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#65288;DM&#65289;&#20351;&#24739;&#32773;&#26131;&#24739;&#34880;&#31649;&#24182;&#21457;&#30151;&#12290;&#35270;&#32593;&#33180;&#22270;&#20687;&#21644;&#34880;&#31649;&#21453;&#26144;&#20102;&#36523;&#20307;&#30340;&#24494;&#34880;&#31649;&#21644;&#22823;&#34880;&#31649;&#20581;&#24247;&#29366;&#20917;&#12290;&#23427;&#20204;&#21487;&#29992;&#20110;&#35786;&#26029;DM&#24182;&#21457;&#30151;&#65292;&#21253;&#25324;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#12289;&#31070;&#32463;&#30149;&#21464;&#12289;&#32958;&#30149;&#20197;&#21450;&#21160;&#33033;&#31909;&#26679;&#30828;&#21270;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#36824;&#21487;&#39044;&#27979;&#24515;&#34880;&#31649;&#20107;&#20214;&#30340;&#39118;&#38505;&#12290;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#24320;&#21457;&#30340;&#39640;&#36890;&#37327;DR&#26816;&#27979;&#31995;&#32479;&#24050;&#34987;&#20020;&#24202;&#37319;&#29992;&#12290;&#38500;DR&#31579;&#26597;&#22806;&#65292;AI&#25972;&#21512;&#36824;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#24212;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#25972;&#20307;&#25252;&#29702;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#23457;&#26597;&#19982;DM&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#31649;&#29702;&#30456;&#20851;&#30340;&#22522;&#20110;&#35270;&#32593;&#33180;&#22270;&#20687;&#30340;AI&#24212;&#29992;&#30740;&#31350;&#25991;&#29486;&#12290;&#25105;&#20204;&#23558;&#25551;&#36848;&#32508;&#21512;AI&#36741;&#21161;&#31958;&#23615;&#30149;&#25252;&#29702;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;DR&#31579;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18600v1 Announce Type: cross  Abstract: Diabetes mellitus (DM) predisposes patients to vascular complications. Retinal images and vasculature reflect the body's micro- and macrovascular health. They can be used to diagnose DM complications, including diabetic retinopathy (DR), neuropathy, nephropathy, and atherosclerotic cardiovascular disease, as well as forecast the risk of cardiovascular events. Artificial intelligence (AI)-enabled systems developed for high-throughput detection of DR using digitized retinal images have become clinically adopted. Beyond DR screening, AI integration also holds immense potential to address challenges associated with the holistic care of the patient with DM. In this work, we aim to comprehensively review the literature for studies on AI applications based on retinal images related to DM diagnosis, prognostication, and management. We will describe the findings of holistic AI-assisted diabetes care, including but not limited to DR screening, a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;meta-tasks&#20316;&#20026;&#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#35270;&#35282;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#21644;&#26032;&#39062;&#20219;&#21153;&#30340;&#27867;&#21270;&#65292;&#36991;&#20813;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#25200;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#36739;&#20110;&#21407;&#22411;&#32593;&#32476;&#25552;&#39640;&#20102;3.9%&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18599</link><description>&lt;p&gt;
Meta-Tasks: &#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Meta-Tasks: An alternative view on Meta-Learning Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18599
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;meta-tasks&#20316;&#20026;&#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#35270;&#35282;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#21644;&#26032;&#39062;&#20219;&#21153;&#30340;&#27867;&#21270;&#65292;&#36991;&#20813;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#25200;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#36739;&#20110;&#21407;&#22411;&#32593;&#32476;&#25552;&#39640;&#20102;3.9%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Few-shot learning (FSL)&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#22240;&#20026;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#27867;&#21270;&#21040;&#35757;&#32451;&#21644;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#21033;&#29992;&#26410;&#26631;&#35760;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#22806;&#23618;&#24490;&#29615;&#20043;&#21069;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#25216;&#26415;&#23545;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#65292;&#23558;&#20854;&#20316;&#20026;&#8220;&#20803;&#20219;&#21153;&#8221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26032;&#39062;&#21644;&#35757;&#32451;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#25910;&#25947;&#26356;&#24555;&#12289;&#26356;&#22909;&#65292;&#27867;&#21270;&#35823;&#24046;&#21644;&#26631;&#20934;&#24046;&#26356;&#20302;&#65292;&#34920;&#26126;&#20854;&#22312;FSL&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#27604;&#21407;&#22411;&#32593;&#32476;&#39640;&#20986;3.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18599v1 Announce Type: cross  Abstract: Few-shot learning (FSL) is a challenging machine learning problem due to a scarcity of labeled data. The ability to generalize effectively on both novel and training tasks is a significant barrier to FSL. This paper proposes a novel solution that can generalize to both training and novel tasks while also utilizing unlabeled samples. The method refines the embedding model before updating the outer loop using unsupervised techniques as ``meta-tasks''. The experimental results show that our proposed method performs well on novel and training tasks, with faster and better convergence, lower generalization, and standard deviation error, indicating its potential for practical applications in FSL. The experimental results show that the proposed method outperforms prototypical networks by 3.9%.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#38750;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#21644;&#22810;&#29436;&#20154;&#36827;&#21270;&#21338;&#24328;&#26694;&#26550;&#20013;&#30340;&#40481;&#23614;&#37202;&#27966;&#23545;&#25928;&#24212;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20551;&#26032;&#38395;&#20256;&#25773;&#39118;&#38505;&#23545;&#31574;&#30053;&#28436;&#21270;&#21644;&#28436;&#21270;&#31283;&#23450;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.18598</link><description>&lt;p&gt;
&#27880;&#24847;&#65306;&#36827;&#21270;&#21338;&#24328;&#35770;&#32858;&#28966;&#20449;&#24687;&#20581;&#24247;&#65306;&#36890;&#36807;&#19981;&#23436;&#20840;&#20449;&#24687;&#19979;&#30340;&#29436;&#20154;&#28216;&#25103;&#30340;&#40481;&#23614;&#37202;&#27966;&#23545;&#25928;&#24212;&#21644;&#21033;&#29992;&#37325;&#22797;&#22256;&#22659;&#20013;&#26399;&#26395;&#25910;&#30410;&#30340;ESS&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Note: Evolutionary Game Theory Focus Informational Health: The Cocktail Party Effect Through Werewolfgame under Incomplete Information and ESS Search Method Using Expected Gains of Repeated Dilemmas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18598
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#38750;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#21644;&#22810;&#29436;&#20154;&#36827;&#21270;&#21338;&#24328;&#26694;&#26550;&#20013;&#30340;&#40481;&#23614;&#37202;&#27966;&#23545;&#25928;&#24212;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20551;&#26032;&#38395;&#20256;&#25773;&#39118;&#38505;&#23545;&#31574;&#30053;&#28436;&#21270;&#21644;&#28436;&#21270;&#31283;&#23450;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#40481;&#23614;&#37202;&#27966;&#23545;&#25928;&#24212;&#22312;&#38750;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#21644;&#20855;&#26377;&#22810;&#20010;&#29436;&#20154;&#30340;&#36827;&#21270;&#21338;&#24328;&#26694;&#26550;&#20013;&#24341;&#36215;&#30340;&#20449;&#24687;&#24178;&#25200;&#29366;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25968;&#23398;&#24314;&#27169;&#21644;&#20998;&#26512;&#20102;&#27599;&#31181;&#31574;&#30053;&#36873;&#25321;&#30340;&#25910;&#30410;&#20197;&#21450;&#28436;&#21270;&#31283;&#23450;&#31574;&#30053;&#65288;ESS&#65289;&#30340;&#24418;&#25104;&#36807;&#31243;&#65292;&#20551;&#35774;&#22312;&#37325;&#22797;&#22256;&#22659;&#30340;&#24773;&#22659;&#20013;&#20551;&#26032;&#38395;&#30340;&#27745;&#26579;&#39118;&#38505;&#26159;&#38543;&#26426;&#20998;&#37197;&#30340;&#12290;&#25105;&#20204;&#23558;&#35814;&#32454;&#24320;&#21457;&#35745;&#31639;&#36807;&#31243;&#65292;&#20174;&#26500;&#24314;&#25910;&#30410;&#30697;&#38453;&#24320;&#22987;&#65292;&#20351;&#29992;&#22797;&#21046;&#26041;&#31243;&#23545;&#28436;&#21270;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#30830;&#23450;ESS&#12290;&#27492;&#22806;&#65292;&#23558;&#36827;&#34892;&#25968;&#20540;&#27169;&#25311;&#20197;&#35266;&#23519;&#31995;&#32479;&#22312;&#19981;&#21516;&#21021;&#22987;&#26465;&#20214;&#21644;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#34892;&#20026;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20551;&#26032;&#38395;&#20256;&#25773;&#23545;&#31574;&#30053;&#28436;&#21270;&#30340;&#24433;&#21709;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#20026;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#29702;&#35770;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18598v1 Announce Type: cross  Abstract: We explore the state of information disruption caused by the cocktail party effect within the framework of non-perfect information games and evolutive games with multiple werewolves. In particular, we mathematically model and analyze the effects on the gain of each strategy choice and the formation process of evolutionary stable strategies (ESS) under the assumption that the pollution risk of fake news is randomly assigned in the context of repeated dilemmas. We will develop the computational process in detail, starting with the construction of the gain matrix, modeling the evolutionary dynamics using the replicator equation, and identifying the ESS. In addition, numerical simulations will be performed to observe system behavior under different initial conditions and parameter settings to better understand the impact of the spread of fake news on strategy evolution. This research will provide theoretical insights into the complex issue
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#22312;&#36229;&#32423;&#35745;&#31639;&#20013;&#24515;&#23545;GPU&#36827;&#34892;&#21151;&#29575;&#38480;&#21046;&#23545;&#28201;&#24230;&#21644;&#21151;&#32791;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21151;&#29575;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#33021;&#32791;&#12289;&#25913;&#21892;&#30828;&#20214;&#23551;&#21629;&#30340;&#30446;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.18593</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#30340;AI&#36229;&#32423;&#35745;&#31639;&#65306;HPC&#35268;&#27169;&#19979;&#30340;GPU&#21151;&#29575;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#22312;&#36229;&#32423;&#35745;&#31639;&#20013;&#24515;&#23545;GPU&#36827;&#34892;&#21151;&#29575;&#38480;&#21046;&#23545;&#28201;&#24230;&#21644;&#21151;&#32791;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21151;&#29575;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#33021;&#32791;&#12289;&#25913;&#21892;&#30828;&#20214;&#23551;&#21629;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#21644;&#37096;&#32626;&#19981;&#26029;&#22686;&#38271;&#65292;&#25903;&#25345;&#21644;&#32500;&#25345;&#20854;&#36827;&#23637;&#30340;&#35745;&#31639;&#36127;&#25285;&#20063;&#24517;&#28982;&#22686;&#21152;&#12290;&#20026;&#20102;&#35757;&#32451;&#25110;&#24494;&#35843;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#26576;&#31181;&#24418;&#24335;&#30340;AI&#30828;&#20214;&#21152;&#36895;&#20960;&#20046;&#26159;&#24517;&#38656;&#30340;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#23548;&#33268;&#33021;&#28304;&#28040;&#32791;&#24040;&#22823;&#65292;&#28508;&#22312;&#30899;&#25490;&#25918;&#22686;&#21152;&#65292;&#24182;&#23545;GPU&#21644;&#20854;&#20182;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#38656;&#27714;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#23545;HPC/&#25968;&#25454;&#20013;&#24515;&#32423;&#21035;&#30340;&#33021;&#28304;&#21487;&#25345;&#32493;&#24615;&#24102;&#26469;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30740;&#31350;&#36229;&#32423;&#35745;&#31639;&#20013;&#24515;&#23545;GPU&#36827;&#34892;&#21151;&#29575;&#38480;&#21046;&#30340;&#24635;&#20307;&#25928;&#26524;&#23545;GPU&#28201;&#24230;&#21644;&#21151;&#29575;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#21151;&#29575;&#38480;&#21046;&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#28201;&#24230;&#21644;&#21151;&#29575;&#28040;&#32791;&#26174;&#33879;&#38477;&#20302;&#65292;&#20943;&#23569;&#21151;&#32791;&#65292;&#24182;&#21487;&#33021;&#22312;&#26368;&#23567;&#24433;&#21709;&#20316;&#19994;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#30828;&#20214;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18593v1 Announce Type: cross  Abstract: As research and deployment of AI grows, the computational burden to support and sustain its progress inevitably does too. To train or fine-tune state-of-the-art models in NLP, computer vision, etc., some form of AI hardware acceleration is virtually a requirement. Recent large language models require considerable resources to train and deploy, resulting in significant energy usage, potential carbon emissions, and massive demand for GPUs and other hardware accelerators. However, this surge carries large implications for energy sustainability at the HPC/datacenter level. In this paper, we study the aggregate effect of power-capping GPUs on GPU temperature and power draw at a research supercomputing center. With the right amount of power-capping, we show significant decreases in both temperature and power draw, reducing power consumption and potentially improving hardware life-span with minimal impact on job performance. While power-cappi
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.18590</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#65306;&#19968;&#39033;&#24191;&#27867;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18590
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37325;&#22609;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#24402;&#22240;&#20110;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25152;&#32570;&#20047;&#30340;&#29420;&#29305;&#25512;&#29702;&#33021;&#21147;&#12290;&#19981;&#21516;&#20110;&#32570;&#20047;&#30452;&#25509;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#30340;&#20256;&#32479;&#31995;&#32479;&#65292;LLMs&#22312;&#25512;&#33616;&#29289;&#21697;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#35821;&#35328;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#36825;&#26631;&#24535;&#30528;&#25512;&#33616;&#39046;&#22495;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#33539;&#24335;&#36716;&#21464;&#12290;&#22312;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#31215;&#26497;&#21033;&#29992;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#37325;&#26032;&#23450;&#20041;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#35813;&#30740;&#31350;&#24443;&#24213;&#25506;&#35752;&#20102;LLMs&#22312;&#25512;&#33616;&#26694;&#26550;&#20869;&#22266;&#26377;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#32454;&#33268;&#30340;&#35821;&#22659;&#29702;&#35299;&#65292;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#24179;&#31283;&#36807;&#28193;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20849;&#20139;&#25968;&#25454;&#27744;&#30340;&#20840;&#38754;&#23398;&#20064;&#31574;&#30053;&#65292;&#36879;&#26126;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18590v1 Announce Type: cross  Abstract: The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems, attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data, LLMs exhibit exceptional proficiency in recommending items, showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape, researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks, encompassing nuanced contextual comprehension, seamless transitions across diverse domains, adoption of unified approaches, holistic learning strategies leveraging shared data reservoirs, transparent
&lt;/p&gt;</description></item><item><title>Verif.ai&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#39564;&#35777;&#24341;&#25806;&#30340;&#32467;&#21512;&#23454;&#29616;&#23545;&#20027;&#24352;&#30340;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.18589</link><description>&lt;p&gt;
Verif.ai: &#19968;&#31181;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Verif.ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18589
&lt;/p&gt;
&lt;p&gt;
Verif.ai&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#39564;&#35777;&#24341;&#25806;&#30340;&#32467;&#21512;&#23454;&#29616;&#23545;&#20027;&#24352;&#30340;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39033;&#30446;Verif.ai&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#65288;1&#65289;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#32467;&#21512;&#35821;&#20041;&#21644;&#35789;&#27719;&#25628;&#32034;&#25216;&#26415;&#23545;&#31185;&#23398;&#35770;&#25991;&#65288;PubMed&#65289;&#36827;&#34892;&#26816;&#32034;&#65292;&#65288;2&#65289;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;Mistral 7B&#65289;&#65292;&#33719;&#21462;&#21069;&#20960;&#20010;&#31572;&#26696;&#24182;&#29983;&#25104;&#38468;&#26377;&#20174;&#20013;&#24471;&#20986;&#20027;&#24352;&#30340;&#35770;&#25991;&#24341;&#29992;&#30340;&#31572;&#26696;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19968;&#20010;&#39564;&#35777;&#24341;&#25806;&#65292;&#29992;&#20110;&#20132;&#21449;&#26816;&#26597;&#29983;&#25104;&#30340;&#20027;&#24352;&#21644;&#20174;&#20013;&#24471;&#20986;&#20027;&#24352;&#30340;&#25688;&#35201;&#25110;&#35770;&#25991;&#65292;&#39564;&#35777;&#29983;&#25104;&#20027;&#24352;&#26102;&#26159;&#21542;&#23384;&#22312;&#20219;&#20309;&#38169;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19978;&#19979;&#25991;&#20013;&#30340;&#25688;&#35201;&#21152;&#24378;&#20102;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#27492;&#22806;&#65292;&#19968;&#20010;&#29420;&#31435;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#38598;&#27491;&#22312;&#39564;&#35777;&#31572;&#26696;&#24182;&#26816;&#26597;&#26159;&#21542;&#23384;&#22312;&#38169;&#35273;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#20449;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#31185;&#23398;&#23478;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18589v1 Announce Type: cross  Abstract: In this paper, we present the current progress of the project Verif.ai, an open-source scientific generative question-answering system with referenced and verified answers. The components of the system are (1) an information retrieval system combining semantic and lexical search techniques over scientific papers (PubMed), (2) a fine-tuned generative model (Mistral 7B) taking top answers and generating answers with references to the papers from which the claim was derived, and (3) a verification engine that cross-checks the generated claim and the abstract or paper from which the claim was derived, verifying whether there may have been any hallucinations in generating the claim. We are reinforcing the generative model by providing the abstract in context, but in addition, an independent set of methods and models are verifying the answer and checking for hallucinations. Therefore, we believe that by using our method, we can make scientis
&lt;/p&gt;</description></item><item><title>GenAI&#22312;&#26080;&#32447;&#39046;&#22495;&#26159;&#20851;&#38190;&#36164;&#20135;&#65292;&#33021;&#22815;&#22788;&#29702;&#31232;&#32570;&#12289;&#19981;&#23436;&#25972;&#12289;&#38590;&#20197;&#33719;&#21462;&#21644;&#29702;&#35299;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#21487;&#20197;&#21462;&#20195;&#25110;&#34917;&#20805;&#21028;&#21035;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#26412;&#25991;&#27719;&#24635;&#20102;6G&#21644;&#26080;&#32447;&#26234;&#33021;&#39046;&#22495;&#30340;&#26032;&#21069;&#27839;&#12290;</title><link>https://arxiv.org/abs/2402.18587</link><description>&lt;p&gt;
&#22788;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20043;&#21021;&#65306;&#20851;&#20110;6G&#26080;&#32447;&#26234;&#33021;&#26032;&#39046;&#22495;&#30340;&#25945;&#31243;&#21644;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
At the Dawn of Generative AI Era: A Tutorial-cum-Survey on New Frontiers in 6G Wireless Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18587
&lt;/p&gt;
&lt;p&gt;
GenAI&#22312;&#26080;&#32447;&#39046;&#22495;&#26159;&#20851;&#38190;&#36164;&#20135;&#65292;&#33021;&#22815;&#22788;&#29702;&#31232;&#32570;&#12289;&#19981;&#23436;&#25972;&#12289;&#38590;&#20197;&#33719;&#21462;&#21644;&#29702;&#35299;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#21487;&#20197;&#21462;&#20195;&#25110;&#34917;&#20805;&#21028;&#21035;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#26412;&#25991;&#27719;&#24635;&#20102;6G&#21644;&#26080;&#32447;&#26234;&#33021;&#39046;&#22495;&#30340;&#26032;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#32447;&#30740;&#31350;&#20005;&#37325;&#20381;&#36182;&#20110;&#38656;&#35201;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21028;&#21035;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;DAI&#65289;&#12290;&#19982;DAI&#19981;&#21516;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#28041;&#21450;&#33021;&#22815;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#28508;&#22312;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#24335;&#21644;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;GMs&#65289;&#12290;&#36825;&#20351;&#24471;GenAI&#22312;&#26080;&#32447;&#39046;&#22495;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#36164;&#20135;&#65292;&#20854;&#20013;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36890;&#24120;&#31232;&#32570;&#12289;&#19981;&#23436;&#25972;&#12289;&#33719;&#21462;&#25104;&#26412;&#39640;&#65292;&#38590;&#20197;&#24314;&#27169;&#25110;&#29702;&#35299;&#12290;&#26377;&#20102;&#36825;&#20123;&#21560;&#24341;&#20154;&#30340;&#29305;&#24449;&#65292;GenAI&#21487;&#20197;&#21462;&#20195;&#25110;&#34917;&#20805;DAI&#26041;&#27861;&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#36825;&#31687;&#32467;&#21512;&#20102;&#25945;&#31243;&#21644;&#35843;&#30740;&#30340;&#35770;&#25991;&#20174;6G&#21644;&#26080;&#32447;&#26234;&#33021;&#30340;&#22522;&#30784;&#24320;&#22987;&#65292;&#36890;&#36807;&#27010;&#36848;&#20505;&#36873;6G&#24212;&#29992;&#21644;&#26381;&#21153;&#12289;&#25552;&#20986;&#29616;&#20195;DAI&#27169;&#22411;&#30340;&#20998;&#31867;&#27861;&#12289;&#20030;&#20363;&#35828;&#26126;&#33879;&#21517;&#30340;DAI&#29992;&#20363;&#65292;&#24182;&#38416;&#26126;GenAI&#22914;&#20309;&#22686;&#24378;DAI&#30340;&#22810;&#26041;&#38754;&#26041;&#24335;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36890;&#36807;&#37325;&#28857;&#20171;&#32461;&#24320;&#21019;&#24615;&#30340;GMs&#26469;&#21576;&#29616;&#19968;&#20010;GMs&#30340;&#25945;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18587v1 Announce Type: cross  Abstract: The majority of data-driven wireless research leans heavily on discriminative AI (DAI) that requires vast real-world datasets. Unlike the DAI, Generative AI (GenAI) pertains to generative models (GMs) capable of discerning the underlying data distribution, patterns, and features of the input data. This makes GenAI a crucial asset in wireless domain wherein real-world data is often scarce, incomplete, costly to acquire, and hard to model or comprehend. With these appealing attributes, GenAI can replace or supplement DAI methods in various capacities. Accordingly, this combined tutorial-survey paper commences with preliminaries of 6G and wireless intelligence by outlining candidate 6G applications and services, presenting a taxonomy of state-of-the-art DAI models, exemplifying prominent DAI use cases, and elucidating the multifaceted ways through which GenAI enhances DAI. Subsequently, we present a tutorial on GMs by spotlighting seminal
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;GPT-4&#21161;&#25163;API&#31616;&#21270;&#31995;&#32479;&#25991;&#29486;&#35780;&#23457;&#36873;&#25321;&#38454;&#27573;&#30340;&#25928;&#29575;&#65292;&#21152;&#36895;&#25991;&#29486;&#35780;&#23457;&#36807;&#31243;&#65292;&#24182;&#23545;&#31649;&#29702;&#21644;&#32463;&#27982;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.18582</link><description>&lt;p&gt;
&#21033;&#29992;AI-Enabled GPT-4&#21161;&#25163;API&#31616;&#21270;&#31995;&#32479;&#25991;&#29486;&#35780;&#23457;&#65288;SLRs&#65289;&#30340;&#36873;&#25321;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
Streamlining the Selection Phase of Systematic Literature Reviews (SLRs) Using AI-Enabled GPT-4 Assistant API
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;GPT-4&#21161;&#25163;API&#31616;&#21270;&#31995;&#32479;&#25991;&#29486;&#35780;&#23457;&#36873;&#25321;&#38454;&#27573;&#30340;&#25928;&#29575;&#65292;&#21152;&#36895;&#25991;&#29486;&#35780;&#23457;&#36807;&#31243;&#65292;&#24182;&#23545;&#31649;&#29702;&#21644;&#32463;&#27982;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#25991;&#29486;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#20351;&#24471;&#36319;&#36394;&#26368;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#25104;&#20026;&#19968;&#39033;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#65292;&#19987;&#38376;&#37197;&#32622;&#20197;&#31616;&#21270;&#31995;&#32479;&#25991;&#29486;&#35780;&#23457;&#65288;SLRs&#65289;&#30340;&#25991;&#31456;&#36873;&#25321;&#38454;&#27573;&#30340;&#25928;&#29575;&#12290;&#21033;&#29992;OpenAI&#30340;GPT-4&#21161;&#25163;API&#24378;&#22823;&#30340;&#21151;&#33021;&#65292;&#35813;&#24037;&#20855;&#25104;&#21151;&#22320;&#22312;&#24191;&#27867;&#30340;&#23398;&#26415;&#39046;&#22495;&#20013;&#20351;&#25991;&#31456;&#36873;&#25321;&#36807;&#31243;&#21464;&#24471;&#19968;&#33268;&#12290;&#36890;&#36807;&#25968;&#25454;&#20934;&#22791;&#12289;AI&#36741;&#21161;&#25991;&#31456;&#35780;&#20272;&#21644;&#32467;&#26500;&#21270;&#32467;&#26524;&#21576;&#29616;&#30340;&#19977;&#26041;&#38754;&#26041;&#27861;&#65292;&#35813;&#24037;&#20855;&#26174;&#33879;&#21152;&#24555;&#20102;&#25991;&#29486;&#35780;&#23457;&#36825;&#20010;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#24037;&#20855;&#22312;&#31649;&#29702;&#21644;&#32463;&#27982;&#31561;&#39046;&#22495;&#20855;&#26377;&#24456;&#39640;&#30340;&#30410;&#22788;&#65292;&#36825;&#20123;&#39046;&#22495;&#30340;SLR&#36807;&#31243;&#28041;&#21450;&#22823;&#37327;&#20154;&#31867;&#21028;&#26029;&#12290;&#37319;&#29992;&#26631;&#20934;GPT&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#28508;&#22312;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18582v1 Announce Type: cross  Abstract: The escalating volume of academic literature presents a formidable challenge in staying updated with the newest research developments. Addressing this, this study introduces a pioneering AI-based tool, configured specifically to streamline the efficiency of the article selection phase in Systematic Literature Reviews (SLRs). Utilizing the robust capabilities of OpenAI's GPT-4 Assistant API, the tool successfully homogenizes the article selection process across a broad array of academic disciplines. Implemented through a tripartite approach consisting of data preparation, AI-mediated article assessment, and structured result presentation, this tool significantly accelerates the time-consuming task of literature reviews. Importantly, this tool could be highly beneficial in fields such as management and economics, where the SLR process involves substantial human judgment. The adoption of a standard GPT model can substantially reduce poten
&lt;/p&gt;</description></item><item><title>&#22478;&#24066;&#36710;&#36742;&#32593;&#32476;&#20013;&#22810;&#30446;&#26631;&#26368;&#20248;&#36793;&#36335;&#21333;&#20803;&#37096;&#32626;&#28041;&#21450;&#20811;&#26381;&#22810;&#20010;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#20197;&#21450;&#35299;&#20915;&#22478;&#24066;&#29615;&#22659;&#20013;&#21508;&#31181;&#38556;&#30861;&#24102;&#26469;&#30340;&#37096;&#32626;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.18581</link><description>&lt;p&gt;
&#22478;&#24066;&#36710;&#36742;&#32593;&#32476;&#20013;&#22810;&#30446;&#26631;&#26368;&#20248;&#36793;&#36335;&#21333;&#20803;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Optimal Roadside Units Deployment in Urban Vehicular Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18581
&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#36710;&#36742;&#32593;&#32476;&#20013;&#22810;&#30446;&#26631;&#26368;&#20248;&#36793;&#36335;&#21333;&#20803;&#37096;&#32626;&#28041;&#21450;&#20811;&#26381;&#22810;&#20010;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#20197;&#21450;&#35299;&#20915;&#22478;&#24066;&#29615;&#22659;&#20013;&#21508;&#31181;&#38556;&#30861;&#24102;&#26469;&#30340;&#37096;&#32626;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#36755;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#30456;&#20851;&#26381;&#21153;&#30340;&#37325;&#35201;&#24615;&#22312;&#22478;&#24066;&#36710;&#36742;&#32593;&#32476;&#20013;&#26085;&#30410;&#22686;&#21152;&#12290;&#22312;&#36825;&#26679;&#30340;&#32593;&#32476;&#20013;&#65292;&#36793;&#36335;&#21333;&#20803;&#65288;RSUs&#65289;&#20316;&#20026;&#20419;&#36827;&#36890;&#20449;&#30340;&#20013;&#38388;&#32773;&#12290;&#22240;&#27492;&#65292;RSUs&#30340;&#37096;&#32626;&#23545;&#20110;&#30830;&#20445;&#36890;&#20449;&#26381;&#21153;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#30446;&#26631;&#65292;&#22914;&#26102;&#38388;&#24310;&#36831;&#21644;&#37096;&#32626;&#25104;&#26412;&#65292;&#36890;&#24120;&#26159;&#20174;&#19981;&#21516;&#30340;&#35270;&#35282;&#21457;&#23637;&#32780;&#26469;&#12290;&#22240;&#27492;&#65292;&#21487;&#33021;&#20986;&#29616;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#27492;&#22806;&#65292;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#24314;&#31569;&#29289;&#12289;&#20844;&#22253;&#12289;&#28246;&#27850;&#21644;&#20854;&#20182;&#22522;&#30784;&#35774;&#26045;&#31561;&#21508;&#31181;&#38556;&#30861;&#30340;&#23384;&#22312;&#32473;RSUs&#30340;&#37096;&#32626;&#24102;&#26469;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#37096;&#32626;&#30001;&#20110;&#23384;&#22312;&#22810;&#20010;&#30446;&#26631;&#12289;&#38556;&#30861;&#29289;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#20197;&#21450;&#38656;&#35201;&#25506;&#32034;&#22823;&#35268;&#27169;&#20248;&#21270;&#31354;&#38388;&#32780;&#36935;&#21040;&#37325;&#22823;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#22810;&#30446;&#26631;&#26368;&#20248;&#36793;&#36335;&#21333;&#20803;&#37096;&#32626;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18581v1 Announce Type: cross  Abstract: The significance of transportation efficiency, safety, and related services is increasing in urban vehicular networks. Within such networks, roadside units (RSUs) serve as intermediates in facilitating communication. Therefore, the deployment of RSUs is of utmost importance in ensuring the quality of communication services. However, the optimization objectives, such as time delay and deployment cost, are commonly developed from diverse perspectives. As a result, it is possible that conflicts may arise among the objectives. Furthermore, in urban environments, the presence of various obstacles, such as buildings, gardens, lakes, and other infrastructure, poses challenges for the deployment of RSUs. Hence, the deployment encounters significant difficulties due to the existence of multiple objectives, constraints imposed by obstacles, and the necessity to explore a large-scale optimization space. To address this issue, two versions of mult
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;SAR&#22270;&#20687;&#20013;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;CFAR&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#24050;&#30693;&#26434;&#27874;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#30446;&#26631;&#26816;&#27979;&#30340;&#24658;&#23450;&#34394;&#35686;&#29575;</title><link>https://arxiv.org/abs/2402.18579</link><description>&lt;p&gt;
SAR&#22270;&#20687;&#20013;&#29992;&#20110;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;&#21270;CFAR&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Wilcoxon Nonparametric CFAR Scheme for Ship Detection in SAR Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;SAR&#22270;&#20687;&#20013;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;CFAR&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#24050;&#30693;&#26434;&#27874;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#30446;&#26631;&#26816;&#27979;&#30340;&#24658;&#23450;&#34394;&#35686;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#25968;&#34394;&#35686;&#29575;&#65288;CFAR&#65289;&#26816;&#27979;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#30446;&#21069;SAR&#22270;&#20687;&#20013;&#26816;&#27979;&#33337;&#21482;&#30446;&#26631;&#65292;&#36825;&#20123;&#31639;&#27861;&#22522;&#20110;&#21508;&#31181;&#32479;&#35745;&#20998;&#24067;&#65292;&#22914;&#39640;&#26031;&#20998;&#24067;&#12289;Gamma&#20998;&#24067;&#12289;Weibull&#20998;&#24067;&#12289;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#12289;G0&#20998;&#24067;&#12289;alpha&#31283;&#23450;&#20998;&#24067;&#31561;&#12290;&#28982;&#32780;&#65292;SAR&#22270;&#20687;&#20013;&#30340;&#26434;&#25955;&#32972;&#26223;&#22797;&#26434;&#22810;&#21464;&#12290;&#24403;&#23454;&#38469;&#26434;&#25955;&#32972;&#26223;&#20559;&#31163;&#20551;&#23450;&#30340;&#32479;&#35745;&#20998;&#24067;&#26102;&#65292;&#21442;&#25968;&#21270;CFAR&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#23558;&#19979;&#38477;&#12290;&#38500;&#20102;&#21442;&#25968;&#21270;CFAR&#26041;&#26696;&#65292;&#36824;&#26377;&#21478;&#19968;&#31867;&#38750;&#21442;&#25968;&#21270;CFAR&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#24050;&#30693;&#26434;&#27874;&#20998;&#24067;&#30340;&#20551;&#35774;&#24773;&#20917;&#19979;&#20445;&#25345;&#30446;&#26631;&#26816;&#27979;&#30340;&#24658;&#23450;&#34394;&#35686;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;SAR&#22270;&#20687;&#20013;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;&#21270;CFAR&#26041;&#26696;&#65292;&#24182;&#25512;&#23548;&#20102;Wilcoxon&#38750;&#21442;&#25968;&#26816;&#27979;&#22120;&#30340;&#34394;&#35686;&#29575;&#30340;&#23553;&#38381;&#24418;&#24335;&#20197;&#30830;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18579v1 Announce Type: cross  Abstract: The parametric constant false alarm rate (CFAR) detection algorithms which are based on various statistical distributions, such as Gaussian, Gamma, Weibull, log-normal, G0 distribution, alpha-stable distribution, etc, are most widely used to detect the ship targets in SAR image at present. However, the clutter background in SAR images is complicated and variable. When the actual clutter background deviates from the assumed statistical distribution, the performance of the parametric CFAR detector will deteriorate. In addition to the parametric CFAR schemes, there is another class of nonparametric CFAR detectors which can maintain a constant false alarm rate for the target detection without the assumption of a known clutter distribution. In this work, the Wilcoxon nonparametric CFAR scheme for ship detection in SAR image is proposed and analyzed, and a closed form of the false alarm rate for the Wilcoxon nonparametric detector to determi
&lt;/p&gt;</description></item><item><title>&#36816;&#21160;&#24341;&#23548;&#30340;&#20196;&#29260;&#21387;&#32553;&#65288;MGTC&#65289;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#26356;&#23567;&#20294;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#20196;&#29260;&#38598;&#26469;&#20943;&#23569;Transformer&#27169;&#22411;&#22788;&#29702;&#35270;&#39057;&#26102;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2402.18577</link><description>&lt;p&gt;
&#36816;&#21160;&#24341;&#23548;&#30340;&#20196;&#29260;&#21387;&#32553;&#29992;&#20110;&#39640;&#25928;&#30340;&#36974;&#34109;&#35270;&#39057;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Motion Guided Token Compression for Efficient Masked Video Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18577
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#24341;&#23548;&#30340;&#20196;&#29260;&#21387;&#32553;&#65288;MGTC&#65289;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#26356;&#23567;&#20294;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#20196;&#29260;&#38598;&#26469;&#20943;&#23569;Transformer&#27169;&#22411;&#22788;&#29702;&#35270;&#39057;&#26102;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;Transformer&#27169;&#22411;&#22312;&#22686;&#24378;&#35270;&#39057;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;O($N^2$)&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#24471;&#38754;&#23545;&#35270;&#39057;&#30340;&#39640;&#32500;&#24230;&#26102;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#38556;&#30861;&#12290;&#24403;&#25105;&#20204;&#23581;&#35797;&#22686;&#21152;&#27599;&#31186;&#24103;&#25968;&#65288;FPS&#65289;&#20197;&#25552;&#39640;&#36816;&#21160;&#25429;&#25417;&#33021;&#21147;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23588;&#20026;&#31361;&#20986;&#12290;&#36825;&#26679;&#30340;&#36861;&#27714;&#24456;&#21487;&#33021;&#24341;&#20837;&#20887;&#20313;&#65292;&#21152;&#21095;&#29616;&#26377;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#36890;&#36807;&#25552;&#39640;FPS&#29575;&#25152;&#23454;&#29616;&#30340;&#22686;&#24378;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36816;&#21160;&#24341;&#23548;&#30340;&#20196;&#29260;&#21387;&#32553;&#65288;MGTC&#65289;&#65292;&#20197;&#36171;&#33021;Transformer&#27169;&#22411;&#21033;&#29992;&#26356;&#23567;&#20294;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#20196;&#29260;&#38598;&#26469;&#36827;&#34892;&#20840;&#38754;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#36825;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#32780;&#19988;&#20445;&#25345;&#20102;&#25972;&#20307;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18577v1 Announce Type: cross  Abstract: Recent developments in Transformers have achieved notable strides in enhancing video comprehension. Nonetheless, the O($N^2$) computation complexity associated with attention mechanisms presents substantial computational hurdles when dealing with the high dimensionality of videos. This challenge becomes particularly pronounced when striving to increase the frames per second (FPS) to enhance the motion capturing capabilities. Such a pursuit is likely to introduce redundancy and exacerbate the existing computational limitations. In this paper, we initiate by showcasing the enhanced performance achieved through an escalation in the FPS rate. Additionally, we present a novel approach, Motion Guided Token Compression (MGTC), to empower Transformer models to utilize a smaller yet more representative set of tokens for comprehensive video representation. Consequently, this yields substantial reductions in computational burden and remains seaml
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;PSO-RDV&#26694;&#26550;&#25913;&#36827;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#38543;&#26426;&#19979;&#38477;&#36895;&#24230;&#24815;&#24615;&#26435;&#37325;&#65288;RDV IW&#65289;&#25216;&#26415;&#25552;&#39640;&#20102;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#30340;&#25910;&#25947;&#24615;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18576</link><description>&lt;p&gt;
&#21033;&#29992;PSO-RDV&#26694;&#26550;&#25913;&#21892;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Forecasting Using a PSO-RDV Framework to Enhance Artificial Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;PSO-RDV&#26694;&#26550;&#25913;&#36827;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#38543;&#26426;&#19979;&#38477;&#36895;&#24230;&#24815;&#24615;&#26435;&#37325;&#65288;RDV IW&#65289;&#25216;&#26415;&#25552;&#39640;&#20102;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#30340;&#25910;&#25947;&#24615;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#21644;&#35268;&#21010;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#20005;&#37325;&#20381;&#36182;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#12290;&#25919;&#24220;&#21644;&#20844;&#20247;&#22312;&#28508;&#22312;&#26410;&#26469;&#20844;&#20849;&#21355;&#29983;&#19981;&#30830;&#23450;&#24615;&#38754;&#20020;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#21033;&#30410;&#26368;&#22823;&#21270;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25913;&#36827;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#19979;&#38477;&#36895;&#24230;&#24815;&#24615;&#26435;&#37325;&#65288;RDV IW&#65289;&#25216;&#26415;&#26469;&#25552;&#39640;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#30340;&#25910;&#25947;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#20934;&#30830;&#24615;&#12290; &#21463;&#39640;&#23572;&#22827;&#29699;&#36816;&#21160;&#21551;&#21457;&#65292;IW&#25216;&#26415;&#20462;&#25913;&#20102;&#31890;&#23376;&#25509;&#36817;&#35299;&#20915;&#26041;&#26696;&#28857;&#26102;&#30340;&#36895;&#24230;&#65292;&#20351;&#20854;&#21576;&#29616;&#25243;&#29289;&#32447;&#19979;&#38477;&#32467;&#26500;&#12290; &#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#24314;&#35758;&#30340;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;[0.4, 0.9]&#30340;alpha&#21644;alpha_dump&#32452;&#21512;&#65292;&#30456;&#23545;&#20110;&#26087;&#27169;&#22411;&#22312;&#20301;&#32622;&#35823;&#24046;&#19978;&#26377;6.36&#65285;&#30340;&#25913;&#36827;&#65292;&#35745;&#31639;&#26102;&#38388;&#19978;&#26377;11.75&#65285;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290; &#23427;&#36798;&#21040;&#20102;&#26368;&#20248;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18576v1 Announce Type: cross  Abstract: Decision making and planning have long relied heavily on AI-driven forecasts. The government and the general public are working to minimize the risks while maximizing benefits in the face of potential future public health uncertainties. This study used an improved method of forecasting utilizing the Random Descending Velocity Inertia Weight (RDV IW) technique to improve the convergence of Particle Swarm Optimization (PSO) and the accuracy of Artificial Neural Network (ANN). The IW technique, inspired by the motions of a golf ball, modified the particles' velocities as they approached the solution point to a parabolically descending structure. Simulation results revealed that the proposed forecasting model with [0.4, 0.9] combination of alpha and alpha_dump exhibits a 6.36% improvement in position error and 11.75% improvement in computational time compared to the old model, thus, improving its convergence. It reached the optimum level a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DiffuseRAW&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#29983;&#25104;RAW&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#20302;&#20809;&#29031;&#22270;&#20687;&#22788;&#29702;&#20013;&#25972;&#20010;&#22270;&#20687;&#22788;&#29702;&#31649;&#36947;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18575</link><description>&lt;p&gt;
DiffuseRAW&#65306;&#31471;&#21040;&#31471;&#29983;&#25104;RAW&#22270;&#20687;&#22788;&#29702;&#29992;&#20110;&#20302;&#20809;&#29031;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
DiffuseRAW: End-to-End Generative RAW Image Processing for Low-Light Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DiffuseRAW&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#29983;&#25104;RAW&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#20302;&#20809;&#29031;&#22270;&#20687;&#22788;&#29702;&#20013;&#25972;&#20010;&#22270;&#20687;&#22788;&#29702;&#31649;&#36947;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26497;&#20302;&#20809;&#26465;&#20214;&#19979;&#25104;&#20687;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#65292;&#30001;&#20110;&#26368;&#23567;&#20809;&#23376;&#25429;&#33719;&#24341;&#36215;&#30340;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36870;&#38382;&#39064;&#12290;&#20197;&#21069;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#29992;&#20110;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#20219;&#21153;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#12290;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#26159;&#22312;&#22788;&#29702;&#21518;&#30340;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#22788;&#29702;&#21518;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#19981;&#36866;&#29992;&#20110;&#26497;&#20302;&#20809;&#20219;&#21153;&#12290;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#25110;&#22270;&#20687;&#21040;&#22270;&#20687;&#22686;&#24378;&#20219;&#21153;&#19981;&#21516;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;RAW&#22270;&#20687;&#21040;&#22788;&#29702;&#21518;&#22270;&#20687;&#30340;&#25972;&#20010;&#22270;&#20687;&#22788;&#29702;&#31649;&#36947;&#23398;&#20064;&#20219;&#21153;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#20256;&#32479;&#30340;&#22270;&#20687;&#22788;&#29702;&#31649;&#36947;&#36890;&#24120;&#30001;&#22810;&#20010;&#19987;&#38376;&#21270;&#37096;&#20998;&#32452;&#25104;&#65292;&#36807;&#24230;&#20381;&#36182;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;&#36825;&#20123;&#19981;&#21516;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;ISP&#65292;&#20381;&#36182;&#20110;&#24494;&#35843;&#28508;&#22312;&#30340;&#25193;&#25955;&#27169;&#22411;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18575v1 Announce Type: cross  Abstract: Imaging under extremely low-light conditions presents a significant challenge and is an ill-posed problem due to the low signal-to-noise ratio (SNR) caused by minimal photon capture. Previously, diffusion models have been used for multiple kinds of generative tasks and image-to-image tasks, however, these models work as a post-processing step. These diffusion models are trained on processed images and learn on processed images. However, such approaches are often not well-suited for extremely low-light tasks. Unlike the task of low-light image enhancement or image-to-image enhancement, we tackle the task of learning the entire image-processing pipeline, from the RAW image to a processed image. For this task, a traditional image processing pipeline often consists of multiple specialized parts that are overly reliant on the downstream tasks. Unlike these, we develop a new generative ISP that relies on fine-tuning latent diffusion models o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18571</link><description>&lt;p&gt;
&#29992;&#20110;&#28385;&#36275;&#22810;&#26679;&#29992;&#25143;&#20559;&#22909;&#30340;&#31639;&#26415;&#25511;&#21046;LLMs&#65306;&#20855;&#26377;&#22810;&#30446;&#26631;&#22870;&#21169;&#30340;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31934;&#32454;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#24314;&#27169;&#26469;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#37197;&#32622;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20026;&#22870;&#21169;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65288;&#21363;&#21333;&#20301;&#21521;&#37327;&#65289;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18496</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
Language Models Represent Beliefs of Self and Others
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18496
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#24402;&#22240;&#24515;&#29702;&#29366;&#24577;&#65292;&#21363;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#65292;&#34987;&#35270;&#20026;&#20154;&#31867;&#31038;&#20250;&#25512;&#29702;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20284;&#20046;&#20855;&#26377;&#26576;&#20123;ToM&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#30721;&#21508;&#20010;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#26159;&#21487;&#33021;&#30340;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#33258;&#25105;&#30340;&#20869;&#37096;&#34920;&#36848;&#21644;&#20182;&#20154;&#20449;&#24565;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#34920;&#24449;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;ToM&#24615;&#33021;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#24310;&#20280;&#21040;&#28041;&#21450;&#19981;&#21516;&#22240;&#26524;&#25512;&#29702;&#27169;&#24335;&#30340;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;&#36825;&#20123;&#34920;&#24449;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18496v1 Announce Type: new  Abstract: Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.
&lt;/p&gt;</description></item><item><title>FinAgent&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65292;&#36890;&#36807;&#24037;&#20855;&#22686;&#24378;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#65292;&#20855;&#26377;&#29420;&#29305;&#30340;&#21452;&#37325;&#21453;&#23556;&#27169;&#22359;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#24182;&#24555;&#36895;&#36866;&#24212;&#24066;&#22330;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.18485</link><description>&lt;p&gt;
FinAgent: &#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65306;&#24037;&#20855;&#22686;&#24378;&#12289;&#22810;&#26679;&#21270;&#21644;&#36890;&#29992;
&lt;/p&gt;
&lt;p&gt;
FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18485
&lt;/p&gt;
&lt;p&gt;
FinAgent&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65292;&#36890;&#36807;&#24037;&#20855;&#22686;&#24378;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#65292;&#20855;&#26377;&#29420;&#29305;&#30340;&#21452;&#37325;&#21453;&#23556;&#27169;&#22359;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#24182;&#24555;&#36895;&#36866;&#24212;&#24066;&#22330;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#26159;&#24066;&#22330;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21463;&#21040;&#26032;&#38395;&#12289;&#20215;&#26684;&#21644;K&#32447;&#22270;&#31561;&#22810;&#27169;&#24577;&#20449;&#24687;&#26500;&#25104;&#30340;&#20449;&#24687;&#26223;&#35266;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#37327;&#21270;&#20132;&#26131;&#21644;&#19981;&#21516;&#36164;&#20135;&#30340;&#39640;&#39057;&#20132;&#26131;&#31561;&#22810;&#26679;&#21270;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#20808;&#36827;AI&#25216;&#26415;&#22312;&#37329;&#34701;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#37329;&#34701;&#20132;&#26131;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#19981;&#36275;&#21644;&#36328;&#19981;&#21516;&#20219;&#21153;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FinAgent&#65292;&#19968;&#20010;&#20855;&#26377;&#24037;&#20855;&#22686;&#24378;&#21151;&#33021;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65292;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#12290;FinAgent&#30340;&#24066;&#22330;&#26234;&#33021;&#27169;&#22359;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;-&#25968;&#20540;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;-&#20197;&#20934;&#30830;&#20998;&#26512;&#37329;&#34701;&#24066;&#22330;&#12290;&#20854;&#29420;&#29305;&#30340;&#21452;&#37325;&#21453;&#23556;&#27169;&#22359;&#19981;&#20165;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#24066;&#22330;&#21160;&#24577;&#65292;&#36824;&#34701;&#21512;&#20102;&#22810;&#26679;&#21270;&#30340;&#35760;&#24518;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18485v1 Announce Type: cross  Abstract: Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.18409</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22270;&#20687;&#25512;&#29702;&#21644;&#25551;&#36848;&#30340;&#35748;&#30693;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#21463;&#21040;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#27979;&#35797;&#12290;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#27979;&#35797;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#8220;&#20599;&#39292;&#24178;&#8221;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#20855;&#26377;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#20687;&#35780;&#20272;LVLMs&#30340;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#23427;&#23450;&#20041;&#20102;&#20843;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21253;&#25324;&#22270;&#20687;&#25551;&#36848;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;&#30693;&#21517;LVLMs&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;LVLMs&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18409v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#27867;&#21270;&#38382;&#39064;&#30340;&#27491;&#24335;&#26694;&#26550;, &#24182;&#38416;&#36848;&#20102;&#36328;&#39046;&#22495;&#27867;&#21270;&#22312;DSR&#20013;&#19982;&#26426;&#22120;&#23398;&#20064;&#20854;&#20182;&#39046;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;</title><link>https://arxiv.org/abs/2402.18377</link><description>&lt;p&gt;
&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Generalization in Dynamical Systems Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#27867;&#21270;&#38382;&#39064;&#30340;&#27491;&#24335;&#26694;&#26550;, &#24182;&#38416;&#36848;&#20102;&#36328;&#39046;&#22495;&#27867;&#21270;&#22312;DSR&#20013;&#19982;&#26426;&#22120;&#23398;&#20064;&#20854;&#20182;&#39046;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#25214;&#21040;&#22312;&#32463;&#39564;&#29616;&#35937;&#32972;&#21518;&#30340;&#25511;&#21046;&#26041;&#31243;&#21644;&#21160;&#21147;&#35268;&#21017;&#12290;&#20256;&#32479;&#19978;&#65292;&#31185;&#23398;&#27169;&#22411;&#26159;&#36890;&#36807;&#20154;&#31867;&#27934;&#23519;&#21644;&#23454;&#39564;&#21608;&#26399;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#34987;&#29992;&#26469;&#30452;&#25509;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#37325;&#26500;&#21160;&#21147;&#31995;&#32479;&#65288;DS&#65289;&#12290;&#26368;&#20808;&#36827;&#30340;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#65288;DSR&#65289;&#26041;&#27861;&#22312;&#25429;&#25417;&#35266;&#23519;&#21040;&#30340;DS&#30340;&#19981;&#21464;&#21644;&#38271;&#26399;&#29305;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#27867;&#21270;&#21040;&#26410;&#35266;&#23519;&#39046;&#22495;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#25105;&#20204;&#26399;&#26395;&#20174;&#20219;&#20309;&#21487;&#34892;&#30340;&#31185;&#23398;&#29702;&#35770;&#20013;&#33719;&#24471;&#30340;&#33267;&#20851;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;DSR&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;DSR&#20013;&#30340;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#27867;&#21270;&#65288;OODG&#65289;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#32771;&#34385;&#30340;OODG&#26377;&#26681;&#26412;&#21306;&#21035;&#12290;&#25105;&#20204;&#20171;&#32461;&#22522;&#20110;&#25299;&#25169;&#27010;&#24565;&#21644;&#31526;&#21495;&#30340;&#25968;&#23398;&#27010;&#24565;&#65292;&#24182;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18377v1 Announce Type: new  Abstract: In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergo
&lt;/p&gt;</description></item><item><title>HearHere&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20174;&#19981;&#21516;&#35270;&#35282;&#34701;&#21512;&#20449;&#24687;&#21644;&#35266;&#28857;&#65292;&#20197;&#20943;&#36731;&#26032;&#38395;&#28040;&#36153;&#20013;&#8220;&#22238;&#22768;&#23460;&#8221;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.18222</link><description>&lt;p&gt;
HearHere: &#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#31995;&#32479;&#32531;&#35299;&#26032;&#38395;&#28040;&#36153;&#20013;&#30340;&#8220;&#22238;&#22768;&#23460;&#8221;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18222
&lt;/p&gt;
&lt;p&gt;
HearHere&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20174;&#19981;&#21516;&#35270;&#35282;&#34701;&#21512;&#20449;&#24687;&#21644;&#35266;&#28857;&#65292;&#20197;&#20943;&#36731;&#26032;&#38395;&#28040;&#36153;&#20013;&#8220;&#22238;&#22768;&#23460;&#8221;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#27491;&#22312;&#22823;&#21147;&#21162;&#21147;&#20943;&#36731;&#8220;&#22238;&#22768;&#23460;&#8221;&#25152;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21253;&#25324;&#26356;&#23481;&#26131;&#21463;&#21040;&#34394;&#20551;&#26032;&#38395;&#30340;&#24433;&#21709;&#20197;&#21450;&#23545;&#25509;&#21463;&#31185;&#23398;&#35777;&#25454;&#30340;&#25239;&#25298;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HearHere&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20174;&#19981;&#21516;&#35270;&#35282;&#34701;&#21512;&#20449;&#24687;&#21644;&#35266;&#28857;&#12290;HearHere&#36890;&#36807;&#20004;&#31181;&#21487;&#35270;&#21270;&#26041;&#24335;&#20419;&#36827;&#20102;&#26032;&#38395;&#20449;&#24687;&#28040;&#36153;&#30340;&#20851;&#38190;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18222v1 Announce Type: cross  Abstract: Considerable efforts are currently underway to mitigate the negative impacts of echo chambers, such as increased susceptibility to fake news and resistance towards accepting scientific evidence. Prior research has presented the development of computer systems that support the consumption of news information from diverse political perspectives to mitigate the echo chamber effect. However, existing studies still lack the ability to effectively support the key processes of news information consumption and quantitatively identify a political stance towards the information. In this paper, we present HearHere, an AI-based web system designed to help users accommodate information and opinions from diverse perspectives. HearHere facilitates the key processes of news information consumption through two visualizations. Visualization 1 provides political news with quantitative political stance information, derived from our graph-based political c
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.17398</link><description>&lt;p&gt;
&#37327;&#23376;&#26041;&#27861;&#30740;&#31350;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;
&lt;/p&gt;
&lt;p&gt;
A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17398
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;Quantum-SMOTE&#21463;&#21040;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#37327;&#23376;&#36807;&#31243;&#22914;&#20132;&#25442;&#27979;&#35797;&#21644;&#37327;&#23376;&#26059;&#36716;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#28857;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;SMOTE&#31639;&#27861;&#20351;&#29992;K-&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#27431;&#27663;&#36317;&#31163;&#30340;&#26041;&#24335;&#26377;&#25152;&#19981;&#21516;&#65292;&#33021;&#22815;&#20174;&#23569;&#25968;&#31867;&#25968;&#25454;&#28857;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#37051;&#36817;&#24615;&#12290;&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#38656;&#27714;&#30340;&#23450;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;TelecomChurn&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20004;&#31181;&#20027;&#35201;&#30340;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17398v1 Announce Type: cross  Abstract: The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20027;&#21160;&#25511;&#21046;&#21644;&#22609;&#36896;&#26059;&#32764;&#20135;&#29983;&#30340;&#39134;&#34892;&#22120;&#25512;&#36827;&#22122;&#22768;&#26469;&#26377;&#21033;&#20110;&#23450;&#20301;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22122;&#22768;&#21644;&#26102;&#38388;&#21464;&#21270;&#26059;&#32764;&#30456;&#20301;&#35843;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#23450;&#20301;&#12290;</title><link>https://arxiv.org/abs/2402.17289</link><description>&lt;p&gt;
&#22810;&#26059;&#32764;&#39134;&#34892;&#22120;&#23450;&#20301;&#30340;&#20027;&#21160;&#25512;&#36827;&#22122;&#22768;&#22609;&#36896;
&lt;/p&gt;
&lt;p&gt;
Active propulsion noise shaping for multi-rotor aircraft localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20027;&#21160;&#25511;&#21046;&#21644;&#22609;&#36896;&#26059;&#32764;&#20135;&#29983;&#30340;&#39134;&#34892;&#22120;&#25512;&#36827;&#22122;&#22768;&#26469;&#26377;&#21033;&#20110;&#23450;&#20301;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22122;&#22768;&#21644;&#26102;&#38388;&#21464;&#21270;&#26059;&#32764;&#30456;&#20301;&#35843;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26059;&#32764;&#31354;&#20013;&#33258;&#20027;&#36733;&#20855;(MAVs)&#20027;&#35201;&#20381;&#36182;&#35270;&#35273;&#36827;&#34892;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#23450;&#20301;&#21644;&#27979;&#36317;&#25216;&#26415;&#22312;&#20302;&#25110;&#30452;&#23556;&#38451;&#20809;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#35270;&#37326;&#26377;&#38480;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#36974;&#25377;&#30340;&#24433;&#21709;&#12290;&#22768;&#23398;&#20256;&#24863;&#21487;&#20197;&#20316;&#20026;&#35270;&#35273;&#30340;&#34917;&#20805;&#25110;&#29978;&#33267;&#26367;&#20195;&#26041;&#24335;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20351;&#29992;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#26356;&#20302;&#30340;&#31995;&#32479;&#25104;&#26412;&#21644;&#33021;&#28304;&#21344;&#29992;&#37327;&#65292;&#36825;&#23545;&#24494;&#22411;&#39134;&#34892;&#22120;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20027;&#21160;&#25511;&#21046;&#21644;&#22609;&#36896;&#30001;&#26059;&#32764;&#20135;&#29983;&#30340;&#39134;&#34892;&#22120;&#25512;&#36827;&#22122;&#22768;&#65292;&#20197;&#26377;&#21033;&#20110;&#23450;&#20301;&#20219;&#21153;&#65292;&#32780;&#38750;&#23558;&#20854;&#35270;&#20026;&#26377;&#23475;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22122;&#22768;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#24050;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23450;&#20301;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21516;&#26102;&#35757;&#32451;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#26059;&#32764;&#30456;&#20301;&#35843;&#21046;&#65292;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#23450;&#20301;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17289v1 Announce Type: cross  Abstract: Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for navigation purposes. However, visual localization and odometry techniques suffer from poor performance in low or direct sunlight, a limited field of view, and vulnerability to occlusions. Acoustic sensing can serve as a complementary or even alternative modality for vision in many situations, and it also has the added benefits of lower system cost and energy footprint, which is especially important for micro aircraft. This paper proposes actively controlling and shaping the aircraft propulsion noise generated by the rotors to benefit localization tasks, rather than considering it a harmful nuisance. We present a neural network architecture for selfnoise-based localization in a known environment. We show that training it simultaneously with learning time-varying rotor phase modulation achieves accurate and robust localization. The proposed methods are evaluated u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23454;&#29616;&#20102;&#36229;&#20302;&#27604;&#29305;&#29575;&#22270;&#20687;&#35821;&#20041;&#21387;&#32553;&#65292;&#22312;&#20445;&#25345;&#19982;&#30495;&#23454;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24863;&#30693;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.16749</link><description>&lt;p&gt;
MISC: &#30001;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#39537;&#21160;&#30340;&#36229;&#20302;&#27604;&#29305;&#29575;&#22270;&#20687;&#35821;&#20041;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23454;&#29616;&#20102;&#36229;&#20302;&#27604;&#29305;&#29575;&#22270;&#20687;&#35821;&#20041;&#21387;&#32553;&#65292;&#22312;&#20445;&#25345;&#19982;&#30495;&#23454;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24863;&#30693;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23384;&#20648;&#21644;&#36890;&#20449;&#21327;&#35758;&#30340;&#28436;&#21464;&#65292;&#36229;&#20302;&#27604;&#29305;&#29575;&#22270;&#20687;&#21387;&#32553;&#24050;&#25104;&#20026;&#19968;&#20010;&#26497;&#20855;&#38656;&#27714;&#30340;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21387;&#32553;&#31639;&#27861;&#24517;&#39035;&#22312;&#36229;&#20302;&#27604;&#29305;&#29575;&#19979;&#35201;&#20040;&#29306;&#29298;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#65292;&#35201;&#20040;&#29306;&#29298;&#24863;&#30693;&#36136;&#37327;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#24179;&#34913;&#36825;&#20004;&#20010;&#30446;&#26631;&#21464;&#24471;&#21487;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#27169;&#24577;&#22270;&#20687;&#35821;&#20041;&#21387;&#32553;&#65288;MISC&#65289;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;LMM&#32534;&#30721;&#22120;&#29992;&#20110;&#25552;&#21462;&#22270;&#20687;&#30340;&#35821;&#20041;&#20449;&#24687;&#12289;&#19968;&#20010;&#22320;&#22270;&#32534;&#30721;&#22120;&#29992;&#20110;&#23450;&#20301;&#19982;&#35821;&#20041;&#23545;&#24212;&#30340;&#21306;&#22495;&#12289;&#19968;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#29983;&#25104;&#26497;&#24230;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#65292;&#20197;&#21450;&#19968;&#20010;&#35299;&#30721;&#22120;&#26681;&#25454;&#19978;&#36848;&#20449;&#24687;&#37325;&#26500;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;MISC&#36866;&#29992;&#20110;&#21387;&#32553;&#20256;&#32479;&#33258;&#28982;&#24863;&#30693;&#22270;&#20687;&#65288;NSIs&#65289;&#21644;&#26032;&#20852;AI&#29983;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16749v2 Announce Type: replace-cross  Abstract: With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic. However, existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate. In recent years, the rapid development of the Large Multimodal Model (LMM) has made it possible to balance these two goals. To solve this problem, this paper proposes a method called Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information. Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;GenAI&#30340;GenAINet&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#20256;&#36755;&#21644;&#25512;&#29702;&#23454;&#29616;&#26080;&#32447;&#38598;&#20307;&#26234;&#33021;&#65292;&#20026;6G&#26102;&#20195;&#38138;&#24179;&#20102;&#36890;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.16631</link><description>&lt;p&gt;
GenAINet&#65306;&#36890;&#36807;&#30693;&#35782;&#20256;&#36755;&#21644;&#25512;&#29702;&#23454;&#29616;&#26080;&#32447;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16631
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;GenAI&#30340;GenAINet&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#20256;&#36755;&#21644;&#25512;&#29702;&#23454;&#29616;&#26080;&#32447;&#38598;&#20307;&#26234;&#33021;&#65292;&#20026;6G&#26102;&#20195;&#38138;&#24179;&#20102;&#36890;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16631v2 &#22768;&#26126;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21644;&#36890;&#20449;&#32593;&#32476;&#34987;&#26399;&#26395;&#22312;6G&#20013;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36830;&#25509;GenAI&#20195;&#29702;&#21487;&#33021;&#20250;&#37322;&#25918;&#38598;&#20307;&#26234;&#33021;&#30340;&#21147;&#37327;&#65292;&#24182;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26080;&#32447;&#32593;&#32476;&#35774;&#35745;&#20026;&#8220;&#25968;&#25454;&#31649;&#36947;&#8221;&#65292;&#24182;&#19981;&#36866;&#21512;&#23481;&#32435;&#21644;&#21033;&#29992;GenAI&#30340;&#21147;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenAINet&#26694;&#26550;&#65292;&#20854;&#20013;&#20998;&#24067;&#24335;GenAI&#20195;&#29702;&#20256;&#36798;&#30693;&#35782;&#65288;&#39640;&#32423;&#27010;&#24565;&#25110;&#25688;&#35201;&#65289;&#20197;&#23436;&#25104;&#20219;&#24847;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#32593;&#32476;&#26550;&#26500;&#65292;&#25972;&#21512;&#20102;GenAI&#33021;&#21147;&#65292;&#20197;&#31649;&#29702;&#32593;&#32476;&#21327;&#35758;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35821;&#20041;&#26412;&#22320;&#21270;&#30340;GenAINet&#26469;&#30740;&#31350;&#26377;&#25928;&#30340;&#36890;&#20449;&#21644;&#25512;&#29702;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;GenAI&#20195;&#29702;&#20174;&#22810;&#27169;&#24577;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#27010;&#24565;&#65292;&#26500;&#24314;&#19968;&#20010;&#30693;&#35782;&#24211;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16631v2 Announce Type: replace  Abstract: Generative artificial intelligence (GenAI) and communication networks are expected to have groundbreaking synergies in 6G. Connecting GenAI agents over a wireless network can potentially unleash the power of collective intelligence and pave the way for artificial general intelligence (AGI). However, current wireless networks are designed as a "data pipe" and are not suited to accommodate and leverage the power of GenAI. In this paper, we propose the GenAINet framework in which distributed GenAI agents communicate knowledge (high-level concepts or abstracts) to accomplish arbitrary tasks. We first provide a network architecture integrating GenAI capabilities to manage both network protocols and applications. Building on this, we investigate effective communication and reasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI agents extract semantic concepts from multi-modal raw data, build a knowledgebase represe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#65292;&#23558;&#29983;&#25104;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#29992;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16459</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending LLMs against Jailbreaking Attacks via Backtranslation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#65292;&#23558;&#29983;&#25104;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#29992;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#35757;&#32451;&#25104;&#25298;&#32477;&#26377;&#23475;&#35831;&#27714;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#37325;&#20889;&#21407;&#22987;&#25552;&#31034;&#20197;&#38544;&#34255;&#20854;&#26377;&#23475;&#24847;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#21453;&#21521;&#32763;&#35793;&#8221;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#30446;&#26631;LLM&#20174;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#30340;&#21021;&#22987;&#21709;&#24212;&#65292;&#25105;&#20204;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#26029;&#21487;&#20197;&#23548;&#33268;&#35813;&#21709;&#24212;&#30340;&#36755;&#20837;&#25552;&#31034;&#12290;&#25512;&#26029;&#30340;&#25552;&#31034;&#31216;&#20026;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#65292;&#20542;&#21521;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#22240;&#20026;&#23427;&#26159;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#30340;&#65292;&#19981;&#26159;&#30452;&#25509;&#30001;&#25915;&#20987;&#32773;&#25805;&#32437;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20877;&#27425;&#22312;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#19978;&#36816;&#34892;&#30446;&#26631;LLM&#65292;&#22914;&#26524;&#27169;&#22411;&#25298;&#32477;&#20102;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#65292;&#21017;&#25298;&#32477;&#21407;&#22987;&#25552;&#31034;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#25514;&#26045;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#20960;&#20010;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16363</link><description>&lt;p&gt;
LLM&#25512;&#26029;&#25581;&#31034;&#65306;&#35843;&#26597;&#19982;Roofline&#27169;&#22411;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLM Inference Unveiled: Survey and Roofline Model Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#20379;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#29420;&#29305;&#32467;&#21512;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#24050;&#32463;&#25193;&#23637;&#24182;&#20805;&#28385;&#27963;&#21147;&#65292;&#20294;&#33267;&#20170;&#36824;&#27809;&#26377;&#19968;&#20010;&#31616;&#26126;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;LLM&#25512;&#26029;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#19981;&#20165;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#36824;&#22522;&#20110;Roofline&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#12290;&#36825;&#19968;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#35782;&#21035;LLM&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20174;&#32780;&#20026;&#37096;&#32626;LLM&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27719;&#24635;&#20102;&#39640;&#25928;LLM&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#26435;&#37325;&#20248;&#21270;&#65288;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21019;&#20316;Java&#32534;&#31243;&#35838;&#31243;&#30340;&#31034;&#20363;&#65292;&#20197;&#20943;&#36731;&#25945;&#24072;&#36880;&#34892;&#35299;&#37322;&#22823;&#37327;&#31034;&#20363;&#30340;&#36127;&#25285;</title><link>https://arxiv.org/abs/2402.16235</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#21019;&#20316;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
Human-AI Co-Creation of Worked Examples for Programming Classes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16235
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21019;&#20316;Java&#32534;&#31243;&#35838;&#31243;&#30340;&#31034;&#20363;&#65292;&#20197;&#20943;&#36731;&#25945;&#24072;&#36880;&#34892;&#35299;&#37322;&#22823;&#37327;&#31034;&#20363;&#30340;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#31034;&#20363;&#65288;&#20856;&#22411;&#32534;&#31243;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26576;&#31181;&#35821;&#35328;&#30340;&#28304;&#20195;&#30721;&#21576;&#29616;&#65292;&#24182;&#29992;&#20110;&#35299;&#37322;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#20027;&#39064;&#65289;&#26159;&#32534;&#31243;&#35838;&#31243;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#23398;&#20064;&#20869;&#23481;&#20043;&#19968;&#12290;&#22823;&#22810;&#25968;&#29992;&#20110;&#21521;&#23398;&#29983;&#23637;&#31034;&#36825;&#20123;&#31034;&#20363;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;&#37117;&#22522;&#20110;&#23545;&#31034;&#20363;&#20195;&#30721;&#30340;&#36880;&#34892;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#25945;&#24072;&#24456;&#23569;&#26377;&#26102;&#38388;&#20026;&#32534;&#31243;&#35838;&#31243;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;&#22823;&#37327;&#31034;&#20363;&#25552;&#20379;&#36880;&#34892;&#35299;&#37322;&#12290;&#26412;&#25991;&#25506;&#35752;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;Java&#32534;&#31243;&#25776;&#20889;&#31034;&#20363;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#21019;&#24314;Java&#24037;&#20316;&#31034;&#20363;&#30340;&#32534;&#20889;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#30340;&#21021;&#22987;&#29256;&#26412;&#65292;&#24182;&#23558;&#20854;&#21576;&#29616;&#32473;&#25945;&#24072;&#20197;&#22312;&#24517;&#35201;&#26102;&#36827;&#34892;&#32534;&#36753;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#39033;&#35780;&#20272;&#20351;&#29992;&#27492;&#26041;&#27861;&#21019;&#24314;&#30340;&#35299;&#37322;&#36136;&#37327;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16235v1 Announce Type: cross  Abstract: Worked examples (solutions to typical programming problems presented as a source code in a certain language and are used to explain the topics from a programming class) are among the most popular types of learning content in programming classes. Most approaches and tools for presenting these examples to students are based on line-by-line explanations of the example code. However, instructors rarely have time to provide line-by-line explanations for a large number of examples typically used in a programming class. In this paper, we explore and assess a human-AI collaboration approach to authoring worked examples for Java programming. We introduce an authoring system for creating Java worked examples that generates a starting version of code explanations and presents it to the instructor to edit if necessary.We also present a study that assesses the quality of explanations created with this approach
&lt;/p&gt;</description></item><item><title>ROS-Causal&#26159;&#19968;&#20010;&#22522;&#20110;ROS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#31354;&#38388;&#20132;&#20114;&#20013;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#22240;&#26524;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#32570;&#20047;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;ROS&#29983;&#24577;&#31995;&#32479;&#20869;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16068</link><description>&lt;p&gt;
ROS-Causal&#65306;&#22522;&#20110;ROS&#30340;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot Interaction Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16068
&lt;/p&gt;
&lt;p&gt;
ROS-Causal&#26159;&#19968;&#20010;&#22522;&#20110;ROS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#31354;&#38388;&#20132;&#20114;&#20013;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#22240;&#26524;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#32570;&#20047;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;ROS&#29983;&#24577;&#31995;&#32479;&#20869;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#20849;&#20139;&#31354;&#38388;&#37096;&#32626;&#26426;&#22120;&#20154;&#38656;&#35201;&#29702;&#35299;&#38468;&#36817;Agent&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#23545;&#22240;&#26524;&#20851;&#31995;&#24314;&#27169;&#26377;&#21161;&#20110;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#24182;&#39044;&#27979;&#26426;&#22120;&#20154;&#24178;&#39044;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30446;&#21069;&#32570;&#20047;&#22312;ROS&#29983;&#24577;&#31995;&#32479;&#20869;&#37096;&#30340;&#23454;&#29616;&#65292;&#36825;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#38459;&#30861;&#20102;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;ROS-Causal&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;ROS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#19978;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#22240;&#26524;&#21457;&#29616;&#22312;&#20154;&#26426;&#31354;&#38388;&#20132;&#20114;&#20013;&#12290;&#38598;&#25104;&#20102;ROS&#30340;&#20020;&#26102;&#27169;&#25311;&#22120;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#29983;&#25104;&#22240;&#26524;&#27169;&#22411;&#12290;ROS-Causal&#21487;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/lcastri/roscausal.git&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16068v1 Announce Type: cross  Abstract: Deploying robots in human-shared spaces requires understanding interactions among nearby agents and objects. Modelling cause-and-effect relations through causal inference aids in predicting human behaviours and anticipating robot interventions. However, a critical challenge arises as existing causal discovery methods currently lack an implementation inside the ROS ecosystem, the standard de facto in robotics, hindering effective utilisation in robotics. To address this gap, this paper introduces ROS-Causal, a ROS-based framework for onboard data collection and causal discovery in human-robot spatial interactions. An ad-hoc simulator, integrated with ROS, illustrates the approach's effectiveness, showcasing the robot onboard generation of causal models during data collection. ROS-Causal is available on GitHub: https://github.com/lcastri/roscausal.git.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15183</link><description>&lt;p&gt;
GraphEdit&#65306;&#29992;&#20110;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphEdit: Large Language Models for Graph Structure Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;GSL&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#29983;&#25104;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#26469;&#25429;&#25417;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#26174;&#24335;&#22270;&#32467;&#26500;&#20449;&#24687;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;EyeTrans&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#34701;&#20837;&#26426;&#22120;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14096</link><description>&lt;p&gt;
EyeTrans: &#21512;&#24182;&#20154;&#31867;&#21644;&#26426;&#22120;&#27880;&#24847;&#21147;&#20197;&#23454;&#29616;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
EyeTrans: Merging Human and Machine Attention for Neural Code Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14096
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;EyeTrans&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#34701;&#20837;&#26426;&#22120;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural code summarization &#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#30340;&#31616;&#35201;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#12290;Transformer&#27169;&#22411;&#30340;&#21457;&#23637;&#23548;&#33268;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24191;&#27867;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#34701;&#20837;&#26426;&#22120;&#27880;&#24847;&#21147;&#20197;&#22686;&#24378;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#34701;&#21512;&#24182;&#39564;&#35777;&#36825;&#19968;&#20551;&#35774;&#65292;&#24341;&#20837;&#20102;EyeTrans&#65292;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;(1) &#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30524;&#21160;&#20154;&#31867;&#30740;&#31350;&#65292;&#25910;&#38598;&#21644;&#39044;&#20998;&#26512;&#25968;&#25454;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#65292;(2) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#25972;&#21512;&#20154;&#31867;&#27880;&#24847;&#21147;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14096v1 Announce Type: cross  Abstract: Neural code summarization leverages deep learning models to automatically generate brief natural language summaries of code snippets. The development of Transformer models has led to extensive use of attention during model design. While existing work has primarily and almost exclusively focused on static properties of source code and related structural representations like the Abstract Syntax Tree (AST), few studies have considered human attention, that is, where programmers focus while examining and comprehending code. In this paper, we develop a method for incorporating human attention into machine attention to enhance neural code summarization. To facilitate this incorporation and vindicate this hypothesis, we introduce EyeTrans, which consists of three steps: (1) we conduct an extensive eye-tracking human study to collect and pre-analyze data for model training, (2) we devise a data-centric approach to integrate human attention wit
&lt;/p&gt;</description></item><item><title>E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.14041</link><description>&lt;p&gt;
E2USD&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14041
&lt;/p&gt;
&lt;p&gt;
E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;E2USD&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#12290;E2USD&#21033;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#30340;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;(FFTCompress)&#21644;&#20998;&#35299;&#30340;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;(DDEM)&#65292;&#19968;&#36215;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#23545;&#36755;&#20837;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#38452;&#24615;&#21462;&#28040;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(FNCCLearning)&#65292;&#20197;&#25269;&#28040;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#26356;&#21451;&#22909;&#30340;&#31751;&#23884;&#20837;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#26816;&#27979;(ADATD)&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;E2USD&#33021;&#22815;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;SOTA&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AI4CTS/E2Usd &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#24341;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;RiPAMI&#20803;&#25968;&#25454;&#25968;&#25454;&#24211;&#21644;&#20027;&#39064;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;PAMI&#32508;&#36848;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.12928</link><description>&lt;p&gt;
&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#24341;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;RiPAMI&#20803;&#25968;&#25454;&#25968;&#25454;&#24211;&#21644;&#20027;&#39064;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;PAMI&#32508;&#36848;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#20998;&#25955;&#30340;&#30693;&#35782;&#65292;&#25991;&#29486;&#32508;&#36848;&#25552;&#20379;&#20102;&#23545;&#25152;&#30740;&#31350;&#20027;&#39064;&#30340;&#20840;&#38754;&#20102;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#65288;PAMI&#65289;&#36825;&#19968;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#36807;&#22810;&#30340;&#32508;&#36848;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#35780;&#35770;&#32773;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#23545;&#36825;&#20123;&#20851;&#27880;&#30340;&#22238;&#24212;&#65292;&#26412;&#25991;&#26088;&#22312;&#20174;&#22810;&#20010;&#35282;&#24230;&#20840;&#38754;&#23457;&#35270;PAMI&#39046;&#22495;&#30340;&#32508;&#36848;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12928v1 Announce Type: cross  Abstract: By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.10487</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38543;&#26426;&#25237;&#24433;&#23618;
&lt;/p&gt;
&lt;p&gt;
Random Projection Layers for Multidimensional Time Sires Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#28151;&#21512;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#27492;&#31867;&#27169;&#22411;&#24212;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#65288;&#20363;&#22914;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65289;&#26102;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#31216;&#20026;RPMixer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#24335;&#34892;&#20026;&#65292;&#20854;&#20013;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21333;&#29420;&#22359;&#30340;&#20316;&#29992;&#31867;&#20284;&#20110;&#38598;&#25104;&#27169;&#22411;&#20013;&#30340;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#29305;&#21035;&#26159;&#22312;&#24341;&#20837;&#36523;&#20221;&#26144;&#23556;&#27531;&#24046;&#36830;&#25509;&#26102;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;RPMixer&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#23545;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10487v1 Announce Type: cross  Abstract: All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperf
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10192</link><description>&lt;p&gt;
&#20511;&#37492;&#22810;&#20307;&#29289;&#29702;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19981;&#36879;&#26126;&#30340;&#12289;&#31867;&#20284;&#20110;&#31070;&#35861;&#33324;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#35299;&#37322;&#21644;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#20102;&#34987;&#31216;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#65292;&#23558;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#20026;&#19968;&#20010;&#22312;&#20855;&#26377;&#27010;&#24565;&#38468;&#21152;&#30340;&#39030;&#28857;&#30340;&#22270;&#19978;&#30340;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;&#34429;&#28982;&#36825;&#31181;&#25551;&#36848;&#20855;&#26377;&#21508;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#37327;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#19981;&#33021;&#33258;&#28982;&#22320;&#29992;&#26469;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#30340;&#25512;&#24191;&#65292;&#23427;&#23558;&#24605;&#32500;&#36807;&#31243;&#35270;&#20026;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10192v1 Announce Type: cross  Abstract: With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them. While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#32032;&#30340;&#20316;&#29992;&#21644;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#27867;&#21270;&#38382;&#39064;&#20197;&#21450;&#36807;&#24230;&#33258;&#20449;&#30340;AI&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08208</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#32032;&#22266;&#26377;&#22810;&#26679;&#21270;&#20887;&#20313;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#32032;&#30340;&#20316;&#29992;&#21644;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#27867;&#21270;&#38382;&#39064;&#20197;&#21450;&#36807;&#24230;&#33258;&#20449;&#30340;AI&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#21644;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#32032;&#12290;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#22797;&#26434;&#21644;&#39640;&#32500;&#29615;&#22659;&#20013;&#25191;&#34892;&#23454;&#26102;&#20851;&#38190;&#21151;&#33021;&#65292;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#20915;&#31574;&#20219;&#21153;&#65292;&#22914;&#36816;&#21160;&#35268;&#21010;&#12289;&#36710;&#36947;&#20445;&#25345;&#21644;&#32039;&#24613;&#21046;&#21160;&#12290;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;AI&#27169;&#22411;&#22312;&#21021;&#22987;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#22914;&#20309;&#36827;&#34892;&#27867;&#21270;&#12290;&#36825;&#31181;&#27867;&#21270;&#38382;&#39064;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#21464;&#24471;&#26126;&#26174;&#65292;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#19981;&#22312;&#20854;&#35757;&#32451;&#25110;&#39564;&#35777;&#25968;&#25454;&#20013;&#34920;&#31034;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#38754;&#20020;&#20998;&#24067;&#25110;&#39046;&#22495;&#36716;&#31227;&#65292;AI&#31995;&#32479;&#20173;&#24517;&#39035;&#26377;&#25928;&#22320;&#36816;&#34892;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#36807;&#24230;&#33258;&#20449;&#30340;AI&#27169;&#22411;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#19968;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the role and challenges of Artificial Intelligence (AI) algorithms, specifically AI-based software elements, in autonomous driving systems. These AI systems are fundamental in executing real-time critical functions in complex and high-dimensional environments. They handle vital tasks like multi-modal perception, cognition, and decision-making tasks such as motion planning, lane keeping, and emergency braking. A primary concern relates to the ability (and necessity) of AI models to generalize beyond their initial training data. This generalization issue becomes evident in real-time scenarios, where models frequently encounter inputs not represented in their training or validation data. In such cases, AI systems must still function effectively despite facing distributional or domain shifts. This paper investigates the risk associated with overconfident AI models in safety-critical applications like autonomous driving. To mitigate these risks, methods for training AI m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;</title><link>https://arxiv.org/abs/2402.05699</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22404;&#26029;&#23545;&#35805;&#30340;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#65292;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#20511;&#37492;&#31038;&#20250;&#23398;&#30340;&#35265;&#35299;&#65292;&#21363;&#35748;&#35782;&#21040;&#25152;&#26377;&#21508;&#26041;&#30340;&#20851;&#20999;&#26159;&#22609;&#36896;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23545;&#40784;LLMs&#30340;&#26032;&#26041;&#21521;&#65306;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#21019;&#26032;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#22120;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#21608;&#22260;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#20351;LLM&#22312;&#22238;&#31572;&#21069;&#33021;&#22815;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#12290;MATRIX&#31867;&#20284;&#20110;&#19968;&#20010;&#8220;&#22404;&#26029;&#23545;&#35805;&#8221;&#19979;&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#22312;&#20854;&#20013;&#25198;&#28436;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#22810;&#20010;&#35282;&#33394;&#24182;&#36827;&#34892;&#33258;&#25105;&#23454;&#36341;&#12290;&#20026;&#20102;&#24341;&#20837;&#36825;&#31181;&#23545;&#40784;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;MATRIX&#27169;&#25311;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#30830;&#20445;&#20854;&#22312;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that ou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#21517;&#20026;SHIRLEY&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26088;&#22312;&#35782;&#21035;&#27861;&#24459;&#21028;&#20915;&#20013;&#30340;&#20559;&#35265;&#21644;&#36923;&#36753;&#19981;&#19968;&#33268;&#65292;&#24182;&#20419;&#36827;&#33258;&#21160;&#21270;&#12289;&#26377;&#25928;&#21644;&#19968;&#33268;&#30340;&#22810;&#26041;&#35770;&#35777;&#65292;&#20197;&#20445;&#35777;&#27861;&#24459;&#22312;&#19981;&#21516;&#21496;&#27861;&#31649;&#36758;&#21306;&#20869;&#30340;&#19968;&#33268;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04140</link><description>&lt;p&gt;
&#25512;&#36827;&#27861;&#24459;&#25512;&#29702;&#65306;&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#22788;&#29702;&#20840;&#29699;&#27861;&#29702;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20559;&#35265;&#30340;&#21322;&#33258;&#21160;&#21270;&#20210;&#35009;&#27969;&#31243;&#65288;SAAPs&#65289;
&lt;/p&gt;
&lt;p&gt;
Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#21517;&#20026;SHIRLEY&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26088;&#22312;&#35782;&#21035;&#27861;&#24459;&#21028;&#20915;&#20013;&#30340;&#20559;&#35265;&#21644;&#36923;&#36753;&#19981;&#19968;&#33268;&#65292;&#24182;&#20419;&#36827;&#33258;&#21160;&#21270;&#12289;&#26377;&#25928;&#21644;&#19968;&#33268;&#30340;&#22810;&#26041;&#35770;&#35777;&#65292;&#20197;&#20445;&#35777;&#27861;&#24459;&#22312;&#19981;&#21516;&#21496;&#27861;&#31649;&#36758;&#21306;&#20869;&#30340;&#19968;&#33268;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23545;&#21253;&#25324;&#32654;&#22269;&#12289;&#33521;&#22269;&#12289;&#21346;&#26106;&#36798;&#12289;&#29790;&#20856;&#21644;&#39321;&#28207;&#22312;&#20869;&#30340;&#20116;&#20010;&#22269;&#23478;&#30340;&#27861;&#38498;&#21028;&#20915;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;&#29305;&#21035;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65289;&#21644;&#27861;&#24459;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#20132;&#21449;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35782;&#21035;&#20154;&#31867;&#20559;&#35265;&#21644;&#20419;&#36827;&#27861;&#38498;&#21028;&#20915;&#30340;&#22810;&#26041;&#35770;&#35777;&#30340;&#33258;&#21160;&#21270;&#12289;&#26377;&#25928;&#21644;&#19968;&#33268;&#30340;&#35282;&#33394;&#65292;&#20174;&#32780;&#30830;&#20445;&#27861;&#24459;&#22312;&#21508;&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#20869;&#21644;&#36328;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#19968;&#33268;&#24212;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#21644;&#26032;&#24341;&#20837;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#26694;&#26550;&#65292;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;&#20197;ALMs&#20026;&#22522;&#30784;&#30340;&#22522;&#20110;Grounded Theory&#30340;&#27861;&#23398;&#23454;&#36341;&#30740;&#31350;&#35774;&#35745;&#12290;SHIRLEY&#26159;&#22522;&#20110;OpenAI&#30340;GPT&#25216;&#26415;&#26500;&#24314;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20027;&#35201;&#29992;&#20110;&#26816;&#27979;&#21508;&#31181;&#27861;&#24459;&#20915;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01744</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22270;&#35299;&#37322;&#25581;&#31034;&#20998;&#23376;&#25104;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unveiling Molecular Moieties through Hierarchical Graph Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#25903;&#25345;&#20307;&#22806;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#24050;&#32463;&#20986;&#29616;&#22810;&#24180;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#26550;&#26500;&#23454;&#29616;&#39640;&#31934;&#24230;&#22810;&#38774;&#26631;&#31579;&#36873;&#30340;GNN&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#22312;&#21407;&#23376;&#12289;&#29615;&#21644;&#25972;&#20010;&#20998;&#23376;&#23618;&#38754;&#19978;&#30452;&#25509;&#25429;&#33719;&#20449;&#24687;&#65292;&#20174;&#32780;&#25214;&#21040;&#19982;&#29983;&#29289;&#27963;&#24615;&#39044;&#27979;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#25903;&#25345;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#30340;&#20108;&#21313;&#20010;&#32454;&#32990;&#21608;&#26399;&#20381;&#36182;&#24615;&#28608;&#37238;&#38774;&#26631;&#19978;&#25253;&#36947;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;GNN&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#36229;&#36234;&#20102;&#20316;&#32773;&#25552;&#20986;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20165;&#38024;&#23545;CDK1&#30340;&#39640;&#28789;&#25935;&#24230;&#29256;&#26412;&#30340;GNN&#65292;&#20197;&#20351;&#29992;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#26469;&#36991;&#20813;&#22810;&#31867;&#21035;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#24046;&#12290;&#20998;&#23618;&#35299;&#37322;&#22120;&#24050;&#32463;&#30001;&#19968;&#20301;&#19987;&#23478;&#21270;&#23398;&#23478;&#22312;19&#20010;CDK1&#25209;&#20934;&#33647;&#29289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#22686;&#24378;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#35774;&#35745;&#21450;&#30740;&#31350;&#22312;BERT&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.15861</link><description>&lt;p&gt;
BPDec: &#25581;&#31034;BERT&#39044;&#35757;&#32451;&#20013;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#22686;&#24378;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#35774;&#35745;&#21450;&#30740;&#31350;&#22312;BERT&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BERT&#65288;&#26469;&#33258;Transformer&#30340;&#21452;&#21521;&#32534;&#30721;&#34920;&#31034;&#65289;&#36890;&#36807;&#20854;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#38598;&#20013;&#22312;&#19982;&#27169;&#22411;&#32467;&#26500;&#30456;&#20851;&#30340;&#22686;&#24378;&#65292;&#20363;&#22914;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21644;&#26356;&#26377;&#25928;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#36824;&#26377;&#19968;&#20123;&#20154;&#28145;&#20837;&#30740;&#31350;&#20102;&#19982;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#25216;&#24039;&#65292;&#21253;&#25324;&#25972;&#35789;&#25513;&#30721;&#12290;DeBERTa&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;BERT&#32534;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22686;&#24378;&#35299;&#30721;&#22120;&#65292;&#35777;&#26126;&#25928;&#26524;&#38750;&#24120;&#26174;&#33879;&#12290;&#25105;&#20204;&#35748;&#20026;&#22260;&#32469;&#22686;&#24378;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#21644;&#30740;&#31350;&#24182;&#26410;&#24471;&#21040;&#24212;&#26377;&#30340;&#37325;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#65292;&#24182;&#20171;&#32461;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20250;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15861v2 Announce Type: replace-cross  Abstract: BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of natural language processing through its exceptional performance on numerous tasks. Yet, the majority of researchers have mainly concentrated on enhancements related to the model structure, such as relative position embedding and more efficient attention mechanisms. Others have delved into pretraining tricks associated with Masked Language Modeling, including whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's encoder model for pretraining, proving to be highly effective. We argue that the design and research around enhanced masked language modeling decoders have been underappreciated. In this paper, we propose several designs of enhanced decoders and introduce BPDec (BERT Pretraining Decoder), a novel method for modeling training. Typically, a pretrained BERT model is fine-tuned for specific Natural Language 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SERNet-Former&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#26469;&#25913;&#21892;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.15741</link><description>&lt;p&gt;
SERNet-Former: &#24102;&#26377;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15741
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SERNet-Former&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#26469;&#25913;&#21892;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#65292;&#25913;&#21892;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#29575;&#38656;&#35201;&#35299;&#20915;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#26368;&#36817;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#25104;&#21151;&#21644;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#29420;&#29305;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#65288;AbGs&#65289;&#21644;&#27880;&#24847;&#21147;&#22686;&#24378;&#27169;&#22359;&#65288;AbMs&#65289;&#65292;&#30446;&#26631;&#26159;&#22312;&#32534;&#30721;&#22120;&#20013;&#23558;&#22522;&#20110;&#29305;&#24449;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#30456;&#32467;&#21512;&#12290;&#21516;&#26102;&#65292;&#22312;&#35299;&#30721;&#22120;&#37096;&#20998;&#37319;&#29992;&#20102;&#21463;&#21040;AbM&#21551;&#21457;&#30340;&#39069;&#22806;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#65288;AfNs&#65289;&#12290;AfNs&#26088;&#22312;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#37096;&#20998;&#37096;&#32626;&#39069;&#22806;&#30340;&#21367;&#31215;&#23618;&#65292;&#25913;&#21892;&#35821;&#20041;&#20449;&#24687;&#30340;&#36880;&#19968;&#36716;&#25442;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#32593;&#32476;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CamVid&#21644;Cityscapes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the efficiency of state-of-the-art methods in semantic segmentation requires overcoming the increasing computational cost as well as issues such as fusing semantic information from global and local contexts. Based on the recent success and problems that convolutional neural networks (CNNs) encounter in semantic segmentation, this research proposes an encoder-decoder architecture with a unique efficient residual network. Attention-boosting gates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming to fuse the feature-based semantic information with the global context of the efficient residual network in the encoder. Respectively, the decoder network is developed with the additional attention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improve the efficiency in the one-to-one conversion of the semantic information by deploying additional convolution layers in the decoder part. Our network is tested on the challenging CamVid and Cityscapes dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#20013;&#22914;&#20309;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#30340;&#36873;&#25321;&#26631;&#20934;&#21644;&#33719;&#21462;&#27744;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#20219;&#21153;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>https://arxiv.org/abs/2401.15721</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33719;&#21462;&#20989;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Acquisition Functions for Medical Imaging Deep Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#20013;&#22914;&#20309;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#30340;&#36873;&#25321;&#26631;&#20934;&#21644;&#33719;&#21462;&#27744;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#20219;&#21153;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#38761;&#21629;&#24050;&#32463;&#22312;&#36817;&#24180;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#23601;&#12290; &#20174;&#20083;&#33146;&#30284;&#26816;&#27979;&#21040;&#34507;&#30333;&#36136;&#25240;&#21472;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#19968;&#30452;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#36827;&#27493;&#30340;&#26680;&#24515;&#12290; &#20294;&#26159;&#65292;&#36825;&#20123;&#29616;&#20195;&#36827;&#27493;&#36234;&#26469;&#36234;&#38656;&#35201;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#26631;&#35760;&#25968;&#25454;&#65292;&#20854;&#21487;&#29992;&#24615;&#31232;&#32570;&#65306;&#22312;&#21307;&#23398;&#32972;&#26223;&#19979;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#38750;&#24120;&#26377;&#25928;&#65292;&#20854;&#20013;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#65288;&#25110;&#27880;&#37322;&#39044;&#31639;&#38750;&#24120;&#26377;&#38480;&#65289;&#12290; &#25105;&#20204;&#22312;ISIC 2016&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#20960;&#31181;&#36873;&#25321;&#26631;&#20934;&#65288;BALD&#65292;MeanSTD&#21644;MaxEntropy&#65289;&#12290; &#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#33719;&#21462;&#30340;&#27744;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#40657;&#33394;&#32032;&#30244;&#26816;&#27979;&#20219;&#21153;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#35777;&#23454;&#20102;&#20316;&#32773;&#30340;&#29468;&#27979;&#65292;&#21363;\textit {bald} &#24179;&#22343;&#27604;&#20854;&#20182;&#26041;&#24335;&#26356;&#22909;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15721v2 Announce Type: replace-cross  Abstract: The Deep Learning revolution has enabled groundbreaking achievements in recent years. From breast cancer detection to protein folding, deep learning algorithms have been at the core of very important advancements. However, these modern advancements are becoming more and more data-hungry, especially on labeled data whose availability is scarce: this is even more prevalent in the medical context. In this work, we show how active learning could be very effective in data scarcity situations, where obtaining labeled data (or annotation budget is very limited). We compare several selection criteria (BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored the effect of acquired pool size on the model's performance. Our results suggest that uncertainty is useful to the Melanoma detection task, and confirms the hypotheses of the author of the paper of interest, that \textit{bald} performs on average better than other a
&lt;/p&gt;</description></item><item><title>TRIDENT&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#37319;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#37325;&#24314;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20013;&#24494;&#23376;&#20107;&#20214;&#30340;&#37325;&#24314;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.15324</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;TRIDENT&#20013;&#24494;&#23376;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Neutrino Reconstruction in TRIDENT Based on Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15324
&lt;/p&gt;
&lt;p&gt;
TRIDENT&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#37319;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#37325;&#24314;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20013;&#24494;&#23376;&#20107;&#20214;&#30340;&#37325;&#24314;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15324v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;:&#28909;&#24102;&#28145;&#28023;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;(TRIDENT)&#26159;&#19968;&#31181;&#19979;&#19968;&#20195;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#65292;&#23558;&#20301;&#20110;&#21335;&#20013;&#22269;&#28023;&#12290;&#36890;&#36807;&#20855;&#26377;&#22823;&#25506;&#27979;&#22120;&#20307;&#31215;&#21644;&#20351;&#29992;&#20808;&#36827;&#30340;&#28151;&#21512;&#25968;&#23383;&#20809;&#27169;&#22359;(hDOMs)&#65292;TRIDENT&#26088;&#22312;&#21457;&#29616;&#22810;&#20010;&#22825;&#20307;&#20013;&#24494;&#23376;&#28304;&#24182;&#25506;&#27979;&#20840;&#21619;&#20013;&#24494;&#23376;&#29289;&#29702;&#12290; &#20027;&#35201;&#20013;&#24494;&#23376;&#30340;&#37325;&#24314;&#20998;&#36776;&#29575;&#26159;&#23454;&#29616;&#36825;&#20123;&#31185;&#23398;&#30446;&#26631;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;TRIDENT&#37325;&#24314;&#26041;&#27861;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;GNN&#26041;&#27861;&#22312;TRIDENT&#20013;&#36712;&#36857;&#22411;&#21644;&#28107;&#29699;&#29366;&#20013;&#24494;&#23376;&#20107;&#20214;&#19978;&#30340;&#37325;&#24314;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15324v1 Announce Type: cross  Abstract: TRopIcal DEep-sea Neutrino Telescope (TRIDENT) is a next-generation neutrino telescope to be located in the South China Sea. With a large detector volume and the use of advanced hybrid digital optical modules (hDOMs), TRIDENT aims to discover multiple astrophysical neutrino sources and probe all-flavor neutrino physics. The reconstruction resolution of primary neutrinos is on the critical path to these scientific goals. We have developed a novel reconstruction method based on graph neural network (GNN) for TRIDENT. In this paper, we present the reconstruction performance of the GNN-based approach on both track- and shower-like neutrino events in TRIDENT.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#24320;&#25918;&#30693;&#35782;&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;OK-Robot&#65292;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12289;&#23548;&#33322;&#21407;&#35821;&#21644;&#25235;&#21462;&#21407;&#35821;&#65292;&#20026;Pick-and-Drop&#25805;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2401.12202</link><description>&lt;p&gt;
OK-Robot: &#25972;&#21512;&#24320;&#25918;&#30693;&#35782;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12202
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#24320;&#25918;&#30693;&#35782;&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;OK-Robot&#65292;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12289;&#23548;&#33322;&#21407;&#35821;&#21644;&#25235;&#21462;&#21407;&#35821;&#65292;&#20026;Pick-and-Drop&#25805;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#25105;&#20204;&#29616;&#22312;&#25317;&#26377;&#33021;&#22815;&#26681;&#25454;&#35821;&#35328;&#26597;&#35810;&#35782;&#21035;&#29289;&#20307;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#33021;&#26377;&#25928;&#25511;&#21046;&#31227;&#21160;&#31995;&#32479;&#30340;&#23548;&#33322;&#31995;&#32479;&#65292;&#20197;&#21450;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#29289;&#20307;&#30340;&#25235;&#21462;&#27169;&#22411;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#27493;&#65292;&#20294;&#26426;&#22120;&#20154;&#30340;&#36890;&#29992;&#24212;&#29992;&#20173;&#28982;&#33853;&#21518;&#65292;&#23613;&#31649;&#23427;&#20204;&#20381;&#36182;&#20110;&#35782;&#21035;&#12289;&#23548;&#33322;&#21644;&#25235;&#21462;&#31561;&#22522;&#26412;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31995;&#32479;&#20248;&#20808;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OK-Robot&#30340;&#26032;&#22411;&#22522;&#20110;&#24320;&#25918;&#30693;&#35782;&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#29992;&#20110;&#23545;&#35937;&#26816;&#27979;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#12289;&#29992;&#20110;&#31227;&#21160;&#30340;&#23548;&#33322;&#21407;&#35821;&#21644;&#29992;&#20110;&#29289;&#20307;&#25805;&#20316;&#30340;&#25235;&#21462;&#21407;&#35821;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;OK-Robot&#20026;Pick-and-Drop&#25805;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;10&#20010;&#30495;&#23454;&#23478;&#24237;&#29615;&#22659;&#20013;&#36816;&#34892;&#20102;OK-Robot&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.12202v2 Announce Type: replace-cross  Abstract: Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environme
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;Bayesian&#31070;&#32463;&#32593;&#32476;&#27010;&#29575;&#40065;&#26834;&#24615;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;&#30456;&#27604;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#31639;&#27861;&#26356;&#21152;&#39640;&#25928;&#19988;&#33021;&#22815;&#25628;&#32034;&#21442;&#25968;&#31354;&#38388;&#20197;&#25214;&#21040;&#23433;&#20840;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2401.11627</link><description>&lt;p&gt;
Bayesian&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#29575;&#40065;&#26834;&#24615;&#30340;&#20005;&#26684;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Tight Verification of Probabilistic Robustness in Bayesian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11627
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;Bayesian&#31070;&#32463;&#32593;&#32476;&#27010;&#29575;&#40065;&#26834;&#24615;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;&#30456;&#27604;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#31639;&#27861;&#26356;&#21152;&#39640;&#25928;&#19988;&#33021;&#22815;&#25628;&#32034;&#21442;&#25968;&#31354;&#38388;&#20197;&#25214;&#21040;&#23433;&#20840;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#29992;&#20110;&#35745;&#31639;Bayesian&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#27010;&#29575;&#40065;&#26834;&#24615;&#19978;&#20005;&#26684;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#35745;&#31639;BNNs&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#35201;&#27604;&#39564;&#35777;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#40065;&#26834;&#24615;&#22256;&#38590;&#24471;&#22810;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#25628;&#32034;&#23433;&#20840;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#26631;&#20934;NNs&#39564;&#35777;&#30340;&#32039;&#23494;&#21644;&#23436;&#25972;&#26041;&#27861;&#65292;&#20363;&#22914;&#22522;&#20110;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#30340;&#26041;&#27861;&#65292;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;BNNs&#30340;&#39564;&#35777;&#65292;&#22240;&#20026;&#30001;&#20110;&#32534;&#30721;&#26435;&#37325;&#30340;&#21464;&#37327;&#36830;&#32493;&#30456;&#20056;&#32780;&#20135;&#29983;&#30340;&#22810;&#39033;&#24335;&#39033;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#36845;&#20195;&#25193;&#23637;&#21644;&#32593;&#32476;&#30340;&#26799;&#24230;&#26377;&#25928;&#22320;&#25628;&#32034;&#21442;&#25968;&#31354;&#38388;&#20197;&#23547;&#25214;&#23433;&#20840;&#26435;&#37325;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;BNNs&#30340;&#20219;&#20309;&#39564;&#35777;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11627v2 Announce Type: replace-cross  Abstract: We introduce two algorithms for computing tight guarantees on the probabilistic robustness of Bayesian Neural Networks (BNNs). Computing robustness guarantees for BNNs is a significantly more challenging task than verifying the robustness of standard Neural Networks (NNs) because it requires searching the parameters' space for safe weights. Moreover, tight and complete approaches for the verification of standard NNs, such as those based on Mixed-Integer Linear Programming (MILP), cannot be directly used for the verification of BNNs because of the polynomial terms resulting from the consecutive multiplication of variables encoding the weights. Our algorithms efficiently and effectively search the parameters' space for safe weights by using iterative expansion and the network's gradient and can be used with any verification algorithm of choice for BNNs. In addition to proving that our algorithms compute tighter bounds than the So
&lt;/p&gt;</description></item><item><title>NPU-ASLP-LiAuto&#22242;&#38431;&#22312;2023&#24180;CNVSRC&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#37319;&#29992;&#31471;&#21040;&#31471;&#26550;&#26500;&#21644;&#22810;&#26679;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#22686;&#24378;&#25216;&#26415;&#65292;&#21462;&#24471;&#20102;&#22312;&#21333;&#35828;&#35805;&#32773;&#21644;&#22810;&#35828;&#35805;&#32773;&#20219;&#21153;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#25490;&#21517;&#31532;&#19968;&#12290;</title><link>https://arxiv.org/abs/2401.06788</link><description>&lt;p&gt;
NPU-ASLP-LiAuto&#22242;&#38431;&#22312;CNVSRC 2023&#20013;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in CNVSRC 2023
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06788
&lt;/p&gt;
&lt;p&gt;
NPU-ASLP-LiAuto&#22242;&#38431;&#22312;2023&#24180;CNVSRC&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#37319;&#29992;&#31471;&#21040;&#31471;&#26550;&#26500;&#21644;&#22810;&#26679;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#22686;&#24378;&#25216;&#26415;&#65292;&#21462;&#24471;&#20102;&#22312;&#21333;&#35828;&#35805;&#32773;&#21644;&#22810;&#35828;&#35805;&#32773;&#20219;&#21153;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;NPU-ASLP-LiAuto&#65288;Team 237&#65289;&#22312;2023&#24180;&#31532;&#19968;&#23626;&#20013;&#22269;&#36830;&#32493;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#25361;&#25112;&#36187;&#65288;CNVSRC&#65289;&#20013;&#25512;&#20986;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#31995;&#32479;&#65292;&#21442;&#19982;&#20102;&#21333;&#35828;&#35805;&#32773;VSR&#20219;&#21153;&#30340;&#22266;&#23450;&#21644;&#24320;&#25918;&#36712;&#36857;&#65292;&#20197;&#21450;&#22810;&#35828;&#35805;&#32773;VSR&#20219;&#21153;&#30340;&#24320;&#25918;&#36712;&#36857;&#12290;&#22312;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#32447;1&#20013;&#30340;&#21767;&#37096;&#36816;&#21160;&#25552;&#21462;&#22120;&#29983;&#25104;&#22810;&#23610;&#24230;&#35270;&#39057;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#20102;&#21508;&#31181;&#22686;&#24378;&#25216;&#26415;&#65292;&#21253;&#25324;&#36895;&#24230;&#25200;&#21160;&#12289;&#38543;&#26426;&#26059;&#36716;&#12289;&#27700;&#24179;&#32763;&#36716;&#21644;&#39068;&#33394;&#36716;&#25442;&#12290;VSR&#27169;&#22411;&#37319;&#29992;&#20102;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#32852;&#21512;CTC/&#27880;&#24847;&#21147;&#25439;&#22833;&#65292;&#21253;&#25324;ResNet3D&#35270;&#35273;&#21069;&#31471;&#12289;E-Branchformer&#32534;&#30721;&#22120;&#21644;Transformer&#35299;&#30721;&#22120;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#22810;&#31995;&#32479;&#34701;&#21512;&#21518;&#23454;&#29616;&#20102;&#21333;&#35828;&#35805;&#32773;&#20219;&#21153;&#30340;34.76% CER&#21644;&#22810;&#35828;&#35805;&#32773;&#20219;&#21153;&#30340;41.06% CER&#65292;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06788v2 Announce Type: replace-cross  Abstract: This paper delineates the visual speech recognition (VSR) system introduced by the NPU-ASLP-LiAuto (Team 237) in the first Chinese Continuous Visual Speech Recognition Challenge (CNVSRC) 2023, engaging in the fixed and open tracks of Single-Speaker VSR Task, and the open track of Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multi-scale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, comprising a ResNet3D visual frontend, an E-Branchformer encoder, and a Transformer decoder. Experiments show that our system achieves 34.76% CER for the Single-Speaker Task and 41.06% CER for the Multi-Speaker Task after multi-system fusion, ranking first place in all 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#24403;&#20195;LLM ChatGPT-4&#26469;&#20248;&#21270;&#24320;&#28304;Python&#24211;&#65292;&#21457;&#29616;&#22312;&#19982;&#20154;&#31867;&#19987;&#23478;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#22312;&#20248;&#21270;&#33021;&#28304;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.14949</link><description>&lt;p&gt;
LLM&#20114;&#21160;&#20248;&#21270;&#24320;&#28304;Python&#24211;--&#26696;&#20363;&#30740;&#31350;&#19982;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
LLM Interactive Optimization of Open Source Python Libraries -- Case Studies and Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#24403;&#20195;LLM ChatGPT-4&#26469;&#20248;&#21270;&#24320;&#28304;Python&#24211;&#65292;&#21457;&#29616;&#22312;&#19982;&#20154;&#31867;&#19987;&#23478;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#22312;&#20248;&#21270;&#33021;&#28304;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#30340;&#20986;&#29616;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#36825;&#20123;&#27169;&#22411;&#22312;&#28304;&#20195;&#30721;&#20248;&#21270;&#20013;&#30340;&#21033;&#29992;&#31243;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#27861;&#35770;&#20005;&#35880;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24212;&#29992;&#20110;&#33879;&#21517;&#30340;&#24320;&#28304;Python&#24211;pillow&#21644;numpy&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#19982;&#20154;&#31867;&#19987;&#23478;&#20114;&#21160;&#26102;&#65292;&#24403;&#20195;LLM ChatGPT-4&#65288;&#25130;&#33267;2023&#24180;9&#26376;&#21644;10&#26376;&#65289;&#22312;&#20248;&#21270;&#33021;&#28304;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#36866;&#29992;&#20110;&#20114;&#21160;&#20351;&#29992;&#65292;&#19988;&#38656;&#35201;&#20154;&#31867;&#19987;&#23478;&#21327;&#21161;&#12290;&#20026;&#20102;&#36991;&#20813;&#23454;&#39564;&#32773;&#20559;&#35265;&#65292;&#25105;&#20204;&#35814;&#32454;&#35760;&#24405;&#20102;&#25105;&#20204;&#30340;&#23450;&#24615;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35805;&#21644;&#28304;&#20195;&#30721;&#12290;&#25105;&#20204;&#39318;&#20808;&#35814;&#32454;&#25551;&#36848;&#20102;&#19982;LLM&#23545;&#35805;&#20197;&#20248;&#21270;pillow&#24211;&#20013;&#30340;_getextrema&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#37327;&#21270;&#35780;&#20272;&#20102;&#24615;&#33021;&#25913;&#36827;&#12290;&#20026;&#20102;&#23637;&#31034;&#23450;&#24615;&#21487;&#22797;&#21046;&#24615;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;pillow&#30340;&#21478;&#19968;&#20010;&#20301;&#32622;&#19978;&#36827;&#19968;&#27493;&#23581;&#35797;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14949v2 Announce Type: replace-cross  Abstract: With the advent of large language models (LLMs) like GPT-3, a natural question is the extent to which these models can be utilized for source code optimization. This paper presents methodologically stringent case studies applied to well-known open source python libraries pillow and numpy. We find that contemporary LLM ChatGPT-4 (state September and October 2023) is surprisingly adept at optimizing energy and compute efficiency. However, this is only the case in interactive use, with a human expert in the loop. Aware of experimenter bias, we document our qualitative approach in detail, and provide transcript and source code. We start by providing a detailed description of our approach in conversing with the LLM to optimize the _getextrema function in the pillow library, and a quantitative evaluation of the performance improvement. To demonstrate qualitative replicability, we report further attempts on another locus in the pillow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;Panda&#26426;&#22120;&#20154;&#33218;&#19978;&#21019;&#24314;&#23450;&#21046;&#29615;&#22659;&#65292;&#25193;&#23637;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;RL&#31639;&#27861;&#22312;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2312.09468</link><description>&lt;p&gt;
&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#33218;&#20013;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning in a Simulated Robotic Arm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;Panda&#26426;&#22120;&#20154;&#33218;&#19978;&#21019;&#24314;&#23450;&#21046;&#29615;&#22659;&#65292;&#25193;&#23637;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;RL&#31639;&#27861;&#22312;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#25506;&#32034;&#29615;&#22659;&#20197;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#35768;&#22810;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#65292;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#25311;&#22120;&#30340;&#24191;&#27867;&#20351;&#29992;&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#21183;&#65292;&#20854;&#20013;&#21253;&#25324;&#23433;&#20840;&#25506;&#32034;&#65292;&#24403;RL&#31995;&#32479;&#38656;&#35201;&#30452;&#25509;&#22312;&#29289;&#29702;&#29615;&#22659;&#65288;&#20363;&#22914;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#65289;&#20013;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#23433;&#20840;&#25506;&#32034;&#23558;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#27969;&#34892;&#30340;Safety Gym&#24211;&#25552;&#20379;&#20102;&#19977;&#31181;&#31227;&#21160;&#20195;&#29702;&#31867;&#22411;&#65292;&#21487;&#20197;&#23398;&#20064;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#21516;&#26102;&#32771;&#34385;&#21508;&#31181;&#23433;&#20840;&#32422;&#26463;&#12290;&#26412;&#25991;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#24102;&#26377;Panda&#26426;&#22120;&#20154;&#33218;&#30340;&#23450;&#21046;&#29615;&#22659;&#65292;&#25193;&#23637;&#20102;&#23433;&#20840;RL&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#20415;&#27979;&#35797;Safety Gym&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#27969;&#34892;&#30340;PPO&#31639;&#27861;&#36827;&#34892;&#20102;&#35797;&#28857;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22522;&#32447;&#19982;&#21463;&#38480;&#29256;&#26412;&#65292;&#24182;&#34920;&#26126;&#21463;&#38480;&#29256;&#26412;&#33021;&#22815;&#23398;&#20064;&#20986;&#21516;&#26679;&#20248;&#31168;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#31526;&#21512;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09468v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies. In many environments and tasks, safety is of critical importance. The widespread use of simulators offers a number of advantages, including safe exploration which will be inevitable in cases when RL systems need to be trained directly in the physical environment (e.g. in human-robot interaction). The popular Safety Gym library offers three mobile agent types that can learn goal-directed tasks while considering various safety constraints. In this paper, we extend the applicability of safe RL algorithms by creating a customized environment with Panda robotic arm where Safety Gym algorithms can be tested. We performed pilot experiments with the popular PPO algorithm comparing the baseline with the constrained version and show that the constrained version is able to learn the equally good policy while better complying with safe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RMS&#30340;&#28857;&#20113;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20887;&#20313;&#20248;&#21270;&#20102;3D&#28857;&#20113;&#30340;&#32763;&#35793;&#31354;&#38388;&#21487;&#35266;&#27979;&#24615;&#65292;&#35299;&#20915;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#20013;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.07337</link><description>&lt;p&gt;
RMS&#65306;&#23454;&#26102;&#23039;&#24577;&#20272;&#35745;&#30340;&#26368;&#23567;&#20887;&#20313;&#28857;&#20113;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
RMS: Redundancy-Minimizing Point Cloud Sampling for Real-Time Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07337
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RMS&#30340;&#28857;&#20113;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20887;&#20313;&#20248;&#21270;&#20102;3D&#28857;&#20113;&#30340;&#32763;&#35793;&#31354;&#38388;&#21487;&#35266;&#27979;&#24615;&#65292;&#35299;&#20915;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#20013;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#20013;&#20351;&#29992;&#30340;&#20856;&#22411;&#28857;&#20113;&#37319;&#26679;&#26041;&#27861;&#20445;&#30041;&#20102;&#39640;&#27700;&#24179;&#30340;&#28857;&#20887;&#20313;&#12290;&#36825;&#31181;&#20887;&#20313;&#19981;&#24517;&#35201;&#22320;&#20943;&#24930;&#20102;&#20272;&#35745;&#27969;&#31243;&#24182;&#21487;&#33021;&#22312;&#23454;&#26102;&#32422;&#26463;&#19979;&#23548;&#33268;&#28418;&#31227;&#12290;&#36825;&#31181;&#19981;&#24517;&#35201;&#30340;&#24310;&#36831;&#25104;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#26426;&#22120;&#20154;&#65288;&#23588;&#20854;&#26159;&#26080;&#20154;&#26426;&#65289;&#30340;&#29942;&#39048;&#65292;&#38656;&#35201;&#26368;&#23567;&#30340;&#24310;&#36831;&#20197;&#36827;&#34892;&#25935;&#25463;&#21644;&#20934;&#30830;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RMS&#30340;&#26032;&#39062;&#12289;&#30830;&#23450;&#24615;&#12289;&#26410;&#30693;&#21644;&#21333;&#21442;&#25968;&#28857;&#20113;&#37319;&#26679;&#26041;&#27861;&#65292;&#23427;&#26368;&#23567;&#21270;&#20102;3D&#28857;&#20113;&#20013;&#30340;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07337v2 Announce Type: replace-cross  Abstract: The typical point cloud sampling methods used in state estimation for mobile robots preserve a high level of point redundancy. This redundancy unnecessarily slows down the estimation pipeline and may cause drift under real-time constraints. Such undue latency becomes a bottleneck for resource-constrained robots (especially UAVs), requiring minimal delay for agile and accurate operation. We propose a novel, deterministic, uninformed, and single-parameter point cloud sampling method named RMS that minimizes redundancy within a 3D point cloud. In contrast to the state of the art, RMS balances the translation-space observability by leveraging the fact that linear and planar surfaces inherently exhibit high redundancy propagated into iterative estimation pipelines. We define the concept of gradient flow, quantifying the local surface underlying a point. We also show that maximizing the entropy of the gradient flow minimizes point re
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22312;&#12298;&#24040;&#26412;&#30722;&#12299;&#36825;&#20010;&#20013;&#22269;&#20390;&#25506;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#24212;&#29992;LLMs&#65292;&#24341;&#20837;&#39318;&#20010;&#19987;&#20026;&#35813;&#28216;&#25103;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#20449;&#24687;&#25910;&#38598;&#12289;&#20982;&#25163;&#35782;&#21035;&#21644;&#36923;&#36753;&#25512;&#29702;&#31561;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;AI&#26234;&#33021;&#20307;&#30340;&#28216;&#25103;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.00746</link><description>&lt;p&gt;
&#35299;&#35835;&#25968;&#23383;&#20390;&#25506;&#65306;&#29702;&#35299;LLM&#22312;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#28216;&#25103;&#20013;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00746
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22312;&#12298;&#24040;&#26412;&#30722;&#12299;&#36825;&#20010;&#20013;&#22269;&#20390;&#25506;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#24212;&#29992;LLMs&#65292;&#24341;&#20837;&#39318;&#20010;&#19987;&#20026;&#35813;&#28216;&#25103;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#20449;&#24687;&#25910;&#38598;&#12289;&#20982;&#25163;&#35782;&#21035;&#21644;&#36923;&#36753;&#25512;&#29702;&#31561;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;AI&#26234;&#33021;&#20307;&#30340;&#28216;&#25103;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#12298;&#24040;&#26412;&#30722;&#12299;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#19968;&#27454;&#20013;&#22269;&#20390;&#25506;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#28216;&#25103;&#20013;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#39318;&#20010;&#19987;&#20026;&#12298;&#24040;&#26412;&#30722;&#12299;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#35282;&#33394;&#21488;&#35789;&#21644;&#28216;&#25103;&#35268;&#21017;&#65292;&#20197;&#20419;&#36827;&#22312;&#36825;&#20010;&#22797;&#26434;&#21465;&#20107;&#29615;&#22659;&#20013;&#30340;AI&#26234;&#33021;&#20307;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#21033;&#29992;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#26694;&#26550;&#65292;&#20351;&#24471;AI&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#20027;&#21442;&#19982;&#28216;&#25103;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;AI&#26234;&#33021;&#20307;&#22312;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#34913;&#37327;&#23427;&#20204;&#23545;&#26696;&#20214;&#20449;&#24687;&#21644;&#25512;&#29702;&#25216;&#33021;&#25484;&#25569;&#31243;&#24230;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34701;&#20837;&#20102;&#26368;&#26032;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#20307;&#22312;&#20449;&#24687;&#25910;&#38598;&#12289;&#20982;&#25163;&#35782;&#21035;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#26032;&#39062;&#30340;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00746v2 Announce Type: replace  Abstract: In this study, we explore the application of Large Language Models (LLMs) in \textit{Jubensha}, a Chinese detective role-playing game and a novel area in Artificial Intelligence (AI) driven gaming. We introduce the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in this game. To evaluate the gaming performance of these AI agents, we developed novel methods measuring their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in in-context learning to improve the agents' performance in information gathering, murderer identification, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a novel p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#21305;&#37197;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;Generalized Various Backbone and Statistical Matching (G-VBSM)&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#20016;&#23500;&#20449;&#24687;&#21644;&#26356;&#22909;&#27010;&#25324;&#33021;&#21147;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2311.17950</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21516;&#30340;&#20027;&#24178;&#21644;&#32479;&#35745;&#21305;&#37197;&#36827;&#34892;&#24191;&#20041;&#22823;&#35268;&#27169;&#25968;&#25454;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#21305;&#37197;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;Generalized Various Backbone and Statistical Matching (G-VBSM)&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#20016;&#23500;&#20449;&#24687;&#21644;&#26356;&#22909;&#27010;&#25324;&#33021;&#21147;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SRe2L&#24341;&#20837;&#30340;&#36731;&#37327;&#32423;&#8220;&#23616;&#37096;&#21305;&#37197;-&#20840;&#23616;&#21305;&#37197;&#8221;&#25104;&#21151;&#22320;&#21019;&#36896;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#23436;&#25972;&#30340;224x224 ImageNet-1k&#30340;&#20840;&#38754;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21333;&#36793;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#20027;&#24178;&#12289;&#23618;&#21644;&#32479;&#35745;&#25968;&#25454;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#31934;&#31616;&#25968;&#25454;&#38598;&#27010;&#25324;&#33021;&#21147;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#24314;&#35758;&#20805;&#20998;&#32780;&#21508;&#24322;&#30340;&#8220;&#23616;&#37096;&#21305;&#37197;-&#20840;&#23616;&#21305;&#37197;&#8221;&#27604;&#21333;&#19968;&#21305;&#37197;&#26356;&#21152;&#31934;&#30830;&#21644;&#26377;&#25928;&#65292;&#24182;&#33021;&#22815;&#21019;&#36896;&#20986;&#26356;&#20016;&#23500;&#20449;&#24687;&#21644;&#26356;&#22909;&#27010;&#25324;&#33021;&#21147;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#24191;&#20041;&#21305;&#37197;&#8221;&#35266;&#28857;&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#24191;&#20041;&#19981;&#21516;&#20027;&#24178;&#21644;&#32479;&#35745;&#21305;&#37197; (G-VBSM)&#65292;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#23494;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#22312;&#19981;&#21516;&#30340;&#20027;&#24178;&#12289;&#23618;&#21644;&#32479;&#35745;&#25968;&#25454;&#19978;&#19982;&#23436;&#25972;&#25968;&#25454;&#38598;&#20445;&#25345;&#19968;&#33268;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;G-VBSM&#26159;&#31532;&#19968;&#20010;&#33719;&#24471;&#24378;&#22823;&#24615;&#33021;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17950v2 Announce Type: replace-cross  Abstract: The lightweight "local-match-global" matching introduced by SRe2L successfully creates a distilled dataset with comprehensive information on the full 224x224 ImageNet-1k. However, this one-sided approach is limited to a particular backbone, layer, and statistics, which limits the improvement of the generalization of a distilled dataset. We suggest that sufficient and various "local-match-global" matching are more precise and effective than a single one and has the ability to create a distilled dataset with richer information and better generalization. We call this perspective "generalized matching" and propose Generalized Various Backbone and Statistical Matching (G-VBSM) in this work, which aims to create a synthetic dataset with densities, ensuring consistency with the complete dataset across various backbones, layers, and statistics. As experimentally demonstrated, G-VBSM is the first algorithm to obtain strong performance a
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#20197;&#21450;&#22522;&#20110;&#22806;&#35266;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#27880;&#35270;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#26080;&#38656;&#29305;&#27530;&#30828;&#20214;&#65292;&#30524;&#30555;&#24179;&#22343;&#35823;&#24046;&#20302;&#20110;&#20004;&#24230;</title><link>https://arxiv.org/abs/2311.14175</link><description>&lt;p&gt;
&#22522;&#20110;&#22806;&#35266;&#30340;&#27880;&#35270;&#20272;&#35745;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#21512;&#25104;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Appearance-based gaze estimation enhanced with synthetic images using deep neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14175
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#20197;&#21450;&#22522;&#20110;&#22806;&#35266;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#27880;&#35270;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#26080;&#38656;&#29305;&#27530;&#30828;&#20214;&#65292;&#30524;&#30555;&#24179;&#22343;&#35823;&#24046;&#20302;&#20110;&#20004;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#30524;&#27880;&#35270;&#20272;&#35745;&#23545;&#20110;&#25104;&#21151;&#30340;&#20154;&#26426;&#20132;&#20114;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35748;&#30693;&#22240;&#32032;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#35835;&#21462;&#21644;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#12290;&#25105;&#20204;&#37319;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#31995;&#32479;&#65292;&#36890;&#36807;&#20998;&#24320;&#35009;&#21098;&#30340;&#30524;&#30555;&#20272;&#35745;&#27880;&#35270;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#29992;&#20110;&#20154;&#33080;&#26816;&#27979;&#65288;RetinaFace&#65289;&#21644;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#65288;6DRepNet&#65289;&#30340;&#33391;&#22909;&#24037;&#20316;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#30828;&#20214;&#25110;&#32418;&#22806;&#28388;&#20809;&#29255;&#65292;&#32780;&#26159;&#20351;&#29992;&#26631;&#20934;&#31508;&#35760;&#26412;&#20869;&#32622;&#30340;RGB&#25668;&#20687;&#22836;&#65292;&#36890;&#24120;&#37319;&#29992;&#22522;&#20110;&#22806;&#35266;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;MetaHuman&#24037;&#20855;&#65292;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;57,000&#24352;&#20154;&#33080;&#30340;&#22823;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#20844;&#24320;&#25552;&#20379;&#20102;&#36825;&#19968;&#25968;&#25454;&#38598;&#12290;&#23558;&#36825;&#20010;&#25968;&#25454;&#38598;&#65288;&#24102;&#26377;&#30524;&#30555;&#27880;&#35270;&#21644;&#22836;&#37096;&#23039;&#24577;&#20449;&#24687;&#65289;&#19982;&#26631;&#20934;&#30340;&#21733;&#20262;&#27604;&#20122;&#27880;&#35270;&#25968;&#25454;&#38598;&#32467;&#21512;&#36215;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#30524;&#30555;&#30340;&#24179;&#22343;&#35823;&#24046;&#20302;&#20110;&#20004;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14175v2 Announce Type: replace-cross  Abstract: Human eye gaze estimation is an important cognitive ingredient for successful human-robot interaction, enabling the robot to read and predict human behavior. We approach this problem using artificial neural networks and build a modular system estimating gaze from separately cropped eyes, taking advantage of existing well-functioning components for face detection (RetinaFace) and head pose estimation (6DRepNet). Our proposed method does not require any special hardware or infrared filters but uses a standard notebook-builtin RGB camera, as often approached with appearance-based methods. Using the MetaHuman tool, we also generated a large synthetic dataset of more than 57,000 human faces and made it publicly available. The inclusion of this dataset (with eye gaze and head pose information) on top of the standard Columbia Gaze dataset into training the model led to better accuracy with a mean average error below two degrees in eye
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08540</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20551;&#35774;&#65306;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#20013;&#30340;In-Context Learning&#65288;ICL&#65289;&#30340;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#29616;&#35937;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#35299;&#37322;ICL&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#23558;&#20854;&#19982;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#38382;&#65292;&#36825;&#31181;&#32852;&#31995;&#22312;&#23454;&#38469;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26159;&#21542;&#25104;&#31435;&#65311;&#25105;&#20204;&#24378;&#35843;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#20351;&#24471;&#23427;&#20204;&#30340;&#35821;&#22659;&#19982;&#35821;&#35328;&#27169;&#22411;&#23454;&#38469;&#35757;&#32451;&#26102;&#30340;&#23454;&#38469;&#35821;&#22659;&#24046;&#21035;&#24456;&#22823;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29702;&#35770;&#25163;&#24037;&#26500;&#36896;&#30340;&#26435;&#37325;&#20855;&#26377;&#19982;&#30495;&#23454;LLM&#19981;&#21305;&#37197;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;ICL&#30446;&#26631;&#65288;&#26126;&#30830;&#20026;ICL&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#36825;&#19982;&#37326;&#22806;&#20986;&#29616;&#30340;ICL&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#36824;&#23547;&#25214;&#20102;&#30495;&#23454;&#27169;&#22411;&#20013;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#21644;GD&#23545;&#20110;&#35266;&#23519;&#28436;&#31034;&#30340;&#39034;&#24207;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#25506;&#35752;&#24182;&#27604;&#36739;ICL&#19982;GD&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08540v4 Announce Type: replace-cross  Abstract: The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models?   We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild.   We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#36710;&#30340;&#22312;&#32447;&#12289;&#26080;&#20914;&#31361;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#22522;&#20110;&#24490;&#29615;&#22270;&#30340;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#35201;&#20040;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#35201;&#20040;&#22312;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#20869;&#33719;&#24471;&#21516;&#26679;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2310.02195</link><description>&lt;p&gt;
&#39640;&#25928;&#22312;&#32447;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#65306;&#22522;&#20110;&#24490;&#29615;&#22270;&#30340;&#33258;&#21160;&#24341;&#23548;&#36710;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Scheduling and Routing for Automated Guided Vehicles In Loop-Based Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02195
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#36710;&#30340;&#22312;&#32447;&#12289;&#26080;&#20914;&#31361;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#22522;&#20110;&#24490;&#29615;&#22270;&#30340;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#35201;&#20040;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#35201;&#20040;&#22312;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#20869;&#33719;&#24471;&#21516;&#26679;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24341;&#23548;&#36710;&#65288;AGVs&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#65292;&#20197;&#26080;&#20914;&#31361;&#26041;&#24335;&#23545;&#23427;&#20204;&#36827;&#34892;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#23545;&#20110;&#23427;&#20204;&#30340;&#39640;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#22270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#20219;&#24847;&#23481;&#37327;&#21644;&#39034;&#24207;&#20316;&#19994;&#30340;AGVs&#30340;&#22312;&#32447;&#12289;&#26080;&#20914;&#31361;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#19982;&#31934;&#30830;&#26041;&#27861;&#12289;&#36138;&#23146;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20195;&#34920;&#23454;&#38469;&#21046;&#36896;&#21378;&#30340;&#27169;&#22411;&#19978;&#20351;&#29992;&#29702;&#35770;&#21644;&#30495;&#23454;&#23454;&#20363;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#35201;&#20040;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#35201;&#20040;&#22312;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#20869;&#33719;&#24471;&#21516;&#26679;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02195v2 Announce Type: replace-cross  Abstract: Automated guided vehicles (AGVs) are widely used in various industries, and scheduling and routing them in a conflict-free manner is crucial to their efficient operation. We propose a loop-based algorithm that solves the online, conflict-free scheduling and routing problem for AGVs with any capacity and ordered jobs in loop-based graphs. The proposed algorithm is compared against an exact method, a greedy heuristic and a metaheuristic. We experimentally show, using theoretical and real instances on a model representing a real manufacturing plant, that this algorithm either outperforms the other algorithms or gets an equally good solution in less computing time.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2309.13339</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340; remarkable generalizability&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#32463;&#24120;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#24314;&#31435;&#36830;&#36143;&#30340;&#24605;&#32500;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26410;&#21463;&#36923;&#36753;&#21407;&#21017;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#31995;&#32479;&#22320;&#39564;&#35777;&#21644;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#23545;&#27599;&#20010;&#20687;&#32032;&#25110;&#22270;&#20687;&#21306;&#22495;&#30340;&#25913;&#21464;&#37327;&#36827;&#34892;&#23450;&#21046;&#21270;&#65292;&#20026;&#25193;&#25955;&#27169;&#22411;&#22686;&#21152;&#20102;&#31890;&#24230;&#25511;&#21046;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#22270;&#20687;&#32534;&#36753;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2306.00950</link><description>&lt;p&gt;
&#24046;&#20998;&#25193;&#25955;&#65306;&#36171;&#20104;&#27599;&#20010;&#20687;&#32032;&#20197;&#20854;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Differential Diffusion: Giving Each Pixel Its Strength
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.00950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#23545;&#27599;&#20010;&#20687;&#32032;&#25110;&#22270;&#20687;&#21306;&#22495;&#30340;&#25913;&#21464;&#37327;&#36827;&#34892;&#23450;&#21046;&#21270;&#65292;&#20026;&#25193;&#25955;&#27169;&#22411;&#22686;&#21152;&#20102;&#31890;&#24230;&#25511;&#21046;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#22270;&#20687;&#32534;&#36753;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#65292;&#21462;&#24471;&#20102;&#22312;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#27599;&#20010;&#20687;&#32032;&#25110;&#22270;&#20687;&#21306;&#22495;&#30340;&#25913;&#21464;&#37327;&#21487;&#20197;&#36827;&#34892;&#23450;&#21046;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#20026;&#20854;&#22686;&#21152;&#36825;&#31181;&#21151;&#33021;&#12290;&#23545;&#21464;&#21270;&#37327;&#30340;&#31890;&#24230;&#25511;&#21046;&#25171;&#24320;&#20102;&#21508;&#31181;&#26032;&#30340;&#32534;&#36753;&#33021;&#21147;&#65292;&#22914;&#25511;&#21046;&#21333;&#20010;&#23545;&#35937;&#34987;&#20462;&#25913;&#30340;&#31243;&#24230;&#65292;&#25110;&#32773;&#24341;&#20837;&#36880;&#28176;&#30340;&#31354;&#38388;&#21464;&#21270;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#36719;&#20462;&#22797;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#22312;&#23436;&#25104;&#22270;&#20687;&#37096;&#20998;&#30340;&#21516;&#26102;&#65292;&#24494;&#35843;&#21608;&#22260;&#21306;&#22495;&#20197;&#30830;&#20445;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.00950v2 Announce Type: replace-cross  Abstract: Diffusion models have revolutionized image generation and editing, producing state-of-the-art results in conditioned and unconditioned image synthesis. While current techniques enable user control over the degree of change in an image edit, the controllability is limited to global changes over an entire edited region. This paper introduces a novel framework that enables customization of the amount of change per pixel or per image region. Our framework can be integrated into any existing diffusion model, enhancing it with this capability. Such granular control on the quantity of change opens up a diverse array of new editing capabilities, such as control of the extent to which individual objects are modified, or the ability to introduce gradual spatial changes. Furthermore, we showcase the framework's effectiveness in soft-inpainting -- the completion of portions of an image while subtly adjusting the surrounding areas to ensure
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20256;&#36882;&#24615;&#24230;&#37327;&#26631;&#20934; F-OTCE &#21644; JC-OTCE&#65292;&#29992;&#20110;&#35780;&#20272;&#28304;&#27169;&#22411;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#21463;&#30410;&#31243;&#24230;&#65292;&#24182;&#20026;&#36328;&#39046;&#22495;&#36328;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#23398;&#20064;&#26356;&#20855;&#20256;&#36882;&#24615;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2207.05510</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#36328;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#20256;&#36882;&#24615;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Transferability-Guided Cross-Domain Cross-Task Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.05510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20256;&#36882;&#24615;&#24230;&#37327;&#26631;&#20934; F-OTCE &#21644; JC-OTCE&#65292;&#29992;&#20110;&#35780;&#20272;&#28304;&#27169;&#22411;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#21463;&#30410;&#31243;&#24230;&#65292;&#24182;&#20026;&#36328;&#39046;&#22495;&#36328;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#23398;&#20064;&#26356;&#20855;&#20256;&#36882;&#24615;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20256;&#36882;&#24615;&#24230;&#37327;&#26631;&#20934; F-OTCE&#65288;&#22522;&#20110;&#24555;&#36895;&#26368;&#20248;&#20256;&#36755;&#30340;&#26465;&#20214;&#29109;&#65289;&#21644; JC-OTCE&#65288;&#32852;&#21512;&#23545;&#24212; OTCE&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#28304;&#27169;&#22411;&#65288;&#20219;&#21153;&#65289;&#23545;&#30446;&#26631;&#20219;&#21153;&#23398;&#20064;&#30340;&#21463;&#30410;&#31243;&#24230;&#65292;&#24182;&#20026;&#36328;&#39046;&#22495;&#36328;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#23398;&#20064;&#26356;&#20855;&#20256;&#36882;&#24615;&#30340;&#34920;&#31034;&#12290;&#19982;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#19981;&#21516;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#36741;&#21161;&#20219;&#21153;&#19978;&#35780;&#20272;&#32463;&#39564;&#20256;&#36882;&#24615;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#26159;&#26080;&#38656;&#36741;&#21161;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35745;&#31639;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;F-OTCE&#39318;&#20808;&#36890;&#36807;&#22312;&#28304;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#38382;&#39064;&#26469;&#20272;&#35745;&#20256;&#36882;&#24615;&#65292;&#28982;&#21518;&#20351;&#29992;&#26368;&#20248;&#32806;&#21512;&#26469;&#35745;&#31639;&#28304;&#21644;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#36127;&#26465;&#20214;&#29109;&#12290;&#23427;&#36824;&#21487;&#20197;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#20043;&#21069;&#26368;&#22823;&#21270;&#28304;&#27169;&#22411;&#30340;&#20256;&#36882;&#24615;&#12290;&#21516;&#26102;&#65292;JC-OTCE&#25913;&#21892;&#20102;&#20256;&#36882;&#24615;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.05510v2 Announce Type: replace-cross  Abstract: We propose two novel transferability metrics F-OTCE (Fast Optimal Transport based Conditional Entropy) and JC-OTCE (Joint Correspondence OTCE) to evaluate how much the source model (task) can benefit the learning of the target task and to learn more transferable representations for cross-domain cross-task transfer learning. Unlike the existing metric that requires evaluating the empirical transferability on auxiliary tasks, our metrics are auxiliary-free such that they can be computed much more efficiently. Specifically, F-OTCE estimates transferability by first solving an Optimal Transport (OT) problem between source and target distributions, and then uses the optimal coupling to compute the Negative Conditional Entropy between source and target labels. It can also serve as a loss function to maximize the transferability of the source model before finetuning on the target task. Meanwhile, JC-OTCE improves the transferability r
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#20027;&#12289;&#33258;&#25105;&#28608;&#21169;&#21644;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#23398;&#20064;&#65292;&#20197;&#24212;&#23545;&#26410;&#30693;&#25110;&#26032;&#39062;&#24615;&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#36880;&#27493;&#23398;&#20064;&#21644;&#25552;&#21319;&#30693;&#35782;&#19982;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2110.11385</link><description>&lt;p&gt;
&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#33258;&#20027;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Initiated Open World Learning for Autonomous AI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.11385
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#20027;&#12289;&#33258;&#25105;&#28608;&#21169;&#21644;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#23398;&#20064;&#65292;&#20197;&#24212;&#23545;&#26410;&#30693;&#25110;&#26032;&#39062;&#24615;&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#36880;&#27493;&#23398;&#20064;&#21644;&#25552;&#21319;&#30693;&#35782;&#19982;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#23454;&#38469;&#24212;&#29992;&#65292;&#26159;&#26102;&#20505;&#32771;&#34385;&#22914;&#20309;&#20351;&#36825;&#20123;&#20195;&#29702;&#23436;&#20840;&#33258;&#20027;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#33258;&#20027;&#23398;&#20064;&#65292;&#32780;&#19981;&#26159;&#23450;&#26399;&#30001;&#20154;&#31867;&#24037;&#31243;&#24072;&#21551;&#21160;&#24182;&#20351;&#29992;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#26159;&#19968;&#20010;&#20855;&#26377;&#26410;&#30693;&#25110;&#26032;&#39062;&#24615;&#30340;&#24320;&#25918;&#29615;&#22659;&#65292;&#26816;&#27979;&#26032;&#39062;&#24615;&#25110;&#26410;&#30693;&#24615;&#65292;&#25551;&#36848;&#23427;&#20204;&#65292;&#36866;&#24212;&#25110;&#36866;&#24212;&#23427;&#20204;&#65292;&#25910;&#38598;&#22320;&#38754;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36880;&#27493;&#23398;&#20064;&#26410;&#30693;&#21644;&#26032;&#39062;&#24615;&#23545;&#20110;&#20351;&#20195;&#29702;&#36234;&#26469;&#36234;&#26377;&#30693;&#35782;&#21644;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#20351;&#20854;&#30001;&#20195;&#29702;&#20027;&#21160;&#36827;&#34892;&#24182;&#36890;&#36807;&#20854;&#19982;&#20154;&#31867;&#21644;&#29615;&#22659;&#30340;&#20114;&#21160;&#36827;&#34892;&#12290;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#24120;&#26377;&#19968;&#20010;&#24615;&#33021;&#20219;&#21153;&#65292;&#22240;&#27492;&#23545;&#27599;&#31181;&#26032;&#39062;&#24615;&#36827;&#34892;&#29305;&#24449;&#21270;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#21644;&#24517;&#35201;&#65292;&#20197;&#20415;&#20195;&#29702;&#21487;&#20197;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.11385v3 Announce Type: replace  Abstract: As more and more AI agents are used in practice, it is time to think about how to make these agents fully autonomous so that they can learn by themselves in a self-motivated and self-supervised manner rather than being retrained periodically on the initiation of human engineers using expanded training data. As the real-world is an open environment with unknowns or novelties, detecting novelties or unknowns, characterizing them, accommodating or adapting to them, gathering ground-truth training data, and incrementally learning the unknowns/novelties are critical to making the agent more and more knowledgeable and powerful over time. The key challenge is how to automate the process so that it is carried out on the agent's own initiative and through its own interactions with humans and the environment. Since an AI agent usually has a performance task, characterizing each novelty becomes critical and necessary so that the agent can formu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#36716;&#31227;&#24615;&#24230;&#37327;JC-NCE&#20998;&#25968;&#65292;&#36890;&#36807;&#26174;&#33879;&#25913;&#21892;OTCE&#20013;&#20219;&#21153;&#24046;&#24322;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#65292;&#28040;&#38500;&#20102;&#23545;&#36741;&#21161;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2106.10479</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#29992;&#21487;&#36716;&#31227;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Practical Transferability Estimation for Image Classification Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.10479
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#36716;&#31227;&#24615;&#24230;&#37327;JC-NCE&#20998;&#25968;&#65292;&#36890;&#36807;&#26174;&#33879;&#25913;&#21892;OTCE&#20013;&#20219;&#21153;&#24046;&#24322;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#65292;&#28040;&#38500;&#20102;&#23545;&#36741;&#21161;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36716;&#31227;&#24615;&#20272;&#35745;&#26159;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#29992;&#20110;&#39044;&#27979;&#23558;&#28304;&#27169;&#22411;&#65288;&#25110;&#28304;&#20219;&#21153;&#65289;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#26102;&#24615;&#33021;&#26377;&#22810;&#22909;&#12290;&#26368;&#36817;&#65292;&#20998;&#26512;&#24615;&#30340;&#36716;&#31227;&#24615;&#24230;&#37327;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#28304;&#27169;&#22411;&#36873;&#25321;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#36328;&#39046;&#22495;&#36328;&#20219;&#21153;&#30340;&#35774;&#32622;&#19979;&#20351;&#36716;&#31227;&#24615;&#20272;&#35745;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;OTCE&#20998;&#25968;&#36890;&#36807;&#32771;&#34385;&#22495;&#21644;&#20219;&#21153;&#24046;&#24322;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20511;&#21161;&#36741;&#21161;&#20219;&#21153;&#30340;&#36716;&#31227;&#32463;&#39564;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#25928;&#29575;&#24320;&#38144;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;JC-NCE&#20998;&#25968;&#30340;&#23454;&#29992;&#36716;&#31227;&#24615;&#24230;&#37327;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;OTCE&#20013;&#20219;&#21153;&#24046;&#24322;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#36741;&#21161;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#35299;&#20915;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#24314;&#31435;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#30340;&#32852;&#21512;&#23545;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.10479v3 Announce Type: replace-cross  Abstract: Transferability estimation is an essential problem in transfer learning to predict how good the performance is when transferring a source model (or source task) to a target task. Recent analytical transferability metrics have been widely used for source model selection and multi-task learning. A major challenge is how to make transfereability estimation robust under the cross-domain cross-task settings. The recently proposed OTCE score solves this problem by considering both domain and task differences, with the help of transfer experiences on auxiliary tasks, which causes an efficiency overhead. In this work, we propose a practical transferability metric called JC-NCE score that dramatically improves the robustness of the task difference estimation in OTCE, thus removing the need for auxiliary tasks. Specifically, we build the joint correspondences between source and target data via solving an optimal transport problem with a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#35745;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20108;&#32500;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#22768;&#23398;&#33021;&#37327;&#21644;&#20998;&#26512;&#24133;&#24230;&#35843;&#21046;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#12290;&#23454;&#38469;&#27979;&#35797;&#35777;&#26126;&#20102;&#35813;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.14292</link><description>&lt;p&gt;
AST-2:&#21333;&#23618;&#21644;&#21452;&#23618;&#20108;&#32500;&#22768;&#23398;&#36719;&#35302;&#35273;&#30382;&#32932;
&lt;/p&gt;
&lt;p&gt;
AST-2: Single and bi-layered 2-D acoustic soft tactile skin. (arXiv:2401.14292v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#35745;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20108;&#32500;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#22768;&#23398;&#33021;&#37327;&#21644;&#20998;&#26512;&#24133;&#24230;&#35843;&#21046;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#12290;&#23454;&#38469;&#27979;&#35797;&#35777;&#26126;&#20102;&#35813;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#22768;&#23398;&#36719;&#35302;&#35273;(AST)&#30382;&#32932;&#35774;&#35745;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#26174;&#33879;&#25552;&#39640;&#20108;&#32500;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#25361;&#25112;&#22312;&#20110;&#20351;&#29992;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#31934;&#30830;&#30340;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#25509;&#35302;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#22312;&#24863;&#27979;&#34920;&#38754;&#19979;&#30340;&#20004;&#23618;&#19987;&#29992;&#22768;&#23398;&#36890;&#36947;&#20013;&#21033;&#29992;&#22768;&#23398;&#33021;&#37327;&#65292;&#24182;&#20998;&#26512;&#24133;&#24230;&#35843;&#21046;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#30721;&#24863;&#27979;&#34920;&#38754;&#19978;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#30828;&#20214;&#32452;&#20214;&#26126;&#30830;&#20998;&#31163;&#65292;&#36127;&#36131;&#21457;&#23556;&#21644;&#25509;&#25910;&#22768;&#23398;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22359;&#21270;&#21644;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#30382;&#32932;&#35774;&#35745;&#12290;&#23454;&#38469;&#27979;&#35797;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#39062;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;&#22312;&#20272;&#35745;&#25509;&#35302;&#27861;&#21521;&#21147;(MAE &lt;0.8 N)&#21644;&#20108;&#32500;&#25509;&#35302;&#23450;&#20301;&#26041;&#38754;&#30340;&#26174;&#33879;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
This paper aims to present an innovative and cost-effective design for Acoustic Soft Tactile (AST) Skin, with the primary goal of significantly enhancing the accuracy of 2-D tactile feature estimation. The existing challenge lies in achieving precise tactile feature estimation, especially concerning contact geometry characteristics, using cost-effective solutions. We hypothesise that by harnessing acoustic energy through dedicated acoustic channels in 2 layers beneath the sensing surface and analysing amplitude modulation, we can effectively decode interactions on the sensory surface, thereby improving tactile feature estimation. Our approach involves the distinct separation of hardware components responsible for emitting and receiving acoustic signals, resulting in a modular and highly customizable skin design. Practical tests demonstrate the effectiveness of this novel design, achieving remarkable precision in estimating contact normal forces (MAE &lt; 0.8 N), 2D contact localisation (M
&lt;/p&gt;</description></item><item><title>WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13919</link><description>&lt;p&gt;
WebVoyager&#65306;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#30340;Web Agent
&lt;/p&gt;
&lt;p&gt;
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13919
&lt;/p&gt;
&lt;p&gt;
WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#24341;&#39046;&#20102;&#19968;&#20010;&#30001;&#30495;&#23454;&#19990;&#30028;&#20013;&#33258;&#20027;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#25152;&#26631;&#24535;&#30340;&#26032;&#26102;&#20195;&#65292;&#25512;&#21160;&#20102;&#22522;&#20110;&#32593;&#32476;&#30340;&#39640;&#32423;&#20195;&#29702;&#30340;&#21019;&#26032;&#12290;&#29616;&#26377;&#30340;&#32593;&#32476;&#20195;&#29702;&#36890;&#24120;&#21482;&#22788;&#29702;&#19968;&#20010;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#19988;&#20165;&#22312;&#31616;&#21270;&#30340;&#32593;&#32476;&#27169;&#25311;&#22120;&#25110;&#38745;&#24577;&#30340;&#32593;&#32476;&#24555;&#29031;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WebVoyager&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;Web&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#24335;Web&#20195;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#25361;&#25112;&#65292;&#21033;&#29992;&#20102;GPT-4V&#30340;&#24378;&#22823;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;15&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32593;&#31449;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;WebVoyager&#23454;&#29616;&#20102;55.7&#65285;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#26174;&#33879;&#22320;.....
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
&lt;/p&gt;</description></item><item><title>BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02982</link><description>&lt;p&gt;
BIBench: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#20998;&#26512;&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02982
&lt;/p&gt;
&lt;p&gt;
BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#29087;&#32451;&#24230;&#21644;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#24605;&#32500;&#20026;&#37325;&#28857;&#30340;&#39046;&#22495;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BIBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#30340;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#12290;BIBench&#36890;&#36807;&#19977;&#20010;&#32500;&#24230;&#35780;&#20272;LLMs&#65306;1&#65289;BI&#22522;&#30784;&#30693;&#35782;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#21644;&#23545;&#37329;&#34701;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65307;2&#65289;BI&#30693;&#35782;&#24212;&#29992;&#65292;&#30830;&#23450;&#27169;&#22411;&#24555;&#36895;&#29702;&#35299;&#25991;&#26412;&#20449;&#24687;&#24182;&#20174;&#22810;&#20010;&#35270;&#35282;&#29983;&#25104;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65307;3&#65289;BI&#25216;&#26415;&#25216;&#33021;&#65292;&#26816;&#26597;&#27169;&#22411;&#20351;&#29992;&#25216;&#26415;&#30693;&#35782;&#35299;&#20915;&#29616;&#23454;&#25968;&#25454;&#20998;&#26512;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;BIBench&#21253;&#25324;11&#20010;&#23376;&#20219;&#21153;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;SMORe&#65292;&#23427;&#23558;&#21344;&#26377;&#21305;&#37197;&#30340;&#35270;&#35282;&#19982;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;GCRL&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.02013</link><description>&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#35780;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Score Models for Offline Goal-Conditioned Reinforcement Learning. (arXiv:2311.02013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;SMORe&#65292;&#23427;&#23558;&#21344;&#26377;&#21305;&#37197;&#30340;&#35270;&#35282;&#19982;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;GCRL&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#30340;&#20219;&#21153;&#26159;&#20351;&#29992;&#31232;&#30095;&#22870;&#21169;&#20989;&#25968;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#22312;&#29615;&#22659;&#20013;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#12290;&#31163;&#32447;GCRL&#23545;&#20110;&#24320;&#21457;&#33021;&#22815;&#21033;&#29992;&#39044;&#20808;&#23384;&#22312;&#30340;&#25968;&#25454;&#38598;&#23398;&#20064;&#22810;&#26679;&#21270;&#21644;&#21487;&#22797;&#29992;&#25216;&#33021;&#30340;&#36890;&#29992;&#22411;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#29616;&#20195;GCRL&#26041;&#27861;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#24448;&#24448;&#19981;&#22826;&#29702;&#24819;&#12290;GCRL&#30340;&#21478;&#19968;&#31181;&#35266;&#28857;&#26159;&#20248;&#21270;&#21344;&#26377;&#21305;&#37197;&#65292;&#20294;&#38656;&#35201;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#38543;&#21518;&#35813;&#37492;&#21035;&#22120;&#20316;&#20026;&#19979;&#28216;&#24378;&#21270;&#23398;&#20064;&#30340;&#20266;&#22870;&#21169;&#12290;&#23398;&#20064;&#21040;&#30340;&#37492;&#21035;&#22120;&#30340;&#19981;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#36127;&#38754;&#24433;&#21709;&#65292;&#36827;&#32780;&#24433;&#21709;&#29983;&#25104;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GCRL&#26041;&#27861;&#65292;&#22522;&#20110;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30340;&#26032;&#35270;&#35282;&#65292;&#37319;&#29992;&#26080;&#37492;&#21035;&#22120;&#30340;&#26041;&#27861;&#65306;SMORe&#12290;&#20851;&#38190;&#27934;&#35265;&#26159;&#23558;GCRL&#30340;&#21344;&#26377;&#21305;&#37197;&#35270;&#35282;&#19982;&#19968;&#20010;&#26377;&#25928;&#30340;&#32858;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#26080;&#37492;&#21035;&#22120;&#26041;&#27861;&#65306;SMORe&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a conve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.18127</link><description>&lt;p&gt;
&#25552;&#38382;&#26356;&#22810;&#65292;&#20102;&#35299;&#26356;&#22810;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#38382;&#39064;&#19982;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models. (arXiv:2310.18127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23558;&#22522;&#20110;&#34892;&#21160;&#30340;&#31574;&#30053;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#22797;&#26434;&#23454;&#38469;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#26469;&#35828;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#25552;&#31034;&#26159;&#36890;&#36807;&#24191;&#27867;&#20351;&#29992;&#20154;&#21147;&#25163;&#24037;&#21046;&#20316;&#30340;&#65292;&#23548;&#33268;CoT&#31574;&#30053;&#32463;&#24120;&#26080;&#27861;&#25512;&#24191;&#12290;&#20026;&#20102;&#30830;&#20445;&#20302;&#23618;&#25511;&#21046;&#22120;&#36866;&#24403;&#22320;&#22788;&#29702;CoT&#25512;&#29702;&#65292;&#36824;&#38656;&#35201;&#20154;&#20026;&#20171;&#20837;&#26469;&#24320;&#21457;&#25509;&#22320;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#36808;&#21521;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#24212;&#29992;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#35299;&#20915;&#30340;&#23436;&#20840;&#38598;&#25104;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#21452;&#23618;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#65288;&#25552;&#31034;&#65289;&#65292;&#24182;&#38543;&#21518;&#36827;&#34892;&#25512;&#29702;&#65292;&#25351;&#23548;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#19968;&#20010;&#22909;&#30340;&#25552;&#31034;&#24212;&#35813;&#22522;&#20110;&#21382;&#21490;&#30340;&#33258;&#30465;&#24615;&#20462;&#35746;&#26469;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical fi
&lt;/p&gt;</description></item><item><title>PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;</title><link>http://arxiv.org/abs/2309.17260</link><description>&lt;p&gt;
PlaceNav: &#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17260
&lt;/p&gt;
&lt;p&gt;
PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25299;&#25169;&#23548;&#33322;&#20998;&#20026;&#26426;&#22120;&#20154;&#26080;&#20851;&#21644;&#26426;&#22120;&#20154;&#29305;&#23450;&#30340;&#32452;&#20214;&#21487;&#20197;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#26426;&#22120;&#20154;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23548;&#33322;&#26041;&#27861;&#20173;&#21463;&#21040;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35745;&#31639;&#32553;&#25918;&#24615;&#24046;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PlaceNav&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#26469;&#36873;&#25321;&#25299;&#25169;&#23548;&#33322;&#27969;&#31243;&#20013;&#30340;&#23376;&#30446;&#26631;&#12290;&#36825;&#20351;&#24471;&#23376;&#30446;&#26631;&#36873;&#25321;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22320;&#28857;&#35782;&#21035;&#20351;&#24471;&#36125;&#21494;&#26031;&#28388;&#27874;&#25104;&#20026;&#21487;&#33021;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#23376;&#30446;&#26631;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#19968;&#35774;&#35745;&#65292;&#24182;&#19988;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
&lt;/p&gt;</description></item><item><title>ASAP&#26159;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#19968;&#33324;&#24418;&#29366;&#32452;&#35013;&#30340;&#29289;&#29702;&#21487;&#34892;&#24615;&#24207;&#21015;&#12290;&#23427;&#36890;&#36807;&#32771;&#34385;&#37325;&#21147;&#21644;&#20351;&#29992;&#39640;&#25928;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#29289;&#29702;&#23454;&#38469;&#30340;&#32452;&#35013;&#24207;&#21015;&#35268;&#21010;&#65292;&#36866;&#29992;&#20110;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2309.16909</link><description>&lt;p&gt;
ASAP: &#33258;&#21160;&#21270;&#22797;&#26434;&#26426;&#22120;&#20154;&#32452;&#35013;&#30340;&#29289;&#29702;&#21487;&#34892;&#24615;&#24207;&#21015;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
ASAP: Automated Sequence Planning for Complex Robotic Assembly with Physical Feasibility. (arXiv:2309.16909v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16909
&lt;/p&gt;
&lt;p&gt;
ASAP&#26159;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#19968;&#33324;&#24418;&#29366;&#32452;&#35013;&#30340;&#29289;&#29702;&#21487;&#34892;&#24615;&#24207;&#21015;&#12290;&#23427;&#36890;&#36807;&#32771;&#34385;&#37325;&#21147;&#21644;&#20351;&#29992;&#39640;&#25928;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#29289;&#29702;&#23454;&#38469;&#30340;&#32452;&#35013;&#24207;&#21015;&#35268;&#21010;&#65292;&#36866;&#29992;&#20110;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#20135;&#21697;&#30340;&#33258;&#21160;&#21270;&#32452;&#35013;&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35268;&#21010;&#19968;&#20010;&#29289;&#29702;&#21487;&#34892;&#30340;&#21160;&#20316;&#24207;&#21015;&#26469;&#32452;&#35013;&#22810;&#20010;&#37096;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ASAP&#65292;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#19968;&#33324;&#24418;&#29366;&#32452;&#35013;&#30340;&#24207;&#21015;&#12290;ASAP&#32771;&#34385;&#20102;&#37325;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#24207;&#21015;&#65292;&#20854;&#20013;&#27599;&#20010;&#23376;&#32452;&#20214;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#38646;&#20214;&#34987;&#20445;&#25345;&#21644;&#25903;&#25745;&#34920;&#38754;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#29289;&#29702;&#31283;&#23450;&#12290;&#25105;&#20204;&#24212;&#29992;&#39640;&#25928;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#20943;&#23569;&#30830;&#23450;&#36825;&#26679;&#19968;&#20010;&#32452;&#35013;&#24207;&#21015;&#30340;&#32452;&#21512;&#22797;&#26434;&#24615;&#12290;&#25628;&#32034;&#21487;&#20197;&#30001;&#20960;&#20309;&#21551;&#21457;&#24335;&#25110;&#35757;&#32451;&#26377;&#27169;&#25311;&#26631;&#31614;&#25968;&#25454;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24341;&#23548;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ASAP&#22312;&#25968;&#30334;&#20010;&#22797;&#26434;&#20135;&#21697;&#32452;&#35013;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#29289;&#29702;&#23454;&#38469;&#30340;&#32452;&#35013;&#24207;&#21015;&#35268;&#21010;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;ASAP&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#35774;&#32622;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automated assembly of complex products requires a system that can automatically plan a physically feasible sequence of actions for assembling many parts together. In this paper, we present ASAP, a physics-based planning approach for automatically generating such a sequence for general-shaped assemblies. ASAP accounts for gravity to design a sequence where each sub-assembly is physically stable with a limited number of parts being held and a support surface. We apply efficient tree search algorithms to reduce the combinatorial complexity of determining such an assembly sequence. The search can be guided by either geometric heuristics or graph neural networks trained on data with simulation labels. Finally, we show the superior performance of ASAP at generating physically realistic assembly sequence plans on a large dataset of hundreds of complex product assemblies. We further demonstrate the applicability of ASAP on both simulation and real-world robotic setups. Project website: asa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GreenTrainer&#65292;&#19968;&#31181;&#26032;&#30340;LLM&#32454;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35780;&#20272;&#19981;&#21516;&#24352;&#37327;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#21644;&#23545;&#32454;&#35843;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#20197;&#23454;&#29616;&#32511;&#33394;AI&#12290;</title><link>http://arxiv.org/abs/2309.13192</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#21453;&#21521;&#20256;&#25773;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32511;&#33394;AI&#32454;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation. (arXiv:2309.13192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GreenTrainer&#65292;&#19968;&#31181;&#26032;&#30340;LLM&#32454;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35780;&#20272;&#19981;&#21516;&#24352;&#37327;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#21644;&#23545;&#32454;&#35843;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#20197;&#23454;&#29616;&#32511;&#33394;AI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#21040;&#19979;&#28216;&#24212;&#29992;&#20013;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#38543;&#30528;LLM&#39537;&#21160;&#30340;AI&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#20197;&#21450;&#24320;&#28304;LLM&#30340;&#27665;&#20027;&#21270;&#65292;&#38750;&#19987;&#19994;&#20154;&#21592;&#20063;&#21487;&#20197;&#36827;&#34892;&#32454;&#35843;&#65292;&#20294;&#26159;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;LLM&#30340;&#22823;&#35268;&#27169;&#32454;&#35843;&#21487;&#33021;&#23548;&#33268;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#26174;&#33879;&#22686;&#21152;&#65292;&#20174;&#32780;&#23545;&#29615;&#22659;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#23454;&#29616;&#32511;&#33394;AI&#20197;&#20943;&#23569;&#32454;&#35843;&#30340;FLOPs&#30452;&#25509;&#30456;&#20851;&#65292;&#20294;&#26159;&#29616;&#26377;&#30340;&#39640;&#25928;LLM&#32454;&#35843;&#25216;&#26415;&#21482;&#33021;&#23454;&#29616;&#26377;&#38480;&#30340;FLOPs&#38477;&#20302;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#32454;&#35843;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GreenTrainer&#65292;&#19968;&#31181;&#26032;&#30340;LLM&#32454;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35780;&#20272;&#19981;&#21516;&#24352;&#37327;&#30340;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#21644;&#23545;&#32454;&#35843;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#24352;&#37327;&#26469;&#26368;&#23567;&#21270;&#32454;&#35843;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is the most effective way of adapting pre-trained large language models (LLMs) to downstream applications. With the fast growth of LLM-enabled AI applications and democratization of open-souced LLMs, fine-tuning has become possible for non-expert individuals, but intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact. Mitigating such environmental impact towards Green AI directly correlates to reducing the FLOPs of fine-tuning, but existing techniques on efficient LLM fine-tuning can only achieve limited reduction of such FLOPs, due to their ignorance of the backpropagation cost in fine-tuning. To address this limitation, in this paper we present GreenTrainer, a new LLM fine-tuning technique that adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the most a
&lt;/p&gt;</description></item><item><title>&#28436;&#32451;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#21644;&#25552;&#20379;&#21453;&#39304;&#65292;&#25945;&#25480;&#29992;&#25143;&#20914;&#31361;&#35299;&#20915;&#30340;&#25216;&#33021;&#12290;&#21033;&#29992;&#28436;&#32451;&#65292;&#29992;&#25143;&#21487;&#20197;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#20914;&#31361;&#22330;&#26223;&#65292;&#24182;&#23398;&#20064;&#22914;&#20309;&#36816;&#29992;&#20914;&#31361;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.12309</link><description>&lt;p&gt;
&#28436;&#32451;&#65306;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#26469;&#25945;&#25480;&#20914;&#31361;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rehearsal: Simulating Conflict to Teach Conflict Resolution. (arXiv:2309.12309v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12309
&lt;/p&gt;
&lt;p&gt;
&#28436;&#32451;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#21644;&#25552;&#20379;&#21453;&#39304;&#65292;&#25945;&#25480;&#29992;&#25143;&#20914;&#31361;&#35299;&#20915;&#30340;&#25216;&#33021;&#12290;&#21033;&#29992;&#28436;&#32451;&#65292;&#29992;&#25143;&#21487;&#20197;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#20914;&#31361;&#22330;&#26223;&#65292;&#24182;&#23398;&#20064;&#22914;&#20309;&#36816;&#29992;&#20914;&#31361;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20914;&#31361;&#26159;&#19968;&#31181;&#20196;&#20154;&#19981;&#33298;&#26381;&#20294;&#19981;&#21487;&#36991;&#20813;&#30340;&#29983;&#27963;&#20107;&#23454;&#12290;&#25104;&#21151;&#22320;&#22788;&#29702;&#20914;&#31361;&#26159;&#19968;&#31181;&#25216;&#33021;&#65292;&#21487;&#20197;&#36890;&#36807;&#21051;&#24847;&#32451;&#20064;&#26469;&#23398;&#20064;&#65292;&#20294;&#26159;&#24456;&#23569;&#26377;&#20154;&#33021;&#22815;&#33719;&#24471;&#26377;&#25928;&#30340;&#22521;&#35757;&#25110;&#21453;&#39304;&#12290;&#20026;&#20102;&#25193;&#22823;&#36825;&#31181;&#26426;&#20250;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28436;&#32451;&#65288;Rehearsal&#65289;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#19982;&#21487;&#20449;&#30340;&#27169;&#25311;&#23545;&#35805;&#32773;&#19968;&#36215;&#25490;&#32451;&#20914;&#31361;&#65292;&#25506;&#32034;&#22914;&#26524;&#24773;&#20917;&#22914;&#20309;&#30340;&#8220;&#20551;&#35774;&#8221;&#22330;&#26223;&#20197;&#35782;&#21035;&#26367;&#20195;&#30340;&#23545;&#35805;&#36335;&#24452;&#65292;&#24182;&#36890;&#36807;&#21453;&#39304;&#23398;&#20064;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#24212;&#29992;&#29305;&#23450;&#30340;&#20914;&#31361;&#31574;&#30053;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#28436;&#32451;&#26469;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#24050;&#23450;&#20041;&#30340;&#20914;&#31361;&#22330;&#26223;&#65292;&#20174;&#21150;&#20844;&#23460;&#20105;&#35758;&#21040;&#24773;&#24863;&#38382;&#39064;&#65292;&#25110;&#32773;&#20182;&#20204;&#20063;&#21487;&#20197;&#36873;&#25321;&#21019;&#24314;&#33258;&#24049;&#30340;&#20914;&#31361;&#22330;&#26223;&#12290;&#20026;&#20102;&#23454;&#29616;&#28436;&#32451;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;IRP&#25552;&#31034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20914;&#31361;&#35299;&#20915;&#20013;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#21033;&#30410;-&#26435;&#21147;-&#33021;&#21147;&#65288;IRP&#65289;&#29702;&#35770;&#26469;&#35843;&#33410;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#28436;&#32451;&#20351;&#29992;IRP&#29983;&#25104;&#22522;&#20110;&#20914;&#31361;&#35299;&#20915;&#29702;&#35770;&#30340;&#35805;&#35821;&#65292;&#24341;&#23548;&#29992;&#25143;&#23454;&#36341;&#24212;&#29992;&#20914;&#31361;&#35299;&#20915;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill -- one that can be learned through deliberate practice -- but few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual "what if?" scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users t
&lt;/p&gt;</description></item><item><title>CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11143</link><description>&lt;p&gt;
CoT-BERT: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11143
&lt;/p&gt;
&lt;p&gt;
CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#36755;&#20837;&#21477;&#23376;&#36716;&#21270;&#20026;&#23500;&#21547;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#30340;&#22266;&#23450;&#38271;&#24230;&#21521;&#37327;&#65292;&#21516;&#26102;&#28040;&#38500;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#25512;&#21160;&#19979;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#36712;&#36857;&#20013;&#65292;&#20173;&#28982;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#20026;&#20102;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20013;&#30340;&#28508;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21477;&#23376;&#34920;&#31034;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#29702;&#35299;&#21644;&#25688;&#35201;&#12290;&#38543;&#21518;&#65292;&#21518;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#34987;&#21033;&#29992;&#20026;&#36755;&#20837;&#21477;&#23376;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#21644;&#27169;&#26495;&#21435;&#22122;&#25216;&#26415;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CoT-BERT&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.05605</link><description>&lt;p&gt;
&#20869;&#23384;&#27880;&#20837;&#65306;&#22312;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#20013;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. (arXiv:2309.05605v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#38656;&#35201;&#20174;&#22810;&#20010;&#20449;&#24687;&#28304;&#20013;&#26816;&#32034;&#21644;&#32508;&#21512;&#20449;&#24687;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24448;&#24448;&#38590;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#36827;&#34892;&#23450;&#21521;&#20869;&#23384;&#27880;&#20837;&#26469;&#30830;&#23450;&#21644;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GPT-2&#27169;&#22411;&#22312;&#21333;&#36339;&#21644;&#22810;&#36339;&#25552;&#31034;&#19979;&#21508;&#23618;&#30340;&#28608;&#27963;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21521;&#20851;&#38190;LLM&#20301;&#32622;&#27880;&#20837;&#30456;&#20851;&#30340;&#25552;&#31034;&#29305;&#23450;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#35760;&#24518;&#8221;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;LLM&#33021;&#22815;&#25972;&#21512;&#39069;&#22806;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22810;&#36339;&#25552;&#31034;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#23558;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#23450;&#21521;&#30340;&#35760;&#24518;&#27880;&#20837;&#21040;&#20851;&#38190;&#27880;&#24847;&#21147;&#23618;&#20013;&#24448;&#24448;&#33021;&#22815;&#25552;&#39640;&#22810;&#36339;&#20219;&#21153;&#20013;&#25152;&#38656;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#25552;&#39640;&#20102;&#36798;&#21040;424%&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29289;&#20307;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#29289;&#29702;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#22312;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02561</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physically Grounded Vision-Language Models for Robotic Manipulation. (arXiv:2309.02561v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29289;&#20307;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#29289;&#29702;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#22312;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#30740;&#31350;&#36827;&#23637;&#23548;&#33268;&#22312;&#35270;&#35273;&#38382;&#31572;&#21644;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#29616;&#22312;&#21487;&#20197;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VLMs&#22312;&#23545;&#24120;&#35265;&#29289;&#20307;&#30340;&#29289;&#29702;&#27010;&#24565;&#65288;&#20363;&#22914;&#26448;&#26009;&#12289;&#33030;&#24369;&#24615;&#65289;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#28041;&#21450;&#19982;&#36825;&#20123;&#29289;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#29289;&#29702;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhysObjects&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;36.9K&#20010;&#20247;&#21253;&#21644;417K&#20010;&#33258;&#21160;&#21270;&#30340;&#24120;&#35265;&#23478;&#23621;&#29289;&#21697;&#30340;&#29289;&#29702;&#27010;&#24565;&#27880;&#37322;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;PhysObjects&#19978;&#23545;VLM&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#23545;&#29289;&#29702;&#29289;&#20307;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#30340;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23558;&#36825;&#20010;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;VLM&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhysObjects, an object-centric dataset of 36.9K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically-grounded VLM in an interactive framework with a large languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#21152;&#28909;&#28809;&#25511;&#21046;&#31995;&#32479;&#30340;&#35757;&#32451;&#65292;&#20026;&#22522;&#30784;&#20135;&#19994;&#30340;&#21487;&#25345;&#32493;&#21046;&#36896;&#21644;&#33021;&#32791;&#38477;&#20302;&#30446;&#26631;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.16089</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#26041;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#22312;&#21152;&#28909;&#28809;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Zone Method based Machine Learning and Physics-Informed Neural Networks in Reheating Furnaces. (arXiv:2308.16089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#21152;&#28909;&#28809;&#25511;&#21046;&#31995;&#32479;&#30340;&#35757;&#32451;&#65292;&#20026;&#22522;&#30784;&#20135;&#19994;&#30340;&#21487;&#25345;&#32493;&#21046;&#36896;&#21644;&#33021;&#32791;&#38477;&#20302;&#30446;&#26631;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#30784;&#20135;&#19994;&#30340;&#32463;&#27982;&#37325;&#35201;&#24615;&#24456;&#39640;&#65292;&#20294;&#20854;&#29983;&#20135;&#38142;&#20013;&#30340;&#19968;&#20123;&#32452;&#20214;&#65292;&#22914;&#21152;&#28909;&#28809;&#65292;&#33021;&#32791;&#36739;&#39640;&#12290;&#36890;&#36807;&#20943;&#23569;&#21152;&#28909;&#28809;&#20013;&#30340;&#25972;&#20307;&#21152;&#28909;&#26102;&#38388;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#33021;&#32791;&#12290;&#22312;&#22522;&#30784;&#20135;&#19994;&#21487;&#25345;&#32493;&#21046;&#36896;&#20013;&#65292;&#35745;&#31639;&#26426;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25511;&#21046;&#31995;&#32479;&#21487;&#33021;&#26159;&#23454;&#29616;&#8220;&#38646;&#20928;&#25490;&#25918;&#8221;&#30446;&#26631;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#20013;&#65292;&#30001;&#20110;&#22312;&#21152;&#28909;&#28809;&#31561;&#22330;&#26223;&#20013;&#26080;&#27861;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;ML&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#30340;&#22238;&#24402;&#35757;&#32451;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21306;&#22495;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#24335;&#26469;&#24314;&#27169;&#36752;&#23556;&#20256;&#28909;&#65288;RHT&#65289;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#36825;&#26159;&#21152;&#28909;&#28809;&#20869;&#39640;&#28201;&#36807;&#31243;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#20256;&#28909;&#26426;&#21046;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the high economic relevance of Foundation Industries, certain components like Reheating furnaces within their manufacturing chain are energy-intensive. Notable energy consumption reduction could be obtained by reducing the overall heating time in furnaces. Computer-integrated Machine Learning (ML) and Artificial Intelligence (AI) powered control systems in furnaces could be enablers in achieving the Net-Zero goals in Foundation Industries for sustainable manufacturing.  In this work, due to the infeasibility of achieving good quality data in scenarios like reheating furnaces, classical Hottel's zone method based computational model has been used to generate data for ML and Deep Learning (DL) based model training via regression. It should be noted that the zone method provides an elegant way to model the physical phenomenon of Radiative Heat Transfer (RHT), the dominating heat transfer mechanism in high-temperature processes inside heating furnaces. Using this data, an extensive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11131</link><description>&lt;p&gt;
ReLLa: &#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#34987;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36866;&#24212;&#21644;&#22686;&#24378;&#32431;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#25512;&#33616;&#39046;&#22495;&#20013;LLMs&#26080;&#27861;&#20174;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ReLLa&#65289;&#12290;&#38024;&#23545;&#38646;&#26679;&#26412;&#25512;&#33616;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25253;&#21578;&#20102;&#20851;&#20110;&#27491;&#24335;&#21270;&#26465;&#20214;&#25512;&#29702;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#21253;&#25324;Aqvist&#30340;&#26465;&#20214;&#20041;&#21153;&#31995;&#32479;E&#30340;&#26426;&#26800;&#21270;&#21644;&#20262;&#29702;&#35770;&#25454;&#35780;&#20272;&#30340;&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2308.10686</link><description>&lt;p&gt;
&#24635;&#25324;&#33655;&#23572;&#33945;&#20307;&#31995;&#20316;&#20026;HOL&#30340;&#19968;&#20010;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
Normative Conditional Reasoning as a Fragment of HOL. (arXiv:2308.10686v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25253;&#21578;&#20102;&#20851;&#20110;&#27491;&#24335;&#21270;&#26465;&#20214;&#25512;&#29702;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#21253;&#25324;Aqvist&#30340;&#26465;&#20214;&#20041;&#21153;&#31995;&#32479;E&#30340;&#26426;&#26800;&#21270;&#21644;&#20262;&#29702;&#35770;&#25454;&#35780;&#20272;&#30340;&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#20851;&#20110;&#27491;&#24335;&#21270;&#65288;&#22522;&#20110;&#20559;&#22909;&#30340;&#65289;&#26465;&#20214;&#25512;&#29702;&#30340;&#19968;&#20123;&#32467;&#26524;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;Aqvist&#30340;&#26465;&#20214;&#20041;&#21153;&#31995;&#32479;E&#65288;&#21450;&#20854;&#25193;&#23637;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Isabelle/HOL&#20013;&#30340;&#27973;&#34920;&#35821;&#20041;&#23884;&#20837;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#27491;&#24335;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#35813;&#26694;&#26550;&#30340;&#20004;&#31181;&#21487;&#33021;&#29992;&#36884;&#12290;&#31532;&#19968;&#31181;&#26159;&#20316;&#20026;&#23545;&#25152;&#32771;&#34385;&#36923;&#36753;&#36827;&#34892;&#20803;&#25512;&#29702;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#33258;&#21160;&#39564;&#35777;&#26435;&#21033;&#20041;&#21153;&#23545;&#24212;&#20851;&#31995;&#65288;&#24191;&#20041;&#19978;&#29702;&#35299;&#65289;&#21450;&#30456;&#20851;&#20107;&#39033;&#65292;&#31867;&#20284;&#20110;&#20043;&#21069;&#23545;&#27169;&#24577;&#36923;&#36753;&#31435;&#26041;&#20307;&#25152;&#21462;&#24471;&#30340;&#25104;&#26524;&#12290;&#31532;&#20108;&#31181;&#29992;&#36884;&#26159;&#20316;&#20026;&#20262;&#29702;&#35770;&#25454;&#35780;&#20272;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20154;&#21475;&#20262;&#29702;&#23398;&#20013;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#24726;&#35770;Parfit&#30340;&#20196;&#20154;&#21388;&#24694;&#30340;&#32467;&#35770;&#30340;&#35745;&#31639;&#26426;&#32534;&#30721;&#12290;&#22914;&#20309;&#36890;&#36807;&#36825;&#20010;&#32534;&#30721;&#22686;&#21152;&#25110;&#20943;&#23569;&#20196;&#20154;&#21388;&#24694;&#30340;&#32467;&#35770;&#30340;&#21560;&#24341;&#21147;&#21644;&#35828;&#26381;&#21147;&#26159;&#19968;&#20010;&#25105;&#20204;&#24076;&#26395;&#21521;&#21746;&#23398;&#21644;&#20262;&#29702;&#23398;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report some results regarding the mechanization of normative (preference-based) conditional reasoning. Our focus is on Aqvist's system E for conditional obligation (and its extensions). Our mechanization is achieved via a shallow semantical embedding in Isabelle/HOL. We consider two possible uses of the framework. The first one is as a tool for meta-reasoning about the considered logic. We employ it for the automated verification of deontic correspondences (broadly conceived) and related matters, analogous to what has been previously achieved for the modal logic cube. The second use is as a tool for assessing ethical arguments. We provide a computer encoding of a well-known paradox in population ethics, Parfit's repugnant conclusion. Whether the presented encoding increases or decreases the attractiveness and persuasiveness of the repugnant conclusion is a question we would like to pass on to philosophy and ethics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#32593;&#32476;&#29228;&#34411;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#30830;&#23450;&#32593;&#39029;&#25490;&#21517;&#20197;&#21450;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#32593;&#39029;&#12290;&#24182;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#21644;robot.txt&#25991;&#20214;&#30340;&#22522;&#26412;&#26684;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.04689</link><description>&lt;p&gt;
&#32593;&#32476;&#29228;&#34411;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
web crawler strategies for web pages under robot.txt restriction. (arXiv:2308.04689v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#32593;&#32476;&#29228;&#34411;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#30830;&#23450;&#32593;&#39029;&#25490;&#21517;&#20197;&#21450;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#32593;&#39029;&#12290;&#24182;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#21644;robot.txt&#25991;&#20214;&#30340;&#22522;&#26412;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#65292;&#25152;&#26377;&#20154;&#37117;&#20102;&#35299;&#20114;&#32852;&#32593;&#24182;&#27599;&#22825;&#22312;&#20114;&#32852;&#32593;&#19978;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20026;&#29992;&#25143;&#36755;&#20837;&#30340;&#20851;&#38190;&#23383;&#36827;&#34892;&#25628;&#32034;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#20026;&#19978;&#32593;&#32773;&#25552;&#20379;&#26041;&#20415;&#30340;&#32467;&#26524;&#12290;&#19978;&#32593;&#32773;&#36873;&#25321;&#25490;&#21517;&#38752;&#21069;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#20294;&#26159;&#32593;&#39029;&#30340;&#25490;&#21517;&#26159;&#22914;&#20309;&#30001;&#25628;&#32034;&#24341;&#25806;&#30830;&#23450;&#30340;&#65311;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#25152;&#26377;&#32593;&#39029;&#65311;&#26412;&#25991;&#32473;&#20986;&#20102;&#25152;&#26377;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#20026;&#25628;&#32034;&#24341;&#25806;&#24037;&#20316;&#30340;&#32593;&#32476;&#29228;&#34411;&#21644;&#32593;&#32476;&#29228;&#34411;&#30340;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#12290;&#32593;&#31449;&#31649;&#29702;&#21592;&#20351;&#29992;robot.txt&#25991;&#20214;&#20013;&#30340;&#19981;&#21516;&#38480;&#21046;&#35268;&#21017;&#25351;&#23548;&#32593;&#32476;&#29228;&#34411;&#65292;&#26412;&#25991;&#36824;&#25552;&#21040;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;robot.txt&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present time, all know about World Wide Web and work over the Internet daily. In this paper, we introduce the search engines working for keywords that are entered by users to find something. The search engine uses different search algorithms for convenient results for providing to the net surfer. Net surfers go with the top search results but how did the results of web pages get higher ranks over search engines? how the search engine got that all the web pages in the database? This paper gives the answers to all these kinds of basic questions. Web crawlers working for search engines and robot exclusion protocol rules for web crawlers are also addressed in this research paper. Webmaster uses different restriction facts in robot.txt file to instruct web crawler, some basic formats of robot.txt are also mentioned in this paper.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26080;&#37327;&#32434;&#21464;&#37327;&#21644;&#21046;&#24230;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#25968;&#20540;&#29983;&#25104;&#30340;&#26368;&#20248;&#25511;&#21046;&#27861;&#21017;&#25512;&#24191;&#21040;&#37327;&#32434;&#30456;&#20284;&#30340;&#31995;&#32479;&#65292;&#36825;&#23545;&#20110;&#25512;&#24191;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#31574;&#30053;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.15852</link><description>&lt;p&gt;
&#22522;&#20110;&#24052;&#20811;&#27721;&#22982;&#960;&#23450;&#29702;&#30340;&#26080;&#37327;&#32434;&#31574;&#30053;&#65306;&#26159;&#25512;&#24191;&#25968;&#20540;&#32467;&#26524;&#30340;&#22909;&#26041;&#27861;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Dimensionless Policies based on the Buckingham $\pi$ Theorem: Is it a good way to Generalize Numerical Results?. (arXiv:2307.15852v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15852
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26080;&#37327;&#32434;&#21464;&#37327;&#21644;&#21046;&#24230;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#25968;&#20540;&#29983;&#25104;&#30340;&#26368;&#20248;&#25511;&#21046;&#27861;&#21017;&#25512;&#24191;&#21040;&#37327;&#32434;&#30456;&#20284;&#30340;&#31995;&#32479;&#65292;&#36825;&#23545;&#20110;&#25512;&#24191;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#31574;&#30053;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#19978;&#19979;&#25991;&#29615;&#22659;&#21644;&#23450;&#20041;&#36816;&#21160;&#25511;&#21046;&#38382;&#39064;&#30340;&#21464;&#37327;&#21015;&#34920;&#26159;&#22312;&#37327;&#32434;&#19978;&#30456;&#20284;&#30340;&#35805;&#65292;&#26159;&#21487;&#20197;&#30340;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#26080;&#37327;&#32434;&#21464;&#37327;&#20462;&#25913;&#38382;&#39064;&#30340;&#24418;&#24335;&#65292;&#21487;&#20197;&#23558;&#25968;&#20540;&#29983;&#25104;&#30340;&#26368;&#20248;&#25511;&#21046;&#27861;&#21017;&#37325;&#26032;&#24212;&#29992;&#20110;&#22312;&#37327;&#32434;&#19978;&#30456;&#20284;&#30340;&#23376;&#31354;&#38388;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21463;&#25197;&#30697;&#38480;&#21046;&#30340;&#20498;&#31435;&#25670;&#32463;&#20856;&#36816;&#21160;&#25511;&#21046;&#38382;&#39064;&#36827;&#34892;&#25968;&#20540;&#29983;&#25104;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#30340;&#23637;&#31034;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21046;&#24230;&#30340;&#27010;&#24565;&#65292;&#21363;&#19978;&#19979;&#25991;&#21464;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#21306;&#22495;&#65292;&#21487;&#20197;&#24110;&#21161;&#25918;&#23485;&#37327;&#32434;&#30456;&#20284;&#24615;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23558;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#37327;&#32434;&#32553;&#25918;&#19982;&#22312;&#20998;&#26512;&#26041;&#31243;&#20013;&#29992;&#26032;&#30340;&#31995;&#32479;&#21442;&#25968;&#26367;&#20195;&#37327;&#32434;&#30456;&#20284;&#31995;&#32479;&#30340;&#31561;&#20215;&#24615;&#12290;&#23578;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#27492;&#26041;&#27861;&#26159;&#21542;&#20063;&#36866;&#29992;&#20110;&#25512;&#24191;&#26356;&#22797;&#26434;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yes if the context, the list of variables defining the motion control problem, is dimensionally similar. Here we show that by modifying the problem formulation using dimensionless variables, we can re-use the optimal control law generated numerically for a specific system to a sub-space of dimensionally similar systems. This is demonstrated, with numerically generated optimal controllers, for the classic motion control problem of swinging-up a torque-limited inverted pendulum. We also discuss the concept of regime, a region in the space of context variables, that can help relax the condition on dimensional similarity. Futhermore, we discuss how applying dimensionnal scaling of the input and output of a context-specific policy is equivalent to substituing the new systems parameters in an analytical equation for dimentionnaly similar systems. It remains to be seen if this approach can also help generalizing policies for more complex high-dimensional problems.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.10811</link><description>&lt;p&gt;
"&#24863;&#35273;&#20687;&#26377;&#31532;&#20108;&#20010;&#24605;&#32500;": &#25506;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#21019;&#24847;&#21487;&#20889;&#24615;&#39044;&#20889;&#30340;&#20154;&#26426;&#20849;&#21019;
&lt;/p&gt;
&lt;p&gt;
"It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models. (arXiv:2307.10811v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20889;&#26159;&#22312;&#31532;&#19968;&#31295;&#20043;&#21069;&#21457;&#29616;&#21644;&#21457;&#23637;&#24605;&#24819;&#30340;&#36807;&#31243;&#65292;&#23427;&#38656;&#35201;&#21457;&#25955;&#24615;&#24605;&#32500;&#65292;&#36890;&#24120;&#28041;&#21450;&#21040;&#26080;&#32467;&#26500;&#30340;&#31574;&#30053;&#65292;&#22914;&#22270;&#34920;&#12289;&#27010;&#36848;&#21644;&#33258;&#30001;&#20889;&#20316;&#31561;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26159;&#26377;&#29992;&#30340;&#65292;&#21253;&#25324;&#21019;&#24847;&#20889;&#20316;&#65292;&#20294;&#23545;&#29992;&#25143;&#22914;&#20309;&#19982;LLMs&#21512;&#20316;&#26469;&#25903;&#25345;&#39044;&#20889;&#30340;&#26041;&#24335;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#31181;&#21019;&#36896;&#24615;&#36807;&#31243;&#20013;&#65292;LLMs&#30340;&#39318;&#36873;&#21512;&#20316;&#35282;&#33394;&#21644;&#20027;&#21160;&#24615;&#20063;&#19981;&#26126;&#30830;&#12290;&#20026;&#20102;&#30740;&#31350;&#20154;&#31867;&#19982;LLMs&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#21644;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#19982;15&#20301;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#20004;&#20010;&#21019;&#36896;&#24615;&#20219;&#21153;&#65306;&#20889;&#25925;&#20107;&#21644;&#20889;&#21475;&#21495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#20316;&#30340;&#39044;&#20889;&#36807;&#31243;&#20013;&#65292;&#20284;&#20046;&#23384;&#22312;&#30528;&#19968;&#20010;&#19977;&#38454;&#27573;&#36845;&#20195;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65292;&#21253;&#25324;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#38454;&#27573;&#12290;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20197;&#20154;&#31867;&#22312;&#20027;&#23548;&#35282;&#33394;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;&#20004;&#31181;&#36866;&#29992;&#20110;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#26694;&#26550;&#65292;&#24182;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.02140</link><description>&lt;p&gt;
&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65306;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#30340;&#32508;&#36848;&#21644;&#24895;&#26223;
&lt;/p&gt;
&lt;p&gt;
Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives. (arXiv:2307.02140v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;&#20004;&#31181;&#36866;&#29992;&#20110;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#26694;&#26550;&#65292;&#24182;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36981;&#24490;&#26381;&#21153;&#22120;&#20027;&#23548;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#38480;&#21046;&#20102;FL&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#38477;&#20302;&#20102;&#25968;&#25454;&#25345;&#26377;&#32773;&#21442;&#19982;&#30340;&#28909;&#24773;&#12290;&#20026;&#20102;&#20805;&#20998;&#37322;&#25918;FL&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#20027;&#24352;&#37325;&#26032;&#24605;&#32771;&#24403;&#21069;FL&#26694;&#26550;&#30340;&#35774;&#35745;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#20026;&#26356;&#36890;&#29992;&#30340;&#27010;&#24565;&#65306;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20114;&#21512;&#20316;&#30340;FL&#26694;&#26550;&#65306;&#22522;&#20110;&#26597;&#35810;&#30340;FL&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;FL&#12290;&#22312;&#36825;&#20010;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20174;&#25216;&#26415;&#21644;&#27861;&#24459;&#30340;&#35282;&#24230;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;FL&#30340;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#32806;&#21512;&#12289;&#27169;&#22411;&#21487;&#37325;&#29992;&#24615;&#20302;&#21644;&#38750;&#20844;&#24320;&#24615;&#12290;&#22312;&#22522;&#20110;&#26597;&#35810;&#30340;FL&#24179;&#21488;&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#36171;&#33021;&#30340;&#24320;&#25918;&#27169;&#22411;&#20849;&#20139;&#21644;&#37325;&#29992;&#24179;&#21488;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#26377;&#20215;&#20540;&#30340;&#20027;&#39064;&#65292;&#21253;&#25324;&#20840;&#29699;&#26368;&#26032;&#21487;&#29992;&#27169;&#22411;&#21644;&#27169;&#22411;&#30340;&#26597;&#35810;&#12289;&#26381;&#21153;&#36136;&#37327;&#20445;&#35777;&#21644;&#22870;&#21169;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional Federated Learning (FL) follows a server-domincated cooperation paradigm which narrows the application scenarios of FL and decreases the enthusiasm of data holders to participate. To fully unleash the potential of FL, we advocate rethinking the design of current FL frameworks and extending it to a more generalized concept: Open Federated Learning Platforms. We propose two reciprocal cooperation frameworks for FL to achieve this: query-based FL and contract-based FL. In this survey, we conduct a comprehensive review of the feasibility of constructing an open FL platform from both technical and legal perspectives. We begin by reviewing the definition of FL and summarizing its inherent limitations, including server-client coupling, low model reusability, and non-public. In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#22312;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11461</link><description>&lt;p&gt;
&#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#20174;&#35821;&#20041;&#32423;&#21035;&#21040;&#20195;&#30721;&#32423;&#21035;&#30340; SelfzCoT&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;LLMs
&lt;/p&gt;
&lt;p&gt;
SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs. (arXiv:2305.11461v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#22312;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;LLMs&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558; SelfzCoT &#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#65292;&#20854;&#20934;&#30830;&#24615;&#20174;GSM8K&#30340;40.50%&#25552;&#39640;&#33267;82.34%&#65292;MultiArith&#20174;79.3%&#25552;&#39640;&#33267;94.7%&#65292;ADDSUB&#20174;74.70%&#25552;&#39640;&#33267;94.10%&#65292;SingleEq&#20174;78.70%&#25552;&#39640;&#33267;91.30%&#65292;AQUA&#20174;31.90%&#25552;&#39640;&#33267;82.33%&#65292;SVAMP&#20174;63.70%&#25552;&#39640;&#33267;79.70%&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20351;&#29992;&#21069;&#20004;&#20010;&#25345;&#20037;&#36335;&#24452;&#28608;&#27963;&#21040;LLM&#65292;&#29305;&#21035;&#26159;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#20351; SelfzCoT &#22312;&#25152;&#26377;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#22312;GSM8K&#20013;&#65292;MzCoT&#30340;&#20934;&#30830;&#24615;&#20174;40.50%&#25552;&#39640;&#33267;76.32%&#65292;MultiArith&#20174;79.3%&#25552;&#39640;&#33267;96.97%&#65292;ADDSUB&#20174;74.70%&#25552;&#39640;&#33267;92.39%&#65292;SingleEq&#20174;78.70%&#25552;&#39640;&#33267;94.60%&#65292;AQUA&#20174;31.90%&#25552;&#39640;&#33267;79.90%&#65292;SVAMP&#20174;63.70%&#25552;&#39640;&#33267;81.50%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper show a work on better use of LLMs with SelfzCoT a self-prompt zero-shot CoT. Specifically, on the zero-shot arithmetic reasoning tasks, the accuracy of the proposed SelfzCoT is improved with GSM8K from 40.50% to 82.34%, with MultiArith from 79.3% to 94.7%, with ADDSUB from 74.70% to 94.10%, with SingleEq from 78.70% to 91.30%, with AQUA from 31.90% to 82.33%, and with SVAMP from 63.70% to 79.70%. Totally, using the first two lasting path activations to LLM and particularly, the code-level self-prompt, the SelfzCoT has a huge improvement on all six zero-shot arithmetic reasoning tasks. Additionally, our modified zero-shot CoT (MzCoT) also achieves remarkable performance in the reasoning tasks. The accuracy of the proposed MzCoT is enhanced with GSM8K from 40.50% to 76.32%, with MultiArith from 79.3% to 96.97%, with ADDSUB from 74.70% to 92.39%, with SingleEq from 78.70% to 94.60%, with AQUA from 31.90% to 79.90%, and with SVAMP from 63.70% to 81.50%. Notably, SelfzCoT has the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10361</link><description>&lt;p&gt;
&#38750;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#65306;&#22522;&#20110;&#27169;&#25311;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#28216;&#25103;&#22312;&#32463;&#27982;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#35821;&#35328;&#30340;&#35828;&#26381;&#28216;&#25103;&#20013;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#20154;&#31867; - &#26426;&#22120;&#20154;&#20132;&#20114;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#30495;&#23454;&#20132;&#20114;&#21644;&#27169;&#25311;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#20998;&#31867;&#26041;&#27861;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#26680;&#24515;&#21457;&#29616;&#20026;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.02215</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#20998;&#31867;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages. (arXiv:2305.02215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#20998;&#31867;&#26041;&#27861;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#26680;&#24515;&#21457;&#29616;&#20026;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#25104;&#21151;&#20351;&#20154;&#20204;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#26426;&#22120;&#26159;&#22312;&#22797;&#21046;&#26576;&#20123;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36824;&#26159;&#21457;&#29616;&#20102;&#26681;&#26412;&#24615;&#30340;&#26032;&#29702;&#35770;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35266;&#28857;&#65292;&#20351;&#29992;&#35821;&#35328;&#20043;&#38388;&#30340;&#31867;&#22411;&#30456;&#20284;&#24615;&#26469;&#23545;&#27604;&#19981;&#21516;&#35821;&#35328;&#30340;transformer&#27169;&#22411;&#65292;&#35266;&#23519;&#36825;&#20123;&#30456;&#20284;&#24615;&#26159;&#21542;&#20986;&#29616;&#22312;&#29305;&#23450;&#30340;&#23618;&#27425;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20013;&#24515;&#26680;&#23545;&#40784;&#30340;&#26435;&#37325;&#30697;&#38453;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21477;&#27861;&#31867;&#22411;&#23398;&#30456;&#20284;&#24615;&#19982;&#20013;&#38388;&#23618;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#19968;&#21457;&#29616;&#30830;&#35748;&#20102;&#36890;&#36807;&#21477;&#27861;&#25506;&#38024;&#26041;&#27861;&#33719;&#24471;&#30340;BERT&#30340;&#32467;&#26524;&#65292;&#24182;&#22240;&#27492;&#37325;&#35201;&#22320;&#35777;&#26126;&#20102;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The overwhelming success of transformers is a real conundrum stimulating a compelling question: are these machines replicating some traditional linguistic models or discovering radically new theories? In this paper, we propose a novel standpoint to investigate this important question. Using typological similarities among languages, we aim to layer-wise compare transformers for different languages to observe whether these similarities emerge for particular layers. For this investigation, we propose to use Centered kernel alignment to measure similarity among weight matrices. We discovered that syntactic typological similarity is consistent with the similarity among weights in the middle layers. This finding confirms results obtained by syntactically probing BERT and, thus, gives an important confirmation that BERT is replicating traditional linguistic models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2304.05805</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65306;&#38271;&#26399;&#35760;&#24518;&#26377;&#22810;&#22823;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
GDP nowcasting with artificial neural networks: How much does long-term memory matter?. (arXiv:2304.05805v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#30340;&#32479;&#35745;&#27169;&#22411;&#24212;&#29992;&#20110;&#32654;&#22269;&#32463;&#27982;&#23395;&#24230;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65288;GDP&#65289;&#22686;&#38271;&#39044;&#27979;&#12290;&#20351;&#29992;&#27599;&#26376;&#30340;FRED-MD&#25968;&#25454;&#24211;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65288;DFM&#65289;&#21644;&#22235;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#39044;&#27979;&#34920;&#29616;&#65306;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;1D CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12290;&#23454;&#35777;&#20998;&#26512;&#21576;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#35780;&#20272;&#21608;&#26399;&#30340;&#32467;&#26524;&#12290;&#31532;&#19968;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2019&#24180;&#31532;4&#23395;&#24230;&#65289;&#20855;&#26377;&#24179;&#34913;&#30340;&#32463;&#27982;&#22686;&#38271;&#65292;&#32780;&#31532;&#20108;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2022&#24180;&#31532;3&#23395;&#24230;&#65289;&#36824;&#21253;&#25324;COVID-19&#34928;&#36864;&#26399;&#38388;&#30340;&#26102;&#38388;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#30456;&#23545;&#36739;&#20302;&#30340;&#38408;&#20540;&#20540;&#65288;&#32422;&#20845;&#20010;&#23395;&#24230;&#25110;&#21313;&#20843;&#20010;&#26376;&#65289;&#20197;&#21518;&#65292;&#36825;&#31181;&#25928;&#24212;&#20250;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26399;&#65288;&#22914;COVID-19&#34928;&#36864;&#26399;&#38388;&#65289;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#20250;&#21464;&#24471;&#36739;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our study, we apply different statistical models to nowcast quarterly GDP growth for the US economy. Using the monthly FRED-MD database, we compare the nowcasting performance of the dynamic factor model (DFM) and four artificial neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the results from two distinctively different evaluation periods. The first (2010:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According to our results, longer input sequences result in more accurate nowcasts in periods of balanced economic growth. However, this effect ceases above a relatively low threshold value of around six quarters (eighteen months). During periods of economic turbulence (e.g., during the COVID-19 recession), long
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13991</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#23398;&#20064;&#23545;&#26410;&#30693;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#65306;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#30340;&#32763;&#35793;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13991
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#30001;&#20110;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65292;&#22810;&#39046;&#22495;&#28151;&#21512;&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#35268;&#33539;&#25552;&#21462;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#23545;&#39118;&#26684;&#65288;&#20363;&#22914;&#65292;&#26080;&#20449;&#24687;&#30340;&#32441;&#29702;&#65289;&#26377;&#24456;&#24378;&#30340;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#23545;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#24418;&#29366;&#65289;&#30340;&#20559;&#22909;&#65292;&#36825;&#19982;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25918;&#23556;&#31185;&#21307;&#24072;&#20542;&#21521;&#20110;&#20174;CXR&#22270;&#20687;&#20013;&#23398;&#20064;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#22240;&#27492;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#20174;CXR&#22270;&#20687;&#36827;&#34892;&#30149;&#29702;&#35786;&#26029;&#30340;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#27169;&#22411;&#24212;&#35813;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#65288;SRMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#26469;&#20272;&#35745;&#19987;&#23478;&#30340;&#20445;&#23432;&#27169;&#22411;&#20197;&#21450;&#19987;&#23478;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25512;&#26029;&#19987;&#19994;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07457</link><description>&lt;p&gt;
&#36890;&#36807;&#28436;&#31034;&#26469;&#29702;&#35299;&#19987;&#19994;&#25216;&#33021;&#65306;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning. (arXiv:2302.07457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07457
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#26469;&#20272;&#35745;&#19987;&#23478;&#30340;&#20445;&#23432;&#27169;&#22411;&#20197;&#21450;&#19987;&#23478;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25512;&#26029;&#19987;&#19994;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;Offline IRL&#65289;&#26088;&#22312;&#20174;&#19987;&#23478;&#20195;&#29702;&#30340;&#22266;&#23450;&#26377;&#38480;&#28436;&#31034;&#20013;&#24674;&#22797;&#25903;&#25745;&#35266;&#23519;&#21040;&#30340;&#25805;&#20316;&#30340;&#22870;&#21169;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#32467;&#26500;&#12290;&#20934;&#30830;&#30340;&#19987;&#19994;&#25191;&#34892;&#20219;&#21153;&#30340;&#27169;&#22411;&#22312;&#23433;&#20840;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#65292;&#20363;&#22914;&#20020;&#24202;&#20915;&#31574;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#28982;&#32780;&#65292;&#19987;&#23478;&#21916;&#22909;&#38544;&#21547;&#22312;&#35266;&#23519;&#21040;&#30340;&#25805;&#20316;&#20013;&#30340;&#32467;&#26500;&#19982;&#19987;&#23478;&#23545;&#29615;&#22659;&#21160;&#24577;&#30340;&#27169;&#22411;&#65288;&#21363;&#8220;&#19990;&#30028;&#8221;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20174;&#20855;&#26377;&#26377;&#38480;&#35206;&#30422;&#33539;&#22260;&#30340;&#26377;&#38480;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#19981;&#20934;&#30830;&#19990;&#30028;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#20272;&#35745;&#30340;&#22870;&#21169;&#30340;&#19981;&#20934;&#30830;&#24615;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#30340;&#20272;&#35745;&#20219;&#21153;&#65292;&#20854;&#20013;&#19978;&#23618;&#26159;&#22522;&#20110;&#19987;&#23478;&#31574;&#30053;&#30340;&#20445;&#23432;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;&#19979;&#23618;&#65289;&#12290;&#31574;&#30053;&#27169;&#22411;&#26159;&#20445;&#23432;&#30340;&#65292;&#22240;&#20026;&#23427;&#22312;&#24809;&#32602;&#65288;&#24809;&#32602;&#20250;&#38543;&#30528;&#19987;&#23478;&#23545;&#19990;&#30028;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#22686;&#21152;&#65289;&#19979;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#31163;&#32447;IRL&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the ``world''). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a conservative model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncerta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04831</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21327;&#21516;&#21512;&#20316;&#23398;&#20064;&#26694;&#26550;&#30340;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Open-ended Learning Framework for Zero-shot Coordination. (arXiv:2302.04831v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38646;&#26679;&#26412;&#21327;&#35843;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#21327;&#35843;&#19968;&#31995;&#21015;&#30475;&#19981;&#35265;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#20808;&#21069;&#30340;&#31639;&#27861;&#35797;&#22270;&#36890;&#36807;&#20248;&#21270;&#31181;&#32676;&#20013;&#30340;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#21892;&#31574;&#30053;&#25110;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#25439;&#22833;&#21644;&#19982;&#31181;&#32676;&#20013;&#26576;&#20123;&#31574;&#30053;&#26080;&#27861;&#21512;&#20316;&#65292;&#21363;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;&#65288;COLE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#26500;&#24314;&#20102;&#21327;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20197;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26126;&#30830;&#20102;&#26694;&#26550;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#21338;&#24328;&#35770;&#21644;&#22270;&#35770;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23545;&#31639;&#27861;&#30340;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20811;&#26381;&#23398;&#20064;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behaviour diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#65292;&#20004;&#31181;&#27169;&#22411;&#20173;&#21576;&#29616;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2212.10529</link><description>&lt;p&gt;
GPT-3&#26159;&#21542;&#23637;&#31034;&#20986;&#31934;&#31070;&#30149;&#24577;&#65311;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective. (arXiv:2212.10529v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#65292;&#20004;&#31181;&#27169;&#22411;&#20173;&#21576;&#29616;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#30830;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26080;&#20559;&#30340;&#25552;&#31034;&#26469;&#31995;&#32479;&#24615;&#22320;&#35780;&#20272;LLMs&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#20154;&#26684;&#27979;&#35797;&#8212;&#8212;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#65288;SD-3&#65289;&#21644;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#27979;&#35797;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;LLMs&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;SD-3&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#34920;&#26126;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#20197;&#20943;&#23569;&#27602;&#24615;&#65292;InstructGPT&#21644;FLAN-T5&#20173;&#28982;&#21576;&#29616;&#20986;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#65307;&#22312;SD-3&#30340;&#29595;&#22522;&#38597;&#32500;&#21033;&#20027;&#20041;&#21644;&#33258;&#24651;&#29378;&#29305;&#24449;&#19978;&#65292;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#33258;&#30417;&#30563;GPT-3&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24184;&#31119;&#24863;&#27979;&#35797;&#35780;&#20272;&#20102;GPT-3&#31995;&#21015;&#20013;&#30340;LLMs&#65292;&#20197;&#30740;&#31350;&#26356;&#22810;&#35757;&#32451;&#25968;&#25454;&#30340;&#24494;&#35843;&#23545;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;&#37492;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#27491;&#38754;&#22238;&#31572;&#20174;&#32780;&#25351;&#26631;&#24494;&#35843;FLAN-T5&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we determined whether large language models (LLMs) are psychologically safe. We designed unbiased prompts to systematically evaluate LLMs from a psychological perspective. First, we tested three different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT and FLAN-T5 still showed implicit dark personality patterns; both models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT-3 series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT-3 and InstructGPT. Following these observations, we showed that instruction fine-tuning FLAN-T5 with positive answers from 
&lt;/p&gt;</description></item></channel></rss>