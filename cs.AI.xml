<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;4.5%-7.9%&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08208</link><description>&lt;p&gt;
&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize for Cross-domain QA. (arXiv:2305.08208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08208
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;4.5%-7.9%&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36328;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#19968;&#30452;&#23384;&#22312;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#25285;&#24551;&#12290;&#24403;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21463;&#21040;&#20102;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#37117;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#21487;&#20197;&#22686;&#24378;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;F1&#24471;&#20998;&#24179;&#22343;&#25552;&#39640;&#20102;4.5%-7.9%&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#24182;&#20026;&#26410;&#20805;&#20998;&#24320;&#21457;&#30340;&#36328;&#22495;&#38382;&#31572;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;GitHub&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;*&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing then fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task. We release our source code at GitHub*.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25968;&#25454;&#38598;&#34701;&#21512;&#8221;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#21512;&#25104;&#31639;&#27861;&#65292;&#21487;&#23558;&#26469;&#33258;&#22810;&#20010;&#21516;&#36136;&#25968;&#25454;&#38598;&#30340;&#21608;&#26399;&#20449;&#21495;&#34701;&#21512;&#20026;&#21333;&#20010;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#20004;&#20010;&#21516;&#36136;&#24863;&#24212;&#30005;&#21160;&#26426;&#65288;IM&#65289;&#25925;&#38556;&#25968;&#25454;&#38598;&#30340;3&#30456;&#30005;&#27969;&#25968;&#25454;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#26377;&#28508;&#21147;&#22312;&#36328;&#22810;&#20010;&#26469;&#28304;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.08197</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21516;&#36136;&#21608;&#26399;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#34701;&#21512;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Dataset Fusion Algorithm for Generalised Anomaly Detection in Homogeneous Periodic Time Series Datasets. (arXiv:2305.08197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25968;&#25454;&#38598;&#34701;&#21512;&#8221;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#21512;&#25104;&#31639;&#27861;&#65292;&#21487;&#23558;&#26469;&#33258;&#22810;&#20010;&#21516;&#36136;&#25968;&#25454;&#38598;&#30340;&#21608;&#26399;&#20449;&#21495;&#34701;&#21512;&#20026;&#21333;&#20010;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#20004;&#20010;&#21516;&#36136;&#24863;&#24212;&#30005;&#21160;&#26426;&#65288;IM&#65289;&#25925;&#38556;&#25968;&#25454;&#38598;&#30340;3&#30456;&#30005;&#27969;&#25968;&#25454;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#26377;&#28508;&#21147;&#22312;&#36328;&#22810;&#20010;&#26469;&#28304;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25512;&#24191;&#24448;&#24448;&#34987;&#25991;&#29486;&#24573;&#30053;&#65292;&#22240;&#20026;NN&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#25968;&#25454;&#28304;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#20013;&#65292;&#30001;&#20110;&#26469;&#33258;&#19981;&#21516;&#20256;&#24863;&#22120;&#21644;&#37319;&#38598;&#35268;&#33539;&#30340;&#36830;&#32493;&#25968;&#25454;&#30340;&#34701;&#21512;&#22256;&#38590;&#65292;&#36825;&#21464;&#24471;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#65292;&#36890;&#29992;&#24615;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;AI&#27169;&#22411;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#20043;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#25968;&#25454;&#38598;&#34701;&#21512;&#8221;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#26469;&#33258;&#22810;&#20010;&#21516;&#36136;&#25968;&#25454;&#38598;&#30340;&#21608;&#26399;&#20449;&#21495;&#34701;&#21512;&#20026;&#21333;&#20010;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#21512;&#25104;&#31639;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#32463;&#36807;&#23454;&#39564;&#65292;&#22312;&#20351;&#29992;&#26080;&#30417;&#30563;LSTMCaps NN&#23545;&#26469;&#33258;2&#20010;&#19981;&#21516;&#21516;&#36136;&#24863;&#24212;&#30005;&#21160;&#26426;&#65288;IM&#65289;&#25925;&#38556;&#25968;&#25454;&#38598;&#30340;3&#30456;&#30005;&#27969;&#25968;&#25454;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#24120;&#35268;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#31034;&#20986;&#22312;&#36328;&#22810;&#20010;&#26469;&#28304;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalisation of Neural Networks (NN) to multiple datasets is often overlooked in literature due to NNs typically being optimised for specific data sources. This becomes especially challenging in time-series-based multi-dataset models due to difficulties in fusing sequential data from different sensors and collection specifications. In a commercial environment, however, generalisation can effectively utilise available data and computational power, which is essential in the context of Green AI, the sustainable development of AI models. This paper introduces "Dataset Fusion," a novel dataset composition algorithm for fusing periodic signals from multiple homogeneous datasets into a single dataset while retaining unique features for generalised anomaly detection. The proposed approach, tested on a case study of 3-phase current data from 2 different homogeneous Induction Motor (IM) fault datasets using an unsupervised LSTMCaps NN, significantly outperforms conventional training approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#8220;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#8221;&#65288;SAM&#65289;&#30340;&#36827;&#23637;&#65292;&#35813;&#27169;&#22411;&#25171;&#30772;&#20102;&#20998;&#21106;&#30028;&#38480;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.08196</link><description>&lt;p&gt;
&#8220;&#35270;&#35273;&#21644;&#26356;&#22810;&#39046;&#22495;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#32508;&#21512;&#35843;&#26597;&#8221;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Segment Anything Model for Vision and Beyond. (arXiv:2305.08196v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#8220;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#8221;&#65288;SAM&#65289;&#30340;&#36827;&#23637;&#65292;&#35813;&#27169;&#22411;&#25171;&#30772;&#20102;&#20998;&#21106;&#30028;&#38480;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27491;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#28436;&#36827;&#65292;&#21363;AI&#31995;&#32479;&#20855;&#22791;&#20687;&#20154;&#19968;&#26679;&#30340;&#24191;&#27867;&#20219;&#21153;&#21644;&#26234;&#33021;&#27700;&#24179;&#12290;&#36825;&#19982;&#31364;&#21270;&#25110;&#19987;&#29992;AI&#30456;&#23545;&#24212;&#65292;&#21518;&#32773;&#26088;&#22312;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#35774;&#35745;&#24191;&#27867;&#25968;&#25454;&#35757;&#32451;&#30340;&#12289;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#8220;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#8220;&#65288;SAM&#65289;&#22312;&#25171;&#30772;&#20998;&#21106;&#30028;&#38480;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#20805;&#20998;&#20102;&#35299;SAM&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#12290;&#20316;&#20026;&#31532;&#19968;&#31687;&#22522;&#20110;SAM&#22522;&#30784;&#27169;&#22411;&#20840;&#38754;&#22238;&#39038;&#35270;&#35273;&#21450;&#20854;&#20182;&#39046;&#22495;&#20219;&#24847;&#20998;&#21106;&#20219;&#21153;&#36827;&#23637;&#30340;&#25991;&#31456;&#65292;&#26412;&#25991;&#30528;&#37325;&#35752;&#35770;&#20102;&#23427;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#31867;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is evolving towards artificial general intelligence, which refers to the ability of an AI system to perform a wide range of tasks and exhibit a level of intelligence similar to that of a human being. This is in contrast to narrow or specialized AI, which is designed to perform specific tasks with a high degree of efficiency. Therefore, it is urgent to design a general class of models, which we term foundation models, trained on broad data that can be adapted to various downstream tasks. The recently proposed segment anything model (SAM) has made significant progress in breaking the boundaries of segmentation, greatly promoting the development of foundation models for computer vision. To fully comprehend SAM, we conduct a survey study. As the first to comprehensively review the progress of segmenting anything task for vision and beyond based on the foundation model of SAM, this work focuses on its applications to various tasks and data types by discussing it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32771;&#34385;&#27169;&#24335;&#20999;&#25442;&#28857;&#20248;&#21270;&#30340;&#31354;&#22320;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#20840;&#23616;&#31354;&#38388;&#35268;&#21010;&#25216;&#26415;&#21644;&#21487;&#20114;&#25442;&#30340;&#25628;&#32034;&#26041;&#27861;&#26469;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12289;&#25628;&#32034;&#36895;&#24230;&#21644;&#20219;&#21153;&#25191;&#34892;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08178</link><description>&lt;p&gt;
&#32771;&#34385;&#27169;&#24335;&#20999;&#25442;&#28857;&#20248;&#21270;&#30340;&#31354;&#22320;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Path Planning for Air-Ground Robot Considering Modal Switching Point Optimization. (arXiv:2305.08178v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32771;&#34385;&#27169;&#24335;&#20999;&#25442;&#28857;&#20248;&#21270;&#30340;&#31354;&#22320;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#20840;&#23616;&#31354;&#38388;&#35268;&#21010;&#25216;&#26415;&#21644;&#21487;&#20114;&#25442;&#30340;&#25628;&#32034;&#26041;&#27861;&#26469;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12289;&#25628;&#32034;&#36895;&#24230;&#21644;&#20219;&#21153;&#25191;&#34892;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#22320;&#26426;&#22120;&#20154;&#26159;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#31227;&#21160;&#24179;&#21488;&#65292;&#21487;&#20197;&#34892;&#39542;&#21644;&#39134;&#34892;&#12290;&#20256;&#32479;&#30340;&#31354;&#22320;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#28789;&#27963;&#30340;&#39134;&#34892;&#38656;&#27714;&#12290;&#20197;&#24448;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#36335;&#24452;&#30340;&#33021;&#28304;&#25928;&#29575;&#19978;&#65292;&#24456;&#23569;&#32771;&#34385;&#25628;&#32034;&#36895;&#24230;&#21644;&#36215;&#38477;&#28857;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#29616;&#22330;&#24212;&#29992;&#29615;&#22659;&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#23616;&#31354;&#38388;&#35268;&#21010;&#25216;&#26415;&#65292;&#22522;&#20110;&#22270;&#25628;&#32034;&#31639;&#27861;&#32771;&#34385;&#27169;&#24335;&#20999;&#25442;&#28857;&#20248;&#21270;&#65292;&#24378;&#35843;&#33021;&#28304;&#25928;&#29575;&#12289;&#25628;&#32034;&#36895;&#24230;&#21644;&#23454;&#38469;&#37096;&#32626;&#30340;&#21487;&#34892;&#24615;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#36890;&#36807;&#37319;&#29992;&#21487;&#20114;&#25442;&#30340;&#25628;&#32034;&#26041;&#27861;&#32467;&#21512;&#24179;&#38754;&#21644;&#31354;&#38388;&#25628;&#32034;&#26469;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#38519;&#38449;&#36991;&#20813;&#26041;&#27861;&#26469;&#20445;&#25252;&#30005;&#27744;&#30340;&#20581;&#24247;&#21644;&#20219;&#21153;&#25191;&#34892;&#30340;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12289;&#25552;&#39640;&#25628;&#32034;&#36895;&#24230;&#21644;&#20445;&#35777;&#23433;&#20840;&#21487;&#38752;&#25191;&#34892;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An innovative sort of mobility platform that can both drive and fly is the air-ground robot. The need for an agile flight cannot be satisfied by traditional path planning techniques for air-ground robots. Prior studies had mostly focused on improving the energy efficiency of paths, seldom taking the seeking speed and optimizing take-off and landing places into account. A robot for the field application environment was proposed, and a lightweight global spatial planning technique for the robot based on the graph-search algorithm taking mode switching point optimization into account, with an emphasis on energy efficiency, searching speed, and the viability of real deployment. The fundamental concept is to lower the computational burden by employing an interchangeable search approach that combines planar and spatial search. Furthermore, to safeguard the health of the power battery and the integrity of the mission execution, a trap escape approach was also provided. Simulations are run to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#27719;&#26367;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#37322;&#20041;&#29983;&#25104;&#22120;&#29983;&#25104;&#26367;&#20195;&#35789;&#20505;&#36873;&#39033;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#35299;&#30721;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08146</link><description>&lt;p&gt;
ParaLS&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#37322;&#20041;&#29983;&#25104;&#22120;&#30340;&#35789;&#27719;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
ParaLS: Lexical Substitution via Pretrained Paraphraser. (arXiv:2305.08146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#27719;&#26367;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#37322;&#20041;&#29983;&#25104;&#22120;&#29983;&#25104;&#26367;&#20195;&#35789;&#20505;&#36873;&#39033;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#35299;&#30721;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#26367;&#25442; (LS) &#26088;&#22312;&#25214;&#21040;&#21477;&#23376;&#20013;&#30446;&#26631;&#35789;&#30340;&#36866;&#24403;&#26367;&#20195;&#35789;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340; LS &#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#20998;&#26512;&#30446;&#26631;&#35789;&#21608;&#22260;&#30340;&#35821;&#22659;&#29983;&#25104;&#28508;&#22312;&#30340;&#26367;&#20195;&#35789;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#29983;&#25104;&#26367;&#20195;&#35789;&#26102;&#24448;&#24448;&#24573;&#35270;&#20102;&#21477;&#23376;&#21547;&#20041;&#30340;&#20445;&#30041;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#37322;&#20041;&#29983;&#25104;&#22120;&#20013;&#29983;&#25104;&#26367;&#20195;&#35789;&#20505;&#36873;&#39033;&#65292;&#22240;&#20026;&#37322;&#20041;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#37322;&#20041;&#21253;&#21547;&#20102;&#35789;&#27719;&#36873;&#25321;&#30340;&#21464;&#21270;&#24182;&#20445;&#30041;&#20102;&#21477;&#23376;&#30340;&#21547;&#20041;&#12290;&#30001;&#20110;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#36890;&#36807;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#29983;&#25104;&#26367;&#20195;&#35789;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#19987;&#27880;&#20110;&#35299;&#30721;&#36807;&#31243;&#20013;&#30446;&#26631;&#35789;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340; LS &#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexical substitution (LS) aims at finding appropriate substitutes for a target word in a sentence. Recently, LS methods based on pretrained language models have made remarkable progress, generating potential substitutes for a target word through analysis of its contextual surroundings. However, these methods tend to overlook the preservation of the sentence's meaning when generating the substitutes. This study explores how to generate the substitute candidates from a paraphraser, as the generated paraphrases from a paraphraser contain variations in word choice and preserve the sentence's meaning. Since we cannot directly generate the substitutes via commonly used decoding strategies, we propose two simple decoding strategies that focus on the variations of the target word during decoding. Experimental results show that our methods outperform state-of-the-art LS methods based on pre-trained language models on three benchmarks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;CPACE&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#24120;&#35782;&#38382;&#31572;&#20013;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.08135</link><description>&lt;p&gt;
&#21306;&#20998;&#20808;&#20110;&#22238;&#31572;&#65306;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#20316;&#20026;&#24120;&#35782;&#38382;&#31572;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering. (arXiv:2305.08135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08135
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;CPACE&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#24120;&#35782;&#38382;&#31572;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#33719;&#21462;&#19981;&#21516;&#30340;&#30693;&#35782;&#65292;&#22312;&#26576;&#20123;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#21463;&#21040;&#26816;&#32034;&#30693;&#35782;&#30340;&#29305;&#24615;&#38480;&#21046;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#21516;&#26102;&#20174;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#21306;&#20998;&#24615;&#26041;&#38754;&#21463;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CPACE&#65292;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#32473;&#23450;&#20505;&#36873;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing knowledge-enhanced methods have achieved remarkable results in certain QA tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishment simultaneously. To address the challenge, we propose CPACE, a Concept-centric Prompt-bAsed Contrastive Explanation Generation model, which aims to convert obtained symbolic knowledge into a contrastive explanation for better distinguishing the differences among given candidates. Firstly, following previous works, we retrieve different types of symbolic knowledge with a concept-centric knowledge extraction module. After that, we generate corresponding contrastive explanations using acquired symbolic knowledge and explanation prompts as guidance for better modeling the knowledge distinguishment and interpretability. Finally, we regard the generated contrastive explanation as external knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32422;&#26463;&#24674;&#22797;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#38024;&#23545;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#12290;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#25351;&#25968;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;</title><link>http://arxiv.org/abs/2305.08130</link><description>&lt;p&gt;
&#24102;&#26377;&#32422;&#26463;&#24674;&#22797;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning With Constraint Recovery. (arXiv:2305.08130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32422;&#26463;&#24674;&#22797;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#38024;&#23545;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#12290;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#25351;&#25968;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#30340;&#26032;&#22411;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#31639;&#27861;&#12290;&#22312;&#26631;&#20934;&#30340;IRL&#38382;&#39064;&#20013;&#65292;&#36870;&#23398;&#20064;&#32773;&#25110;&#20195;&#29702;&#20154;&#23547;&#27714;&#24674;&#22797;MDP&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#32473;&#23450;&#26368;&#20248;&#31574;&#30053;&#30340;&#36712;&#36857;&#28436;&#31034;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19981;&#20165;&#23547;&#27714;&#25512;&#26029;CMDP&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36824;&#35201;&#25512;&#26029;&#32422;&#26463;&#12290;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#25105;&#20204;&#34920;&#26126;IRL&#19982;&#32422;&#26463;&#24674;&#22797;&#65288;IRL-CR&#65289;&#38382;&#39064;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20854;&#20943;&#23569;&#20026;&#20132;&#26367;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#23376;&#38382;&#39064;&#26159;&#20984;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel inverse reinforcement learning (IRL) algorithm for constrained Markov decision process (CMDP) problems. In standard IRL problems, the inverse learner or agent seeks to recover the reward function of the MDP, given a set of trajectory demonstrations for the optimal policy. In this work, we seek to infer not only the reward functions of the CMDP, but also the constraints. Using the principle of maximum entropy, we show that the IRL with constraint recovery (IRL-CR) problem can be cast as a constrained non-convex optimization problem. We reduce it to an alternating constrained optimization problem whose sub-problems are convex. We use exponentiated gradient descent algorithm to solve it. Finally, we demonstrate the efficacy of our algorithm for the grid world environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Theta&#24207;&#21015;&#20316;&#20026;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#36164;&#26684;&#36861;&#36394;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#27169;&#25311;&#35777;&#26126;&#20102;Theta&#24207;&#21015;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#32500;&#25252;&#38271;&#26102;&#38388;&#35760;&#24518;&#30165;&#36857;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#23545;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#24341;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.08124</link><description>&lt;p&gt;
Theta&#24207;&#21015;&#20316;&#20026;&#36164;&#26684;&#36861;&#36394;&#65306;&#29983;&#29289;&#23398;&#20013;&#30340;&#20449;&#36151;&#20998;&#37197;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Theta sequences as eligibility traces: a biological solution to credit assignment. (arXiv:2305.08124v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Theta&#24207;&#21015;&#20316;&#20026;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#36164;&#26684;&#36861;&#36394;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#27169;&#25311;&#35777;&#26126;&#20102;Theta&#24207;&#21015;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#32500;&#25252;&#38271;&#26102;&#38388;&#35760;&#24518;&#30165;&#36857;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#23545;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#36151;&#20998;&#37197;&#38382;&#39064;&#65292;&#20363;&#22914;RL&#20013;&#30340;&#31574;&#30053;&#35780;&#20272;&#65292;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#20808;&#21069;&#30340;&#29366;&#24577;&#25110;&#32500;&#25252;&#26102;&#38388;&#19978;&#24310;&#20280;&#30340;&#35760;&#24518;&#30165;&#36857;&#26469;&#24341;&#23548;&#39044;&#27979;&#35823;&#24046;;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#31070;&#32463;&#20803;&#30340;&#29983;&#29289;&#32593;&#32476;&#26469;&#35828;&#26159;&#19981;&#21033;&#25110;&#19981;&#21487;&#20449;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;Theta&#24207;&#21015;-&#28023;&#39532;&#20013;theta&#25391;&#33633;&#26399;&#38388;&#30340;&#31070;&#32463;&#27963;&#21160;&#38142;&#65292;&#34987;&#35748;&#20026;&#20195;&#34920;&#28165;&#37266;&#34892;&#20026;&#30340;&#24555;&#36895;&#25773;&#25918;-through&#65292;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#27169;&#25311;Theta&#24207;&#21015;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#20204;&#21387;&#32553;&#34892;&#20026;&#65292;&#20197;&#20351;&#29616;&#26377;&#20294;&#30701;&#26242;&#30340;O&#65288;10&#65289;ms&#31070;&#32463;&#35760;&#24518;&#30165;&#36857;&#24471;&#20197;&#26377;&#25928;&#24310;&#20280;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#38656;&#38271;&#26399;&#35760;&#24518;&#30165;&#36857;&#30340;&#22522;&#20110;&#36164;&#26684;&#30340;&#36861;&#36394;&#65292;&#30456;&#24403;&#20110;&#22312;TD&#65288;&#955;&#65289;&#20013;&#20351;&#29992;&#36164;&#26684;&#36861;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credit assignment problems, for example policy evaluation in RL, often require bootstrapping prediction errors through preceding states \textit{or} maintaining temporally extended memory traces; solutions which are unfavourable or implausible for biological networks of neurons. We propose theta sequences -- chains of neural activity during theta oscillations in the hippocampus, thought to represent rapid playthroughs of awake behaviour -- as a solution. By analysing and simulating a model for theta sequences we show they compress behaviour such that existing but short $\mathsf{O}(10)$ ms neuronal memory traces are effectively extended allowing for bootstrap-free credit assignment without long memory traces, equivalent to the use of eligibility traces in TD($\lambda$).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#34920;&#23618;&#24615;&#26469;&#31616;&#21333;&#24314;&#27169;&#22797;&#26434;&#29305;&#24449;&#65292;&#20174;&#32780;&#25484;&#25569;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.08116</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#65292;&#20197;&#21450;&#20854;&#34920;&#23618;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
The Structure and Dynamics of Knowledge Graphs, with Superficiality. (arXiv:2305.08116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08116
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#34920;&#23618;&#24615;&#26469;&#31616;&#21333;&#24314;&#27169;&#22797;&#26434;&#29305;&#24449;&#65292;&#20174;&#32780;&#25484;&#25569;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#32508;&#21512;&#20102;&#20174;&#23398;&#26415;&#26426;&#26500;&#21644;&#20225;&#19994;&#21040;&#22823;&#20247;&#38598;&#36164;&#31561;&#39033;&#30446;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#30693;&#35782;&#65292;&#27599;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#20195;&#34920;&#36825;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#22522;&#26412;&#20107;&#23454;&#12290;&#20851;&#31995;&#35821;&#20041;&#30340;&#22810;&#26679;&#24615;&#32452;&#25104;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#20016;&#23500;&#24615;&#65292;&#23548;&#33268;&#20986;&#29616;&#26377;&#26102;&#28151;&#20081;&#30340;&#22855;&#24322;&#25299;&#25169;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#34920;&#23618;&#24615;&#30340;&#27010;&#24565;&#26469;&#31616;&#21333;&#24314;&#27169;&#65292;&#34920;&#23618;&#24615;&#25511;&#21046;&#30528;&#29420;&#31435;&#29983;&#25104;&#20107;&#23454;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#21472;&#24773;&#20917;&#65292;&#20063;&#36890;&#36807;&#30830;&#23450;&#38169;&#35823;&#25551;&#36848;&#23454;&#20307;&#30340;&#27604;&#20363;&#26469;&#25511;&#21046;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;&#36825;&#26159;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#21160;&#24577;&#26041;&#38754;&#30340;&#39318;&#20010;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#27491;&#24335;&#30693;&#35782;&#33719;&#21462;&#21644;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large knowledge graphs combine human knowledge garnered from projects ranging from academia and institutions to enterprises and crowdsourcing. Within such graphs, each relationship between two nodes represents a basic fact involving these two entities. The diversity of the semantics of relationships constitutes the richness of knowledge graphs, leading to the emergence of singular topologies, sometimes chaotic in appearance. However, this complex characteristic can be modeled in a simple way by introducing the concept of superficiality, which controls the overlap between relationships whose facts are generated independently. Superficiality also regulates the balance of the global distribution of knowledge by determining the proportion of misdescribed entities. This is the first model for the structure and dynamics of knowledge graphs. It leads to a better understanding of formal knowledge acquisition and organization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20307;&#39564;&#24773;&#24863;&#20154;&#31867;&#20570;&#20986;&#20915;&#31574;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#27604;&#36739;&#20102;&#37327;&#23376;&#29702;&#35770;&#21644;&#32463;&#20856;&#26415;&#35821;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#35770;&#25991;&#38416;&#26126;&#20102;&#20869;&#22312;&#22122;&#22768;&#19979;&#37327;&#23376;&#27979;&#37327;&#21644;&#24773;&#24863;&#20915;&#31574;&#21046;&#23450;&#20043;&#38388;&#30340;&#31867;&#27604;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#35748;&#30693;&#36807;&#31243;&#22312;&#24418;&#24335;&#19978;&#19982;&#37327;&#23376;&#27979;&#37327;&#20855;&#26377;&#35768;&#22810;&#30456;&#20284;&#29305;&#24449;&#12290;&#35813;&#35770;&#25991;&#30340;&#26368;&#32456;&#30446;&#30340;&#26159;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#36890;&#36807;&#21482;&#37319;&#29992;&#32463;&#20856;&#27010;&#24565;&#30340;&#20844;&#29702;&#26041;&#27861;&#36827;&#34892;&#36816;&#31639;&#12290;</title><link>http://arxiv.org/abs/2305.08112</link><description>&lt;p&gt;
&#12298;&#24773;&#24863;&#20154;&#24037;&#26234;&#33021;&#30340;&#37327;&#23376;&#25805;&#20316;&#12299;
&lt;/p&gt;
&lt;p&gt;
Quantum Operation of Affective Artificial Intelligence. (arXiv:2305.08112v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08112
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20307;&#39564;&#24773;&#24863;&#20154;&#31867;&#20570;&#20986;&#20915;&#31574;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#27604;&#36739;&#20102;&#37327;&#23376;&#29702;&#35770;&#21644;&#32463;&#20856;&#26415;&#35821;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#35770;&#25991;&#38416;&#26126;&#20102;&#20869;&#22312;&#22122;&#22768;&#19979;&#37327;&#23376;&#27979;&#37327;&#21644;&#24773;&#24863;&#20915;&#31574;&#21046;&#23450;&#20043;&#38388;&#30340;&#31867;&#27604;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#35748;&#30693;&#36807;&#31243;&#22312;&#24418;&#24335;&#19978;&#19982;&#37327;&#23376;&#27979;&#37327;&#20855;&#26377;&#35768;&#22810;&#30456;&#20284;&#29305;&#24449;&#12290;&#35813;&#35770;&#25991;&#30340;&#26368;&#32456;&#30446;&#30340;&#26159;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#36890;&#36807;&#21482;&#37319;&#29992;&#32463;&#20856;&#27010;&#24565;&#30340;&#20844;&#29702;&#26041;&#27861;&#36827;&#34892;&#36816;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20154;&#24037;&#26234;&#33021;&#22522;&#26412;&#21407;&#29702;&#65292;&#20197;&#27169;&#20223;&#20307;&#39564;&#24773;&#24863;&#20154;&#31867;&#20570;&#20986;&#20915;&#31574;&#36807;&#31243;&#12290;&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#37327;&#23376;&#29702;&#35770;&#21644;&#22522;&#20110;&#32463;&#20856;&#26415;&#35821;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20855;&#26377;&#24456;&#22810;&#30456;&#20284;&#20043;&#22788;&#65292;&#20027;&#35201;&#26159;&#27010;&#29575;&#24615;&#30340;&#12290;&#35770;&#25991;&#38416;&#26126;&#20102;&#20869;&#22312;&#22122;&#22768;&#19979;&#37327;&#23376;&#27979;&#37327;&#21644;&#24773;&#24863;&#20915;&#31574;&#21046;&#23450;&#20043;&#38388;&#30340;&#31867;&#27604;&#20851;&#31995;&#12290;&#35770;&#25991;&#36824;&#34920;&#26126;&#65292;&#35748;&#30693;&#36807;&#31243;&#22312;&#24418;&#24335;&#19978;&#19982;&#37327;&#23376;&#27979;&#37327;&#20855;&#26377;&#35768;&#22810;&#30456;&#20284;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#24182;&#19981;&#24847;&#21619;&#30528;&#20026;&#20102;&#27169;&#20223;&#20154;&#31867;&#20915;&#31574;&#65292;&#24773;&#24863;&#20154;&#24037;&#26234;&#33021;&#24517;&#39035;&#20381;&#36182;&#20110;&#37327;&#23376;&#31995;&#32479;&#30340;&#36816;&#20316;&#12290;&#20174;&#37327;&#23376;&#27979;&#37327;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#20849;&#21516;&#29305;&#24449;&#20986;&#21457;&#65292;&#26377;&#21161;&#20110;&#21046;&#23450;&#21482;&#37319;&#29992;&#32463;&#20856;&#27010;&#24565;&#30340;&#20844;&#29702;&#26041;&#27861;&#65292;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The review analyzes the fundamental principles which Artificial Intelligence should be based on in order to imitate the realistic process of taking decisions by humans experiencing emotions. Two approaches are compared, one based on quantum theory and the other employing classical terms. Both these approaches have a number of similarities, being principally probabilistic. The analogies between quantum measurements under intrinsic noise and affective decision making are elucidated. It is shown that cognitive processes have many features that are formally similar to quantum measurements. This, however, in no way means that for the imitation of human decision making Affective Artificial Intelligence has necessarily to rely on the functioning of quantum systems. Appreciating the common features between quantum measurements and decision making helps for the formulation of an axiomatic approach employing only classical notions. Artificial Intelligence, following this approach, operates simil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861; QFedTD&#65292;&#22312;&#26377;&#38480;&#36895;&#25273;&#36890;&#36947;&#19979;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#20197;&#36798;&#21040;&#32447;&#24615;&#21152;&#36895;&#30340;&#25928;&#26524;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.08104</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#36895;&#25273;&#36890;&#36947;&#30340;&#32852;&#37030; TD &#23398;&#20064;&#65306;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#30340;&#32447;&#24615;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Federated TD Learning over Finite-Rate Erasure Channels: Linear Speedup under Markovian Sampling. (arXiv:2305.08104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861; QFedTD&#65292;&#22312;&#26377;&#38480;&#36895;&#25273;&#36890;&#36947;&#19979;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#20197;&#36798;&#21040;&#32447;&#24615;&#21152;&#36895;&#30340;&#25928;&#26524;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#22312;&#36890;&#20449;&#21644;&#38544;&#31169;&#32422;&#26463;&#19979;&#21152;&#36895;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064; (FL)&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#21152;&#36895;&#65292;&#22312;&#29702;&#35770;&#19978;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#38024;&#23545;&#36825;&#20010;&#26041;&#21521;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#37030;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#22312;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20013;&#22830;&#32858;&#21512;&#22120;&#36827;&#34892;&#36890;&#20449;&#65292;&#20197;&#21152;&#24555;&#20849;&#21516;&#31574;&#30053;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#25429;&#25417; FL &#20013;&#30340;&#20856;&#22411;&#36890;&#20449;&#32422;&#26463;&#65292;&#25105;&#20204;&#32771;&#34385;&#21487;&#20197;&#26681;&#25454;&#20271;&#21162;&#21033;&#25830;&#25325;&#27169;&#22411;&#20002;&#24323;&#25968;&#25454;&#21253;&#30340;&#26377;&#38480;&#23481;&#37327;&#19978;&#34892;&#38142;&#36335;&#36890;&#36947;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102; QFedTD-&#19968;&#31181;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#37327;&#21270;&#32852;&#37030;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#25552;&#20379; QFedTD &#30340;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#65292;&#35813;&#20998;&#26512;&#31361;&#20986;&#20102;&#37327;&#21270;&#21644;&#25273;&#38500;&#23545;&#25910;&#25947;&#36895;&#29575;&#30340;&#24433;&#21709;&#65307;&#24182;&#24314;&#31435;&#20102;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#30340;&#21152;&#36895;&#24230;&#12290;(&#32763;&#35793;&#20165;&#20379;&#21442;&#32771;&#65292;&#19981;&#20195;&#34920;&#36798;&#24847;&#23436;&#20840;&#27491;&#30830;)
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has recently gained much attention due to its effectiveness in speeding up supervised learning tasks under communication and privacy constraints. However, whether similar speedups can be established for reinforcement learning remains much less understood theoretically. Towards this direction, we study a federated policy evaluation problem where agents communicate via a central aggregator to expedite the evaluation of a common policy. To capture typical communication constraints in FL, we consider finite capacity up-link channels that can drop packets based on a Bernoulli erasure model. Given this setting, we propose and analyze QFedTD - a quantized federated temporal difference learning algorithm with linear function approximation. Our main technical contribution is to provide a finite-sample analysis of QFedTD that (i) highlights the effect of quantization and erasures on the convergence rate; and (ii) establishes a linear speedup w.r.t. the number of agents un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#26412;&#36136;&#65292;&#21363;&#26469;&#33258;&#20110;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#22522;&#20110;&#35789;&#32423;&#21035;&#30340;&#30693;&#35782;&#33976;&#39311;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;Top-1 Information&#12290;</title><link>http://arxiv.org/abs/2305.08096</link><description>&lt;p&gt;
&#25506;&#31350;&#21644;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation. (arXiv:2305.08096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#26412;&#36136;&#65292;&#21363;&#26469;&#33258;&#20110;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#22522;&#20110;&#35789;&#32423;&#21035;&#30340;&#30693;&#35782;&#33976;&#39311;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;Top-1 Information&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#20013;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22312;&#21738;&#37324;&#38544;&#34255;&#30340;&#38382;&#39064;&#20173;&#19981;&#28165;&#26970;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#30693;&#35782;&#33976;&#39311;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#23454;&#35777;&#35282;&#24230;&#25581;&#24320;&#20102;&#36825;&#20010;&#35868;&#22242;&#65292;&#24182;&#23637;&#31034;&#20102;&#30693;&#35782;&#26469;&#33258;&#25945;&#24072;&#30340;top-1&#39044;&#27979;&#65292;&#36825;&#20063;&#24110;&#21161;&#25105;&#20204;&#24314;&#31435;&#20102;&#35789;&#32423;&#21644;&#24207;&#21015;&#32423;&#33976;&#39311;&#20043;&#38388;&#30340;&#28508;&#22312;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#25351;&#20986;&#20102;&#22522;&#30784;&#35789;&#32423;&#33976;&#39311;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#30693;&#35782;&#30340;&#24403;&#21069;&#30446;&#26631;&#26159;&#23558;&#27880;&#24847;&#21147;&#25193;&#25955;&#21040;&#25972;&#20010;&#20998;&#24067;&#19978;&#23398;&#20064;&#30693;&#35782;&#65292;&#20294;&#32570;&#20047;&#23545;&#26368;&#20851;&#38190;&#30340;top-1&#20449;&#24687;&#30340;&#29305;&#27530;&#22788;&#29702;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#25945;&#24072;&#30340;top-1&#39044;&#27979;&#19982;&#22320;&#38754;&#23454;&#20917;&#26631;&#35760;&#37325;&#21472;&#65292;&#22240;&#27492;&#30693;&#35782;&#34987;&#40644;&#37329;&#20449;&#24687;&#25152;&#21344;&#25454;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#28508;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{T}op-1 \textbf{I}nformation&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the knowledge comes from the top-1 predictions of teachers, which also helps us build a potential connection between word- and sequence-level KD. Further, we point out two inherent issues in vanilla word-level KD based on this finding. Firstly, the current objective of KD spreads its focus to whole distributions to learn the knowledge, yet lacks special treatment on the most crucial top-1 information. Secondly, the knowledge is largely covered by the golden information due to the fact that most top-1 predictions of teachers overlap with ground-truth tokens, which further restricts the potential of KD. To address these issues, we propose a novel method named \textbf{T}op-1 \textbf{I}nformation \te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#21644;&#32437;&#21521;&#20803;&#20998;&#26512;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#25935;&#25463;&#36719;&#20214;&#24320;&#21457;&#20013;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#20197;&#20840;&#38754;&#20102;&#35299;&#20854;&#24433;&#21709;&#21644;&#24212;&#23545;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.08093</link><description>&lt;p&gt;
&#25935;&#25463;&#24320;&#21457;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#20803;&#20998;&#26512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI for Agile development: a Meta-Analysis. (arXiv:2305.08093v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#21644;&#32437;&#21521;&#20803;&#20998;&#26512;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#25935;&#25463;&#36719;&#20214;&#24320;&#21457;&#20013;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#20197;&#20840;&#38754;&#20102;&#35299;&#20854;&#24433;&#21709;&#21644;&#24212;&#23545;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25935;&#25463;&#36719;&#20214;&#24320;&#21457;&#26041;&#27861;&#20013;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#20197;&#25913;&#21892;&#25345;&#32493;&#38598;&#25104;&#21644;&#20132;&#20184;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#12290;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#32437;&#21521;&#20803;&#20998;&#26512;&#26469;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#30340;&#20316;&#29992;&#21450;&#20854;&#22312;&#25935;&#25463;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;&#32508;&#36848;&#21457;&#29616;&#20102;&#35768;&#22810;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#38656;&#35201;&#19987;&#19994;&#30340;&#31038;&#20250;&#25216;&#26415;&#19987;&#19994;&#30693;&#35782;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#20026;&#36719;&#20214;&#24320;&#21457;&#23454;&#36341;&#24102;&#26469;&#20102;&#24076;&#26395;&#65292;&#20294;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#23545;&#36807;&#31243;&#21644;&#20174;&#19994;&#20154;&#21592;&#30340;&#24433;&#21709;&#65292;&#24182;&#35299;&#20915;&#19982;&#20854;&#23454;&#26045;&#30456;&#20851;&#30340;&#38388;&#25509;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the benefits and challenges of integrating Artificial Intelligence with Agile software development methodologies, focusing on improving continuous integration and delivery. A systematic literature review and longitudinal meta-analysis of the retrieved studies was conducted to analyse the role of Artificial Intelligence and it's future applications within Agile software development. The review helped identify critical challenges, such as the need for specialised socio-technical expertise. While Artificial Intelligence holds promise for improved software development practices, further research is needed to better understand its impact on processes and practitioners, and to address the indirect challenges associated with its implementation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BBT-RGB&#65292;&#19968;&#22871;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#65292;&#21253;&#25324;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#12289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#20197;&#21450;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.08088</link><description>&lt;p&gt;
&#20351;&#22522;&#20110;&#25552;&#31034;&#30340;&#40657;&#30418;&#35843;&#20248;&#26356;&#21152;&#20016;&#23500;&#22810;&#24425;&#65306;&#20174;&#19977;&#20010;&#27491;&#20132;&#35282;&#24230;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives. (arXiv:2305.08088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BBT-RGB&#65292;&#19968;&#22871;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#65292;&#21253;&#25324;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#12289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#20197;&#21450;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#24040;&#39069;&#30340;&#20195;&#20215;&#25110;&#30001;&#20110;&#21830;&#19994;&#32771;&#34385;&#32780;&#19981;&#21487;&#29992;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#40657;&#30418;&#35843;&#20248;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#32780;&#19981;&#35775;&#38382;&#26799;&#24230;&#21644;&#38544;&#34255;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20316;&#21697;&#36824;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;BBT-RGB&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#32452;&#20214;&#65306;&#65288;1&#65289;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#25910;&#25947;&#24182;&#32531;&#35299;&#36807;&#25311;&#21512;&#65307;&#65288;2&#65289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#65307;&#65288;3&#65289;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#65292;&#22522;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#35328;&#23398;&#21160;&#26426;&#21477;&#27861;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown increasing power on various natural language processing (NLP) tasks. However, tuning these models for downstream tasks usually needs exorbitant costs or is unavailable due to commercial considerations. Recently, black-box tuning has been proposed to address this problem by optimizing task-specific prompts without accessing the gradients and hidden representations. However, most existing works have yet fully exploited the potential of gradient-free optimization under the scenario of few-shot learning. In this paper, we describe BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization. Specifically, our method includes three plug-and-play components: (1) Two-stage derivative-free optimization strategy that facilitates fast convergence and mitigates overfitting; (2) Automatic verbalizer construction with its novel usage under few-shot settings; (3) Better prompt initializ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#35270;&#35282;&#26469;&#25913;&#36827; Release 17 Type-II &#30721;&#26412;&#20013;&#30340; CSI &#21453;&#39304;&#24615;&#33021;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36873;&#25321;&#29305;&#23450;&#30340;&#35282;&#24230;-&#24310;&#36831;&#22495;&#31471;&#21475;&#20197;&#25552;&#39640;&#31934;&#24230;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#21453;&#39304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21453;&#39304;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.08081</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340; Type-II &#30721;&#26412;&#65306;&#22686;&#24378; CSI &#21453;&#39304;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Empowered Type-II Codebook: New Perspectives for Enhancing CSI Feedback. (arXiv:2305.08081v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#35270;&#35282;&#26469;&#25913;&#36827; Release 17 Type-II &#30721;&#26412;&#20013;&#30340; CSI &#21453;&#39304;&#24615;&#33021;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36873;&#25321;&#29305;&#23450;&#30340;&#35282;&#24230;-&#24310;&#36831;&#22495;&#31471;&#21475;&#20197;&#25552;&#39640;&#31934;&#24230;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#21453;&#39304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21453;&#39304;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39057;&#20998;&#21452;&#24037;&#31995;&#32479;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#21453;&#39304;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#23558;&#26080;&#32447;&#36890;&#20449;&#26631;&#20934;&#20013;&#30340; Type-II &#30721;&#26412;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640; CSI &#21453;&#39304;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340; Release 16 Type-II &#30721;&#26412;&#30740;&#31350;&#19981;&#21516;&#65292;Release 17&#65288;R17&#65289;&#20013;&#30340; Type-II &#30721;&#26412;&#21033;&#29992;&#19978;&#34892;&#21644;&#19979;&#34892;&#20449;&#36947;&#20043;&#38388;&#35282;&#24230;-&#24310;&#36831;&#22495;&#20559;&#25391;&#37096;&#20998;&#20114;&#26131;&#24615;&#36873;&#25321;&#37096;&#20998;&#35282;&#24230;-&#24310;&#36831;&#22495;&#31471;&#21475;&#36827;&#34892;&#27979;&#37327;&#21644;&#21453;&#39304;&#19979;&#34892; CSI&#65292;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#30001;&#20110;&#31232;&#30095;&#32467;&#26500;&#30340;&#32570;&#38519;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#25913;&#36827; R17 Type-II &#30721;&#26412;&#30340;&#26032;&#35270;&#35282;&#12290;&#39318;&#20808;&#65292;&#32771;&#34385;&#21040;&#19978;&#34892;&#20449;&#36947;&#30340;&#20302;&#20449;&#22122;&#27604;&#65292;&#28145;&#24230;&#23398;&#20064;&#34987;&#29992;&#26469;&#31934;&#30830;&#36873;&#25321;&#29305;&#23450;&#30340;&#35282;&#24230;-&#24310;&#36831;&#22495;&#31471;&#21475;&#65292;&#20174;&#32780;&#25552;&#39640; CSI &#30340;&#31934;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#34394;&#25311;&#22495;&#21644;&#23454;&#38469;&#22495;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#21453;&#39304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; CSI &#30340;&#21453;&#39304;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based channel state information (CSI) feedback in frequency division duplex systems has drawn widespread attention in both academia and industry. In this paper, we focus on integrating the Type-II codebook in the wireless communication standards with deep learning to enhance the performance of CSI feedback. In contrast to the existing deep learning based studies on the Release 16 Type-II codebook, the Type-II codebook in Release 17 (R17) exploits the angular-delay-domain partial reciprocity between uplink and downlink channels to select part of angular-delay-domain ports for measuring and feeding back the downlink CSI, where the performance of deep learning based conventional methods is limited due to the deficiency of sparse structures. To address this issue, we propose two new perspectives of adopting deep learning to improve the R17 Type-II codebook. Firstly, considering the low signal-to-noise ratio of uplink channels, deep learning is utilized to accurately select th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;OffCEM&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#23545;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12290;&#35813;&#20272;&#35745;&#22120;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#20445;&#25345;&#26080;&#20559;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08062</link><description>&lt;p&gt;
&#22522;&#20110;&#36830;&#35789;&#25928;&#24212;&#24314;&#27169;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling. (arXiv:2305.08062v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;OffCEM&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#23545;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12290;&#35813;&#20272;&#35745;&#22120;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#20445;&#25345;&#26080;&#20559;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23545;&#20110;&#20256;&#32479;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#26041;&#24040;&#30340;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#26041;&#24040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;OffCEM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36830;&#35789;&#25928;&#24212;&#27169;&#22411;&#65288;CEM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25928;&#24212;&#20998;&#20026;&#32676;&#38598;&#25928;&#24212;&#21644;&#27531;&#24046;&#25928;&#24212;&#12290;OffCEM&#20165;&#23545;&#34892;&#21160;&#32676;&#38598;&#24212;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#65292;&#35813;&#20272;&#35745;&#22120;&#26159;&#26080;&#20559;&#30340;&#65292;&#35813;&#26465;&#20214;&#20165;&#35201;&#27714;&#27531;&#24046;&#25928;&#24212;&#27169;&#22411;&#20445;&#30041;&#27599;&#20010;&#32676;&#38598;&#20013;&#34892;&#21160;&#30340;&#30456;&#23545;&#26399;&#26395;&#22870;&#21169;&#24046;&#24322;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;CEM&#21644;&#26412;&#22320;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#36807;&#31243;&#65292;&#29992;&#20110;&#25191;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#20272;&#35745;&#65292;&#31532;&#19968;&#27493;&#26368;&#23567;&#21270;&#20559;&#24046;&#65292;&#31532;&#20108;&#27493;&#26368;&#23567;&#21270;&#26041;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#24471;&#21040;&#30340;OPE&#20272;&#35745;&#22120;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#37117;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study off-policy evaluation (OPE) of contextual bandit policies for large discrete action spaces where conventional importance-weighting approaches suffer from excessive variance. To circumvent this variance issue, we propose a new estimator, called OffCEM, that is based on the conjunct effect model (CEM), a novel decomposition of the causal effect into a cluster effect and a residual effect. OffCEM applies importance weighting only to action clusters and addresses the residual causal effect through model-based reward estimation. We show that the proposed estimator is unbiased under a new condition, called local correctness, which only requires that the residual-effect model preserves the relative expected reward differences of the actions within each cluster. To best leverage the CEM and local correctness, we also propose a new two-step procedure for performing model-based estimation that minimizes bias in the first step and variance in the second step. We find that the resulting O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#22810;&#20010;&#36890;&#29992;&#20223;&#30495;&#22120;&#65292;&#24378;&#21270;&#20102;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08060</link><description>&lt;p&gt;
&#20004;&#20010;&#20248;&#20110;&#19968;&#20010;&#65306;&#25968;&#23383;&#23402;&#29983;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Two is Better Than One: Digital Siblings to Improve Autonomous Driving Testing. (arXiv:2305.08060v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#22810;&#20010;&#36890;&#29992;&#20223;&#30495;&#22120;&#65292;&#24378;&#21270;&#20102;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#26159;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#23454;&#38469;&#20013;&#65292;&#24403;&#20225;&#19994;&#20381;&#36182;&#31532;&#19977;&#26041;&#36890;&#29992;&#20223;&#30495;&#22120;&#36827;&#34892;&#20869;&#37096;&#25110;&#22806;&#21253;&#27979;&#35797;&#26102;&#65292;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21463;&#21040;&#23041;&#32961;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#30340;&#27010;&#24565;&#21152;&#24378;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;AV&#22312;&#22810;&#20010;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#30340;&#36890;&#29992;&#20223;&#30495;&#22120;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#27599;&#20010;&#21333;&#29420;&#30340;&#20223;&#30495;&#22120;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23558;&#27979;&#35797;&#36801;&#31227;&#33267;&#21508;&#20010;&#20223;&#30495;&#22120;&#20043;&#38388;&#65292;&#20197;&#34920;&#24449;&#25152;&#36827;&#34892;&#30340;&#34892;&#39542;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#35745;&#31639;&#32852;&#21512;&#39044;&#27979;&#22833;&#25928;&#27010;&#29575;&#65292;&#24182;&#20165;&#22312;&#23402;&#29983;&#20043;&#38388;&#36798;&#25104;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#25253;&#21578;&#25925;&#38556;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#24320;&#28304;&#20223;&#30495;&#22120;&#23454;&#29616;&#20102;&#35813;&#26694;&#26550;&#65292;&#24182;&#22312;&#25968;&#23383;&#23402;&#29983;&#30340;&#29289;&#29702;&#27604;&#20363;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based testing represents an important step to ensure the reliability of autonomous driving software. In practice, when companies rely on third-party general-purpose simulators, either for in-house or outsourced testing, the generalizability of testing results to real autonomous vehicles is at stake.  In this paper, we strengthen simulation-based testing by introducing the notion of digital siblings, a novel framework in which the AV is tested on multiple general-purpose simulators, built with different technologies. First, test cases are automatically generated for each individual simulator. Then, tests are migrated between simulators, using feature maps to characterize of the exercised driving conditions. Finally, the joint predicted failure probability is computed and a failure is reported only in cases of agreement among the siblings.  We implemented our framework using two open-source simulators and we empirically compared it against a digital twin of a physical scaled a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#35821;&#20041;&#24863;&#30693;&#21160;&#24577;&#22238;&#39038;-&#21069;&#30651;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#20351;&#29992;&#38382;&#39064;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#32467;&#26500;&#65292;&#26681;&#25454;&#38382;&#39064;SRL&#32467;&#26500;&#30340;&#21738;&#19968;&#37096;&#20998;&#34987;&#20851;&#27880;&#26469;&#20419;&#36827;&#36328;&#35270;&#39057;&#24103;&#30340;&#22797;&#26434;&#25512;&#29702;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2305.08059</link><description>&lt;p&gt;
&#20107;&#20214;&#32423;&#35270;&#39057;&#38382;&#31572;&#30340;&#35821;&#20041;&#24863;&#30693;&#21160;&#24577;&#22238;&#39038;-&#21069;&#30651;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware Dynamic Retrospective-Prospective Reasoning for Event-level Video Question Answering. (arXiv:2305.08059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#35821;&#20041;&#24863;&#30693;&#21160;&#24577;&#22238;&#39038;-&#21069;&#30651;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#20351;&#29992;&#38382;&#39064;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#32467;&#26500;&#65292;&#26681;&#25454;&#38382;&#39064;SRL&#32467;&#26500;&#30340;&#21738;&#19968;&#37096;&#20998;&#34987;&#20851;&#27880;&#26469;&#20419;&#36827;&#36328;&#35270;&#39057;&#24103;&#30340;&#22797;&#26434;&#25512;&#29702;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#32423;&#35270;&#39057;&#38382;&#31572;&#38656;&#35201;&#36328;&#36234;&#35270;&#39057;&#20107;&#20214;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#65292;&#20197;&#33719;&#21462;&#25552;&#20379;&#26368;&#20339;&#31572;&#26696;&#25152;&#38656;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#27169;&#22411;&#24615;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#22312;&#20107;&#20214;&#23618;&#38754;&#19978;&#20351;&#29992;&#38382;&#39064;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#26174;&#24335;&#35821;&#20041;&#32852;&#31995;&#12290;&#26377;&#24517;&#35201;&#21033;&#29992;&#36825;&#31181;&#35821;&#20041;&#32852;&#31995;&#65292;&#20419;&#36827;&#36328;&#35270;&#39057;&#24103;&#30340;&#22797;&#26434;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#38382;&#31572;&#30340;&#35821;&#20041;&#24863;&#30693;&#21160;&#24577;&#22238;&#39038;-&#21069;&#30651;&#25512;&#29702;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#21160;&#24577;&#25512;&#29702;&#36807;&#31243;&#20013;&#26126;&#30830;&#20351;&#29992;&#38382;&#39064;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#32467;&#26500;&#65292;&#26681;&#25454;&#38382;&#39064;SRL&#32467;&#26500;&#30340;&#21738;&#19968;&#37096;&#20998;&#65288;agent&#12289;verb&#12289;patient&#31561;&#65289;&#34987;&#20851;&#27880;&#26469;&#20915;&#23450;&#26159;&#21542;&#31227;&#21160;&#21040;&#19979;&#19968;&#24103;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;EVQA&#25968;&#25454;&#38598;&#8212;&#8212;TrafficQA&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the question and visual information especially at the event level. There is need for using such semantic connections to facilitate complex reasoning across video frames. Therefore, we propose a semantic-aware dynamic retrospective-prospective reasoning approach for video-based question answering. Specifically, we explicitly use the Semantic Role Labeling (SRL) structure of the question in the dynamic reasoning process where we decide to move to the next frame based on which part of the SRL structure (agent, verb, patient, etc.) of the question is being focused on. We conduct experiments on a benchmark EVQA dataset - TrafficQA. Results show that our proposed approach achieves superior performance 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LCEOPT&#30340;&#31616;&#21333;&#22312;&#32447;&#36830;&#32493;&#21160;&#20316;POMDP&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#31574;&#30053;&#26641;&#30340;&#25042;&#24816;&#20132;&#21449;&#29109;&#25628;&#32034;&#23454;&#29616;&#20102;&#39640;&#25928;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#24615;POMDP&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26368;&#20808;&#36827;&#30340;POMDP&#27714;&#35299;&#22120;&#21487;&#23454;&#29616;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2305.08049</link><description>&lt;p&gt;
&#20196;&#20154;&#24778;&#35766;&#30340;&#31616;&#21333;&#36830;&#32493;&#34892;&#21160;POMDP&#27714;&#35299;&#22120;&#65306;&#22522;&#20110;&#31574;&#30053;&#26641;&#30340;&#25042;&#24816;&#20132;&#21449;&#29109;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
A Surprisingly Simple Continuous-Action POMDP Solver: Lazy Cross-Entropy Search Over Policy Trees. (arXiv:2305.08049v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LCEOPT&#30340;&#31616;&#21333;&#22312;&#32447;&#36830;&#32493;&#21160;&#20316;POMDP&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#31574;&#30053;&#26641;&#30340;&#25042;&#24816;&#20132;&#21449;&#29109;&#25628;&#32034;&#23454;&#29616;&#20102;&#39640;&#25928;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#24615;POMDP&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26368;&#20808;&#36827;&#30340;POMDP&#27714;&#35299;&#22120;&#21487;&#23454;&#29616;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#25552;&#20379;&#20102;&#22312;&#38543;&#26426;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#20294;&#23545;&#20110;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#25552;&#20379;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Lazy Cross-Entropy Search Over Policy Trees (LCEOPT) &#30340;&#31616;&#21333;&#22312;&#32447;POMDP&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#35745;&#21010;&#27493;&#39588;&#20013;&#20351;&#29992;&#25042;&#24816;&#20132;&#21449;&#29109;&#26041;&#27861;&#26469;&#25628;&#32034;&#31574;&#30053;&#26641;&#31354;&#38388;&#65292;&#35813;&#26641;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32500;&#25252;&#19968;&#20010;&#20998;&#24067;&#22312;&#26377;&#21069;&#36884;&#30340;&#26377;&#38480;&#26102;&#38388;&#31574;&#30053;&#26641;&#19978;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#25277;&#26679;&#31574;&#30053;&#12289;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#35780;&#20272;&#23427;&#20204;&#24182;&#23558;&#23427;&#20204;&#37325;&#26032;&#25311;&#21512;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#31574;&#30053;&#19978;&#65292;&#36845;&#20195;&#26356;&#26032;&#27492;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#25042;&#24816;&#30340;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#31574;&#30053;&#26641;&#34920;&#31034;&#26469;&#36991;&#20813;&#31574;&#30053;&#25277;&#26679;&#12289;&#35780;&#20272;&#21644;&#20998;&#24067;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#35745;&#31639;&#12290;&#19982;&#20197;&#21069;&#38024;&#23545;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#30340;&#26368;&#20808;&#36827;&#30340;POMDP&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#36825;&#23548;&#33268;&#21487;&#33410;&#30465;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LCEOPT&#21487;&#20197;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#34892;&#21160;POMDP&#12290;
&lt;/p&gt;
&lt;p&gt;
The Partially Observable Markov Decision Process (POMDP) provides a principled framework for decision making in stochastic partially observable environments. However, computing good solutions for problems with continuous action spaces remains challenging. To ease this challenge, we propose a simple online POMDP solver, called Lazy Cross-Entropy Search Over Policy Trees (LCEOPT). At each planning step, our method uses a lazy Cross-Entropy method to search the space of policy trees, which provide a simple policy representation. Specifically, we maintain a distribution on promising finite-horizon policy trees. The distribution is iteratively updated by sampling policies, evaluating them via Monte Carlo simulation, and refitting them to the top-performing ones. Our method is lazy in the sense that it exploits the policy tree representation to avoid redundant computations in policy sampling, evaluation, and distribution update. This leads to computational savings of up to two orders of magn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#20102;&#24433;&#21709;&#27867;&#21270;&#38388;&#38553;&#30340;&#20307;&#31995;&#32467;&#26500;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.08048</link><description>&lt;p&gt;
&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Generalization of Graph Neural Networks. (arXiv:2305.08048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#20102;&#24433;&#21709;&#27867;&#21270;&#38388;&#38553;&#30340;&#20307;&#31995;&#32467;&#26500;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#21644;&#34920;&#31034;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#21151;&#65292;&#20294;&#20174;&#29702;&#35770;&#19978;&#29702;&#35299;&#23427;&#20204;&#30340;&#24037;&#20316;&#26426;&#21046;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#26412;&#25991;&#20174;&#27867;&#21270;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#39318;&#20808;&#32771;&#34385;&#20102;&#38543;&#26426;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#20256;&#36882;&#23398;&#20064;&#30340;&#27867;&#21270;&#38388;&#38553;&#21644;&#26799;&#24230;&#30340;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#20026;&#27969;&#34892;&#30340;GNN&#25552;&#20379;&#20102;&#27867;&#21270;&#38388;&#38553;&#30340;&#39640;&#27010;&#29575;&#19978;&#38480;&#12290;&#29702;&#35770;&#32467;&#26524;&#25581;&#31034;&#20102;&#24433;&#21709;&#27867;&#21270;&#38388;&#38553;&#30340;&#20307;&#31995;&#32467;&#26500;&#29305;&#23450;&#22240;&#32032;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#29702;&#35770;&#32467;&#26524;&#21644;&#32463;&#39564;&#35777;&#25454;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#29702;&#35299;GNN&#27867;&#21270;&#30340;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are the most widely adopted model in graph-structured data oriented learning and representation. Despite their extraordinary success in real-world applications, understanding their working mechanism by theory is still on primary stage. In this paper, we move towards this goal from the perspective of generalization. To be specific, we first establish high probability bounds of generalization gap and gradients in transductive learning with consideration of stochastic optimization. After that, we provide high probability bounds of generalization gap for popular GNNs. The theoretical results reveal the architecture specific factors affecting the generalization gap. Experimental results on benchmark datasets show the consistency between theoretical results and empirical evidence. Our results provide new insights in understanding the generalization of GNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08040</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#27744;&#21270;&#30340;&#21487;&#35777;&#26126;&#22810;&#23454;&#20363;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26032;&#22411;&#24212;&#29992;&#65292;&#29992;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#65292;&#20854;&#20013;&#23558;&#21333;&#20010;&#31867;&#26631;&#31614;&#20998;&#37197;&#32473;&#19968;&#32452;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#24739;&#32773;&#30340;&#22810;&#20010;CT&#25195;&#25551;&#30340;&#22810;&#20010;2D&#20999;&#29255;&#65289;&#12290;&#25105;&#20204;&#22312;DAM&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;MIL&#20013;&#34987;&#24573;&#30053;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21363;&#21253;&#22823;&#23567;&#36807;&#22823;&#65292;&#26080;&#27861;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#21152;&#36733;&#21040;GPU&#20869;&#23384;&#20013;&#65292;&#36825;&#26159;MIL&#26631;&#20934;&#27744;&#21270;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20851;&#20110;&#27719;&#32858;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#26500;&#36896;&#20026;&#22810;&#32423;&#32452;&#21512;&#20989;&#25968;&#12290;&#36890;&#36807;&#32508;&#21512;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#21644;&#38750;&#20984;&#26497;&#23567;&#26368;&#22823;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19988;&#21487;&#35777;&#26126;&#30340;&#22810;&#23454;&#20363;DAM&#65288;MIDAM&#65289;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#26368;&#22823;&#27744;&#21270;&#25110;&#38543;&#26426;&#27880;&#24847;&#21147;&#27744;&#21270;&#65292;&#20165;&#23545;&#27599;&#20010;&#21253;&#23545;&#24212;&#30340;&#23454;&#20363;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#26469;&#35745;&#31639; sto&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
&lt;/p&gt;</description></item><item><title>DNN-Defender&#26159;&#19968;&#31181;&#22522;&#20110;DRAM&#30340;&#21463;&#23475;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#37327;&#21270;DNN&#65292;&#21033;&#29992;&#20869;&#23384;&#20013;&#20132;&#25442;&#30340;&#28508;&#21147;&#20197;&#25269;&#24481;&#26377;&#38024;&#23545;&#24615;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.08034</link><description>&lt;p&gt;
DNN-Defender: &#19968;&#31181;&#29992;&#20110;&#23545;&#25239; Adversarial Weight Attack &#30340;&#20869;&#23384;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
DNN-Defender: An in-DRAM Deep Neural Network Defense Mechanism for Adversarial Weight Attack. (arXiv:2305.08034v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08034
&lt;/p&gt;
&lt;p&gt;
DNN-Defender&#26159;&#19968;&#31181;&#22522;&#20110;DRAM&#30340;&#21463;&#23475;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#37327;&#21270;DNN&#65292;&#21033;&#29992;&#20869;&#23384;&#20013;&#20132;&#25442;&#30340;&#28508;&#21147;&#20197;&#25269;&#24481;&#26377;&#38024;&#23545;&#24615;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#23433;&#20840;&#25935;&#24863;&#39046;&#22495;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;DRAM&#30340;RowHammer&#28431;&#27934;&#65292;&#20197;&#30830;&#23450;&#24615;&#21644;&#31934;&#30830;&#24615;&#22320;&#32763;&#36716;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#26435;&#37325;&#30340;&#20301;&#65292;&#20174;&#32780;&#24433;&#21709;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#26159;&#22522;&#20110;&#36719;&#20214;&#30340;&#65292;&#20363;&#22914;&#37325;&#26500;&#26435;&#37325;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#24320;&#38144;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#36890;&#29992;&#30828;&#20214;&#30340;&#21463;&#23475;&#32773;/&#25915;&#20987;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#20250;&#23548;&#33268;&#26114;&#36149;&#30340;&#30828;&#20214;&#24320;&#38144;&#65292;&#24182;&#20445;&#30041;&#21463;&#23475;&#32773;&#21644;&#25915;&#20987;&#32773;&#34892;&#20043;&#38388;&#30340;&#31354;&#38388;&#36830;&#25509;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#38024;&#23545;&#37327;&#21270;DNN&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;DRAM&#30340;&#21463;&#23475;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#65292;&#31216;&#20026;DNN-Defender&#65292;&#21033;&#29992;&#20102;&#20869;&#23384;&#20013;&#20132;&#25442;&#30340;&#28508;&#21147;&#20197;&#25269;&#24481;&#26377;&#38024;&#23545;&#24615;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DNN-Defender&#21487;&#20197;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
With deep learning deployed in many security-sensitive areas, machine learning security is becoming progressively important. Recent studies demonstrate attackers can exploit system-level techniques exploiting the RowHammer vulnerability of DRAM to deterministically and precisely flip bits in Deep Neural Networks (DNN) model weights to affect inference accuracy. The existing defense mechanisms are software-based, such as weight reconstruction requiring expensive training overhead or performance degradation. On the other hand, generic hardware-based victim-/aggressor-focused mechanisms impose expensive hardware overheads and preserve the spatial connection between victim and aggressor rows. In this paper, we present the first DRAM-based victim-focused defense mechanism tailored for quantized DNNs, named DNN-Defender that leverages the potential of in-DRAM swapping to withstand the targeted bit-flip attacks. Our results indicate that DNN-Defender can deliver a high level of protection dow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#24615;&#25193;&#25955;&#25216;&#26415;&#20316;&#20026;&#23545;&#25239;&#24615;&#20928;&#21270;&#22120;&#65292;&#20197;&#28040;&#38500;&#25915;&#20987;&#32773;&#22312;&#21407;&#22987;&#22270;&#20687;&#20013;&#24341;&#20837;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ViT&#27169;&#22411;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08031</link><description>&lt;p&gt;
&#25552;&#39640;&#35270;&#35273;Transformer&#30340;&#40065;&#26834;&#24615;&#65306;&#38450;&#24481;&#24615;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
On enhancing the robustness of Vision Transformers: Defensive Diffusion. (arXiv:2305.08031v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#24615;&#25193;&#25955;&#25216;&#26415;&#20316;&#20026;&#23545;&#25239;&#24615;&#20928;&#21270;&#22120;&#65292;&#20197;&#28040;&#38500;&#25915;&#20987;&#32773;&#22312;&#21407;&#22987;&#22270;&#20687;&#20013;&#24341;&#20837;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ViT&#27169;&#22411;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#20445;&#23494;&#24615;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;ViTs&#26159;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#26377;&#20851;&#25968;&#25454;&#23433;&#20840;&#21644;&#28508;&#22312;&#26410;&#32463;&#25480;&#26435;&#30340;&#35775;&#38382;&#30340;&#25285;&#24551;&#12290;&#25915;&#20987;&#32773;&#21487;&#33021;&#21033;&#29992;ViTs&#20013;&#30340;&#28431;&#27934;&#25552;&#21462;&#25935;&#24863;&#24739;&#32773;&#20449;&#24687;&#65292;&#20174;&#32780;&#21361;&#21450;&#24739;&#32773;&#38544;&#31169;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#28431;&#27934;&#65292;&#20197;&#30830;&#20445;ViTs&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#21487;&#20449;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38450;&#24481;&#24615;&#25193;&#25955;&#25216;&#26415;&#20316;&#20026;&#23545;&#25239;&#24615;&#20928;&#21270;&#22120;&#65292;&#20197;&#28040;&#38500;&#25915;&#20987;&#32773;&#22312;&#21407;&#22987;&#22270;&#20687;&#20013;&#24341;&#20837;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#33021;&#21147;&#65292;&#25105;&#20204;&#37319;&#29992;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#26377;&#25928;&#22320;&#28040;&#38500;&#25915;&#20987;&#31034;&#20363;&#20013;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#26356;&#24178;&#20928;&#30340;&#22270;&#20687;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;ViT&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#28040;&#38500;&#23545;&#25239;&#24615;&#25915;&#20987;&#25200;&#21160;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21307;&#30103;&#24212;&#29992;&#20013;ViT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy and confidentiality of medical data are of utmost importance in healthcare settings. ViTs, the SOTA vision model, rely on large amounts of patient data for training, which raises concerns about data security and the potential for unauthorized access. Adversaries may exploit vulnerabilities in ViTs to extract sensitive patient information and compromising patient privacy. This work address these vulnerabilities to ensure the trustworthiness and reliability of ViTs in medical applications. In this work, we introduced a defensive diffusion technique as an adversarial purifier to eliminate adversarial noise introduced by attackers in the original image. By utilizing the denoising capabilities of the diffusion model, we employ a reverse diffusion process to effectively eliminate the adversarial noise from the attack sample, resulting in a cleaner image that is then fed into the ViT blocks. Our findings demonstrate the effectiveness of the diffusion model in eliminating attack-agnost
&lt;/p&gt;</description></item><item><title>SongDriver2&#23454;&#29616;&#20102;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#65292;&#24182;&#25552;&#20986;&#20102;&#26580;&#21644;&#36807;&#28193;&#26426;&#21046;&#65292;&#20351;&#38899;&#20048;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#12290;</title><link>http://arxiv.org/abs/2305.08029</link><description>&lt;p&gt;
SongDriver2&#65306;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#19982;&#26580;&#21644;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
SongDriver2: Real-time Emotion-based Music Arrangement with Soft Transition. (arXiv:2305.08029v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08029
&lt;/p&gt;
&lt;p&gt;
SongDriver2&#23454;&#29616;&#20102;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#65292;&#24182;&#25552;&#20986;&#20102;&#26580;&#21644;&#36807;&#28193;&#26426;&#21046;&#65292;&#20351;&#38899;&#20048;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#26088;&#22312;&#23558;&#32473;&#23450;&#30340;&#38899;&#20048;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#33021;&#22815;&#23454;&#26102;&#24341;&#36215;&#29992;&#25143;&#29305;&#23450;&#24773;&#24863;&#20849;&#40483;&#30340;&#38899;&#20048;&#65292;&#22312;&#38899;&#20048;&#30103;&#27861;&#12289;&#28216;&#25103;&#37197;&#20048;&#21644;&#30005;&#24433;&#37197;&#20048;&#31561;&#21508;&#31181;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#26631;&#24773;&#24863;&#30340;&#32454;&#31890;&#24230;&#21644;&#21487;&#21464;&#24615;&#65292;&#24179;&#34913;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#21644;&#26580;&#21644;&#24773;&#24863;&#36716;&#25442;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#29616;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#65292;&#32780;&#26580;&#21644;&#36807;&#28193;&#30340;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#24433;&#21709;&#20102;&#38899;&#20048;&#30340;&#25972;&#20307;&#24773;&#24863;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SongDriver2&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#26368;&#21518;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#38899;&#20048;&#24773;&#32490;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#24403;&#21069;&#26102;&#38388;&#27493;&#30340;&#30446;&#26631;&#36755;&#20837;&#24773;&#32490;&#34701;&#21512;&#12290;&#34701;&#21512;&#30340;&#24773;&#24863;&#38543;&#21518;&#20316;&#20026;SongDriver2&#26681;&#25454;&#36755;&#20837;&#26059;&#24459;&#25968;&#25454;&#29983;&#25104;&#21363;&#23558;&#21040;&#26469;&#30340;&#38899;&#20048;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#35843;&#25972;&#38899;&#20048;&#30456;&#20284;&#24615;&#21644;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#65292;&#20197;&#23454;&#29616;&#20004;&#31181;&#19981;&#21516;&#24773;&#24863;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36719;&#36807;&#28193;&#26426;&#21046;&#65292;&#23558;&#25554;&#20540;&#21644;&#24179;&#28369;&#28388;&#27874;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;SongDriver2&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#30340;&#24773;&#24863;&#38899;&#20048;&#65292;&#36825;&#34920;&#26126;&#20854;&#22312;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time emotion-based music arrangement, which aims to transform a given music piece into another one that evokes specific emotional resonance with the user in real-time, holds significant application value in various scenarios, e.g., music therapy, video game soundtracks, and movie scores. However, balancing emotion real-time fit with soft emotion transition is a challenge due to the fine-grained and mutable nature of the target emotion. Existing studies mainly focus on achieving emotion real-time fit, while the issue of soft transition remains understudied, affecting the overall emotional coherence of the music. In this paper, we propose SongDriver2 to address this balance. Specifically, we first recognize the last timestep's music emotion and then fuse it with the current timestep's target input emotion. The fused emotion then serves as the guidance for SongDriver2 to generate the upcoming music based on the input melody data. To adjust music similarity and emotion real-time fit f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#32467;&#26500;&#30340;&#26694;&#26550;&#65292;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#21160;&#24577;&#37325;&#36830;&#26469;&#30830;&#20445;&#36880;&#28176;&#23494;&#38598;&#21270;&#30340;&#22270;&#24418;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#20801;&#35768;&#36328;&#23618;&#33410;&#28857;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2305.08018</link><description>&lt;p&gt;
DRew&#65306;&#24102;&#24310;&#36831;&#30340;&#21160;&#24577;&#37325;&#36830;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
DRew: Dynamically Rewired Message Passing with Delay. (arXiv:2305.08018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#32467;&#26500;&#30340;&#26694;&#26550;&#65292;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#21160;&#24577;&#37325;&#36830;&#26469;&#30830;&#20445;&#36880;&#28176;&#23494;&#38598;&#21270;&#30340;&#22270;&#24418;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#20801;&#35768;&#36328;&#23618;&#33410;&#28857;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#23384;&#22312;&#36807;&#24230;&#21387;&#32553;&#29616;&#35937;&#65292;&#23548;&#33268;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#21482;&#22312;&#33410;&#28857;&#30340;&#30456;&#37051;&#23621;&#20043;&#38388;&#36827;&#34892;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#12290;&#35797;&#22270;&#20351;&#22270;&#24418;&#8220;&#26356;&#36830;&#36890;&#8221;&#24182;&#19988;&#26356;&#36866;&#21512;&#38271;&#31243;&#20219;&#21153;&#30340;&#37325;&#36830;&#26041;&#27861;&#36890;&#24120;&#20250;&#22833;&#21435;&#22522;&#20110;&#22270;&#24418;&#36317;&#31163;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20351;&#36828;&#31243;&#33410;&#28857;&#22312;&#27599;&#19968;&#23618;&#20013;&#31435;&#21363;&#36890;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#26550;&#26500;&#65292;&#20197;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#37325;&#36830;&#65292;&#20197;&#30830;&#20445;&#36880;&#28176;&#21152;&#23494;&#22270;&#24418;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#23618;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#36317;&#31163;&#22312;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#36339;&#36291;&#36830;&#25509;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#38271;&#31243;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#20854;&#20248;&#20110;&#22270;&#24418;&#21464;&#25442;&#22120;&#21644;&#22810;&#36339;MPNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing neural networks (MPNNs) have been shown to suffer from the phenomenon of over-squashing that causes poor performance for tasks relying on long-range interactions. This can be largely attributed to message passing only occurring locally, over a node's immediate neighbours. Rewiring approaches attempting to make graphs `more connected', and supposedly better suited to long-range tasks, often lose the inductive bias provided by distance on the graph since they make distant nodes communicate instantly at every layer. In this paper we propose a framework, applicable to any MPNN architecture, that performs a layer-dependent rewiring to ensure gradual densification of the graph. We also propose a delay mechanism that permits skip connections between nodes depending on the layer and their mutual distance. We validate our approach on several long-range tasks and show that it outperforms graph Transformers and multi-hop MPNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#30340;&#22810;&#31181;&#23433;&#20840;&#39118;&#38505;&#21644;&#32469;&#36807;&#20445;&#25252;&#25514;&#26045;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#21457;&#29616;&#21363;&#20351;&#20445;&#25252;&#25514;&#26045;&#23384;&#22312;&#65292;LLMs&#20173;&#23384;&#22312;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#20026;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#25552;&#20379;&#20102;&#28508;&#22312;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.08005</link><description>&lt;p&gt;
&#36229;&#36234;&#20445;&#38556;&#65306;&#25506;&#32034;ChatGPT&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Beyond the Safeguards: Exploring the Security Risks of ChatGPT. (arXiv:2305.08005v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#30340;&#22810;&#31181;&#23433;&#20840;&#39118;&#38505;&#21644;&#32469;&#36807;&#20445;&#25252;&#25514;&#26045;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#21457;&#29616;&#21363;&#20351;&#20445;&#25252;&#25514;&#26045;&#23384;&#22312;&#65292;LLMs&#20173;&#23384;&#22312;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#20026;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#25552;&#20379;&#20102;&#28508;&#22312;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#23433;&#20840;&#24615;&#12289;&#23433;&#20840;&#39118;&#38505;&#21644;&#20262;&#29702;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;ChatGPT&#30456;&#20851;&#30340;&#19981;&#21516;&#31867;&#22411;&#23433;&#20840;&#39118;&#38505;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#24694;&#24847;&#25991;&#26412;&#21644;&#20195;&#30721;&#29983;&#25104;&#12289;&#31169;&#20154;&#25968;&#25454;&#25259;&#38706;&#12289;&#27450;&#35784;&#24615;&#26381;&#21153;&#12289;&#20449;&#24687;&#25628;&#38598;&#21644;&#29983;&#25104;&#19981;&#36947;&#24503;&#20869;&#23481;&#31561;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#26816;&#26597;&#20102;ChatGPT&#20869;&#23481;&#36807;&#28388;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#32469;&#36807;&#36825;&#20123;&#20445;&#25252;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#24403;&#20445;&#25252;&#25514;&#26045;&#23384;&#22312;&#26102;LLMs&#20013;&#20173;&#23384;&#22312;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26681;&#25454;&#23433;&#20840;&#39118;&#38505;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#28508;&#22312;&#31574;&#30053;&#65292;&#24182;&#21521;&#30740;&#31350;&#20154;&#21592;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#34892;&#19994;&#19987;&#19994;&#20154;&#21592;&#20171;&#32461;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;LLMs&#25152;&#38754;&#20020;&#30340;&#22797;&#26434;&#23433;&#20840;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26377;&#21161;&#20110;&#23545;LLMs&#24102;&#26469;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#36827;&#34892;&#25345;&#32493;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of large language models (LLMs) such as ChatGPT has led to growing concerns about their safety, security risks, and ethical implications. This paper aims to provide an overview of the different types of security risks associated with ChatGPT, including malicious text and code generation, private data disclosure, fraudulent services, information gathering, and producing unethical content. We present an empirical study examining the effectiveness of ChatGPT's content filters and explore potential ways to bypass these safeguards, demonstrating the ethical implications and security risks that persist in LLMs even when protections are in place. Based on a qualitative analysis of the security implications, we discuss potential strategies to mitigate these risks and inform researchers, policymakers, and industry professionals about the complex security challenges posed by LLMs like ChatGPT. This study contributes to the ongoing discussion on the ethical and security 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#38271;&#26399;&#25345;&#32493;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26159;&#21542;&#33021;&#25552;&#39640;&#33021;&#37327;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#39044;&#31639;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07973</link><description>&lt;p&gt;
&#35770;&#38543;&#26426;&#23433;&#20840;&#24615;&#30340;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
On the Computational Cost of Stochastic Security. (arXiv:2305.07973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#38271;&#26399;&#25345;&#32493;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26159;&#21542;&#33021;&#25552;&#39640;&#33021;&#37327;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#39044;&#31639;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30340;&#38271;&#26399;&#25345;&#32493;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26159;&#21542;&#20250;&#25552;&#39640;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#25152;&#36798;&#21040;&#30340;&#34920;&#24449;&#36136;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#26041;&#26696;&#65292;&#20854;&#20013;&#20351;&#29992;&#35757;&#32451;&#36807;&#30340;EBM&#30340;&#25193;&#25955;&#36807;&#31243;&#30340;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#29992;&#20110;&#25552;&#39640;&#29420;&#31435;&#20998;&#31867;&#22120;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25345;&#32493;&#23545;&#27604;&#25955;&#24230;&#30340;&#35745;&#31639;&#39044;&#31639;&#22686;&#21152;&#21513;&#24067;&#26031;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#28548;&#28165;&#20102;&#23454;&#29616;&#26377;&#25928;&#20174;&#36830;&#32493;&#33021;&#37327;&#21183;&#20013;&#36827;&#34892;&#21513;&#24067;&#26031;&#37319;&#26679;&#30340;&#26032;&#37327;&#23376;&#21644;&#32463;&#20856;&#30828;&#20214;&#21644;&#36719;&#20214;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate whether long-run persistent chain Monte Carlo simulation of Langevin dynamics improves the quality of the representations achieved by energy-based models (EBM). We consider a scheme wherein Monte Carlo simulation of a diffusion process using a trained EBM is used to improve the adversarial robustness and the calibration score of an independent classifier network. Our results show that increasing the computational budget of Gibbs sampling in persistent contrastive divergence improves the calibration and adversarial robustness of the model, elucidating the practical merit of realizing new quantum and classical hardware and software for efficient Gibbs sampling from continuous energy potentials.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#21487;&#20197;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#36866;&#24403;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#21306;&#20998;&#32454;&#24494;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#27700;&#24179;&#26041;&#38754;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#20026;&#20351;&#29992;LLMs&#22312;&#20154;&#31867;&#20915;&#31574;&#21046;&#23450;&#32972;&#26223;&#19979;&#30340;&#20262;&#29702;&#24847;&#20041;&#21644;&#23616;&#38480;&#24615;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.07970</link><description>&lt;p&gt;
&#21033;&#29992;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#31867;&#20284;&#30446;&#26631;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics. (arXiv:2305.07970v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#21487;&#20197;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#36866;&#24403;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#21306;&#20998;&#32454;&#24494;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#27700;&#24179;&#26041;&#38754;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#20026;&#20351;&#29992;LLMs&#22312;&#20154;&#31867;&#20915;&#31574;&#21046;&#23450;&#32972;&#26223;&#19979;&#30340;&#20262;&#29702;&#24847;&#20041;&#21644;&#23616;&#38480;&#24615;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;GPT-3.5&#65292;&#23454;&#29616;&#21512;&#20316;&#12289;&#31454;&#20105;&#12289;&#21033;&#20182;&#21644;&#33258;&#31169;&#34892;&#20026;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#22312;&#31038;&#20250;&#22256;&#22659;&#19979;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#36845;&#20195;&#22234;&#24466;&#22256;&#22659;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#38646;&#21644;&#20114;&#21160;&#30340;&#32463;&#20856;&#20363;&#23376;&#65292;&#20294;&#25105;&#20204;&#30340;&#26356;&#24191;&#27867;&#30740;&#31350;&#35745;&#21010;&#21253;&#25324;&#19968;&#31995;&#21015;&#23454;&#39564;&#32463;&#27982;&#23398;&#22330;&#26223;&#65292;&#21253;&#25324;&#26368;&#21518;&#36890;&#29266;&#21338;&#24328;&#12289;&#29420;&#35009;&#32773;&#21338;&#24328;&#21644;&#20844;&#20849;&#29289;&#21697;&#28216;&#25103;&#12290;&#20351;&#29992;&#34987;&#35797;&#20869;&#23454;&#39564;&#35774;&#35745;&#65292;&#25105;&#20204;&#36816;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#20449;&#24687;&#23454;&#20363;&#21270;&#30001;LLM&#29983;&#25104;&#30340;&#26234;&#33021;&#20307;&#65292;&#34920;&#36798;&#19981;&#21516;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#31435;&#22330;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26234;&#33021;&#20307;&#22312;&#36845;&#20195;&#22234;&#24466;&#22256;&#22659;&#20013;&#30340;&#21512;&#20316;&#27700;&#24179;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23427;&#20204;&#23545;&#21512;&#20316;&#25110;&#20986;&#23572;&#21453;&#23572;&#30340;&#20249;&#20276;&#34892;&#21160;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23558;&#21033;&#20182;&#21644;&#33258;&#31169;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#36866;&#24403;&#30340;&#34892;&#20026;&#65292;&#20294;&#23637;&#31034;&#20986;&#21306;&#20998;&#21512;&#20316;&#21644;&#31454;&#20105;&#27700;&#24179;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#20154;&#31867;&#20915;&#31574;&#21046;&#23450;&#30340;&#32972;&#26223;&#19979;&#20351;&#29992;LLMs&#30340;&#20262;&#29702;&#24847;&#20041;&#21644;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate the capacity of large language models (LLMs), specifically GPT-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of a non-zero-sum interaction, but our broader research program encompasses a range of experimental economics scenarios, including the ultimatum game, dictator game, and public goods game. Using a within-subject experimental design, we instantiated LLM-generated agents with various prompts that conveyed different cooperative and competitive stances. We then assessed the agents' level of cooperation in the iterated Prisoner's Dilemma, taking into account their responsiveness to the cooperative or defection actions of their partners. Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#20445;&#35777;&#23433;&#20840;&#31574;&#30053;&#20248;&#21270;(SPI)&#30340;&#24615;&#33021;&#65292;&#24182;&#38477;&#20302;&#20102;SPIBB&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.07958</link><description>&lt;p&gt;
&#26356;&#23569;&#30340;&#25968;&#25454;&#26356;&#24378;&#30340;&#23433;&#20840;&#31574;&#30053;&#20248;&#21270;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
More for Less: Safe Policy Improvement With Stronger Performance Guarantees. (arXiv:2305.07958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#20445;&#35777;&#23433;&#20840;&#31574;&#30053;&#20248;&#21270;(SPI)&#30340;&#24615;&#33021;&#65292;&#24182;&#38477;&#20302;&#20102;SPIBB&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#23433;&#20840;&#31574;&#30053;&#20248;&#21270;(SPI)&#38382;&#39064;&#26088;&#22312;&#26681;&#25454;&#29983;&#25104;&#26679;&#26412;&#25968;&#25454;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;SPI&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#36739;&#39640;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#25552;&#20379;&#23545;&#25913;&#36827;&#31574;&#30053;&#24615;&#33021;&#30340;&#23454;&#38469;&#27010;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;SPI&#38382;&#39064;&#65292;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#21363;&#21487;&#33719;&#24471;&#36825;&#26679;&#30340;&#20445;&#35777;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#35777;&#26126;&#36825;&#20123;&#20445;&#35777;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#21644;&#22522;&#30784;&#29615;&#22659;&#27169;&#22411;&#19978;&#30340;&#38544;&#24335;&#36716;&#25442;&#65292;&#36825;&#20123;&#36716;&#25442;&#20316;&#20026;&#25512;&#23548;&#26356;&#32039;&#23494;&#30340;SPI&#25913;&#36827;&#30028;&#38480;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#20351;&#29992;&#19987;&#19994;&#30340;SPI&#19982;&#22522;&#32447;&#24341;&#23548;(SPIBB)&#31639;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#23454;&#26174;&#33879;&#38477;&#20302;&#20102;SPIBB&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an offline reinforcement learning setting, the safe policy improvement (SPI) problem aims to improve the performance of a behavior policy according to which sample data has been generated. State-of-the-art approaches to SPI require a high number of samples to provide practical probabilistic guarantees on the improved policy's performance. We present a novel approach to the SPI problem that provides the means to require less data for such guarantees. Specifically, to prove the correctness of these guarantees, we devise implicit transformations on the data set and the underlying environment model that serve as theoretical foundations to derive tighter improvement bounds for SPI. Our empirical evaluation, using the well-established SPI with baseline bootstrapping (SPIBB) algorithm, on standard benchmarks shows that our method indeed significantly reduces the sample complexity of the SPIBB algorithm.
&lt;/p&gt;</description></item><item><title>AMTSS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#25945;&#24072;&#21333;&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#25903;&#25345;&#25104;&#26412;&#25928;&#30410;&#30340;&#35821;&#35328;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;XNLI&#25968;&#25454;&#38598;&#21644;AliExpress&#20013;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07928</link><description>&lt;p&gt;
AMTSS&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#35328;&#25512;&#29702;&#30340;&#33258;&#36866;&#24212;&#22810;&#25945;&#24072;&#21333;&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AMTSS: An Adaptive Multi-Teacher Single-Student Knowledge Distillation Framework For Multilingual Language Inference. (arXiv:2305.07928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07928
&lt;/p&gt;
&lt;p&gt;
AMTSS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#25945;&#24072;&#21333;&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#25903;&#25345;&#25104;&#26412;&#25928;&#30410;&#30340;&#35821;&#35328;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;XNLI&#25968;&#25454;&#38598;&#21644;AliExpress&#20013;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#23545;&#20110;&#25512;&#20986;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#25903;&#25345;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#35821;&#35328;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMTSS&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#25945;&#24072;&#21333;&#23398;&#29983;&#33976;&#39311;&#26694;&#26550;&#65292;&#23427;&#20801;&#35768;&#20174;&#22810;&#20010;&#32769;&#24072;&#20013;&#25552;&#28860;&#30693;&#35782;&#21040;&#19968;&#20010;&#23398;&#29983;&#20013;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#31574;&#30053;&#21644;&#25945;&#24072;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#26368;&#22823;&#36793;&#32536;&#32769;&#24072;&#20013;&#23398;&#20064;&#65292;&#24182;&#36731;&#26494;&#36866;&#24212;&#26032;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20849;&#20139;&#30340;&#23398;&#29983;&#32534;&#30721;&#22120;&#65292;&#20197;&#19981;&#21516;&#30340;&#25237;&#24433;&#23618;&#25903;&#25345;&#22810;&#31181;&#35821;&#35328;&#65292;&#36825;&#26377;&#21161;&#20110;&#22823;&#22823;&#38477;&#20302;&#24320;&#21457;&#21644;&#26426;&#22120;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;AMTSS&#22312;&#20844;&#20849;XNLI&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#30340;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#25968;&#25454;&#38598;AliExpress&#65288;AE&#65289;&#20013;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is of key importance to launching multilingual pre-trained language models for real applications. To support cost-effective language inference in multilingual settings, we propose AMTSS, an adaptive multi-teacher single-student distillation framework, which allows distilling knowledge from multiple teachers to a single student. We first introduce an adaptive learning strategy and teacher importance weight, which enables a student to effectively learn from max-margin teachers and easily adapt to new languages. Moreover, we present a shared student encoder with different projection layers in support of multiple languages, which contributes to largely reducing development and machine cost. Experimental results show that AMTSS gains competitive results on the public XNLI dataset and the realistic industrial dataset AliExpress (AE) in the E-commerce scenario.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#36890;&#36807;&#36974;&#30422;&#31574;&#30053;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.07912</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion. (arXiv:2305.07912v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07912
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#36890;&#36807;&#36974;&#30422;&#31574;&#30053;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;TKGC&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22312;&#24050;&#30693;&#30340;&#26102;&#38388;&#25139;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#32570;&#22833;&#37096;&#20998;&#30340;&#20107;&#23454;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#21516;&#26102;&#31895;&#30053;&#22320;&#25552;&#21462;&#26102;&#38388;&#25139;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#19981;&#20805;&#20998;&#21033;&#29992;&#20851;&#31995;&#20013;&#38544;&#21547;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;TKGC&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;&#37319;&#26679;&#30340;&#22235;&#20803;&#32452;&#36716;&#25442;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#24182;&#23558;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#38388;&#38548;&#36716;&#25442;&#20026;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#20197;&#24418;&#25104;&#24102;&#26377;&#38544;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#36830;&#36143;&#21477;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#36974;&#30422;&#31574;&#30053;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#24191;&#27867;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;SUMO-K&#32763;&#35793;&#25104;&#39640;&#38454;&#38598;&#21512;&#35770;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#36229;&#36234;&#19968;&#38454;&#36923;&#36753;&#30340;SUMO&#37096;&#20998;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#35821;&#20041;&#12290;&#21516;&#26102;&#20063;&#23884;&#20837;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#24120;&#35782;&#26412;&#20307;&#35770;&#21040;&#19968;&#20010;&#23433;&#20840;&#30340;&#20114;&#21160;&#23450;&#29702;&#35777;&#26126;&#31995;&#32479;&#20013;&#65292;&#36825;&#21487;&#29992;&#20110;&#21019;&#24314;&#19968;&#20123;&#39640;&#38454;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07903</link><description>&lt;p&gt;
&#23558;SUMO-K&#32763;&#35793;&#33267;&#39640;&#38454;&#38598;&#21512;&#35770;
&lt;/p&gt;
&lt;p&gt;
Translating SUMO-K to Higher-Order Set Theory. (arXiv:2305.07903v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;SUMO-K&#32763;&#35793;&#25104;&#39640;&#38454;&#38598;&#21512;&#35770;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#36229;&#36234;&#19968;&#38454;&#36923;&#36753;&#30340;SUMO&#37096;&#20998;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#35821;&#20041;&#12290;&#21516;&#26102;&#20063;&#23884;&#20837;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#24120;&#35782;&#26412;&#20307;&#35770;&#21040;&#19968;&#20010;&#23433;&#20840;&#30340;&#20114;&#21160;&#23450;&#29702;&#35777;&#26126;&#31995;&#32479;&#20013;&#65292;&#36825;&#21487;&#29992;&#20110;&#21019;&#24314;&#19968;&#20123;&#39640;&#38454;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;SUMO&#30340;&#19968;&#20010;&#23376;&#38598;&#65288;SUMO-K&#65289;&#32763;&#35793;&#25104;&#39640;&#38454;&#38598;&#21512;&#35770;&#12290;&#35813;&#32763;&#35793;&#20026;&#36229;&#36234;&#19968;&#38454;&#36923;&#36753;&#30340;SUMO&#37096;&#20998;&#25552;&#20379;&#20102;&#24418;&#24335;&#35821;&#20041;&#65292;&#36825;&#20123;&#37096;&#20998;&#20197;&#21069;&#21482;&#26377;&#38750;&#27491;&#24335;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#23427;&#36824;&#39318;&#27425;&#23558;&#19968;&#20010;&#22823;&#22411;&#30340;&#24120;&#35782;&#26412;&#20307;&#35770;&#23884;&#20837;&#19968;&#20010;&#38750;&#24120;&#23433;&#20840;&#30340;&#20114;&#21160;&#23450;&#29702;&#35777;&#26126;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#20174;&#19968;&#38454;&#32467;&#26500;&#20013;&#25214;&#21040;SUMO&#20013;&#30340;&#30683;&#30462;&#65292;&#23558;&#20854;&#21253;&#25324;&#39640;&#38454;&#32467;&#26500;&#30340;&#19968;&#37096;&#20998;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#36825;&#20010;&#32763;&#35793;&#65292;&#25105;&#20204;&#21487;&#20197;&#21019;&#24314;&#19968;&#20123;&#21487;&#20351;&#29992;&#39640;&#38454;&#20114;&#21160;&#24335;&#21644;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#35777;&#26126;&#30340;&#38382;&#39064;&#12290;&#36825;&#22312;&#20960;&#20010;&#31995;&#32479;&#20013;&#24471;&#21040;&#27979;&#35797;&#65292;&#24182;&#21487;&#29992;&#20110;&#24418;&#25104;&#19968;&#20010;&#39640;&#38454;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a translation from a fragment of SUMO (SUMO-K) into higher-order set theory. The translation provides a formal semantics for portions of SUMO which are beyond first-order and which have previously only had an informal interpretation. It also for the first time embeds a large common-sense ontology into a very secure interactive theorem proving system. We further extend our previous work in finding contradictions in SUMO from first order constructs to include a portion of SUMO's higher order constructs. Finally, using the translation, we can create problems that can be proven using higher-order interactive and automated theorem provers. This is tested in several systems and can be used to form a corpus of higher-order common-sense reasoning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;PESTS&#65292;&#24182;&#36890;&#36807;&#27874;&#26031;&#35821;-&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07893</link><description>&lt;p&gt;
PESTS: &#27874;&#26031;&#35821;-&#33521;&#35821;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity. (arXiv:2305.07893v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;PESTS&#65292;&#24182;&#36890;&#36807;&#27874;&#26031;&#35821;-&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22791;&#21463;&#20851;&#27880;&#30340;&#32452;&#20214;&#12290;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#35780;&#20272;&#21333;&#35789;&#12289;&#30701;&#35821;&#12289;&#27573;&#33853;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24456;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#35201;&#27714;&#22312;&#28304;&#21644;&#30446;&#26631;&#35821;&#35328;&#20013;&#25552;&#20379;&#20855;&#26377;&#19968;&#23450;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23545;&#12290;&#35768;&#22810;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#27169;&#22411;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#26469;&#24357;&#34917;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#19981;&#21487;&#29992;&#30340;&#19981;&#36275;&#65292;&#20294;&#26426;&#22120;&#32763;&#35793;&#30340;&#35823;&#24046;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#29305;&#24449;&#23454;&#29616;&#26426;&#22120;&#32763;&#35793;&#26102;&#65292;&#29992;&#30456;&#21516;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the components of natural language processing that has received a lot of investigation recently is semantic textual similarity. In computational linguistics and natural language processing, assessing the semantic similarity of words, phrases, paragraphs, and texts is crucial. Calculating the degree of semantic resemblance between two textual pieces, paragraphs, or phrases provided in both monolingual and cross-lingual versions is known as semantic similarity. Cross lingual semantic similarity requires corpora in which there are sentence pairs in both the source and target languages with a degree of semantic similarity between them. Many existing cross lingual semantic similarity models use a machine translation due to the unavailability of cross lingual semantic similarity dataset, which the propagation of the machine translation error reduces the accuracy of the model. On the other hand, when we want to use semantic similarity features for machine translation the same machine t
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#25552;&#39640;&#20803;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#30693;&#35782;&#20449;&#24687;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36866;&#24403;&#30340;MR&#30446;&#26631;&#23558;&#20803;&#30693;&#35782;&#21021;&#27493;&#38598;&#25104;&#21040;&#20803;&#30446;&#26631;&#20013;&#65292;&#26469;&#35268;&#33539;&#20803;&#27169;&#22411;&#20989;&#25968;&#31867;&#30340;&#23481;&#37327;&#22797;&#26434;&#24230;&#65292;&#26377;&#21161;&#20110;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07892</link><description>&lt;p&gt;
DAC-MR: &#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#19968;&#33268;&#24615;&#30340;&#20803;&#23398;&#20064;&#20803;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning. (arXiv:2305.07892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07892
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#20803;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#30693;&#35782;&#20449;&#24687;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36866;&#24403;&#30340;MR&#30446;&#26631;&#23558;&#20803;&#30693;&#35782;&#21021;&#27493;&#38598;&#25104;&#21040;&#20803;&#30446;&#26631;&#20013;&#65292;&#26469;&#35268;&#33539;&#20803;&#27169;&#22411;&#20989;&#25968;&#31867;&#30340;&#23481;&#37327;&#22797;&#26434;&#24230;&#65292;&#26377;&#21161;&#20110;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20803;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20869;&#22791;&#21463;&#20851;&#27880;&#24182;&#25512;&#21160;&#20102;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;&#34920;&#29616;&#33391;&#22909;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#20855;&#26377;&#39640;&#36136;&#37327;&#20803;&#25968;&#25454;&#30340;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#34920;&#31034;&#24213;&#23618;&#20219;&#21153;&#27867;&#21270;&#30446;&#26631;&#65292;&#26377;&#26102;&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#32780;&#35328;&#38590;&#20197;&#33719;&#24471;&#12290;&#24403;&#21069;&#22522;&#20110;&#20803;&#25968;&#25454;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#20351;&#29992;&#19981;&#23436;&#32654;&#30340;&#35757;&#32451;&#20219;&#21153;&#35757;&#32451;&#20196;&#20154;&#28385;&#24847;&#30340;&#20803;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#30693;&#35782;&#20449;&#24687;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65288;MKIML&#65289;&#65292;&#36890;&#36807;&#23558;&#34917;&#20607;&#30340;&#20803;&#30693;&#35782;&#38598;&#25104;&#21040;&#20803;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#20840;&#38754;&#25552;&#39640;&#20803;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36866;&#24403;&#30340;&#20803;&#27491;&#21017;&#21270;&#65288;MR&#65289;&#30446;&#26631;&#23558;&#20803;&#30693;&#35782;&#21021;&#27493;&#38598;&#25104;&#21040;&#20803;&#30446;&#26631;&#20013;&#65292;&#26469;&#35268;&#33539;&#20803;&#27169;&#22411;&#20989;&#25968;&#31867;&#30340;&#23481;&#37327;&#22797;&#26434;&#24230;&#65292;&#26377;&#21161;&#20110;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20316;&#20026;&#19968;&#31181;&#23454;&#29992;&#21270;&#23454;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta learning recently has been heavily researched and helped advance the contemporary machine learning. However, achieving well-performing meta-learning model requires a large amount of training tasks with high-quality meta-data representing the underlying task generalization goal, which is sometimes difficult and expensive to obtain for real applications. Current meta-data-driven meta-learning approaches, however, are fairly hard to train satisfactory meta-models with imperfect training tasks. To address this issue, we suggest a meta-knowledge informed meta-learning (MKIML) framework to improve meta-learning by additionally integrating compensated meta-knowledge into meta-learning process. We preliminarily integrate meta-knowledge into meta-objective via using an appropriate meta-regularization (MR) objective to regularize capacity complexity of the meta-model function class to facilitate better generalization on unseen tasks. As a practical implementation, we introduce data augmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;VINO&#65292;&#36890;&#36807;&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#21069;&#21521;&#39044;&#27979;&#21644;&#21453;&#21521;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#26041;&#38754;&#21487;&#20197;&#27604;&#20256;&#32479;&#26377;&#38480;&#20803;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2305.07889</link><description>&lt;p&gt;
&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural operator for structural simulation and bridge health monitoring. (arXiv:2305.07889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;VINO&#65292;&#36890;&#36807;&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#21069;&#21521;&#39044;&#27979;&#21644;&#21453;&#21521;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#26041;&#38754;&#21487;&#20197;&#27604;&#20256;&#32479;&#26377;&#38480;&#20803;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#32467;&#26500;&#24037;&#31243;&#30456;&#32467;&#21512;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#29992;&#20110;&#21069;&#21521;&#38382;&#39064;&#65288;&#32467;&#26500;&#27169;&#25311;&#65289;&#21644;&#21453;&#21521;&#38382;&#39064;&#65288;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65289;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#65292;&#25552;&#20986;&#20102;VINO&#65288;&#36710;&#36742;-&#26725;&#26753;&#30456;&#20114;&#20316;&#29992;&#31070;&#32463;&#36816;&#31639;&#22120;&#65289;&#65292;&#20316;&#20026;&#26725;&#26753;&#32467;&#26500;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;VINO&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36816;&#34892;&#21442;&#25968;&#26377;&#38480;&#20803;&#65288;FE&#65289;&#27169;&#25311;&#65292;&#32771;&#34385;&#32467;&#26500;&#21021;&#22987;&#25439;&#20260;&#22330;&#30340;&#38543;&#26426;&#20998;&#24067;&#65292;&#24314;&#31435;&#20102;VBI-FE&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#22312;&#22235;&#31181;&#25439;&#20260;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#20135;&#29983;&#20102;VBI-EXP&#25968;&#25454;&#38598;&#12290;&#22312;VINO&#36890;&#36807;VBI-FE&#39044;&#35757;&#32451;&#24182;&#22312;&#20581;&#24247;&#29366;&#24577;&#19979;&#36890;&#36807;VBI-EXP&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#23454;&#29616;&#20102;&#20197;&#19979;&#20004;&#20010;&#25913;&#36827;&#12290;&#39318;&#20808;&#65292;&#21069;&#21521;&#30340;VINO&#27604;FE&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#20174;&#25439;&#20260;&#22330;&#36755;&#20837;&#39044;&#27979;&#32467;&#26500;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;&#21453;&#21521;&#30340;VINO&#21487;&#20197;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infusing deep learning with structural engineering has received widespread attention for both forward problems (structural simulation) and inverse problems (structural health monitoring). Based on Fourier Neural Operator, this study proposes VINO (Vehicle-bridge Interaction Neural Operator) to serve as the digital twin of bridge structures. VINO learns mappings between structural response fields and damage fields. In this study, VBI-FE dataset was established by running parametric finite element (FE) simulations considering a random distribution of structural initial damage field. Subsequently, VBI-EXP dataset was produced by conducting an experimental study under four damage scenarios. After VINO was pre-trained by VBI-FE and fine-tuned by VBI-EXP from the bridge at the healthy state, the model achieved the following two improvements. First, forward VINO can predict structural responses from damage field inputs more accurately than the FE model. Second, inverse VINO can determine, loc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#26412;&#21644;&#31185;&#23398;&#38382;&#39064;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20248;&#31168;&#30340;&#25945;&#32946;&#38382;&#39064;&#33258;&#21160;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.07871</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#25945;&#23398;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scalable Educational Question Generation with Pre-trained Language Models. (arXiv:2305.07871v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07871
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#26412;&#21644;&#31185;&#23398;&#38382;&#39064;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20248;&#31168;&#30340;&#25945;&#32946;&#38382;&#39064;&#33258;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#20154;&#21475;&#22312;&#25506;&#32034;&#20010;&#24615;&#21270;&#23398;&#20064;&#20043;&#26053;&#26102;&#65292;&#25945;&#32946;&#38382;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23558;&#22312;&#22312;&#32447;&#25945;&#32946;&#30340;&#25193;&#23637;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#23454;&#29616;&#22823;&#35268;&#27169;&#30340;&#33258;&#25105;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;EduQG&#65292;&#36890;&#36807;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26500;&#24314;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EduQG&#33021;&#22815;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#26412;&#21644;&#31185;&#23398;&#38382;&#39064;&#25968;&#25454;&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#20986;&#26356;&#20248;&#31168;&#30340;&#25945;&#32946;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic generation of educational questions will play a key role in scaling online education, enabling self-assessment at scale when a global population is manoeuvring their personalised learning journeys. We develop \textit{EduQG}, a novel educational question generation model built by adapting a large language model. Our extensive experiments demonstrate that \textit{EduQG} can produce superior educational questions by further pre-training and fine-tuning a pre-trained language model on the scientific text and science question data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#35780;&#20272;&#20102;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;AI&#25216;&#26415;&#22312;&#21382;&#21490;&#20107;&#23454;&#26680;&#26597;&#21644;&#22635;&#34917;&#31354;&#30333;&#26041;&#38754;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;GPT 4&#34920;&#29616;&#26368;&#20026;&#20248;&#24322;&#65292;&#36825;&#35828;&#26126;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;AI&#25216;&#26415;&#22312;&#21382;&#21490;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20415;&#20016;&#23500;&#25105;&#20204;&#23545;&#36807;&#21435;&#30340;&#29702;&#35299;&#21644;&#24357;&#21512;&#21382;&#21490;&#30693;&#35782;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.07868</link><description>&lt;p&gt;
AI&#25216;&#26415;&#22312;&#21382;&#21490;&#20107;&#23454;&#26680;&#26597;&#20013;&#30340;&#24212;&#29992;&#8212;&#8212;GPT 3.5&#12289;GPT4&#21644;GoogleBARD&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bridging History with AI A Comparative Evaluation of GPT 3.5, GPT4, and GoogleBARD in Predictive Accuracy and Fact Checking. (arXiv:2305.07868v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#35780;&#20272;&#20102;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;AI&#25216;&#26415;&#22312;&#21382;&#21490;&#20107;&#23454;&#26680;&#26597;&#21644;&#22635;&#34917;&#31354;&#30333;&#26041;&#38754;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;GPT 4&#34920;&#29616;&#26368;&#20026;&#20248;&#24322;&#65292;&#36825;&#35828;&#26126;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;AI&#25216;&#26415;&#22312;&#21382;&#21490;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20415;&#20016;&#23500;&#25105;&#20204;&#23545;&#36807;&#21435;&#30340;&#29702;&#35299;&#21644;&#24357;&#21512;&#21382;&#21490;&#30693;&#35782;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#26102;&#20195;&#20449;&#24687;&#30340;&#36805;&#36895;&#25193;&#25955;&#20984;&#26174;&#20102;&#20934;&#30830;&#30340;&#21382;&#21490;&#35760;&#24405;&#21644;&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#24050;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#20854;&#22312;&#21382;&#21490;&#20107;&#23454;&#26680;&#26597;&#21644;&#22635;&#34917;&#31354;&#30333;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#19981;&#20026;&#20154;&#30693;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#65292;&#21363; GPT 3.5&#12289;GPT 4&#21644;GoogleBARD&#65292;&#23427;&#20204;&#33021;&#22815;&#26681;&#25454;&#32473;&#23450;&#25968;&#25454;&#39044;&#27979;&#21644;&#39564;&#35777;&#21382;&#21490;&#20107;&#20214;&#30340;&#34920;&#29616;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#19982;&#29616;&#23454;&#30340;&#36317;&#31163;&#65288;DTR&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#36755;&#20986;&#32467;&#26524;&#19982;&#24050;&#26377;&#30340;&#21382;&#21490;&#20107;&#23454;&#26159;&#21542;&#19968;&#33268;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#22312;&#21382;&#21490;&#30740;&#31350;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;GPT 4&#34920;&#29616;&#26368;&#20026;&#20248;&#24322;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;AI&#22312;&#20016;&#23500;&#25105;&#20204;&#23545;&#36807;&#21435;&#30340;&#29702;&#35299;&#21644;&#24357;&#21512;&#21382;&#21490;&#30693;&#35782;&#24046;&#36317;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of information in the digital era underscores the importance of accurate historical representation and interpretation. While artificial intelligence has shown promise in various fields, its potential for historical fact-checking and gap-filling remains largely untapped. This study evaluates the performance of three large language models LLMs GPT 3.5, GPT 4, and GoogleBARD in the context of predicting and verifying historical events based on given data. A novel metric, Distance to Reality (DTR), is introduced to assess the models' outputs against established historical facts. The results reveal a substantial potential for AI in historical studies, with GPT 4 demonstrating superior performance. This paper underscores the need for further research into AI's role in enriching our understanding of the past and bridging historical knowledge gaps.
&lt;/p&gt;</description></item><item><title>STEER&#26159;&#19968;&#31181;&#21487;&#20197;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#24322;&#27493;&#21160;&#20316;&#21327;&#35843;&#38382;&#39064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23618;&#27425;&#20915;&#31574;&#32467;&#26500;&#12289;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#25506;&#32034;&#24615;&#23398;&#20064;&#26041;&#27861;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.07856</link><description>&lt;p&gt;
Stackelberg&#20915;&#31574;&#21464;&#25442;&#22120;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#24322;&#27493;&#21160;&#20316;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Stackelberg Decision Transformer for Asynchronous Action Coordination in Multi-Agent Systems. (arXiv:2305.07856v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07856
&lt;/p&gt;
&lt;p&gt;
STEER&#26159;&#19968;&#31181;&#21487;&#20197;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#24322;&#27493;&#21160;&#20316;&#21327;&#35843;&#38382;&#39064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23618;&#27425;&#20915;&#31574;&#32467;&#26500;&#12289;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#25506;&#32034;&#24615;&#23398;&#20064;&#26041;&#27861;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#21160;&#20316;&#21327;&#35843;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#20013;&#38754;&#20020;&#30528;&#26222;&#36941;&#30340;&#25361;&#25112;&#65292;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;Stackelberg&#28216;&#25103;&#65288;SG&#65289;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;SG&#30340;&#29616;&#26377;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#32593;&#32476;&#32467;&#26500;&#25110;&#29615;&#22659;&#38480;&#21046;&#30340;&#20005;&#37325;&#21046;&#32422;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Stackelberg&#20915;&#31574;&#21464;&#25442;&#22120;&#65288;STEER&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#20195;&#29702;&#20043;&#38388;&#23618;&#27425;&#21327;&#35843;&#30340;&#22256;&#38590;&#12290;STEER&#36890;&#36807;&#23558;SG&#30340;&#23618;&#27425;&#20915;&#31574;&#32467;&#26500;&#12289;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;MARL&#30340;&#25506;&#32034;&#24615;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#24322;&#27493;&#21160;&#20316;&#21327;&#35843;&#26041;&#27861;&#65292;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;MAS&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#21644;&#29615;&#22659;&#37197;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20855;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asynchronous action coordination presents a pervasive challenge in Multi-Agent Systems (MAS), which can be represented as a Stackelberg game (SG). However, the scalability of existing Multi-Agent Reinforcement Learning (MARL) methods based on SG is severely constrained by network structures or environmental limitations. To address this issue, we propose the Stackelberg Decision Transformer (STEER), a heuristic approach that resolves the difficulties of hierarchical coordination among agents. STEER efficiently manages decision-making processes in both spatial and temporal contexts by incorporating the hierarchical decision structure of SG, the modeling capability of autoregressive sequence models, and the exploratory learning methodology of MARL. Our research contributes to the development of an effective and adaptable asynchronous action coordination method that can be widely applied to various task types and environmental configurations in MAS. Experimental results demonstrate that ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#31639;&#27861;&#26469;&#21306;&#20998;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07854</link><description>&lt;p&gt;
&#22522;&#20110;&#21305;&#37197;&#29305;&#24449;&#25552;&#21462;&#30340;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction. (arXiv:2305.07854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#31639;&#27861;&#26469;&#21306;&#20998;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#38656;&#35201;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#24320;&#21457;&#20934;&#30830;&#21487;&#38752;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20005;&#26684;&#30340;&#25968;&#25454;&#38544;&#31169;&#27861;&#24459;&#21644;&#20016;&#23500;&#30340;&#36793;&#32536;&#24037;&#19994;&#25968;&#25454;&#38656;&#35201;&#20998;&#25955;&#24335;&#25968;&#25454;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#20010;&#20998;&#25955;&#24335;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#38750;&#24120;&#36866;&#29992;&#20110;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#30001;&#20110;&#19981;&#21516;&#30340;&#36864;&#21270;&#26426;&#21046;&#21644;&#19981;&#24179;&#31561;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#25152;&#23548;&#33268;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#24320;&#21457;&#31934;&#24230;&#39640;&#30340;&#35757;&#32451;&#27169;&#22411;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#32479;&#35745;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;FL&#22312;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#21442;&#25968;&#32858;&#21512;&#31639;&#27861;&#30340; FL &#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven industrial health prognostics require rich training data to develop accurate and reliable predictive models. However, stringent data privacy laws and the abundance of edge industrial data necessitate decentralized data utilization. Thus, the industrial health prognostics field is well suited to significantly benefit from federated learning (FL), a decentralized and privacy-preserving learning technique. However, FL-based health prognostics tasks have hardly been investigated due to the complexities of meaningfully aggregating model parameters trained from heterogeneous data to form a high performing federated model. Specifically, data heterogeneity among edge devices, stemming from dissimilar degradation mechanisms and unequal dataset sizes, poses a critical statistical challenge for developing accurate federated models. We propose a pioneering FL-based health prognostic model with a feature similarity-matched parameter aggregation algorithm to discriminatingly learn from h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.07845</link><description>&lt;p&gt;
&#29702;&#35299;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#24179;&#22343;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#20250;&#32858;&#38598;&#35757;&#32451;&#20110;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#20197;&#33719;&#24471;&#34920;&#29616;&#33391;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#29702;&#23578;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#26469;&#30740;&#31350;&#27169;&#22411;&#24179;&#22343;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21363;&#20351;&#20840;&#23616;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20063;&#21487;&#33021;&#20559;&#31163;&#30406;&#22320;&#24213;&#37096;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; RepAL &#30340;&#31616;&#21333;&#26131;&#29992;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#34920;&#31034;&#12290;&#36890;&#36807;&#20943;&#24369;&#21477;&#23376;&#23884;&#20837;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#20041;&#21305;&#37197;&#21644;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#26080;&#39035;&#35757;&#32451;&#65292;&#21487;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21477;&#23376;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.07824</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#26131;&#29992;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Simple and Plug-and-play Method for Unsupervised Sentence Representation Enhancement. (arXiv:2305.07824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; RepAL &#30340;&#31616;&#21333;&#26131;&#29992;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#34920;&#31034;&#12290;&#36890;&#36807;&#20943;&#24369;&#21477;&#23376;&#23884;&#20837;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#20041;&#21305;&#37197;&#21644;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#26080;&#39035;&#35757;&#32451;&#65292;&#21487;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21477;&#23376;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#29983;&#25104;&#21477;&#23376;&#30340;&#23884;&#20837;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#21305;&#37197;&#21644;&#26816;&#32034;&#38382;&#39064;&#26159;&#26377;&#30410;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; Representation ALchemy (RepAL)&#65292;&#36825;&#26159;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#34920;&#31034;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;RepAL &#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20943;&#24369;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#21477;&#23376;&#23884;&#20837;&#20013;&#20887;&#20313;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; RepAL &#26159;&#19968;&#31181;&#19981;&#38656;&#35201;&#35757;&#32451;&#19988;&#21487;&#20197;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21477;&#23376;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#20197;&#29702;&#35299; RepAL&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating proper embedding of sentences through an unsupervised way is beneficial to semantic matching and retrieval problems in real-world scenarios. This paper presents Representation ALchemy (RepAL), an extremely simple post-processing method that enhances sentence representations. The basic idea in RepAL is to de-emphasize redundant information of sentence embedding generated by pre-trained models. Through comprehensive experiments, we show that RepAL is free of training and is a plug-and-play method that can be combined with most existing unsupervised sentence learning models. We also conducted in-depth analysis to understand RepAL.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21453;&#23556;&#19981;&#21464;&#24615;&#30340;&#28857;&#20113;&#20998;&#26512;&#26694;&#26550;Cloud-RAIN&#65292;&#20351;&#29992;&#20108;&#27425;&#31070;&#32463;&#20803;&#21644;PCA&#35268;&#33539;&#34920;&#31034;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07814</link><description>&lt;p&gt;
Cloud-RAIN: &#20855;&#26377;&#21453;&#23556;&#19981;&#21464;&#24615;&#30340;&#28857;&#20113;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cloud-RAIN: Point Cloud Analysis with Reflectional Invariance. (arXiv:2305.07814v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07814
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21453;&#23556;&#19981;&#21464;&#24615;&#30340;&#28857;&#20113;&#20998;&#26512;&#26694;&#26550;Cloud-RAIN&#65292;&#20351;&#29992;&#20108;&#27425;&#31070;&#32463;&#20803;&#21644;PCA&#35268;&#33539;&#34920;&#31034;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#20219;&#21153;&#30340;&#32593;&#32476;&#34987;&#26399;&#26395;&#22312;&#28857;&#20113;&#34987;&#20223;&#23556;&#21464;&#25442;&#65288;&#22914;&#26059;&#36716;&#21644;&#21453;&#23556;&#65289;&#26102;&#20445;&#25345;&#19981;&#21464;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#19982;&#36807;&#21435;&#20960;&#24180;&#20013;&#21463;&#21040;&#20027;&#35201;&#30740;&#31350;&#20851;&#27880;&#30340;&#26059;&#36716;&#19981;&#21464;&#24615;&#30456;&#27604;&#65292;&#21453;&#23556;&#19981;&#21464;&#24615;&#24456;&#23569;&#34987;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#21453;&#23556;&#23545;&#31216;&#24615;&#21487;&#20197;&#20986;&#29616;&#22312;&#38750;&#24120;&#24120;&#35265;&#21644;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#65306;&#32467;&#26500;&#21270;&#34903;&#36947;&#30340;&#38745;&#24577;&#21453;&#23556;&#23545;&#31216;&#24615;&#65292;&#26469;&#33258;&#31227;&#21160;&#29289;&#20307;&#65288;&#22914;&#34892;&#20154;&#65289;&#30340;&#21452;&#21521;&#36816;&#21160;&#30340;&#21160;&#24577;&#21453;&#23556;&#23545;&#31216;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#22269;&#23478;&#30340;&#24038;&#21491;&#20132;&#36890;&#24815;&#20363;&#12290;&#21487;&#20197;&#24796;&#30340;&#26159;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#20851;&#20110;&#28857;&#20113;&#20998;&#26512;&#20013;&#20855;&#26377;&#21453;&#23556;&#19981;&#21464;&#24615;&#30340;&#32593;&#32476;&#30340;&#25253;&#36947;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20108;&#27425;&#31070;&#32463;&#20803;&#21644;PCA&#35268;&#33539;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Cloud-RAIN&#65292;&#26469;&#36171;&#20104;&#28857;&#20113;&#27169;&#22411;&#21453;&#23556;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The networks for point cloud tasks are expected to be invariant when the point clouds are affinely transformed such as rotation and reflection. So far, relative to the rotational invariance that has been attracting major research attention in the past years, the reflection invariance is little addressed. Notwithstanding, reflection symmetry can find itself in very common and important scenarios, e.g., static reflection symmetry of structured streets, dynamic reflection symmetry from bidirectional motion of moving objects (such as pedestrians), and left- and right-hand traffic practices in different countries. To the best of our knowledge, unfortunately, no reflection-invariant network has been reported in point cloud analysis till now. To fill this gap, we propose a framework by using quadratic neurons and PCA canonical representation, referred to as Cloud-RAIN, to endow point \underline{Cloud} models with \underline{R}eflection\underline{A}l \underline{IN}variance. We prove a theorem 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Deepfake&#25216;&#26415;&#20135;&#29983;&#27809;&#26377;&#37325;&#38899;&#30340;&#35821;&#38899;&#26469;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#37325;&#38899;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#35821;&#38899;&#21644;&#21475;&#36848;&#35821;&#38899;&#65292;&#33021;&#22815;&#20998;&#31163;&#20986;&#30456;&#23545;&#23481;&#26131;&#26816;&#27979;&#21040;&#30340;&#37325;&#38899;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.07791</link><description>&lt;p&gt;
&#20351;&#29992;Deepfake&#25216;&#26415;&#23454;&#29616;&#35821;&#38899;&#37325;&#38899;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Using Deepfake Technologies for Word Emphasis Detection. (arXiv:2305.07791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Deepfake&#25216;&#26415;&#20135;&#29983;&#27809;&#26377;&#37325;&#38899;&#30340;&#35821;&#38899;&#26469;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#37325;&#38899;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#35821;&#38899;&#21644;&#21475;&#36848;&#35821;&#38899;&#65292;&#33021;&#22815;&#20998;&#31163;&#20986;&#30456;&#23545;&#23481;&#26131;&#26816;&#27979;&#21040;&#30340;&#37325;&#38899;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#33258;&#21160;&#35821;&#38899;&#37325;&#38899;&#26816;&#27979;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#37325;&#38899;&#21463;&#21040;&#35762;&#35805;&#32773;&#35821;&#38899;&#29305;&#28857;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#21475;&#38899;&#12289;&#26041;&#35328;&#25110;&#22768;&#38899;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;Deepfake&#25216;&#26415;&#20026;&#36825;&#20010;&#35828;&#35805;&#32773;&#20135;&#29983;&#19968;&#20010;&#27809;&#26377;&#37325;&#38899;&#30340;&#35821;&#38899;&#12290;&#36825;&#38656;&#35201;&#25552;&#21462;&#21475;&#36848;&#35821;&#38899;&#30340;&#25991;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#26469;&#33258;&#21516;&#19968;&#28436;&#35762;&#32773;&#30340;&#35821;&#38899;&#26679;&#26412;&#26469;&#20026;&#36825;&#20010;&#20219;&#21153;&#20135;&#29983;&#27809;&#26377;&#37325;&#38899;&#30340;&#35821;&#38899;&#12290;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#35821;&#38899;&#21644;&#21475;&#36848;&#35821;&#38899;&#65292;&#25105;&#20204;&#33021;&#22815;&#20998;&#31163;&#20986;&#30456;&#23545;&#23481;&#26131;&#26816;&#27979;&#21040;&#30340;&#37325;&#38899;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the task of automated emphasis detection for spoken language. This problem is challenging in that emphasis is affected by the particularities of speech of the subject, for example the subject accent, dialect or voice. To address this task, we propose to utilize deep fake technology to produce an emphasis devoid speech for this speaker. This requires extracting the text of the spoken voice, and then using a voice sample from the same speaker to produce emphasis devoid speech for this task. By comparing the generated speech with the spoken voice, we are able to isolate patterns of emphasis which are relatively easy to detect.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#21644;&#25191;&#34892;&#26694;&#26550;&#65292;&#22312;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;&#20013;&#23454;&#29616;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#26512;&#38382;&#39064;&#20026;H&#34920;&#36798;&#24335;&#24182;&#35774;&#35745;&#28151;&#21512;&#25191;&#34892;&#22120;&#23454;&#29616;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#21644;&#25928;&#29575;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07789</link><description>&lt;p&gt;
&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#19982;&#25191;&#34892;&#31572;&#26696;&#22797;&#26434;&#38382;&#39064;&#30340;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Answering Complex Questions over Text by Hybrid Question Parsing and Execution. (arXiv:2305.07789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#21644;&#25191;&#34892;&#26694;&#26550;&#65292;&#22312;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;&#20013;&#23454;&#29616;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#26512;&#38382;&#39064;&#20026;H&#34920;&#36798;&#24335;&#24182;&#35774;&#35745;&#28151;&#21512;&#25191;&#34892;&#22120;&#23454;&#29616;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#21644;&#25928;&#29575;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#30340;&#20027;&#23548;&#27169;&#24335;&#26159;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22312;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#20294;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#36825;&#19982;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#26041;&#27861;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#20851;&#31995;&#25968;&#25454;&#24211;&#12289;&#30693;&#35782;&#22270;&#35889;&#65289;&#19978;&#24191;&#27867;&#36866;&#24212;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#26597;&#35810;&#24341;&#25806;&#36827;&#34892;&#25191;&#34892;&#12290;&#20026;&#20102;&#32467;&#21512;&#31070;&#32463;&#21644;&#31526;&#21495;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#20013;&#36827;&#34892;&#35299;&#26512;&#21644;&#25191;&#34892;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#20013;&#24515;&#25903;&#26609;&#65306;&#65288;1&#65289;&#25105;&#20204;&#23558;&#21508;&#31181;&#22797;&#26434;&#38382;&#39064;&#35299;&#26512;&#25104;&#20013;&#38388;&#34920;&#31034;&#65292;&#31216;&#20026;H&#34920;&#36798;&#24335;&#65292;&#23427;&#30001;&#31616;&#21333;&#38382;&#39064;&#32452;&#25104;&#21407;&#35821;&#21644;&#34920;&#31034;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#31526;&#21495;&#25805;&#20316;&#32452;&#25104;&#65307;&#65288;2&#65289;&#20026;&#20102;&#25191;&#34892;&#20135;&#29983;&#30340;H&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#25191;&#34892;&#22120;&#65292;&#23427;&#38598;&#25104;&#20102;&#30830;&#23450;&#35268;&#21017;&#26469;&#32763;&#35793;&#31526;&#21495;&#25805;&#20316;&#65292;&#19982;&#22788;&#29702;&#21407;&#22987;&#38382;&#39064;&#30340;&#25554;&#20837;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;&#22797;&#26434;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#25351;&#26631;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm of textual question answering systems is based on end-to-end neural networks, which excels at answering natural language questions but falls short on complex ones. This stands in contrast to the broad adaptation of semantic parsing approaches over structured data sources (e.g., relational database, knowledge graphs), that convert natural language questions to logical forms and execute them with query engines. Towards combining the strengths of neural and symbolic methods, we propose a framework of question parsing and execution on textual QA. It comprises two central pillars: (1) We parse the question of varying complexity into an intermediate representation, named H-expression, which is composed of simple questions as the primitives and symbolic operations representing the relationships among them; (2) To execute the resulting H-expressions, we design a hybrid executor, which integrates the deterministic rules to translate the symbolic operations with a drop-in n
&lt;/p&gt;</description></item><item><title>KALMRA&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#23427;&#25193;&#23637;&#20102;&#33521;&#35821;&#21629;&#20196;&#30340;&#21151;&#33021;&#65292;&#20197;&#20801;&#35768;&#22312;KALM&#20013;&#36827;&#34892;&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#21019;&#20316;&#65292;&#24182;&#36890;&#36807;&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#21019;&#20316;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07763</link><description>&lt;p&gt;
&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#30693;&#35782;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Knowledge Authoring for Rules and Actions. (arXiv:2305.07763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07763
&lt;/p&gt;
&lt;p&gt;
KALMRA&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#23427;&#25193;&#23637;&#20102;&#33521;&#35821;&#21629;&#20196;&#30340;&#21151;&#33021;&#65292;&#20197;&#20801;&#35768;&#22312;KALM&#20013;&#36827;&#34892;&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#21019;&#20316;&#65292;&#24182;&#36890;&#36807;&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#21019;&#20316;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#31995;&#32479;&#20197;&#20107;&#23454;&#21644;&#35268;&#21017;&#30340;&#24418;&#24335;&#25551;&#36848;&#21644;&#25512;&#29702;&#22797;&#26434;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#37096;&#32626;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#31995;&#32479;&#26102;&#65292;&#39046;&#22495;&#19987;&#23478;&#24448;&#24448;&#24456;&#38590;&#26500;&#24314;&#27491;&#30830;&#30340;&#36923;&#36753;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KALMRA&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#20197;&#23454;&#29616;&#22312;KALM&#20013;&#36827;&#34892;&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#21019;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge representation and reasoning (KRR) systems describe and reason with complex concepts and relations in the form of facts and rules. Unfortunately, wide deployment of KRR systems runs into the problem that domain experts have great difficulty constructing correct logical representations of their domain knowledge. Knowledge engineers can help with this construction process, but there is a deficit of such specialists. The earlier Knowledge Authoring Logic Machine (KALM) based on Controlled Natural Language (CNL) was shown to have very high accuracy for authoring facts and questions. More recently, KALMFL, a successor of KALM, replaced CNL with factual English, which is much less restrictive and requires very little training from users. However, KALMFL has limitations in representing certain types of knowledge, such as authoring rules for multi-step reasoning or understanding actions with timestamps. To address these limitations, we propose KALMRA to enable authoring of rules and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2305.07759</link><description>&lt;p&gt;
TinyStories: &#35821;&#35328;&#27169;&#22411;&#33021;&#31616;&#23567;&#21040;&#20160;&#20040;&#31243;&#24230;&#21364;&#20381;&#28982;&#33021;&#22815;&#35762;&#36848;&#36830;&#36143;&#30340;&#33521;&#25991;&#25925;&#20107;&#65311;
&lt;/p&gt;
&lt;p&gt;
TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#23567;&#22411;&#21270;&#26102;&#32463;&#24120;&#38590;&#20197;&#20135;&#29983;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026; TinyStories &#30340;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#35268;&#27169;&#23567;&#12289;&#22797;&#26434;&#24230;&#20302;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#30701;&#25925;&#20107;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).  In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet stil
&lt;/p&gt;</description></item><item><title>AI&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#22823;&#22810;&#25968;&#20915;&#31574;&#29615;&#22659;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.07722</link><description>&lt;p&gt;
&#23547;&#27714;&#21487;&#39564;&#35777;&#24615;: &#35299;&#37322;&#24456;&#23569;&#33021;&#22815;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#25552;&#39640;&#20915;&#31574;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making. (arXiv:2305.07722v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07722
&lt;/p&gt;
&lt;p&gt;
AI&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#22823;&#22810;&#25968;&#20915;&#31574;&#29615;&#22659;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#25991;&#29486;&#65292;&#28041;&#21450;&#21487;&#35299;&#37322;&#30340;AI&#31995;&#32479;&#20026;&#20154;&#31867;&#20915;&#31574;&#32773;&#25552;&#20379;&#24314;&#35758;&#65292;&#24182;&#21576;&#29616;&#20986;&#19968;&#31995;&#21015;&#19981;&#30830;&#23450;&#21644;&#20196;&#20154;&#22256;&#24785;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#32508;&#21512;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#65292;&#38416;&#26126;&#20102;AI&#35299;&#37322;&#32463;&#24120;&#26080;&#27861;&#20419;&#20351;&#36866;&#24403;&#30340;&#20381;&#36182;&#21644;&#20114;&#34917;&#20915;&#31574;&#34920;&#29616;&#30340;&#22833;&#36133;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#26399;&#26395;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#25110;&#28165;&#26224;&#38416;&#36848;AI&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20915;&#31574;&#29615;&#22659;&#20013;&#65292;AI&#35299;&#37322;&#24182;&#26410;&#20419;&#36827;&#36825;&#31181;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#35299;&#37322;&#26041;&#27861;&#22914;&#20309;&#65292;&#22823;&#22810;&#25968;&#29615;&#22659;&#22522;&#26412;&#19978;&#37117;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#26356;&#26377;&#25928;&#30340;&#21487;&#35299;&#37322;AI&#36741;&#21161;&#20915;&#31574;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;GPT-2&#24494;&#35843;&#20026;&#26426;&#22120;&#20154;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#25512;&#29702;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#35268;&#33539;&#24182;&#39034;&#24207;&#25191;&#34892;&#12290;&#20351;&#29992;&#22330;&#26223;&#22270;&#36827;&#34892;&#36755;&#20837;&#25509;&#22320;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#20154;&#31867;&#35831;&#27714;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#65292;&#26377;&#25928;&#22320;&#25191;&#34892;&#38271;&#31243;&#20219;&#21153;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2305.07716</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#22330;&#26223;&#22270;&#19978;&#25512;&#29702;&#65306;&#23558; GPT-2 &#24494;&#35843;&#20026;&#26426;&#22120;&#20154;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22522;&#20110;&#22330;&#26223;&#30340;&#20219;&#21153;&#35268;&#21010;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning to Reason over Scene Graphs: A Case Study of Finetuning GPT-2 into a Robot Language Model for Grounded Task Planning. (arXiv:2305.07716v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;GPT-2&#24494;&#35843;&#20026;&#26426;&#22120;&#20154;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#25512;&#29702;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#35268;&#33539;&#24182;&#39034;&#24207;&#25191;&#34892;&#12290;&#20351;&#29992;&#22330;&#26223;&#22270;&#36827;&#34892;&#36755;&#20837;&#25509;&#22320;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#20154;&#31867;&#35831;&#27714;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#65292;&#26377;&#25928;&#22320;&#25191;&#34892;&#38271;&#31243;&#20219;&#21153;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31243;&#20219;&#21153;&#35268;&#21010;&#23545;&#20110;&#26234;&#33021;&#36741;&#21161;&#21644;&#26381;&#21153;&#26426;&#22120;&#20154;&#30340;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26356;&#23567;&#31867;&#21035;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21363;GPT-2&#65292;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#23398;&#20064;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#35268;&#33539;&#65292;&#20026;&#25191;&#34892;&#32773;&#39034;&#24207;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;LLM&#30340;&#36755;&#20837;&#25509;&#22320;&#20110;&#34920;&#31034;&#20026;&#22330;&#26223;&#22270;&#30340;&#22495;&#19978;&#65292;&#20351;&#20854;&#33021;&#22815;&#23558;&#20154;&#31867;&#35831;&#27714;&#32763;&#35793;&#25104;&#21487;&#25191;&#34892;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#65292;&#20174;&#32780;&#23398;&#20064;&#25512;&#29702;&#38271;&#26399;&#20219;&#21153;&#65292;&#22914;&#22312;ALFRED&#22522;&#20934;&#27979;&#35797;&#25152;&#36935;&#21040;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#32463;&#20856;&#35268;&#21010;&#21644;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#32771;&#23519;&#22522;&#20110;LLM&#30340;&#35745;&#21010;&#22120;&#30340;&#36866;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23384;&#20648;&#22312;LLM&#20013;&#30340;&#30693;&#35782;&#21487;&#20197;&#26377;&#25928;&#22320;&#25509;&#22320;&#20197;&#25191;&#34892;&#38271;&#31243;&#20219;&#21153;&#35268;&#21010;&#65292;&#23637;&#31034;&#20102;&#31070;&#32463;&#31526;&#21495;&#35268;&#21010;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#26410;&#26469;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-horizon task planning is essential for the development of intelligent assistive and service robots. In this work, we investigate the applicability of a smaller class of large language models (LLMs), specifically GPT-2, in robotic task planning by learning to decompose tasks into subgoal specifications for a planner to execute sequentially. Our method grounds the input of the LLM on the domain that is represented as a scene graph, enabling it to translate human requests into executable robot plans, thereby learning to reason over long-horizon tasks, as encountered in the ALFRED benchmark. We compare our approach with classical planning and baseline methods to examine the applicability and generalizability of LLM-based planners. Our findings suggest that the knowledge stored in an LLM can be effectively grounded to perform long-horizon task planning, demonstrating the promising potential for the future application of neuro-symbolic planning methods in robotics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AWFSD&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#35780;&#20998;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#20808;&#39564;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#30001;&#20809;&#23398;&#25104;&#20687;&#31995;&#32479;&#24341;&#36215;&#30340;&#27979;&#37327;&#25968;&#25454;&#34987;&#28151;&#21512;&#30340;&#27850;&#26494;&#21644;&#39640;&#26031;&#22122;&#22768;&#65288;PG&#22122;&#22768;&#65289;&#25152;&#30772;&#22351;&#30340;&#20840;&#24687;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07712</link><description>&lt;p&gt;
AWFSD:&#22522;&#20110;&#35780;&#20998;&#25193;&#25955;&#22270;&#20687;&#20808;&#39564;&#30340;&#21152;&#36895;Wirtinger&#27969;&#29992;&#20110;&#27850;&#26494;&#39640;&#26031;&#20840;&#24687;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
AWFSD: Accelerated Wirtinger Flow with Score-based Diffusion Image Prior for Poisson-Gaussian Holographic Phase Retrieval. (arXiv:2305.07712v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AWFSD&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#35780;&#20998;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#20808;&#39564;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#30001;&#20809;&#23398;&#25104;&#20687;&#31995;&#32479;&#24341;&#36215;&#30340;&#27979;&#37327;&#25968;&#25454;&#34987;&#28151;&#21512;&#30340;&#27850;&#26494;&#21644;&#39640;&#26031;&#22122;&#22768;&#65288;PG&#22122;&#22768;&#65289;&#25152;&#30772;&#22351;&#30340;&#20840;&#24687;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#24674;&#22797;&#26159;&#35768;&#22810;&#30456;&#24178;&#25104;&#20687;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#30001;&#20809;&#23398;&#25104;&#20687;&#31995;&#32479;&#24341;&#36215;&#30340;&#27979;&#37327;&#25968;&#25454;&#34987;&#28151;&#21512;&#30340;&#27850;&#26494;&#21644;&#39640;&#26031;&#22122;&#22768;&#65288;PG&#22122;&#22768;&#65289;&#25152;&#30772;&#22351;&#30340;&#20840;&#24687;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#36895;Wirtinger&#27969;&#30340;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#35780;&#20998;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#20808;&#39564;&#65288;AWFSD&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#21253;&#21547;&#25968;&#25454;&#36866;&#37197;&#39033;&#21644;&#27491;&#21017;&#21270;&#39033;&#30340;&#20248;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;PG&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#26799;&#24230;&#21450;&#20854;&#30456;&#24212;&#30340;Lipschitz&#24120;&#25968;&#65292;&#20174;&#32780;&#20445;&#35777;&#23454;&#38469;&#27979;&#37327;&#30340;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#39033;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#35780;&#20998;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#33719;&#65288;&#26799;&#24230;&#65289;&#22270;&#20687;&#20808;&#39564;&#20998;&#24067;&#65292;&#23558;&#29983;&#25104;&#20808;&#39564;&#20316;&#20026;&#25105;&#20204;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#24341;&#20837;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#20808;&#39564;&#19982;&#22270;&#20687;&#30456;&#20301;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#38190;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase retrieval (PR) is an essential problem in a number of coherent imaging systems. This work aims at resolving the holographic phase retrieval problem in real world scenarios where the measurements are corrupted by a mixture of Poisson and Gaussian (PG) noise that stems from optical imaging systems. To solve this problem, we develop a novel algorithm based on Accelerated Wirtinger Flow that uses Score-based Diffusion models as the generative prior (AWFSD). In particular, we frame the PR problem as an optimization task that involves both a data fidelity term and a regularization term. We derive the gradient of the PG log-likelihood function along with its corresponding Lipschitz constant, ensuring a more accurate data consistency term for practical measurements. We introduce a generative prior as part of our regularization approach by using a score-based diffusion model to capture (the gradient of) the image prior distribution. We provide theoretical analysis that establishes a criti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;Davinci&#20013;&#30340;&#24515;&#36523;&#20998;&#31163;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#24369;&#21270;&#30340;&#20108;&#20803;&#35770;&#20542;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.07667</link><description>&lt;p&gt;
&#36798;&#33452;&#22855;&#20108;&#37325;&#35770;&#32773;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24515;&#36523;&#20998;&#31163;&#21644;&#20154;&#31867;&#23398;&#20064;&#32773;&#20013;&#30340;&#24515;&#36523;&#20108;&#20803;&#35770;
&lt;/p&gt;
&lt;p&gt;
Davinci the Dualist: the mind-body divide in large language models and in human learners. (arXiv:2305.07667v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;Davinci&#20013;&#30340;&#24515;&#36523;&#20998;&#31163;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#24369;&#21270;&#30340;&#20108;&#20803;&#35770;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#25991;&#29486;&#34920;&#26126;&#65292;&#20154;&#31867;&#20855;&#26377;&#30452;&#35266;&#30340;&#20108;&#20803;&#35770;&#24605;&#24819;&#65292;&#35748;&#20026;&#31934;&#31070;&#21644;&#36523;&#20307;&#26159;&#19981;&#21516;&#30340;&#23384;&#22312;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20063;&#34920;&#26126;&#65292;&#20108;&#20803;&#35770;&#26159;&#36890;&#36807;&#23398;&#20064;&#32780;&#20986;&#29616;&#30340;&#65288;&#20363;&#22914;Barlev&#65286;Shtulman&#65292;2021&#65289;&#12290;&#20294;&#23398;&#20064;&#26159;&#21542;&#36275;&#20197;&#23548;&#33268;&#20108;&#20803;&#35770;&#23578;&#19981;&#28165;&#26970;&#12290;&#36890;&#36807;&#25506;&#31350;Davinci&#65288;&#19968;&#31181;&#19981;&#20855;&#26377;&#20219;&#20309;&#20869;&#22312;&#26680;&#24515;&#30693;&#35782;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#20013;&#30340;&#24515;&#36523;&#20998;&#31163;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23398;&#20064;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Davinci&#20173;&#28982;&#20542;&#21521;&#20110;&#20108;&#20803;&#35770;&#65292;&#24182;&#19988;&#36825;&#31181;&#20559;&#35265;&#38543;&#30528;&#23398;&#20064;&#32773;&#30340;&#24402;&#32435;&#28508;&#21147;&#36880;&#28176;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large literature suggests that people are intuitive Dualists--they consider the mind ethereal, distinct from the body. Past research also shows that Dualism emerges, in part, via learning (e.g., Barlev &amp; Shtulman, 2021). But whether learning is sufficient to give rise to Dualism is unknown.The evidence from human learners does address this question because humans are endowed not only with general learning capacities but also with core knowledge capacities. And recent results suggest that core knowledge begets Dualism (Berent, Theodore &amp; Valencia, 2021; Berent, 2023). To evaluate the role of learning, here, we probe for a mind-body divide in Davinci--a large language model (LLM) that is devoid of any innate core knowledge. We show that Davinci still leans towards Dualism, and that this bias increases systematically with the learner's inductive potential. Thus, davinci (a GPT-3 model) exhibits mild Dualist tendencies, whereas its descendent, text-davinci-003 (a GPT-3.5 model), shows a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#26426;&#22120;&#19982;&#20154;&#31867;&#20799;&#31461;&#22312;&#27169;&#20223;&#21644;&#21019;&#26032;&#26041;&#38754;&#30340;&#19981;&#21516;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#38656;&#35201;&#26356;&#22810;&#30340;&#20449;&#24687;&#25165;&#33021;&#36798;&#21040;&#23401;&#23376;&#25152;&#33021;&#20570;&#21040;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07666</link><description>&lt;p&gt;
&#27169;&#20223;&#19982;&#21019;&#26032;&#65306;&#23401;&#23376;&#20204;&#33021;&#20570;&#21040;&#30340;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#23578;&#19981;&#33021;&#20570;&#21040;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?. (arXiv:2305.07666v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#26426;&#22120;&#19982;&#20154;&#31867;&#20799;&#31461;&#22312;&#27169;&#20223;&#21644;&#21019;&#26032;&#26041;&#38754;&#30340;&#19981;&#21516;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#38656;&#35201;&#26356;&#22810;&#30340;&#20449;&#24687;&#25165;&#33021;&#36798;&#21040;&#23401;&#23376;&#25152;&#33021;&#20570;&#21040;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#26159;&#21542;&#26159;&#26234;&#33021;&#20307;&#19968;&#30452;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#35270;&#35282;&#65292;&#35748;&#20026;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26159;&#22312;&#29616;&#20195;&#19990;&#30028;&#20013;&#22686;&#24378;&#25991;&#21270;&#20256;&#25773;&#30340;&#25991;&#21270;&#25216;&#26415;&#65292;&#26159;&#39640;&#25928;&#30340;&#27169;&#20223;&#24341;&#25806;&#12290;&#36890;&#36807;&#35780;&#20272;AI&#27169;&#22411;&#35774;&#35745;&#26032;&#24037;&#20855;&#21644;&#21457;&#29616;&#26032;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;AI&#27169;&#22411;&#23545;&#27169;&#20223;&#21644;&#21019;&#26032;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#20799;&#31461;&#30340;&#21453;&#24212;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#30830;&#23450;&#20174;&#29305;&#23450;&#30340;&#23398;&#20064;&#25216;&#26415;&#21644;&#25968;&#25454;&#20013;&#21487;&#25512;&#23548;&#20986;&#21738;&#20123;&#29305;&#23450;&#34920;&#31034;&#21644;&#33021;&#21147;&#65292;&#20197;&#21450;&#21738;&#20123;&#30693;&#35782;&#25110;&#25216;&#33021;&#30340;&#31532;&#19968;&#27493;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#22270;&#20687;&#65292;&#25165;&#33021;&#36798;&#21040;&#19968;&#20010;&#23401;&#23376;&#25152;&#33021;&#20570;&#21040;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much discussion about large language models and language-and-vision models has focused on whether these models are intelligent agents. We present an alternative perspective. We argue that these artificial intelligence models are cultural technologies that enhance cultural transmission in the modern world, and are efficient imitation engines. We explore what AI models can tell us about imitation and innovation by evaluating their capacity to design new tools and discover novel causal structures, and contrast their responses with those of human children. Our work serves as a first step in determining which particular representations and competences, as well as which kinds of knowledge or skill, can be derived from particular learning techniques and data. Critically, our findings suggest that machines may need more than large scale language and images to achieve what a child can do.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#24773;&#24863;&#35745;&#31639;&#30340;&#37325;&#35201;&#24615;&#12289;&#24605;&#24819;&#12289;&#26041;&#27861;&#21644;&#32467;&#26524;&#65292;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28151;&#21512;&#29616;&#23454;&#30456;&#32467;&#21512;&#30340;&#24773;&#24863;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2305.07665</link><description>&lt;p&gt;
&#24773;&#24863;&#35745;&#31639;&#32508;&#36848;&#65306;&#25361;&#25112;&#12289;&#36235;&#21183;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Affective Computing; Challenges, Trends, Applications, and Future Directions. (arXiv:2305.07665v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#24773;&#24863;&#35745;&#31639;&#30340;&#37325;&#35201;&#24615;&#12289;&#24605;&#24819;&#12289;&#26041;&#27861;&#21644;&#32467;&#26524;&#65292;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28151;&#21512;&#29616;&#23454;&#30456;&#32467;&#21512;&#30340;&#24773;&#24863;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20854;&#21517;&#65292;&#24773;&#24863;&#35745;&#31639;&#26088;&#22312;&#35782;&#21035;&#20154;&#31867;&#30340;&#24773;&#32490;&#12289;&#24773;&#24863;&#21644;&#24863;&#21463;&#12290;&#21253;&#25324;&#35821;&#35328;&#23398;&#12289;&#31038;&#20250;&#23398;&#12289;&#24515;&#29702;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#29983;&#29702;&#23398;&#31561;&#24191;&#27867;&#39046;&#22495;&#30740;&#31350;&#24773;&#24863;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#30740;&#31350;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28151;&#21512;&#29616;&#23454;&#65288;XR&#65289;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#24773;&#24863;&#35745;&#31639;&#30340;&#37325;&#35201;&#24615;&#12289;&#20854;&#24605;&#24819;&#12289;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;ML&#21644;XR&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35843;&#26597;&#21644;&#35752;&#35770;&#20102;&#24773;&#24863;&#35745;&#31639;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20197;&#21450;&#24403;&#21069;&#24773;&#24863;&#25968;&#25454;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#65292;&#20854;&#20013;&#24773;&#24863;&#35745;&#31639;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#36825;&#23558;&#24110;&#21161;&#26410;&#26469;&#23398;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#37325;&#35201;&#24615;&#21644;&#23454;&#38469;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the name suggests, affective computing aims to recognize human emotions, sentiments, and feelings. There is a wide range of fields that study affective computing, including languages, sociology, psychology, computer science, and physiology. However, no research has ever been done to determine how machine learning (ML) and mixed reality (XR) interact together. This paper discusses the significance of affective computing, as well as its ideas, conceptions, methods, and outcomes. By using approaches of ML and XR, we survey and discuss recent methodologies in affective computing. We survey the state-of-the-art approaches along with current affective data resources. Further, we discuss various applications where affective computing has a significant impact, which will aid future scholars in gaining a better understanding of its significance and practical relevance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.07663</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23450;&#37327;&#35821;&#20041;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Quantified Semantic Comparison of Convolutional Neural Networks. (arXiv:2305.07663v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24212;&#29992;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#24037;&#20316;&#21407;&#29702;&#21364;&#24456;&#38590;&#38416;&#26126;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36825;&#31867;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#27169;&#22411;&#36873;&#25321;&#36824;&#24212;&#32771;&#34385;&#20505;&#36873;&#27169;&#22411;&#22312;&#27169;&#22411;&#36879;&#26126;&#24615;&#26041;&#38754;&#22914;&#20309;&#34920;&#31034;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;CNN&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#33879;&#21517;&#25216;&#26415;&#20316;&#20026;&#22522;&#30784;&#65292;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#33719;&#24471;&#27599;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#27010;&#24565;&#30340;&#20840;&#23616;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#22312;&#27979;&#35797;&#36755;&#20837;&#19978;&#30340;&#28608;&#27963;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#24037;&#20316;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#20004;&#20010;&#19981;&#21516;&#33539;&#22260;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state-of-the-art in convolutional neural networks (CNNs) for computer vision excels in performance, while remaining opaque. But due to safety regulations for safety-critical applications, like perception for automated driving, the choice of model should also take into account how candidate models represent semantic information for model transparency reasons. To tackle this yet unsolved problem, our work proposes two methods for quantifying the similarity between semantic information in CNN latent spaces. These allow insights into both the flow and similarity of semantic information within CNN layers, and into the degree of their similitude between different networks. As a basis, we use renown techniques from the field of explainable artificial intelligence (XAI), which are used to obtain global vector representations of semantic concepts in each latent space. These are compared with respect to their activation on test inputs. When applied to three diverse object detectors and two d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#8212;&#8212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;C-LLM&#65289;&#65292;&#20197;&#21450;&#23427;&#22312;&#22797;&#26434;&#23398;&#29983;&#20316;&#21697;&#30340;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#21644;&#23457;&#26597;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#31181;&#25216;&#26415;&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.07605</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#25945;&#32946;&#30340;&#24433;&#21709;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative AI: Implications and Applications for Education. (arXiv:2305.07605v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#8212;&#8212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;C-LLM&#65289;&#65292;&#20197;&#21450;&#23427;&#22312;&#22797;&#26434;&#23398;&#29983;&#20316;&#21697;&#30340;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#21644;&#23457;&#26597;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#31181;&#25216;&#26415;&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;11&#26376;ChatGPT&#30340;&#25512;&#20986;&#24341;&#21457;&#20102;&#19968;&#20123;&#25945;&#32946;&#24037;&#20316;&#32773;&#30340;&#24656;&#24908;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#20854;&#20182;&#20154;&#30340;&#28909;&#24773;&#12290;&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#36825;&#20010;&#22823;&#20254;&#19979;&#65292;ChatGPT&#26159;&#19968;&#31995;&#21015;&#25216;&#26415;&#30340;&#20363;&#23376;&#65292;&#29992;&#20110;&#25552;&#20379;&#35745;&#31639;&#26426;&#29983;&#25104;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#20854;&#20182;&#25968;&#23383;&#21270;&#23186;&#20307;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20043;&#19968;&#8212;&#8212;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(C-LLM)&#65292;&#20197;&#21450;&#20854;&#22312;&#22797;&#26434;&#23398;&#29983;&#20316;&#21697;&#30340;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#21644;&#23457;&#26597;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22312;&#35752;&#35770;&#20013;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20869;&#22312;&#38480;&#21046;&#65292;&#21363;&#32465;&#23450;&#20110;&#35821;&#26009;&#24211;&#21450;&#20854;&#36890;&#36807;&#20108;&#36827;&#21046;&#34920;&#31034;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#22312;&#36825;&#20123;&#38480;&#21046;&#20043;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#20013;&#26032;&#20852;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
The launch of ChatGPT in November 2022 precipitated a panic among some educators while prompting qualified enthusiasm from others. Under the umbrella term Generative AI, ChatGPT is an example of a range of technologies for the delivery of computer-generated text, image, and other digitized media. This paper examines the implications for education of one generative AI technology, chatbots responding from large language models, or C-LLM. It reports on an application of a C-LLM to AI review and assessment of complex student work. In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation. Within these limits, we suggest the range of emerging and potential applications of Generative AI in education.
&lt;/p&gt;</description></item><item><title>PillarAcc&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#22686;&#24378;&#22522;&#20110;Pillar&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#65292;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#35745;&#31639;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.07522</link><description>&lt;p&gt;
PillarAcc: &#31232;&#30095;&#28857;&#20113;Pillars&#21152;&#36895;&#22120;&#8212;&#8212;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PillarAcc: Sparse PointPillars Accelerator for Real-Time Point Cloud 3D Object Detection on Edge Devices. (arXiv:2305.07522v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07522
&lt;/p&gt;
&lt;p&gt;
PillarAcc&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#22686;&#24378;&#22522;&#20110;Pillar&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#65292;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#35745;&#31639;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28857;&#20113;(PC)&#25968;&#25454;&#36827;&#34892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#31649;&#36947;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#39640;&#25928;&#30340;&#32534;&#30721;&#26159;&#28385;&#36275;&#20005;&#26684;&#36164;&#28304;&#21644;&#24310;&#36831;&#35201;&#27714;&#30340;&#20851;&#38190;&#12290;PointPillars&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#40479;&#30640;&#22270;(BEV)&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#32858;&#21512;&#21040;&#20108;&#32500;pillar&#20013;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#37319;&#29992;PointPillar&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;pillar&#32534;&#30721;&#30340;&#22266;&#26377;&#31232;&#30095;&#24615;&#65292;&#38169;&#22833;&#20102;&#22823;&#37327;&#35745;&#31639;&#20943;&#23569;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#21152;&#36895;&#31232;&#30095;&#21367;&#31215;&#22788;&#29702;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;pillar&#30340;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#22522;&#20110;pillar&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;pillar pruning&#26041;&#27861;&#23545;&#31232;&#30095;&#21270;&#26426;&#20250;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;PillarAcc&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#31232;&#30095;&#25903;&#25345;&#26426;&#21046;&#65292;&#36890;&#36807;&#36755;&#20837;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22686;&#24378;&#31232;&#30095;pillar&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D object detection using point cloud (PC) data is vital for autonomous driving perception pipelines, where efficient encoding is key to meeting stringent resource and latency requirements. PointPillars, a widely adopted bird's-eye view (BEV) encoding, aggregates 3D point cloud data into 2D pillars for high-accuracy 3D object detection. However, most state-of-the-art methods employing PointPillar overlook the inherent sparsity of pillar encoding, missing opportunities for significant computational reduction. In this study, we propose a groundbreaking algorithm-hardware co-design that accelerates sparse convolution processing and maximizes sparsity utilization in pillar-based 3D object detection networks. We investigate sparsification opportunities using an advanced pillar-pruning method, achieving an optimal balance between accuracy and sparsity. We introduce PillarAcc, a state-of-the-art sparsity support mechanism that enhances sparse pillar convolution through linear complexity input
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.07375</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#26029;&#22120;&#21527;&#65311;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#20247;&#22810;NLP&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20294;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#20294;&#26159;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#30340;&#24187;&#35273;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#22240;&#26524;&#20851;&#31995;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#30340;&#25253;&#21578;&#20559;&#35265;&#65292;&#20197;&#21450;ChatGPT&#30340;&#21319;&#32423;&#36807;&#31243;&#65292;&#22914;RLHF&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;COT&#65289;&#25216;&#26415;&#26041;&#38754;&#65292;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#21152;&#21095;&#36825;&#31181;&#22240;&#26524;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#22312;&#25552;&#31034;&#20013;&#34920;&#36798;&#22240;&#26524;&#27010;&#24565;&#30340;&#35789;&#35821;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#19988;&#23553;&#38381;&#25552;&#31034;&#27604;&#24320;&#25918;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#21477;&#23376;&#20013;&#30340;&#20107;&#20214;&#65292;ChatGPT&#25797;&#38271;&#25429;&#25417;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#30446;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24635;&#32467;&#20102;&#37096;&#32626;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#20173;&#38656;&#20811;&#26381;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.07348</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#30446;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#32508;&#36848;&#65306;&#24403;&#21069;&#29366;&#24577;&#12289;&#38480;&#21046;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects. (arXiv:2305.07348v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#30446;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24635;&#32467;&#20102;&#37096;&#32626;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#20173;&#38656;&#20811;&#26381;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#31639;&#19981;&#37197;&#21512;&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#36712;&#33258;&#21160;&#35270;&#35273;&#31995;&#32479;&#30340;&#37096;&#32626;&#65292;&#20854;&#24212;&#29992;&#33539;&#22260;&#20174;&#36712;&#36947;&#32500;&#20462;&#21040;&#22826;&#31354;&#30862;&#29255;&#28165;&#38500;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36235;&#21183;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24037;&#20316;&#22312;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20294;&#26159;&#23613;&#31649;&#22312;&#30740;&#31350;&#38454;&#27573;&#33719;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20173;&#23384;&#22312;&#38459;&#27490;&#36825;&#31181;&#26041;&#27861;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#37096;&#32626;&#36825;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#31639;&#27861;&#20173;&#28982;&#21463;&#21040;&#23569;&#37327;&#30740;&#31350;&#65292;&#32780;&#22312;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#24615;&#33021;&#19979;&#38477;&#20173;&#28982;&#38656;&#35201;&#20943;&#36731;&#12290;&#26412;&#25991;&#20027;&#35201;&#30446;&#30340;&#26159;&#20840;&#38754;&#25551;&#36848;&#24403;&#21069;&#22522;&#20110;DL&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36741;&#21161;&#30830;&#23450;&#26377;&#25928;&#37096;&#32626;DL&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the pose of an uncooperative spacecraft is an important computer vision problem for enabling the deployment of automatic vision-based systems in orbit, with applications ranging from on-orbit servicing to space debris removal. Following the general trend in computer vision, more and more works have been focusing on leveraging Deep Learning (DL) methods to address this problem. However and despite promising research-stage results, major challenges preventing the use of such methods in real-life missions still stand in the way. In particular, the deployment of such computation-intensive algorithms is still under-investigated, while the performance drop when training on synthetic and testing on real images remains to mitigate. The primary goal of this survey is to describe the current DL-based methods for spacecraft pose estimation in a comprehensive manner. The secondary goal is to help define the limitations towards the effective deployment of DL-based spacecraft pose estimat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36229;&#26641;&#20998;&#35299;&#26469;&#22686;&#24378;Datalog&#25512;&#29702;&#25928;&#29575;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#26631;&#20934;Datalog&#31639;&#27861;&#30456;&#32467;&#21512;&#20197;&#20943;&#23569;&#39069;&#22806;&#24320;&#38144;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#21644;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2305.06854</link><description>&lt;p&gt;
&#21033;&#29992;&#36229;&#26641;&#20998;&#35299;&#22686;&#24378;Datalog&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Datalog Reasoning with Hypertree Decompositions. (arXiv:2305.06854v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36229;&#26641;&#20998;&#35299;&#26469;&#22686;&#24378;Datalog&#25512;&#29702;&#25928;&#29575;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#26631;&#20934;Datalog&#31639;&#27861;&#30456;&#32467;&#21512;&#20197;&#20943;&#23569;&#39069;&#22806;&#24320;&#38144;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#21644;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21322;&#26420;&#32032;&#35780;&#20272;&#31574;&#30053;&#30340;Datalog&#25512;&#29702;&#20351;&#29992;&#20256;&#32479;&#30340;&#36830;&#25509;&#35745;&#21010;&#35780;&#20272;&#35268;&#21017;&#65292;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#23548;&#33268;&#20887;&#20313;&#21644;&#20302;&#25928;&#65292;&#23588;&#20854;&#26159;&#24403;&#35268;&#21017;&#24456;&#22797;&#26434;&#26102;&#12290;&#36229;&#26641;&#20998;&#35299;&#26377;&#21161;&#20110;&#30830;&#23450;&#26377;&#25928;&#30340;&#26597;&#35810;&#35745;&#21010;&#24182;&#20943;&#23569;&#26597;&#35810;&#21709;&#24212;&#20013;&#31867;&#20284;&#30340;&#20887;&#20313;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#36882;&#24402;Datalog&#31243;&#24207;&#30340;&#29289;&#21270;&#21644;&#22686;&#37327;&#25512;&#29702;&#23578;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#36229;&#26641;&#20998;&#35299;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#22240;&#27492;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#26041;&#38754;&#24341;&#20837;&#20102;&#19981;&#21487;&#24573;&#30053;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21033;&#29992;&#36229;&#26641;&#20998;&#35299;&#36827;&#34892;Datalog&#31243;&#24207;&#30340;&#29289;&#21270;&#21644;&#22686;&#37327;&#35780;&#20272;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#26631;&#20934;Datalog&#25512;&#29702;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#20943;&#23569;&#20998;&#35299;&#24341;&#36215;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#35268;&#21017;&#26102;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#21644;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Datalog reasoning based on the semina\"ive evaluation strategy evaluates rules using traditional join plans, which often leads to redundancy and inefficiency in practice, especially when the rules are complex. Hypertree decompositions help identify efficient query plans and reduce similar redundancy in query answering. However, it is unclear how this can be applied to materialisation and incremental reasoning with recursive Datalog programs. Moreover, hypertree decompositions require additional data structures and thus introduce nonnegligible overhead in both runtime and memory consumption. In this paper, we provide algorithms that exploit hypertree decompositions for the materialisation and incremental evaluation of Datalog programs. Furthermore, we combine this approach with standard Datalog reasoning algorithms in a modular fashion so that the overhead caused by the decompositions is reduced. Our empirical evaluation shows that, when the program contains complex rules, the combined 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.06784</link><description>&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#28040;&#36153;&#32773;&#30340;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning. (arXiv:2305.06784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;AFL&#65289;&#22240;&#36890;&#36807;&#32463;&#27982;&#25163;&#27573;&#28608;&#21169;&#25968;&#25454;&#25317;&#26377;&#32773;&#21152;&#20837;FL&#32780;&#21463;&#21040;&#24191;&#27867;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#29616;&#26377;&#24037;&#20316;&#20551;&#35774;&#22312;AFL&#24066;&#22330;&#19978;&#20165;&#23384;&#22312;&#19968;&#20010;&#25968;&#25454;&#28040;&#36153;&#32773;&#21644;&#22810;&#20010;&#25968;&#25454;&#25317;&#26377;&#32773;&#65288;&#21363;&#22404;&#26029;&#24066;&#22330;&#65289;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#25317;&#26377;&#32773;&#31454;&#26631;&#21152;&#20837;&#25968;&#25454;&#28040;&#36153;&#32773;&#36827;&#34892;FL&#12290;&#20294;&#26159;&#65292;&#22312;&#23454;&#38469;&#30340;AFL&#24066;&#22330;&#20013;&#65292;&#22810;&#20010;&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#33021;&#20250;&#31454;&#20105;&#20197;&#21560;&#24341;&#25968;&#25454;&#25317;&#26377;&#32773;&#21152;&#20837;&#20182;&#20204;&#21508;&#33258;&#30340;FL&#20219;&#21153;&#65292;&#36825;&#31181;&#20551;&#35774;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#65292;&#24182;&#25552;&#20379;&#20102;&#33021;&#22815;&#23481;&#32435;&#19981;&#21516;&#24066;&#22330;&#21160;&#24577;&#30340;&#21508;&#31181;&#33719;&#32988;&#20989;&#25968;&#30340;&#25928;&#29992;&#20272;&#35745;&#33021;&#21147;&#12290;&#22522;&#20110;&#20845;&#20010;&#24120;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20102;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auction-based Federated Learning (AFL) has attracted extensive research interest due to its ability to motivate data owners to join FL through economic means. Existing works assume that only one data consumer and multiple data owners exist in an AFL marketplace (i.e., a monopoly market). Therefore, data owners bid to join the data consumer for FL. However, this assumption is not realistic in practical AFL marketplaces in which multiple data consumers can compete to attract data owners to join their respective FL tasks. In this paper, we bridge this gap by proposing a first-of-its-kind utility-maximizing bidding strategy for data consumers in federated learning (Fed-Bidder). It enables multiple FL data consumers to compete for data owners via AFL effectively and efficiently by providing with utility estimation capabilities which can accommodate diverse forms of winning functions, each reflecting different market dynamics. Extensive experiments based on six commonly adopted benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;</title><link>http://arxiv.org/abs/2305.06657</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65306;&#30456;&#37051;&#19981;&#30830;&#23450;&#24615;&#38598;&#21644;&#21452;&#20195;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm. (arXiv:2305.06657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#19978;&#20248;&#21270;&#26368;&#24046;&#24615;&#33021;&#12290;&#32473;&#23450;&#19968;&#20010;&#20135;&#29983;&#35757;&#32451;&#26679;&#26412;&#30340;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;N-MDP&#65289;&#65292;&#35813;&#38598;&#21512;&#21253;&#21547;&#36890;&#36807;&#23545;N-MDP&#36827;&#34892;&#26576;&#20123;&#25200;&#21160;&#32780;&#33719;&#24471;&#30340;MDP&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#27604;&#29616;&#26377;&#38598;&#21512;&#26356;&#23454;&#38469;&#30340;MDP&#12290;&#20351;&#29992;&#36825;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;RL&#31639;&#27861;&#65292;&#21517;&#20026;ARQ-Learning&#65292;&#29992;&#20110;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#35823;&#24046;&#30028;&#24182;&#35777;&#26126;&#23427;&#19982;Q-Learning&#21644;&#40065;&#26834;Q-Learning&#65288;&#21363;&#29616;&#26377;&#30340;&#40065;&#26834;RL&#26041;&#27861;&#65289;&#19968;&#26679;&#24555;&#22320;&#25910;&#25947;&#65292;&#21516;&#26102;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23558;ARQ-Learning&#25193;&#23637;&#21040;&#22823;&#22411;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;&#20851;&#38190;&#29942;&#39048;&#12290;&#21033;&#29992;&#36825;&#19968;&#25216;&#26415;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;PRQ-Learning&#12290;&#25509;&#30528;&#65292;&#23558;&#20854;&#19982;DQN&#21644;DDPG&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#20998;&#21035;&#24320;&#21457;&#20102;PR-DQN&#21644;PR-DDPG&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#39046;&#22495;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) aims at learning a policy that optimizes the worst-case performance over an uncertainty set. Given nominal Markov decision process (N-MDP) that generates samples for training, the set contains MDPs obtained by some perturbations from N-MDP. In this paper, we introduce a new uncertainty set containing more realistic MDPs in practice than the existing sets. Using this uncertainty set, we present a robust RL, named ARQ-Learning, for tabular cases. Also, we characterize the finite-time error bounds and prove that it converges as fast as Q-Learning and robust Q-Learning (i.e., the state-of-the-art robust RL method) while providing better robustness for real applications. We propose {\em pessimistic agent} that efficiently tackles the key bottleneck for the extension of ARQ-Learning into large or continuous state spaces. Using this technique, we first propose PRQ-Learning. To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG, respect
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HAHE&#27169;&#22411;&#65292;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#23398;&#20064;&#20102;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#39034;&#24207;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06588</link><description>&lt;p&gt;
HAHE: &#22522;&#20110;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22411;&#29992;&#20110;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level. (arXiv:2305.06588v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06588
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HAHE&#27169;&#22411;&#65292;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#23398;&#20064;&#20102;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#39034;&#24207;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#26159;&#20540;&#24471;&#23581;&#35797;&#30340;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#8212;&#8212;HAHE&#65292;&#21253;&#25324;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#34920;&#31034;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#37319;&#29992;&#36229;&#22270;&#21452;&#37325;&#27880;&#24847;&#21147;&#23618;&#65292;&#20840;&#23616;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21487;&#20197;&#24314;&#27169;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#65307;&#32780;&#37319;&#29992;&#24322;&#36136;&#24615;&#33258;&#27880;&#24847;&#23618;&#65292;&#23616;&#37096;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21017;&#21487;&#20197;&#23398;&#20064;H-Facts&#20869;&#37096;&#30340;&#39034;&#24207;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HAHE&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs' representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06360</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#36951;&#24536;&#30340;&#39046;&#22495;&#65306;&#19968;&#31687;&#32508;&#36848;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#38656;&#35201;&#21024;&#38500;&#25110;&#20462;&#25913;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#21644;&#20934;&#30830;&#65292;&#20294;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#65289;&#65292;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#33879;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25915;&#20987;&#22797;&#26434;&#24615;&#12289;&#26631;&#20934;&#21270;&#12289;&#21487;&#36716;&#31227;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#35752;&#35770;MU&#30340;&#28508;&#22312;&#30410;&#22788;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
&lt;/p&gt;</description></item><item><title>&#34917;&#19969;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06217</link><description>&lt;p&gt;
&#34917;&#19969;&#23398;&#20064;&#65306;&#23454;&#29616;&#36328;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#28304;&#30340;&#32508;&#21512;&#20998;&#26512;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources. (arXiv:2305.06217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06217
&lt;/p&gt;
&lt;p&gt;
&#34917;&#19969;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25552;&#20379;&#20102;&#35768;&#22810;&#22686;&#24378;&#24739;&#32773;&#25252;&#29702;&#12289;&#20154;&#21475;&#20581;&#24247;&#21644;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#24037;&#20316;&#27969;&#31243;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#65292;&#29616;&#23454;&#20013;&#30340;&#20020;&#24202;&#21644;&#25104;&#26412;&#25928;&#30410;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#36825;&#31687;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;&#34917;&#19969;&#23398;&#20064;&#8221;&#65288;PL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#38598;&#25104;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#65288;&#20363;&#22914;&#65292;&#20020;&#24202;&#20813;&#36153;&#25991;&#26412;&#12289;&#21307;&#23398;&#22270;&#20687;&#12289;&#32452;&#23398;&#65289;&#21644;&#20998;&#24067;&#22312;&#19981;&#21516;&#23433;&#20840;&#31449;&#28857;&#19978;&#30340;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;PL&#20801;&#35768;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#34917;&#19969;&#23398;&#20064;&#30340;&#27010;&#24565;&#20197;&#21450;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24403;&#21069;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#30340;&#28508;&#22312;&#26426;&#20250;&#21644;&#36866;&#29992;&#25968;&#25454;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) in healthcare presents numerous opportunities for enhancing patient care, population health, and healthcare providers' workflows. However, the real-world clinical and cost benefits remain limited due to challenges in data privacy, heterogeneous data sources, and the inability to fully leverage multiple data modalities. In this perspective paper, we introduce "patchwork learning" (PL), a novel paradigm that addresses these limitations by integrating information from disparate datasets composed of different data modalities (e.g., clinical free-text, medical images, omics) and distributed across separate and secure sites. PL allows the simultaneous utilization of complementary data sources while preserving data privacy, enabling the development of more holistic and generalizable ML models. We present the concept of patchwork learning and its current implementations in healthcare, exploring the potential opportunities and applicable data sources for addressing various
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.05280</link><description>&lt;p&gt;
VCSUM&#65306;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VCSUM: A Versatile Chinese Meeting Summarization Dataset. (arXiv:2305.05280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05280
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26032;&#38395;&#21644;&#32842;&#22825;&#25688;&#35201;&#30456;&#27604;&#65292;&#30001;&#20110;&#25968;&#25454;&#21463;&#38480;&#65292;&#20250;&#35758;&#25688;&#35201;&#30340;&#21457;&#23637;&#21463;&#21040;&#26497;&#22823;&#30340;&#20943;&#36895;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#25105;&#20204;&#22768;&#31216;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#22810;&#21151;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20026;&#27599;&#20010;&#20250;&#35758;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#20027;&#39064;&#21010;&#20998;&#12289;&#22836;&#26465;&#12289;&#20998;&#27573;&#25688;&#35201;&#12289;&#25972;&#20010;&#20250;&#35758;&#25688;&#35201;&#21644;&#26174;&#35201;&#21477;&#23376;&#31561;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#23454;&#20102;VCSum&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#32452;&#20851;&#20110;&#19981;&#21516;&#19979;&#28216;&#25688;&#35201;&#20219;&#21153;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#30740;&#31350;VCSum&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312; \url{https://github.com/hahahawu/VCSum} &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research. The dataset and code will be released at \url{https://github.com/hahahawu/VCSum}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05050</link><description>&lt;p&gt;
ANALOGICAL- &#19968;&#31181;&#26032;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#31867;&#27604;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20197;&#35789;&#32423;&#21035;&#30340;&#31867;&#27604;&#20026;&#24418;&#24335;&#30340;&#31867;&#27604;&#22312;&#34913;&#37327;&#35832;&#22914;word2vec&#20043;&#31867;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#30340;&#36136;&#37327;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20027;&#35201;&#26681;&#25454;GLUE&#21644;SuperGLUE&#31561;&#22522;&#20934;&#30340;&#22806;&#22312;&#37327;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#22312;LLMs&#26159;&#21542;&#33021;&#22815;&#22312;&#38271;&#25991;&#26412;&#20013;&#32472;&#21046;&#31867;&#27604;&#30340;&#26041;&#38754;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#20197;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#23545;LLMs&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20998;&#21035;&#20026; (i)&#21333;&#35789;&#12289;(ii)&#21333;&#35789;vs&#21477;&#23376;&#12289;(iii)&#35821;&#27861;&#12289;(iv)&#21542;&#23450;&#12289;(v)&#34164;&#21547;&#21644;(vi)&#38544;&#21947;&#12290;&#21033;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;(&#20363;&#22914;&#65292;&#8220;&#25105;&#33021;&#35828;&#20004;&#31181;&#35821;&#35328;&#8221;&#24212;&#35813;&#26356;&#25509;&#36817;&#8220;&#25105;&#26159;&#21452;&#35821;&#30340;&#8221;&#65292;&#32780;&#8220;&#25105;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#21644;&#8220;&#25105;&#19981;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#24212;&#35813;&#26159;&#27491;&#20132;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space (e.g., "I can speak two languages" should be closer to "I am bilingual" while "I like chocolate" and "I do not like chocolate" should be orthog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#20219;&#26694;&#26550;&#65292;&#24110;&#21161;&#38750;&#19987;&#23478;&#23454;&#29616;&#22312;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#20013;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#20449;&#20219;&#30340;&#28508;&#21147;&#65307;&#30456;&#20851;&#30740;&#31350;&#21457;&#29616;&#20102;&#26576;&#20123;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#30340;&#29992;&#25143;&#20449;&#20219;&#35823;&#35299;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#25269;&#21046;&#35774;&#35745;&#20197;&#25216;&#26415;&#20026;&#20013;&#24515;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#20132;&#20114;&#26041;&#24335;&#30340;&#36235;&#21183;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.03306</link><description>&lt;p&gt;
&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#20219;&#26694;&#26550;&#65306;&#19968;&#20010;&#20154;&#26426;&#20132;&#20114;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Human-centered trust framework: An HCI perspective. (arXiv:2305.03306v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#20219;&#26694;&#26550;&#65292;&#24110;&#21161;&#38750;&#19987;&#23478;&#23454;&#29616;&#22312;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#20013;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#20449;&#20219;&#30340;&#28508;&#21147;&#65307;&#30456;&#20851;&#30740;&#31350;&#21457;&#29616;&#20102;&#26576;&#20123;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#30340;&#29992;&#25143;&#20449;&#20219;&#35823;&#35299;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#25269;&#21046;&#35774;&#35745;&#20197;&#25216;&#26415;&#20026;&#20013;&#24515;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#20132;&#20114;&#26041;&#24335;&#30340;&#36235;&#21183;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20986;&#21457;&#28857;&#22522;&#20110;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#29992;&#25143;&#20449;&#20219;&#35805;&#39064;&#30340;&#35752;&#35770;&#65292;&#26088;&#22312;&#25552;&#20986;&#26032;&#39062;&#30340;&#20197;&#20449;&#20219;&#20026;&#36741;&#21161;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#65292;&#20174;&#32780;&#20419;&#36827;&#24403;&#21069;&#25216;&#26415;&#30340;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65288;HCTFrame&#65289;&#65292;&#20197;&#25351;&#23548;&#38750;&#19987;&#23478;&#22312;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#20013;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#20449;&#20219;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#25991;&#29486;&#32508;&#36848;&#30340;&#21457;&#29616;&#36827;&#34892;&#25968;&#25454;&#19977;&#35282;&#21270;&#65292;&#21487;&#28040;&#38500;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#20851;&#20110;&#29992;&#25143;&#20449;&#20219;&#30340;&#19968;&#20123;&#35823;&#35299;&#65292;&#24182;&#36827;&#34892;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#24515;&#29702;&#27979;&#37327;&#37327;&#34920;&#22312;&#26144;&#23556;&#28508;&#22312;&#29992;&#25143;&#20449;&#20219;&#30772;&#22351;&#21644;&#25285;&#24551;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20027;&#35201;&#36129;&#29486;&#20110;&#25269;&#21046;&#35774;&#35745;&#20197;&#25216;&#26415;&#20026;&#20013;&#24515;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#20132;&#20114;&#26041;&#24335;&#30340;&#36235;&#21183;&#65292;&#36825;&#21487;&#33021;&#26368;&#32456;&#23548;&#33268;&#26356;&#22810;&#23454;&#38469;&#21644;&#24863;&#30693;&#19978;&#30340;&#20449;&#20219;&#30772;&#35010;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#25351;&#23548;&#31995;&#32479;&#35774;&#35745;&#20154;&#21592;&#22914;&#20309;&#26144;&#23556;&#21644;&#23450;&#20041;&#29992;&#25143;&#20449;&#20219;&#20197;&#21450;&#31038;&#20250;&#20262;&#29702;&#21644;&#32452;&#32455;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rationale of this work is based on the current user trust discourse of Artificial Intelligence (AI). We aim to produce novel HCI approaches that use trust as a facilitator for the uptake (or appropriation) of current technologies. We propose a framework (HCTFrame) to guide non-experts to unlock the full potential of user trust in AI design. Results derived from a data triangulation of findings from three literature reviews demystify some misconceptions of user trust in computer science and AI discourse, and three case studies are conducted to assess the effectiveness of a psychometric scale in mapping potential users' trust breakdowns and concerns. This work primarily contributes to the fight against the tendency to design technical-centered vulnerable interactions, which can eventually lead to additional real and perceived breaches of trust. The proposed framework can be used to guide system designers on how to map and define user trust and the socioethical and organisational need
&lt;/p&gt;</description></item><item><title>CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.00969</link><description>&lt;p&gt;
CryCeleb: &#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00969
&lt;/p&gt;
&lt;p&gt;
CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Ubenwa CryCeleb&#25968;&#25454;&#38598;&#8212;&#8212;&#19968;&#20010;&#26631;&#35760;&#30340;&#23156;&#20799;&#21741;&#22768;&#25910;&#38598;&#65292;&#20197;&#21450;&#38468;&#24102;&#30340;CryCeleb 2023&#20219;&#21153;&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#20844;&#20849;&#35828;&#35805;&#20154;&#39564;&#35777;&#25361;&#25112;&#12290;&#25105;&#20204;&#37322;&#25918;&#20986;786&#21517;&#26032;&#29983;&#20799;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#20197;&#40723;&#21169;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23454;&#20307;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00382</link><description>&lt;p&gt;
&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database. (arXiv:2305.00382v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23454;&#20307;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#20363;&#22914;&#28431;&#27934;&#35780;&#20272;&#21644;&#23041;&#32961;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#30340;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#12289;&#20197;&#21450;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#12289;&#21551;&#21457;&#24335;&#35268;&#21017;&#21644;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#23454;&#20307;&#39044;&#27979;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs have shown promise for several cybersecurity tasks, such as vulnerability assessment and threat analysis. In this work, we present a new method for constructing a vulnerability knowledge graph from information in the National Vulnerability Database (NVD). Our approach combines named entity recognition (NER), relation extraction (RE), and entity prediction using a combination of neural models, heuristic rules, and knowledge graph embeddings. We demonstrate how our method helps to fix missing entities in knowledge graphs used for cybersecurity and evaluate the performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Temporal Adversarial Augmentation&#65288;TA&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#35270;&#39057;&#22686;&#24378;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#26102;&#38388;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#39057;&#29255;&#27573;&#30340;&#27880;&#24847;&#20998;&#24067;&#12290;&#21033;&#29992;TA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Temporal Video Adversarial Fine-tuning&#65288;TAF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#35270;&#39057;&#34920;&#31034;&#24182;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14601</link><description>&lt;p&gt;
&#29992;&#26102;&#24207;&#23545;&#25239;&#22686;&#24378;&#25216;&#26415;&#25913;&#36827;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improve Video Representation with Temporal Adversarial Augmentation. (arXiv:2304.14601v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Temporal Adversarial Augmentation&#65288;TA&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#35270;&#39057;&#22686;&#24378;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#26102;&#38388;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#39057;&#29255;&#27573;&#30340;&#27880;&#24847;&#20998;&#24067;&#12290;&#21033;&#29992;TA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Temporal Video Adversarial Fine-tuning&#65288;TAF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#35270;&#39057;&#34920;&#31034;&#24182;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#20197;&#36866;&#24403;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#23545;&#25239;&#22686;&#24378;&#26377;&#21161;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#35270;&#39057;&#22686;&#24378;&#25216;&#26415;&#8212;&#8212;Temporal Adversarial Augmentation (TA)&#12290;&#19982;&#20256;&#32479;&#30340;&#23545;&#25239;&#22686;&#24378;&#19981;&#21516;&#65292;TA&#19987;&#20026;&#36890;&#36807;&#26368;&#22823;&#21270;&#26102;&#38388;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#39057;&#29255;&#27573;&#30340;&#27880;&#24847;&#20998;&#24067;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;TA&#23558;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#26102;&#38388;&#35270;&#35282;&#65292;&#36825;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#28966;&#28857;&#12290;&#20351;&#29992;&#36825;&#20123;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#20462;&#22797;&#20102;&#19981;&#24179;&#34913;&#30340;&#26102;&#38388;&#20449;&#24687;&#24863;&#30693;&#32570;&#38519;&#65292;&#24182;&#22686;&#24378;&#20102;&#25269;&#24481;&#26102;&#38388;&#20559;&#31227;&#30340;&#33021;&#21147;&#65292;&#26368;&#32456;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#21033;&#29992;TA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Temporal Video Adversarial Fine-tuning (TAF)&#26694;&#26550;&#26469;&#25913;&#36827;&#35270;&#39057;&#34920;&#31034;&#12290;TAF&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#22411;&#26080;&#20851;&#12289;&#21487;&#35299;&#37322;&#24615;&#21451;&#22909;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works reveal that adversarial augmentation benefits the generalization of neural networks (NNs) if used in an appropriate manner. In this paper, we introduce Temporal Adversarial Augmentation (TA), a novel video augmentation technique that utilizes temporal attention. Unlike conventional adversarial augmentation, TA is specifically designed to shift the attention distributions of neural networks with respect to video clips by maximizing a temporal-related loss function. We demonstrate that TA will obtain diverse temporal views, which significantly affect the focus of neural networks. Training with these examples remedies the flaw of unbalanced temporal information perception and enhances the ability to defend against temporal shifts, ultimately leading to better generalization. To leverage TA, we propose Temporal Video Adversarial Fine-tuning (TAF) framework for improving video representations. TAF is a model-agnostic, generic, and interpretability-friendly training strategy. We
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#24320;&#21457;&#30340;Segment Anything Model&#65288;SAM&#65289;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#26377;&#25928;&#20998;&#21106;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#23545;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#39033;&#30740;&#31350;&#25506;&#35752;SAM&#22312;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#39033;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.13000</link><description>&lt;p&gt;
&#20174;&#31354;&#38388;&#20013;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment anything, from space?. (arXiv:2304.13000v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13000
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;Segment Anything Model&#65288;SAM&#65289;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#26377;&#25928;&#20998;&#21106;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#23545;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#39033;&#30740;&#31350;&#25506;&#35752;SAM&#22312;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#39033;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20026;&#35270;&#35273;&#20219;&#21153;&#19987;&#38376;&#24320;&#21457;&#30340;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#34987;&#31216;&#20026;&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#12290;SAM&#21487;&#20197;&#26681;&#25454;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#20998;&#21106;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#12290;&#20316;&#32773;&#20204;&#22312;&#22823;&#37327;&#30340;&#35270;&#35273;&#22522;&#20934;&#20219;&#21153;&#19978;&#30740;&#31350;&#20102;SAM&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;SAM&#36890;&#24120;&#36798;&#21040;&#20102;&#19982;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#30456;&#20284;&#25110;&#26377;&#26102;&#29978;&#33267;&#36229;&#36234;&#20854;&#35782;&#21035;&#31934;&#24230;&#12290;SAM&#22312;&#20998;&#21106;&#26041;&#38754;&#30340;&#21331;&#36234;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#20174;&#20107;&#33258;&#28982;&#22270;&#20687;&#30740;&#31350;&#30340;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SAM&#30340;&#21331;&#36234;&#24615;&#33021;&#26159;&#21542;&#25193;&#23637;&#21040;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#65292;&#24182;&#24110;&#21161;&#25351;&#23548;&#31038;&#21306;&#23545;&#20854;&#21457;&#23637;&#30340;&#22238;&#24212;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#22810;&#26679;&#21270;&#21644;&#24191;&#27867;&#30740;&#31350;&#36807;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#30740;&#31350;SAM&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SAM&#36890;&#24120;&#22312;&#31354;&#20013;&#22270;&#20687;&#19978;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20250;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the first foundation model developed specifically for vision tasks was developed, termed the "Segment Anything Model" (SAM). SAM can segment objects in input imagery based upon cheap input prompts, such as one (or more) points, a bounding box, or a mask. The authors examined the zero-shot image segmentation accuracy of SAM on a large number of vision benchmark tasks and found that SAM usually achieved recognition accuracy similar to, or sometimes exceeding, vision models that had been trained on the target tasks. The impressive generalization of SAM for segmentation has major implications for vision researchers working on natural imagery. In this work, we examine whether SAM's impressive performance extends to overhead imagery problems, and help guide the community's response to its development. We examine SAM's performance on a set of diverse and widely-studied benchmark tasks. We find that SAM does often generalize well to overhead imagery, although it fails in some cases d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#27979;&#35797;&#28085;&#30422;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.12986</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Measuring Massive Multitask Chinese Understanding. (arXiv:2304.12986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#27979;&#35797;&#28085;&#30422;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#21457;&#27491;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#30456;&#24212;&#30340;&#33021;&#21147;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26377;15&#20010;&#23376;&#20219;&#21153;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;8&#20010;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#24179;&#22343;&#27604;&#34920;&#29616;&#26368;&#24046;&#30340;&#27169;&#22411;&#39640;&#20986;&#36817;22&#20010;&#30334;&#20998;&#28857;&#12290;&#22312;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#20013;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24179;&#22343;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#22343;&#26410;&#36229;&#36807;0.5&#12290;&#22312;&#23376;&#39046;&#22495;&#20013;&#65292;&#21482;&#26377;GPT-3.5-turbo&#27169;&#22411;&#22312;&#20020;&#24202;&#21307;&#23398;&#20013;&#23454;&#29616;&#20102;0.703&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#65292;&#36825;&#26159;&#25152;&#26377;&#27169;&#22411;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#26368;&#39640;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#20165;&#36798;&#21040;0.259&#12290;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#22810;&#20010;&#23398;&#31185;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worst-performing models by nearly 22 percentage points on average. Across the four major domains, the average zero-shot accuracy of all models did not exceed 0.5. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot accuracy of 0.703 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.259. By comprehensively evaluating the breadth and depth of knowledge across multiple discipli
&lt;/p&gt;</description></item><item><title>AGI&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#65292;&#21487;&#20197;&#24314;&#31435;e-learning&#24179;&#21488;&#12289;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#65292;&#24357;&#34917;&#20256;&#32479;AI&#27169;&#22411;&#22240;&#21463;&#38480;&#20110;&#25968;&#25454;&#21644;&#20154;&#38469;&#20132;&#20114;&#38480;&#21046;&#32780;&#26080;&#27861;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2304.12479</link><description>&lt;p&gt;
&#29992;&#20110;&#25945;&#32946;&#30340;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence (AGI) for Education. (arXiv:2304.12479v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12479
&lt;/p&gt;
&lt;p&gt;
AGI&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#65292;&#21487;&#20197;&#24314;&#31435;e-learning&#24179;&#21488;&#12289;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#65292;&#24357;&#34917;&#20256;&#32479;AI&#27169;&#22411;&#22240;&#21463;&#38480;&#20110;&#25968;&#25454;&#21644;&#20154;&#38469;&#20132;&#20114;&#38480;&#21046;&#32780;&#26080;&#27861;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;&#22914;GPT-4&#21644;ChatGPT&#65289;&#30340;&#20986;&#29616;&#65292;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#20316;&#20026;&#26410;&#26469;&#25216;&#26415;&#24050;&#32463;&#24471;&#21040;&#20840;&#29699;&#35748;&#21487;&#12290;AGI&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#26426;&#31995;&#32479;&#22797;&#21046;&#20154;&#31867;&#26234;&#33021;&#65292;&#26159;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#38024;&#23545;&#26377;&#38480;&#33539;&#22260;&#30340;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#65292;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#33021;&#26080;&#27861;&#32771;&#34385;&#25945;&#32946;&#20013;&#22797;&#26434;&#30340;&#20154;&#38469;&#21160;&#24577;&#12290;&#21463;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#39537;&#21160;&#65292;AGI&#20195;&#34920;&#20102;&#26426;&#22120;&#22312;&#25191;&#34892;&#38656;&#35201;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#20363;&#22914;&#25512;&#29702;&#12289;&#35299;&#20915;&#38382;&#39064;&#12289;&#20570;&#20986;&#20915;&#31574;&#65292;&#29978;&#33267;&#29702;&#35299;&#20154;&#31867;&#24773;&#24863;&#21644;&#31038;&#20132;&#20114;&#21160;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;AGI&#30340;&#20851;&#38190;&#27010;&#24565;&#12289;&#33021;&#21147;&#12289;&#33539;&#22260;&#21644;&#22312;&#26410;&#26469;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#24314;&#31435;e-learning&#24179;&#21488;&#21644;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. AGI aims to replicate human intelligence through computer systems, which is one of the critical technologies having the potential to revolutionize the field of education. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This work reviews AGI's key concepts, capabilities, scope, and potential within future education, including setting e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#30340;&#24207;&#21015;&#25512;&#33616;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;-&#31526;&#21495;SR&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#29305;&#24449;&#23884;&#20837;&#21644;&#36923;&#36753;&#23884;&#20837;&#20998;&#31163;&#65292;SR-PLR&#32467;&#21512;&#30456;&#20284;&#24615;&#21305;&#37197;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#21475;&#21619;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#28436;&#21464;&#12290;</title><link>http://arxiv.org/abs/2304.11383</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#30340;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommendation with Probabilistic Logical Reasoning. (arXiv:2304.11383v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#30340;&#24207;&#21015;&#25512;&#33616;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;-&#31526;&#21495;SR&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#29305;&#24449;&#23884;&#20837;&#21644;&#36923;&#36753;&#23884;&#20837;&#20998;&#31163;&#65292;SR-PLR&#32467;&#21512;&#30456;&#20284;&#24615;&#21305;&#37197;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#21475;&#21619;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#31526;&#21495;&#23398;&#20064;&#26159;&#24207;&#21015;&#25512;&#33616;&#65288;SR&#65289;&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#31070;&#32463;-&#31526;&#21495;SR&#27169;&#22411;&#23637;&#31034;&#20102;&#23427;&#20204;&#20351;SR&#20855;&#22791;&#24182;&#21457;&#24863;&#30693;&#21644;&#35748;&#30693;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;-&#31526;&#21495;SR&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#65292;&#22914;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#34920;&#31034;&#29992;&#25143;&#21644;&#29289;&#21697;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;Deep Neural Network&#65288;DNN&#65289;SR&#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#31216;&#20026;&#24102;&#26377;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#30340;&#24207;&#21015;&#25512;&#33616;&#65288;&#31616;&#31216;SR-PLR&#65289;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;DNN&#21644;&#27010;&#29575;&#36923;&#36753;&#32593;&#32476;&#20013;&#35299;&#24320;&#29305;&#24449;&#23884;&#20837;&#21644;&#36923;&#36753;&#23884;&#20837;&#65292;&#20801;&#35768;SR-PLR&#20174;&#30456;&#20284;&#24615;&#21305;&#37197;&#21644;&#36923;&#36753;&#25512;&#29702;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#21475;&#21619;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#28436;&#21464;&#65292;SR-PLR&#20351;&#29992;&#27010;&#29575;&#26041;&#27861;&#23884;&#20837;&#29992;&#25143;&#21644;&#29289;&#21697;&#65292;&#24182;&#23545;&#29992;&#25143;&#30340;&#20132;&#20114;&#27169;&#24335;&#36827;&#34892;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning and symbolic learning are two frequently employed methods in Sequential Recommendation (SR). Recent neural-symbolic SR models demonstrate their potential to enable SR to be equipped with concurrent perception and cognition capacities. However, neural-symbolic SR remains a challenging problem due to open issues like representing users and items in logical reasoning. In this paper, we combine the Deep Neural Network (DNN) SR models with logical reasoning and propose a general framework named Sequential Recommendation with Probabilistic Logical Reasoning (short for SR-PLR). This framework allows SR-PLR to benefit from both similarity matching and logical reasoning by disentangling feature embedding and logic embedding in the DNN and probabilistic logic network. To better capture the uncertainty and evolution of user tastes, SR-PLR embeds users and items with a probabilistic method and conducts probabilistic logical reasoning on users' interaction patterns. Then the feature a
&lt;/p&gt;</description></item><item><title>Metropolis&#31639;&#27861;&#30340;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#36827;&#21270;&#31639;&#27861;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#23436;&#21892;&#12290;</title><link>http://arxiv.org/abs/2304.10848</link><description>&lt;p&gt;
Metropolis&#31639;&#27861;&#22312;&#22788;&#29702;&#23616;&#37096;&#26368;&#20248;&#26102;&#30340;&#25928;&#26524;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Well Does the Metropolis Algorithm Cope With Local Optima?. (arXiv:2304.10848v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10848
&lt;/p&gt;
&lt;p&gt;
Metropolis&#31639;&#27861;&#30340;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#36827;&#21270;&#31639;&#27861;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#23436;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metropolis&#31639;&#27861;&#65288;MA&#65289;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#20598;&#23572;&#25509;&#21463;&#27425;&#20248;&#35299;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#24182;&#20197;&#20005;&#26684;&#30340;&#26041;&#24335;&#29702;&#35299;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;CLIFF&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;MA&#36827;&#34892;&#20102;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#12290;&#38500;&#20102;&#19968;&#20010;&#23616;&#37096;&#26368;&#20248;&#35299;&#22806;&#65292;cliff&#20989;&#25968;&#21521;&#20840;&#23616;&#26368;&#20248;&#35299;&#21333;&#35843;&#36882;&#22686;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20248;&#21270;cliff&#20989;&#25968;&#65292;MA&#21482;&#38656;&#35201;&#19968;&#27425;&#25509;&#21463;&#19968;&#20010;&#21155;&#36136;&#35299;&#12290;&#23613;&#31649;&#30475;&#36215;&#26469;&#36825;&#26159;MA&#20174;&#20854;&#20027;&#35201;&#24037;&#20316;&#21407;&#29702;&#20013;&#33719;&#21033;&#30340;&#29702;&#24819;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#25105;&#20204;&#30340;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#34920;&#26126;&#36825;&#19968;&#24076;&#26395;&#24182;&#27809;&#26377;&#23454;&#29616;&#12290;&#21363;&#20351;&#22312;&#26368;&#20248;&#28201;&#24230;&#19979;&#65288;MA&#30340;&#21807;&#19968;&#21442;&#25968;&#65289;&#65292;MA&#20248;&#21270;&#22823;&#22810;&#25968;cliff&#20989;&#25968;&#30340;&#25928;&#29575;&#20063;&#19981;&#22914;&#31616;&#21333;&#30340;&#31934;&#33521;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#65292;&#21518;&#32773;&#21482;&#33021;&#36890;&#36807;&#29983;&#25104;&#21487;&#33021;&#30456;&#36317;&#24456;&#36828;&#30340;&#20248;&#31168;&#35299;&#26469;&#31163;&#24320;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#23545;MA&#20026;&#20160;&#20040;&#26377;&#25928;&#30340;&#29702;&#35299;&#38656;&#35201;&#36827;&#19968;&#27493;&#23436;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Metropolis algorithm (MA) is a classic stochastic local search heuristic. It avoids getting stuck in local optima by occasionally accepting inferior solutions. To better and in a rigorous manner understand this ability, we conduct a mathematical runtime analysis of the MA on the CLIFF benchmark. Apart from one local optimum, cliff functions are monotonically increasing towards the global optimum. Consequently, to optimize a cliff function, the MA only once needs to accept an inferior solution. Despite seemingly being an ideal benchmark for the MA to profit from its main working principle, our mathematical runtime analysis shows that this hope does not come true. Even with the optimal temperature (the only parameter of the MA), the MA optimizes most cliff functions less efficiently than simple elitist evolutionary algorithms (EAs), which can only leave the local optimum by generating a superior solution possibly far away. This result suggests that our understanding of why the MA is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.09868</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?. (arXiv:2304.09868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#20854;&#35745;&#31639;&#26114;&#36149;&#30340;&#31751;&#20998;&#37197;&#27493;&#39588;&#65292;&#23427;&#38754;&#20020;&#30528;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#23569;&#37327;&#35889;&#34920;&#31034;&#30340;&#32858;&#21512;&#25968;&#25454;&#28857;&#65292;&#28982;&#21518;&#22312;&#21387;&#32553;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#26631;&#20934;&#30340;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#26368;&#21518;&#23558;&#21387;&#32553;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#32467;&#26524;&#26144;&#23556;&#22238;&#21407;&#22987;&#25968;&#25454;&#38598;&#20197;&#21457;&#29616;&#31751;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support vector clustering is an important clustering method. However, it suffers from a scalability issue due to its computational expensive cluster assignment step. In this paper we accelertate the support vector clustering via spectrum-preserving data compression. Specifically, we first compress the original data set into a small amount of spectrally representative aggregated data points. Then, we perform standard support vector clustering on the compressed data set. Finally, we map the clustering results of the compressed data set back to discover the clusters in the original data set. Our extensive experimental results on real-world data set demonstrate dramatically speedups over standard support vector clustering without sacrificing clustering quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#19979;&#28216;&#19987;&#23478;&#25968;&#25454;&#20174;&#31163;&#32447;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24615;&#22320;&#26597;&#35810;&#30456;&#20851;&#34892;&#20026;&#65288;&#21253;&#25324;&#35768;&#22810;&#27425;&#20248;&#34892;&#20026;&#65289;&#65292;&#23454;&#29616;&#20102;&#34892;&#20026;&#26816;&#32034;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.08742</link><description>&lt;p&gt;
&#34892;&#20026;&#26816;&#32034;&#65306;&#36890;&#36807;&#26597;&#35810;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#23454;&#29616;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets. (arXiv:2304.08742v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#19979;&#28216;&#19987;&#23478;&#25968;&#25454;&#20174;&#31163;&#32447;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24615;&#22320;&#26597;&#35810;&#30456;&#20851;&#34892;&#20026;&#65288;&#21253;&#25324;&#35768;&#22810;&#27425;&#20248;&#34892;&#20026;&#65289;&#65292;&#23454;&#29616;&#20102;&#34892;&#20026;&#26816;&#32034;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#20351;&#26426;&#22120;&#20154;&#23398;&#20064;&#26032;&#30340;&#35270;&#35273;&#21160;&#20316;&#25216;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#65292;&#26377;&#35768;&#22810;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#33539;&#24335;&#26159;&#21033;&#29992;&#22823;&#22411;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#35768;&#22810;&#34892;&#20026;&#65292;&#28982;&#21518;&#20351;&#29992;&#23569;&#37327;&#20219;&#21153;&#29305;&#23450;&#30340;&#20154;&#31867;&#30417;&#30563;&#65288;&#21363;&#20171;&#20837;&#25110;&#28436;&#31034;&#65289;&#26469;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;&#20294;&#26159;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;&#29421;&#31364;&#30340;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#25968;&#25454;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#22312;&#20110;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#19981;&#20165;&#20026;&#20195;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36824;&#21487;&#20197;&#20026;&#20195;&#29702;&#30340;&#23398;&#20064;&#25552;&#20379;&#26377;&#20851;&#20808;&#21069;&#25968;&#25454;&#31867;&#22411;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#19979;&#28216;&#19987;&#23478;&#25968;&#25454;&#20174;&#31163;&#32447;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24615;&#22320;&#26597;&#35810;&#30456;&#20851;&#34892;&#20026;&#65288;&#21253;&#25324;&#35768;&#22810;&#27425;&#20248;&#34892;&#20026;&#65289;&#12290;&#28982;&#21518;&#20195;&#29702;&#34987;&#32852;&#21512;&#35757;&#32451;&#22312;&#19987;&#23478;&#21644;&#26597;&#35810;&#25968;&#25454;&#19978;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert data to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07689</link><description>&lt;p&gt;
&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#29992;&#20110;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Empirical Bregman Divergence for Uncertain Distance Representation. (arXiv:2304.07689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25216;&#26415;&#24050;&#24212;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#26679;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#26041;&#27861;&#37319;&#29992;&#22266;&#23450;&#36317;&#31163;&#24230;&#37327;&#20316;&#20026;&#20004;&#20010;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21487;&#33021;&#23548;&#33268;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#26368;&#20248;&#24615;&#33021;&#12290;Bregman&#25955;&#24230;&#27010;&#25324;&#20102;&#21508;&#31181;&#36317;&#31163;&#24230;&#37327;&#30340;&#24230;&#37327;&#65292;&#24182;&#22312;&#35768;&#22810;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#39046;&#22495;&#20013;&#20135;&#29983;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;Bregman&#25955;&#24230;&#33719;&#24471;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35774;&#32622;&#23545;Bregman&#25955;&#24230;&#19979;&#30340;&#20984;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;SOTA&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#27969;&#34892;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14061</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Reward Machines in Cooperative Multi-Agent Tasks. (arXiv:2303.14061v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#32534;&#30721;&#23376;&#20219;&#21153;&#30340;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#24212;&#23545;&#37096;&#20998;&#35266;&#27979;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#25152;&#23398;&#20064;&#30340;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#23436;&#25104;&#21512;&#20316;&#20219;&#21153;&#12290;&#19982;&#27599;&#20010;&#23376;&#20219;&#21153;&#30456;&#20851;&#32852;&#30340;&#22870;&#21169;&#26426;&#21046;&#26159;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#23398;&#20064;&#30340;&#65292;&#28982;&#21518;&#29992;&#20110;&#25351;&#23548;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#24471;&#21040;&#20102;&#38477;&#20302;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26410;&#26469;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26377; promising &#30340;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to Multi-Agent Reinforcement Learning (MARL) that combines cooperative task decomposition with the learning of reward machines (RMs) encoding the structure of the sub-tasks. The proposed method helps deal with the non-Markovian nature of the rewards in partially observable environments and improves the interpretability of the learnt policies required to complete the cooperative task. The RMs associated with each sub-task are learnt in a decentralised manner and then used to guide the behaviour of each agent. By doing so, the complexity of a cooperative multi-agent problem is reduced, allowing for more effective learning. The results suggest that our approach is a promising direction for future research in MARL, especially in complex environments with large state spaces and multiple agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2303.13465</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#34892;&#20026;&#31354;&#38388;&#26497;&#20854;&#24222;&#22823;&#65292;&#22240;&#27492;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#65292;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#24517;&#39035;&#20351;&#29992;&#31574;&#30053;&#25913;&#36827;&#21644;&#34892;&#20026;&#37319;&#26679;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#26377;&#20215;&#20540;&#30340;&#22238;&#24212;&#38750;&#24120;&#31232;&#30095;&#65292;&#22240;&#27492;&#20351;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#36138;&#24515;&#31574;&#30053;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#31890;&#24230;&#30340; Q-function &#24182;&#36890;&#36807;&#25506;&#32034;&#26368;&#26377;&#21069;&#36884;&#30340;&#22238;&#24212;&#31867;&#21035;&#26469;&#32531;&#35299;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;&#35813;&#31639;&#27861;&#20174;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#30340;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, since the natural language action space is astronomical, approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning (RL) because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. This paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. We introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene in the sampling. It extracts the actions following the grained hierarchy, which can achieve the optimum with fewer policy iterations. Our approach learns in the way of offline RL from multiple reward functions designed to recognize human emotional details. Empirical studies demonstrate that our algorithm outperforms the baseline metho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10650</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#36923;&#36753;&#30340;&#36923;&#36753;&#65306;&#36208;&#21521;DL&#30340;&#32479;&#19968;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#21487;&#24494;&#20998;&#36923;&#36753;&#65288;DL&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#28385;&#36275;&#36923;&#36753;&#35268;&#33539;&#30340;&#26041;&#27861;&#12290;DL&#21253;&#25324;&#35821;&#27861;&#21644;&#23558;&#35821;&#27861;&#20013;&#30340;&#34920;&#36798;&#24335;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#35299;&#37322;&#20989;&#25968;&#12290;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290; &#29616;&#26377;DL&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#20854;&#24418;&#24335;&#21270;&#31243;&#24230;&#30340;&#19981;&#21516;&#22788;&#29702;&#20351;&#24471;&#23545;&#23427;&#20204;&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#20316;&#20026;DL&#23450;&#20041;&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31119;&#21033;&#26368;&#22823;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#31639;&#32422;&#26463;&#19979;&#22810;&#32452;&#20214;POMDP&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#26368;&#20248;&#39044;&#31639;&#20998;&#37197;&#26469;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.10302</link><description>&lt;p&gt;
&#35299;&#20915;&#39044;&#31639;&#32422;&#26463;&#19979;&#22810;&#32452;&#20214;POMDP&#30340;&#31119;&#21033;&#26368;&#22823;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Welfare Maximization Algorithm for Solving Budget-Constrained Multi-Component POMDPs. (arXiv:2303.10302v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10302
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31119;&#21033;&#26368;&#22823;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#31639;&#32422;&#26463;&#19979;&#22810;&#32452;&#20214;POMDP&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#26368;&#20248;&#39044;&#31639;&#20998;&#37197;&#26469;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#23454;&#38469;&#30340;&#24207;&#36143;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#25991;&#38024;&#23545;&#29420;&#31435;&#21160;&#24577;&#22522;&#30784;&#35774;&#26045;&#32452;&#20214;&#30340;&#32500;&#25252;&#21644;&#26816;&#26597;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#23547;&#25214;&#22810;&#32452;&#20214;&#39044;&#31639;&#32422;&#26463;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#24102;&#39044;&#31639;&#30340;POMDP&#27169;&#22411;&#65288;b-POMDP&#65289;&#65292;&#22312;&#36981;&#23432;&#39044;&#31639;&#32422;&#26463;&#30340;&#21516;&#26102;&#65292;&#20801;&#35768;&#25105;&#20204;&#25214;&#21040;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#26399;&#30340;&#24773;&#20917;&#19979;&#65292;b-POMDP&#30340;&#20540;&#20989;&#25968;&#25110;&#26368;&#22823;&#25910;&#30410;&#26159;&#39044;&#31639;&#30340;&#20985;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#21508;&#20010;&#32452;&#20214;POMDP&#20043;&#38388;&#30340;&#26368;&#20248;&#39044;&#31639;&#20998;&#37197;&#26469;&#35745;&#31639;&#22810;&#32452;&#20214;&#39044;&#31639;&#32422;&#26463;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#26368;&#20248;&#39044;&#31639;&#20998;&#37197;&#34987;&#34920;&#31034;&#20026;&#19968;&#31181;&#31119;&#21033;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#21033;&#29992;&#20540;&#20989;&#25968;&#30340;&#20985;&#24615;&#36136;&#26469;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Markov Decision Processes (POMDPs) provide an efficient way to model real-world sequential decision making processes. Motivated by the problem of maintenance and inspection of a group of infrastructure components with independent dynamics, this paper presents an algorithm to find the optimal policy for a multi-component budget-constrained POMDP. We first introduce a budgeted-POMDP model (b-POMDP) which enables us to find the optimal policy for a POMDP while adhering to budget constraints. Next, we prove that the value function or maximal collected reward for a b-POMDP is a concave function of the budget for the finite horizon case. Our second contribution is an algorithm to calculate the optimal policy for a multi-component budget-constrained POMDP by finding the optimal budget split among the individual component POMDPs. The optimal budget split is posed as a welfare maximization problem and the solution is computed by exploiting the concave nature of the value fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.06318</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#19987;&#23478;&#28151;&#21512;&#24182;&#34892;&#26041;&#27861;&#26469;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixture-of-Experts&#65288;MoE&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#28155;&#21152;&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#22359;&#26469;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#65288;&#22522;&#26412;&#27169;&#22411;&#65289;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#25913;&#21464;&#35757;&#32451;&#25110;&#25512;&#29702;&#30340;&#24635;&#28014;&#28857;&#25805;&#20316;&#25968;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#31181;&#26550;&#26500;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#20219;&#24847;&#22823;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19982;&#22522;&#26412;&#27169;&#22411;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;64&#21040;128&#20010;&#19987;&#23478;&#22359;&#20043;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35266;&#23519;&#21040;&#36825;&#20123;MoE&#27169;&#22411;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#36882;&#20943;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;MoE&#27169;&#22411;&#38656;&#35201;&#25105;&#20204;&#25193;&#23637;&#22522;&#26412;&#27169;&#22411;&#30340;&#22823;&#23567;&#20197;&#21450;&#19987;&#23478;&#22359;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#32500;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#27493;&#39588;&#20013;&#25552;&#20986;&#20102;&#20869;&#23384;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
&lt;/p&gt;</description></item><item><title>BrainCLIP&#26159;&#19968;&#31181;&#20174;fMRI&#20013;&#33719;&#21462;&#33258;&#28982;&#22270;&#20687;&#20449;&#24687;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;CLIP&#36328;&#27169;&#24577;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#32479;&#19968;&#35270;&#35273;&#21050;&#28608;&#20998;&#31867;&#21644;&#37325;&#26500;&#20219;&#21153;&#65292;&#21516;&#26102;&#25991;&#26412;&#30417;&#30563;&#20063;&#33021;&#25552;&#39640;&#35299;&#30721;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12971</link><description>&lt;p&gt;
BrainCLIP&#65306;&#36890;&#36807;CLIP&#26694;&#26550;&#23454;&#29616;&#20174;fMRI&#20013;&#33719;&#21462;&#33258;&#28982;&#35270;&#35273;&#20449;&#24687;&#30340;&#36890;&#29992;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BrainCLIP: Bridging Brain and Visual-Linguistic Representation via CLIP for Generic Natural Visual Stimulus Decoding from fMRI. (arXiv:2302.12971v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12971
&lt;/p&gt;
&lt;p&gt;
BrainCLIP&#26159;&#19968;&#31181;&#20174;fMRI&#20013;&#33719;&#21462;&#33258;&#28982;&#22270;&#20687;&#20449;&#24687;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;CLIP&#36328;&#27169;&#24577;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#32479;&#19968;&#35270;&#35273;&#21050;&#28608;&#20998;&#31867;&#21644;&#37325;&#26500;&#20219;&#21153;&#65292;&#21516;&#26102;&#25991;&#26412;&#30417;&#30563;&#20063;&#33021;&#25552;&#39640;&#35299;&#30721;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;fMRI&#20449;&#21495;&#37325;&#26500;&#24863;&#30693;&#21040;&#30340;&#33258;&#28982;&#22270;&#20687;&#25110;&#35299;&#30721;&#23427;&#20204;&#30340;&#31867;&#21035;&#26159;&#20855;&#26377;&#31185;&#23398;&#24847;&#20041;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#30001;&#20110;&#32570;&#20047;&#25104;&#23545;&#26679;&#26412;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#29983;&#25104;&#35821;&#20041;&#21487;&#35782;&#21035;&#30340;&#37325;&#26500;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#26032;&#39062;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#22823;&#33041;&#35299;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#32479;&#19968;&#35270;&#35273;&#21050;&#28608;&#20998;&#31867;&#21644;&#37325;&#26500;&#20219;&#21153;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#23558;&#23427;&#21629;&#21517;&#20026;BrainCLIP&#65292;&#21033;&#29992;&#20102;CLIP&#36328;&#27169;&#24577;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#22635;&#34917;&#22823;&#33041;&#27963;&#21160;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BrainCLIP&#26159;&#19968;&#31181;&#22522;&#20110;VAE&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#30417;&#30563;&#65292;&#23558;fMRI&#27169;&#24335;&#36716;&#25442;&#20026;CLIP&#23884;&#20837;&#31354;&#38388;&#12290;&#27880;&#24847;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20351;&#29992;&#22810;&#27169;&#24577;&#30417;&#30563;&#36827;&#34892;&#35270;&#35273;&#21050;&#28608;&#35299;&#30721;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25991;&#26412;&#30417;&#30563;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#35299;&#30721;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing perceived natural images or decoding their categories from fMRI signals are challenging tasks with great scientific significance. Due to the lack of paired samples, most existing methods fail to generate semantically recognizable reconstruction and are difficult to generalize to novel classes. In this work, we propose, for the first time, a task-agnostic brain decoding model by unifying the visual stimulus classification and reconstruction tasks in a semantic space. We denote it as BrainCLIP, which leverages CLIP's cross-modal generalization ability to bridge the modality gap between brain activities, images, and texts. Specifically, BrainCLIP is a VAE-based architecture that transforms fMRI patterns into the CLIP embedding space by combining visual and textual supervision. Note that previous works rarely use multi-modal supervision for visual stimulus decoding. Our experiments demonstrate that textual supervision can significantly boost the performance of decoding model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25935;&#25463;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;&#20219;&#20309;&#20027;&#35266;&#35270;&#35273;&#27010;&#24565;&#36716;&#21270;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;30&#20998;&#38047;&#20869;&#36731;&#26494;&#21019;&#24314;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12948</link><description>&lt;p&gt;
&#25935;&#25463;&#24314;&#27169;&#65306;&#20174;&#27010;&#24565;&#21040;&#20998;&#31867;&#22120;&#21482;&#38656;&#20960;&#20998;&#38047;
&lt;/p&gt;
&lt;p&gt;
Agile Modeling: From Concept to Classifier in Minutes. (arXiv:2302.12948v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25935;&#25463;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;&#20219;&#20309;&#20027;&#35266;&#35270;&#35273;&#27010;&#24565;&#36716;&#21270;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;30&#20998;&#38047;&#20869;&#36731;&#26494;&#21019;&#24314;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#20027;&#35266;&#32454;&#24494;&#24212;&#29992;&#26041;&#38754;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#34429;&#28982;&#20247;&#21253;&#23545;&#20110;&#22823;&#22810;&#25968;&#23458;&#35266;&#20219;&#21153;&#65288;&#22914;&#26631;&#35760;&#8220;&#26001;&#39532;&#8221;&#65289;&#24050;&#32463;&#20026;&#35270;&#35273;&#31038;&#21306;&#26381;&#21153;&#24471;&#24456;&#22909;&#65292;&#20294;&#22312;&#27010;&#24565;&#20855;&#26377;&#23454;&#36136;&#24615;&#20027;&#35266;&#24615;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#35782;&#21035;&#8220;&#32654;&#39135;&#37329;&#26538;&#40060;&#8221;&#65289;&#19978;&#65292;&#23427;&#29616;&#22312;&#38754;&#20020;&#22833;&#36133;&#12290;&#28982;&#32780;&#65292;&#35753;&#20219;&#20309;&#29992;&#25143;&#24320;&#21457;&#20854;&#27010;&#24565;&#30340;&#20998;&#31867;&#22120;&#22312;&#25216;&#26415;&#19978;&#26159;&#22256;&#38590;&#30340;&#65306;&#29992;&#25143;&#26082;&#19981;&#26159;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#65292;&#20063;&#27809;&#26377;&#32784;&#24515;&#26631;&#35760;&#25968;&#21315;&#20010;&#31034;&#20363;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25935;&#25463;&#24314;&#27169;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#23454;&#26102;&#29992;&#25143;&#21442;&#19982;&#23558;&#20219;&#20309;&#20027;&#35266;&#35270;&#35273;&#27010;&#24565;&#36716;&#21270;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20026;&#22270;&#20687;&#20998;&#31867;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#25935;&#25463;&#24314;&#27169;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65288;N=14&#65289;&#34920;&#26126;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;30&#20998;&#38047;&#20869;&#36731;&#26494;&#21019;&#24314;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#29992;&#25143;&#39537;&#21160;&#30340;&#36807;&#31243;&#19982;&#20256;&#32479;&#30340;&#20247;&#21253;&#33539;&#24335;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#21457;&#29616;&#32676;&#20307;&#30340;&#35266;&#24565;&#24120;&#24120;&#19982;&#29992;&#25143;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of computer vision to nuanced subjective use cases is growing. While crowdsourcing has served the vision community well for most objective tasks (such as labeling a "zebra"), it now falters on tasks where there is substantial subjectivity in the concept (such as identifying "gourmet tuna"). However, empowering any user to develop a classifier for their concept is technically difficult: users are neither machine learning experts, nor have the patience to label thousands of examples. In reaction, we introduce the problem of Agile Modeling: the process of turning any subjective visual concept into a computer vision model through a real-time user-in-the-loop interactions. We instantiate an Agile Modeling prototype for image classification and show through a user study (N=14) that users can create classifiers with minimal effort under 30 minutes. We compare this user driven process with the traditional crowdsourcing paradigm and find that the crowd's notion often differs fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31934;&#32454;&#30340;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#35270;&#35273;&#20998;&#31867;&#20013;&#21487;&#35270;&#21270;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#26356;&#24544;&#23454;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08508</link><description>&lt;p&gt;
&#21407;&#22411;&#22270;&#20687;&#20998;&#31867;&#20013;&#34917;&#19969;&#21487;&#35270;&#21270;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Sanity checks and improvements for patch visualisation in prototype-based image classification. (arXiv:2302.08508v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31934;&#32454;&#30340;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#35270;&#35273;&#20998;&#31867;&#20013;&#21487;&#35270;&#21270;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#26356;&#24544;&#23454;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#21407;&#22411;&#30340;&#35270;&#35273;&#20998;&#31867;&#20013;&#23454;&#26045;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20351;&#29992;&#20004;&#20010;&#31934;&#32454;&#30340;&#25968;&#25454;&#38598;&#65288;CUB-200-2011 &#21644; Stanford Cars&#65289;&#39318;&#20808;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#33021;&#27491;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#22240;&#27492;&#19981;&#33021;&#21453;&#26144;&#20986;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#21024;&#38500;&#24230;&#37327;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#35777;&#26126;&#20102; Smoothgrads &#25110; PRP &#31561;&#26174;&#33879;&#24615;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#24544;&#23454;&#30340;&#22270;&#20687;&#34917;&#19969;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26576;&#20123;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914; CUB-200-2011&#65289;&#20013;&#25552;&#20379;&#30340;&#23545;&#35937;&#20998;&#21106;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102; ProtoPNet &#21644; ProtoTree &#20135;&#29983;&#30340;&#19981;&#31934;&#30830;&#30340;&#34917;&#19969;&#21487;&#35270;&#21270;&#21487;&#33021;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#20559;&#35265;&#24863;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26356;&#24544;&#23454;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#20854;&#20182;&#20351;&#29992;&#30456;&#21516;&#21487;&#35270;&#21270;&#26041;&#27861;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we perform an in-depth analysis of the visualisation methods implemented in two popular self-explaining models for visual classification based on prototypes - ProtoPNet and ProtoTree. Using two fine-grained datasets (CUB-200-2011 and Stanford Cars), we first show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour. Secondly, using a deletion metric, we demonstrate quantitatively that saliency methods such as Smoothgrads or PRP provide more faithful image patches. We also propose a new relevance metric based on the segmentation of the object provided in some datasets (e.g. CUB-200-2011) and show that the imprecise patch visualisations generated by ProtoPNet and ProtoTree can create a false sense of bias that can be mitigated by the use of more faithful methods. Finally, we discuss the implications of our findings for other prototype-based models sharing the same visualisation method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;</title><link>http://arxiv.org/abs/2302.05045</link><description>&lt;p&gt;
&#21033;&#29992;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31232;&#30095;&#24615;&#26469;&#20248;&#21270;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training. (arXiv:2302.05045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach that exploits sparse subnetworks to optimize memory utilization and communication in two popular algorithms for parallel deep learning, and demonstrates significant reductions in memory consumption and communication time on a 2.7 billion parameter model using 512 NVIDIA V100 GPUs.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36890;&#20449;&#24320;&#38144;&#30340;&#26174;&#33879;&#22686;&#21152;&#65292;&#35268;&#27169;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#35757;&#32451;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#21098;&#26525;&#31639;&#27861;&#65292;&#33021;&#22815;&#21098;&#26525;&#65288;&#21363;&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#65289;80-90&#65285;&#30340;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#19982;&#26410;&#21098;&#26525;&#29238;&#32593;&#32476;&#30456;&#31561;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;AxoNN&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20381;&#36182;&#20110;&#25968;&#25454;&#21644;&#23618;&#38388;&#24182;&#34892;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#20449;&#26102;&#38388;&#21644;&#20869;&#23384;&#21033;&#29992;&#30340;&#20943;&#23569;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#22238;&#39038;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.03122</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
State-wise Safe Reinforcement Learning: A Survey. (arXiv:2302.03122v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#22238;&#39038;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20173;&#28982;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#23433;&#20840;&#24615;&#65292;&#20063;&#23601;&#26159;&#32422;&#26463;&#28385;&#36275;&#12290;&#29366;&#24577;&#32422;&#26463;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#26368;&#24120;&#35265;&#19988;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#32422;&#26463;&#20043;&#19968;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#32780;&#35328;&#26159;&#24517;&#35201;&#21644;&#20851;&#38190;&#30340;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#30340;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#29366;&#24577;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#19979;&#65292;&#20174;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#65292;&#35752;&#35770;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potentia
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#65292;&#25552;&#20986;&#22235;&#20010;&#35823;&#24046;&#37096;&#20998;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;169&#20010;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20026;SSL&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.03068</link><description>&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03068
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#65292;&#25552;&#20986;&#22235;&#20010;&#35823;&#24046;&#37096;&#20998;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;169&#20010;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20026;SSL&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#27969;&#31243;&#35774;&#35745;&#28041;&#21450;&#26550;&#26500;&#12289;&#22686;&#24378;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#31561;&#35832;&#22810;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;SSL&#36890;&#24120;&#20351;&#29992;&#21333;&#19968;&#24230;&#37327;&#26469;&#35780;&#20272;&#65292;&#36825;&#24182;&#19981;&#33021;&#25552;&#20379;&#28145;&#20837;&#30340;&#27934;&#23519;&#21644;&#25913;&#36827;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SSL&#39118;&#38505;&#20998;&#35299;&#65292;&#20174;&#36924;&#36817;&#12289;&#34920;&#31034;&#21487;&#29992;&#24615;&#12289;&#25506;&#38024;&#27867;&#21270;&#21644;&#32534;&#30721;&#22120;&#27867;&#21270;&#31561;&#35282;&#24230;&#23545;&#38169;&#35823;&#36827;&#34892;&#20998;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;30&#20010;&#35774;&#35745;&#36873;&#25321;&#23545;169&#20010;&#22312;ImageNet&#19978;&#35780;&#20272;&#30340;SSL&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#20026;&#27599;&#20010;&#32452;&#20214;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#65292;&#20026;SSL&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. Yet SSL is typically evaluated using a single metric: linear probing on ImageNet. This does not provide much insight into why or when a model is better, now how to improve it. To address this, we propose an SSL risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. Our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main sources of error and shows how to improve SSL in specific settings (full- vs 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#36731;&#37327;&#32423;&#27169;&#22411;&#24182;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#37096;&#32626;&#65292;&#21516;&#26102;&#23454;&#29616;&#23545;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#31934;&#32454;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2301.08143</link><description>&lt;p&gt;
&#32852;&#37030;&#25512;&#33616;&#20013;&#30340;&#21452;&#37325;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dual Personalization on Federated Recommendation. (arXiv:2301.08143v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#36731;&#37327;&#32423;&#27169;&#22411;&#24182;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#37096;&#32626;&#65292;&#21516;&#26102;&#23454;&#29616;&#23545;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#31934;&#32454;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25512;&#33616;&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;&#32852;&#37030;&#29615;&#22659;&#19979;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#26381;&#21153;&#30340;&#26032;&#22411;Internet&#26381;&#21153;&#26550;&#26500;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#32452;&#21512;&#20998;&#24067;&#24335;&#25512;&#33616;&#31639;&#27861;&#21644;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#65292;&#22240;&#27492;&#20174;&#26681;&#26412;&#19978;&#37319;&#29992;&#26381;&#21153;&#22120;&#19978;&#30340;&#37325;&#37327;&#32423;&#27169;&#22411;&#65292;&#38459;&#30861;&#20102;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#26234;&#33021;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#25512;&#33616;&#65288;PFedRec&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#35768;&#22810;&#29992;&#25143;&#29305;&#23450;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#37096;&#32626;&#65292;&#32780;&#19981;&#26159;&#22312;&#26381;&#21153;&#22120;&#19978;&#20351;&#29992;&#37325;&#37327;&#32423;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#20010;&#24615;&#21270;&#26426;&#21046;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#32454;&#31890;&#24230;&#20010;&#24615;&#21270;&#12290;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#32852;&#37030;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated recommendation is a new Internet service architecture that aims to provide privacy-preserving recommendation services in federated settings. Existing solutions are used to combine distributed recommendation algorithms and privacy-preserving mechanisms. Thus it inherently takes the form of heavyweight models at the server and hinders the deployment of on-device intelligent models to end-users. This paper proposes a novel Personalized Federated Recommendation (PFedRec) framework to learn many user-specific lightweight models to be deployed on smart devices rather than a heavyweight model on a server. Moreover, we propose a new dual personalization mechanism to effectively learn fine-grained personalization on both users and items. The overall learning process is formulated into a unified federated optimization framework. Specifically, unlike previous methods that share exactly the same item embeddings across users in a federated system, dual personalization allows mild finetuni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22823;&#35268;&#27169;&#35757;&#32451;&#19979;&#25513;&#27169;&#33258;&#32534;&#30721;&#21644;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#35268;&#27169;&#35757;&#32451;&#20013;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#20351;&#29992;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#27809;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.07836</link><description>&lt;p&gt;
&#25513;&#27169;&#33258;&#32534;&#30721;&#22312;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#20013;&#27809;&#26377;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoding Does Not Help Natural Language Supervision at Scale. (arXiv:2301.07836v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22823;&#35268;&#27169;&#35757;&#32451;&#19979;&#25513;&#27169;&#33258;&#32534;&#30721;&#21644;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#35268;&#27169;&#35757;&#32451;&#20013;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#20351;&#29992;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#27809;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#21644;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#24050;&#25104;&#20026;&#35757;&#32451;&#36890;&#29992;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#20004;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#20351;&#29992;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#20027;&#35201;&#20351;&#29992;&#20102;&#23567;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&lt;50M&#26679;&#26412;&#65289;&#65292;&#24182;&#19988;&#27809;&#26377;&#26377;&#25928;&#22320;&#21453;&#26144;&#20986;&#24120;&#29992;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&gt;100M&#26679;&#26412;&#65289;&#30340;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#26356;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;11.3M&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25513;&#27169;&#33258;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#32452;&#21512;&#21487;&#20197;&#27604;&#21482;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#26356;&#22909;&#65292;&#20294;&#22312;&#23545;1.4B&#20010;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#23427;&#19982;&#20165;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30456;&#27604;&#65292;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20248;&#21183;&#65288;&#22312;&#19968;&#22871;&#24120;&#35265;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#35780;&#20272;&#65289;&#12290;&#26412;&#30740;&#31350;&#20026;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#19968;&#20123;&#38656;&#35201;&#30340;&#28165;&#26224;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (&lt;50M samples) and don't effectively reflect the large-scale regime (&gt;100M examples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#26435;&#21147;&#25351;&#25968;&#26469;&#27979;&#37327;&#28082;&#24577;&#27665;&#20027;&#36873;&#20030;&#20013;&#36873;&#27665;&#30340;&#20808;&#39564;&#25237;&#31080;&#26435;&#65292;&#29305;&#21035;&#26159;&#22312;&#24213;&#23618;&#32593;&#32476;&#38480;&#21046;&#22996;&#25176;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#20123;&#25351;&#25968;&#30340;&#35745;&#31639;&#24182;&#19981;&#23481;&#26131;&#65292;&#20294;&#26159;&#23545;&#20110;&#29305;&#23450;&#30340;&#35774;&#32622;&#21487;&#20197;&#22312;&#20266;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2301.02462</link><description>&lt;p&gt;
&#27979;&#37327;&#20808;&#39564;&#25237;&#31080;&#26435;&#8212;&#8212;&#35748;&#30495;&#23545;&#24453;&#22996;&#25176;
&lt;/p&gt;
&lt;p&gt;
Measuring a Priori Voting Power -- Taking Delegations Seriously. (arXiv:2301.02462v4 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#26435;&#21147;&#25351;&#25968;&#26469;&#27979;&#37327;&#28082;&#24577;&#27665;&#20027;&#36873;&#20030;&#20013;&#36873;&#27665;&#30340;&#20808;&#39564;&#25237;&#31080;&#26435;&#65292;&#29305;&#21035;&#26159;&#22312;&#24213;&#23618;&#32593;&#32476;&#38480;&#21046;&#22996;&#25176;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#20123;&#25351;&#25968;&#30340;&#35745;&#31639;&#24182;&#19981;&#23481;&#26131;&#65292;&#20294;&#26159;&#23545;&#20110;&#29305;&#23450;&#30340;&#35774;&#32622;&#21487;&#20197;&#22312;&#20266;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#26435;&#21147;&#25351;&#25968;&#26469;&#27979;&#37327;&#28082;&#24577;&#27665;&#20027;&#36873;&#20030;&#20013;&#36873;&#27665;&#30340;&#20808;&#39564;&#25237;&#31080;&#26435;&#65292;&#20854;&#20013;&#24213;&#23618;&#32593;&#32476;&#38480;&#21046;&#20102;&#22996;&#25176;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#26435;&#21147;&#25351;&#25968;&#26159;&#31616;&#21333;&#25237;&#31080;&#28216;&#25103;&#20013;&#26631;&#20934;Penrose-Banzhaf&#25351;&#25968;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#22312;&#25237;&#31080;&#26435;&#37325;&#22312;&#23454;&#20363;&#30340;&#35268;&#27169;&#19978;&#26159;&#22810;&#39033;&#24335;&#26377;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#36873;&#27665;&#30340;&#20851;&#38190;&#24615;&#20063;&#26159;&#65283;P&#22256;&#38590;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#35774;&#32622;&#65292;&#20363;&#22914;&#24403;&#24213;&#23618;&#32593;&#32476;&#26159;&#20108;&#20998;&#22270;&#25110;&#23436;&#20840;&#22270;&#26102;&#65292;&#36882;&#24402;&#20844;&#24335;&#21487;&#20197;&#22312;&#20266;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#36825;&#20123;&#25351;&#25968;&#20197;&#21152;&#26435;&#25237;&#31080;&#28216;&#25103;&#12290;&#25105;&#20204;&#24378;&#35843;&#23427;&#20204;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#24182;&#25552;&#20379;&#25968;&#23383;&#32467;&#26524;&#65292;&#20197;&#35828;&#26126;&#38480;&#21046;&#21487;&#33021;&#30340;&#22996;&#25176;&#22914;&#20309;&#25913;&#21464;&#36873;&#27665;&#30340;&#25237;&#31080;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce new power indices to measure the a priori voting power of voters in liquid democracy elections where an underlying network restricts delegations. We argue that our power indices are natural extensions of the standard Penrose-Banzhaf index in simple voting games. We show that computing the criticality of a voter is #P-hard even when voting weights are polynomially-bounded in the size of the instance. However, for specific settings, such as when the underlying network is a bipartite or complete graph, recursive formulas can compute these indices for weighted voting games in pseudo-polynomial time. We highlight their theoretical properties and provide numerical results to illustrate how restricting the possible delegations can alter voters' voting power.
&lt;/p&gt;</description></item><item><title>gRoMA&#26159;&#19968;&#31181;&#34913;&#37327;DNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#21019;&#26032;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#35780;&#20272;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36973;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#35813;&#24037;&#20855;&#21487;&#36816;&#34892;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#24182;&#23545;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20135;&#29983;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.02288</link><description>&lt;p&gt;
gRoMA: &#19968;&#31181;&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness. (arXiv:2301.02288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02288
&lt;/p&gt;
&lt;p&gt;
gRoMA&#26159;&#19968;&#31181;&#34913;&#37327;DNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#21019;&#26032;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#35780;&#20272;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36973;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#35813;&#24037;&#20855;&#21487;&#36816;&#34892;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#24182;&#23545;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20135;&#29983;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#21069;&#27839;&#25216;&#26415;&#30340;&#20195;&#34920;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#65288;&#22914;&#33322;&#31354;&#25110;&#27773;&#36710;&#39046;&#22495;&#65289;&#26102;&#65292;&#30001;&#20110;&#23545;&#25239;&#24615;&#36755;&#20837;&#65288;&#21363;&#21487;&#33021;&#23548;&#33268;DNN&#29359;&#38169;&#30340;&#36755;&#20837;&#25200;&#21160;&#65289;&#30340;&#23041;&#32961;&#65292;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#21363;&#20415;&#26159;&#29616;&#20195;DNN&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#24517;&#39035;&#27979;&#37327;&#24182;&#38477;&#20302;&#36825;&#31181;&#39118;&#38505;&#25165;&#33021;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#37096;&#32626;DNN&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;gRoMA&#65288;&#20840;&#23616;&#40065;&#26834;&#24615;&#27979;&#37327;&#21644;&#35780;&#20272;&#65289;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#26469;&#27979;&#37327;DNN&#30340;&#20840;&#23616;&#20998;&#31867;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;gRoMA&#27979;&#37327;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36935;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#65292;&#20135;&#29983;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;DNN&#22312;&#28909;&#38376;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#40065;&#26834;&#24615;&#24182;&#20998;&#26512;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are at the forefront of cutting-edge technology, and have been achieving remarkable performance in a variety of complex tasks. Nevertheless, their integration into safety-critical systems, such as in the aerospace or automotive domains, poses a significant challenge due to the threat of adversarial inputs: perturbations in inputs that might cause the DNN to make grievous mistakes. Multiple studies have demonstrated that even modern DNNs are susceptible to adversarial inputs; and this risk must thus be measured and mitigated to allow the deployment of DNNs in safety-critical systems.  Here, we present gRoMA (global Robustness Measurement and Assessment), an innovative and scalable tool that implements a probabilistic verification approach to measure the global categorial robustness of a DNN. Specifically, gRoMA measures the probability of encountering adversarial inputs for a specific output category. Our tool operates on pre-trained, black-box classification
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2212.07530</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07530
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#20174;&#19981;&#21516;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#21327;&#21516;&#20013;&#33719;&#30410;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#21463;&#21040;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#20808;&#36827;&#26041;&#27861;&#26088;&#22312;&#28040;&#38500;&#24178;&#25200;&#65292;&#20294;&#25105;&#20204;&#23545;&#24178;&#25200;&#29616;&#35937;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#23548;&#33268;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#24178;&#25200;&#65288;&#25110;&#21327;&#21516;&#65289;&#20027;&#35201;&#30001;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#22823;&#23567;&#21644;&#27599;&#20010;&#35821;&#35328;&#23545;&#22312;&#24635;&#25968;&#25454;&#38598;&#20013;&#25152;&#21344;&#27604;&#20363;&#26469;&#20915;&#23450;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#27169;&#22411;&#30456;&#23545;&#20110;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#23567;&#30340;&#26102;&#20505;&#65292;&#20250;&#20986;&#29616;&#20005;&#37325;&#30340;&#24178;&#25200;&#65292;&#32780;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;</title><link>http://arxiv.org/abs/2212.06951</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;AI&#20262;&#29702;: &#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#21306;&#22359;&#38142;&#23433;&#20840;&#20027;&#39064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#20351;&#29992;&#20998;&#24067;&#24335;&#32593;&#32476;&#35753;&#35745;&#31639;&#26426;&#31995;&#32479;&#26356;&#23433;&#20840;&#65292;&#20294;&#24403;&#21069;&#30340;&#21306;&#22359;&#38142;&#35774;&#35745;&#22312;&#20132;&#26131;&#39034;&#24207;&#26041;&#38754;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#30719;&#24037;&#21487;&#20197;&#37325;&#26032;&#25490;&#24207;&#20132;&#26131;&#20197;&#29983;&#25104;&#21033;&#28070;&#65292;&#36825;&#34987;&#31216;&#20026;&#30719;&#24037;&#21487;&#25552;&#21462;&#20215;&#20540;&#65288;MEV&#65289;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#35748;&#20026;MEV&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#30693;&#21517;&#30340;Flashbots&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#20998;&#26512;&#20102;&#21306;&#22359;&#38142;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;MEV&#22312;&#26356;&#24191;&#27867;&#30340;AI&#31038;&#20250;&#20013;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#20840;&#38754;&#20998;&#26512;&#20102;MEV&#25512;&#25991;&#20013;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#36229;&#36807;20,000&#20010;MEV&#21644;Flashbots&#26631;&#31614;&#30340;&#25512;&#25991;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21306;&#22359;&#38142;&#19978;MEV&#27963;&#21160;&#30340;&#20849;&#21516;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.02733</link><description>&lt;p&gt;
&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for Relative Overgeneralization. (arXiv:2212.02733v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (MARL) &#20013;&#65292;&#35768;&#22810;&#27969;&#34892;&#26041;&#27861;&#22914; VDN &#21644; QMIX&#65292;&#37117;&#23481;&#26131;&#21463;&#21040;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#36825;&#19968;&#20851;&#38190;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#30149;&#29702;&#30340;&#24433;&#21709;&#12290;&#24403;&#21512;&#20316;&#20219;&#21153;&#20013;&#26368;&#20339;&#32852;&#21512;&#34892;&#21160;&#30340;&#25928;&#29992;&#20302;&#20110;&#27425;&#20248;&#32852;&#21512;&#34892;&#21160;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;RO&#12290;RO&#21487;&#33021;&#23548;&#33268;&#26234;&#33021;&#20307;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#25110;&#26080;&#27861;&#35299;&#20915;&#38656;&#35201;&#26234;&#33021;&#20307;&#20043;&#38388;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#38271;&#20869;&#36827;&#34892;&#22823;&#37327;&#21327;&#35843;&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;MARL&#31639;&#27861;&#65292;&#22914;QPLEX&#21644;WQMIX&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20811;&#26381;RO&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20811;&#26381;RO&#12290;&#22312;CURO&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#29983;&#25104;&#36866;&#21512;&#24403;&#21069;&#33021;&#21147;&#30340;&#28304;&#20219;&#21153;&#26469;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#30446;&#26631;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent reinforcement learning (MARL), many popular methods, such as VDN and QMIX, are susceptible to a critical multi-agent pathology known as relative overgeneralization (RO), which arises when the optimal joint action's utility falls below that of a sub-optimal joint action in cooperative tasks. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks that require significant coordination between agents within a given timestep. Recent value-based MARL algorithms such as QPLEX and WQMIX can overcome RO to some extent. However, our experimental results show that they can still fail to solve cooperative tasks that exhibit strong RO. In this work, we propose a novel approach called curriculum learning for relative overgeneralization (CURO) to better overcome RO. To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks that are tailored to the current ability of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21475;-&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#26469;&#35757;&#32451;&#21644;&#32452;&#21512;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#36991;&#20813;&#38656;&#35201;&#26356;&#22810;&#22797;&#21512;&#31995;&#32479;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.00893</link><description>&lt;p&gt;
&#20351;&#29992;&#31471;&#21475;-&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#30340;&#32452;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositional Learning of Dynamical System Models Using Port-Hamiltonian Neural Networks. (arXiv:2212.00893v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21475;-&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#26469;&#35757;&#32451;&#21644;&#32452;&#21512;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#36991;&#20813;&#38656;&#35201;&#26356;&#22810;&#22797;&#21512;&#31995;&#32479;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21160;&#24577;&#31995;&#32479;&#37117;&#28041;&#21450;&#35768;&#22810;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#65292;&#20174;&#19982;&#20854;&#21608;&#22260;&#29615;&#22659;&#20132;&#20114;&#30340;&#26426;&#22120;&#20154;&#21040;&#22823;&#35268;&#27169;&#30340;&#22810;&#29289;&#29702;&#31995;&#32479;&#12290;&#20026;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#31181;&#22797;&#21512;&#31995;&#32479;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20197;&#21450;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#32452;&#21512;&#24050;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#26368;&#32456;&#32467;&#26524;&#26159;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65306;&#31070;&#32463;&#32593;&#32476;&#23376;&#27169;&#22411;&#26159;&#22312;&#30456;&#23545;&#31616;&#21333;&#30340;&#23376;&#31995;&#32479;&#29983;&#25104;&#30340;&#36712;&#36857;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#28982;&#21518;&#39044;&#27979;&#26356;&#22797;&#26434;&#30340;&#22797;&#21512;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#32780;&#19981;&#38656;&#35201;&#35201;&#27714;&#22797;&#21512;&#31995;&#32479;&#26412;&#36523;&#29983;&#25104;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many dynamical systems -- from robots interacting with their surroundings to large-scale multiphysics systems -- involve a number of interacting subsystems. Toward the objective of learning composite models of such systems from data, we present i) a framework for compositional neural networks, ii) algorithms to train these models, iii) a method to compose the learned models, iv) theoretical results that bound the error of the resulting composite models, and v) a method to learn the composition itself, when it is not known a priori. The end result is a modular approach to learning: neural network submodels are trained on trajectory data generated by relatively simple subsystems, and the dynamics of more complex composite systems are then predicted without requiring additional data generated by the composite systems themselves. We achieve this compositionality by representing the system of interest, as well as each of its subsystems, as a port-Hamiltonian neural network (PHNN) -- a class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;"&#36923;&#36753;&#21644;&#24120;&#35782;&#24341;&#23548;&#30340;&#23884;&#20837;&#27169;&#22411;"&#65288;LCGE&#65289;&#65292;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#26102;&#38388;&#25935;&#24863;&#24615;&#34920;&#31034;&#65288;&#28041;&#21450;&#26102;&#24577;&#21644;&#22240;&#26524;&#24615;&#30340;&#20107;&#20214;&#34920;&#31034;&#65289;&#19982;&#24120;&#35782;&#35282;&#24230;&#19978;&#30340;&#26102;&#38388;&#26080;&#20851;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#34920;&#31034;&#20107;&#20214;&#30340;&#23454;&#26102;&#24615;&#21644;&#22240;&#26524;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.16865</link><description>&lt;p&gt;
&#36923;&#36753;&#21644;&#24120;&#35782;&#24341;&#23548;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Logic and Commonsense-Guided Temporal Knowledge Graph Completion. (arXiv:2211.16865v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;"&#36923;&#36753;&#21644;&#24120;&#35782;&#24341;&#23548;&#30340;&#23884;&#20837;&#27169;&#22411;"&#65288;LCGE&#65289;&#65292;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#26102;&#38388;&#25935;&#24863;&#24615;&#34920;&#31034;&#65288;&#28041;&#21450;&#26102;&#24577;&#21644;&#22240;&#26524;&#24615;&#30340;&#20107;&#20214;&#34920;&#31034;&#65289;&#19982;&#24120;&#35782;&#35282;&#24230;&#19978;&#30340;&#26102;&#38388;&#26080;&#20851;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#34920;&#31034;&#20107;&#20214;&#30340;&#23454;&#26102;&#24615;&#21644;&#22240;&#26524;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#23384;&#20648;&#19982;&#26102;&#38388;&#26377;&#20851;&#30340;&#25968;&#25454;&#20107;&#20214;&#12290;&#39044;&#27979;&#20107;&#20214;&#30001;&#20110;&#20854;&#26102;&#24577;&#29305;&#24615;&#32780;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#20197;&#24448;&#30340;TKG&#34917;&#20840;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#34920;&#31034;&#20107;&#20214;&#30340;&#23454;&#26102;&#24615;&#21644;&#22240;&#26524;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36923;&#36753;&#21644;&#24120;&#35782;&#24341;&#23548;&#30340;&#23884;&#20837;&#27169;&#22411;&#8221;&#65288;LCGE&#65289;&#65292;&#33021;&#22815;&#20849;&#21516;&#23398;&#20064;&#20107;&#20214;&#30340;&#26102;&#38388;&#25935;&#24863;&#24615;&#34920;&#31034;&#65288;&#28041;&#21450;&#26102;&#24577;&#21644;&#22240;&#26524;&#24615;&#30340;&#20107;&#20214;&#34920;&#31034;&#65289;&#19982;&#24120;&#35782;&#35282;&#24230;&#19978;&#30340;&#26102;&#38388;&#26080;&#20851;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#35268;&#21017;&#23398;&#20064;&#31639;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#35268;&#21017;&#24341;&#23548;&#30340;&#35859;&#35789;&#23884;&#20837;&#35268;&#33539;&#31574;&#30053;&#65292;&#23398;&#20064;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#36741;&#21161;&#24120;&#35782;&#30693;&#35782;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20107;&#20214;&#30340;&#21512;&#29702;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;TKGC&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
A temporal knowledge graph (TKG) stores the events derived from the data involving time. Predicting events is extremely challenging due to the time-sensitive property of events. Besides, the previous TKG completion (TKGC) approaches cannot represent both the timeliness and the causality properties of events, simultaneously. To address these challenges, we propose a Logic and Commonsense-Guided Embedding model (LCGE) to jointly learn the time-sensitive representation involving timeliness and causality of events, together with the time-independent representation of events from the perspective of commonsense. Specifically, we design a temporal rule learning algorithm to construct a rule-guided predicate embedding regularization strategy for learning the causality among events. Furthermore, we could accurately evaluate the plausibility of events via auxiliary commonsense knowledge. The experimental results of TKGC task illustrate the significant performance improvements of our model compar
&lt;/p&gt;</description></item><item><title>RITA&#26159;&#19968;&#20010;&#38598;&#25104;&#32452;&#20214;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#12290;&#23427;&#30001;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#65292;&#25903;&#25345;&#30495;&#23454;&#20132;&#20114;&#24335;&#20132;&#36890;&#27969;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#25511;&#21046;&#20132;&#36890;&#27969;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2211.03408</link><description>&lt;p&gt;
RITA:&#36890;&#36807;&#30495;&#23454;&#20132;&#20114;&#24335;&#20132;&#36890;&#27969;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
RITA: Boost Autonomous Driving Simulators with Realistic Interactive Traffic Flow. (arXiv:2211.03408v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03408
&lt;/p&gt;
&lt;p&gt;
RITA&#26159;&#19968;&#20010;&#38598;&#25104;&#32452;&#20214;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#12290;&#23427;&#30001;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#65292;&#25903;&#25345;&#30495;&#23454;&#20132;&#20114;&#24335;&#20132;&#36890;&#27969;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#25511;&#21046;&#20132;&#36890;&#27969;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#29983;&#25104;&#26159;&#26500;&#24314;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;&#30340;&#26680;&#24515;&#27169;&#22359;&#12290; &#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#21487;&#29992;&#30340;&#27169;&#25311;&#22120;&#26080;&#27861;&#22797;&#21046;&#20934;&#30830;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21508;&#31181;&#29305;&#24449;&#30340;&#20132;&#36890;&#27169;&#24335;&#65292;&#24182;&#21516;&#26102;&#27169;&#25311;&#23545;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30340;&#20154;&#31867;&#21453;&#24212;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Realistic Interactive TrAffic flow (RITA)&#20316;&#20026;&#29616;&#26377;&#39550;&#39542;&#27169;&#25311;&#22120;&#30340;&#38598;&#25104;&#32452;&#20214;&#65292;&#20026;&#27979;&#35797;&#21644;&#20248;&#21270;&#39550;&#39542;&#31574;&#30053;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#12290; RITA&#30340;&#24320;&#21457;&#32771;&#34385;&#20102;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65292;&#21363;&#30495;&#23454;&#24230;&#65292;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#30001;&#31216;&#20026;RITABackend&#21644;RITAKit&#30340;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#12290; RITABackend&#25903;&#25345;&#36710;&#36742;&#25511;&#21046;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20132;&#36890;&#29983;&#25104;&#27169;&#22411;&#65292;&#32780;RITAKit&#21017;&#24320;&#21457;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#25509;&#21475;&#65292;&#20197;&#22312;&#27169;&#25311;&#22330;&#26223;&#20013;&#29983;&#25104;&#21487;&#25511;&#30340;&#20132;&#36890;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality traffic flow generation is the core module in building simulators for autonomous driving. However, the majority of available simulators are incapable of replicating traffic patterns that accurately reflect the various features of real-world data while also simulating human-like reactive responses to the tested autopilot driving strategies. Taking one step forward to addressing such a problem, we propose Realistic Interactive TrAffic flow (RITA) as an integrated component of existing driving simulators to provide high-quality traffic flow for the evaluation and optimization of the tested driving strategies. RITA is developed with consideration of three key features, i.e., fidelity, diversity, and controllability, and consists of two core modules called RITABackend and RITAKit. RITABackend is built to support vehicle-wise control and provide traffic generation models from real-world datasets, while RITAKit is developed with easy-to-use interfaces for controllable traffic gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;data2vec-aqc&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2211.01246</link><description>&lt;p&gt;
data2vec-aqc&#65306;&#22312;&#24072;&#29983;&#35757;&#32451;&#20013;&#23547;&#25214;&#21512;&#36866;&#30340;&#21161;&#25945;&#12290;
&lt;/p&gt;
&lt;p&gt;
data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup. (arXiv:2211.01246v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;data2vec-aqc&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;data2vec-aqc&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25913;&#36827;&#22312;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#37117;&#24456;&#26377;&#38480;&#30340;&#35821;&#38899;&#39046;&#22495;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#26368;&#36817;&#24341;&#20837;&#30340;data2vec&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#27169;&#22359;&#26469;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#12289;&#37327;&#21270;&#34920;&#31034;&#21644;&#32858;&#31867;&#30340;&#20248;&#21183;&#12290;&#36825;&#20123;&#27169;&#22359;&#20043;&#38388;&#30340;&#20132;&#20114;&#24110;&#21161;&#35299;&#20915;&#20102;&#20132;&#21449;&#23545;&#27604;&#25439;&#22833;&#20316;&#20026;&#39069;&#22806;&#30340;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#12290;&#22312;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#24773;&#20917;&#19979;&#65292;data2vec-aqc&#22312;LibriSpeech&#30340;test-clean&#38598;&#21644;test-other&#38598;&#19978;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;data2vec&#31995;&#32479;&#20998;&#21035;&#21462;&#24471;&#20102;14.1&#65285;&#21644;20.9&#65285;&#30340;&#30456;&#23545;WER&#25552;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Switchboard&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#24494;&#35843;&#26102;&#20063;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;17.8&#65285;&#30340;&#30456;&#23545;WER&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new Self-Supervised Learning (SSL) algorithm called data2vec-aqc, for speech representation learning from unlabeled speech data. Our goal is to improve SSL for speech in domains where both unlabeled and labeled data are limited. Building on the recently introduced data2vec, we introduce additional modules to the data2vec framework that leverage the benefit of data augmentations, quantized representations, and clustering. The interaction between these modules helps solve the cross-contrastive loss as an additional self-supervised objective. data2vec-aqc achieves up to 14.1% and 20.9% relative WER improvement over the existing state-of-the-art data2vec system over the test-clean and test-other sets, respectively of LibriSpeech, without the use of any language model (LM). Our proposed model also achieves up to 17.8\% relative WER gains over the baseline data2vec when fine-tuned on a subset of the Switchboard dataset. Code: https://github.com/Speech-Lab-IITM/dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#22312;&#32447;&#25511;&#21046;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#36873;&#25321;&#21551;&#21457;&#24335;&#31574;&#30053;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#65292;&#20197;&#33719;&#24471;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#65292;&#23545;&#24212;&#29992;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2211.00759</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#22312;&#32447;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning. (arXiv:2211.00759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#22312;&#32447;&#25511;&#21046;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#36873;&#25321;&#21551;&#21457;&#24335;&#31574;&#30053;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#65292;&#20197;&#33719;&#24471;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#65292;&#23545;&#24212;&#29992;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#65288;ALNS&#65289;&#31639;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65288;COPs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#25104;&#21151;&#12290;ALNS&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21508;&#31181;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#21183;&#26469;&#25214;&#21040;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#12290;&#28982;&#32780;&#65292;ALNS&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20854;&#36873;&#25321;&#21644;&#25509;&#21463;&#21442;&#25968;&#30340;&#27491;&#30830;&#37197;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#36873;&#25321;&#21551;&#21457;&#24335;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#22522;&#20110;&#25628;&#32034;&#30340;&#29366;&#24577;&#23398;&#20064;&#22914;&#20309;&#37197;&#32622;&#19979;&#19968;&#27425;ALNS&#36845;&#20195;&#20197;&#33719;&#24471;&#22909;&#30340;&#20248;&#21270;&#38382;&#39064;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;&#21547;&#26377;&#38543;&#26426;&#26435;&#37325;&#21644;&#26102;&#38388;&#31383;&#21475;&#30340;&#23548;&#33322;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#38382;&#39064;&#29992;&#20110;IJCAI&#31454;&#36187;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26222;&#36890;&#30340;ALNS&#21644;&#20855;&#26377;&#40664;&#35748;&#21442;&#25968;&#35774;&#32622;&#30340;ALNS&#65292;&#23637;&#31034;&#20102;DRL&#26041;&#27861;&#22312;&#22312;&#32447;&#25511;&#21046;ALNS&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive Large Neighborhood Search (ALNS) algorithm has shown considerable success in solving complex combinatorial optimization problems (COPs). ALNS selects various heuristics adaptively during the search process, leveraging their strengths to find good solutions for optimization problems. However, the effectiveness of ALNS depends on the proper configuration of its selection and acceptance parameters. To address this limitation, we propose a Deep Reinforcement Learning (DRL) approach that selects heuristics, adjusts parameters, and controls the acceptance criteria during the search process. The proposed method aims to learn, based on the state of the search, how to configure the next iteration of the ALNS to obtain good solutions to the underlying optimization problem. We evaluate the proposed method on a time-dependent orienteering problem with stochastic weights and time windows, used in an IJCAI competition. The results show that our approach outperforms vanilla ALNS and ALNS
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#25991;&#26412;&#25110;&#22522;&#20110;&#22270;&#20687;&#30340;&#36807;&#28388;&#22120;&#26080;&#27861;&#26816;&#27979;&#21040;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.14616</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Late Multi-Modal Fusion Model for Detecting Hybrid Spam E-mail. (arXiv:2210.14616v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#25991;&#26412;&#25110;&#22522;&#20110;&#22270;&#20687;&#30340;&#36807;&#28388;&#22120;&#26080;&#27861;&#26816;&#27979;&#21040;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to design an effective approach filtering out hybrid spam e-mails and proposes a late multi-modal fusion model to solve the problem of traditional text-based or image-based filters failing to detect hybrid spam e-mails.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22403;&#22334;&#37038;&#20214;&#21457;&#36865;&#32773;&#24320;&#22987;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#21644;&#25991;&#26412;&#37096;&#20998;&#30340;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#26469;&#28151;&#28102;&#20854;&#24847;&#22270;&#65292;&#36825;&#27604;&#20165;&#21253;&#21547;&#25991;&#26412;&#25110;&#22270;&#20687;&#30340;&#30005;&#23376;&#37038;&#20214;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#21160;&#26426;&#26159;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#65292;&#20197;&#36991;&#20813;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#25110;&#22522;&#20110;&#22270;&#20687;&#30340;&#36807;&#28388;&#22120;&#26080;&#27861;&#26816;&#27979;&#21040;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#30340;&#24773;&#20917;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#26088;&#22312;&#26816;&#27979;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#12290;&#36890;&#24120;&#65292;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#29992;&#20110;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#26469;&#28040;&#38500;&#22403;&#22334;&#37038;&#20214;&#30340;&#22270;&#20687;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#38382;&#39064;&#26159;&#65292;&#23613;&#31649;OCR&#25195;&#25551;&#26159;&#22788;&#29702;&#25991;&#26412;&#21644;&#22270;&#20687;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#30340;&#38750;&#24120;&#25104;&#21151;&#30340;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#25152;&#38656;&#30340;CPU&#21151;&#29575;&#21644;&#25195;&#25551;&#30005;&#23376;&#37038;&#20214;&#25991;&#20214;&#25152;&#38656;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#23427;&#19981;&#26159;&#22788;&#29702;&#22823;&#37327;&#22403;&#22334;&#37038;&#20214;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, spammers are now trying to obfuscate their intents by introducing hybrid spam e-mail combining both image and text parts, which is more challenging to detect in comparison to e-mails containing text or image only. The motivation behind this research is to design an effective approach filtering out hybrid spam e-mails to avoid situations where traditional text-based or image-baesd only filters fail to detect hybrid spam e-mails. To the best of our knowledge, a few studies have been conducted with the goal of detecting hybrid spam e-mails. Ordinarily, Optical Character Recognition (OCR) technology is used to eliminate the image parts of spam by transforming images into text. However, the research questions are that although OCR scanning is a very successful technique in processing text-and-image hybrid spam, it is not an effective solution for dealing with huge quantities due to the CPU power required and the execution time it takes to scan e-mail files. And the OCR tech
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.15206</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20160;&#20040;&#26356;&#36866;&#21512;&#38646;&#26679;&#26412;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20256;&#32479;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#20250;&#22240;&#20026;&#36807;&#25311;&#21512;&#19981;&#20855;&#20195;&#34920;&#24615;&#30340;&#26631;&#27880;&#25968;&#25454;&#32780;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;prompt learning&#26356;&#26377;&#25928;&#30340;&#20551;&#35774;&#65292;&#22240;&#20026;&#23427;&#20351;&#24314;&#31435;&#22312;&#28023;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#39046;&#22495;&#30456;&#20851;&#20154;&#31867;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#22810;&#22320;&#21442;&#19982;&#39044;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#23567;&#22411;&#35757;&#32451;&#38598;&#25552;&#20379;&#30340;&#26377;&#38480;&#26631;&#31614;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;&#65292;&#35821;&#35328;&#24046;&#24322;&#21487;&#20197;&#34913;&#37327;&#25552;&#31034;&#36136;&#37327;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#26356;&#20026;&#37325;&#35201;&#30340;&#26159;&#65292;&#21463;&#21040;&#29702;&#35770;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#20107;&#20808;&#8220;&#39044;&#27979;&#8221;&#25552;&#31034;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#20540;&#24471;&#40723;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a theoretical framework to explain the efficacy of prompt learning in zero/few-shot scenarios. First, we prove that conventional pre-training and fine-tuning paradigm fails in few-shot scenarios due to overfitting the unrepresentative labelled data. We then detail the assumption that prompt learning is more effective because it empowers pre-trained language model that is built upon massive text corpora, as well as domain-related human knowledge to participate more in prediction and thereby reduces the impact of limited label information provided by the small training set. We further hypothesize that language discrepancy can measure the quality of prompting. Comprehensive experiments are performed to verify our assumptions. More remarkably, inspired by the theoretical framework, we propose an annotation-agnostic template selection method based on perplexity, which enables us to ``forecast'' the prompting performance in advance. This approach is especially encou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.06049</link><description>&lt;p&gt;
&#38754;&#21521;&#27861;&#24459;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65306;&#20197;&#21360;&#24230;&#27861;&#24459;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22686;&#22810;&#65292;&#29305;&#21035;&#26159;&#22312;&#27431;&#32654;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#65292;PLMs&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21360;&#24230;&#31561;&#20854;&#20182;&#22269;&#23478;&#30340;&#27861;&#24459;&#25991;&#26412;&#20855;&#26377;&#24456;&#22810;&#29305;&#27530;&#29305;&#24449;&#65292;&#22240;&#27492;&#20063;&#38656;&#35201;&#22312;&#36825;&#20123;&#26041;&#38754;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21360;&#24230;&#27861;&#24459;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#65288;&#32487;&#32493;&#39044;&#35757;&#32451;&#65289;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#27861;&#24459;PLMs, LegalBERT&#21644;CaseLawBERT&#65292;&#20197;&#21450;&#20351;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;PLMs&#24212;&#29992;&#20110;&#19977;&#20010;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#8212;&#8212;&#20174;&#20107;&#23454;&#20013;&#35782;&#21035;&#27861;&#24459;&#27861;&#35268;&#12289;&#23545;&#27861;&#38498;&#21028;&#20915;&#25991;&#20214;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#20197;&#21450;&#39044;&#27979;&#27861;&#38498;&#19978;&#35785;&#21028;&#20915;--&#22312;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#25991;&#26412;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36793;&#32536;&#20113;&#21327;&#20316;&#30340;&#23433;&#20840;&#39640;&#25928;&#30340;&#26426;&#26800;&#33218;&#25235;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;GAN&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#22270;&#20687;&#21152;&#23494;&#21387;&#32553;&#65292;&#21516;&#26102;&#22312;OCID&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;92%&#30340;&#20934;&#30830;&#29575;&#65292;&#22270;&#20687;&#21387;&#32553;&#27604;&#29575;&#36798;&#21040;&#20102;0.03%&#12290;</title><link>http://arxiv.org/abs/2209.03511</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#26800;&#33218;&#30340;&#39640;&#25928;&#23433;&#20840;&#22810;&#30446;&#26631;&#25235;&#21462;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Secure and Efficient Multi-Object Grasping Detection Approach for Robotic Arms. (arXiv:2209.03511v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36793;&#32536;&#20113;&#21327;&#20316;&#30340;&#23433;&#20840;&#39640;&#25928;&#30340;&#26426;&#26800;&#33218;&#25235;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;GAN&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#22270;&#20687;&#21152;&#23494;&#21387;&#32553;&#65292;&#21516;&#26102;&#22312;OCID&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;92%&#30340;&#20934;&#30830;&#29575;&#65292;&#22270;&#20687;&#21387;&#32553;&#27604;&#29575;&#36798;&#21040;&#20102;0.03%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#33218;&#22312;&#33258;&#21160;&#21270;&#34892;&#19994;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#26426;&#26800;&#33218;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20986;&#29616;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22914;&#25235;&#21462;&#35745;&#31639;&#33021;&#21147;&#30340;&#20998;&#37197;&#21644;&#23545;&#23433;&#20840;&#24615;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36793;&#32536;&#20113;&#21327;&#20316;&#30340;&#26426;&#26800;&#33218;&#25235;&#21462;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26426;&#26800;&#33218;&#30340;&#20219;&#24847;&#25235;&#21462;&#35268;&#21010;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#25235;&#21462;&#25928;&#29575;&#21644;&#20449;&#24687;&#23433;&#20840;&#12290;&#27492;&#22806;&#65292;&#30001;GAN&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20351;&#24471;&#22270;&#20687;&#22312;&#21387;&#32553;&#30340;&#21516;&#26102;&#34987;&#21152;&#23494;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#38544;&#31169;&#30340;&#23433;&#20840;&#24615;&#12290;&#35813;&#27169;&#22411;&#22312;OCID&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;92%&#30340;&#20934;&#30830;&#29575;&#65292;&#22270;&#20687;&#21387;&#32553;&#27604;&#29575;&#36798;&#21040;&#20102;0.03%&#65292;&#32467;&#26500;&#24046;&#24322;&#20540;&#39640;&#20110;0.91&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic arms are widely used in automatic industries. However, with wide applications of deep learning in robotic arms, there are new challenges such as the allocation of grasping computing power and the growing demand for security. In this work, we propose a robotic arm grasping approach based on deep learning and edge-cloud collaboration. This approach realizes the arbitrary grasp planning of the robot arm and considers the grasp efficiency and information security. In addition, the encoder and decoder trained by GAN enable the images to be encrypted while compressing, which ensures the security of privacy. The model achieves 92% accuracy on the OCID dataset, the image compression ratio reaches 0.03%, and the structural difference value is higher than 0.91.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#20803;&#30693;&#35782;&#28304;&#22312;&#32447;&#19968;-shot&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#65292;&#36890;&#36807;&#25972;&#21512;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12289;&#20219;&#21153;&#25191;&#34892;&#21644;&#25628;&#32034;&#30693;&#35782;&#12289;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#26469;&#23398;&#20064;&#27491;&#30830;&#20219;&#21153;&#30693;&#35782;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20195;&#29702;&#23545;&#22810;&#20803;&#30693;&#35782;&#28304;&#36827;&#34892;&#30340;&#22312;&#32447;&#25972;&#21512;&#24635;&#20307;&#19978;&#25552;&#39640;&#20102;&#19968;-shot&#20219;&#21153;&#23398;&#20064;&#30340;&#27700;&#24179;&#65292;&#20943;&#23569;&#20102;&#20154;&#31867;&#21453;&#39304;&#65292;&#20351;&#20219;&#21153;&#23398;&#20064;&#26356;&#24555;&#36895;&#19988;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2208.09554</link><description>&lt;p&gt;
&#25972;&#21512;&#22810;&#20803;&#30693;&#35782;&#28304;&#30340;&#22312;&#32447;&#19968;-shot&#23398;&#20064;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks. (arXiv:2208.09554v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#20803;&#30693;&#35782;&#28304;&#22312;&#32447;&#19968;-shot&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#65292;&#36890;&#36807;&#25972;&#21512;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12289;&#20219;&#21153;&#25191;&#34892;&#21644;&#25628;&#32034;&#30693;&#35782;&#12289;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#26469;&#23398;&#20064;&#27491;&#30830;&#20219;&#21153;&#30693;&#35782;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20195;&#29702;&#23545;&#22810;&#20803;&#30693;&#35782;&#28304;&#36827;&#34892;&#30340;&#22312;&#32447;&#25972;&#21512;&#24635;&#20307;&#19978;&#25552;&#39640;&#20102;&#19968;-shot&#20219;&#21153;&#23398;&#20064;&#30340;&#27700;&#24179;&#65292;&#20943;&#23569;&#20102;&#20154;&#31867;&#21453;&#39304;&#65292;&#20351;&#20219;&#21153;&#23398;&#20064;&#26356;&#24555;&#36895;&#19988;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#24191;&#27867;&#30340;&#20219;&#21153;&#30693;&#35782;&#28508;&#22312;&#26469;&#28304;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#20803;&#30693;&#35782;&#28304;&#22312;&#32447;&#19968;-shot&#23398;&#20064;&#27169;&#25311;&#21150;&#20844;&#23460;&#31227;&#21160;&#26426;&#22120;&#20154;&#26032;&#20219;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#24433;&#21709;&#12290;&#25152;&#24471;&#21040;&#30340;&#20195;&#29702;&#26159;&#22522;&#20110;Soar&#35748;&#30693;&#26550;&#26500;&#24320;&#21457;&#30340;&#65292;&#21033;&#29992;&#20197;&#19979;&#39046;&#22495;&#21644;&#20219;&#21153;&#30693;&#35782;&#30340;&#30693;&#35782;&#28304;&#65306;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12289;&#20219;&#21153;&#25191;&#34892;&#21644;&#25628;&#32034;&#30693;&#35782;&#12289;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20197;&#21450;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-3&#65289;&#26816;&#32034;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#30693;&#35782;&#26469;&#28304;&#30340;&#29420;&#29305;&#36129;&#29486;&#65292;&#24182;&#38024;&#23545;&#27491;&#30830;&#23398;&#20064;&#20219;&#21153;&#30693;&#35782;&#21644;&#20943;&#23569;&#20154;&#31867;&#24037;&#20316;&#36127;&#25285;&#30340;&#26041;&#26696;&#32452;&#21512;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20195;&#29702;&#23545;&#22810;&#20803;&#30693;&#35782;&#28304;&#36827;&#34892;&#30340;&#22312;&#32447;&#25972;&#21512;&#24635;&#20307;&#19978;&#25552;&#39640;&#20102;&#19968;-shot&#20219;&#21153;&#23398;&#20064;&#30340;&#27700;&#24179;&#65292;&#20943;&#23569;&#20102;&#19981;&#26029;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#20351;&#20219;&#21153;&#23398;&#20064;&#26356;&#24555;&#36895;&#19988;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents are able to draw on a wide variety of potential sources of task knowledge; however current approaches invariably focus on only one or two. Here we investigate the challenges and impact of exploiting diverse knowledge sources to learn online, in one-shot, new tasks for a simulated office mobile robot. The resulting agent, developed in the Soar cognitive architecture, uses the following sources of domain and task knowledge: interaction with the environment, task execution and search knowledge, human natural language instruction, and responses retrieved from a large language model (GPT-3). We explore the distinct contributions of these knowledge sources and evaluate the performance of different combinations in terms of learning correct task knowledge and human workload. Results show that an agent's online integration of diverse knowledge sources improves one-shot task learning overall, reducing human feedback needed for rapid and reliable task learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#30340;CASE&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#29992;&#25143;&#30340;&#31895;&#32454;&#31890;&#24230;&#30340;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#21305;&#37197;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20855;&#20849;&#24773;&#21644;&#20449;&#24687;&#24615;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2208.08845</link><description>&lt;p&gt;
CASE&#65306;&#35843;&#25972;&#31895;&#32454;&#31890;&#24230;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#21305;&#37197;&#20197;&#20135;&#29983;&#20849;&#24773;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation. (arXiv:2208.08845v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#30340;CASE&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#29992;&#25143;&#30340;&#31895;&#32454;&#31890;&#24230;&#30340;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#21305;&#37197;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20855;&#20849;&#24773;&#21644;&#20449;&#24687;&#24615;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#20132;&#27969;&#24212;&#35813;&#26159;&#20849;&#24773;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#26377;&#24847;&#35782;&#35843;&#25972;&#21644;&#20132;&#20114;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20849;&#24773;&#23545;&#35805;&#27169;&#22411;&#36890;&#24120;&#21482;&#32771;&#34385;&#24773;&#24863;&#26041;&#38754;&#25110;&#22312;&#35748;&#30693;&#21644;&#24773;&#24863;&#19978;&#21333;&#29420;&#22788;&#29702;&#65292;&#36825;&#38480;&#21046;&#20102;&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#30340;CASE&#27169;&#22411;&#12290;&#23427;&#39318;&#20808;&#24314;&#31435;&#22312;&#24120;&#35782;&#35748;&#30693;&#22270;&#21644;&#24773;&#24863;&#27010;&#24565;&#22270;&#30340;&#22522;&#30784;&#19978;&#65292;&#28982;&#21518;&#22312;&#31895;&#32454;&#30340;&#20004;&#20010;&#23618;&#38754;&#19978;&#23545;&#29992;&#25143;&#30340;&#35748;&#30693;&#21644;&#24773;&#24863;&#36827;&#34892;&#21305;&#37197;&#12290;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;CASE&#20248;&#20110;&#20849;&#24773;&#23545;&#35805;&#30340;&#26368;&#26032;&#22522;&#32447;&#65292;&#24182;&#19988;&#21487;&#20197;&#20135;&#29983;&#26356;&#20855;&#20849;&#24773;&#21644;&#20449;&#24687;&#24615;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user's cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses.
&lt;/p&gt;</description></item><item><title>&#27431;&#20960;&#37324;&#24471;&#20559;&#22909;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25509;&#36817;&#20219;&#24847;&#12289;&#30495;&#23454;&#12289;&#20154;&#31867;&#30340;&#20559;&#22909;&#65292;&#29978;&#33267;&#26377;&#19968;&#20123;&#24773;&#20917;&#19979;&#20960;&#20046;&#25152;&#26377;&#20559;&#22909;&#37197;&#32622;&#37117;&#19981;&#33021;&#29992;&#27431;&#20960;&#37324;&#24471;&#27169;&#22411;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2208.08160</link><description>&lt;p&gt;
&#27431;&#20960;&#37324;&#24471;&#20559;&#22909;&#27169;&#22411;&#30340;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Error in the Euclidean Preference Model. (arXiv:2208.08160v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08160
&lt;/p&gt;
&lt;p&gt;
&#27431;&#20960;&#37324;&#24471;&#20559;&#22909;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25509;&#36817;&#20219;&#24847;&#12289;&#30495;&#23454;&#12289;&#20154;&#31867;&#30340;&#20559;&#22909;&#65292;&#29978;&#33267;&#26377;&#19968;&#20123;&#24773;&#20917;&#19979;&#20960;&#20046;&#25152;&#26377;&#20559;&#22909;&#37197;&#32622;&#37117;&#19981;&#33021;&#29992;&#27431;&#20960;&#37324;&#24471;&#27169;&#22411;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#20559;&#22909;&#27169;&#22411;&#26159;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#65289;&#23398;&#21040;&#30340;&#30690;&#37327;&#23884;&#20837;&#30340;&#24418;&#24335;&#12290;&#36890;&#24120;&#36825;&#20123;&#27169;&#22411;&#34987;&#20551;&#23450;&#20026;&#36817;&#20284;&#27431;&#20960;&#37324;&#24471;&#32467;&#26500;&#65292;&#20854;&#20013;&#20010;&#20154;&#26356;&#21916;&#27426;&#36317;&#20854;&#8220;&#29702;&#24819;&#28857;&#8221;&#26356;&#36817;&#30340;&#36873;&#25321;&#65292;&#36825;&#26159;&#30001;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#34913;&#37327;&#24471;&#26469;&#30340;&#12290;&#28982;&#32780;&#65292;Bogomolnaia &#21644; Laslier (2007) &#34920;&#26126;&#65292;&#22914;&#26524;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#32500;&#24230;&#27604;&#20010;&#20154;&#25110;&#36873;&#25321;&#30340;&#25968;&#37327;&#23569;&#20004;&#20010;&#65292;&#21017;&#23384;&#22312;&#26080;&#27861;&#29992;&#27492;&#32467;&#26500;&#34920;&#31034;&#30340;&#24207;&#20559;&#22909;&#37197;&#32622;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20010;&#32467;&#26524;&#65292;&#34920;&#26126;&#26377;&#20123;&#24773;&#20917;&#19979;&#20960;&#20046;&#25152;&#26377;&#20559;&#22909;&#37197;&#32622;&#37117;&#19981;&#33021;&#29992;&#27431;&#20960;&#37324;&#24471;&#27169;&#22411;&#34920;&#31034;&#65292;&#24182;&#25512;&#23548;&#20986;&#22312;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#27169;&#22411;&#36817;&#20284;&#38750;&#27431;&#20960;&#37324;&#24471;&#20559;&#22909;&#37197;&#32622;&#26102;&#39044;&#26399;&#35823;&#24046;&#30340;&#29702;&#35770;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#30690;&#37327;&#23884;&#20837;&#30340;&#35299;&#37322;&#21644;&#20351;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#27431;&#20960;&#37324;&#24471;&#20559;&#22909;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25509;&#36817;&#20219;&#24847;&#12289;&#30495;&#23454;&#12289;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial models of preference, in the form of vector embeddings, are learned by many deep learning and multiagent systems, including recommender systems. Often these models are assumed to approximate a Euclidean structure, where an individual prefers alternatives positioned closer to their "ideal point", as measured by the Euclidean metric. However, Bogomolnaia and Laslier (2007) showed that there exist ordinal preference profiles that cannot be represented with this structure if the Euclidean space has two fewer dimensions than there are individuals or alternatives. We extend this result, showing that there are situations in which almost all preference profiles cannot be represented with the Euclidean model, and derive a theoretical lower bound on the expected error when using the Euclidean model to approximate non-Euclidean preference profiles. Our results have implications for the interpretation and use of vector embeddings, because in some cases close approximation of arbitrary, tru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21450;&#20854;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20511;&#21161;&#20960;&#20309;&#27979;&#37327;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#23454;&#34892;&#19968;&#20010;&#31616;&#21333;&#30340;&#31751;&#32467;&#26500;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.10541</link><description>&lt;p&gt;
&#25581;&#31034;&#25512;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Latent Space Geometry of Push-Forward Generative Models. (arXiv:2207.10541v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21450;&#20854;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20511;&#21161;&#20960;&#20309;&#27979;&#37327;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#23454;&#34892;&#19968;&#20010;&#31616;&#21333;&#30340;&#31751;&#32467;&#26500;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#37117;&#26159;&#36890;&#36807;&#36830;&#32493;&#29983;&#25104;&#22120;&#25512;&#36827;&#39640;&#26031;&#27979;&#37327;&#32780;&#23450;&#20041;&#30340;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#25110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#26412;&#25991;&#25506;&#31350;&#20102;&#36825;&#20123;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#22312;&#23398;&#20064;&#19981;&#36830;&#36890;&#20998;&#24067;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#36755;&#20986;&#36229;&#20986;&#30446;&#26631;&#20998;&#24067;&#25903;&#25345;&#33539;&#22260;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#23427;&#20204;&#30340;&#28508;&#22312;&#31354;&#38388;&#20960;&#20309;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20511;&#21161;&#20960;&#20309;&#27979;&#37327;&#29702;&#35770;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25105;&#20204;&#22312;&#28508;&#22312;&#31354;&#38388;&#30340;&#32500;&#24230;&#22823;&#20110;&#27169;&#30340;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#20248;&#21270;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#23545;GAN&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#29702;&#35770;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20960;&#20309;&#32467;&#26500;&#33719;&#24471;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#23454;&#34892;&#19968;&#20010;&#31616;&#21333;&#30340;&#31751;&#32467;&#26500;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many deep generative models are defined as a push-forward of a Gaussian measure by a continuous generator, such as Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). This work explores the latent space of such deep generative models. A key issue with these models is their tendency to output samples outside of the support of the target distribution when learning disconnected distributions. We investigate the relationship between the performance of these models and the geometry of their latent space. Building on recent developments in geometric measure theory, we prove a sufficient condition for optimality in the case where the dimension of the latent space is larger than the number of modes. Through experiments on GANs, we demonstrate the validity of our theoretical results and gain new insights into the latent space geometry of these models. Additionally, we propose a truncation method that enforces a simplicial cluster structure in the latent space and improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#26497;&#21270;&#30721;&#26500;&#36896;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#22797;&#26434;&#24230;&#19982;&#22359;&#38271;&#24230;&#21644;&#30721;&#29575;&#26080;&#20851;&#65292;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.01105</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#26497;&#21270;&#30721;&#26500;&#36896;&#26041;&#27861;: &#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#21462;&#28040;&#21015;&#34920;&#35793;&#30721;
&lt;/p&gt;
&lt;p&gt;
Scalable Polar Code Construction for Successive Cancellation List Decoding: A Graph Neural Network-Based Approach. (arXiv:2207.01105v4 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#26497;&#21270;&#30721;&#26500;&#36896;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#22797;&#26434;&#24230;&#19982;&#22359;&#38271;&#24230;&#21644;&#30721;&#29575;&#26080;&#20851;&#65292;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#30721;&#30340;&#26500;&#36896;&#21487;&#20197;&#36890;&#36807;&#25490;&#24207;&#20301;&#36890;&#36947;&#23454;&#29616;&#36830;&#32493;&#21462;&#28040;&#35793;&#30721;&#30340;&#39640;&#25928;&#24615;&#65292;&#20294;&#23545;&#20110;&#24490;&#29615;&#20887;&#20313;&#26657;&#39564;&#36741;&#21161;&#30340;&#36830;&#32493;&#21462;&#28040;&#21015;&#34920;&#35793;&#30721;&#30340;&#26368;&#20248;&#26497;&#21270;&#30721;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#26500;&#36896;&#20173;&#38656;&#25506;&#31350;&#12290;&#26412;&#25991;&#39318;&#20808;&#23558;&#26497;&#21270;&#30721;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#26497;&#21270;&#30721;&#26500;&#36896;&#28040;&#24687;&#20256;&#36882;&#65288;PCCMP&#65289;&#22270;&#30340;&#29420;&#29305;&#24322;&#26500;&#22270;&#20013;&#12290;&#25509;&#30528;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#36845;&#20195;&#28040;&#24687;&#20256;&#36882;&#65288;IMP&#65289;&#31639;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040;&#23545;&#24212;&#20110;CA-SCL&#35793;&#30721;&#19979;&#20855;&#26377;&#26368;&#23567;&#24103;&#38169;&#35823;&#29575;&#30340;PCCMP&#22270;&#12290;&#36825;&#31181;&#26032;&#30340;IMP&#31639;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;&#21363;&#27169;&#22411;&#22797;&#26434;&#24230;&#19982;&#22359;&#38271;&#24230;&#21644;&#30721;&#29575;&#26080;&#20851;&#65292;&#19988;&#22312;&#30701;&#26497;&#21270;&#30721;&#19978;&#35757;&#32451;&#30340;IMP&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#38271;&#26497;&#21270;&#30721;&#30340;&#26500;&#36896;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;IMP&#30340;&#26497;&#21270;&#30721;&#26500;&#36896;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While constructing polar codes for successive-cancellation decoding can be implemented efficiently by sorting the bit-channels, finding optimal polar codes for cyclic-redundancy-check-aided successive-cancellation list (CA-SCL) decoding in an efficient and scalable manner still awaits investigation. This paper first maps a polar code to a unique heterogeneous graph called the polar-code-construction message-passing (PCCMP) graph. Next, a heterogeneous graph-neural-network-based iterative message-passing (IMP) algorithm is proposed which aims to find a PCCMP graph that corresponds to the polar code with minimum frame error rate under CA-SCL decoding. This new IMP algorithm's major advantage lies in its scalability power. That is, the model complexity is independent of the blocklength and code rate, and a trained IMP model over a short polar code can be readily applied to a long polar code's construction. Numerical experiments show that IMP-based polar-code constructions outperform class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#20915;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#36890;&#20449;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#21355;&#26143;&#38388;&#38142;&#36335;&#12289;&#30701;&#26242;&#26102;&#38388;&#36807;&#31243;&#21644;&#21355;&#26143;&#35206;&#30422;&#33539;&#22260;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.00414</link><description>&lt;p&gt;
&#19979;&#19968;&#20195;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Techniques for Next-Generation Mega Satellite Networks. (arXiv:2207.00414v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#20915;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#36890;&#20449;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#21355;&#26143;&#38388;&#38142;&#36335;&#12289;&#30701;&#26242;&#26102;&#38388;&#36807;&#31243;&#21644;&#21355;&#26143;&#35206;&#30422;&#33539;&#22260;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22826;&#31354;&#21457;&#23556;&#12289;&#30005;&#23376;&#12289;&#22788;&#29702;&#33021;&#21147;&#21644;&#24494;&#22411;&#21270;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#31354;&#38388;&#36890;&#20449;&#65292;&#29305;&#21035;&#26159;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#65292;&#37325;&#26032;&#25104;&#20026;&#19979;&#19968;&#20195;&#32593;&#32476;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#20505;&#36873;&#32773;&#12290;&#28982;&#32780;&#65292;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#20381;&#36182;&#20110;&#26080;&#25968;&#30340;&#22522;&#30784;&#21644;&#30456;&#20114;&#20132;&#32455;&#30340;&#36807;&#31243;&#65292;&#20256;&#32479;&#27169;&#22411;&#19981;&#33021;&#30495;&#27491;&#25429;&#25417;&#23427;&#20204;&#30340;&#21160;&#24577;&#21644;&#29420;&#29305;&#29305;&#24449;&#65292;&#22914;&#36712;&#36947;&#36895;&#24230;&#12289;&#21355;&#26143;&#38388;&#38142;&#36335;&#12289;&#30701;&#26242;&#30340;&#26102;&#38388;&#36807;&#31243;&#21644;&#21355;&#26143;&#35206;&#30422;&#33539;&#22260;&#31561;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#20351;&#32593;&#32476;&#20027;&#21160;&#36866;&#24212;&#19982;&#38142;&#25509;&#20013;&#24555;&#36895;&#21464;&#21270;&#30340;&#26465;&#20214;&#12290;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#25429;&#25417;&#36825;&#20123;&#36807;&#31243;&#12289;&#20998;&#26512;&#23427;&#20204;&#30340;&#34892;&#20026;&#24182;&#27169;&#25311;&#23427;&#20204;&#23545;&#32593;&#32476;&#24433;&#21709;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24212;&#29992;AI&#25216;&#26415;&#20110;&#32508;&#21512;&#22320;&#38754;&#21355;&#26143;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#36890;&#20449;&#30340;&#24212;&#29992;&#12290;&#23427;&#35814;&#32454;&#20171;&#32461;&#20102;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#30340;&#29420;&#29305;&#29305;&#24449;&#21644;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;AI&#30340;&#25216;&#26415;&#22914;&#20309;&#24110;&#21161;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Space communications, particularly mega satellite networks, re-emerged as an appealing candidate for next generation networks due to major advances in space launching, electronics, processing power, and miniaturization. However, mega satellite networks rely on numerous underlying and intertwined processes that cannot be truly captured using conventionally used models, due to their dynamic and unique features such as orbital speed, inter-satellite links, short time pass, and satellite footprint, among others. Hence, new approaches are needed to enable the network to proactively adjust to the rapidly varying conditions associated within the link. Artificial intelligence (AI) provides a pathway to capture these processes, analyze their behavior, and model their effect on the network. This article introduces the application of AI techniques for integrated terrestrial satellite networks, particularly mega satellite network communications. It details the unique features of mega satellite net
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DRAGNET++, &#36890;&#36807;&#32771;&#34385;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#12289;&#20256;&#25773;&#32467;&#26500;&#21644;&#29992;&#25143;&#20132;&#20114;&#65292;&#20197;&#39044;&#27979;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21487;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25552;&#20379;&#22312;&#24694;&#24847;&#23545;&#35805;&#21319;&#32423;&#20043;&#21069;&#35782;&#21035;&#21644;&#31649;&#29702;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2206.08406</link><description>&lt;p&gt;
&#39044;&#27979;Twitter&#23545;&#35805;&#32447;&#31243;&#20013;&#30340;&#20167;&#24680;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DRAGNET++, &#36890;&#36807;&#32771;&#34385;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#12289;&#20256;&#25773;&#32467;&#26500;&#21644;&#29992;&#25143;&#20132;&#20114;&#65292;&#20197;&#39044;&#27979;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21487;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25552;&#20379;&#22312;&#24694;&#24847;&#23545;&#35805;&#21319;&#32423;&#20043;&#21069;&#35782;&#21035;&#21644;&#31649;&#29702;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#25991;&#26159;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#26368;&#31616;&#27905;&#30340;&#20132;&#27969;&#24418;&#24335;&#65292;&#19968;&#26465;&#25512;&#25991;&#26377;&#21487;&#33021;&#26159;&#23545;&#35805;&#20013;&#25171;&#36896;&#25110;&#30772;&#22351;&#35752;&#35770;&#30340;&#28508;&#22312;&#23186;&#20171;&#12290;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#23481;&#26131;&#33719;&#24471;&#65292;&#38459;&#27490;&#20854;&#20256;&#25773;&#23545;&#20110;&#31038;&#20132;&#23186;&#20307;&#20844;&#21496;&#21644;&#29992;&#25143;&#26469;&#35828;&#26159;&#26497;&#20026;&#37325;&#35201;&#30340;&#65292;&#21487;&#20197;&#25512;&#36827;&#33391;&#22909;&#30340;&#20132;&#27969;&#26041;&#24335;&#12290;&#30446;&#21069;&#38500;&#20102;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22806;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#19987;&#27880;&#20110;&#20998;&#31867;&#21333;&#20010;&#25512;&#25991;&#65292;&#32780;&#24573;&#30053;&#20102;&#25512;&#25991;&#20043;&#38388;&#30340;&#23545;&#35805;&#32447;&#31243;/&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGNET++&#65292;&#26088;&#22312;&#36890;&#36807;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#39044;&#27979;&#23427;&#21487;&#33021;&#24102;&#26469;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#21644;&#20256;&#25773;&#32467;&#26500;&#20197;&#21450;&#32447;&#31243;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;DRAGNET++&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35748;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21487;&#20197;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#39044;&#27979;&#21644;&#31649;&#29702;&#24694;&#24847;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tweets are the most concise form of communication in online social media, wherein a single tweet has the potential to make or break the discourse of the conversation. Online hate speech is more accessible than ever, and stifling its propagation is of utmost importance for social media companies and users for congenial communication. Most of the research barring a recent few has focused on classifying an individual tweet regardless of the tweet thread/context leading up to that point. One of the classical approaches to curb hate speech is to adopt a reactive strategy after the hate speech postage. The ex-post facto strategy results in neglecting subtle posts that do not show the potential to instigate hate speech on their own but may portend in the subsequent discussion ensuing in the post's replies. In this paper, we propose DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring in through its reply chain in the future. It uses the semantic and propagating stru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27010;&#24565;&#30340;&#8220;&#38750;&#22238;&#28335;&#8221;&#30697;&#38453;&#24182;&#35777;&#26126;&#20102;&#30456;&#24212;&#30340;Ihara-Bass&#22411;&#20844;&#24335;&#65292;&#21033;&#29992;&#35813;&#29702;&#35770;&#35777;&#26126;&#20102;&#38543;&#26426;k-CSP&#23454;&#20363;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#24378;&#35777;&#26126;&#30340;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.10881</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#24067;&#23572;&#30697;&#38453;&#30340;Ihara-Bass&#20844;&#24335;&#21450;&#38543;&#26426;CSPs&#30340;&#24378;&#35777;&#26126;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Ihara-Bass Formula for Non-Boolean Matrices and Strong Refutations of Random CSPs. (arXiv:2204.10881v2 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27010;&#24565;&#30340;&#8220;&#38750;&#22238;&#28335;&#8221;&#30697;&#38453;&#24182;&#35777;&#26126;&#20102;&#30456;&#24212;&#30340;Ihara-Bass&#22411;&#20844;&#24335;&#65292;&#21033;&#29992;&#35813;&#29702;&#35770;&#35777;&#26126;&#20102;&#38543;&#26426;k-CSP&#23454;&#20363;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#24378;&#35777;&#26126;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#38750;&#22238;&#28335;&#8221;&#30697;&#38453;&#27010;&#24565;&#65292;&#21487;&#19982;&#20219;&#20309;&#23545;&#31216;&#30697;&#38453;&#30456;&#20851;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#24212;&#30340;Ihara-Bass&#22411;&#20844;&#24335;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#29702;&#35770;&#35777;&#26126;&#20102;&#20851;&#20110;$k$&#20010;&#21464;&#37327;&#27599;&#32422;&#26463;&#30340;&#38543;&#26426;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;k-CSPs&#65289;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#24378;&#35777;&#26126;&#30340;&#26032;&#32467;&#26524;&#12290;&#23545;&#20110;&#30001;&#19968;&#20010;&#30001;$p$&#20998;&#25968;&#30340;&#20998;&#37197;&#28385;&#36275;&#30340;&#32422;&#26463;&#26500;&#24314;&#30340;&#38543;&#26426;k-CSP&#23454;&#20363;&#65292;&#22914;&#26524;&#23454;&#20363;&#21253;&#21547;$n$&#20010;&#21464;&#37327;&#21644;$n^{k/2} / \epsilon^2$&#20010;&#32422;&#26463;&#65292;&#21017;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#20986;&#26368;&#20248;&#35299;&#20165;&#28385;&#36275;$p+O_k(\epsilon)$&#20010;&#32422;&#26463;&#30340;&#35777;&#20070;&#12290;&#20197;&#21069;&#65292;&#36825;&#20165;&#23545;&#20110;&#20598;&#25968;$k$&#26159;&#24050;&#30693;&#30340;&#65292;&#20294;&#23545;&#20110;&#22855;&#25968;$k$&#65292;&#38656;&#35201;$n^{k/2} (\log n)^{O(1)} / \epsilon^2$&#20010;&#38543;&#26426;&#32422;&#26463;&#25165;&#33021;&#24471;&#20986;&#30456;&#21516;&#30340;&#32467;&#35770;&#12290;&#34429;&#28982;&#25913;&#36827;&#20165;&#26159;&#23545;&#25968;&#32423;&#21035;&#30340;&#65292;&#20294;&#23427;&#20811;&#26381;&#20102;&#36825;&#31867;&#32467;&#26524;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define a novel notion of ``non-backtracking'' matrix associated to any symmetric matrix, and we prove a ``Ihara-Bass'' type formula for it.  We use this theory to prove new results on polynomial-time strong refutations of random constraint satisfaction problems with $k$ variables per constraints (k-CSPs). For a random k-CSP instance constructed out of a constraint that is satisfied by a $p$ fraction of assignments, if the instance contains $n$ variables and $n^{k/2} / \epsilon^2$ constraints, we can efficiently compute a certificate that the optimum satisfies at most a $p+O_k(\epsilon)$ fraction of constraints.  Previously, this was known for even $k$, but for odd $k$ one needed $n^{k/2} (\log n)^{O(1)} / \epsilon^2$ random constraints to achieve the same conclusion.  Although the improvement is only polylogarithmic, it overcomes a significant barrier to these types of results. Strong refutation results based on current approaches construct a certificate that a certain matrix associ
&lt;/p&gt;</description></item><item><title>COOL&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#35266;&#23519;&#22120;&#65292;&#29992;&#20110;&#32534;&#30721;&#23616;&#37096;&#21477;&#27861;&#19978;&#19979;&#25991;&#65292;&#33021;&#22815;&#25913;&#36827;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2204.09593</link><description>&lt;p&gt;
COOL&#65306;&#19968;&#31181;&#19978;&#19979;&#25991;&#35266;&#23519;&#22120;&#21450;&#20854;&#22312;&#38382;&#31572;&#21644;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks. (arXiv:2204.09593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09593
&lt;/p&gt;
&lt;p&gt;
COOL&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#35266;&#23519;&#22120;&#65292;&#29992;&#20110;&#32534;&#30721;&#23616;&#37096;&#21477;&#27861;&#19978;&#19979;&#25991;&#65292;&#33021;&#22815;&#25913;&#36827;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19978;&#19979;&#25991;&#35266;&#23519;&#22120;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#23616;&#37096;&#27880;&#24847;&#21147;&#24418;&#24335;&#30340;&#35266;&#23519;&#27880;&#24847;&#21147;&#25913;&#36827;&#20102;&#35270;&#35273;Transformer&#30340;&#24615;&#33021;&#12290;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#39046;&#22495;&#19968;&#26679;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26500;&#25104;&#20102;&#22823;&#22810;&#25968;&#22788;&#29702;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35266;&#23519;&#27880;&#24847;&#21147;&#26426;&#21046;COOL&#12290;COOL&#28155;&#21152;&#21040;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#23618;&#20043;&#19978;&#65292;&#32771;&#34385;&#21333;&#35789;&#30340;&#25509;&#36817;&#24615;&#21644;&#26356;&#22810;&#30340;&#25104;&#23545;&#32422;&#26463;&#65292;&#32534;&#30721;&#23616;&#37096;&#21477;&#27861;&#19978;&#19979;&#25991;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23545;COOL&#30340;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#23454;&#35777;&#24615;&#33021;&#35780;&#20272;&#65292;&#35777;&#23454;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25913;&#36827;&#22522;&#32447;&#27169;&#22411;&#30340;&#26426;&#20250;&#65292;&#21253;&#25324;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision outlooker improves the performance of vision transformers, which implements a self-attention mechanism by adding an outlook attention, a form of local attention.  In natural language processing, as has been the case in computer vision and other domains, transformer-based models constitute the state-of-the-art for most processing tasks. In this domain, too, many authors have argued and demonstrated the importance of local context.  We present an outlook attention mechanism, COOL, for natural language processing. COOL, added on top of the self-attention layers of a transformer-based model, encodes local syntactic context considering word proximity and more pair-wise constraints than dynamic convolution used by existing approaches.  A comparative empirical performance evaluation of an implementation of COOL with different transformer-based models confirms the opportunity for improvement over a baseline using the original models alone for various natural language processing tasks, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#37329;&#23383;&#22612;&#34701;&#21512;&#26041;&#27861;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#26469;&#25366;&#25496;&#36328;&#29305;&#24449;&#37329;&#23383;&#22612;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2201.04019</link><description>&lt;p&gt;
&#37329;&#23383;&#22612;&#34701;&#21512;&#21464;&#25442;&#22120;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#37329;&#23383;&#22612;&#34701;&#21512;&#26041;&#27861;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#26469;&#25366;&#25496;&#36328;&#29305;&#24449;&#37329;&#23383;&#22612;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;MaskFormer&#20026;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65306;&#23427;&#20174;&#27969;&#34892;&#30340;&#20687;&#32032;&#32423;&#20998;&#31867;&#33539;&#24335;&#36716;&#31227;&#21040;&#20102;&#19968;&#31181;&#38754;&#21521;&#25513;&#30721;&#32423;&#21035;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#26412;&#36136;&#19978;&#65292;&#23427;&#29983;&#25104;&#19982;&#31867;&#21035;&#29255;&#27573;&#23545;&#24212;&#30340;&#25104;&#23545;&#27010;&#29575;&#21644;&#25513;&#30721;&#65292;&#24182;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#20998;&#21106;&#22270;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#22522;&#20110;&#21333;&#23610;&#24230;&#29305;&#24449;&#30340;&#27599;&#20010;&#25513;&#30721;&#20998;&#31867;&#35299;&#30721;&#22120;&#19981;&#36275;&#20197;&#25552;&#21462;&#21487;&#38752;&#30340;&#27010;&#29575;&#25110;&#25513;&#30721;&#12290;&#20026;&#20102;&#25366;&#25496;&#36328;&#29305;&#24449;&#37329;&#23383;&#22612;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#29992;&#20110;&#21333;&#23610;&#24230;&#35821;&#20041;&#20998;&#21106;&#30340;&#37329;&#23383;&#22612;&#34701;&#21512;&#21464;&#25442;&#22120;&#65288;PFT&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#22312;&#27599;&#20010;&#31354;&#38388;&#29305;&#24449;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#20043;&#38388;&#24182;&#34892;&#25191;&#34892;&#20132;&#21449;&#27880;&#24847;&#65292;&#24182;&#20351;&#29992;&#36328;&#23610;&#24230;&#38388;&#26597;&#35810;&#27880;&#24847;&#26469;&#20132;&#25442;&#34917;&#20805;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed MaskFormer gives a refreshed perspective on the task of semantic segmentation: it shifts from the popular pixel-level classification paradigm to a mask-level classification method. In essence, it generates paired probabilities and masks corresponding to category segments and combines them during inference for the segmentation maps. In our study, we find that per-mask classification decoder on top of a single-scale feature is not effective enough to extract reliable probability or mask. To mine for rich semantic information across the feature pyramid, we propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask approach semantic segmentation with multi-scale features. The proposed transformer decoder performs cross-attention between the learnable queries and each spatial feature from the feature pyramid in parallel and uses cross-scale inter-query attention to exchange complimentary information. We achieve competitive performance on three widely use
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;&#21487;&#31359;&#25140;&#35774;&#22791;&#30417;&#27979;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#25214;&#21040;&#19982;&#25351;&#23450;&#20027;&#39064;&#30456;&#20851;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#30561;&#30496;&#36136;&#37327;&#27963;&#21160;&#24314;&#35758;&#65292;&#20026;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2110.13745</link><description>&lt;p&gt;
PARIS&#65306;&#29992;&#20110;&#25913;&#21892;&#30561;&#30496;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PARIS: Personalized Activity Recommendation for Improving Sleep Quality. (arXiv:2110.13745v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;&#21487;&#31359;&#25140;&#35774;&#22791;&#30417;&#27979;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#25214;&#21040;&#19982;&#25351;&#23450;&#20027;&#39064;&#30456;&#20851;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#30561;&#30496;&#36136;&#37327;&#27963;&#21160;&#24314;&#35758;&#65292;&#20026;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#36136;&#37327;&#23545;&#20154;&#20204;&#30340;&#36523;&#20307;&#21644;&#24515;&#29702;&#20581;&#24247;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#30561;&#30496;&#19981;&#36275;&#30340;&#20154;&#26356;&#23481;&#26131;&#25253;&#21578;&#36523;&#20307;&#21644;&#24515;&#29702;&#22256;&#25200;&#12289;&#27963;&#21160;&#21463;&#38480;&#12289;&#28966;&#34385;&#21644;&#30140;&#30171;&#12290;&#27492;&#22806;&#65292;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#27963;&#21160;&#30417;&#27979;&#21644;&#20581;&#24247;&#36319;&#36394;&#30340;&#24212;&#29992;&#21644;&#35774;&#22791;&#26041;&#20852;&#26410;&#33406;&#12290;&#20174;&#36825;&#20123;&#21487;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#21040;&#30340;&#20449;&#21495;&#21487;&#29992;&#20110;&#30740;&#31350;&#21644;&#25913;&#21892;&#30561;&#30496;&#36136;&#37327;&#12290;&#26412;&#25991;&#21033;&#29992;&#23454;&#20307;&#27963;&#21160;&#21644;&#30561;&#30496;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25214;&#21040;&#21327;&#21161;&#20154;&#20204;&#25913;&#21892;&#30561;&#30496;&#30340;&#26041;&#27861;&#12290;&#23545;&#27963;&#21160;&#25968;&#25454;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#65292;&#25105;&#20204;&#25214;&#21040;&#19982;&#29305;&#23450;&#20027;&#39064;&#26368;&#26174;&#30528;&#30340;&#34892;&#20026;&#27169;&#24335;&#30456;&#20851;&#30340;&#31751;&#20013;&#24515;&#12290;&#28982;&#21518;&#20026;&#27599;&#20010;&#34892;&#20026;&#27169;&#24335;&#20013;&#30340;&#27599;&#20010;&#31751;&#29983;&#25104;&#26377;&#21161;&#20110;&#33391;&#22909;&#30561;&#30496;&#36136;&#37327;&#30340;&#27963;&#21160;&#24314;&#35758;&#12290;&#36825;&#20123;&#27963;&#21160;&#24314;&#35758;&#20379;&#24212;&#32473;&#27599;&#20301;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of sleep has a deep impact on people's physical and mental health. People with insufficient sleep are more likely to report physical and mental distress, activity limitation, anxiety, and pain. Moreover, in the past few years, there has been an explosion of applications and devices for activity monitoring and health tracking. Signals collected from these wearable devices can be used to study and improve sleep quality. In this paper, we utilize the relationship between physical activity and sleep quality to find ways of assisting people improve their sleep using machine learning techniques. People usually have several behavior modes that their bio-functions can be divided into. Performing time series clustering on activity data, we find cluster centers that would correlate to the most evident behavior modes for a specific subject. Activity recipes are then generated for good sleep quality for each behavior mode within each cluster. These activity recipes are supplied to an a
&lt;/p&gt;</description></item><item><title>AxoNN&#26159;&#19968;&#31181;&#21033;&#29992;&#24322;&#27493;&#24615;&#21644;&#28040;&#24687;&#39537;&#21160;&#25191;&#34892;&#35843;&#24230;&#27599;&#20010;GPU&#19978;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;CPU&#20869;&#23384;&#20316;&#20026;&#20887;&#20313;&#31354;&#38388;&#65292;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#23558;&#27599;&#20010;GPU&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#24517;&#38656;&#30340;GPU&#25968;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2110.13005</link><description>&lt;p&gt;
AxoNN: &#19968;&#31181;&#24322;&#27493;&#12289;&#28040;&#24687;&#39537;&#21160;&#30340;&#26497;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13005
&lt;/p&gt;
&lt;p&gt;
AxoNN&#26159;&#19968;&#31181;&#21033;&#29992;&#24322;&#27493;&#24615;&#21644;&#28040;&#24687;&#39537;&#21160;&#25191;&#34892;&#35843;&#24230;&#27599;&#20010;GPU&#19978;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;CPU&#20869;&#23384;&#20316;&#20026;&#20887;&#20313;&#31354;&#38388;&#65292;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#23558;&#27599;&#20010;GPU&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#24517;&#38656;&#30340;GPU&#25968;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#23384;&#20648;&#22120;&#23481;&#37327;&#24050;&#36828;&#36828;&#36229;&#20986;&#29616;&#20195;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;DRAM&#23481;&#37327;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#22522;&#20110;GPU&#30340;&#38598;&#32676;&#19978;&#24320;&#21457;&#39640;&#25928;&#31639;&#27861;&#24182;&#34892;&#35757;&#32451;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#29616;&#20195;GPU&#19978;&#65292;&#35745;&#31639;&#30456;&#23545;&#24265;&#20215;&#65292;&#20026;&#20102;&#25552;&#21462;&#26368;&#22823;&#24615;&#33021;&#65292;&#35774;&#35745;&#21644;&#23454;&#29616;&#36825;&#20123;&#24182;&#34892;&#35757;&#32451;&#31639;&#27861;&#20013;&#26497;&#20854;&#39640;&#25928;&#30340;&#36890;&#20449;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AxoNN&#65292;&#19968;&#31181;&#21033;&#29992;&#24322;&#27493;&#24615;&#21644;&#28040;&#24687;&#39537;&#21160;&#25191;&#34892;&#35843;&#24230;&#27599;&#20010;GPU&#19978;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#31354;&#38386;&#26102;&#38388;&#65292;&#26368;&#22823;&#21270;&#30828;&#20214;&#25928;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;CPU&#20869;&#23384;&#20316;&#20026;&#20887;&#20313;&#31354;&#38388;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#23450;&#26399;&#21368;&#36733;&#25968;&#25454;&#65292;AxoNN&#33021;&#22815;&#23558;GPU&#20869;&#23384;&#28040;&#32791;&#38477;&#20302;4&#20493;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#27599;&#20010;GPU&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#24517;&#38656;&#30340;GPU&#25968;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, the memory requirements to train state-of-the-art neural networks have far exceeded the DRAM capacities of modern hardware accelerators. This has necessitated the development of efficient algorithms to train these neural networks in parallel on large-scale GPU-based clusters. Since computation is relatively inexpensive on modern GPUs, designing and implementing extremely efficient communication in these parallel training algorithms is critical for extracting the maximum performance. This paper presents AxoNN, a parallel deep learning framework that exploits asynchrony and message-driven execution to schedule neural network operations on each GPU, thereby reducing GPU idle time and maximizing hardware efficiency. By using the CPU memory as a scratch space for offloading data periodically during training, AxoNN is able to reduce GPU memory consumption by four times. This allows us to increase the number of parameters per GPU by four times, thus reducing the amount 
&lt;/p&gt;</description></item><item><title>"Label-Assemble"&#26159;&#19968;&#31181;&#25972;&#21512;&#24102;&#26377;Partial Labels&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#26469;&#37322;&#25918;&#20854;&#20840;&#37096;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#22312;&#26032;&#22411;&#30142;&#30149;&#35786;&#26029;&#20013;&#65292;&#20174;&#36127;&#26679;&#26412;&#20013;&#23398;&#20064;&#23545;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#21644;&#26816;&#27979;&#37117;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;NIH ChestX-ray14&#20013;&#30340;&#29616;&#26377;&#26631;&#31614;&#32452;&#35013;&#36215;&#26469;&#65292;&#21487;&#20197;&#23558;COVID-19&#30340;&#20934;&#30830;&#35786;&#26029;&#29575;&#20174;96.3&#65285;&#25552;&#39640;&#21040;99.3&#65285;&#12290;&#27492;&#22806;&#65292;&#25972;&#21512;&#26631;&#31614;&#36824;&#21487;&#20197;&#25552;&#39640;&#30142;&#30149;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2109.12265</link><description>&lt;p&gt;
&#22810;&#32452;&#26377;Partial Labels&#30340;&#25968;&#25454;&#38598;&#25972;&#21512;&#30340;Label-Assemble&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Label-Assemble: Leveraging Multiple Datasets with Partial Labels. (arXiv:2109.12265v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12265
&lt;/p&gt;
&lt;p&gt;
"Label-Assemble"&#26159;&#19968;&#31181;&#25972;&#21512;&#24102;&#26377;Partial Labels&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#26469;&#37322;&#25918;&#20854;&#20840;&#37096;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#22312;&#26032;&#22411;&#30142;&#30149;&#35786;&#26029;&#20013;&#65292;&#20174;&#36127;&#26679;&#26412;&#20013;&#23398;&#20064;&#23545;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#21644;&#26816;&#27979;&#37117;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;NIH ChestX-ray14&#20013;&#30340;&#29616;&#26377;&#26631;&#31614;&#32452;&#35013;&#36215;&#26469;&#65292;&#21487;&#20197;&#23558;COVID-19&#30340;&#20934;&#30830;&#35786;&#26029;&#29575;&#20174;96.3&#65285;&#25552;&#39640;&#21040;99.3&#65285;&#12290;&#27492;&#22806;&#65292;&#25972;&#21512;&#26631;&#31614;&#36824;&#21487;&#20197;&#25552;&#39640;&#30142;&#30149;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#20294;&#25105;&#20204;&#36890;&#24120;&#21482;&#33021;&#35775;&#38382;&#20960;&#20010;&#24102;&#26377;Partial Labels&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20513;&#35758;&#65292;&#21363;&#8220;Label-Assemble&#8221;&#65292;&#26088;&#22312;&#20174;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#20013;&#37322;&#25918;Partial Labels&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#20174;&#36127;&#26679;&#26412;&#20013;&#23398;&#20064;&#26377;&#21161;&#20110;&#35745;&#31639;&#26426;&#36741;&#21161;&#30142;&#30149;&#35786;&#26029;&#21644;&#26816;&#27979;&#65292;&#36825;&#20010;&#21457;&#29616;&#22312;&#26032;&#22411;&#30142;&#30149;&#35786;&#26029;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#27491;&#26679;&#26412;&#24448;&#24448;&#38590;&#20197;&#25910;&#38598;&#65292;&#32780;&#36127;&#26679;&#26412;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#12290;&#20363;&#22914;&#65292;&#20174;NIH ChestX-ray14 &#65288;&#33258;2017&#24180;&#20197;&#26469;&#21487;&#29992;&#65289;&#20013;&#32452;&#35013;&#29616;&#26377;&#26631;&#31614;&#21487;&#23558; COVID-19 &#30340;&#35786;&#26029;&#20934;&#30830;&#29575;&#20174;96.3&#65285;&#25552;&#39640;&#21040;99.3&#65285;&#12290;&#38500;&#20102;&#35786;&#26029;&#22806;&#65292;&#32452;&#35013;&#26631;&#31614;&#36824;&#21487;&#20197;&#25913;&#21892;&#30142;&#30149;&#30340;&#26816;&#27979;&#65292;&#20363;&#22914;&#33008;&#33146;&#23548;&#31649;&#33146;&#30284; (PDAC) &#30340;&#26816;&#27979;&#21487;&#20197;&#20174;&#21033;&#29992;&#22218;&#32959;&#21644;&#33008;&#33146;&#20869;&#20998;&#27852;&#32959;&#30244; (&#20004;&#31181;&#30456;&#20851;&#30142;&#30149;) &#30340;&#26631;&#31614;&#20013;&#33719;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#30456;&#21516;&#30340;&#25104;&#20687;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of deep learning relies heavily on large labeled datasets, but we often only have access to several small datasets associated with partial labels. To address this problem, we propose a new initiative, "Label-Assemble", that aims to unleash the full potential of partial labels from an assembly of public datasets. We discovered that learning from negative examples facilitates both computer-aided disease diagnosis and detection. This discovery will be particularly crucial in novel disease diagnosis, where positive examples are hard to collect, yet negative examples are relatively easier to assemble. For example, assembling existing labels from NIH ChestX-ray14 (available since 2017) significantly improves the accuracy of COVID-19 diagnosis from 96.3% to 99.3%. In addition to diagnosis, assembling labels can also improve disease detection, e.g., the detection of pancreatic ductal adenocarcinoma (PDAC) can greatly benefit from leveraging the labels of Cysts and PanNets (two othe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#12289;&#39640;&#32500;&#34920;&#31034;&#30340;AIS&#25968;&#25454;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#26174;&#24335;&#32771;&#34385;&#24322;&#36136;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#21464;&#21387;&#22120;&#32593;&#32476;&#26469;&#39044;&#27979;&#33337;&#33334;&#20301;&#32622;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2109.03958</link><description>&lt;p&gt;
TrAISformer&#8212;&#8212;&#29992;&#20110;AIS&#36712;&#36857;&#39044;&#27979;&#30340;&#29983;&#25104;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
TrAISformer-A generative transformer for AIS trajectory prediction. (arXiv:2109.03958v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#12289;&#39640;&#32500;&#34920;&#31034;&#30340;AIS&#25968;&#25454;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#26174;&#24335;&#32771;&#34385;&#24322;&#36136;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#21464;&#21387;&#22120;&#32593;&#32476;&#26469;&#39044;&#27979;&#33337;&#33334;&#20301;&#32622;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33337;&#33334;&#22312;&#26410;&#26469;&#29305;&#23450;&#26102;&#38388;&#28857;&#30340;&#20301;&#32622;&#26159;&#35768;&#22810;&#28023;&#20107;&#24212;&#29992;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#34429;&#28982;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#65288;AIS&#65289;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#26469;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#65292;&#20294;&#20351;&#29992;AIS&#25968;&#25454;&#26469;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#20173;&#28982;&#20855;&#26377;&#26497;&#22823;&#30340;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20063;&#26159;&#22914;&#27492;&#65292;&#22240;&#20026;&#36816;&#21160;&#25968;&#25454;&#26412;&#36136;&#19978;&#20855;&#26377;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;AIS&#25968;&#25454;&#31163;&#25955;&#21270;&#39640;&#32500;&#34920;&#31034;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26174;&#24335;&#32771;&#34385;&#24322;&#36136;&#24615;&#21644;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#8212;&#8212;TrAISformer&#8212;&#8212;&#26159;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#22312;&#25152;&#25552;&#20986;&#30340;&#20016;&#23500;&#31354;&#38388;&#20013;&#25552;&#21462;AIS&#36712;&#36857;&#30340;&#38271;&#26399;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#20960;&#20010;&#23567;&#26102;&#21518;&#33337;&#33334;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#30495;&#23454;AIS&#25968;&#25454;&#19978;&#25253;&#21578;&#23454;&#39564;&#32467;&#26524;&#12290;TrAISformer&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of vessel positions at a specified point in the future is a fundamental aspect of many maritime applications. While Automatic Identification System (AIS) provides a rich source of information to enable this task, vessel trajectory forecasting using AIS data remains formidably challenging, even for modern machine learning/deep learning, because of the complexity and multimodality inherent in motion data. In this paper, we address these challenges by introducing a novel discrete, high-dimensional representation of AIS data and a new loss function to explicitly account for heterogeneity and multimodality. The proposed model -- referred to as TrAISformer -- is a modified transformer network that extracts long-term correlations of AIS trajectories in the proposed enriched space to forecast the positions of vessels several hours into the future. We report experimental results on publicly available, real AIS data. TrAISformer significantly outperforms state-of-the-art methods a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2107.11972</link><description>&lt;p&gt;
&#24403;&#26426;&#20250;&#26469;&#20020;&#26102;&#36827;&#34892;&#20132;&#26131;&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#30340;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Iterative Refinement Labeling. (arXiv:2107.11972v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26088;&#22312;&#26681;&#25454;&#24403;&#21069;&#24066;&#22330;&#24773;&#20917;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#39044;&#27979;&#37329;&#34701;&#36164;&#20135;&#30340;&#26410;&#26469;&#36235;&#21183;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37329;&#34701;&#25968;&#25454;&#30340;&#20302;&#20449;&#22122;&#27604;&#21644;&#38543;&#26426;&#24615;&#26497;&#24378;&#65292;&#22909;&#30340;&#20132;&#26131;&#26426;&#20250;&#26497;&#20026;&#31232;&#23569;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#19981;&#20180;&#32454;&#36873;&#25321;&#28508;&#22312;&#30340;&#30408;&#21033;&#26679;&#26412;&#65292;&#36825;&#20123;ML&#26041;&#27861;&#23481;&#26131;&#25429;&#25417;&#21040;&#22122;&#22768;&#32780;&#19981;&#26159;&#30495;&#23454;&#20449;&#21495;&#30340;&#27169;&#24335;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#65288;LA-Attention&#65289;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#65288;IRL&#65289;&#12290;LA-Attention&#26088;&#22312;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#37329;&#34701;&#25968;&#25454;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#32780;IRL&#21017;&#26088;&#22312;&#36845;&#20195;&#22320;&#32454;&#21270;&#26631;&#27880;&#36807;&#31243;&#65292;&#36807;&#28388;&#25481;&#22122;&#22768;&#21644;&#26080;&#20851;&#26679;&#26412;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Price movement forecasting aims at predicting the future trends of financial assets based on the current market conditions and other relevant information. Recently, machine learning (ML) methods have become increasingly popular and achieved promising results for price movement forecasting in both academia and industry. Most existing ML solutions formulate the forecasting problem as a classification (to predict the direction) or a regression (to predict the return) problem over the entire set of training data. However, due to the extremely low signal-to-noise ratio and stochastic nature of financial data, good trading opportunities are extremely scarce. As a result, without careful selection of potentially profitable samples, such ML methods are prone to capture the patterns of noises instead of real signals. To address this issue, we propose a novel price movement forecasting framework named LARA consisting of two main components: Locality-Aware Attention (LA-Attention) and Iterative R
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#24037;&#31243;&#20195;&#34920;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#23454;&#29616;&#30340;&#35745;&#31639;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#26412;&#25991;&#20998;&#21035;&#27604;&#36739;&#20102;&#33258;&#19979;&#32780;&#19978;&#20197;&#21450;&#33258;&#19978;&#32780;&#19979;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#21644;&#35752;&#35770;&#20102;&#31070;&#32463;&#24418;&#24577;&#22120;&#20214;&#30340;&#29289;&#29702;&#35774;&#35745;&#21644;&#24037;&#33402;&#21046;&#36896;&#12289;&#24212;&#29992;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2106.01288</link><description>&lt;p&gt;
&#33258;&#28982;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#21644;&#21327;&#21516;&#65306;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#31995;&#32479;&#30340;&#33258;&#19979;&#32780;&#19978;&#21644;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bottom-up and top-down approaches for the design of neuromorphic processing systems: Tradeoffs and synergies between natural and artificial intelligence. (arXiv:2106.01288v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01288
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#24037;&#31243;&#20195;&#34920;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#23454;&#29616;&#30340;&#35745;&#31639;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#26412;&#25991;&#20998;&#21035;&#27604;&#36739;&#20102;&#33258;&#19979;&#32780;&#19978;&#20197;&#21450;&#33258;&#19978;&#32780;&#19979;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#21644;&#35752;&#35770;&#20102;&#31070;&#32463;&#24418;&#24577;&#22120;&#20214;&#30340;&#29289;&#29702;&#35774;&#35745;&#21644;&#24037;&#33402;&#21046;&#36896;&#12289;&#24212;&#29992;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#30340;&#25509;&#36817;&#23613;&#22836;&#65292;&#23545;&#35745;&#31639;&#33021;&#21147;&#30340;&#25351;&#25968;&#26399;&#26395;&#27491;&#22312;&#25512;&#21160;&#23547;&#27714;&#25552;&#39640;&#25972;&#20307;&#31995;&#32479;&#24615;&#33021;&#30340;&#26032;&#36884;&#24452;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#25506;&#32034;&#26088;&#22312;&#23454;&#29616;&#29983;&#29289;&#31070;&#32463;&#22788;&#29702;&#31995;&#32479;&#30340;&#28789;&#27963;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#26367;&#20195;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#20307;&#31995;&#32467;&#26500;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#31070;&#32463;&#24418;&#24577;&#24037;&#31243;&#20195;&#34920;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#23454;&#29616;&#30340;&#35745;&#31639;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#22312;&#36825;&#31181;&#24418;&#24577;&#20013;&#65292;&#22788;&#29702;&#21644;&#23384;&#20648;&#34987;&#32039;&#23494;&#22320;&#25918;&#32622;&#22312;&#19968;&#36215;&#12290;&#26412;&#25991;&#23545;&#36825;&#19968;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#31361;&#20986;&#20102;&#23454;&#29616;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#30340;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#65292;&#24182;&#27604;&#36739;&#20102;&#19987;&#27880;&#20110;&#22797;&#21046;&#33258;&#28982;&#26234;&#33021;&#65288;&#33258;&#19979;&#32780;&#19978;&#65289;&#19982;&#37027;&#20123;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65288;&#33258;&#19978;&#32780;&#19979;&#65289;&#30340;&#35774;&#35745;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27169;&#25311;&#12289;&#28151;&#21512;&#20449;&#21495;&#21644;&#25968;&#23383;&#30005;&#36335;&#35774;&#35745;&#39118;&#26684;&#65292;&#30830;&#23450;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#31070;&#32463;&#24418;&#24577;&#22120;&#20214;&#27169;&#26495;&#30340;&#29289;&#29702;&#35774;&#35745;&#21644;&#24037;&#33402;&#21046;&#36896;&#65292;&#20197;&#21450;&#36825;&#20123;&#24037;&#33402;&#21046;&#36896;&#30340;&#24322;&#36136;&#24615;&#21644;&#30005;&#24615;&#33021;&#23545;&#22120;&#20214;&#24615;&#33021;&#21644;&#25972;&#20307;&#31995;&#32479;&#35774;&#35745;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31070;&#32463;&#24418;&#24577;&#24037;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#24863;&#22120;&#24212;&#29992;&#12289;&#26426;&#22120;&#24863;&#30693;&#12289;&#20302;&#21151;&#32791;/&#39640;&#25928;&#33021;&#22788;&#29702;&#21644;&#26234;&#33021;&#36793;&#32536;&#35774;&#22791;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Moore's law has driven exponential computing power expectations, its nearing end calls for new avenues for improving the overall system performance. One of these avenues is the exploration of alternative brain-inspired computing architectures that aim at achieving the flexibility and computational efficiency of biological neural processing systems. Within this context, neuromorphic engineering represents a paradigm shift in computing based on the implementation of spiking neural network architectures in which processing and memory are tightly co-located. In this paper, we provide a comprehensive overview of the field, highlighting the different levels of granularity at which this paradigm shift is realized and comparing design approaches that focus on replicating natural intelligence (bottom-up) versus those that aim at solving practical artificial intelligence applications (top-down). First, we present the analog, mixed-signal and digital circuit design styles, identifying the b
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;Uni-Encoder&#65292;&#35299;&#20915;&#20102;Cross-Encoder&#22810;&#27425;&#32534;&#30721;&#30456;&#21516;&#19978;&#19979;&#25991;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;Poly-Encoder&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#35813;&#33539;&#20363;&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2106.01263</link><description>&lt;p&gt;
Uni-Encoder: &#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#24555;&#36895;&#20934;&#30830;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01263
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;Uni-Encoder&#65292;&#35299;&#20915;&#20102;Cross-Encoder&#22810;&#27425;&#32534;&#30721;&#30456;&#21516;&#19978;&#19979;&#25991;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;Poly-Encoder&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#35813;&#33539;&#20363;&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26679;&#26412;&#19982;&#25490;&#24207;&#26159;&#29616;&#20195;&#29983;&#25104;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#20174;&#29983;&#25104;&#30340;&#23569;&#37327;&#20505;&#36873;&#31572;&#26696;&#20013;&#36873;&#25321;&#19968;&#20010;&#31572;&#26696;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25490;&#24207;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#31216;&#20026;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#33539;&#20363;&#65292;&#35813;&#32534;&#30721;&#22120;&#20998;&#21035;&#23545;&#27599;&#20010;&#19978;&#19979;&#25991;-&#20505;&#36873;&#23545;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#26681;&#25454;&#20854;&#36866;&#24212;&#24230;&#24471;&#20998;&#23545;&#20505;&#36873;&#36827;&#34892;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#20132;&#21449;&#32534;&#30721;&#22120;&#20026;&#27599;&#20010;&#20505;&#36873;&#37325;&#22797;&#32534;&#30721;&#30456;&#21516;&#30340;&#20887;&#38271;&#19978;&#19979;&#25991;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;Poly-Encoder&#36890;&#36807;&#20943;&#23569;&#19978;&#19979;&#25991;&#21644;&#20505;&#36873;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#20294;&#20195;&#20215;&#26159;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#31216;&#20026;Uni-Encoder&#65292;&#23427;&#20687;&#20132;&#21449;&#32534;&#30721;&#22120;&#19968;&#26679;&#23436;&#20840;&#20851;&#27880;&#27599;&#20010;&#20505;&#36873;&#23545;&#65292;&#21516;&#26102;&#20687;Poly-&#32534;&#30721;&#22120;&#19968;&#26679;&#21482;&#32534;&#30721;&#19968;&#27425;&#19978;&#19979;&#25991;&#12290;Uni-Encoder&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#38024;&#23545;&#25152;&#26377;&#20505;&#36873;&#20351;&#29992;&#30456;&#21516;&#30340;&#20301;&#32622;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called Cross-Encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, Cross-Encoder repeatedly encodes the same lengthy context for each candidate, resulting in high computational costs. Poly-Encoder addresses the above problems by reducing the interaction between context and candidates, but with a price of performance drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps the full attention over each pair as in Cross-Encoder while only encoding the context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with the context in one forward pass. We use the same positional embedding for all candidates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2102.12227</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#29992;&#20110;&#35770;&#36848;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#22312;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#26550;&#26500;&#65292;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#19981;&#23545;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#29992;&#25143;&#29983;&#25104;&#35780;&#35770;&#12289;&#31185;&#23398;&#20986;&#29256;&#29289;&#21644;&#21149;&#35828;&#24615;&#35770;&#25991;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#20855;&#26377;&#26356;&#39640;&#35745;&#31639;&#21360;&#35760;&#25110;&#29305;&#23450;&#20110;&#35821;&#26009;&#24211;&#35774;&#35745;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#30340;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#20195;&#34920;&#20102;&#36890;&#29992;&#24615;&#12289;&#24615;&#33021;&#31934;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#20043;&#38388;&#30340;&#26377;&#36259;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.
&lt;/p&gt;</description></item></channel></rss>