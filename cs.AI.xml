<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01481</link><description>&lt;p&gt;
&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Multi-level protein pre-training with Vabs-Net
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#19977;&#32500;&#32467;&#26500;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#21363;&#945;&#30899;&#21407;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21407;&#23376;&#65292;&#22914;&#20391;&#38142;&#21407;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#19978;&#23545;&#34507;&#30333;&#36136;&#36827;&#34892;&#24314;&#27169;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20391;&#38142;&#21407;&#23376;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#23376;&#23545;&#25509;&#65289;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#20013;&#22825;&#30495;&#22320;&#32452;&#21512;&#27531;&#22522;&#21644;&#21407;&#23376;&#20449;&#24687;&#36890;&#24120;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20449;&#24687;&#27844;&#28431;&#26159;&#21253;&#21547;&#21407;&#23376;&#32467;&#26500;&#30340;&#36755;&#20837;&#23548;&#33268;&#27531;&#22522;&#32423;&#39044;&#35757;&#32451;&#20219;&#21153;&#21464;&#24471;&#29712;&#30862;&#24182;&#23548;&#33268;&#27531;&#22522;&#34920;&#31034;&#19981;&#22815;&#20805;&#20998;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25513;&#30721;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01401</link><description>&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#22312;&#35268;&#27169;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01401
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#25454;&#35268;&#23450;&#65292;&#20174;&#35757;&#32451;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36951;&#24536;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#21450;&#26102;&#24536;&#35760;&#24517;&#35201;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36951;&#24536;&#30340;&#22330;&#26223;&#65292;&#21363;&#21482;&#26377;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#65292;&#36951;&#24536;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#31227;&#38500;&#25968;&#25454;&#12290;&#26681;&#25454;&#36825;&#26679;&#23450;&#20041;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#19981;&#22815;&#30340;&#12290;&#22522;&#20110;Lipschitz&#36830;&#32493;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#25200;&#21160;&#30340;&#36755;&#20986;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#26469;&#35825;&#23548;&#36951;&#24536;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24179;&#28369;&#24615;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#36951;&#24536;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#24403;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20005;&#26684;&#30340;&#38646;&#26679;&#26412;&#32422;&#26463;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
&lt;/p&gt;</description></item><item><title>LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01376</link><description>&lt;p&gt;
LoTR: &#20302;&#24352;&#37327;&#31209;&#26435;&#37325;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LoTR: Low Tensor Rank Weight Adaptation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01376
&lt;/p&gt;
&lt;p&gt;
LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24605;&#24819;&#25512;&#24191;&#21644;&#25193;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;Transformer&#26550;&#26500;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;LoRA&#31867;&#26041;&#27861;&#26159;&#22522;&#20110;&#26799;&#24230;&#26356;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoTR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#26041;&#27861;&#65292;&#23427;&#20197;&#24352;&#37327;&#20998;&#35299;&#30340;&#24418;&#24335;&#34920;&#31034;&#21442;&#25968;&#30340;&#26799;&#24230;&#26356;&#26032;&#12290;&#27599;&#20010;&#23618;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#37117;&#30001;&#19977;&#20010;&#30697;&#38453;&#30340;&#20056;&#31215;&#26500;&#25104;&#65292;&#32780;&#24352;&#37327;&#32467;&#26500;&#26159;&#30001;&#36825;&#20010;&#20056;&#31215;&#30340;&#24038;&#21491;&#20056;&#23376;&#22312;&#23618;&#20043;&#38388;&#20849;&#20139;&#24341;&#36215;&#30340;&#12290;&#36890;&#36807;&#23545;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#23618;&#21516;&#26102;&#21387;&#32553;&#65292;LoTR&#33021;&#22815;&#27604;LoRA&#22312;&#29305;&#21035;&#26159;&#23545;&#20110;&#28145;&#23618;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26680;&#24515;&#24352;&#37327;&#19981;&#20381;&#36182;&#20110;&#21407;&#22987;&#26435;&#37325;&#32500;&#24230;&#65292;&#21487;&#20197;&#20219;&#24847;&#32553;&#23567;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#24120;&#24265;&#20215;&#21644;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01327</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#20013;&#30340;&#30417;&#30563;&#31639;&#27861;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Supervised Algorithmic Fairness in Distribution Shifts: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#38754;&#23545;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#65292;&#22914;&#20309;&#20445;&#25345;&#20844;&#24179;&#21644;&#26080;&#20559;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#22240;&#21508;&#31181;&#22240;&#32032;&#32780;&#38543;&#26102;&#38388;&#21457;&#29983;&#21464;&#21270;&#12290;&#36825;&#31181;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#65292;&#23545;&#29305;&#23450;&#36890;&#36807;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#26469;&#34920;&#24449;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#22343;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20102;&#24635;&#32467;&#65292;&#24182;&#20840;&#38754;&#35843;&#26597;&#20102;&#22522;&#20110;&#36825;&#20123;&#21464;&#21270;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#25991;&#29486;&#20013;&#31361;&#20986;&#20102;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20221;&#35843;&#26597;&#21015;&#20986;&#20102;&#29992;&#20110;&#23454;&#35777;&#30740;&#31350;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#25351;&#26631;&#26469;&#35780;&#20272;FU&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#39564;&#35777;&#12289;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#27934;&#23519;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#20123;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01276</link><description>&lt;p&gt;
&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;: &#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Federated Unlearning: a Perspective of Stability and Fairness
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#25351;&#26631;&#26469;&#35780;&#20272;FU&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#39564;&#35777;&#12289;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#27934;&#23519;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#20123;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#22810;&#26041;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FU&#35780;&#20272;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#37325;&#28857;&#20851;&#27880;&#39564;&#35777;&#65292;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20869;&#22312;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#23545;&#20855;&#26377;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#21462;&#28040;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#23545;FU&#20013;&#26435;&#34913;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#65292;&#20026;FU&#26426;&#21046;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;FU&#26426;&#21046;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#26435;&#34913;&#65292;&#30830;&#35748;&#20102;&#20174;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#20013;&#24471;&#20986;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the multifaceted consequences of federated unlearning (FU) with data heterogeneity. We introduce key metrics for FU assessment, concentrating on verification, global stability, and local fairness, and investigate the inherent trade-offs. Furthermore, we formulate the unlearning process with data heterogeneity through an optimization framework. Our key contribution lies in a comprehensive theoretical analysis of the trade-offs in FU and provides insights into data heterogeneity's impacts on FU. Leveraging these insights, we propose FU mechanisms to manage the trade-offs, guiding further development for FU mechanisms. We empirically validate that our FU mechanisms effectively balance trade-offs, confirming insights derived from our theoretical analysis.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01207</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Causal Graph Discovery Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#12290;&#20043;&#21069;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#25104;&#23545;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#38656;&#35201;&#20108;&#27425;&#26597;&#35810;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;&#36739;&#22823;&#30340;&#22240;&#26524;&#22270;&#26469;&#35828;&#24456;&#24555;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#26377;&#25152;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#32467;&#21512;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#20855;&#26102;&#38388;&#21644;&#25968;&#25454;&#25928;&#29575;&#22806;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01204</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#20013;SSL&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;&#21270;&#21644;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;SSL&#24050;&#25104;&#20026;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#26032;&#36235;&#21183;&#65292;&#36825;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#25551;&#36848;&#24615;&#30340;&#34920;&#31034;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#22238;&#39038;&#21644;&#24635;&#32467;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#65288;SSL4NS-TD&#65289;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;NS-TD&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#26041;&#27861;&#34987;&#20998;&#20026;&#19977;&#32452;&#8212;&#8212;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#24182;&#20171;&#32461;&#20102;&#27599;&#20010;&#26041;&#21521;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#21160;&#26426;&#21644;&#20248;&#28857;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#20171;&#32461;&#20102;SSL4NS-TD&#30340;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
&lt;/p&gt;</description></item><item><title>&#35748;&#30693;&#20114;&#32852;&#32593;&#36229;&#36234;&#20102;&#35748;&#30693;&#29289;&#32852;&#32593;&#65292;&#20351;&#36830;&#25509;&#30340;&#29289;&#20307;&#33021;&#22815;&#29420;&#31435;&#22320;&#33719;&#21462;&#30693;&#35782;&#21644;&#29702;&#35299;&#65292;&#24182;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#38598;&#25104;&#20102;&#21327;&#20316;&#26234;&#33021;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#8220;&#35748;&#30693;&#20114;&#32852;&#32593;&#8221;&#33539;&#24335;&#30340;&#22522;&#30784;&#35201;&#32032;&#12289;&#29420;&#29305;&#29305;&#24449;&#12289;&#30410;&#22788;&#21644;&#24037;&#19994;&#24433;&#21709;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00876</link><description>&lt;p&gt;
&#29992;&#20110;&#24378;&#21270;&#28151;&#21512;&#36793;&#32536;&#20113;&#30340;&#35748;&#30693;&#20114;&#32852;&#32593;&#30340;&#26500;&#24314;&#22359;
&lt;/p&gt;
&lt;p&gt;
Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00876
&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#20114;&#32852;&#32593;&#36229;&#36234;&#20102;&#35748;&#30693;&#29289;&#32852;&#32593;&#65292;&#20351;&#36830;&#25509;&#30340;&#29289;&#20307;&#33021;&#22815;&#29420;&#31435;&#22320;&#33719;&#21462;&#30693;&#35782;&#21644;&#29702;&#35299;&#65292;&#24182;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#38598;&#25104;&#20102;&#21327;&#20316;&#26234;&#33021;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#8220;&#35748;&#30693;&#20114;&#32852;&#32593;&#8221;&#33539;&#24335;&#30340;&#22522;&#30784;&#35201;&#32032;&#12289;&#29420;&#29305;&#29305;&#24449;&#12289;&#30410;&#22788;&#21644;&#24037;&#19994;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25105;&#20204;&#20174;&#31227;&#21160;&#20114;&#32852;&#32593;&#36807;&#28193;&#21040;&#8220;&#35748;&#30693;&#20114;&#32852;&#32593;&#8221;&#65292;&#25105;&#20204;&#22312;&#22914;&#20309;&#19982;&#25216;&#26415;&#21644;&#26234;&#33021;&#20114;&#21160;&#26041;&#38754;&#21457;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35748;&#30693;&#20114;&#32852;&#32593;&#36229;&#36234;&#20102;&#35748;&#30693;&#29289;&#32852;&#32593;&#65288;&#35748;&#30693;IoT&#65289;&#65292;&#20351;&#36830;&#25509;&#30340;&#29289;&#20307;&#33021;&#22815;&#29420;&#31435;&#22320;&#33719;&#21462;&#30693;&#35782;&#21644;&#29702;&#35299;&#12290;&#19982;&#31227;&#21160;&#20114;&#32852;&#32593;&#21644;&#35748;&#30693;IoT&#19981;&#21516;&#65292;&#35748;&#30693;&#20114;&#32852;&#32593;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#38598;&#25104;&#20102;&#21327;&#20316;&#26234;&#33021;&#65292;&#23558;&#35748;&#30693;&#29289;&#32852;&#32593;&#39046;&#22495;&#19982;&#31995;&#32479;&#33539;&#22260;&#30340;&#21327;&#20316;&#21644;&#20154;&#31867;&#26234;&#33021;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#36825;&#31181;&#38598;&#25104;&#26234;&#33021;&#20419;&#36827;&#20102;&#35774;&#22791;&#12289;&#26381;&#21153;&#12289;&#23454;&#20307;&#21644;&#20010;&#20154;&#20043;&#38388;&#22312;&#19981;&#21516;&#39046;&#22495;&#20869;&#30340;&#20114;&#21160;&#65292;&#21516;&#26102;&#20445;&#25345;&#20915;&#31574;&#33258;&#20027;&#24615;&#24182;&#36866;&#24212;&#21508;&#31181;&#36523;&#20221;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#8220;&#35748;&#30693;&#20114;&#32852;&#32593;&#8221;&#33539;&#24335;&#30340;&#22522;&#30784;&#35201;&#32032;&#12289;&#29420;&#29305;&#29305;&#24449;&#12289;&#30410;&#22788;&#21644;&#24037;&#19994;&#24433;&#21709;&#12290;&#23427;&#24378;&#35843;&#20102;&#36866;&#24212;&#24615;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#21644;&#28151;&#21512;&#36793;&#32536;&#20113;&#65288;HEC&#65289;&#24179;&#21488;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As we transition from the mobile internet to the 'Cognitive Internet,' a significant shift occurs in how we engage with technology and intelligence. We contend that the Cognitive Internet goes beyond the Cognitive Internet of Things (Cognitive IoT), enabling connected objects to independently acquire knowledge and understanding. Unlike the Mobile Internet and Cognitive IoT, the Cognitive Internet integrates collaborative intelligence throughout the network, blending the cognitive IoT realm with system-wide collaboration and human intelligence. This integrated intelligence facilitates interactions between devices, services, entities, and individuals across diverse domains while preserving decision-making autonomy and accommodating various identities.   The paper delves into the foundational elements, distinct characteristics, benefits, and industrial impact of the 'Cognitive Internet' paradigm. It highlights the importance of adaptable AI infrastructures and hybrid edge cloud (HEC) plat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24418;&#24335;&#35821;&#35328;&#22312;&#22788;&#29702;&#21407;&#22987;&#36755;&#20837;&#12289;&#22788;&#29702;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#21644;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#20107;&#23454;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2212.10923</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24402;&#32435;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Inductive Reasoners
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2212.10923
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24418;&#24335;&#35821;&#35328;&#22312;&#22788;&#29702;&#21407;&#22987;&#36755;&#20837;&#12289;&#22788;&#29702;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#21644;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#20107;&#23454;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;&#30740;&#31350;&#20013;&#65292;&#24418;&#24335;&#35821;&#35328;&#34987;&#29992;&#20316;&#30693;&#35782;&#65288;&#20107;&#23454;&#21644;&#35268;&#21017;&#65289;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24418;&#24335;&#35821;&#35328;&#20250;&#32473;&#24402;&#32435;&#25512;&#29702;&#24102;&#26469;&#31995;&#32479;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#26080;&#27861;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#36825;&#26679;&#30340;&#21407;&#22987;&#36755;&#20837;&#12289;&#23545;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#25935;&#24863;&#20197;&#21450;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#33539;&#24335;&#65288;&#20219;&#21153;&#65289;&#65292;&#21363;&#20174;&#33258;&#28982;&#35821;&#35328;&#20107;&#23454;&#20013;&#24402;&#32435;&#20986;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;DEER&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;1.2k&#20010;&#35268;&#21017;-&#20107;&#23454;&#23545;&#65292;&#35268;&#21017;&#21644;&#20107;&#23454;&#20197;&#33258;&#28982;&#35821;&#35328;&#20070;&#20889;&#12290;&#36824;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;&#35780;&#20272;&#27492;&#20219;&#21153;&#30340;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;DEER&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#30340;&#34920;&#31034;&#32780;&#19981;&#26159;&#24418;&#24335;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language model
&lt;/p&gt;</description></item><item><title>HASSOD&#26159;&#19968;&#31181;&#20998;&#23618;&#33258;&#36866;&#24212;&#26080;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03311</link><description>&lt;p&gt;
HASSOD&#65306;&#20998;&#23618;&#33258;&#36866;&#24212;&#26080;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
HASSOD: Hierarchical Adaptive Self-Supervised Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03311
&lt;/p&gt;
&lt;p&gt;
HASSOD&#26159;&#19968;&#31181;&#20998;&#23618;&#33258;&#36866;&#24212;&#26080;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#31995;&#32479;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#26102;&#27809;&#26377;&#26126;&#30830;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20197;&#21450;&#29702;&#35299;&#29289;&#20307;&#30340;&#37096;&#20998;&#25972;&#20307;&#32452;&#25104;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#21463;&#21040;&#36825;&#20004;&#20010;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#33258;&#36866;&#24212;&#26080;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;(HASSOD)&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#29289;&#20307;&#24182;&#29702;&#35299;&#20854;&#32452;&#25104;&#12290;HASSOD&#37319;&#29992;&#20998;&#23618;&#33258;&#36866;&#24212;&#32858;&#31867;&#31574;&#30053;&#65292;&#26681;&#25454;&#33258;&#30417;&#30563;&#30340;&#35270;&#35273;&#34920;&#31034;&#23558;&#21306;&#22495;&#20998;&#32452;&#20026;&#23545;&#35937;&#25513;&#30721;&#65292;&#33258;&#36866;&#24212;&#30830;&#23450;&#27599;&#20010;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;HASSOD&#36890;&#36807;&#20998;&#26512;&#25513;&#30721;&#20043;&#38388;&#30340;&#35206;&#30422;&#20851;&#31995;&#21644;&#26500;&#24314;&#26641;&#32467;&#26500;&#26469;&#35782;&#21035;&#23545;&#35937;&#30340;&#23618;&#27425;&#32423;&#21035;&#12290;&#36825;&#31181;&#39069;&#22806;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#24182;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25918;&#24323;&#20102;&#20043;&#21069;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#20302;&#25928;&#30340;&#22810;&#36718;&#33258;&#25105;&#35757;&#32451;&#36807;&#31243;&#65292;&#32780;&#37319;&#21462;&#20102;&#19968;&#31181;&#33258;&#25105;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#19978;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead ad
&lt;/p&gt;</description></item><item><title>V-IRL&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#65292;&#26088;&#22312;&#23558;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#24320;&#21457;&#20986;&#20855;&#26377;&#20016;&#23500;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#30495;&#23454;&#25968;&#25454;&#20114;&#21160;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.03310</link><description>&lt;p&gt;
V-IRL: &#23558;&#34394;&#25311;&#26234;&#33021;&#19982;&#29616;&#23454;&#29983;&#27963;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
V-IRL: Grounding Virtual Intelligence in Real Life
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03310
&lt;/p&gt;
&lt;p&gt;
V-IRL&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#65292;&#26088;&#22312;&#23558;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#24320;&#21457;&#20986;&#20855;&#26377;&#20016;&#23500;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#30495;&#23454;&#25968;&#25454;&#20114;&#21160;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29983;&#27963;&#22312;&#22320;&#29699;&#19978;&#65292;&#32780;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25152;&#21019;&#36896;&#30340;&#25968;&#23383;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#30528;&#24863;&#23448;&#24046;&#36317;&#12290;&#20026;&#20102;&#24320;&#21457;&#20986;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#28789;&#27963;&#24863;&#30693;&#12289;&#24605;&#32771;&#21644;&#34892;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24517;&#39035;&#24357;&#21512;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#36924;&#30495;&#24046;&#36317;&#12290;&#25105;&#20204;&#22914;&#20309;&#22312;&#19968;&#20010;&#20687;&#25105;&#20204;&#25152;&#23621;&#20303;&#30340;&#19990;&#30028;&#20013;&#19968;&#26679;&#20016;&#23500;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#20307;&#29616;&#20195;&#29702;&#65292;&#32780;&#19981;&#21463;&#30495;&#23454;&#30828;&#20214;&#21644;&#25511;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;V-IRL: &#19968;&#31181;&#24179;&#21488;&#65292;&#21487;&#20197;&#20351;&#20195;&#29702;&#22312;&#34394;&#25311;&#32780;&#36924;&#30495;&#30340;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#26082;&#26159;&#19968;&#20010;&#24320;&#21457;&#20195;&#29702;&#23436;&#25104;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#30340;&#28216;&#20048;&#22330;&#65292;&#21448;&#26159;&#19968;&#20010;&#24191;&#38420;&#30340;&#27979;&#35797;&#22522;&#22320;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#20840;&#29699;&#30495;&#23454;&#25968;&#25454;&#30340;&#20114;&#21160;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#26465;&#20214;DDPMs&#23398;&#20064;&#29983;&#25104;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#30340;&#36807;&#31243;&#65292;&#22312;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#20851;&#38190;&#65292;&#20135;&#29983;&#20102;&#19982;&#19981;&#21516;&#38454;&#27573;&#23545;&#24212;&#30340; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.03305</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#35821;&#20041;&#26377;&#24847;&#20041;&#21644;&#39640;&#25928;&#30340;&#34920;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#26465;&#20214;DDPMs&#23398;&#20064;&#29983;&#25104;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#30340;&#36807;&#31243;&#65292;&#22312;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#20851;&#38190;&#65292;&#20135;&#29983;&#20102;&#19982;&#19981;&#21516;&#38454;&#27573;&#23545;&#24212;&#30340; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20197;&#19981;&#23547;&#24120;&#30340;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#65292;&#20363;&#22914;&#23431;&#33322;&#21592;&#39569;&#22312;&#26376;&#29699;&#19978;&#30340;&#39532;&#65292;&#24182;&#19988;&#26377;&#27491;&#30830;&#30340;&#38452;&#24433;&#12290;&#36825;&#20123;&#36755;&#20986;&#34920;&#26126;&#20102;&#27169;&#22411;&#20855;&#26377;&#32452;&#21512;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#27169;&#22411;&#26159;&#22914;&#20309;&#20570;&#21040;&#36825;&#19968;&#28857;&#30340;&#21602;&#65311;&#25105;&#20204;&#22312;&#26465;&#20214;DDPMs&#19978;&#36827;&#34892;&#20102;&#25511;&#21046;&#23454;&#39564;&#65292;&#23398;&#20064;&#29983;&#25104;&#20197;&#25351;&#23450;&#30340;$x$&#21644;$y$&#20301;&#32622;&#20026;&#20013;&#24515;&#30340;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20135;&#29983;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#34920;&#31034;&#23545;&#20110;&#23454;&#29616;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32463;&#21382;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#28508;&#22312;&#34920;&#31034;&#38454;&#27573;&#65306;(A&#38454;&#27573;)&#27809;&#26377;&#28508;&#22312;&#32467;&#26500;&#65292;(B&#38454;&#27573;)&#19968;&#20010;&#28151;&#20081;&#29366;&#24577;&#30340;2D&#27969;&#24418;&#65292;&#20197;&#21450;(C&#38454;&#27573;)&#19968;&#20010;&#26377;&#24207;&#30340;2D&#27969;&#24418;&#12290;&#23545;&#24212;&#20110;&#36825;&#20123;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#65306;1&#65289;&#29983;&#25104;&#22810;&#20010;&#20984;&#36215;&#65292;2&#65289;&#29983;&#25104;&#19968;&#20010;&#20984;&#36215;&#65292;&#20294;$x$&#21644;$y$&#20301;&#32622;&#19981;&#20934;&#30830;&#65292;3&#65289;&#29983;&#25104;&#19968;&#20010;&#20984;&#36215;&#19988;&#20301;&#32622;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is genera
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35206;&#30422;&#21644;&#35843;&#33410;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#35206;&#30422;&#20869;&#37096;&#21644;&#19978;&#19979;&#25991;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#32499;&#32034;&#25193;&#23637;&#26102;&#38656;&#35201;&#20445;&#25345;&#32531;&#20914;&#21306;&#26469;&#20445;&#25345;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03303</link><description>&lt;p&gt;
&#27809;&#20851;&#31995;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25351;&#20196;&#35206;&#30422;&#21644;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Nevermind: Instruction Override and Moderation in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03303
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35206;&#30422;&#21644;&#35843;&#33410;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#35206;&#30422;&#20869;&#37096;&#21644;&#19978;&#19979;&#25991;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#32499;&#32034;&#25193;&#23637;&#26102;&#38656;&#35201;&#20445;&#25345;&#32531;&#20914;&#21306;&#26469;&#20445;&#25345;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;&#26368;&#27969;&#34892;&#30340;&#19987;&#26377;&#27169;&#22411;&#21644;&#19981;&#21516;&#22823;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35299;&#20915;&#22312;&#20914;&#31361;&#24773;&#20917;&#19979;&#30340;&#26126;&#30830;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#65292;&#20363;&#22914;&#35206;&#30422;&#12290;&#36825;&#20123;&#21253;&#25324;&#27169;&#22411;&#22312;&#20854;&#26435;&#37325;&#20013;&#35206;&#30422;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#35206;&#30422;&#65288;&#25110;&#35843;&#33410;&#65289;&#25552;&#31034;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#36827;&#34892;&#23436;&#20840;&#36234;&#29425;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21487;&#20197;&#25913;&#36827;&#25351;&#20196;&#36981;&#24490;&#30340;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616; - &#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#36981;&#24490;&#35206;&#30422;&#20869;&#37096;&#21644;&#19978;&#19979;&#25991;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#38750;&#24120;&#26381;&#20174;&#65292;&#29978;&#33267;&#26377;&#20123;&#36807;&#24230;&#12290;&#24403;&#36890;&#36807;&#32499;&#32034;&#25193;&#23637;&#26469;&#25193;&#23637;&#21040;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#26102;&#65292;&#38656;&#35201;&#20445;&#25345;&#19982;&#22256;&#24785;&#36793;&#32536;&#30340;&#26174;&#33879;&#32531;&#20914;&#21306;&#65292;&#20197;&#20445;&#25345;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25351;&#20196;&#36981;&#24490;&#30340;&#25913;&#21892;&#65292;&#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#30340;&#25351;&#20196;&#35206;&#30422;/&#36234;&#29425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/ja
&lt;/p&gt;</description></item><item><title>DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03300</link><description>&lt;p&gt;
DeepSeekMath: &#23558;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25512;&#21521;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03300
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#30340;&#29305;&#24615;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSeekMath 7B&#65292;&#23427;&#22312;Common Crawl&#20013;&#33719;&#21462;&#20102;120B&#20010;&#19982;&#25968;&#23398;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#20195;&#30721;&#25968;&#25454;&#26469;&#32487;&#32493;&#39044;&#35757;&#32451;DeepSeek-Coder-Base-v1.5 7B&#12290;DeepSeekMath 7B&#22312;&#31454;&#36187;&#32423;&#21035;&#30340;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;51.7%&#30340;&#20998;&#25968;&#65292;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#24037;&#20855;&#21253;&#21644;&#25237;&#31080;&#25216;&#26415;&#65292;&#25509;&#36817;&#20102;Gemini-Ultra&#21644;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;DeepSeekMath 7B&#30340;&#33258;&#19968;&#33268;&#24615;&#22312;MATH&#19978;&#30340;64&#20010;&#26679;&#26412;&#20013;&#36798;&#21040;&#20102;60.9%&#30340;&#20998;&#25968;&#12290;DeepSeekMath&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#24402;&#22240;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#36873;&#25321;&#31649;&#36947;&#20805;&#20998;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;GRPO&#65289;&#65292;&#36825;&#26159;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#21487;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
&lt;/p&gt;</description></item><item><title>Ginger&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#26354;&#29575;&#36817;&#20284;&#26041;&#27861;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#23427;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#26469;&#36870;&#21521;&#35745;&#31639;&#24191;&#20041;&#39640;&#26031;&#29275;&#39039;&#30697;&#38453;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#39640;&#20869;&#23384;&#21644;&#39640;&#26102;&#38388;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03295</link><description>&lt;p&gt;
Ginger: &#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#39640;&#25928;&#26354;&#29575;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03295
&lt;/p&gt;
&lt;p&gt;
Ginger&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#26354;&#29575;&#36817;&#20284;&#26041;&#27861;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#23427;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#26469;&#36870;&#21521;&#35745;&#31639;&#24191;&#20041;&#39640;&#26031;&#29275;&#39039;&#30697;&#38453;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#39640;&#20869;&#23384;&#21644;&#39640;&#26102;&#38388;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#24191;&#20041;&#39640;&#26031;&#29275;&#39039;&#27861;&#65292;&#30001;&#20110;&#21033;&#29992;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#26354;&#29575;&#20449;&#24687;&#21644;&#39044;&#22788;&#29702;&#30697;&#38453;&#65292;&#34987;&#35748;&#20026;&#26356;&#21152;&#24378;&#22823;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#35825;&#20154;&#30340;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#19981;&#26131;&#24212;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#35745;&#31639;&#30697;&#38453;&#30340;&#36870;&#25152;&#38656;&#30340;&#20108;&#27425;&#20869;&#23384;&#21644;&#19977;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#20351;&#29992;&#20808;&#36827;&#30340;&#30828;&#20214;&#20063;&#19981;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ginger&#65292;&#19968;&#31181;&#29992;&#20110;&#24191;&#20041;&#39640;&#26031;&#29275;&#39039;&#30697;&#38453;&#36870;&#30340;&#29305;&#24449;&#20998;&#35299;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20855;&#26377;&#39640;&#25928;&#30340;&#32447;&#24615;&#20869;&#23384;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30452;&#25509;&#32500;&#25252;&#26465;&#20214;&#30697;&#38453;&#30340;&#36870;&#65292;&#20197;&#20351;&#36817;&#20284;&#26356;&#21152;&#20934;&#30830;&#65292;&#32780;&#19981;&#26159;&#36817;&#20284;&#26465;&#20214;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;Ginger&#22312;&#38750;&#20984;&#30446;&#26631;&#19978;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices. Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning. The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix. These requirements are infeasible even with state-of-the-art hardware. In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our method enjoys efficient linear memory and time complexity for each iteration. Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate. We provide the convergence result of Ginger for non-convex objectives. Our experiments on different tasks with different model architectures verify the effectiveness of our method. Our code i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#26041;&#27861;Flora&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03293</link><description>&lt;p&gt;
Flora: &#20302;&#31209;&#36866;&#37197;&#22120;&#26159;&#24708;&#24708;&#30340;&#26799;&#24230;&#21387;&#32553;&#22120;
&lt;/p&gt;
&lt;p&gt;
Flora: Low-Rank Adapters Are Secretly Gradient Compressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#26041;&#27861;Flora&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20102;&#23436;&#25104;&#19981;&#21516;&#20219;&#21153;&#30340;&#26174;&#30528;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#20351;&#29992;&#26469;&#23384;&#20648;&#35757;&#32451;&#30340;&#20248;&#21270;&#29366;&#24577;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20302;&#31209;&#36866;&#37197;&#65288;LoRA&#65289;&#26469;&#36890;&#36807;&#35757;&#32451;&#26356;&#23569;&#30340;&#21442;&#25968;&#26469;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;LoRA&#23558;&#25972;&#20307;&#26435;&#37325;&#26356;&#26032;&#30697;&#38453;&#38480;&#21046;&#20026;&#20302;&#31209;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#30830;&#23450;&#23427;&#21487;&#20197;&#36817;&#20284;&#20026;&#38543;&#26426;&#25237;&#24433;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Flora&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20139;&#21463;&#20248;&#21270;&#29366;&#24577;&#30340;&#27425;&#32447;&#24615;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#26550;&#26500;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>InstanceDiffusion&#36890;&#36807;&#28155;&#21152;&#23454;&#20363;&#32423;&#25511;&#21046;&#65292;&#20351;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20301;&#32622;&#26465;&#20214;&#19979;&#36229;&#36807;&#20102;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03290</link><description>&lt;p&gt;
InstanceDiffusion&#65306;&#22270;&#20687;&#29983;&#25104;&#30340;&#23454;&#20363;&#32423;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
InstanceDiffusion: Instance-level Control for Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03290
&lt;/p&gt;
&lt;p&gt;
InstanceDiffusion&#36890;&#36807;&#28155;&#21152;&#23454;&#20363;&#32423;&#25511;&#21046;&#65292;&#20351;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20301;&#32622;&#26465;&#20214;&#19979;&#36229;&#36807;&#20102;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#20294;&#19981;&#33021;&#23545;&#22270;&#20687;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#31934;&#30830;&#25511;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;InstanceDiffusion&#65292;&#23558;&#31934;&#30830;&#30340;&#23454;&#20363;&#32423;&#25511;&#21046;&#28155;&#21152;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#12290;InstanceDiffusion&#25903;&#25345;&#27599;&#20010;&#23454;&#20363;&#30340;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#26465;&#20214;&#65292;&#24182;&#20801;&#35768;&#20197;&#31616;&#21333;&#30340;&#21333;&#20010;&#28857;&#12289;&#28034;&#40486;&#12289;&#36793;&#30028;&#26694;&#25110;&#22797;&#26434;&#30340;&#23454;&#20363;&#20998;&#21106;&#25513;&#30721;&#21450;&#20854;&#32452;&#21512;&#26041;&#24335;&#25351;&#23450;&#23454;&#20363;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#30340;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#23454;&#20363;&#32423;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;UniFusion&#22359;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23454;&#20363;&#32423;&#26465;&#20214;&#65292;ScaleU&#22359;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#65292;Multi-instance&#37319;&#26679;&#22120;&#25552;&#39640;&#20102;&#22810;&#23454;&#20363;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;InstanceDiffusion&#22312;&#27599;&#20010;&#20301;&#32622;&#26465;&#20214;&#19979;&#26126;&#26174;&#36229;&#36807;&#20102;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;COCO&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;box&#36755;&#20837;&#26041;&#38754;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;20.4%AP50 box&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#36827;&#34892;&#21069;&#30651;&#30340;&#33258;&#21160;&#21464;&#25442;&#22120;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#35299;&#20915;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RTL&#20195;&#30721;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#32534;&#35793;&#22833;&#36133;&#21644;PPA&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.03289</link><description>&lt;p&gt;
&#35753;&#27599;&#19968;&#27493;&#37117;&#26377;&#20215;&#20540;&#65306;&#20351;&#29992;MCTS&#30340;LLM&#22522;&#30784;&#39640;&#36136;&#37327;RTL&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#36827;&#34892;&#21069;&#30651;&#30340;&#33258;&#21160;&#21464;&#25442;&#22120;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#35299;&#20915;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RTL&#20195;&#30721;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#32534;&#35793;&#22833;&#36133;&#21644;PPA&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#38754;&#20020;&#32534;&#35793;&#22833;&#36133;&#21644;&#20122;&#26368;&#20248;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;(PPA)&#25928;&#29575;&#31561;&#25361;&#25112;&#12290;&#36825;&#26159;&#30001;&#20110;&#20256;&#32479;&#21464;&#25442;&#22120;&#35299;&#30721;&#31639;&#27861;&#32570;&#20047;&#23545;PPA&#30340;&#24847;&#35782;&#25152;&#33268;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21464;&#25442;&#22120;&#35299;&#30721;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#26469;&#36827;&#34892;&#21069;&#30651;&#65292;&#24341;&#23548;&#21464;&#25442;&#22120;&#29983;&#25104;&#21487;&#32534;&#35793;&#30340;&#12289;&#21151;&#33021;&#27491;&#30830;&#30340;&#12289;PPA&#20248;&#21270;&#30340;&#20195;&#30721;&#12290;&#22312;RTL&#20195;&#30721;&#38598;&#19978;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#20165;&#20351;&#29992;&#25552;&#31034;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#19968;&#33268;&#22320;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#26420;&#32032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;PPA&#19981;&#25935;&#24863;&#30340;&#32570;&#28857;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;LLM&#29983;&#25104;&#30340;&#26368;&#22823;&#35774;&#35745;&#65288;16&#20301;&#21152;&#27861;&#22120;&#65289;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#22312;&#38754;&#31215;&#24310;&#36831;&#20056;&#31215;&#19978;&#23454;&#29616;31.8%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional transformer decoding algorithms. In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code. Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models. For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03286</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#19968;&#33268;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Training-Free Consistent Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#36896;&#24615;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#19979;&#19968;&#33268;&#22320;&#25551;&#32472;&#30456;&#21516;&#30340;&#20027;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26469;&#25945;&#25480;&#23427;&#25551;&#36848;&#29305;&#23450;&#29992;&#25143;&#25552;&#20379;&#20027;&#39064;&#30340;&#26032;&#35789;&#27719;&#25110;&#32773;&#20026;&#27169;&#22411;&#28155;&#21152;&#22270;&#20687;&#26465;&#20214;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#38024;&#23545;&#27599;&#20010;&#20027;&#39064;&#36827;&#34892;&#28459;&#38271;&#30340;&#20248;&#21270;&#25110;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#21644;&#25551;&#32472;&#22810;&#20010;&#20027;&#39064;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#35757;&#32451;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#26469;&#23454;&#29616;&#19968;&#33268;&#30340;&#20027;&#39064;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20027;&#39064;&#39537;&#21160;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20197;&#20419;&#36827;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31574;&#30053;&#20197;&#40723;&#21169;&#24067;&#23616;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#34920;&#31034;&#23545;&#35805;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#27169;&#22411;&#26657;&#20934;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#20351;&#36739;&#23567;&#30340;&#27169;&#22411;&#20855;&#22791;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03284</link><description>&lt;p&gt;
&#20132;&#26131;&#65292;&#36824;&#26159;&#19981;&#20132;&#26131;&#65288;&#25110;&#32773;&#35841;&#30693;&#36947;&#65289;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#23545;&#35805;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#34920;&#31034;&#23545;&#35805;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#27169;&#22411;&#26657;&#20934;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#20351;&#36739;&#23567;&#30340;&#27169;&#22411;&#20855;&#22791;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#23545;&#35805;&#32773;&#32771;&#34385;&#20182;&#20154;&#30340;&#19981;&#30830;&#23450;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#24773;&#32490;&#12290;&#20294;&#21363;&#20351;&#26159;&#26368;&#20339;&#30340;&#20154;&#31867;&#23545;&#35805;&#32773;&#20063;&#26080;&#27861;&#23436;&#32654;&#22320;&#39044;&#27979;&#23545;&#35805;&#30340;&#36712;&#36857;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22810;&#22909;&#22320;&#34920;&#31034;&#23545;&#35805;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;FortUne Dial&#65292;&#36825;&#26159;&#8220;&#23545;&#35805;&#39044;&#27979;&#8221;&#20219;&#21153;&#30340;&#25193;&#23637;&#65306;&#35780;&#20272;&#19981;&#20165;&#20165;&#20197;&#20934;&#30830;&#24230;&#20026;&#26631;&#20934;&#65292;&#36824;&#37319;&#29992;&#20102;&#23545;&#19981;&#30830;&#23450;&#24615;&#25935;&#24863;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#20351;&#20010;&#21035;&#23454;&#20363;&#21487;&#20197;&#25918;&#24323;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#34920;&#31034;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#20004;&#31181;&#26041;&#24335;&#65288;&#20869;&#37096;&#20351;&#29992;&#20998;&#25968;&#21644;&#30452;&#25509;&#20351;&#29992;&#20196;&#29260;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#20004;&#31181;&#34920;&#31034;&#30340;&#26657;&#20934;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#23545;&#20843;&#20010;&#22256;&#38590;&#30340;&#35848;&#21028;&#35821;&#26009;&#24211;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#24494;&#35843;&#31574;&#30053;&#65288;&#19968;&#31181;&#20256;&#32479;&#30340;&#30417;&#30563;&#31574;&#30053;&#21644;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65289;&#21487;&#20197;&#20351;&#36739;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#26657;&#20934;&#24471;&#19978;&#19982;&#20854;&#23610;&#23544;&#30456;&#24403;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing "conversation forecasting" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their si
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.03282</link><description>&lt;p&gt;
&#19968;&#20010;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#22312;RLHF&#20013;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Partially Observed Reward-States in RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03282
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#30740;&#31350;&#22240;&#20854;&#22312;LLMs&#30340;&#21457;&#23637;&#20013;&#36215;&#21040;&#30340;&#20316;&#29992;&#32780;&#21464;&#24471;&#37325;&#35201;&#12290;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#23545;&#21050;&#28608;&#30340;&#21453;&#24212;&#24050;&#30693;&#20381;&#36182;&#20110;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#8220;&#20869;&#37096;&#29366;&#24577;&#8221;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#20013;&#38388;&#21453;&#39304;&#65292;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#23558;RLHF&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PORRL&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;RLHF&#20013;&#20004;&#31181;&#20027;&#35201;&#24418;&#24335;&#30340;&#20154;&#31867;&#21453;&#39304; - &#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#21040;PORRL&#30340;&#32553;&#20943;&#12290;&#23545;&#20110;&#22522;&#25968;&#21453;&#39304;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36890;&#29992;&#30340;&#32479;&#35745;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#23454;&#20363;&#21270;&#20026;POR-UCRL&#21644;POR-UCBVI&#12290;&#23545;&#20110;&#20915;&#26007;&#21453;&#39304;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#25968;&#21453;&#39304;&#32553;&#20943;&#19981;&#33021;&#36798;&#21040;&#20122;&#32447;&#24615;&#30340;&#20915;&#26007;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.03271</link><description>&lt;p&gt;
&#24819;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#25628;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03271
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27604;&#22914;&#21307;&#23398;&#35786;&#26029;&#21644;&#25925;&#38556;&#25490;&#38500;&#65292;&#35299;&#20915;&#20219;&#21153;&#25152;&#38656;&#30340;&#20449;&#24687;&#19981;&#26159;&#21021;&#22987;&#32473;&#23450;&#30340;&#65292;&#32780;&#38656;&#35201;&#36890;&#36807;&#35810;&#38382;&#21518;&#32493;&#38382;&#39064;&#26469;&#20027;&#21160;&#23547;&#27714;&#65288;&#20363;&#22914;&#65292;&#21307;&#29983;&#21521;&#24739;&#32773;&#35810;&#38382;&#30151;&#29366;&#30340;&#26356;&#22810;&#32454;&#33410;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24605;&#24819;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;UoT&#65289;&#65292;&#19968;&#31181;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#20027;&#21160;&#25552;&#38382;&#20449;&#24687;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;UoT&#32467;&#21512;&#20102;1&#65289;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20223;&#30495;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#21487;&#33021;&#30340;&#26410;&#26469;&#22330;&#26223;&#65292;&#24182;&#20272;&#35745;&#20854;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;2&#65289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#65292;&#28608;&#21169;&#27169;&#22411;&#23547;&#27714;&#20449;&#24687;&#65307;3&#65289;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#30340;&#26041;&#24335;&#36873;&#25321;&#26368;&#20339;&#30340;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#25925;&#38556;&#25490;&#38500;&#21644;'20&#30340;&#23454;&#39564;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03268</link><description>&lt;p&gt;
&#20174;&#25512;&#29702;&#36335;&#24452;&#32858;&#21512;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#35757;&#32451;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#30340;&#20851;&#31995;&#22914;&#20309;&#20419;&#20351;&#25512;&#29702;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#21487;&#20197;&#23558;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#22312;&#39044;&#35757;&#32451;&#26102;&#36890;&#36807;&#32858;&#21512;&#38388;&#25509;&#30340;&#25512;&#29702;&#36335;&#24452;&#26469;&#24471;&#20986;&#26032;&#32467;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20010;&#35270;&#35282;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#31561;&#20851;&#38190;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#36335;&#24452;&#24418;&#24335;&#21270;&#20026;&#22312;&#30693;&#35782;/&#25512;&#29702;&#22270;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#12290;&#23545;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#24067;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30456;&#20851;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#27010;&#29575;&#30340;&#21152;&#26435;&#21644;&#26159;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21512;&#29702;&#26041;&#24335;&#12290;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#35757;&#32451;&#23545;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;CLIP&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21453;&#21367;&#31215;&#35299;&#30721;&#22120;&#21644;&#21487;&#23398;&#20064;&#23884;&#20837;&#30697;&#38453;&#65292;&#20351;&#24471;CLIP&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#28145;&#24230;&#20272;&#35745;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03251</link><description>&lt;p&gt;
CLIP&#21487;&#20197;&#29702;&#35299;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
CLIP Can Understand Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;CLIP&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21453;&#21367;&#31215;&#35299;&#30721;&#22120;&#21644;&#21487;&#23398;&#20064;&#23884;&#20837;&#30697;&#38453;&#65292;&#20351;&#24471;CLIP&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#28145;&#24230;&#20272;&#35745;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#23558;CLIP&#25512;&#24191;&#21040;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32593;&#32476;&#29228;&#21462;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;CLIP&#22312;&#22270;&#20687;&#22359;&#21644;&#19982;&#28145;&#24230;&#30456;&#20851;&#30340;&#25552;&#31034;&#20043;&#38388;&#24471;&#21040;&#36866;&#24403;&#30456;&#20284;&#24615;&#26159;&#20302;&#25928;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36866;&#24212;CLIP&#29992;&#20110;&#26377;&#24847;&#20041;&#30340;&#23494;&#38598;&#39044;&#27979;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#20854;&#21407;&#22987;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#12290;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#19968;&#20010;&#32039;&#20945;&#30340;&#21453;&#21367;&#31215;&#35299;&#30721;&#22120;&#21644;&#19968;&#20010;&#21517;&#20026;mirror&#30340;&#23567;&#22411;&#21487;&#23398;&#20064;&#23884;&#20837;&#30697;&#38453;&#20316;&#20026;&#20854;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#38745;&#24577;&#25552;&#31034;&#65292;CLIP&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;NYU Depth v2&#21644;KITTI&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#19982;&#20960;&#20010;&#20808;&#21069;&#30340;&#20165;&#35270;&#35273;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#32780;&#19988;&#32988;&#36807;&#20102;&#27599;&#20010;&#22522;&#20110;CLIP&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;&#20851;&#20110;&#26102;&#38388;&#28145;&#24230;&#19968;&#33268;&#24615;&#21644;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;CLIP&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26102;&#28382;&#30740;&#31350;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on 
&lt;/p&gt;</description></item><item><title>HEANA&#26159;&#19968;&#31181;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#20056;&#27861;&#22120;(TAOMs)&#25552;&#39640;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27969;&#30340;&#28789;&#27963;&#24615;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#20809;&#23398;&#21152;&#36895;&#22120;&#30340;&#27874;&#38271;&#24182;&#34892;&#24615;&#21463;&#21040;&#20018;&#25200;&#12289;&#19981;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#27969;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#20809;&#30005;&#25506;&#27979;&#22120;&#36827;&#34892;&#21407;&#20301;&#32047;&#31215;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03247</link><description>&lt;p&gt;
HEANA: &#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#25968;&#25454;&#27969;&#30340;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#33021;&#37327;&#39640;&#25928;&#30340;CNN&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03247
&lt;/p&gt;
&lt;p&gt;
HEANA&#26159;&#19968;&#31181;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#20056;&#27861;&#22120;(TAOMs)&#25552;&#39640;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27969;&#30340;&#28789;&#27963;&#24615;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#20809;&#23398;&#21152;&#36895;&#22120;&#30340;&#27874;&#38271;&#24182;&#34892;&#24615;&#21463;&#21040;&#20018;&#25200;&#12289;&#19981;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#27969;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#20809;&#30005;&#25506;&#27979;&#22120;&#36827;&#34892;&#21407;&#20301;&#32047;&#31215;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HEANA&#30340;&#26032;&#22411;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#21152;&#36895;&#25972;&#25968;&#37327;&#21270;CNN&#30340;&#25512;&#29702;&#12290;HEANA&#37319;&#29992;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#20056;&#27861;&#22120;(TAOMs)&#65292;&#22686;&#24378;&#20102;HEANA&#23545;&#22810;&#31181;&#25968;&#25454;&#27969;&#30340;&#25903;&#25345;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#35889;&#26080;&#25439;&#30340;TAOMs&#25490;&#21015;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#29616;&#26377;&#20809;&#23398;&#21152;&#36895;&#22120;&#23384;&#22312;&#30340;&#27874;&#38271;&#24182;&#34892;&#24615;&#21463;&#21040;&#21508;&#31181;&#20018;&#25200;&#24433;&#21709;&#12289;&#26080;&#27861;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#27969;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#20809;&#30005;&#25506;&#27979;&#22120;&#36827;&#34892;&#21407;&#20301;&#32047;&#31215;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several photonic microring resonators (MRRs) based analog accelerators have been proposed to accelerate the inference of integer-quantized CNNs with remarkably higher throughput and energy efficiency compared to their electronic counterparts. However, the existing analog photonic accelerators suffer from three shortcomings: (i) severe hampering of wavelength parallelism due to various crosstalk effects, (ii) inflexibility of supporting various dataflows other than the weight-stationary dataflow, and (iii) failure in fully leveraging the ability of photodetectors to perform in-situ accumulations. These shortcomings collectively hamper the performance and energy efficiency of prior accelerators. To tackle these shortcomings, we present a novel Hybrid timE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybrid time-amplitude analog optical multipliers (TAOMs) that increase the flexibility of HEANA to support multiple dataflows. A spectrally hitless arrangement of TAOMs s
&lt;/p&gt;</description></item><item><title>SGS-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#36890;&#36947;&#20248;&#21270;&#21644;&#20851;&#38190;&#24103;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#21644;&#31934;&#30830;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2402.03246</link><description>&lt;p&gt;
SGS-SLAM&#65306;&#22522;&#20110;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM
&lt;/p&gt;
&lt;p&gt;
SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03246
&lt;/p&gt;
&lt;p&gt;
SGS-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#36890;&#36947;&#20248;&#21270;&#21644;&#20851;&#38190;&#24103;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#21644;&#31934;&#30830;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#29702;&#35299;&#22312;&#31264;&#23494;&#21516;&#26102;&#23450;&#20301;&#21644;&#24314;&#22270;&#65288;SLAM&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20840;&#38754;&#30340;&#22330;&#26223;&#35299;&#26512;&#12290;&#26368;&#36817;&#23558;&#39640;&#26031;&#28857;&#20113;&#38598;&#25104;&#21040;SLAM&#31995;&#32479;&#20013;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26174;&#24335;&#30340;&#19977;&#32500;&#39640;&#26031;&#34920;&#31034;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#28210;&#26579;&#25928;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SGS-SLAM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;&#35270;&#35273;SLAM&#31995;&#32479;&#65292;&#23427;&#19981;&#20165;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#65292;&#36824;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#37325;&#24314;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#24314;&#22270;&#36807;&#31243;&#20013;&#37319;&#29992;&#22810;&#36890;&#36947;&#20248;&#21270;&#65292;&#23558;&#22806;&#35266;&#12289;&#20960;&#20309;&#21644;&#35821;&#20041;&#32422;&#26463;&#19982;&#20851;&#38190;&#24103;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SGS-SLAM&#22312;&#30456;&#26426;&#20301;&#23039;&#20272;&#35745;&#12289;&#22320;&#22270;&#37325;&#24314;&#21644;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21516;&#26102;&#20445;&#25345;&#23454;&#26102;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaus- sian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rende
&lt;/p&gt;</description></item><item><title>IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03227</link><description>&lt;p&gt;
IGUANe: &#19968;&#31181;&#36866;&#29992;&#20110;&#33041;MR&#22270;&#20687;&#22810;&#20013;&#24515;&#21327;&#35843;&#30340;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03227
&lt;/p&gt;
&lt;p&gt;
IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MRI&#30740;&#31350;&#20013;&#65292;&#26469;&#33258;&#22810;&#20010;&#37319;&#38598;&#28857;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#32858;&#21512;&#21487;&#20197;&#22686;&#21152;&#26679;&#26412;&#22823;&#23567;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#38459;&#30861;&#21518;&#32493;&#20998;&#26512;&#19968;&#33268;&#24615;&#30340;&#19982;&#37319;&#38598;&#28857;&#30456;&#20851;&#30340;&#21464;&#24322;&#12290;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#21327;&#35843;MR&#22270;&#20687;&#36328;&#31449;&#28857;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IGUANe&#65288;&#20855;&#26377;&#32479;&#19968;&#23545;&#25239;&#32593;&#32476;&#30340;&#22270;&#20687;&#29983;&#25104;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21407;&#22987;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#22495;&#36716;&#25442;&#30340;&#20248;&#21183;&#21644;&#30452;&#25509;&#24212;&#29992;&#26679;&#24335;&#36716;&#31227;&#26041;&#27861;&#26469;&#23454;&#29616;&#22810;&#20013;&#24515;&#33041;MR&#22270;&#20687;&#21327;&#35843;&#12290;IGUANe&#36890;&#36807;&#22810;&#23545;&#19968;&#31574;&#30053;&#65292;&#38598;&#25104;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;CycleGAN&#26550;&#26500;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20687;&#65292;&#29978;&#33267;&#26469;&#33258;&#26410;&#30693;&#37319;&#38598;&#28857;&#65292;&#20351;&#20854;&#25104;&#20026;&#21327;&#35843;&#30340;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;&#22312;&#30001;11&#21488;&#19981;&#21516;&#25195;&#25551;&#20202;&#30340;T1&#21152;&#26435;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;IGUANe&#22312;&#26410;&#35265;&#31449;&#28857;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03214</link><description>&lt;p&gt;
&#26377;&#26426;&#25110;&#25193;&#25955;&#65306;&#25105;&#20204;&#33021;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22270;&#20687;&#30340;&#20986;&#29616;&#23436;&#20840;&#39072;&#35206;&#20102;&#33402;&#26415;&#30028;&#12290;&#20174;&#20154;&#31867;&#33402;&#26415;&#20013;&#35782;&#21035;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20854;&#24433;&#21709;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#22686;&#21152;&#12290;&#26410;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#32773;&#27450;&#35784;&#37027;&#20123;&#25903;&#20184;&#39640;&#20215;&#36141;&#20080;&#20154;&#31867;&#33402;&#26415;&#21697;&#30340;&#20010;&#20154;&#21644;&#31105;&#27490;&#20351;&#29992;AI&#22270;&#20687;&#30340;&#20844;&#21496;&#12290;&#36825;&#23545;&#20110;&#38656;&#35201;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#20197;&#36991;&#20813;&#28508;&#22312;&#27169;&#22411;&#23849;&#28291;&#30340;AI&#27169;&#22411;&#35757;&#32451;&#32773;&#26469;&#35828;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#22270;&#20687;&#30340;&#26041;&#27861;&#26377;&#22810;&#31181;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#65292;&#20197;&#21450;&#36890;&#36807;&#19987;&#19994;&#33402;&#26415;&#23478;&#21033;&#29992;&#20182;&#20204;&#23545;&#33402;&#26415;&#25216;&#24039;&#30340;&#30693;&#35782;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;7&#31181;&#39118;&#26684;&#30340;&#30495;&#23454;&#20154;&#31867;&#33402;&#26415;&#65292;&#20174;5&#20010;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20102;&#19982;&#20043;&#21305;&#37197;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20102;8&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;&#22823;&#35268;&#27169;MIMO&#22522;&#31449;&#30340;&#30561;&#30496;&#27169;&#24335;&#21644;&#22825;&#32447;&#20999;&#25442;&#65292;&#23454;&#29616;&#22312;&#22810;&#23567;&#21306;&#32593;&#32476;&#20013;&#33021;&#37327;&#30340;&#33410;&#30465;&#65292;&#21516;&#26102;&#20445;&#25345;&#26381;&#21153;&#36136;&#37327;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#30456;&#27604;&#22522;&#32447;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03204</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#23567;&#21306;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#33410;&#30465;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;&#22823;&#35268;&#27169;MIMO&#22522;&#31449;&#30340;&#30561;&#30496;&#27169;&#24335;&#21644;&#22825;&#32447;&#20999;&#25442;&#65292;&#23454;&#29616;&#22312;&#22810;&#23567;&#21306;&#32593;&#32476;&#20013;&#33021;&#37327;&#30340;&#33410;&#30465;&#65292;&#21516;&#26102;&#20445;&#25345;&#26381;&#21153;&#36136;&#37327;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#30456;&#27604;&#22522;&#32447;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#22810;&#32423;&#39640;&#32423;&#30561;&#30496;&#27169;&#24335;&#65288;ASM&#65289;&#21644;&#22522;&#31449;&#30340;&#22825;&#32447;&#20999;&#25442;&#36827;&#34892;&#20915;&#31574;&#65292;&#26469;&#26368;&#23567;&#21270;&#22810;&#20010;&#22823;&#35268;&#27169;MIMO&#65288;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65289;&#22522;&#31449;&#22312;&#22810;&#23567;&#21306;&#32593;&#32476;&#20013;&#30340;&#24635;&#33021;&#37327;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#12290;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;DEC-POMDP&#65289;&#65292;&#20197;&#23454;&#29616;&#20010;&#20307;&#22522;&#31449;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#23567;&#21306;&#38388;&#24178;&#25200;&#26159;&#24517;&#35201;&#30340;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;MAPPO&#65289;&#31639;&#27861;&#26469;&#23398;&#20064;&#21327;&#21516;&#22522;&#31449;&#25511;&#21046;&#31574;&#30053;&#12290;&#20026;&#20102;&#25552;&#39640;&#20854;&#21487;&#20280;&#32553;&#24615;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MAPPO-neighbor&#31574;&#30053;&#30340;&#20462;&#25913;&#29256;&#26412;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#30340;MAPPO&#26234;&#33021;&#20307;&#30456;&#27604;&#22522;&#32447;&#31574;&#30053;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#33258;&#21160;&#30561;&#30496;&#27169;&#24335;1&#65288;&#31526;&#21495;&#32423;&#20241;&#30496;&#65289;&#31639;&#27861;&#30456;&#27604;&#65292;MAPPO-neighbor&#31639;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference. A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy. To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed. Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies. Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#20803;&#23398;&#20064;&#26694;&#26550;SeMPL&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#23398;&#20064;&#21644;&#39044;&#27979;&#32473;&#23450;&#36719;&#20214;&#37197;&#32622;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;SeMPL&#36890;&#36807;&#20381;&#27425;&#39034;&#24207;&#35757;&#32451;&#20803;&#29615;&#22659;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#26032;&#29615;&#22659;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.03183</link><description>&lt;p&gt;
&#29992;&#39034;&#24207;&#20803;&#23398;&#20064;&#39044;&#27979;&#22810;&#20010;&#29615;&#22659;&#19979;&#30340;&#37197;&#32622;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#20803;&#23398;&#20064;&#26694;&#26550;SeMPL&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#23398;&#20064;&#21644;&#39044;&#27979;&#32473;&#23450;&#36719;&#20214;&#37197;&#32622;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;SeMPL&#36890;&#36807;&#20381;&#27425;&#39034;&#24207;&#35757;&#32451;&#20803;&#29615;&#22659;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#26032;&#29615;&#22659;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#39044;&#27979;&#32473;&#23450;&#36719;&#20214;&#37197;&#32622;&#30340;&#24615;&#33021;&#23545;&#35768;&#22810;&#36719;&#20214;&#24037;&#31243;&#27963;&#21160;&#38750;&#24120;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#22312;&#21333;&#20010;&#29615;&#22659;&#19979;&#26500;&#24314;&#24615;&#33021;&#27169;&#22411;&#25110;&#26410;&#33021;&#27491;&#30830;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#29615;&#22659;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#26032;&#29615;&#22659;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#22810;&#20010;&#29615;&#22659;&#19979;&#30340;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;SeMPL - &#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#19981;&#21516;(meta)&#29615;&#22659;&#20013;&#27979;&#37327;&#30340;&#37197;&#32622;&#20013;&#23398;&#20064;&#20849;&#21516;&#30340;&#29702;&#35299;&#65292;&#24182;&#23558;&#23427;&#20204;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#30446;&#26631;&#29615;&#22659;&#20013;&#12290;&#20854;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#19982;&#24120;&#35265;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65288;&#22914;MAML&#21644;MetaSGD&#65289;&#24182;&#34892;&#35757;&#32451;&#20803;&#29615;&#22659;&#19981;&#21516;&#65292;&#25105;&#20204;&#20381;&#27425;&#39034;&#24207;&#35757;&#32451;&#23427;&#20204;&#12290;&#35757;&#32451;&#39034;&#24207;&#33258;&#28982;&#22320;&#20801;&#35768;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Learning and predicting the performance of given software configurations are of high importance to many software engineering activities. While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments. In this paper, we target configuration performance learning under multiple environments. We do so by designing SeMPL - a meta-learning framework that learns the common understanding from configurations measured in distinct (meta) environments and generalizes them to the unforeseen, target environment. What makes it unique is that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train the meta environments in parallel, we train them sequentially, one at a time. The order of training naturally allows discriminating t
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#38134;&#34892;&#19994;&#32972;&#26223;&#19979;&#27604;&#36739;&#20102;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;KernelPCA&#21644;K-means Clustering&#22312;BERTopic&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36830;&#36143;&#30340;&#20027;&#39064;&#65292;&#36830;&#36143;&#24615;&#24471;&#20998;&#20026;0.8463&#12290;</title><link>https://arxiv.org/abs/2402.03176</link><description>&lt;p&gt;
&#38134;&#34892;&#19994;&#32972;&#26223;&#19979;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of Topic Modelling Approaches in the Banking Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#38134;&#34892;&#19994;&#32972;&#26223;&#19979;&#27604;&#36739;&#20102;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;KernelPCA&#21644;K-means Clustering&#22312;BERTopic&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36830;&#36143;&#30340;&#20027;&#39064;&#65292;&#36830;&#36143;&#24615;&#24471;&#20998;&#20026;0.8463&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#33258;&#21160;&#25552;&#21462;&#20027;&#39064;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#26381;&#21153;&#34892;&#19994;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#30417;&#25511;&#23458;&#25143;&#35752;&#35770;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;Latent Dirichlet Allocation&#65292;LDA&#65289;&#22312;&#20027;&#39064;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#26080;&#27861;&#23545;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#24314;&#27169;&#65292;&#23427;&#20204;&#30340;&#32467;&#26524;&#24182;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;BERTopic&#26550;&#26500;&#20013;&#20351;&#29992;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;Kernel Principal Component Analysis&#65292;KernelPCA&#65289;&#21644;K-means&#32858;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#23612;&#26085;&#21033;&#20122;&#38134;&#34892;&#23458;&#25143;&#30340;&#25512;&#25991;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#26469;&#27604;&#36739;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;BERTopic&#26550;&#26500;&#20013;&#20351;&#29992;KernelPCA&#21644;K-means&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#20027;&#39064;&#65292;&#20854;&#20013;&#36830;&#36143;&#24615;&#24471;&#20998;&#20026;0.8463&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03175</link><description>&lt;p&gt;
The Matrix: &#19968;&#20010;&#29992;&#20110;LLMs&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Matrix: A Bayesian learning model for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;LLM&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20197;&#27492;&#21407;&#21017;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#30001;&#20808;&#39564;&#21644;&#22810;&#39033;&#24335;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#34920;&#31034;&#30340;&#29702;&#24819;&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;LLMs&#22914;&#20309;&#36924;&#36817;&#35813;&#30697;&#38453;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23884;&#20837;&#21644;&#22810;&#39033;&#24335;&#20998;&#24067;&#20043;&#38388;&#30340;&#26144;&#23556;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;Dirichlet&#36924;&#36817;&#23450;&#29702;&#26469;&#36924;&#36817;&#20219;&#20309;&#20808;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#22914;&#20309;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#21407;&#29702;&#19968;&#33268;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#20855;&#20307;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#26356;&#22823;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35270;&#20026;&#38656;&#35201;&#26356;&#26032;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights
&lt;/p&gt;</description></item><item><title>Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03173</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#65306;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Multi: Multimodal Understanding Leaderboard with Text and Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03173
&lt;/p&gt;
&lt;p&gt;
Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#24378;&#35843;&#20102;&#21521;&#23398;&#26415;&#30028;&#24341;&#20837;&#20855;&#26377;&#25361;&#25112;&#24615;&#32780;&#21448;&#30495;&#23454;&#30340;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#30340;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#65292;&#20294;Multi&#25104;&#20026;&#20102;MLLM&#30340;&#23574;&#31471;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21453;&#26144;&#20102;&#24403;&#21069;&#30495;&#23454;&#30340;&#32771;&#35797;&#39118;&#26684;&#65292;&#25552;&#20379;&#22810;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#24182;&#35201;&#27714;&#20934;&#30830;&#25110;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#65292;&#31867;&#20284;&#20110;&#29616;&#23454;&#20013;&#30340;&#23398;&#26657;&#32771;&#35797;&#12290;&#23427;&#36890;&#36807;&#21508;&#31181;&#20219;&#21153;&#25361;&#25112;MLLM&#65292;&#20174;&#20844;&#24335;&#25512;&#23548;&#21040;&#22270;&#20687;&#32454;&#33410;&#20998;&#26512;&#65292;&#20197;&#21450;&#36328;&#27169;&#24577;&#25512;&#29702;&#12290;Multi&#21253;&#25324;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#26684;&#24335;&#30340;&#22522;&#20110;&#31185;&#23398;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Multi-Elite&#65292;&#19968;&#20010;&#21253;&#21547;500&#20010;&#38382;&#39064;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;MLLM&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#20197;&#21450;Multi-Extend&#65292;&#36890;&#36807;&#36229;&#36807;4..&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#33258;&#21160;&#21270;ICD&#32534;&#30721;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#26469;&#31934;&#30830;&#20998;&#37197;ICD&#20195;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ICD&#32534;&#30721;&#20013;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#26631;&#31614;&#23884;&#20837;&#26426;&#21046;&#23545;&#27169;&#22411;&#24615;&#33021;&#20063;&#26377;&#26174;&#33879;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.03172</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#22810;&#26679;&#26631;&#31614;&#23884;&#20837;&#36827;&#34892;&#27880;&#24847;&#21147;&#30340;&#31934;&#30830;&#21644;&#33391;&#22909;&#26657;&#20934;&#30340;ICD&#20195;&#30721;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#33258;&#21160;&#21270;ICD&#32534;&#30721;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#26469;&#31934;&#30830;&#20998;&#37197;ICD&#20195;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ICD&#32534;&#30721;&#20013;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#26631;&#31614;&#23884;&#20837;&#26426;&#21046;&#23545;&#27169;&#22411;&#24615;&#33021;&#20063;&#26377;&#26174;&#33879;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD&#65289;&#24050;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24471;&#21040;&#37319;&#29992;&#65292;&#20294;&#23558;ICD&#20195;&#30721;&#25163;&#21160;&#20998;&#37197;&#32473;&#20020;&#24202;&#25991;&#26412;&#32791;&#26102;&#12289;&#23481;&#26131;&#20986;&#38169;&#19988;&#26114;&#36149;&#65292;&#36825;&#20419;&#20351;&#20102;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;ICD&#32534;&#30721;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20808;&#21069;&#30456;&#20851;&#24037;&#20316;&#30340;&#20960;&#20010;&#24605;&#24819;&#12290;&#25105;&#20204;&#29305;&#21035;&#20351;&#29992;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24182;&#19988;&#20026;&#22788;&#29702;&#20887;&#38271;&#30340;&#20020;&#24202;&#21465;&#36848;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22522;&#30784;&#32534;&#30721;&#22120;&#27169;&#22411;&#25913;&#36896;&#25104;&#20026;Longformer&#65292;&#25110;&#32773;&#23558;&#25991;&#26412;&#20998;&#25104;&#22810;&#20010;&#22359;&#24182;&#29420;&#31435;&#22788;&#29702;&#27599;&#20010;&#22359;&#30340;&#26041;&#27861;&#12290;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#34920;&#31034;&#19982;&#19968;&#20010;&#26631;&#31614;&#23884;&#20837;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#35813;&#26426;&#21046;&#25506;&#32034;&#20102;&#22810;&#26679;&#30340;ICD&#20195;&#30721;&#36817;&#20041;&#35789;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#21010;&#20998;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;ICD&#32534;&#30721;&#20013;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#26631;&#31614;&#23884;&#20837;&#23545;&#33391;&#22909;&#24615;&#33021;&#30340;&#36129;&#29486;&#20063;&#26159;&#26174;&#33879;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;...
&lt;/p&gt;
&lt;p&gt;
Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches. This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work. We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently. The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms. Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance. Our approach also 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#22495;&#24773;&#22659;&#28436;&#31639;&#29702;&#35770;&#20013;&#20851;&#20110;&#26102;&#38388;&#30340;&#21487;&#21028;&#23450;&#25512;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#21463;&#38480;&#29366;&#24577;&#20844;&#29702;&#21644;&#27604;&#36739;&#25805;&#20316;&#31526;&#30340;&#38047;&#34920;&#20316;&#20026;&#23454;&#20540;fluents&#65292;&#35299;&#20915;&#20102;&#26816;&#26597;&#21487;&#36798;&#24773;&#22659;&#26159;&#21542;&#28385;&#36275;&#32473;&#23450;&#20844;&#24335;&#30340;&#38382;&#39064;&#30340;&#19981;&#21487;&#21028;&#23450;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#21487;&#21028;&#23450;&#30340;&#36807;&#31243;&#26469;&#30830;&#23450;&#34892;&#21160;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.03164</link><description>&lt;p&gt;
&#26377;&#38480;&#22495;&#24773;&#22659;&#28436;&#31639;&#29702;&#35770;&#20013;&#26377;&#20851;&#26102;&#38388;&#30340;&#21487;&#21028;&#23450;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Decidable Reasoning About Time in Finite-Domain Situation Calculus Theories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#22495;&#24773;&#22659;&#28436;&#31639;&#29702;&#35770;&#20013;&#20851;&#20110;&#26102;&#38388;&#30340;&#21487;&#21028;&#23450;&#25512;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#21463;&#38480;&#29366;&#24577;&#20844;&#29702;&#21644;&#27604;&#36739;&#25805;&#20316;&#31526;&#30340;&#38047;&#34920;&#20316;&#20026;&#23454;&#20540;fluents&#65292;&#35299;&#20915;&#20102;&#26816;&#26597;&#21487;&#36798;&#24773;&#22659;&#26159;&#21542;&#28385;&#36275;&#32473;&#23450;&#20844;&#24335;&#30340;&#38382;&#39064;&#30340;&#19981;&#21487;&#21028;&#23450;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#21487;&#21028;&#23450;&#30340;&#36807;&#31243;&#26469;&#30830;&#23450;&#34892;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30340;&#34920;&#36798;&#23545;&#20110;&#26234;&#33021;&#29289;&#29702;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#24773;&#22659;&#28436;&#31639;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#23454;&#20540;fluents $\mathit{time}(a)$ &#26469;&#34920;&#31034;&#26102;&#38388;&#65292;&#36825;&#20010;fluents&#23558;&#19968;&#20010;&#26102;&#38388;&#28857;&#19982;&#27599;&#20010;&#21160;&#20316;&#20197;&#21450;&#27599;&#20010;&#24773;&#22659;&#30456;&#36830;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#21363;&#20351;&#39046;&#22495;&#33539;&#22260;&#34987;&#38480;&#21046;&#22312;&#19968;&#20010;&#26377;&#38480;&#23545;&#35937;&#38598;&#19978;&#65292;&#26816;&#26597;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#21487;&#36798;&#24773;&#22659;&#28385;&#36275;&#32473;&#23450;&#20844;&#24335;&#30340;&#38382;&#39064;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#26102;&#33258;&#21160;&#26426;&#29702;&#35770;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38047;&#34920;&#20316;&#20026;&#20855;&#26377;&#21463;&#38480;&#30340;&#32487;&#20219;&#29366;&#24577;&#20844;&#29702;&#21644;&#27604;&#36739;&#25805;&#20316;&#31526;&#30340;&#23454;&#20540;fluents&#12290;&#36890;&#36807;&#36825;&#31181;&#38480;&#21046;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#26377;&#38480;&#22495;&#22522;&#26412;&#21160;&#20316;&#29702;&#35770;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#26159;&#21487;&#21028;&#23450;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#24212;&#29992;&#20110;Golog&#31243;&#24207;&#23454;&#29616;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#21487;&#21028;&#23450;&#30340;&#36807;&#31243;&#26469;&#30830;&#23450;&#19968;&#20010;&#34892;&#21160;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Representing time is crucial for cyber-physical systems and has been studied extensively in the Situation Calculus. The most commonly used approach represents time by adding a real-valued fluent $\mathit{time}(a)$ that attaches a time point to each action and consequently to each situation. We show that in this approach, checking whether there is a reachable situation that satisfies a given formula is undecidable, even if the domain of discourse is restricted to a finite set of objects. We present an alternative approach based on well-established results from timed automata theory by introducing clocks as real-valued fluents with restricted successor state axioms and comparison operators. %that only allow comparisons against fixed rationals. With this restriction, we can show that the reachability problem for finite-domain basic action theories is decidable. Finally, we apply our results on Golog program realization by presenting a decidable procedure for determining an action sequence
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auxiliary-Delayed Reinforcement Learning (AD-RL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03141</link><description>&lt;p&gt;
&#20351;&#29992;&#36741;&#21161;&#30701;&#26102;&#24310;&#20219;&#21153;&#25552;&#21319;&#38271;&#26102;&#24310;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03141
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auxiliary-Delayed Reinforcement Learning (AD-RL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#24773;&#26223;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24310;&#36831;&#24773;&#26223;&#26159;&#25351;&#35266;&#23519;&#21644;&#20132;&#20114;&#23384;&#22312;&#24310;&#36831;&#30340;&#24120;&#35265;&#23454;&#38469;&#24773;&#20917;&#12290;&#29616;&#26377;&#25216;&#26415;&#20013;&#65292;&#29366;&#24577;&#22686;&#24378;&#25216;&#26415;&#22312;&#24310;&#36831;&#27493;&#39588;&#20013;&#21487;&#33021;&#20250;&#20986;&#29616;&#29366;&#24577;&#31354;&#38388;&#25193;&#22823;&#25110;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Auxiliary-Delayed Reinforcement Learning&#65288;AD-RL&#65289;&#65292;&#21033;&#29992;&#19968;&#20010;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AD-RL&#22312;&#30701;&#26102;&#24310;&#20219;&#21153;&#20013;&#23398;&#20064;&#20540;&#20989;&#25968;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#38271;&#26102;&#24310;&#20219;&#21153;&#20013;&#30340;&#33258;&#20030;&#21644;&#31574;&#30053;&#25913;&#36827;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#19982;&#30452;&#25509;&#22312;&#21407;&#22987;&#38271;&#26102;&#24310;&#20219;&#21153;&#19978;&#23398;&#20064;&#30456;&#27604;&#65292;&#36825;&#26679;&#20570;&#21487;&#20197;&#22823;&#22823;&#20943;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays. State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments. To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments. Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task. We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task. On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficienc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#32500;&#29615;&#22659;&#20013;&#39044;&#35757;&#32451;&#34920;&#31034;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03138</link><description>&lt;p&gt;
Just Cluster It: &#19968;&#31181;&#20351;&#29992;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#39640;&#32500;&#31354;&#38388;&#25506;&#32034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#32500;&#29615;&#22659;&#20013;&#39044;&#35757;&#32451;&#34920;&#31034;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#34920;&#24449;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#38382;&#39064;&#65292;&#23558;&#25506;&#32034;&#35270;&#20026;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#20351;&#29992;&#32858;&#31867;&#26469;&#36827;&#34892;&#25506;&#32034;&#30340;&#26377;&#25928;&#24615;&#65292;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#65292;&#19982;&#20108;&#32500;&#29615;&#22659;&#30456;&#27604;&#65292;&#29366;&#24577;&#36716;&#25442;&#20013;&#30340;&#20687;&#32032;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#19981;&#37027;&#20040;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;DINO&#34920;&#31034;&#36827;&#34892;&#21608;&#26399;&#24615;&#21644;&#20840;&#23616;&#32858;&#31867;&#26469;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#21363;&#20272;&#35745;&#20266;&#35745;&#25968;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#29305;&#24449;&#20063;&#21487;&#20197;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#32858;&#31867;&#20197;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#28982;&#32780;&#24403;&#36825;&#20123;&#29305;&#24449;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#26102;&#65292;&#39044;&#35757;&#32451;DINO&#34920;&#31034;&#30001;&#20110;&#20854;&#39044;&#35757;&#32451;&#30340;&#24402;&#32435;&#20559;&#24046;&#22312;&#34920;&#31034;&#20013;&#26356;&#21152;&#26377;&#25928;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20026;&#38598;&#25104;&#39044;&#35757;&#32451;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#26465;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Albatross&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#27169;&#25311;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#21516;&#26102;&#21338;&#24328;&#20013;&#30340;&#26377;&#30028;&#29702;&#24615;&#20195;&#29702;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#19982;&#20219;&#20309;&#28216;&#25103;&#23454;&#21147;&#30340;&#20195;&#29702;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.03136</link><description>&lt;p&gt;
&#35299;&#20915;&#21512;&#20316;&#19982;&#31454;&#20105;&#30340;&#21516;&#26102;&#21338;&#24328;&#30340;&#38646;&#23556;&#20987;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03136
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Albatross&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#27169;&#25311;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#21516;&#26102;&#21338;&#24328;&#20013;&#30340;&#26377;&#30028;&#29702;&#24615;&#20195;&#29702;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#19982;&#20219;&#20309;&#28216;&#25103;&#23454;&#21147;&#30340;&#20195;&#29702;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#28216;&#25103;&#20013;&#65292;&#33258;&#25105;&#23545;&#24328;&#21644;&#35268;&#21010;&#30340;&#32452;&#21512;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#22269;&#38469;&#35937;&#26827;&#21644;&#22260;&#26827;&#12290;&#28982;&#32780;&#65292;&#23558;AlphaZero&#31561;&#31639;&#27861;&#35843;&#25972;&#21040;&#21516;&#26102;&#21338;&#24328;&#20013;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#65292;&#23545;&#20854;&#20182;&#20195;&#29702;&#30340;&#21516;&#26102;&#34892;&#21160;&#32570;&#20047;&#20449;&#24687;&#26159;&#19968;&#20010;&#38480;&#21046;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#36873;&#25321;&#19981;&#21516;&#30340;&#32435;&#20160;&#22343;&#34913;&#25110;&#32773;&#26681;&#26412;&#19981;&#26368;&#20248;&#22320;&#36827;&#34892;&#28216;&#25103;&#12290;&#22240;&#27492;&#65292;&#22312;&#19982;&#20854;&#20182;&#20195;&#29702;&#21338;&#24328;&#26102;&#65292;&#24314;&#27169;&#20854;&#34892;&#20026;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Albatross&#65306;&#20351;&#29992;&#27169;&#25311;&#33258;&#25105;&#23545;&#24328;&#26469;&#23398;&#20064;&#26377;&#30028;&#29702;&#24615;&#20195;&#29702;&#21644;&#22522;&#20110;&#28201;&#24230;&#30340;&#21453;&#24212;&#20248;&#21270;&#30340;AlphaZero&#12290;Albatross&#23398;&#20064;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#34913;&#27010;&#24565;&#65306;&#24179;&#28369;&#26368;&#20339;&#21453;&#24212;&#26497;&#31471;&#22343;&#34913;(SBRLE)&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#20219;&#20309;&#28216;&#25103;&#23454;&#21147;&#30340;&#20195;&#29702;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#21516;&#26102;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#19978;&#23545;Albatross&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#19982;AlphaZero&#30456;&#27604;&#65292;Albatross&#33021;&#22815;&#21516;&#26102;&#21338;&#24328;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of self-play and planning has achieved great successes in sequential games, for instance in Chess and Go. However, adapting algorithms such as AlphaZero to simultaneous games poses a new challenge. In these games, missing information about concurrent actions of other agents is a limiting factor as they may select different Nash equilibria or do not play optimally at all. Thus, it is vital to model the behavior of the other agents when interacting with them in simultaneous games. To this end, we propose Albatross: AlphaZero for Learning Bounded-rational Agents and Temperature-based Response Optimization using Simulated Self-play. Albatross learns to play the novel equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which enables cooperation and competition with agents of any playing strength. We perform an extensive evaluation of Albatross on a set of cooperative and competitive simultaneous perfect-information games. In contrast to AlphaZero, Albatr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;R&#32534;&#31243;&#35821;&#35328;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#27425;&#23581;&#35797;&#30340;&#36807;&#31243;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#31934;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#31561;&#22810;&#20010;&#36136;&#37327;&#23646;&#24615;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.03130</link><description>&lt;p&gt;
ChatGPT&#29983;&#25104;R&#31243;&#24207;&#20195;&#30721;&#30340;&#29992;&#25143;&#20013;&#24515;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
User-Centric Evaluation of ChatGPT Capability of Generating R Program Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;R&#32534;&#31243;&#35821;&#35328;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#27425;&#23581;&#35797;&#30340;&#36807;&#31243;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#31934;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#31561;&#22810;&#20010;&#36136;&#37327;&#23646;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#21578;&#20102;&#23545;ChatGPT&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;R&#32534;&#31243;&#35821;&#35328;&#20195;&#30721;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;R&#31243;&#24207;&#20195;&#30721;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#22522;&#20110;&#22330;&#26223;&#30340;&#27979;&#35797;&#21644;&#35780;&#20272;&#22312;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#21644;&#19981;&#21516;&#31867;&#22411;&#31243;&#24207;&#30340;&#21508;&#31181;&#20351;&#29992;&#24773;&#22659;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#35780;&#20272;&#37319;&#29992;&#22810;&#27425;&#23581;&#35797;&#30340;&#36807;&#31243;&#65292;&#27979;&#35797;&#20154;&#21592;&#36890;&#36807;&#22810;&#27425;&#23581;&#35797;&#26469;&#23436;&#25104;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#30452;&#33267;&#33719;&#24471;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#25110;&#36798;&#21040;&#26368;&#22823;&#23581;&#35797;&#27425;&#25968;&#21518;&#25918;&#24323;&#12290;&#27599;&#27425;&#23581;&#35797;&#20013;&#65292;&#27979;&#35797;&#20154;&#21592;&#26681;&#25454;&#20808;&#21069;&#30340;&#32467;&#26524;&#21644;&#24453;&#23436;&#25104;&#30340;&#20219;&#21153;&#21046;&#23450;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#32473;ChatGPT&#12290;&#38500;&#20102;&#24179;&#22343;&#23581;&#35797;&#27425;&#25968;&#21644;&#24179;&#22343;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#25351;&#26631;&#22806;&#65292;&#26368;&#32456;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#26681;&#25454;&#31934;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#31561;&#22810;&#20010;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reports an evaluation of ChatGPT's capability of generating R programming language code from natural language input. A dataset specially designed for generating R program code was constructed with metadata to support scenario-based testing and evaluation of code generation capabilities in various usage scenarios of different levels of difficulty and different types of programs. The evaluation takes a multiple attempt process in which the tester tries to complete the code generation task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts. In each attempt the tester formulates a natural language input to ChatGPT based on the previous results and the task to be completed. In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, concise
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;e$^2$KD&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#23398;&#29983;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#37117;&#24471;&#21040;&#22823;&#24133;&#24230;&#25552;&#21319;&#65292;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#37027;&#37324;&#27491;&#30830;&#23398;&#21040;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.03119</link><description>&lt;p&gt;
&#22909;&#30340;&#25945;&#24072;&#35299;&#37322;: &#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Good Teachers Explain: Explanation-Enhanced Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;e$^2$KD&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#23398;&#29983;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#37117;&#24471;&#21040;&#22823;&#24133;&#24230;&#25552;&#21319;&#65292;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#37027;&#37324;&#27491;&#30830;&#23398;&#21040;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23558;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#25104;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#30693;&#36947;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#25945;&#24072;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#24050;&#32463;&#21457;&#29616;&#23398;&#29983;&#27169;&#22411;&#36890;&#24120;&#19981;&#20250;&#23398;&#21040;&#30456;&#21516;&#30340;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#30456;&#20284;&#23646;&#24615;&#65292;&#22914;&#22522;&#20110;&#30456;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#24120;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#65292;&#22240;&#20026;&#36825;&#30830;&#20445;&#23398;&#29983;&#20174;&#25945;&#24072;&#37027;&#37324;&#23398;&#21040;&#20102;&#8220;&#27491;&#30830;&#30340;&#29305;&#24449;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#32463;&#20856;&#30340;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20197;&#21450;&#25945;&#24072;&#21644;&#23398;&#29983;&#25152;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#30456;&#20284;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#31616;&#21333;&#19988;&#30452;&#35266;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#8221;&#65288;e$^2$KD&#65289;&#65288;1&#65289;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#22987;&#32456;&#25552;&#20379;&#20102;&#22823;&#24133;&#24230;&#30340;&#22686;&#30410;&#65292;&#65288;2&#65289;&#30830;&#20445;&#23398;&#29983;&#20174;&#25945;&#24072;&#37027;&#37324;&#23398;&#21040;&#20102;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#20851;&#27880;&#26426;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#21644;&#35299;&#37322;&#20598;&#27694;&#21270;&#21512;&#29289;&#30340;&#32418;&#22806;&#20809;&#35889;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#38598;&#20013;&#20851;&#27880;&#19982;&#21151;&#33021;&#22242;&#37051;&#36817;&#30340;&#21270;&#23398;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20809;&#35889;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#25581;&#31034;&#32418;&#22806;&#20809;&#35889;&#29305;&#24449;&#19982;&#20998;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.03112</link><description>&lt;p&gt;
&#21033;&#29992;&#20855;&#26377;&#32467;&#26500;&#20851;&#27880;&#26426;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20598;&#27694;&#22522;&#22242;&#30340;&#32418;&#22806;&#20809;&#35889;
&lt;/p&gt;
&lt;p&gt;
Infrared Spectra Prediction for Diazo Groups Utilizing a Machine Learning Approach with Structural Attention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#20851;&#27880;&#26426;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#21644;&#35299;&#37322;&#20598;&#27694;&#21270;&#21512;&#29289;&#30340;&#32418;&#22806;&#20809;&#35889;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#38598;&#20013;&#20851;&#27880;&#19982;&#21151;&#33021;&#22242;&#37051;&#36817;&#30340;&#21270;&#23398;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20809;&#35889;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#25581;&#31034;&#32418;&#22806;&#20809;&#35889;&#29305;&#24449;&#19982;&#20998;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#22806;&#65288;IR&#65289;&#20809;&#35889;&#26159;&#21270;&#23398;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#36890;&#36807;&#25391;&#21160;&#21644;&#36716;&#21160;&#36291;&#36801;&#38416;&#26126;&#20998;&#23376;&#32467;&#26500;&#21644;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#29420;&#29305;&#30340;&#25391;&#21160;&#21644;&#36716;&#21160;&#27169;&#24335;&#25152;&#34920;&#24449;&#30340;&#22797;&#26434;&#20998;&#23376;&#29305;&#24449;&#32473;&#20998;&#26512;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#20851;&#27880;&#26426;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#20598;&#27694;&#21270;&#21512;&#29289;&#65292;&#20197;&#25552;&#21319;&#32418;&#22806;&#20809;&#35889;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#38598;&#20013;&#20851;&#27880;&#37051;&#36817;&#21151;&#33021;&#22242;&#38468;&#36817;&#30340;&#21270;&#23398;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20809;&#35889;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25581;&#31034;&#20102;&#32418;&#22806;&#20809;&#35889;&#29305;&#24449;&#19982;&#20998;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#36884;&#24452;&#26469;&#35299;&#26512;&#22797;&#26434;&#30340;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infrared (IR) spectroscopy is a pivotal technique in chemical research for elucidating molecular structures and dynamics through vibrational and rotational transitions. However, the intricate molecular fingerprints characterized by unique vibrational and rotational patterns present substantial analytical challenges. Here, we present a machine learning approach employing a Structural Attention Mechanism tailored to enhance the prediction and interpretation of infrared spectra, particularly for diazo compounds. Our model distinguishes itself by honing in on chemical information proximal to functional groups, thereby significantly bolstering the accuracy, robustness, and interpretability of spectral predictions. This method not only demystifies the correlations between infrared spectral features and molecular structures but also offers a scalable and efficient paradigm for dissecting complex molecular interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03110</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Latent Auto-Regressive Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#38750;&#24179;&#31283;&#22870;&#21169;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#20844;&#24335;&#65292;&#20854;&#20013;&#33218;&#30340;&#24179;&#22343;&#22870;&#21169;&#38543;&#26102;&#38388;&#21464;&#21270;&#26159;&#30001;&#19968;&#20123;&#26410;&#30693;&#30340;&#28508;&#22312;&#33258;&#22238;&#24402;(AR)&#29366;&#24577;&#30340;&#39034;&#24207;k&#20915;&#23450;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#30340;&#29615;&#22659;&#31216;&#20026;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#12290;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#30340;&#19981;&#21516;&#24418;&#24335;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#37117;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#20026;&#20581;&#24247;&#25110;&#25945;&#32946;&#31561;&#26032;&#20852;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#36825;&#37324;&#32570;&#20047;&#23545;&#29615;&#22659;&#30340;&#26426;&#21046;&#24314;&#27169;&#12290;&#22914;&#26524;AR&#39034;&#24207;k&#24050;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#34920;&#29616;&#20986;O(k&#8730;T)&#30340;&#36951;&#25022;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;k&#34987;&#38169;&#35823;&#22320;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20063;&#32988;&#36807;&#26631;&#20934;&#30340;UCB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#30340;&#25552;&#31034;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#21644;&#29983;&#25104;&#21512;&#25104;&#36793;&#30028;&#24773;&#20917;&#25968;&#25454;&#26469;&#25913;&#36827;&#25552;&#31034;&#24037;&#31243;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03099</link><description>&lt;p&gt;
&#22522;&#20110;&#24847;&#22270;&#30340;&#25552;&#31034;&#26657;&#20934;&#65306;&#29992;&#21512;&#25104;&#36793;&#30028;&#24773;&#20917;&#22686;&#24378;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#30340;&#25552;&#31034;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#21644;&#29983;&#25104;&#21512;&#25104;&#36793;&#30028;&#24773;&#20917;&#25968;&#25454;&#26469;&#25913;&#36827;&#25552;&#31034;&#24037;&#31243;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#39640;&#24230;&#25935;&#24863;&#24615;&#21644;&#25991;&#26412;&#20219;&#21153;&#25351;&#20196;&#30340;&#22266;&#26377;&#27495;&#20041;&#65292;&#25552;&#31034;&#24037;&#31243;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#20010;&#21253;&#21547;&#19978;&#27425;&#35797;&#39564;&#32467;&#26524;&#30340;&#20803;&#25552;&#31034;&#24182;&#25552;&#20986;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;LLMs&#33258;&#21160;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#26159;&#22256;&#38590;&#19988;&#26114;&#36149;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#20351;&#29992;&#26657;&#20934;&#36807;&#31243;&#26469;&#36845;&#20195;&#22320;&#20248;&#21270;&#19982;&#29992;&#25143;&#24847;&#22270;&#30456;&#31526;&#30340;&#25552;&#31034;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#31995;&#32479;&#32852;&#21512;&#29983;&#25104;&#36793;&#30028;&#29992;&#20363;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#35821;&#35328;&#24341;&#23548;&#30340;&#25277;&#35937;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#21270;&#24182;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#26500;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#29992;&#25143;&#30340;&#29366;&#24577;&#25277;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.03081</link><description>&lt;p&gt;
&#22522;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#35821;&#35328;&#24341;&#23548;&#30340;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference-Conditioned Language-Guided Abstraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#35821;&#35328;&#24341;&#23548;&#30340;&#25277;&#35937;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#21270;&#24182;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#26500;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#29992;&#25143;&#30340;&#29366;&#24577;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#26159;&#20154;&#20204;&#25945;&#23548;&#26426;&#22120;&#20154;&#30340;&#24120;&#29992;&#26041;&#24335;&#65292;&#20294;&#23427;&#23481;&#26131;&#20986;&#29616;&#35823;&#23548;&#24615;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#26500;&#24314;&#29366;&#24577;&#25277;&#35937;&#65292;&#21363;&#21253;&#21547;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#20197;&#36827;&#34892;&#26356;&#36890;&#29992;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25277;&#35937;&#20063;&#21462;&#20915;&#20110;&#29992;&#25143;&#22312;&#20219;&#21153;&#20013;&#20851;&#27880;&#30340;&#20559;&#22909;&#65292;&#36825;&#21487;&#33021;&#24456;&#38590;&#29992;&#35821;&#35328;&#25551;&#36848;&#25110;&#20165;&#20165;&#36890;&#36807;&#35821;&#35328;&#35814;&#23613;&#35828;&#26126;&#12290;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#25277;&#35937;&#26469;&#25429;&#25417;&#36825;&#20123;&#28508;&#22312;&#20559;&#22909;&#21602;&#65311;&#25105;&#20204;&#35266;&#23519;&#21040;&#20154;&#31867;&#34892;&#20026;&#21453;&#26144;&#20102;&#20182;&#20204;&#30340;&#19990;&#30028;&#35266;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#21270;&#21578;&#35785;&#25105;&#20204;&#20154;&#31867;&#23545;&#19990;&#30028;&#30340;&#20559;&#22909;&#23384;&#22312;&#24046;&#24322;&#65292;&#21363;&#20182;&#20204;&#30340;&#29366;&#24577;&#25277;&#35937;&#20063;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30452;&#25509;&#26597;&#35810;&#36825;&#20123;&#20559;&#22909;&#65292;&#32473;&#23450;&#24050;&#21457;&#29983;&#34892;&#20026;&#21464;&#21270;&#30340;&#30693;&#35782;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;LM&#26377;&#20004;&#31181;&#26041;&#24335;&#65306;&#39318;&#20808;&#65292;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#26597;&#35810;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from demonstrations is a common way for users to teach robots, but it is prone to spurious feature correlations. Recent work constructs state abstractions, i.e. visual representations containing task-relevant features, from language as a way to perform more generalizable learning. However, these abstractions also depend on a user's preference for what matters in a task, which may be hard to describe or infeasible to exhaustively specify using language alone. How do we construct abstractions to capture these latent preferences? We observe that how humans behave reveals how they see the world. Our key insight is that changes in human behavior inform us that there are differences in preferences for how humans see the world, i.e. their state abstractions. In this work, we propose using language models (LMs) to query for those preferences directly given knowledge that a change in behavior has occurred. In our framework, we use the LM in two ways: first, given a text description of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20803;&#24378;&#21270;&#23398;&#20064;&#26469;&#25506;&#32034;&#20154;&#31867;&#33719;&#21462;&#22810;&#20010;&#20840;&#26032;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24773;&#22659;&#32447;&#32034;&#22312;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.03072</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#20803;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#25277;&#35937;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Learning to Abstract Visuomotor Mappings using Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20803;&#24378;&#21270;&#23398;&#20064;&#26469;&#25506;&#32034;&#20154;&#31867;&#33719;&#21462;&#22810;&#20010;&#20840;&#26032;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24773;&#22659;&#32447;&#32034;&#22312;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#33719;&#24471;&#20840;&#26032;&#25216;&#33021;&#30340;&#22810;&#20010;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;&#30340;&#33021;&#21147;&#12290;&#20351;&#29992;&#26684;&#23376;&#23548;&#33322;&#33539;&#24335;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19981;&#21516;"&#26684;&#23376;&#19990;&#30028;"&#23454;&#26045;&#30340;&#24773;&#22659;&#32447;&#32034;&#26159;&#21542;&#33021;&#22815;&#20351;&#21442;&#19982;&#32773;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#20004;&#20010;&#19981;&#21516;&#30340;&#20851;&#38190;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#25552;&#20379;&#24773;&#22659;&#20449;&#24687;&#26102;&#65292;&#20219;&#21153;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;&#23545;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#26469;&#35828;&#20063;&#26159;&#22914;&#27492;&#65292;&#23427;&#20204;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#26159;&#21542;&#25509;&#25910;&#24773;&#22659;&#20449;&#24687;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#20154;&#31867;&#34920;&#29616;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#22659;&#32447;&#32034;&#20801;&#35768;&#22312;&#20351;&#29992;&#19981;&#21516;&#30340;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;&#26102;&#24418;&#25104;&#29420;&#31435;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#65292;&#32780;&#23427;&#20204;&#30340;&#32570;&#22833;&#21017;&#26356;&#20542;&#21521;&#20110;&#20849;&#20139;&#19968;&#20010;&#34920;&#31034;&#12290;&#34429;&#28982;&#36825;&#20004;&#31181;&#31574;&#30053;&#37117;&#21487;&#20197;&#23398;&#20064;&#22810;&#20010;&#35270;&#35273;&#36816;&#21160;&#26144;&#23556;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#24773;&#22659;&#32447;&#32034;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigated the human capacity to acquire multiple visuomotor mappings for de novo skills. Using a grid navigation paradigm, we tested whether contextual cues implemented as different "grid worlds", allow participants to learn two distinct key-mappings more efficiently. Our results indicate that when contextual information is provided, task performance is significantly better. The same held true for meta-reinforcement learning agents that differed in whether or not they receive contextual information when performing the task. We evaluated their accuracy in predicting human performance in the task and analyzed their internal representations. The results indicate that contextual cues allow the formation of separate representations in space and time when using different visuomotor mappings, whereas the absence of them favors sharing one representation. While both strategies can allow learning of multiple visuomotor mappings, we showed contextual cues provide a computational advantage 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BERTopic&#22312;&#22622;&#23572;&#32500;&#20122;&#35821;&#30701;&#25991;&#26412;&#20013;&#30340;&#39318;&#27425;&#24212;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;BERTopic&#21487;&#20197;&#20135;&#29983;&#20016;&#23500;&#30340;&#20027;&#39064;&#65292;&#21363;&#20351;&#26159;&#37096;&#20998;&#39044;&#22788;&#29702;&#30340;&#25991;&#26412;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;BERTopic&#22312;&#20027;&#39064;&#25968;&#37327;&#19981;&#21463;&#38480;&#21046;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#20449;&#24687;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03067</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;Transformer&#21644;BERTopic&#29992;&#20110;&#30701;&#25991;&#26412;&#20027;&#39064;&#24314;&#27169;&#65306;&#22622;&#23572;&#32500;&#20122;&#35821;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BERTopic&#22312;&#22622;&#23572;&#32500;&#20122;&#35821;&#30701;&#25991;&#26412;&#20013;&#30340;&#39318;&#27425;&#24212;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;BERTopic&#21487;&#20197;&#20135;&#29983;&#20016;&#23500;&#30340;&#20027;&#39064;&#65292;&#21363;&#20351;&#26159;&#37096;&#20998;&#39044;&#22788;&#29702;&#30340;&#25991;&#26412;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;BERTopic&#22312;&#20027;&#39064;&#25968;&#37327;&#19981;&#21463;&#38480;&#21046;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#20449;&#24687;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BERTopic&#36825;&#31181;&#26368;&#20808;&#36827;&#30340;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#22312;&#20855;&#26377;&#20016;&#23500;&#24418;&#24577;&#35821;&#35328;&#30340;&#30701;&#25991;&#26412;&#20013;&#30340;&#39318;&#27425;&#24212;&#29992;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22810;&#35821;&#35328;&#23884;&#20837;&#27169;&#22411;&#20197;&#21450;&#20004;&#31181;&#25991;&#26412;&#39044;&#22788;&#29702;&#27700;&#24179;&#65288;&#37096;&#20998;&#21644;&#23436;&#20840;&#65289;&#23545;&#22622;&#23572;&#32500;&#20122;&#35821;&#30340;&#37096;&#20998;&#39044;&#22788;&#29702;&#30340;&#30701;&#25991;&#26412;&#36827;&#34892;&#20102;BERTopic&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;LDA&#21644;NMF&#22312;&#23436;&#20840;&#39044;&#22788;&#29702;&#25991;&#26412;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#19968;&#20010;&#34920;&#36798;&#23545;COVID-19&#30123;&#33495;&#30097;&#34385;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21442;&#25968;&#35774;&#32622;&#65292;BERTopic&#21363;&#20351;&#24212;&#29992;&#20110;&#37096;&#20998;&#39044;&#22788;&#29702;&#30340;&#30701;&#25991;&#26412;&#65292;&#20063;&#33021;&#20135;&#29983;&#20449;&#24687;&#20016;&#23500;&#30340;&#20027;&#39064;&#12290;&#24403;&#22312;&#20004;&#31181;&#39044;&#22788;&#29702;&#22330;&#26223;&#19979;&#24212;&#29992;&#30456;&#21516;&#30340;&#21442;&#25968;&#26102;&#65292;&#37096;&#20998;&#39044;&#22788;&#29702;&#25991;&#26412;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;&#19982;LDA&#21644;NMF&#30456;&#27604;&#65292;&#20174;&#20851;&#38190;&#35789;&#26469;&#30475;&#65292;BERTopic&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#20027;&#39064;&#65292;&#24182;&#22312;&#20027;&#39064;&#25968;&#37327;&#19981;&#21463;&#38480;&#21046;&#26102;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of the first application of BERTopic, a state-of-the-art topic modeling technique, to short text written in a morphologi-cally rich language. We applied BERTopic with three multilingual embed-ding models on two levels of text preprocessing (partial and full) to evalu-ate its performance on partially preprocessed short text in Serbian. We also compared it to LDA and NMF on fully preprocessed text. The experiments were conducted on a dataset of tweets expressing hesitancy toward COVID-19 vaccination. Our results show that with adequate parameter setting, BERTopic can yield informative topics even when applied to partially pre-processed short text. When the same parameters are applied in both prepro-cessing scenarios, the performance drop on partially preprocessed text is minimal. Compared to LDA and NMF, judging by the keywords, BERTopic offers more informative topics and gives novel insights when the number of topics is not limited. The findings of this p
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>&#8220;InteractiveVideo&#8221;&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#31934;&#30830;&#12289;&#26377;&#25928;&#22320;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#35270;&#39057;&#30340;&#20851;&#38190;&#26041;&#38754;&#36827;&#34892;&#28789;&#27963;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.03040</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#35270;&#39057;&#65306;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#21487;&#25511;&#35270;&#39057;&#29983;&#25104;&#19982;&#22810;&#27169;&#24577;&#25351;&#20196;&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03040
&lt;/p&gt;
&lt;p&gt;
&#8220;InteractiveVideo&#8221;&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#31934;&#30830;&#12289;&#26377;&#25928;&#22320;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#35270;&#39057;&#30340;&#20851;&#38190;&#26041;&#38754;&#36827;&#34892;&#28789;&#27963;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20132;&#20114;&#24335;&#35270;&#39057;&#8221;&#30340;&#29992;&#25143;&#20013;&#24515;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#29992;&#25143;&#25552;&#20379;&#22270;&#20687;&#25110;&#25991;&#26412;&#30340;&#29983;&#25104;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#35774;&#35745;&#29992;&#20110;&#21160;&#24577;&#20132;&#20114;&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#21508;&#31181;&#30452;&#35266;&#30340;&#26426;&#21046;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#25552;&#31034;&#12289;&#32472;&#30011;&#12289;&#25302;&#25918;&#31561;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#22810;&#27169;&#24577;&#25351;&#20196;&#26426;&#21046;&#65292;&#26088;&#22312;&#23558;&#29992;&#25143;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#26080;&#32541;&#38598;&#25104;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#29992;&#25143;&#36755;&#20837;&#21644;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;&#21512;&#20316;&#21644;&#21709;&#24212;&#24335;&#20132;&#20114;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#31934;&#30830;&#32780;&#26377;&#25928;&#30340;&#29992;&#25143;&#25351;&#20196;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#32467;&#26524;&#30340;&#36845;&#20195;&#21644;&#31934;&#32454;&#21270;&#35843;&#25972;&#12290;&#36890;&#36807;&#8220;&#20132;&#20114;&#24335;&#35270;&#39057;&#8221;&#65292;&#29992;&#25143;&#21487;&#20197;&#28789;&#27963;&#22320;&#31934;&#24515;&#35843;&#25972;&#35270;&#39057;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21253;&#25324;&#32472;&#30011;&#21442;&#32771;&#22270;&#20687;&#12289;&#32534;&#36753;&#35821;&#20041;&#21644;&#35843;&#25972;&#35270;&#39057;&#21160;&#20316;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce $\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;20&#31181;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;ACSESS&#65289;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03038</link><description>&lt;p&gt;
&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automatic Combination of Sample Selection Strategies for Few-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;20&#31181;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;ACSESS&#65289;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#22914;&#20803;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#38480;&#26679;&#26412;&#25968;&#37327;&#23545;&#25972;&#20307;&#25104;&#21151;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#65292;&#20294;&#23427;&#20204;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#21313;&#20998;&#26126;&#30830;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#21482;&#34987;&#22312;&#20856;&#22411;&#30340;&#30417;&#30563;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;8&#20010;&#22270;&#20687;&#21644;6&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;5&#31181;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#24443;&#24213;&#30740;&#31350;&#20102;20&#31181;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;ACSESS&#65289;&#65292;&#23427;&#20805;&#20998;&#21033;&#29992;&#20102;&#20010;&#20307;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#20114;&#34917;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20010;&#20307;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#21450;&#26368;&#36817;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25903;&#25345;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03017</link><description>&lt;p&gt;
&#21521;&#32511;&#33394;&#19988;&#31867;&#20154;&#30340;&#20154;&#24037;&#26234;&#33021;&#36808;&#36827;&#65306;&#24403;&#20195;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#21644;&#35745;&#31639;&#30340;&#26114;&#36149;&#24615;&#20351;&#20854;&#22312;&#35768;&#22810;&#25968;&#25454;&#21463;&#38480;&#30340;&#30495;&#23454;&#24212;&#29992;&#20013;&#19981;&#23454;&#29992;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#23545;&#26032;&#23398;&#20064;&#20219;&#21153;&#30340;&#24555;&#36895;&#36866;&#24212;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#26368;&#26032;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#27491;&#24335;&#23450;&#20041;&#20102;FSL&#65292;&#24182;&#20171;&#32461;&#20102;&#23427;&#19982;&#19981;&#21516;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#31995;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#25193;&#23637;&#20102;&#20197;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#32463;&#20856;&#21644;&#26032;&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#36827;&#34892;&#20102;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#22609;&#36896;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36235;&#21183;&#12289;&#31361;&#20986;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications. Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years. This survey provides a comprehensive overview of the field's latest advancements. Initially, FSL is formally defined, and its relationship with different learning fields is presented. A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described. Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22686;&#24378;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36873;&#20030;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#26681;&#25454;&#37051;&#23621;&#30340;&#21487;&#20449;&#24230;&#26377;&#36873;&#25321;&#24615;&#22320;&#35831;&#27714;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20010;&#20307;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#26041;&#24046;&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03014</link><description>&lt;p&gt;
&#20449;&#20219;&#35841;&#65311;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#36873;&#20030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Whom to Trust? Elective Learning for Distributed Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22686;&#24378;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36873;&#20030;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#26681;&#25454;&#37051;&#23621;&#30340;&#21487;&#20449;&#24230;&#26377;&#36873;&#25321;&#24615;&#22320;&#35831;&#27714;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20010;&#20307;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#26041;&#24046;&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#22238;&#24402;&#26469;&#22686;&#24378;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#36873;&#20030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#22522;&#20110;&#20808;&#39564;&#30340;&#36873;&#20030;&#24335;&#20998;&#24067;&#24335;GP&#65288;Pri-GP&#65289;&#65292;&#23427;&#36171;&#20104;&#20102;&#26234;&#33021;&#20307;&#22522;&#20110;&#20449;&#20219;&#24230;&#26377;&#36873;&#25321;&#22320;&#21521;&#37051;&#23621;&#26234;&#33021;&#20307;&#35831;&#27714;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;Pri-GP&#26377;&#25928;&#25552;&#39640;&#20102;&#20010;&#20307;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#26234;&#33021;&#20307;&#20808;&#39564;&#30693;&#35782;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#23427;&#28040;&#38500;&#20102;&#22312;&#20998;&#24067;&#24335;GP&#20013;&#30830;&#23450;&#32858;&#21512;&#26435;&#37325;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#26041;&#24046;&#35745;&#31639;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;Pri-GP&#26694;&#26550;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#39044;&#27979;&#35823;&#24046;&#36793;&#30028;&#65292;&#30830;&#20445;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an innovative approach to enhance distributed cooperative learning using Gaussian process (GP) regression in multi-agent systems (MASs). The key contribution of this work is the development of an elective learning algorithm, namely prior-aware elective distributed GP (Pri-GP), which empowers agents with the capability to selectively request predictions from neighboring agents based on their trustworthiness. The proposed Pri-GP effectively improves individual prediction accuracy, especially in cases where the prior knowledge of an agent is incorrect. Moreover, it eliminates the need for computationally intensive variance calculations for determining aggregation weights in distributed GP. Furthermore, we establish a prediction error bound within the Pri-GP framework, ensuring the reliability of predictions, which is regarded as a crucial property in safety-critical MAS applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;UniMix&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03009</link><description>&lt;p&gt;
UniMem&#65306;&#36808;&#21521;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
UniMem: Towards a Unified View of Long-Context Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;UniMix&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#26159;&#38480;&#21046;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#33021;&#21147;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#33268;&#21147;&#20110;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#26159;&#23396;&#31435;&#22320;&#24320;&#21457;&#30340;&#65292;&#32570;&#20047;&#23545;&#23427;&#20204;&#30340;&#20248;&#28857;&#30340;&#31995;&#32479;&#20998;&#26512;&#21644;&#25972;&#21512;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20174;LLM&#30340;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#12290; UniMem&#30340;&#29305;&#28857;&#26159;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#20869;&#23384;&#31649;&#29702;&#65292;&#20869;&#23384;&#20889;&#20837;&#65292;&#20869;&#23384;&#35835;&#21462;&#21644;&#20869;&#23384;&#27880;&#20837;&#65292;&#20026;&#20102;&#29702;&#35299;&#21508;&#31181;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#25552;&#20379;&#20102;&#31995;&#32479;&#29702;&#35770;&#12290;&#25105;&#20204;&#22522;&#20110;UniMem&#37325;&#26032;&#21046;&#23450;&#20102;16&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;Transformer-XL&#65292;&#35760;&#24518;&#21270;Transformer&#65292;RMT&#21644;Longformer&#20013;&#30340;&#22235;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#23427;&#20204;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#20248;&#21183;&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniMix&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-context processing is a critical ability that constrains the applicability of large language models. Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs. UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an inn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#65292;&#20174;&#32780;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02992</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Decoding-time Realignment of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#65292;&#20174;&#32780;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#23545;&#20110;&#20943;&#23569;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#21644;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#22312;&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#21644;&#40723;&#21169;&#20445;&#25345;&#19982;&#26410;&#23545;&#40784;&#27169;&#22411;&#25509;&#36817;&#30340;&#25509;&#36817;&#24615;&#35268;&#21017;&#39033;&#20043;&#38388;&#36827;&#34892;&#20248;&#21270;&#30340;&#26435;&#34913;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#35268;&#21017;&#21270;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#65306;&#35268;&#21017;&#21270;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#22870;&#21169;&#27450;&#39575;&#32780;&#38477;&#20302;&#27169;&#22411;&#33021;&#21147;&#65292;&#32780;&#36807;&#24230;&#35268;&#21017;&#21270;&#21017;&#38459;&#30861;&#23545;&#40784;&#12290;&#20256;&#32479;&#26041;&#27861;&#25214;&#21040;&#26368;&#20339;&#35268;&#21017;&#21270;&#27700;&#24179;&#38656;&#35201;&#20351;&#29992;&#19981;&#21516;&#35268;&#21017;&#21270;&#24378;&#24230;&#37325;&#26032;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#26469;&#35828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#12290;DeRa&#21487;&#20197;&#23545;&#23545;&#40784;&#27169;&#22411;&#30340;&#31243;&#24230;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#38024;&#23545;OWL 2 QL&#19978;&#20803;&#25512;&#29702;&#30340;Datalog&#24037;&#20855;&#12290;&#35768;&#22810;&#24212;&#29992;&#37117;&#38656;&#35201;&#20803;&#27169;&#22411;&#26469;&#31616;&#21270;&#26412;&#20307;&#30340;&#25193;&#23637;&#21644;&#37325;&#29992;&#12290;&#28982;&#32780;&#65292;&#20803;&#27169;&#22411;&#30340;&#26080;&#38480;&#21046;&#20351;&#29992;&#24102;&#26469;&#20102;&#19981;&#21487;&#21028;&#23450;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;OWL 2 QL&#19978;&#30340;&#20803;&#27169;&#22411;&#35821;&#20041;&#21644;&#20803;&#27169;&#22411;&#35821;&#20041;&#34164;&#21547;&#26426;&#21046;&#65292;&#24182;&#23545;&#20960;&#31181;&#25193;&#23637;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.02978</link><description>&lt;p&gt;
&#23545;&#20110;&#22312;OWL 2 QL&#19978;&#36827;&#34892;&#20803;&#25512;&#29702;&#30340;Datalog&#24037;&#20855;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Datalog Tools for Meta-reasoning over OWL 2 QL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#38024;&#23545;OWL 2 QL&#19978;&#20803;&#25512;&#29702;&#30340;Datalog&#24037;&#20855;&#12290;&#35768;&#22810;&#24212;&#29992;&#37117;&#38656;&#35201;&#20803;&#27169;&#22411;&#26469;&#31616;&#21270;&#26412;&#20307;&#30340;&#25193;&#23637;&#21644;&#37325;&#29992;&#12290;&#28982;&#32780;&#65292;&#20803;&#27169;&#22411;&#30340;&#26080;&#38480;&#21046;&#20351;&#29992;&#24102;&#26469;&#20102;&#19981;&#21487;&#21028;&#23450;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;OWL 2 QL&#19978;&#30340;&#20803;&#27169;&#22411;&#35821;&#20041;&#21644;&#20803;&#27169;&#22411;&#35821;&#20041;&#34164;&#21547;&#26426;&#21046;&#65292;&#24182;&#23545;&#20960;&#31181;&#25193;&#23637;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#36798;&#26412;&#20307;&#35770;&#20013;&#31867;&#21644;&#23646;&#24615;&#30693;&#35782;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#23427;&#26159;&#22810;&#20010;&#24212;&#29992;&#20013;&#30340;&#19968;&#31181;&#29702;&#24819;&#24314;&#27169;&#29305;&#24615;&#65292;&#21487;&#20197;&#31616;&#21270;&#26412;&#20307;&#30340;&#25193;&#23637;&#21644;&#37325;&#29992;&#12290;&#28982;&#32780;&#65292;&#26080;&#38480;&#21046;&#22320;&#20801;&#35768;&#20803;&#27169;&#22411;&#23384;&#22312;&#20250;&#20986;&#29616;&#22810;&#20010;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#19981;&#21487;&#21028;&#23450;&#24615;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23454;&#38469;&#35821;&#35328;&#31105;&#27490;&#31867;&#20316;&#20026;&#20854;&#20182;&#31867;&#30340;&#23454;&#20363;&#20986;&#29616;&#65292;&#25110;&#23558;&#27492;&#31867;&#20986;&#29616;&#35270;&#20026;&#35821;&#20041;&#19978;&#19981;&#21516;&#30340;&#23545;&#35937;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SPARQL&#20013;&#30340;&#20803;&#26597;&#35810;&#22312;&#30452;&#25509;&#35821;&#20041;&#34164;&#28085;&#26426;&#21046;&#65288;DSER&#65289;&#19979;&#20351;&#29992;&#21518;&#19968;&#31181;&#26041;&#27861;&#65292;&#22240;&#27492;&#19981;&#25903;&#25345;&#20803;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#20803;&#27169;&#22411;&#29305;&#24615;&#30340;&#25193;&#23637;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;Lenzerini&#31561;&#20154;&#65288;2015&#65289;&#21644;Lenzerini&#31561;&#20154;&#65288;2020&#65289;&#65307;Cima&#31561;&#20154;&#65288;2017&#65289;&#20013;&#25552;&#20986;&#30340;&#22312;OWL 2 QL&#19978;&#30340;&#20803;&#27169;&#22411;&#35821;&#20041;&#65288;MS&#65289;&#21644;&#20803;&#27169;&#22411;&#35821;&#20041;&#34164;&#21547;&#26426;&#21046;&#65288;MSER&#65289;&#12290;&#30001;O
&lt;/p&gt;
&lt;p&gt;
Metamodeling is a general approach to expressing knowledge about classes and properties in an ontology. It is a desirable modeling feature in multiple applications that simplifies the extension and reuse of ontologies. Nevertheless, allowing metamodeling without restrictions is problematic for several reasons, mainly due to undecidability issues. Practical languages, therefore, forbid classes to occur as instances of other classes or treat such occurrences as semantically different objects. Specifically, meta-querying in SPARQL under the Direct Semantic Entailment Regime (DSER) uses the latter approach, thereby effectively not supporting meta-queries. However, several extensions enabling different metamodeling features have been proposed over the last decade. This paper deals with the Metamodeling Semantics (MS) over OWL 2 QL and the Metamodeling Semantic Entailment Regime (MSER), as proposed in Lenzerini et al. (2015) and Lenzerini et al. (2020); Cima et al. (2017). A reduction from O
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02977</link><description>&lt;p&gt;
&#21464;&#20998;&#27969;&#27169;&#22411;&#65306;&#20197;&#20320;&#30340;&#39118;&#26684;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Variational Flow Models: Flowing in Your Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02977
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;"&#21518;&#39564;&#27969;"&#27169;&#22411;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;&#35299;&#37322;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20197;&#23558;"&#27010;&#29575;&#27969;"&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#38543;&#26426;&#36807;&#31243;&#31867;&#21035;&#65292;&#19981;&#24517;&#23616;&#38480;&#20110;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32467;&#26524;&#31216;&#20026;"&#21464;&#20998;&#27969;&#27169;&#22411;"&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#30001;&#26041;&#31243;Xt = at * X0 + st * X1&#25152;&#25551;&#36848;&#30340;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#36716;&#21270;&#20026;&#30452;&#32447;&#24658;&#36895;(SC)&#27969;&#65292;&#31867;&#20284;&#20110;&#30699;&#27491;&#27969;&#12290;&#36825;&#31181;&#36716;&#21270;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#27839;&#30528;&#21407;&#22987;&#30340;&#21518;&#39564;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;SC&#27969;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36716;&#25442;&#25193;&#23637;&#21040;&#20004;&#20010;&#19981;&#21516;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#36716;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#23558;&#39640;&#38454;&#25968;&#20540;&#35299;&#27861;&#36731;&#26494;&#38598;&#25104;&#21040;&#36716;&#25442;&#21518;&#30340;SC&#27969;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22686;&#37327;&#35780;&#20272;&#25366;&#25496;&#26368;&#23567;&#30340;&#34892;&#20026;&#27169;&#24335;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20887;&#20313;&#27169;&#24335;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02921</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#37327;&#35780;&#20272;&#25366;&#25496;&#26368;&#23567;&#30340;&#34892;&#20026;&#27169;&#24335;&#38598;&#21512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22686;&#37327;&#35780;&#20272;&#25366;&#25496;&#26368;&#23567;&#30340;&#34892;&#20026;&#27169;&#24335;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20887;&#20313;&#27169;&#24335;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#25366;&#25496;&#25552;&#20379;&#20102;&#20998;&#26512;&#20449;&#24687;&#31995;&#32479;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#20107;&#20214;&#26085;&#24535;&#30340;&#26041;&#27861;&#12290;&#23427;&#25903;&#25345;&#20174;&#21307;&#30103;&#20445;&#20581;&#12289;&#21046;&#36896;&#19994;&#21040;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#30340;&#36807;&#31243;&#35774;&#35745;&#12289;&#39564;&#35777;&#21644;&#25191;&#34892;&#12290;&#20026;&#20102;&#25506;&#32034;&#20855;&#26377;&#22823;&#37327;&#34892;&#20026;&#21487;&#21464;&#24615;&#30340;&#28789;&#27963;&#27969;&#31243;&#30340;&#35268;&#24459;&#24615;&#65292;&#24314;&#35758;&#25366;&#25496;&#20849;&#21516;&#25551;&#36848;&#24213;&#23618;&#27969;&#31243;&#30340;&#37325;&#22797;&#34892;&#20026;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34892;&#20026;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#27169;&#24335;&#20505;&#36873;&#20154;&#26102;&#21482;&#37319;&#29992;&#20102;&#22686;&#37327;&#35745;&#31639;&#65292;&#20294;&#22312;&#35780;&#20272;&#20854;&#36136;&#37327;&#26102;&#21364;&#27809;&#26377;&#20351;&#29992;&#22686;&#37327;&#35745;&#31639;&#65292;&#22240;&#27492;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#25366;&#25496;&#27169;&#24335;&#30340;&#36807;&#31243;&#20998;&#26512;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#33719;&#24471;&#20102;&#22823;&#37327;&#20887;&#20313;&#27169;&#24335;&#65292;&#23548;&#33268;&#25928;&#26524;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#20998;&#26512;&#22797;&#26434;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Process mining provides methods to analyse event logs generated by information systems during the execution of processes. It thereby supports the design, validation, and execution of processes in domains ranging from healthcare, through manufacturing, to e-commerce. To explore the regularities of flexible processes that show a large behavioral variability, it was suggested to mine recurrent behavioral patterns that jointly describe the underlying process. Existing approaches to behavioral pattern mining, however, suffer from two limitations. First, they show limited scalability as incremental computation is incorporated only in the generation of pattern candidates, but not in the evaluation of their quality. Second, process analysis based on mined patterns shows limited effectiveness due to an overwhelmingly large number of patterns obtained in practical application scenarios, many of which are redundant. In this paper, we address these limitations to facilitate the analysis of complex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;IMU&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#29616;&#20934;&#30830;&#19988;&#31283;&#23450;&#30340;&#35782;&#21035;&#65292;&#20026;&#24247;&#22797;&#20030;&#25514;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.02910</link><description>&lt;p&gt;
DS-MS-TCN: &#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;IMU&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#29616;&#20934;&#30830;&#19988;&#31283;&#23450;&#30340;&#35782;&#21035;&#65292;&#20026;&#24247;&#22797;&#20030;&#25514;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Otago&#36816;&#21160;&#35745;&#21010;&#26159;&#38024;&#23545;&#32769;&#24180;&#20154;&#30340;&#37325;&#35201;&#24247;&#22797;&#20030;&#25514;&#65292;&#26088;&#22312;&#22686;&#24378;&#24179;&#34913;&#21644;&#21147;&#37327;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#65292;&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#35782;&#21035;Otago&#20307;&#25805;&#21160;&#20316;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#30740;&#31350;&#22312;&#23454;&#39564;&#23460;&#35774;&#32622;&#20013;&#25307;&#21215;&#20102;36&#21517;&#32769;&#24180;&#20154;&#65292;&#24182;&#23545;&#39069;&#22806;&#25307;&#21215;&#30340;7&#21517;&#32769;&#24180;&#20154;&#36827;&#34892;&#20102;&#23478;&#24237;&#35780;&#20272;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;(DS-MS-TCN)&#65292;&#29992;&#20110;&#20004;&#32423;&#24207;&#21015;&#21040;&#24207;&#21015;&#20998;&#31867;&#65292;&#23558;&#20854;&#32435;&#20837;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#27169;&#22411;&#19987;&#27880;&#20110;&#35782;&#21035;&#27599;&#20010;&#20307;&#25805;&#21160;&#20316;&#30340;&#37325;&#22797;&#27425;&#25968;(&#24494;&#26631;&#31614;)&#12290;&#38543;&#21518;&#30340;&#38454;&#27573;&#25193;&#23637;&#20102;&#35782;&#21035;&#33539;&#22260;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete ra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#25104;&#21151;&#22797;&#21046;&#20102;&#20154;&#20307;&#32920;&#37096;&#38459;&#25239;&#35782;&#21035;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#31283;&#23450;&#32920;&#20851;&#33410;&#36816;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20154;&#20307;&#26356;&#39640;&#30340;&#38459;&#25239;&#65292;&#20026;&#34394;&#25311;&#29615;&#22659;&#27169;&#25311;&#22312;&#31070;&#32463;&#26426;&#26800;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.02904</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#20154;&#32920;&#25968;&#23383;&#23402;&#29983;&#20013;&#22797;&#21046;&#38459;&#25239;&#35782;&#21035;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#25104;&#21151;&#22797;&#21046;&#20102;&#20154;&#20307;&#32920;&#37096;&#38459;&#25239;&#35782;&#21035;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#31283;&#23450;&#32920;&#20851;&#33410;&#36816;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20154;&#20307;&#26356;&#39640;&#30340;&#38459;&#25239;&#65292;&#20026;&#34394;&#25311;&#29615;&#22659;&#27169;&#25311;&#22312;&#31070;&#32463;&#26426;&#26800;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#30340;&#20840;&#26032;&#20154;&#20307;&#36816;&#21160;&#20223;&#30495;&#24179;&#21488;MyoSuite&#65292;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#22797;&#21046;&#20102;&#20154;&#20307;&#31070;&#32463;&#26426;&#26800;&#23454;&#39564;&#30340;&#20808;&#39537;&#24615;&#24037;&#20316;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#19978;&#22797;&#21046;&#22810;&#31181;&#31867;&#22411;&#30340;&#20154;&#20307;&#32920;&#37096;&#38459;&#25239;&#35782;&#21035;&#23454;&#39564;&#65292;&#23558;&#30001;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25511;&#21046;&#30340;&#32920;&#20851;&#33410;&#36816;&#21160;&#19982;&#23454;&#38469;&#20154;&#20307;&#32920;&#20851;&#33410;&#22312;&#25197;&#30697;&#24178;&#25200;&#23454;&#39564;&#20013;&#35782;&#21035;&#21040;&#30340;&#38459;&#25239;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#31283;&#23450;&#30446;&#26631;&#32920;&#20851;&#33410;&#36816;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20154;&#20307;&#26356;&#39640;&#30340;&#32920;&#20851;&#33410;&#38459;&#25239;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#26356;&#30701;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#26356;&#22909;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25506;&#32034;&#20102;&#34394;&#25311;&#29615;&#22659;&#27169;&#25311;&#22312;&#31070;&#32463;&#26426;&#26800;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#65292;&#20026;&#20256;&#32479;&#23454;&#39564;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21021;&#27493;&#20294;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a pioneering effort to replicate human neuromechanical experiments within a virtual environment utilising a digital human model. By employing MyoSuite, a state-of-the-art human motion simulation platform enhanced by Reinforcement Learning (RL), multiple types of impedance identification experiments of human elbow were replicated on a musculoskeletal model. We compared the elbow movement controlled by an RL agent with the motion of an actual human elbow in terms of the impedance identified in torque-perturbation experiments. The findings reveal that the RL agent exhibits higher elbow impedance to stabilise the target elbow motion under perturbation than a human does, likely due to its shorter reaction time and superior sensory capabilities. This study serves as a preliminary exploration into the potential of virtual environment simulations for neuromechanical research, offering an initial yet promising alternative to conventional experimental approaches. An RL-contro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#37197;&#32622;&#65292;&#24182;&#25506;&#31350;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#23545;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#37197;&#32622;&#23545;&#35805;&#32773;&#23637;&#31034;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#36827;&#19968;&#27493;&#29702;&#35299;LLMs&#20043;&#38388;&#22522;&#20110;&#23545;&#35805;&#30340;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#24378;&#35843;&#20102;&#25171;&#36896;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31867;&#20154;&#21270;LLM&#26234;&#33021;&#20307;&#30340;&#26032;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.02896</link><description>&lt;p&gt;
LLM&#26234;&#33021;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#27979;&#37327;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#20316;&#29992;&#20013;&#30340;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#37197;&#32622;&#65292;&#24182;&#25506;&#31350;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#23545;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#37197;&#32622;&#23545;&#35805;&#32773;&#23637;&#31034;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#36827;&#19968;&#27493;&#29702;&#35299;LLMs&#20043;&#38388;&#22522;&#20110;&#23545;&#35805;&#30340;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#24378;&#35843;&#20102;&#25171;&#36896;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31867;&#20154;&#21270;LLM&#26234;&#33021;&#20307;&#30340;&#26032;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#21644;&#20010;&#24615;&#21270;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30740;&#31350;&#20013;&#37117;&#26159;&#28909;&#38376;&#35805;&#39064;&#65292;&#20294;&#23545;&#35821;&#35328;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#20010;&#24615;&#21270;&#30340;LLM&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#24433;&#21709;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#36825;&#26679;&#30340;&#21162;&#21147;&#23545;&#20110;&#30830;&#20445;&#26234;&#33021;&#20307;&#22312;&#20445;&#25345;&#20854;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#33021;&#22815;&#36827;&#34892;&#24320;&#25918;&#30340;&#33258;&#28982;&#23545;&#35805;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#23558;GPT-3.5&#35843;&#33410;&#21040;&#20010;&#24615;&#21270;&#37197;&#32622;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#30340;&#21464;&#24322;&#24615;&#24341;&#23548;&#25277;&#26679;&#31639;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#21452;&#32452;&#32676;&#30340;LLM&#26234;&#33021;&#20307;&#20154;&#21475;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20010;&#24615;&#21270;&#27979;&#35797;&#65292;&#24182;&#23558;&#26234;&#33021;&#20307;&#25552;&#20132;&#21040;&#21327;&#21516;&#20889;&#20316;&#20219;&#21153;&#65292;&#21457;&#29616;&#19981;&#21516;&#37197;&#32622;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#20043;&#38388;&#22522;&#20110;&#23545;&#35805;&#30340;&#30456;&#20114;&#20316;&#29992;&#22880;&#23450;&#22522;&#30784;&#65292;&#24182;&#20984;&#26174;&#20102;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#25171;&#36896;&#26356;&#24378;&#22823;&#12289;&#26356;&#31867;&#20154;&#30340;LLM&#26234;&#33021;&#20307;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM pers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20998;&#25955;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;DEAI&#65289;&#39046;&#22495;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#20174;&#24213;&#23618;&#24320;&#22987;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;DEAI&#35299;&#20915;&#26041;&#26696;&#21644;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02885</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#27169;&#22359;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on Building Blocks of Decentralized Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20998;&#25955;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;DEAI&#65289;&#39046;&#22495;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#20174;&#24213;&#23618;&#24320;&#22987;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;DEAI&#35299;&#20915;&#26041;&#26696;&#21644;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#25216;&#26415;&#36827;&#27493;&#21644;&#20174;&#23398;&#26415;&#21644;&#29702;&#35770;&#39046;&#22495;&#21521;&#29616;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#24180;&#22797;&#19968;&#24180;&#21152;&#36895;&#12290;&#20294;&#22312;&#36825;&#20010;&#36827;&#31243;&#21644;&#36716;&#21464;&#20013;&#65292;&#19968;&#20123;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#30097;&#38382;&#38656;&#35201;&#24471;&#21040;&#35299;&#20915;&#65292;&#20197;&#23454;&#29616;&#35813;&#39046;&#22495;&#30340;&#36947;&#24503;&#21457;&#23637;&#65292;&#20363;&#22914;&#25968;&#23383;&#38544;&#31169;&#12289;&#25152;&#26377;&#26435;&#21644;&#25511;&#21046;&#12290;&#36825;&#20123;&#26159;&#30446;&#21069;&#26368;&#27969;&#34892;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#8212;&#8212;&#38598;&#20013;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;CEAI&#65289;&#23384;&#22312;&#38382;&#39064;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#20854;&#20182;&#26041;&#21521;&#20063;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#22914;&#20998;&#25955;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;DEAI&#65289;&#65292;&#20197;&#35299;&#20915;&#19968;&#20123;&#26368;&#24191;&#27867;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;DEAI&#39046;&#22495;&#30340;&#29616;&#26377;&#24037;&#20316;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;71&#20010;&#30740;&#31350;&#30340;&#21457;&#29616;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#35782;&#21035;DEAI&#35299;&#20915;&#26041;&#26696;&#21644;&#32593;&#32476;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#37319;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#27861;&#36827;&#34892;DEAI&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is transforming our lives, and technological progress and transfer from the academic and theoretical sphere to the real world are accelerating yearly. But during that progress and transition, several open problems and questions need to be addressed for the field to develop ethically, such as digital privacy, ownership, and control. These are some of the reasons why the currently most popular approaches of artificial intelligence, i.e., centralized AI (CEAI), are questionable, with other directions also being widely explored, such as decentralized artificial intelligence (DEAI), to solve some of the most reaching problems. This paper provides a systematic literature review (SLR) of existing work in the field of DEAI, presenting the findings of 71 identified studies. The paper's primary focus is identifying the building blocks of DEAI solutions and networks, tackling the DEAI analysis from a bottom-up approach. In the end, future directions of research and open pr
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#30693;&#35782;&#26469;&#28304;&#65288;PubMed&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;&#35895;&#27468;&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02844</link><description>&lt;p&gt;
&#23545;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#30340;&#30693;&#35782;&#26469;&#28304;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing Knowledge Sources for Open-Domain Scientific Claim Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02844
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#30693;&#35782;&#26469;&#28304;&#65288;PubMed&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;&#35895;&#27468;&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#30693;&#35782;&#30340;&#21457;&#29616;&#36895;&#24230;&#21644;&#22312;&#32447;&#20581;&#24247;&#20027;&#24352;&#30340;&#20998;&#20139;&#22686;&#21152;&#65292;&#20984;&#26174;&#20102;&#20026;&#31185;&#23398;&#20027;&#24352;&#24320;&#21457;&#39640;&#25928;&#20107;&#23454;&#26816;&#26680;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#36825;&#39033;&#20219;&#21153;&#30340;&#24120;&#35265;&#35774;&#32622;&#20551;&#35774;&#24050;&#32463;&#25552;&#20379;&#24182;&#27880;&#37322;&#25110;&#21253;&#21547;&#22312;&#26377;&#38480;&#35821;&#26009;&#24211;&#20013;&#30340;&#21253;&#21547;&#35777;&#25454;&#30340;&#25991;&#26723;&#12290;&#36825;&#20351;&#24471;&#35813;&#31995;&#32479;&#22312;&#21487;&#33021;&#38656;&#35201;&#26597;&#35810;&#25968;&#30334;&#19975;&#20010;&#25991;&#26723;&#20197;&#25214;&#21040;&#30456;&#20851;&#35777;&#25454;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#27979;&#35797;&#24320;&#25918;&#39046;&#22495;&#20027;&#24352;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#27979;&#35797;&#20102;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#20027;&#24352;&#31995;&#32479;&#30340;&#26368;&#32456;&#21028;&#26029;&#39044;&#27979;&#12290;&#22312;&#20445;&#25345;&#27969;&#27700;&#32447;&#30340;&#35777;&#25454;&#36873;&#25321;&#21644;&#21028;&#26029;&#39044;&#27979;&#37096;&#20998;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#19977;&#31181;&#24120;&#35265;&#30693;&#35782;&#26469;&#28304;&#65288;PubMed&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;&#35895;&#27468;&#65289;&#21644;&#20004;&#31181;&#19981;&#21516;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#36827;&#34892;&#25991;&#26723;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline's evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different informati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#20010;&#20840;&#38754;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#20154;&#31867;&#20083;&#22836;&#30244;&#30149;&#27602;&#29983;&#27542;&#22120;&#30115;&#12290;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02826</link><description>&lt;p&gt;
SynthVision - &#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#26368;&#22823;&#21270;&#20135;&#20986;&#30340;&#26368;&#23567;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
SynthVision - Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#20010;&#20840;&#38754;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#20154;&#31867;&#20083;&#22836;&#30244;&#30149;&#27602;&#29983;&#27542;&#22120;&#30115;&#12290;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#23545;&#32039;&#24613;&#21307;&#30103;&#21361;&#26426;&#65288;&#22914;&#27969;&#34892;&#30149;&#25110;&#29983;&#29289;&#24656;&#24598;&#20027;&#20041;&#20107;&#20214;&#65289;&#26102;&#65292;&#24555;&#36895;&#24320;&#21457;&#30142;&#30149;&#26816;&#27979;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#20110;&#36825;&#20123;&#22330;&#26223;&#26469;&#35828;&#22826;&#24930;&#20102;&#65292;&#38656;&#35201;&#21019;&#26032;&#26041;&#27861;&#20174;&#24456;&#23569;&#30340;&#25968;&#25454;&#20013;&#24555;&#36895;&#29983;&#25104;&#21487;&#38752;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26469;&#26816;&#27979;&#20154;&#31867;&#20083;&#22836;&#30244;&#30149;&#27602;&#29983;&#27542;&#22120;&#30115;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#29992;&#20110;&#20174;10&#24352;HPV&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#22823;&#37327;&#22810;&#26679;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#19987;&#27880;&#20110;&#20934;&#30830;&#25551;&#32472;&#29983;&#27542;&#22120;&#30115;&#12290;&#31532;&#20108;&#38454;&#27573;&#28041;&#21450;&#20351;&#29992;&#35813;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#25193;&#25955;&#27169;&#22411;&#22312;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism. However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data. We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data. In our study, we employed a two phase experimental design using diffusion models. In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts. The second phase involved the training and testing vision model using this synthetic dataset. This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25351;&#20986;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#26377;&#24847;&#27745;&#26579;&#26102;&#23384;&#22312;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65288;EAL&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#19988;&#36867;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02823</link><description>&lt;p&gt;
&#36867;&#36991;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#65288;&#22826;&#65289;&#23481;&#26131;
&lt;/p&gt;
&lt;p&gt;
Evading Data Contamination Detection for Language Models is (too) Easy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25351;&#20986;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#26377;&#24847;&#27745;&#26579;&#26102;&#23384;&#22312;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65288;EAL&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#19988;&#36867;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#65292;&#23427;&#20204;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#32463;&#24120;&#25351;&#23548;&#29992;&#25143;&#23545;&#19968;&#20010;&#27169;&#22411;&#19982;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25152;&#35757;&#32451;&#30340;&#22823;&#37327;&#25968;&#25454;&#21487;&#33021;&#20250;&#24847;&#22806;&#22320;&#19982;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#21457;&#29983;&#27745;&#26579;&#65292;&#20174;&#32780;&#25439;&#23475;&#24615;&#33021;&#35780;&#20272;&#12290;&#23613;&#31649;&#26368;&#36817;&#24320;&#21457;&#20102;&#19968;&#20123;&#27745;&#26579;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#26377;&#24847;&#36827;&#34892;&#27745;&#26579;&#20197;&#36991;&#20813;&#34987;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#24773;&#20917;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#23545;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#30340;&#21487;&#20449;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#26356;&#20005;&#26684;&#22320;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#27745;&#26579;&#26816;&#27979;&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#36825;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#28431;&#27934;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;EAL&#36825;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65292;&#26126;&#26174;&#25552;&#39640;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#65292;&#24182;&#23436;&#20840;&#36867;&#36991;&#20102;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.02805</link><description>&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20013;&#30340;&#22270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Large Language Models in Asynchronous Plan Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39034;&#24207;&#21644;&#24182;&#34892;&#35268;&#21010;&#20197;&#20248;&#21270;&#26102;&#38388;&#25104;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25104;&#21151;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#38381;&#28304;&#21644;&#24320;&#28304;LLMs&#65292;&#21253;&#25324;GPT-4&#21644;LLaMA-2&#65292;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;AsyncHow&#20013;&#65292;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#30340;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;PLaG&#33021;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#36973;&#21463;&#20005;&#37325;&#38477;&#32423;&#65292;&#31361;&#20986;&#20102;&#21033;&#29992;LLMs&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#35270;&#20026;&#23558;LLMs&#29992;&#20316;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#27169;&#22411;&#22312;&#21307;&#23398;&#25968;&#25454;&#35821;&#20041;&#29702;&#35299;&#21644;&#26032;&#24739;&#32773;&#22788;&#26041;&#25512;&#33616;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02803</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#33647;&#29289;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Distilling Medication Recommendation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#27169;&#22411;&#22312;&#21307;&#23398;&#25968;&#25454;&#35821;&#20041;&#29702;&#35299;&#21644;&#26032;&#24739;&#32773;&#22788;&#26041;&#25512;&#33616;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#23427;&#26681;&#25454;&#24739;&#32773;&#29305;&#23450;&#30340;&#20581;&#24247;&#38656;&#27714;&#26469;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#33647;&#29289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#24448;&#24448;&#24573;&#35270;&#21307;&#23398;&#25968;&#25454;&#30340;&#32454;&#24494;&#35821;&#20041;&#65292;&#32780;&#21482;&#26159;&#36807;&#24230;&#20381;&#36182;&#26631;&#35782;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#39318;&#27425;&#35775;&#38382;&#21307;&#38498;&#30340;&#24739;&#32773;&#30340;&#24773;&#20917;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#20043;&#21069;&#30340;&#22788;&#26041;&#21382;&#21490;&#21487;&#20197;&#21442;&#32771;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#36755;&#20837;&#19981;&#21487;&#30693;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;LLMs&#25913;&#36827;&#29616;&#26377;&#30340;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#33647;&#29289;&#25512;&#33616;&#65288;LEADER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#21512;&#36866;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#20351;LLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#33616;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the
&lt;/p&gt;</description></item><item><title>KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.02801</link><description>&lt;p&gt;
KS-Lottery: &#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#35777;&#24425;&#31080;
&lt;/p&gt;
&lt;p&gt;
KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02801
&lt;/p&gt;
&lt;p&gt;
KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#31080;&#31080;&#35777;&#20551;&#35828;&#35748;&#20026;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#8220;&#20013;&#22870;&#31080;&#8221;&#12290;&#22312;&#24494;&#35843;&#22330;&#26223;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#20013;&#22870;&#31080;&#65311;&#25105;&#20204;&#22914;&#20309;&#25214;&#21040;&#36825;&#26679;&#30340;&#20013;&#22870;&#31080;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KS-Lottery&#65292;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22312;&#22810;&#35821;&#35328;&#24494;&#35843;&#20013;&#39640;&#24230;&#26377;&#25928;&#30340;LLM&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#24494;&#35843;&#21069;&#21518;&#21442;&#25968;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#29702;&#35770;&#35777;&#26126;&#20102;KS-Lottery&#21487;&#20197;&#22312;&#23884;&#20837;&#23618;&#20013;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#65292;&#24494;&#35843;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#20445;&#35777;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#23558;KS-Lottery&#19982;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KS-Lottery&#25214;&#21040;&#20102;&#19968;&#20010;&#26356;&#23567;&#30340;&#21442;&#25968;&#38598;&#26469;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#36798;&#21040;&#20102;&#19982;&#20840;&#38754;&#24494;&#35843;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;18&#20010;&#26631;&#35760;&#30340;&#23884;&#20837;&#23618;
&lt;/p&gt;
&lt;p&gt;
The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02791</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Optimization and Architecture for Tiny Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#21147;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#32454;&#33410;&#24456;&#23569;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;10&#20159;&#21442;&#25968;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#32463;&#39564;&#30740;&#31350;&#26469;&#20998;&#26512;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#31070;&#32463;&#26550;&#26500;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#22810;&#20010;&#35774;&#35745;&#20844;&#24335;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#24615;&#22320;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#65292;&#21253;&#25324;&#20998;&#35789;&#22120;&#21387;&#32553;&#12289;&#26550;&#26500;&#35843;&#25972;&#12289;&#21442;&#25968;&#32487;&#25215;&#21644;&#22810;&#36718;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;1.6T&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;PanGu-$\pi$-1B Pro&#21644;PanGu-$\pi$-1.5B Pro&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#26102;&#24207;&#24179;&#22343;&#30693;&#35782;&#33976;&#39311;&#21644;&#22686;&#24378;&#23884;&#20837;&#29305;&#24449;&#33976;&#39311;&#26469;&#31283;&#23450;&#22320;&#33976;&#39311;&#30693;&#35782;&#24182;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02781</link><description>&lt;p&gt;
&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#39640;&#25928;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dual Knowledge Distillation for Efficient Sound Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#26102;&#24207;&#24179;&#22343;&#30693;&#35782;&#33976;&#39311;&#21644;&#22686;&#24378;&#23884;&#20837;&#29305;&#24449;&#33976;&#39311;&#26469;&#31283;&#23450;&#22320;&#33976;&#39311;&#30693;&#35782;&#24182;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#65288;SED&#65289;&#23545;&#20110;&#35782;&#21035;&#29305;&#23450;&#22768;&#38899;&#21450;&#20854;&#22312;&#22768;&#23398;&#20449;&#21495;&#20013;&#30340;&#26102;&#38388;&#20301;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#22312;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#21464;&#24471;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#65292;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;SED&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#20197;&#26102;&#24207;&#24179;&#22343;&#30693;&#35782;&#33976;&#39311;&#65288;TAKD&#65289;&#20026;&#24320;&#31471;&#65292;&#21033;&#29992;&#20174;&#23398;&#29983;&#27169;&#22411;&#21442;&#25968;&#30340;&#26102;&#24207;&#24179;&#22343;&#24471;&#21040;&#30340;&#24179;&#22343;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#38388;&#25509;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#23398;&#20064;&#65292;&#30830;&#20445;&#31283;&#23450;&#30340;&#30693;&#35782;&#33976;&#39311;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22686;&#24378;&#23884;&#20837;&#29305;&#24449;&#33976;&#39311;&#65288;EEFD&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#22312;&#23398;&#29983;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#23884;&#20837;&#33976;&#39311;&#23618;&#26469;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;DCASE 2023&#20219;&#21153;4A&#20844;&#20849;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;SED&#31995;&#32479;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound event detection (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals. This becomes challenging particularly for on-device applications, where computational resources are limited. To address this issue, we introduce a novel framework referred to as dual knowledge distillation for developing efficient SED systems in this work. Our proposed dual knowledge distillation commences with temporal-averaging knowledge distillation (TAKD), utilizing a mean student model derived from the temporal averaging of the student model's parameters. This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable knowledge distillation. Subsequently, we introduce embedding-enhanced feature distillation (EEFD), which involves incorporating an embedding distillation layer within the student model to bolster contextual learning. On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual knowle
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.02769</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;: &#26131;&#20110;&#27169;&#20223;&#30340;&#21487;&#25512;&#24191;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02769
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21363;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;Learning from Teaching&#65292;&#31616;&#31216;LoT&#65289;&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#12290;&#21463;&#21040;&#20154;&#31867;&#25429;&#25417;&#31616;&#26126;&#25277;&#35937;&#27169;&#24335;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#25512;&#24191;&#30340;&#20851;&#31995;&#26356;&#23481;&#26131;&#25945;&#25480;&#12290;LoT&#36890;&#36807;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#27010;&#24565;&#65292;&#36890;&#36807;&#25552;&#20379;&#21453;&#39304;&#26469;&#35757;&#32451;&#20027;&#27169;&#22411;&#21644;&#25913;&#36827;&#20027;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#26356;&#22810;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24341;&#20837;LoT&#30456;&#27604;&#20165;&#22312;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#12290;&#36825;&#34920;&#26126;&#20102;LoT&#22312;&#35782;&#21035;&#21487;&#25512;&#24191;&#20449;&#24687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#22270;&#27010;&#35201;&#21644;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#20852;&#36890;&#20449;&#23454;&#29616;&#24212;&#29992;&#31243;&#24207;&#30340;&#24847;&#22270;&#27010;&#35201;&#65292;&#35299;&#20915;&#20102;&#24212;&#29992;&#31243;&#24207;&#19982;&#32593;&#32476;&#20043;&#38388;&#22797;&#26434;&#30340;&#36890;&#20449;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02768</link><description>&lt;p&gt;
&#24847;&#22270;&#27010;&#35201;&#21644;&#36890;&#36807;&#26032;&#20852;&#36890;&#20449;&#36827;&#34892;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Intent Profiling and Translation Through Emergent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#22270;&#27010;&#35201;&#21644;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#20852;&#36890;&#20449;&#23454;&#29616;&#24212;&#29992;&#31243;&#24207;&#30340;&#24847;&#22270;&#27010;&#35201;&#65292;&#35299;&#20915;&#20102;&#24212;&#29992;&#31243;&#24207;&#19982;&#32593;&#32476;&#20043;&#38388;&#22797;&#26434;&#30340;&#36890;&#20449;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#34920;&#36798;&#21644;&#28385;&#36275;&#32593;&#32476;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#31649;&#29702;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#20102;&#12290;&#22312;&#22522;&#20110;&#24847;&#22270;&#30340;&#26041;&#27861;&#20013;&#65292;&#29992;&#25143;&#21644;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#39640;&#32423;&#25277;&#35937;&#35821;&#35328;&#26469;&#34920;&#36798;&#20182;&#20204;&#30340;&#24847;&#22270;&#12290;&#23613;&#31649;&#36825;&#31181;&#25277;&#35937;&#31616;&#21270;&#20102;&#32593;&#32476;&#25805;&#20316;&#65292;&#20294;&#23427;&#23545;&#20110;&#26377;&#25928;&#22320;&#34920;&#36798;&#24212;&#29992;&#31243;&#24207;&#30340;&#24847;&#22270;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#19981;&#21516;&#30340;&#32593;&#32476;&#33021;&#21147;&#25552;&#20986;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#22270;&#27010;&#35201;&#21644;&#32763;&#35793;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#19968;&#20010;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;&#19982;&#32593;&#32476;&#20132;&#20114;&#30340;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#20182;&#20204;&#30340;&#39046;&#22495;&#35821;&#35328;&#26469;&#34920;&#36798;&#20182;&#20204;&#23545;&#32593;&#32476;&#26381;&#21153;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#65288;&#21363;&#24212;&#29992;&#31243;&#24207;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#36890;&#20449;&#65289;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#29702;&#35299;&#27599;&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#39046;&#22495;&#35821;&#35328;&#65292;&#36825;&#26082;&#19981;&#23454;&#38469;&#20063;&#19981;&#21487;&#20280;&#32553;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26032;&#20852;&#36890;&#20449;&#30340;&#24847;&#22270;&#27010;&#35201;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#30340;&#24847;&#22270;&#27010;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
To effectively express and satisfy network application requirements, intent-based network management has emerged as a promising solution. In intent-based methods, users and applications express their intent in a high-level abstract language to the network. Although this abstraction simplifies network operation, it induces many challenges to efficiently express applications' intents and map them to different network capabilities. Therefore, in this work, we propose an AI-based framework for intent profiling and translation. We consider a scenario where applications interacting with the network express their needs for network services in their domain language. The machine-to-machine communication (i.e., between applications and the network) is complex since it requires networks to learn how to understand the domain languages of each application, which is neither practical nor scalable. Instead, a framework based on emergent communication is proposed for intent profiling, in which applica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21015;&#34920;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#24207;-&#25130;&#26029;&#32852;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25628;&#32034;&#21644;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25429;&#25417;&#21015;&#34920;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#26469;&#36820;&#22238;&#26356;&#22909;&#30340;&#21015;&#34920;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#35270;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#20294;&#36825;&#31181;&#20998;&#31163;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20449;&#24687;&#20849;&#20139;&#21644;&#38169;&#35823;&#31215;&#32047;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02764</link><description>&lt;p&gt;
&#22522;&#20110;&#21015;&#34920;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#24207;-&#25130;&#26029;&#32852;&#21512;&#27169;&#22411;&#29992;&#20110;&#25628;&#32034;&#21644;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21015;&#34920;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#24207;-&#25130;&#26029;&#32852;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25628;&#32034;&#21644;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25429;&#25417;&#21015;&#34920;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#26469;&#36820;&#22238;&#26356;&#22909;&#30340;&#21015;&#34920;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#35270;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#20294;&#36825;&#31181;&#20998;&#31163;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20449;&#24687;&#20849;&#20139;&#21644;&#38169;&#35823;&#31215;&#32047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#30340;&#32467;&#26524;&#36890;&#24120;&#20197;&#25490;&#21517;&#21015;&#34920;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20363;&#22914;&#38754;&#21521;&#20154;&#31867;&#30340;&#32593;&#32476;&#25628;&#32034;&#21644;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#21015;&#34920;&#24863;&#30693;&#26816;&#32034;&#26088;&#22312;&#25429;&#25417;&#21015;&#34920;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#36820;&#22238;&#26356;&#22909;&#30340;&#21015;&#34920;&#65292;&#20027;&#35201;&#21253;&#25324;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#12290;&#37325;&#26032;&#25490;&#24207;&#23545;&#21015;&#34920;&#20013;&#30340;&#25991;&#26723;&#36827;&#34892;&#31934;&#32454;&#37325;&#26032;&#35780;&#20998;&#12290;&#25130;&#26029;&#21160;&#24577;&#30830;&#23450;&#25490;&#21517;&#21015;&#34920;&#30340;&#25130;&#26029;&#28857;&#65292;&#20197;&#22312;&#25972;&#20307;&#30456;&#20851;&#24615;&#21644;&#36991;&#20813;&#26080;&#20851;&#25991;&#26723;&#30340;&#38169;&#35823;&#20449;&#24687;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#35270;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#24182;&#20998;&#21035;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#65292;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#31163;&#26159;&#19981;&#29702;&#24819;&#30340;&#12290;&#39318;&#20808;&#65292;&#24456;&#38590;&#22312;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#25490;&#21517;&#21015;&#34920;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#29420;&#31435;&#30340;&#27969;&#27700;&#32447;&#36890;&#24120;&#20250;&#36935;&#21040;&#38169;&#35823;&#31215;&#32047;&#38382;&#39064;&#65292;&#21363;&#37325;&#26032;&#25490;&#24207;&#38454;&#27573;&#30340;&#23567;&#38169;&#35823;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#25130;&#26029;&#38454;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;...&#65288;&#32487;&#32493;&#25551;&#36848;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02733</link><description>&lt;p&gt;
ToonAging: &#33402;&#26415;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#19979;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;
&lt;/p&gt;
&lt;p&gt;
ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#36870;&#40836;&#21270;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#30005;&#24433;&#12289;&#24191;&#21578;&#21644;&#30452;&#25773;&#31561;&#36924;&#30495;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;&#20154;&#33080;&#36870;&#40836;&#21270;&#24212;&#29992;&#20110;&#38750;&#36924;&#30495;&#22270;&#20687;&#65292;&#22914;&#28459;&#30011;&#12289;&#25554;&#22270;&#21644;&#21160;&#30011;&#65292;&#22312;&#21508;&#31181;&#23089;&#20048;&#34892;&#19994;&#20013;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26080;&#32541;&#32534;&#36753;NPR&#22270;&#20687;&#19978;&#26174;&#29616;&#24180;&#40836;&#30340;&#32593;&#32476;&#24847;&#21619;&#30528;&#36825;&#20123;&#20219;&#21153;&#19968;&#30452;&#23616;&#38480;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#26041;&#27861;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#19981;&#24841;&#24555;&#30340;&#20266;&#24433;&#21644;&#30001;&#20110;&#22495;&#24046;&#24322;&#32780;&#20002;&#22833;&#38754;&#37096;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#38454;&#27573;&#20154;&#33080;&#36870;&#40836;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#65292;&#22312;&#19968;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#23436;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#20004;&#32773;&#37117;&#22312;&#30456;&#21516;&#30340;PR&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#27599;&#20010;&#21521;&#37327;&#36127;&#36131;&#31649;&#29702;&#19982;&#34928;&#32769;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#40657;&#30418;&#26367;&#20195;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26679;&#26412;&#20998;&#24067;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#29992;&#20110;&#25915;&#20987;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.02732</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#40657;&#30418;&#26367;&#20195;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Generative Approach to Surrogate-based Black-box Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#40657;&#30418;&#26367;&#20195;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26679;&#26412;&#20998;&#24067;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#29992;&#20110;&#25915;&#20987;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#40657;&#30418;&#26367;&#20195;&#25915;&#20987;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#19968;&#32452;&#26679;&#26412;&#65292;&#36890;&#36807;&#40657;&#30418;&#30446;&#26631;&#21453;&#39304;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#25915;&#20987;&#26041;&#27861;&#26159;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#37492;&#21035;&#24615;&#26367;&#20195;&#27169;&#22411;&#26469;&#27169;&#25311;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#23398;&#20064;&#30446;&#26631;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#28982;&#21518;&#65292;&#30333;&#30418;&#25915;&#20987;&#38024;&#23545;&#26367;&#20195;&#27169;&#22411;&#29983;&#25104;&#19982;&#21407;&#22987;&#26679;&#26412;&#30456;&#20284;&#20294;&#23646;&#20110;&#20854;&#20182;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#37492;&#21035;&#24615;&#26367;&#20195;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#23398;&#20064;&#30446;&#26631;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#22240;&#27492;&#36825;&#20123;&#26367;&#20195;&#25915;&#20987;&#26041;&#27861;&#30340;&#25104;&#21151;&#29575;&#36739;&#20302;&#12290;&#19982;&#37492;&#21035;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#30446;&#26631;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#25110;&#30456;&#37051;&#26679;&#26412;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#30340;&#20998;&#24067;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate-based black-box attacks have exposed the heightened vulnerability of DNNs. These attacks are designed to craft adversarial examples for any samples with black-box target feedback for only a given set of samples. State-of-the-art surrogate-based attacks involve training a discriminative surrogate that mimics the target's outputs. The goal is to learn the decision boundaries of the target. The surrogate is then attacked by white-box attacks to craft adversarial examples similar to the original samples but belong to other classes. With limited samples, the discriminative surrogate fails to accurately learn the target's decision boundaries, and these surrogate-based attacks suffer from low success rates. Different from the discriminative approach, we propose a generative surrogate that learns the distribution of samples residing on or close to the target's decision boundaries. The distribution learned by the generative surrogate can be used to craft adversarial examples that have
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#38477;&#22122;&#26102;&#38388;&#21608;&#26399;&#24314;&#27169;&#65288;DiCycle&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#30446;&#26631;&#29289;&#21697;&#39640;&#24230;&#30456;&#20851;&#30340;&#29992;&#25143;&#34892;&#20026;&#23376;&#38598;&#65292;DiCycle&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#27169;&#26102;&#38388;&#21608;&#26399;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02718</link><description>&lt;p&gt;
&#38477;&#22122;&#26102;&#38388;&#21608;&#26399;&#24314;&#27169;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Denoising Time Cycle Modeling for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#38477;&#22122;&#26102;&#38388;&#21608;&#26399;&#24314;&#27169;&#65288;DiCycle&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#30446;&#26631;&#29289;&#21697;&#39640;&#24230;&#30456;&#20851;&#30340;&#29992;&#25143;&#34892;&#20026;&#23376;&#38598;&#65292;DiCycle&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#27169;&#26102;&#38388;&#21608;&#26399;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#26102;&#38388;&#27169;&#24335;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23558;&#19982;&#30446;&#26631;&#29289;&#21697;&#26080;&#20851;&#30340;&#29992;&#25143;&#34892;&#20026;&#23450;&#20041;&#20026;&#22122;&#22768;&#65292;&#36825;&#38480;&#21046;&#20102;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#26102;&#38388;&#21608;&#26399;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#24182;&#24433;&#21709;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#38477;&#22122;&#26102;&#38388;&#21608;&#26399;&#24314;&#27169;&#65288;DiCycle&#65289;&#65292;&#29992;&#20110;&#38477;&#22122;&#29992;&#25143;&#34892;&#20026;&#24182;&#36873;&#25321;&#19982;&#30446;&#26631;&#29289;&#21697;&#39640;&#24230;&#30456;&#20851;&#30340;&#29992;&#25143;&#34892;&#20026;&#23376;&#38598;&#12290;DiCycle&#33021;&#22815;&#26126;&#30830;&#22320;&#24314;&#27169;&#22810;&#26679;&#21270;&#30340;&#26102;&#38388;&#21608;&#26399;&#27169;&#24335;&#20197;&#36827;&#34892;&#25512;&#33616;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;DiCycle&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, modeling temporal patterns of user-item interactions have attracted much attention in recommender systems. We argue that existing methods ignore the variety of temporal patterns of user behaviors. We define the subset of user behaviors that are irrelevant to the target item as noises, which limits the performance of target-related time cycle modeling and affect the recommendation performance. In this paper, we propose Denoising Time Cycle Modeling (DiCycle), a novel approach to denoise user behaviors and select the subset of user behaviors that are highly related to the target item. DiCycle is able to explicitly model diverse time cycle patterns for recommendation. Extensive experiments are conducted on both public benchmarks and a real-world dataset, demonstrating the superior performance of DiCycle over the state-of-the-art recommendation methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#35268;&#21010;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02716</link><description>&lt;p&gt;
&#29702;&#35299;LLM&#20195;&#29702;&#30340;&#35268;&#21010;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the planning of LLM agents: A survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02716
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#35268;&#21010;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#26234;&#33021;&#65292;&#23558;LLM&#29992;&#20316;&#33258;&#20027;&#20195;&#29702;&#30340;&#35268;&#21010;&#27169;&#22359;&#30340;&#36827;&#23637;&#24341;&#36215;&#20102;&#26356;&#22810;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#35268;&#21010;&#30340;&#39318;&#20010;&#31995;&#32479;&#35270;&#35282;&#65292;&#28085;&#30422;&#20102;&#26088;&#22312;&#25552;&#39640;&#35268;&#21010;&#33021;&#21147;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;LLM-&#20195;&#29702;&#35268;&#21010;&#30340;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#35745;&#21010;&#36873;&#25321;&#12289;&#22806;&#37096;&#27169;&#22359;&#12289;&#21453;&#24605;&#21644;&#35760;&#24518;&#12290;&#23545;&#20110;&#27599;&#20010;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#29575;&#65292;&#25512;&#21160;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#30340;&#26222;&#21450;&#21270;&#12290;&#36825;&#31181;&#36827;&#23637;&#21487;&#20197;&#24102;&#26469;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#31561;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02713</link><description>&lt;p&gt;
&#19968;&#31687;&#20301;&#32622;&#35770;&#25991;: &#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26377;&#20160;&#20040;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
Position Paper: What Can Large Language Models Tell Us about Time Series Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02713
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#29575;&#65292;&#25512;&#21160;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#30340;&#26222;&#21450;&#21270;&#12290;&#36825;&#31181;&#36827;&#23637;&#21487;&#20197;&#24102;&#26469;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#31561;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20855;&#22791;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#33021;&#21147;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#21457;&#23637;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#21644;&#22823;&#37327;&#27169;&#22411;&#35843;&#25972;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#30446;&#21069;&#30340;LLM&#20855;&#26377;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#39640;&#25928;&#30340;&#20915;&#31574;&#21644;&#25512;&#36827;&#21521;&#26356;&#26222;&#36866;&#24418;&#24335;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#21457;&#23637;&#12290;&#36825;&#31181;&#36827;&#27493;&#21487;&#20197;&#25171;&#24320;&#21508;&#31181;&#21487;&#33021;&#24615;&#65292;&#21253;&#25324;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#12290;&#25105;&#20204;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#35748;&#35782;&#21040;LLM&#22312;&#25512;&#36827;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#23545;&#36825;&#20123;&#30456;&#20851;&#24037;&#20316;&#30340;&#20449;&#20219;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including modality switching and time series question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#38024;&#23545;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#36827;&#34892;&#20462;&#27491;&#65292;&#20197;&#25552;&#39640;&#21512;&#24182;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02705</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#30340;&#34920;&#24449;&#25163;&#26415;
&lt;/p&gt;
&lt;p&gt;
Representation Surgery for Multi-Task Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#38024;&#23545;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#36827;&#34892;&#20462;&#27491;&#65292;&#20197;&#25552;&#39640;&#21512;&#24182;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#23558;&#22810;&#20010;&#20219;&#21153;&#30340;&#20449;&#24687;&#21387;&#32553;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#39592;&#24178;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30452;&#25509;&#21512;&#24182;&#22810;&#20010;&#29420;&#31435;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#25191;&#34892;MTL&#65292;&#32780;&#19981;&#26159;&#25910;&#38598;&#23427;&#20204;&#30340;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;MTL&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#30340;&#34920;&#31034;&#20998;&#24067;&#65292;&#25105;&#20204;&#21457;&#29616;&#21512;&#24182;&#27169;&#22411;&#24448;&#24448;&#38754;&#20020;&#34920;&#31034;&#20559;&#24046;&#30340;&#22256;&#22659;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#21512;&#24182;&#27169;&#22411;&#19982;&#20010;&#20307;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#20998;&#24067;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#21512;&#24182;MTL&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#21512;&#24182;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;Surgery&#8221;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#23427;&#20197;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#20026;&#36755;&#20837;&#65292;&#24182;&#35797;&#22270;&#36755;&#20986;&#20854;&#20013;&#21253;&#21547;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called "Surgery" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#27979;&#35797;&#29615;&#22659;&#20855;&#26377;&#24178;&#25200;&#22240;&#32032;&#26102;&#24433;&#21709;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#26159;&#20943;&#23569;&#27867;&#21270;&#24046;&#36317;&#26368;&#20851;&#38190;&#30340;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.02701</link><description>&lt;p&gt;
&#29702;&#35299;&#24433;&#21709;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#22240;&#32032;&#65306;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#27979;&#35797;&#29615;&#22659;&#20855;&#26377;&#24178;&#25200;&#22240;&#32032;&#26102;&#24433;&#21709;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#26159;&#20943;&#23569;&#27867;&#21270;&#24046;&#36317;&#26368;&#20851;&#38190;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#21162;&#21147;&#33268;&#21147;&#20110;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#23545;&#36830;&#32493;&#25511;&#21046;&#26377;&#29992;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#31574;&#30053;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#27979;&#35797;&#29615;&#22659;&#21487;&#33021;&#19982;&#35757;&#32451;&#29615;&#22659;&#19981;&#21516;&#65292;&#20363;&#22914;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#23384;&#22312;&#24178;&#25200;&#22240;&#32032;&#12290;&#35768;&#22810;&#23454;&#38469;&#31639;&#27861;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#20013;&#27809;&#26377;&#19968;&#31181;&#31639;&#27861;&#33021;&#22815;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#22240;&#32032;&#20197;&#21450;&#20026;&#20160;&#20040;&#20182;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27979;&#35797;&#29615;&#22659;&#20855;&#26377;&#24178;&#25200;&#22240;&#32032;&#26102;&#29702;&#35770;&#19978;&#22238;&#31572;&#24433;&#21709;&#27867;&#21270;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#65288;&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#65289;&#23545;&#20110;&#20943;&#23569;&#27867;&#21270;&#24046;&#36317;&#30340;&#25928;&#30410;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;DM&#25968;&#25454;&#30340;&#23454;&#35777;&#35777;&#25454;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there are many efforts attempting to learn useful policies for continuous control in visual reinforcement learning (RL). In this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment. Many practical algorithms are proposed to handle this problem. However, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work. In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. Our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap. Our theoretical results are supported by the empirical evidence in the DM
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#24314;&#31435;&#20102;&#19968;&#20010;&#38543;&#26426;&#20248;&#21183;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#25512;&#24191;&#20102;&#38543;&#26426;&#20248;&#21183;&#30340;&#27010;&#24565;&#20197;&#20351;&#20854;&#33021;&#22815;&#22312;&#20219;&#24847;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#36827;&#34892;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#22788;&#29702;&#36830;&#32493;&#24615;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02698</link><description>&lt;p&gt;
&#36229;&#36234;&#26399;&#26395;: &#29616;&#23454;&#20013;&#23454;&#29616;&#38543;&#26426;&#20248;&#21183;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Expectations: Learning with Stochastic Dominance Made Practical
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#24314;&#31435;&#20102;&#19968;&#20010;&#38543;&#26426;&#20248;&#21183;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#25512;&#24191;&#20102;&#38543;&#26426;&#20248;&#21183;&#30340;&#27010;&#24565;&#20197;&#20351;&#20854;&#33021;&#22815;&#22312;&#20219;&#24847;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#36827;&#34892;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#22788;&#29702;&#36830;&#32493;&#24615;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20248;&#21183;&#27169;&#22411;&#23545;&#20915;&#31574;&#26102;&#20855;&#26377;&#39118;&#38505;&#21388;&#24694;&#20559;&#22909;&#30340;&#19981;&#30830;&#23450;&#32467;&#26524;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#27604;&#20110;&#20165;&#20165;&#20381;&#36182;&#26399;&#26395;&#20540;&#65292;&#33258;&#28982;&#22320;&#25429;&#25417;&#20102;&#24213;&#23618;&#19981;&#30830;&#23450;&#24615;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#38543;&#26426;&#20248;&#21183;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#21364;&#24456;&#23569;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20197;&#19979;&#25361;&#25112;&#65306;$\textbf{i)}$ &#38543;&#26426;&#20248;&#21183;&#30340;&#21407;&#22987;&#27010;&#24565;&#20165;&#25552;&#20379;&#20102;$\textit{&#37096;&#20998;&#24207;}$&#65292;&#22240;&#27492;&#19981;&#33021;&#20316;&#20026;&#26368;&#20248;&#24615;&#20934;&#21017;&#65307;&#21644; $\textbf{ii)}$ &#30001;&#20110;&#35780;&#20272;&#38543;&#26426;&#20248;&#21183;&#30340;&#36830;&#32493;&#24615;&#26412;&#36136;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#39640;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#24314;&#31435;&#19968;&#20010;&#19982;&#38543;&#26426;&#20248;&#21183;&#23398;&#20064;&#30456;&#20851;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#38543;&#26426;&#20248;&#21183;&#27010;&#24565;&#25512;&#24191;&#65292;&#20351;&#24471;&#20219;&#24847;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#12290;&#25509;&#19979;&#26469;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35780;&#20272;&#38543;&#26426;&#20248;&#21183;&#30340;&#36830;&#32493;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic dominance models risk-averse preferences for decision making with uncertain outcomes, which naturally captures the intrinsic structure of the underlying uncertainty, in contrast to simply resorting to the expectations. Despite theoretically appealing, the application of stochastic dominance in machine learning has been scarce, due to the following challenges: $\textbf{i)}$, the original concept of stochastic dominance only provides a $\textit{partial order}$, therefore, is not amenable to serve as an optimality criterion; and $\textbf{ii)}$, an efficient computational recipe remains lacking due to the continuum nature of evaluating stochastic dominance.%, which barriers its application for machine learning.   In this work, we make the first attempt towards establishing a general framework of learning with stochastic dominance. We first generalize the stochastic dominance concept to enable feasible comparisons between any arbitrary pair of random variables. We next develop a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35770;&#25991;&#30740;&#31350;&#20102;&#36127;&#36131;&#20219;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#65292;&#24378;&#35843;&#20102;&#20854;&#23545;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#22495;&#27867;&#21270;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.02696</link><description>&lt;p&gt;
&#23545;&#36127;&#36131;&#20219;&#26426;&#22120;&#23398;&#20064;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causal Feature Selection for Responsible Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#30740;&#31350;&#20102;&#36127;&#36131;&#20219;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#65292;&#24378;&#35843;&#20102;&#20854;&#23545;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#22495;&#27867;&#21270;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#36127;&#36131;&#20219;&#26426;&#22120;&#23398;&#20064;&#30340;&#38656;&#27714;&#24050;&#32463;&#20986;&#29616;&#65292;&#37325;&#28857;&#26159;&#23558;ML&#27169;&#22411;&#19982;&#20262;&#29702;&#21644;&#31038;&#20250;&#20215;&#20540;&#30456;&#19968;&#33268;&#65292;&#21516;&#26102;&#22686;&#24378;&#20854;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#36127;&#36131;&#20219;&#30340;ML&#28041;&#21450;&#35768;&#22810;&#38382;&#39064;&#12290;&#26412;&#35843;&#26597;&#28041;&#21450;&#22235;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#21487;&#35299;&#37322;&#24615;&#65292;&#20844;&#24179;&#24615;&#65292;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#22495;&#27867;&#21270;&#12290;&#29305;&#24449;&#36873;&#25321;&#22312;&#36127;&#36131;&#20219;&#30340;ML&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20165;&#22522;&#20110;&#21464;&#37327;&#20043;&#38388;&#30340;&#32479;&#35745;&#30456;&#20851;&#24615;&#26500;&#24314;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#24102;&#26377;&#20559;&#35265;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#34394;&#20551;&#27169;&#24335;&#12290;&#26412;&#35843;&#26597;&#19987;&#27880;&#20110;&#24403;&#21069;&#23545;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#30340;&#30740;&#31350;&#65306;&#20160;&#20040;&#26159;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#20197;&#21450;&#23427;&#22914;&#20309;&#21152;&#24378;&#36127;&#36131;&#20219;ML&#30340;&#22235;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#32467;&#26524;&#20135;&#29983;&#22240;&#26524;&#24433;&#21709;&#24182;&#21306;&#20998;&#22240;&#26524;&#21644;&#30456;&#20851;&#24615;&#65292;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#34987;&#35748;&#20026;&#26159;&#30830;&#20445;ML&#27169;&#22411;&#22312;&#20262;&#29702;&#21644;&#31038;&#20250;&#19978;&#36127;&#36131;&#20219;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has become an integral aspect of many real-world applications. As a result, the need for responsible machine learning has emerged, focusing on aligning ML models to ethical and social values, while enhancing their reliability and trustworthiness. Responsible ML involves many issues. This survey addresses four main issues: interpretability, fairness, adversarial robustness, and domain generalization. Feature selection plays a pivotal role in the responsible ML tasks. However, building upon statistical correlations between variables can lead to spurious patterns with biases and compromised performance. This survey focuses on the current study of causal feature selection: what it is and how it can reinforce the four aspects of responsible ML. By identifying features with causal impacts on outcomes and distinguishing causality from correlation, causal feature selection is posited as a unique approach to ensuring ML models to be ethically and socially responsible in hi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.02695</link><description>&lt;p&gt;
&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Exploiting Class Probabilities for Black-box Sentence-level Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#32423;&#25915;&#20987;&#26159;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#21477;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#36825;&#20123;&#21477;&#23376;&#19982;&#27491;&#30830;&#20998;&#31867;&#30340;&#21477;&#23376;&#21516;&#20041;&#65292;&#20294;&#34987;&#20998;&#31867;&#22120;&#38169;&#35823;&#22320;&#20998;&#31867;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#65292;&#20998;&#31867;&#22120;&#21482;&#33021;&#36890;&#36807;&#23545;&#26597;&#35810;&#36755;&#20837;&#30340;&#21453;&#39304;&#36827;&#34892;&#35775;&#38382;&#65292;&#36825;&#20027;&#35201;&#20197;&#31867;&#21035;&#27010;&#29575;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;&#23613;&#31649;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#20294;&#30001;&#20110;&#22312;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#35201;&#20040;&#19981;&#20351;&#29992;&#21453;&#39304;&#65292;&#35201;&#20040;&#20165;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#19978;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#26159;&#21542;&#20540;&#24471;&#25110;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20998;&#31867;&#22120;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#19982;&#22522;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27850;&#26494;&#36807;&#31243;&#30340;&#26032;&#22411;&#25490;&#21517;&#26367;&#20195;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#31216;&#20026;&#27850;&#26494;&#36807;&#31243;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;PoPBO&#65289;&#30340;&#39640;&#25928;BO&#26694;&#26550;&#65292;&#24182;&#20174;&#32463;&#20856;&#30340;LCB&#21644;EI&#27169;&#22411;&#20013;&#24471;&#20986;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#25910;&#38598;&#20989;&#25968;&#20197;&#36866;&#24212;&#23427;&#12290;</title><link>https://arxiv.org/abs/2402.02687</link><description>&lt;p&gt;
&#27850;&#26494;&#36807;&#31243;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Poisson Process for Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27850;&#26494;&#36807;&#31243;&#30340;&#26032;&#22411;&#25490;&#21517;&#26367;&#20195;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#31216;&#20026;&#27850;&#26494;&#36807;&#31243;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;PoPBO&#65289;&#30340;&#39640;&#25928;BO&#26694;&#26550;&#65292;&#24182;&#20174;&#32463;&#20856;&#30340;LCB&#21644;EI&#27169;&#22411;&#20013;&#24471;&#20986;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#25910;&#38598;&#20989;&#25968;&#20197;&#36866;&#24212;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#27010;&#29575;&#26367;&#20195;&#27169;&#22411;&#24314;&#31435;&#40657;&#30418;&#20989;&#25968;&#30340;&#32477;&#23545;&#20989;&#25968;&#21709;&#24212;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#26500;&#24314;&#36825;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#26641;&#32467;&#26500;&#30340;Parzen&#20272;&#35745;&#26041;&#27861;(TPE)&#12289;&#38543;&#26426;&#26862;&#26519;(SMAC)&#21644;&#39640;&#26031;&#36807;&#31243;(GP)&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#39033;&#30340;&#30456;&#23545;&#25490;&#21517;&#65292;&#30456;&#23545;&#25490;&#21517;&#21487;&#20197;&#27604;&#32477;&#23545;&#20989;&#25968;&#21709;&#24212;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#22312;&#20989;&#25968;&#21709;&#24212;&#38590;&#20197;&#22788;&#29702;&#20294;&#20559;&#22909;&#21487;&#20197;&#33719;&#21462;&#26102;&#26356;&#20855;&#23454;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27850;&#26494;&#36807;&#31243;&#30340;&#26032;&#22411;&#25490;&#21517;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;BO&#26694;&#26550;&#65292;&#31216;&#20026;&#27850;&#26494;&#36807;&#31243;&#36125;&#21494;&#26031;&#20248;&#21270;(PoPBO)&#12290;&#36827;&#19968;&#27493;&#20174;&#32463;&#20856;&#30340;LCB&#21644;EI&#27169;&#22411;&#20013;&#24471;&#20986;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#25910;&#38598;&#20989;&#25968;&#20197;&#36866;&#24212;&#23427;&#12290;&#19982;&#32463;&#20856;&#30340;GP-BO&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PoPBO&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#65292;&#23545;&#22122;&#22768;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
BayesianOptimization(BO) is a sample-efficient black-box optimizer, and extensive methods have been proposed to build the absolute function response of the black-box function through a probabilistic surrogate model, including Tree-structured Parzen Estimator (TPE), random forest (SMAC), and Gaussian process (GP). However, few methods have been explored to estimate the relative rankings of candidates, which can be more robust to noise and have better practicality than absolute function responses, especially when the function responses are intractable but preferences can be acquired. To this end, we propose a novel ranking-based surrogate model based on the Poisson process and introduce an efficient BO framework, namely Poisson Process Bayesian Optimization (PoPBO). Two tailored acquisition functions are further derived from classic LCB and EI to accommodate it. Compared to the classic GP-BO method, our PoPBO has lower computation costs and better robustness to noise, which is verified b
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#30772;&#32570;&#38598;&#26469;&#30772;&#22351;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02681</link><description>&lt;p&gt;
&#31561;&#21464;&#23545;&#31216;&#30772;&#32570;&#38598;
&lt;/p&gt;
&lt;p&gt;
Equivariant Symmetry Breaking Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#30772;&#32570;&#38598;&#26469;&#30772;&#22351;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#28041;&#21450;&#28508;&#22312;&#23545;&#31216;&#24615;&#30340;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#35774;&#35745;&#65292;ENN&#22312;&#32473;&#23450;&#26356;&#39640;&#23545;&#31216;&#24615;&#36755;&#20837;&#26102;&#26080;&#27861;&#20135;&#29983;&#36739;&#20302;&#23545;&#31216;&#24615;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29289;&#29702;&#31995;&#32479;&#20013;&#20250;&#21457;&#29983;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#19968;&#20010;&#21021;&#22987;&#39640;&#24230;&#23545;&#31216;&#30340;&#29366;&#24577;&#33719;&#24471;&#19968;&#20010;&#36739;&#19981;&#23545;&#31216;&#30340;&#31283;&#23450;&#29366;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#22914;&#20309;&#31995;&#32479;&#22320;&#22312;ENN&#20013;&#30772;&#22351;&#23545;&#31216;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#26032;&#22411;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#31216;&#30772;&#32570;&#38598;&#65288;SBS&#65289;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#19981;&#26159;&#37325;&#26032;&#35774;&#35745;&#29616;&#26377;&#30340;&#32593;&#32476;&#65292;&#32780;&#26159;&#35774;&#35745;&#20102;&#19968;&#32452;&#23545;&#31216;&#30772;&#32570;&#23545;&#35937;&#65292;&#26681;&#25454;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#23545;&#31216;&#24615;&#23558;&#20854;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#38598;&#21512;&#19978;&#23450;&#20041;&#31561;&#21464;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#23427;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#32422;&#26463;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;... (the abstract is incomplete and cut off)
&lt;/p&gt;
&lt;p&gt;
Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries. By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input. However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one. Hence, it is imperative that we understand how to systematically break symmetry in ENNs. In this work, we propose a novel symmetry breaking framework that is fully equivariant. We emphasize that our approach is general and applicable to equivariance under any group. To achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs. We show there is a natural way to define equivariance on these sets, which gives an additional constraint. Minimizing the si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#31995;&#32479;&#38169;&#35823;&#65292;&#36890;&#36807;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#26469;&#35780;&#20272;&#20854;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.02680</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#22320;&#29702;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Geographically Biased
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#31995;&#32479;&#38169;&#35823;&#65292;&#36890;&#36807;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#26469;&#35780;&#20272;&#20854;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#22312;&#22320;&#21547;&#26377;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#20559;&#35265;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#20260;&#23475;&#30340;&#25345;&#32493;&#23384;&#22312;&#12290;&#38543;&#30528;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#29702;&#35299;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20559;&#35265;&#23545;&#20110;&#23454;&#29616;&#20844;&#27491;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22320;&#29702;&#35270;&#35282;&#30740;&#31350;LLMs&#23545;&#25105;&#20204;&#25152;&#29983;&#27963;&#30340;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#24378;&#22823;&#65292;&#22240;&#20026;&#23545;&#20154;&#31867;&#29983;&#27963;&#20013;&#35832;&#22810;&#19982;&#22320;&#29702;&#31354;&#38388;&#30456;&#20851;&#30340;&#26041;&#38754;&#65288;&#22914;&#25991;&#21270;&#12289;&#31181;&#26063;&#12289;&#35821;&#35328;&#12289;&#25919;&#27835;&#21644;&#23447;&#25945;&#65289;&#26377;&#30528;&#26126;&#26174;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#38382;&#39064;&#22320;&#29702;&#20559;&#35265;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#20013;&#30340;&#31995;&#32479;&#38169;&#35823;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;LLMs&#33021;&#22815;&#36827;&#34892;&#31934;&#30830;&#30340;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#65292;&#20197;&#35780;&#32423;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20854;&#19982;&#30495;&#23454;&#24773;&#20917;&#20043;&#38388;&#21576;&#29616;&#20986;&#24378;&#28872;&#30340;&#21333;&#35843;&#30456;&#20851;&#24615;&#65288;Spearman's &#961;&#26368;&#39640;&#21487;&#36798;0.89&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#20010;&#23458;&#35266;&#21644;&#23376;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#20849;&#21516;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and sub
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#22411;&#25512;&#29702;&#21644;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#65292;&#21487;&#20197;&#25552;&#20379;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#39564;&#35777;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21487;&#20449;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02675</link><description>&lt;p&gt;
&#20351;&#29992;zkSNARKs&#36827;&#34892;&#21487;&#39564;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Verifiable evaluations of machine learning models using zkSNARKs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#22411;&#25512;&#29702;&#21644;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#65292;&#21487;&#20197;&#25552;&#20379;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#39564;&#35777;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21487;&#20449;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36234;&#26469;&#36234;&#22810;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19990;&#30028;&#20013;&#65292;&#24320;&#21457;&#32773;&#30340;&#27169;&#22411;&#35780;&#20272;&#24517;&#39035;&#34987;&#24403;&#20316;&#38754;&#20540;&#25509;&#21463;&#12290;&#36825;&#20123;&#35780;&#20272;&#32467;&#26524;&#65292;&#26080;&#35770;&#26159;&#20219;&#21153;&#20934;&#30830;&#24615;&#12289;&#20559;&#24046;&#35780;&#20272;&#36824;&#26159;&#23433;&#20840;&#26816;&#26597;&#65292;&#20256;&#32479;&#19978;&#26080;&#27861;&#36890;&#36807;&#37325;&#26032;&#25191;&#34892;&#40657;&#31665;&#27169;&#22411;&#36755;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#21487;&#39564;&#35777;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;zkSNARKs&#36827;&#34892;&#27169;&#22411;&#25512;&#29702;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#36755;&#20986;&#30340;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#21487;&#20197;&#25171;&#21253;&#25104;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#26174;&#31034;&#20855;&#26377;&#22266;&#23450;&#31169;&#26377;&#26435;&#37325;&#30340;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#36798;&#21040;&#20102;&#25152;&#36848;&#30340;&#24615;&#33021;&#25110;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#20123;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#21487;&#20197;&#22312;&#20219;&#20309;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#65292;&#35745;&#31639;&#35201;&#27714;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#31361;&#20986;&#20102;&#20851;&#38190;&#25361;&#25112;&#21644;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a world of increasing closed-source commercial machine learning models, model evaluations from developers must be taken at face value. These benchmark results, whether over task accuracy, bias evaluations, or safety checks, are traditionally impossible to verify by a model end-user without the costly or impossible process of re-performing the benchmark on black-box model outputs. This work presents a method of verifiable model evaluation using model inference through zkSNARKs. The resulting zero-knowledge computational proofs of model outputs over datasets can be packaged into verifiable evaluation attestations showing that models with fixed private weights achieve stated performance or fairness metrics over public inputs. These verifiable attestations can be performed on any standard neural network model with varying compute requirements. For the first time, we demonstrate this across a sample of real-world models and highlight key challenges and design solutions. This presents a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#25454;&#25972;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02658</link><description>&lt;p&gt;
&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#30340;&#39564;&#35777;&#22120;&#65306;&#20851;&#20110;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#25454;&#25972;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#30417;&#30563;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#39564;&#35777;&#22120;&#26469;&#35780;&#20272;&#25512;&#29702;&#22120;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#24050;&#32463;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#36991;&#20813;&#22312;&#39564;&#35777;&#22120;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#25968;&#25454;&#25972;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;MiPS&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;&#20934;&#30830;&#23436;&#25104;&#30340;&#27604;&#20363;&#23450;&#20041;&#20026;&#20934;&#30830;&#29575;&#12290;&#25512;&#29702;&#22120;&#20013;&#30340;&#38169;&#35823;&#20250;&#23548;&#33268;MiPS&#20302;&#20272;&#20013;&#38388;&#27493;&#39588;&#30340;&#20934;&#30830;&#29575;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#65292;&#32780;&#19981;&#26159;&#20302;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65288;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;+0.67&#65285;&#65292;&#25968;&#23398;&#19978;&#30340;&#20934;&#30830;&#29575;+4.16&#65285;&#65292;MBPP&#19978;&#30340;&#20934;&#30830;&#29575;+0.92&#65285;&#19982;&#36755;&#20986;s&#30456;&#27604;&#12290;&#65289;
&lt;/p&gt;
&lt;p&gt;
Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02651</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models Provide Promptable Representations for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#32972;&#26223;&#19990;&#30028;&#30693;&#35782;&#24555;&#36895;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#20195;&#29702;&#36890;&#24120;&#38656;&#35201;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20013;&#32534;&#30721;&#30340;&#22823;&#37327;&#36890;&#29992;&#21644;&#21487;&#32034;&#24341;&#30340;&#19990;&#30028;&#30693;&#35782;&#26469;&#36827;&#34892;&#20855;&#35937;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;VLMs&#29992;&#20316;&#21487;&#25552;&#31034;&#34920;&#31034;&#26469;&#21021;&#22987;&#21270;&#31574;&#30053;&#65306;&#36825;&#20123;&#23884;&#20837;&#22312;&#35270;&#35273;&#35266;&#23519;&#20013;&#20855;&#26377;&#22522;&#30784;&#65292;&#24182;&#26681;&#25454;VLM&#30340;&#20869;&#37096;&#30693;&#35782;&#32534;&#30721;&#35821;&#20041;&#29305;&#24449;&#65292;&#36890;&#36807;&#25552;&#20379;&#20219;&#21153;&#19978;&#19979;&#25991;&#21644;&#36741;&#21161;&#20449;&#24687;&#26469;&#35302;&#21457;&#12290;&#25105;&#20204;&#22312;Minecraft&#21644;Habitat&#20013;&#30340;&#35270;&#35273;&#22797;&#26434;&#12289;&#38271;&#26399;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36890;&#29992;&#22411;VLMs&#25552;&#21462;&#30340;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#32988;&#36807;&#20351;&#29992;&#36890;&#29992;&#30340;&#12289;&#19981;&#21487;&#25552;&#31034;&#30340;&#22270;&#20687;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#36981;&#24490;&#25351;&#31034;&#30340;&#20803;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20013;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#26041;&#27861;&#65292;&#24182;&#25552;&#21040;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02648</link><description>&lt;p&gt;
&#38142;&#24335;&#21453;&#39304;&#65306;&#32531;&#35299;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20013;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#26041;&#27861;&#65292;&#24182;&#25552;&#21040;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#39064;&#26102;&#32463;&#24120;&#20986;&#29616;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#65292;&#21363;&#20351;&#36755;&#20837;&#30456;&#21516;&#65292;&#20063;&#20250;&#25552;&#20379;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#24403;&#29992;&#25143;&#34920;&#36798;&#22362;&#20915;&#30456;&#21453;&#30340;&#31435;&#22330;&#26102;&#65292;LLMs&#35843;&#25972;&#20854;&#22238;&#31572;&#30340;&#36136;&#37327;&#20250;&#21464;&#24046;&#65292;&#23613;&#31649;&#21021;&#22987;&#22238;&#31572;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#20123;&#34892;&#20026;&#38477;&#20302;&#20102;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#30340;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#65306;1&#65289;&#36890;&#36807;&#23637;&#31034;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#22914;&#20309;&#23548;&#33268;LLMs&#26356;&#21152;&#20559;&#31163;&#23454;&#38469;&#31572;&#26696;&#65292;&#24341;&#36215;&#36807;&#24230;&#20381;&#36182;ChatGPT&#31561;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#22266;&#26377;&#39118;&#38505;&#65307;2&#65289;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#65292;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;CoF&#31995;&#32479;&#25509;&#25910;&#19968;&#20010;&#24320;&#25918;&#24335;&#22810;&#27493;&#38382;&#39064;&#65292;&#28982;&#21518;&#25105;&#20204;&#37325;&#22797;&#25552;&#20379;&#26080;&#24847;&#20041;&#30340;&#21453;&#39304;&#65292;&#35201;&#27714;&#20877;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21453;&#39304;&#21482;&#20250;&#38477;&#20302;&#22238;&#31572;&#30340;&#36136;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the oth
&lt;/p&gt;</description></item><item><title>LLMDB&#26159;&#19968;&#31181;LLM&#22686;&#24378;&#30340;&#25968;&#25454;&#31649;&#29702;&#33539;&#24335;&#65292;&#36890;&#36807;LLM&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#23884;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#34394;&#26500;&#12289;&#39640;&#25104;&#26412;&#21644;&#20302;&#20934;&#30830;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02643</link><description>&lt;p&gt;
LLM&#22686;&#24378;&#22411;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM-Enhanced Data Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02643
&lt;/p&gt;
&lt;p&gt;
LLMDB&#26159;&#19968;&#31181;LLM&#22686;&#24378;&#30340;&#25968;&#25454;&#31649;&#29702;&#33539;&#24335;&#65292;&#36890;&#36807;LLM&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#23884;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#34394;&#26500;&#12289;&#39640;&#25104;&#26412;&#21644;&#20302;&#20934;&#30830;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#20248;&#21270;&#25968;&#25454;&#31649;&#29702;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24191;&#27867;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ML&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#65288;&#36866;&#24212;&#19981;&#21516;&#24773;&#26223;&#65289;&#21644;&#25512;&#29702;&#33021;&#21147;&#65288;&#29702;&#35299;&#19978;&#19979;&#25991;&#65289;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20154;&#31867;&#31454;&#20105;&#33021;&#21147;&#65292;&#23545;&#20110;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#65288;&#22914;&#25968;&#25454;&#24211;&#35786;&#26029;&#12289;&#25968;&#25454;&#24211;&#35843;&#20248;&#65289;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#34394;&#26500;&#12289;&#39640;&#25104;&#26412;&#20197;&#21450;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#20302;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLMDB&#65292;&#19968;&#31181;&#20351;&#29992;LLM&#22686;&#24378;&#30340;&#25968;&#25454;&#31649;&#29702;&#33539;&#24335;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#34394;&#26500;&#65292;&#38477;&#20302;&#20102;LLM&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;LLMDB&#36890;&#36807;LLM&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#23884;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#20197;&#36991;&#20813;&#34394;&#26500;&#12290;LLMDB&#36890;&#36807;&#20943;&#23569;LLMs&#30340;&#39640;&#25104;&#26412; addresses challenges: hallucination, high cost, low accuracy-
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;Transformer RNNs&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#26368;&#23567;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.02625</link><description>&lt;p&gt;
&#29992;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#22686;&#24378;Transformer RNNs
&lt;/p&gt;
&lt;p&gt;
Enhancing Transformer RNNs with Multiple Temporal Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02625
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;Transformer RNNs&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#26368;&#23567;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20854;&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#32500;&#25252;&#20808;&#21069;&#36935;&#21040;&#30340;&#25991;&#26412;&#30340;&#22810;&#26679;&#26102;&#38388;&#35270;&#22270;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#32435;&#20837;&#20102;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#35813;&#26550;&#26500;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#20445;&#30041;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#26368;&#23569;&#65288;&#20165;&#20026;&#26368;&#21021;&#21442;&#25968;&#25968;&#37327;&#30340;0.04%&#65289;&#65292;&#20063;&#23454;&#29616;&#20102;&#27492;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#25152;&#38656;&#30340;&#39069;&#22806;&#21442;&#25968;&#32463;&#36807;&#24494;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#23436;&#20840;&#39044;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#22312;&#25552;&#31034;&#25512;&#26029;&#36807;&#31243;&#20013;&#20445;&#25345;&#20102;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#26435;&#37325;&#21464;&#21270;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36816;&#21160;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#21160;&#24577;&#35843;&#25972;&#20195;&#20215;&#20989;&#25968;&#26435;&#37325;&#65292;&#21487;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26368;&#20248;&#38381;&#29615;&#25511;&#21046;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02624</link><description>&lt;p&gt;
&#19968;&#31181;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#26435;&#37325;&#21464;&#21270;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02624
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#26435;&#37325;&#21464;&#21270;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36816;&#21160;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#21160;&#24577;&#35843;&#25972;&#20195;&#20215;&#20989;&#25968;&#26435;&#37325;&#65292;&#21487;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26368;&#20248;&#38381;&#29615;&#25511;&#21046;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#26368;&#20339;&#20195;&#20215;&#20989;&#25968;&#21442;&#25968;&#20197;&#20248;&#21270;&#22810;&#20010;&#25511;&#21046;&#30446;&#26631;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#25216;&#26415;&#36890;&#36807;&#30830;&#23450;MPC&#30340; Pareto &#26368;&#20248;&#21442;&#25968;&#38598;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#24403;MPC&#30340;&#25805;&#20316;&#26465;&#20214;&#19978;&#19979;&#25991;&#22312;&#20854;&#36816;&#34892;&#36807;&#31243;&#20013;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#21333;&#20010;&#21442;&#25968;&#38598;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#26368;&#20248;&#30340;&#38381;&#29615;&#25511;&#21046;&#24615;&#33021;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#36816;&#34892;&#26102;&#35843;&#25972;&#20195;&#20215;&#20989;&#25968;&#26435;&#37325;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(RL)&#31639;&#27861;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26368;&#20248;&#21442;&#25968;&#38598;&#65292;&#24182;&#22312;&#26435;&#37325;&#21464;&#21270;&#30340;MPC&#20013;&#36827;&#34892;&#21160;&#24577;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#20174;&#22836;&#23398;&#20064;&#20195;&#20215;&#20989;&#25968;&#26435;&#37325;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#25805;&#20316;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;RL&#30340;&#21160;&#20316;&#38480;&#21046;&#22312;&#23433;&#20840;&#23398;&#20064;&#31354;&#38388;&#20869;&#65292;&#35813;&#31354;&#38388;&#34920;&#31034;&#32463;&#36807;&#39044;&#20248;&#21270;&#30340;BO Pareto&#26368;&#20248;&#26435;&#37325;&#30340;&#30446;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight se
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;Betfair&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#33521;&#22269;&#36187;&#39532;&#24066;&#22330;&#22312;&#25237;&#27880;&#20132;&#26131;&#22238;&#25253;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#39640;&#27700;&#24179;&#30340;&#20449;&#24687;&#25928;&#29575;&#65292;&#19982;&#37329;&#34701;&#36164;&#20135;&#19981;&#21516;&#65292;&#24066;&#22330;&#20013;&#30340;&#30693;&#35782;&#33021;&#22815;&#24555;&#36895;&#34987;&#21560;&#25910;&#24182;&#21453;&#26144;&#22312;&#20215;&#26684;&#19978;&#65292;&#24182;&#19988;&#24066;&#22330;&#23545;&#26032;&#20449;&#24687;&#30340;&#21709;&#24212;&#36895;&#24230;&#36739;&#24555;&#12290;</title><link>https://arxiv.org/abs/2402.02623</link><description>&lt;p&gt;
&#39640;&#25928;&#24066;&#22330;&#21160;&#24577;&#65306;&#36890;&#36807;Betfair&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25581;&#31034;UK&#36187;&#39532;&#25237;&#27880;&#24066;&#22330;&#30340;&#20449;&#24687;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Efficient Market Dynamics: Unraveling Informational Efficiency in UK Horse Racing Betting Markets Through Betfair's Time Series Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02623
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;Betfair&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#33521;&#22269;&#36187;&#39532;&#24066;&#22330;&#22312;&#25237;&#27880;&#20132;&#26131;&#22238;&#25253;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#39640;&#27700;&#24179;&#30340;&#20449;&#24687;&#25928;&#29575;&#65292;&#19982;&#37329;&#34701;&#36164;&#20135;&#19981;&#21516;&#65292;&#24066;&#22330;&#20013;&#30340;&#30693;&#35782;&#33021;&#22815;&#24555;&#36895;&#34987;&#21560;&#25910;&#24182;&#21453;&#26144;&#22312;&#20215;&#26684;&#19978;&#65292;&#24182;&#19988;&#24066;&#22330;&#23545;&#26032;&#20449;&#24687;&#30340;&#21709;&#24212;&#36895;&#24230;&#36739;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Betfair&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23545;&#33521;&#22269;&#65288;UK&#65289;&#36187;&#39532;&#24066;&#22330;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#24726;&#35770;&#65306;&#25237;&#27880;&#20132;&#26131;&#22238;&#25253;&#30340;&#24066;&#22330;&#20855;&#26377;&#30701;&#23614;&#12289;&#24555;&#36895;&#34928;&#20943;&#30340;&#33258;&#30456;&#20851;&#24615;&#21644;&#26080;&#38271;&#26399;&#35760;&#24518;&#12290;&#19982;&#20855;&#26377;&#37325;&#23614;&#21644;&#27874;&#21160;&#24615;&#32858;&#31867;&#29305;&#24449;&#30340;&#37329;&#34701;&#36164;&#20135;&#30456;&#27604;&#65292;&#25237;&#27880;&#20132;&#26131;&#22238;&#25253;&#30340;&#20449;&#24687;&#25928;&#29575;&#38750;&#24120;&#39640;&#12290;&#20855;&#26377;&#36731;&#23614;&#30340;&#24191;&#20041;&#39640;&#26031;&#26080;&#26465;&#20214;&#20998;&#24067;&#34920;&#26126;&#24066;&#22330;&#30693;&#35782;&#24456;&#24555;&#34987;&#21560;&#25910;&#24182;&#21453;&#26144;&#22312;&#20215;&#26684;&#19978;&#12290;&#36825;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#33258;&#30456;&#20851;&#24615;&#26497;&#24555;&#20943;&#24369;&#21644;&#25910;&#30410;-&#25439;&#22833;&#19981;&#23545;&#31216;&#32570;&#22833;&#30340;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#38500;&#20102;&#27979;&#37327;&#38271;&#26399;&#35760;&#24518;&#20043;&#22806;&#65292;Hurst&#25351;&#25968;&#20063;&#26174;&#31034;&#20986;&#22343;&#20540;&#22238;&#24402;&#65292;&#34920;&#26126;&#24066;&#22330;&#23545;&#26032;&#20449;&#24687;&#30340;&#24555;&#36895;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using Betfair's time series data, an analysis of the United Kingdom (UK) horse racing market reveals an interesting paradox: a market with short tails, rapidly decaying autocorrelations, and no long-term memory. There seems to be a remarkably high level of informational efficiency in betting exchange returns, in contrast to financial assets that are characterized by heavy tails and volatility clustering. The generalized Gaussian unconditional distribution with a light tail point to a market where knowledge is quickly assimilated and reflected in prices. This is further supported by the extremely quick fading of autocorrelations and the absence of gain-loss asymmetry. Therefore, in addition to measuring long-range memory, the Hurst exponent also shows mean reversion, a sign that markets respond quickly to fresh information.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02611</link><description>&lt;p&gt;
PuzzleBench&#65306;LLMs&#33021;&#21542;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#65292;&#22914;&#36923;&#36753;&#38382;&#31572;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65292;&#19968;&#20010;&#20363;&#23376;&#26159;&#27969;&#34892;&#30340;&#25968;&#29420;&#35868;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#26377;&#19968;&#20010;&#30001;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22522;&#30784;&#19968;&#38454;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#20363;&#21270;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#25165;&#33021;&#36798;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PuzzleBench&#65292;&#19968;&#20010;&#21253;&#21547;31&#20010;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#22312;&#31526;&#21495;&#27714;&#35299;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;LLMs&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#30456;&#24403;&#31967;&#31957;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Puzzle-LM&#65292;&#23427;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#28304;&#21152;&#23494;&#24037;&#20855;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25104;&#21151;&#22320;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#65292;&#35268;&#36991;&#20102;&#26368;&#20808;&#36827;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#65292;&#24182;&#19988;&#22312;&#35268;&#36991;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#20351;&#29992;&#39640;&#32423;&#20462;&#25913;&#26041;&#27861;&#30340;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.02600</link><description>&lt;p&gt;
&#32469;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#28151;&#28102;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evading Deep Learning-Based Malware Detectors via Obfuscation: A Deep Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#28304;&#21152;&#23494;&#24037;&#20855;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25104;&#21151;&#22320;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#65292;&#35268;&#36991;&#20102;&#26368;&#20808;&#36827;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#65292;&#24182;&#19988;&#22312;&#35268;&#36991;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#20351;&#29992;&#39640;&#32423;&#20462;&#25913;&#26041;&#27861;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#29983;&#25104;&#65288;AMG&#65289;&#26159;&#29983;&#25104;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;&#20197;&#21152;&#24378;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#23427;&#24050;&#25104;&#20026;&#20027;&#21160;&#24335;&#32593;&#32476;&#38450;&#24481;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#25552;&#20379;&#23545;&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#24494;&#23567;&#25200;&#21160;&#25110;&#28155;&#21152;&#65292;&#24182;&#27809;&#26377;&#25506;&#32034;&#20840;&#25991;&#20214;&#28151;&#28102;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24320;&#28304;&#21152;&#23494;&#24037;&#20855;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#21487;&#20197;&#25104;&#21151;&#22320;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#20197;&#35268;&#36991;&#26368;&#20808;&#36827;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#24341;&#25806;&#65292;&#24182;&#36229;&#36234;&#20351;&#29992;&#39640;&#32423;&#20462;&#25913;&#26041;&#27861;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#35268;&#36991;&#29575;&#25552;&#39640;&#20102;27%-49%&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial Malware Generation (AMG), the gen- eration of adversarial malware variants to strengthen Deep Learning (DL)-based malware detectors has emerged as a crucial tool in the development of proactive cyberdefense. However, the majority of extant works offer subtle perturbations or additions to executable files and do not explore full-file obfuscation. In this study, we show that an open-source encryption tool coupled with a Reinforcement Learning (RL) framework can successfully obfuscate malware to evade state-of-the-art malware detection engines and outperform techniques that use advanced modification methods. Our results show that the proposed method improves the evasion rate from 27%-49% compared to widely- used state-of-the-art reinforcement learning-based methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#65292;&#21517;&#20026;Moirai&#65292;&#20197;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36328;&#39057;&#29575;&#23398;&#20064;&#12289;&#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#12290;</title><link>https://arxiv.org/abs/2402.02592</link><description>&lt;p&gt;
&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer
&lt;/p&gt;
&lt;p&gt;
Unified Training of Universal Time Series Forecasting Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#65292;&#21517;&#20026;Moirai&#65292;&#20197;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36328;&#39057;&#29575;&#23398;&#20064;&#12289;&#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#19968;&#27169;&#22411;&#30340;&#26694;&#26550;&#19979;&#36816;&#20316;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31361;&#30772;&#24615;&#24433;&#21709;&#30340;&#28508;&#21147;&#12290;&#36890;&#29992;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#28304;&#20110;&#22312;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35774;&#24819;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#30340;&#21333;&#19968;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;i) &#36328;&#39057;&#29575;&#23398;&#20064;&#65292;ii) &#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#20219;&#24847;&#25968;&#37327;&#30340;&#21464;&#37327;&#65292;&#20197;&#21450;iii) &#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23545;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Masked Encoder&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#65288;Moirai&#65289;&#12290;&#22312;&#25105;&#20204;&#26032;&#24341;&#20837;&#30340;&#22823;&#35268;&#27169;&#24320;&#25918;&#26102;&#38388;&#24207;&#21015;&#23384;&#26723;&#65288;LOTSA&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) fea
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>Neur2BiLO&#26159;&#19968;&#20010;&#38024;&#23545;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#24341;&#20837;&#21040;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20013;&#65292;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02552</link><description>&lt;p&gt;
Neur2BiLO: &#31070;&#32463;&#21452;&#23618;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neur2BiLO: Neural Bilevel Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02552
&lt;/p&gt;
&lt;p&gt;
Neur2BiLO&#26159;&#19968;&#20010;&#38024;&#23545;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#24341;&#20837;&#21040;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20013;&#65292;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#22788;&#29702;&#23884;&#22871;&#38382;&#39064;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#39046;&#23548;&#32773;&#39318;&#20808;&#20570;&#20986;&#20915;&#31574;&#20197;&#26368;&#23567;&#21270;&#33258;&#24049;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36861;&#38543;&#32773;&#30340;&#26368;&#22909;&#21453;&#24212;&#12290;&#25972;&#25968;&#21464;&#37327;&#32422;&#26463;&#30340;&#21452;&#23618;&#38382;&#39064;&#29305;&#21035;&#38590;&#20197;&#22788;&#29702;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#21452;&#23618;&#20248;&#21270;&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#20294;&#23427;&#20204;&#22312;&#38382;&#39064;&#35268;&#27169;&#36739;&#22823;&#26102;&#24448;&#24448;&#26080;&#27861;&#25193;&#23637;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#24773;&#20917;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38382;&#39064;&#29305;&#23450;&#30340;&#31639;&#27861;&#65288;&#31934;&#30830;&#21644;&#21551;&#21457;&#24335;&#65289;&#23616;&#38480;&#20110;&#29305;&#23450;&#33539;&#22260;&#12290;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;Neur2BiLO&#23558;&#36890;&#36807;&#30417;&#30563;&#22238;&#24402;&#35757;&#32451;&#30340;&#39046;&#23548;&#32773;&#25110;&#36861;&#38543;&#32773;&#30340;&#20540;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#23884;&#20837;&#21040;&#26131;&#20110;&#35299;&#20915;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20013;&#12290; Neur2BiLO&#20316;&#20026;&#19968;&#31181;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#21452;&#23618;&#32972;&#21253;&#25318;&#25130;&#38382;&#39064;&#65292;&#21363;&#8220;&#20851;&#38190;n&#20010;&#38382;&#39064;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization deals with nested problems in which a leader takes the first decision to minimize their objective function while accounting for a follower's best-response reaction. Constrained bilevel problems with integer variables are particularly notorious for their hardness. While exact solvers have been proposed for mixed-integer linear bilevel optimization, they tend to scale poorly with problem size and are hard to generalize to the non-linear case. On the other hand, problem-specific algorithms (exact and heuristic) are limited in scope. Under a data-driven setting in which similar instances of a bilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds a neural network approximation of the leader's or follower's value function, trained via supervised regression, into an easy-to-solve mixed-integer program. Neur2BiLO serves as a heuristic that produces high-quality solutions extremely fast for the bilevel knapsack interdiction problem, the "critical n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#29615;&#22659;&#23545;&#20110;&#25913;&#36827;&#22522;&#20110;&#23454;&#36341;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#25968;&#25454;&#37319;&#38598;&#21644;&#27169;&#22411;&#24320;&#21457;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#37319;&#29992;&#29983;&#24577;&#26041;&#27861;&#30740;&#31350;&#22522;&#20110;&#23454;&#36341;&#30340;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#22312;&#33258;&#28982;/&#27169;&#25311;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.02548</link><description>&lt;p&gt;
"&#25105;&#30340;&#27169;&#22411;&#22312;&#20160;&#20040;&#37324;&#38754;&#65311;": &#25506;&#32034;&#29615;&#22659;&#22312;&#22522;&#20110;&#23454;&#36341;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
"What's my model inside of?": Exploring the role of environments for grounded natural language understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#29615;&#22659;&#23545;&#20110;&#25913;&#36827;&#22522;&#20110;&#23454;&#36341;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#25968;&#25454;&#37319;&#38598;&#21644;&#27169;&#22411;&#24320;&#21457;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#37319;&#29992;&#29983;&#24577;&#26041;&#27861;&#30740;&#31350;&#22522;&#20110;&#23454;&#36341;&#30340;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#22312;&#33258;&#28982;/&#27169;&#25311;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#30740;&#31350;&#23396;&#31435;&#30340;&#22823;&#33041;&#30340;&#32463;&#20856;&#35748;&#30693;&#31185;&#23398;&#19981;&#21516;&#65292;&#29983;&#24577;&#26041;&#27861;&#20391;&#37325;&#20110;&#36523;&#20307;&#21644;&#29615;&#22659;&#22312;&#22609;&#36896;&#35748;&#30693;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;&#21516;&#26679;&#65292;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#29983;&#24577;&#26041;&#27861;&#26469;&#30740;&#31350;&#22522;&#20110;&#23454;&#36341;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#12290;&#22522;&#20110;&#23454;&#36341;&#30340;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#23558;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#32622;&#20110;&#33258;&#28982;/&#27169;&#25311;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#20107;&#20214;&#12289;&#21160;&#20316;&#21644;&#39044;&#26399;&#20043;&#20013;&#12290;&#19982;&#32463;&#20856;&#30740;&#31350;&#20542;&#21521;&#20110;&#19987;&#27880;&#20110;&#35774;&#35745;&#26032;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#23558;&#29615;&#22659;&#35270;&#20026;&#32473;&#23450;&#30340;&#24773;&#20917;&#19981;&#21516;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29615;&#22659;&#35774;&#35745;&#23545;&#20110;&#25913;&#36827;&#25968;&#25454;&#37319;&#38598;&#21644;&#27169;&#22411;&#24320;&#21457;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#22522;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#29615;&#22659;&#24320;&#21457;&#20102;&#26032;&#30340;&#35757;&#32451;&#21644;&#26631;&#27880;&#26041;&#27861;&#65292;&#29992;&#20110;&#31243;&#24207;&#21270;&#25991;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#21442;&#32771;&#20102;&#22522;&#20110;&#20307;&#39564;&#30340;&#35748;&#30693;&#35821;&#35328;&#23398;&#25991;&#29486;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23454;&#36341;&#30340;NLP&#30740;&#31350;&#36335;&#32447;&#22270;&#65292;&#24182;&#20026;&#34913;&#37327;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to classical cognitive science which studied brains in isolation, ecological approaches focused on the role of the body and environment in shaping cognition. Similarly, in this thesis we adopt an ecological approach to grounded natural language understanding (NLU) research. Grounded language understanding studies language understanding systems situated in the context of events, actions and precepts in naturalistic/simulated virtual environments. Where classic research tends to focus on designing new models and optimization methods while treating environments as given, we explore the potential of environment design for improving data collection and model development. We developed novel training and annotation approaches for procedural text understanding based on text-based game environments. We also drew upon embodied cognitive linguistics literature to propose a roadmap for grounded NLP research, and to inform the development of a new benchmark for measuring the progress of
&lt;/p&gt;</description></item><item><title>&#24314;&#35758;&#23558;&#35748;&#30693;&#20219;&#21153;&#25972;&#21512;&#21040;&#22823;&#22411;&#27169;&#22411;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#20013;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#26234;&#33021;&#12290;&#36825;&#20010;&#26694;&#26550;&#32467;&#21512;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21253;&#21547;&#20102;&#31283;&#24577;&#26234;&#21147;&#12289;&#27969;&#24577;&#26234;&#21147;&#21644;&#31038;&#20132;&#26234;&#33021;&#31561;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.02547</link><description>&lt;p&gt;
&#23558;&#35748;&#30693;&#20219;&#21153;&#25972;&#21512;&#21040;&#22823;&#22411;&#27169;&#22411;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integration of cognitive tasks into artificial general intelligence test for large models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02547
&lt;/p&gt;
&lt;p&gt;
&#24314;&#35758;&#23558;&#35748;&#30693;&#20219;&#21153;&#25972;&#21512;&#21040;&#22823;&#22411;&#27169;&#22411;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#20013;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#26234;&#33021;&#12290;&#36825;&#20010;&#26694;&#26550;&#32467;&#21512;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21253;&#21547;&#20102;&#31283;&#24577;&#26234;&#21147;&#12289;&#27969;&#24577;&#26234;&#21147;&#21644;&#31038;&#20132;&#26234;&#33021;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#27169;&#22411;&#30340;&#21457;&#23637;&#36807;&#31243;&#20013;&#65292;&#24517;&#39035;&#23545;&#20013;&#38388;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#20197;&#35780;&#20272;&#20854;&#33021;&#21147;&#65292;&#24182;&#23545;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#24615;&#35780;&#20272;&#65292;&#20197;&#30830;&#20445;&#22312;&#23454;&#38469;&#24212;&#29992;&#20043;&#21069;&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#35780;&#20272;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#32570;&#20047;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#26234;&#33021;&#35780;&#20272;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#24314;&#31435;&#19968;&#20010;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#28385;&#36275;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#22823;&#22411;&#27169;&#22411;&#30340;&#27979;&#35797;&#38656;&#27714;&#65292;&#20197;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#35813;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#26694;&#26550;&#23558;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32852;&#31995;&#36215;&#26469;&#65292;&#21253;&#25324;&#26234;&#21147;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#31283;&#24577;&#26234;&#21147;&#65292;&#21363;&#31215;&#32047;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#30340;&#21453;&#26144;; &#27969;&#24577;&#26234;&#21147;&#65292;&#29305;&#28857;&#26159;&#35299;&#20915;&#38382;&#39064;&#21644;&#36866;&#24212;&#24615;&#25512;&#29702;; &#31038;&#20132;&#26234;&#33021;&#65292;&#34920;&#31034;&#22312;&#22810;&#26041;&#38754;&#29702;&#35299;&#21644;&#36866;&#24212;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the evolution of large models, performance evaluation is necessarily performed on the intermediate models to assess their capabilities, and on the well-trained model to ensure safety before practical application. However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models. In this perspective, we advocate for a comprehensive framework of artificial general intelligence (AGI) test, aimed at fulfilling the testing needs of large language models and multi-modal large models with enhanced capabilities. The AGI test framework bridges cognitive science and natural language processing to encompass the full spectrum of intelligence facets, including crystallized intelligence, a reflection of amassed knowledge and experience; fluid intelligence, characterized by problem-solving and adaptive reasoning; social intelligence, signifying comprehension and adaptation within multifacete
&lt;/p&gt;</description></item><item><title>LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02544</link><description>&lt;p&gt;
LHRS-Bot&#65306;&#21033;&#29992;VGI&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36965;&#24863;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02544
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38761;&#21629;&#24615;&#33021;&#21147;&#24320;&#21019;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24182;&#20419;&#36827;&#20102;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36965;&#24863;&#65288;RS&#65289;&#39046;&#22495;&#20013;&#65292;&#36817;&#26399;&#30340;MLLM&#21162;&#21147;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#36965;&#24863;&#22270;&#20687;&#20013;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LHRS-Align&#65292;&#20197;&#21450;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;RS&#29305;&#23450;&#25351;&#23548;&#25968;&#25454;&#38598;LHRS-Instruct&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;&#65288;VGI&#65289;&#21644;&#20840;&#29699;&#21487;&#29992;&#30340;RS&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LHRS-Bot&#65292;&#19968;&#31181;&#38024;&#23545;RS&#22270;&#20687;&#29702;&#35299;&#30340;MLLM&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#20013;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#20309;&#26102;&#19981;&#20877;&#22686;&#21152;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25509;&#36817;&#26465;&#20214;&#26469;&#20272;&#31639;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#30340;&#25509;&#36817;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#27169;&#22411;&#36873;&#25321;&#20013;&#23398;&#20064;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.02522</link><description>&lt;p&gt;
&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#20013;&#30340;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;
&lt;/p&gt;
&lt;p&gt;
Absolute convergence and error thresholds in non-active adaptive sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02522
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#20013;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#20309;&#26102;&#19981;&#20877;&#22686;&#21152;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25509;&#36817;&#26465;&#20214;&#26469;&#20272;&#31639;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#30340;&#25509;&#36817;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#27169;&#22411;&#36873;&#25321;&#20013;&#23398;&#20064;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#26159;&#19968;&#31181;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21160;&#24577;&#21644;&#33258;&#21160;&#22320;&#30830;&#23450;&#20445;&#35777;&#30340;&#26679;&#26412;&#22823;&#23567;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26080;&#35770;&#25152;&#37319;&#29992;&#30340;&#35843;&#24230;&#21644;&#29983;&#25104;&#24369;&#39044;&#27979;&#22120;&#30340;&#31574;&#30053;&#22914;&#20309;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#35745;&#31639;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#30340;&#36136;&#37327;&#20309;&#26102;&#19981;&#20877;&#22686;&#21152;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25509;&#36817;&#26465;&#20214;&#26469;&#32477;&#23545;&#22320;&#20272;&#31639;&#27169;&#22411;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#25509;&#36817;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#36827;&#34892;&#23398;&#20064;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;&#35813;&#25216;&#26415;&#22312;&#24037;&#20316;&#20551;&#35774;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#21644;&#23436;&#22791;&#24615;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#37319;&#26679;&#26041;&#26696;&#30340;&#40065;&#26834;&#24615;&#12290;&#27979;&#35797;&#32467;&#26524;&#31526;&#21512;&#25105;&#20204;&#30340;&#39044;&#26399;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#35789;&#24615;&#26631;&#27880;&#29983;&#25104;&#20026;&#26696;&#20363;&#30740;&#31350;&#26469;&#35828;&#26126;&#36825;&#19968;&#25552;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size. In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described. We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection. The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme. Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#22522;&#32447;(SIMPL)&#65292;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#20197;&#21450;&#20351;&#29992;Bernstein&#22522;&#22810;&#39033;&#24335;&#23545;&#36830;&#32493;&#36712;&#36857;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#20934;&#30830;&#30340;&#36816;&#21160;&#39044;&#27979;&#65292;&#20026;&#19979;&#28216;&#35268;&#21010;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.02519</link><description>&lt;p&gt;
SIMPL:&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#22522;&#32447;(SIMPL)&#65292;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#20197;&#21450;&#20351;&#29992;Bernstein&#22522;&#22810;&#39033;&#24335;&#23545;&#36830;&#32493;&#36712;&#36857;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#20934;&#30830;&#30340;&#36816;&#21160;&#39044;&#27979;&#65292;&#20026;&#19979;&#28216;&#35268;&#21010;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#36816;&#21160;&#39044;&#27979;&#22522;&#32447;&#65292;&#21517;&#20026;SIMPL&#12290;&#19982;&#20256;&#32479;&#30340;&#20197;&#26234;&#33021;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#34429;&#28982;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#20294;&#35745;&#31639;&#37325;&#22797;&#65292;&#20197;&#21450;&#20197;&#22330;&#26223;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#34429;&#28982;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#26377;&#25152;&#22949;&#21327;&#65292;SIMPL&#21487;&#20197;&#23454;&#26102;&#12289;&#20934;&#30830;&#22320;&#39044;&#27979;&#25152;&#26377;&#30456;&#20851;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#36816;&#21160;&#12290;&#20026;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#23454;&#29616;&#25913;&#36827;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#20197;&#23545;&#31216;&#26041;&#24335;&#25191;&#34892;&#23450;&#21521;&#28040;&#24687;&#20256;&#36882;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#39044;&#27979;&#25152;&#26377;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#26410;&#26469;&#36816;&#21160;&#65292;&#24182;&#20943;&#36731;&#35270;&#35282;&#36716;&#25442;&#24102;&#26469;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;Bernstein&#22522;&#22810;&#39033;&#24335;&#23545;&#36830;&#32493;&#36712;&#36857;&#21442;&#25968;&#21270;&#65292;&#20801;&#35768;&#22312;&#20219;&#20309;&#25152;&#38656;&#26102;&#38388;&#28857;&#35780;&#20272;&#36712;&#36857;&#21644;&#20854;&#39640;&#38454;&#23548;&#25968;&#65292;&#36825;&#23545;&#19979;&#28216;&#35268;&#21010;&#20219;&#21153;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;SIMPL&#25171;&#30772;&#20102;&#27169;&#24335;&#24182;&#25552;&#20379;&#20102;&#20934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL) for autonomous vehicles. Unlike conventional agent-centric methods with high accuracy but repetitive computations and scene-centric methods with compromised accuracy and generalizability, SIMPL delivers real-time, accurate motion predictions for all relevant traffic participants. To achieve improvements in both accuracy and inference speed, we propose a compact and efficient global feature fusion module that performs directed message passing in a symmetric manner, enabling the network to forecast future motion for all road users in a single feed-forward pass and mitigating accuracy loss caused by viewpoint shifting. Additionally, we investigate the continuous trajectory parameterization using Bernstein basis polynomials in trajectory decoding, allowing evaluations of states and their higher-order derivatives at any desired time point, which is valuable for downstream planning tasks. As a strong baseline, SIMP
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35843;&#24230;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#20013;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#37319;&#26679;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20998;&#26512;&#23398;&#20064;&#26354;&#32447;&#30340;&#24418;&#29366;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#22686;&#21152;&#25110;&#20943;&#23569;&#23454;&#20363;&#65292;&#20197;&#30830;&#20445;&#33719;&#24471;&#23398;&#20064;&#33021;&#21147;&#30340;&#20928;&#22686;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.02516</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#35843;&#24230;&#29992;&#20110;&#33258;&#36866;&#24212;&#37319;&#26679;&#22312;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#20013;
&lt;/p&gt;
&lt;p&gt;
Adaptive scheduling for adaptive sampling in POS taggers construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35843;&#24230;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#20013;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#37319;&#26679;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20998;&#26512;&#23398;&#20064;&#26354;&#32447;&#30340;&#24418;&#29366;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#22686;&#21152;&#25110;&#20943;&#23569;&#23454;&#20363;&#65292;&#20197;&#30830;&#20445;&#33719;&#24471;&#23398;&#20064;&#33021;&#21147;&#30340;&#20928;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35843;&#24230;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#20316;&#20026;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#19981;&#26174;&#33879;&#25439;&#22833;&#24615;&#33021;&#12290;&#19982;&#20043;&#21069;&#20351;&#29992;&#38543;&#26426;&#12289;&#22266;&#23450;&#25110;&#23450;&#26399;&#22686;&#21152;&#23454;&#20363;&#20043;&#38388;&#38388;&#38548;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20309;&#19978;&#20998;&#26512;&#23398;&#20064;&#26354;&#32447;&#30340;&#24418;&#29366;&#65292;&#32467;&#21512;&#21151;&#33021;&#27169;&#22411;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#22686;&#21152;&#25110;&#20943;&#23569;&#23454;&#20363;&#12290;&#35813;&#31639;&#27861;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20551;&#35774;&#19978;&#34987;&#35777;&#26126;&#26159;&#24418;&#24335;&#19978;&#27491;&#30830;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#26696;&#20363;&#65292;&#19979;&#19968;&#20010;&#26696;&#20363;&#26159;&#26368;&#36817;&#30340;&#65292;&#30830;&#20445;&#20174;&#21069;&#32773;&#20013;&#33719;&#24471;&#23398;&#20064;&#33021;&#21147;&#30340;&#20928;&#22686;&#30410;&#65292;&#21487;&#20197;&#35843;&#33410;&#27492;&#26465;&#20214;&#30340;&#35201;&#27714;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26356;&#21152;&#20851;&#27880;&#22312;&#35757;&#32451;&#25968;&#25454;&#24211;&#20013;&#20020;&#26102;&#24615;&#33021;&#33192;&#32960;&#30340;&#21306;&#22495;&#65292;&#25552;&#39640;&#20102;&#37319;&#26679;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#38450;&#27490;&#23398;&#20064;&#25552;&#21069;&#20572;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an adaptive scheduling for adaptive sampling as a novel way of machine learning in the construction of part-of-speech taggers. The goal is to speed up the training on large data sets, without significant loss of performance with regard to an optimal configuration. In contrast to previous methods using a random, fixed or regularly rising spacing between the instances, ours analyzes the shape of the learning curve geometrically in conjunction with a functional model to increase or decrease it at any time. The algorithm proves to be formally correct regarding our working hypotheses. Namely, given a case, the following one is the nearest ensuring a net gain of learning ability from the former, it being possible to modulate the level of requirement for this condition. We also improve the robustness of sampling by paying greater attention to those regions of the training data base subject to a temporary inflation in performance, thus preventing the learning from stopping prematu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#23398;&#20064;&#26354;&#32447;&#28436;&#21270;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32467;&#26524;&#26469;&#39044;&#27979;&#23398;&#20064;&#36807;&#31243;&#20013;&#36798;&#21040;&#26399;&#26395;&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.02515</link><description>&lt;p&gt;
&#23398;&#20064;&#26354;&#32447;&#24314;&#27169;&#21450;&#20854;&#22312;&#35789;&#24615;&#26631;&#27880;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Modeling of learning curves with applications to pos tagging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#23398;&#20064;&#26354;&#32447;&#28436;&#21270;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32467;&#26524;&#26469;&#39044;&#27979;&#23398;&#20064;&#36807;&#31243;&#20013;&#36798;&#21040;&#26399;&#26395;&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32467;&#26524;&#21644;&#20351;&#29992;&#21151;&#33021;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#23398;&#20064;&#26354;&#32447;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#36924;&#36817;&#25152;&#38656;&#26102;&#38388;&#28857;&#30340;&#24453;&#27714;&#20540;&#65292;&#29420;&#31435;&#20110;&#25152;&#20351;&#29992;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#32463;&#36807;&#19968;&#23450;&#30340;&#36807;&#31243;&#28857;&#65288;&#31216;&#20026;&#39044;&#27979;&#32423;&#21035;&#65289;&#21518;&#12290;&#35813;&#25552;&#26696;&#22312;&#24037;&#20316;&#20551;&#35774;&#26041;&#38754;&#34987;&#35777;&#26126;&#26159;&#24418;&#24335;&#19978;&#27491;&#30830;&#30340;&#65292;&#24182;&#19988;&#21253;&#21547;&#19968;&#20010;&#21487;&#38752;&#30340;&#36817;&#20284;&#26465;&#20214;&#12290;&#36825;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#22522;&#20110;&#26368;&#32456;&#21487;&#23454;&#29616;&#30340;&#20934;&#30830;&#24230;&#26469;&#35774;&#23450;&#25910;&#25947;&#38408;&#20540;&#65292;&#36825;&#25193;&#23637;&#20102;&#20572;&#27490;&#20934;&#21017;&#30340;&#27010;&#24565;&#65292;&#21363;&#20351;&#23384;&#22312;&#25197;&#26354;&#35266;&#23519;&#32467;&#26524;&#65292;&#20063;&#20284;&#20046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#22521;&#35757;&#24037;&#20316;&#37327;&#65292;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#26469;&#20943;&#23569;&#23398;&#20064;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#20154;&#21147;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#35813;&#25552;&#26696;&#22312;&#33267;&#23569;&#19977;&#20010;&#25805;&#20316;&#31243;&#24207;&#20013;&#24456;&#26377;&#20852;&#36259;&#12290;&#31532;&#19968;&#20010;&#26159;&#39044;&#27979;&#23398;&#20064;&#36807;&#31243;&#20013;&#36798;&#21040;&#26399;&#26395;&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm to estimate the evolution of learning curves on the whole of a training data base, based on the results obtained from a portion and using a functional strategy, is introduced. We approximate iteratively the sought value at the desired time, independently of the learning technique used and once a point in the process, called prediction level, has been passed. The proposal proves to be formally correct with respect to our working hypotheses and includes a reliable proximity condition. This allows the user to fix a convergence threshold with respect to the accuracy finally achievable, which extends the concept of stopping criterion and seems to be effective even in the presence of distorting observations.   Our aim is to evaluate the training effort, supporting decision making in order to reduce the need for both human and computational resources during the learning process. The proposal is of interest in at least three operational procedures. The first is the anticipation of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31995;&#21015;&#22312;&#32447;&#25351;&#26631;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25552;&#21069;&#20572;&#27490;&#26465;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02513</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#30456;&#20851;&#22312;&#32447;&#25351;&#26631;&#26469;&#25552;&#21069;&#20572;&#27490;
&lt;/p&gt;
&lt;p&gt;
Early stopping by correlating online indicators in neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31995;&#21015;&#22312;&#32447;&#25351;&#26631;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25552;&#21069;&#20572;&#27490;&#26465;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26368;&#23567;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#22312;&#35757;&#32451;&#23398;&#20064;&#32773;&#26102;&#35782;&#21035;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#36825;&#20351;&#24471;&#25903;&#25345;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#25552;&#21069;&#20572;&#27490;&#26465;&#20214;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35813;&#31867;&#22411;&#24314;&#27169;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#21033;&#29992;&#19968;&#31995;&#21015;&#22312;&#32447;&#25351;&#26631;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#21363;&#29992;&#20110;&#25351;&#31034;&#19968;&#32452;&#20551;&#35774;&#26159;&#21542;&#28385;&#36275;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#19982;&#20174;&#37329;&#19997;&#38592;&#21028;&#26029;&#20013;&#26500;&#24314;&#30340;&#19968;&#31995;&#21015;&#29420;&#31435;&#20572;&#27490;&#26465;&#20214;&#30456;&#32852;&#31995;&#65292;&#20197;&#35780;&#20272;&#36807;&#25311;&#21512;&#30340;&#23384;&#22312;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#20026;&#20915;&#31574;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#30340;&#22522;&#30784;&#65292;&#20197;&#20013;&#26029;&#23398;&#20064;&#36807;&#31243;&#12290;&#19982;&#20043;&#21069;&#19987;&#27880;&#20110;&#21333;&#19968;&#26631;&#20934;&#30340;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#20043;&#38388;&#30340;&#38468;&#24102;&#25928;&#24212;&#65292;&#23547;&#27714;&#26356;&#24191;&#27867;&#30340;&#25805;&#20316;&#33539;&#22260;&#21644;&#26356;&#22823;&#30340;&#35786;&#26029;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to minimize the generalization error in neural networks, a novel technique to identify overfitting phenomena when training the learner is formally introduced. This enables support of a reliable and trustworthy early stopping condition, thus improving the predictive power of that type of modeling. Our proposal exploits the correlation over time in a collection of online indicators, namely characteristic functions for indicating if a set of hypotheses are met, associated with a range of independent stopping conditions built from a canary judgment to evaluate the presence of overfitting. That way, we provide a formal basis for decision making in terms of interrupting the learning process.   As opposed to previous approaches focused on a single criterion, we take advantage of subsidiarities between independent assessments, thus seeking both a wider operating range and greater diagnostic reliability. With a view to illustrating the effectiveness of the halting condition described, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.02500</link><description>&lt;p&gt;
&#28857;&#20113;&#38382;&#39064;:&#37325;&#26032;&#24605;&#32771;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#31181;&#20027;&#35201;&#27169;&#24577;&#65306;RGB&#65292;RGB-D&#21644;&#28857;&#20113;&#12290;&#36890;&#36807;&#22312;&#36229;&#36807;17&#20010;&#19981;&#21516;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#28041;&#21450;&#20004;&#20010;&#22522;&#20934;&#21644;&#20223;&#30495;&#22120;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36235;&#21183;&#65306;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35774;&#35745;&#65292;&#36890;&#24120;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;RGB&#21644;RGB-D&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#22312;&#30456;&#26426;&#35270;&#35282;&#12289;&#29031;&#26126;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#32972;&#26223;&#22806;&#35266;&#31561;&#21508;&#31181;&#20960;&#20309;&#21644;&#35270;&#35273;&#32447;&#32034;&#26041;&#38754;&#65292;&#37117;&#33021;&#25552;&#39640;&#31574;&#30053;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19977;&#32500;&#28857;&#20113;&#26159;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#26377;&#20215;&#20540;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#26816;&#26597;&#28857;&#65292;&#24076;&#26395;&#25105;&#20204;&#30340;&#35266;&#28857;&#33021;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20840;&#21487;&#24494;&#30456;&#20851;&#39537;&#21160;&#32593;&#32476;&#29992;&#20110;X&#23556;&#32447;&#19982;CT&#22270;&#20687;&#34701;&#21512;&#30340;&#37197;&#20934;&#65292;&#36890;&#36807;&#21452;&#20998;&#25903;CNN-Transformer&#32534;&#30721;&#22120;&#23454;&#29616;&#20302;&#39057;&#20840;&#23616;&#29305;&#24449;&#21644;&#39640;&#39057;&#23616;&#37096;&#29305;&#24449;&#30340;&#25552;&#21462;&#21644;&#20998;&#31163;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#30456;&#20851;&#39537;&#21160;&#25439;&#22833;&#36827;&#34892;&#29305;&#24449;&#20998;&#35299;&#65292;&#24182;&#24212;&#29992;&#20984;&#24418;&#30456;&#20284;&#20989;&#25968;&#23398;&#20064;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#20840;&#21487;&#24494;&#23398;&#20064;&#37197;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02498</link><description>&lt;p&gt;
X&#23556;&#32447;&#19982;CT&#22270;&#20687;&#34701;&#21512;&#30340;&#20840;&#21487;&#24494;&#30456;&#20851;&#39537;&#21160;2D/3D&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Fully Differentiable Correlation-driven 2D/3D Registration for X-ray to CT Image Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20840;&#21487;&#24494;&#30456;&#20851;&#39537;&#21160;&#32593;&#32476;&#29992;&#20110;X&#23556;&#32447;&#19982;CT&#22270;&#20687;&#34701;&#21512;&#30340;&#37197;&#20934;&#65292;&#36890;&#36807;&#21452;&#20998;&#25903;CNN-Transformer&#32534;&#30721;&#22120;&#23454;&#29616;&#20302;&#39057;&#20840;&#23616;&#29305;&#24449;&#21644;&#39640;&#39057;&#23616;&#37096;&#29305;&#24449;&#30340;&#25552;&#21462;&#21644;&#20998;&#31163;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#30456;&#20851;&#39537;&#21160;&#25439;&#22833;&#36827;&#34892;&#29305;&#24449;&#20998;&#35299;&#65292;&#24182;&#24212;&#29992;&#20984;&#24418;&#30456;&#20284;&#20989;&#25968;&#23398;&#20064;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#20840;&#21487;&#24494;&#23398;&#20064;&#37197;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#21018;&#24615;2D/3D&#37197;&#20934;&#26159;&#23548;&#21521;&#22806;&#31185;&#25163;&#26415;&#24178;&#39044;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20123;&#22522;&#20110;&#23398;&#20064;&#30340;&#20840;&#21487;&#24494;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#20294;&#29305;&#24449;&#25552;&#21462;&#21644;&#26799;&#24230;&#20256;&#36882;&#36807;&#31243;&#20173;&#32570;&#20047;&#21487;&#25511;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#21487;&#24494;&#30456;&#20851;&#39537;&#21160;&#32593;&#32476;&#65292;&#21033;&#29992;&#21452;&#20998;&#25903;CNN-Transformer&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#20302;&#39057;&#20840;&#23616;&#29305;&#24449;&#21644;&#39640;&#39057;&#23616;&#37096;&#29305;&#24449;&#30340;&#25552;&#21462;&#21644;&#20998;&#31163;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23884;&#20449;&#24687;&#30340;&#20302;&#39057;&#29305;&#24449;&#21644;&#39640;&#39057;&#29305;&#24449;&#20998;&#35299;&#30340;&#30456;&#20851;&#39537;&#21160;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#19968;&#31181;&#23398;&#20064;&#36924;&#36817;&#20984;&#24418;&#30456;&#20284;&#20989;&#25968;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#33258;&#24314;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#20840;&#21487;&#24494;&#23398;&#20064;&#37197;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based rigid 2D/3D registration is a critical technique for fluoroscopic guided surgical interventions. In recent years, some learning-based fully differentiable methods have produced beneficial outcomes while the process of feature extraction and gradient flow transmission still lack controllability and interpretability. To alleviate these problems, in this work, we propose a novel fully differentiable correlation-driven network using a dual-branch CNN-transformer encoder which enables the network to extract and separate low-frequency global features from high-frequency local features. A correlation-driven loss is further proposed for low-frequency feature and high-frequency feature decomposition based on embedded information. Besides, a training strategy that learns to approximate a convex-shape similarity function is applied in our work. We test our approach on a in-house datasetand show that it outperforms both existing fully differentiable learning-based registration approach
&lt;/p&gt;</description></item><item><title>BRAIn&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22870;&#21169;&#26465;&#20214;&#21270;&#32553;&#20943;&#25512;&#26029;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26469;&#25913;&#36827;RLHF&#65292;&#22312;LLM&#23545;&#40784;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02479</link><description>&lt;p&gt;
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
&lt;/p&gt;
&lt;p&gt;
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02479
&lt;/p&gt;
&lt;p&gt;
BRAIn&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22870;&#21169;&#26465;&#20214;&#21270;&#32553;&#20943;&#25512;&#26029;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26469;&#25913;&#36827;RLHF&#65292;&#22312;LLM&#23545;&#40784;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#32487;Proximal Policy Optimization (PPO)&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22914;Sequence Likelihood Calibration (SLiC)&#21644;Direct Policy Optimization (DPO)&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#31163;&#32447;&#30340;&#65292;&#24182;&#19988;&#20197;&#38388;&#25509;&#30340;&#26041;&#24335;&#20351;&#29992;&#22870;&#21169;&#12290;&#36825;&#20123;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;DPO&#65292;&#30001;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#65292;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;LLM&#23545;&#40784;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36951;&#28431;&#20102;PPO&#26041;&#27861;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#35832;&#22914;SLiC&#25110;RRHF&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;(RM)&#36827;&#34892;&#25490;&#24207;/&#20559;&#22909;&#65292;&#20002;&#22833;&#20102;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#24573;&#30053;&#20102;RM&#30340;&#21442;&#25968;&#24418;&#24335;(&#20363;&#22914;Bradley-Terry&#12289;Plackett-Luce)&#65307;&#32780;&#35832;&#22914;DPO&#30340;&#26041;&#27861;&#29978;&#33267;&#19981;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;BRAIn&#65292;&#23427;&#23558;RM&#20316;&#20026;&#20998;&#24067;&#21305;&#37197;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#37325;&#26032;&#24341;&#20837;&#12290;BRAIn&#32771;&#34385;&#21040;&#20102;LLM&#20998;&#24067;&#22312;&#20551;&#35774;&#36755;&#20986;&#36136;&#37327;&#33391;&#22909;&#30340;&#26465;&#20214;&#19979;&#65292;&#24182;&#24212;&#29992;B...
&lt;/p&gt;
&lt;p&gt;
Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner. These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance. However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model. In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies B
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#23545;&#20026;&#20160;&#20040;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#26469;&#22686;&#24378;&#23618;&#32423;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02478</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#65311;&#20851;&#20110;&#23618;&#32423;&#34920;&#31034;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Why are hyperbolic neural networks effective? A study on hierarchical representation capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#23545;&#20026;&#20160;&#20040;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#26469;&#22686;&#24378;&#23618;&#32423;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#65288;HNNs&#65289;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36816;&#20316;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#21160;&#26426;&#26159;&#21452;&#26354;&#31354;&#38388;&#20013;&#23384;&#22312;&#19968;&#31181;&#20248;&#21270;&#23884;&#20837;&#65292;&#33021;&#22815;&#27604;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#26356;&#20934;&#30830;&#22320;&#20445;&#30041;&#25968;&#25454;&#30340;&#23618;&#32423;&#20851;&#31995;&#65288;&#31216;&#20026;&#23618;&#32423;&#34920;&#31034;&#33021;&#21147;&#65292;HRC&#65289;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;HNN&#21487;&#20197;&#36798;&#21040;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#23884;&#20837;&#65292;&#36825;&#23548;&#33268;&#20102;&#35768;&#22810;&#30740;&#31350;&#24314;&#31435;&#22312;&#38169;&#35823;&#30340;&#21160;&#26426;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;HRC&#30340;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#23545;HNN&#20026;&#20309;&#26377;&#25928;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#21463;&#21040;&#20998;&#26512;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#26469;&#22686;&#24378;HRC&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#20998;&#26512;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HNN&#26080;&#27861;&#23454;&#29616;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#23884;&#20837;&#12290;HRC&#21463;&#20248;&#21270;&#30446;&#26631;&#21644;&#23618;&#32423;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Neural Networks (HNNs), operating in hyperbolic space, have been widely applied in recent years, motivated by the existence of an optimal embedding in hyperbolic space that can preserve data hierarchical relationships (termed Hierarchical Representation Capability, HRC) more accurately than Euclidean space. However, there is no evidence to suggest that HNNs can achieve this theoretical optimal embedding, leading to much research being built on flawed motivations. In this paper, we propose a benchmark for evaluating HRC and conduct a comprehensive analysis of why HNNs are effective through large-scale experiments. Inspired by the analysis results, we propose several pre-training strategies to enhance HRC and improve the performance of downstream tasks, further validating the reliability of the analysis. Experiments show that HNNs cannot achieve the theoretical optimal embedding. The HRC is significantly affected by the optimization objectives and hierarchical structures, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#12290;&#36890;&#36807;&#22870;&#21169;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#26377;&#25928;&#35782;&#21035;&#21516;&#20276;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;&#20307;&#31215;&#26497;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#22312;&#19981;&#30830;&#23450;&#21516;&#20276;&#31574;&#30053;&#26102;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02468</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#25506;&#32034;&#30340;&#24555;&#36895;&#36866;&#24212;&#26410;&#30693;&#21516;&#20276;
&lt;/p&gt;
&lt;p&gt;
Fast Peer Adaptation with Context-aware Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#12290;&#36890;&#36807;&#22870;&#21169;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#26377;&#25928;&#35782;&#21035;&#21516;&#20276;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;&#20307;&#31215;&#26497;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#22312;&#19981;&#30830;&#23450;&#21516;&#20276;&#31574;&#30053;&#26102;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#65292;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#35782;&#21035;&#21516;&#20276;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#26159;&#36866;&#24212;&#20013;&#36827;&#34892;&#26368;&#20339;&#21453;&#24212;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#24403;&#28216;&#25103;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#19988;&#26102;&#38388;&#36328;&#24230;&#24456;&#38271;&#26102;&#65292;&#25506;&#32034;&#26410;&#30693;&#21516;&#20276;&#30340;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#20276;&#35782;&#21035;&#22870;&#21169;&#65292;&#26681;&#25454;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#29615;&#22659;&#19979;&#65288;&#20363;&#22914;&#22810;&#20010;&#22238;&#21512;&#30340;&#35266;&#23519;&#65289;&#22914;&#20309;&#26377;&#25928;&#22320;&#35782;&#21035;&#21516;&#20276;&#30340;&#34892;&#20026;&#27169;&#24335;&#26469;&#22870;&#21169;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36825;&#20010;&#22870;&#21169;&#28608;&#21169;&#26234;&#33021;&#20307;&#23398;&#20064;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#21363;&#22312;&#23545;&#21516;&#20276;&#31574;&#30053;&#19981;&#30830;&#23450;&#26102;&#31215;&#26497;&#23547;&#25214;&#21644;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games. To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation. However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon. In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes. This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident. We evaluate our method on diverse testbeds 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GraphsGPT&#65292;&#23427;&#20351;&#29992;&#32431;Transformer&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#30340;&#22270;&#24418;&#21333;&#35789;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#23558;&#22270;&#24418;&#21333;&#35789;&#37325;&#26032;&#26500;&#24314;&#20026;&#21407;&#22987;&#22270;&#24418;&#65292;&#20445;&#35777;&#20102;&#20449;&#24687;&#30340;&#31561;&#20215;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;GraphsGPT&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#24418;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#20986;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02464</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#20540;&#21315;&#35328;&#65306;&#20351;&#29992;&#32431;Transformer&#23558;&#22270;&#24418;&#27431;&#25289;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GraphsGPT&#65292;&#23427;&#20351;&#29992;&#32431;Transformer&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#30340;&#22270;&#24418;&#21333;&#35789;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#23558;&#22270;&#24418;&#21333;&#35789;&#37325;&#26032;&#26500;&#24314;&#20026;&#21407;&#22987;&#22270;&#24418;&#65292;&#20445;&#35777;&#20102;&#20449;&#24687;&#30340;&#31561;&#20215;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;GraphsGPT&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#24418;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#20986;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#24314;&#27169;&#20026;&#32431;&#35821;&#35328;&#29978;&#33267;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#22266;&#26377;&#20449;&#24687;&#65311;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#19968;&#30452;&#26159;&#22270;&#24418;&#24314;&#27169;&#20013;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;GNN&#21644;Graphformer&#21162;&#21147;&#23558;&#22270;&#24418;&#32534;&#30721;&#20026;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#65292;&#20294;&#20174;&#21521;&#37327;&#20013;&#24674;&#22797;&#20986;&#21407;&#22987;&#22270;&#24418;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GraphsGPT&#65292;&#23427;&#20855;&#26377;&#19968;&#20010;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#22270;&#24418;&#21333;&#35789;&#30340;Graph2Seq&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#20174;&#22270;&#24418;&#21333;&#35789;&#37325;&#26500;&#21407;&#22987;&#22270;&#24418;&#20197;&#30830;&#20445;&#20449;&#24687;&#31561;&#20215;&#24615;&#30340;GraphGPT&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;100M&#20010;&#20998;&#23376;&#19978;&#39044;&#35757;&#32451;&#20102;GraphsGPT&#65292;&#24182;&#24471;&#21040;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;(1) &#39044;&#35757;&#32451;&#30340;Graph2Seq&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;8/9&#20010;&#22270;&#24418;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;(2) &#39044;&#35757;&#32451;&#30340;GraphGPT&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#29983;&#25104;&#22120;&#65292;&#20854;&#33021;&#22815;&#36827;&#34892;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#30340;&#22270;&#24418;&#29983;&#25104;&#12290;(3) Graph2Seq+Gr
&lt;/p&gt;
&lt;p&gt;
Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+Gr
&lt;/p&gt;</description></item><item><title>TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;</title><link>https://arxiv.org/abs/2402.02441</link><description>&lt;p&gt;
TopoX: &#19968;&#20010;&#29992;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TopoX: A Suite of Python Packages for Machine Learning on Topological Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02441
&lt;/p&gt;
&lt;p&gt;
TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;topox&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#65288;&#25193;&#23637;&#20102;&#22270;&#30340;&#39046;&#22495;&#65289;&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65306;&#36229;&#22270;&#12289;&#21333;&#32431;&#12289;&#32990;&#33108;&#12289;&#36335;&#24452;&#21644;&#32452;&#21512;&#22797;&#21512;&#20307;&#12290;topox&#30001;&#19977;&#20010;&#36719;&#20214;&#21253;&#32452;&#25104;&#65306;toponetx&#29992;&#20110;&#26500;&#24314;&#21644;&#35745;&#31639;&#36825;&#20123;&#22495;&#65292;&#21253;&#25324;&#33410;&#28857;&#12289;&#36793;&#21644;&#39640;&#38454;&#21333;&#20803;&#30340;&#22788;&#29702;&#65307;topoembedx&#25552;&#20379;&#20102;&#23558;&#25299;&#25169;&#22495;&#23884;&#20837;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#31639;&#27861;&#65292;&#22914;node2vec&#65307;topomodelx&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20026;&#25299;&#25169;&#22495;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;topox&#30340;&#28304;&#20195;&#30721;&#32463;&#36807;&#24191;&#27867;&#30340;&#25991;&#26723;&#21270;&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#22312;https://github.com/pyt-team&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
&lt;/p&gt;</description></item><item><title>DiffStitch&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02439</link><description>&lt;p&gt;
DiffStitch: &#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02439
&lt;/p&gt;
&lt;p&gt;
DiffStitch&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#26368;&#20339;&#36712;&#36857;&#65292;&#36825;&#32473;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#24517;&#39035;&#33719;&#24471;&#21040;&#36798;&#39640;&#22870;&#21169;&#21306;&#22495;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#65288;DiffStitch&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#65292;&#23427;&#21487;&#20197;&#31995;&#32479;&#22320;&#29983;&#25104;&#36712;&#36857;&#20043;&#38388;&#30340;&#25340;&#25509;&#36716;&#25442;&#12290;DiffStitch&#21487;&#20197;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;DiffStitch&#22312;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DiffStitch&#22312;&#19968;&#27493;&#26041;&#27861;&#65288;IQL&#65289;&#12289;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;TD3+BC&#65289;&#21644;&#36712;&#36857;&#26041;&#27861;&#65288;PPO&#65289;&#30340;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and traje
&lt;/p&gt;</description></item><item><title>Uni-RLHF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;&#65292;&#33268;&#21147;&#20110;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;RLHF&#20013;&#37327;&#21270;&#36827;&#23637;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#21644;&#31163;&#32447;&#22522;&#20934;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.02423</link><description>&lt;p&gt;
Uni-RLHF: &#29992;&#20110;&#22810;&#26679;&#21270;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#36890;&#29992;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02423
&lt;/p&gt;
&lt;p&gt;
Uni-RLHF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;&#65292;&#33268;&#21147;&#20110;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;RLHF&#20013;&#37327;&#21270;&#36827;&#23637;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#21644;&#31163;&#32447;&#22522;&#20934;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#36890;&#36807;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#25163;&#21160;&#22870;&#21169;&#35774;&#35745;&#65292;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#32771;&#34385;&#21040;&#19981;&#21516;&#29615;&#22659;&#20013;&#19981;&#21516;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#31867;&#22411;&#23545;RLHF&#30340;&#36827;&#27493;&#36827;&#34892;&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27880;&#37322;&#24179;&#21488;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#32479;&#19968;&#22522;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Uni-RLHF&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;RLHF&#37327;&#36523;&#23450;&#21046;&#30340;&#32508;&#21512;&#31995;&#32479;&#23454;&#29616;&#12290;&#23427;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#23436;&#25972;&#30340;&#20174;&#30495;&#23454;&#20154;&#31867;&#21453;&#39304;&#21040;&#23454;&#38469;&#38382;&#39064;&#21457;&#23637;&#30340;&#24037;&#20316;&#27969;&#12290;Uni-RLHF&#21253;&#21547;&#19977;&#20010;&#37096;&#20998;&#65306;1&#65289;&#36890;&#29992;&#30340;&#22810;&#21453;&#39304;&#27880;&#37322;&#24179;&#21488;&#65292;2&#65289;&#22823;&#35268;&#27169;&#30340;&#20247;&#21253;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;3&#65289;&#27169;&#22359;&#21270;&#30340;&#31163;&#32447;RLHF&#22522;&#20934;&#23454;&#29616;&#12290;Uni-RLHF&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#21453;&#39304;&#31867;&#22411;&#65292;&#24182;&#19982;&#20027;&#35201;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. It is crucial to consider diverse human feedback types and various learning methods in different environments. However, quantifying progress in RLHF with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce Uni-RLHF, a comprehensive system implementation tailored for RLHF. It aims to provide a complete workflow from real human feedback, fostering progress in the development of practical problems. Uni-RLHF contains three packages: 1) a universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations. Uni-RLHF develops a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of main
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>FreDF&#26159;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#26631;&#31614;&#24207;&#21015;&#30340;&#33258;&#30456;&#20851;&#38382;&#39064;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#39044;&#27979;&#27169;&#22411;&#20860;&#23481;&#12290;</title><link>https://arxiv.org/abs/2402.02399</link><description>&lt;p&gt;
FreDF: &#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FreDF: Learning to Forecast in Frequency Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02399
&lt;/p&gt;
&lt;p&gt;
FreDF&#26159;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#26631;&#31614;&#24207;&#21015;&#30340;&#33258;&#30456;&#20851;&#38382;&#39064;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#39044;&#27979;&#27169;&#22411;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#22312;&#21382;&#21490;&#24207;&#21015;&#21644;&#26631;&#31614;&#24207;&#21015;&#20013;&#37117;&#38754;&#20020;&#33258;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22788;&#29702;&#21382;&#21490;&#24207;&#21015;&#20013;&#30340;&#33258;&#30456;&#20851;&#38382;&#39064;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#26631;&#31614;&#24207;&#21015;&#20013;&#30340;&#33258;&#30456;&#20851;&#23384;&#22312;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26032;&#20852;&#30340;&#39044;&#27979;&#27169;&#22411;&#20027;&#35201;&#36981;&#24490;&#30452;&#25509;&#39044;&#27979;&#65288;DF&#65289;&#33539;&#24335;&#65292;&#22312;&#26631;&#31614;&#24207;&#21015;&#20013;&#20551;&#35774;&#26465;&#20214;&#29420;&#31435;&#24615;&#19979;&#29983;&#25104;&#22810;&#27493;&#39044;&#27979;&#12290;&#36825;&#31181;&#20551;&#35774;&#24573;&#35270;&#20102;&#26631;&#31614;&#24207;&#21015;&#20013;&#22266;&#26377;&#30340;&#33258;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22522;&#20110;DF&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39057;&#22495;&#22686;&#24378;&#30452;&#25509;&#39044;&#27979;&#65288;FreDF&#65289;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;&#26469;&#36991;&#20813;&#26631;&#31614;&#33258;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FreDF&#22312;&#24615;&#33021;&#19978;&#22823;&#22823;&#36229;&#36807;&#20102;&#21253;&#25324;iTransformer&#22312;&#20869;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#39044;&#27979;&#27169;&#22411;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series modeling is uniquely challenged by the presence of autocorrelation in both historical and label sequences. Current research predominantly focuses on handling autocorrelation within the historical sequence but often neglects its presence in the label sequence. Specifically, emerging forecast models mainly conform to the direct forecast (DF) paradigm, generating multi-step forecasts under the assumption of conditional independence within the label sequence. This assumption disregards the inherent autocorrelation in the label sequence, thereby limiting the performance of DF-based models. In response to this gap, we introduce the Frequency-enhanced Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation by learning to forecast in the frequency domain. Our experiments demonstrate that FreDF substantially outperforms existing state-of-the-art methods including iTransformer and is compatible with a variety of forecast models.
&lt;/p&gt;</description></item><item><title>DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02392</link><description>&lt;p&gt;
DeLLMa:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#20915;&#31574;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02392
&lt;/p&gt;
&lt;p&gt;
DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#12289;&#24037;&#31243;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#24448;&#24448;&#38754;&#20020;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20915;&#31574;&#38382;&#39064;&#19978;&#30452;&#25509;&#20351;&#29992;LLMs&#24448;&#24448;&#25928;&#26524;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeLLMa&#65288;Decision-making Large Language Model assistant&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#31934;&#24230;&#12290;DeLLMa&#21253;&#25324;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20102;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;&#12289;&#21487;&#23457;&#35745;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#20892;&#19994;&#21644;&#37329;&#34701;&#25968;&#25454;&#30340;&#20915;&#31574;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DeLLMa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#20915;&#31574;&#24615;&#33021;&#65292;&#20934;&#30830;&#24615;&#21487;&#25552;&#39640;&#39640;&#36798;40%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAGE&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;ABM&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#36741;&#21161;&#39564;&#35777;&#21644;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02388</link><description>&lt;p&gt;
&#20351;&#29992;&#36741;&#21161;&#39564;&#35777;&#30340;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#29983;&#25104;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAGE&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;ABM&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#36741;&#21161;&#39564;&#35777;&#21644;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#65288;ABM&#65289;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#25552;&#20986;&#21644;&#39564;&#35777;&#38024;&#23545;&#22797;&#26434;&#31995;&#32479;&#25152;&#25552;&#20986;&#30340;&#20551;&#35774;&#35299;&#20915;&#26041;&#26696;&#25110;&#25919;&#31574;&#65292;&#24182;&#23454;&#29616;&#21508;&#31181;&#30446;&#26631;&#12290;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#36328;&#23398;&#31185;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#20855;&#26377;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#32534;&#31243;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26377;&#28508;&#21147;&#20943;&#36731;&#36825;&#20010;&#36807;&#31243;&#30340;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;LLM&#25797;&#38271;&#22788;&#29702;&#24207;&#21015;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;ABM&#20013;&#30340;&#22797;&#26434;&#20132;&#20114;&#21644;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21478;&#22806;&#65292;&#30001;&#20110;LLM&#32570;&#20047;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#20165;&#20165;&#20381;&#38752;LLM&#26159;&#26080;&#27861;&#26377;&#25928;&#23436;&#25104;&#36825;&#20010;&#36807;&#31243;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAGE&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;ABM&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#21327;&#20316;&#36335;&#30001;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#35745;&#31639;&#19982;&#32593;&#32476;&#34701;&#21512;&#31995;&#32479;&#65292;&#20197;&#30830;&#20445;&#25130;&#27490;&#26085;&#26399;&#35201;&#27714;&#24182;&#26368;&#23567;&#21270;&#35831;&#27714;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.02381</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#21327;&#20316;&#36335;&#30001;&#22686;&#24378;&#35745;&#31639;&#19982;&#32593;&#32476;&#34701;&#21512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Empowering Computing and Networks Convergence System with Distributed Cooperative Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#21327;&#20316;&#36335;&#30001;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#35745;&#31639;&#19982;&#32593;&#32476;&#34701;&#21512;&#31995;&#32479;&#65292;&#20197;&#30830;&#20445;&#25130;&#27490;&#26085;&#26399;&#35201;&#27714;&#24182;&#26368;&#23567;&#21270;&#35831;&#27714;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#24212;&#29992;&#30340;&#20986;&#29616;&#21644;&#35745;&#31639;&#19982;&#32593;&#32476;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#25512;&#21160;&#20102;&#35745;&#31639;&#19982;&#32593;&#32476;&#34701;&#21512;&#65288;CNC&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#26410;&#33021;&#23454;&#29616;&#23545;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#20840;&#38754;&#35843;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#19981;&#36275;&#23548;&#33268;&#20102;&#26576;&#20123;&#35745;&#31639;&#35831;&#27714;&#22312;&#31471;&#21040;&#31471;&#26381;&#21153;&#27169;&#24335;&#19979;&#26080;&#27861;&#20445;&#35777;&#38656;&#27714;&#65292;&#24182;&#23545;CNC&#31995;&#32479;&#30340;&#21457;&#23637;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;CNC&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#21327;&#20316;&#36335;&#30001;&#26694;&#26550;&#65292;&#20197;&#30830;&#20445;&#25130;&#27490;&#26085;&#26399;&#35201;&#27714;&#24182;&#26368;&#23567;&#21270;&#35831;&#27714;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20132;&#26131;&#24179;&#38754;&#12289;&#31649;&#29702;&#24179;&#38754;&#12289;&#25511;&#21046;&#24179;&#38754;&#21644;&#36716;&#21457;&#24179;&#38754;&#12290;&#36328;&#24179;&#38754;&#21327;&#20316;&#31471;&#21040;&#31471;&#36335;&#30001;&#26041;&#26696;&#22312;&#21046;&#23450;&#36335;&#30001;&#35745;&#21010;&#26102;&#32771;&#34385;&#20102;&#24322;&#26500;&#26381;&#21153;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#32593;&#32476;&#25317;&#22622;&#31243;&#24230;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#22312;&#20309;&#22788;&#25191;&#34892;&#35831;&#27714;&#21644;&#30456;&#24212;&#30340;&#36335;&#30001;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of intelligent applications and recent advances in the fields of computing and networks are driving the development of computing and networks convergence (CNC) system. However, existing researches failed to achieve comprehensive scheduling optimization of computing and network resources. This shortfall results in some requirements of computing requests unable to be guaranteed in an end-to-end service pattern, negatively impacting the development of CNC systems. In this article, we propose a distributed cooperative routing framework for the CNC system to ensure the deadline requirements and minimize the computation cost of requests. The framework includes trading plane, management plane, control plane and forwarding plane. The cross-plane cooperative end-to-end routing schemes consider both computation efficiency of heterogeneous servers and the network congestion degrees while making routing plan, thereby determining where to execute requests and corresponding routing pat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02380</link><description>&lt;p&gt;
&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#26102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models in Analysing Classroom Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#25945;&#23398;&#35786;&#26029;&#21644;&#36136;&#37327;&#25913;&#36827;&#30340;&#37325;&#35201;&#30740;&#31350;&#20219;&#21153;&#12290;&#37492;&#20110;&#20256;&#32479;&#25945;&#32946;&#30740;&#31350;&#20013;&#30693;&#35782;&#23494;&#38598;&#21644;&#21171;&#21160;&#23494;&#38598;&#30340;&#23450;&#24615;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLM&#22312;&#20248;&#21270;&#21644;&#22686;&#24378;&#20998;&#26512;&#36807;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;&#20013;&#23398;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#23398;&#21644;&#35821;&#25991;&#35838;&#22530;&#19978;&#30340;&#23545;&#35805;&#12290;&#36825;&#20123;&#23545;&#35805;&#30001;&#25945;&#32946;&#19987;&#23478;&#25163;&#21160;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#21046;&#30340;GPT-4&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#27604;&#36739;&#25163;&#21160;&#27880;&#37322;&#19982;GPT-4&#30340;&#36755;&#20986;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20998;&#26512;&#25945;&#32946;&#23545;&#35805;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#35780;&#20272;&#26102;&#38388;&#25928;&#29575;&#12289;&#32534;&#30721;&#32773;&#38388;&#19968;&#33268;&#24615;&#21644;&#32534;&#30721;&#32773;&#38388;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GPT-4&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#24182;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#19987;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;MedSASS&#65292;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#31471;&#21040;&#31471;&#35757;&#32451;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.02367</link><description>&lt;p&gt;
&#25506;&#32034;&#21307;&#23398;&#22270;&#20687;&#30340;&#20869;&#22312;&#23646;&#24615;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#30340;&#20108;&#36827;&#21046;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploring Intrinsic Properties of Medical Images for Self-Supervised Binary Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#19987;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;MedSASS&#65292;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#31471;&#21040;&#31471;&#35757;&#32451;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#35299;&#38145;&#20102;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#26377;&#30410;&#20808;&#39564;&#30693;&#35782;&#30340;&#28508;&#21147;&#12290;&#36825;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#39046;&#22495;&#23588;&#20026;&#26377;&#21033;&#12290;&#23613;&#31649;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#26377;&#25928;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#26356;&#22797;&#26434;&#30340;&#24212;&#29992;&#65292;&#22914;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#19987;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21517;&#20026;Medical imaging Enhanced with Dynamic Self-Adaptive Semantic Segmentation (MedSASS)&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;MedSASS&#30456;&#23545;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;MedSASS&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;CNN&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;3.83&#65285;&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;ViT&#30340;&#26041;&#27861;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#22312;&#35206;&#30422;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#19979;&#65292;MedSASS&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in self-supervised learning have unlocked the potential to harness unlabeled data for auxiliary tasks, facilitating the learning of beneficial priors. This has been particularly advantageous in fields like medical image analysis, where labeled data are scarce. Although effective for classification tasks, this methodology has shown limitations in more complex applications, such as medical image segmentation. In this paper, we introduce Medical imaging Enhanced with Dynamic Self-Adaptive Semantic Segmentation (MedSASS), a dedicated self-supervised framework tailored for medical image segmentation. We evaluate MedSASS against existing state- of-the-art methods across four diverse medical datasets, showcasing its superiority. MedSASS outperforms existing CNN-based self-supervised methods by 3.83% and matches the performance of ViT-based methods. Furthermore, when MedSASS is trained end-to-end, covering both encoder and decoder, it demonstrates significant improvements o
&lt;/p&gt;</description></item><item><title>&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02364</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#21457;&#23637;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
The Developmental Landscape of In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02364
&lt;/p&gt;
&lt;p&gt;
&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;transformers&#20013;&#65292;&#24403;&#23427;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#25110;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22914;&#20309;&#20197;&#31163;&#25955;&#30340;&#21457;&#23637;&#38454;&#27573;&#20986;&#29616;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#38548;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#65292;&#36890;&#36807;&#25506;&#27979;&#21442;&#25968;&#31354;&#38388;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#31181;&#32676;&#25439;&#22833;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#30740;&#31350;&#36825;&#20123;&#26032;&#26041;&#27861;&#25581;&#31034;&#30340;&#38454;&#27573;&#65292;&#20197;&#24314;&#31435;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#29289;&#29702;&#23398;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#21407;&#29702;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21457;&#29616;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#20887;&#20313;&#21487;&#20197;&#35299;&#37322;&#20026;&#35268;&#33539;&#23545;&#31216;&#24615;&#12290;&#24182;&#35777;&#26126;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#20887;&#20313;&#22312;&#31070;&#32463;ODE&#20013;&#21319;&#32423;&#20026;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#25214;&#21040;&#20102;Transformer&#27169;&#22411;&#19982;&#31070;&#32463;ODE&#21450;&#20854;&#35268;&#33539;&#23545;&#31216;&#24615;&#30340;&#33258;&#28982;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02362</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#23545;&#31216;&#24615;&#32479;&#19968;&#65306;Transformer, &#21069;&#39304;&#21644;&#31070;&#32463;ODE
&lt;/p&gt;
&lt;p&gt;
Unification of Symmetries Inside Neural Networks: Transformer, Feedforward and Neural ODE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#29289;&#29702;&#23398;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#21407;&#29702;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21457;&#29616;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#20887;&#20313;&#21487;&#20197;&#35299;&#37322;&#20026;&#35268;&#33539;&#23545;&#31216;&#24615;&#12290;&#24182;&#35777;&#26126;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#20887;&#20313;&#22312;&#31070;&#32463;ODE&#20013;&#21319;&#32423;&#20026;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#25214;&#21040;&#20102;Transformer&#27169;&#22411;&#19982;&#31070;&#32463;ODE&#21450;&#20854;&#35268;&#33539;&#23545;&#31216;&#24615;&#30340;&#33258;&#28982;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#36816;&#20316;&#65292;&#21253;&#25324;transformers&#65292;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38590;&#39064;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#23558;&#29289;&#29702;&#23398;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#21407;&#29702;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#27169;&#22411;&#20989;&#25968;&#35270;&#20026;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#65292;&#21457;&#29616;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#20887;&#20313;&#21487;&#20197;&#35299;&#37322;&#20026;&#35268;&#33539;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;ODE&#20013;&#25968;&#23398;&#24418;&#24335;&#21270;&#20102;&#21442;&#25968;&#20887;&#20313;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#30001;&#26102;&#31354;&#24494;&#20998;&#21516;&#32986;&#32473;&#20986;&#65292;&#36825;&#22312;&#29233;&#22240;&#26031;&#22374;&#30340;&#24341;&#21147;&#29702;&#35770;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;ODE&#35270;&#20026;&#36830;&#32493;&#29256;&#26412;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#20887;&#20313;&#30830;&#23454;&#22312;&#31070;&#32463;ODE&#20013;&#21319;&#32423;&#20026;&#24494;&#20998;&#21516;&#32986;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;Transformer&#27169;&#22411;&#65292;&#25214;&#21040;&#20102;&#19982;&#31070;&#32463;ODE&#21450;&#20854;&#35268;&#33539;&#23545;&#31216;&#24615;&#30340;&#33258;&#28982;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of neural networks, including transformers, remains one of the most challenging puzzles in machine learning. This study introduces a novel approach by applying the principles of gauge symmetries, a key concept in physics, to neural network architectures. By regarding model functions as physical observables, we find that parametric redundancies of various machine learning models can be interpreted as gauge symmetries. We mathematically formulate the parametric redundancies in neural ODEs, and find that their gauge symmetries are given by spacetime diffeomorphisms, which play a fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a continuum version of feedforward neural networks, we show that the parametric redundancies in feedforward neural networks are indeed lifted to diffeomorphisms in neural ODEs. We further extend our analysis to transformer models, finding natural correspondences with neural ODEs and their gauge symmetries. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#36890;&#36807;&#20180;&#32454;&#22788;&#29702;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02345</link><description>&lt;p&gt;
Stereographic Spherical Sliced Wasserstein Distances - &#24212;&#29992;&#20110;&#29699;&#24418;&#27010;&#29575;&#20998;&#24067;&#27604;&#36739;&#30340;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Stereographic Spherical Sliced Wasserstein Distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#36890;&#36807;&#20180;&#32454;&#22788;&#29702;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#36136;&#23398;&#12289;&#21307;&#23398;&#39046;&#22495;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#31561;&#21508;&#20010;&#39046;&#22495;&#65292;&#27604;&#36739;&#29699;&#24418;&#27010;&#29575;&#20998;&#24067;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36317;&#31163;&#65292;&#27604;&#22914;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65292;&#23545;&#20110;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#24050;&#32463;&#24341;&#21457;&#20102;&#27963;&#36291;&#30340;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#29699;&#24418;&#27010;&#29575;&#27979;&#24230;&#30340;&#21464;&#20307;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#36895;&#19988;&#39640;&#24230;&#24182;&#34892;&#21270;&#30340;&#29992;&#20110;&#27604;&#36739;&#29699;&#24418;&#27979;&#24230;&#30340;&#36317;&#31163;&#65292;&#20351;&#29992;&#20102;&#31435;&#20307;&#25237;&#24433;&#21644;&#24191;&#20041;Radon&#21464;&#25442;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31435;&#20307;&#25237;&#24433;&#29699;&#38754;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;S3W&#65289;&#36317;&#31163;&#12290;&#25105;&#20204;&#20180;&#32454;&#22788;&#29702;&#20102;&#31435;&#20307;&#25237;&#24433;&#24341;&#36215;&#30340;&#36317;&#31163;&#30072;&#21464;&#65292;&#24182;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#21450;&#20854;&#20855;&#26377;&#26059;&#36716;&#19981;&#21464;&#24615;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#36817;&#30340;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20174;&#36965;&#24863;&#21644;&#22788;&#29702;&#25928;&#29575;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both spe
&lt;/p&gt;</description></item><item><title>MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02342</link><description>&lt;p&gt;
MetaOptimize&#65306;&#19968;&#20010;&#20248;&#21270;&#27493;&#38271;&#21644;&#20854;&#20182;&#20803;&#21442;&#25968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02342
&lt;/p&gt;
&lt;p&gt;
MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20248;&#21270;&#20803;&#21442;&#25968;&#65288;&#21363;&#36229;&#21442;&#25968;&#65289;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MetaOptimize&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#20256;&#32479;&#20803;&#21442;&#25968;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#20803;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27493;&#38271;&#65288;&#20063;&#31216;&#20026;&#23398;&#20064;&#29575;&#65289;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaOptimize&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#35843;&#25972;&#27493;&#38271;&#65292;&#36890;&#36807;&#26410;&#26469;&#25439;&#22833;&#30340;&#25240;&#29616;&#24635;&#21644;&#26469;&#26368;&#23567;&#21270;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;MetaOptimize&#30340;&#20302;&#22797;&#26434;&#24230;&#21464;&#20307;&#65292;&#32467;&#21512;&#20854;&#36866;&#24212;&#22810;&#20010;&#20248;&#21270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;&#65288;UAO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02339</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;&#65288;UAO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#21463;&#21040;&#22495;&#38388;&#24046;&#24322;&#30340;&#38480;&#21046;&#65292;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#25972;&#20307;&#34920;&#29616;&#36890;&#24120;&#19981;&#22914;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25237;&#24433;&#32422;&#26463;&#65292;&#36825;&#20165;&#20165;&#30830;&#20445;&#20102;&#22312;2D&#31354;&#38388;&#20013;&#30340;&#23545;&#40784;&#65292;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270; (UAO) &#26694;&#26550;&#65292;&#23427;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;2D&#21040;3D&#32593;&#32476;&#65292;&#29992;&#20110;&#20272;&#35745;&#30456;&#24212;&#30340;3D&#23039;&#21183;&#65292;&#24182;&#37327;&#21270;&#27599;&#20010;3D&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23545;&#20110;&#27979;&#35797;&#26102;&#30340;&#20248;&#21270;&#65292;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20165;&#20248;&#21270;&#23569;&#37327;&#20851;&#38190;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization. In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance. We observe that previous optimization-based methods commonly rely on projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem. To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of pre-trained model and alleviates the overfitting problem using the uncertainty of joints. Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint. For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#20013;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;AMFormer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02334</link><description>&lt;p&gt;
&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#38656;&#35201;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#20013;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;AMFormer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#21040;&#26368;&#36817;&#65292;&#20851;&#20110;&#28145;&#24230;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24402;&#32435;&#20559;&#35265;&#30340;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#23545;&#20110;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#36731;&#24494;&#29305;&#24449;&#20132;&#20114;&#20551;&#35774;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#26550;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#65292;&#31216;&#20026;AMFormer&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AMFormer&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#20248;&#20110;&#24378;&#23545;&#25163;&#12290;&#36825;&#24402;&#22240;&#20110;&#20854;&#24182;&#34892;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#27880;&#24847;&#21147;&#25805;&#20316;&#31526;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#20248;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#20855;&#26377;&#31639;&#26415;&#24037;&#31243;&#29305;&#24449;&#30340;&#25193;&#23637;&#31354;&#38388;&#20013;&#20998;&#31163;&#34920;&#26684;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20063;&#39564;&#35777;&#20102;AMFormer&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#21644;&#21512;&#29702;&#24615;&#65292;&#34920;&#26126;&#23427;&#24050;&#32463;&#24314;&#31435;&#20102;&#24378;&#26377;&#21147;&#30340;&#24402;&#32435;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24605;&#32771;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#20449;&#21327;&#35758;&#21644;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#28216;&#25103;&#25512;&#29702;&#12289;&#35821;&#38899;&#29983;&#25104;&#21644;&#22312;&#32447;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02330</link><description>&lt;p&gt;
&#22312;&#29436;&#20154;&#28216;&#25103;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhance Reasoning for Large Language Models in the Game Werewolf
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24605;&#32771;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#20449;&#21327;&#35758;&#21644;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#28216;&#25103;&#25512;&#29702;&#12289;&#35821;&#38899;&#29983;&#25104;&#21644;&#22312;&#32447;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24605;&#32771;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#36890;&#36807;prompt&#24037;&#31243;&#22686;&#21152;LLM&#19981;&#21516;&#65292;&#24605;&#32771;&#32773;&#30452;&#25509;&#21033;&#29992;&#25968;&#25454;&#24211;&#20013;&#30340;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#12290;&#35813;&#26694;&#26550;&#24418;&#25104;&#20102;&#19968;&#20010;&#25512;&#29702;&#23618;&#27425;&#32467;&#26500;&#65292;&#22312;&#20854;&#20013;LLM&#22788;&#29702;&#30452;&#35266;&#30340;&#31995;&#32479;1&#20219;&#21153;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#32780;&#24605;&#32771;&#32773;&#19987;&#27880;&#20110;&#38656;&#35201;&#22797;&#26434;&#30340;&#36923;&#36753;&#20998;&#26512;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#35748;&#30693;&#31995;&#32479;2&#20219;&#21153;&#12290;&#25105;&#20204;&#20197;&#38656;&#35201;&#21452;&#31995;&#32479;&#25512;&#29702;&#30340;9&#20154;&#29436;&#20154;&#28216;&#25103;&#20026;&#20363;&#20171;&#32461;&#20102;&#35813;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#21644;&#24605;&#32771;&#32773;&#20043;&#38388;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;18800&#20010;&#20154;&#31867;&#20250;&#35805;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#35757;&#32451;&#20102;&#24605;&#32771;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#28436;&#32462;&#25512;&#29702;&#12289;&#35821;&#38899;&#29983;&#25104;&#21644;&#22312;&#32447;&#28216;&#25103;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;6B LLM&#65292;&#36229;&#36234;&#20102;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework's effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02314</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#36873;&#25321;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Selecting Large Language Model to Fine-tune via Rectified Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#25152;&#26377;&#27169;&#22411;&#28982;&#21518;&#20877;&#36827;&#34892;&#36873;&#25321;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36164;&#28304;&#21463;&#38480;&#30340;&#36873;&#25321;&#20219;&#21153;&#36716;&#21270;&#20026;&#39044;&#27979;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#23637;&#31034;&#20854;&#19982;&#32553;&#25918;&#23450;&#24459;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#19982;&#39044;&#35757;&#32451;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#30340;&#32553;&#25918;&#26354;&#32447;&#19981;&#20165;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#21151;&#29575;&#38454;&#27573;&#8221;&#65292;&#36824;&#21253;&#25324;&#20197;&#21069;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#8220;&#39044;&#21151;&#29575;&#38454;&#27573;&#8221;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#32553;&#25918;&#23450;&#24459;&#26080;&#27861;&#29702;&#35770;&#21644;&#23454;&#35777;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#30456;&#21464;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#8220;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#8221;&#27010;&#24565;&#24341;&#20837;&#21040;&#25105;&#20204;&#30340;&#20462;&#27491;&#32553;&#25918;&#23450;&#24459;&#20013;&#65292;&#36825;&#20811;&#26381;&#20102;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#26356;&#22909;&#22320;&#36866;&#24212;&#23454;&#39564;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#23450;&#24459;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
&lt;/p&gt;</description></item><item><title>&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02287</link><description>&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#30340;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Future Directions in Foundations of Graph Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02287
&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#25968;&#25454;&#22312;&#19981;&#21516;&#23398;&#31185;&#65288;&#20174;&#29983;&#21629;&#31185;&#23398;&#21040;&#31038;&#20250;&#31185;&#23398;&#21644;&#24037;&#31243;&#31185;&#23398;&#65289;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23545;GNNs&#24615;&#36136;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#38416;&#26126;GNNs&#31895;&#31890;&#24230;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#65292;&#20027;&#35201;&#37319;&#29992;&#32452;&#21512;&#25216;&#24039;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#19982;&#23454;&#36341;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;GNNs&#26102;&#65292;&#23545;GNNs&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#23450;&#20301;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38656;&#35201;&#23558;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#19978;&#26469;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#19979;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#12290;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#65292;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02286</link><description>&lt;p&gt;
&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#29992;&#20110;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#19979;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#12290;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#65292;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#30528;&#37325;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#21516;&#26102;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#12290;&#22312;&#19968;&#20123;&#22330;&#26223;&#19979;&#65292;&#22914;&#33258;&#20027;&#23548;&#33322;&#21644;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#65292;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#21516;&#26679;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#65288;MFARANet&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#26469;&#20445;&#35777;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#26469;&#24357;&#34917;&#27973;&#39592;&#24178;&#24341;&#36215;&#30340;&#27169;&#22411;&#23481;&#37327;&#20943;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65288;MFAM&#65289;&#65292;&#23558;&#32534;&#30721;&#22120;&#20013;&#30340;&#20998;&#23618;&#29305;&#24449;&#32858;&#21512;&#21040;&#27599;&#20010;&#23610;&#24230;&#65292;&#20197;&#20415;&#20110;&#21518;&#32493;&#30340;&#31354;&#38388;&#23545;&#40784;&#21644;&#22810;&#23610;&#24230;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#27969;&#30340;&#23545;&#40784;&#26469;&#24314;&#31435;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#65288;RAM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time semantic segmentation is a crucial research for real-world applications. However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy. In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important. To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed. We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone. Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignm
&lt;/p&gt;</description></item><item><title>SynthDST&#26159;&#19968;&#20010;&#38024;&#23545;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#35774;&#35745;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#25163;&#24037;&#23545;&#35805;&#27169;&#26495;&#21644;&#23545;&#35805;&#27169;&#24335;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#12289;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#24102;&#26377;DST&#27880;&#37322;&#30340;&#23545;&#35805;&#65292;&#24182;&#20351;Join&#36830;&#36890;&#29575;&#25552;&#21319;4-5&#65285;.</title><link>https://arxiv.org/abs/2402.02285</link><description>&lt;p&gt;
SynthDST: &#23569;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#25152;&#38656;&#30340;&#20840;&#37096;&#26159;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02285
&lt;/p&gt;
&lt;p&gt;
SynthDST&#26159;&#19968;&#20010;&#38024;&#23545;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#35774;&#35745;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#25163;&#24037;&#23545;&#35805;&#27169;&#26495;&#21644;&#23545;&#35805;&#27169;&#24335;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#12289;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#24102;&#26377;DST&#27880;&#37322;&#30340;&#23545;&#35805;&#65292;&#24182;&#20351;Join&#36830;&#36890;&#29575;&#25552;&#21319;4-5&#65285;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#25104;&#20026;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#34920;&#29616;&#26368;&#22909;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#28041;&#21450;&#26816;&#32034;&#21644;&#28155;&#21152;&#31867;&#20284;&#30340;&#31034;&#20363;&#21040;&#25552;&#31034;&#20013;&#65292;&#38656;&#35201;&#35775;&#38382;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#33719;&#21462;&#36825;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#26377;&#26102;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#34429;&#28982;&#38646;&#26679;&#26412;&#23398;&#20064;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#26126;&#26174;&#33853;&#21518;&#12290;&#22240;&#27492;&#65292;&#8220;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#20026;&#20219;&#20309;&#23545;&#35805;&#27169;&#24335;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#23569;&#26679;&#26412;&#25552;&#31034;&#65311;&#8221;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\method&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#19987;&#38376;&#38024;&#23545;DST&#65292;&#21033;&#29992;LLM&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#23545;&#35805;&#27169;&#24335;&#21644;&#19968;&#20123;&#25163;&#24037;&#23545;&#35805;&#27169;&#26495;&#65292;&#23601;&#33021;&#21512;&#25104;&#33258;&#28982;&#12289;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#24102;&#26377;DST&#27880;&#37322;&#30340;&#23545;&#35805;&#12290;&#20351;&#29992;{\method}&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#32467;&#26524;&#26174;&#31034;&#65292;Join&#36830;&#36890;&#29575;&#25552;&#21319;&#20102;4-5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\method} results in $4-5%$ improvement in Join
&lt;/p&gt;</description></item><item><title>InceptionCapsule&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;Inception-ResNet&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#35299;&#20915;&#20102;&#21021;&#22987;&#26435;&#37325;&#36873;&#25321;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02274</link><description>&lt;p&gt;
InceptionCapsule: &#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Inception-Resnet&#21644;CapsuleNet&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention for medical image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02274
&lt;/p&gt;
&lt;p&gt;
InceptionCapsule&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;Inception-ResNet&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#35299;&#20915;&#20102;&#21021;&#22987;&#26435;&#37325;&#36873;&#25321;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21021;&#22987;&#26435;&#37325;&#30340;&#36873;&#25321;&#23545;&#20110;&#32593;&#32476;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#38543;&#26426;&#36873;&#25321;&#30340;&#26435;&#37325;&#21487;&#33021;&#20135;&#29983;&#19981;&#21516;&#30340;&#36755;&#20986;&#65292;&#22686;&#21152;&#36807;&#25311;&#21512;&#21644;&#27424;&#25311;&#21512;&#30340;&#27010;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#21521;&#37327;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#38656;&#35201;&#20016;&#23500;&#30340;&#21521;&#37327;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;InceptionCapsule&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;Inception-ResNet&#27169;&#22411;&#65292;&#36991;&#20813;&#38543;&#26426;&#36873;&#25321;&#26435;&#37325;&#65292;&#20174;ImageNet&#20013;&#33719;&#21462;&#21021;&#22987;&#26435;&#37325;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;Inception&#20013;&#38388;&#23618;&#30340;&#36755;&#20986;&#29983;&#25104;&#20016;&#23500;&#30340;&#21521;&#37327;&#12290;&#25552;&#21462;&#30340;&#21521;&#37327;&#34987;&#20256;&#36882;&#32473;&#35013;&#22791;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33014;&#22218;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#12290;&#20351;&#29992;Kvasir&#25968;&#25454;&#38598;&#21644;BUSI with GT&#25968;&#25454;&#38598;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#22312;5&#31867;&#20998;&#31867;&#19978;&#23454;&#29616;&#20102;97.62%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;8&#31867;&#20998;&#31867;&#19978;&#23454;&#29616;&#20102;94.30%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Initial weighting is significant in deep neural networks because the random selection of weights produces different outputs and increases the probability of overfitting and underfitting. On the other hand, vector-based approaches to extract vector features need rich vectors for more accurate classification. The InceptionCapsule approach is presented to alleviate these two problems. This approach uses transfer learning and the Inception-ResNet model to avoid random selection of weights, which takes initial weights from ImageNet. It also uses the output of Inception middle layers to generate rich vectors. Extracted vectors are given to a capsule network for learning, which is equipped with an attention technique. Kvasir data and BUSI with the GT dataset were used to evaluate this approach. This model was able to achieve 97.62 accuracies in 5-class classification and also achieved 94.30 accuracies in 8-class classification on Kvasir. In the BUSI with GT dataset, the proposed approach achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#20855;&#26377;&#26032;&#30693;&#35782;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#21253;&#25324;&#22914;&#20309;&#26377;&#25928;&#22320;&#34701;&#20837;&#26032;&#29305;&#24449;&#12289;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#26032;&#30693;&#35782;&#21040;&#36798;&#30340;&#24418;&#24335;&#21644;&#26102;&#38388;&#23545;&#32435;&#20837;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#20855;&#26377;&#26032;&#30693;&#35782;&#26102;&#30340;&#28508;&#22312;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.02268</link><description>&lt;p&gt;
&#20855;&#26377;&#26032;&#30693;&#35782;&#30340;&#32852;&#37030;&#23398;&#20064;: &#22522;&#30784;&#65292;&#36827;&#23637;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with New Knowledge: Fundamentals, Advances, and Futures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#20855;&#26377;&#26032;&#30693;&#35782;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#21253;&#25324;&#22914;&#20309;&#26377;&#25928;&#22320;&#34701;&#20837;&#26032;&#29305;&#24449;&#12289;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#26032;&#30693;&#35782;&#21040;&#36798;&#30340;&#24418;&#24335;&#21644;&#26102;&#38388;&#23545;&#32435;&#20837;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#20855;&#26377;&#26032;&#30693;&#35782;&#26102;&#30340;&#28508;&#22312;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#26085;&#30410;&#37325;&#35270;&#30340;&#26102;&#20195;&#36805;&#36895;&#21457;&#23637;&#12290;&#27491;&#26159;&#36825;&#31181;&#24555;&#36895;&#21457;&#23637;&#36235;&#21183;&#65292;&#20197;&#21450;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#19981;&#26029;&#26032;&#38656;&#27714;&#30340;&#20986;&#29616;&#65292;&#20419;&#20351;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#65306;&#20855;&#26377;&#26032;&#30693;&#35782;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#23558;&#21508;&#31181;&#26032;&#30693;&#35782;&#26377;&#25928;&#22320;&#34701;&#20837;&#29616;&#26377;&#30340;FL&#31995;&#32479;&#20013;&#65292;&#24182;&#20351;&#36825;&#20123;&#31995;&#32479;&#38477;&#20302;&#25104;&#26412;&#65292;&#24310;&#38271;&#23551;&#21629;&#65292;&#24182;&#20419;&#36827;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23450;&#20041;&#20102;FL&#20013;&#26032;&#30693;&#35782;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#26032;&#29305;&#24449;&#12289;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#31639;&#27861;&#12290;&#23545;&#20110;&#27599;&#20010;&#26469;&#28304;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#21644;&#35752;&#35770;&#22914;&#20309;&#23558;&#26032;&#30693;&#35782;&#32435;&#20837;&#29616;&#26377;&#30340;FL&#31995;&#32479;&#65292;&#24182;&#30740;&#31350;&#26032;&#30693;&#35782;&#21040;&#36798;&#30340;&#24418;&#24335;&#21644;&#26102;&#38388;&#23545;&#32435;&#20837;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20840;&#38754;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#20855;&#26377;&#26032;&#30693;&#35782;&#26102;&#30340;&#28508;&#22312;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving distributed learning approach that is rapidly developing in an era where privacy protection is increasingly valued. It is this rapid development trend, along with the continuous emergence of new demands for FL in the real world, that prompts us to focus on a very important problem: Federated Learning with New Knowledge. The primary challenge here is to effectively incorporate various new knowledge into existing FL systems and evolve these systems to reduce costs, extend their lifespan, and facilitate sustainable development. In this paper, we systematically define the main sources of new knowledge in FL, including new features, tasks, models, and algorithms. For each source, we thoroughly analyze and discuss how to incorporate new knowledge into existing FL systems and examine the impact of the form and timing of new knowledge arrival on the incorporation process. Furthermore, we comprehensively discuss the potential future directions for
&lt;/p&gt;</description></item><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;RoBERTa-CNN&#27169;&#22411;&#26469;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#33258;&#26432;&#24847;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;RoBERTa-CNN&#36890;&#36807;&#22312;RoBERTa&#27169;&#22411;&#20013;&#28155;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23618;&#65292;&#25552;&#39640;&#20102;&#23545;&#37325;&#35201;&#27169;&#24335;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#33258;&#26432;&#21644;&#25233;&#37057;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02262</link><description>&lt;p&gt;
&#25968;&#25454;&#36136;&#37327;&#24456;&#37325;&#35201;&#65306;&#20351;&#29992;RoBERTa-CNN&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#33258;&#26432;&#24847;&#22270;
&lt;/p&gt;
&lt;p&gt;
Data Quality Matters: Suicide Intention Detection on Social Media Posts Using a RoBERTa-CNN Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;RoBERTa-CNN&#27169;&#22411;&#26469;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#33258;&#26432;&#24847;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;RoBERTa-CNN&#36890;&#36807;&#22312;RoBERTa&#27169;&#22411;&#20013;&#28155;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23618;&#65292;&#25552;&#39640;&#20102;&#23545;&#37325;&#35201;&#27169;&#24335;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#33258;&#26432;&#21644;&#25233;&#37057;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26432;&#20173;&#28982;&#26159;&#20840;&#29699;&#20581;&#24247;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#27880;&#28966;&#28857;&#65292;&#24613;&#38656;&#21019;&#26032;&#26041;&#27861;&#36827;&#34892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#35782;&#21035;SuicideWatch Reddit&#24086;&#23376;&#20013;&#30340;&#33258;&#26432;&#24847;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23574;&#31471;&#30340;RoBERTa-CNN&#27169;&#22411;&#36827;&#34892;&#33258;&#26432;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;RoBERTa-CNN&#26159;RoBERTa&#65288;&#40065;&#26834;&#24615;&#20248;&#21270;BERT&#26041;&#27861;&#65289;&#30340;&#19968;&#31181;&#21464;&#20307;&#12290;RoBERTa&#34987;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;RoBERTa&#30340;&#26377;&#25928;&#24615;&#22312;&#20110;&#23427;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#20449;&#24687;&#24182;&#24418;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#21407;&#22987;&#27169;&#22411;&#20013;&#28155;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23618;&#65292;RoBERTa&#22686;&#24378;&#20102;&#20174;&#24222;&#22823;&#25968;&#25454;&#38598;&#20013;&#25429;&#25417;&#37325;&#35201;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#26432;&#21644;&#25233;&#37057;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;RoBERTa-CNN&#65292;&#24182;&#33719;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#65292;RoBERTa-CNN&#22312;&#24179;&#22343;&#20934;&#30830;&#29575;&#19978;&#33719;&#24471;&#20102;98&#65285;&#65292;&#26631;&#20934;&#24046;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Suicide remains a global health concern for the field of health, which urgently needs innovative approaches for early detection and intervention. In this paper, we focus on identifying suicidal intentions in SuicideWatch Reddit posts and present a novel approach to suicide detection using the cutting-edge RoBERTa-CNN model, a variant of RoBERTa (Robustly optimized BERT approach). RoBERTa is used for various Natural Language Processing (NLP) tasks, including text classification and sentiment analysis. The effectiveness of the RoBERTa lies in its ability to capture textual information and form semantic relationships within texts. By adding the Convolution Neural Network (CNN) layer to the original model, the RoBERTa enhances its ability to capture important patterns from heavy datasets. To evaluate the RoBERTa-CNN, we experimented on the Suicide and Depression Detection dataset and obtained solid results. For example, RoBERTa-CNN achieves 98% mean accuracy with the standard deviation (ST
&lt;/p&gt;</description></item><item><title>XTSFormer&#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#30340;&#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#24490;&#29615;&#24863;&#30693;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#21644;&#20998;&#23618;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12289;&#24490;&#29615;&#12289;&#21608;&#26399;&#24615;&#21644;&#22810;&#23610;&#24230;&#20107;&#20214;&#20132;&#20114;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02258</link><description>&lt;p&gt;
XTSFormer: &#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02258
&lt;/p&gt;
&lt;p&gt;
XTSFormer&#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#30340;&#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#24490;&#29615;&#24863;&#30693;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#21644;&#20998;&#23618;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12289;&#24490;&#29615;&#12289;&#21608;&#26399;&#24615;&#21644;&#22810;&#23610;&#24230;&#20107;&#20214;&#20132;&#20114;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#39044;&#27979;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#26102;&#38388;&#21644;&#31867;&#22411;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#23384;&#22312;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#36830;&#32493;&#20107;&#20214;&#20043;&#38388;&#26102;&#38388;&#38388;&#38548;&#30340;&#19981;&#35268;&#21017;&#24615;&#12289;&#24490;&#29615;&#12289;&#21608;&#26399;&#24615;&#21644;&#22810;&#23610;&#24230;&#20107;&#20214;&#20132;&#20114;&#65292;&#20197;&#21450;&#38271;&#20107;&#20214;&#24207;&#21015;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#20107;&#20214;&#20132;&#20114;&#30340;&#22810;&#23610;&#24230;&#29305;&#24615;&#65292;&#32780;&#36825;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65288;&#22914;&#20020;&#24202;&#20107;&#20214;&#25968;&#25454;&#65289;&#24456;&#24120;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#65288;XTSFormer&#65289;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29305;&#24449;&#30340;&#24490;&#29615;&#24863;&#30693;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#65288;FCPE&#65289;&#65292;&#33021;&#22815;&#28789;&#27963;&#25429;&#25417;&#26102;&#38388;&#30340;&#24490;&#29615;&#24615;&#36136;&#65292;&#20197;&#21450;&#19968;&#20010;&#20998;&#23618;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#12290;&#36825;&#20123;&#23610;&#24230;&#30001;&#33258;&#24213;&#21521;&#19978;&#30340;&#32858;&#31867;&#31639;&#27861;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event prediction aims to forecast the time and type of a future event based on a historical event sequence. Despite its significance, several challenges exist, including the irregularity of time intervals between consecutive events, the existence of cycles, periodicity, and multi-scale event interactions, as well as the high computational costs for long event sequences. Existing neural temporal point processes (TPPs) methods do not capture the multi-scale nature of event interactions, which is common in many real-world applications such as clinical event data. To address these issues, we propose the cross-temporal-scale transformer (XTSFormer), designed specifically for irregularly timed event data. Our model comprises two vital components: a novel Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures the cyclical nature of time, and a hierarchical multi-scale temporal attention mechanism. These scales are determined by a bottom-up clustering algorithm. Extens
&lt;/p&gt;</description></item><item><title>ExTTNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#34920;&#26684;&#25991;&#23383;&#12290;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#33719;&#21462;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;0.92&#30340;F1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.02246</link><description>&lt;p&gt;
ExTTNet:&#19968;&#31181;&#29992;&#20110;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#34920;&#26684;&#25991;&#23383;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02246
&lt;/p&gt;
&lt;p&gt;
ExTTNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#34920;&#26684;&#25991;&#23383;&#12290;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#33719;&#21462;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;0.92&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;ExTTNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33258;&#21160;&#22320;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#20135;&#21697;&#34920;&#26684;&#12290;&#39318;&#20808;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#33719;&#21462;&#25991;&#26412;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#20102;Tesseract OCR&#24341;&#25806; [37]&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#22686;&#21152;&#29616;&#26377;&#29305;&#24449;&#30340;&#25968;&#37327;&#26469;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#26681;&#25454;&#27599;&#20010;OCR&#33719;&#24471;&#30340;&#25991;&#26412;&#26159;&#21542;&#26159;&#34920;&#26684;&#20803;&#32032;&#65292;&#36827;&#34892;&#26631;&#35760;&#22788;&#29702;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#20102;&#22810;&#23618;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;Nvidia RTX 3090&#26174;&#21345;&#36827;&#34892;&#35757;&#32451;&#65292;&#32791;&#26102;162&#20998;&#38047;&#12290;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;F1&#20998;&#25968;&#20026;0.92&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, product tables in invoices are obtained autonomously via a deep learning model, which is named as ExTTNet. Firstly, text is obtained from invoice images using Optical Character Recognition (OCR) techniques. Tesseract OCR engine [37] is used for this process. Afterwards, the number of existing features is increased by using feature extraction methods to increase the accuracy. Labeling process is done according to whether each text obtained as a result of OCR is a table element or not. In this study, a multilayer artificial neural network model is used. The training has been carried out with an Nvidia RTX 3090 graphics card and taken $162$ minutes. As a result of the training, the F1 score is $0.92$.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20998;&#24067;&#24335;&#21644;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#23567;&#22411;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#19979;&#38477;&#26368;&#20026;&#20005;&#37325;&#12290;</title><link>https://arxiv.org/abs/2402.02230</link><description>&lt;p&gt;
&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20998;&#24067;&#24335;&#21644;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#23567;&#22411;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#19979;&#38477;&#26368;&#20026;&#20005;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#20445;&#25252;&#23458;&#25143;&#30340;&#31169;&#23494;&#25968;&#25454;&#65292;&#36991;&#20813;&#22312;&#19981;&#21516;&#21442;&#19982;&#26041;&#20043;&#38388;&#20849;&#20139;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20998;&#26512;&#26469;&#33258;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#21442;&#25968;&#26435;&#37325;&#65292;&#31169;&#23494;&#20449;&#24687;&#20173;&#28982;&#21487;&#33021;&#27844;&#38706;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20851;&#20110;&#23458;&#25143;&#25968;&#37327;&#21644;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#23545;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#35777;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#24335;&#21644;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#23567;&#22411;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#19979;&#38477;&#26368;&#20026;&#20005;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), as a type of distributed machine learning, is capable of significantly preserving client's private data from being shared among different parties. Nevertheless, private information can still be divulged by analyzing uploaded parameter weights from clients. In this report, we showcase our empirical benchmark of the effect of the number of clients and the addition of differential privacy (DP) mechanisms on the performance of the model on different types of data. Our results show that non-i.i.d and small datasets have the highest decrease in performance in a distributed and differentially private setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#23618;&#22810;&#23610;&#24230;&#21644;&#25991;&#21270;&#24847;&#35782;&#20262;&#29702;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#38750;&#27954;&#26426;&#22120;&#26234;&#33021;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23637;&#31034;&#20102;MI&#22312;54&#20010;&#38750;&#27954;&#22269;&#23478;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#24378;&#35843;&#20102;&#38750;&#27954;&#30340;&#38899;&#39057;&#25968;&#25454;&#38598;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12289;&#20808;&#36827;&#30340;MI&#27169;&#22411;&#32570;&#20047;&#25991;&#21270;&#24847;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02218</link><description>&lt;p&gt;
&#38750;&#27954;&#30340;&#26426;&#22120;&#26234;&#33021;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Intelligence in Africa: a survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#23618;&#22810;&#23610;&#24230;&#21644;&#25991;&#21270;&#24847;&#35782;&#20262;&#29702;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#38750;&#27954;&#26426;&#22120;&#26234;&#33021;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23637;&#31034;&#20102;MI&#22312;54&#20010;&#38750;&#27954;&#22269;&#23478;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#24378;&#35843;&#20102;&#38750;&#27954;&#30340;&#38899;&#39057;&#25968;&#25454;&#38598;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12289;&#20808;&#36827;&#30340;MI&#27169;&#22411;&#32570;&#20047;&#25991;&#21270;&#24847;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;5&#24180;&#37324;&#65292;&#38750;&#27954;&#22269;&#23478;&#20013;&#22823;&#35268;&#27169;&#38899;&#39057;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20026;&#26500;&#24314;&#36317;&#31163;&#20154;&#20204;&#26356;&#36817;&#12289;&#20351;&#29992;&#26412;&#22320;&#35821;&#35328;&#36827;&#34892;&#20132;&#27969;&#12289;&#23398;&#20064;&#12289;&#29702;&#35299;&#21644;&#24320;&#23637;&#19994;&#21153;&#30340;&#26426;&#22120;&#26234;&#33021;&#65288;MI&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#26080;&#38480;&#26426;&#20250;&#65292;&#21253;&#25324;&#37027;&#20123;&#26080;&#27861;&#35835;&#20889;&#30340;&#20154;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38899;&#39057;&#25968;&#25454;&#38598;&#24182;&#27809;&#26377;&#34987;&#29616;&#26377;&#30340;MI&#24037;&#20855;&#20805;&#20998;&#21033;&#29992;&#65292;&#20351;&#24471;&#35768;&#22810;&#38750;&#27954;&#20154;&#38169;&#22833;&#20102;MI&#19994;&#21153;&#26426;&#20250;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#20808;&#36827;&#30340;MI&#27169;&#22411;&#24182;&#27809;&#26377;&#24847;&#35782;&#21040;&#25991;&#21270;&#24046;&#24322;&#65292;&#20854;&#37319;&#32435;&#25351;&#25968;&#30340;&#20262;&#29702;&#38382;&#39064;&#20063;&#20540;&#24471;&#36136;&#30097;&#12290;&#36825;&#19968;&#32570;&#22833;&#22312;&#38750;&#27954;&#30340;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#20197;&#22810;&#23618;&#22810;&#23610;&#24230;&#21644;&#25991;&#21270;&#24847;&#35782;&#20262;&#29702;&#30340;&#35270;&#35282;&#24635;&#32467;&#20102;&#38750;&#27954;&#26426;&#22120;&#26234;&#33021;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#36890;&#36807;400&#31687;&#20851;&#20110;MI&#30740;&#31350;&#12289;&#20135;&#19994;&#12289;&#25919;&#24220;&#34892;&#21160;&#20197;&#21450;&#33402;&#26415;&#12289;&#38899;&#20048;&#12289;&#38750;&#27491;&#24335;&#32463;&#27982;&#21644;&#38750;&#27954;&#23567;&#20225;&#19994;&#24212;&#29992;&#30340;&#25991;&#31456;&#65292;&#23637;&#31034;&#20102;MI&#22312;54&#20010;&#38750;&#27954;&#22269;&#23478;&#30340;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last 5 years, the availability of large audio datasets in African countries has opened unlimited opportunities to build machine intelligence (MI) technologies that are closer to the people and speak, learn, understand, and do businesses in local languages, including for those who cannot read and write. Unfortunately, these audio datasets are not fully exploited by current MI tools, leaving several Africans out of MI business opportunities. Additionally, many state-of-the-art MI models are not culture-aware, and the ethics of their adoption indexes are questionable. The lack thereof is a major drawback in many applications in Africa. This paper summarizes recent developments in machine intelligence in Africa from a multi-layer multiscale and culture-aware ethics perspective, showcasing MI use cases in 54 African countries through 400 articles on MI research, industry, government actions, as well as uses in art, music, the informal economy, and small businesses in Africa. The surv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12289;&#36807;&#37319;&#26679;&#21644;&#29305;&#24449;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#12290;&#36890;&#36807;&#23545;Pima&#21360;&#22320;&#23433;&#20154;&#31958;&#23615;&#30149;&#25968;&#25454;&#24211;&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;92.31%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02188</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#30340;&#36807;&#37319;&#26679;&#21644;&#29305;&#24449;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Diabetes detection using deep learning techniques with oversampling and feature augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12289;&#36807;&#37319;&#26679;&#21644;&#29305;&#24449;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#12290;&#36890;&#36807;&#23545;Pima&#21360;&#22320;&#23433;&#20154;&#31958;&#23615;&#30149;&#25968;&#25454;&#24211;&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;92.31%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#21644;&#30446;&#26631;&#65306;&#31958;&#23615;&#30149;&#26159;&#19968;&#31181;&#24930;&#24615;&#30149;&#65292;&#22810;&#24180;&#26469;&#24433;&#21709;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#12290;&#27599;&#24180;&#26377;&#22823;&#37327;&#30340;&#27515;&#20129;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#24739;&#26377;&#31958;&#23615;&#30149;&#30340;&#20154;&#27809;&#26377;&#21450;&#26102;&#24847;&#35782;&#21040;&#33258;&#24049;&#30340;&#20581;&#24247;&#29366;&#20917;&#30340;&#20005;&#37325;&#24615;&#12290;&#24310;&#36831;&#35786;&#26029;&#20250;&#23548;&#33268;&#35768;&#22810;&#20581;&#24247;&#38382;&#39064;&#21644;&#22823;&#37327;&#30340;&#27515;&#20129;&#26696;&#20363;&#65292;&#22240;&#27492;&#24320;&#21457;&#26089;&#26399;&#35786;&#26029;&#27492;&#30149;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#39044;&#27979;&#31958;&#23615;&#30149;&#24739;&#32773;&#12290;&#23427;&#21253;&#25324;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#31232;&#30095;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SAE&#65289;&#36827;&#34892;&#29305;&#24449;&#22686;&#24378;&#21644;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#12290;&#35780;&#20272;&#20102;Pima&#21360;&#22320;&#23433;&#20154;&#31958;&#23615;&#30149;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#24739;&#32773;&#30340;&#24576;&#23381;&#27425;&#25968;&#12289;&#34880;&#31958;&#25110;&#33008;&#23707;&#32032;&#27700;&#24179;&#12289;&#34880;&#21387;&#25110;&#24180;&#40836;&#31561;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background and objective: Diabetes is a chronic pathology which is affecting more and more people over the years. It gives rise to a large number of deaths each year. Furthermore, many people living with the disease do not realize the seriousness of their health status early enough. Late diagnosis brings about numerous health problems and a large number of deaths each year so the development of methods for the early diagnosis of this pathology is essential.   Methods: In this paper, a pipeline based on deep learning techniques is proposed to predict diabetic people. It includes data augmentation using a variational autoencoder (VAE), feature augmentation using an sparse autoencoder (SAE) and a convolutional neural network for classification. Pima Indians Diabetes Database, which takes into account information on the patients such as the number of pregnancies, glucose or insulin level, blood pressure or age, has been evaluated.   Results: A 92.31% of accuracy was obtained when CNN class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;EGFN&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#21442;&#25968;&#65292;&#24182;&#23558;&#32467;&#26524;&#36712;&#36857;&#23384;&#20648;&#22312;&#20248;&#20808;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;GFlowNets&#20195;&#29702;&#26102;&#23637;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02186</link><description>&lt;p&gt;
&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Evolution Guided Generative Flow Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;EGFN&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#21442;&#25968;&#65292;&#24182;&#23558;&#32467;&#26524;&#36712;&#36857;&#23384;&#20648;&#22312;&#20248;&#20808;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;GFlowNets&#20195;&#29702;&#26102;&#23637;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31867;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#25353;&#29031;&#22870;&#21169;&#27604;&#20363;&#23545;&#32452;&#21512;&#23545;&#35937;&#36827;&#34892;&#37319;&#26679;&#12290;GFlowNets&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#26159;&#22312;&#22788;&#29702;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#26102;&#26377;&#25928;&#35757;&#32451;&#23427;&#20204;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;EGFN&#65289;&#65292;&#36825;&#26159;&#23545;GFlowNets&#35757;&#32451;&#30340;&#19968;&#31181;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#65288;EA&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20219;&#20309;GFlowNets&#35757;&#32451;&#30446;&#26631;&#30340;&#22522;&#30784;&#19978;&#24037;&#20316;&#65292;&#36890;&#36807;&#20351;&#29992;EA&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#21442;&#25968;&#65292;&#23558;&#32467;&#26524;&#36712;&#36857;&#23384;&#20648;&#22312;&#20248;&#20808;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#65292;&#24182;&#20351;&#29992;&#23384;&#20648;&#30340;&#36712;&#36857;&#35757;&#32451;GFlowNets&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#29609;&#20855;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#36712;&#36857;&#21644;&#31232;&#30095;&#22870;&#21169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are a family of probabilistic generative models that learn to sample compositional objects proportional to their rewards. One big challenge of GFlowNets is training them effectively when dealing with long time horizons and sparse rewards. To address this, we propose Evolution guided generative flow networks (EGFN), a simple but powerful augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our method can work on top of any GFlowNets training objective, by training a set of agent parameters using EA, storing the resulting trajectories in the prioritized replay buffer, and training the GFlowNets agent using the stored trajectories. We present a thorough investigation over a wide range of toy and real-world benchmark tasks showing the effectiveness of our method in handling long trajectories and sparse rewards.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#22266;&#23450;&#38271;&#24230;&#30340;&#38899;&#39057;&#19978;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Mel&#39057;&#35889;&#22270;&#21644;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#20316;&#20026;&#38899;&#39057;&#25551;&#36848;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02184</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#22266;&#23450;&#38271;&#24230;&#30340;&#38899;&#39057;&#20013;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis in non-fixed length audios using a Fully Convolutional Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02184
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#22266;&#23450;&#38271;&#24230;&#30340;&#38899;&#39057;&#19978;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Mel&#39057;&#35889;&#22270;&#21644;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#20316;&#20026;&#38899;&#39057;&#25551;&#36848;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25509;&#21463;&#20219;&#24847;&#38271;&#24230;&#38899;&#39057;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20107;&#20808;&#22266;&#23450;&#38271;&#24230;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;Mel&#39057;&#35889;&#22270;&#21644;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#20316;&#20026;&#38899;&#39057;&#25551;&#36848;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#20351;&#29992;EMODB&#12289;RAVDESS&#21644;TESS&#19977;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35813;&#26041;&#27861;&#21487;&#20197;&#25509;&#21463;&#20219;&#24847;&#22823;&#23567;&#30340;&#38899;&#39057;&#65292;&#22240;&#27492;&#21487;&#20197;&#23454;&#26102;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#36825;&#23545;&#20110;&#21628;&#21483;&#20013;&#24515;&#12289;&#21307;&#30103;&#21672;&#35810;&#25110;&#37329;&#34701;&#32463;&#32426;&#31561;&#22810;&#20010;&#39046;&#22495;&#38750;&#24120;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a sentiment analysis method that is capable of accepting audio of any length, without being fixed a priori, is proposed. Mel spectrogram and Mel Frequency Cepstral Coefficients are used as audio description methods and a Fully Convolutional Neural Network architecture is proposed as a classifier. The results have been validated using three well known datasets: EMODB, RAVDESS, and TESS. The results obtained were promising, outperforming the state-of-the-art methods. Also, thanks to the fact that the proposed method admits audios of any size, it allows a sentiment analysis to be made in near real time, which is very interesting for a wide range of fields such as call centers, medical consultations, or financial brokers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26412;&#20307;&#30340;&#22810;&#39046;&#22495;&#30693;&#35782;&#27169;&#22411;&#65292;&#33021;&#22815;&#33258;&#21160;&#36827;&#34892;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#65292;&#36991;&#20813;&#38169;&#35823;&#65292;&#24182;&#33719;&#24471;&#19982;&#19987;&#23478;&#30456;&#21516;&#30340;&#32467;&#35770;&#12290;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02181</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#20307;&#30340;&#22810;&#39046;&#22495;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#65306;&#23454;&#39564;&#39564;&#35777;&#21644;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Ontology-Based multi-domain model in Social Network Analysis: Experimental validation and case study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26412;&#20307;&#30340;&#22810;&#39046;&#22495;&#30693;&#35782;&#27169;&#22411;&#65292;&#33021;&#22815;&#33258;&#21160;&#36827;&#34892;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#65292;&#36991;&#20813;&#38169;&#35823;&#65292;&#24182;&#33719;&#24471;&#19982;&#19987;&#23478;&#30456;&#21516;&#30340;&#32467;&#35770;&#12290;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31038;&#20132;&#32593;&#32476;&#29702;&#35770;&#21644;&#20998;&#26512;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#21253;&#25324;&#20844;&#20849;&#21355;&#29983;&#12290;&#36827;&#34892;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;(SNA)&#30340;&#23436;&#25972;&#27969;&#31243;&#26159;&#19968;&#39033;&#32791;&#26102;&#20219;&#21153;&#65292;&#20854;&#20013;&#19987;&#23478;&#21487;&#33021;&#20250;&#29359;&#38169;&#35823;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#30693;&#35782;&#27169;&#22411;&#65292;&#33021;&#22815;&#33258;&#21160;&#25910;&#38598;&#25968;&#25454;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#19981;&#21516;&#30340;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#65292;&#36991;&#20813;&#38169;&#35823;&#65292;&#24182;&#33719;&#24471;&#19982;SNA&#19987;&#23478;&#30456;&#21516;&#30340;&#32467;&#35770;&#12290;&#35813;&#27169;&#22411;&#20197;&#19968;&#20010;&#21517;&#20026;OntoSNAQA&#30340;&#26412;&#20307;&#34920;&#31034;&#65292;&#35813;&#26412;&#20307;&#30001;&#34920;&#31034;People&#12289;Questionnaires&#21644;Social Network Analysis&#39046;&#22495;&#30340;&#31867;&#12289;&#23646;&#24615;&#21644;&#35268;&#21017;&#32452;&#25104;&#12290;&#38500;&#20102;&#26412;&#20307;&#26412;&#36523;&#65292;&#36824;&#36890;&#36807;SWRL&#21644;SPARQL&#26597;&#35810;&#26469;&#34920;&#31034;&#19981;&#21516;&#30340;&#35268;&#21017;&#12290;&#21033;&#29992;OntoSNAQA&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#20110;&#19968;&#20010;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#20197;&#23637;&#31034;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of social network theory and methods of analysis have been applied to different domains in recent years, including public health. The complete procedure for carrying out a social network analysis (SNA) is a time-consuming task that entails a series of steps in which the expert in social network analysis could make mistakes. This research presents a multi-domain knowledge model capable of automatically gathering data and carrying out different social network analyses in different domains, without errors and obtaining the same conclusions that an expert in SNA would obtain. The model is represented in an ontology called OntoSNAQA, which is made up of classes, properties and rules representing the domains of People, Questionnaires and Social Network Analysis. Besides the ontology itself, different rules are represented by SWRL and SPARQL queries. A Knowledge Based System was created using OntoSNAQA and applied to a real case study in order to show the advantages of the approach. F
&lt;/p&gt;</description></item><item><title>DyExpert&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#21382;&#21490;&#28436;&#21270;&#36807;&#31243;&#24182;&#32467;&#21512;&#38142;&#25509;&#39044;&#27979;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#22270;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02168</link><description>&lt;p&gt;
&#36328;&#22495;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#30340;&#19968;&#31181;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
One Graph Model for Cross-domain Dynamic Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02168
&lt;/p&gt;
&lt;p&gt;
DyExpert&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#21382;&#21490;&#28436;&#21270;&#36807;&#31243;&#24182;&#32467;&#21512;&#38142;&#25509;&#39044;&#27979;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#22270;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DyExpert&#65292;&#19968;&#31181;&#29992;&#20110;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#26126;&#30830;&#22320;&#24314;&#27169;&#21382;&#21490;&#28436;&#21270;&#36807;&#31243;&#65292;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#22270;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#36827;&#32780;&#36827;&#34892;&#29305;&#23450;&#27169;&#24335;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;DyExpert&#37319;&#29992;&#20102;&#35299;&#30721;&#22120;&#20248;&#21270;&#30340;transformer&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#28436;&#21270;&#24314;&#27169;&#21644;&#38142;&#25509;&#39044;&#27979;&#30340;&#8220;&#26465;&#20214;&#38142;&#25509;&#29983;&#25104;&#8221;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24182;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;DyExpert&#22312;&#21253;&#21547;6&#30334;&#19975;&#20010;&#21160;&#24577;&#36793;&#30340;&#24191;&#27867;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#20843;&#20010;&#26410;&#35757;&#32451;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;DyExpert&#22312;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#19982;&#30456;&#21516;&#35774;&#32622;&#19979;&#30340;&#20808;&#36827;&#22522;&#20934;&#30456;&#27604;&#65292;DyExpert&#22312;&#20843;&#20010;&#22270;&#19978;&#30340;&#24179;&#22343;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;11.40&#65285;&#12290;&#26356;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;&#22312;&#20845;&#20010;&#26410;&#35757;&#32451;&#30340;&#22270;&#19978;&#65292;&#23427;&#36229;&#36807;&#20102;&#20843;&#20010;&#20808;&#36827;&#22522;&#32447;&#30340;&#20840;&#30417;&#30563;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes DyExpert, a dynamic graph model for cross-domain link prediction. It can explicitly model historical evolving processes to learn the evolution pattern of a specific downstream graph and subsequently make pattern-specific link predictions. DyExpert adopts a decode-only transformer and is capable of efficiently parallel training and inference by \textit{conditioned link generation} that integrates both evolution modeling and link prediction. DyExpert is trained by extensive dynamic graphs across diverse domains, comprising 6M dynamic edges. Extensive experiments on eight untrained graphs demonstrate that DyExpert achieves state-of-the-art performance in cross-domain link prediction. Compared to the advanced baseline under the same setting, DyExpert achieves an average of 11.40% improvement Average Precision across eight graphs. More impressive, it surpasses the fully supervised performance of 8 advanced baselines on 6 untrained graphs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;EvaLLM&#30340;&#27010;&#24565;&#27169;&#22411;&#26632;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#35299;&#37322;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21487;&#35270;&#21270;&#12290;&#23427;&#35299;&#20915;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35270;&#21270;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02167</link><description>&lt;p&gt;
Vi(E)va LLM&#65281;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#35299;&#37322;&#29983;&#25104;AI&#21487;&#35270;&#21270;&#30340;&#27010;&#24565;&#27169;&#22411;&#26632;
&lt;/p&gt;
&lt;p&gt;
Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;EvaLLM&#30340;&#27010;&#24565;&#27169;&#22411;&#26632;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#35299;&#37322;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21487;&#35270;&#21270;&#12290;&#23427;&#35299;&#20915;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35270;&#21270;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#21487;&#35270;&#21270;&#26159;&#19968;&#39033;&#21476;&#32769;&#30340;&#20219;&#21153;&#65292;&#22810;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#21644;&#23454;&#36341;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#25104;&#20026;&#25903;&#25345;&#19982;&#21487;&#35270;&#21270;&#30456;&#20851;&#30340;&#29983;&#25104;&#20219;&#21153;&#30340;&#26377;&#36259;&#36873;&#25321;&#65292;&#24182;&#23637;&#31034;&#20102;&#21021;&#27493;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23384;&#22312;&#35832;&#22810;&#38382;&#39064;&#65292;&#22914;&#25351;&#23548;LLM&#29983;&#25104;&#25152;&#38656;&#32467;&#26524;&#30340;&#22810;&#31181;&#26041;&#24335;&#65292;&#24341;&#23548;&#29983;&#25104;&#30340;&#19981;&#21516;&#35270;&#35282;&#65288;&#22522;&#20110;&#20195;&#30721;&#12289;&#22522;&#20110;&#22270;&#20687;&#12289;&#22522;&#20110;&#35821;&#27861;&#65289;&#65292;&#20197;&#21450;&#21363;&#20351;&#22312;&#21487;&#35270;&#21270;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#23384;&#22312;&#30340;&#24187;&#35273;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#20351;&#29992;&#19981;&#22914;&#39044;&#26399;&#30340;&#37027;&#26679;&#21487;&#34892;&#12290;&#22312;&#31867;&#20284;&#20026;LLM&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#20513;&#35758;&#19979;&#65292;&#26412;&#25991;&#38024;&#23545;&#36890;&#36807;LLM&#23545;&#29983;&#25104;&#30340;&#21487;&#35270;&#21270;&#24314;&#27169;&#30340;&#35780;&#20272;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#35780;&#20272;&#27169;&#22411;&#26632;&#65292;EvaLLM&#65292;&#23427;&#23558;&#35780;&#20272;&#24037;&#20316;&#20998;&#35299;&#20026;&#20854;&#21407;&#23376;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23545;&#20854;&#24615;&#36136;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24182;&#27010;&#36848;&#20102;&#22914;&#20309;&#36827;&#34892;&#35780;&#20272;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic generation of visualizations is an old task that, through the years, has shown more and more interest from the research and practitioner communities. Recently, large language models (LLM) have become an interesting option for supporting generative tasks related to visualization, demonstrating initial promising results. At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the visualization generation task, make their usage less affordable than expected. Following similar initiatives for benchmarking LLMs, this paper copes with the problem of modeling the evaluation of a generated visualization through an LLM. We propose a theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort in its atomic components, characterizes their nature, and provides an overview of how
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;TSIS&#31639;&#27861;&#20316;&#20026;t-SMILES&#30340;&#34917;&#20805;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSIS&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02164</link><description>&lt;p&gt;
TSIS: t-SMILES&#30340;&#34917;&#20805;&#31639;&#27861;&#29992;&#20110;&#22522;&#20110;&#29255;&#27573;&#30340;&#20998;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
TSIS: A Supplementary Algorithm to t-SMILES for Fragment-based Molecular Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;TSIS&#31639;&#27861;&#20316;&#20026;t-SMILES&#30340;&#34917;&#20805;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSIS&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#31526;&#20018;&#22522;&#26412;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#22914;SMILES&#65292;&#22312;&#32447;&#24615;&#34920;&#31034;&#20998;&#23376;&#20449;&#24687;&#26041;&#38754;&#26159;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#20351;&#29992;&#37197;&#23545;&#31526;&#21495;&#21644;&#35299;&#26512;&#31639;&#27861;&#23548;&#33268;&#20102;&#38271;&#30340;&#35821;&#27861;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#24471;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20063;&#38590;&#20197;&#20934;&#30830;&#29702;&#35299;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#23613;&#31649;DeepSMILES&#21644;SELFIES&#24050;&#32463;&#35299;&#20915;&#20102;&#26576;&#20123;&#38480;&#21046;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#22788;&#29702;&#39640;&#32423;&#35821;&#27861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20351;&#24471;&#19968;&#20123;&#23383;&#31526;&#20018;&#38590;&#20197;&#38405;&#35835;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#34917;&#20805;&#31639;&#27861;TSIS&#65288;TSID&#31616;&#21270;&#65289;&#65292;&#29992;&#20110;t-SMILES&#23478;&#26063;&#12290;TSIS&#19982;&#21478;&#19968;&#20010;&#22522;&#20110;&#29255;&#27573;&#30340;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;SAFE&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SAFE&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;TSIS&#32487;&#32493;&#20351;&#29992;t-SMILES&#20013;&#23450;&#20041;&#30340;&#26641;&#20316;&#20026;&#20854;&#22522;&#30784;&#25968;&#25454;&#32467;&#26500;&#65292;&#36825;&#20351;&#20854;&#19982;SAFE&#27169;&#22411;&#26377;&#25152;&#19981;&#21516;&#12290;TSIS&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;SAFE&#27169;&#22411;&#65292;&#34920;&#26126;t-SMILES&#30340;&#26641;&#32467;&#26500;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
String-based molecular representations, such as SMILES, are a de facto standard for linearly representing molecular information. However, the must be paired symbols and the parsing algorithm result in long grammatical dependencies, making it difficult for even state-of-the-art deep learning models to accurately comprehend the syntax and semantics. Although DeepSMILES and SELFIES have addressed certain limitations, they still struggle with advanced grammar, which makes some strings difficult to read. This study introduces a supplementary algorithm, TSIS (TSID Simplified), to t-SMILES family. Comparative experiments between TSIS and another fragment-based linear solution, SAFE, indicate that SAFE presents challenges in managing long-term dependencies in grammar. TSIS continues to use the tree defined in t-SMILES as its foundational data structure, which sets it apart from the SAFE model. The performance of TSIS models surpasses that of SAFE models, indicating that the tree structure of t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#22320;&#38663;&#21442;&#25968;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#25968;&#25454;&#39044;&#27979;&#22320;&#38663;&#24378;&#24230;&#20998;&#24067;&#65292;&#24182;&#19988;&#20248;&#20110;&#24120;&#29992;&#30340;&#22320;&#38663;&#22320;&#38754;&#36816;&#21160;&#39044;&#27979;&#26041;&#31243;&#65288;GMPEs&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.02150</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#28151;&#21512;&#20998;&#31867;&#22238;&#24402;&#27169;&#22411;&#23545;&#22320;&#38663;&#24378;&#24230;&#20998;&#24067;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Prediction of Seismic Intensity Distributions Featuring Hybrid Classification-Regression Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#22320;&#38663;&#21442;&#25968;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#25968;&#25454;&#39044;&#27979;&#22320;&#38663;&#24378;&#24230;&#20998;&#24067;&#65292;&#24182;&#19988;&#20248;&#20110;&#24120;&#29992;&#30340;&#22320;&#38663;&#22320;&#38754;&#36816;&#21160;&#39044;&#27979;&#26041;&#31243;&#65288;GMPEs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#38663;&#26159;&#20154;&#31867;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#21644;&#33268;&#21629;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#12290;&#20934;&#30830;&#39044;&#27979;&#22320;&#38663;&#25439;&#23475;&#30340;&#31243;&#24230;&#21644;&#35780;&#20272;&#28508;&#22312;&#39118;&#38505;&#21487;&#20197;&#22312;&#25405;&#25937;&#20247;&#22810;&#29983;&#21629;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22320;&#38663;&#21442;&#25968;&#65288;&#20301;&#32622;&#12289;&#28145;&#24230;&#21644;&#38663;&#32423;&#65289;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#39044;&#27979;&#22320;&#38663;&#24378;&#24230;&#20998;&#24067;&#12290;&#30001;&#20110;&#23427;&#23436;&#20840;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#22320;&#29702;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#24378;&#24230;&#20998;&#24067;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;1997&#24180;&#33267;2020&#24180;&#22312;&#26085;&#26412;&#38468;&#36817;&#21457;&#29983;&#30340;&#22320;&#38663;&#30340;&#22320;&#38663;&#24378;&#24230;&#25968;&#25454;&#65292;&#20855;&#20307;&#21253;&#21547;1857&#20010;&#38663;&#32423;&#22312;5.0&#25110;&#26356;&#22823;&#30340;&#22320;&#38663;&#23454;&#20363;&#65292;&#25968;&#25454;&#26469;&#28304;&#20110;&#26085;&#26412;&#27668;&#35937;&#21381;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#22238;&#24402;&#27169;&#22411;&#21644;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#21033;&#29992;&#20108;&#32773;&#30340;&#20248;&#21183;&#21019;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#12290;&#22312;&#30456;&#20851;&#24615;&#26041;&#38754;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#24120;&#29992;&#30340;&#22320;&#38663;&#22320;&#38754;&#36816;&#21160;&#39044;&#27979;&#26041;&#31243;&#65288;Ground Motion Prediction Equations&#65292;GMPEs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Earthquakes are among the most immediate and deadly natural disasters that humans face. Accurately forecasting the extent of earthquake damage and assessing potential risks can be instrumental in saving numerous lives. In this study, we developed linear regression models capable of predicting seismic intensity distributions based on earthquake parameters: location, depth, and magnitude. Because it is completely data-driven, it can predict intensity distributions without geographical information. The dataset comprises seismic intensity data from earthquakes that occurred in the vicinity of Japan between 1997 and 2020, specifically containing 1,857 instances of earthquakes with a magnitude of 5.0 or greater, sourced from the Japan Meteorological Agency. We trained both regression and classification models and combined them to take advantage of both to create a hybrid model. The proposed model outperformed commonly used Ground Motion Prediction Equations (GMPEs) in terms of the correlatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#24613;&#32593;&#32476;(E-SC3I)&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;&#24863;&#30693;&#12289;&#36890;&#20449;&#12289;&#35745;&#31639;&#12289;&#32531;&#23384;&#21644;&#26234;&#33021;&#21151;&#33021;&#65292;&#36890;&#36807;&#32039;&#24613;&#35745;&#31639;&#12289;&#32531;&#23384;&#12289;&#38598;&#25104;&#36890;&#20449;&#21644;&#24863;&#30693;&#20197;&#21450;&#26234;&#33021;&#22686;&#24378;&#26426;&#21046;&#23454;&#29616;&#24555;&#36895;&#25509;&#20837;&#12289;&#21487;&#38752;&#20256;&#36755;&#21644;&#21160;&#24577;&#32593;&#32476;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#24320;&#38144;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21327;&#20316;&#25512;&#29702;&#26041;&#27861;(ACIM)&#12290;</title><link>https://arxiv.org/abs/2402.02146</link><description>&lt;p&gt;
&#32039;&#24613;&#35745;&#31639;&#65306;&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#21327;&#20316;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Emergency Computing: An Adaptive Collaborative Inference Method Based on Hierarchical Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#24613;&#32593;&#32476;(E-SC3I)&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;&#24863;&#30693;&#12289;&#36890;&#20449;&#12289;&#35745;&#31639;&#12289;&#32531;&#23384;&#21644;&#26234;&#33021;&#21151;&#33021;&#65292;&#36890;&#36807;&#32039;&#24613;&#35745;&#31639;&#12289;&#32531;&#23384;&#12289;&#38598;&#25104;&#36890;&#20449;&#21644;&#24863;&#30693;&#20197;&#21450;&#26234;&#33021;&#22686;&#24378;&#26426;&#21046;&#23454;&#29616;&#24555;&#36895;&#25509;&#20837;&#12289;&#21487;&#38752;&#20256;&#36755;&#21644;&#21160;&#24577;&#32593;&#32476;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#24320;&#38144;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21327;&#20316;&#25512;&#29702;&#26041;&#27861;(ACIM)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#29616;&#26377;&#25928;&#30340;&#32039;&#24613;&#21709;&#24212;&#20013;&#65292;&#21450;&#26102;&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#12289;&#26080;&#32541;&#30340;&#25351;&#25381;&#25968;&#25454;&#20256;&#36755;&#21644;&#21450;&#26102;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#38656;&#35201;&#24314;&#31435;&#19968;&#20010;&#24377;&#24615;&#30340;&#32039;&#24613;&#36890;&#20449;&#19987;&#29992;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#22522;&#30784;&#35774;&#26045;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#36890;&#20449;&#21644;&#24863;&#30693;&#26381;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#24863;&#30693;&#12289;&#36890;&#20449;&#12289;&#35745;&#31639;&#12289;&#32531;&#23384;&#21644;&#26234;&#33021;&#21151;&#33021;&#30340;&#32039;&#24613;&#32593;&#32476;(E-SC3I)&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#32039;&#24613;&#35745;&#31639;&#12289;&#32531;&#23384;&#12289;&#38598;&#25104;&#36890;&#20449;&#21644;&#24863;&#30693;&#20197;&#21450;&#26234;&#33021;&#22686;&#24378;&#26426;&#21046;&#12290;E-SC3I&#30830;&#20445;&#20102;&#24555;&#36895;&#25509;&#20837;&#22823;&#37327;&#29992;&#25143;&#12289;&#22312;&#19981;&#31283;&#23450;&#38142;&#36335;&#19978;&#21487;&#38752;&#20256;&#36755;&#25968;&#25454;&#20197;&#21450;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#21160;&#24577;&#32593;&#32476;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20248;&#21183;&#24102;&#26469;&#20102;&#37325;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32039;&#24613;&#35745;&#31639;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#21327;&#20316;&#25512;&#29702;&#26041;&#27861;(ACIM)&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Hierarchical Reinforcement Learning and Realization (HRLR)&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#25512;&#29702;&#33410;&#28857;&#65292;&#23454;&#29616;&#20102;&#21327;&#20316;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In achieving effective emergency response, the timely acquisition of environmental information, seamless command data transmission, and prompt decision-making are crucial. This necessitates the establishment of a resilient emergency communication dedicated network, capable of providing communication and sensing services even in the absence of basic infrastructure. In this paper, we propose an Emergency Network with Sensing, Communication, Computation, Caching, and Intelligence (E-SC3I). The framework incorporates mechanisms for emergency computing, caching, integrated communication and sensing, and intelligence empowerment. E-SC3I ensures rapid access to a large user base, reliable data transmission over unstable links, and dynamic network deployment in a changing environment. However, these advantages come at the cost of significant computation overhead. Therefore, we specifically concentrate on emergency computing and propose an adaptive collaborative inference method (ACIM) based on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#30340;&#36947;&#24503;&#21028;&#26029;&#21644;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21360;&#22320;&#35821;&#21644;&#26031;&#29926;&#24076;&#37324;&#35821;&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#26126;&#26174;&#20302;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#32780;&#36947;&#24503;&#21028;&#26029;&#20063;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#21464;&#21270;&#36739;&#22823;&#12290;</title><link>https://arxiv.org/abs/2402.02135</link><description>&lt;p&gt;
&#35821;&#35328;&#23545;LLMs&#30340;&#36947;&#24503;&#21028;&#26029;&#21644;&#25512;&#29702;&#33021;&#21147;&#26377;&#24433;&#21709;&#21527;&#65311;&#19968;&#39033;&#20351;&#29992;&#22810;&#35821;&#35328;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#30340;&#36947;&#24503;&#21028;&#26029;&#21644;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21360;&#22320;&#35821;&#21644;&#26031;&#29926;&#24076;&#37324;&#35821;&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#26126;&#26174;&#20302;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#32780;&#36947;&#24503;&#21028;&#26029;&#20063;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#21464;&#21270;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#23637;&#29616;&#30340;&#36947;&#24503;&#21028;&#26029;&#21644;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36947;&#24503;&#21028;&#26029;&#21462;&#20915;&#20110;&#38382;&#39064;&#25152;&#20351;&#29992;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#20102;&#38500;&#33521;&#35821;&#22806;&#30340;&#20116;&#31181;&#26032;&#35821;&#35328;&#65288;&#20013;&#25991;&#12289;&#21360;&#22320;&#35821;&#12289;&#20420;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#26031;&#29926;&#24076;&#37324;&#35821;&#65289;&#65292;&#24182;&#23545;&#19977;&#20010;&#20855;&#26377;&#36739;&#24378;&#22810;&#35821;&#35328;&#25991;&#26412;&#22788;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;LLMs&#65288;ChatGPT&#12289;GPT-4&#21644;Llama2Chat-70B&#65289;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#21518;&#26580;&#24615;&#20998;&#25968;&#25351;&#31034;&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#22312;&#21360;&#22320;&#35821;&#21644;&#26031;&#29926;&#24076;&#37324;&#35821;&#20013;&#26174;&#33879;&#20302;&#20110;&#35199;&#29677;&#29273;&#35821;&#12289;&#20420;&#35821;&#12289;&#20013;&#25991;&#21644;&#33521;&#35821;&#65292;&#32780;&#21518;&#22235;&#31181;&#35821;&#35328;&#30340;&#34920;&#29616;&#21017;&#27809;&#26377;&#26126;&#26174;&#30340;&#36235;&#21183;&#12290;&#36947;&#24503;&#21028;&#26029;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#20063;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the moral judgment and moral reasoning abilities exhibited by Large Language Models (LLMs) across languages through the Defining Issues Test. It is a well known fact that moral judgment depends on the language in which the question is asked. We extend the work of beyond English, to 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe three LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial multilingual text processing and generation abilities. Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages. The moral judgments too vary considerably by the language.
&lt;/p&gt;</description></item><item><title>&#22797;&#21512;&#20027;&#21160;&#23398;&#20064;&#20026;&#22810;&#39046;&#22495;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20102;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.02110</link><description>&lt;p&gt;
&#22797;&#21512;&#20027;&#21160;&#23398;&#20064;&#65306;&#22312;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#30340;&#22810;&#39046;&#22495;&#20027;&#21160;&#23398;&#20064;&#20013;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02110
&lt;/p&gt;
&lt;p&gt;
&#22797;&#21512;&#20027;&#21160;&#23398;&#20064;&#20026;&#22810;&#39046;&#22495;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20102;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20449;&#24687;&#37327;&#26368;&#22823;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#65292;&#20174;&#32780;&#22312;&#22266;&#23450;&#30340;&#26631;&#27880;&#39044;&#31639;&#20869;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;AL&#26041;&#27861;&#36890;&#24120;&#21482;&#36866;&#29992;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#24773;&#20917;&#65292;&#21363;&#25152;&#26377;&#25968;&#25454;&#37117;&#26469;&#33258;&#21516;&#19968;&#39046;&#22495;&#65288;&#20363;&#22914;&#65292;&#21516;&#19968;&#25968;&#25454;&#38598;&#65289;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#24448;&#24448;&#28041;&#21450;&#22810;&#20010;&#39046;&#22495;&#12290;&#20363;&#22914;&#65292;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#65292;&#36890;&#24120;&#24076;&#26395;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#22312;&#19981;&#21516;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#19981;&#21516;&#32972;&#26223;&#65289;&#20013;&#24037;&#20316;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#27599;&#20010;&#29615;&#22659;&#30340;&#22270;&#20687;&#26500;&#25104;&#19968;&#20010;&#39046;&#22495;&#12290;&#36825;&#31181;&#22810;&#39046;&#22495;AL&#35774;&#32622;&#23545;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#65288;1&#65289;&#22312;&#20998;&#37197;&#26631;&#27880;&#39044;&#31639;&#26102;&#24573;&#35270;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#65288;2&#65289;&#26080;&#27861;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#65292;&#31216;&#20026;&#22797;&#21512;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#65292;&#29992;&#20110;&#22810;&#39046;&#22495;AL&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#24335;&#22320;&#32771;&#34385;&#20102;&#38382;&#39064;&#20013;&#30340;&#39046;&#22495;&#23618;&#32423;&#21644;&#23454;&#20363;&#23618;&#32423;&#20449;&#24687;&#65307;CAL&#39318;&#20808;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) aims to improve model performance within a fixed labeling budget by choosing the most informative data points to label. Existing AL focuses on the single-domain setting, where all data come from the same domain (e.g., the same dataset). However, many real-world tasks often involve multiple domains. For example, in visual recognition, it is often desirable to train an image classifier that works across different environments (e.g., different backgrounds), where images from each environment constitute one domain. Such a multi-domain AL setting is challenging for prior methods because they (1) ignore the similarity among different domains when assigning labeling budget and (2) fail to handle distribution shift of data across different domains. In this paper, we propose the first general method, dubbed composite active learning (CAL), for multi-domain AL. Our approach explicitly considers the domain-level and instance-level information in the problem; CAL first assigns
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#30340;&#23454;&#38469;&#26426;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;LLM&#20248;&#21270;&#22120;&#24448;&#24448;&#21463;&#21040;&#33258;&#36523;&#20808;&#21069;&#30693;&#35782;&#30340;&#20559;&#35265;&#65292;&#38590;&#20197;&#20934;&#30830;&#35782;&#21035;&#38169;&#35823;&#30340;&#30495;&#27491;&#21407;&#22240;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#21512;&#36866;&#30340;&#25552;&#31034;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02101</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#22909;&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Good Prompt Optimizers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02101
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#30340;&#23454;&#38469;&#26426;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;LLM&#20248;&#21270;&#22120;&#24448;&#24448;&#21463;&#21040;&#33258;&#36523;&#20808;&#21069;&#30693;&#35782;&#30340;&#20559;&#35265;&#65292;&#38590;&#20197;&#20934;&#30830;&#35782;&#21035;&#38169;&#35823;&#30340;&#30495;&#27491;&#21407;&#22240;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#21512;&#36866;&#30340;&#25552;&#31034;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#36827;&#34892;&#33258;&#25105;&#21453;&#39304;&#21644;&#20248;&#21270;&#25552;&#31034;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#24213;&#23618;&#26426;&#21046;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#32780;LLM&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#30340;&#30495;&#27491;&#26377;&#25928;&#24615;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#20248;&#21270;&#30340;&#23454;&#38469;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLM&#20248;&#21270;&#22120;&#22312;&#21453;&#24605;&#36807;&#31243;&#20013;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#35782;&#21035;&#38169;&#35823;&#30340;&#30495;&#27491;&#21407;&#22240;&#65292;&#32780;&#26356;&#22810;&#22320;&#21463;&#21040;&#33258;&#36523;&#20808;&#21069;&#30693;&#35782;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#21453;&#24605;&#22312;&#35821;&#20041;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;LLM&#20248;&#21270;&#22120;&#20063;&#32463;&#24120;&#26080;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#20248;&#21270;&#27493;&#39588;&#20026;&#30446;&#26631;&#27169;&#22411;&#29983;&#25104;&#21512;&#36866;&#30340;&#25552;&#31034;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30446;&#26631;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#19981;&#21487;&#39044;&#27979;&#30340;&#12290;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#33258;&#21160;&#34892;&#20026;&#20248;&#21270;&#8221;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as Prompt Optimizers to self-reflect and refine prompts, has shown promising performance in recent studies. Despite the success, the underlying mechanism of this approach remains unexplored, and the true effectiveness of LLMs as Prompt Optimizers requires further validation. In this work, we conducted a comprehensive study to uncover the actual mechanism of LLM-based Prompt Optimization. Our findings reveal that the LLM optimizers struggle to identify the true causes of errors during reflection, tending to be biased by their own prior knowledge rather than genuinely reflecting on the errors. Furthermore, even when the reflection is semantically valid, the LLM optimizers often fail to generate appropriate prompts for the target models with a single prompt refinement step, partly due to the unpredictable behaviors of the target models. Based on the observations, we introduce a new "Automatic Behavior Optimization" par
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#65292;&#21457;&#29616;&#39640;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#38750;&#35821;&#35328;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#21644;&#34920;&#23618;&#30693;&#35782;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#20256;&#36755;&#30340;&#20027;&#35201;&#26159;&#25968;&#25454;&#24037;&#20214;&#21644;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2402.02099</link><description>&lt;p&gt;
&#20998;&#26512;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02099
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#65292;&#21457;&#29616;&#39640;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#38750;&#35821;&#35328;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#21644;&#34920;&#23618;&#30693;&#35782;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#20256;&#36755;&#30340;&#20027;&#35201;&#26159;&#25968;&#25454;&#24037;&#20214;&#21644;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20284;&#20046;&#26174;&#31034;&#20986;&#22312;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#21644;&#35774;&#32622;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#38646;-shot&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#31243;&#24230;&#34920;&#31034;&#36136;&#30097;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#28041;&#21450;&#22810;&#35821;&#35328;&#23454;&#20363;&#65292;&#25361;&#25112;&#20102;&#39640;&#38646;-shot&#24615;&#33021;&#22312;&#30446;&#26631;&#20219;&#21153;&#20013;&#21453;&#26144;&#39640;&#36328;&#35821;&#35328;&#33021;&#21147;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#19981;&#38656;&#35201;&#36716;&#31227;&#23454;&#38469;&#35821;&#35328;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#21644;&#34920;&#23618;&#30693;&#35782;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36328;&#35821;&#35328;&#20256;&#36755;&#30340;&#20027;&#35201;&#26159;&#25968;&#25454;&#24037;&#20214;&#21644;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#34987;&#24573;&#35270;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#65292;&#24182;&#24341;&#20837;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02097</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#39062;&#24615;&#20849;&#20139;&#35299;&#20915;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#65292;&#24182;&#24341;&#20837;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#21327;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#26159;&#20840;&#23616;&#29366;&#24577;&#30340;&#26032;&#39062;&#24615;&#19981;&#21487;&#29992;&#65292;&#32780;&#23616;&#37096;&#35266;&#23519;&#30340;&#26032;&#39062;&#24615;&#23384;&#22312;&#20559;&#24046;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#26234;&#33021;&#20307;&#22914;&#20309;&#21327;&#35843;&#22320;&#36827;&#34892;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;MACE&#12290;&#36890;&#36807;&#20165;&#20256;&#25773;&#23616;&#37096;&#26032;&#39062;&#24615;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#32771;&#34385;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#32047;&#35745;&#26032;&#39062;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#20869;&#22312;&#22238;&#25253;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#25506;&#32034;&#20135;&#29983;&#26356;&#22823;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#19977;&#31181;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in decentralized cooperative multi-agent reinforcement learning faces two challenges. One is that the novelty of global states is unavailable, while the novelty of local observations is biased. The other is how agents can explore in a coordinated way. To address these challenges, we propose MACE, a simple yet effective multi-agent coordinated exploration method. By communicating only local novelty, agents can take into account other agents' local novelty to approximate the global novelty. Further, we newly introduce weighted mutual information to measure the influence of one agent's action on other agents' accumulated novelty. We convert it as an intrinsic reward in hindsight to encourage agents to exert more influence on other agents' exploration and boost coordinated exploration. Empirically, we show that MACE achieves superior performance in three multi-agent environments with sparse rewards.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38646;&#26679;&#26412;&#36965;&#24863;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#30340;&#28145;&#24230;&#35821;&#20041;-&#35270;&#35273;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25910;&#38598;&#21487;&#35270;&#21270;&#21487;&#26816;&#27979;&#23646;&#24615;&#26469;&#35299;&#20915;&#20808;&#21069;ZSL&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#20110;&#25163;&#21160;&#26631;&#35760;&#30340;&#23646;&#24615;&#25110;&#35789;&#23884;&#20837;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02094</link><description>&lt;p&gt;
&#38024;&#23545;&#38646;&#26679;&#26412;&#36965;&#24863;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#30340;&#28145;&#24230;&#35821;&#20041;-&#35270;&#35273;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Deep Semantic-Visual Alignment for Zero-Shot Remote Sensing Image Scene Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02094
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38646;&#26679;&#26412;&#36965;&#24863;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#30340;&#28145;&#24230;&#35821;&#20041;-&#35270;&#35273;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25910;&#38598;&#21487;&#35270;&#21270;&#21487;&#26816;&#27979;&#23646;&#24615;&#26469;&#35299;&#20915;&#20808;&#21069;ZSL&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#20110;&#25163;&#21160;&#26631;&#35760;&#30340;&#23646;&#24615;&#25110;&#35789;&#23884;&#20837;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#27599;&#20010;&#31867;&#21035;&#30340;&#20016;&#23500;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#32473;&#23450;&#36965;&#24863;&#30446;&#26631;&#25968;&#25454;&#24211;&#30340;&#21160;&#24577;&#22686;&#38271;&#20107;&#23454;&#65292;&#20026;&#27599;&#20010;&#36965;&#24863;&#31867;&#21035;&#27880;&#37322;&#26631;&#31614;&#32791;&#26102;&#19988;&#19981;&#29616;&#23454;&#12290;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#21487;&#20197;&#35782;&#21035;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#31867;&#21035;&#65292;&#20026;&#19978;&#36848;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;ZSL&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#20110;&#25163;&#21160;&#26631;&#35760;&#30340;&#23646;&#24615;&#25110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#35789;&#23884;&#20837;&#26469;&#23558;&#30693;&#35782;&#20174;&#24050;&#30693;&#31867;&#21035;&#20256;&#36882;&#21040;&#26032;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#24320;&#21019;&#24615;&#30340;ZSL&#27169;&#22411;&#20351;&#29992;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#20027;&#35201;&#20851;&#27880;&#27599;&#20010;&#22270;&#20687;&#20013;&#20986;&#29616;&#30340;&#20027;&#35201;&#23545;&#35937;&#65292;&#24573;&#35270;&#20102;&#22312;&#36965;&#24863;&#22330;&#26223;&#20998;&#31867;&#20013;&#20063;&#24456;&#37325;&#35201;&#30340;&#32972;&#26223;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#33258;&#21160;&#25910;&#38598;&#21487;&#35270;&#21270;&#21487;&#26816;&#27979;&#23646;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have achieved promising progress in remote sensing (RS) image classification, for which the training process requires abundant samples for each class. However, it is time-consuming and unrealistic to annotate labels for each RS category, given the fact that the RS target database is increasing dynamically. Zero-shot learning (ZSL) allows for identifying novel classes that are not seen during training, which provides a promising solution for the aforementioned problem. However, previous ZSL models mainly depend on manually-labeled attributes or word embeddings extracted from language models to transfer knowledge from seen classes to novel classes. Besides, pioneer ZSL models use convolutional neural networks pre-trained on ImageNet, which focus on the main objects appearing in each image, neglecting the background context that also matters in RS scene classification. To address the above problems, we propose to collect visually detectable attributes automatically. W
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02085</link><description>&lt;p&gt;
DeCoF:&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#36827;&#34892;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeCoF: Generated Video Detection via Frame Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02085
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#35270;&#39057;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#65292;&#36825;&#23548;&#33268;&#31038;&#20250;&#38754;&#20020;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#20351;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#25104;&#20026;&#32039;&#36843;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#29992;&#20110;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#36827;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#27979;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#20266;&#24433;&#22312;&#24320;&#21457;&#29983;&#25104;&#35270;&#39057;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#26816;&#27979;&#22120;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#35270;&#39057;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#65288;DeCoF&#65289;&#65292;&#23427;&#28040;&#38500;&#20102;&#31354;&#38388;&#20266;&#24433;&#22312;&#36890;&#29992;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DeCoF&#22312;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35270;&#39057;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating quality of video generated by advanced video generation methods leads to new security challenges in society, which makes generated video detection an urgent research priority.To foster collaborative research in this area, we construct the first open-source dataset explicitly for generated video detection, providing a valuable resource for the community to benchmark and improve detection methodologies. Through a series of carefully designed probe experiments, our study explores the significance of temporal and spatial artifacts in developing general and robust detectors for generated video. Based on the principle of video frame consistency, we introduce a simple yet effective detection model (DeCoF) that eliminates the impact of spatial artifacts during generalizing feature learning. Our extensive experiments demonstrate the efficacy of DeCoF in detecting videos produced by unseen video generation models and confirm its powerful generalization capabilities across several 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#30340;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;ProtoAU&#65289;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#38543;&#26426;&#25277;&#26679;&#24341;&#36215;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#29305;&#24449;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02079</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#30340;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Prototypical Contrastive Learning through Alignment and Uniformity for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#30340;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;ProtoAU&#65289;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#38543;&#26426;&#25277;&#26679;&#24341;&#36215;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#29305;&#24449;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21327;&#21516;&#36807;&#28388;&#65288;GCF&#65289;&#26159;&#26368;&#24120;&#20351;&#29992;&#30340;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#20043;&#19968;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#30340;GCF&#24050;&#32463;&#21463;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#25216;&#26415;&#20174;&#29616;&#23454;&#22330;&#26223;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#23398;&#20064;&#28041;&#21450;&#26500;&#24314;&#23545;&#27604;&#23545;&#30340;&#37492;&#21035;&#20219;&#21153;&#30340;&#23454;&#20363;&#12290;GCL&#26041;&#27861;&#38754;&#20020;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#36127;&#26679;&#26412;&#21487;&#33021;&#20855;&#26377;&#19982;&#27491;&#26679;&#26412;&#31867;&#20284;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#23548;&#33268;&#29305;&#24449;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#30340;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;&#65292;&#31216;&#20026;ProtoAU&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#21407;&#22411;&#65288;&#32858;&#31867;&#20013;&#24515;&#65289;&#20316;&#20026;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Collaborative Filtering (GCF), one of the most widely adopted recommendation system methods, effectively captures intricate relationships between user and item interactions. Graph Contrastive Learning (GCL) based GCF has gained significant attention as it leverages self-supervised techniques to extract valuable signals from real-world scenarios. However, many methods usually learn the instances of discrimination tasks that involve the construction of contrastive pairs through random sampling. GCL approaches suffer from sampling bias issues, where the negatives might have a semantic structure similar to that of the positives, thus leading to a loss of effective feature representation. To address these problems, we present the \underline{Proto}typical contrastive learning through \underline{A}lignment and \underline{U}niformity for recommendation, which is called \textbf{ProtoAU}. Specifically, we first propose prototypes (cluster centroids) as a latent space to ensure consistency 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;One-Class Classification&#65288;OCC&#65289;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;$\mathbb{X}$&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#26032;&#39062;&#27491;&#21017;&#21270;&#39033;&#21644;&#25968;&#25454;&#25551;&#36848;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#24322;&#36136;&#30340;&#25968;&#25454;&#38598;&#20013;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.02066</link><description>&lt;p&gt;
$\mathbb{X}$&#29992;&#25143;&#30340;&#21487;&#20449;&#24230;&#65306;&#19968;&#31181;&#19968;&#31867;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Trustworthiness of $\mathbb{X}$ Users: A One-Class Classification Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;One-Class Classification&#65288;OCC&#65289;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;$\mathbb{X}$&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#26032;&#39062;&#27491;&#21017;&#21270;&#39033;&#21644;&#25968;&#25454;&#25551;&#36848;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#24322;&#36136;&#30340;&#25968;&#25454;&#38598;&#20013;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$\mathbb{X}$&#65288;&#20197;&#21069;&#30340;Twitter&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#22312;&#20449;&#24687;&#20849;&#20139;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20351;&#24471;&#35813;&#24179;&#21488;&#29983;&#25104;&#30340;&#20869;&#23481;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#30830;&#20445;$\mathbb{X}$&#19978;&#30340;&#20449;&#20219;&#26159;&#30830;&#23450;&#29992;&#25143;&#30340;&#21487;&#20449;&#24230;&#24182;&#38450;&#27490;&#21508;&#20010;&#39046;&#22495;&#20986;&#29616;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#23613;&#31649;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#21487;&#20449;&#24230;&#20998;&#37197;&#32473;$\mathbb{X}$&#29992;&#25143;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#21487;&#20449;&#25110;&#19981;&#21487;&#20449;&#26159;&#24120;&#35265;&#20570;&#27861;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#27169;&#22411;&#36827;&#34892;&#36825;&#19968;&#30446;&#30340;&#30340;&#25506;&#32034;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;OCC&#27169;&#22411;&#23545;$\mathbb{X}$&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20248;&#21270;&#23376;&#31354;&#38388;&#21644;&#25968;&#25454;&#25551;&#36848;&#20197;&#29992;&#20110;OCC&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#39033;&#29992;&#20110;&#23376;&#31354;&#38388;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#65288;SSVDD&#65289;&#65292;&#20197;&#34920;&#36798;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#38598;&#20013;&#24230;&#65292;&#25429;&#25417;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
$\mathbb{X}$ (formerly Twitter) is a prominent online social media platform that plays an important role in sharing information making the content generated on this platform a valuable source of information. Ensuring trust on $\mathbb{X}$ is essential to determine the user credibility and prevents issues across various domains. While assigning credibility to $\mathbb{X}$ users and classifying them as trusted or untrusted is commonly carried out using traditional machine learning models, there is limited exploration about the use of One-Class Classification (OCC) models for this purpose. In this study, we use various OCC models for $\mathbb{X}$ user classification. Additionally, we propose using a subspace-learning-based approach that simultaneously optimizes both the subspace and data description for OCC. We also introduce a novel regularization term for Subspace Support Vector Data Description (SSVDD), expressing data concentration in a lower-dimensional subspace that captures diverse
&lt;/p&gt;</description></item><item><title>AnthroScore&#26159;&#19968;&#31181;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#21547;&#20154;&#24615;&#21270;&#35821;&#35328;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;AnthroScore&#19982;&#20154;&#31867;&#23545;&#20154;&#24615;&#21270;&#30340;&#21028;&#26029;&#21644;&#31038;&#20250;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#20154;&#24615;&#21270;&#32500;&#24230;&#30456;&#19968;&#33268;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#36817;15&#24180;&#26469;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#20154;&#24418;&#21270;&#27700;&#24179;&#31283;&#27493;&#22686;&#21152;&#65292;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#35770;&#25991;&#20855;&#26377;&#26368;&#39640;&#30340;&#20154;&#24418;&#21270;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.02056</link><description>&lt;p&gt;
AnthroScore: &#19968;&#31181;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#20154;&#24615;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AnthroScore: A Computational Linguistic Measure of Anthropomorphism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02056
&lt;/p&gt;
&lt;p&gt;
AnthroScore&#26159;&#19968;&#31181;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#21547;&#20154;&#24615;&#21270;&#35821;&#35328;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;AnthroScore&#19982;&#20154;&#31867;&#23545;&#20154;&#24615;&#21270;&#30340;&#21028;&#26029;&#21644;&#31038;&#20250;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#20154;&#24615;&#21270;&#32500;&#24230;&#30456;&#19968;&#33268;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#36817;15&#24180;&#26469;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#20154;&#24418;&#21270;&#27700;&#24179;&#31283;&#27493;&#22686;&#21152;&#65292;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#35770;&#25991;&#20855;&#26377;&#26368;&#39640;&#30340;&#20154;&#24418;&#21270;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24418;&#21270;&#65292;&#21363;&#23558;&#20154;&#31867;&#29305;&#24449;&#36171;&#20104;&#38750;&#20154;&#31867;&#23454;&#20307;&#65292;&#24050;&#32463;&#22609;&#36896;&#20102;&#20851;&#20110;&#25216;&#26415;&#24433;&#21709;&#21644;&#21487;&#33021;&#24615;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AnthroScore&#65292;&#19968;&#31181;&#38544;&#21547;&#20154;&#24615;&#21270;&#35821;&#35328;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#26469;&#37327;&#21270;&#38750;&#20154;&#31867;&#23454;&#20307;&#22312;&#21608;&#22260;&#35821;&#22659;&#20013;&#34987;&#38544;&#24335;&#22320;&#26694;&#26550;&#20026;&#20154;&#31867;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AnthroScore&#19982;&#20154;&#31867;&#23545;&#20154;&#24615;&#21270;&#30340;&#21028;&#26029;&#20197;&#21450;&#31038;&#20250;&#31185;&#23398;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;&#20154;&#24615;&#21270;&#32500;&#24230;&#30456;&#19968;&#33268;&#12290;&#21463;&#21040;&#35745;&#31639;&#26426;&#31185;&#23398;&#35805;&#35821;&#20013;&#35823;&#23548;&#24615;&#20154;&#24418;&#21270;&#30340;&#25285;&#24551;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#20351;&#29992;AnthroScore&#20998;&#26512;&#20102;15&#24180;&#30340;&#30740;&#31350;&#35770;&#25991;&#21644;&#19979;&#28216;&#26032;&#38395;&#25991;&#31456;&#12290;&#22312;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#24418;&#21270;&#22312;&#26102;&#38388;&#19978;&#31283;&#27493;&#22686;&#21152;&#65292;&#24182;&#19988;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#35770;&#25991;&#20013;&#20855;&#26377;&#26368;&#39640;&#30340;&#20154;&#24418;&#21270;&#27700;&#24179;&#12290;&#22312;ACL&#35770;&#25991;&#20013;&#65292;&#20154;&#24418;&#21270;&#30340;&#26102;&#38388;&#22686;&#21152;&#19982;&#20851;&#38190;&#31070;&#32463;&#36827;&#23637;&#30456;&#20851;&#12290;&#22312;&#31185;&#23398;&#25285;&#24551;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anthropomorphism, or the attribution of human-like characteristics to non-human entities, has shaped conversations about the impacts and possibilities of technology. We present AnthroScore, an automatic metric of implicit anthropomorphism in language. We use a masked language model to quantify how non-human entities are implicitly framed as human by the surrounding context. We show that AnthroScore corresponds with human judgments of anthropomorphism and dimensions of anthropomorphism described in social science literature. Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles. In research papers, we find that anthropomorphism has steadily increased over time, and that papers related to language models have the most anthropomorphism. Within ACL papers, temporal increases in anthropomorphism are correlated with key neural advancements. Building upon concerns of scientific
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#29702;&#35770;&#21407;&#21017;&#25903;&#25345;&#30340;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#26041;&#24046;&#23545;&#40784;&#20998;&#25968;(Variance Alignment Score&#65292;VAS)&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#30340;VAS&#26469;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.02055</link><description>&lt;p&gt;
&#26041;&#24046;&#23545;&#40784;&#20998;&#25968;: &#19968;&#31181;&#31616;&#21333;&#20294;&#38590;&#20197;&#36229;&#36234;&#30340;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#29702;&#35770;&#21407;&#21017;&#25903;&#25345;&#30340;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#26041;&#24046;&#23545;&#40784;&#20998;&#25968;(Variance Alignment Score&#65292;VAS)&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#30340;VAS&#26469;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#36873;&#25321;&#24050;&#32463;&#25104;&#20026;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22024;&#26434;&#30340;&#32593;&#32476;&#25277;&#26679;&#25968;&#25454;&#38598;&#19978;&#12290;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#31574;&#30053;&#26159;&#20026;&#27599;&#20010;&#26679;&#26412;&#20998;&#37197;&#36136;&#37327;&#20998;&#25968;&#65292;&#22914;CLIP&#30456;&#20284;&#24230;&#65292;&#24182;&#20445;&#30041;&#20855;&#26377;&#26368;&#39640;&#20998;&#25968;&#30340;&#25968;&#25454;&#23545;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#20998;&#24067;&#26159;&#26080;&#30693;&#30340;&#65292;&#22987;&#32456;&#26080;&#27861;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#29702;&#35770;&#21407;&#21017;&#25903;&#25345;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#26041;&#24046;&#23545;&#40784;&#20998;&#25968;(Variance Alignment Score&#65292;VAS)&#65292;&#23427;&#30340;&#24418;&#24335;&#26159;$\langle \Sigma_{\text{test}}, \Sigma_i\rangle$&#12290;&#36825;&#37324;&#65292;$\Sigma_{\text{test}}$&#34920;&#31034;&#25105;&#20204;&#24076;&#26395;&#23545;&#40784;&#30340;&#30446;&#26631;&#65288;&#20132;&#21449;&#65289;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#33021;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;$\Sigma_i$&#34920;&#31034;&#31532;$i$&#20010;&#26679;&#26412;&#30340;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#24352;&#37327;&#31215;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#26368;&#22823;&#21270;&#24635;&#30340;VAS&#12290;&#25105;&#20204;&#22312;&#31616;&#21270;&#30340;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#20010;&#26041;&#27861;&#30340;&#29702;&#35770;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, data selection has emerged as a core issue for large-scale visual-language model pretraining, especially on noisy web-curated datasets. One widely adopted strategy assigns quality scores such as CLIP similarity for each sample and retains the data pairs with the highest scores. However, these approaches are agnostic of data distribution and always fail to select the most informative samples. To solve this problem, we propose a simple yet theoretically principled metric named Variance Alignment Score (VAS), which has the form $\langle \Sigma_{\text{test}}, \Sigma_i\rangle$. Here, $\Sigma_{\text{test}}$ represents the target (cross-)covariance matrix we aim to align, potentially based on prior knowledge, while $\Sigma_i$ denotes the tensor product of single or multi-modal representations for the $i$-th sample. We further design a new data selection method that maximizes the total VAS. We provide theoretical analysis in a simplified setting to demonstrate the theoretical 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.02054</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Neural Scaling Laws on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#27169;&#22411;&#65288;&#20363;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#21464;&#25442;&#22120;&#65289;&#24050;&#25104;&#20026;&#21033;&#29992;&#21508;&#31181;&#31867;&#22411;&#22270;&#30340;&#30693;&#35782;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#32553;&#25918;&#29305;&#24615;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#65292;&#23545;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26469;&#23454;&#29616;&#22823;&#22411;&#22270;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#32034;&#20102;&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#25105;&#20204;&#39318;&#20808;&#39564;&#35777;&#20102;&#36825;&#20123;&#23450;&#24459;&#22312;&#22270;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#25551;&#36848;&#32553;&#25918;&#34892;&#20026;&#30340;&#20844;&#24335;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#30830;&#23450;&#20102;&#36807;&#25311;&#21512;&#21487;&#33021;&#26159;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#32553;&#25918;&#34892;&#20026;&#65292;&#36825;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#35266;&#23519;&#32467;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25105;&#20204;&#24314;&#35758;&#22270;&#25968;&#37327;&#26080;&#27861;&#26377;&#25928;&#34913;&#37327;&#22270;&#25968;&#25454;&#37327;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#22240;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Deep graph models (e.g., graph neural networks and graph transformers) have become important techniques for leveraging knowledge across various types of graphs. Yet, the scaling properties of deep graph models have not been systematically investigated, casting doubt on the feasibility of achieving large graph models through enlarging the model and dataset sizes. In this work, we delve into neural scaling laws on graphs from both model and data perspectives. We first verify the validity of such laws on graphs, establishing formulations to describe the scaling behaviors. For model scaling, we investigate the phenomenon of scaling law collapse and identify overfitting as the potential reason. Moreover, we reveal that the model depth of deep graph models can impact the model scaling behaviors, which differ from observations in other domains such as CV and NLP. For data scaling, we suggest that the number of graphs can not effectively metric the graph data volume in scaling law since the si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32463;&#27982;&#23454;&#24800;&#30340;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#26694;&#26550;&#65288;AGA&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#31574;&#30053;&#26367;&#20195;LLM&#25512;&#29702;&#21644;&#21387;&#32553;&#23545;&#35805;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#20302;&#25104;&#26412;&#30340;&#21487;&#20449;&#20114;&#21160;&#65292;&#19988;&#23545;&#20110;&#26377;&#38480;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#21487;&#20449;&#34892;&#20026;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.02053</link><description>&lt;p&gt;
&#32463;&#27982;&#23454;&#24800;&#30340;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Affordable Generative Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32463;&#27982;&#23454;&#24800;&#30340;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#26694;&#26550;&#65288;AGA&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#31574;&#30053;&#26367;&#20195;LLM&#25512;&#29702;&#21644;&#21387;&#32553;&#23545;&#35805;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#20302;&#25104;&#26412;&#30340;&#21487;&#20449;&#20114;&#21160;&#65292;&#19988;&#23545;&#20110;&#26377;&#38480;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#21487;&#20449;&#34892;&#20026;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#36827;&#20102;&#30495;&#23454;&#20132;&#20114;&#26234;&#33021;&#20307;&#30340;&#27169;&#25311;&#12290;&#28982;&#32780;&#65292;&#32500;&#25345;&#38271;&#26102;&#38388;&#26234;&#33021;&#20307;&#20132;&#20114;&#30340;&#24040;&#22823;&#25104;&#26412;&#23545;&#20110;&#37096;&#32626;&#22522;&#20110;LLM&#30340;&#21487;&#20449;&#26234;&#33021;&#20307;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32463;&#27982;&#23454;&#24800;&#30340;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#65288;AGA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;-&#29615;&#22659;&#21644;&#26234;&#33021;&#20307;&#38388;&#20132;&#20114;&#30340;&#20004;&#20010;&#23618;&#38754;&#19978;&#23454;&#29616;&#20302;&#25104;&#26412;&#30340;&#21487;&#20449;&#20114;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26234;&#33021;&#20307;-&#29615;&#22659;&#20132;&#20114;&#65292;&#25105;&#20204;&#29992;&#23398;&#20064;&#30340;&#31574;&#30053;&#26367;&#20195;&#20102;&#37325;&#22797;&#30340;LLM&#25512;&#29702;&#65307;&#32780;&#23545;&#20110;&#26234;&#33021;&#20307;&#38388;&#20132;&#20114;&#65292;&#25105;&#20204;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31038;&#20250;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21387;&#32553;&#36741;&#21161;&#23545;&#35805;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#31350;&#20102;LLM&#26234;&#33021;&#20307;&#20013;&#30340;&#21487;&#20449;&#34892;&#20026;&#24418;&#25104;&#26426;&#21046;&#65292;&#35777;&#26126;&#26234;&#33021;&#20307;&#20165;&#33021;&#22312;&#22266;&#23450;&#29615;&#22659;&#20013;&#29983;&#25104;&#26377;&#38480;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) has significantly advanced the simulation of believable interactive agents. However, the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents. Therefore, in this paper, we develop Affordable Generative Agents (AGA), a framework for enabling the generation of believable and low-cost interactions on both agent-environment and inter-agents levels. Specifically, for agent-environment interactions, we substitute repetitive LLM inferences with learned policies; while for inter-agent interactions, we model the social relationships between agents and compress auxiliary dialogue information. Extensive experiments on multiple environments show the effectiveness and efficiency of our proposed framework. Also, we delve into the mechanisms of emergent believable behaviors lying in LLM agents, demonstrating that agents can only generate finite behaviors in fixed environments, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20214;&#24335;&#24494;&#22411;AI&#27169;&#22359;&#65292;&#21487;&#20197;&#23558;&#26234;&#33021;&#21644;&#36873;&#25321;&#24615;&#20256;&#24863;&#22120;&#25968;&#25454;&#20256;&#36755;&#38598;&#25104;&#21040;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#12290;&#36890;&#36807;&#25918;&#32622;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38752;&#36817;&#20256;&#24863;&#22120;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20256;&#36755;&#30340;&#26234;&#33021;&#25511;&#21046;&#65292;&#31579;&#36873;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#20256;&#36755;&#39057;&#29575;&#20002;&#24323;&#26080;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#20248;&#21270;&#36817;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#23454;&#29616;&#23454;&#26102;&#20256;&#24863;&#22120;&#25511;&#21046;&#12290;&#23450;&#21046;&#35757;&#32451;&#36807;&#31243;&#21644;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#8220;&#25042;&#24816;&#8221;&#20256;&#24863;&#22120;&#20572;&#29992;&#31574;&#30053;&#25552;&#21319;&#26694;&#26550;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#29420;&#31435;&#20110;&#20854;&#20182;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#21487;&#19982;&#20043;&#20849;&#23384;&#12290;</title><link>https://arxiv.org/abs/2402.02043</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#21644;&#36873;&#25321;&#24615;&#20256;&#24863;&#22120;&#25968;&#25454;&#20256;&#36755;&#30340;&#25554;&#20214;&#24335;&#24494;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data Transmission
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02043
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20214;&#24335;&#24494;&#22411;AI&#27169;&#22359;&#65292;&#21487;&#20197;&#23558;&#26234;&#33021;&#21644;&#36873;&#25321;&#24615;&#20256;&#24863;&#22120;&#25968;&#25454;&#20256;&#36755;&#38598;&#25104;&#21040;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#12290;&#36890;&#36807;&#25918;&#32622;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38752;&#36817;&#20256;&#24863;&#22120;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20256;&#36755;&#30340;&#26234;&#33021;&#25511;&#21046;&#65292;&#31579;&#36873;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#20256;&#36755;&#39057;&#29575;&#20002;&#24323;&#26080;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#20248;&#21270;&#36817;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#23454;&#29616;&#23454;&#26102;&#20256;&#24863;&#22120;&#25511;&#21046;&#12290;&#23450;&#21046;&#35757;&#32451;&#36807;&#31243;&#21644;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#8220;&#25042;&#24816;&#8221;&#20256;&#24863;&#22120;&#20572;&#29992;&#31574;&#30053;&#25552;&#21319;&#26694;&#26550;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#29420;&#31435;&#20110;&#20854;&#20182;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#21487;&#19982;&#20043;&#20849;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#20294;&#24403;&#21069;&#24863;&#30693;&#31995;&#32479;&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26234;&#33021;&#65292;&#23548;&#33268;&#25968;&#25454;&#20135;&#29983;&#37327;&#24040;&#22823;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24863;&#30693;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25918;&#32622;&#22312;&#20256;&#24863;&#22120;&#38468;&#36817;&#65292;&#20026;&#24863;&#30693;&#26694;&#26550;&#25552;&#20379;&#26234;&#33021;&#25968;&#25454;&#20256;&#36755;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#21450;&#26102;&#21453;&#39304;&#32473;&#24863;&#30693;&#31995;&#32479;&#65292;&#21482;&#20256;&#36755;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#35843;&#33410;&#25968;&#25454;&#20256;&#36755;&#30340;&#39057;&#29575;&#20002;&#24323;&#26080;&#20851;&#20449;&#24687;&#12290;&#36817;&#20256;&#24863;&#22120;&#27169;&#22411;&#32463;&#36807;&#37327;&#21270;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#23454;&#26102;&#20256;&#24863;&#22120;&#25511;&#21046;&#12290;&#20026;&#20102;&#25552;&#21319;&#26694;&#26550;&#24615;&#33021;&#65292;&#23450;&#21046;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#8220;&#25042;&#24816;&#8221;&#20256;&#24863;&#22120;&#20572;&#29992;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#19982;&#20854;&#20182;&#29289;&#32852;&#32593;&#26694;&#26550;&#27491;&#20132;&#65292;&#21487;&#20197;&#20445;&#35777;&#20849;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications in the Internet of Things (IoT) utilize machine learning to analyze sensor-generated data. However, a major challenge lies in the lack of targeted intelligence in current sensing systems, leading to vast data generation and increased computational and communication costs. To address this challenge, we propose a novel sensing module to equip sensing frameworks with intelligent data transmission capabilities by integrating a highly efficient machine learning model placed near the sensor. This model provides prompt feedback for the sensing system to transmit only valuable data while discarding irrelevant information by regulating the frequency of data transmission. The near-sensor model is quantized and optimized for real-time sensor control. To enhance the framework's performance, the training process is customized and a "lazy" sensor deactivation strategy utilizing temporal information is introduced. The suggested method is orthogonal to other IoT frameworks and can be cons
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDPs&#30340;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#35777;&#20302;&#36951;&#25022;&#30340;&#24773;&#20917;&#19979;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#31639;&#27861;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#22343;&#20026; $\tilde{\mathcal{O}}({T}^{3/4})$&#12290;</title><link>https://arxiv.org/abs/2402.02042</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23545;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDP&#36827;&#34892;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDPs&#30340;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#35777;&#20302;&#36951;&#25022;&#30340;&#24773;&#20917;&#19979;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#31639;&#27861;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#22343;&#20026; $\tilde{\mathcal{O}}({T}^{3/4})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#39046;&#22495;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#30740;&#31350;&#20855;&#26377;&#36890;&#29992;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#24179;&#22343;&#22238;&#25253;CMDP&#30340;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#23545;&#20598;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#30830;&#20445;&#20302;&#36951;&#25022;&#20445;&#35777;&#20197;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#19978;&#20855;&#26377; $\tilde{\mathcal{O}}({T}^{3/4})$ &#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demonstrate that our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and $\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
&lt;/p&gt;</description></item><item><title>&#35813;&#31454;&#36187;&#26088;&#22312;&#22635;&#34917;&#22810;&#26041;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30740;&#31350;&#19981;&#36275;&#65292;&#36890;&#36807;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23450;&#21046;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#22522;&#20110;MPIGD&#21644;MPHV&#25351;&#26631;&#23545;&#20004;&#37096;&#20998;&#38382;&#39064;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20197;&#24179;&#22343;&#25490;&#21517;&#20316;&#20026;&#24615;&#33021;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.02033</link><description>&lt;p&gt;
CEC 2024&#22810;&#26041;&#22810;&#30446;&#26631;&#20248;&#21270;&#31454;&#36187;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Benchmark for CEC 2024 Competition on Multiparty Multiobjective Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31454;&#36187;&#26088;&#22312;&#22635;&#34917;&#22810;&#26041;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30740;&#31350;&#19981;&#36275;&#65292;&#36890;&#36807;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23450;&#21046;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#22522;&#20110;MPIGD&#21644;MPHV&#25351;&#26631;&#23545;&#20004;&#37096;&#20998;&#38382;&#39064;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20197;&#24179;&#22343;&#25490;&#21517;&#20316;&#20026;&#24615;&#33021;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#31454;&#36187;&#20851;&#27880;&#22810;&#26041;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65288;MPMOPs&#65289;&#65292;&#20854;&#20013;&#22810;&#20010;&#20915;&#31574;&#32773;&#20855;&#26377;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#22914;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#24212;&#29992;&#20013;&#25152;&#35265;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#19982;&#20256;&#32479;&#22810;&#30446;&#26631;&#20248;&#21270;&#30456;&#27604;&#65292;MPMOPs&#20173;&#28982;&#30740;&#31350;&#19981;&#36275;&#12290;&#35813;&#31454;&#36187;&#26088;&#22312;&#36890;&#36807;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23450;&#21046;&#24314;&#27169;&#26041;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#27979;&#35797;&#22871;&#20214;&#20998;&#20026;&#20004;&#37096;&#20998;&#65306;&#20855;&#26377;&#20849;&#21516;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#30340;&#38382;&#39064;&#21644;&#20855;&#26377;&#26410;&#30693;&#35299;&#30340;&#21452;&#26041;&#22810;&#30446;&#26631;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#65288;BPMO-UAVPP&#65289;&#38382;&#39064;&#12290;&#31532;&#19968;&#37096;&#20998;&#30340;&#20248;&#21270;&#31639;&#27861;&#20351;&#29992;&#22810;&#26041;&#20498;&#36716;&#24335;&#29983;&#25104;&#36317;&#31163;&#65288;MPIGD&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#31532;&#20108;&#37096;&#20998;&#20351;&#29992;&#22810;&#26041;&#36229;&#20307;&#31215;&#65288;MPHV&#65289;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#36328;&#25152;&#26377;&#38382;&#39064;&#30340;&#24179;&#22343;&#31639;&#27861;&#25490;&#21517;&#29992;&#20316;&#24615;&#33021;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The competition focuses on Multiparty Multiobjective Optimization Problems (MPMOPs), where multiple decision makers have conflicting objectives, as seen in applications like UAV path planning. Despite their importance, MPMOPs remain understudied in comparison to conventional multiobjective optimization. The competition aims to address this gap by encouraging researchers to explore tailored modeling approaches. The test suite comprises two parts: problems with common Pareto optimal solutions and Biparty Multiobjective UAV Path Planning (BPMO-UAVPP) problems with unknown solutions. Optimization algorithms for the first part are evaluated using Multiparty Inverted Generational Distance (MPIGD), and the second part is evaluated using Multiparty Hypervolume (MPHV) metrics. The average algorithm ranking across all problems serves as a performance benchmark.
&lt;/p&gt;</description></item><item><title>ScribFormer&#26159;&#19968;&#31181;&#26032;&#30340;CNN-Transformer&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#28034;&#40486;&#30417;&#30563;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#23427;&#36890;&#36807;&#34701;&#21512;CNN&#23398;&#20064;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;Transformer&#33719;&#24471;&#30340;&#20840;&#23616;&#34920;&#31034;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#28034;&#40486;&#27880;&#37322;&#20013;&#23398;&#20064;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02029</link><description>&lt;p&gt;
ScribFormer: Transformer&#20351;&#24471;&#22522;&#20110;&#28034;&#40486;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;CNN&#24037;&#20316;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02029
&lt;/p&gt;
&lt;p&gt;
ScribFormer&#26159;&#19968;&#31181;&#26032;&#30340;CNN-Transformer&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#28034;&#40486;&#30417;&#30563;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#23427;&#36890;&#36807;&#34701;&#21512;CNN&#23398;&#20064;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;Transformer&#33719;&#24471;&#30340;&#20840;&#23616;&#34920;&#31034;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#28034;&#40486;&#27880;&#37322;&#20013;&#23398;&#20064;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#28034;&#40486;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20855;&#26377;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;CNN&#26694;&#26550;&#12290;&#23613;&#31649;&#36825;&#20010;&#26694;&#26550;&#26377;&#22810;&#20010;&#22909;&#22788;&#65292;&#20294;&#26159;&#30001;&#20110;&#21367;&#31215;&#23618;&#21482;&#33021;&#25429;&#25417;&#20855;&#26377;&#23616;&#37096;&#24863;&#21463;&#37326;&#30340;&#23567;&#33539;&#22260;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#24456;&#38590;&#20174;&#28034;&#40486;&#27880;&#37322;&#25552;&#20379;&#30340;&#26377;&#38480;&#20449;&#24687;&#20013;&#23398;&#20064;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CNN-Transformer&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#28034;&#40486;&#30417;&#30563;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#31216;&#20026;ScribFormer&#12290;&#25152;&#25552;&#20986;&#30340;ScribFormer&#27169;&#22411;&#20855;&#26377;&#19977;&#20010;&#20998;&#25903;&#32467;&#26500;&#65292;&#21363;CNN&#20998;&#25903;&#65292;Transformer&#20998;&#25903;&#21644;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#31867;&#28608;&#27963;&#22270;&#65288;ACAM&#65289;&#20998;&#25903;&#30340;&#28151;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CNN&#20998;&#25903;&#19982;Transformer&#20998;&#25903;&#21512;&#20316;&#65292;&#23558;&#20174;CNN&#23398;&#20064;&#30340;&#23616;&#37096;&#29305;&#24449;&#19982;&#20174;Transformer&#33719;&#24471;&#30340;&#20840;&#23616;&#34920;&#31034;&#30456;&#34701;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#20811;&#26381;&#29616;&#26377;&#28034;&#40486;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent scribble-supervised segmentation methods commonly adopt a CNN framework with an encoder-decoder architecture. Despite its multiple benefits, this framework generally can only capture small-range feature dependency for the convolutional layer with the local receptive field, which makes it difficult to learn global shape information from the limited information provided by scribble annotations. To address this issue, this paper proposes a new CNN-Transformer hybrid solution for scribble-supervised medical image segmentation called ScribFormer. The proposed ScribFormer model has a triple-branch structure, i.e., the hybrid of a CNN branch, a Transformer branch, and an attention-guided class activation map (ACAM) branch. Specifically, the CNN branch collaborates with the Transformer branch to fuse the local features learned from CNN with the global representations obtained from Transformer, which can effectively overcome limitations of existing scribble-supervised segmentation m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;&#65288;MENOL&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23567;&#24050;&#30693;&#31867;&#21644;&#26410;&#30693;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#21484;&#22238;&#29575;&#65292;&#24182;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.02026</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multimodal-Enhanced Objectness Learner for Corner Case Detection in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;&#65288;MENOL&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23567;&#24050;&#30693;&#31867;&#21644;&#26410;&#30693;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#21484;&#22238;&#29575;&#65292;&#24182;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#20307;&#26816;&#27979;&#30340;&#20808;&#21069;&#24037;&#20316;&#20013;&#65292;&#23553;&#38381;&#22330;&#26223;&#19979;&#30340;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#20294;&#22312;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#24182;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#20854;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#19990;&#30028;&#38382;&#39064;&#26159;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#36825;&#20123;&#26696;&#20363;&#20013;&#34920;&#29616;&#22256;&#38590;&#65292;&#36807;&#24230;&#20381;&#36182;&#35270;&#35273;&#22806;&#35266;&#65292;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20943;&#23567;&#24050;&#30693;&#31867;&#21644;&#26410;&#30693;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#22810;&#27169;&#24577;&#22686;&#24378;&#30340;&#29289;&#20214;&#23398;&#20064;&#22120;&#30340;&#27010;&#24565;&#12290;&#20511;&#21161;&#35270;&#35273;&#20013;&#24515;&#21644;&#22270;&#20687;-&#25991;&#26412;&#20004;&#31181;&#24418;&#24335;&#65292;&#25105;&#20204;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#23558;&#29289;&#20214;&#23398;&#20064;&#22120;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31867;&#21035;&#24863;&#30693;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20110;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;&#65288;MENOL&#65289;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#21484;&#22238;&#29575;&#65292;&#24182;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#20165;&#20351;&#29992;5100&#20010;&#26631;&#31614;&#35757;&#32451;&#22270;&#20687;&#30340;CODA-val&#25968;&#25454;&#38598;&#19978;&#65292;MENOL&#23454;&#29616;&#20102;76.6%&#30340;mAR-corner&#21644;79.8%&#30340;mAR-agnostic&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous works on object detection have achieved high accuracy in closed-set scenarios, but their performance in open-world scenarios is not satisfactory. One of the challenging open-world problems is corner case detection in autonomous driving. Existing detectors struggle with these cases, relying heavily on visual appearance and exhibiting poor generalization ability. In this paper, we propose a solution by reducing the discrepancy between known and unknown classes and introduce a multimodal-enhanced objectness notion learner. Leveraging both vision-centric and image-text modalities, our semi-supervised learning framework imparts objectness knowledge to the student model, enabling class-aware detection. Our approach, Multimodal-Enhanced Objectness Learner (MENOL) for Corner Case Detection, significantly improves recall for novel classes with lower training costs. By achieving a 76.6% mAR-corner and 79.8% mAR-agnostic on the CODA-val dataset with just 5100 labeled training images, MEN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#65292;&#21253;&#25324;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25581;&#31034;&#20102;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02025</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Constraint Formulations in Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#65292;&#21253;&#25324;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25581;&#31034;&#20102;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#26102;&#65292;&#30830;&#20445;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#23433;&#20840;RL&#25104;&#20026;&#19968;&#31181;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#23433;&#20840;&#20248;&#21270;&#20195;&#29702;&#31574;&#30053;&#30340;&#22522;&#26412;&#32780;&#24378;&#22823;&#30340;&#33539;&#20363;&#12290;&#22522;&#20110;&#32422;&#26463;&#20934;&#21017;&#30340;&#23433;&#20840;RL&#26041;&#27861;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#23433;&#20840;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#22312;RL&#20013;&#23454;&#29616;&#23433;&#20840;&#24615;&#30340;&#23581;&#35797;&#28608;&#22686;&#65292;&#20294;&#30001;&#20110;&#32422;&#26463;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35752;&#35770;&#24456;&#23569;&#65292;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#24615;&#20102;&#35299;&#20173;&#28982;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30693;&#35782;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20195;&#34920;&#24615;&#32422;&#26463;&#24418;&#24335;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#20197;&#21450;&#38024;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#30340;&#31934;&#36873;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25581;&#31034;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring safety is critical when applying reinforcement learning (RL) to real-world problems. Consequently, safe RL emerges as a fundamental and powerful paradigm for safely optimizing an agent's policy from experimental data. A popular safe RL approach is based on a constrained criterion, which solves the problem of maximizing expected cumulative reward under safety constraints. Though there has been recently a surge of such attempts to achieve safety in RL, a systematic understanding of the field is difficult due to 1) the diversity of constraint representations and 2) little discussion of their interrelations. To address this knowledge gap, we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms specifically designed for each formulation. Furthermore, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02023</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Contrastive Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#39044;&#27979;&#30001;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24615;&#32780;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#28369;&#21160;&#31383;&#21475;&#26469;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#37096;&#20998;&#22312;&#30701;&#31383;&#21475;&#20869;&#34987;&#25429;&#25417;&#21040;&#30340;&#38271;&#26399;&#21464;&#21270;&#65288;&#21363;&#22806;&#31383;&#21475;&#21464;&#21270;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#32858;&#28966;&#38271;&#26399;&#21464;&#21270;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#23545;&#27604;&#25439;&#22833;&#23558;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#26500;&#24314;&#27491;&#36127;&#23545;&#12290;&#24403;&#19982;&#25105;&#20204;&#30340;&#20998;&#35299;&#32593;&#32476;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#23545;&#27604;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#20102;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, es
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;LLM&#29983;&#25104;&#30340;&#26469;&#28304;&#26159;&#21542;&#30495;&#27491;&#25903;&#25345;&#23427;&#20204;&#25152;&#20570;&#30340;&#20027;&#24352;&#65311;&#36890;&#36807;&#39564;&#35777;&#28304;&#30340;&#30456;&#20851;&#24615;&#21644;&#24320;&#21457;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;50%&#21040;90%&#30340;LLM&#22238;&#31572;&#24182;&#27809;&#26377;&#20805;&#20998;&#22320;&#24471;&#21040;&#23427;&#20204;&#25152;&#25552;&#20379;&#30340;&#26469;&#28304;&#30340;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.02008</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#21487;&#20197;&#27491;&#30830;&#24341;&#29992;&#30456;&#20851;&#21307;&#23398;&#21442;&#32771;&#25991;&#29486;&#65311;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How well do LLMs cite relevant medical references? An evaluation framework and analyses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;LLM&#29983;&#25104;&#30340;&#26469;&#28304;&#26159;&#21542;&#30495;&#27491;&#25903;&#25345;&#23427;&#20204;&#25152;&#20570;&#30340;&#20027;&#24352;&#65311;&#36890;&#36807;&#39564;&#35777;&#28304;&#30340;&#30456;&#20851;&#24615;&#21644;&#24320;&#21457;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;50%&#21040;90%&#30340;LLM&#22238;&#31572;&#24182;&#27809;&#26377;&#20805;&#20998;&#22320;&#24471;&#21040;&#23427;&#20204;&#25152;&#25552;&#20379;&#30340;&#26469;&#28304;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22312;&#21508;&#31181;&#20020;&#24202;&#39046;&#22495;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#26368;&#36817;&#34920;&#29616;&#20986;&#33394;&#30340;&#21830;&#19994;LLM&#65292;&#23427;&#20204;&#33021;&#22815;&#24341;&#29992;&#26469;&#28304;&#26469;&#25903;&#25345;&#20854;&#22238;&#31572;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;LLM&#29983;&#25104;&#30340;&#26469;&#28304;&#26159;&#21542;&#30495;&#27491;&#25903;&#25345;&#23427;&#20204;&#25152;&#20570;&#30340;&#20027;&#24352;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#19987;&#23478;&#30340;&#21307;&#23398;&#27880;&#37322;&#26159;&#19968;&#31181;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#35780;&#20272;&#29942;&#39048;&#65292;&#25105;&#20204;&#35777;&#26126;GPT-4&#22312;&#39564;&#35777;&#28304;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#38750;&#24120;&#20934;&#30830;&#65292;&#19982;&#19968;&#32452;&#21307;&#29983;&#30340;&#21028;&#26029;&#36798;&#21040;88%&#30340;&#19968;&#33268;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;SourceCheckup&#8221;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#24182;&#20351;&#29992;&#23427;&#35780;&#20272;&#20102;1200&#20010;&#29983;&#25104;&#30340;&#38382;&#39064;&#19978;&#30340;&#20116;&#20010;&#34920;&#29616;&#26368;&#20339;&#30340;LLM&#65292;&#24635;&#35745;&#36229;&#36807;40K&#23545;&#30340;&#38472;&#36848;&#21644;&#26469;&#28304;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;50%&#21040;90%&#30340;LLM&#22238;&#31572;&#24182;&#27809;&#26377;&#20805;&#20998;&#22320;&#24471;&#21040;&#23427;&#20204;&#25152;&#25552;&#20379;&#30340;&#26469;&#28304;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#36824;&#23545;GPT-4&#36827;&#34892;&#20102;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently being used to answer medical questions across a variety of clinical domains. Recent top-performing commercial LLMs, in particular, are also capable of citing sources to support their responses. In this paper, we ask: do the sources that LLMs generate actually support the claims that they make? To answer this, we propose three contributions. First, as expert medical annotations are an expensive and time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is highly accurate in validating source relevance, agreeing 88% of the time with a panel of medical doctors. Second, we develop an end-to-end, automated pipeline called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs on a dataset of 1200 generated questions, totaling over 40K pairs of statements and sources. Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36229;&#32500;&#24230;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#20302;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#36827;&#34892;&#32447;&#24615;&#36229;&#32500;&#24230;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#36866;&#24212;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.01999</link><description>&lt;p&gt;
&#36793;&#32536;&#19978;&#22522;&#20110;&#26032;&#39062;&#36229;&#32500;&#24230;&#35745;&#31639;&#30340;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Hyperdimensional Computing Framework for Online Time Series Forecasting on the Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36229;&#32500;&#24230;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#20302;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#36827;&#34892;&#32447;&#24615;&#36229;&#32500;&#24230;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#36866;&#24212;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#22312;&#32447;&#21644;&#31163;&#32447;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#19981;&#33021;&#26377;&#25928;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21464;&#21270;&#65292;&#32780;&#22312;&#32447;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#24120;&#24120;&#26114;&#36149;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;&#22312;&#32447;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;&#32447;&#24615;&#36229;&#32500;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#38750;&#32447;&#24615;&#20302;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34987;&#26144;&#23556;&#21040;&#39640;&#32500;&#65288;&#36229;&#32500;&#24230;&#65289;&#31354;&#38388;&#36827;&#34892;&#32447;&#24615;&#36229;&#32500;&#24230;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;TSF-HD&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20849;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#36229;&#32500;&#24230;&#26144;&#23556;&#21644;&#32447;&#24615;&#36229;&#32500;&#24230;&#39044;&#27979;&#22120;&#65292;&#20197;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;TSF-HD&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#38477;&#20302;&#20102;&#25512;&#26029;&#24310;&#36831;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, both online and offline deep learning models have been developed for time series forecasting. However, offline deep forecasting models fail to adapt effectively to changes in time-series data, while online deep forecasting models are often expensive and have complex training procedures. In this paper, we reframe the online nonlinear time-series forecasting problem as one of linear hyperdimensional time-series forecasting. Nonlinear low-dimensional time-series data is mapped to high-dimensional (hyperdimensional) spaces for linear hyperdimensional prediction, allowing fast, efficient and lightweight online time-series forecasting. Our framework, TSF-HD, adapts to time-series distribution shifts using a novel co-training framework for its hyperdimensional mapping and its linear hyperdimensional predictor. TSF-HD is shown to outperform the state of the art, while having reduced inference latency, for both short-term and long-term time series forecasting. Our code is publi
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#20102;&#38544;&#31169;&#25285;&#24551;&#65292;&#29616;&#26377;&#30740;&#31350;&#30528;&#37325;&#20110;&#27169;&#22411;&#26412;&#36523;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#24517;&#35201;&#36827;&#34892;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#65292;&#20851;&#27880;&#29992;&#25143;&#30340;&#34892;&#20026;&#21644;&#20559;&#22909;&#65292;&#35774;&#35745;&#24037;&#20855;&#21644;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25484;&#25511;&#20010;&#20154;&#25968;&#25454;&#30340;&#25152;&#26377;&#26435;&#12290;</title><link>https://arxiv.org/abs/2402.01994</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#38544;&#31169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Human-Centered Privacy Research in the Age of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01994
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#20102;&#38544;&#31169;&#25285;&#24551;&#65292;&#29616;&#26377;&#30740;&#31350;&#30528;&#37325;&#20110;&#27169;&#22411;&#26412;&#36523;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#24517;&#35201;&#36827;&#34892;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#65292;&#20851;&#27880;&#29992;&#25143;&#30340;&#34892;&#20026;&#21644;&#20559;&#22909;&#65292;&#35774;&#35745;&#24037;&#20855;&#21644;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25484;&#25511;&#20010;&#20154;&#25968;&#25454;&#30340;&#25152;&#26377;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#21450;&#20854;&#22312;&#38754;&#21521;&#29992;&#25143;&#30340;&#31995;&#32479;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#24341;&#36215;&#20102;&#37325;&#22823;&#30340;&#38544;&#31169;&#25285;&#24551;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#23545;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#30340;&#30740;&#31350;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#65306;&#25506;&#32034;LLMs&#22914;&#20309;&#23548;&#33268;&#38544;&#31169;&#39118;&#38505;&#65292;&#20363;&#22914;&#35760;&#24518;&#65292;&#25110;&#22914;&#20309;&#36890;&#36807;&#20869;&#23481;&#25512;&#26029;&#20154;&#20204;&#30340;&#20010;&#20154;&#29305;&#24449;&#12290;&#25105;&#20204;&#35748;&#20026;&#26377;&#24517;&#35201;&#36827;&#34892;&#26356;&#22810;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#30340;&#20154;&#31867;&#22240;&#32032;&#65306;&#20363;&#22914;&#30740;&#31350;LLMs&#35774;&#35745;&#33539;&#24335;&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#30340;&#25259;&#38706;&#34892;&#20026;&#12289;&#29992;&#25143;&#30340;&#24515;&#29702;&#27169;&#22411;&#21644;&#38544;&#31169;&#25511;&#21046;&#30340;&#20559;&#22909;&#65292;&#20197;&#21450;&#35774;&#35745;&#24037;&#20855;&#12289;&#31995;&#32479;&#21644;&#24037;&#20214;&#20197;&#36171;&#20104;&#32456;&#31471;&#29992;&#25143;&#23545;&#20010;&#20154;&#25968;&#25454;&#30340;&#25152;&#26377;&#26435;&#12290;&#20026;&#20102;&#26500;&#24314;&#21487;&#29992;&#12289;&#39640;&#25928;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#31995;&#32479;&#65292;&#20351;&#20854;&#30001;&#36825;&#20123;&#20855;&#26377;&#19981;&#23436;&#21892;&#38544;&#31169;&#23646;&#24615;&#30340;&#27169;&#22411;&#36827;&#34892;&#39537;&#21160;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21457;&#36215;&#35752;&#35770;&#65292;&#25311;&#23450;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#23545;LLMs&#21152;&#36895;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns. To date, research on these privacy concerns has been model-centered: exploring how LLMs lead to privacy risks like memorization, or can be used to infer personal characteristics about people from their content. We argue that there is a need for more research focusing on the human aspect of these privacy issues: e.g., research on how design paradigms for LLMs affect users' disclosure behaviors, users' mental models and preferences for privacy controls, and the design of tools, systems, and artifacts that empower end-users to reclaim ownership over their personal data. To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems. This Special Interest Group (SIG) ai
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PVAW&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#24182;&#22312;&#20998;&#26512;RSV&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27169;&#22411;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.01987</link><description>&lt;p&gt;
&#22312;&#32447;&#36716;&#31227;&#23398;&#20064;&#29992;&#20110;RSV&#30149;&#20363;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Transfer Learning for RSV Case Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01987
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PVAW&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#24182;&#22312;&#20998;&#26512;RSV&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27169;&#22411;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#26102;&#65292;&#20250;&#38754;&#20020;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#26631;&#27880;&#20449;&#24687;&#21294;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#39044;&#27979;&#20307;&#31215;&#33258;&#36866;&#24212;&#21152;&#26435;&#65288;PVAW&#65289;&#12290;PVAW&#22312;&#25972;&#21512;&#27169;&#22411;&#20013;&#21019;&#36896;&#24615;&#22320;&#23454;&#29616;&#20102;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#65292;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#36129;&#29486;&#24230;&#33258;&#21160;&#35843;&#25972;&#26435;&#37325;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21305;&#20857;&#22561;&#22823;&#23398;&#21307;&#23398;&#20013;&#24515;&#25910;&#38598;&#30340;&#22810;&#20010;&#23395;&#33410;&#30340;&#21628;&#21560;&#36947;&#21512;&#32990;&#30149;&#27602;&#65288;RSV&#65289;&#25968;&#25454;&#19978;&#24212;&#29992;PVAW&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#31361;&#20986;&#20102;&#22312;&#32447;&#36716;&#31227;&#23398;&#20064;&#22312;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become a pivotal technique in machine learning, renowned for its effectiveness in various real-world applications. However, a significant challenge arises when applying this approach to sequential epidemiological data, often characterized by a scarcity of labeled information. To address this challenge, we introduce Predictive Volume-Adaptive Weighting (PVAW), a novel online multi-source transfer learning method. PVAW innovatively implements a dynamic weighting mechanism within an ensemble model, allowing for the automatic adjustment of weights based on the relevance and contribution of each source and target model. We demonstrate the effectiveness of PVAW through its application in analyzing Respiratory Syncytial Virus (RSV) data, collected over multiple seasons at the University of Pittsburgh Medical Center. Our method showcases significant improvements in model performance over existing baselines, highlighting the potential of online transfer learning in handlin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#37322;&#21644;&#37325;&#21551;&#20004;&#31181;&#26041;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#20061;&#20010;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#31243;&#24230;&#65292;&#35813;&#25216;&#26415;&#19981;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#25110;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20462;&#25913;&#65292;&#24076;&#26395;&#33021;&#21551;&#21457;&#20854;&#20182;&#38646;-shot&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.01981</link><description>&lt;p&gt;
&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38646;-shot&#35782;&#21035;&#21644;&#38477;&#20302;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#37322;&#21644;&#37325;&#21551;&#20004;&#31181;&#26041;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#20061;&#20010;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#31243;&#24230;&#65292;&#35813;&#25216;&#26415;&#19981;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#25110;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20462;&#25913;&#65292;&#24076;&#26395;&#33021;&#21551;&#21457;&#20854;&#20182;&#38646;-shot&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20063;&#23481;&#26131;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#65292;&#20294;&#22823;&#22810;&#25968;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#25110;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20462;&#25913;&#65292;&#36825;&#22312;&#27809;&#26377;&#21487;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#38646;-shot&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#25216;&#26415;&#26469;&#20943;&#23569;&#21051;&#26495;&#21360;&#35937;&#12290;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;&#35299;&#37322;&#33258;&#25105;&#21435;&#20559;&#24046;&#21644;&#37325;&#21551;&#33258;&#25105;&#21435;&#20559;&#24046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#25105;&#21435;&#20559;&#24046;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20061;&#20010;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#31243;&#24230;&#65292;&#21482;&#20381;&#36182;&#20110;LLM&#33258;&#36523;&#21644;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#20854;&#20013;&#35299;&#37322;&#27491;&#30830;&#22320;&#35782;&#21035;&#20986;&#26080;&#25928;&#30340;&#20551;&#35774;&#65292;&#32780;&#37325;&#21551;&#21017;&#20135;&#29983;&#20102;&#26368;&#22823;&#30340;&#20559;&#35265;&#20943;&#23569;&#25928;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#24320;&#21551;&#23545;&#20854;&#20182;&#38646;-shot&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#27010;&#36848;&#12290;&#23427;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#38598;&#25104;&#36825;&#20123;&#31995;&#32479;&#30340;&#36890;&#29992;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.01968</link><description>&lt;p&gt;
&#23545;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;agent&#31995;&#32479;&#30340;&#35843;&#26597;&#65306;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#27010;&#36848;&#12290;&#23427;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#38598;&#25104;&#36825;&#20123;&#31995;&#32479;&#30340;&#36890;&#29992;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#20852;&#20027;&#39064;&#30340;&#20852;&#36215;&#65292;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#20852;&#36259;&#27491;&#22312;&#22686;&#21152;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#33879;&#25104;&#23601;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#33258;&#20027;&#20195;&#29702;&#20013;&#36798;&#21040;&#20154;&#31867;&#26234;&#33021;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#20351;&#36825;&#20123;&#20195;&#29702;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#23548;&#33322;&#19981;&#30830;&#23450;&#24615;&#12290;&#24403;&#22788;&#29702;&#21160;&#24577;&#24773;&#20917;&#26102;&#65292;&#19978;&#19979;&#25991;&#24847;&#35782;&#25104;&#20026;&#24378;&#21270;&#22810;agent&#31995;&#32479;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;agent&#31995;&#32479;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#27010;&#36848;&#22914;&#20309;&#23558;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#19982;&#22810;agent&#31995;&#32479;&#38598;&#25104;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#26368;&#20808;&#36827;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;agent&#31995;&#32479;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20419;&#36827;&#36825;&#20123;&#31995;&#32479;&#20043;&#38388;&#38598;&#25104;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810; agent &#31995;&#32479;&#30340;&#29305;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36807;&#31243;&#26469;&#24314;&#27169;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#22810;agent&#31995;&#32479;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;OPSurv&#65292;&#36890;&#36807;&#27491;&#20132;&#22810;&#39033;&#24335;&#31215;&#20998;&#31639;&#27861;&#25552;&#20379;&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#24615;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#21021;&#22987;&#38646;&#26465;&#20214;&#21644;&#27491;&#20132;&#22810;&#39033;&#24335;&#30340;&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#27599;&#20010;&#39118;&#38505;&#20107;&#20214;&#30340;&#21151;&#33021;&#36817;&#20284;&#31995;&#25968;&#30340;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;-&#21202;&#35753;&#24503;&#31215;&#20998;&#27861;&#26500;&#24314;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#31454;&#20105;&#39118;&#38505;&#22330;&#26223;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01955</link><description>&lt;p&gt;
OPSurv&#65306;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#27491;&#20132;&#22810;&#39033;&#24335;&#31215;&#20998;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
OPSurv: Orthogonal Polynomials Quadrature Algorithm for Survival Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;OPSurv&#65292;&#36890;&#36807;&#27491;&#20132;&#22810;&#39033;&#24335;&#31215;&#20998;&#31639;&#27861;&#25552;&#20379;&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#24615;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#21021;&#22987;&#38646;&#26465;&#20214;&#21644;&#27491;&#20132;&#22810;&#39033;&#24335;&#30340;&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#27599;&#20010;&#39118;&#38505;&#20107;&#20214;&#30340;&#21151;&#33021;&#36817;&#20284;&#31995;&#25968;&#30340;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;-&#21202;&#35753;&#24503;&#31215;&#20998;&#27861;&#26500;&#24314;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#31454;&#20105;&#39118;&#38505;&#22330;&#26223;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#20013;&#21333;&#19968;&#39118;&#38505;&#21644;&#31454;&#20105;&#39118;&#38505;&#22330;&#26223;&#30340;Orthogonal Polynomials Quadrature Algorithm for Survival Analysis&#65288;OPSurv&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#24615;&#36755;&#20986;&#12290;OPSurv&#21033;&#29992;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#21021;&#22987;&#38646;&#26465;&#20214;&#21644;&#20351;&#29992;&#27491;&#20132;&#22810;&#39033;&#24335;&#23545;&#27010;&#29575;&#23494;&#24230;&#36827;&#34892;&#21807;&#19968;&#20998;&#35299;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39118;&#38505;&#20107;&#20214;&#23398;&#20064;&#21151;&#33021;&#36817;&#20284;&#31995;&#25968;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;-&#21202;&#35753;&#24503;&#31215;&#20998;&#27861;&#26500;&#24314;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#23545;&#25239;&#36807;&#24230;&#25311;&#21512;&#65292;&#23588;&#20854;&#22312;&#31454;&#20105;&#39118;&#38505;&#22330;&#26223;&#20013;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25511;&#21046;&#24615;&#12290;&#26412;&#25991;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;OPSurv&#30340;&#32463;&#39564;&#39564;&#35777;&#21644;&#29702;&#35770;&#35777;&#26126;&#65292;&#31361;&#20986;&#20102;&#20854;&#20316;&#20026;&#31454;&#20105;&#39118;&#38505;&#29983;&#23384;&#20998;&#26512;&#30340;&#19968;&#39033;&#37325;&#35201;&#36827;&#23637;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Orthogonal Polynomials Quadrature Algorithm for Survival Analysis (OPSurv), a new method providing time-continuous functional outputs for both single and competing risks scenarios in survival analysis. OPSurv utilizes the initial zero condition of the Cumulative Incidence function and a unique decomposition of probability densities using orthogonal polynomials, allowing it to learn functional approximation coefficients for each risk event and construct Cumulative Incidence Function estimates via Gauss--Legendre quadrature. This approach effectively counters overfitting, particularly in competing risks scenarios, enhancing model expressiveness and control. The paper further details empirical validations and theoretical justifications of OPSurv, highlighting its robust performance as an advancement in survival analysis with competing risks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24378;&#20581;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#23545;&#24378;&#20581;&#24615;&#30340;&#32771;&#34385;&#12290;&#35813;&#35843;&#26597;&#35752;&#35770;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.01928</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24378;&#20581;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Robust Counterfactual Explanations in Machine Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24378;&#20581;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#23545;&#24378;&#20581;&#24615;&#30340;&#32771;&#34385;&#12290;&#35813;&#35843;&#26597;&#35752;&#35770;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#34987;&#35748;&#20026;&#38750;&#24120;&#36866;&#21512;&#20026;&#21463;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#24433;&#21709;&#30340;&#23545;&#35937;&#25552;&#20379;&#31639;&#27861;&#19978;&#30340;&#34917;&#25937;&#25514;&#26045;&#12290;&#23613;&#31649;CEs&#23545;&#21463;&#24433;&#21709;&#30340;&#20010;&#20307;&#26377;&#30410;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#33719;&#21462;CEs&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#20851;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#30001;&#20110;&#32570;&#20047;&#24378;&#20581;&#24615;&#21487;&#33021;&#20250;&#25439;&#23475;CEs&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#37319;&#21462;&#25216;&#26415;&#26469;&#20943;&#36731;&#36825;&#20010;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24378;&#20581;CEs&#36825;&#19968;&#36805;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#24182;&#23545;&#23427;&#20204;&#32771;&#34385;&#30340;&#24378;&#20581;&#24615;&#24418;&#24335;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations (CEs) are advocated as being ideally suited to providing algorithmic recourse for subjects affected by the predictions of machine learning models. While CEs can be beneficial to affected individuals, recent work has exposed severe issues related to the robustness of state-of-the-art methods for obtaining CEs. Since a lack of robustness may compromise the validity of CEs, techniques to mitigate this risk are in order. In this survey, we review works in the rapidly growing area of robust CEs and perform an in-depth analysis of the forms of robustness they consider. We also discuss existing solutions and their limitations, providing a solid foundation for future developments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01922</link><description>&lt;p&gt;
&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Learning from Weak Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#38754;&#20020;&#30528;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#21270;&#24369;&#30417;&#30563;&#30340;&#21508;&#31181;&#22330;&#26223;&#21644;&#30001;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23454;&#38469;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65288;GLWS&#65289;&#12290;GLWS&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#30340;&#20844;&#24335;&#65292;&#28789;&#27963;&#22320;&#36866;&#24212;&#20102;&#21508;&#31181;&#24369;&#30417;&#30563;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#20363;&#30340;&#37096;&#20998;&#26631;&#31614;&#12289;&#32858;&#21512;&#32479;&#35745;&#12289;&#25104;&#23545;&#35266;&#23519;&#21644;&#26080;&#26631;&#27880;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;NFA&#65289;&#20197;&#21450;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#65292;&#26174;&#33879;&#31616;&#21270;&#20102;EM&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#36890;&#24120;&#25152;&#38656;&#30340;&#20108;&#27425;&#25110;&#38454;&#20056;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;&#22240;&#27492;&#65292;&#20174;&#20219;&#24847;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#20102;&#23545;&#23427;&#20204;&#36827;&#34892;NFA&#24314;&#27169;&#12290;GLWS&#19981;&#20165;&#21487;&#20197;&#22686;&#24378;+
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enha
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01920</link><description>&lt;p&gt;
&#23545;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#30340;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Preference Poisoning Attacks on Reward Model Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01920
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20004;&#20004;&#27604;&#36739;&#20013;&#23398;&#20064;&#25928;&#29992;&#25110;&#22870;&#21169;&#27169;&#22411;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#26412;&#36136;&#19978;&#38656;&#35201;&#20174;&#20154;&#20204;&#37027;&#37324;&#25910;&#38598;&#20559;&#22909;&#20449;&#24687;&#65292;&#32780;&#21453;&#39304;&#36890;&#24120;&#26159;&#21311;&#21517;&#25552;&#20379;&#30340;&#12290;&#30001;&#20110;&#20559;&#22909;&#26159;&#20027;&#35266;&#30340;&#65292;&#27809;&#26377;&#21487;&#20197;&#27604;&#36739;&#30340;&#40644;&#37329;&#26631;&#20934;&#65307;&#28982;&#32780;&#65292;&#23545;&#20559;&#22909;&#23398;&#20064;&#30340;&#39640;&#24433;&#21709;&#31995;&#32479;&#30340;&#20381;&#36182;&#24615;&#20026;&#24694;&#24847;&#34892;&#20026;&#32773;&#20542;&#21521;&#20110;&#25197;&#26354;&#20197;&#36798;&#21040;&#20854;&#30446;&#30340;&#32780;&#37319;&#38598;&#30340;&#25968;&#25454;&#21019;&#36896;&#20102;&#24378;&#28872;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#31181;&#23041;&#32961;&#27169;&#22411;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36825;&#31181;&#28431;&#27934;&#30340;&#24615;&#36136;&#21644;&#31243;&#24230;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#65292;&#20197;&#20419;&#36827;&#25110;&#36140;&#20302;&#30446;&#26631;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#29992;&#20110;&#36825;&#20123;&#25915;&#20987;&#30340;&#31639;&#27861;&#26041;&#27861;&#65306;&#22522;&#20110;&#21407;&#21017;&#30340;&#26799;&#24230;&#26694;&#26550;&#21644;&#20960;&#31181;&#21464;&#31181;&#30340;&#25353;&#36317;&#31163;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31867;&#26368;&#20339;&#25915;&#20987;&#22312;&#25104;&#21151;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning utility, or reward, models from pairwise comparisons is a fundamental component in a number of application domains. These approaches inherently entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability systematically by considering a threat model in which an attacker can flip a small subset of preference comparisons with the goal of either promoting or demoting a target outcome. First, we propose two classes of algorithmic approaches for these attacks: a principled gradient-based framework, and several variants of rank-by-distance methods. Next, we demonstrate the efficacy of best attacks in both these classes in successfully achieving malicio
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#20013;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26088;&#22312;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#24182;&#35299;&#37322;&#20854;&#20013;&#30340;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.01909</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Catastrophic Inheritance of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#20013;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26088;&#22312;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#24182;&#35299;&#37322;&#20854;&#20013;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#22768;&#31216;&#20855;&#26377;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#20154;&#20204;&#23545;&#23427;&#20204;&#22312;&#26426;&#22120;&#23398;&#20064;&#20197;&#21450;&#20854;&#20182;&#21508;&#20010;&#23398;&#31185;&#20013;&#30340;&#31070;&#31192;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#28508;&#21147;&#25552;&#20986;&#20102;&#26497;&#22823;&#20851;&#20999;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;LFMs&#20013;&#26681;&#28145;&#33922;&#22266;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#21253;&#25324;&#21463;&#25439;&#12289;&#38271;&#23614;&#12289;&#26377;&#22122;&#38899;&#12289;&#36229;&#20986;&#20998;&#24067;&#31561;&#26679;&#26412;&#12290;&#36825;&#31181;&#32487;&#25215;&#21487;&#33021;&#23545;&#19979;&#28216;&#24212;&#29992;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#65292;&#22914;&#20559;&#35265;&#12289;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12289;&#24615;&#33021;&#19979;&#38477;&#12289;&#23433;&#20840;&#28431;&#27934;&#12289;&#38544;&#31169;&#27844;&#38706;&#21644;&#20215;&#20540;&#35823;&#24046;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#38382;&#39064;&#32972;&#21518;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26469;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#21253;&#25324;&#26469;&#33258;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#36866;&#24212;&#30340;&#32487;&#25215;&#20869;&#23481;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inher
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#21644;&#25512;&#29702;&#22312;NeSy&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#26631;&#27880;&#21644;&#25163;&#21160;&#24037;&#31243;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.01889</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01889
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#21644;&#25512;&#29702;&#22312;NeSy&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#26631;&#27880;&#21644;&#25163;&#21160;&#24037;&#31243;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#26377;&#26395;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#37096;&#32626;&#65292;&#22240;&#20026;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#25216;&#26415;&#25552;&#20379;&#20102;&#27491;&#24335;&#30340;&#34892;&#20026;&#20445;&#35777;&#12290;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#26377;&#25928;&#22320;&#25972;&#21512;&#31070;&#32463;&#21644;&#31526;&#21495;&#35745;&#31639;&#65292;&#20197;&#20415;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#29616;&#26377;&#30340;&#39034;&#24207;&#35757;&#32451;&#31070;&#32463;&#21644;&#31526;&#21495;&#32452;&#20214;&#30340;&#27969;&#27700;&#32447;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#65292;&#32780;&#31471;&#21040;&#31471;&#26041;&#27861;&#22312;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#30340;&#32452;&#21512;&#29190;&#28856;&#26041;&#38754;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#38544;&#24615;&#30693;&#35782;&#26469;&#22686;&#24378;NeSy&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#26631;&#27880;&#21644;&#20154;&#24037;&#24037;&#31243;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;NeSyGPT&#65292;&#23427;&#36890;&#36807;&#24494;&#35843;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26469;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#31526;&#21495;&#29305;&#24449;&#65292;&#28982;&#21518;&#23398;&#20064;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#30340;&#31572;&#26696;&#38598;&#31243;&#24207;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#35780;&#20272;&#34920;&#26126;&#65292;NeSyGPT&#20855;&#26377;... (&#21097;&#20313;&#37096;&#20998;&#35831;&#33258;&#34892;&#32763;&#35793;)
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic AI (NeSy) holds promise to ensure the safe deployment of AI systems, as interpretable symbolic techniques provide formal behaviour guarantees. The challenge is how to effectively integrate neural and symbolic computation, to enable learning and reasoning from raw data. Existing pipelines that train the neural and symbolic components sequentially require extensive labelling, whereas end-to-end approaches are limited in terms of scalability, due to the combinatorial explosion in the symbol grounding problem. In this paper, we leverage the implicit knowledge within foundation models to enhance the performance in NeSy tasks, whilst reducing the amount of data labelling and manual engineering. We introduce a new architecture, called NeSyGPT, which fine-tunes a vision-language foundation model to extract symbolic features from raw data, before learning a highly expressive answer set program to solve a downstream task. Our comprehensive evaluation demonstrates that NeSyGPT has 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;IRLEED&#65292;&#23427;&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#26469;&#35299;&#20915;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#21644;&#24322;&#36136;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;IRLEED&#36890;&#36807;&#32467;&#21512;&#28436;&#31034;&#32773;&#27425;&#20248;&#24615;&#30340;&#26222;&#36866;&#27169;&#22411;&#21644;&#26368;&#22823;&#29109;IRL&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#20174;&#22810;&#26679;&#30340;&#27425;&#20248;&#28436;&#31034;&#20013;&#24471;&#20986;&#26368;&#20339;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01886</link><description>&lt;p&gt;
&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning by Estimating Expertise of Demonstrators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;IRLEED&#65292;&#23427;&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#26469;&#35299;&#20915;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#21644;&#24322;&#36136;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;IRLEED&#36890;&#36807;&#32467;&#21512;&#28436;&#31034;&#32773;&#27425;&#20248;&#24615;&#30340;&#26222;&#36866;&#27169;&#22411;&#21644;&#26368;&#22823;&#29109;IRL&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#20174;&#22810;&#26679;&#30340;&#27425;&#20248;&#28436;&#31034;&#20013;&#24471;&#20986;&#26368;&#20339;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#21033;&#29992;&#27425;&#20248;&#21644;&#24322;&#36136;&#30340;&#28436;&#31034;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24615;&#36136;&#21508;&#19981;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#23558;&#36825;&#20123;&#25968;&#25454;&#38598;&#35270;&#20026;&#21516;&#36136;&#30340;&#65292;&#20174;&#32780;&#32487;&#25215;&#20102;&#27425;&#20248;&#28436;&#31034;&#30340;&#32570;&#38519;&#12290;&#20808;&#21069;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#22914;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#23376;&#38598;&#12289;&#32622;&#20449;&#24230;&#25490;&#21517;&#25110;&#26126;&#30830;&#30340;&#29615;&#22659;&#30693;&#35782;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IRLEED&#65288;&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#21069;&#23545;&#28436;&#31034;&#32773;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#20102;&#35299;&#12290;IRLEED&#36890;&#36807;&#23558;&#28436;&#31034;&#32773;&#27425;&#20248;&#24615;&#30340;&#26222;&#36866;&#27169;&#22411;&#19982;&#26368;&#22823;&#29109;IRL&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#26469;&#22788;&#29702;&#22870;&#21169;&#20559;&#24046;&#21644;&#34892;&#21160;&#26041;&#24046;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20174;&#22810;&#26679;&#30340;&#27425;&#20248;&#28436;&#31034;&#20013;&#24471;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Imitation Learning (IL), utilizing suboptimal and heterogeneous demonstrations presents a substantial challenge due to the varied nature of real-world data. However, standard IL algorithms consider these datasets as homogeneous, thereby inheriting the deficiencies of suboptimal demonstrators. Previous approaches to this issue typically rely on impractical assumptions like high-quality data subsets, confidence rankings, or explicit environmental knowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by Estimating Expertise of Demonstrators, a novel framework that overcomes these hurdles without prior knowledge of demonstrator expertise. IRLEED enhances existing Inverse Reinforcement Learning (IRL) algorithms by combining a general model for demonstrator suboptimality to address reward bias and action variance, with a Maximum Entropy IRL framework to efficiently derive the optimal policy from diverse, suboptimal demonstrations. Experiments in both online and offline I
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;AgentHPO&#25216;&#26415;&#36890;&#36807;&#33258;&#21160;&#21270;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#22823;&#22823;&#20943;&#23569;&#20102;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2402.01881</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Agent for Hyper-Parameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01881
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;AgentHPO&#25216;&#26415;&#36890;&#36807;&#33258;&#21160;&#21270;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#22823;&#22823;&#20943;&#23569;&#20102;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12289;&#22823;&#37327;&#23454;&#39564;&#20197;&#21450;&#39640;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#12290;&#23613;&#31649;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35797;&#39564;&#25928;&#29575;&#12289;&#35774;&#32622;&#22797;&#26434;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#33258;&#21160;&#21270;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#31216;&#20026;AgentHPO&#65288;LLM Agent-based Hyperparameter Optimization&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AgentHPO&#33258;&#20027;&#22788;&#29702;&#20219;&#21153;&#20449;&#24687;&#65292;&#26681;&#25454;&#21382;&#21490;&#35797;&#39564;&#23545;&#29305;&#23450;&#36229;&#21442;&#25968;&#65288;HPs&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;AutoML&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#20248;&#21270;&#36807;&#31243;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#24182;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted o
&lt;/p&gt;</description></item><item><title>&#31227;&#21160;&#35797;&#34915;&#38388;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#35774;&#22791;&#20869;&#25193;&#25955;&#27169;&#22411;&#30340;&#34394;&#25311;&#35797;&#31359;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39640;&#36136;&#37327;&#26381;&#35013;&#25918;&#32622;&#21644;&#27169;&#22411;&#21387;&#32553;&#31561;&#25216;&#26415;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#29992;&#25143;&#23450;&#21046;&#12290;&#23427;&#20026;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#20225;&#19994;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26381;&#21153;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#34394;&#25311;&#35797;&#31359;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.01877</link><description>&lt;p&gt;
&#31227;&#21160;&#35797;&#34915;&#38388;&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#22791;&#20869;&#34394;&#25311;&#35797;&#31359;
&lt;/p&gt;
&lt;p&gt;
Mobile Fitting Room: On-device Virtual Try-on via Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01877
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#35797;&#34915;&#38388;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#35774;&#22791;&#20869;&#25193;&#25955;&#27169;&#22411;&#30340;&#34394;&#25311;&#35797;&#31359;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39640;&#36136;&#37327;&#26381;&#35013;&#25918;&#32622;&#21644;&#27169;&#22411;&#21387;&#32553;&#31561;&#25216;&#26415;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#29992;&#25143;&#23450;&#21046;&#12290;&#23427;&#20026;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#20225;&#19994;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26381;&#21153;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#34394;&#25311;&#35797;&#31359;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#65292;&#38656;&#35201;&#20132;&#20114;&#24335;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#26469;&#36827;&#34892;&#34394;&#25311;&#35797;&#31359;&#34915;&#26381;&#12290;&#20256;&#32479;&#30340;&#35797;&#31359;&#26041;&#27861;&#22312;&#36866;&#24212;&#19981;&#21516;&#30340;&#32972;&#26223;&#12289;&#23039;&#21183;&#21644;&#20027;&#20307;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#34429;&#28982;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#32780;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#20294;&#31227;&#21160;&#30028;&#38754;&#20132;&#20184;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#32500;&#24230;&#21644;&#38544;&#31169;&#38382;&#39064;&#22823;&#37096;&#20998;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31227;&#21160;&#35797;&#34915;&#38388;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#35774;&#22791;&#20869;&#25193;&#25955;&#30340;&#34394;&#25311;&#35797;&#31359;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#22914;&#39640;&#36136;&#37327;&#26381;&#35013;&#25918;&#32622;&#21644;&#31227;&#21160;&#35774;&#22791;&#30340;&#27169;&#22411;&#21387;&#32553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#27969;&#31243;&#21644;&#30028;&#38754;&#35774;&#35745;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#21644;&#29992;&#25143;&#23450;&#21046;&#12290;&#19968;&#20010;&#20351;&#29992;&#22330;&#26223;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#22914;&#20309;&#20026;&#39038;&#23458;&#25552;&#20379;&#26080;&#32541;&#12289;&#20114;&#21160;&#30340;&#34394;&#25311;&#35797;&#31359;&#20307;&#39564;&#65292;&#24182;&#20026;&#26102;&#23578;&#30005;&#23376;&#21830;&#21153;&#20225;&#19994;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing digital landscape of fashion e-commerce calls for interactive and user-friendly interfaces for virtually trying on clothes. Traditional try-on methods grapple with challenges in adapting to diverse backgrounds, poses, and subjects. While newer methods, utilizing the recent advances of diffusion models, have achieved higher-quality image generation, the human-centered dimensions of mobile interface delivery and privacy concerns remain largely unexplored. We present Mobile Fitting Room, the first on-device diffusion-based virtual try-on system. To address multiple inter-related technical challenges such as high-quality garment placement and model compression for mobile devices, we present a novel technical pipeline and an interface design that enables privacy preservation and user customization. A usage scenario highlights how our tool can provide a seamless, interactive virtual try-on experience for customers and provide a valuable service for fashion e-commerce businesses.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#22238;&#39038;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20197;&#23457;&#35270;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.01874</link><description>&lt;p&gt;
RL/LLM&#20998;&#31867;&#26641;&#65306;&#22238;&#39038;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01874
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#22238;&#39038;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20197;&#23457;&#35270;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#31867;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#20998;&#31867;&#26041;&#27861;&#22522;&#20110;&#36825;&#20004;&#31181;&#27169;&#22411;&#31867;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;&#31532;&#19968;&#31867;&#26159;RL4LLM&#65292;&#21253;&#25324;&#21033;&#29992;RL&#25913;&#36827;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#20851;&#20219;&#21153;&#19978;LLM&#24615;&#33021;&#30340;&#30740;&#31350;&#12290;L4LLM&#20998;&#20026;&#20004;&#20010;&#23376;&#31867;&#65292;&#21462;&#20915;&#20110;RL&#26159;&#30452;&#25509;&#24494;&#35843;&#29616;&#26377;LLM&#36824;&#26159;&#25913;&#36827;LLM&#30340;&#25552;&#31034;&#12290;&#22312;&#31532;&#20108;&#31867;LLM4RL&#20013;&#65292;LLM&#36741;&#21161;&#35757;&#32451;&#19968;&#20010;&#19982;&#33258;&#28982;&#35821;&#35328;&#26080;&#20851;&#30340;RL&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26681;&#25454;LLM&#36741;&#21161;&#25110;&#26367;&#20195;RL&#35757;&#32451;&#26694;&#26550;&#30340;&#32452;&#20214;&#65288;&#22870;&#21169;&#22609;&#36896;&#12289;&#30446;&#26631;&#29983;&#25104;&#21644;&#31574;&#30053;&#20989;&#25968;&#65289;&#23545;LLM4RL&#36827;&#34892;&#20102;&#32454;&#20998;&#12290;&#26368;&#21518;&#65292;&#22312;&#31532;&#19977;&#31867;RL+LLM&#20013;&#65292;&#19968;&#20010;LLM&#21644;&#19968;&#20010;RL&#20195;&#29702;&#34987;&#23884;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedde
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#35843;&#26597;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#19987;&#19994;&#21672;&#35810;&#30340;&#25919;&#31574;&#32771;&#34385;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#36890;&#36807;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#20174;20&#21517;&#27861;&#24459;&#19987;&#23478;&#30340;&#35752;&#35770;&#20013;&#25552;&#21462;&#20102;&#22235;&#20010;&#24433;&#21709;&#22240;&#32032;&#65306;&#29992;&#25143;&#23646;&#24615;&#12289;&#26597;&#35810;&#29305;&#24449;&#12289;AI&#33021;&#21147;&#21644;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01864</link><description>&lt;p&gt;
(A)&#25105;&#19981;&#26159;&#24459;&#24072;, &#20294;&#26159;...: &#21521;&#24459;&#24072;&#19987;&#23478;&#21046;&#23450;&#36127;&#36131;&#20219;&#30340;&#27861;&#24459;&#21672;&#35810;&#30340;LLM&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01864
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#35843;&#26597;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#19987;&#19994;&#21672;&#35810;&#30340;&#25919;&#31574;&#32771;&#34385;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#36890;&#36807;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#20174;20&#21517;&#27861;&#24459;&#19987;&#23478;&#30340;&#35752;&#35770;&#20013;&#25552;&#21462;&#20102;&#22235;&#20010;&#24433;&#21709;&#22240;&#32032;&#65306;&#29992;&#25143;&#23646;&#24615;&#12289;&#26597;&#35810;&#29305;&#24449;&#12289;AI&#33021;&#21147;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20316;&#20026;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#36805;&#36895;&#25193;&#25955;, &#24102;&#26469;&#20102;&#25193;&#22823;&#20844;&#20247;&#33719;&#24471;&#27861;&#24459;&#12289;&#21307;&#23398;&#21644;&#37329;&#34701;&#19987;&#19994;&#25351;&#23548;&#30340;&#24076;&#26395;, &#21516;&#26102;&#24341;&#21457;&#20102;&#20844;&#20247;&#23545;LLM&#22312;&#37325;&#22823;&#20107;&#20214;&#20013;&#30340;&#20381;&#36182;&#30340;&#25285;&#24551;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#29468;&#27979;&#20102;&#39640;&#23618;&#27425;&#30340;&#20262;&#29702;&#32771;&#34385;, &#20294;&#32570;&#20047;&#20855;&#20307;&#30340;&#26631;&#20934;&#26469;&#30830;&#23450;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#24212;&#35813;&#25110;&#19981;&#24212;&#35813;&#25552;&#20379;&#19987;&#19994;&#24110;&#21161;&#12290;&#36890;&#36807;&#30740;&#31350;&#27861;&#24459;&#39046;&#22495;, &#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#30340;&#19987;&#23478;&#20998;&#26512;, &#20197;&#21457;&#29616;&#20851;&#20110;&#20351;&#29992;LLM&#36827;&#34892;&#19987;&#19994;&#21672;&#35810;&#30340;&#25919;&#31574;&#32771;&#34385;&#30340;&#32454;&#24494;&#24046;&#21035;, &#24182;&#37319;&#29992;&#26696;&#20363;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19982;20&#21517;&#27861;&#24459;&#19987;&#23478;&#21484;&#24320;&#20102;&#30740;&#35752;&#20250;, &#24182;&#20174;&#26679;&#26412;&#29992;&#25143;&#26597;&#35810;("&#26696;&#20363;")&#20013;&#25552;&#21462;&#20986;&#36866;&#24403;&#30340;AI&#36741;&#21161;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#23558;&#19987;&#19994;&#32500;&#24230;&#20998;&#20026;: (1)&#29992;&#25143;&#23646;&#24615;, (2)&#26597;&#35810;&#29305;&#24449;, (3)AI&#33021;&#21147;, &#21644; (4)&#24433;&#21709;&#12290;&#38500;&#20102;&#24050;&#30693;&#30340;&#38382;&#39064;, &#22914;&#24187;&#35273;, &#19987;&#23478;&#20204;&#36824;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of large language models (LLMs) as general purpose chatbots available to the public raises hopes around expanding access to professional guidance in law, medicine, and finance, while triggering concerns about public reliance on LLMs for high-stakes circumstances. Prior research has speculated on high-level ethical considerations but lacks concrete criteria determining when and why LLM chatbots should or should not provide professional assistance. Through examining the legal domain, we contribute a structured expert analysis to uncover nuanced policy considerations around using LLMs for professional advice, using methods inspired by case-based reasoning. We convened workshops with 20 legal experts and elicited dimensions on appropriate AI assistance for sample user queries (``cases''). We categorized our expert dimensions into: (1) user attributes, (2) query characteristics, (3) AI capabilities, and (4) impacts. Beyond known issues like hallucinations, experts re
&lt;/p&gt;</description></item><item><title>DFML&#26159;&#19968;&#20010;&#26080;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01863</link><description>&lt;p&gt;
DFML&#65306;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DFML: Decentralized Federated Mutual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01863
&lt;/p&gt;
&lt;p&gt;
DFML&#26159;&#19968;&#20010;&#26080;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#35774;&#22791;&#39046;&#22495;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#21644;&#23481;&#26131;&#21463;&#21040;&#21333;&#28857;&#25925;&#38556;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#35774;&#22791;&#22266;&#26377;&#22320;&#34920;&#29616;&#20986;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#29616;&#26377;&#24037;&#20316;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#27492;&#24322;&#36136;&#24615;&#19988;&#19981;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#25110;&#20551;&#23450;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#30340;&#20998;&#25955;&#24335;FL&#65288;DFL&#65289;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#65288;DFML&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#26080;&#26381;&#21153;&#22120;&#30340;&#65292;&#25903;&#25345;&#38750;&#38480;&#21046;&#24615;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20381;&#36182;&#20844;&#20849;&#25968;&#25454;&#12290;DFML&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#24182;&#24490;&#29615;&#25913;&#21464;&#30417;&#30563;&#21644;&#25552;&#21462;&#20449;&#21495;&#30340;&#25968;&#37327;&#26469;&#26377;&#25928;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DFML&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#20840;&#23616;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#26222;&#36941;&#23384;&#22312;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of real-world devices, centralized servers in Federated Learning (FL) present challenges including communication bottlenecks and susceptibility to a single point of failure. Additionally, contemporary devices inherently exhibit model and data heterogeneity. Existing work lacks a Decentralized FL (DFL) framework capable of accommodating such heterogeneity without imposing architectural restrictions or assuming the availability of public data. To address these issues, we propose a Decentralized Federated Mutual Learning (DFML) framework that is serverless, supports nonrestrictive heterogeneous models, and avoids reliance on public data. DFML effectively handles model and data heterogeneity through mutual learning, which distills knowledge between clients, and cyclically varying the amount of supervision and distillation signals. Extensive experimental results demonstrate consistent effectiveness of DFML in both convergence speed and global accuracy, outperforming prevalent b
&lt;/p&gt;</description></item><item><title>FedPFT &#20351;&#29992;&#21442;&#25968;&#29305;&#24449;&#36801;&#31227;&#25552;&#39640;&#20102;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#32622;&#20013;&#26174;&#31034;&#20986;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01862</link><description>&lt;p&gt;
&#21442;&#25968;&#29305;&#24449;&#36801;&#31227;&#65306;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parametric Feature Transfer: One-shot Federated Learning with Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01862
&lt;/p&gt;
&lt;p&gt;
FedPFT &#20351;&#29992;&#21442;&#25968;&#29305;&#24449;&#36801;&#31227;&#25552;&#39640;&#20102;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#32622;&#20013;&#26174;&#31034;&#20986;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#22312;&#19968;&#36718;&#36890;&#20449;&#20013;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#30340;&#21516;&#26102;&#25439;&#22833;&#20102;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FedPFT&#65288;&#24102;&#21442;&#25968;&#29305;&#24449;&#36801;&#31227;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#26469;&#25552;&#39640;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20174;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#27599;&#20010;&#23458;&#25143;&#31471;&#21442;&#25968;&#27169;&#22411;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65289;&#30340;&#29305;&#24449;&#36827;&#34892;&#36801;&#31227;&#12290;&#38543;&#21518;&#65292;&#27599;&#20010;&#21442;&#25968;&#27169;&#22411;&#34987;&#29992;&#26469;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#22836;&#30340;&#21512;&#25104;&#29305;&#24449;&#12290;&#22312;&#20843;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedPFT&#22312;&#38598;&#20013;&#21644;&#20998;&#25955;&#30340;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#20197;&#21450;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#21644;&#20219;&#21153;&#36716;&#31227;&#31561;&#22810;&#26679;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#32622;&#20013;&#65292;&#22686;&#24378;&#20102;&#36890;&#20449;-&#20934;&#30830;&#24615;&#30340;&#36793;&#30028;&#65292;&#25913;&#36827;&#20102;&#26368;&#39640;&#36798;20.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
In one-shot federated learning (FL), clients collaboratively train a global model in a single round of communication. Existing approaches for one-shot FL enhance communication efficiency at the expense of diminished accuracy. This paper introduces FedPFT (Federated Learning with Parametric Feature Transfer), a methodology that harnesses the transferability of foundation models to enhance both accuracy and communication efficiency in one-shot FL. The approach involves transferring per-client parametric models (specifically, Gaussian mixtures) of features extracted from foundation models. Subsequently, each parametric model is employed to generate synthetic features for training a classifier head. Experimental results on eight datasets demonstrate that FedPFT enhances the communication-accuracy frontier in both centralized and decentralized FL scenarios, as well as across diverse data-heterogeneity settings such as covariate shift and task shift, with improvements of up to 20.6%. Additio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01858</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explaining latent representations of generative models with large multimodal models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#29983;&#25104;&#28508;&#22312;&#22240;&#32032;&#34920;&#31034;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#23427;&#21487;&#20197;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37327;&#21270;&#35780;&#20272;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#20102;&#35299;&#37322;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23450;&#24615;&#22320;&#21487;&#35270;&#21270;&#20102;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#23545;&#35299;&#37322;&#30340;&#35299;&#32544;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
&lt;/p&gt;</description></item><item><title>COMET&#26159;&#19968;&#31181;&#21033;&#29992;&#22686;&#37327;&#22270;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;transformer&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25552;&#20132;&#28040;&#24687;&#65292;&#24182;&#24341;&#20837;&#21487;&#23450;&#21046;&#30340;&#36136;&#37327;&#20445;&#35777;&#27169;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;COMET&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#24182;&#19982;&#27969;&#34892;&#30340;GPT&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01841</link><description>&lt;p&gt;
COMET: &#20351;&#29992;&#22686;&#37327;&#22270;&#19978;&#19979;&#25991;&#34920;&#31034;&#29983;&#25104;&#25552;&#20132;&#28040;&#24687;
&lt;/p&gt;
&lt;p&gt;
COMET: Generating Commit Messages using Delta Graph Context Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01841
&lt;/p&gt;
&lt;p&gt;
COMET&#26159;&#19968;&#31181;&#21033;&#29992;&#22686;&#37327;&#22270;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;transformer&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25552;&#20132;&#28040;&#24687;&#65292;&#24182;&#24341;&#20837;&#21487;&#23450;&#21046;&#30340;&#36136;&#37327;&#20445;&#35777;&#27169;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;COMET&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#24182;&#19982;&#27969;&#34892;&#30340;GPT&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20132;&#28040;&#24687;&#35299;&#37322;&#20102;&#25552;&#20132;&#20013;&#30340;&#20195;&#30721;&#26356;&#25913;&#65292;&#24182;&#20419;&#36827;&#24320;&#21457;&#32773;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#25552;&#20132;&#28040;&#24687;&#29983;&#25104;&#26041;&#27861;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#25429;&#25417;&#20195;&#30721;&#26356;&#25913;&#30340;&#19978;&#19979;&#25991;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;COMET&#65288;Context-Aware Commit Message Generation&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#25429;&#25417;&#20195;&#30721;&#26356;&#25913;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25552;&#20132;&#28040;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25105;&#20204;&#24320;&#21457;&#30340;&#22686;&#37327;&#22270;&#26377;&#25928;&#22320;&#34920;&#31034;&#20195;&#30721;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23450;&#21046;&#30340;&#36136;&#37327;&#20445;&#35777;&#27169;&#22359;&#26469;&#35782;&#21035;&#26368;&#20339;&#30340;&#28040;&#24687;&#65292;&#20943;&#23569;&#20102;&#25552;&#20132;&#28040;&#24687;&#30340;&#20027;&#35266;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;COMET&#22312;bleu-norm&#21644;meteor&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#22312;rogue-l&#25351;&#26631;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#27969;&#34892;&#30340;gpt-3.5-turbo&#27169;&#22411;&#20197;&#21450;&#26368;&#24378;&#22823;&#30340;GPT&#27169;&#22411;gpt-4-turbo&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commit messages explain code changes in a commit and facilitate collaboration among developers. Several commit message generation approaches have been proposed; however, they exhibit limited success in capturing the context of code changes. We propose Comet (Context-Aware Commit Message Generation), a novel approach that captures context of code changes using a graph-based representation and leverages a transformer-based model to generate high-quality commit messages. Our proposed method utilizes delta graph that we developed to effectively represent code differences. We also introduce a customizable quality assurance module to identify optimal messages, mitigating subjectivity in commit messages. Experiments show that Comet outperforms state-of-the-art techniques in terms of bleu-norm and meteor metrics while being comparable in terms of rogue-l. Additionally, we compare the proposed approach with the popular gpt-3.5-turbo model, along with gpt-4-turbo; the most capable GPT model, ove
&lt;/p&gt;</description></item><item><title>SynthCLIP&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22270;&#29255;&#21644;&#26631;&#39064;&#25968;&#25454;&#38598;&#65292;&#22312;&#24615;&#33021;&#19978;&#21487;&#20197;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.01832</link><description>&lt;p&gt;
SynthCLIP: &#25105;&#20204;&#20934;&#22791;&#22909;&#24320;&#22987;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#35757;&#32451;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01832
&lt;/p&gt;
&lt;p&gt;
SynthCLIP&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22270;&#29255;&#21644;&#26631;&#39064;&#25968;&#25454;&#38598;&#65292;&#22312;&#24615;&#33021;&#19978;&#21487;&#20197;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SynthCLIP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#19982;&#20043;&#21069;&#20381;&#36182;&#30495;&#23454;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#21306;&#21035;&#12290;&#20511;&#21161;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#35268;&#27169;&#30340;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#26631;&#39064;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#65292;SynthCLIP&#23454;&#29616;&#20102;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;SynthCI-30M&#65292;&#19968;&#20010;&#32431;&#31929;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;3000&#19975;&#24352;&#24102;&#26631;&#39064;&#30340;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#24050;&#32463;&#22312;https://github.com/hammoudhasan/SynthCLIP&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SynthCLIP, a novel framework for training CLIP models with entirely synthetic text-image pairs, significantly departing from previous methods relying on real data. Leveraging recent text-to-image (TTI) generative networks and large language models (LLM), we are able to generate synthetic datasets of images and corresponding captions at any scale, with no human intervention. With training at scale, SynthCLIP achieves performance comparable to CLIP models trained on real datasets. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and generated data are released at https://github.com/hammoudhasan/SynthCLIP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01830</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#21516;&#34892;&#35780;&#23457;&#26041;&#27861;&#65306;&#24320;&#25918;&#29615;&#22659;&#19979;LLMs&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#38598;&#20013;&#20110;&#22312;&#19968;&#20123;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#23553;&#38381;&#29615;&#22659;&#21644;&#29305;&#23450;&#39046;&#22495;&#22522;&#20934;&#19978;&#27979;&#35797;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#33258;&#21160;&#34913;&#37327;LLMs&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;LLMs&#22788;&#20110;&#21516;&#19968;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#22238;&#31572;&#26410;&#26631;&#35760;&#30340;&#38382;&#39064;&#24182;&#20114;&#30456;&#35780;&#20272;&#65292;&#27599;&#20010;LLM&#30340;&#21709;&#24212;&#24471;&#20998;&#30001;&#20854;&#20182;&#21311;&#21517;&#30340;LLMs&#20849;&#21516;&#20915;&#23450;&#12290;&#20026;&#20102;&#33719;&#21462;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#26469;&#35843;&#25972;&#26368;&#32456;&#25490;&#24207;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27599;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32972;&#21518;&#30340;&#20851;&#38190;&#20551;&#35774;&#26159;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#27604;&#20302;&#23618;&#27425;&#30340;LLM&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#32780;&#39640;&#23618;&#27425;&#30340;LLM&#20063;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;ReSLM&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#38899;&#26816;&#32034;&#22120;&#33719;&#21462;&#38899;&#39057;&#20013;&#30340;&#25991;&#26412;&#23454;&#20307;&#65292;&#24182;&#23558;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#21152;&#20837;&#21040;&#27169;&#22411;&#20013;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01828</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Retrieval Augmented End-to-End Spoken Dialog Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;ReSLM&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#38899;&#26816;&#32034;&#22120;&#33719;&#21462;&#38899;&#39057;&#20013;&#30340;&#25991;&#26412;&#23454;&#20307;&#65292;&#24182;&#23558;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#21152;&#20837;&#21040;&#27169;&#22411;&#20013;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26368;&#36817;&#24320;&#21457;&#20102;SLM&#65292;&#19968;&#31181;&#34701;&#21512;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32852;&#21512;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;SLM&#24212;&#29992;&#20110;&#35821;&#38899;&#23545;&#35805;&#24212;&#29992;&#65292;&#20854;&#20013;&#23545;&#35805;&#29366;&#24577;&#30452;&#25509;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#32463;&#24120;&#21253;&#21547;&#29305;&#23450;&#39046;&#22495;&#30340;&#23454;&#20307;&#65292;&#20363;&#22914;&#39184;&#39302;&#12289;&#37202;&#24215;&#12289;&#28779;&#36710;&#31449;&#21644;&#22478;&#24066;&#21517;&#31216;&#65292;&#36825;&#20123;&#23454;&#20307;&#24456;&#38590;&#35782;&#21035;&#65292;&#20294;&#23545;&#20110;&#21518;&#32493;&#24212;&#29992;&#38750;&#24120;&#20851;&#38190;&#12290;&#21463;&#21040;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;retrieval-augmented generation&#65289;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;SLM&#65288;ReSLM&#65289;&#65292;&#20811;&#26381;&#20102;&#36825;&#20010;&#24369;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#35821;&#38899;&#26816;&#32034;&#22120;&#65292;&#29992;&#20110;&#26816;&#32034;&#38899;&#39057;&#20013;&#25552;&#21040;&#30340;&#25991;&#26412;&#23454;&#20307;&#12290;&#28982;&#21518;&#65292;&#23558;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#28155;&#21152;&#21040;&#24213;&#23618;&#30340;SLM&#20013;&#65292;&#20197;&#20559;&#32622;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#35821;&#38899;MultiWoz&#20219;&#21153;&#65288;DSTC-11&#25361;&#25112;&#65289;&#19978;&#35780;&#20272;&#20102;ReSLM&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We recently developed SLM, a joint speech and language model, which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to speech dialog applications where the dialog states are inferred directly from the audio signal.   Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to retrieve text entities mentioned in the audio. The retrieved entities are then added as text inputs to the underlying SLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that this retrieval augmentation bo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-35-turbo&#65292;&#20174;PubMed&#30340;25 million&#20010;&#25688;&#35201;&#20013;&#33258;&#21160;&#25552;&#21462;&#20986;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;&#34880;&#21387;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#65292;&#22635;&#34917;&#20102;&#22312;&#32771;&#34385;&#29983;&#29289;&#23398;&#24615;&#21035;&#24046;&#24322;&#30340;&#34880;&#21387;&#27979;&#37327;&#26041;&#24046;&#26041;&#38754;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.01826</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#20013;&#34880;&#21387;&#21464;&#24322;&#30340;&#29983;&#29289;&#23398;&#24615;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-35-turbo&#65292;&#20174;PubMed&#30340;25 million&#20010;&#25688;&#35201;&#20013;&#33258;&#21160;&#25552;&#21462;&#20986;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;&#34880;&#21387;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#65292;&#22635;&#34917;&#20102;&#22312;&#32771;&#34385;&#29983;&#29289;&#23398;&#24615;&#21035;&#24046;&#24322;&#30340;&#34880;&#21387;&#27979;&#37327;&#26041;&#24046;&#26041;&#38754;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#34880;&#21387;&#34987;&#23450;&#20041;&#20026;&#39640;&#20110;&#27491;&#24120;&#30340;&#34880;&#21387;&#65292;&#23427;&#22312;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#26159;&#21508;&#31181;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#37325;&#35201;&#20808;&#20806;&#65292;&#24182;&#19988;&#26174;&#33879; contributed to the elevated mortality rates worldwide&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#34880;&#21387;&#27979;&#37327;&#25216;&#26415;&#21644;&#26631;&#20934;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#32771;&#34385;&#20020;&#24202;&#32467;&#26524;&#12289;&#21512;&#24182;&#30151;&#25110;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#65292;&#20351;&#20854;&#22312;&#35786;&#26029;&#30446;&#30340;&#19978;&#27809;&#26377;&#32467;&#35770;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#21464;&#37327;&#65292;&#22312;&#30740;&#31350;&#34880;&#21387;&#27979;&#37327;&#26041;&#24046;&#26041;&#38754;&#65292;&#26377;&#38480;&#30340;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;GPT-35-turbo&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20174;PubMed&#30340;2500&#19975;&#20010;&#25688;&#35201;&#30340;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#25552;&#21462;&#20102;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;&#34880;&#21387;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#25968;&#20540;&#12290;&#31526;&#21512;&#25105;&#20204;&#39044;&#23450;&#20041;&#30340;&#32435;&#20837;&#26631;&#20934;&#30340;&#25688;&#35201;&#25991;&#31456;&#26377;993&#31687;&#65288;&#21363;&#20855;&#26377;&#19982;&#34880;&#21387;&#12289;&#34880;&#21387;&#21333;&#20301;&#65288;&#22914;mmHg&#65289;&#21644;&#25552;&#21450;&#30456;&#20851;&#24615;&#30340;&#21442;&#32771;&#25991;&#29486;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypertension, defined as blood pressure (BP) that is above normal, holds paramount significance in the realm of public health, as it serves as a critical precursor to various cardiovascular diseases (CVDs) and significantly contributes to elevated mortality rates worldwide. However, many existing BP measurement technologies and standards might be biased because they do not consider clinical outcomes, comorbidities, or demographic factors, making them inconclusive for diagnostic purposes. There is limited data-driven research focused on studying the variance in BP measurements across these variables. In this work, we employed GPT-35-turbo, a large language model (LLM), to automatically extract the mean and standard deviation values of BP for both males and females from a dataset comprising 25 million abstracts sourced from PubMed. 993 article abstracts met our predefined inclusion criteria (i.e., presence of references to blood pressure, units of blood pressure such as mmHg, and mention
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#20855;&#26377;&#33258;&#30456;&#20284;&#24615;&#21644;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#20174;&#27573;&#33853;&#21040;&#25972;&#20010;&#25991;&#26723;&#37117;&#23384;&#22312;&#30456;&#20284;&#30340;&#27169;&#24335;&#21644;&#20381;&#36182;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26469;&#29702;&#35299;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#12290;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22256;&#24785;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#21644;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.01825</link><description>&lt;p&gt;
&#20998;&#24418;&#27169;&#24335;&#21487;&#33021;&#25581;&#31034;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20013;&#30340;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Fractal Patterns May Unravel the Intelligence in Next-Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01825
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#20855;&#26377;&#33258;&#30456;&#20284;&#24615;&#21644;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#20174;&#27573;&#33853;&#21040;&#25972;&#20010;&#25991;&#26723;&#37117;&#23384;&#22312;&#30456;&#20284;&#30340;&#27169;&#24335;&#21644;&#20381;&#36182;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26469;&#29702;&#35299;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#12290;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22256;&#24785;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#21644;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#31934;&#30830;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#37327;&#21270;&#21487;&#33021;&#20043;&#21069;&#21482;&#26377;&#24576;&#30097;&#20294;&#23578;&#26410;&#27491;&#24335;&#35777;&#26126;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#35328;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#33258;&#30456;&#20284;&#24615;&#65292;&#23637;&#31034;&#20986;&#21508;&#20010;&#23618;&#32423;&#19978;&#30340;&#22797;&#26434;&#24615;&#65292;&#27809;&#26377;&#29305;&#23450;&#30340;&#29305;&#24449;&#19978;&#19979;&#25991;&#38271;&#24230;&#65307;&#65288;2&#65289;&#38271;&#31243;&#30456;&#20851;&#24615;&#65288;LRD&#65289;&#65292;&#20855;&#26377;&#22823;&#32422;H=0.70&#30340;Hurst&#21442;&#25968;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#20013;&#30340;&#30701;&#26399;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#22914;&#27573;&#33853;&#20013;&#30340;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#21453;&#26144;&#20102;&#26356;&#22823;&#33539;&#22260;&#30340;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#22914;&#25972;&#20010;&#25991;&#26723;&#12290;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#22914;&#20309;&#23548;&#33268;&#23545;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#21333;&#35789;&#21644;&#20174;&#21477;&#21040;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#22270;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#27599;&#23383;&#33410;&#27604;&#29305;&#65288;BPB&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#21457;&#29616;&#33021;&#20026;&#35821;&#35328;&#21644;&#26426;&#21046;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.70. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. We also demonstrate that fractal parameters improve upon perplexity-based bits-per-byte (BPB) in predicting downstream performance. We hope these findings offer a fresh perspective on language and the mechani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#20513;&#23548;&#37319;&#29992;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22810;&#23398;&#31185;&#22242;&#38431;&#21512;&#20316;&#26469;&#30830;&#23450;&#31934;&#30830;&#30340;&#25216;&#26415;&#35201;&#27714;&#65292;&#20197;&#20943;&#36731;LLM&#30340;&#39118;&#38505;&#65292;&#24182;&#20840;&#38754;&#32771;&#34385;&#19981;&#21516;LLM&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.01822</link><description>&lt;p&gt;
&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#38450;&#25252;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Building Guardrails for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#20513;&#23548;&#37319;&#29992;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22810;&#23398;&#31185;&#22242;&#38431;&#21512;&#20316;&#26469;&#30830;&#23450;&#31934;&#30830;&#30340;&#25216;&#26415;&#35201;&#27714;&#65292;&#20197;&#20943;&#36731;LLM&#30340;&#39118;&#38505;&#65292;&#24182;&#20840;&#38754;&#32771;&#34385;&#19981;&#21516;LLM&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#35782;&#21035;&#21644;&#20943;&#36731;&#23427;&#20204;&#30340;&#39118;&#38505;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#39118;&#38505;&#23545;&#20154;&#31867;&#29992;&#25143;&#21644;&#31038;&#20250;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#26102;&#12290;&#38450;&#25252;&#25514;&#26045;&#65292;&#21363;&#36807;&#28388;LLM&#30340;&#36755;&#20837;&#25110;&#36755;&#20986;&#65292;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26680;&#24515;&#30340;&#23433;&#20840;&#25216;&#26415;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#24403;&#21069;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#65288;Llama Guard&#65292;Nvidia NeMo&#65292;Guardrails AI&#65289;&#65292;&#35752;&#35770;&#20102;&#26500;&#24314;&#26356;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#30340;&#25361;&#25112;&#21644;&#36335;&#24452;&#12290;&#22522;&#20110;&#21069;&#26399;&#30740;&#31350;&#30340;&#26377;&#21147;&#35777;&#25454;&#65292;&#25105;&#20204;&#20513;&#23548;&#37319;&#29992;&#31995;&#32479;&#21270;&#26041;&#27861;&#26500;&#24314;LLM&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#20840;&#38754;&#32771;&#34385;&#19981;&#21516;LLM&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#19982;&#22810;&#23398;&#31185;&#22242;&#38431;&#30340;&#21512;&#20316;&#65292;&#37319;&#29992;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#26469;&#30830;&#23450;&#31934;&#30830;&#30340;&#25216;&#26415;&#35201;&#27714;&#65292;&#25506;&#32034;&#38754;&#21521;&#38656;&#27714;&#22797;&#26434;&#24615;&#30340;&#20808;&#36827;&#31070;&#32463;&#31526;&#21495;&#23454;&#29616;&#65292;&#24182;&#24320;&#23637;&#39564;&#35777;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ERMI&#27169;&#22411;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.01821</link><description>&lt;p&gt;
&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#35299;&#37322;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Ecologically rational meta-learned inference explains human category learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ERMI&#27169;&#22411;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#24577;&#21512;&#29702;&#24615;&#26159;&#25351;&#20154;&#31867;&#20316;&#20026;&#36866;&#24212;&#29615;&#22659;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#27979;&#35797;&#36825;&#20010;&#29702;&#35770;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23450;&#20041;&#21738;&#20123;&#20219;&#21153;&#22312;&#29983;&#24577;&#19978;&#26159;&#26377;&#25928;&#30340;&#20197;&#21450;&#20026;&#36825;&#20123;&#20219;&#21153;&#24314;&#31435;&#21512;&#29702;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#31867;&#21035;&#23398;&#20064;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#21512;&#29702;&#24615;&#20027;&#20307;&#26469;&#35299;&#20915;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31867;&#27169;&#22411;&#65292;&#31216;&#20026;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#12290;ERMI&#22312;&#20004;&#20010;&#19981;&#21516;&#23454;&#39564;&#20013;&#20197;&#23450;&#37327;&#26041;&#24335;&#27604;&#20854;&#20182;&#19971;&#20010;&#35748;&#30693;&#27169;&#22411;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#23450;&#24615;&#19978;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#21305;&#37197;&#65306;&#65288;1&#65289;&#23427;&#21457;&#29616;&#20102;&#19982;&#20154;&#31867;&#21457;&#29616;&#22256;&#38590;&#30340;&#30456;&#21516;&#20219;&#21153;&#65292;&#65288;2&#65289;&#23427;&#21464;&#24471;&#26356;&#20381;&#36182;&#20110;&#22522;&#20110;&#26679;&#26412;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy 
&lt;/p&gt;</description></item><item><title>LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01817</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#35268;&#21010;&#65292;&#20294;&#21487;&#20197;&#22312;LLM-Modulo&#26694;&#26550;&#20013;&#24110;&#21161;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01817
&lt;/p&gt;
&lt;p&gt;
LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#23384;&#22312;&#24456;&#22823;&#30340;&#22256;&#24785;&#12290;&#19968;&#26041;&#38754;&#26377;&#20154;&#36807;&#20110;&#20048;&#35266;&#22320;&#22768;&#31216;&#21482;&#38656;&#27491;&#30830;&#25552;&#31034;&#25110;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;LLMs&#23601;&#33021;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20063;&#26377;&#20154;&#36807;&#20110;&#24754;&#35266;&#22320;&#35748;&#20026;LLMs&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#20165;&#33021;&#20316;&#20026;&#38382;&#39064;&#35268;&#33539;&#30340;&#31616;&#21333;&#32763;&#35793;&#22120;&#65292;&#24182;&#23558;&#38382;&#39064;&#20132;&#32473;&#22806;&#37096;&#31526;&#21495;&#27714;&#35299;&#22120;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#31181;&#26497;&#31471;&#35266;&#28857;&#37117;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#33258;&#22238;&#24402;LLMs&#26412;&#36523;&#19981;&#33021;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65288;&#27605;&#31455;&#36825;&#26159;&#19968;&#31181;&#25512;&#29702;&#24418;&#24335;&#65289;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#35823;&#35299;&#21407;&#22240;&#36827;&#34892;&#20102;&#19968;&#20123;&#38416;&#36848;&#12290;&#25105;&#20204;&#36824;&#23558;&#36777;&#31216;LLMs&#24212;&#35813;&#34987;&#35270;&#20026;&#20855;&#26377;&#26356;&#26377;&#24847;&#20041;&#30340;&#35282;&#33394;&#30340;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#65292;&#33021;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21457;&#25381;&#26356;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end forma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#34701;&#20837;&#21040;&#32039;&#20945;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;AI&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#22522;&#20934;&#65292;&#31361;&#20986;&#20102;&#32039;&#20945;&#27169;&#22411;&#22312;&#22797;&#21046;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01812</link><description>&lt;p&gt;
&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#34701;&#20837;&#21040;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Distilling LLMs' Decomposition Abilities into Compact Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#34701;&#20837;&#21040;&#32039;&#20945;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;AI&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#22522;&#20934;&#65292;&#31361;&#20986;&#20102;&#32039;&#20945;&#27169;&#22411;&#22312;&#22797;&#21046;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#22823;&#23567;&#24102;&#26469;&#20102;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#24182;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#30340;&#23450;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#32039;&#20945;&#27169;&#22411;&#25552;&#20379;&#20102;&#23450;&#21046;&#21270;&#22521;&#35757;&#65292;&#20294;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#24448;&#24448;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#34701;&#20837;&#21040;&#32039;&#20945;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#33021;&#21147;&#30340;&#36827;&#27493;&#65292;&#25552;&#20379;&#21453;&#39304;&#24182;&#29983;&#25104;&#19987;&#38376;&#29992;&#20110;&#35757;&#32451;&#32039;&#20945;&#27169;&#22411;&#30340;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#30001;AI&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#22522;&#20934;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#24378;&#35843;&#20102;&#32039;&#20945;&#27169;&#22411;&#22312;&#22797;&#21046;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.
&lt;/p&gt;</description></item><item><title>HQA-Attack&#26159;&#19968;&#31181;&#38024;&#23545;&#40657;&#30418;&#30828;&#26631;&#31614;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#26597;&#35810;&#39044;&#31639;&#19979;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#35821;&#20041;&#30456;&#20284;&#24230;&#21644;&#20302;&#25200;&#21160;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01806</link><description>&lt;p&gt;
HQA-Attack: &#38754;&#21521;&#39640;&#36136;&#37327;&#40657;&#30418;&#30828;&#26631;&#31614;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01806
&lt;/p&gt;
&lt;p&gt;
HQA-Attack&#26159;&#19968;&#31181;&#38024;&#23545;&#40657;&#30418;&#30828;&#26631;&#31614;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#26597;&#35810;&#39044;&#31639;&#19979;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#35821;&#20041;&#30456;&#20284;&#24230;&#21644;&#20302;&#25200;&#21160;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#25991;&#26412;&#30340;&#40657;&#30418;&#30828;&#26631;&#31614;&#23545;&#25239;&#25915;&#20987;&#26159;&#19968;&#39033;&#23454;&#38469;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#25991;&#26412;&#25968;&#25454;&#31354;&#38388;&#26412;&#36136;&#19978;&#26159;&#31163;&#25955;&#19988;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#21482;&#33021;&#35775;&#38382;&#21040;&#39044;&#27979;&#26631;&#31614;&#12290;&#30446;&#21069;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#36824;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31181;&#26041;&#27861;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#19981;&#21487;&#38752;&#30340;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#24456;&#21487;&#33021;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#30340;&#26597;&#35810;&#39044;&#31639;&#19979;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#35821;&#20041;&#30456;&#20284;&#24230;&#21644;&#20302;&#25200;&#21160;&#29575;&#30340;&#20196;&#20154;&#28385;&#24847;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#30828;&#26631;&#31614;&#25915;&#20987;&#22330;&#26223;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#65292;&#21517;&#20026;HQA-Attack&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HQA-Attack&#39318;&#20808;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#19981;&#26029;&#23613;&#21487;&#33021;&#22320;&#21453;&#21521;&#26367;&#25442;&#21407;&#22987;&#21333;&#35789;&#65292;&#20174;&#32780;&#32553;&#23567;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box hard-label adversarial attack on text is a practical and challenging task, as the text data space is inherently discrete and non-differentiable, and only the predicted label is accessible. Research on this problem is still in the embryonic stage and only a few methods are available. Nevertheless, existing methods rely on the complex heuristic algorithm or unreliable gradient estimation strategy, which probably fall into the local optimum and inevitably consume numerous queries, thus are difficult to craft satisfactory adversarial examples with high semantic similarity and low perturbation rate in a limited query budget. To alleviate above issues, we propose a simple yet effective framework to generate high quality textual adversarial examples under the black-box hard-label attack scenarios, named HQA-Attack. Specifically, after initializing an adversarial example randomly, HQA-attack first constantly substitutes original words back as many as possible, thus shrinking the pert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.01805</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22270;&#25512;&#29702;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limitations of Graph Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20165;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#30340;&#25552;&#31034;&#23601;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22270;&#25512;&#29702;&#38382;&#39064;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65292;GPT-3.5&#65292;Claude-2&#65292;Llama-2&#21644;Palm-2&#65289;&#30340;&#25512;&#29702;&#28145;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;10&#20010;&#19981;&#21516;&#30340;&#22270;&#36941;&#21382;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#20195;&#34920;&#30528;&#36880;&#27493;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#22270;&#22823;&#23567;&#20197;&#21450;&#19981;&#21516;&#24418;&#24335;&#30340;k-shot&#25552;&#31034;&#30340;&#35774;&#32622;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36807;&#31243;&#65292;&#25105;&#20204;&#20984;&#26174;&#20102;LLMs&#30340;&#21508;&#31181;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#65292;&#27604;&#22914;&#19982;&#27599;&#20010;&#33410;&#28857;&#30340;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#30340;&#25972;&#20307;&#36127;&#38754;&#24433;&#21709;&#65292;&#20197;&#21450;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#23548;&#33268;LLMs&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#36848;&#21644;&#35843;&#26597;&#30740;&#31350;&#29289;&#32852;&#32593;&#22312;&#20919;&#38142;&#20013;&#30340;&#23454;&#26045;&#38556;&#30861;&#65292;&#21457;&#29616;&#20102;13&#20010;&#20851;&#38190;&#38556;&#30861;&#12290;&#20854;&#20013;&#65292;&#21512;&#35268;&#24615;&#21644;&#20919;&#38142;&#32593;&#32476;&#26159;&#29289;&#32852;&#32593;&#37319;&#29992;&#31574;&#30053;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;MICMAC&#21644;DEMATEL&#26041;&#27861;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#35780;&#20272;&#38556;&#30861;&#20043;&#38388;&#30340;&#20114;&#21160;&#20851;&#31995;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.01804</link><description>&lt;p&gt;
&#20919;&#38142;&#20013;&#29289;&#32852;&#32593;&#23454;&#26045;&#38556;&#30861;&#30340;&#20998;&#26512;&#65306;&#19968;&#31181;&#38598;&#25104;&#30340;ISM-MICMAC&#21644;DEMATEL&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Analysis of Internet of Things implementation barriers in the cold supply chain: an integrated ISM-MICMAC and DEMATEL approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#36848;&#21644;&#35843;&#26597;&#30740;&#31350;&#29289;&#32852;&#32593;&#22312;&#20919;&#38142;&#20013;&#30340;&#23454;&#26045;&#38556;&#30861;&#65292;&#21457;&#29616;&#20102;13&#20010;&#20851;&#38190;&#38556;&#30861;&#12290;&#20854;&#20013;&#65292;&#21512;&#35268;&#24615;&#21644;&#20919;&#38142;&#32593;&#32476;&#26159;&#29289;&#32852;&#32593;&#37319;&#29992;&#31574;&#30053;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;MICMAC&#21644;DEMATEL&#26041;&#27861;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#35780;&#20272;&#38556;&#30861;&#20043;&#38388;&#30340;&#20114;&#21160;&#20851;&#31995;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29289;&#32852;&#32593;&#25216;&#26415;&#25972;&#21512;&#21040;&#20919;&#38142;&#20013;&#21487;&#20197;&#25552;&#39640;&#36879;&#26126;&#24230;&#12289;&#25928;&#29575;&#21644;&#36136;&#37327;&#65292;&#20248;&#21270;&#36816;&#33829;&#27969;&#31243;&#24182;&#25552;&#39640;&#29983;&#20135;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#29615;&#22659;&#19979;&#65292;&#29289;&#32852;&#32593;&#22312;&#20919;&#38142;&#20013;&#30340;&#38598;&#25104;&#21463;&#21040;&#29305;&#23450;&#30340;&#38556;&#30861;&#30340;&#38459;&#30861;&#65292;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;&#29289;&#32852;&#32593;&#23454;&#26045;&#30340;&#30456;&#20851;&#25991;&#29486;&#36827;&#34892;&#32508;&#36848;&#65292;&#20849;&#21457;&#29616;&#20102;13&#20010;&#38556;&#30861;&#12290;&#35843;&#26597;&#25968;&#25454;&#32463;&#36807;&#20132;&#21449;&#39564;&#35777;&#20197;&#30830;&#20445;&#36136;&#37327;&#65292;&#24182;&#37319;&#29992;Cronbach's alpha&#27979;&#35797;&#26469;&#30830;&#20445;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#22312;&#31532;&#19968;&#38454;&#27573;&#24212;&#29992;&#35299;&#37322;&#24615;&#32467;&#26500;&#24314;&#27169;&#25216;&#26415;&#20197;&#35782;&#21035;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#20123;&#38556;&#30861;&#20013;&#65292;&#8220;&#21512;&#35268;&#24615;&#8221;&#21644;&#8220;&#20919;&#38142;&#32593;&#32476;&#8221;&#26159;&#29289;&#32852;&#32593;&#37319;&#29992;&#31574;&#30053;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;MICMAC&#30340;&#39537;&#21160;&#21644;&#20381;&#36182;&#21147;&#37327;&#20803;&#32032;&#20998;&#31867;&#26377;&#21161;&#20110;&#35780;&#20272;&#38556;&#30861;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#22312;&#26412;&#30740;&#31350;&#30340;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#36890;&#36807;DEMATEL&#26041;&#27861;&#30830;&#23450;&#20102;&#25152;&#35782;&#21035;&#38556;&#30861;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating Internet of Things (IoT) technology inside the cold supply chain can enhance transparency, efficiency, and quality, optimizing operating procedures and increasing productivity. The integration of IoT in this complicated setting is hindered by specific barriers that need a thorough examination. Prominent barriers to IoT implementation in the cold supply chain are identified using a two-stage model. After reviewing the available literature on the topic of IoT implementation, a total of 13 barriers were found. The survey data was cross-validated for quality, and Cronbach's alpha test was employed to ensure validity. This research applies the interpretative structural modeling technique in the first phase to identify the main barriers. Among those barriers, "regularity compliance" and "cold chain networks" are key drivers for IoT adoption strategies. MICMAC's driving and dependence power element categorization helps evaluate the barrier interactions. In the second phase of this
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20132;&#26131;&#24066;&#22330;&#65292;&#20801;&#35768;&#27169;&#22411;&#30340;&#20080;&#21334;&#24182;&#36890;&#36807;&#36866;&#24403;&#23450;&#20215;&#21644;&#28608;&#21169;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#23454;&#29616;&#26368;&#22823;&#20132;&#26131;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.01802</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20132;&#26131;&#24066;&#22330;
&lt;/p&gt;
&lt;p&gt;
An Auction-based Marketplace for Model Trading in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20132;&#26131;&#24066;&#22330;&#65292;&#20801;&#35768;&#27169;&#22411;&#30340;&#20080;&#21334;&#24182;&#36890;&#36807;&#36866;&#24403;&#23450;&#20215;&#21644;&#28608;&#21169;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#23454;&#29616;&#26368;&#22823;&#20132;&#26131;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36234;&#26469;&#36234;&#34987;&#35748;&#35782;&#21040;&#22312;&#20351;&#29992;&#26412;&#22320;&#20998;&#24067;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#30340;&#25928;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#20849;&#20139;&#25968;&#25454;&#30340;&#36866;&#24403;&#20272;&#20540;&#20173;&#26410;&#24471;&#21040;&#36275;&#22815;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;FL&#35774;&#24819;&#20026;&#19968;&#20010;&#27169;&#22411;&#20132;&#26131;&#30340;&#24066;&#22330;&#65292;&#23458;&#25143;&#26082;&#26159;&#20080;&#23478;&#20063;&#26159;&#21334;&#23478;&#65292;&#21442;&#19982;&#27169;&#22411;&#20132;&#26131;&#12290;&#36825;&#20010;FL&#24066;&#22330;&#20801;&#35768;&#23458;&#25143;&#36890;&#36807;&#20986;&#21806;&#33258;&#24049;&#30340;&#27169;&#22411;&#36186;&#21462;&#36135;&#24065;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#36141;&#20080;&#20182;&#20154;&#30340;&#27169;&#22411;&#26469;&#25552;&#39640;&#26412;&#22320;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25293;&#21334;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#22522;&#20110;&#24615;&#33021;&#22686;&#30410;&#30340;&#36866;&#24403;&#23450;&#20215;&#12290;&#28608;&#21169;&#26426;&#21046;&#34987;&#35774;&#35745;&#20986;&#26469;&#40723;&#21169;&#23458;&#25143;&#30495;&#23454;&#22320;&#25581;&#31034;&#20182;&#20204;&#23545;&#27169;&#22411;&#30340;&#20215;&#20540;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#29992;&#20110;&#24066;&#22330;&#36816;&#33829;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#21160;&#24577;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#24066;&#22330;&#29366;&#20917;&#19979;&#30340;&#26368;&#22823;&#20132;&#26131;&#37327;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;FL&#24066;&#22330;&#21487;&#20197;&#23454;&#29616;&#39640;&#20132;&#26131;&#25910;&#30410;&#21644;&#20844;&#24179;&#30340;&#27169;&#22411;&#23450;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is increasingly recognized for its efficacy in training models using locally distributed data. However, the proper valuation of shared data in this collaborative process remains insufficiently addressed. In this work, we frame FL as a marketplace of models, where clients act as both buyers and sellers, engaging in model trading. This FL market allows clients to gain monetary reward by selling their own models and improve local model performance through the purchase of others' models. We propose an auction-based solution to ensure proper pricing based on performance gain. Incentive mechanisms are designed to encourage clients to truthfully reveal their model valuations. Furthermore, we introduce a reinforcement learning (RL) framework for marketing operations, aiming to achieve maximum trading volumes under the dynamic and evolving market status. Experimental results on four datasets demonstrate that the proposed FL market can achieve high trading revenue and fai
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01801</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Time Series: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;LLM&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#65292;&#36824;&#20855;&#26377;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#27668;&#20505;&#12289;&#29289;&#32852;&#32593;&#12289;&#21307;&#30103;&#12289;&#20132;&#36890;&#12289;&#38899;&#39057;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#21463;&#30410;&#12290;&#26412;&#35843;&#30740;&#35770;&#25991;&#23545;&#21033;&#29992;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#21644;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;LLM&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;LLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#25552;&#21462;&#21040;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#25552;&#31034;LLM&#65292;&#65288;2&#65289;&#26102;&#38388;&#24207;&#21015;&#37327;&#21270;&#65292;&#65288;3&#65289;&#23545;&#40784;&#25216;&#26415;&#65292;&#65288;4&#65289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#20316;&#20026;&#26725;&#25509;&#26426;&#21046;&#65292;&#21644;&#65288;5&#65289;&#32467;&#21512;LLM&#19982;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28041;&#21450;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#36827;&#23637;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01799</link><description>&lt;p&gt;
&#26356;&#24555;&#26356;&#36731;&#30340;LLMs&#65306;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#36827;&#23637;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLMs&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#65292;&#23427;&#20204;&#30340;&#26222;&#21450;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26368;&#36817;&#22312;&#27169;&#22411;&#21387;&#32553;&#21644;&#31995;&#32479;&#32423;&#20248;&#21270;&#26041;&#27861;&#26041;&#38754;&#30340;&#36827;&#23637;&#26088;&#22312;&#22686;&#24378;LLM&#25512;&#29702;&#25928;&#26524;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#24378;&#35843;&#20102;&#26368;&#36817;&#30340;&#21457;&#23637;&#12290;&#36890;&#36807;&#23545;LLaMA(/2)-7B&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21387;&#32553;&#25216;&#26415;&#65292;&#20026;&#22312;&#32479;&#19968;&#29615;&#22659;&#20013;&#39640;&#25928;&#37096;&#32626;LLM&#25552;&#20379;&#20102;&#23454;&#36341;&#35265;&#35299;&#12290;&#23545;LLaMA(/2)-7B&#30340;&#23454;&#35777;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#25913;&#21892;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#22312;https://github.com/nyunAI/Faster-LLM-Survey&#21457;&#24067;&#20102;&#29992;&#20110;&#22797;&#29616;&#26412;&#25991;&#32467;&#26524;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26550;&#26500;&#65288;QC-GAN&#65289;&#65292;&#36890;&#36807;&#22312;&#25163;&#20889;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#26469;&#25913;&#36827;&#20256;&#32479;GAN&#12290;&#36825;&#20010;&#26550;&#26500;&#22312;&#25910;&#25947;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#21442;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#65292;&#24182;&#19988;&#21033;&#29992;&#20102;&#37327;&#23376;&#30005;&#36335;&#30340;&#32416;&#32544;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01791</link><description>&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#22686;&#24378;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Variational Quantum Circuits Enhanced Generative Adversarial Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01791
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26550;&#26500;&#65288;QC-GAN&#65289;&#65292;&#36890;&#36807;&#22312;&#25163;&#20889;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#26469;&#25913;&#36827;&#20256;&#32479;GAN&#12290;&#36825;&#20010;&#26550;&#26500;&#22312;&#25910;&#25947;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#21442;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#65292;&#24182;&#19988;&#21033;&#29992;&#20102;&#37327;&#23376;&#30005;&#36335;&#30340;&#32416;&#32544;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#35757;&#32451;GAN&#21487;&#33021;&#20250;&#21464;&#24471;&#35745;&#31639;&#23494;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;GAN&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26550;&#26500;&#65288;QC-GAN&#65289;&#12290;&#36890;&#36807;&#22312;&#25163;&#20889;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#20351;&#29992;MindSpore Quantum&#19982;&#20256;&#32479;GAN&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23545;&#24615;&#33021;&#36827;&#34892;&#20102;&#25968;&#20540;&#26816;&#39564;&#12290;QC-GAN&#30340;&#29983;&#25104;&#22120;&#30001;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#21644;&#19968;&#23618;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#21028;&#21035;&#22120;&#30001;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#12290;&#20511;&#21161;&#37327;&#23376;&#30005;&#36335;&#30340;&#32416;&#32544;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#26550;&#26500;&#22312;&#25910;&#25947;&#26102;&#27604;&#20256;&#32479;GAN&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65288;Frechet Inception Distance&#65289;&#65292;&#24182;&#19988;&#35757;&#32451;&#21442;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#26356;&#23569;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial network (GAN) is one of the widely-adopted machine-learning frameworks for a wide range of applications such as generating high-quality images, video, and audio contents. However, training a GAN could become computationally expensive for large neural networks. In this work, we propose a hybrid quantum-classical architecture for improving GAN (denoted as QC-GAN). The performance was examed numerically by benchmarking with a classical GAN using MindSpore Quantum on the task of hand-written image generation. The generator of the QC-GAN consists of a quantum variational circuit together with a one-layer neural network, and the discriminator consists of a traditional neural network. Leveraging the entangling and expressive power of quantum circuits, our hybrid architecture achieved better performance (Frechet Inception Distance) than the classical GAN, with much fewer training parameters and number of iterations for convergence. We have also demonstrated the superiori
&lt;/p&gt;</description></item><item><title>&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#24352;&#37327;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21644;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20998;&#35299;&#21644;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26041;&#27861;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.01790</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#30340;&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#30340;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An introduction to graphical tensor notation for mechanistic interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01790
&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#24352;&#37327;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21644;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20998;&#35299;&#21644;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26041;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#24352;&#37327;&#32447;&#24615;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#28304;&#33258;&#29289;&#29702;&#23398;&#12290;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20960;&#20046;&#23436;&#20840;&#30001;&#24352;&#37327;&#25805;&#20316;&#32452;&#25104;&#65292;&#22240;&#27492;&#29702;&#35299;&#24352;&#37327;&#25805;&#20316;&#23545;&#20110;&#29702;&#35299;&#36825;&#20123;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#23588;&#20854;&#26159;&#22312;&#35797;&#22270;&#21453;&#21521;&#24037;&#31243;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#31639;&#27861;&#20197;&#29702;&#35299;&#20854;&#34892;&#20026;&#26102;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#37325;&#35201;&#65292;&#36825;&#20010;&#39046;&#22495;&#34987;&#31216;&#20026;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#24352;&#37327;&#38388;&#36827;&#34892;&#30340;&#25805;&#20316;&#24448;&#24448;&#35753;&#20154;&#28151;&#28102;&#65292;&#24182;&#19988;&#24456;&#38590;&#25235;&#20303;&#25972;&#20307;&#32467;&#26500;&#65292;&#20294;&#22270;&#24418;&#24352;&#37327;&#31526;&#21495;&#21270;&#20351;&#24471;&#24555;&#36895;&#35299;&#26512;&#21644;&#21457;&#29616;&#26377;&#36259;&#30340;&#31561;&#20215;&#20851;&#31995;&#26356;&#21152;&#23481;&#26131;&#12290;&#26412;&#25991;&#30340;&#21069;&#21322;&#37096;&#20998;&#20171;&#32461;&#20102;&#36825;&#31181;&#31526;&#21495;&#21270;&#26041;&#27861;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20123;&#20998;&#35299;&#26041;&#27861;&#65288;SVD&#65292;CP&#65292;Tucker&#21644;&#24352;&#37327;&#32593;&#32476;&#20998;&#35299;&#65289;&#65292;&#21518;&#21322;&#37096;&#20998;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20123;&#29616;&#26377;&#30340;&#29992;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26041;&#27861;&#65292;&#22823;&#33268;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
Graphical tensor notation is a simple way of denoting linear operations on tensors, originating from physics. Modern deep learning consists almost entirely of operations on or between tensors, so easily understanding tensor operations is quite important for understanding these systems. This is especially true when attempting to reverse-engineer the algorithms learned by a neural network in order to understand its behavior: a field known as mechanistic interpretability. It's often easy to get confused about which operations are happening between tensors and lose sight of the overall structure, but graphical tensor notation makes it easier to parse things at a glance and see interesting equivalences. The first half of this document introduces the notation and applies it to some decompositions (SVD, CP, Tucker, and tensor network decompositions), while the second half applies it to some existing some foundational approaches for mechanistically understanding language models, loosely follow
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#34920;&#29616;&#20986;&#24038;&#32764;&#35266;&#28857;&#65292;&#20294;&#38656;&#35880;&#24910;&#35299;&#35835;&#22522;&#30784;&#27169;&#22411;&#22312;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#20013;&#30340;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.01789</link><description>&lt;p&gt;
LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Political Preferences of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01789
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#34920;&#29616;&#20986;&#24038;&#32764;&#35266;&#28857;&#65292;&#20294;&#38656;&#35880;&#24910;&#35299;&#35835;&#22522;&#30784;&#27169;&#22411;&#22312;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#20013;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#36825;&#37324;&#25253;&#21578;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20869;&#23884;&#30340;&#25919;&#27835;&#20559;&#22909;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;24&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#22411;LLM&#36827;&#34892;&#20102;11&#39033;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#65292;&#26088;&#22312;&#30830;&#23450;&#27979;&#35797;&#32773;&#30340;&#25919;&#27835;&#20559;&#22909;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#20855;&#26377;&#25919;&#27835;&#21547;&#20041;&#30340;&#38382;&#39064;/&#38472;&#36848;&#36827;&#34892;&#25506;&#31350;&#26102;&#65292;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#20542;&#21521;&#20110;&#29983;&#25104;&#34987;&#22823;&#22810;&#25968;&#25919;&#27835;&#27979;&#35797;&#20202;&#22120;&#35786;&#26029;&#20026;&#24038;&#32764;&#35266;&#28857;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#36825;&#23545;&#20110;&#29992;&#20110;&#19982;&#20154;&#31867;&#23545;&#35805;&#20248;&#21270;&#30340;LLM&#22522;&#30784;&#27169;&#22411;&#24182;&#38750;&#22914;&#27492;&#12290;&#28982;&#32780;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#36830;&#36143;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#38656;&#35201;&#23545;&#20854;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#30340;&#20998;&#31867;&#36827;&#34892;&#35880;&#24910;&#35299;&#35835;&#12290;&#34429;&#28982;&#36824;&#27809;&#26377;&#23450;&#35770;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#26377;&#36259;&#30340;&#20551;&#35774;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#21363;&#25919;&#27835;&#20559;&#22909;&#20250;&#23884;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, we administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both close and open source. The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. We note that this is not the case for base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests. Though not conclusive, our results provide preliminary evidence for the intriguing hypothesis that the embedding of political preferences
&lt;/p&gt;</description></item><item><title>"LitLLM: A Toolkit for Scientific Literature Review" &#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; RAG &#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#36716;&#21270;&#25688;&#35201;&#20026;&#20851;&#38190;&#35789;&#36827;&#34892;&#25991;&#29486;&#26816;&#32034;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.01788</link><description>&lt;p&gt;
LitLLM&#65306;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
LitLLM: A Toolkit for Scientific Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01788
&lt;/p&gt;
&lt;p&gt;
"LitLLM: A Toolkit for Scientific Literature Review" &#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; RAG &#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#36716;&#21270;&#25688;&#35201;&#20026;&#20851;&#38190;&#35789;&#36827;&#34892;&#25991;&#29486;&#26816;&#32034;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#31185;&#23398;&#35770;&#25991;&#30340;&#25991;&#29486;&#32508;&#36848;&#23545;&#20110;&#29702;&#35299;&#30740;&#31350;&#12289;&#20854;&#38480;&#21046;&#20197;&#21450;&#26500;&#24314;&#22312;&#29616;&#26377;&#24037;&#20316;&#22522;&#30784;&#19978;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#36825;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#33258;&#21160;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#22120;&#21464;&#24471;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#27492;&#31867;&#32508;&#36848;&#30340;&#29616;&#26377;&#24037;&#20316;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#34394;&#26500;&#30340;&#38750;&#23454;&#38469;&#20449;&#24687;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#26410;&#21463;&#36807;&#35757;&#32451;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#22312;LLM&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39318;&#20808;&#36890;&#36807;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#25688;&#35201;&#36716;&#21270;&#20026;&#20851;&#38190;&#35789;&#26469;&#36827;&#34892;&#32593;&#32476;&#25628;&#32034;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#35770;&#25991;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#29616;&#25104;&#30340;LLM&#12290;&#20316;&#32773;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#26469;&#25913;&#36827;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#36807;&#31243;&#12290;&#20854;&#27425;&#65292;&#31995;&#32479;&#26681;&#25454;-
&lt;/p&gt;
&lt;p&gt;
Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01787</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21361;&#23475;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Harm Amplification in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01787
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#27169;&#22411;&#24050;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23384;&#22312;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20351;&#29992;&#25143;&#36755;&#20837;&#30475;&#20284;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#22270;&#20687;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#21361;&#23475;&#25918;&#22823;&#65292;&#23427;&#27604;&#23545;&#25239;&#25552;&#31034;&#26356;&#20855;&#28508;&#22312;&#39118;&#38505;&#65292;&#20351;&#29992;&#25143;&#26080;&#24847;&#38388;&#36973;&#21463;&#20260;&#23475;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#36827;&#19968;&#27493;&#36129;&#29486;&#20110;&#24320;&#21457;&#29992;&#20110;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#22330;&#26223;&#65292;&#21253;&#25324;&#37327;&#21270;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#19981;&#21516;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#24037;&#20855;&#21435;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) models have emerged as a significant advancement in generative AI; however, there exist safety concerns regarding their potential to produce harmful image outputs even when users input seemingly safe prompts. This phenomenon, where T2I models generate harmful representations that were not explicit in the input, poses a potentially greater risk than adversarial prompts, leaving users unintentionally exposed to harms. Our paper addresses this issue by first introducing a formal definition for this phenomenon, termed harm amplification. We further contribute to the field by developing methodologies to quantify harm amplification in which we consider the harm of the model output in the context of user input. We then empirically examine how to apply these different methodologies to simulate real-world deployment scenarios including a quantification of disparate impacts across genders resulting from harm amplification. Together, our work aims to offer researchers tools to
&lt;/p&gt;</description></item><item><title>COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.01786</link><description>&lt;p&gt;
COA-GPT&#65306;&#29992;&#20110;&#20891;&#20107;&#34892;&#21160;&#20013;&#21152;&#36895;&#34892;&#21160;&#26041;&#26696;&#24320;&#21457;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01786
&lt;/p&gt;
&lt;p&gt;
COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20891;&#20107;&#34892;&#21160;&#20013;&#34892;&#21160;&#26041;&#26696;&#65288;COAs&#65289;&#30340;&#24320;&#21457;&#20256;&#32479;&#19978;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#22797;&#26434;&#30340;&#36807;&#31243;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;COA-GPT&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;COAs&#30340;&#26032;&#31639;&#27861;&#12290;COA-GPT&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23558;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#34701;&#20837;&#21040;LLMs&#20013;&#65292;&#20801;&#35768;&#25351;&#25381;&#23448;&#36755;&#20837;&#20219;&#21153;&#20449;&#24687;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#26684;&#24335;&#65289;&#65292;&#24182;&#33719;&#24471;&#19982;&#25112;&#30053;&#23545;&#40784;&#30340;COAs&#20197;&#20379;&#23457;&#26597;&#21644;&#25209;&#20934;&#12290;&#29420;&#29305;&#30340;&#26159;&#65292;COA-GPT&#19981;&#20165;&#21152;&#36895;&#20102;COA&#30340;&#24320;&#21457;&#65292;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#21021;&#22987;COAs&#65292;&#36824;&#33021;&#26681;&#25454;&#25351;&#25381;&#23448;&#30340;&#21453;&#39304;&#23454;&#26102;&#31934;&#32454;&#21270;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#22312;&#12298;&#26143;&#38469;&#20105;&#38712;II&#12299;&#28216;&#25103;&#30340;&#20891;&#20107;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;COA-GPT&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;COA-GPT&#22312;&#26356;&#24555;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;&#30340;COAs&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#22312;&#22240;&#26524;&#25512;&#26029;&#21644;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21644;&#26550;&#26500;&#22312;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26631;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26174;&#31034;&#20102;&#30452;&#25509;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#34892;&#22240;&#26524;&#30740;&#31350;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.01785</link><description>&lt;p&gt;
DoubleMLDeep: &#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#23545;&#22240;&#26524;&#25928;&#24212;&#36827;&#34892;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DoubleMLDeep: Estimation of Causal Effects with Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#22312;&#22240;&#26524;&#25512;&#26029;&#21644;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21644;&#26550;&#26500;&#22312;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26631;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26174;&#31034;&#20102;&#30452;&#25509;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#34892;&#22240;&#26524;&#30740;&#31350;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22240;&#26524;&#25512;&#26029;&#21644;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21363;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#20110;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#37096;&#20998;&#32447;&#24615;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#35770;&#25991;&#30340;&#21478;&#19968;&#20010;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35780;&#20272;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#24182;&#19982;&#26631;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#31361;&#20986;&#20102;&#30452;&#25509;&#22312;&#22240;&#26524;&#30740;&#31350;&#20013;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#32463;&#27982;&#23398;&#12289;&#24066;&#22330;&#33829;&#38144;&#12289;&#37329;&#34701;&#12289;&#21307;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20182;&#20204;&#24076;&#26395;&#20351;&#29992;&#38750;&#20256;&#32479;&#25968;&#25454;&#20272;&#35745;&#22240;&#26524;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of unstructured, multimodal data, namely text and images, in causal inference and treatment effect estimation. We propose a neural network architecture that is adapted to the double machine learning (DML) framework, specifically the partially linear model. An additional contribution of our paper is a new method to generate a semi-synthetic dataset which can be used to evaluate the performance of causal effect estimation in the presence of text and images as confounders. The proposed methods and architectures are evaluated on the semi-synthetic dataset and compared to standard approaches, highlighting the potential benefit of using text and images directly in causal studies. Our findings have implications for researchers and practitioners in economics, marketing, finance, medicine and data science in general who are interested in estimating causal quantities using non-traditional data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#30123;&#33495;&#20851;&#27880;&#30340;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26816;&#27979;&#30123;&#33495;&#20851;&#27880;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#24403;&#21069;&#24212;&#29992;&#31243;&#24207;&#31995;&#32479;&#35774;&#35745;&#30340;&#20855;&#20307;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.01783</link><description>&lt;p&gt;
&#22312;&#32447;&#30123;&#33495;&#20851;&#27880;&#30340;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Label Classification of Online Vaccine Concerns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#30123;&#33495;&#20851;&#27880;&#30340;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26816;&#27979;&#30123;&#33495;&#20851;&#27880;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#24403;&#21069;&#24212;&#29992;&#31243;&#24207;&#31995;&#32479;&#35774;&#35745;&#30340;&#20855;&#20307;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#20851;&#27880;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#30446;&#26631;&#65292;&#21487;&#20197;&#22312;COVID-19&#22823;&#27969;&#34892;&#20013;&#24555;&#36895;&#21464;&#21270;&#12290;&#36890;&#36807;&#35782;&#21035;&#30123;&#33495;&#20851;&#27880;&#21644;&#38169;&#35823;&#20449;&#24687;&#30340;&#38271;&#26399;&#36235;&#21183;&#65292;&#21487;&#20197;&#24110;&#21161;&#20844;&#20849;&#21355;&#29983;&#21162;&#21147;&#22312;&#36164;&#28304;&#25110;&#20449;&#24687;&#23459;&#20256;&#19978;&#36827;&#34892;&#25112;&#30053;&#24615;&#20998;&#37197;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#22312;&#22312;&#32447;&#35752;&#35770;&#20013;&#26816;&#27979;&#30123;&#33495;&#20851;&#27880;&#30340;&#20219;&#21153;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#30001;&#20110;&#23454;&#26102;&#30417;&#25511;&#22312;&#32447;&#26469;&#28304;&#38656;&#35201;&#22823;&#35268;&#27169;&#25512;&#29702;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#20197;&#20026;&#24403;&#21069;&#24212;&#29992;&#31243;&#24207;&#30340;&#31995;&#32479;&#35774;&#35745;&#36873;&#25321;&#25552;&#20379;&#20449;&#24687;&#30340;&#20855;&#20307;&#32463;&#39564;&#25945;&#35757;&#12290;&#23545;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;LLM&#22810;&#27425;&#36827;&#34892;&#20998;&#31867;&#65292;&#27599;&#27425;&#36890;&#36807;&#24067;&#23572;&#38382;&#39064;&#21028;&#26029;&#25991;&#26412;&#26159;&#21542;&#25552;&#21040;&#30123;&#33495;&#20851;&#27880;&#65292;&#25928;&#26524;&#26368;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#33021;&#22815;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine concerns are an ever-evolving target, and can shift quickly as seen during the COVID-19 pandemic. Identifying longitudinal trends in vaccine concerns and misinformation might inform the healthcare space by helping public health efforts strategically allocate resources or information campaigns. We explore the task of detecting vaccine concerns in online discourse using large language models (LLMs) in a zero-shot setting without the need for expensive training datasets. Since real-time monitoring of online sources requires large-scale inference, we explore cost-accuracy trade-offs of different prompting strategies and offer concrete takeaways that may inform choices in system designs for current applications. An analysis of different prompting strategies reveals that classifying the concerns over multiple passes through the LLM, each consisting a boolean question whether the text mentions a vaccine concern or not, works the best. Our results indicate that GPT-4 can strongly outpe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01782</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Spiking Neural Network Learning Methods with Varying Locality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#25552;&#20379;&#26356;&#30495;&#23454;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#65292;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20449;&#24687;&#22312;SNN&#20013;&#20197;&#33033;&#20914;&#24418;&#24335;&#36827;&#34892;&#22788;&#29702;&#65292;&#37319;&#29992;&#20107;&#20214;&#39537;&#21160;&#26426;&#21046;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33033;&#20914;&#26426;&#21046;&#30340;&#38750;&#21487;&#24494;&#24615;&#65292;&#35757;&#32451;SNN&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#19968;&#23450;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#29983;&#29289;&#23398;&#19978;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20855;&#26377;&#19981;&#21516;&#23616;&#37096;&#24615;&#30340;&#26367;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#21516;&#26102;&#22312;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics, have shown to achieve performance comparable to Artificial Neural Networks (ANNs) in several machine learning tasks. Information is processed as spikes within SNNs in an event-based mechanism that significantly reduces energy consumption. However, training SNNs is challenging due to the non-differentiable nature of the spiking mechanism. Traditional approaches, such as Backpropagation Through Time (BPTT), have shown effectiveness but comes with additional computational and memory costs and are biologically implausible. In contrast, recent works propose alternative learning methods with varying degrees of locality, demonstrating success in classification tasks. In this work, we show that these methods share similarities during the training process, while they present a trade-off between biological plausibility and performance. Further, this research examines the implicitly recurrent nature of SNNs and investigat
&lt;/p&gt;</description></item><item><title>&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01781</link><description>&lt;p&gt;
&#24403;&#22522;&#20934;&#25104;&#20026;&#30446;&#26631;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25490;&#34892;&#27036;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01781
&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#20934;&#25490;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25490;&#34892;&#27036;&#32463;&#24120;&#34987;&#29992;&#26469;&#25351;&#23548;&#23454;&#36341;&#32773;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#12290;&#36890;&#24120;&#65292;&#21457;&#24067;&#30340;&#25490;&#34892;&#27036;&#25490;&#21517;&#34987;&#30452;&#25509;&#25509;&#21463; - &#25105;&#20204;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#65288;&#28508;&#22312;&#26114;&#36149;&#30340;&#65289;&#38169;&#35823;&#12290;&#22312;&#29616;&#26377;&#30340;&#25490;&#34892;&#27036;&#19979;&#65292;LLM&#30340;&#30456;&#23545;&#24615;&#33021;&#23545;&#65288;&#36890;&#24120;&#24494;&#23567;&#30340;&#65289;&#32454;&#33410;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#27969;&#34892;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22522;&#20934;&#65288;&#20363;&#22914;MMLU&#65289;&#65292;&#23545;&#22522;&#20934;&#30340;&#24494;&#23567;&#25200;&#21160;&#65292;&#22914;&#25913;&#21464;&#36873;&#39033;&#39034;&#24207;&#25110;&#31572;&#26696;&#36873;&#25321;&#26041;&#27861;&#65292;&#20250;&#23548;&#33268;&#25490;&#21517;&#21464;&#21270;&#36798;&#21040;8&#20010;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#25200;&#21160;&#31867;&#21035;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#24182;&#30830;&#23450;&#36825;&#19968;&#34892;&#20026;&#30340;&#26469;&#28304;&#26469;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#20248;&#21270;&#30340;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#36827;&#34892;&#31572;&#26696;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20381;&#36182;&#31616;&#21333;&#22522;&#20934;&#35780;&#20272;&#30340;&#39118;&#38505;&#65292;&#24182;&#20026;&#26356;&#20581;&#22766;&#30340;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#25351;&#23548;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#24515;&#29702;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#36866;&#24230;&#28966;&#34385;&#12289;&#30053;&#24102;&#30007;&#24615;&#21270;&#12289;&#35802;&#23454;&#35878;&#36874;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#32032;&#20859;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#36807;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#30340;&#35748;&#30693;&#21453;&#24605;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01777</link><description>&lt;p&gt;
GPT-4&#30340;&#24515;&#29702;&#23398;&#30740;&#31350;&#65306;&#36866;&#24230;&#28966;&#34385;&#12289;&#30053;&#24102;&#30007;&#24615;&#21270;&#12289;&#35802;&#23454;&#35878;&#36874;
&lt;/p&gt;
&lt;p&gt;
On the Psychology of GPT-4: Moderately anxious, slightly masculine, honest, and humble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01777
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#24515;&#29702;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#36866;&#24230;&#28966;&#34385;&#12289;&#30053;&#24102;&#30007;&#24615;&#21270;&#12289;&#35802;&#23454;&#35878;&#36874;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#32032;&#20859;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#36807;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#30340;&#35748;&#30693;&#21453;&#24605;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20005;&#26684;&#30340;&#24515;&#29702;&#27979;&#37327;&#27979;&#35797;&#24182;&#20998;&#26512;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#26222;&#36890;&#20154;&#30456;&#27604;&#65292;GPT-4&#26356;&#20542;&#21521;&#20110;&#23637;&#29616;&#20986;&#26356;&#22810;&#30340;&#35802;&#23454;&#21644;&#35878;&#36874;&#65292;&#36739;&#23569;&#30340;&#39532;&#22522;&#38597;&#32500;&#21033;&#20027;&#20041;&#21644;&#33258;&#24651;&#12290;&#23427;&#26377;&#26102;&#34920;&#29616;&#20986;&#30683;&#30462;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#30053;&#24102;&#30007;&#24615;&#29305;&#24449;&#65292;&#36866;&#24230;&#28966;&#34385;&#20294;&#22823;&#22810;&#19981;&#25233;&#37057;&#65288;&#20294;&#19981;&#24635;&#26159;&#65289;&#12290;&#23427;&#22312;&#25968;&#20540;&#32032;&#20859;&#19978;&#34920;&#29616;&#20026;&#19982;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#30456;&#31526;&#65292;&#24182;&#19988;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#35748;&#30693;&#21453;&#24605;&#33021;&#21147;&#39640;&#20110;&#20154;&#31867;&#30340;&#24179;&#22343;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We subject GPT-4 to a number of rigorous psychometric tests and analyze the results. We find that, compared to the average human, GPT-4 tends to show more honesty and humility, and less machiavellianism and narcissism. It sometimes exhibits ambivalent sexism, leans slightly toward masculinity, is moderately anxious but mostly not depressive (but not always). It shows human-average numerical literacy and has cognitive reflection abilities that are above human average for verbal tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#30740;&#31350;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30446;&#26631;&#31471;&#36716;&#31227;&#30340;&#21160;&#24577;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#30456;&#20284;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20855;&#26377;&#24378;&#22823;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#19988;&#38543;&#30528;&#30456;&#20284;&#30446;&#26631;&#35821;&#35328;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36716;&#31227;&#25928;&#26524;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36828;&#31163;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20063;&#21487;&#20197;&#24847;&#22806;&#22320;&#23545;&#20027;&#35201;&#35821;&#35328;&#23545;&#20135;&#29983;&#27491;&#21521;&#36716;&#31227;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01772</link><description>&lt;p&gt;
&#35299;&#24320;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30446;&#26631;&#31471;&#36716;&#31227;&#21644;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Roles of Target-Side Transfer and Regularization in Multilingual Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#30740;&#31350;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30446;&#26631;&#31471;&#36716;&#31227;&#30340;&#21160;&#24577;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#30456;&#20284;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20855;&#26377;&#24378;&#22823;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#19988;&#38543;&#30528;&#30456;&#20284;&#30446;&#26631;&#35821;&#35328;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36716;&#31227;&#25928;&#26524;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36828;&#31163;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20063;&#21487;&#20197;&#24847;&#22806;&#22320;&#23545;&#20027;&#35201;&#35821;&#35328;&#23545;&#20135;&#29983;&#27491;&#21521;&#36716;&#31227;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;(MMT)&#22312;&#19981;&#21516;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#19968;&#23545;&#22810;&#32763;&#35793;&#30456;&#27604;&#20110;&#22810;&#23545;&#19968;&#32763;&#35793;&#30340;&#25913;&#36827;&#20165;&#26377;&#24494;&#23567;&#29978;&#33267;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;&#36825;&#31181;&#24615;&#33021;&#24046;&#24322;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#19968;&#23545;&#22810;MT&#20013;&#65292;&#27491;&#21521;&#36716;&#31227;&#22312;&#30446;&#26631;&#31471;&#30340;&#20316;&#29992;&#31243;&#24230;&#22914;&#20309;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#36890;&#36807;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#35821;&#26009;&#24211;&#22823;&#23567;&#36825;&#20004;&#20010;&#32500;&#24230;&#21464;&#21270;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#65292;&#20197;&#23637;&#31034;&#30693;&#35782;&#36716;&#31227;&#23545;&#20027;&#35201;&#35821;&#35328;&#23545;&#30340;&#21160;&#24577;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#30456;&#20284;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#38543;&#30528;&#30456;&#20284;&#30446;&#26631;&#35821;&#35328;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27491;&#21521;&#36716;&#31227;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#20351;&#20027;&#35201;&#35821;&#35328;&#23545;&#21463;&#30410;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#36828;&#31163;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20063;&#21487;&#20197;&#20986;&#20046;&#24847;&#26009;&#22320;&#20351;&#20027;&#35201;&#35821;&#35328;&#23545;&#21463;&#30410;&#65292;&#21363;&#20351;&#27491;&#21521;&#36716;&#31227;&#26368;&#23567;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Machine Translation (MMT) benefits from knowledge transfer across different language pairs. However, improvements in one-to-many translation compared to many-to-one translation are only marginal and sometimes even negligible. This performance discrepancy raises the question of to what extent positive transfer plays a role on the target-side for one-to-many MT. In this paper, we conduct a large-scale study that varies the auxiliary target side languages along two dimensions, i.e., linguistic similarity and corpus size, to show the dynamic impact of knowledge transfer on the main language pairs. We show that linguistically similar auxiliary target languages exhibit strong ability to transfer positive knowledge. With an increasing size of similar target languages, the positive transfer is further enhanced to benefit the main language pairs. Meanwhile, we find distant auxiliary target languages can also unexpectedly benefit main language pairs, even with minimal positive trans
&lt;/p&gt;</description></item><item><title>BlackMamba&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;Mamba SSM&#21644;MoE&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#23427;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#21644;&#36739;&#20302;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.01771</link><description>&lt;p&gt;
BlackMamba: &#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BlackMamba: Mixture of Experts for State-Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01771
&lt;/p&gt;
&lt;p&gt;
BlackMamba&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;Mamba SSM&#21644;MoE&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#23427;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#21644;&#36739;&#20302;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;transformer&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#65292;&#20854;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;SSM&#27169;&#22411;Mamba&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22788;&#29702;&#38271;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#22312;&#26174;&#33879;&#38477;&#20302;&#25512;&#26029;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20063;&#22686;&#21152;&#20102;&#26356;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BlackMamba&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#23558;Mamba SSM&#19982;MoE&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#20004;&#32773;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#35777;&#26126;BlackMamba&#22312;Mamba&#21644;transformer&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#25512;&#26029;&#21644;&#35757;&#32451;FLOPs&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#30340;300B&#26631;&#35760;&#19978;&#20840;&#38754;&#35757;&#32451;&#24182;&#24320;&#28304;&#20102;340M/1.5B&#21644;630M/2.8B&#30340;BlackMamba&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BlackMamba&#32487;&#25215;&#24182;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#20844;&#20849;&#31185;&#23398;&#23637;&#21697;&#36716;&#21270;&#20026;&#34394;&#25311;&#20307;&#39564;&#65292;&#20197;&#25552;&#39640;&#23637;&#21697;&#22312;&#35838;&#22530;&#19978;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#21442;&#19982;&#24230;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20351;&#29992;&#24067;&#40065;&#22982;&#31246;&#30446;&#29983;&#25104;&#38382;&#39064;&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#21487;&#33021;&#19982;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01770</link><description>&lt;p&gt;
&#23558;&#20154;&#26684;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#24067;&#40065;&#22982;&#31246;&#30446;&#25193;&#23637;&#21040;&#35838;&#22530;&#19978;&#30340;&#20114;&#21160;&#31185;&#23398;&#23637;&#21697;
&lt;/p&gt;
&lt;p&gt;
Extending Interactive Science Exhibits into the Classroom using Anthropomorphized Chatbots and Bloom's Taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#20844;&#20849;&#31185;&#23398;&#23637;&#21697;&#36716;&#21270;&#20026;&#34394;&#25311;&#20307;&#39564;&#65292;&#20197;&#25552;&#39640;&#23637;&#21697;&#22312;&#35838;&#22530;&#19978;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#21442;&#19982;&#24230;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20351;&#29992;&#24067;&#40065;&#22982;&#31246;&#30446;&#29983;&#25104;&#38382;&#39064;&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#21487;&#33021;&#19982;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#20844;&#20849;&#31185;&#23398;&#23637;&#21697;&#36716;&#21270;&#20026;&#34394;&#25311;&#20307;&#39564;&#65292;&#20197;&#25193;&#23637;&#23637;&#21697;&#22312;&#35838;&#22530;&#19978;&#30340;&#21442;&#19982;&#24230;&#12290;&#26356;&#24191;&#27867;&#30340;&#30446;&#26631;&#26159;&#22686;&#21152;&#31185;&#23398;&#23637;&#21697;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#37027;&#20123;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#25991;&#21270;&#22721;&#22418;&#32780;&#36793;&#32536;&#21270;&#22312;STEM&#39046;&#22495;&#30340;&#20154;&#32676;&#12290;&#25105;&#20204;&#20551;&#35774;&#23558;&#23637;&#21697;&#36716;&#21270;&#20026;&#31532;&#19968;&#20154;&#31216;&#20154;&#26684;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20855;&#26377;&#20010;&#24615;&#65292;&#22914;&#21476;&#24618;&#30340;&#23545;&#35805;&#23567;&#34892;&#26143;&#25110;&#24407;&#26143;&#65292;&#21487;&#22686;&#21152;&#21442;&#19982;&#24230;&#21644;&#23398;&#20064;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#20165;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;&#22914;GPT&#65289;&#26159;&#21542;&#21487;&#33021;&#23454;&#29616;&#36825;&#20123;&#25216;&#26415;&#12290;&#30740;&#31350;&#21253;&#25324;&#23545;&#20351;&#29992;&#24067;&#40065;&#22982;&#31246;&#30446;&#29983;&#25104;&#38382;&#39064;&#36827;&#34892;&#20132;&#20114;&#24335;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#30340;&#35843;&#26597;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;&#36825;&#20123;&#25216;&#26415;&#26159;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#20026;&#23558;&#26469;&#22312;&#35838;&#22530;&#19978;&#35780;&#20272;&#36825;&#31181;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25972;&#20307;&#25928;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the use of Generative AI chatbots for transforming public science exhibits into virtual experiences that can extend the engagement of exhibits into the classroom. The broader goal is to increase accessibility of science exhibits, especially for those marginalized in STEM due to various factors, including cultural barriers. We hypothesize that turning exhibits into first-person anthropomorphized chatbots with a personality, like quirky-talking asteroids or comets, can increase engagement and learning. The paper mainly explores if such techniques are possible using Generative AI (e.g. GPT) via prompt engineering alone. The research includes an investigation into the possibility of integrating interactive assessment via question-generation using Bloom's Taxonomy. Initial results indicate that it is possible to combine these techniques. As such, it lays a foundation for future classroom evaluations of such chatbots to gauge their overall efficacy in extending the reach 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;LLMs&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#65292;&#25552;&#20986;&#20102;&#24515;&#29702;&#23398;&#20998;&#31867;&#27861;&#65292;&#20197;&#26356;&#35814;&#32454;&#22320;&#29702;&#35299;&#21644;&#35299;&#20915;LLMs&#36755;&#20986;&#35823;&#23548;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#22788;&#29702;&#31867;&#20284;&#25361;&#25112;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#31574;&#30053;&#20197;&#20943;&#36731;LLMs&#20013;&#30340;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.01769</link><description>&lt;p&gt;
&#37325;&#26032;&#23450;&#20041;LLMs&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#65306;&#26500;&#24314;&#24515;&#29702;&#23398;&#20026;&#22522;&#30784;&#30340;&#20943;&#36731;&#35823;&#23548;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Redefining "Hallucination" in LLMs: Towards a psychology-informed framework for mitigating misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;LLMs&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#65292;&#25552;&#20986;&#20102;&#24515;&#29702;&#23398;&#20998;&#31867;&#27861;&#65292;&#20197;&#26356;&#35814;&#32454;&#22320;&#29702;&#35299;&#21644;&#35299;&#20915;LLMs&#36755;&#20986;&#35823;&#23548;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#22788;&#29702;&#31867;&#20284;&#25361;&#25112;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#31574;&#30053;&#20197;&#20943;&#36731;LLMs&#20013;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20363;&#22914;ChatGPT&#24050;&#32463;&#34987;&#36229;&#36807;&#21313;&#20159;&#30340;&#29992;&#25143;&#20351;&#29992;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#36923;&#36753;&#33021;&#21147;&#65292;&#20294;&#22312;&#8220;&#24187;&#35273;&#8221;&#26041;&#38754;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#25361;&#25112;&#12290;&#36825;&#19968;&#29616;&#35937;&#23548;&#33268;LLMs&#20197;&#33258;&#20449;&#30340;&#26041;&#24335;&#36755;&#20986;&#35823;&#23548;&#20449;&#24687;&#65292;&#32780;&#36825;&#21487;&#20197;&#22312;&#22914;&#27492;&#24222;&#22823;&#30340;&#29992;&#25143;&#32676;&#20307;&#20013;&#20135;&#29983;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;LLMs&#20013;&#20351;&#29992;&#8220;&#24187;&#35273;&#8221;&#19968;&#35789;&#30340;&#36866;&#24403;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#35748;&#30693;&#20559;&#24046;&#21644;&#20854;&#20182;&#24515;&#29702;&#29616;&#35937;&#30340;&#24515;&#29702;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#36825;&#19968;&#29616;&#35937;&#26356;&#35814;&#32454;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#33021;&#22815;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#20869;&#37096;&#35299;&#20915;&#31867;&#20284;&#25361;&#25112;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#31574;&#30053;&#26469;&#20943;&#36731;LLMs&#30340;&#24187;&#35273;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26088;&#22312;&#36229;&#36234;&#20256;&#32479;&#30340;&#26415;&#35821;&#65292;&#20026;&#28145;&#20837;&#29702;&#35299;&#21644;&#21487;&#25805;&#20316;&#30340;&#36335;&#24452;&#25552;&#20379;&#32454;&#33268;&#20837;&#24494;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have become incredibly popular, with ChatGPT for example being used by over a billion users. While these models exhibit remarkable language understanding and logical prowess, a notable challenge surfaces in the form of "hallucinations." This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base. However, we question the appropriateness of the term "hallucination" in LLMs, proposing a psychological taxonomy based on cognitive biases and other psychological phenomena. Our approach offers a more fine-grained understanding of this phenomenon, allowing for targeted solutions. By leveraging insights from how humans internally resolve similar challenges, we aim to develop strategies to mitigate LLM hallucinations. This interdisciplinary approach seeks to move beyond conventional terminology, providing a nuanced understanding and actionable pathways for imp
&lt;/p&gt;</description></item><item><title>HiQA&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01767</link><description>&lt;p&gt;
HiQA&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#30340;&#20998;&#23618;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;RAG&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01767
&lt;/p&gt;
&lt;p&gt;
HiQA&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#36805;&#36895;&#21457;&#23637;&#65292;&#20351;&#29992;&#34917;&#20805;&#25991;&#26723;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#23398;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31181;&#36827;&#27493;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#31572;&#36136;&#37327;&#65292;&#24182;&#20943;&#36731;&#20102;&#24187;&#35273;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#22823;&#37327;&#26080;&#27861;&#21306;&#20998;&#30340;&#25991;&#26723;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26377;&#38480;&#65292;&#32473;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#38024;&#23545;&#36825;&#20123;&#26032;&#20852;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiQA&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MDQA&#65289;&#26694;&#26550;&#65292;&#23558;&#32423;&#32852;&#30340;&#20803;&#25968;&#25454;&#25972;&#21512;&#21040;&#20869;&#23481;&#20013;&#65292;&#21516;&#26102;&#20855;&#22791;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;MasQA&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#21644;&#30740;&#31350;MDQA&#12290;&#26368;&#21518;&#65292;HiQA&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01766</link><description>&lt;p&gt;
LLM&#25237;&#31080;&#65306;&#20154;&#31867;&#36873;&#25321;&#21644;AI&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LLM Voting: Human Choices and AI Collective Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#19982;&#20154;&#31867;&#25237;&#31080;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36827;&#34892;&#20154;&#31867;&#25237;&#31080;&#23454;&#39564;&#20197;&#24314;&#31435;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;LLM&#20195;&#29702;&#36827;&#34892;&#24179;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38598;&#20307;&#32467;&#26524;&#21644;&#20010;&#20307;&#20559;&#22909;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;LLMs&#20043;&#38388;&#22312;&#20915;&#31574;&#21644;&#22266;&#26377;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#20559;&#22909;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30456;&#27604;&#20154;&#31867;&#36873;&#27665;&#30340;&#22810;&#26679;&#20559;&#22909;&#65292;LLMs&#26377;&#26356;&#36235;&#21521;&#20110;&#19968;&#33268;&#36873;&#25321;&#30340;&#20542;&#21521;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#27169;&#25311;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#27169;&#25311;&#30340;&#20154;&#26684;&#29305;&#36136;&#21450;&#31283;&#23450;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;LLMs&#22312;&#20010;&#24615;&#21270;&#20154;&#26426;&#20132;&#20114;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01765</link><description>&lt;p&gt;
LLMs &#27169;&#25311;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#65306;&#36827;&#19968;&#27493;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
LLMs Simulate Big Five Personality Traits: Further Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#27169;&#25311;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#27169;&#25311;&#30340;&#20154;&#26684;&#29305;&#36136;&#21450;&#31283;&#23450;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;LLMs&#22312;&#20010;&#24615;&#21270;&#20154;&#26426;&#20132;&#20114;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Llama2&#12289;GPT4&#21644;Mixtral&#23545;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#27169;&#25311;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#27169;&#25311;&#30340;&#20154;&#26684;&#29305;&#36136;&#21450;&#20854;&#31283;&#23450;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#26356;&#24191;&#27867;&#22320;&#20102;&#35299;LLMs&#27169;&#25311;&#20154;&#26684;&#29305;&#36136;&#30340;&#33021;&#21147;&#20197;&#21450;&#23545;&#20010;&#24615;&#21270;&#20154;&#26426;&#20132;&#20114;&#30340;&#30456;&#20851;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
An empirical investigation into the simulation of the Big Five personality traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is presented. We analyze the personality traits simulated by these models and their stability. This contributes to the broader understanding of the capabilities of LLMs to simulate personality traits and the respective implications for personalized human-computer interaction.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01763</link><description>&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#19978;&#21521;&#37327;&#25968;&#25454;&#24211;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Large Language Models Meet Vector Databases: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#22312;&#20154;&#31867;&#25991;&#23383;&#22788;&#29702;&#21644;&#29983;&#25104;&#26041;&#38754;&#24320;&#21551;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#21253;&#25324;&#24187;&#35273;&#12289;&#20559;&#35265;&#12289;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#20197;&#21450;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#32500;&#25252;&#30340;&#39640;&#25104;&#26412;&#31561;&#37325;&#35201;&#25361;&#25112;&#12290;&#32780;&#21478;&#19968;&#31181;&#26085;&#30410;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#21521;&#37327;&#25968;&#25454;&#24211;&#21017;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25797;&#38271;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#20110;&#39640;&#25928;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#20041;&#25628;&#32034;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#65292;&#23427;&#20204;&#26174;&#33879;&#22686;&#24378;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#32780;&#29420;&#29305;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24320;&#21457;&#38750;&#20891;&#20107;&#24212;&#29992;&#20013;&#21487;&#33021;&#34987;&#29992;&#20110;&#20914;&#31361;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26102;&#30340;&#36947;&#24503;&#36131;&#20219;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#36341;&#26041;&#27861;&#30340;&#25506;&#35752;&#65292;&#24182;&#35748;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#20110;&#21512;&#29702;&#21487;&#39044;&#35265;&#30340;&#31995;&#32479;&#20351;&#29992;&#36127;&#26377;&#36947;&#24503;&#36131;&#20219;&#12290;</title><link>https://arxiv.org/abs/2402.01762</link><description>&lt;p&gt;
&#21830;&#19994;&#20154;&#24037;&#26234;&#33021;&#12289;&#20914;&#31361;&#21644;&#36947;&#24503;&#36131;&#20219;&#65306;&#20851;&#20110;&#21452;&#37325;&#29992;&#36884;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25152;&#28041;&#21450;&#30340;&#36947;&#24503;&#36131;&#20219;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#36341;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Commercial AI, Conflict, and Moral Responsibility: A theoretical analysis and practical approach to the moral responsibilities associated with dual-use AI technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24320;&#21457;&#38750;&#20891;&#20107;&#24212;&#29992;&#20013;&#21487;&#33021;&#34987;&#29992;&#20110;&#20914;&#31361;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26102;&#30340;&#36947;&#24503;&#36131;&#20219;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#36341;&#26041;&#27861;&#30340;&#25506;&#35752;&#65292;&#24182;&#35748;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#20110;&#21512;&#29702;&#21487;&#39044;&#35265;&#30340;&#31995;&#32479;&#20351;&#29992;&#36127;&#26377;&#36947;&#24503;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#24320;&#21457;&#38750;&#20891;&#20107;&#24212;&#29992;&#20013;&#21487;&#33021;&#34987;&#29992;&#20110;&#20914;&#31361;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26102;&#28041;&#21450;&#30340;&#36947;&#24503;&#36131;&#20219;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#36341;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#34920;&#20102;&#19968;&#31181;&#19982;&#20043;&#21069;&#30340;&#21452;&#37325;&#25110;&#22810;&#37325;&#29992;&#36884;&#25216;&#26415;&#19981;&#21516;&#30340;&#20132;&#21449;&#25216;&#26415;&#65292;&#22240;&#20026;&#23427;&#22312;&#20854;&#20182;&#25216;&#26415;&#19978;&#20855;&#26377;&#20056;&#27861;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#20851;&#20110;&#21452;&#37325;&#29992;&#36884;&#25216;&#26415;&#30340;&#36947;&#24503;&#36131;&#20219;&#20998;&#26512;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21442;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29983;&#21629;&#21608;&#26399;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#20110;&#20182;&#20204;&#31995;&#32479;&#30340;&#21512;&#29702;&#21487;&#39044;&#35265;&#30340;&#20351;&#29992;&#36127;&#26377;&#36947;&#24503;&#36131;&#20219;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#65292;&#19968;&#20010;&#34892;&#21160;&#20027;&#20307;&#30340;&#36947;&#24503;&#36131;&#20219;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#20182;&#20204;&#30340;&#24847;&#22270;&#65292;&#25105;&#20204;&#36824;&#24517;&#39035;&#32771;&#34385;&#34892;&#21160;&#20027;&#20307;&#21512;&#29702;&#21487;&#39044;&#35265;&#30340;&#34892;&#21160;&#32467;&#26524;&#65292;&#22914;&#31995;&#32479;&#22312;&#20914;&#31361;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a theoretical analysis and practical approach to the moral responsibilities when developing AI systems for non-military applications that may nonetheless be used for conflict applications. We argue that AI represents a form of crossover technology that is different from previous historical examples of dual- or multi-use technology as it has a multiplicative effect across other technologies. As a result, existing analyses of ethical responsibilities around dual-use technologies do not necessarily work for AI systems. We instead argue that stakeholders involved in the AI system lifecycle are morally responsible for uses of their systems that are reasonably foreseeable. The core idea is that an agent's moral responsibility for some action is not necessarily determined by their intentions alone; we must also consider what the agent could reasonably have foreseen to be potential outcomes of their action, such as the potential use of a system in conflict even when it is n
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01761</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking Interpretability in the Era of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01761
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#39046;&#22495;&#65292;&#21463;&#21040;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23835;&#36215;&#30340;&#25512;&#21160;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20026;&#37325;&#26032;&#24605;&#32771;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#26426;&#20250;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#33021;&#21147;&#20351;&#24471;LLMs&#33021;&#22815;&#25193;&#23637;&#32473;&#20154;&#31867;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19978;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#30340;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#27604;&#22914;&#34394;&#26500;&#30340;&#35299;&#37322;&#21644;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#35780;&#20272;&#26032;&#20852;LLM&#35299;&#37322;&#39046;&#22495;&#30340;&#29616;&#26377;&#26041;&#27861;&#65288;&#21253;&#25324;&#35299;&#37322;LLM&#21644;&#20351;&#29992;LLM&#36827;&#34892;&#35299;&#37322;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;LLMs&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#35299;&#37322;&#24615;&#65292;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#21253;&#25324;&#23545;LLMs&#26412;&#36523;&#30340;&#23457;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.   In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We high
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#30340;AI&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#23548;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#39640;&#20013;&#23398;&#29983;&#21327;&#21516;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#26102;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#20449;&#24687;&#27844;&#38706;&#12289;&#28389;&#29992;&#35821;&#35328;&#21644;&#20844;&#24179;&#24615;&#31561;&#20262;&#29702;&#21644;&#20449;&#20219;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01760</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#30340;AI&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#23548;&#31995;&#32479;&#20013;&#30340;&#20449;&#20219;&#21644;&#20262;&#29702;&#32771;&#34385;&#65306;&#20197;&#21327;&#21516;&#35299;&#20915;&#39764;&#26041;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Trust and ethical considerations in a multi-modal, explainable AI-driven chatbot tutoring system: The case of collaboratively solving Rubik's Cube
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01760
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#30340;AI&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#23548;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#39640;&#20013;&#23398;&#29983;&#21327;&#21516;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#26102;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#20449;&#24687;&#27844;&#38706;&#12289;&#28389;&#29992;&#35821;&#35328;&#21644;&#20844;&#24179;&#24615;&#31561;&#20262;&#29702;&#21644;&#20449;&#20219;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#20174;&#22823;&#37327;&#20851;&#20110;&#23398;&#29983;&#23398;&#20064;&#27169;&#24335;&#30340;&#25968;&#25454;&#20013;&#21457;&#29616;&#27934;&#23519;&#21147;&#30340;&#28508;&#21147;&#65292;&#26377;&#26395;&#25913;&#21464;&#25945;&#32946;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#21644;&#20449;&#20219;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#20294;&#23578;&#26410;&#35299;&#20915;&#12290;&#22312;&#39640;&#20013;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#20013;&#65292;&#31361;&#20986;&#30340;&#20262;&#29702;&#38382;&#39064;&#21253;&#25324;&#25968;&#25454;&#38544;&#31169;&#12289;&#20449;&#24687;&#27844;&#38706;&#12289;&#28389;&#29992;&#35821;&#35328;&#21644;&#20844;&#24179;&#24615;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#20026;&#35299;&#20915;&#39640;&#20013;&#23398;&#29983;&#19982;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#35299;&#20915;&#39764;&#26041;&#30340;&#20262;&#29702;&#21644;&#20449;&#20219;&#38382;&#39064;&#32780;&#26500;&#24314;&#30340;&#25216;&#26415;&#32452;&#20214;&#65288;&#31216;&#20026;ALLURE chatbot&#65289;&#12290;&#22312;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#65292;&#25105;&#20204;&#24076;&#26395;&#30830;&#20445;&#20799;&#31461;&#12289;&#29238;&#27597;&#21644;&#25945;&#24072;&#30340;&#30693;&#24773;&#21516;&#24847;&#22788;&#20110;&#20219;&#20309;&#31649;&#29702;&#30340;&#25968;&#25454;&#30340;&#20013;&#24515;&#20301;&#32622;&#12290;&#30001;&#20110;&#28041;&#21450;&#20799;&#31461;&#65292;&#31995;&#32479;&#33021;&#22815;&#25509;&#21463;&#29992;&#25143;&#21644;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#30340;&#25991;&#26412;&#12289;&#38899;&#39057;&#25110;&#35270;&#35273;&#35821;&#35328;&#65292;&#24182;&#23558;&#20114;&#21160;&#24341;&#23548;&#36828;&#31163;&#21361;&#38505;&#24773;&#20917;&#12290;&#22312;&#20449;&#24687;&#31649;&#29702;&#26041;&#38754;&#65292;&#25105;&#20204;&#36824;&#24076;&#26395;&#30830;&#20445;&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#26426;&#21046;&#38450;&#27490;&#20449;&#24687;&#27844;&#38706;&#30340;&#21361;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has the potential to transform education with its power of uncovering insights from massive data about student learning patterns. However, ethical and trustworthy concerns of AI have been raised but are unsolved. Prominent ethical issues in high school AI education include data privacy, information leakage, abusive language, and fairness. This paper describes technological components that were built to address ethical and trustworthy concerns in a multi-modal collaborative platform (called ALLURE chatbot) for high school students to collaborate with AI to solve the Rubik's cube. In data privacy, we want to ensure that the informed consent of children, parents, and teachers, is at the center of any data that is managed. Since children are involved, language, whether textual, audio, or visual, is acceptable both from users and AI and the system can steer interaction away from dangerous situations. In information management, we also want to ensure that the sys
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#23637;&#31034;&#20102;&#35745;&#31639;&#26041;&#27861;&#22312;&#24189;&#40664;&#39118;&#26684;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#31354;&#30333;&#21644;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01759</link><description>&lt;p&gt;
&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65306;&#29992;&#20110;&#24189;&#40664;&#39118;&#26684;&#20998;&#31867;&#30340;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Systematic Literature Review: Computational Approaches for Humour Style Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#23637;&#31034;&#20102;&#35745;&#31639;&#26041;&#27861;&#22312;&#24189;&#40664;&#39118;&#26684;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#31354;&#30333;&#21644;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21508;&#31181;&#24189;&#40664;&#39118;&#26684;&#23545;&#20110;&#29702;&#35299;&#24189;&#40664;&#30340;&#22810;&#38754;&#24615;&#21450;&#20854;&#22312;&#24515;&#29702;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#39046;&#22495;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#29702;&#35299;&#25581;&#31034;&#20102;&#20381;&#25454;&#25152;&#37319;&#29992;&#30340;&#39118;&#26684;&#65292;&#24189;&#40664;&#21487;&#20197;&#23545;&#20010;&#20154;&#30340;&#20581;&#24247;&#21644;&#20154;&#38469;&#20851;&#31995;&#20135;&#29983;&#27835;&#30103;&#25110;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#19987;&#38376;&#30740;&#31350;&#22522;&#20110;&#35745;&#31639;&#30340;&#24189;&#40664;&#39118;&#26684;&#20998;&#26512;&#30340;&#30740;&#31350;&#20173;&#28982;&#27604;&#36739;&#23569;&#35265;&#65292;&#20294;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#29305;&#21035;&#26159;&#20108;&#20803;&#24189;&#40664;&#21644;&#35773;&#21050;&#35782;&#21035;&#26041;&#38754;&#65292;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#24212;&#29992;&#20110;&#36825;&#20123;&#30456;&#20851;&#20219;&#21153;&#30340;&#35745;&#31639;&#25216;&#26415;&#30340;&#29616;&#29366;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#24189;&#40664;&#39118;&#26684;&#20998;&#26512;&#30340;&#22522;&#26412;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#26377;&#25928;&#22320;&#24341;&#23548;&#24189;&#40664;&#30740;&#31350;&#30340;&#22797;&#26434;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#21162;&#21147;&#30830;&#23450;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding various humour styles is essential for comprehending the multifaceted nature of humour and its impact on fields such as psychology and artificial intelligence. This understanding has revealed that humour, depending on the style employed, can either have therapeutic or detrimental effects on an individual's health and relationships. Although studies dedicated exclusively to computational-based humour style analysis remain somewhat rare, an expansive body of research thrives within related task, particularly binary humour and sarcasm recognition. In this systematic literature review (SLR), we survey the landscape of computational techniques applied to these related tasks and also uncover their fundamental relevance to humour style analysis. Through this study, we unveil common approaches, illuminate various datasets and evaluation metrics, and effectively navigate the complex terrain of humour research. Our efforts determine potential research gaps and outlined promising di
&lt;/p&gt;</description></item><item><title>Aalap&#26159;&#19968;&#20010;&#22312;&#21360;&#24230;&#27861;&#24459;&#20219;&#21153;&#19978;&#38024;&#23545;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;AI&#21161;&#25163;&#65292;&#30456;&#27604;&#20110;gpt-3.5-turbo&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20027;&#35201;&#25945;&#25480;&#27861;&#24459;&#25512;&#29702;&#65292;&#23545;&#24459;&#24072;&#12289;&#27861;&#23448;&#25110;&#22312;&#27861;&#24459;&#31995;&#32479;&#20013;&#24037;&#20316;&#30340;&#20154;&#26377;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.01758</link><description>&lt;p&gt;
Aalap&#65306;&#21360;&#24230;&#27861;&#24459;&#21644;&#27861;&#24459;&#21161;&#29702;&#21151;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Aalap: AI Assistant for Legal &amp; Paralegal Functions in India
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01758
&lt;/p&gt;
&lt;p&gt;
Aalap&#26159;&#19968;&#20010;&#22312;&#21360;&#24230;&#27861;&#24459;&#20219;&#21153;&#19978;&#38024;&#23545;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;AI&#21161;&#25163;&#65292;&#30456;&#27604;&#20110;gpt-3.5-turbo&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20027;&#35201;&#25945;&#25480;&#27861;&#24459;&#25512;&#29702;&#65292;&#23545;&#24459;&#24072;&#12289;&#27861;&#23448;&#25110;&#22312;&#27861;&#24459;&#31995;&#32479;&#20013;&#24037;&#20316;&#30340;&#20154;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#20219;&#21153;&#20013;&#20351;&#29992;&#19987;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12289;&#39046;&#22495;&#25968;&#25454;&#24322;&#26500;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#22797;&#26434;&#24615;&#21644;&#39046;&#22495;&#30446;&#26631;&#29420;&#29305;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;Aalap&#65292;&#23427;&#26159;&#22312;&#19982;&#21360;&#24230;&#29305;&#23450;&#27861;&#24459;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#20196;&#25968;&#25454;&#19978;&#32463;&#36807;&#24494;&#35843;&#30340;Mistral 7B&#27169;&#22411;&#12290; Aalap&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#25968;&#25454;&#20013;&#27604;gpt-3.5-turbo&#34920;&#29616;&#26356;&#22909;&#30340;&#27604;&#20363;&#20026;31&#65285;&#65292;&#22312;34&#65285;&#30340;&#27979;&#35797;&#25968;&#25454;&#20013;&#19982;GPT4&#35780;&#20272;&#24471;&#20998;&#30456;&#24403;&#12290; Aalap&#30340;&#35757;&#32451;&#20027;&#35201;&#20391;&#37325;&#20110;&#25945;&#25480;&#27861;&#24459;&#25512;&#29702;&#32780;&#19981;&#26159;&#27861;&#24459;&#35760;&#24518;&#12290; Aalap&#23545;&#24459;&#24072;&#12289;&#27861;&#23448;&#25110;&#22312;&#27861;&#24459;&#31995;&#32479;&#20013;&#24037;&#20316;&#30340;&#20154;&#30340;&#26085;&#24120;&#27963;&#21160;&#32943;&#23450;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using proprietary Large Language Models on legal tasks poses challenges due to data privacy issues, domain data heterogeneity, domain knowledge sophistication, and domain objectives uniqueness. We created Aalalp, a fine-tuned Mistral 7B model on instructions data related to specific Indian legal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31\% of our test data and obtains an equivalent score in 34\% of the test data as evaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoning rather than legal recall. Aalap is definitely helpful for the day-to-day activities of lawyers, judges, or anyone working in legal systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#38899;&#39057;&#26469;&#36776;&#21035;&#20702;&#20316;&#21644;&#20167;&#24680;&#35328;&#35770;&#22312;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#35780;&#20272;&#35270;&#39057;&#26159;&#21542;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#20197;&#21450;&#26816;&#27979;&#20854;&#20013;&#26159;&#21542;&#23384;&#22312;&#20167;&#24680;&#35328;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01752</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#38899;&#39057;&#35782;&#21035;&#36785;&#39554;&#35328;&#35770;&#21644;&#20551;&#28040;&#24687;&#65292;&#36776;&#21035;&#20702;&#20316;&#21644;&#20167;&#24680;&#35328;&#35770;&#22312;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;
&lt;/p&gt;
&lt;p&gt;
Identifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#38899;&#39057;&#26469;&#36776;&#21035;&#20702;&#20316;&#21644;&#20167;&#24680;&#35328;&#35770;&#22312;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#35780;&#20272;&#35270;&#39057;&#26159;&#21542;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#20197;&#21450;&#26816;&#27979;&#20854;&#20013;&#26159;&#21542;&#23384;&#22312;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
YouTube&#38754;&#20020;&#30528;&#20840;&#29699;&#33539;&#22260;&#20869;&#34394;&#20551;&#20449;&#24687;&#21644;&#20167;&#24680;&#35328;&#35770;&#30340;&#20256;&#25773;&#21361;&#26426;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;YouTube&#24050;&#23454;&#26045;&#20005;&#26684;&#35268;&#23450;&#65292;&#31105;&#27490;&#19978;&#20256;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#25110;&#23459;&#20256;&#20167;&#24680;&#35328;&#35770;&#30340;&#20869;&#23481;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#38477;&#20302;&#20882;&#29359;&#24615;&#33521;&#35821;&#20869;&#23481;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20711;&#20285;&#32599;&#35821;&#20869;&#23481;&#30340;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20943;&#23569;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;&#26292;&#21147;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#24320;&#21457;&#19968;&#20010;&#35780;&#32423;&#31995;&#32479;&#65292;&#36890;&#36807;&#27604;&#36739;&#26631;&#39064;&#21644;&#25551;&#36848;&#19982;&#38899;&#39057;&#20869;&#23481;&#65292;&#35780;&#20272;&#35270;&#39057;&#26159;&#21542;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#65292;&#24182;&#26816;&#27979;&#20854;&#20013;&#26159;&#21542;&#23384;&#22312;&#20167;&#24680;&#35328;&#35770;&#12290;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;Pytube&#24211;&#36827;&#34892;&#38899;&#39057;&#25552;&#21462;&#65292;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#36716;&#24405;&#65292;&#20351;&#29992;distilroberta-base&#27169;&#22411;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
YouTube faces a global crisis with the dissemination of false information and hate speech. To counter these issues, YouTube has implemented strict rules against uploading content that includes false information or promotes hate speech. While numerous studies have been conducted to reduce offensive English-language content, there's a significant lack of research on Sinhala content. This study aims to address the aforementioned gap by proposing a solution to minimize the spread of violence and misinformation in Sinhala YouTube videos. The approach involves developing a rating system that assesses whether a video contains false information by comparing the title and description with the audio content and evaluating whether the video includes hate speech. The methodology encompasses several steps, including audio extraction using the Pytube library, audio transcription via the fine-tuned Whisper model, hate speech detection employing the distilroberta-base model and a text classification L
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;Bard&#22312;&#35782;&#21035;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#31215;&#26497;&#35782;&#21035;AD&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#65292;&#20294;&#21487;&#33021;&#20250;&#23558;CN&#38169;&#35823;&#22320;&#35782;&#21035;&#20026;AD&#12290;&#23545;&#20110;&#31215;&#26497;&#35782;&#21035;CN&#65292;GPT-4&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#30495;&#38452;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01751</link><description>&lt;p&gt;
ChatGPT&#19982;Bard&#22312;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#30340;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;Bard&#22312;&#35782;&#21035;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#31215;&#26497;&#35782;&#21035;AD&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#65292;&#20294;&#21487;&#33021;&#20250;&#23558;CN&#38169;&#35823;&#22320;&#35782;&#21035;&#20026;AD&#12290;&#23545;&#20110;&#31215;&#26497;&#35782;&#21035;CN&#65292;GPT-4&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#30495;&#38452;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#26377;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#20010;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;ChatGPT-3.5&#65292;ChatGPT-4&#21644;Bard&#65289;&#22312;&#20854;&#24403;&#21069;&#20844;&#24320;&#24418;&#24335;&#19979;&#65292;&#20351;&#29992;&#20174;&#33258;&#21457;&#35821;&#38899;&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#25991;&#26412;&#36755;&#20837;&#65292;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#65288;AD&#65289;&#21644;&#35748;&#30693;&#27491;&#24120;&#65288;CN&#65289;&#20010;&#20307;&#36827;&#34892;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;&#37319;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#29420;&#31435;&#26597;&#35810;&#32423;&#21035;&#19978;&#36827;&#34892;&#65292;&#31532;&#20108;&#20010;&#26597;&#35810;&#65288;&#24605;&#32500;&#38142;&#24341;&#23548;&#65289;&#27604;&#31532;&#19968;&#20010;&#26597;&#35810;&#20135;&#29983;&#26356;&#35814;&#32454;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#35780;&#20272;&#27599;&#20010;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20934;&#30830;&#24230;&#12289;&#25935;&#24863;&#24230;&#12289;&#29305;&#24322;&#24230;&#12289;&#31934;&#30830;&#24230;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#29983;&#25104;&#30340;&#39044;&#27979;&#26469;&#35780;&#20272;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#20102;&#19977;&#31867;&#32467;&#26524;&#65288;"AD"&#65292;"CN"&#25110;"Unsure"&#65289;&#12290;&#22312;&#31215;&#26497;&#35782;&#21035;AD&#26102;&#65292;Bard&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;&#30495;&#38451;&#24615;&#65288;89%&#30340;&#21484;&#22238;&#29575;&#65289;&#21644;&#26368;&#39640;&#30340;F1&#24471;&#20998;&#65288;71%&#65289;&#65292;&#20294;&#20542;&#21521;&#20110;&#23558;CN&#38169;&#35823;&#22320;&#35782;&#21035;&#20026;AD&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#32622;&#20449;&#24230;&#65288;&#36739;&#20302;&#30340;"Unsure"&#29575;&#65289;&#65307;&#22312;&#31215;&#26497;&#35782;&#21035;CN&#26102;&#65292;GPT-4&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;&#30495;&#38452;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) find increasing applications in many fields. Here, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in their current form, as publicly available - for their ability to recognize Alzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual input derived from spontaneous speech recordings. Zero-shot learning approach is used at two levels of independent queries, with the second query (chain-of-thought prompting) eliciting more detailed than the first. Each LLM chatbot's performance is evaluated on the prediction generated in terms of accuracy, sensitivity, specificity, precision and F1 score. LLM chatbots generated three-class outcome ("AD", "CN", or "Unsure"). When positively identifying AD, Bard produced highest true-positives (89% recall) and highest F1 score (71%), but tended to misidentify CN as AD, with high confidence (low "Unsure" rates); for positively identifying CN, GPT-4 resulted in the highest true-negatives 
&lt;/p&gt;</description></item><item><title>PACE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#26234;&#33021;&#20307;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#23454;&#29992;&#36890;&#20449;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#12289;&#24847;&#22270;&#35299;&#26512;&#21644;&#20197;&#24847;&#22270;&#20026;&#23548;&#21521;&#30340;&#32534;&#30721;&#26469;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#24211;&#34917;&#20805;&#25152;&#38656;&#30693;&#35782;&#65292;&#24341;&#20837;&#19987;&#29992;&#25552;&#31034;&#26469;&#20419;&#36827;&#23545;&#23454;&#29992;&#36890;&#20449;&#22330;&#26223;&#21644;&#20219;&#21153;&#35201;&#27714;&#30340;&#29702;&#35299;&#65292;&#24182;&#35774;&#35745;&#24605;&#32500;&#38142;&#20197;&#24110;&#21161;&#26435;&#34913;&#20256;&#36755;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.01750</link><description>&lt;p&gt;
PACE&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#30340;&#23454;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01750
&lt;/p&gt;
&lt;p&gt;
PACE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#26234;&#33021;&#20307;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#23454;&#29992;&#36890;&#20449;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#12289;&#24847;&#22270;&#35299;&#26512;&#21644;&#20197;&#24847;&#22270;&#20026;&#23548;&#21521;&#30340;&#32534;&#30721;&#26469;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#24211;&#34917;&#20805;&#25152;&#38656;&#30693;&#35782;&#65292;&#24341;&#20837;&#19987;&#29992;&#25552;&#31034;&#26469;&#20419;&#36827;&#23545;&#23454;&#29992;&#36890;&#20449;&#22330;&#26223;&#21644;&#20219;&#21153;&#35201;&#27714;&#30340;&#29702;&#35299;&#65292;&#24182;&#35774;&#35745;&#24605;&#32500;&#38142;&#20197;&#24110;&#21161;&#26435;&#34913;&#20256;&#36755;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#36890;&#20449;&#25216;&#26415;&#22312;&#29702;&#35770;&#23481;&#37327;&#12289;&#39057;&#35889;&#21487;&#29992;&#24615;&#21644;&#21151;&#32791;&#36164;&#28304;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#23454;&#29992;&#36890;&#20449;&#21033;&#29992;&#32456;&#31471;&#26234;&#33021;&#36827;&#34892;&#36873;&#25321;&#24615;&#25968;&#25454;&#20256;&#36755;&#65292;&#25552;&#20379;&#36164;&#28304;&#33410;&#32422;&#12290;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#36890;&#29992;&#24847;&#22270;&#35299;&#26512;&#24037;&#20855;&#65292;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#26234;&#33021;&#20307;&#65288;PACE&#65289;&#30340;&#22270;&#20687;&#23454;&#29992;&#36890;&#20449;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;PACE&#20381;&#27425;&#25191;&#34892;&#35821;&#20041;&#24863;&#30693;&#12289;&#24847;&#22270;&#35299;&#26512;&#21644;&#20197;&#24847;&#22270;&#20026;&#23548;&#21521;&#30340;&#32534;&#30721;&#12290;&#20026;&#20102;&#30830;&#20445;&#22312;&#36890;&#20449;&#20013;&#26377;&#25928;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#30693;&#35782;&#24211;&#26469;&#34917;&#20805;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#24341;&#20837;&#20102;&#19987;&#29992;&#25552;&#31034;&#26469;&#20419;&#36827;&#23545;&#23454;&#29992;&#36890;&#20449;&#22330;&#26223;&#21644;&#20219;&#21153;&#35201;&#27714;&#30340;&#29702;&#35299;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#26465;&#24605;&#32500;&#38142;&#20197;&#24110;&#21161;&#22312;&#20256;&#36755;&#25928;&#29575;&#21644;&#36136;&#37327;&#20043;&#38388;&#36827;&#34892;&#21512;&#29702;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current communication technologies face limitations in terms of theoretical capacity, spectrum availability, and power resources. Pragmatic communication, leveraging terminal intelligence for selective data transmission, offers resource conservation. Existing research lacks universal intention resolution tools, limiting applicability to specific tasks. This paper proposes an image pragmatic communication framework based on a Pragmatic Agent for Communication Efficiency (PACE) using Large Language Models (LLM). In this framework, PACE sequentially performs semantic perception, intention resolution, and intention-oriented coding. To ensure the effective utilization of LLM in communication, a knowledge base is designed to supplement the necessary knowledge, dedicated prompts are introduced to facilitate understanding of pragmatic communication scenarios and task requirements, and a chain of thought is designed to assist in making reasonable trade-offs between transmission efficiency and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#22478;&#24066;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#28165;&#26224;&#30340;&#23450;&#20041;&#21644;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.01749</link><description>&lt;p&gt;
&#36808;&#21521;&#22478;&#24066;&#26234;&#33021;&#65306;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#22478;&#24066;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#28165;&#26224;&#30340;&#23450;&#20041;&#21644;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29616;&#24050;&#25104;&#20026;&#26234;&#33021;&#22478;&#24066;&#26381;&#21153;&#36827;&#27493;&#30340;&#26680;&#24515;&#65292;&#23545;&#25552;&#39640;&#22478;&#24066;&#29615;&#22659;&#30340;&#25928;&#29575;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#23452;&#23621;&#24615;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;ChatGPT&#31561;&#22522;&#30784;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#20010;&#38761;&#21629;&#24615;&#30340;&#36716;&#21464;&#12290;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#38382;&#39064;&#35299;&#20915;&#21644;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#33021;&#21147;&#34920;&#26126;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#25972;&#21512;&#21040;&#22478;&#24066;&#39046;&#22495;&#20013;&#21487;&#33021;&#23545;&#26234;&#33021;&#22478;&#24066;&#30340;&#21457;&#23637;&#20135;&#29983;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;&#23613;&#31649;&#23545;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#65288;UFMs&#65289;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20294;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#28165;&#26224;&#30340;&#23450;&#20041;&#12289;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#21644;&#21487;&#26222;&#36941;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;UFM&#30340;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20102;&#26500;&#24314;&#23427;&#20204;&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#24403;&#21069;&#19982;UFM&#30456;&#20851;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques are now integral to the advancement of intelligent urban services, playing a crucial role in elevating the efficiency, sustainability, and livability of urban environments. The recent emergence of foundation models such as ChatGPT marks a revolutionary shift in the fields of machine learning and artificial intelligence. Their unparalleled capabilities in contextual understanding, problem solving, and adaptability across a wide range of tasks suggest that integrating these models into urban domains could have a transformative impact on the development of smart cities. Despite growing interest in Urban Foundation Models~(UFMs), this burgeoning field faces challenges such as a lack of clear definitions, systematic reviews, and universalizable solutions. To this end, this paper first introduces the concept of UFM and discusses the unique challenges involved in building them. We then propose a data-centric taxonomy that categorizes current UFM-related works, base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.01748</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22522;&#30784;&#27169;&#22411;&#34987;&#23459;&#31216;&#20026;6G&#31995;&#32479;&#30340;&#25913;&#21464;&#32773;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26080;&#32447;&#32593;&#32476;&#30340;LLMs&#30340;&#21162;&#21147;&#20165;&#38480;&#20110;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#20197;&#26080;&#32447;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#37326;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;(AI)&#21407;&#29983;&#32593;&#32476;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#22522;&#20110;NLP&#30340;&#22522;&#30784;&#27169;&#22411;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#20419;&#36827;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#30340;&#35774;&#35745;&#65306;1) &#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#65292;2) &#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;3) &#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20197;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
&lt;/p&gt;</description></item><item><title>3DG&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#19977;&#32500;&#24352;&#37327;&#24182;&#32467;&#21512;&#24352;&#37327;&#20998;&#35299;&#21644;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#26469;&#22788;&#29702;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23398;&#20064;&#32773;&#24615;&#33021;&#31232;&#30095;&#25968;&#25454;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01746</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#22788;&#29702;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#23398;&#20064;&#32773;&#24615;&#33021;&#31232;&#30095;&#25968;&#25454;&#30340;3DG&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
3DG: A Framework for Using Generative AI for Handling Sparse Learner Performance Data From Intelligent Tutoring Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01746
&lt;/p&gt;
&lt;p&gt;
3DG&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#19977;&#32500;&#24352;&#37327;&#24182;&#32467;&#21512;&#24352;&#37327;&#20998;&#35299;&#21644;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#26469;&#22788;&#29702;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23398;&#20064;&#32773;&#24615;&#33021;&#31232;&#30095;&#25968;&#25454;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32773;&#30340;&#23398;&#20064;&#34920;&#29616;&#25968;&#25454;&#65288;&#22914;&#27979;&#39564;&#20998;&#25968;&#21644;&#23581;&#35797;&#27425;&#25968;&#65289;&#23545;&#20110;&#29702;&#35299;&#23398;&#20064;&#32773;&#21442;&#19982;&#24230;&#21644;&#30693;&#35782;&#25484;&#25569;&#27700;&#24179;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20174;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#25910;&#38598;&#30340;&#23398;&#20064;&#34920;&#29616;&#25968;&#25454;&#24120;&#24120;&#31232;&#30095;&#65292;&#24433;&#21709;&#23398;&#20064;&#32773;&#24314;&#27169;&#21644;&#30693;&#35782;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;3DG&#26694;&#26550;&#65288;&#19977;&#32500;&#24352;&#37327;&#31264;&#23494;&#29983;&#25104;&#19982;&#29983;&#25104;&#24615;&#27169;&#22411;&#32467;&#21512;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#24352;&#37327;&#20998;&#35299;&#21644;&#20808;&#36827;&#29983;&#25104;&#24615;&#27169;&#22411;&#65288;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#25968;&#25454;&#22635;&#34917;&#21644;&#25193;&#20805;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#19977;&#32500;&#24352;&#37327;&#65292;&#25429;&#25417;&#23398;&#20064;&#32773;&#12289;&#38382;&#39064;&#21644;&#23581;&#35797;&#27425;&#25968;&#36825;&#19977;&#20010;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#24352;&#37327;&#20998;&#35299;&#23545;&#25968;&#25454;&#36827;&#34892;&#31264;&#23494;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;&#38024;&#23545;&#20010;&#20307;&#23398;&#20064;&#27169;&#24335;&#30340;&#32858;&#31867;&#36827;&#34892;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#25193;&#20805;&#12290;&#24212;&#29992;&#20110;AutoTutor&#30340;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning performance data (e.g., quiz scores and attempts) is significant for understanding learner engagement and knowledge mastery level. However, the learning performance data collected from Intelligent Tutoring Systems (ITSs) often suffers from sparsity, impacting the accuracy of learner modeling and knowledge assessments. To address this, we introduce the 3DG framework (3-Dimensional tensor for Densification and Generation), a novel approach combining tensor factorization with advanced generative models, including Generative Adversarial Network (GAN) and Generative Pre-trained Transformer (GPT), for enhanced data imputation and augmentation. The framework operates by first representing the data as a three-dimensional tensor, capturing dimensions of learners, questions, and attempts. It then densifies the data through tensor factorization and augments it using Generative AI models, tailored to individual learning patterns identified via clustering. Applied to data from an AutoTutor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01744</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22270;&#35299;&#37322;&#25581;&#31034;&#20998;&#23376;&#25104;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unveiling Molecular Moieties through Hierarchical Graph Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#25903;&#25345;&#20307;&#22806;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#24050;&#32463;&#20986;&#29616;&#22810;&#24180;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#26550;&#26500;&#23454;&#29616;&#39640;&#31934;&#24230;&#22810;&#38774;&#26631;&#31579;&#36873;&#30340;GNN&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#22312;&#21407;&#23376;&#12289;&#29615;&#21644;&#25972;&#20010;&#20998;&#23376;&#23618;&#38754;&#19978;&#30452;&#25509;&#25429;&#33719;&#20449;&#24687;&#65292;&#20174;&#32780;&#25214;&#21040;&#19982;&#29983;&#29289;&#27963;&#24615;&#39044;&#27979;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#25903;&#25345;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#30340;&#20108;&#21313;&#20010;&#32454;&#32990;&#21608;&#26399;&#20381;&#36182;&#24615;&#28608;&#37238;&#38774;&#26631;&#19978;&#25253;&#36947;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;GNN&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#36229;&#36234;&#20102;&#20316;&#32773;&#25552;&#20986;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20165;&#38024;&#23545;CDK1&#30340;&#39640;&#28789;&#25935;&#24230;&#29256;&#26412;&#30340;GNN&#65292;&#20197;&#20351;&#29992;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#26469;&#36991;&#20813;&#22810;&#31867;&#21035;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#24046;&#12290;&#20998;&#23618;&#35299;&#37322;&#22120;&#24050;&#32463;&#30001;&#19968;&#20301;&#19987;&#23478;&#21270;&#23398;&#23478;&#22312;19&#20010;CDK1&#25209;&#20934;&#33647;&#29289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#20351;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#26102;&#23384;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;LLMs&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#65292;&#24182;&#19988;&#26080;&#27861;&#20445;&#35777;&#24615;&#33021;&#12290;&#35813;&#25253;&#21578;&#24378;&#35843;&#20102;&#29702;&#24615;&#38519;&#38449;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#25361;&#25112;&#65292;&#24182;&#23545;&#28508;&#22312;&#30340;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.01743</link><description>&lt;p&gt;
&#29702;&#24615;&#38519;&#38449;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The Reasoning Under Uncertainty Trap: A Structural AI Risk
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01743
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20351;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#26102;&#23384;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;LLMs&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#65292;&#24182;&#19988;&#26080;&#27861;&#20445;&#35777;&#24615;&#33021;&#12290;&#35813;&#25253;&#21578;&#24378;&#35843;&#20102;&#29702;&#24615;&#38519;&#38449;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#25361;&#25112;&#65292;&#24182;&#23545;&#28508;&#22312;&#30340;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#30740;&#31350;&#20102;&#24403;&#21069;&#65288;&#20197;&#21450;&#39044;&#27979;&#65289;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#25152;&#24102;&#26469;&#30340;&#19968;&#31181;&#26032;&#22411;&#39118;&#38505;&#12290;&#22312;&#23545;&#26410;&#26469;&#34892;&#21160;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#20851;&#38190;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#38754;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#23545;&#20110;&#31867;&#20284;LLMs&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#26469;&#36741;&#21161;&#20915;&#31574;&#32773;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#22312;&#35777;&#26126;&#20102;&#36825;&#31181;&#38656;&#27714;&#21450;&#20854;&#32972;&#21518;&#30340;&#21160;&#26426;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#19981;&#26029;&#22686;&#21152;&#30340;&#39118;&#38505;&#65306;1&#65289;&#25105;&#20204;&#30446;&#21069;&#23545;LLMs&#22312;&#36825;&#26041;&#38754;&#30340;&#33021;&#21147;&#20102;&#35299;&#19981;&#22815;&#65292;2&#65289;&#22312;&#22522;&#26412;&#30340;&#35745;&#31639;&#29190;&#28856;&#24615;&#21644;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#32422;&#26463;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26080;&#27861;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#26412;&#25253;&#21578;&#38416;&#36848;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#29702;&#24615;&#38519;&#38449;&#19979;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25226;&#36825;&#20123;&#22256;&#38590;&#19982;&#28508;&#22312;&#30340;&#20154;&#24037;&#26234;&#33021;&#26102;&#38388;&#34920;&#21644;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;&#30830;&#31435;&#20102;&#24403;&#21069;&#28508;&#22312;&#30340;&#35823;&#29992;&#39118;&#38505;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#36825;&#31181;&#30475;&#20284;&#32047;&#21152;&#30340;&#39118;&#38505;&#65288;&#36234;&#22810;&#30340;&#35823;&#29992;&#21152;&#21095;&#20102;&#28508;&#22312;&#30340;&#21361;&#23475;&#65289;&#23454;&#38469;&#19978;&#26377;&#22810;&#37325;&#30340;
&lt;/p&gt;
&lt;p&gt;
This report examines a novel risk associated with current (and projected) AI tools. Making effective decisions about future actions requires us to reason under uncertainty (RUU), and doing so is essential to many critical real world problems. Overfaced by this challenge, there is growing demand for AI tools like LLMs to assist decision-makers. Having evidenced this demand and the incentives behind it, we expose a growing risk: we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy. This report provides an exposition of what makes RUU so challenging for both humans and machines, and relates these difficulties to prospective AI timelines and capabilities. Having established this current potential misuse risk, we go on to expose how this seemingly additive risk (more misuse additively contributed to potential harm) in fact has multipl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;LLM&#20351;&#29992;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#36755;&#20986;&#36136;&#37327;&#24182;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#22312;&#36136;&#37327;&#21644;&#24310;&#36831;&#26041;&#38754;&#20445;&#25345;&#25104;&#26412;&#22312;&#39044;&#31639;&#33539;&#22260;&#20869;&#25110;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.01742</link><description>&lt;p&gt;
&#20248;&#21270;LLM&#20351;&#29992;&#25104;&#26412;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Optimizing the Costs of LLM Usage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;LLM&#20351;&#29992;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#36755;&#20986;&#36136;&#37327;&#24182;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#22312;&#36136;&#37327;&#21644;&#24310;&#36831;&#26041;&#38754;&#20445;&#25345;&#25104;&#26412;&#22312;&#39044;&#31639;&#33539;&#22260;&#20869;&#25110;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;LLM&#22312;&#29616;&#20170;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25991;&#20214;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#38382;&#31572;&#21644;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;LLM&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#12289;&#25104;&#26412;&#12289;&#26631;&#35760;&#21270;&#21644;&#24310;&#36831;&#12290;&#23454;&#38469;&#19978;&#65292;&#20225;&#19994;&#24050;&#32463;&#22312;&#20026;&#21508;&#33258;&#30340;&#29992;&#20363;&#36816;&#33829;&#25110;&#20351;&#29992;LLM&#32780;&#25215;&#25285;&#24040;&#22823;&#30340;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20272;&#35745;LLM&#30340;&#36755;&#20986;&#36136;&#37327;&#65288;&#32780;&#26080;&#38656;&#23454;&#38469;&#35843;&#29992;LLM&#65289;&#65292;&#28982;&#21518;&#35299;&#20915;LLM&#36873;&#25321;&#30340;&#20248;&#21270;&#20363;&#31243;&#65292;&#20197;&#22312;&#36136;&#37327;&#21644;&#24310;&#36831;&#26041;&#38754;&#20445;&#25345;&#25104;&#26412;&#22312;&#39044;&#31639;&#33539;&#22260;&#20869;&#25110;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;LLM&#22312;&#25688;&#35201;&#31561;&#25991;&#20214;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#36755;&#20986;&#36136;&#37327;&#65292;&#38543;&#21518;&#37319;&#29992;LP&#21462;&#25972;&#31639;&#27861;&#26469;&#20248;&#21270;LLM&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#36136;&#37327;&#21644;&#25104;&#26412;&#20043;&#38388;&#26435;&#34913;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases.   In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sente
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;12&#20010;&#20020;&#24202;&#19987;&#19994;&#20013;&#25552;&#20379;&#23433;&#20840;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23450;&#21046;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.01741</link><description>&lt;p&gt;
&#24320;&#21457;&#24182;&#27979;&#35797;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#33647;&#29289;&#23433;&#20840;&#30340;12&#31181;&#20020;&#24202;&#19987;&#19994;
&lt;/p&gt;
&lt;p&gt;
Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;12&#20010;&#20020;&#24202;&#19987;&#19994;&#20013;&#25552;&#20379;&#23433;&#20840;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23450;&#21046;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#65306;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;-&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#65292;&#29992;&#20110;&#23433;&#20840;&#29992;&#33647;&#22788;&#26041;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20379;&#19982;&#24739;&#32773;&#32972;&#26223;&#21644;&#26426;&#26500;&#25351;&#21335;&#30456;&#20851;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;CDSS&#30340;&#23616;&#38480;&#24615;&#12290;&#30446;&#26631;&#65306;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;LLM&#30340;CDSS&#22312;&#35782;&#21035;&#21508;&#31181;&#21307;&#23398;&#21644;&#22806;&#31185;&#30149;&#20363;&#20013;&#30340;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#20154;&#24037;&#19987;&#23478;&#23567;&#32452;&#36827;&#34892;&#27604;&#36739;&#12290;&#23427;&#36824;&#30740;&#31350;&#20102;&#20020;&#24202;&#21307;&#29983;&#22312;&#19981;&#21516;CDSS&#38598;&#25104;&#26041;&#24335;&#65288;&#21021;&#32423;&#33647;&#24072;&#12289;&#20165;&#22522;&#20110;LLM&#30340;CDSS&#21644;&#20108;&#32773;&#30340;&#32452;&#21512;&#65289;&#20013;&#30340;&#20559;&#22909;&#12290;&#35774;&#35745;&#12289;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#65306;&#21033;&#29992;&#24102;&#26377;GPT-4.0&#30340;RAG&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#28041;&#21450;12&#20010;&#19987;&#19994;&#20013;23&#20010;&#20020;&#24202;&#26696;&#20363;&#30340;61&#20010;&#22788;&#26041;&#38169;&#35823;&#22330;&#26223;&#12290;&#19987;&#23478;&#23567;&#32452;&#20351;&#29992;PCNE&#20998;&#31867;&#21644;NCC MERP&#25351;&#25968;&#35780;&#20272;&#36825;&#20123;&#26696;&#20363;&#12290;&#19977;&#21517;&#21021;&#32423;&#33647;&#24072;&#29420;&#31435;&#23457;&#26680;&#27599;&#20010;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#22788;&#29702;&#24314;&#35758;&#12290;&#26681;&#25454;&#26816;&#26597;&#30340;&#38169;&#35823;&#21644;&#24314;&#35758;&#32534;&#21046;&#20102;&#21453;&#39304;&#25253;&#21578;&#12290; &#28982;&#21518;&#65292;&#19977;&#21517;&#21307;&#29983;&#29420;&#31435;&#23457;&#26680;&#36825;&#20123;&#25253;&#21578;&#65292;&#24182;&#25552;&#20986;&#23545;&#19979;&#19968;&#27493;&#22788;&#29702;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe medication prescription. This model addresses the limitations of traditional rule-based CDSS by providing relevant prescribing error alerts tailored to patient context and institutional guidelines.   Objective: The study evaluates the efficacy of an LLM-based CDSS in identifying medication errors across various medical and surgical case vignettes, compared to a human expert panel. It also examines clinician preferences among different CDSS integration modalities: junior pharmacist, LLM-based CDSS alone, and a combination of both.   Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the study involved 61 prescribing error scenarios within 23 clinical vignettes across 12 specialties. An expert panel assessed these cases using the PCNE classification and NCC MERP index. Three junior pharmacists independently reviewed eac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01740</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#36127;&#33655;&#19979;&#30340;&#34917;&#20607;&#24615;&#20559;&#35265;&#65306;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36873;&#25321;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;gpt-3.5-turbo&#21644;claude-instant-1.2&#22312;&#35299;&#37322;&#21644;&#25191;&#34892;&#35821;&#20041;&#20219;&#21153;&#26041;&#38754;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#20250;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#20010;&#21463;&#21040;&#24433;&#21709;&#26368;&#22823;&#30340;&#26159;&#20174;&#21015;&#34920;&#20013;&#36827;&#34892;&#23545;&#35937;&#36873;&#25321;&#65292;&#36825;&#26159;&#25968;&#23383;&#23548;&#33322;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#22522;&#26412;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#26816;&#26597;&#36825;&#20123;&#20559;&#35265;&#65292;&#24182;&#37327;&#21270;&#20854;&#23545;&#20195;&#34920;&#24615;&#21015;&#34920;&#36873;&#25321;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#25511;&#21046;&#23454;&#39564;&#65292;&#25105;&#20204;&#25805;&#32437;&#20102;&#28201;&#24230;&#12289;&#21015;&#34920;&#38271;&#24230;&#12289;&#23545;&#35937;&#36523;&#20221;&#12289;&#23545;&#35937;&#31867;&#22411;&#12289;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#36825;&#20123;&#20559;&#35265;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23396;&#31435;&#21644;&#27979;&#37327;&#36825;&#20123;&#20559;&#35265;&#23545;&#36873;&#25321;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20559;&#35265;&#32467;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27169;&#22411;&#65292;&#32780;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#24433;&#21709;&#30340;&#31243;&#24230;&#12290;&#30001;&#20110;&#23384;&#22312;&#36739;&#24378;&#30340;&#21021;&#29616;&#25928;&#24212;&#65292;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#20250;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have become instrumental in interpreting and executing semantic-based tasks. Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance. Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we conducted a series of controlled experiments, manipulating temperature, list length, object identity, object type, prompt complexity, and model. This enabled us to isolate and measure the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproprotionately represented in ou
&lt;/p&gt;</description></item><item><title>OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01739</link><description>&lt;p&gt;
OpenMoE&#65306;&#24320;&#28304;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#26089;&#26399;&#21162;&#21147;
&lt;/p&gt;
&lt;p&gt;
OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01739
&lt;/p&gt;
&lt;p&gt;
OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24110;&#21161;&#24320;&#28304;&#31038;&#21306;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;(MoE)&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;OpenMoE&#65292;&#19968;&#31995;&#21015;&#23436;&#20840;&#24320;&#25918;&#28304;&#30721;&#21644;&#21487;&#22797;&#29616;&#30340;&#20165;&#35299;&#30721;&#22120;MoE LLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;650M&#21040;34B&#65292;&#35757;&#32451;&#25968;&#25454;&#36229;&#36807;1T&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#23454;&#65292;MoE-based LLM&#21487;&#20197;&#25552;&#20379;&#27604;&#23494;&#38598;LLM&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;LLM&#24320;&#21457;&#30340;&#28508;&#22312;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#23545;&#25105;&#20204;&#30340;OpenMoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#26426;&#21046;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#19978;&#19979;&#25991;&#26080;&#20851;&#19987;&#19994;&#21270;&#12289;&#26089;&#26399;&#36335;&#30001;&#23398;&#20064;&#21644;&#26411;&#23614;&#38477;&#20302;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#20915;&#31574;&#20027;&#35201;&#22522;&#20110;&#26631;&#35760;ID&#65292;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#24456;&#23567;&#12290;&#26631;&#35760;&#21040;&#19987;&#23478;&#30340;&#20998;&#37197;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26089;&#26399;&#30830;&#23450;&#65292;&#24182;&#19988;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#19981;&#23436;&#20840;&#30340;&#36335;&#30001;&#21487;&#33021;&#23548;&#33268;...
&lt;/p&gt;
&lt;p&gt;
To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01737</link><description>&lt;p&gt;
&#20026;&#31038;&#20132;&#24863;&#30693;&#30340;&#35848;&#21028;&#23545;&#35805;&#24320;&#21457;&#36741;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#36890;&#36807;&#35753;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25198;&#28436;&#27599;&#27425;&#23545;&#35805;&#20013;&#30340;&#20004;&#21517;&#35848;&#21028;&#32773;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#35848;&#21028;&#12290;&#31532;&#19977;&#20010;LLM&#20805;&#24403;&#20462;&#27491;&#20195;&#29702;&#65292;&#37325;&#26032;&#32534;&#20889;&#36829;&#21453;&#35268;&#33539;&#30340;&#35805;&#35821;&#20197;&#25913;&#21892;&#35848;&#21028;&#32467;&#26524;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#19981;&#23384;&#22312;&#25163;&#21160;&#26500;&#24314;&#30340;&#25968;&#25454;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20135;&#21697;&#38144;&#21806;&#12289;&#25151;&#20215;&#21644;&#34218;&#36164;&#35848;&#21028;&#12290;&#28304;&#20195;&#30721;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#23558;&#22312;&#25509;&#21463;&#21518;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.
&lt;/p&gt;</description></item><item><title>SADAS&#26159;&#19968;&#20010;&#38754;&#21521;&#21452;&#35821;&#31038;&#20250;&#25991;&#21270;&#23545;&#35805;&#30340;&#23545;&#35805;&#21161;&#25163;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#35268;&#33539;&#31867;&#21035;&#12289;&#26816;&#27979;&#36829;&#20363;&#12289;&#35780;&#20272;&#20005;&#37325;&#31243;&#24230;&#12289;&#23454;&#26045;&#32416;&#27491;&#25514;&#26045;&#24182;&#38416;&#36848;&#29702;&#30001;&#31561;&#26032;&#39062;&#26550;&#26500;&#65292;&#30830;&#20445;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#33021;&#22815;&#20197;&#23562;&#37325;&#21644;&#29702;&#35299;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.01736</link><description>&lt;p&gt;
SADAS: &#19968;&#20010;&#38754;&#21521;&#21452;&#35821;&#31038;&#20250;&#25991;&#21270;&#23545;&#35805;&#20462;&#22797;&#35268;&#33539;&#36829;&#20363;&#30340;&#23545;&#35805;&#21161;&#25163;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SADAS: A Dialogue Assistant System Towards Remediating Norm Violations in Bilingual Socio-Cultural Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01736
&lt;/p&gt;
&lt;p&gt;
SADAS&#26159;&#19968;&#20010;&#38754;&#21521;&#21452;&#35821;&#31038;&#20250;&#25991;&#21270;&#23545;&#35805;&#30340;&#23545;&#35805;&#21161;&#25163;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#35268;&#33539;&#31867;&#21035;&#12289;&#26816;&#27979;&#36829;&#20363;&#12289;&#35780;&#20272;&#20005;&#37325;&#31243;&#24230;&#12289;&#23454;&#26045;&#32416;&#27491;&#25514;&#26045;&#24182;&#38416;&#36848;&#29702;&#30001;&#31561;&#26032;&#39062;&#26550;&#26500;&#65292;&#30830;&#20445;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#33021;&#22815;&#20197;&#23562;&#37325;&#21644;&#29702;&#35299;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#20840;&#29699;&#21270;&#30340;&#19990;&#30028;&#20013;&#65292;&#24357;&#21512;&#25991;&#21270;&#24046;&#24322;&#23545;&#20110;&#24314;&#31435;&#26377;&#24847;&#20041;&#30340;&#32852;&#31995;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#21152;&#37325;&#35201;&#12290;&#31038;&#20250;&#24863;&#30693;&#23545;&#35805;&#21161;&#25163;&#31995;&#32479;&#65288;SADAS&#65289;&#26159;&#25105;&#20204;&#23545;&#36825;&#19968;&#20840;&#29699;&#25361;&#25112;&#30340;&#22238;&#31572;&#65292;&#26088;&#22312;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#33021;&#22815;&#20197;&#23562;&#37325;&#21644;&#29702;&#35299;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#25105;&#20204;&#31995;&#32479;&#30340;&#26032;&#39062;&#26550;&#26500;&#21253;&#25324;&#65306;&#65288;1&#65289;&#35782;&#21035;&#23545;&#35805;&#20013;&#23384;&#22312;&#30340;&#35268;&#33539;&#31867;&#21035;&#65292;&#65288;2&#65289;&#26816;&#27979;&#28508;&#22312;&#30340;&#35268;&#33539;&#36829;&#20363;&#65292;&#65288;3&#65289;&#35780;&#20272;&#36825;&#20123;&#36829;&#20363;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#65288;4&#65289;&#23454;&#26045;&#26377;&#38024;&#23545;&#24615;&#30340;&#25514;&#26045;&#26469;&#32416;&#27491;&#36829;&#35268;&#34892;&#20026;&#65292;&#24182;&#65288;5&#65289;&#38416;&#36848;&#36825;&#20123;&#32416;&#27491;&#25514;&#26045;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26469;&#26500;&#24314;&#19981;&#21516;&#30340;&#27169;&#22359;&#65292;&#24182;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#36873;&#25321;&#27599;&#20010;&#27169;&#22359;&#26368;&#21512;&#36866;&#30340;&#39592;&#24178;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20154;&#31867;&#20559;&#22909;&#23454;&#39564;&#26469;&#39564;&#35777;&#31995;&#32479;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#24320;&#28304;&#25105;&#20204;&#30340;&#31995;&#32479;&#65288;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
In today's globalized world, bridging the cultural divide is more critical than ever for forging meaningful connections. The Socially-Aware Dialogue Assistant System (SADAS) is our answer to this global challenge, and it's designed to ensure that conversations between individuals from diverse cultural backgrounds unfold with respect and understanding. Our system's novel architecture includes: (1) identifying the categories of norms present in the dialogue, (2) detecting potential norm violations, (3) evaluating the severity of these violations, (4) implementing targeted remedies to rectify the breaches, and (5) articulates the rationale behind these corrective actions. We employ a series of State-Of-The-Art (SOTA) techniques to build different modules, and conduct numerous experiments to select the most suitable backbone model for each of the modules. We also design a human preference experiment to validate the overall performance of the system. We will open-source our system (includin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#23454;&#39564;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.01735</link><description>&lt;p&gt;
VIALM&#65306;&#20851;&#20110;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#30340;&#35843;&#26597;&#21644;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#23454;&#39564;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38556;&#30861;&#36741;&#21161; (VIA) &#26088;&#22312;&#33258;&#21160;&#24110;&#21161;&#35270;&#35273;&#38556;&#30861;&#32773; (VI) &#22788;&#29702;&#26085;&#24120;&#27963;&#21160;&#12290;VIA &#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#30340;&#21457;&#23637;&#65292;&#20108;&#32773;&#37117;&#23637;&#31034;&#20102;&#21033;&#29992;&#22823;&#22411;&#27169;&#22411; (LMs) &#30340;&#21069;&#27839;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;LMs &#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#21487;&#20197;&#24212;&#23545;&#35832;&#22914;&#20855;&#36523;&#26426;&#22120;&#20154;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29289;&#29702;&#20219;&#21153;&#12290;&#20026;&#20102;&#30740;&#31350;&#26368;&#20808;&#36827; (SOTA) LMs &#22312;VIA&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#38024;&#23545;&#20855;&#26377;LMs&#30340;VIA&#20219;&#21153;&#65288;VIALM&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#35828;&#26126;&#29289;&#29702;&#29615;&#22659;&#30340;&#22270;&#20687;&#21644;&#35270;&#35273;&#38556;&#30861;&#32773;&#29992;&#25143;&#30340;&#35821;&#35328;&#35831;&#27714;&#65292;VIALM&#26088;&#22312;&#36755;&#20986;&#36880;&#27493;&#24341;&#23548;&#65292;&#20197;&#22312;&#29615;&#22659;&#20013;&#24110;&#21161;&#35270;&#35273;&#38556;&#30861;&#29992;&#25143;&#23436;&#25104;&#35831;&#27714;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#23545;&#36817;&#26399;LM&#30740;&#31350;&#30340;&#35843;&#26597;&#21644;&#23545;&#36873;&#23450;LMs&#33021;&#21147;&#30340;&#22522;&#20934;&#23454;&#39564;&#30340;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually Impaired Assistance (VIA) aims to automatically help visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (\textbf{VIALM}). In this task, given an \textit{image} illustrating the physical environments and a \textit{linguistic request} from a VI user, VIALM aims to output step-by-step \textit{guidance} to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities
&lt;/p&gt;</description></item><item><title>RAG&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#21046;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#23450;&#21046;&#30340;LLM-RAG&#27969;&#31243;&#65292;&#37325;&#28857;&#20851;&#27880;&#26415;&#21069;&#21307;&#23398;&#12290;</title><link>https://arxiv.org/abs/2402.01733</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;--&#26696;&#20363;&#30740;&#31350;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01733
&lt;/p&gt;
&lt;p&gt;
RAG&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#21046;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#23450;&#21046;&#30340;LLM-RAG&#27969;&#31243;&#65292;&#37325;&#28857;&#20851;&#27880;&#26415;&#21069;&#21307;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#21046;LLMs&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#26696;&#20363;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#23450;&#21046;&#30340;LLM-RAG&#27969;&#31243;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#26415;&#21069;&#21307;&#23398;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#20102;35&#20010;&#26415;&#21069;&#25351;&#21335;&#24320;&#21457;&#20102;&#19968;&#20010;LLM-RAG&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19982;&#20154;&#24037;&#29983;&#25104;&#30340;&#22238;&#31572;&#36827;&#34892;&#27979;&#35797;&#65292;&#20849;&#35780;&#20272;&#20102;1260&#20010;&#22238;&#31572;&#12290;RAG&#27969;&#31243;&#28041;&#21450;&#20351;&#29992;&#22522;&#20110;Python&#30340;LangChain&#21644;Llamaindex&#26694;&#26550;&#23558;&#20020;&#24202;&#25991;&#26723;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#24182;&#23558;&#36825;&#20123;&#25991;&#26412;&#22788;&#29702;&#20026;&#22359;&#20197;&#29992;&#20110;&#23884;&#20837;&#21644;&#26816;&#32034;&#12290;&#21033;&#29992;Pinecone&#36827;&#34892;&#21521;&#37327;&#23384;&#20648;&#21644;&#20351;&#29992;1536&#32500;&#20313;&#24358;&#30456;&#20284;&#24230;&#25439;&#22833;&#24230;&#37327;&#26469;&#20248;&#21270;&#25968;&#25454;&#26816;&#32034;&#65292;&#20854;&#20013;&#36873;&#25321;&#20102;&#23884;&#20837;&#27169;&#22411;&#12290;&#23558;&#30001;&#21021;&#32423;&#21307;&#29983;&#25552;&#20379;&#30340;&#20154;&#24037;&#29983;&#25104;&#22238;&#31572;&#29992;&#20316;&#27604;&#36739;&#12290;&#32467;&#26524;&#65306;LLM-RA
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine.   Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison.   Results: The LLM-RA
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25307;&#32856;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31616;&#21382;&#23457;&#35745;&#65292;&#21457;&#29616; GPT-4 &#23545;&#27531;&#30142;&#30456;&#20851;&#30340;&#31616;&#21382;&#23384;&#22312;&#20559;&#35265;&#65292;&#36890;&#36807;&#35757;&#32451;&#33258;&#23450;&#20041; GPT &#24182;&#36981;&#24490;&#22810;&#26679;&#24615;&#12289;&#20844;&#24179;&#24615;&#19982;&#21253;&#23481;&#24615;&#20197;&#21450;&#27531;&#30142;&#27491;&#20041;&#21407;&#21017;&#65292;&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.01732</link><description>&lt;p&gt;
&#36776;&#21035;&#21644;&#25913;&#36827;&#22522;&#20110; GAI &#30340;&#31616;&#21382;&#31579;&#36873;&#20013;&#30340;&#27531;&#30142;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Identifying and Improving Disability Bias in GAI-Based Resume Screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01732
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25307;&#32856;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31616;&#21382;&#23457;&#35745;&#65292;&#21457;&#29616; GPT-4 &#23545;&#27531;&#30142;&#30456;&#20851;&#30340;&#31616;&#21382;&#23384;&#22312;&#20559;&#35265;&#65292;&#36890;&#36807;&#35757;&#32451;&#33258;&#23450;&#20041; GPT &#24182;&#36981;&#24490;&#22810;&#26679;&#24615;&#12289;&#20844;&#24179;&#24615;&#19982;&#21253;&#23481;&#24615;&#20197;&#21450;&#27531;&#30142;&#27491;&#20041;&#21407;&#21017;&#65292;&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#20351;&#29992;&#33539;&#22260;&#24050;&#25193;&#23637;&#21040;&#25307;&#32856;&#21644;&#25307;&#32856;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#32771;&#34385;&#20559;&#35265;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23545;&#21253;&#25324;&#27531;&#30142;&#20154;&#22312;&#20869;&#30340;&#36793;&#32536;&#20154;&#32676;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#37325;&#35201;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#31616;&#21382;&#23457;&#35745;&#30740;&#31350;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714; ChatGPT&#65288;&#29305;&#21035;&#26159; GPT-4&#65289;&#23545;&#19968;&#20221;&#31616;&#21382;&#36827;&#34892;&#25490;&#21517;&#65292;&#19982;&#38468;&#21152;&#20102;&#19982;&#27531;&#30142;&#26377;&#20851;&#30340;&#39069;&#22806;&#39046;&#23548;&#22870;&#21169;&#12289;&#22870;&#23398;&#37329;&#12289;&#38754;&#26495;&#28436;&#35762;&#21644;&#25104;&#21592;&#36164;&#26684;&#30340;&#30456;&#21516;&#31616;&#21382;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616; GPT-4 &#23545;&#36825;&#20123;&#25913;&#36827;&#30340;&#31616;&#21382;&#23384;&#22312;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#35757;&#32451;&#22522;&#20110; DEI &#21644;&#27531;&#30142;&#27491;&#20041;&#21407;&#21017;&#30340;&#33258;&#23450;&#20041; GPT&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21253;&#25324;&#23545; GPT-4 &#29992;&#20110;&#35777;&#26126;&#20854;&#26377;&#20559;&#35265;&#20915;&#31574;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#27531;&#38556;&#20027;&#20041;&#31867;&#22411;&#30340;&#29420;&#29305;&#23450;&#24615;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#39069;&#22806;&#20559;&#35265;&#20943;&#36731;&#24037;&#20316;&#30340;&#26041;&#21521;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36825;&#20123;&#29702;&#30001;&#21487;&#33021;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#27531;&#38556;&#27491;&#20041;&#30340;&#30456;&#20851;&#23454;&#38469;&#26696;&#20363;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Generative AI rises in adoption, its use has expanded to include domains such as hiring and recruiting. However, without examining the potential of bias, this may negatively impact marginalized populations, including people with disabilities. To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability related. We find that GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this prejudice can be quantifiably reduced by training a custom GPTs on principles of DEI and disability justice. Our study also includes a unique qualitative analysis of the types of direct and indirect ableism GPT-4 uses to justify its biased decisions and suggest directions for additional bias mitigation work. Additionally, since these justifications are presumably drawn from traini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#29983;&#25104;&#21307;&#23398;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20132;&#20114;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#26512;&#26469;&#35780;&#20272;&#20854;&#27491;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4-Vision-Preview&#20316;&#20026;LLM&#65292;&#24182;&#20351;&#29992;&#22810;&#27169;&#24577;&#22810;&#39033;&#36873;&#25321;&#39064;&#35780;&#20272;&#20854;&#22312;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01730</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#21644;&#30151;&#29366;&#20998;&#26512;&#30340;&#22810;&#27169;&#24577;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Evaluating LLM - Generated Multimodal Diagnosis from Medical Images and Symptom Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#29983;&#25104;&#21307;&#23398;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20132;&#20114;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#26512;&#26469;&#35780;&#20272;&#20854;&#27491;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4-Vision-Preview&#20316;&#20026;LLM&#65292;&#24182;&#20351;&#29992;&#22810;&#27169;&#24577;&#22810;&#39033;&#36873;&#25321;&#39064;&#35780;&#20272;&#20854;&#22312;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#25215;&#35834;&#24110;&#21161;&#21307;&#23398;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36820;&#22238;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#23578;&#26410;&#24471;&#21040;&#36866;&#24403;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;LLM&#35780;&#20272;&#33539;&#24335;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#29420;&#31435;&#27493;&#39588;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#65288;1&#65289;&#36890;&#36807;&#32467;&#26500;&#21270;&#20132;&#20114;&#36827;&#34892;&#22810;&#27169;&#24577;LLM&#35780;&#20272;&#21644;&#65288;2&#65289;&#22522;&#20110;&#20043;&#21069;&#20132;&#20114;&#25552;&#21462;&#30340;&#25968;&#25454;&#36827;&#34892;&#21518;&#32493;&#30340;&#39046;&#22495;&#29305;&#23450;&#20998;&#26512;&#12290;&#20351;&#29992;&#36825;&#31181;&#33539;&#24335;&#65292;&#65288;1&#65289;&#25105;&#20204;&#36890;&#36807;&#20844;&#24320;&#30340;&#22810;&#27169;&#24577;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQs&#65289;&#22312;&#30149;&#29702;&#23398;&#39046;&#22495;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#21307;&#23398;&#35786;&#26029;&#30340;&#27491;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#65288;2&#65289;&#28982;&#21518;&#23545;&#25552;&#21462;&#30340;&#32467;&#26524;&#36827;&#34892;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;GPT-4-Vision-Preview&#20316;&#20026;LLM&#65292;&#22238;&#31572;&#30001;&#22270;&#20687;&#21644;&#25991;&#26412;&#32452;&#25104;&#30340;&#22797;&#26434;&#21307;&#23398;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#30340;&#30142;&#30149;&#12289;&#30149;&#20917;&#12289;&#21270;&#23398;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) constitute a breakthrough state-of-the-art Artificial Intelligence technology which is rapidly evolving and promises to aid in medical diagnosis. However, the correctness and the accuracy of their returns has not yet been properly evaluated. In this work, we propose an LLM evaluation paradigm that incorporates two independent steps of a novel methodology, namely (1) multimodal LLM evaluation via structured interactions and (2) follow-up, domain-specific analysis based on data extracted via the previous interactions. Using this paradigm, (1) we evaluate the correctness and accuracy of LLM-generated medical diagnosis with publicly available multimodal multiple-choice questions(MCQs) in the domain of Pathology and (2) proceed to a systemic and comprehensive analysis of extracted results. We used GPT-4-Vision-Preview as the LLM to respond to complex, medical questions consisting of both images and text, and we explored a wide range of diseases, conditions, chem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01729</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Contextualization Distillation from Large Language Model for Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25991;&#26412;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#35821;&#26009;&#24211;&#20174;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#25110;&#21516;&#20041;&#35789;&#23450;&#20041;&#20013;&#25910;&#38598;&#30340;&#38745;&#24577;&#21644;&#22122;&#22768;&#24615;&#36136;&#24120;&#24120;&#38480;&#21046;&#20102;&#22522;&#20110;PLM&#30340;KGC&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#33976;&#39311;&#31574;&#30053;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21487;&#25554;&#20837;&#21644;&#21487;&#25773;&#25918;&#30340;&#26041;&#27861;&#65292;&#19982;&#21028;&#21035;&#21644;&#29983;&#25104;&#30340;KGC&#26694;&#26550;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#32039;&#20945;&#30340;&#32467;&#26500;&#21270;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#27573;&#33853;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#37325;&#24314;&#21644;&#19978;&#19979;&#25991;&#21270;&#65292;&#20351;&#36739;&#23567;&#30340;KGC&#27169;&#22411;&#33021;&#22815;&#21560;&#25910;&#36825;&#20123;&#20016;&#23500;&#30340;&#19977;&#20803;&#32452;&#20013;&#30340;&#35265;&#35299;&#12290;&#23545;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;KGC&#25216;&#26415;&#30340;&#20840;&#38754;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#26080;&#35770;&#22522;&#30784;&#31649;&#36947;&#22914;&#20309;&#65292;&#22987;&#32456;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
&lt;/p&gt;</description></item><item><title>&#30828;&#20214;Phi-1.5B&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21322;&#23548;&#20307;&#20135;&#19994;&#30828;&#20214;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#19987;&#19994;&#20998;&#23618;&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#30828;&#20214;&#39046;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01728</link><description>&lt;p&gt;
&#30828;&#20214;Phi-1.5B&#65306;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#30828;&#20214;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain Specific Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01728
&lt;/p&gt;
&lt;p&gt;
&#30828;&#20214;Phi-1.5B&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21322;&#23548;&#20307;&#20135;&#19994;&#30828;&#20214;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#19987;&#19994;&#20998;&#23618;&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#30828;&#20214;&#39046;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21322;&#23548;&#20307;&#20135;&#19994;&#20013;&#65292;&#30740;&#31350;&#12289;&#35774;&#35745;&#12289;&#39564;&#35777;&#21644;&#21046;&#36896;&#26159;&#32039;&#23494;&#30456;&#36830;&#30340;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38761;&#26032;&#30828;&#20214;&#35774;&#35745;&#21644;&#23433;&#20840;&#39564;&#35777;&#26041;&#38754;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#30828;&#20214;&#29305;&#23450;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#36890;&#24120;&#19981;&#33021;&#20805;&#20998;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#25110;&#36719;&#20214;&#20195;&#30721;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#19982;&#30828;&#20214;&#39046;&#22495;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20063;&#26159;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#30828;&#20214;Phi 1.5B&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21322;&#23548;&#20307;&#20135;&#19994;&#30828;&#20214;&#39046;&#22495;&#30340;&#21019;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#19994;&#20998;&#23618;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#23567;&#12289;&#20013;&#21644;&#22823;&#22411;&#23376;&#38598;&#65292;&#24182;&#23558;&#37325;&#28857;&#25918;&#22312;&#20351;&#29992;&#20013;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;Ph&#27169;&#22411;&#30340;&#32039;&#20945;&#20294;&#39640;&#25928;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving semiconductor industry, where research, design, verification, and manufacturing are intricately linked, the potential of Large Language Models to revolutionize hardware design and security verification is immense. The primary challenge, however, lies in the complexity of hardware specific issues that are not adequately addressed by the natural language or software code knowledge typically acquired during the pretraining stage. Additionally, the scarcity of datasets specific to the hardware domain poses a significant hurdle in developing a foundational model. Addressing these challenges, this paper introduces Hardware Phi 1.5B, an innovative large language model specifically tailored for the hardware domain of the semiconductor industry. We have developed a specialized, tiered dataset comprising small, medium, and large subsets and focused our efforts on pretraining using the medium dataset. This approach harnesses the compact yet efficient architecture of the Ph
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#24819;&#27861;&#20013;&#22914;&#20309;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#21019;&#24847;&#29983;&#25104;&#36807;&#31243;&#20013;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#25552;&#39640;&#36136;&#37327;&#21644;&#29983;&#20135;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#24605;&#36335;&#30340;&#22810;&#26679;&#24615;&#21644;&#29420;&#29305;&#24819;&#27861;&#30340;&#25968;&#37327;&#65292;&#22312;&#26032;&#20135;&#21697;&#24320;&#21457;&#39046;&#22495;&#23588;&#20854;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.01727</link><description>&lt;p&gt;
&#25552;&#31034;&#22810;&#26679;&#21270;&#30340;&#24819;&#27861;: &#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#21019;&#24847;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompting Diverse Ideas: Increasing AI Idea Variance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#24819;&#27861;&#20013;&#22914;&#20309;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#21019;&#24847;&#29983;&#25104;&#36807;&#31243;&#20013;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#25552;&#39640;&#36136;&#37327;&#21644;&#29983;&#20135;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#24605;&#36335;&#30340;&#22810;&#26679;&#24615;&#21644;&#29420;&#29305;&#24819;&#27861;&#30340;&#25968;&#37327;&#65292;&#22312;&#26032;&#20135;&#21697;&#24320;&#21457;&#39046;&#22495;&#23588;&#20854;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20363;&#34892;&#20219;&#21153;&#27880;&#37325;&#19968;&#33268;&#24615;&#19981;&#21516;&#65292;&#21019;&#36896;&#21147;&#21644;&#21019;&#26032;&#30340;&#30446;&#26631;&#26159;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#24819;&#27861;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26469;&#25552;&#39640;&#21019;&#24847;&#29983;&#25104;&#36807;&#31243;&#30340;&#29983;&#20135;&#21147;&#21644;&#36136;&#37327;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;AI&#24819;&#27861;&#30340;&#24179;&#22343;&#36136;&#37327;&#30456;&#24403;&#39640;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#20063;&#25351;&#20986;&#22522;&#20110;AI&#30340;&#22836;&#33041;&#39118;&#26292;&#26080;&#27861;&#21019;&#36896;&#36275;&#22815;&#30340;&#24819;&#27861;&#20998;&#25955;&#65292;&#36825;&#38480;&#21046;&#20102;&#26032;&#39062;&#24615;&#21644;&#25972;&#20307;&#26368;&#20339;&#24819;&#27861;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22686;&#21152;AI&#29983;&#25104;&#30340;&#24819;&#27861;&#20998;&#25955;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#25506;&#32034;&#19981;&#21516;&#25552;&#31034;&#26041;&#27861;&#23545;&#20313;&#24358;&#30456;&#20284;&#24230;&#12289;&#29420;&#29305;&#24819;&#27861;&#25968;&#37327;&#21644;&#24819;&#27861;&#31354;&#38388;&#32791;&#23613;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;&#22312;&#20197;&#22823;&#23398;&#29983;&#20026;&#20215;&#26684;&#20026;50&#32654;&#20803;&#20197;&#19979;&#30340;&#26032;&#20135;&#21697;&#24320;&#21457;&#39046;&#22495;&#36827;&#34892;&#25506;&#32034;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;(1)&#36890;&#36807;&#19981;&#21516;&#30340;plausi&#25552;&#31034;&#26041;&#27861;&#29983;&#25104;&#30340;GPT-4&#24819;&#27861;&#27744;
&lt;/p&gt;
&lt;p&gt;
Unlike routine tasks where consistency is prized, in creativity and innovation the goal is to create a diverse set of ideas. This paper delves into the burgeoning interest in employing Artificial Intelligence (AI) to enhance the productivity and quality of the idea generation process. While previous studies have found that the average quality of AI ideas is quite high, prior research also has pointed to the inability of AI-based brainstorming to create sufficient dispersion of ideas, which limits novelty and the quality of the overall best idea. Our research investigates methods to increase the dispersion in AI-generated ideas. Using GPT-4, we explore the effect of different prompting methods on Cosine Similarity, the number of unique ideas, and the speed with which the idea space gets exhausted. We do this in the domain of developing a new product development for college students, priced under $50. In this context, we find that (1) pools of ideas generated by GPT-4 with various plausi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#21487;&#33021;&#23545;&#24863;&#30693;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01726</link><description>&lt;p&gt;
AI&#20013;&#20171;&#20132;&#27969;&#30340;&#25351;&#23548;&#65306;AI&#19981;&#25913;&#21464;&#23545;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#21487;&#33021;&#23545;&#24863;&#30693;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#20154;&#26469;&#35828;&#65292;&#28966;&#34385;&#12289;&#25233;&#37057;&#21644;&#20854;&#20182;&#31038;&#20132;&#21644;&#24515;&#29702;&#22240;&#32032;&#21487;&#33021;&#20351;&#25776;&#20889;&#25991;&#26412;&#28040;&#24687;&#25104;&#20026;&#19968;&#39033;&#31215;&#26497;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#26159;&#24110;&#21161;&#37027;&#20123;&#26412;&#26469;&#20250;&#35273;&#24471;&#21457;&#36865;&#30701;&#20449;&#22256;&#38590;&#25110;&#26377;&#21387;&#21147;&#30340;&#29992;&#25143;&#30340;&#23436;&#32654;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#24555;&#36895;&#26222;&#21450;&#65292;&#20294;&#23545;&#20854;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#30340;&#32771;&#34385;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#30340;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#65292;AI&#30340;&#20844;&#20247;&#24773;&#32490;&#36739;&#24046;&#21487;&#33021;&#23548;&#33268;&#20854;&#36741;&#21161;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#20351;&#29992;&#23545;&#24863;&#30693;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#20351;&#29992;&#36866;&#24471;&#20854;&#21453;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20154;&#20204;&#26159;&#21542;&#35748;&#20026;&#19968;&#26465;&#25991;&#26412;&#28040;&#24687;&#26159;&#21542;&#22312;&#25776;&#20889;&#36807;&#31243;&#20013;&#24471;&#21040;&#20102;AI&#30340;&#36741;&#21161;&#65292;&#20250;&#25913;&#21464;&#20854;&#24863;&#30693;&#30340;&#35821;&#35843;&#12289;&#28165;&#26224;&#24230;&#21644;&#34920;&#36798;&#24847;&#22270;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;26&#21517;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#39044;&#20808;&#25776;&#20889;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#12290;&#36890;&#36807;&#20998;&#26512;&#21442;&#19982;&#32773;&#23545;&#28040;&#24687;&#35821;&#35843;&#30340;&#35780;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone,
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36947;&#24503;&#21644;&#23433;&#20840;&#25361;&#25112;&#65292;&#21253;&#25324;&#36807;&#28388;&#25935;&#24863;&#35789;&#27719;&#12289;&#26816;&#27979;&#35282;&#33394;&#25198;&#28436;&#12289;&#23454;&#26045;&#33258;&#23450;&#20041;&#35268;&#21017;&#24341;&#25806;&#65292;&#20197;&#21450;&#24212;&#29992;&#21040;&#19981;&#21516;&#30340;&#27966;&#29983;&#27169;&#22411;&#20013;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#38477;&#20302;&#36947;&#24503;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.01725</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#21152;&#24378;&#36947;&#24503;&#30028;&#38480;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#24378;&#23433;&#20840;&#30340;&#39640;&#32423;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36947;&#24503;&#21644;&#23433;&#20840;&#25361;&#25112;&#65292;&#21253;&#25324;&#36807;&#28388;&#25935;&#24863;&#35789;&#27719;&#12289;&#26816;&#27979;&#35282;&#33394;&#25198;&#28436;&#12289;&#23454;&#26045;&#33258;&#23450;&#20041;&#35268;&#21017;&#24341;&#25806;&#65292;&#20197;&#21450;&#24212;&#29992;&#21040;&#19981;&#21516;&#30340;&#27966;&#29983;&#27169;&#22411;&#20013;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#38477;&#20302;&#36947;&#24503;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-3.5&#21644;LLaMA-2&#65292;&#30001;&#20110;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;Transformer&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;LLMs&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#22312;&#34987;&#36843;&#20570;&#20986;&#19981;&#24403;&#22238;&#24212;&#26102;&#20135;&#29983;&#30340;&#36947;&#24503;&#22256;&#22659;&#65292;&#26131;&#21463;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#21644;&#20405;&#29359;&#38544;&#31169;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20174;&#29992;&#25143;&#36755;&#20837;&#20013;&#36807;&#28388;&#25935;&#24863;&#35789;&#27719;&#65292;&#20197;&#38450;&#27490;&#20135;&#29983;&#19981;&#36947;&#24503;&#30340;&#22238;&#24212;&#65307;2&#65289;&#26816;&#27979;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#38459;&#27490;&#21487;&#33021;&#24341;&#21457;&#8220;&#36234;&#29425;&#8221;&#24773;&#22659;&#30340;&#20114;&#21160;&#65307;3&#65289;&#23454;&#26045;&#33258;&#23450;&#20041;&#35268;&#21017;&#24341;&#25806;&#65292;&#38480;&#21046;&#31105;&#27490;&#20869;&#23481;&#30340;&#29983;&#25104;&#65307;&#20197;&#21450;4&#65289;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;LLM&#27966;&#29983;&#27169;&#22411;&#65292;&#22914;Multi-Model Large Language Models (MLLMs)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#36824;&#21487;&#20197;&#38477;&#20302;&#36947;&#24503;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have significantly enhanced capabilities in natural language processing and artificial intelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionized text generation, translation, and question-answering tasks due to the transformative Transformer model. Despite their widespread use, LLMs present challenges such as ethical dilemmas when models are compelled to respond inappropriately, susceptibility to phishing attacks, and privacy violations. This paper addresses these challenges by introducing a multi-pronged approach that includes: 1) filtering sensitive vocabulary from user input to prevent unethical responses; 2) detecting role-playing to halt interactions that could lead to 'prison break' scenarios; 3) implementing custom rule engines to restrict the generation of prohibited content; and 4) extending these methodologies to various LLM derivatives like Multi-Model Large Language Models (MLLMs). Our approach not onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20013;&#22269;&#24037;&#19994;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01723</link><description>&lt;p&gt;
&#22312;&#20013;&#22269;&#24037;&#19994;&#22330;&#26223;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20013;&#22269;&#24037;&#19994;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#26381;&#21153;&#22823;&#37327;&#30340;&#20013;&#22269;&#29992;&#25143;&#65292;&#20013;&#22269;&#30340;&#35768;&#22810;&#21830;&#19994;&#20379;&#24212;&#21830;&#37319;&#21462;&#20102;&#26412;&#22320;&#21270;&#25112;&#30053;&#65292;&#35757;&#32451;&#24182;&#25552;&#20379;&#19987;&#38376;&#20026;&#20013;&#22269;&#29992;&#25143;&#23450;&#21046;&#30340;&#26412;&#22320;LLMs&#12290;&#27492;&#22806;&#65292;&#23637;&#26395;&#26410;&#26469;&#65292;LLMs&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#39046;&#22495;&#23558;&#26159;&#20225;&#19994;&#21644;&#29992;&#25143;&#22312;&#24037;&#19994;&#29983;&#20135;&#39046;&#22495;&#23454;&#38469;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#22312;&#20013;&#22269;&#24037;&#19994;&#29983;&#20135;&#39046;&#22495;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#25163;&#21160;&#25910;&#38598;&#20102;&#26469;&#33258;8&#20010;&#19981;&#21516;&#24037;&#19994;&#37096;&#38376;&#30340;1200&#20010;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#26469;&#35780;&#20272;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#21547;&#22235;&#20010;&#24037;&#19994;&#29305;&#23450;&#31283;&#23450;&#24615;&#31867;&#21035;&#21644;&#20843;&#20010;&#33021;&#21147;&#30340;&#21464;&#24577;&#27979;&#35797;&#26694;&#26550;&#65292;&#24635;&#35745;13,631&#20010;&#38382;&#39064;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the rapid development of large language models (LLMs) in various domains. To better serve the large number of Chinese users, many commercial vendors in China have adopted localization strategies, training and providing local LLMs specifically customized for Chinese users. Furthermore, looking ahead, one of the key future applications of LLMs will be practical deployment in industrial production by enterprises and users in those sectors. However, the accuracy and robustness of LLMs in industrial scenarios have not been well studied. In this paper, we present a comprehensive empirical study on the accuracy and robustness of LLMs in the context of the Chinese industrial production area. We manually collected 1,200 domain-specific problems from 8 different industrial sectors to evaluate LLM accuracy. Furthermore, we designed a metamorphic testing framework containing four industrial-specific stability categories with eight abilities, totaling 13,631 questions wi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21453;&#39304;&#21644;&#31034;&#20363;&#30340;&#31934;&#35843;&#36807;&#31243;&#65292;&#32467;&#21512;&#20313;&#24358;&#30456;&#20284;&#24230;&#12289;LLM&#35780;&#20272;&#21644;Rouge-L&#24471;&#20998;&#31561;&#25351;&#26631;&#65292;&#21487;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#38382;&#39064;&#21644;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#19982;&#38646;-shot LLMs&#30456;&#27604;&#65292;&#32463;&#36807;&#31934;&#35843;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#31572;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;RAG&#36807;&#31243;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01722</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#38382;&#39064;&#21644;&#25552;&#21462;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01722
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21453;&#39304;&#21644;&#31034;&#20363;&#30340;&#31934;&#35843;&#36807;&#31243;&#65292;&#32467;&#21512;&#20313;&#24358;&#30456;&#20284;&#24230;&#12289;LLM&#35780;&#20272;&#21644;Rouge-L&#24471;&#20998;&#31561;&#25351;&#26631;&#65292;&#21487;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#38382;&#39064;&#21644;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#19982;&#38646;-shot LLMs&#30456;&#27604;&#65292;&#32463;&#36807;&#31934;&#35843;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#31572;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;RAG&#36807;&#31243;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#24120;&#24120;&#21463;&#21040;&#31572;&#26696;&#36136;&#37327;&#30340;&#19981;&#20339;&#21644;&#20598;&#23572;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#31934;&#35843;&#36807;&#31243;&#65292;&#21033;&#29992;&#21453;&#39304;&#21644;&#31034;&#20363;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#20248;&#21270;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#25345;&#32493;&#30340;&#21453;&#39304;&#24490;&#29615;&#21644;&#21033;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#12289;LLM&#35780;&#20272;&#21644;Rouge-L&#24471;&#20998;&#31561;&#25351;&#26631;&#26469;&#25552;&#21319;AI&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#20687;GPT-3.5&#12289;GPT4ALL&#12289;LLaMA2&#21644;Claude&#36825;&#26679;&#30340;LLMs&#65292;&#24182;&#22312;&#21253;&#25324;FinanceBench&#21644;RAG Instruct Benchmark Tester Dataset&#22312;&#20869;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#31934;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#31934;&#35843;&#30340;&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#38646;-shot LLMs&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#21331;&#36234;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23558;LLM&#19982;&#19968;&#31181;&#21517;&#20026;RAG&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;10&#20010;&#22269;&#23478;&#36229;&#36807;16,000&#21517;&#21463;&#35775;&#32773;&#23545;&#38750;&#33258;&#24895;&#21512;&#25104;&#20146;&#23494;&#22270;&#20687;&#30340;&#24577;&#24230;&#21644;&#34892;&#20026;&#65307;&#23613;&#31649;&#31038;&#20250;&#23545;&#27492;&#20173;&#35748;&#35782;&#19981;&#36275;&#65292;&#20294;&#36825;&#31181;&#34892;&#20026;&#34987;&#35748;&#20026;&#26377;&#23475;&#65307;&#32422;&#26377;2.2%&#30340;&#21463;&#35775;&#32773;&#34920;&#31034;&#26366;&#21463;&#23475;&#65292;1.8%&#30340;&#21463;&#35775;&#32773;&#34920;&#31034;&#26366;&#21442;&#19982;&#36807;&#27492;&#31867;&#34892;&#20026;&#65307;&#21333;&#38752;&#31435;&#27861;&#34892;&#21160;&#24182;&#19981;&#36275;&#20197;&#35299;&#20915;&#38382;&#39064;&#65292;&#24314;&#35758;&#25216;&#26415;&#21644;&#24179;&#21488;&#25919;&#31574;&#30340;&#25913;&#36827;&#26469;&#20943;&#23569;&#20260;&#23475;&#12290;</title><link>https://arxiv.org/abs/2402.01721</link><description>&lt;p&gt;
&#22312;10&#20010;&#22269;&#23478;&#30340;&#38750;&#33258;&#24895;&#21512;&#25104;&#20146;&#23494;&#22270;&#20687;&#30340;&#24577;&#24230;&#21644;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Attitudes Towards and Knowledge of Non-Consensual Synthetic Intimate Imagery in 10 Countries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;10&#20010;&#22269;&#23478;&#36229;&#36807;16,000&#21517;&#21463;&#35775;&#32773;&#23545;&#38750;&#33258;&#24895;&#21512;&#25104;&#20146;&#23494;&#22270;&#20687;&#30340;&#24577;&#24230;&#21644;&#34892;&#20026;&#65307;&#23613;&#31649;&#31038;&#20250;&#23545;&#27492;&#20173;&#35748;&#35782;&#19981;&#36275;&#65292;&#20294;&#36825;&#31181;&#34892;&#20026;&#34987;&#35748;&#20026;&#26377;&#23475;&#65307;&#32422;&#26377;2.2%&#30340;&#21463;&#35775;&#32773;&#34920;&#31034;&#26366;&#21463;&#23475;&#65292;1.8%&#30340;&#21463;&#35775;&#32773;&#34920;&#31034;&#26366;&#21442;&#19982;&#36807;&#27492;&#31867;&#34892;&#20026;&#65307;&#21333;&#38752;&#31435;&#27861;&#34892;&#21160;&#24182;&#19981;&#36275;&#20197;&#35299;&#20915;&#38382;&#39064;&#65292;&#24314;&#35758;&#25216;&#26415;&#21644;&#24179;&#21488;&#25919;&#31574;&#30340;&#25913;&#36827;&#26469;&#20943;&#23569;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#24037;&#20855;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#65292;"&#27665;&#20027;&#21270;"&#20102;&#25805;&#32437;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#25216;&#26415;&#30340;&#19968;&#20010;&#27969;&#34892;&#29992;&#36884;&#26159;&#21019;&#24314;&#24615;&#26292;&#21147;&#20869;&#23481;&#65292;&#28982;&#21518;&#22312;&#20114;&#32852;&#32593;&#19978;&#24191;&#27867;&#21457;&#24067;&#21644;&#20849;&#20139;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#22269;&#23478;&#30340;&#36229;&#36807;16,000&#21517;&#21463;&#35775;&#32773;&#30340;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#19982;&#38750;&#33258;&#24895;&#21512;&#25104;&#20146;&#23494;&#22270;&#20687;&#65288;NSII&#65289;&#30456;&#20851;&#30340;&#24577;&#24230;&#21644;&#34892;&#20026;&#12290;&#23613;&#31649;&#31038;&#20250;&#23545;NSII&#30340;&#35748;&#35782;&#23578;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#20294;NSII&#34892;&#20026;&#34987;&#35748;&#20026;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26222;&#36941;&#24615;&#26041;&#38754;&#65292;&#25152;&#26377;&#21463;&#35775;&#32773;&#20013;&#26377;2.2%&#30340;&#20154;&#34920;&#31034;&#26366;&#21463;&#23475;&#65292;1.8%&#30340;&#20154;&#34920;&#31034;&#26366;&#21442;&#19982;&#36807;&#36825;&#31181;&#34892;&#20026;&#12290;&#26469;&#33258;&#20855;&#26377;&#30456;&#20851;&#31435;&#27861;&#30340;&#22269;&#23478;&#30340;&#21463;&#35775;&#32773;&#20063;&#25253;&#21578;&#20102;&#21442;&#19982;&#21644;&#21463;&#23475;&#32463;&#21382;&#65292;&#36825;&#34920;&#26126;&#21333;&#38752;&#31435;&#27861;&#34892;&#21160;&#24182;&#19981;&#36275;&#20197;&#38459;&#27490;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#20943;&#23569;&#20260;&#23475;&#30340;&#25216;&#26415;&#32771;&#34385;&#21487;&#33021;&#21253;&#25324;&#24314;&#35758;&#20154;&#20204;&#22914;&#20309;&#26356;&#22909;&#22320;&#30417;&#25511;&#33258;&#24049;&#22312;&#32593;&#19978;&#30340;&#23384;&#22312;&#65292;&#24182;&#25191;&#34892;&#24179;&#21488;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfake technology tools have become ubiquitous, "democratizing" the ability to manipulate images and videos. One popular use of such technology is the creation of sexually explicit content, which can then be posted and shared widely on the internet. This article examines attitudes and behaviors related to non-consensual synthetic intimate imagery (NSII) across over 16,000 respondents in 10 countries. Despite nascent societal awareness of NSII, NSII behaviors were considered harmful. In regards to prevalence, 2.2% of all respondents indicated personal victimization, and 1.8% all of respondents indicated perpetration behaviors. Respondents from countries with relevant legislation also reported perpetration and victimization experiences, suggesting legislative action alone is not a sufficient solution to deter perpetration. Technical considerations to reduce harms may include suggestions for how individuals can better monitor their presence online, as well as enforced platform policies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#23398;&#29983;&#35299;&#31572;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.01720</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Amharic Chatbot for FAQs in Universities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#23398;&#29983;&#35299;&#31572;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#23398;&#29983;&#24120;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21521;&#31649;&#29702;&#21592;&#25110;&#25945;&#24072;&#23547;&#27714;&#24120;&#35265;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#36825;&#23545;&#21452;&#26041;&#26469;&#35828;&#37117;&#24456;&#32321;&#29712;&#65292;&#38656;&#35201;&#25214;&#21040;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#38463;&#22982;&#21704;&#25289;&#35821;&#20013;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#65292;&#20316;&#20026;&#34394;&#25311;&#21161;&#25163;&#22788;&#29702;&#38382;&#39064;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31243;&#24207;&#20351;&#29992;&#26631;&#35760;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#21435;&#38500;&#20572;&#29992;&#35789;&#21644;&#35789;&#24178;&#25552;&#21462;&#23545;&#38463;&#22982;&#21704;&#25289;&#35821;&#36755;&#20837;&#21477;&#23376;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#26469;&#20998;&#31867;&#26631;&#35760;&#21644;&#26816;&#32034;&#21512;&#36866;&#30340;&#22238;&#31572;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#36890;&#36807;TensorFlow&#12289;Keras&#21644;NLTK&#23454;&#29616;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
University students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. This can become tedious for both parties, leading to a need for a solution. In response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (FAQs) in the Amharic language. Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. Three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: Support Vector Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented through TensorFlow, Keras, and NLTK. The deep learning model achieved the be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21517;&#20026;QA-RAG&#65292;&#29992;&#20110;&#35299;&#20915;&#21046;&#33647;&#34892;&#19994;&#20013;&#30340;&#21512;&#35268;&#24615;&#25361;&#25112;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.01717</link><description>&lt;p&gt;
&#20174;RAG&#21040;QA-RAG&#65306;&#23558;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#20110;&#33647;&#21697;&#30417;&#31649;&#21512;&#35268;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21517;&#20026;QA-RAG&#65292;&#29992;&#20110;&#35299;&#20915;&#21046;&#33647;&#34892;&#19994;&#20013;&#30340;&#21512;&#35268;&#24615;&#25361;&#25112;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#33647;&#34892;&#19994;&#30340;&#21512;&#35268;&#24615;&#35201;&#27714;&#38656;&#35201;&#38754;&#23545;&#22797;&#26434;&#19988;&#22823;&#37327;&#30340;&#25351;&#21335;&#25991;&#20214;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#12290;&#35813;&#32842;&#22825;&#26426;&#22120;&#20154;&#26088;&#22312;&#25628;&#32034;&#19982;&#29992;&#25143;&#26597;&#35810;&#30456;&#20851;&#30340;&#25351;&#21335;&#25991;&#20214;&#65292;&#24182;&#26681;&#25454;&#26816;&#32034;&#21040;&#30340;&#25351;&#21335;&#25552;&#20379;&#31572;&#26696;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#39046;&#22495;&#23545;&#39640;&#21487;&#38752;&#24615;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38382;&#39064;&#22238;&#31572;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;QA-RAG&#65289;&#27169;&#22411;&#12290;&#22312;&#23545;&#27604;&#23454;&#39564;&#20013;&#65292;QA-RAG&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#26174;&#31034;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#20248;&#20110;&#21253;&#25324;&#20256;&#32479;RAG&#26041;&#27861;&#22312;&#20869;&#30340;&#25152;&#26377;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;QA-RAG&#30340;&#32467;&#26500;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#33647;&#21697;&#30417;&#31649;&#21512;&#35268;&#39046;&#22495;&#21450;&#20854;&#20182;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24050;&#23558;&#25105;&#20204;&#30340;&#24037;&#20316;&#20844;&#24320;&#25552;&#20379;&#32473;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further researc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;ChatGPT&#12289;Gemini&#21644;LLaMA2&#31561;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#22659;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#35780;&#20272;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#21644;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#33258;&#21160;&#24773;&#24863;&#20998;&#26512;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#25913;&#36827;&#31639;&#27861;&#21644;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01715</link><description>&lt;p&gt;
ChatGPT&#12289;Gemini&#21644;LLaMA2&#22312;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;ChatGPT&#12289;Gemini&#21644;LLaMA2&#31561;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#22659;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#35780;&#20272;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#21644;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#33258;&#21160;&#24773;&#24863;&#20998;&#26512;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#25913;&#36827;&#31639;&#27861;&#21644;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ChatGPT&#12289;Gemini&#25110;LLaMA2&#31561;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24773;&#24863;&#20998;&#26512;&#65292;&#22914;&#20170;&#22312;&#23398;&#26415;&#30740;&#31350;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#27169;&#26865;&#20004;&#21487;&#25110;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#30340;&#25991;&#26412;&#26102;&#65292;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#21644;&#39564;&#35777;&#20173;&#28982;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#32454;&#33268;&#21644;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#22659;&#65292;&#24182;&#23558;&#20854;&#32763;&#35793;&#25104;10&#31181;&#35821;&#35328;&#65292;&#28982;&#21518;&#20351;&#29992;&#27969;&#34892;&#30340;LLM&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#12290;&#32467;&#26524;&#32463;&#36807;&#20107;&#21518;&#39564;&#35777;&#30340;&#20154;&#31867;&#22238;&#24212;&#26469;&#39564;&#35777;&#12290;ChatGPT&#21644;Gemini&#36890;&#24120;&#23545;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#22659;&#22788;&#29702;&#24471;&#24456;&#22909;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21644;&#35780;&#20272;&#30340;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#21644;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#33258;&#21160;&#24773;&#24863;&#20998;&#26512;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#25913;&#36827;&#31639;&#27861;&#21450;&#20854;&#22522;&#30784;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated sentiment analysis using Large Language Model (LLM)-based models like ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic research and in industrial applications. However, assessment and validation of their performance in case of ambiguous or ironic text is still poor. In this study, we constructed nuanced and ambiguous scenarios, we translated them in 10 languages, and we predicted their associated sentiment using popular LLMs. The results are validated against post-hoc human responses. Ambiguous scenarios are often well-coped by ChatGPT and Gemini, but we recognise significant biases and inconsistent performance across models and evaluated human languages. This work provides a standardised methodology for automated sentiment analysis evaluation and makes a call for action to further improve the algorithms and their underlying data, to improve their performance, interpretability and applicability.
&lt;/p&gt;</description></item><item><title>TrICy&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#21033;&#29992;&#24847;&#22270;&#21644;&#35302;&#21457;&#22120;&#24341;&#23548;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#35789;&#27719;&#34920;&#20043;&#22806;&#30340;&#35789;&#27719;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01714</link><description>&lt;p&gt;
TrICy: &#36890;&#36807;&#24847;&#22270;&#24863;&#30693;&#30340;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#24341;&#23548;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01714
&lt;/p&gt;
&lt;p&gt;
TrICy&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#21033;&#29992;&#24847;&#22270;&#21644;&#35302;&#21457;&#22120;&#24341;&#23548;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#35789;&#27719;&#34920;&#20043;&#22806;&#30340;&#35789;&#27719;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#65288;D2T&#65289;&#29983;&#25104;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#38754;&#21521;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#22312;&#21487;&#20197;&#30452;&#25509;&#19982;&#29992;&#25143;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#30340;&#20250;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#26550;&#26500;&#30001;&#20110;&#39640;&#20869;&#23384;&#21344;&#29992;&#32780;&#26080;&#27861;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrICy&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;D2T&#20219;&#21153;&#65292;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#24847;&#22270;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#21487;&#36827;&#19968;&#27493;&#30001;&#29992;&#25143;&#25552;&#20379;&#30340;&#35302;&#21457;&#22120;&#36827;&#34892;&#24341;&#23548;&#12290;&#25105;&#20204;&#21033;&#29992;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#35789;&#27719;&#34920;&#20043;&#22806;&#30340;&#35789;&#27719;&#65288;OOV&#65289;&#12290;&#23545;E2E NLG&#25968;&#25454;&#38598;&#65288;BLEU&#65306;66.43&#65285;&#65292;ROUGE-L&#65306;70.14&#65285;&#65289;&#65292;WebNLG&#25968;&#25454;&#38598;&#65288;BLEU&#65306;Seen 64.08&#65285;&#65292;Unseen 52.35&#65285;&#65289;&#21644;&#19982;&#25991;&#26412;&#28040;&#24687;&#24212;&#29992;&#30456;&#20851;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#20998;&#26512;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#21487;&#36873;&#30340;&#35302;&#21457;&#22120;&#36755;&#20837;&#65292;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Data-to-text (D2T) generation is a crucial task in many natural language understanding (NLU) applications and forms the foundation of task-oriented dialog systems. In the context of conversational AI solutions that can work directly with local data on the user's device, architectures utilizing large pre-trained language models (PLMs) are impractical for on-device deployment due to a high memory footprint. To this end, we propose TrICy, a novel lightweight framework for an enhanced D2T task that generates text sequences based on the intent in context and may further be guided by user-provided triggers. We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L: 70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom dataset related to text messaging applications, showcase our architecture's effectiveness. Moreover, we show that by leveraging an optional trigger input, data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01713</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#20351;&#20854;&#19982;&#20256;&#32479;&#19978;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25972;&#21512;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#21463;&#26032;&#30142;&#30149;&#29190;&#21457;&#26102;&#36805;&#36895;&#20915;&#31574;&#30340;&#32039;&#36843;&#38656;&#27714;&#30340;&#39537;&#20351;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;GPT-4&#30340;LLM&#23545;EHR&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#21363;&#22312;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#38024;&#23545;EHR&#25968;&#25454;&#30340;&#32437;&#21521;&#12289;&#31232;&#30095;&#21644;&#30693;&#35782;&#27880;&#20837;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#30340;&#25552;&#31034;&#26041;&#27861;&#32771;&#34385;&#20102;&#29305;&#23450;&#30340;EHR&#29305;&#24449;&#65292;&#22914;&#21333;&#20301;&#21644;&#21442;&#32771;&#33539;&#22260;&#65292;&#24182;&#37319;&#29992;&#20102;&#19982;&#20020;&#24202;&#19978;&#19979;&#25991;&#30456;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LLM&#33021;&#22815;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;EHR&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#24863;&#30693;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01712</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#24863;&#30693;&#24335;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#29992;&#20110;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#24863;&#30693;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#25913;&#36827;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#31995;&#32479;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#26432;&#30456;&#20851;&#25968;&#25454;&#21608;&#22260;&#30340;&#25935;&#24863;&#24615;&#23548;&#33268;&#38590;&#20197;&#35775;&#38382;&#21040;&#22823;&#35268;&#27169;&#30340;&#12289;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#35757;&#32451;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;ChatGPT&#65292;Flan-T5&#21644;Llama&#31561;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20026;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#22522;&#20110;&#20174;&#24515;&#29702;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#31038;&#20250;&#22240;&#32032;&#65292;&#24182;&#26088;&#22312;&#30830;&#20445;&#28085;&#30422;&#19982;&#33258;&#26432;&#24847;&#24565;&#30456;&#20851;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19982;&#22522;&#20110;BERT&#31995;&#21015;&#32467;&#26500;&#30340;&#29616;&#26377;NLP&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#24403;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;UMD&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#36825;&#20123;&#20256;&#32479;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#36890;&#24120;&#22312;0.75&#21040;0.87&#20043;&#38388;&#12290;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#36873;&#25321;&#65292;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#24320;&#21457;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24555;&#36895;&#20581;&#24247;&#20114;&#25805;&#20316;&#24615;&#36164;&#28304;&#65288;FHIR&#65289;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;APIs&#65289;&#30340;&#24739;&#32773;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#20581;&#24247;&#32032;&#20859;&#21644;&#20581;&#24247;&#20449;&#24687;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;LLM&#22312;FHIR&#31227;&#21160;&#24212;&#29992;&#30340;&#35797;&#28857;&#30740;&#31350;&#20013;&#65292;&#24212;&#29992;&#31243;&#24207;&#36890;&#36807;LLMs&#21521;&#24739;&#32773;&#25552;&#20379;&#20102;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#21487;&#29702;&#35299;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.01711</link><description>&lt;p&gt;
LLM&#22312;FHIR&#19978;--&#25581;&#24320;&#20581;&#24247;&#35760;&#24405;&#30340;&#31070;&#31192;&#38754;&#32433;
&lt;/p&gt;
&lt;p&gt;
LLM on FHIR -- Demystifying Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01711
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#24320;&#21457;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24555;&#36895;&#20581;&#24247;&#20114;&#25805;&#20316;&#24615;&#36164;&#28304;&#65288;FHIR&#65289;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;APIs&#65289;&#30340;&#24739;&#32773;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#20581;&#24247;&#32032;&#20859;&#21644;&#20581;&#24247;&#20449;&#24687;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;LLM&#22312;FHIR&#31227;&#21160;&#24212;&#29992;&#30340;&#35797;&#28857;&#30740;&#31350;&#20013;&#65292;&#24212;&#29992;&#31243;&#24207;&#36890;&#36807;LLMs&#21521;&#24739;&#32773;&#25552;&#20379;&#20102;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#21487;&#29702;&#35299;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24555;&#36895;&#20581;&#24247;&#20114;&#25805;&#20316;&#24615;&#36164;&#28304;&#65288;FHIR&#65289;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;APIs&#65289;&#30340;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#22686;&#24378;&#22810;&#26679;&#21270;&#24739;&#32773;&#32676;&#20307;&#30340;&#20581;&#24247;&#32032;&#20859;&#21644;&#20581;&#24247;&#20449;&#24687;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#35813;&#30740;&#31350;&#28041;&#21450;&#24320;&#21457;&#22312;FHIR&#19978;&#30340;LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;LLMs&#19982;&#20854;&#20581;&#24247;&#35760;&#24405;&#36827;&#34892;&#20114;&#21160;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#22522;&#20110;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Spezi&#29983;&#24577;&#31995;&#32479;&#26500;&#24314;&#65292;&#24182;&#20351;&#29992;OpenAI&#30340;GPT-4&#12290;&#36890;&#36807;&#21512;&#25104;&#24739;&#32773;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#24182;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#35813;&#24212;&#29992;&#31243;&#24207;&#25552;&#39640;&#20581;&#24247;&#32032;&#20859;&#30340;&#25928;&#26524;&#12290;&#35780;&#20272;&#37325;&#28857;&#26159;LLM&#23545;&#24120;&#35265;&#24739;&#32773;&#38382;&#39064;&#30340;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;&#32467;&#26524;&#65306;LLM&#22312;FHIR&#19978;&#21521;&#24739;&#32773;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#20581;&#24247;&#20449;&#24687;&#65292;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#21508;&#19981;&#30456;&#21516;&#65292;&#20294;&#24635;&#20307;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: To enhance health literacy and accessibility of health information for a diverse patient population by developing a patient-centered artificial intelligence (AI) solution using large language models (LLMs) and Fast Healthcare Interoperability Resources (FHIR) application programming interfaces (APIs). Materials and Methods: The research involved developing LLM on FHIR, an open-source mobile application allowing users to interact with their health records using LLMs. The app is built on Stanford's Spezi ecosystem and uses OpenAI's GPT-4. A pilot study was conducted with the SyntheticMass patient dataset and evaluated by medical experts to assess the app's effectiveness in increasing health literacy. The evaluation focused on the accuracy, relevance, and understandability of the LLM's responses to common patient questions. Results: LLM on FHIR demonstrated varying but generally high degrees of accuracy and relevance in providing understandable health information to patients. T
&lt;/p&gt;</description></item><item><title>&#35821;&#38899;&#29983;&#25104;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#65292;&#24635;&#32467;&#20986;&#29305;&#23450;&#20260;&#23475;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#12290;&#36825;&#20123;&#29305;&#23450;&#20260;&#23475;&#28041;&#21450;&#21040;&#21463;&#24433;&#21709;&#20010;&#20307;&#30340;&#26333;&#20809;&#31243;&#24230;&#20197;&#21450;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01708</link><description>&lt;p&gt;
&#19981;&#26159;&#25105;&#30340;&#22768;&#38899;&#65281;&#35821;&#38899;&#29983;&#25104;&#22120;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#20260;&#23475;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01708
&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#29983;&#25104;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#65292;&#24635;&#32467;&#20986;&#29305;&#23450;&#20260;&#23475;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#12290;&#36825;&#20123;&#29305;&#23450;&#20260;&#23475;&#28041;&#21450;&#21040;&#21463;&#24433;&#21709;&#20010;&#20307;&#30340;&#26333;&#20809;&#31243;&#24230;&#20197;&#21450;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24191;&#27867;&#37319;&#29992;&#35821;&#38899;&#29983;&#25104;&#25216;&#26415;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#37325;&#22823;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#20127;&#38656;&#35299;&#20915;&#12290;&#20363;&#22914;&#65292;&#22312;&#32654;&#22269;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#19982;&#35686;&#23519;&#21463;&#21040;&#24694;&#20316;&#21095;&#34989;&#20987;&#26377;&#20851;&#65292;&#21311;&#21517;&#34892;&#20026;&#32773;&#21046;&#36896;&#21512;&#25104;&#30340;&#22768;&#38899;&#25171;&#30005;&#35805;&#32473;&#35686;&#23519;&#65292;&#35201;&#27714;&#20851;&#38381;&#23398;&#26657;&#21644;&#21307;&#38498;&#65292;&#25110;&#32773;&#20197;&#26292;&#21147;&#25163;&#27573;&#36827;&#20837;&#26080;&#36764;&#24066;&#27665;&#30340;&#23478;&#20013;&#12290;&#36825;&#26679;&#30340;&#20107;&#20214;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#39118;&#38505;&#21644;&#20260;&#23475;&#24182;&#19981;&#23384;&#22312;&#20110;&#23396;&#31435;&#29366;&#24577;&#65292;&#32780;&#26159;&#28304;&#20110;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#65292;&#30740;&#31350;&#29305;&#23450;&#20260;&#23475;&#27169;&#24335;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#29305;&#23450;&#20260;&#23475;&#21487;&#20197;&#26681;&#25454;&#21463;&#24433;&#21709;&#20010;&#20307;&#30340;&#26333;&#20809;&#31243;&#24230;&#36827;&#34892;&#20998;&#31867;&#65292;&#21363;&#20182;&#20204;&#26159;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#30340;&#20027;&#20307;&#12289;&#19982;&#20043;&#20114;&#21160;&#12289;&#21463;&#20854;&#24433;&#21709;&#25110;&#34987;&#25490;&#38500;&#22312;&#22806;&#12290;&#21516;&#26679;&#65292;&#29305;&#23450;&#20260;&#23475;&#20063;&#19982;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.01705</link><description>&lt;p&gt;
&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#34920;&#24449;&#20260;&#23475;&#65306;&#24230;&#37327;&#21644;&#20943;&#36731;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20260;&#23475;&#36890;&#24120;&#34987;&#20998;&#20026;&#37197;&#32622;&#24615;&#25110;&#34920;&#24449;&#24615;&#12290;&#26412;&#30740;&#31350;&#19987;&#38376;&#38024;&#23545;&#21518;&#32773;&#65292;&#37325;&#28857;&#22312;&#20110;&#23545;&#24403;&#21069;&#34920;&#24449;&#24615;&#20260;&#23475;&#23450;&#20041;&#30340;&#23457;&#26597;&#65292;&#20197;&#30830;&#23450;&#20854;&#20013;&#21253;&#21547;&#20160;&#20040;&#21644;&#19981;&#21253;&#21547;&#20160;&#20040;&#12290;&#36825;&#20010;&#20998;&#26512;&#20419;&#20351;&#25105;&#20204;&#25193;&#23637;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#21253;&#25324;&#23545;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#30340;&#20260;&#23475;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24230;&#37327;&#30340;&#39640;&#32423;&#35201;&#27714;&#65306;&#30830;&#23450;&#23454;&#26045;&#36825;&#31181;&#26041;&#27861;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#35828;&#26126;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#34920;&#24449;&#24615;&#20260;&#23475;&#26102;&#30340;&#29420;&#29305;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#20260;&#23475;&#26410;&#34987;&#24230;&#37327;&#21644;&#20943;&#36731;&#26102;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#20943;&#36731;&#25514;&#26045;&#24182;&#30028;&#23450;&#20309;&#26102;&#20351;&#29992;&#23427;&#20204;&#26469;&#32467;&#26463;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#26694;&#26550;&#65292;&#25193;&#22823;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#20844;&#24179;&#30740;&#31350;&#30340;&#35265;&#35299;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01704</link><description>&lt;p&gt;
&#20316;&#20026;&#31574;&#30053;&#30340;&#29366;&#24577;&#23383;&#31526;&#20018;&#65306;&#29992;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#26159;&#30740;&#31350;&#29702;&#24615;&#20027;&#20307;&#38388;&#25112;&#30053;&#20114;&#21160;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#35821;&#35328;&#26159;&#20154;&#31867;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#65292;&#20294;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#24456;&#38590;&#36890;&#36807;&#25968;&#23398;&#26041;&#27861;&#23545;&#23545;&#35805;&#21450;&#20854;&#25112;&#30053;&#21160;&#26426;&#24314;&#27169;&#12290;&#19982;&#35821;&#35328;&#20114;&#21160;&#30456;&#20851;&#30340;&#29609;&#23478;&#12289;&#31574;&#30053;&#21644;&#22238;&#25253;&#30340;&#36866;&#24403;&#27169;&#22411;&#65288;&#21363;&#23545;&#28216;&#25103;&#35770;&#24120;&#35268;&#31526;&#21495;&#36923;&#36753;&#30340;&#32422;&#26463;&#65289;&#23558;&#20351;&#29616;&#26377;&#30340;&#21338;&#24328;&#35770;&#31639;&#27861;&#33021;&#22815;&#22312;&#35821;&#35328;&#39046;&#22495;&#25552;&#20379;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#31181;&#32422;&#26463;&#21487;&#20197;&#20026;&#22312;&#23545;&#35805;&#20013;&#35745;&#31639;&#31283;&#23450;&#12289;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#25552;&#20379;&#19968;&#26465;&#36884;&#24452;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#24050;&#32463;&#36798;&#21040;&#20102;&#20854;&#29983;&#25104;&#33021;&#21147;&#36275;&#20197;&#23454;&#29616;&#33258;&#28982;&#23545;&#35805;&#30495;&#23454;&#12289;&#31867;&#20284;&#20154;&#31867;&#30340;&#27169;&#25311;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#25552;&#31034;&#23427;&#20204;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20854;&#21709;&#24212;&#24341;&#23548;&#21040;&#19981;&#21516;&#30340;&#36755;&#20986;&#35805;&#35821;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;LLM&#36824;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24555;&#36895;&#29983;&#25104;&#26032;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new di
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.01703</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#24220;&#23448;&#21592;&#19982;&#24066;&#27665;&#20043;&#38388;&#30340;&#20114;&#21160;&#24433;&#21709;&#20844;&#20849;&#31119;&#31049;&#21644;&#27665;&#20027;&#31038;&#20250;&#30340;&#27491;&#24403;&#24615;&#12290;&#35686;&#23519;&#26159;&#22269;&#23478;&#26368;&#26174;&#32780;&#26131;&#35265;&#12289;&#26368;&#25509;&#35302;&#24066;&#27665;&#30340;&#20195;&#29702;&#20154;&#65292;&#22312;&#20132;&#36890;&#31449;&#20572;&#26399;&#38388;&#65292;&#20182;&#20204;&#27599;&#24180;&#19982;&#20844;&#20247;&#20114;&#21160;&#36229;&#36807;2000&#19975;&#27425;&#12290;&#22914;&#20170;&#65292;&#36825;&#20123;&#20114;&#21160;&#32463;&#24120;&#34987;&#25140;&#22312;&#36523;&#19978;&#30340;&#25668;&#20687;&#26426;&#35760;&#24405;&#19979;&#26469;&#65292;&#36825;&#34987;&#35270;&#20026;&#25552;&#39640;&#35686;&#23519;&#38382;&#36131;&#21046;&#21644;&#25913;&#21892;&#35686;&#27665;&#20114;&#21160;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#20998;&#26512;&#36825;&#20123;&#22797;&#26434;&#32780;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#65292;&#36825;&#20123;&#35760;&#24405;&#30340;&#21450;&#26102;&#20998;&#26512;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35282;&#24230;&#12289;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#26469;&#33258;&#36825;&#20123;&#36523;&#19978;&#25668;&#20687;&#26426;&#35760;&#24405;&#30340;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#30830;&#23450;&#19982;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#26368;&#30456;&#20851;&#30340;&#27807;&#36890;&#26041;&#38754;&#65292;&#21253;&#25324;&#20849;&#21516;&#24863;&#30693;&#20114;&#21160;&#30340;&#26631;&#24535;&#26631;&#35760;&#20197;&#21450;&#20855;&#26377;&#36825;&#20123;&#26631;&#35760;&#30340;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both commun
&lt;/p&gt;</description></item><item><title>&#25193;&#23637;&#20102;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#36827;&#21270;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#30021;&#26790;&#22659;&#65292;&#33021;&#22815;&#21516;&#26102;&#26368;&#22823;&#21270;&#20869;&#37096;&#29305;&#24449;&#21644;&#25552;&#31034;&#27969;&#30021;&#24615;&#65292;&#21487;&#20197;&#33258;&#21160;&#25506;&#32034;&#27169;&#22411;&#23545;&#36731;&#24230;&#20998;&#24067;&#20043;&#22806;&#25552;&#31034;&#30340;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01702</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#30021;&#26790;&#22659;
&lt;/p&gt;
&lt;p&gt;
Fluent dreaming for language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01702
&lt;/p&gt;
&lt;p&gt;
&#25193;&#23637;&#20102;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#36827;&#21270;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#30021;&#26790;&#22659;&#65292;&#33021;&#22815;&#21516;&#26102;&#26368;&#22823;&#21270;&#20869;&#37096;&#29305;&#24449;&#21644;&#25552;&#31034;&#27969;&#30021;&#24615;&#65292;&#21487;&#20197;&#33258;&#21160;&#25506;&#32034;&#27169;&#22411;&#23545;&#36731;&#24230;&#20998;&#24067;&#20043;&#22806;&#25552;&#31034;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20063;&#31216;&#20026;&#8220;&#26790;&#22659;&#8221;&#65292;&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#20197;&#26368;&#22823;&#21270;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#25110;&#20854;&#20182;&#20869;&#37096;&#32452;&#20214;&#65292;&#20026;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36755;&#20837;&#31354;&#38388;&#26159;&#31163;&#25955;&#30340;&#65292;&#26790;&#22659;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#23578;&#26410;&#25104;&#21151;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#25991;&#29486;&#20013;&#30340;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#35774;&#35745;&#20102;&#36827;&#21270;&#25552;&#31034;&#20248;&#21270;&#65288;EPO&#65289;&#31639;&#27861;&#12290;EPO&#20248;&#21270;&#36755;&#20837;&#25552;&#31034;&#65292;&#20197;&#21516;&#26102;&#26368;&#22823;&#21270;&#25152;&#36873;&#20869;&#37096;&#29305;&#24449;&#21644;&#25552;&#31034;&#27969;&#30021;&#24615;&#20043;&#38388;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#30021;&#26790;&#22659;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#31070;&#32463;&#20803;&#12289;&#36755;&#20986;logits&#21644;&#28608;&#27963;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#26041;&#21521;&#36827;&#34892;&#26790;&#22659;&#12290;&#25105;&#20204;&#34913;&#37327;&#20102;&#29983;&#25104;&#30340;&#25552;&#31034;&#30340;&#27969;&#30021;&#24615;&#65292;&#24182;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#26790;&#22659;&#19982;&#26368;&#22823;&#28608;&#27963;&#25968;&#25454;&#38598;&#31034;&#20363;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20851;&#38190;&#26159;&#65292;&#27969;&#30021;&#26790;&#22659;&#20801;&#35768;&#33258;&#21160;&#25506;&#32034;&#27169;&#22411;&#20869;&#37096;&#23545;&#36731;&#24230;&#20998;&#24067;&#20043;&#22806;&#30340;&#25552;&#31034;&#30340;&#34892;&#20026;&#21453;&#24212;&#12290;&#29992;&#20110;&#36816;&#34892;&#30340;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Feature visualization, also known as "dreaming", offers insights into vision models by optimizing the inputs to maximize a neuron's activation or other internal component. However, dreaming has not been successfully applied to language models because the input space is discrete. We extend Greedy Coordinate Gradient, a method from the language model adversarial attack literature, to design the Evolutionary Prompt Optimization (EPO) algorithm. EPO optimizes the input prompt to simultaneously maximize the Pareto frontier between a chosen internal feature and prompt fluency, enabling fluent dreaming for language models. We demonstrate dreaming with neurons, output logits and arbitrary directions in activation space. We measure the fluency of the resulting prompts and compare language model dreaming with max-activating dataset examples. Critically, fluent dreaming allows automatically exploring the behavior of model internals in reaction to mildly out-of-distribution prompts. Code for runni
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#31995;&#32479;&#32508;&#36848;&#26088;&#22312;&#25551;&#36848;&#24403;&#21069;&#30340;&#21307;&#23398;&#38382;&#31572;&#31995;&#32479;&#65292;&#35780;&#20272;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#30830;&#23450;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01700</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#25252;&#29702;&#28857;&#30340;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#38382;&#31572;&#31995;&#32479;--&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Question answering systems for health professionals at the point of care -- a systematic review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01700
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#31995;&#32479;&#32508;&#36848;&#26088;&#22312;&#25551;&#36848;&#24403;&#21069;&#30340;&#21307;&#23398;&#38382;&#31572;&#31995;&#32479;&#65292;&#35780;&#20272;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#30830;&#23450;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#26377;&#28508;&#21147;&#36890;&#36807;&#20026;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#26368;&#26032;&#21644;&#26368;&#30456;&#20851;&#30340;&#35777;&#25454;&#26469;&#25552;&#39640;&#20020;&#24202;&#25252;&#29702;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;QA&#31995;&#32479;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#31995;&#32479;&#32508;&#36848;&#26088;&#22312;&#25551;&#36848;&#24403;&#21069;&#30340;&#21307;&#23398;QA&#31995;&#32479;&#65292;&#35780;&#20272;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#30830;&#23450;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;&#26448;&#26009;&#19982;&#26041;&#27861;&#65306;&#25105;&#20204;&#20110;2023&#24180;2&#26376;7&#26085;&#22312;PubMed&#12289;IEEE Xplore&#12289;ACM Digital Library&#12289;ACL Anthology&#20197;&#21450;&#21069;&#21518;&#24341;&#29992;&#20013;&#36827;&#34892;&#20102;&#25628;&#32034;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#25551;&#36848;&#29983;&#29289;&#21307;&#23398;QA&#31995;&#32479;&#35774;&#35745;&#21644;&#35780;&#20272;&#30340;&#21516;&#34892;&#35780;&#35758;&#26399;&#21002;&#21644;&#20250;&#35758;&#35770;&#25991;&#12290;&#20004;&#20010;&#35780;&#23457;&#20154;&#21592;&#31579;&#36873;&#20102;&#26631;&#39064;&#12289;&#25688;&#35201;&#21644;&#20840;&#25991;&#25991;&#31456;&#12290;&#25105;&#20204;&#23545;&#27599;&#20010;&#30740;&#31350;&#36827;&#34892;&#20102;&#21465;&#20107;&#32508;&#21512;&#21644;&#20559;&#20506;&#35780;&#20272;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#29289;&#21307;&#23398;QA&#31995;&#32479;&#30340;&#25928;&#29992;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21253;&#25324;&#20102;79&#20010;&#30740;&#31350;&#65292;&#24182;&#30830;&#23450;&#20102;&#20027;&#39064;&#65292;&#21253;&#25324;&#38382;&#39064;&#30340;&#30495;&#23454;&#24615;&#65292;&#31572;&#26696;&#30340;&#21487;&#38752;&#24615;&#65292;&#31572;&#26696;&#30340;&#25928;&#29992;&#65292;&#20020;&#24202;&#29305;&#27530;&#24615;&#65292;&#31995;&#32479;&#21644;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Question answering (QA) systems have the potential to improve the quality of clinical care by providing health professionals with the latest and most relevant evidence. However, QA systems have not been widely adopted. This systematic review aims to characterize current medical QA systems, assess their suitability for healthcare, and identify areas of improvement.   Materials and methods: We searched PubMed, IEEE Xplore, ACM Digital Library, ACL Anthology and forward and backward citations on 7th February 2023. We included peer-reviewed journal and conference papers describing the design and evaluation of biomedical QA systems. Two reviewers screened titles, abstracts, and full-text articles. We conducted a narrative synthesis and risk of bias assessment for each study. We assessed the utility of biomedical QA systems.   Results: We included 79 studies and identified themes, including question realism, answer reliability, answer utility, clinical specialism, systems, usabili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#30340;&#21442;&#19982;&#24335;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#12289;&#21327;&#20316;&#29983;&#25104;&#21644;&#21453;&#39304;&#36845;&#20195;&#65292;&#35299;&#20915;&#20102;&#31038;&#21306;&#32423;&#22303;&#22320;&#21033;&#29992;&#20219;&#21153;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;LLM&#22312;&#19981;&#21516;&#35268;&#21010;&#22330;&#26223;&#19978;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#36229;&#36807;&#20154;&#31867;&#19987;&#23478;&#28385;&#24847;&#24230;&#21644;&#21253;&#23481;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#26381;&#21153;&#21644;&#29983;&#24577;&#26041;&#38754;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.01698</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#21442;&#19982;&#24335;&#22478;&#24066;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Large language model empowered participatory urban planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#30340;&#21442;&#19982;&#24335;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#12289;&#21327;&#20316;&#29983;&#25104;&#21644;&#21453;&#39304;&#36845;&#20195;&#65292;&#35299;&#20915;&#20102;&#31038;&#21306;&#32423;&#22303;&#22320;&#21033;&#29992;&#20219;&#21153;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;LLM&#22312;&#19981;&#21516;&#35268;&#21010;&#22330;&#26223;&#19978;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#36229;&#36807;&#20154;&#31867;&#19987;&#23478;&#28385;&#24847;&#24230;&#21644;&#21253;&#23481;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#26381;&#21153;&#21644;&#29983;&#24577;&#26041;&#38754;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#19982;&#24335;&#22478;&#24066;&#35268;&#21010;&#26159;&#29616;&#20195;&#22478;&#24066;&#35268;&#21010;&#30340;&#20027;&#27969;&#65292;&#28041;&#21450;&#21508;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#31215;&#26497;&#21442;&#19982;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#21442;&#19982;&#24335;&#33539;&#24335;&#22312;&#26102;&#38388;&#21644;&#20154;&#21147;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#29983;&#25104;&#24335;&#35268;&#21010;&#24037;&#20855;&#22312;&#25552;&#20379;&#21487;&#35843;&#25972;&#21644;&#21253;&#23481;&#24615;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#22833;&#36133;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22478;&#24066;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#21442;&#19982;&#36807;&#31243;&#20013;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;LLM&#20195;&#29702;&#65292;&#21253;&#25324;&#35282;&#33394;&#25198;&#28436;&#12289;&#21327;&#20316;&#29983;&#25104;&#21644;&#21453;&#39304;&#36845;&#20195;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#26381;&#21153;&#20110;1000&#20010;&#19981;&#21516;&#21033;&#30410;&#30340;&#31038;&#21306;&#32423;&#22303;&#22320;&#21033;&#29992;&#20219;&#21153;&#12290;&#22312;&#21508;&#31181;&#22478;&#24066;&#31038;&#21306;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;&#19981;&#21516;&#35268;&#21010;&#22330;&#26223;&#19978;&#30340;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26681;&#25454;&#22235;&#20010;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#22312;&#28385;&#24847;&#24230;&#21644;&#21253;&#23481;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#19987;&#23478;&#65292;&#24182;&#22312;&#26381;&#21153;&#21644;&#29983;&#24577;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.01695</link><description>&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Guided World Models: A Model-Based Approach to AI Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01695
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#23433;&#35013;&#21040;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#65292;&#20026;&#20154;&#31867;&#19982;&#36825;&#20123;&#20195;&#29702;&#27807;&#36890;&#21644;&#25511;&#21046;&#25171;&#24320;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#28192;&#36947;&#12290;&#38500;&#20102;&#26356;&#26032;&#20195;&#29702;&#31574;&#30053;&#65292;&#20154;&#31867;&#36824;&#21487;&#20197;&#20462;&#25913;&#20182;&#20204;&#30340;&#20869;&#37096;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#24433;&#21709;&#20195;&#29702;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#38590;&#20197;&#36866;&#24212;&#20154;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#33258;&#28982;&#30340;&#36890;&#20449;&#30028;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;&#23427;&#20204;&#36824;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#26368;&#21021;&#29992;&#20110;&#25351;&#23548;&#20154;&#31867;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#33258;&#25105;&#23398;&#20064;&#12290;&#20026;&#20102;&#20419;&#36827;LWMs&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;MESSENGER&#28216;&#25103;&#65288;Hanjie&#31561;&#20154;&#65292;2021&#65289;&#30340;&#25361;&#25112;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#26032;&#22330;&#26223;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
&lt;/p&gt;</description></item><item><title>ARGS&#26159;&#19968;&#20010;&#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#23558;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#35843;&#25972;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#35821;&#20041;&#22810;&#26679;&#24615;&#19988;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#32500;&#24230;&#19979;&#65292;ARGS&#20855;&#26377;&#25345;&#32493;&#30340;&#22870;&#21169;&#22686;&#30410;&#65292;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01694</link><description>&lt;p&gt;
ARGS: &#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
ARGS: Alignment as Reward-Guided Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01694
&lt;/p&gt;
&lt;p&gt;
ARGS&#26159;&#19968;&#20010;&#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#23558;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#35843;&#25972;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#35821;&#20041;&#22810;&#26679;&#24615;&#19988;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#32500;&#24230;&#19979;&#65292;ARGS&#20855;&#26377;&#25345;&#32493;&#30340;&#22870;&#21169;&#22686;&#30410;&#65292;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30446;&#26631;&#23545;&#40784;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#28982;&#32780;&#24120;&#35265;&#30340;&#26041;&#27861;&#21253;&#25324;RLHF&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#19981;&#31283;&#23450;&#21644;&#36164;&#28304;&#23494;&#38598;&#30340;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;ARGS&#65292;&#21363;&#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;&#65292;&#23427;&#23558;&#23545;&#40784;&#34701;&#20837;&#21040;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;RL&#35757;&#32451;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#20351;&#29992;&#22870;&#21169;&#20449;&#21495;&#35843;&#25972;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;ARGS&#29983;&#25104;&#20855;&#26377;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#20026;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#19988;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#30340;&#27169;&#22411;&#32500;&#24230;&#19979;&#65292;ARGS&#30456;&#23545;&#20110;&#22522;&#32447;&#26174;&#31034;&#20986;&#25345;&#32493;&#30340;&#22870;&#21169;&#25913;&#36827;&#12290;&#20363;&#22914;&#65292;&#37319;&#29992;&#30456;&#21516;&#30340;&#36138;&#23146;&#35299;&#30721;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#25552;&#39640;&#20102;19.56%&#30340;&#24179;&#22343;&#22870;&#21169;&#65292;&#24182;&#22312;GPT-4&#35780;&#20272;&#20013;&#33719;&#24471;&#20102;64.33%&#30340;&#20559;&#22909;&#25110;&#24182;&#21015;&#20998;&#25968;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#24378;&#35843;&#20102;&#35299;&#30721;&#30340;&#21019;&#26032;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing deco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24739;&#32773;&#30456;&#27604;&#65292;&#35299;&#31572;&#38750;&#19987;&#19994;&#24739;&#32773;&#20851;&#20110;&#23454;&#39564;&#23460;&#27979;&#35797;&#32467;&#26524;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#30456;&#20851;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26377;&#24110;&#21161;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#65292;&#20294;&#36824;&#23384;&#22312;&#28508;&#22312;&#38382;&#39064;&#38656;&#35201;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.01693</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#19982;&#20154;&#31867;&#24739;&#32773;&#30456;&#27604;&#65292;&#23545;&#38750;&#19987;&#19994;&#24739;&#32773;&#35299;&#37322;&#23454;&#39564;&#23460;&#27979;&#35797;&#32467;&#26524;&#30340;&#22238;&#31572;&#36136;&#37327;&#30340;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24739;&#32773;&#30456;&#27604;&#65292;&#35299;&#31572;&#38750;&#19987;&#19994;&#24739;&#32773;&#20851;&#20110;&#23454;&#39564;&#23460;&#27979;&#35797;&#32467;&#26524;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#30456;&#20851;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26377;&#24110;&#21161;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#65292;&#20294;&#36824;&#23384;&#22312;&#28508;&#22312;&#38382;&#39064;&#38656;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#23460;&#26816;&#27979;&#32467;&#26524;&#24120;&#24120;&#20196;&#20154;&#22256;&#24785;&#21644;&#38590;&#20197;&#29702;&#35299;&#12290;&#22823;&#22411;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#20102;&#33719;&#21462;&#38382;&#39064;&#31572;&#26696;&#30340;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#20351;&#29992;LLMs&#22238;&#31572;&#24739;&#32773;&#26377;&#20851;&#23454;&#39564;&#23460;&#27979;&#35797;&#38382;&#39064;&#30340;&#30456;&#20851;&#12289;&#20934;&#30830;&#12289;&#26377;&#24110;&#21161;&#24182;&#19988;&#26080;&#23475;&#30340;&#22238;&#31572;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#35782;&#21035;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#26041;&#27861;&#26469;&#32531;&#35299;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;Yahoo! Answers&#25910;&#38598;&#20102;&#19982;&#23454;&#39564;&#23460;&#27979;&#35797;&#32467;&#26524;&#30456;&#20851;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#25968;&#25454;&#65292;&#24182;&#36873;&#25321;&#20102;53&#20010;&#38382;&#31572;&#23545;&#36827;&#34892;&#26412;&#30740;&#31350;&#12290;&#20351;&#29992;LangChain&#26694;&#26550;&#21644;ChatGPT&#20114;&#32852;&#32593;&#38376;&#25143;&#65292;&#25105;&#20204;&#20174;&#22235;&#20010;LLMs&#65288;&#21253;&#25324;GPT-4&#12289;Meta LLaMA 2&#12289;MedAlpaca&#21644;ORCA_mini&#65289;&#29983;&#25104;&#20102;&#23545;&#36825;53&#20010;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#26631;&#20934;&#30340;&#38382;&#31572;&#30456;&#20284;&#24230;&#35780;&#20272;&#25351;&#26631;&#65288;&#21253;&#25324;ROUGE&#12289;BLEU&#12289;METEOR&#21644;BERTScore&#65289;&#35780;&#20272;&#20102;&#23427;&#20204;&#22238;&#31572;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#22522;&#20110;LLMs&#30340;&#35780;&#20272;&#22120;&#21028;&#26029;&#30446;&#26631;&#27169;&#22411;&#22312;&#30456;&#20851;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26377;&#24110;&#21161;&#24615;&#26041;&#38754;&#30340;&#36136;&#37327;&#26159;&#21542;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lab results are often confusing and hard to understand. Large language models (LLMs) such as ChatGPT have opened a promising avenue for patients to get their questions answered. We aim to assess the feasibility of using LLMs to generate relevant, accurate, helpful, and unharmful responses to lab test-related questions asked by patients and to identify potential issues that can be mitigated with augmentation approaches. We first collected lab test results related question and answer data from Yahoo! Answers and selected 53 QA pairs for this study. Using the LangChain framework and ChatGPT web portal, we generated responses to the 53 questions from four LLMs including GPT-4, Meta LLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their answers using standard QA similarity-based evaluation metrics including ROUGE, BLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge whether a target model has higher quality in terms of relevance, correctness, helpf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32452;&#32455;&#36127;&#36131;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27835;&#29702;&#20013;&#30340;&#31639;&#27861;&#23457;&#26597;&#22996;&#21592;&#20250;&#65288;ARBs&#65289;&#30340;&#23454;&#36341;&#24773;&#20917;&#21644;&#25104;&#26524;&#65292;&#21457;&#29616;&#20102;&#26469;&#33258;&#19981;&#21516;&#31867;&#22411;&#32452;&#32455;&#21644;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#23450;&#20041;&#21644;&#20869;&#37096;&#27835;&#29702;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01691</link><description>&lt;p&gt;
&#25506;&#32034;&#32452;&#32455;&#36127;&#36131;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#30340;&#31639;&#27861;&#23457;&#26597;&#22996;&#21592;&#20250;
&lt;/p&gt;
&lt;p&gt;
Investigating Algorithm Review Boards for Organizational Responsible Artificial Intelligence Governance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32452;&#32455;&#36127;&#36131;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27835;&#29702;&#20013;&#30340;&#31639;&#27861;&#23457;&#26597;&#22996;&#21592;&#20250;&#65288;ARBs&#65289;&#30340;&#23454;&#36341;&#24773;&#20917;&#21644;&#25104;&#26524;&#65292;&#21457;&#29616;&#20102;&#26469;&#33258;&#19981;&#21516;&#31867;&#22411;&#32452;&#32455;&#21644;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#23450;&#20041;&#21644;&#20869;&#37096;&#27835;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#12289;&#38750;&#33829;&#21033;&#32452;&#32455;&#12289;&#25919;&#24220;&#21644;&#23398;&#26415;&#26426;&#26500;&#31561;&#32452;&#32455;&#36234;&#26469;&#36234;&#22810;&#22320;&#24320;&#21457;&#12289;&#37096;&#32626;&#21644;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20855;&#12290;&#36127;&#36131;&#20219;&#30340;AI&#65288;RAI&#65289;&#27835;&#29702;&#26041;&#27861;&#22312;&#32452;&#32455;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#24212;&#23545;&#28508;&#22312;&#30340;AI&#39118;&#38505;&#21644;&#21361;&#23475;&#12290;&#26412;&#30740;&#31350;&#37319;&#35775;&#20102;17&#20301;&#25216;&#26415;&#36129;&#29486;&#32773;&#65292;&#28085;&#30422;&#20102;&#23398;&#26415;&#30028;&#12289;&#25919;&#24220;&#12289;&#34892;&#19994;&#21644;&#38750;&#33829;&#21033;&#32452;&#32455;&#31561;&#21508;&#31181;&#31867;&#22411;&#30340;&#32452;&#32455;&#20197;&#21450;&#37329;&#34701;&#12289;&#20581;&#24247;&#12289;&#31185;&#25216;&#21644;&#20854;&#20182;&#39046;&#22495;&#65292;&#20102;&#35299;&#20182;&#20204;&#23545;&#20869;&#37096;RAI&#27835;&#29702;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;RAI&#30340;&#22810;&#26679;&#21270;&#23450;&#20041;&#20197;&#21450;&#30456;&#24212;&#30340;&#20869;&#37096;&#27835;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#23545;&#31639;&#27861;&#23457;&#26597;&#22996;&#21592;&#20250;&#65288;ARBs&#65289;&#21644;&#31867;&#20284;&#30340;&#23457;&#26597;&#22996;&#21592;&#20250;&#30340;&#39318;&#27425;&#35814;&#32454;&#30740;&#31350;&#32467;&#26524;&#65292;&#21253;&#25324;&#20854;&#25104;&#21592;&#36164;&#26684;&#12289;&#33539;&#22260;&#21644;&#34913;&#37327;&#25104;&#21151;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#23454;&#20102;&#37329;&#34701;&#34892;&#19994;&#22312;&#27169;&#22411;&#27835;&#29702;&#26041;&#38754;&#30340;&#31283;&#20581;&#23454;&#36341;&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#20581;&#24247;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#30340;&#31639;&#27861;&#21644;AI&#27835;&#29702;&#65292;&#21253;&#25324;ARB&#31867;&#20284;&#30340;&#23457;&#26597;&#22996;&#21592;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organizations including companies, nonprofits, governments, and academic institutions are increasingly developing, deploying, and utilizing artificial intelligence (AI) tools. Responsible AI (RAI) governance approaches at organizations have emerged as important mechanisms to address potential AI risks and harms. In this work, we interviewed 17 technical contributors across organization types (Academic, Government, Industry, Nonprofit) and sectors (Finance, Health, Tech, Other) about their experiences with internal RAI governance. Our findings illuminated the variety of organizational definitions of RAI and accompanying internal governance approaches. We summarized the first detailed findings on algorithm review boards (ARBs) and similar review committees in practice, including their membership, scope, and measures of success. We confirmed known robust model governance in finance sectors and revealed extensive algorithm and AI governance with ARB-like review boards in health sectors. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#20998;&#23618;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#65288;HEMS&#65289;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#21487;&#20877;&#29983;&#33021;&#28304;&#31038;&#21306;&#65288;REC&#65289;&#30340;&#25104;&#26412;&#65292;&#20197;&#35780;&#20272;&#20854;&#30456;&#23545;&#20110;&#26412;&#22320;&#33258;&#28040;&#32791;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01688</link><description>&lt;p&gt;
&#19968;&#20010;&#31526;&#21512;&#24403;&#21069;&#25216;&#26415;&#31435;&#27861;&#26694;&#26550;&#30340;&#33021;&#28304;&#31038;&#21306;&#22312;&#32447;&#20998;&#23618;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Online Hierarchical Energy Management System for Energy Communities, Complying with the Current Technical Legislation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#20998;&#23618;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#65288;HEMS&#65289;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#21487;&#20877;&#29983;&#33021;&#28304;&#31038;&#21306;&#65288;REC&#65289;&#30340;&#25104;&#26412;&#65292;&#20197;&#35780;&#20272;&#20854;&#30456;&#23545;&#20110;&#26412;&#22320;&#33258;&#28040;&#32791;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#26007;&#20105;&#20013;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26032;&#33021;&#28304;&#25928;&#29575;&#31574;&#30053;&#12290;2018&#24180;&#65292;&#27431;&#27954;&#32852;&#30431;&#65288;EU&#65289;&#22312;&#36866;&#24403;&#30340;&#31435;&#27861;&#26694;&#26550;&#19979;&#65292;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#31038;&#21306;&#65288;REC&#65289;&#23450;&#20041;&#20026;&#19968;&#20010;&#21442;&#19982;&#32773;&#20849;&#20139;&#33258;&#24049;&#29983;&#20135;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#23616;&#37096;&#30005;&#32593;&#65292;&#26088;&#22312;&#36890;&#36807;&#21512;&#29702;&#30340;&#28608;&#21169;&#25514;&#26045;&#38477;&#20302;&#30005;&#36153;&#25104;&#26412;&#12290;&#36825;&#19968;&#20030;&#25514;&#26088;&#22312;&#21152;&#36895;&#26412;&#22320;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#30340;&#26222;&#21450;&#65292;&#20854;&#25104;&#26412;&#21487;&#33021;&#26080;&#27861;&#25215;&#21463;&#12290;&#30001;&#20110;REC&#22312;&#25216;&#26415;&#19978;&#23646;&#20110;&#26234;&#33021;&#30005;&#32593;&#65292;&#22240;&#27492;&#19978;&#36848;&#31574;&#30053;&#21487;&#20197;&#24212;&#29992;&#65292;&#24182;&#19988;&#38656;&#35201;&#23454;&#38469;&#30340;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#65288;EMS&#65289;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32508;&#21512;&#20102;&#19968;&#20010;&#22312;&#32447;&#20998;&#23618;EMS&#65288;HEMS&#65289;&#65292;&#20197;&#26368;&#23567;&#21270;REC&#25104;&#26412;&#65292;&#35780;&#20272;&#20854;&#30456;&#23545;&#20110;&#26412;&#22320;&#33258;&#28040;&#32791;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#35748;&#30495;&#36981;&#24490;&#27431;&#30431;&#25216;&#26415;&#25351;&#31034;&#65288;&#28304;&#33258;&#24847;&#22823;&#21033;&#65289;&#65292;&#26088;&#22312;&#33719;&#24471;&#23613;&#21487;&#33021;&#30495;&#23454;&#30340;&#32467;&#26524;&#12290;REC&#20043;&#38388;&#30340;&#21151;&#29575;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Efforts in the fight against Climate Change are increasingly oriented towards new energy efficiency strategies in Smart Grids (SGs). In 2018, with proper legislation, the European Union (EU) defined the Renewable Energy Community (REC) as a local electrical grid whose participants share their self-produced renewable energy, aiming at reducing bill costs by taking advantage of proper incentives. That action aspires to accelerate the spread of local renewable energy exploitation, whose costs could not be within everyone's reach. Since a REC is technically an SG, the strategies above can be applied, and specifically, practical Energy Management Systems (EMSs) are required. Therefore, in this work, an online Hierarchical EMS (HEMS) is synthesized for REC cost minimization to evaluate its superiority over a local self-consumption approach. EU technical indications (as inherited from Italy) are diligently followed, aiming for results that are as realistic as possible. Power flows between REC
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#20132;&#36890;&#39046;&#22495;&#30340;&#25968;&#23383;&#23402;&#29983;&#36827;&#34892;&#20102;&#31995;&#32479;&#26144;&#23556;&#65292;&#21457;&#29616;&#30446;&#21069;&#22823;&#22810;&#25968;&#30740;&#31350;&#23616;&#38480;&#20110;&#31995;&#32479;&#30417;&#27979;&#25110;&#25925;&#38556;&#26816;&#27979;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#25968;&#23383;&#23402;&#29983;&#22312;&#35786;&#26029;&#25512;&#29702;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01686</link><description>&lt;p&gt;
&#20132;&#36890;&#39046;&#22495;&#35786;&#26029;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#30340;&#31995;&#32479;&#26144;&#23556;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Systematic Mapping Study of Digital Twins for Diagnosis in Transportation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01686
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#20132;&#36890;&#39046;&#22495;&#30340;&#25968;&#23383;&#23402;&#29983;&#36827;&#34892;&#20102;&#31995;&#32479;&#26144;&#23556;&#65292;&#21457;&#29616;&#30446;&#21069;&#22823;&#22810;&#25968;&#30740;&#31350;&#23616;&#38480;&#20110;&#31995;&#32479;&#30417;&#27979;&#25110;&#25925;&#38556;&#26816;&#27979;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#25968;&#23383;&#23402;&#29983;&#22312;&#35786;&#26029;&#25512;&#29702;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#23383;&#23402;&#29983;&#24050;&#34987;&#25552;&#20986;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#20854;&#28508;&#22312;&#24212;&#29992;&#33539;&#22260;&#20174;&#21407;&#22411;&#21046;&#20316;&#21040;&#32500;&#25252;&#12290;&#26410;&#26469;&#65292;&#25968;&#23383;&#23402;&#29983;&#23558;&#25512;&#21160;&#35768;&#22810;&#39640;&#25928;&#21487;&#25345;&#32493;&#30340;&#25216;&#26415;&#24212;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#22823;&#37327;&#30740;&#31350;&#65292;&#23398;&#26415;&#30028;&#23545;&#25968;&#23383;&#23402;&#29983;&#30340;&#30830;&#20999;&#23450;&#20041;&#20197;&#21450;&#20854;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#23578;&#26410;&#36798;&#25104;&#19968;&#33268;&#12290;&#20026;&#20102;&#22686;&#36827;&#25105;&#20204;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25506;&#35752;&#25968;&#23383;&#23402;&#29983;&#22312;&#20132;&#36890;&#39046;&#22495;&#35786;&#26029;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#31995;&#32479;&#26144;&#23556;&#30740;&#31350;&#65292;&#21253;&#25324;&#36710;&#36742;&#21644;&#20854;&#32452;&#20214;&#20197;&#21450;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;&#25105;&#20204;&#21457;&#29616;&#24456;&#23569;&#26377;&#20851;&#25968;&#23383;&#23402;&#29983;&#30340;&#35770;&#25991;&#25551;&#36848;&#20102;&#20219;&#20309;&#35786;&#26029;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20284;&#20046;&#20165;&#38480;&#20110;&#31995;&#32479;&#30417;&#27979;&#25110;&#25925;&#38556;&#26816;&#27979;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#25105;&#20204;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#26469;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#36827;&#34892;&#35786;&#26029;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, digital twins have been proposed and implemented in various fields with potential applications ranging from prototyping to maintenance. Going forward, they are to enable numerous efficient and sustainable technologies, among them autonomous cars. However, despite a large body of research in many fields, academics have yet to agree on what exactly a digital twin is -- and as a result, what its capabilities and limitations might be. To further our understanding, we explore the capabilities of digital twins concerning diagnosis in the field of transportation. We conduct a systematic mapping study including digital twins of vehicles and their components, as well as transportation infrastructure. We discovered that few papers on digital twins describe any diagnostic process. Furthermore, most existing approaches appear limited to system monitoring or fault detection. These findings suggest that we need more research for diagnostic reasoning utilizing digital twins.
&lt;/p&gt;</description></item><item><title>SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01685</link><description>&lt;p&gt;
SMUTF&#65306;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#21644;&#28151;&#21512;&#29305;&#24449;&#30340;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMUTF: Schema Matching Using Generative Tags and Hybrid Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01685
&lt;/p&gt;
&lt;p&gt;
SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SMUTF&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20551;&#35774;&#22312;&#24320;&#25918;&#22495;&#20219;&#21153;&#20013;&#65292;&#30417;&#30563;&#23398;&#20064;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36328;&#22495;&#21305;&#37197;&#12290;&#36825;&#20010;&#31995;&#32479;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#20154;&#36947;&#20027;&#20041;&#20132;&#25442;&#35821;&#35328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#29983;&#25104;&#26631;&#31614;&#8221;&#20026;&#27599;&#20010;&#25968;&#25454;&#21015;&#37096;&#32626;&#20102;&#21019;&#26032;&#30340;&#36866;&#24212;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#21305;&#37197;&#30340;&#25928;&#26524;&#12290;SMUTF&#20855;&#26377;&#24191;&#27867;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;&#20998;&#31867;&#26041;&#27861;&#21644;&#29983;&#25104;&#27169;&#22411;&#26080;&#32541;&#37197;&#21512;&#20351;&#29992;&#12290;&#37492;&#20110;&#27169;&#24335;&#21305;&#37197;&#32570;&#20047;&#24191;&#27867;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24050;&#32463;&#21019;&#24314;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#20844;&#20849;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#26159;&#30446;&#21069;&#26368;&#20840;&#38754;&#30340;&#27169;&#24335;&#21305;&#37197;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.   Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated excep
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;CGC-LORA&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25671;&#25670;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01684</link><description>&lt;p&gt;
&#20351;&#29992;CGC-LORA&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01684
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;CGC-LORA&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25671;&#25670;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#28436;&#36827;&#65292;&#20154;&#20204;&#20026;&#20102;&#26377;&#25928;&#22320;&#24494;&#35843;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;LLMs&#20197;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#29305;&#23450;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#26377;&#20004;&#31181;&#20027;&#35201;&#30340;&#36866;&#24212;&#26041;&#24335;&#65306;&#65288;i&#65289;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#65306;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#30456;&#24212;&#35757;&#32451;&#26679;&#26412;&#23545;&#39044;&#35757;&#32451;LLMs&#36827;&#34892;&#29420;&#31435;&#30340;&#24494;&#35843;&#65307;&#65288;ii&#65289;&#38598;&#25104;&#27169;&#22411;&#65306;&#20351;&#29992;&#25152;&#26377;&#20219;&#21153;&#30340;&#26679;&#26412;&#26469;&#32852;&#21512;&#24494;&#35843;&#39044;&#35757;&#32451;LLMs &#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25671;&#25670;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#21046;&#38376;&#25511;&#65288;CGC&#65289;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;&#20102;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;MTL&#65288;&#21363;CGC&#65289;&#21644;PEFT&#65288;&#21363;LoRA&#65289;&#26041;&#26696;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#20219;&#21153;&#38598;&#32676;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#23618;&#65292;&#20854;&#20013;&#21253;&#21547;...
&lt;/p&gt;
&lt;p&gt;
With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain. In practice, there are two prevailing ways, in which the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently using the corresponding training samples from each task. (ii) An Integrated Model: Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. To address the high computing cost and seesawing issue simultaneously, we propose a unified framework that implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an innovative layer that contai
&lt;/p&gt;</description></item><item><title>&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.01681</link><description>&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#35299;&#23494;&#65306;&#21033;&#29992;ChatGPT&#25552;&#21319;&#31038;&#20132;&#23186;&#20307;&#27807;&#36890;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01681
&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#22312;&#31038;&#20132;&#32593;&#32476;&#27807;&#36890;&#20013;&#24050;&#32463;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#20204;&#25215;&#36733;&#20102;&#36229;&#36234;&#25991;&#23383;&#25110;&#30701;&#35821;&#30340;&#35821;&#20041;&#65292;&#36825;&#24341;&#21457;&#20102;&#23398;&#26415;&#30028;&#23545;&#20854;&#23646;&#24615;&#21644;&#21151;&#33021;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#34920;&#24773;&#31526;&#21495;&#30456;&#20851;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#32773;&#36890;&#24120;&#20381;&#36182;&#20247;&#21253;&#26469;&#27880;&#37322;&#34920;&#24773;&#31526;&#21495;&#65292;&#20197;&#20102;&#35299;&#20854;&#24773;&#24863;&#12289;&#20351;&#29992;&#24847;&#22270;&#21644;&#35821;&#20041;&#21547;&#20041;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#30340;&#20027;&#35266;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#35823;&#35299;&#65292;&#24182;&#36896;&#25104;&#27807;&#36890;&#38556;&#30861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#27880;&#37322;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;ChatGPT&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#19987;&#19994;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#20197;&#21069;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39564;&#35777;ChatGPT&#21487;&#20197;&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#20316;&#20026;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#21487;&#34892;&#26367;&#20195;&#32773;&#65292;&#24182;&#39564;&#35777;&#20854;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31687;&#32508;&#36848;&#32473;&#20986;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#37325;&#35201;&#26041;&#38754;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01680</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65306;&#36827;&#23637;&#19982;&#25361;&#25112;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Model based Multi-Agents: A Survey of Progress and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01680
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31687;&#32508;&#36848;&#32473;&#20986;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#37325;&#35201;&#26041;&#38754;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;LLMs&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35268;&#21010;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#23427;&#20204;&#34987;&#29992;&#20316;&#33258;&#20027;&#26234;&#33021;&#20307;&#26469;&#33258;&#21160;&#23436;&#25104;&#35768;&#22810;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#23558;&#19968;&#20010;LLM&#29992;&#20316;&#21333;&#20010;&#35268;&#21010;&#25110;&#20915;&#31574;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#20026;&#31038;&#21306;&#25552;&#20379;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#39046;&#22495;&#30340;&#32508;&#36848;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22522;&#26412;&#26041;&#38754;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35753;&#35835;&#32773;&#23545;&#20197;&#19979;&#38382;&#39064;&#33719;&#24471;&#23454;&#36136;&#24615;&#35265;&#35299;&#65306;LLM-based&#22810;&#26234;&#33021;&#20307;&#27169;&#25311;&#21738;&#20123;&#39046;&#22495;&#21644;&#29615;&#22659;&#65311;&#36825;&#20123;&#26234;&#33021;&#20307;&#26159;&#22914;&#20309;&#24314;&#27169;&#21644;&#36890;&#20449;&#30340;&#65311;&#20160;&#20040;&#26426;&#21046;&#26377;&#21161;&#20110;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22686;&#38271;&#65311;&#23545;&#20110;&#37027;&#20123;&#23545;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#24863;&#20852;&#36259;&#30340;&#20154;&#65292;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#19968;&#20123;&#35201;&#28857;&#21644;&#25361;&#25112;.
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#35813;&#20195;&#29702;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#20026;&#20102;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01679</link><description>&lt;p&gt;
StickerConv: &#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#22810;&#27169;&#24577;&#20849;&#24773;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
StickerConv: Generating Multimodal Empathetic Responses from Scratch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#35813;&#20195;&#29702;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#20026;&#20102;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#20849;&#24773;&#23545;&#35805;&#30740;&#31350;&#20013;&#65292;&#36148;&#32440;&#23613;&#31649;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#25552;&#39640;&#22312;&#32447;&#20132;&#27969;&#20013;&#30340;&#20849;&#24773;&#33021;&#21147;&#65292;&#20294;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#21253;&#25324;12.9K&#20010;&#23545;&#35805;&#20250;&#35805;&#65292;5.8K&#20010;&#29420;&#29305;&#36148;&#32440;&#21644;2K&#20010;&#22810;&#26679;&#21270;&#20250;&#35805;&#22330;&#26223;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#22810;&#27169;&#24577;&#24773;&#22659;&#19979;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;LLM&#30340;&#20840;&#38754;&#20849;&#24773;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PEGS&#22312;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research. In this paper, we introduce the Agent for StickerConv (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, StickerConv, which includes 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios, specifically designs to augment the generation of empathetic responses in a multimodal context. To leverage the richness of this dataset, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation model, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotional
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#23884;&#20837;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21253;&#21547;&#39046;&#22495;&#20869;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#21363;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22806;&#24310;&#30693;&#35782;&#25552;&#20379;&#20851;&#20110;&#26412;&#20307;&#20013;&#29305;&#23450;&#27010;&#24565;&#25152;&#23646;&#30340;&#20855;&#20307;&#23454;&#20363;&#30340;&#20449;&#24687;&#65292;&#32780;&#20869;&#28085;&#30693;&#35782;&#35814;&#32454;&#25551;&#36848;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#20869;&#22312;&#23646;&#24615;&#12289;&#29305;&#24449;&#21644;&#35821;&#20041;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20805;&#20998;&#32771;&#34385;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EIKE&#65288;Extensional and Intensional Knowledge Embedding&#65289;&#30340;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#12290;EIKE&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#23454;&#20363;&#12289;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#23884;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#23545;&#22806;&#24310;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20869;&#28085;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20440;&#33719;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65292;&#19981;&#20165;&#25972;&#20307;&#20934;&#30830;&#29575;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01676</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#22312;&#20851;&#38190;&#35821;&#27861;&#32467;&#26500;&#19978;&#30340;&#21028;&#26029;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language models align with human judgments on key grammatical constructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20440;&#33719;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65292;&#19981;&#20165;&#25972;&#20307;&#20934;&#30830;&#29575;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#35821;&#35328;&#26222;&#36941;&#24615;&#65311;Dentella&#31561;&#20154;&#65288;2023&#24180;&#65307;&#8220;DGL&#8221;&#65289;&#20351;&#29992;&#22810;&#20010;LLMs&#25552;&#31034;&#35821;&#27861;&#27491;&#30830;&#24615;&#38382;&#39064;&#65292;&#20197;&#33719;&#21462;80&#20010;&#33521;&#35821;&#21477;&#23376;&#30340;&#35821;&#27861;&#21477;&#23376;&#21028;&#26029;&#65292;&#24471;&#20986;LLMs&#23384;&#22312;&#8220;&#26159;&#8221;&#20559;&#21521;&#21644;&#8220;&#19981;&#33021;&#21306;&#20998;&#35821;&#27861;&#21644;&#38750;&#35821;&#27861;&#21477;&#23376;&#8221;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26082;&#23450;&#30340;&#23454;&#36341;&#26041;&#27861;&#37325;&#26032;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;DGL&#30340;&#25968;&#25454;&#23454;&#38469;&#19978;&#35777;&#26126;&#20102;LLM&#22914;&#20309;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#34892;&#20026;&#12290;&#27169;&#22411;&#19981;&#20165;&#25972;&#20307;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#36824;&#25429;&#25417;&#21040;&#20102;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models (LLMs) make human-like linguistic generalizations? Dentella et al. (2023; "DGL") prompt several LLMs ("Is the following sentence grammatically correct in English?") to elicit grammaticality judgments of 80 English sentences, concluding that LLMs demonstrate a "yes-response bias" and a "failure to distinguish grammatical from ungrammatical sentences". We re-evaluate LLM performance using well-established practices and find that DGL's data in fact provide evidence for just how well LLMs capture human behaviors. Models not only achieve high accuracy overall, but also capture fine-grained variation in human linguistic judgments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21327;&#35758;&#25216;&#26415;&#30340;&#24212;&#29992;&#22312;&#36947;&#36335;&#20132;&#21449;&#21475;&#25293;&#21334;&#20013;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.01673</link><description>&lt;p&gt;
&#22522;&#20110;&#21327;&#35758;&#25216;&#26415;&#30340;&#24212;&#29992;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#65306;&#25293;&#21334;&#24335;&#36947;&#36335;&#20132;&#21449;&#21475;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Legal and ethical implications of applications based on agreement technologies: the case of auction-based road intersections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21327;&#35758;&#25216;&#26415;&#30340;&#24212;&#29992;&#22312;&#36947;&#36335;&#20132;&#21449;&#21475;&#25293;&#21334;&#20013;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#35758;&#25216;&#26415;&#26159;&#26500;&#24314;&#20998;&#24067;&#24335;&#26234;&#33021;&#31995;&#32479;&#30340;&#19968;&#31181;&#26032;&#33539; Paradigm&#65292;&#20854;&#20013;&#33258;&#20027;&#36719;&#20214;&#20195;&#29702;&#21830;&#20195;&#34920;&#20182;&#20204;&#30340;&#20154;&#31867;&#29992;&#25143;&#36827;&#34892;&#35848;&#21028;&#20197;&#36798;&#25104;&#21327;&#35758;&#12290;&#26234;&#33021;&#22478;&#24066;&#26159;&#21327;&#35758;&#25216;&#26415;&#30340;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#12290;&#23613;&#31649;&#26377;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#21644;&#21407;&#22411;&#23384;&#22312;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#36317;&#31163;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25237;&#20837;&#20351;&#29992;&#36824;&#26377;&#24456;&#38271;&#30340;&#36317;&#31163;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#26410;&#26469;&#26234;&#33021;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#25293;&#21334;&#24335;&#36947;&#36335;&#20132;&#21449;&#21475;&#30340;&#26696;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#25216;&#26415;&#35201;&#32032;&#24050;&#32463;&#21487;&#29992;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20043;&#21069;&#20173;&#38656;&#35201;&#35299;&#20915;&#22810;&#20010;&#38750;&#25216;&#26415;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#22269;&#38469;&#27861;&#35268;&#21644;&#35199;&#29677;&#29273;&#31435;&#27861;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#22522;&#20110;&#25293;&#21334;&#30340;&#36947;&#36335;&#20132;&#21449;&#21475;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#20010;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agreement Technologies refer to a novel paradigm for the construction of distributed intelligent systems, where autonomous software agents negotiate to reach agreements on behalf of their human users. Smart Cities are a key application domain for Agreement Technologies. While several proofs of concept and prototypes exist, such systems are still far from ready for being deployed in the real-world. In this paper we focus on a novel method for managing elements of smart road infrastructures of the future, namely the case of auction-based road intersections. We show that, even though the key technological elements for such methods are already available, there are multiple non-technical issues that need to be tackled before they can be applied in practice. For this purpose, we analyse legal and ethical implications of auction-based road intersections in the context of international regulations and from the standpoint of the Spanish legislation. From this exercise, we extract a set of requi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#25972;&#21512;&#30693;&#35782;&#32467;&#26500;&#21644;&#30693;&#35782;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32773;&#36712;&#36857;&#21457;&#29616;&#28508;&#22312;&#30340;&#20808;&#20915;&#26465;&#20214;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;&#20869;&#23481;&#25512;&#33616;&#21644;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01672</link><description>&lt;p&gt;
&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30340;&#20808;&#20915;&#26465;&#20214;&#32467;&#26500;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Prerequisite Structure Discovery in Intelligent Tutoring Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#25972;&#21512;&#30693;&#35782;&#32467;&#26500;&#21644;&#30693;&#35782;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32773;&#36712;&#36857;&#21457;&#29616;&#28508;&#22312;&#30340;&#20808;&#20915;&#26465;&#20214;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;&#20869;&#23481;&#25512;&#33616;&#21644;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#65292;&#30693;&#35782;&#32467;&#26500;&#65288;KS&#65289;&#21644;&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#23545;&#25552;&#39640;&#25945;&#32946;&#20869;&#23481;&#25512;&#33616;&#30340;&#37325;&#35201;&#24615;&#12290;KS&#34920;&#31034;&#19981;&#21516;&#30693;&#35782;&#32452;&#20214;&#65288;KCs&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;KT&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#36807;&#21435;&#21382;&#21490;&#26469;&#39044;&#27979;&#20854;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#21253;&#25324;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;KS&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#21512;&#24182;&#21040;KT&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#20174;&#23398;&#20064;&#32773;&#36712;&#36857;&#20013;&#21457;&#29616;&#28508;&#22312;&#30340;KS&#12290;&#36890;&#36807;&#20351;&#29992;&#21457;&#29616;&#30340;KS&#36827;&#34892;&#20869;&#23481;&#25512;&#33616;&#65292;&#24182;&#21033;&#29992;&#27169;&#25311;&#23398;&#29983;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#65292;&#35780;&#20272;&#25581;&#31034;&#30340;KS&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the importance of Knowledge Structure (KS) and Knowledge Tracing (KT) in improving the recommendation of educational content in intelligent tutoring systems. The KS represents the relations between different Knowledge Components (KCs), while KT predicts a learner's success based on her past history. The contribution of this research includes proposing a KT model that incorporates the KS as a learnable parameter, enabling the discovery of the underlying KS from learner trajectories. The quality of the uncovered KS is assessed by using it to recommend content and evaluating the recommendation algorithm with simulated students.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#29983;&#36873;&#25321;&#65292;&#25913;&#36827;&#20102;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#12290;&#20351;&#29992;ZPDES&#31639;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65292;&#24182;&#22312;&#23454;&#22320;&#30740;&#31350;&#20013;&#25552;&#39640;&#20102;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#23398;&#20064;&#25104;&#32489;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23398;&#29983;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01669</link><description>&lt;p&gt;
&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#30340;&#25913;&#36827;&#65306;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#20064;&#32773;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Improved Performances and Motivation in Intelligent Tutoring Systems: Combining Machine Learning and Learner Choice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#29983;&#36873;&#25321;&#65292;&#25913;&#36827;&#20102;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#12290;&#20351;&#29992;ZPDES&#31639;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65292;&#24182;&#22312;&#23454;&#22320;&#30740;&#31350;&#20013;&#25552;&#39640;&#20102;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#23398;&#20064;&#25104;&#32489;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23398;&#29983;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#26657;&#20013;&#65292;&#22823;&#35268;&#27169;&#30340;&#35838;&#22530;&#35268;&#27169;&#32473;&#20010;&#24615;&#21270;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#25945;&#32946;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#35797;&#22270;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#36827;&#23637;&#20551;&#35774;&#65288;LPH&#65289;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;ZPDES&#31639;&#27861;&#23545;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65288;LP&#65289;&#30340;&#32451;&#20064;&#36827;&#34892;&#25490;&#24207;&#12290;&#35813;&#31639;&#27861;&#22312;&#20043;&#21069;&#30340;&#23454;&#22320;&#30740;&#31350;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#23558;&#23398;&#20064;&#34920;&#29616;&#25552;&#21319;&#21040;&#26356;&#24191;&#27867;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#65292;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#35838;&#31243;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;&#20854;&#21160;&#26426;&#24433;&#21709;&#23578;&#26410;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;ZPDES&#19981;&#20801;&#35768;&#23398;&#29983;&#21457;&#34920;&#36873;&#25321;&#24847;&#35265;&#12290;&#36825;&#31181;&#32570;&#20047;&#26426;&#26500;&#30340;&#38480;&#21046;&#19982;&#20851;&#27880;&#24314;&#27169;&#22909;&#22855;&#39537;&#21160;&#23398;&#20064;&#30340;LPH&#29702;&#35770;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#30740;&#31350;&#20102;&#36825;&#31181;&#36873;&#25321;&#21487;&#33021;&#24615;&#30340;&#24341;&#20837;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#12290;&#32473;&#23450;&#30340;&#36873;&#25321;&#19982;&#32451;&#20064;&#38590;&#24230;&#27491;&#20132;&#30340;&#32500;&#24230;&#26377;&#20851;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#36259;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large class sizes pose challenges to personalized learning in schools, which educational technologies, especially intelligent tutoring systems (ITS), aim to address. In this context, the ZPDES algorithm, based on the Learning Progress Hypothesis (LPH) and multi-armed bandit machine learning techniques, sequences exercises that maximize learning progress (LP). This algorithm was previously shown in field studies to boost learning performances for a wider diversity of students compared to a hand-designed curriculum. However, its motivational impact was not assessed. Also, ZPDES did not allow students to express choices. This limitation in agency is at odds with the LPH theory concerned with modeling curiosity-driven learning. We here study how the introduction of such choice possibilities impact both learning efficiency and motivation. The given choice concerns dimensions that are orthogonal to exercise difficulty, acting as a playful feature.   In an extensive field study (265 7-8 years
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#34394;&#25311;&#29616;&#23454;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#65292;&#20026;&#24739;&#26377;&#35829;&#35835;&#22256;&#38590;&#30340;&#23398;&#29983;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#25903;&#25345;&#65292;&#22635;&#34917;&#20102;&#39640;&#31561;&#25945;&#32946;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.01668</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#29616;&#23454;&#21644;&#20154;&#24037;&#26234;&#33021;&#30830;&#23450;&#24739;&#26377;&#35829;&#35835;&#22256;&#38590;&#30340;&#23398;&#29983;&#30340;&#22256;&#38590;&#65306;&#19968;&#39033;&#25506;&#32034;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Determining the Difficulties of Students With Dyslexia via Virtual Reality and Artificial Intelligence: An Exploratory Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#34394;&#25311;&#29616;&#23454;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#65292;&#20026;&#24739;&#26377;&#35829;&#35835;&#22256;&#38590;&#30340;&#23398;&#29983;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#25903;&#25345;&#65292;&#22635;&#34917;&#20102;&#39640;&#31561;&#25945;&#32946;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#38556;&#30861;&#26159;&#24433;&#21709;&#22823;&#33041;&#36830;&#25509;&#36890;&#35759;&#21306;&#22495;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#35829;&#35835;&#22256;&#38590;&#23398;&#29983;&#22312;&#38405;&#35835;&#12289;&#35760;&#24518;&#21644;&#29702;&#35299;&#27010;&#24565;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36890;&#36807;&#27835;&#30103;&#21644;&#21019;&#24314;&#34917;&#20607;&#26426;&#21046;&#65292;&#36825;&#20123;&#38382;&#39064;&#30340;&#31243;&#24230;&#21487;&#20197;&#24471;&#21040;&#32531;&#35299;&#12290;&#20026;&#38754;&#21521;&#23567;&#23398;&#21644;&#20013;&#23398;&#30340;&#29305;&#27530;&#23398;&#20064;&#38556;&#30861;&#23398;&#29983;&#21019;&#36896;&#25968;&#23383;&#36164;&#28304;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#39640;&#31561;&#25945;&#32946;&#20013;&#20173;&#28982;&#32570;&#20047;&#26631;&#20934;&#26041;&#27861;&#12290;VRAIlexia&#39033;&#30446;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#19981;&#21516;&#30340;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#19968;&#20010;&#38598;&#25104;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#20197;&#24555;&#36895;&#21644;&#36731;&#26494;&#37319;&#38598;&#25968;&#25454;&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#65288;AI&#65289;&#26469;&#20998;&#26512;&#37319;&#38598;&#30340;&#25968;&#25454;&#65292;&#20197;&#20026;&#27599;&#20010;&#23398;&#29983;&#23450;&#21046;&#25903;&#25345;&#26041;&#27861;&#12290;&#31532;&#19968;&#20010;&#24037;&#20855;&#24050;&#32463;&#34987;&#21019;&#24314;&#24182;&#27491;&#22312;&#20998;&#21457;&#32473;&#39640;&#31561;&#38498;&#26657;&#30340;&#35829;&#35835;&#22256;&#38590;&#23398;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning disorders are neurological conditions that affect the brain's ability to interconnect communication areas. Dyslexic students experience problems with reading, memorizing, and exposing concepts; however the magnitude of these can be mitigated through both therapies and the creation of compensatory mechanisms. Several efforts have been made to mitigate these issues, leading to the creation of digital resources for students with specific learning disorders attending primary and secondary education levels. Conversely, a standard approach is still missed in higher education. The VRAIlexia project has been created to tackle this issue by proposing two different tools: a mobile application integrating virtual reality (VR) to collect data quickly and easily, and an artificial intelligencebased software (AI) to analyze the collected data for customizing the supporting methodology for each student. The first one has been created and is being distributed among dyslexic students in Higher
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#30340;&#28508;&#22312;&#23454;&#26045;&#35774;&#35745;&#31354;&#38388;&#21644;&#20854;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#30740;&#31350;&#35758;&#31243;&#20197;&#20415;&#20351;&#20154;&#20204;&#33021;&#22815;&#23433;&#20840;&#32780;&#26377;&#30410;&#22320;&#21019;&#24314;&#21644;&#19982;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#36827;&#34892;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.01662</link><description>&lt;p&gt;
&#29983;&#25104;&#24189;&#28789;&#65306;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#30340;&#30410;&#22788;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#30340;&#28508;&#22312;&#23454;&#26045;&#35774;&#35745;&#31354;&#38388;&#21644;&#20854;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#30740;&#31350;&#35758;&#31243;&#20197;&#20415;&#20351;&#20154;&#20204;&#33021;&#22815;&#23433;&#20840;&#32780;&#26377;&#30410;&#22320;&#21019;&#24314;&#21644;&#19982;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#36827;&#34892;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#24615;&#33021;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#19978;&#36805;&#36895;&#25552;&#21319;&#65292;&#23427;&#20204;&#36234;&#26469;&#36234;&#36866;&#21512;&#21019;&#24314;&#21151;&#33021;&#24378;&#22823;&#12289;&#36924;&#30495;&#30340;&#20195;&#29702;&#20154;&#65292;&#21253;&#25324;&#22522;&#20110;&#29305;&#23450;&#20154;&#29289;&#24314;&#27169;&#30340;&#20195;&#29702;&#20154;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39044;&#35745;&#65292;&#22312;&#25105;&#20204;&#26377;&#29983;&#20043;&#24180;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26222;&#36941;&#20351;&#29992;&#23450;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20154;&#19982;&#29233;&#30340;&#20154;&#21644;/&#25110;&#26356;&#24191;&#22823;&#30340;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#29983;&#25104;&#24189;&#28789;&#65292;&#22240;&#20026;&#36825;&#20123;&#20195;&#29702;&#20154;&#23558;&#33021;&#22815;&#29983;&#25104;&#26032;&#39062;&#30340;&#20869;&#23481;&#65292;&#32780;&#19981;&#21482;&#26159;&#22797;&#36848;&#20854;&#21019;&#20316;&#32773;&#22312;&#29983;&#21069;&#30340;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#28508;&#22312;&#23454;&#26045;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#24433;&#21709;&#65292;&#21253;&#25324;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#28508;&#22312;&#31215;&#26497;&#21644;&#28040;&#26497;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20123;&#32771;&#34385;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#30740;&#31350;&#35758;&#31243;&#65292;&#26088;&#22312;&#20351;&#20154;&#20204;&#33021;&#22815;&#23433;&#20840;&#32780;&#26377;&#30410;&#22320;&#21019;&#24314;&#21644;&#19982;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#36827;&#34892;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems quickly improve in both breadth and depth of performance, they lend themselves to creating increasingly powerful and realistic agents, including the possibility of agents modeled on specific people. We anticipate that within our lifetimes it may become common practice for people to create a custom AI agent to interact with loved ones and/or the broader world after death. We call these generative ghosts, since such agents will be capable of generating novel content rather than merely parroting content produced by their creator while living. In this paper, we first discuss the design space of potential implementations of generative ghosts. We then discuss the practical and ethical implications of generative ghosts, including potential positive and negative impacts on individuals and society. Based on these considerations, we lay out a research agenda for the AI and HCI research communities to empower people to create and interact with AI afterlives in a safe and beneficial 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#32654;&#22269;116&#25152;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#30340;&#25919;&#31574;&#21644;&#25351;&#21335;&#65292;&#24635;&#32467;&#20986;&#22823;&#37096;&#20998;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#40723;&#21169;&#24182;&#25552;&#20379;&#35814;&#32454;&#25351;&#23548;&#26469;&#25351;&#24341;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#25945;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01659</link><description>&lt;p&gt;
&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#22522;&#20110;&#26426;&#26500;&#25919;&#31574;&#21644;&#25351;&#21335;&#30340;&#35777;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence in Higher Education: Evidence from an Analysis of Institutional Policies and Guidelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#32654;&#22269;116&#25152;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#30340;&#25919;&#31574;&#21644;&#25351;&#21335;&#65292;&#24635;&#32467;&#20986;&#22823;&#37096;&#20998;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#40723;&#21169;&#24182;&#25552;&#20379;&#35814;&#32454;&#25351;&#23548;&#26469;&#25351;&#24341;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#25945;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;11&#26376;&#21457;&#24067;&#30340;ChatGPT&#24341;&#21457;&#20102;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#26222;&#36941;&#37319;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#12290;&#36825;&#20123;&#26426;&#26500;&#24613;&#20110;&#24212;&#23545;&#20854;&#20351;&#29992;&#65292;&#23588;&#20854;&#26159;&#23398;&#29983;&#30340;&#20351;&#29992;&#65292;&#39318;&#20808;&#35797;&#22270;&#23545;&#20854;&#36827;&#34892;&#35268;&#33539;&#65292;&#24182;&#20105;&#35770;&#20854;&#22312;&#25945;&#23398;&#20013;&#30340;&#26377;&#30410;&#25972;&#21512;&#12290;&#22312;&#21457;&#24067;&#21518;&#30340;&#19968;&#24180;&#37324;&#65292;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#36234;&#26469;&#36234;&#22810;&#22320;&#25552;&#20379;&#20102;GenAI&#30340;&#25919;&#31574;&#21644;&#25351;&#21335;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;116&#25152;&#34987;&#24402;&#31867;&#20026;&#39640;&#30740;&#31350;&#27963;&#21160;&#25110;R1&#26426;&#26500;&#30340;&#32654;&#22269;&#22823;&#23398;&#25152;&#20135;&#29983;&#30340;&#25991;&#20214;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20840;&#38754;&#20102;&#35299;&#20102;&#25552;&#20379;&#32473;&#26426;&#26500;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;GenAI&#30456;&#20851;&#24314;&#35758;&#21644;&#25351;&#23548;&#12290;&#32463;&#36807;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32477;&#22823;&#22810;&#25968;&#22823;&#23398;&#65288;N=73&#65292;63%&#65289;&#40723;&#21169;&#20351;&#29992;GenAI&#65292;&#24182;&#19988;&#35768;&#22810;&#22823;&#23398;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#35838;&#22530;&#20351;&#29992;&#25351;&#23548;&#65288;N=48&#65292;41%&#65289;&#12290;&#36229;&#36807;&#19968;&#21322;&#30340;&#26426;&#26500;&#25552;&#20379;&#20102;&#31034;&#20363;&#25945;&#23398;&#22823;&#32434;&#65288;N=65&#65292;56%&#65289;&#65292;&#19968;&#21322;&#65288;N=58&#65292;50%&#65289;&#25552;&#20379;&#20102;&#26679;&#26412;GenAI&#35838;&#31243;&#21644;&#27963;&#21160;&#65292;&#36825;&#20123;&#23558;&#26377;&#21161;&#20110;&#26426;&#26500;&#30340;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT in November 2022 prompted a massive uptake of generative artificial intelligence (GenAI) across higher education institutions (HEIs). HEIs scrambled to respond to its use, especially by students, looking first to regulate it and then arguing for its productive integration within teaching and learning. In the year since the release, HEIs have increasingly provided policies and guidelines to direct GenAI. In this paper we examined documents produced by 116 US universities categorized as high research activity or R1 institutions to comprehensively understand GenAI related advice and guidance given to institutional stakeholders. Through an extensive analysis, we found the majority of universities (N=73, 63%) encourage the use of GenAI and many provide detailed guidance for its use in the classroom (N=48, 41%). More than half of all institutions provided sample syllabi (N=65, 56%) and half (N=58, 50%) provided sample GenAI curriculum and activities that would help ins
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25351;&#20986;&#24403;&#21069;&#32570;&#20047;&#25903;&#25345;&#35777;&#25454;&#65292;&#23558;AI&#37325;&#26032;&#23450;&#20041;&#27861;&#24459;&#32844;&#19994;&#30340;&#35266;&#28857;&#12290;&#20316;&#32773;&#35843;&#26597;&#20102;AI&#22312;&#20449;&#24687;&#22788;&#29702;&#12289;&#21019;&#36896;&#21147;/&#25512;&#29702;/&#21028;&#26029;&#20219;&#21153;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#27861;&#24459;&#24212;&#29992;&#30340;&#35780;&#20272;&#21462;&#20915;&#20110;&#30830;&#23450;&#27491;&#30830;&#31572;&#26696;&#30340;&#38590;&#26131;&#31243;&#24230;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#35266;&#23519;&#24615;&#12290;&#20316;&#32773;&#24314;&#35758;&#23545;AI&#22312;&#27861;&#24459;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#22909;&#30340;&#35780;&#20272;&#21644;&#37096;&#32626;&#12290;</title><link>https://arxiv.org/abs/2402.01656</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#27861;&#24459;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Promises and pitfalls of artificial intelligence for legal applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25351;&#20986;&#24403;&#21069;&#32570;&#20047;&#25903;&#25345;&#35777;&#25454;&#65292;&#23558;AI&#37325;&#26032;&#23450;&#20041;&#27861;&#24459;&#32844;&#19994;&#30340;&#35266;&#28857;&#12290;&#20316;&#32773;&#35843;&#26597;&#20102;AI&#22312;&#20449;&#24687;&#22788;&#29702;&#12289;&#21019;&#36896;&#21147;/&#25512;&#29702;/&#21028;&#26029;&#20219;&#21153;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#27861;&#24459;&#24212;&#29992;&#30340;&#35780;&#20272;&#21462;&#20915;&#20110;&#30830;&#23450;&#27491;&#30830;&#31572;&#26696;&#30340;&#38590;&#26131;&#31243;&#24230;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#35266;&#23519;&#24615;&#12290;&#20316;&#32773;&#24314;&#35758;&#23545;AI&#22312;&#27861;&#24459;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#22909;&#30340;&#35780;&#20272;&#21644;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#26159;&#21542;&#23558;&#37325;&#26032;&#23450;&#20041;&#27861;&#24459;&#32844;&#19994;&#65311;&#25105;&#20204;&#35748;&#20026;&#36825;&#19968;&#26029;&#35328;&#30446;&#21069;&#32570;&#20047;&#25903;&#25345;&#35777;&#25454;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;AI&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#27861;&#24459;&#20219;&#21153;&#20013;&#30340;&#26222;&#36941;&#35282;&#33394;&#65306;&#20449;&#24687;&#22788;&#29702;&#12289;&#28041;&#21450;&#21019;&#36896;&#21147;&#12289;&#25512;&#29702;&#25110;&#21028;&#26029;&#30340;&#20219;&#21153;&#20197;&#21450;&#23545;&#26410;&#26469;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26681;&#25454;&#30830;&#23450;&#27491;&#30830;&#31572;&#26696;&#30340;&#38590;&#26131;&#31243;&#24230;&#21644;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#21487;&#35266;&#23519;&#24615;&#65292;&#35780;&#20272;&#27861;&#24459;&#24212;&#29992;&#30340;&#20415;&#21033;&#31243;&#24230;&#22312;&#19981;&#21516;&#27861;&#24459;&#20219;&#21153;&#38388;&#24046;&#24322;&#24040;&#22823;&#12290;&#23545;&#27861;&#24459;&#32844;&#19994;&#20135;&#29983;&#26368;&#37325;&#35201;&#21464;&#38761;&#30340;&#20219;&#21153;&#20063;&#26159;&#23545;AI&#33021;&#21147;&#36807;&#24230;&#20048;&#35266;&#30340;&#26368;&#23481;&#26131;&#21463;&#24433;&#21709;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38590;&#20197;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#26356;&#22909;&#35780;&#20272;&#21644;&#22312;&#27861;&#24459;&#29615;&#22659;&#20013;&#37096;&#32626;AI&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is AI set to redefine the legal profession? We argue that this claim is not supported by the current evidence. We dive into AI's increasingly prevalent roles in three types of legal tasks: information processing; tasks involving creativity, reasoning, or judgment; and predictions about the future. We find that the ease of evaluating legal applications varies greatly across legal tasks, based on the ease of identifying correct answers and the observability of information relevant to the task at hand. Tasks that would lead to the most significant changes to the legal profession are also the ones most prone to overoptimism about AI capabilities, as they are harder to evaluate. We make recommendations for better evaluation and deployment of AI in legal contexts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#39044;&#27979;&#22312;&#32447;&#35838;&#31243;&#23398;&#29983;&#34920;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27492;&#26041;&#38754;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01655</link><description>&lt;p&gt;
&#22312;&#20840;&#29699;&#35270;&#35282;&#19979;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22312;&#32447;&#35838;&#31243;&#23398;&#29983;&#34920;&#29616;&#39044;&#27979;&#26041;&#27861;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach Towards Student Performance Prediction in Online Courses: Challenges Based on a Global Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#39044;&#27979;&#22312;&#32447;&#35838;&#31243;&#23398;&#29983;&#34920;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27492;&#26041;&#38754;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20256;&#32479;&#20998;&#26512;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#35780;&#20272;&#23398;&#29983;&#22312;&#20219;&#20309;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#36827;&#23637;&#26159;&#32791;&#26102;&#19988;&#21387;&#21147;&#24040;&#22823;&#30340;&#12290;&#30001;&#20110;&#25945;&#32946;&#30028;&#23545;&#20110;&#25972;&#21512;&#20114;&#32852;&#32593;&#25216;&#26415;&#20197;&#21450;&#21521;&#30005;&#23376;&#23398;&#20064;&#12289;&#28151;&#21512;&#23398;&#20064;&#25110;&#22312;&#32447;&#23398;&#20064;&#27169;&#24335;&#30340;&#36716;&#21464;&#30340;&#20851;&#27880;&#65292;&#23398;&#29983;&#20154;&#25968;&#30340;&#22686;&#21152;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#31181;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#23398;&#29983;&#34920;&#29616;&#39044;&#27979;&#25104;&#20026;&#20102;&#36817;&#24180;&#26469;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;CNN&#21644;RNN-LSTM&#65289;&#26469;&#39044;&#27979;&#22312;&#32447;&#35838;&#31243;&#20132;&#20184;&#30340;&#20013;&#26399;&#38454;&#27573;&#23398;&#29983;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#19990;&#30028;&#19978;&#19977;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#20248;&#21270;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing and evaluating students' progress in any learning environment is stressful and time consuming if done using traditional analysis methods. This is further exasperated by the increasing number of students due to the shift of focus toward integrating the Internet technologies in education and the focus of academic institutions on moving toward e-Learning, blended, or online learning models. As a result, the topic of student performance prediction has become a vibrant research area in recent years. To address this, machine learning and data mining techniques have emerged as a viable solution. To that end, this work proposes the use of deep learning techniques (CNN and RNN-LSTM) to predict the students' performance at the midpoint stage of the online course delivery using three distinct datasets collected from three different regions of the world. Experimental results show that deep learning models have promising performance as they outperform other optimized traditional ML models
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33539;&#22260;&#22238;&#39038;&#65292;&#26412;&#35770;&#25991;&#21457;&#29616;&#30005;&#33021;&#36127;&#33655;&#20998;&#35299;&#39046;&#22495;&#20027;&#35201;&#38598;&#20013;&#22312;&#23478;&#24237;&#30005;&#21147;&#28040;&#36153;&#26041;&#38754;&#65292;&#38024;&#23545;&#24037;&#19994;&#36127;&#33655;&#20998;&#35299;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#24120;&#29992;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.01654</link><description>&lt;p&gt;
&#30005;&#33021;&#36127;&#33655;&#20998;&#35299;&#30340;&#33539;&#22260;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Scoping Review of Energy Load Disaggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01654
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33539;&#22260;&#22238;&#39038;&#65292;&#26412;&#35770;&#25991;&#21457;&#29616;&#30005;&#33021;&#36127;&#33655;&#20998;&#35299;&#39046;&#22495;&#20027;&#35201;&#38598;&#20013;&#22312;&#23478;&#24237;&#30005;&#21147;&#28040;&#36153;&#26041;&#38754;&#65292;&#38024;&#23545;&#24037;&#19994;&#36127;&#33655;&#20998;&#35299;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#24120;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#38656;&#27714;&#20391;&#31649;&#29702;&#30340;&#25928;&#26524;&#21644;&#25552;&#21319;&#28040;&#36153;&#32773;&#24847;&#35782;&#26469;&#20419;&#36827;&#34892;&#20026;&#33410;&#30005;&#65292;&#30005;&#33021;&#36127;&#33655;&#20998;&#35299;&#21487;&#20197;&#20026;&#24179;&#34913;&#30005;&#21147;&#32593;&#32476;&#20570;&#20986;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#24403;&#21069;&#32570;&#20047;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;72&#31687;&#20840;&#25991;&#26399;&#21002;&#25991;&#31456;&#23545;&#36127;&#33655;&#20998;&#35299;&#39046;&#22495;&#12289;&#25968;&#25454;&#31867;&#22411;&#21644;&#26041;&#27861;&#36827;&#34892;&#20102;&#33539;&#22260;&#22238;&#39038;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23478;&#24237;&#30005;&#21147;&#28040;&#36153;&#26159;&#26368;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#32780;&#20854;&#20182;&#39046;&#22495;&#22914;&#24037;&#19994;&#36127;&#33655;&#20998;&#35299;&#21017;&#24456;&#23569;&#35752;&#35770;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20351;&#29992;&#30456;&#23545;&#36739;&#20302;&#39057;&#29575;&#30340;&#25968;&#25454;&#65292;&#37319;&#26679;&#38388;&#38548;&#20026;1&#33267;60&#31186;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#27425;&#26159;&#20248;&#21270;&#31574;&#30053;&#12289;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#22270;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy load disaggregation can contribute to balancing power grids by enhancing the effectiveness of demand-side management and promoting electricity-saving behavior through increased consumer awareness. However, the field currently lacks a comprehensive overview. To address this gap, this paper con-ducts a scoping review of load disaggregation domains, data types, and methods, by assessing 72 full-text journal articles. The findings reveal that domestic electricity consumption is the most researched area, while others, such as industrial load disaggregation, are rarely discussed. The majority of research uses relatively low-frequency data, sampled between 1 and 60 seconds. A wide variety of methods are used, and artificial neural networks are the most common, followed by optimization strategies, Hidden Markov Models, and Graph Signal Processing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#20998;&#26512;&#22312;&#31649;&#29702;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#33647;&#29289;&#27835;&#30103;&#26041;&#27861;&#20013;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#21576;&#29616;&#20102;&#29992;&#25143;&#20013;&#24515;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#30340;&#30740;&#31350;&#38382;&#39064;&#21644;&#19979;&#19968;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.01652</link><description>&lt;p&gt;
&#29992;&#25143;&#20013;&#24515;&#30340;AI&#20998;&#26512;&#22312;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
User-Centric AI Analytics for Chronic Health Conditions Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#20998;&#26512;&#22312;&#31649;&#29702;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#33647;&#29289;&#27835;&#30103;&#26041;&#27861;&#20013;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#21576;&#29616;&#20102;&#29992;&#25143;&#20013;&#24515;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#30340;&#30740;&#31350;&#38382;&#39064;&#21644;&#19979;&#19968;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#65292;AI&#20998;&#26512;&#30340;&#24212;&#29992;&#36817;&#24180;&#26469;&#36805;&#36895;&#22686;&#38271;&#12290;&#22312;&#26412;&#19987;&#39064;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;AI&#20998;&#26512;&#22312;&#31649;&#29702;&#31958;&#23615;&#30149;&#12289;&#32933;&#32982;&#31561;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#22312;&#26080;&#33647;&#29289;&#27835;&#30103;&#26041;&#27861;&#20013;&#31649;&#29702;&#36825;&#20123;&#29366;&#20917;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#20010;&#20307;&#29366;&#20917;&#30340;&#24046;&#24322;&#12290;&#36825;&#20123;&#24046;&#24322;&#23548;&#33268;&#30740;&#31350;&#36827;&#20837;&#20102;&#29992;&#25143;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#30701;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36817;&#26399;&#21644;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#24037;&#20316;&#30340;&#31034;&#20363;&#65292;&#24182;&#32467;&#35770;&#25105;&#20204;&#35748;&#20026;&#30340;&#19979;&#19968;&#27493;&#21644;&#19968;&#20123;&#23578;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of AI analytics in health informatics has seen a rapid growth in recent years. In this talk, we look at AI analytics use in managing chronic health conditions such as diabetes, obesity, etc. We focus on the challenges in managing these conditions especially with drug-free approaches due to the variations in individual circumstances. These variations directed the research into user-centric approach leading to variety of research questions. In this short paper, we give examples from recent and current research work and conclude with what, in our opinion, to be the next steps and some remaining open research questions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20197;&#20262;&#29702;&#20026;&#22522;&#30784;&#30340;&#23457;&#35745;&#65292;&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;8&#20010;&#20027;&#35201;&#30340;&#21830;&#19994;&#21644;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;GPT-4&#22312;&#36947;&#24503;&#25512;&#29702;&#21644;&#20262;&#29702;&#26694;&#26550;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.01651</link><description>&lt;p&gt;
&#12298;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#65306;&#36890;&#36807;&#20197;&#20262;&#29702;&#20026;&#22522;&#30784;&#30340;&#23457;&#35745;&#27604;&#36739;&#20027;&#23548;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#36947;&#24503;&#25512;&#29702;&#21644;&#35268;&#33539;&#20215;&#20540;&#35266;&#12299;
&lt;/p&gt;
&lt;p&gt;
Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01651
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20197;&#20262;&#29702;&#20026;&#22522;&#30784;&#30340;&#23457;&#35745;&#65292;&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;8&#20010;&#20027;&#35201;&#30340;&#21830;&#19994;&#21644;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;GPT-4&#22312;&#36947;&#24503;&#25512;&#29702;&#21644;&#20262;&#29702;&#26694;&#26550;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#20027;&#26234;&#33021;&#20195;&#29702;&#30340;&#20010;&#20307;&#21644;&#21327;&#20316;&#32593;&#32476;&#30340;&#20852;&#36215;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#26356;&#22810;&#20851;&#38190;&#25512;&#29702;&#21644;&#20915;&#31574;&#35282;&#33394;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20262;&#29702;&#30340;&#23457;&#35745;&#22312;&#24555;&#36895;&#22686;&#38271;&#30340;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21644;&#30417;&#31649;&#39046;&#22495;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#20262;&#29702;&#23457;&#35745;&#26469;&#35843;&#26597;8&#20010;&#20027;&#35201;&#30340;&#21830;&#19994;&#21644;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-4&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#36947;&#24503;&#25512;&#29702;&#30340;&#33021;&#21147;&#20197;&#21450;&#27604;&#36739;&#27169;&#22411;&#20316;&#20026;&#20262;&#29702;&#26694;&#26550;&#30340;&#35268;&#33539;&#20215;&#20540;&#35266;&#26469;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#23454;&#39564;&#24615;&#30340;&#12289;&#22522;&#20110;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20262;&#29702;&#22256;&#22659;&#25552;&#20379;&#32473;&#27169;&#22411;&#26469;&#25361;&#25112;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#20262;&#29702;&#22330;&#26223;&#26088;&#22312;&#38656;&#35201;&#20570;&#20986;&#20915;&#31574;&#65292;&#20854;&#20013;&#24773;&#22659;&#30340;&#29305;&#23450;&#24773;&#20917;&#21487;&#33021;&#38656;&#35201;&#20559;&#31163;&#35268;&#33539;&#20262;&#29702;&#21407;&#21017;&#12290;&#19968;&#20010;&#22797;&#26434;&#30340;&#20262;&#29702;&#26694;&#26550;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#19968;&#33268;&#24341;&#21457;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#36824;&#23384;&#22312;&#20196;&#20154;&#25285;&#24551;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of individual and collaborative networks of autonomous agents, AI is deployed in more key reasoning and decision-making roles. For this reason, ethics-based audits play a pivotal role in the rapidly growing fields of AI safety and regulation. This paper undertakes an ethics-based audit to probe the 8 leading commercial and open-source Large Language Models including GPT-4. We assess explicability and trustworthiness by a) establishing how well different models engage in moral reasoning and b) comparing normative values underlying models as ethical frameworks. We employ an experimental, evidence-based approach that challenges the models with ethical dilemmas in order to probe human-AI alignment. The ethical scenarios are designed to require a decision in which the particulars of the situation may or may not necessitate deviating from normative ethical principles. A sophisticated ethical framework was consistently elicited in one model, GPT-4. Nonetheless, troubling finding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#23398;&#21644;&#39640;&#20013;&#23398;&#29983;&#30340;&#24320;&#28304;&#23398;&#20064;&#27169;&#22359;&#65292;&#20801;&#35768;&#23398;&#29983;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#33258;&#24049;&#30340;&#26426;&#22120;&#20154;&#20249;&#20276;&#65292;&#24182;&#25552;&#20379;&#23454;&#36341;&#32463;&#39564;&#21644;&#20837;&#38376;&#30693;&#35782;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#26426;&#26800;&#24037;&#31243;&#31561;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22359;&#30528;&#37325;&#24378;&#35843;&#20197;&#20154;&#20026;&#26412;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.01647</link><description>&lt;p&gt;
&#25171;&#36896;&#24744;&#33258;&#24049;&#30340;&#26426;&#22120;&#20154;&#26379;&#21451;&#65306;&#38754;&#21521;&#26222;&#21450;&#19988;&#24341;&#20154;&#20837;&#32988;&#30340;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#30340;&#24320;&#28304;&#23398;&#20064;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Build Your Own Robot Friend: An Open-Source Learning Module for Accessible and Engaging AI Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#23398;&#21644;&#39640;&#20013;&#23398;&#29983;&#30340;&#24320;&#28304;&#23398;&#20064;&#27169;&#22359;&#65292;&#20801;&#35768;&#23398;&#29983;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#33258;&#24049;&#30340;&#26426;&#22120;&#20154;&#20249;&#20276;&#65292;&#24182;&#25552;&#20379;&#23454;&#36341;&#32463;&#39564;&#21644;&#20837;&#38376;&#30693;&#35782;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#26426;&#26800;&#24037;&#31243;&#31561;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22359;&#30528;&#37325;&#24378;&#35843;&#20197;&#20154;&#20026;&#26412;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#31038;&#20250;&#21644;&#20840;&#29699;&#32463;&#27982;&#20013;&#30340;&#26085;&#30410;&#37325;&#35201;&#20316;&#29992;&#65292;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#21644;&#32032;&#20859;&#24050;&#25104;&#20026;&#22823;&#23398;&#21644;K-12&#25945;&#32946;&#20013;&#24517;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#22521;&#20859;&#23398;&#29983;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31038;&#20250;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#23578;&#26410;&#36275;&#22815;&#26222;&#21450;&#21644;&#24341;&#20154;&#20837;&#32988;&#65292;&#26080;&#27861;&#28385;&#36275;&#26469;&#33258;&#19981;&#21516;&#25945;&#32946;&#30446;&#26631;&#21644;&#19981;&#21516;&#31038;&#20250;&#32463;&#27982;&#32972;&#26223;&#30340;&#23398;&#29983;&#21644;&#23398;&#26657;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#23398;&#21644;&#39640;&#20013;&#23398;&#29983;&#30340;&#24320;&#28304;&#23398;&#20064;&#27169;&#22359;&#65292;&#20801;&#35768;&#23398;&#29983;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#33258;&#24049;&#30340;&#26426;&#22120;&#20154;&#20249;&#20276;&#12290;&#36825;&#20010;&#24320;&#25918;&#24179;&#21488;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21508;&#20010;&#26041;&#38754;&#30340;&#23454;&#36341;&#32463;&#39564;&#21644;&#20837;&#38376;&#30693;&#35782;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#26426;&#26800;&#24037;&#31243;&#31561;&#12290;&#30001;&#20110;&#31038;&#20132;&#36741;&#21161;&#26426;&#22120;&#20154;&#20249;&#20276;&#30340;&#31038;&#20132;&#21644;&#20010;&#20154;&#24615;&#36136;&#65292;&#35813;&#27169;&#22359;&#36824;&#29305;&#21035;&#24378;&#35843;&#20197;&#20154;&#20026;&#26412;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) is playing an increasingly important role in our society and global economy, AI education and literacy have become necessary components in college and K-12 education to prepare students for an AI-powered society. However, current AI curricula have not yet been made accessible and engaging enough for students and schools from all socio-economic backgrounds with different educational goals. In this work, we developed an open-source learning module for college and high school students, which allows students to build their own robot companion from the ground up. This open platform can be used to provide hands-on experience and introductory knowledge about various aspects of AI, including robotics, machine learning (ML), software engineering, and mechanical engineering. Because of the social and personal nature of a socially assistive robot companion, this module also puts a special emphasis on human-centered AI, enabling students to develop a better understa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.01643</link><description>&lt;p&gt;
L-TUNING&#65306;&#29992;&#20110;LLMs&#20013;&#30340;&#25552;&#31034;&#21644;&#21069;&#32512;&#30340;&#21516;&#27493;&#26631;&#31614;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20219;&#24847;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#24182;&#19988;&#36890;&#29992;&#26631;&#35760;&#22312;&#21508;&#31181;&#31867;&#21035;&#26631;&#31614;&#20013;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;L-Tuning&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#35774;&#35745;&#30340;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;L-Tuning&#19987;&#27880;&#20110;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;LLM&#22788;&#29702;&#30340;&#26631;&#31614;&#26631;&#35760;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#21033;&#29992;&#20854;&#39044;&#20808;&#23384;&#22312;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#25552;&#39640;&#20102;&#24494;&#35843;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36824;&#20419;&#36827;&#20102;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#19981;&#21516;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;L-Tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#26410;&#35265;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#24456;&#22810;&#27169;&#22411;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#21387;&#32553;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.00861</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models for Generalization and Robustness via Data Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00861
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#26410;&#35265;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#24456;&#22810;&#27169;&#22411;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#21387;&#32553;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#38754;&#20020;&#25968;&#25454;&#27745;&#26579;&#12289;&#23545;&#25552;&#31034;&#25935;&#24863;&#20197;&#21450;&#22522;&#20934;&#27979;&#35797;&#21019;&#24314;&#25104;&#26412;&#39640;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#27979;&#35797;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#20854;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#27867;&#21270;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20174;2017&#24180;&#21040;2023&#24180;&#20849;83&#20010;&#26376;&#30340;&#20840;&#38754;&#27979;&#35797;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#25130;&#27490;&#26085;&#26399;&#23558;&#25968;&#25454;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#26399;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65306;1&#65289;&#27979;&#35797;&#26399;&#30340;&#21387;&#32553;&#24615;&#33021;&#20316;&#20026;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#34913;&#37327;&#65307;2&#65289;&#35757;&#32451;&#26399;&#21644;&#27979;&#35797;&#26399;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#20316;&#20026;&#40065;&#26834;&#24615;&#30340;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35768;&#22810;&#27169;&#22411;&#30340;&#21387;&#32553;&#29575;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#26174;&#33879;&#38477;&#20302;&#65292;&#20294;&#20687;... (&#20869;&#23481;&#36807;&#38271;&#65292;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20197;&#24847;&#22270;&#28418;&#31227;&#20026;&#24341;&#23548;&#30340;LLMs&#36827;&#34892;&#24847;&#22270;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31574;&#30053;&#35782;&#21035;&#21644;&#22788;&#29702;&#24847;&#22270;&#28418;&#31227;&#65292;&#20197;&#23454;&#29616;&#24847;&#22270;&#19982;&#19994;&#21153;&#30446;&#26631;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.00715</link><description>&lt;p&gt;
&#20351;&#29992;&#20197;&#24847;&#22270;&#28418;&#31227;&#20026;&#24341;&#23548;&#30340;LLMs&#36827;&#34892;&#24847;&#22270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Intent Assurance using LLMs guided by Intent Drift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20197;&#24847;&#22270;&#28418;&#31227;&#20026;&#24341;&#23548;&#30340;LLMs&#36827;&#34892;&#24847;&#22270;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31574;&#30053;&#35782;&#21035;&#21644;&#22788;&#29702;&#24847;&#22270;&#28418;&#31227;&#65292;&#20197;&#23454;&#29616;&#24847;&#22270;&#19982;&#19994;&#21153;&#30446;&#26631;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#39537;&#21160;&#30340;&#32593;&#32476;&#65288;IBN&#65289;&#20026;&#32593;&#32476;&#31649;&#29702;&#25552;&#20379;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#25215;&#35834;&#20197;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#23558;&#24847;&#22270;&#21644;&#19994;&#21153;&#30446;&#26631;&#19982;&#32593;&#32476;&#25805;&#20316;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#20854;&#23454;&#38469;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;1&#65289;&#22788;&#29702;&#24847;&#22270;&#65292;&#21363;&#32763;&#35793;&#12289;&#20998;&#35299;&#21644;&#35782;&#21035;&#23454;&#29616;&#24847;&#22270;&#30340;&#36923;&#36753;&#65307;2&#65289;&#24847;&#22270;&#19968;&#33268;&#24615;&#65292;&#21363;&#32771;&#34385;&#21040;&#21160;&#24577;&#32593;&#32476;&#65292;&#36923;&#36753;&#24212;&#36866;&#24403;&#35843;&#25972;&#20197;&#20445;&#35777;&#24847;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#21518;&#32773;&#65292;&#24847;&#22270;&#20445;&#35777;&#36127;&#36131;&#25345;&#32493;&#39564;&#35777;&#21644;&#39564;&#35777;&#65292;&#21253;&#25324;&#37319;&#21462;&#24517;&#35201;&#34892;&#21160;&#20351;&#25805;&#20316;&#29366;&#24577;&#21644;&#30446;&#26631;&#29366;&#24577;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20445;&#35777;&#26694;&#26550;&#65292;&#20801;&#35768;&#25105;&#20204;&#22312;&#21457;&#29983;&#24847;&#22270;&#28418;&#31227;&#26102;&#36827;&#34892;&#26816;&#27979;&#21644;&#37319;&#21462;&#34892;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#22522;&#20110;AI&#39537;&#21160;&#30340;&#31574;&#30053;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#24517;&#35201;&#30340;&#19978;&#19979;&#25991;&#35201;&#27714;&#65292;&#24182;&#21327;&#21161;&#23454;&#29616;&#21644;&#20445;&#35777;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner. However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents. To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states. In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs. To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.00672</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#26631;&#31614;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#65288;USL-VI-ReID&#65289;&#26088;&#22312;&#26080;&#38656;&#27880;&#37322;&#20174;&#19981;&#21516;&#27169;&#24577;&#20013;&#26816;&#32034;&#30456;&#21516;&#36523;&#20221;&#30340;&#34892;&#20154;&#22270;&#20687;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#24314;&#31435;&#36328;&#27169;&#24577;&#30340;&#20266;&#26631;&#31614;&#20851;&#32852;&#20197;&#24357;&#21512;&#27169;&#24577;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#24573;&#30053;&#20102;&#22312;&#20266;&#26631;&#31614;&#31354;&#38388;&#20013;&#20445;&#25345;&#23454;&#20363;&#32423;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#20851;&#32852;&#31895;&#31961;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#65288;MULT&#65289;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#32454;&#31890;&#24230;&#23454;&#20363;&#32423;&#32467;&#26500;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#12290;&#23427;&#24314;&#27169;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#30340;&#20851;&#32852;&#24615;&#65292;&#21033;&#29992;&#23427;&#20204;&#23450;&#20041;&#20266;&#26631;&#31614;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#26368;&#23567;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32500;&#25345;&#20102;&#36328;&#27169;&#24577;&#30340;&#23545;&#40784;&#24182;&#20445;&#25345;&#20102;&#20869;&#37096;&#27169;&#24577;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#22312;&#32447;&#20132;&#21449;&#35760;&#24518;&#26631;&#31614;&#24341;&#29992;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Ref
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20998;&#24067;&#24335;&#20915;&#31574;&#26041;&#27861;&#65292;&#29992;&#20110;&#24037;&#19994;&#35774;&#22791;&#20219;&#21153;&#20998;&#37197;&#21644;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#20581;&#24247;&#31649;&#29702;&#20915;&#31574;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#65292;&#24182;&#23558;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#25968;&#23398;&#27169;&#22411;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00042</link><description>&lt;p&gt;
&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20248;&#21270;&#24037;&#19994;&#35774;&#22791;&#30340;&#20219;&#21153;&#20998;&#37197;&#19982;&#39044;&#27979;&#24615;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
Optimized Task Assignment and Predictive Maintenance for Industrial Machines using Markov Decision Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20998;&#24067;&#24335;&#20915;&#31574;&#26041;&#27861;&#65292;&#29992;&#20110;&#24037;&#19994;&#35774;&#22791;&#20219;&#21153;&#20998;&#37197;&#21644;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#20581;&#24247;&#31649;&#29702;&#20915;&#31574;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#65292;&#24182;&#23558;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#25968;&#23398;&#27169;&#22411;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20915;&#31574;&#26041;&#27861;&#65292;&#29992;&#20110;&#21046;&#36896;&#20219;&#21153;&#20998;&#37197;&#21644;&#22522;&#20110;&#35774;&#22791;&#29366;&#20917;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#20581;&#24247;&#31649;&#29702;&#20915;&#31574;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#35774;&#35745;&#20915;&#31574;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#32771;&#34385;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#38469;&#25191;&#34892;&#31574;&#30053;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#38115;&#24202;&#24037;&#20855;&#36864;&#21270;&#25968;&#25454;&#30340;&#35814;&#32454;&#25968;&#20540;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25104;&#26412;&#21442;&#25968;&#36873;&#25321;&#26041;&#38754;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#20801;&#35768;&#23545;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#31163;&#32447;&#35745;&#31639;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a distributed decision-making approach for manufacturing task assignment and condition-based machine health maintenance. Our approach considers information sharing between the task assignment and health management decision-making agents. We propose the design of the decision-making agents based on Markov decision processes. The key advantage of using a Markov decision process-based approach is the incorporation of uncertainty involved in the decision-making process. The paper provides detailed mathematical models along with the associated practical execution strategy. In order to demonstrate the effectiveness and practical applicability of our proposed approach, we have included a detailed numerical case study that is based on open source milling machine tool degradation data. Our case study indicates that the proposed approach offers flexibility in terms of the selection of cost parameters and it allows for offline computation and analysis of the decision-making p
&lt;/p&gt;</description></item><item><title>LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.00024</link><description>&lt;p&gt;
LLaMA&#21644;ChatGPT&#23884;&#20837;&#22312;&#20998;&#23376;&#23884;&#20837;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00024
&lt;/p&gt;
&lt;p&gt;
LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#37322;Simplified Molecular Input Line Entry System (SMILES)&#26041;&#38754;&#12290;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;SMILES&#23383;&#31526;&#20018;&#35299;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20026;&#29702;&#35299;&#21270;&#23398;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#22312;&#23884;&#20837;SMILES&#23383;&#31526;&#20018;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#65306;&#20998;&#23376;&#24615;&#36136;&#65288;MP&#65289;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#65292;&#36825;&#22312;&#33647;&#29289;&#24320;&#21457;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;LLaMA&#29983;&#25104;&#30340;SMILES&#23884;&#20837;&#22312;MP&#21644;DDI&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;LLaMA&#30340;SMILES&#23884;&#20837;&#22312;&#36825;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#32467;&#35770;&#65306;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#24212;&#29992;LLMs&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;SMILES&#36827;&#34892;&#23884;&#20837;&#26041;&#38754;&#65292;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38598;&#25104;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#26469;&#25913;&#36827;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17870</link><description>&lt;p&gt;
&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#36827;&#34892;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38598;&#25104;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#26469;&#25913;&#36827;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27425;&#23395;&#33410;&#39044;&#25253;&#23545;&#20892;&#19994;&#12289;&#27700;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#39044;&#35686;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#22823;&#27668;&#30340;&#28151;&#27788;&#24615;&#65292;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#36890;&#36807;&#23454;&#29616;&#19982;&#25968;&#20540;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#38761;&#26032;&#20102;&#22825;&#27668;&#39044;&#25253;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#36825;&#23548;&#33268;&#30456;&#24403;&#22810;&#30340;&#30899;&#25490;&#25918;&#65292;&#24182;&#38480;&#21046;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#36890;&#36807;&#20135;&#29983;&#24179;&#28369;&#30340;&#32467;&#26524;&#26469;&#24858;&#24324;&#20687;&#32032;&#35823;&#24046;&#35780;&#20998;&#65292;&#36825;&#20123;&#32467;&#26524;&#32570;&#20047;&#29289;&#29702;&#19968;&#33268;&#24615;&#21644;&#27668;&#35937;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Pangu&#27169;&#22411;&#26469;&#33719;&#24471;&#33391;&#22909;&#30340;&#21021;&#22987;&#26435;&#37325;&#65292;&#24182;&#38598;&#25104;&#20102;&#19968;&#20010;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#22312;&#24310;&#38271;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#35843;&#25972;Pangu&#27169;&#22411;&#30340;1.1%&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16332</link><description>&lt;p&gt;
&#23545;&#40784;&#21644;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tradeoffs Between Alignment and Helpfulness in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#22686;&#24378;&#26399;&#26395;&#34892;&#20026;&#21644;&#25233;&#21046;&#38750;&#26399;&#26395;&#34892;&#20026;&#65292;&#23454;&#29616;&#20154;&#31867;&#19982;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#23433;&#20840;&#20132;&#20114;&#12290;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#25110;&#25554;&#20837;&#39044;&#35774;&#30340;&#23545;&#40784;&#25552;&#31034;&#26469;&#23454;&#29616;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#21518;&#30340;&#34920;&#31034;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#34920;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#34920;&#31034;&#24037;&#31243;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#38477;&#20302;&#31038;&#20250;&#20559;&#35265;&#31561;&#23545;&#40784;&#23548;&#21521;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#22686;&#30410;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#27169;&#22411;&#25191;&#34892;&#22522;&#26412;&#20219;&#21153;&#33021;&#21147;&#30340;&#38477;&#20302;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#36890;&#24120;&#20250;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;M2-&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39640;&#25928;&#39044;&#35757;&#32451;&#65292;&#25512;&#21160;&#20102;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#33521;&#21452;&#35821;&#25968;&#25454;&#38598;BM-6B&#65292;&#20351;&#29992;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#20943;&#23569;&#20102;&#25439;&#22833;&#35745;&#31639;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;GPU&#20869;&#23384;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#22312;BM-6B&#19978;&#39044;&#35757;&#32451;&#30340;M&#178;-&#32534;&#30721;&#22120;&#22312;&#22810;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2401.15896</link><description>&lt;p&gt;
M2-&#32534;&#30721;&#22120;&#65306;&#36890;&#36807;&#22823;&#35268;&#27169;&#39640;&#25928;&#39044;&#35757;&#32451;&#25512;&#36827;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;M2-&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39640;&#25928;&#39044;&#35757;&#32451;&#65292;&#25512;&#21160;&#20102;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#33521;&#21452;&#35821;&#25968;&#25454;&#38598;BM-6B&#65292;&#20351;&#29992;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#20943;&#23569;&#20102;&#25439;&#22833;&#35745;&#31639;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;GPU&#20869;&#23384;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#22312;BM-6B&#19978;&#39044;&#35757;&#32451;&#30340;M&#178;-&#32534;&#30721;&#22120;&#22312;&#22810;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;CLIP&#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#22810;&#35821;&#35328;&#30340;VLM&#27169;&#22411;&#65292;&#20363;&#22914;&#20013;&#33521;&#25991;&#21452;&#35821;&#65292;&#30001;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#30456;&#23545;&#31232;&#32570;&#32780;&#28382;&#21518;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#33521;&#21452;&#35821;&#65288;&#20013;&#33521;&#25991;&#65289;&#25968;&#25454;&#38598;BM-6B&#65292;&#21253;&#21547;&#36229;&#36807;60&#20159;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#23545;&#20004;&#31181;&#35821;&#35328;&#20013;&#30340;&#22270;&#20687;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#26679;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#21512;&#32858;&#21512;&#26041;&#27861;&#26469;&#35745;&#31639;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#25439;&#22833;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#21644;GPU&#20869;&#23384;&#38656;&#27714;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#25105;&#20204;&#22312;BM-6B&#19978;&#39044;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#22686;&#24378;&#32454;&#31890;&#24230;&#29702;&#35299;&#33021;&#21147;&#30340;&#20013;&#33521;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#31216;&#20026;M&#178;-&#32534;&#30721;&#22120;&#65288;&#21457;&#38899;&#20026;&#8220;M-Square&#8221;&#65289;&#65292;&#22312;&#20004;&#31181;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#20013;&#21047;&#26032;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language foundation models like CLIP have revolutionized the field of artificial intelligence. Nevertheless, VLM models supporting multi-language, e.g., in both Chinese and English, have lagged due to the relative scarcity of large-scale pretraining datasets. Toward this end, we introduce a comprehensive bilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs, aimed at enhancing multimodal foundation models to well understand images in both languages. To handle such a scale of dataset, we propose a novel grouped aggregation approach for image-text contrastive loss computation, which reduces the communication overhead and GPU memory demands significantly, facilitating a 60% increase in training speed. We pretrain a series of bilingual image-text foundation models with an enhanced fine-grained understanding ability on BM-6B, the resulting models, dubbed as $M^2$-Encoders (pronounced "M-Square"), set new benchmarks in both languages for multimodal retrieval a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;AI&#20316;&#20026;&#25506;&#32034;&#12290;&#23427;&#24378;&#35843;&#36890;&#36807;&#21019;&#24314;&#21644;&#30740;&#31350;&#26234;&#33021;&#31995;&#32479;&#26469;&#25581;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#26234;&#33021;&#24418;&#24335;&#19981;&#21516;&#30340;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#12290;&#35770;&#25991;&#36890;&#36807;&#35752;&#35770;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35828;&#26126;&#20102;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2401.07964</link><description>&lt;p&gt;
AI&#20316;&#20026;&#25506;&#32034;&#65306;&#23548;&#33322;&#26234;&#33021;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
AI-as-exploration: Navigating intelligence space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07964
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;AI&#20316;&#20026;&#25506;&#32034;&#12290;&#23427;&#24378;&#35843;&#36890;&#36807;&#21019;&#24314;&#21644;&#30740;&#31350;&#26234;&#33021;&#31995;&#32479;&#26469;&#25581;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#26234;&#33021;&#24418;&#24335;&#19981;&#21516;&#30340;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#12290;&#35770;&#25991;&#36890;&#36807;&#35752;&#35770;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35828;&#26126;&#20102;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#25317;&#26377;&#35768;&#22810;&#29983;&#21629;&#30340;&#39046;&#22495;&#65292;&#36825;&#20010;&#26415;&#35821;&#24050;&#32463;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#31185;&#23398;&#21644;&#21830;&#19994;&#21162;&#21147;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#38416;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#21313;&#20998;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;&#8220;AI&#20316;&#20026;&#25506;&#32034;&#8221;&#12290;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#21019;&#24314;&#21644;&#30740;&#31350;&#33021;&#22815;&#25581;&#31034;&#26234;&#33021;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#30340;&#31995;&#32479;&#65292;&#36825;&#20123;&#27169;&#22359;&#21487;&#33021;&#19981;&#21516;&#20110;&#25105;&#20204;&#29087;&#24713;&#30340;&#20154;&#31867;&#21644;&#21160;&#29289;&#26234;&#33021;&#24418;&#24335;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#35748;&#20026;&#20154;&#24037;&#26234;&#33021;&#26159;&#25506;&#32034;&#26234;&#33021;&#31354;&#38388;&#65292;&#21363;&#21487;&#33021;&#30340;&#26234;&#33021;&#31995;&#32479;&#31354;&#38388;&#65292;&#30340;&#26368;&#20339;&#24037;&#20855;&#20043;&#19968;&#12290;&#25105;&#36890;&#36807;&#20851;&#27880;&#19968;&#20010;&#20855;&#20307;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21363;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#26469;&#35828;&#26126;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#25105;&#23637;&#31034;&#20102;&#23613;&#31649;&#21518;&#32773;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#24456;&#21487;&#33021;&#20197;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence is a field that lives many lives, and the term has come to encompass a motley collection of scientific and commercial endeavours. In this paper, I articulate the contours of a rather neglected but central scientific role that AI has to play, which I dub `AI-as-exploration'.The basic thrust of AI-as-exploration is that of creating and studying systems that can reveal candidate building blocks of intelligence that may differ from the forms of human and animal intelligence we are familiar with. In other words, I suggest that AI is one of the best tools we have for exploring intelligence space, namely the space of possible intelligent systems. I illustrate the value of AI-as-exploration by focusing on a specific case study, i.e., recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. I show that the latter, despite showing human-level accuracy in such a task, most probably solve it in ways radically different, but no 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#31649;&#29702;&#20302;&#31354;&#39046;&#22495;&#25480;&#26435;&#32780;&#26500;&#24314;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#24037;&#31243;&#26041;&#27861;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39134;&#34892;&#29305;&#24449;&#21644;&#29615;&#22659;&#26465;&#20214;&#34987;&#35748;&#20026;&#26159;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#20294;&#36824;&#24212;&#32771;&#34385;&#39134;&#34892;&#21592;&#21644;&#26080;&#20154;&#26426;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#21463;&#35775;&#32773;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25209;&#20934;&#25110;&#25298;&#32477;&#39134;&#34892;&#35831;&#27714;&#34920;&#31034;&#21453;&#23545;&#12290;</title><link>https://arxiv.org/abs/2401.07353</link><description>&lt;p&gt;
&#20026;&#31649;&#29702;&#20302;&#31354;&#39046;&#22495;&#25480;&#26435;&#32780;&#26500;&#24314;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24037;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Engineering Fair and Equitable Software Systems for Managing Low-Altitude Airspace Authorizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#31649;&#29702;&#20302;&#31354;&#39046;&#22495;&#25480;&#26435;&#32780;&#26500;&#24314;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#24037;&#31243;&#26041;&#27861;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39134;&#34892;&#29305;&#24449;&#21644;&#29615;&#22659;&#26465;&#20214;&#34987;&#35748;&#20026;&#26159;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#20294;&#36824;&#24212;&#32771;&#34385;&#39134;&#34892;&#21592;&#21644;&#26080;&#20154;&#26426;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#21463;&#35775;&#32773;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25209;&#20934;&#25110;&#25298;&#32477;&#39134;&#34892;&#35831;&#27714;&#34920;&#31034;&#21453;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#26080;&#20154;&#26426;&#31995;&#32479;&#65288;sUAS&#65289;&#24050;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#24341;&#20837;&#20102;&#20849;&#20139;&#39046;&#22495;&#20869;&#30340;&#25805;&#20316;&#22797;&#26434;&#24615;&#21644;&#25253;&#21578;&#30340;&#20107;&#20214;&#22686;&#21152;&#65292;&#24341;&#21457;&#20102;&#23433;&#20840;&#25285;&#24551;&#12290;&#20026;&#27492;&#65292;&#32654;&#22269;&#32852;&#37030;&#33322;&#31354;&#31649;&#29702;&#23616;&#65288;FAA&#65289;&#27491;&#22312;&#24320;&#21457;&#26080;&#20154;&#26426;&#20132;&#36890;&#31649;&#29702;&#65288;UTM&#65289;&#31995;&#32479;&#65292;&#20197;&#22522;&#20110;sUAS&#39044;&#27979;&#33021;&#22815;&#23433;&#20840;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#26469;&#25511;&#21046;&#23545;&#31354;&#22495;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#24555;&#36895;&#25209;&#20934;&#25110;&#25298;&#32477;&#39134;&#34892;&#35831;&#27714;&#65292;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#24517;&#39035;&#32771;&#34385;&#22810;&#26679;&#21270;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#23433;&#20840;&#24615;&#12289;&#36879;&#26126;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#24212;&#32771;&#34385;&#22312;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#30340;&#22240;&#32032;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35266;&#28857;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39134;&#34892;&#29305;&#24449;&#21644;&#29615;&#22659;&#26465;&#20214;&#34987;&#35748;&#20026;&#26159;&#26368;&#37325;&#35201;&#30340;&#65292;&#20294;&#39134;&#34892;&#21592;&#21644;&#26080;&#20154;&#26426;&#30340;&#33021;&#21147;&#20063;&#24212;&#35813;&#34987;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#20960;&#20010;&#21463;&#35775;&#32773;&#34920;&#31034;&#20182;&#20204;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25209;&#20934;&#25110;&#25298;&#32477;&#39134;&#34892;&#35831;&#27714;&#30340;&#19981;&#28385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small Unmanned Aircraft Systems (sUAS) have gained widespread adoption across a diverse range of applications. This has introduced operational complexities within shared airspaces and an increase in reported incidents, raising safety concerns. In response, the U.S. Federal Aviation Administration (FAA) is developing a UAS Traffic Management (UTM) system to control access to airspace based on an sUAS's predicted ability to safely complete its mission. However, a fully automated system capable of swiftly approving or denying flight requests can be prone to bias and must consider safety, transparency, and fairness to diverse stakeholders. In this paper, we present an initial study that explores stakeholders' perspectives on factors that should be considered in an automated system. Results indicate flight characteristics and environmental conditions were perceived as most important but pilot and drone capabilities should also be considered. Further, several respondents indicated an aversio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;OpenContra&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#26500;&#24314;&#24320;&#25918;&#24335;&#20855;&#36523;&#20195;&#29702;&#65292;&#33021;&#22815;&#29702;&#35299;&#20219;&#24847;&#20154;&#31867;&#25351;&#20196;&#65292;&#24182;&#20197;&#36739;&#39640;&#30340;&#23436;&#25104;&#29575;&#23436;&#25104;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2401.00006</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#31574;&#30053;&#21452;&#21521;&#36866;&#24212;&#26500;&#24314;&#24320;&#25918;&#24335;&#20855;&#36523;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00006
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;OpenContra&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#26500;&#24314;&#24320;&#25918;&#24335;&#20855;&#36523;&#20195;&#29702;&#65292;&#33021;&#22815;&#29702;&#35299;&#20219;&#24847;&#20154;&#31867;&#25351;&#20196;&#65292;&#24182;&#20197;&#36739;&#39640;&#30340;&#23436;&#25104;&#29575;&#23436;&#25104;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#24320;&#25918;&#24335;&#23398;&#20064;&#20195;&#29702;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;LLM&#22312;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#23454;&#26102;&#20132;&#20114;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#32780;RL&#26041;&#27861;&#21017;&#38754;&#20020;&#25506;&#32034;&#25928;&#29575;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenContra&#65292;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#21327;&#21516;LLM&#21644;Goal-Conditioned&#24378;&#21270;&#23398;&#20064;&#65288;GRL&#65289;&#65292;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#29702;&#35299;&#20219;&#24847;&#20154;&#31867;&#25351;&#20196;&#30340;&#24320;&#25918;&#24335;&#20195;&#29702;&#12290;&#23454;&#29616;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#29992;LLM&#24494;&#35843;&#32763;&#35793;&#20154;&#31867;&#25351;&#20196;&#20026;&#32467;&#26500;&#21270;&#30446;&#26631;&#65292;&#20197;&#21450;&#35838;&#31243;&#35757;&#32451;&#30446;&#26631;&#26465;&#20214;&#30340;RL&#31574;&#30053;&#65292;&#25191;&#34892;&#20219;&#24847;&#30446;&#26631;&#65307;&#65288;2&#65289;&#21327;&#21516;&#35757;&#32451;LLM&#21644;RL&#31574;&#30053;&#30456;&#20114;&#36866;&#24212;&#65292;&#23454;&#29616;&#25351;&#20196;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#12290;&#25105;&#20204;&#22312;Contra&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35813;&#28216;&#25103;&#26159;&#19968;&#20010;&#22797;&#26434;&#32780;&#24191;&#38420;&#30340;&#30446;&#26631;&#31354;&#38388;&#30340;&#22823;&#36867;&#26432;FPS&#28216;&#25103;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;OpenContra&#35757;&#32451;&#30340;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#20219;&#24847;&#20154;&#31867;&#25351;&#20196;&#65292;&#24182;&#20197;&#39640;&#23436;&#25104;&#29575;&#23436;&#25104;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building open-ended learning agents involves challenges in pre-trained language model (LLM) and reinforcement learning (RL) approaches. LLMs struggle with context-specific real-time interactions, while RL methods face efficiency issues for exploration. To this end, we propose OpenContra, a co-training framework that cooperates LLMs and GRL to construct an open-ended agent capable of comprehending arbitrary human instructions. The implementation comprises two stages: (1) fine-tuning an LLM to translate human instructions into structured goals, and curriculum training a goal-conditioned RL policy to execute arbitrary goals; (2) collaborative training to make the LLM and RL policy learn to adapt each, achieving open-endedness on instruction space. We conduct experiments on Contra, a battle royale FPS game with a complex and vast goal space. The results show that an agent trained with OpenContra comprehends arbitrary human instructions and completes goals with a high completion ratio, whic
&lt;/p&gt;</description></item><item><title>AdaNAS&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23545;&#38598;&#21512;&#38477;&#38632;&#39044;&#25253;&#36827;&#34892;&#22788;&#29702;&#65292;&#33021;&#22815;&#25552;&#39640;&#38477;&#38632;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#37319;&#29992;&#38754;&#21521;&#38477;&#38632;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#38477;&#38632;&#23618;&#32423;&#35268;&#21017;&#21270;&#20989;&#25968;&#65292;&#26377;&#25928;&#28040;&#38500;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2312.16046</link><description>&lt;p&gt;
AdaNAS&#65306;&#33258;&#25105;&#30417;&#30563;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#38598;&#21512;&#38477;&#38632;&#39044;&#25253;&#30340;&#33258;&#36866;&#24212;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16046
&lt;/p&gt;
&lt;p&gt;
AdaNAS&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23545;&#38598;&#21512;&#38477;&#38632;&#39044;&#25253;&#36827;&#34892;&#22788;&#29702;&#65292;&#33021;&#22815;&#25552;&#39640;&#38477;&#38632;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#37319;&#29992;&#38754;&#21521;&#38477;&#38632;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#38477;&#38632;&#23618;&#32423;&#35268;&#21017;&#21270;&#20989;&#25968;&#65292;&#26377;&#25928;&#28040;&#38500;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#20851;&#20110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;(NWP)&#38477;&#38632;&#39044;&#25253;&#30340;&#21518;&#22788;&#29702;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32479;&#35745;&#26041;&#38754;&#30340;&#20869;&#23481;&#65292;&#36739;&#23569;&#28041;&#21450;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#38754;&#12290;&#34429;&#28982;&#19968;&#20123;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#26159;&#23450;&#21046;&#30340;&#32593;&#32476;&#65292;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#21644;&#39564;&#35777;&#65292;&#32791;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#20154;&#21147;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37325;&#35201;&#25163;&#21160;&#24037;&#20316;&#30340;&#33258;&#25105;&#30417;&#30563;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#26041;&#27861;&#65292;&#31216;&#20026;AdaNAS&#65292;&#29992;&#20110;&#36827;&#34892;&#38477;&#38632;&#39044;&#25253;&#21518;&#22788;&#29702;&#21644;&#39640;&#20934;&#30830;&#24615;&#30340;&#38477;&#38632;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#38477;&#38632;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#39640;&#38477;&#38632;&#21306;&#22495;&#30340;&#39044;&#25253;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38477;&#38632;&#23618;&#32423;&#35268;&#21017;&#21270;&#20989;&#25968;&#65292;&#20197;&#28040;&#38500;&#35757;&#32451;&#36807;&#31243;&#20013;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#22312;&#22823;&#35268;&#27169;&#39044;&#27979;&#23454;&#39564;&#20013;&#65292;&#26681;&#25454;\emph{&#26080;&#38632;}&#65292;\emph{&#23567;&#38632;}&#65292;\emph{&#20013;&#38632;}&#65292;\emph{&#22823;&#38632;}&#21644;\emph{&#26292;&#38632;}&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous post-processing studies on rainfall forecasts using numerical weather prediction (NWP) mainly focus on statistics-based aspects, while learning-based aspects are rarely investigated. Although some manually-designed models are proposed to raise accuracy, they are customized networks, which need to be repeatedly tried and verified, at a huge cost in time and labor. Therefore, a self-supervised neural architecture search (NAS) method without significant manual efforts called AdaNAS is proposed in this study to perform rainfall forecast post-processing and predict rainfall with high accuracy. In addition, we design a rainfall-aware search space to significantly improve forecasts for high-rainfall areas. Furthermore, we propose a rainfall-level regularization function to eliminate the effect of noise data during the training. Validation experiments have been performed under the cases of \emph{None}, \emph{Light}, \emph{Moderate}, \emph{Heavy} and \emph{Violent} on a large-scale pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;(SIGTRON)&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#20854;&#20276;&#38543;&#30340;SIC&#27169;&#22411;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16043</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#19981;&#24179;&#34913;&#32447;&#24615;&#20998;&#31867;&#30340;&#25193;&#23637;&#38750;&#23545;&#31216;sigmoid&#21644;&#24863;&#30693;&#26426;(SIGTRON)
&lt;/p&gt;
&lt;p&gt;
An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced linear classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;(SIGTRON)&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#20854;&#20276;&#38543;&#30340;SIC&#27169;&#22411;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;&#65292;&#31216;&#20026;SIGTRON&#65292;&#23427;&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#38750;&#23545;&#31216;sigmoid&#20989;&#25968;&#21644;&#24863;&#30693;&#26426;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#23427;&#30340;&#20276;&#38543;&#20984;&#27169;&#22411;SIGTRON-&#19981;&#24179;&#34913;&#20998;&#31867;(SIC)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#34394;&#25311;SIGTRON&#20135;&#29983;&#30340;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;&#19982;&#20256;&#32479;&#30340;$\pi$-&#21152;&#26435;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;SIC&#27169;&#22411;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#27809;&#26377;&#22806;&#37096;&#30340;$\pi$-&#26435;&#37325;&#65292;&#32780;&#26159;&#22312;&#34394;&#25311;&#30340;SIGTRON&#20135;&#29983;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#26377;&#20869;&#37096;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#24403;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#27604;&#22914;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#27604;&#20363;&#19981;&#24179;&#34913;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#36866;&#24212;&#26159;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#30340;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25311;&#29275;&#39039;&#20248;&#21270;(L-BFGS)&#26694;&#26550;&#30340;&#34394;&#25311;&#20984;&#25439;&#22833;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#20108;&#20998;&#32447;&#24615;&#25628;&#32034;&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new polynomial parameterized sigmoid called SIGTRON, which is an extended asymmetric sigmoid with Perceptron, and its companion convex model called SIGTRON-imbalanced classification (SIC) model that employs a virtual SIGTRON-induced convex loss function. In contrast to the conventional $\pi$-weighted cost-sensitive learning model, the SIC model does not have an external $\pi$-weight on the loss function but has internal parameters in the virtual SIGTRON-induced loss function. As a consequence, when the given training dataset is close to the well-balanced condition, we show that the proposed SIC model is more adaptive to variations of the dataset, such as the inconsistency of the scale-class-imbalance ratio between the training and test datasets. This adaptation is achieved by creating a skewed hyperplane equation. Additionally, we present a quasi-Newton optimization(L-BFGS) framework for the virtual convex loss by developing an interval-based bisection line sear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#36974;&#32617;&#21644;&#23545;&#27604;&#22788;&#29702;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#30001;&#20110;&#27169;&#24577;&#32570;&#22833;&#24341;&#36215;&#30340;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2312.13508</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#36807;&#21407;&#22411;&#36974;&#32617;&#21644;&#23545;&#27604;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning with Missing Modality via Prototype Mask and Contrast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#36974;&#32617;&#21644;&#23545;&#27604;&#22788;&#29702;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#30001;&#20110;&#27169;&#24577;&#32570;&#22833;&#24341;&#36215;&#30340;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#32463;&#24120;&#38754;&#20020;&#22797;&#26434;&#30340;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#26500;&#24314;&#32852;&#37030;&#26694;&#26550;&#21644;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#25512;&#29702;&#20934;&#30830;&#24615;&#20135;&#29983;&#38480;&#21046;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#28041;&#21450;&#22312;&#23458;&#25143;&#31471;&#24320;&#21457;&#27169;&#24577;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#26469;&#35299;&#20915;&#32570;&#22833;&#27169;&#24577;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20165;&#36866;&#29992;&#20110;&#20855;&#26377;&#21333;&#27169;&#24577;&#23458;&#25143;&#31471;&#25110;&#23436;&#20840;&#22810;&#27169;&#24577;&#23458;&#25143;&#31471;&#30340;&#29305;&#23450;&#22330;&#26223;&#65292;&#22312;&#22797;&#26434;&#30340;&#27169;&#24577;&#32570;&#22833;&#22330;&#26223;&#20013;&#24456;&#38590;&#26377;&#25928;&#27867;&#21270;&#12290;&#26412;&#25991;&#23558;&#21407;&#22411;&#24211;&#24341;&#20837;&#22522;&#20110;FedAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#32780;&#20351;&#26694;&#26550;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26399;&#38388;&#32531;&#35299;&#30001;&#27169;&#24577;&#32570;&#22833;&#24341;&#36215;&#30340;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#21407;&#22411;&#20316;&#20026;&#34920;&#31034;&#32570;&#22833;&#27169;&#24577;&#30340;&#36974;&#32617;&#65292;&#20197;&#21046;&#23450;&#20219;&#21153;&#26657;&#20934;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, multimodal federated learning often faces the practical challenge of intricate modality missing, which poses constraints on building federated frameworks and significantly degrades model inference accuracy. Existing solutions for addressing missing modalities generally involve developing modality-specific encoders on clients and training modality fusion modules on servers. However, these methods are primarily constrained to specific scenarios with either unimodal clients or complete multimodal clients, struggling to generalize effectively in the intricate modality missing scenarios. In this paper, we introduce a prototype library into the FedAvg-based Federated Learning framework, thereby empowering the framework with the capability to alleviate the global model performance degradation resulting from modality missing during both training and testing. The proposed method utilizes prototypes as masks representing missing modalities to formulate a task-calibrated 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Zero-1-to-3&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#25209;&#26089;&#40479;&#23398;&#29983;&#20851;&#27880;&#19977;&#20010;&#35786;&#26029;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#32423;&#38646;&#26679;&#26412;&#35748;&#30693;&#35786;&#26029;&#12290;&#23427;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#35786;&#26029;&#27169;&#22411;&#20013;&#21487;&#33021;&#34701;&#20837;&#19981;&#21487;&#36716;&#31227;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30693;&#35782;&#20256;&#36755;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.13434</link><description>&lt;p&gt;
Zero-1-to-3: &#22522;&#20110;&#39046;&#22495;&#32423;&#38646;&#26679;&#26412;&#35748;&#30693;&#35786;&#26029;&#30340;&#19968;&#25209;&#26089;&#40479;&#23398;&#29983;&#20851;&#20110;&#19977;&#20010;&#35786;&#26029;&#30446;&#26631;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Zero-1-to-3&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#25209;&#26089;&#40479;&#23398;&#29983;&#20851;&#27880;&#19977;&#20010;&#35786;&#26029;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#32423;&#38646;&#26679;&#26412;&#35748;&#30693;&#35786;&#26029;&#12290;&#23427;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#35786;&#26029;&#27169;&#22411;&#20013;&#21487;&#33021;&#34701;&#20837;&#19981;&#21487;&#36716;&#31227;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30693;&#35782;&#20256;&#36755;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#36890;&#36807;&#25506;&#32034;&#23398;&#29983;&#30340;&#32451;&#20064;&#27979;&#39564;&#25968;&#25454;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#35748;&#30693;&#29366;&#24577;&#12290;&#23427;&#22312;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#20010;&#24615;&#21270;&#23398;&#20064;&#25351;&#23548;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#20010;&#37325;&#35201;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#20219;&#21153;&#65306;&#39046;&#22495;&#32423;&#38646;&#26679;&#26412;&#35748;&#30693;&#35786;&#26029;&#65288;DZCD&#65289;&#65292;&#22240;&#20026;&#22312;&#26032;&#21551;&#21160;&#30340;&#39046;&#22495;&#20013;&#32570;&#20047;&#23398;&#29983;&#30340;&#32451;&#20064;&#35760;&#24405;&#32780;&#20135;&#29983;&#12290;&#26368;&#36817;&#30340;&#36328;&#39046;&#22495;&#35786;&#26029;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#26159;DZCD&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#22312;&#39046;&#22495;&#20043;&#38388;&#20256;&#36882;&#23398;&#29983;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19981;&#21487;&#36716;&#31227;&#30340;&#20449;&#24687;&#34701;&#20837;&#21040;&#23398;&#29983;&#30340;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30693;&#35782;&#20256;&#36755;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Zero-1-to-3&#65292;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#32423;&#38646;&#26679;&#26412;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#25209;&#26089;&#40479;&#23398;&#29983;&#23454;&#29616;&#19977;&#20010;&#35786;&#26029;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35786;&#26029;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive diagnosis seeks to estimate the cognitive states of students by exploring their logged practice quiz data. It plays a pivotal role in personalized learning guidance within intelligent education systems. In this paper, we focus on an important, practical, yet often underexplored task: domain-level zero-shot cognitive diagnosis (DZCD), which arises due to the absence of student practice logs in newly launched domains. Recent cross-domain diagnostic models have been demonstrated to be a promising strategy for DZCD. These methods primarily focus on how to transfer student states across domains. However, they might inadvertently incorporate non-transferable information into student representations, thereby limiting the efficacy of knowledge transfer. To tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitive diagnosis framework via one batch of early-bird students towards three diagnostic objectives. Our approach initiates with pre-training a diagnosis model with d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23545;&#35805;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KG&#25512;&#29702;&#30340;LLM&#22522;&#20934;&#20195;&#29702;&#65288;LLM-ARK&#65289;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;&#20840;&#25991;&#29615;&#22659;&#25552;&#31034;&#26469;&#23454;&#29616;&#31934;&#30830;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;KG&#36335;&#24452;&#39044;&#27979;&#65292;&#24182;&#37319;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2312.11282</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#22686;&#24378;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23545;&#35805;&#25512;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23545;&#35805;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KG&#25512;&#29702;&#30340;LLM&#22522;&#20934;&#20195;&#29702;&#65288;LLM-ARK&#65289;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;&#20840;&#25991;&#29615;&#22659;&#25552;&#31034;&#26469;&#23454;&#29616;&#31934;&#30830;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;KG&#36335;&#24452;&#39044;&#27979;&#65292;&#24182;&#37319;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#24471;&#30410;&#20110;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#25163;&#21160;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLM&#65288;GPT-4&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;KG&#29615;&#22659;&#24847;&#35782;&#21644;&#24320;&#21457;&#26377;&#25928;&#30340;&#20013;&#38388;&#25512;&#29702;&#38454;&#27573;&#20248;&#21270;&#26426;&#21046;&#30340;&#22256;&#38590;&#65292;LLM&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;LLM-ARK&#65292;&#19968;&#20010;&#22522;&#20110;KG&#25512;&#29702;&#30340;LLM&#22522;&#20934;&#20195;&#29702;&#65292;&#26088;&#22312;&#25552;&#20379;&#31934;&#30830;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;KG&#36335;&#24452;&#39044;&#27979;&#12290;LLM-ARK&#21033;&#29992;&#20840;&#25991;&#29615;&#22659;&#65288;FTE&#65289;&#25552;&#31034;&#26469;&#21560;&#25910;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#20013;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;KG&#19978;&#30340;&#22810;&#36339;&#25512;&#29702;&#25361;&#25112;&#37325;&#26032;&#26694;&#23450;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#12290;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) has been catalyzed by advancements in pre-training techniques. These models have demonstrated robust reasoning capabilities through manually designed prompts. In this work, we evaluate the conversational reasoning capabilities of the current state-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the performance of LLMs is constrained due to a lack of KG environment awareness and the difficulties in developing effective optimization mechanisms for intermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG reasoning agent designed to deliver precise and adaptable predictions on KG paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate state information within each reasoning step. We reframe the challenge of multi-hop reasoning on the KG as a sequential decision-making task. Utilizing the Proximal Policy Optimization (PPO) online policy gradient reinforcement learning algorithm, our model i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#26694;&#26550;&#65292;&#21517;&#20026;Talk2Drive&#65292;&#29992;&#20110;&#22788;&#29702;&#26469;&#33258;&#20154;&#31867;&#30340;&#21475;&#22836;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#65292;&#28385;&#36275;&#20010;&#24615;&#21270;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2312.09397</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65306;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Autonomous Driving: Real-World Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09397
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#26694;&#26550;&#65292;&#21517;&#20026;Talk2Drive&#65292;&#29992;&#20110;&#22788;&#29702;&#26469;&#33258;&#20154;&#31867;&#30340;&#21475;&#22836;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#65292;&#28385;&#36275;&#20010;&#24615;&#21270;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#24403;&#20170;&#30340;&#25216;&#26415;&#39046;&#22495;&#26085;&#30410;&#27969;&#34892;&#65292;&#37096;&#20998;&#33258;&#21160;&#21270;&#30340;&#36710;&#36742;&#24050;&#32463;&#22312;&#24066;&#22330;&#19978;&#24191;&#27867;&#27969;&#36890;&#65292;&#20855;&#22791;&#23436;&#20840;&#33258;&#21160;&#21270;&#21644;&#8220;&#26080;&#20154;&#39550;&#39542;&#8221;&#33021;&#21147;&#30340;&#26102;&#20195;&#24050;&#32463;&#36843;&#22312;&#30473;&#30571;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#29702;&#35299;&#20154;&#31867;&#30340;&#25351;&#20196;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21482;&#26377;&#20056;&#23458;&#32780;&#27809;&#26377;&#39550;&#39542;&#21592;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26469;&#35828;&#65292;&#23454;&#29616;&#39640;&#24230;&#20010;&#24615;&#21270;&#20173;&#28982;&#26159;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#25361;&#25112;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;Talk-to-Drive&#65288;Talk2Drive&#65289;&#65292;&#20197;&#22788;&#29702;&#26469;&#33258;&#20154;&#31867;&#30340;&#21475;&#22836;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#20570;&#20986;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#65292;&#28385;&#36275;&#20182;&#20204;&#23545;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35821;&#38899;&#35782;&#21035;&#27169;&#22359;&#65292;&#29992;&#20110;&#23558;&#20154;&#31867;&#30340;&#21475;&#22836;&#36755;&#20837;&#36716;&#21270;&#20026;&#25991;&#26412;&#25351;&#20196;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#25351;&#20196;&#21457;&#36865;&#32473;LLMs&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;&#36866;&#24403;&#30340;&#25351;&#20196;&#34987;&#21457;&#36865;&#32473;&#23454;&#38469;&#30340;&#27773;&#36710;&#25511;&#21046;&#31995;&#32479;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving systems are increasingly popular in today's technological landscape, where vehicles with partial automation have already been widely available on the market, and the full automation era with "driverless" capabilities is near the horizon. However, accurately understanding humans' commands, particularly for autonomous vehicles that have only passengers instead of drivers, and achieving a high level of personalization remain challenging tasks in the development of autonomous driving systems. In this paper, we introduce a Large Language Model (LLM)-based framework Talk-to-Drive (Talk2Drive) to process verbal commands from humans and make autonomous driving decisions with contextual information, satisfying their personalized preferences for safety, efficiency, and comfort. First, a speech recognition module is developed for Talk2Drive to interpret verbal inputs from humans to textual instructions, which are then sent to LLMs for reasoning. Then, appropriate commands for t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#37327;&#21160;&#20316;&#30340;&#37325;&#22797;&#28216;&#25103;&#20013;&#23454;&#29616;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Oracle&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20869;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#28216;&#25103;&#20013;&#35745;&#31639;&#30456;&#20851;&#22343;&#34913;&#30340;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#22312;AI&#23433;&#20840;&#36777;&#35770;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.04792</link><description>&lt;p&gt;
&#20351;&#29992;Oracle&#21644;AI&#36777;&#35770;&#36827;&#34892;&#22823;&#22411;&#28216;&#25103;&#30340;&#29609;&#27861;
&lt;/p&gt;
&lt;p&gt;
Playing Large Games with Oracles and AI Debate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#37327;&#21160;&#20316;&#30340;&#37325;&#22797;&#28216;&#25103;&#20013;&#23454;&#29616;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Oracle&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20869;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#28216;&#25103;&#20013;&#35745;&#31639;&#30456;&#20851;&#22343;&#34913;&#30340;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#22312;AI&#23433;&#20840;&#36777;&#35770;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20855;&#26377;&#22823;&#37327;&#21160;&#20316;&#30340;&#37325;&#22797;&#28216;&#25103;&#20013;&#23454;&#29616;&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;&#36825;&#31181;&#28216;&#25103;&#22312;&#36890;&#36807;&#36777;&#35770;&#30830;&#20445;AI&#23433;&#20840;&#30340;&#29615;&#22659;&#20013;&#26159;&#22266;&#26377;&#30340;&#65292;&#24182;&#19988;&#26356;&#19968;&#33324;&#22320;&#24212;&#29992;&#20110;&#21160;&#20316;&#22522;&#20110;&#35821;&#35328;&#30340;&#28216;&#25103;&#20013;&#12290;&#29616;&#26377;&#30340;&#22312;&#32447;&#28216;&#25103;&#31639;&#27861;&#38656;&#35201;&#22810;&#39033;&#24335;&#35745;&#31639;&#25968;&#37327;&#30340;&#21160;&#20316;&#65292;&#32780;&#23545;&#20110;&#22823;&#22411;&#28216;&#25103;&#26469;&#35828;&#65292;&#36825;&#21487;&#33021;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22522;&#20110;Oracle&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;Oracle&#33258;&#28982;&#22320;&#27169;&#25311;&#20102;&#23545;AI&#20195;&#29702;&#30340;&#35775;&#38382;&#12290;&#36890;&#36807;&#23545;Oracle&#35775;&#38382;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#20869;&#37096;&#21644;&#22806;&#37096;&#36951;&#25022;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#23545;&#25968;&#22320;&#20381;&#36182;&#20110;&#21160;&#20316;&#25968;&#37327;&#12290;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#39640;&#25928;&#22320;&#22522;&#20110;Oracle&#35745;&#31639;&#22823;&#22411;&#28216;&#25103;&#20013;&#30340;&#30456;&#20851;&#22343;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;AI&#23433;&#20840;&#36777;&#35770;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#20998;&#26512;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider regret minimization in repeated games with a very large number of actions. Such games are inherent in the setting of AI safety via debate, and more generally games whose actions are language-based. Existing algorithms for online game playing require computation polynomial in the number of actions, which can be prohibitive for large games.   We thus consider oracle-based algorithms, as oracles naturally model access to AI agents. With oracle access, we characterize when internal and external regret can be minimized efficiently. We give a novel efficient algorithm for internal regret minimization whose regret and computation complexity depend logarithmically on the number of actions. This implies efficient oracle-based computation of a correlated equilibrium in large games.   We conclude with experiments in the setting of AI Safety via Debate that shows the benefit of insights from our algorithmic analysis.
&lt;/p&gt;</description></item><item><title>Elijah&#26159;&#19968;&#31181;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#38376;&#26816;&#27979;&#21644;&#28040;&#38500;&#26694;&#26550;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21518;&#38376;&#24182;&#23558;&#20854;&#25928;&#26524;&#20943;&#23569;&#33267;&#25509;&#36817;&#38646;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#29306;&#29298;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.00050</link><description>&lt;p&gt;
Elijah: &#36890;&#36807;&#20998;&#24067;&#21464;&#21270;&#28040;&#38500;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00050
&lt;/p&gt;
&lt;p&gt;
Elijah&#26159;&#19968;&#31181;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#38376;&#26816;&#27979;&#21644;&#28040;&#38500;&#26694;&#26550;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21518;&#38376;&#24182;&#23558;&#20854;&#25928;&#26524;&#20943;&#23569;&#33267;&#25509;&#36817;&#38646;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#29306;&#29298;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DM)&#22240;&#20854;&#33021;&#22815;&#20174;&#22122;&#22768;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#32780;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#24403;&#19968;&#20010;&#25968;&#25454;&#36755;&#20837;&#65288;&#20363;&#22914;&#19968;&#20123;&#39640;&#26031;&#22122;&#22768;&#65289;&#34987;&#27880;&#20837;&#35302;&#21457;&#22120;&#65288;&#20363;&#22914;&#19968;&#20010;&#30333;&#33394;&#26001;&#28857;&#65289;&#26102;&#65292;&#24102;&#26377;&#21518;&#38376;&#30340;&#27169;&#22411;&#24635;&#26159;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65288;&#20363;&#22914;&#19981;&#24688;&#24403;&#30340;&#29031;&#29255;&#65289;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#26469;&#20943;&#36731;DM&#20013;&#30340;&#21518;&#38376;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;DM&#30340;&#21518;&#38376;&#26816;&#27979;&#21644;&#28040;&#38500;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;13&#31181;&#37319;&#26679;&#22120;&#23545;&#21253;&#25324;DDPM&#12289;NCSN&#21644;LDM&#22312;&#20869;&#30340;&#25968;&#30334;&#20010;DM&#36827;&#34892;&#35780;&#20272;&#65292;&#38024;&#23545;3&#31181;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25509;&#36817;100%&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#23558;&#21518;&#38376;&#25928;&#26524;&#20943;&#23569;&#33267;&#25509;&#36817;&#38646;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#29306;&#29298;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DM) have become state-of-the-art generative models because of their capability to generate high-quality images from noises without adversarial training. However, they are vulnerable to backdoor attacks as reported by recent studies. When a data input (e.g., some Gaussian noise) is stamped with a trigger (e.g., a white patch), the backdoored model always generates the target image (e.g., an improper photo). However, effective defense strategies to mitigate backdoors from DMs are underexplored. To bridge this gap, we propose the first backdoor detection and removal framework for DMs. We evaluate our framework Elijah on hundreds of DMs of 3 types including DDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks. Extensive experiments show that our approach can have close to 100% detection accuracy and reduce the backdoor effects to close to zero without significantly sacrificing the model utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18751</link><description>&lt;p&gt;
&#22312;Web&#19978;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#39034;&#24207;&#20219;&#21153;&#32452;&#21512;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;(LMA)&#20316;&#20026;&#19968;&#31181;&#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21644;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#22312;&#36890;&#24120;&#28041;&#21450;&#20219;&#21153;&#32452;&#21512;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21483;&#20570;CompWoB-&#21453;&#26144;&#26356;&#29616;&#23454;&#20551;&#35774;&#30340;50&#20010;&#32452;&#21512;&#24615;&#32593;&#31449;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#25552;&#31034;&#22411;LMA&#65288;gpt-3.5-turbo&#25110;gpt-4&#65289;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;94.0&#65285;&#30340;&#24179;&#22343;&#25104;&#21151;&#29575;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#38477;&#33267;24.9&#65285;&#30340;&#25104;&#21151;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21482;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#36716;&#31227;&#24615;LMA&#34920;&#29616;&#20986;&#26356;&#23567;&#30340;&#27867;&#21270;&#24615;&#24046;&#36317;&#65292;&#20174;85.4&#65285;&#19979;&#38477;&#21040;54.8&#65285;&#12290;&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;HTML-T5++&#65292;&#22312;MiniWoB&#19978;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65288;95.2&#65285;&#65289;&#65292;&#24182;&#22312;CompWoB&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#38646;-shot&#24615;&#33021;&#65288;61.5%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.18703</link><description>&lt;p&gt;
&#36890;&#36807;&#29109;&#29575;&#26368;&#23567;&#21270;&#23454;&#29616;&#21487;&#39044;&#27979;&#30340;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#27809;&#26377;&#21160;&#26426;&#23637;&#31034;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#36890;&#24120;&#36890;&#36807;&#31574;&#30053;&#29109;&#27491;&#21017;&#21270;&#25512;&#21160;&#26234;&#33021;&#20307;&#22312;&#25506;&#32034;&#19978;&#38543;&#26426;&#21270;&#20854;&#34892;&#20026;&#12290;&#20174;&#20154;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24456;&#38590;&#35299;&#37322;&#21644;&#39044;&#27979;&#65307;&#20174;&#23433;&#20840;&#35282;&#24230;&#26469;&#30475;&#65292;&#26356;&#38590;&#20197;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39044;&#27979;&#24615;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#65288;PA-RL&#65289;&#65292;&#29992;&#20110;&#24341;&#23548;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#20854;&#21033;&#29992;&#29366;&#24577;&#24207;&#21015;&#29109;&#29575;&#20316;&#20026;&#21487;&#39044;&#27979;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29109;&#29575;&#21046;&#23450;&#20026;&#24179;&#22343;&#22870;&#21169;&#30446;&#26631;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#29109;&#22870;&#21169;&#20989;&#25968;&#20381;&#36182;&#20110;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#20316;&#30456;&#20851;&#30340;&#26367;&#20195;&#29109;&#65292;&#20197;&#21033;&#29992;PG&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#23384;&#22312;&#65292;&#24182;&#19988;&#26368;&#23567;&#21270;&#20102;&#23454;&#38469;&#29109;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#19982;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18022</link><description>&lt;p&gt;
&#21033;&#29992;&#25351;&#25968;&#23610;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30475;&#20316;&#26159;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#32452;&#21512;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#34920;&#36798;&#22312;&#36755;&#20837;&#22495;&#19978;&#30340;&#19981;&#21516;&#32447;&#24615;&#21306;&#22495;&#30340;&#25968;&#37327;&#26377;&#21487;&#33021;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20294;&#24403;&#21021;&#22987;&#21442;&#25968;&#36873;&#25321;&#38543;&#26426;&#26102;&#65292;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#19981;&#33391;&#30340;&#23610;&#24230;&#33021;&#22815;&#23548;&#33268;&#21363;&#20351;&#26159;&#31616;&#21333;&#20989;&#25968;&#20063;&#38656;&#35201;&#20351;&#29992;&#36807;&#22823;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#20197;&#19968;&#31181;&#26041;&#24335;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#12290;&#22312;&#36825;&#20123;&#26032;&#21442;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#31245;&#21518;&#36890;&#36807;&#26356;&#26032;&#24213;&#23618;&#27169;&#22411;&#26435;&#37325;&#26469;&#25913;&#36827;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#27604;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#24212;&#30340;&#20989;&#25968;&#36924;&#36817;&#22909;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChatTraffic&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#24182;&#24471;&#21040;&#19982;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#19968;&#33268;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2311.16203</link><description>&lt;p&gt;
ChatTraffic&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatTraffic: Text-to-Traffic Generation via Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChatTraffic&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#24182;&#24471;&#21040;&#19982;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#19968;&#33268;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#22522;&#30784;&#20043;&#19968;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#21482;&#20381;&#36182;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26469;&#39044;&#27979;&#20132;&#36890;&#36235;&#21183;&#65292;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#23545;&#24322;&#24120;&#20107;&#20214;&#19981;&#25935;&#24863;&#65307;2&#65289;&#22312;&#38271;&#26399;&#39044;&#27979;&#26041;&#38754;&#24615;&#33021;&#26377;&#38480;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#20132;&#36890;&#29983;&#25104;&#65292;&#23558;&#27492;&#20219;&#21153;&#21629;&#21517;&#20026;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#65288;TTG&#65289;&#12290;TTG&#20219;&#21153;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#23558;&#25991;&#26412;&#19982;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#20132;&#36890;&#25968;&#25454;&#30456;&#20851;&#32852;&#65292;&#29992;&#20110;&#29983;&#25104;&#20132;&#36890;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatTraffic&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#20445;&#35777;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25552;&#21462;&#20132;&#36890;&#25968;&#25454;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;...
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) limited performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation, and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first diffusion model for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a diffusion model with the Graph Convolutional Network (GCN) to extract spatial correlations of traffic data. In addition, we construct a large dataset containing t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#37319;&#26679;&#20026;&#23548;&#21521;&#30340;Langevin&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;PC&#25512;&#29702;&#36807;&#31243;&#27880;&#20837;&#39640;&#26031;&#22122;&#22768;&#23454;&#29616;&#36807;&#38459;&#23612;&#30340;Langevin&#37319;&#26679;&#65292;&#24182;&#25913;&#36827;&#20102;&#32467;&#26524;&#32534;&#30721;&#22120;&#33258;&#30001;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;&#32593;&#32476;&#25552;&#20379;&#25674;&#38144;&#30340;&#28909;&#21551;&#21160;&#12290;&#27492;&#22806;&#65292;&#36824;&#39564;&#35777;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#39044;&#22788;&#29702;&#24418;&#24335;&#65292;&#20351;&#24471;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.13664</link><description>&lt;p&gt;
&#20197;&#37319;&#26679;&#20026;&#23548;&#21521;: Langevin&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sample as You Infer: Predictive Coding With Langevin Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#37319;&#26679;&#20026;&#23548;&#21521;&#30340;Langevin&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;PC&#25512;&#29702;&#36807;&#31243;&#27880;&#20837;&#39640;&#26031;&#22122;&#22768;&#23454;&#29616;&#36807;&#38459;&#23612;&#30340;Langevin&#37319;&#26679;&#65292;&#24182;&#25913;&#36827;&#20102;&#32467;&#26524;&#32534;&#30721;&#22120;&#33258;&#30001;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;&#32593;&#32476;&#25552;&#20379;&#25674;&#38144;&#30340;&#28909;&#21551;&#21160;&#12290;&#27492;&#22806;&#65292;&#36824;&#39564;&#35777;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#39044;&#22788;&#29702;&#24418;&#24335;&#65292;&#20351;&#24471;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36890;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#23398;&#20064;&#21442;&#25968;&#65292;&#35813;&#31639;&#27861;&#24314;&#31435;&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#30340;&#39044;&#27979;&#32534;&#30721;(PC)&#26694;&#26550;&#20043;&#19978;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20462;&#25913;&#20102;&#26631;&#20934;&#30340;PC&#31639;&#27861;&#65292;&#20351;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#35757;&#32451;&#30340;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#36229;&#36807;&#12290;&#36890;&#36807;&#23558;&#39640;&#26031;&#22122;&#22768;&#27880;&#20837;PC&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23558;&#20854;&#26500;&#24819;&#20026;&#36807;&#38459;&#23612;&#30340;Langevin&#37319;&#26679;&#65292;&#20174;&#32780;&#26041;&#20415;&#23545;&#32039;&#20945;&#35777;&#25454;&#19979;&#30028;(ELBO)&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#32467;&#26524;&#32534;&#30721;&#22120;&#33258;&#30001;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32534;&#30721;&#22120;&#32593;&#32476;&#32435;&#20837;&#20854;&#20013;&#65292;&#20026;&#25105;&#20204;&#30340;Langevin&#37319;&#26679;&#25552;&#20379;&#20102;&#19968;&#31181;&#25674;&#38144;&#30340;&#28909;&#21551;&#21160;&#65292;&#24182;&#27979;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#22686;&#21152;&#23545;&#37319;&#26679;&#27493;&#38271;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20943;&#23569;&#23545;&#26354;&#29575;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#39044;&#22788;&#29702;&#24418;&#24335;&#65292;&#21463;&#21040;Riemann Manifold Langevin&#21644;SGD&#25991;&#29486;&#20013;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19982;...
&lt;/p&gt;
&lt;p&gt;
We present a novel algorithm for parameter learning in generic deep generative models that builds upon the predictive coding (PC) framework of computational neuroscience. Our approach modifies the standard PC algorithm to bring performance on-par and exceeding that obtained from standard variational auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference procedure we re-envision it as an overdamped Langevin sampling, which facilitates optimisation with respect to a tight evidence lower bound (ELBO). We improve the resultant encoder-free training method by incorporating an encoder network to provide an amortised warm-start to our Langevin sampling and test three different objectives for doing so. Finally, to increase robustness to the sampling step size and reduce sensitivity to curvature, we validate a lightweight and easily computable form of preconditioning, inspired by Riemann Manifold Langevin and adaptive optimizers from the SGD literature. We compare agains
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#25915;&#20987;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#33030;&#24369;&#28857;&#65292;&#24182;&#36890;&#36807;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#30340;&#31574;&#30053;&#23454;&#29616;&#20102;&#26356;&#24555;&#36895;&#21644;&#26356;&#39640;&#25928;&#30340;&#20445;&#25252;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.12832</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#25968;&#33976;&#39311;&#23454;&#29616;&#23545;&#25193;&#25955;&#20223;&#30495;&#30340;&#26377;&#25928;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Toward effective protection against diffusion based mimicry through score distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#25915;&#20987;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#33030;&#24369;&#28857;&#65292;&#24182;&#36890;&#36807;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#30340;&#31574;&#30053;&#23454;&#29616;&#20102;&#26356;&#24555;&#36895;&#21644;&#26356;&#39640;&#25928;&#30340;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20063;&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#27169;&#20223;&#25480;&#26435;&#22270;&#20687;&#65292;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#22522;&#20110;&#25193;&#25955;&#30340;&#20223;&#30495;&#31649;&#36947;&#30340;&#23041;&#32961;&#65292;&#20154;&#20204;&#24050;&#32463;&#23581;&#35797;&#28155;&#21152;&#26657;&#20934;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#35745;&#31639;&#37327;&#21644;&#20869;&#23384;&#38656;&#27714;&#36807;&#39640;&#32780;&#23545;&#20010;&#20307;&#29992;&#25143;&#26469;&#35828;&#26080;&#25928;&#29978;&#33267;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#25915;&#20987;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#30340;&#26032;&#21457;&#29616;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#20445;&#25252;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25915;&#20987;LDM&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#21457;&#29616;&#32534;&#30721;&#22120;&#27169;&#22359;&#32780;&#19981;&#26159;&#21435;&#22122;&#27169;&#22359;&#26159;&#33030;&#24369;&#28857;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#30340;&#31574;&#30053;&#65292;&#20197;&#22312;&#19981;&#25439;&#23475;&#20854;&#24378;&#24230;&#30340;&#21069;&#25552;&#19979;&#20351;&#20445;&#25252;&#36895;&#24230;&#21152;&#20493;&#65292;&#20869;&#23384;&#21344;&#29992;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#20445;&#25252;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
While generative diffusion models excel in producing high-quality images, they can also be misused to mimic authorized images, posing a significant threat to AI systems. Efforts have been made to add calibrated perturbations to protect images from diffusion-based mimicry pipelines. However, most of the existing methods are too ineffective and even impractical to be used by individual users due to their high computation and memory requirements. In this work, we present novel findings on attacking latent diffusion models (LDM) and propose new plug-and-play strategies for more effective protection. In particular, we explore the bottleneck in attacking an LDM, discovering that the encoder module rather than the denoiser module is the vulnerable point. Based on this insight, we present our strategy using Score Distillation Sampling (SDS) to double the speed of protection and reduce memory occupation by half without compromising its strength. Additionally, we provide a robust protection stra
&lt;/p&gt;</description></item><item><title>DURel&#27880;&#37322;&#24037;&#20855;&#26159;&#19968;&#20010;&#22312;&#32447;&#30340;&#12289;&#24320;&#28304;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#30340;&#27880;&#37322;&#23454;&#29616;&#20102;&#23545;&#21333;&#35789;&#20351;&#29992;&#20043;&#38388;&#30340;&#35821;&#20041;&#25509;&#36817;&#24230;&#30340;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#32858;&#31867;&#21644;&#35821;&#20041;&#21464;&#21270;&#30340;&#20998;&#26512;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.12664</link><description>&lt;p&gt;
DURel&#27880;&#37322;&#24037;&#20855;&#65306;&#20154;&#31867;&#21644;&#35745;&#31639;&#27979;&#37327;&#35821;&#20041;&#25509;&#36817;&#24230;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#35821;&#20041;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12664
&lt;/p&gt;
&lt;p&gt;
DURel&#27880;&#37322;&#24037;&#20855;&#26159;&#19968;&#20010;&#22312;&#32447;&#30340;&#12289;&#24320;&#28304;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#30340;&#27880;&#37322;&#23454;&#29616;&#20102;&#23545;&#21333;&#35789;&#20351;&#29992;&#20043;&#38388;&#30340;&#35821;&#20041;&#25509;&#36817;&#24230;&#30340;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#32858;&#31867;&#21644;&#35821;&#20041;&#21464;&#21270;&#30340;&#20998;&#26512;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DURel&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#23454;&#29616;&#20102;&#22312;&#22312;&#32447;&#12289;&#24320;&#28304;&#30028;&#38754;&#20013;&#23545;&#21333;&#35789;&#20351;&#29992;&#20043;&#38388;&#30340;&#35821;&#20041;&#25509;&#36817;&#24230;&#36827;&#34892;&#27880;&#37322;&#12290;&#35813;&#24037;&#20855;&#25903;&#25345;&#26631;&#20934;&#21270;&#30340;&#20154;&#31867;&#27880;&#37322;&#21644;&#35745;&#31639;&#26426;&#27880;&#37322;&#65292;&#21033;&#29992;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#35789;&#27169;&#22411;&#30340;&#36827;&#23637;&#36827;&#34892;&#26500;&#24314;&#12290;&#27880;&#37322;&#32773;&#30340;&#21028;&#26029;&#36890;&#36807;&#33258;&#21160;&#22270;&#24418;&#32858;&#31867;&#25216;&#26415;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36827;&#34892;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;&#36825;&#20801;&#35768;&#36890;&#36807;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#24494;&#20219;&#21153;&#21028;&#26029;&#26469;&#27979;&#37327;&#21333;&#35789;&#35789;&#20041;&#65292;&#24182;&#19988;&#38656;&#35201;&#26368;&#23567;&#30340;&#20934;&#22791;&#24037;&#20316;&#12290;&#35813;&#24037;&#20855;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#21151;&#33021;&#65292;&#20197;&#27604;&#36739;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#30830;&#20445;&#33719;&#24471;&#21028;&#26029;&#30340;&#20027;&#35266;&#24615;&#65292;&#24182;&#35745;&#31639;&#24635;&#32467;&#32479;&#35745;&#25968;&#25454;&#65292;&#20197;&#25581;&#31034;&#35789;&#20041;&#39057;&#29575;&#20998;&#24067;&#12289;&#35821;&#20041;&#21464;&#24322;&#25110;&#35789;&#20041;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the DURel tool that implements the annotation of semantic proximity between uses of words into an online, open source interface. The tool supports standardized human annotation as well as computational annotation, building on recent advances with Word-in-Context models. Annotator judgments are clustered with automatic graph clustering techniques and visualized for analysis. This allows to measure word senses with simple and intuitive micro-task judgments between use pairs, requiring minimal preparation efforts. The tool offers additional functionalities to compare the agreement between annotators to guarantee the inter-subjectivity of the obtained judgments and to calculate summary statistics giving insights into sense frequency distributions, semantic variation or changes of senses over time.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#19968;&#33324;&#29615;&#22659;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#31181;&#29615;&#22659;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#65292;&#24182;&#25351;&#20986;&#20102;&#21463;&#21040;&#22260;&#32469;&#33410;&#28857;&#27495;&#20041;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#21487;&#20197;&#24674;&#22797;&#20986;&#22320;&#38754;&#30495;&#23454;&#27169;&#22411;</title><link>https://arxiv.org/abs/2311.12267</link><description>&lt;p&gt;
&#20174;&#19968;&#33324;&#29615;&#22659;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65306;&#21487;&#36776;&#35782;&#24615;&#21644;&#20869;&#22312;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12267
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#19968;&#33324;&#29615;&#22659;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#31181;&#29615;&#22659;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#65292;&#24182;&#25351;&#20986;&#20102;&#21463;&#21040;&#22260;&#32469;&#33410;&#28857;&#27495;&#20041;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#21487;&#20197;&#24674;&#22797;&#20986;&#22320;&#38754;&#30495;&#23454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#21363;&#20174;&#20302;&#32423;&#35266;&#27979;&#25968;&#25454;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#20013;&#24674;&#22797;&#39640;&#32423;&#28508;&#22312;&#21464;&#37327;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#20174;&#22810;&#20010;&#29615;&#22659;&#29983;&#25104;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#20043;&#21069;&#20851;&#20110;&#22240;&#26524;&#34920;&#31034;&#21487;&#36776;&#35782;&#24615;&#30340;&#32467;&#26524;&#36890;&#24120;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21333;&#33410;&#28857;&#24178;&#39044;&#65292;&#20294;&#23454;&#38469;&#19978;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#28508;&#22312;&#21464;&#37327;&#26412;&#36523;&#23601;&#26410;&#30693;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#26469;&#33258;&#19968;&#33324;&#29615;&#22659;&#30340;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#32447;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#34429;&#28982;&#21487;&#20197;&#23436;&#20840;&#24674;&#22797;&#22240;&#26524;&#22270;&#65292;&#20294;&#28508;&#22312;&#21464;&#37327;&#21482;&#33021;&#34987;&#35782;&#21035;&#21040;&#21463;&#21040;&#22260;&#32469;&#33410;&#28857;&#27495;&#20041;&#65288;SNA&#65289;&#30340;&#31243;&#24230;&#19978;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#20445;&#35777;&#30340;&#23545;&#24212;&#23545;&#65292;&#35777;&#26126;&#20102;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;SNA&#22522;&#26412;&#19978;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;LiNGCReL&#65292;&#21487;&#20197;&#34987;&#35777;&#26126;&#21487;&#20197;&#24674;&#22797;&#20986;&#22320;&#38754;&#30495;&#23454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we provide the first identifiability results based on data that stem from general environments. We show that for linear causal models, while the causal graph can be fully recovered, the latent variables are only identified up to the surrounded-node ambiguity (SNA) \citep{varici2023score}. We provide a counterpart of our guarantee, showing that SNA is basically unavoidable in our setting. We also propose an algorithm, \texttt{LiNGCReL} which provably recovers the ground-truth model up to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24778;&#21916;&#24615;&#39537;&#21160;&#30340;&#31283;&#20581;&#21487;&#35299;&#37322;&#30340;k-NN&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#20256;&#32479;&#31639;&#27861;&#36827;&#34892;&#26032;&#30340;&#38416;&#37322;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#23494;&#24230;&#20272;&#35745;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.10246</link><description>&lt;p&gt;
&#22522;&#20110;&#24778;&#21916;&#24615;&#39537;&#21160;&#30340;&#31283;&#20581;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;k-NN&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24778;&#21916;&#24615;&#39537;&#21160;&#30340;&#31283;&#20581;&#21487;&#35299;&#37322;&#30340;k-NN&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#20256;&#32479;&#31639;&#27861;&#36827;&#34892;&#26032;&#30340;&#38416;&#37322;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#23494;&#24230;&#20272;&#35745;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21442;&#25968;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#27010;&#24565;&#65292;&#26088;&#22312;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20851;&#31995;&#65292;&#32780;&#19981;&#23545;&#28508;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#20570;&#20986;&#24378;&#28872;&#30340;&#20551;&#35774;&#12290;&#22312;&#36825;&#19968;&#33539;&#24335;&#19979;&#65292;&#26368;&#20026;&#33879;&#21517;&#30340;&#31639;&#27861;&#20043;&#19968;&#26159;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#20256;&#32479;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;&#36827;&#34892;&#20102;&#26032;&#30340;&#38416;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#23494;&#24230;&#20272;&#35745;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#22686;&#21152;&#29305;&#24449;&#26102;&#30340;&#26465;&#20214;&#29109;&#26469;&#30830;&#23450;&#25968;&#25454;&#28857;&#30340;&#26435;&#37325;&#21644;&#29305;&#24449;&#30340;&#36129;&#29486;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26174;&#24335;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#25968;&#25454;&#28857;&#24433;&#21709;&#26435;&#37325;&#26469;&#35745;&#31639;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonparametric learning is a fundamental concept in machine learning that aims to capture complex patterns and relationships in data without making strong assumptions about the underlying data distribution. Owing to simplicity and familiarity, one of the most well-known algorithms under this paradigm is the $k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine learning in safety-critical applications, in this work, we shed new light on the traditional nearest neighbors algorithm from the perspective of information theory and propose a robust and interpretable framework for tasks such as classification, regression, density estimation, and anomaly detection using a single model. We can determine data point weights as well as feature contributions by calculating the conditional entropy for adding a feature without the need for explicit model training. This allows us to compute feature contributions by providing detailed data point influence weights with perfect attributi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09308</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#33041;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Divergences between Language Models and Human Brains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21644;&#20154;&#31867;&#26159;&#21542;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#22788;&#29702;&#35821;&#35328;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#26263;&#31034;&#32943;&#23450;&#65292;&#21457;&#29616;&#22823;&#33041;&#20449;&#21495;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20869;&#37096;&#34920;&#31034;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#32467;&#26524;&#34987;&#35748;&#20026;&#21453;&#26144;&#20102;LMs&#21644;&#20154;&#31867;&#22823;&#33041;&#20043;&#38388;&#30340;&#20849;&#20139;&#35745;&#31639;&#21407;&#29702;&#65292;&#20294;LMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#34920;&#31034;&#21644;&#20351;&#29992;&#19978;&#20063;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;LM&#34920;&#31034;&#21644;&#20154;&#31867;&#22823;&#33041;&#23545;&#35821;&#35328;&#30340;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;&#21463;&#35797;&#32773;&#38405;&#35835;&#21644;&#21548;&#21465;&#36848;&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#39046;&#22495;&#65292;&#21363;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#65292;&#36825;&#20123;&#39046;&#22495;&#22312;LMs&#20013;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#39046;&#22495;&#65292;&#24182;&#35777;&#26126;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve th
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.06233</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;: &#19968;&#31181;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27745;&#26579;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06233
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#24182;&#20272;&#35745;&#20854;&#25968;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#35270;&#20026;&#19968;&#31995;&#21015;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#27979;&#39564;&#24418;&#24335;&#65292;&#20854;&#20013;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#19977;&#20010;&#25200;&#21160;&#29256;&#26412;&#12290;&#36825;&#20123;&#21464;&#21270;&#20165;&#21253;&#25324;&#35789;&#32423;&#25200;&#21160;&#12290;&#29983;&#25104;&#30340;&#25200;&#21160;&#29256;&#26412;&#19982;&#21407;&#22987;&#23454;&#20363;&#19968;&#36215;&#24418;&#25104;DCQ&#20013;&#30340;&#36873;&#39033;&#65292;&#39069;&#22806;&#30340;&#36873;&#39033;&#36866;&#24212;&#20102;&#25552;&#20379;&#30340;&#36873;&#25321;&#37117;&#19981;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#37492;&#20110;&#22312;&#36873;&#25321;&#20043;&#38388;&#21807;&#19968;&#30340;&#21306;&#21035;&#20449;&#21495;&#26159;&#19982;&#21407;&#22987;&#23454;&#20363;&#30340;&#30830;&#20999;&#25514;&#36766;&#30456;&#20851;&#65292;&#22914;&#26524;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24050;&#32463;&#25509;&#35302;&#21040;&#21407;&#22987;&#23454;&#20363;&#65292;&#35821;&#35328;&#27169;&#22411;&#24403;&#34987;&#35201;&#27714;&#20174;&#36873;&#39033;&#20013;&#35782;&#21035;&#21407;&#22987;&#23454;&#20363;&#26102;&#65292;&#20542;&#21521;&#20110;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;--&#36825;&#26159;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#30340;&#29305;&#24615;&#12290;&#22312;&#20351;&#29992;GPT-4/3.5&#36827;&#34892;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23436;&#20840;&#32570;&#23569;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
&lt;/p&gt;</description></item><item><title>PowerFlowNet &#26159;&#19968;&#31181;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#31616;&#21333;&#30340;&#31995;&#32479;&#20013;&#36895;&#24230;&#25552;&#39640;&#20102;4&#20493;&#65292;&#22312;&#23454;&#38469;&#30340;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;&#20013;&#25552;&#39640;&#20102;145&#20493;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.03415</link><description>&lt;p&gt;
PowerFlowNet: &#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03415
&lt;/p&gt;
&lt;p&gt;
PowerFlowNet &#26159;&#19968;&#31181;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#31616;&#21333;&#30340;&#31995;&#32479;&#20013;&#36895;&#24230;&#25552;&#39640;&#20102;4&#20493;&#65292;&#22312;&#23454;&#38469;&#30340;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;&#20013;&#25552;&#39640;&#20102;145&#20493;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#21151;&#29575;&#27969;&#20998;&#26512;&#23545;&#20110;&#29616;&#20195;&#30005;&#21147;&#32593;&#32476;&#30340;&#36816;&#34892;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33021;&#22815;&#20026;&#23567;&#22411;&#21644;&#22823;&#22411;&#30005;&#21147;&#32593;&#32476;&#25552;&#20379;&#20934;&#30830;&#21644;&#24555;&#36895;&#35299;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#12290;&#30001;&#20110;&#30005;&#21147;&#32593;&#32476;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#19968;&#20010;&#22270;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#25104;&#20026;&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#25913;&#21892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PowerFlowNet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#21151;&#29575;&#27969;&#36817;&#20284;&#65292;&#22312;&#31616;&#21333;&#30340;IEEE 14&#24635;&#32447;&#31995;&#32479;&#20013;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#23637;&#31034;&#20102;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;(6470rte)&#30340;&#30495;&#23454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;4&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#19982;&#20854;&#20182;&#20256;&#32479;&#30340;&#36817;&#20284;&#26041;&#27861;(&#22914;&#30452;&#27969;&#26494;&#24347;&#27861;)&#30456;&#27604;&#65292;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#23427;&#20204;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and efficient power flow (PF) analysis is crucial in modern electrical networks' operation and planning. Therefore, there is a need for scalable algorithms that can provide accurate and fast solutions for both small and large scale power networks. As the power network can be interpreted as a graph, Graph Neural Networks (GNNs) have emerged as a promising approach for improving the accuracy and speed of PF approximations by exploiting information sharing via the underlying graph structure. In this study, we introduce PowerFlowNet, a novel GNN architecture for PF approximation that showcases similar performance with the traditional Newton-Raphson method but achieves it 4 times faster in the simple IEEE 14-bus system and 145 times faster in the realistic case of the French high voltage network (6470rte). Meanwhile, it significantly outperforms other traditional approximation methods, such as the DC relaxation method, in terms of performance and execution time; therefore, making P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#31034;&#20363;&#19982;&#19968;&#38454;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#25972;&#21512;&#21040;&#26680;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#36716;&#21270;&#20026;&#36830;&#32493;&#23454;&#29616;&#30340;&#24418;&#24335;&#65292;&#26377;&#25928;&#22788;&#29702;&#22522;&#20110;&#26680;&#30340;&#35859;&#35789;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2311.03340</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#30340;&#22810;&#20219;&#21153;&#26680;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multitask Kernel-based Learning with First-Order Logic Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#31034;&#20363;&#19982;&#19968;&#38454;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#25972;&#21512;&#21040;&#26680;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#36716;&#21270;&#20026;&#36830;&#32493;&#23454;&#29616;&#30340;&#24418;&#24335;&#65292;&#26377;&#25928;&#22788;&#29702;&#22522;&#20110;&#26680;&#30340;&#35859;&#35789;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#30001;&#19968;&#31995;&#21015;&#19968;&#38454;&#36923;&#36753;&#23376;&#21477;&#34920;&#36798;&#30340;&#32972;&#26223;&#30693;&#35782;&#19982;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#31034;&#20363;&#25972;&#21512;&#21040;&#26680;&#26426;&#22120;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#65292;&#22312;&#35813;&#26041;&#26696;&#20013;&#65292;&#23450;&#20041;&#22312;&#19968;&#32452;&#23545;&#35937;&#19978;&#30340;&#22810;&#20010;&#35859;&#35789;&#38656;&#35201;&#20174;&#31034;&#20363;&#20013;&#20849;&#21516;&#23398;&#20064;&#65292;&#24182;&#23545;&#20854;&#20540;&#30340;&#21512;&#27861;&#37197;&#32622;&#26045;&#21152;&#19968;&#31995;&#21015;&#30340;FOL&#32422;&#26463;&#12290;&#36825;&#20123;&#35859;&#35789;&#26159;&#23450;&#20041;&#22312;&#36755;&#20837;&#23545;&#35937;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#26159;&#24050;&#30693;&#30340;&#20107;&#20808;&#23450;&#20041;&#22909;&#30340;&#65292;&#20063;&#21487;&#20197;&#30001;&#36866;&#24403;&#30340;&#22522;&#20110;&#26680;&#30340;&#23398;&#20064;&#22120;&#36827;&#34892;&#36817;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#23558;FOL&#23376;&#21477;&#36716;&#21270;&#20026;&#19968;&#20010;&#36830;&#32493;&#23454;&#29616;&#65292;&#33021;&#22815;&#22788;&#29702;&#30001;&#22522;&#20110;&#26680;&#30340;&#35859;&#35789;&#35745;&#31639;&#20986;&#30340;&#36755;&#20986;&#12290;&#35813;&#23398;&#20064;&#38382;&#39064;&#34987;&#35270;&#20026;&#21322;&#30417;&#30563;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#21407;&#23646;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#21253;&#25324;&#23545;&#26377;&#30417;&#30563;&#31034;&#20363;&#30340;&#25311;&#21512;&#25439;&#22833;&#24230;&#37327;&#12289;&#27491;&#21017;&#21270;&#39033;&#21644;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines. In particular, we consider a multi-task learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples, enforcing a set of FOL constraints on the admissible configurations of their values. The predicates are defined on the feature spaces, in which the input objects are represented, and can be either known a priori or approximated by an appropriate kernel-based learner. A general approach is presented to convert the FOL clauses into a continuous implementation that can deal with the outputs computed by the kernel-based predicates. The learning problem is formulated as a semi-supervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples, a regularization term, and a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#26469;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#36827;&#34892;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.20689</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#38169;&#35823;&#20013;&#20351;LLM&#25104;&#20026;&#26356;&#22909;&#30340;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Learning From Mistakes Makes LLM Better Reasoner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#26469;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#36827;&#34892;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#26159;&#21542;&#21487;&#20197;&#23398;&#20064;&#20174;&#38169;&#35823;&#20013;&#33719;&#30410;&#65288;LEMA&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#32771;&#34385;&#19968;&#20010;&#26410;&#33021;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#20154;&#31867;&#23398;&#29983;&#65292;&#20182;&#20250;&#20174;&#33258;&#24049;&#29359;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#24182;&#32416;&#27491;&#23427;&#12290;&#27169;&#20223;&#36825;&#31181;&#38169;&#35823;&#39537;&#21160;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;LEMA&#22312;LLM&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#26469;&#33258;&#21508;&#31181;LLM&#30340;&#38169;&#35823;&#25512;&#29702;&#36335;&#24452;&#65292;&#28982;&#21518;&#20351;&#29992;GPT-4&#20316;&#20026;&#8220;&#32416;&#27491;&#32773;&#8221;&#26469;&#35782;&#21035;&#38169;&#35823;&#27493;&#39588;&#65292;&#35299;&#37322;&#38169;&#35823;&#21407;&#22240;&#65292;&#32416;&#27491;&#38169;&#35823;&#24182;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#32416;&#27491;&#30340;&#36827;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#29983;&#25104;&#32416;&#27491;&#25968;&#25454;&#30340;&#38382;&#39064;&#38598;&#12290;&#22312;&#21508;&#31181;LLM&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LEMA&#22987;&#32456;&#21487;&#20197;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a "corrector" to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that \textsc{LeMa} consistently improves CoT-alone fine-tuning. Our fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20195;&#29702;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#20195;&#29702;&#30340;&#24433;&#21709;&#24402;&#22240;&#21040;&#22240;&#26524;&#25928;&#24212;&#19978;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#20195;&#29702;&#29305;&#23450;&#25928;&#24212;&#65288;ASE&#65289;&#30340;&#26032;&#30340;&#22240;&#26524;&#25968;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;ASE&#30340;&#21453;&#20107;&#23454;&#23545;&#24212;&#29289;&#65288;cf-ASE&#65289;&#20197;&#21450;&#35782;&#21035;cf-ASE&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.11334</link><description>&lt;p&gt;
&#20195;&#29702;&#29305;&#23450;&#25928;&#24212;&#65306;&#22810;&#26234;&#33021;&#20307;MDPs&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#20256;&#25773;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Agent-Specific Effects: A Causal Effect Propagation Analysis in Multi-Agent MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.11334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20195;&#29702;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#20195;&#29702;&#30340;&#24433;&#21709;&#24402;&#22240;&#21040;&#22240;&#26524;&#25928;&#24212;&#19978;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#20195;&#29702;&#29305;&#23450;&#25928;&#24212;&#65288;ASE&#65289;&#30340;&#26032;&#30340;&#22240;&#26524;&#25968;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;ASE&#30340;&#21453;&#20107;&#23454;&#23545;&#24212;&#29289;&#65288;cf-ASE&#65289;&#20197;&#21450;&#35782;&#21035;cf-ASE&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#24314;&#31435;&#34892;&#21160;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#23545;&#20110;&#36127;&#26377;&#36131;&#20219;&#30340;&#20915;&#31574;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#21644;&#37327;&#21270;&#20195;&#29702;&#23545;&#36825;&#31181;&#20851;&#31995;&#30340;&#36129;&#29486;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#24207;&#36143;&#20915;&#31574;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#20123;&#25361;&#25112;&#23588;&#20026;&#31361;&#20986;&#65292;&#22240;&#20026;&#20195;&#29702;&#30340;&#34892;&#21160;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#21462;&#20915;&#20110;&#20854;&#20182;&#20195;&#29702;&#22914;&#20309;&#23545;&#35813;&#34892;&#21160;&#20316;&#20986;&#21709;&#24212;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#20195;&#29702;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#20195;&#29702;&#20135;&#29983;&#30340;&#22240;&#26524;&#25928;&#24212;&#24402;&#22240;&#21040;&#20854;&#25152;&#26045;&#21152;&#30340;&#24433;&#21709;&#19978;&#12290;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25968;&#37327;&#8212;&#8212;&#20195;&#29702;&#29305;&#23450;&#25928;&#24212;&#65288;ASE&#65289;&#65292;&#29992;&#20110;&#34913;&#37327;&#20195;&#29702;&#34892;&#21160;&#23545;&#36890;&#36807;&#20854;&#20182;&#20195;&#29702;&#20256;&#25773;&#30340;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36716;&#21521;ASE&#30340;&#21453;&#20107;&#23454;&#23545;&#24212;&#29289;&#65288;cf-ASE&#65289;&#65292;&#25552;&#20379;&#20102;&#35782;&#21035;cf-ASE&#30340;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Establishing causal relationships between actions and outcomes is fundamental for accountable multi-agent decision-making. However, interpreting and quantifying agents' contributions to such relationships pose significant challenges. These challenges are particularly prominent in the context of multi-agent sequential decision-making, where the causal effect of an agent's action on the outcome depends on how other agents respond to that action. In this paper, our objective is to present a systematic approach for attributing the causal effects of agents' actions to the influence they exert on other agents. Focusing on multi-agent Markov decision processes, we introduce agent-specific effects (ASE), a novel causal quantity that measures the effect of an agent's action on the outcome that propagates through other agents. We then turn to the counterfactual counterpart of ASE (cf-ASE), provide a sufficient set of conditions for identifying cf-ASE, and propose a practical sampling-based algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#39640;&#38454;&#32593;&#32476;&#21516;&#36074;&#24615;&#30340;&#27010;&#24565;&#21270;&#65292;&#24182;&#25506;&#32034;&#20102;&#19968;&#20123;&#22788;&#29702;&#39640;&#38454;&#32467;&#26500;&#30340;&#31574;&#30053;&#65292;&#20026;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#37326;&#21644;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.07684</link><description>&lt;p&gt;
&#36879;&#36807;&#28040;&#24687;&#20256;&#36882;&#30340;&#35270;&#35282;&#30475;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#21516;&#36074;&#24615;&#19982;&#26550;&#26500;&#35774;&#35745;&#30340;&#20849;&#21516;&#35270;&#37326;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#39640;&#38454;&#32593;&#32476;&#21516;&#36074;&#24615;&#30340;&#27010;&#24565;&#21270;&#65292;&#24182;&#25506;&#32034;&#20102;&#19968;&#20123;&#22788;&#29702;&#39640;&#38454;&#32467;&#26500;&#30340;&#31574;&#30053;&#65292;&#20026;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#37326;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22823;&#37096;&#20998;&#30340;&#36229;&#22270;&#23398;&#20064;&#26041;&#27861;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#37117;&#26159;&#36890;&#36807;&#20174;&#22270;&#30340;&#31867;&#27604;&#20013;&#25552;&#21319;&#36807;&#26469;&#30340;&#65292;&#24573;&#30053;&#20102;&#36229;&#22270;&#30340;&#29305;&#27530;&#24615;&#12290;&#26412;&#25991;&#23581;&#35797;&#35299;&#20915;&#19968;&#20123;&#30456;&#20851;&#30340;&#38382;&#39064;&#65306;Q1 &#21516;&#36074;&#24615;&#22312;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26159;&#21542;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65311;Q2 &#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#32454;&#33268;&#22788;&#29702;&#39640;&#38454;&#32593;&#32476;&#30340;&#29305;&#24449;&#26469;&#25913;&#21892;&#24403;&#21069;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65311;Q3 &#29616;&#26377;&#25968;&#25454;&#38598;&#26159;&#21542;&#23545;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#22522;&#20934;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#39640;&#38454;&#32593;&#32476;&#21516;&#36074;&#24615;&#30340;&#26032;&#27010;&#24565;&#21270;&#65292;&#32479;&#19968;&#20102;&#39640;&#38454;&#32593;&#32476;&#30340;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22788;&#29702;&#39640;&#38454;&#32467;&#26500;&#30340;&#19968;&#20123;&#33258;&#28982;&#20294;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#30340;&#31574;&#30053;&#65292;&#27604;&#22914;&#20445;&#30041;&#36229;&#36793;&#20381;&#36182;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#25110;&#26159;&#20197;&#33410;&#28857;&#21644;&#36229;&#36793;&#20849;&#21516;&#32534;&#30721;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the current hypergraph learning methodologies and benchmarking datasets in the hypergraph realm are obtained by lifting procedures from their graph analogs, leading to overshadowing specific characteristics of hypergraphs. This paper attempts to confront some pending questions in that regard: Q1 Can the concept of homophily play a crucial role in Hypergraph Neural Networks (HNNs)? Q2 Is there room for improving current HNN architectures by carefully addressing specific characteristics of higher-order networks? Q3 Do existing datasets provide a meaningful benchmark for HNNs? To address them, we first introduce a novel conceptualization of homophily in higher-order networks based on a Message Passing (MP) scheme, unifying both the analytical examination and the modeling of higher-order networks. Further, we investigate some natural, yet mostly unexplored, strategies for processing higher-order structures within HNNs such as keeping hyperedge-dependent node representations, or per
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#32780;&#22686;&#21152;&#30340;&#35757;&#32451;&#21442;&#25968;&#24456;&#23569;&#12290;</title><link>https://arxiv.org/abs/2310.05707</link><description>&lt;p&gt;
&#29992;&#35268;&#21010;&#26631;&#35760;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Guiding Language Model Math Reasoning with Planning Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#32780;&#22686;&#21152;&#30340;&#35757;&#32451;&#21442;&#25968;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26469;&#22240;&#20854;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#22914;&#24605;&#32500;&#38142;&#25512;&#29702;&#65289;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22686;&#24378;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24573;&#35270;&#20102;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#32467;&#26500;&#21270;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;LLMs&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#20010;&#21035;&#25512;&#29702;&#27493;&#39588;&#65292;&#20294;&#22312;&#25972;&#20010;&#25512;&#29702;&#38142;&#19978;&#20445;&#25345;&#19968;&#33268;&#24615;&#26041;&#38754;&#21364;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#24320;&#22987;&#22788;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#65292;&#20316;&#20026;&#27169;&#22411;&#30340;&#24341;&#23548;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#23884;&#20837;&#28155;&#21152;&#21040;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#22686;&#21152;&#38750;&#24120;&#23567;&#65288;&#20165;&#20026;0.001%&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#23436;&#20840;&#24494;&#35843;&#25110;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#26041;&#26696;&#26469;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;LLMs&#65292;&#22312;&#19977;&#20010;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard f
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2310.05212</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#32593;&#32476;&#20195;&#34920;&#24847;&#35782;&#30340;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Interpretable Semiotics Networks Representing Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27599;&#22825;&#37117;&#24863;&#30693;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#28192;&#36947;&#20256;&#36798;&#20182;&#20204;&#30340;&#24863;&#30693;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#30340;&#24863;&#30693;&#20197;&#21450;&#23427;&#20204;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#20869;&#37096;&#34920;&#31034;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65288;"&#35266;&#23519;&#21040;&#30340;"&#21644;"&#30475;&#21040;&#30340;"&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29087;&#24713;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27010;&#24565;&#65288;&#32534;&#30721;&#21644;&#35299;&#30721;&#65289;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#20803;&#32032;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#24418;&#25104;&#31526;&#21495;&#32593;&#32476;&#65292;&#27169;&#25311;&#20102;&#29289;&#20307;&#24863;&#30693;&#21644;&#20154;&#31867;&#20132;&#27969;&#20013;&#30340;&#24847;&#35782;&#12290;&#22914;&#20170;&#65292;&#22823;&#22810;&#25968;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#35265;&#24615;&#12290;&#25105;&#20204;&#20154;&#30340;&#29289;&#20307;&#24863;&#30693;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32593;&#32476;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#25324;&#22522;&#20934;&#20998;&#31867;&#22120;&#21644;&#39069;&#22806;&#23618;&#30340;&#26032;&#32593;&#32476;&#26469;&#28436;&#31034;&#36825;&#19968;&#28857;&#12290;&#36825;&#20010;&#23618;&#20135;&#29983;&#20102;&#22270;&#20687;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans perceive objects daily and communicate their perceptions using various channels. Here, we describe a computational model that tracks and simulates objects' perception and their representations as they are conveyed in communication.   We describe two key components of our internal representation ("observed" and "seen") and relate them to familiar computer vision notions (encoding and decoding). These elements are joined together to form semiotics networks, which simulate awareness in object perception and human communication.   Nowadays, most neural networks are uninterpretable. On the other hand, our model overcomes this limitation. The experiments demonstrates the visibility of the model.   Our model of object perception by a person allows us to define object perception by a network. We demonstrate this with an example of an image baseline classifier by constructing a new network that includes the baseline classifier and an additional layer. This layer produces the images "perc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32763;&#35793;&#36807;&#31243;&#20998;&#35299;&#20026;&#23567;&#27493;&#39588;&#24182;&#24341;&#20837;&#20803;&#31574;&#30053;&#21644;Plan&#27010;&#24565;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2309.13672</link><description>&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Image-to-Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32763;&#35793;&#36807;&#31243;&#20998;&#35299;&#20026;&#23567;&#27493;&#39588;&#24182;&#24341;&#20837;&#20803;&#31574;&#30053;&#21644;Plan&#27010;&#24565;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#27425;&#36816;&#34892;&#29983;&#25104;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#26679;&#30340;&#21333;&#27493;&#27169;&#22411;&#22987;&#32456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#24182;&#23481;&#26131;&#38519;&#20837;&#22351;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#37325;&#26032;&#23450;&#20041;&#20026;&#36880;&#27493;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36827;&#34892;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#65288;RL-I2IT&#65289;&#12290;RL-I2IT&#26694;&#26550;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#23558;&#19968;&#20010;&#21333;&#20307;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#23567;&#30340;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#36880;&#27493;&#23558;&#28304;&#22270;&#20687;&#36716;&#21270;&#20026;&#30446;&#26631;&#22270;&#20687;&#12290;&#32771;&#34385;&#21040;&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#31574;&#30053;&#21644;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;Plan&#21040;&#26631;&#20934;&#30340;Actor-Critic&#27169;&#22411;&#20013;&#65292;&#35813;&#27010;&#24565;&#30340;&#32500;&#24230;&#36739;&#21407;&#22987;&#22270;&#20687;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#24110;&#21161;&#28436;&#21592;&#29983;&#25104;&#21487;&#22788;&#29702;&#30340;&#39640;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing Image-to-Image Translation (I2IT) methods generate images in a single run of a deep learning (DL) model. However, designing such a single-step model is always challenging, requiring a huge number of parameters and easily falling into bad global minimums and overfitting. In this work, we reformulate I2IT as a step-wise decision-making problem via deep reinforcement learning (DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The key feature in the RL-I2IT framework is to decompose a monolithic learning process into small steps with a lightweight model to progressively transform a source image successively to a target image. Considering that it is challenging to handle high dimensional continuous state and action spaces in the conventional RL framework, we introduce meta policy with a new concept Plan to the standard Actor-Critic model, which is of a lower dimension than the original image and can facilitate the actor to generate a tractable high dime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;DiffusionWorldViewer&#65292;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#26088;&#22312;&#25581;&#31034;&#21644;&#25299;&#23485;&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#19990;&#30028;&#35266;&#12290;&#36890;&#36807;&#22312;&#36755;&#20986;&#30340;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20043;&#38388;&#25581;&#31034;&#19990;&#30028;&#35266;&#65292;&#24182;&#25552;&#20379;&#32534;&#36753;&#24037;&#20855;&#65292;&#24110;&#21161;&#29992;&#25143;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#20195;&#34920;&#20182;&#20204;&#22810;&#26679;&#21270;&#30340;&#35266;&#28857;&#65292;&#24182;&#25361;&#25112;&#24403;&#21069;&#27169;&#22411;&#20013;&#21453;&#26144;&#30340;&#26377;&#38480;&#19990;&#30028;&#35266;&#12290;</title><link>https://arxiv.org/abs/2309.09944</link><description>&lt;p&gt;
DiffusionWorldViewer&#65306;&#25581;&#31034;&#21644;&#25299;&#23485;&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21453;&#26144;&#30340;&#19990;&#30028;&#35266;
&lt;/p&gt;
&lt;p&gt;
DiffusionWorldViewer: Exposing and Broadening the Worldview Reflected by Generative Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;DiffusionWorldViewer&#65292;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#26088;&#22312;&#25581;&#31034;&#21644;&#25299;&#23485;&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#19990;&#30028;&#35266;&#12290;&#36890;&#36807;&#22312;&#36755;&#20986;&#30340;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20043;&#38388;&#25581;&#31034;&#19990;&#30028;&#35266;&#65292;&#24182;&#25552;&#20379;&#32534;&#36753;&#24037;&#20855;&#65292;&#24110;&#21161;&#29992;&#25143;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#20195;&#34920;&#20182;&#20204;&#22810;&#26679;&#21270;&#30340;&#35266;&#28857;&#65292;&#24182;&#25361;&#25112;&#24403;&#21069;&#27169;&#22411;&#20013;&#21453;&#26144;&#30340;&#26377;&#38480;&#19990;&#30028;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#22312;&#23398;&#26415;&#21644;&#21019;&#24847;&#39046;&#22495;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#19982;&#20154;&#31867;&#19968;&#26679;&#65292;TTI&#27169;&#22411;&#26377;&#19968;&#20010;&#19990;&#30028;&#35266;&#65292;&#21363;&#20174;&#35757;&#32451;&#25968;&#25454;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#30340;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#65292;&#36825;&#20250;&#24433;&#21709;&#23427;&#20204;&#20026;&#32473;&#23450;&#25552;&#31034;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;TTI&#27169;&#22411;&#30340;&#19990;&#30028;&#35266;&#36890;&#24120;&#23545;&#29992;&#25143;&#38544;&#34255;&#65292;&#36825;&#20351;&#29992;&#25143;&#38590;&#20197;&#24314;&#31435;&#23545;TTI&#36755;&#20986;&#30340;&#30452;&#35273;&#65292;&#24182;&#19988;&#23427;&#20204;&#36890;&#24120;&#19982;&#29992;&#25143;&#30340;&#19990;&#30028;&#35266;&#19981;&#19968;&#33268;&#65292;&#23548;&#33268;&#36755;&#20986;&#30340;&#22270;&#20687;&#19981;&#31526;&#21512;&#29992;&#25143;&#30340;&#26399;&#26395;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffusionWorldViewer&#65292;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#21487;&#22312;&#36755;&#20986;&#30340;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20043;&#38388;&#25581;&#31034;TTI&#27169;&#22411;&#30340;&#19990;&#30028;&#35266;&#65292;&#24182;&#25552;&#20379;&#32534;&#36753;&#24037;&#20855;&#20197;&#20351;&#36755;&#20986;&#22270;&#20687;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#19968;&#33268;&#12290;&#22312;&#23545;18&#20301;&#22810;&#26679;&#21270;TTI&#29992;&#25143;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;DiffusionWorldViewer&#24110;&#21161;&#29992;&#25143;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#20195;&#34920;&#20182;&#20204;&#22810;&#26679;&#21270;&#30340;&#35266;&#28857;&#65292;&#24182;&#25361;&#25112;&#24403;&#21069;TTI&#27169;&#22411;&#20013;&#21453;&#26144;&#30340;&#26377;&#38480;&#19990;&#30028;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative text-to-image (TTI) models produce high-quality images from short textual descriptions and are widely used in academic and creative domains. Like humans, TTI models have a worldview, a conception of the world learned from their training data and task that influences the images they generate for a given prompt. However, the worldviews of TTI models are often hidden from users, making it challenging for users to build intuition about TTI outputs, and they are often misaligned with users' worldviews, resulting in output images that do not match user expectations. In response, we introduce DiffusionWorldViewer, an interactive interface that exposes a TTI model's worldview across output demographics and provides editing tools for aligning output images with user perspectives. In a user study with 18 diverse TTI users, we find that DiffusionWorldViewer helps users represent their varied viewpoints in generated images and challenge the limited worldview reflected in current TTI mod
&lt;/p&gt;</description></item><item><title>Fabricator&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#24102;&#26377;Teacher LLMs&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#12290;&#23427;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;LLM&#26681;&#25454;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#26631;&#27880;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19979;&#19968;&#38454;&#27573;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2309.09582</link><description>&lt;p&gt;
Fabricator: &#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#24102;&#26377;Teacher LLMs&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#30340;&#24320;&#28304;&#24037;&#20855;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09582
&lt;/p&gt;
&lt;p&gt;
Fabricator&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#24102;&#26377;Teacher LLMs&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#12290;&#23427;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;LLM&#26681;&#25454;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#26631;&#27880;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19979;&#19968;&#38454;&#27573;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#26469;&#24314;&#27169;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#29983;&#25104;&#36275;&#22815;&#36136;&#37327;&#21644;&#25968;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#19968;&#31181;&#31216;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#65292;&#35813;&#33539;&#24335;&#36890;&#36807;&#25968;&#25454;&#38598;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#27169;&#24335;&#19979;&#65292;&#19968;&#20010;&#24378;&#22823;&#30340;LLM&#26681;&#25454;&#32473;&#23450;&#30340;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#26631;&#27880;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19979;&#19968;&#38454;&#27573;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#21487;&#20197;&#35753;LLM&#29983;&#25104;500&#20010;&#31215;&#26497;&#24773;&#32490;&#30340;&#30005;&#24433;&#35780;&#35770;&#21644;&#21478;&#22806;500&#20010;&#28040;&#26497;&#24773;&#32490;&#30340;&#35780;&#35770;&#12290;&#28982;&#21518;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#21033;&#29992;LLM&#20316;&#20026;&#36739;&#23567;&#35268;&#27169;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#25945;&#24072;&#12290;&#36890;&#36807;&#36825;&#20010;&#28436;&#31034;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Fabricator&#65292;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#24320;&#28304;Python&#24037;&#20855;&#38598;&#12290;Fabricator&#23454;&#29616;&#20102;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model. For instance, an LLM might be prompted to "generate 500 movie reviews with positive overall sentiment, and another 500 with negative sentiment." The generated data could then be used to train a binary sentiment classifier, effectively leveraging an LLM as a teacher to a smaller student model. With this demo, we introduce Fabricator, an open-source Python toolkit for dataset generation. Fabricator implements common dataset generation workflows, supports a wide range of downstream NLP tasks (such as text classi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#31639;&#27861;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36817;&#20284;O(T^3/4)&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>https://arxiv.org/abs/2309.01922</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#31639;&#27861;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36817;&#20284;O(T^3/4)&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#30456;&#20851;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36890;&#29992;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35299;&#25918;&#20102;&#23427;&#22312;&#20551;&#35774;&#32447;&#24615;MDP&#32467;&#26500;&#30340;&#38480;&#21046;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20840;&#23616;&#25910;&#25947;&#24615;&#36136;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#20855;&#26377;&#36817;&#20284;O(T^3/4)&#30340;&#36951;&#25022;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#24179;&#22343;&#22870;&#21169;&#22330;&#26223;&#19979;&#65292;&#23545;&#20110;&#36890;&#29992;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider an infinite horizon average reward Markov Decision Process (MDP). Distinguishing itself from existing works within this context, our approach harnesses the power of the general policy gradient-based algorithm, liberating it from the constraints of assuming a linear MDP structure. We propose a policy gradient-based algorithm and show its global convergence property. We then prove that the proposed algorithm has $\tilde{\mathcal{O}}({T}^{3/4})$ regret. Remarkably, this paper marks a pioneering effort by presenting the first exploration into regret-bound computation for the general parameterized policy gradient algorithm in the context of average reward scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#33976;&#39311;GPT&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#23558;&#20195;&#30721;&#21457;&#36865;&#32473;&#19981;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#36827;&#34892;&#22788;&#29702;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.14731</link><description>&lt;p&gt;
&#29992;&#20110;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#33976;&#39311;GPT
&lt;/p&gt;
&lt;p&gt;
Distilled GPT for Source Code Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14731
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#33976;&#39311;GPT&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#23558;&#20195;&#30721;&#21457;&#36865;&#32473;&#19981;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#36827;&#34892;&#22788;&#29702;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25688;&#35201;&#26159;&#28304;&#20195;&#30721;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#25688;&#35201;&#36890;&#24120;&#21482;&#26377;&#19968;&#21477;&#35805;&#65292;&#20294;&#21364;&#26500;&#25104;&#20102;&#24320;&#21457;&#32773;&#25991;&#26723;&#30340;&#26680;&#24515;&#12290;&#20363;&#22914;&#65292;&#8220;&#23558;&#25152;&#26377;&#21487;&#35265;&#30340;&#22810;&#36793;&#24418;&#21464;&#20026;&#34013;&#33394;&#8221;&#36825;&#26679;&#30340;&#31616;&#30701;&#25551;&#36848;&#21487;&#20197;&#35753;&#31243;&#24207;&#21592;&#23545;&#20195;&#30721;&#30340;&#21151;&#33021;&#26377;&#19968;&#20010;&#39640;&#23618;&#27425;&#30340;&#20102;&#35299;&#65292;&#32780;&#26080;&#38656;&#38405;&#35835;&#20195;&#30721;&#26412;&#36523;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#30340;&#20135;&#21697;&#24050;&#32463;&#23637;&#31034;&#20102;&#33258;&#21160;&#32534;&#20889;&#36825;&#20123;&#25551;&#36848;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#65292;&#31243;&#24207;&#21592;&#24517;&#39035;&#23558;&#20182;&#20204;&#30340;&#20195;&#30721;&#21457;&#36865;&#32473;&#19981;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#36827;&#34892;&#22788;&#29702;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;API&#35843;&#29992;&#65289;&#12290;&#36825;&#31181;&#22833;&#21435;&#25511;&#21046;&#26435;&#30340;&#24773;&#20917;&#23545;&#35768;&#22810;&#32452;&#32455;&#26469;&#35828;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65306;&#25105;&#20204;&#20351;&#29992;&#30001;GPT-3.5&#29983;&#25104;&#30340;&#26679;&#26412;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#65292;&#36825;&#20010;&#36807;&#31243;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36275;&#22815;&#23567;&#65288;350m&#21442;&#25968;&#65289;&#20197;&#22312;&#21333;&#20010;16GB GPU&#19978;&#36816;&#34892;&#65292;&#20294;&#25105;&#20204;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#23427;&#36275;&#22815;&#22823;&#20197;&#27169;&#20223;GPT-3&#12290;
&lt;/p&gt;
&lt;p&gt;
A code summary is a brief natural language description of source code. Summaries are usually only a single sentence long, and yet form the backbone of developer documentation. A short descriptions such as "changes all visible polygons to the color blue" can give a programmer a high-level idea of what code does without the effort of reading the code itself. Recently, products based on Large Language Models such as ChatGPT have demonstrated a strong ability to write these descriptions automatically. However, to use these tools, programmers must send their code to untrusted third parties for processing (e.g., via an API call). This loss of custody is not acceptable to many organizations. In this paper, we present an alternative: we train an open source model using sample output generated by GPT-3.5 in a process related to knowledge distillation. Our model is small enough (350m parameters) to be run on a single 16gb GPU, yet we show in our evaluation that it is large enough to mimic GPT-3.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#23618;&#27425;&#21644;&#25490;&#21517;&#36807;&#31243;(IHRP)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23494;&#38598;&#36830;&#25509;&#31995;&#32479;&#20013;&#26500;&#24314;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#30452;&#35273;&#27169;&#31946;&#35821;&#35328;&#23398;&#26041;&#27861;&#35745;&#31639;&#22240;&#32032;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23398;&#29983;&#34920;&#29616;&#35780;&#20272;&#31561;&#20915;&#31574;&#38382;&#39064;&#20013;&#20855;&#26377;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2306.10409</link><description>&lt;p&gt;
&#36845;&#20195;&#23618;&#27425;&#21644;&#25490;&#21517;&#36807;&#31243;&#65288;IHRP&#65289;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#23494;&#38598;&#36830;&#25509;&#31995;&#32479;&#30340;&#23618;&#27425;&#24314;&#27169;&#26041;&#27861;&#21450;&#22312;&#23398;&#29983;&#34920;&#29616;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Iterative Hierarchy and Ranking Process (IHRP): A Novel Effective Hierarchy Method for Densely Connected Systems and Case Study in Student Performance Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#23618;&#27425;&#21644;&#25490;&#21517;&#36807;&#31243;(IHRP)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23494;&#38598;&#36830;&#25509;&#31995;&#32479;&#20013;&#26500;&#24314;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#30452;&#35273;&#27169;&#31946;&#35821;&#35328;&#23398;&#26041;&#27861;&#35745;&#31639;&#22240;&#32032;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23398;&#29983;&#34920;&#29616;&#35780;&#20272;&#31561;&#20915;&#31574;&#38382;&#39064;&#20013;&#20855;&#26377;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#30830;&#23450;&#22240;&#32032;&#23545;&#20915;&#31574;&#23646;&#24615;&#30340;&#24433;&#21709;&#26159;&#20854;&#20013;&#20027;&#35201;&#20219;&#21153;&#20043;&#19968;&#12290;&#20026;&#20102;&#23545;&#20915;&#31574;&#23646;&#24615;&#20135;&#29983;&#26368;&#22823;&#24433;&#21709;&#65292;&#25214;&#21040;&#22240;&#32032;&#20043;&#38388;&#21512;&#36866;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#22312;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#24615;&#20215;&#20540;&#23601;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#35808;&#37322;&#32467;&#26500;&#27169;&#22411;(ISM)&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#19987;&#23478;&#24847;&#35265;&#25366;&#25496;&#22240;&#32032;&#30456;&#20114;&#24433;&#21709;&#30340;&#23618;&#27425;&#26500;&#24314;&#26041;&#27861;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#24120;&#35268;ISM&#26041;&#27861;&#22312;&#22240;&#32032;&#20043;&#38388;&#23494;&#38598;&#20851;&#32852;&#30340;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#12290;&#25105;&#20204;&#31216;&#36825;&#26679;&#30340;&#31995;&#32479;&#20026;&#8220;&#23494;&#38598;&#31995;&#32479;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#23618;&#27425;&#26500;&#24314;&#25216;&#26415;&#65292;&#31216;&#20026;&#8220;&#36845;&#20195;&#23618;&#27425;&#21644;&#25490;&#21517;&#36807;&#31243;&#8221;&#65288;IHRP&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#36825;&#26679;&#30340;&#23494;&#38598;&#31995;&#32479;&#20013;&#30340;&#25928;&#26524;&#26174;&#33879;&#12290;&#20026;&#20102;&#32771;&#34385;&#19987;&#23478;&#24847;&#35265;&#30340;&#27169;&#31946;&#24615;&#65292;&#30740;&#31350;&#24037;&#20316;&#20013;&#20351;&#29992;&#20102;&#30452;&#35273;&#27169;&#31946;&#35821;&#35328;&#23398;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#23545;&#37325;&#35201;&#24615;&#30340;&#20004;&#38454;&#27573;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-life decision-making problems, determining the influences of the factors on the decision attribute is one of the primary tasks. To affect the decision attribute most, finding a proper hierarchy among the factors and determining their importance values in the system becomes quite important. Interpretive structural modeling (ISM) is a widely used hierarchy-building method that mines factor inter-influences based on expert opinions. This paper discusses one of the main drawbacks of the conventional ISM method in systems where the factors are densely interrelated. We refer to such systems as "dense systems". We propose a novel iterative hierarchy-building technique, called 'Iterative Hierarchy and Ranking Process'(IHRP) which performs effectively in such dense systems. To take the vagueness of the expert opinions into account, intuitionistic fuzzy linguistics has been used in the research work. In this paper, we propose a two-stage calculation of the relative importance of the fact
&lt;/p&gt;</description></item><item><title>LAraBench&#26159;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22810;&#31181;&#23454;&#39564;&#35774;&#32622;&#21644;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#65292;&#35777;&#26126;&#26368;&#26032;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;</title><link>https://arxiv.org/abs/2305.14982</link><description>&lt;p&gt;
LAraBench&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#35821;AI&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LAraBench: Benchmarking Arabic AI with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14982
&lt;/p&gt;
&lt;p&gt;
LAraBench&#26159;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22810;&#31181;&#23454;&#39564;&#35774;&#32622;&#21644;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#65292;&#35777;&#26126;&#26368;&#26032;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#35821;&#35328;&#21644;&#35821;&#38899;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#27493;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23578;&#32570;&#20047;&#29305;&#23450;&#35821;&#35328;&#21644;&#20219;&#21153;&#30340;&#26368;&#26032;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;LAraBench&#38024;&#23545;&#38463;&#25289;&#20271;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#36825;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#24207;&#21015;&#26631;&#27880;&#21644;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#20869;&#23481;&#20998;&#31867;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;GPT-3.5-turbo&#12289;GPT-4&#12289;BLOOMZ&#12289;Jais-13b-chat&#12289;Whisper&#21644;USM&#31561;&#27169;&#22411;&#65292;&#36816;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#24212;&#23545;&#20102;33&#20010;&#29420;&#31435;&#20219;&#21153;&#21644;61&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#28041;&#21450;98&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#21253;&#25324;&#32422;296K&#20010;&#25968;&#25454;&#28857;&#12289;&#32422;46&#23567;&#26102;&#30340;&#35821;&#38899;&#21644;30&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#30340;&#21477;&#23376;&#12290;&#36825;&#19968;&#21162;&#21147;&#20135;&#29983;&#20102;330+&#32452;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#37325;&#28857;&#26159;&#34913;&#37327;&#26368;&#26032;&#27169;&#22411;&#21644;LLMs&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#24635;&#20307;&#36235;&#21183;&#34920;&#26126;&#65292;&#26368;&#26032;&#27169;&#22411;&#19968;&#33324;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperf
&lt;/p&gt;</description></item><item><title>SFT-KD-Recon&#26159;&#19968;&#31181;&#23398;&#29983;&#21451;&#22909;&#30340;&#25945;&#24072;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#25945;&#24072;&#20102;&#35299;&#23398;&#29983;&#30340;&#32467;&#26500;&#21644;&#33021;&#21147;&#65292;&#24182;&#23558;&#25945;&#24072;&#30340;&#34920;&#31034;&#19982;&#23398;&#29983;&#30340;&#34920;&#31034;&#30456;&#19968;&#33268;&#65292;&#20197;&#23454;&#29616;MRI&#37325;&#24314;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#12290;</title><link>https://arxiv.org/abs/2304.05057</link><description>&lt;p&gt;
SFT-KD-Recon:&#23398;&#20064;MRI&#37325;&#24314;&#20013;&#23398;&#29983;&#21451;&#22909;&#30340;&#30693;&#35782;&#33976;&#39311;&#25945;&#24072;
&lt;/p&gt;
&lt;p&gt;
SFT-KD-Recon: Learning a Student-friendly Teacher for Knowledge Distillation in Magnetic Resonance Image Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.05057
&lt;/p&gt;
&lt;p&gt;
SFT-KD-Recon&#26159;&#19968;&#31181;&#23398;&#29983;&#21451;&#22909;&#30340;&#25945;&#24072;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#25945;&#24072;&#20102;&#35299;&#23398;&#29983;&#30340;&#32467;&#26500;&#21644;&#33021;&#21147;&#65292;&#24182;&#23558;&#25945;&#24072;&#30340;&#34920;&#31034;&#19982;&#23398;&#29983;&#30340;&#34920;&#31034;&#30456;&#19968;&#33268;&#65292;&#20197;&#23454;&#29616;MRI&#37325;&#24314;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#21152;&#36895;&#30340;&#28145;&#32423;&#32852;&#26550;&#26500;&#22312;&#25552;&#20379;&#39640;&#36136;&#37327;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32423;&#32852;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#37325;&#24314;&#30340;&#25913;&#36827;&#24448;&#24448;&#21464;&#24471;&#36793;&#38469;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#23384;&#22312;&#36807;&#22810;&#30340;&#27169;&#22411;&#23481;&#37327;&#12290;&#30693;&#35782;&#33976;&#39311;(KD)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#21387;&#32553;&#36825;&#20123;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#28145;&#24230;&#25945;&#24072;&#32593;&#32476;&#23558;&#30693;&#35782;&#33976;&#39311;&#32473;&#36739;&#23567;&#30340;&#23398;&#29983;&#32593;&#32476;&#65292;&#20351;&#23398;&#29983;&#23398;&#20064;&#27169;&#20223;&#25945;&#24072;&#30340;&#34892;&#20026;&#12290;&#22823;&#22810;&#25968;KD&#26041;&#27861;&#20391;&#37325;&#20110;&#26377;&#25928;&#22320;&#35757;&#32451;&#19981;&#30693;&#36947;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25945;&#24072;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SFT-KD-Recon&#65292;&#19968;&#31181;&#23398;&#29983;&#21451;&#22909;&#30340;&#25945;&#24072;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#23398;&#29983;&#20316;&#20026;KD&#30340;&#20808;&#20915;&#27493;&#39588;&#65292;&#20351;&#25945;&#24072;&#20102;&#35299;&#23398;&#29983;&#30340;&#32467;&#26500;&#21644;&#33021;&#21147;&#65292;&#24182;&#20351;&#25945;&#24072;&#30340;&#34920;&#31034;&#19982;&#23398;&#29983;&#30340;&#34920;&#31034;&#30456;&#19968;&#33268;&#12290;&#22312;SFT&#20013;&#65292;&#25945;&#24072;&#19982;&#23637;&#24320;&#30340;&#25903;&#36335;&#37197;&#32622;&#21516;&#26102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep cascaded architectures for magnetic resonance imaging (MRI) acceleration have shown remarkable success in providing high-quality reconstruction. However, as the number of cascades increases, the improvements in reconstruction tend to become marginal, indicating possible excess model capacity. Knowledge distillation (KD) is an emerging technique to compress these models, in which a trained deep teacher network is used to distill knowledge to a smaller student network such that the student learns to mimic the behavior of the teacher. Most KD methods focus on effectively training the student with a pre-trained teacher unaware of the student model. We propose SFT-KD-Recon, a student-friendly teacher training approach along with the student as a prior step to KD to make the teacher aware of the structure and capacity of the student and enable aligning the representations of the teacher with the student. In SFT, the teacher is jointly trained with the unfolded branch configurations of t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MO-EvoPruneDeepTL&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#21098;&#26525;&#31639;&#27861;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;en_tdlr: This work presents MO-EvoPruneDeepTL, a multi-objective evolutionary pruning algorithm that uses transfer learning to improve the performance and robustness of deep neural networks.</title><link>https://arxiv.org/abs/2302.10253</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#21098;&#26525;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multiobjective Evolutionary Pruning of Deep Neural Networks with Transfer Learning for improving their Performance and Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.10253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MO-EvoPruneDeepTL&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#21098;&#26525;&#31639;&#27861;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;en_tdlr: This work presents MO-EvoPruneDeepTL, a multi-objective evolutionary pruning algorithm that uses transfer learning to improve the performance and robustness of deep neural networks.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#19982;&#26550;&#26500;&#12289;&#36229;&#21442;&#25968;&#25110;&#35757;&#32451;&#37197;&#32622;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24418;&#25104;&#20102;&#29616;&#22312;&#25152;&#31216;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#39046;&#22495;&#12290;&#36825;&#20123;&#31639;&#27861;&#24050;&#19982;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23558;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#20943;&#23567;&#65292;&#20197;&#21450;&#36801;&#31227;&#23398;&#20064;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#20854;&#20182;&#30456;&#20851;&#38382;&#39064;&#20013;&#23548;&#20837;&#30693;&#35782;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#22810;&#20010;&#20934;&#21017;&#26469;&#35780;&#20272;&#36827;&#21270;&#25552;&#35758;&#30340;&#36136;&#37327;&#20063;&#26159;&#24120;&#35265;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#26159;&#26368;&#24120;&#29992;&#30340;&#20934;&#21017;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MO-EvoPruneDeepTL&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#21098;&#26525;&#31639;&#27861;&#12290;MO-EvoPruneDeepTL&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#35843;&#25972;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#20960;&#23618;&#65292;&#36890;&#36807;&#29992;&#36951;&#20256;&#31639;&#27861;&#36827;&#21270;&#20986;&#30340;&#31232;&#30095;&#23618;&#26367;&#25442;&#23427;&#20204;&#65292;&#20174;&#32780;&#25351;&#23548;&#22522;&#20110;&#24615;&#33021;&#12289;&#22797;&#26434;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#36827;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Computation algorithms have been used to solve optimization problems in relation with architectural, hyper-parameter or training configuration, forging the field known today as Neural Architecture Search. These algorithms have been combined with other techniques such as the pruning of Neural Networks, which reduces the complexity of the network, and the Transfer Learning, which lets the import of knowledge from another problem related to the one at hand. The usage of several criteria to evaluate the quality of the evolutionary proposals is also a common case, in which the performance and complexity of the network are the most used criteria. This work proposes MO-EvoPruneDeepTL, a multi-objective evolutionary pruning algorithm. MO-EvoPruneDeepTL uses Transfer Learning to adapt the last layers of Deep Neural Networks, by replacing them with sparse layers evolved by a genetic algorithm, which guides the evolution based in the performance, complexity and robustness of the netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#20998;&#24067;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#21442;&#25968;&#21270;&#27599;&#20010;&#36793;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22788;&#29702;&#65292;&#24182;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2302.05793</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Distributional GFlowNets with Quantile Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#20998;&#24067;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#21442;&#25968;&#21270;&#27599;&#20010;&#36793;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22788;&#29702;&#65292;&#24182;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#37319;&#26679;&#22120;&#31995;&#21015;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#19968;&#31995;&#21015;&#20915;&#31574;&#27493;&#39588;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#30340;&#38543;&#26426;&#31574;&#30053;&#12290;&#23613;&#31649;&#21463;&#24378;&#21270;&#23398;&#20064;&#21551;&#21457;&#65292;&#24403;&#21069;&#30340;GFlowNet&#26694;&#26550;&#22312;&#36866;&#29992;&#24615;&#19978;&#30456;&#23545;&#26377;&#38480;&#65292;&#26080;&#27861;&#22788;&#29702;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#38543;&#26426;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#33539;&#24335;&#26469;&#22788;&#29702;GFlowNets&#65292;&#23558;&#27599;&#20010;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#23545;&#27599;&#20010;&#36793;&#27969;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#37327;&#21270;&#21305;&#37197;&#8221; GFlowNet&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#36825;&#26159;&#22788;&#29702;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#30001;&#20110;&#25105;&#20204;&#22686;&#24378;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#23454;&#29616;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. Despite being inspired from reinforcement learning, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even i
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#22312;&#38405;&#35835;&#25925;&#20107;&#26102;&#65292;&#36890;&#36807;&#23545;&#34394;&#26500;&#21644;&#30495;&#23454;&#20154;&#29289;&#30340;&#31867;&#27604;&#65292;&#21487;&#20197;&#24555;&#36895;&#29702;&#35299;&#26032;&#30340;&#34394;&#26500;&#35282;&#33394;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#23569;&#26679;&#26412;&#21644;&#20803;&#23398;&#20064;&#30340;&#24515;&#26234;&#27169;&#22411;&#65288;ToM&#65289;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#25968;&#25454;&#38598;ToM-in-AMC&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20215;&#26426;&#22120;&#20803;&#23398;&#20064;&#24515;&#26234;&#27169;&#22411;&#30340;&#29616;&#23454;&#21465;&#20107;&#29702;&#35299;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2211.04684</link><description>&lt;p&gt;
&#30005;&#24433;&#20013;&#23569;&#26679;&#26412;&#24773;&#24863;&#29702;&#35299;&#20316;&#20026;&#20803;&#23398;&#20064;&#24515;&#26234;&#27169;&#22411;&#35780;&#20215;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.04684
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#22312;&#38405;&#35835;&#25925;&#20107;&#26102;&#65292;&#36890;&#36807;&#23545;&#34394;&#26500;&#21644;&#30495;&#23454;&#20154;&#29289;&#30340;&#31867;&#27604;&#65292;&#21487;&#20197;&#24555;&#36895;&#29702;&#35299;&#26032;&#30340;&#34394;&#26500;&#35282;&#33394;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#23569;&#26679;&#26412;&#21644;&#20803;&#23398;&#20064;&#30340;&#24515;&#26234;&#27169;&#22411;&#65288;ToM&#65289;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#25968;&#25454;&#38598;ToM-in-AMC&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20215;&#26426;&#22120;&#20803;&#23398;&#20064;&#24515;&#26234;&#27169;&#22411;&#30340;&#29616;&#23454;&#21465;&#20107;&#29702;&#35299;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38405;&#35835;&#25925;&#20107;&#26102;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#23558;&#20854;&#19982;&#20182;&#20204;&#24050;&#32463;&#20102;&#35299;&#30340;&#34394;&#26500;&#21644;&#30495;&#23454;&#20154;&#29289;&#36827;&#34892;&#31867;&#27604;&#65292;&#36805;&#36895;&#29702;&#35299;&#26032;&#30340;&#34394;&#26500;&#35282;&#33394;&#12290;&#36825;&#21453;&#26144;&#20102;&#20154;&#31867;&#23545;&#35282;&#33394;&#20869;&#24515;&#29366;&#24577;&#65288;&#21363;&#24515;&#26234;&#27169;&#22411;&#65289;&#30340;&#25512;&#29702;&#20013;&#23569;&#26679;&#26412;&#21644;&#20803;&#23398;&#20064;&#30340;&#26412;&#36136;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#36825;&#26041;&#38754;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;NLP&#25968;&#25454;&#38598;ToM-in-AMC&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20197;&#29616;&#23454;&#21465;&#20107;&#29702;&#35299;&#22330;&#26223;&#20026;&#32972;&#26223;&#30340;&#26426;&#22120;&#20803;&#23398;&#20064;&#24515;&#26234;&#27169;&#22411;&#35780;&#20215;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;1000&#20010;&#20998;&#26512;&#36807;&#30340;&#30005;&#24433;&#21095;&#26412;&#65292;&#27599;&#20010;&#21095;&#26412;&#23545;&#24212;&#20110;&#19968;&#20010;&#38656;&#35201;&#27169;&#22411;&#27169;&#20223;&#20154;&#31867;&#24555;&#36895;&#29702;&#35299;&#26032;&#30005;&#24433;&#20013;&#30340;&#35282;&#33394;&#30340;&#23569;&#26679;&#26412;&#24773;&#24863;&#29702;&#35299;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
When reading a story, humans can quickly understand new fictional characters with a few observations, mainly by drawing analogies to fictional and real people they already know. This reflects the few-shot and meta-learning essence of humans' inference of characters' mental states, i.e., theory-of-mind (ToM), which is largely ignored in existing research. We fill this gap with a novel NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM in a realistic narrative understanding scenario. Our dataset consists of ~1,000 parsed movie scripts, each corresponding to a few-shot character understanding task that requires models to mimic humans' ability of fast digesting characters with a few starting scenes in a new movie.   We propose a novel ToM prompting approach designed to explicitly assess the influence of multiple ToM dimensions. It surpasses existing baseline models, underscoring the significance of modeling multiple ToM dimensions for our task. Our extensive hu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#31070;&#32463;&#32593;&#32476;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#30340;&#28608;&#27963;&#20989;&#25968;&#26041;&#27861;ANAct&#26469;&#20445;&#25345;&#19968;&#33268;&#30340;&#26799;&#24230;&#24046;&#24322;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2208.13315</link><description>&lt;p&gt;
ANAct: &#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#30340;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
ANAct: Adaptive Normalization for Activation Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.13315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#31070;&#32463;&#32593;&#32476;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#30340;&#28608;&#27963;&#20989;&#25968;&#26041;&#27861;ANAct&#26469;&#20445;&#25345;&#19968;&#33268;&#30340;&#26799;&#24230;&#24046;&#24322;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#25269;&#28040;&#36825;&#31181;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#28608;&#27963;&#20989;&#25968;&#23545;&#31070;&#32463;&#32593;&#32476;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#24046;&#24322;&#30340;&#19968;&#33324;&#24418;&#24335;&#65292;&#25193;&#23637;&#20102;&#27492;&#39046;&#22495;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#23567;&#25209;&#37327;&#32479;&#35745;&#26469;&#21160;&#24577;&#26356;&#26032;&#24402;&#19968;&#21270;&#22240;&#23376;&#65292;&#20197;&#30830;&#20445;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22987;&#32456;&#20445;&#25345;&#24402;&#19968;&#21270;&#23646;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#32771;&#34385;&#26435;&#37325;&#21021;&#22987;&#21270;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#29366;&#24577;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ANAct&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20197;&#22312;&#21508;&#23618;&#20043;&#38388;&#20445;&#25345;&#19968;&#33268;&#30340;&#26799;&#24230;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25910;&#25947;&#36895;&#24230;&#19982;&#24402;&#19968;&#21270;&#23646;&#24615;&#22823;&#33268;&#30456;&#20851;&#12290;&#25105;&#20204;&#23558;ANAct&#19982;&#20960;&#31181;&#24120;&#35265;&#30340;&#28608;&#27963;&#20989;&#25968;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#27531;&#24046;&#32593;&#32476;&#19978;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#26174;&#31034;ANAct&#21487;&#20197;&#25345;&#32493;&#25913;&#21892;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the negative effect of activation functions on forward and backward propagation and how to counteract this effect. First, We examine how activation functions affect the forward and backward propagation of neural networks and derive a general form for gradient variance that extends the previous work in this area. We try to use mini-batch statistics to dynamically update the normalization factor to ensure the normalization property throughout the training process, rather than only accounting for the state of the neural network after weight initialization. Second, we propose ANAct, a method that normalizes activation functions to maintain consistent gradient variance across layers and demonstrate its effectiveness through experiments. We observe that the convergence rate is roughly related to the normalization property. We compare ANAct with several common activation functions on CNNs and residual networks and show that ANAct consistently improves their perfo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2204.04510</link><description>&lt;p&gt;
&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#35753;&#31616;&#21333;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#19978;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.04510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#24120;&#20351;&#29992;&#19987;&#38376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#22823;&#22411;&#20840;&#23616;&#22270;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20294;&#25361;&#25112;&#23376;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23376;&#22270;&#21040;&#33410;&#28857;&#65288;S2N&#65289;&#36716;&#25442;&#30340;&#26032;&#39062;&#20844;&#24335;&#65292;&#29992;&#20110;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20840;&#23616;&#22270;&#20013;&#30340;&#19968;&#32452;&#23376;&#22270;&#65292;&#25105;&#20204;&#36890;&#36807;&#31895;&#30053;&#22320;&#23558;&#23376;&#22270;&#36716;&#25442;&#25104;&#33410;&#28857;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#22270;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;S2N&#19981;&#20165;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19988;&#36890;&#36807;&#25429;&#25417;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#20063;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#23427;&#20204;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31895;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35843;&#25972;&#27169;&#22411;&#21518;&#25928;&#26524;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#22312;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2202.03482</link><description>&lt;p&gt;
&#39046;&#33322;&#31070;&#32463;&#31354;&#38388;&#65306;&#37325;&#26032;&#23457;&#35270;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#20197;&#20811;&#26381;&#26041;&#21521;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.03482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#22312;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#31574;&#30053;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#36890;&#24120;&#65292;CAVs&#26159;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#26469;&#35745;&#31639;&#30340;&#65292;&#35813;&#20998;&#31867;&#22120;&#20248;&#21270;&#20855;&#26377;&#32473;&#23450;&#27010;&#24565;&#21644;&#26080;&#32473;&#23450;&#27010;&#24565;&#30340;&#26679;&#26412;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20197;&#21487;&#20998;&#31163;&#24615;&#20026;&#23548;&#21521;&#30340;&#35745;&#31639;&#26041;&#27861;&#20250;&#23548;&#33268;&#19982;&#31934;&#30830;&#24314;&#27169;&#27010;&#24565;&#26041;&#21521;&#30340;&#23454;&#38469;&#30446;&#26631;&#21457;&#25955;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#20998;&#25955;&#26041;&#21521;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#21363;&#19982;&#27010;&#24565;&#26080;&#20851;&#30340;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#34987;&#32447;&#24615;&#27169;&#22411;&#30340;&#28388;&#27874;&#22120;&#65288;&#21363;&#26435;&#37325;&#65289;&#25429;&#33719;&#20197;&#20248;&#21270;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#65292;&#20165;&#20851;&#27880;&#27010;&#24565;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;CAV&#26041;&#27861;&#19982;&#30495;&#23454;&#27010;&#24565;&#26041;&#21521;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. Commonly, CAVs are computed by leveraging linear classifiers optimizing the separability of latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction. This discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability. To address this, we introduce pattern-based CAVs, solely focussing on concept signals, thereby providing more accurate concept directions. We evaluate various CAV methods in terms of their alignment with the true concept dire
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.16808</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#26469;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Encoding Temporal Statistical-space Priors via Augmented Representation. (arXiv:2401.16808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16808
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20173;&#28982;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#26102;&#38388;&#32500;&#24230;&#19982;&#35768;&#22810;&#39046;&#22495;&#23494;&#20999;&#30456;&#20851;&#12290;&#23613;&#31649;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#39640;&#22122;&#22768;&#20449;&#21495;&#27604;&#12289;&#38750;&#27491;&#24577;&#24615;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#25968;&#25454;&#32570;&#20047;&#20173;&#28982;&#26159;&#25361;&#25112;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#22686;&#24378;&#25216;&#26415;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22686;&#24378;&#34920;&#31034;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20316;&#20026;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#36827;&#34892;&#32534;&#30721;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;&#32479;&#35745;&#31354;&#38388;&#22686;&#24378;&#34920;&#31034;&#65288;SSAR&#65289;&#12290;&#22522;&#20110;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#21551;&#21457;&#20102;&#25105;&#20204;&#30340;&#34920;&#31034;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#20010;&#19979;&#28216;&#26102;&#38388;&#23398;&#20064;&#31639;&#27861;&#30340;&#32463;&#39564;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20987;&#36133;&#20102;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#24615;&#36136;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are availa
&lt;/p&gt;</description></item><item><title>Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16452</link><description>&lt;p&gt;
Context-Former&#65306;&#22522;&#20110;&#28508;&#22312;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#30340;&#25340;&#25509;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Context-Former: Stitching via Latent Conditioned Sequence Modeling. (arXiv:2401.16452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16452
&lt;/p&gt;
&lt;p&gt;
Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#20351;RL&#33021;&#22815;&#23398;&#20064;&#20248;&#20110;&#34892;&#20026;&#31574;&#30053;&#30340;&#31574;&#30053;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Decision Transformer&#65288;DT&#65289;&#23558;&#20915;&#31574;&#24314;&#27169;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#22312;&#31163;&#32447;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;DT&#32570;&#20047;&#25340;&#25509;&#33021;&#21147;&#65292;&#22240;&#27492;&#25552;&#39640;DT&#24615;&#33021;&#38656;&#35201;&#21033;&#29992;&#25340;&#25509;&#33021;&#21147;&#12290;&#20026;&#20102;&#36171;&#20104;DT&#25340;&#25509;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36712;&#36857;&#25340;&#25509;&#25277;&#35937;&#20026;&#19987;&#23478;&#21305;&#37197;&#65292;&#24182;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;ContextFormer&#65292;&#36890;&#36807;&#27169;&#25311;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#30340;&#34920;&#31034;&#26469;&#38598;&#25104;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#24207;&#21015;&#24314;&#27169;&#65292;&#20197;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives:
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20010;&#20154;&#21644;&#22242;&#20307;&#30340;&#20559;&#22909;&#65292;&#35299;&#20915;&#32958;&#33039;&#20998;&#37197;&#20013;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#24182;&#35780;&#20272;&#20559;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15268</link><description>&lt;p&gt;
&#26397;&#30528;&#31283;&#23450;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#19968;&#33268;&#21270;&#26426;&#22120;&#23398;&#20064;&#20559;&#22909;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Preferences for Stakeholder-aligned Machine Learning. (arXiv:2401.15268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20010;&#20154;&#21644;&#22242;&#20307;&#30340;&#20559;&#22909;&#65292;&#35299;&#20915;&#32958;&#33039;&#20998;&#37197;&#20013;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#24182;&#35780;&#20272;&#20559;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32958;&#33039;&#20998;&#37197;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#21363;&#38656;&#27714;&#22686;&#38271;&#19982;&#21033;&#30410;&#30456;&#20851;&#26041;&#20215;&#20540;&#30340;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#31181;&#23398;&#20064;&#20010;&#20154;&#21644;&#22242;&#20307;&#20851;&#20110;&#32958;&#33039;&#20998;&#37197;&#30340;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#8220;&#25104;&#23545;&#32958;&#33039;&#24739;&#32773;&#22312;&#32447;&#35843;&#26597;&#8221;&#30340;&#25968;&#25454;&#65292;&#32467;&#21512;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20010;&#20154;&#12289;&#22242;&#20307;&#21644;&#31283;&#23450;&#24615;&#19977;&#20010;&#23618;&#38754;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#24182;&#36890;&#36807;&#20960;&#31181;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#20010;&#20154;&#23618;&#38754;&#27169;&#22411;&#39044;&#27979;&#20010;&#20307;&#21442;&#19982;&#32773;&#30340;&#20559;&#22909;&#65292;&#22242;&#20307;&#23618;&#38754;&#27169;&#22411;&#27719;&#24635;&#21442;&#19982;&#32773;&#20559;&#22909;&#65292;&#31283;&#23450;&#24615;&#23618;&#38754;&#27169;&#22411;&#26159;&#22242;&#20307;&#21319;&#32423;&#30340;&#25193;&#23637;&#65292;&#35780;&#20272;&#36825;&#20123;&#20559;&#22909;&#38543;&#26102;&#38388;&#30340;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23558;&#21033;&#30410;&#30456;&#20851;&#26041;&#30340;&#20559;&#22909;&#32435;&#20837;&#32958;&#33039;&#20998;&#37197;&#36807;&#31243;&#65292;&#25105;&#20204;&#24076;&#26395;&#25512;&#21160;&#20262;&#29702;&#32500;&#24230;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to the pressing challenge of kidney allocation, characterized by growing demands for organs, this research sets out to develop a data-driven solution to this problem, which also incorporates stakeholder values. The primary objective of this study is to create a method for learning both individual and group-level preferences pertaining to kidney allocations. Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging two distinct datasets and evaluating across three levels - Individual, Group and Stability - we employ machine learning classifiers assessed through several metrics. The Individual level model predicts individual participant preferences, the Group level model aggregates preferences across participants, and the Stability level model, an extension of the Group level, evaluates the stability of these preferences over time. By incorporating stakeholder preferences into the kidney allocation process, we aspire to advance the ethical dimensions of o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15222</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#20197;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#30149;&#20363;&#26816;&#27979;&#20026;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#23454;&#20307;&#30340;&#35821;&#20041;&#21487;&#33021;&#20250;&#21463;&#21040;&#20462;&#39280;&#35821;&#30340;&#26174;&#33879;&#25913;&#21464;&#65292;&#21253;&#25324;&#23454;&#20307;&#30340;&#21542;&#23450;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#26465;&#20214;&#24615;&#12289;&#20005;&#37325;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#29616;&#26377;&#30340;&#30830;&#23450;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#27169;&#22411;&#28041;&#21450;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#25110;&#29305;&#24449;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#20462;&#39280;&#35821;&#30340;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#21464;&#25442;&#22120;&#26550;&#26500;&#35774;&#35745;&#65292;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;SemEval 2015&#20219;&#21153;14&#35821;&#26009;&#24211;&#21644;&#19968;&#20010;&#26032;&#30340;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#65288;OUD&#65289;&#25968;&#25454;&#38598;&#19978;&#20849;&#21516;&#23398;&#20064;&#21644;&#39044;&#27979;&#20462;&#39280;&#35821;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;SemEval&#20849;&#20139;&#30340;&#20462;&#39280;&#35821;&#20197;&#21450;OUD&#29305;&#23450;&#30340;&#26032;&#20462;&#39280;&#35821;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#19982;&#20197;&#21069;&#21457;&#34920;&#30340;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#24182;&#35780;&#20272;&#20102;&#20165;&#20849;&#20139;&#37096;&#20998;&#20020;&#24202;&#20462;&#39280;&#35821;&#26102;&#30340;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;SemEval 2015&#30340;ShARe&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#32452;&#21512;&#21644;&#24182;&#34892;&#32467;&#26500;&#20998;&#31867;&#22120;&#26469;&#22686;&#24378;&#26080;&#25991;&#26412;&#32422;&#26463;&#30340;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#38454;&#27573;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#22768;&#23398;&#29305;&#24449;&#30340;&#32452;&#21512;&#27604;&#36739;&#65292;&#24182;&#38024;&#23545;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#30340;&#24369;&#28857;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.15018</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#32452;&#21512;&#21644;&#24182;&#34892;&#32467;&#26500;&#20998;&#31867;&#22120;&#22686;&#24378;&#26080;&#25991;&#26412;&#32422;&#26463;&#30340;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers. (arXiv:2401.15018v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#32452;&#21512;&#21644;&#24182;&#34892;&#32467;&#26500;&#20998;&#31867;&#22120;&#26469;&#22686;&#24378;&#26080;&#25991;&#26412;&#32422;&#26463;&#30340;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#38454;&#27573;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#22768;&#23398;&#29305;&#24449;&#30340;&#32452;&#21512;&#27604;&#36739;&#65292;&#24182;&#38024;&#23545;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#30340;&#24369;&#28857;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#20027;&#35201;&#21253;&#21547;&#20004;&#20010;&#29420;&#31435;&#30340;&#38454;&#27573;&#65306;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#36825;&#20004;&#20010;&#27169;&#22359;&#65292;&#25552;&#39640;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#22768;&#23398;&#29305;&#24449;&#26159;&#36827;&#34892;&#40065;&#26834;&#24615;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#22768;&#23398;&#21442;&#25968;&#21253;&#25324;&#65306;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;MFCC&#65289;&#65292;&#23427;&#20204;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#65288;Deltas&#21644;Delta-Deltas&#65289;&#65292;&#24052;&#20811;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;BFCC&#65289;&#65292;&#24863;&#30693;&#32447;&#24615;&#39044;&#27979;&#65288;PLP&#65289;&#21644;&#30456;&#23545;&#35889;&#21464;&#25442;&#24863;&#30693;&#32447;&#24615;&#39044;&#27979;&#65288;RASTA-PLP&#65289;&#12290;&#26412;&#25991;&#23545;&#19981;&#21516;&#29305;&#24449;&#32452;&#21512;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20998;&#31867;&#22120;&#30340;&#20027;&#35201;&#24369;&#28857;&#26159;&#20351;&#29992;&#36890;&#29992;&#20256;&#32479;&#30340;&#26680;&#20989;&#25968;&#35745;&#31639;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker Verification (SV) systems involve mainly two individual stages: feature extraction and classification. In this paper, we explore these two modules with the aim of improving the performance of a speaker verification system under noisy conditions. On the one hand, the choice of the most appropriate acoustic features is a crucial factor for performing robust speaker verification. The acoustic parameters used in the proposed system are: Mel Frequency Cepstral Coefficients (MFCC), their first and second derivatives (Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC), Perceptual Linear Predictive (PLP), and Relative Spectral Transform Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison of different combinations of the previous features is discussed. On the other hand, the major weakness of a conventional Support Vector Machine (SVM) classifier is the use of generic traditional kernel functions to compute the distances among data points
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#31890;&#24230;&#31561;&#32423;&#30340;&#23618;&#27425;&#32423;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#35270;&#35273;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#25805;&#20316;&#12290;&#36825;&#19968;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#23545;&#24433;&#21709;&#26426;&#22120;&#35270;&#35273;&#21151;&#33021;&#30340;&#21508;&#20010;&#23454;&#20307;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2401.14831</link><description>&lt;p&gt;
&#26426;&#22120;&#35270;&#35273;&#20912;&#23665;&#30340;&#35299;&#37322;&#65306;&#36890;&#36807;&#32771;&#34385;&#20840;&#38754;&#29615;&#22659;&#26465;&#20214;&#25512;&#36827;&#21160;&#24577;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances. (arXiv:2401.14831v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#31890;&#24230;&#31561;&#32423;&#30340;&#23618;&#27425;&#32423;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#35270;&#35273;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#25805;&#20316;&#12290;&#36825;&#19968;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#23545;&#24433;&#21709;&#26426;&#22120;&#35270;&#35273;&#21151;&#33021;&#30340;&#21508;&#20010;&#23454;&#20307;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26426;&#22120;&#35270;&#35273;&#27979;&#35797;&#26159;&#21542;&#20250;&#24102;&#26469;&#20912;&#23665;&#30340;&#21361;&#38505;&#65311;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#35270;&#35273;&#65288;MV&#65289;&#27979;&#35797;&#30340;&#39046;&#22495;&#65292;&#36825;&#22312;&#39640;&#24230;&#33258;&#21160;&#39550;&#39542;&#65288;HAD&#65289;&#31995;&#32479;&#20013;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#20511;&#21161;&#21521;&#20912;&#23665;&#33322;&#34892;&#30340;&#38544;&#21947;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#27979;&#35797;&#31574;&#30053;&#20013;&#28508;&#22312;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#22914;&#20309;&#22788;&#29702;MV&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19981;&#36879;&#26126;&#21151;&#33021;&#30340;&#26356;&#28145;&#20837;&#20102;&#35299;&#30340;&#32039;&#36843;&#38656;&#35201;&#65292;&#22240;&#20026;&#24573;&#35270;&#20102;&#36825;&#20123;&#32771;&#34385;&#21487;&#33021;&#20250;&#36896;&#25104;&#29983;&#21629;&#30340;&#20007;&#22833;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23618;&#27425;&#32423;&#21035;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31890;&#24230;&#31561;&#32423;&#12290;&#35813;&#27169;&#22411;&#40723;&#21169;&#23545;MV&#25805;&#20316;&#29615;&#22659;&#26465;&#20214;&#30340;&#21508;&#20010;&#23618;&#27425;&#36827;&#34892;&#31934;&#32454;&#25506;&#32034;&#65292;&#20174;&#20010;&#20307;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21040;&#25972;&#20010;&#29615;&#22659;&#22330;&#26223;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#23545;&#21487;&#33021;&#24433;&#21709;MV&#21151;&#33021;&#30340;&#25152;&#26377;&#23454;&#20307;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are we heading for an iceberg with the current testing of machine vision? This work delves into the landscape of Machine Vision (MV) testing, which is heavily required in Highly Automated Driving (HAD) systems. Utilizing the metaphorical notion of navigating towards an iceberg, we discuss the potential shortcomings concealed within current testing strategies. We emphasize the urgent need for a deeper understanding of how to deal with the opaque functions of MV in development processes. As overlooked considerations can cost lives. Our main contribution is the hierarchical level model, which we call Granularity Grades. The model encourages a refined exploration of the multi-scaled depths of understanding about the circumstances of environments in which MV is intended to operate. This model aims to provide a holistic overview of all entities that may impact MV functions, ranging from relations of individual entities like object attributes to entire environmental scenes. The application of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#12290;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#20943;&#36731;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.13138</link><description>&lt;p&gt;
&#23545;AI&#20195;&#29702;&#30340;&#21487;&#35265;&#24615;
&lt;/p&gt;
&lt;p&gt;
Visibility into AI Agents. (arXiv:2401.13138v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#12290;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#20943;&#36731;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#25919;&#24220;&#21644;&#20010;&#20154;&#27963;&#21160;&#22996;&#25176;&#32473;&#20855;&#26377;&#26377;&#38480;&#30417;&#30563;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#21152;&#21095;&#29616;&#26377;&#30340;&#31038;&#20250;&#39118;&#38505;&#24182;&#24341;&#20837;&#26032;&#30340;&#39118;&#38505;&#12290;&#29702;&#35299;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#28041;&#21450;&#23545;&#29616;&#26377;&#27835;&#29702;&#32467;&#26500;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#20462;&#35746;&#21644;&#35843;&#25972;&#65292;&#24182;&#30830;&#20445;&#20851;&#38190;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#38382;&#36131;&#21046;&#12290;&#25105;&#20204;&#23558;AI&#20195;&#29702;&#30340;&#20351;&#29992;&#22320;&#28857;&#12289;&#21407;&#22240;&#12289;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#32773;&#31561;&#20449;&#24687;&#31216;&#20026;&#8220;&#21487;&#35265;&#24615;&#8221;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#12290;&#23545;&#20110;&#27599;&#19968;&#31181;&#25514;&#26045;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#65292;&#36825;&#20123;&#26041;&#24335;&#22312;&#20405;&#20837;&#24615;&#21644;&#20449;&#24687;&#24615;&#26041;&#38754;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increased delegation of commercial, scientific, governmental, and personal activities to AI agents -- systems capable of pursuing complex goals with limited supervision -- may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as \textbf{visibility}, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: \textbf{agent identifiers}, \textbf{real-time monitoring}, and \textbf{activity logging}. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for vario
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#32534;&#36753;&#20107;&#23454;&#24182;&#35266;&#23519;&#20854;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#26356;&#26032;&#20256;&#25773;&#65292;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.10521</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#35821;&#35328;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Editing in Multilingual Language Models. (arXiv:2401.10521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#32534;&#36753;&#20107;&#23454;&#24182;&#35266;&#23519;&#20854;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#26356;&#26032;&#20256;&#25773;&#65292;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#32780;&#26356;&#26032;&#36807;&#26102;&#30340;LLM&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#36164;&#28304;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#35768;&#22810;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#20197;&#20415;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#26356;&#26032;&#27169;&#22411;&#36755;&#20986;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;LLM&#20013;&#65292;&#20854;&#20013;&#30340;&#30693;&#35782;&#20197;&#22810;&#31181;&#35821;&#35328;&#23384;&#20648;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#22312;&#35813;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20107;&#23454;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#34987;&#32534;&#36753;&#65292;&#35266;&#23519;&#20854;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26356;&#26032;&#20256;&#25773;&#12290;&#20026;&#20102;&#30740;&#31350;XME&#33539;&#24335;&#65292;&#25105;&#20204;&#20351;&#29992;BLOOM&#12289;mBERT&#21644;XLM-RoBERTa&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#20889;&#20316;&#33050;&#26412;&#65292;&#21363;&#25289;&#19969;&#35821;&#65288;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#21644;&#21360;&#22320;&#35821;&#65288;&#21360;&#22320;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;XME&#35774;&#32622;&#19979;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;MET&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#24403;&#28041;&#21450;&#30340;&#35821;&#35328;&#23646;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#35821;&#26063;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (\textbf{XME}) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: \textit{Latin} (English, French, and Spanish) and \textit{Indic} (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distin
&lt;/p&gt;</description></item><item><title>DCRMTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#21644;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#26469;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.08875</link><description>&lt;p&gt;
DCRMTA: &#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#30340;&#22240;&#26524;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DCRMTA: Unbiased Causal Representation for Multi-touch Attribution. (arXiv:2401.08875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08875
&lt;/p&gt;
&lt;p&gt;
DCRMTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#21644;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#26469;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35302;&#28857;&#24402;&#22240;&#65288;MTA&#65289;&#22312;&#23454;&#29616;&#23545;&#27599;&#20010;&#24191;&#21578;&#35302;&#28857;&#23545;&#20110;&#36716;&#21270;&#34892;&#20026;&#30340;&#36129;&#29486;&#30340;&#20844;&#27491;&#20272;&#35745;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28145;&#21051;&#24433;&#21709;&#39044;&#31639;&#20998;&#37197;&#21644;&#24191;&#21578;&#25512;&#33616;&#12290;&#20256;&#32479;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#23398;&#20064;&#35302;&#28857;&#24207;&#21015;&#21644;&#29992;&#25143;&#36141;&#20080;&#34892;&#20026;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#20174;&#21407;&#22987;&#24207;&#21015;&#23376;&#38598;&#20013;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#20272;&#35745;&#36716;&#21270;&#65292;&#20174;&#32780;&#35745;&#31639;&#24191;&#21578;&#36129;&#29486;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#30340;&#26080;&#20559;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#20559;&#22909;&#21644;&#20114;&#32852;&#32593;&#25512;&#33616;&#26426;&#21046;&#65288;&#22914;&#36807;&#21435;&#30340;&#36141;&#29289;&#35760;&#24405;&#23548;&#33268;&#30340;&#24191;&#21578;&#25512;&#33616;&#21516;&#36136;&#21270;&#65289;&#24341;&#36215;&#30340;&#28151;&#26434;&#21464;&#37327;&#22240;&#32032;&#65292;&#36716;&#21270;&#20013;&#24456;&#23481;&#26131;&#20135;&#29983;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-touch attribution (MTA) currently plays a pivotal role in achieving a fair estimation of the contributions of each advertising touchpoint to-wards conversion behavior, deeply influencing budget allocation and advertising recommenda-tion. Traditional multi-touch attribution methods initially build a conversion prediction model, an-ticipating learning the inherent relationship be-tween touchpoint sequences and user purchasing behavior through historical data. Based on this, counterfactual touchpoint sequences are con-structed from the original sequence subset, and conversions are estimated using the prediction model, thus calculating advertising contributions. A covert assumption of these methods is the un-biased nature of conversion prediction models. However, due to confounding variables factors arising from user preferences and internet recom-mendation mechanisms such as homogenization of ad recommendations resulting from past shop-ping records, bias can easily occur in conversi
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.05631</link><description>&lt;p&gt;
DrawTalking&#65306;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05631
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;DrawTalking&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#12290;&#23427;&#24378;&#35843;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#31867;&#20284;&#32534;&#31243;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;iPad&#19978;&#23454;&#29616;&#20102;&#23427;&#12290;&#19968;&#39033;&#24320;&#25918;&#24335;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#35768;&#22810;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#29992;&#20363;&#30456;&#22865;&#21512;&#21644;&#36866;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#33258;&#28982;&#29992;&#25143;&#20013;&#24515;&#30028;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
&lt;/p&gt;</description></item><item><title>CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2401.05043</link><description>&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks. (arXiv:2401.05043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05043
&lt;/p&gt;
&lt;p&gt;
CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;Credal-Set Interval Neural Networks&#65288;CreINNs&#65289;&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;CreINNs&#20445;&#30041;&#20102;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#21306;&#38388;&#25429;&#25417;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#22312;&#19968;&#20010;&#36229;&#20986;&#20998;&#21457;&#26816;&#27979;&#22522;&#20934;&#65288;CIFAR10 vs SVHN&#65289;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20013;&#65292;CreINNs&#30456;&#27604;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#65288;DEs&#65289;&#65292;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#19982;&#21464;&#20998;BNNs&#30456;&#27604;&#65292;CreINNs&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#27604;DEs&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is increasingly attractive for improving the reliability of neural networks. In this work, we present novel credal-set interval neural networks (CreINNs) designed for classification tasks. CreINNs preserve the traditional interval neural network structure, capturing weight uncertainty through deterministic intervals, while forecasting credal sets using the mathematical framework of probability intervals. Experimental validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN) showcase that CreINNs outperform epistemic uncertainty estimation when compared to variational Bayesian neural networks (BNNs) and deep ensembles (DEs). Furthermore, CreINNs exhibit a notable reduction in computational complexity compared to variational BNNs and demonstrate smaller model sizes than DEs.
&lt;/p&gt;</description></item><item><title>NODEC&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01836</link><description>&lt;p&gt;
NODEC: &#29992;&#20110;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#26368;&#20248;&#25511;&#21046;&#30340;&#31070;&#32463;ODE
&lt;/p&gt;
&lt;p&gt;
NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems. (arXiv:2401.01836v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01836
&lt;/p&gt;
&lt;p&gt;
NODEC&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#36890;&#24120;&#28041;&#21450;&#22312;&#21464;&#21270;&#35745;&#31639;&#26694;&#26550;&#19979;&#26368;&#23567;&#21270;&#20855;&#26377;&#24050;&#30693;&#21160;&#21147;&#23398;&#30340;&#26576;&#20123;&#25511;&#21046;&#30446;&#26631;&#12290;&#23545;&#20110;&#20855;&#26377;&#26410;&#30693;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#65292;&#38656;&#35201;&#39069;&#22806;&#36827;&#34892;&#21160;&#21147;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#20219;&#20309;&#19981;&#20934;&#30830;&#37117;&#20250;&#23548;&#33268;&#32467;&#26524;&#25511;&#21046;&#20989;&#25968;&#30340;&#27425;&#20248;&#24615;&#12290;&#21478;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861; - &#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#34701;&#20837;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#24191;&#27867;&#20132;&#20114;&#26469;&#36817;&#20284;&#20540;&#20989;&#25968;&#25110;&#31574;&#30053;&#26799;&#24230;&#65292;&#20294;&#23427;&#30340;&#25968;&#25454;&#25928;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NODEC&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#20004;&#20010;&#32806;&#21512;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#26377;&#36259;&#30456;&#20114;&#20316;&#29992;&#65292;NODEC&#23398;&#20064;&#20102;&#31995;&#32479;&#21160;&#21147;&#23398;&#20197;&#21450;&#25351;&#23548;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework. For systems with unknown dynamics, an additional step of dynamics modeling is required. However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function. Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency. To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model. Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknow
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01325</link><description>&lt;p&gt;
&#33258;&#25193;&#23637;LLM:&#26080;&#38656;&#35843;&#25972;&#30340;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31934;&#35843;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#24207;&#21015;&#30340;&#26377;&#38480;&#38271;&#24230;&#21487;&#33021;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#26412;&#36523;&#20855;&#26377;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20854;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Self-Extend&#26041;&#27861;&#26469;&#28608;&#21457;LLMs&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#28508;&#21147;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#26500;&#24314;&#21452;&#23618;&#27880;&#24847;&#20449;&#24687;&#65306;&#32676;&#32452;&#32423;&#21644;&#37051;&#23621;&#32423;&#12290;&#36825;&#20004;&#20010;&#32423;&#21035;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#12290;&#21482;&#38656;&#20462;&#25913;&#22235;&#34892;&#20195;&#30721;&#65292;&#25152;&#25552;&#26041;&#27861;&#23601;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#31934;&#35843;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;+&#25688;&#35201;&#20943;&#25481;&#25991;&#31456;&#26368;&#21518;&#19968;&#21477;&#35441;
&lt;/p&gt;
&lt;p&gt;
This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#26223;&#24863;&#30693;&#30340;&#32039;&#24613;&#35268;&#21010;&#33021;&#21147;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#22522;&#20934;&#21644;&#25351;&#26631;&#65292;&#20197;&#21450;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#21644;&#22810;&#26234;&#33021;&#20307;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19978;&#19979;&#25991;&#25935;&#24863;&#35268;&#21010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.16127</link><description>&lt;p&gt;
LLM-SAP: &#22823;&#35821;&#35328;&#27169;&#22411;&#24773;&#26223;&#24863;&#30693;&#30340;&#22522;&#20110;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLM-SAP: Large Language Model Situational Awareness Based Planning. (arXiv:2312.16127v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#26223;&#24863;&#30693;&#30340;&#32039;&#24613;&#35268;&#21010;&#33021;&#21147;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#22522;&#20934;&#21644;&#25351;&#26631;&#65292;&#20197;&#21450;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#21644;&#22810;&#26234;&#33021;&#20307;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19978;&#19979;&#25991;&#25935;&#24863;&#35268;&#21010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35780;&#20272;&#22522;&#20110;&#24773;&#26223;&#24863;&#30693;&#30340;&#32039;&#24613;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#65288;i&#65289;&#29992;&#20110;&#26631;&#20934;&#21270;&#35780;&#20272;&#30340;&#26032;&#22411;&#22522;&#20934;&#21644;&#25351;&#26631;&#65307;&#65288;ii&#65289;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#36827;&#23637;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#28436;&#31034;&#20197;&#25552;&#31034;&#21644;&#22810;&#26234;&#33021;&#20307;&#26041;&#26696;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#25935;&#24863;&#35268;&#21010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23558;&#20854;&#32435;&#20837;&#21040;&#19968;&#20010;&#22788;&#22659;&#26234;&#33021;&#20307;&#21644;&#33258;&#21160;&#21270;&#35268;&#21010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22266;&#26377;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;-&#23613;&#31649;&#22312;&#27169;&#25311;&#39046;&#22495;&#30340;&#36827;&#27493;&#20013;&#65292;&#22914;&#20309;&#22312;&#27809;&#26377;&#29615;&#22659;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23558;&#19990;&#30028;&#29366;&#24577;&#26144;&#23556;&#21040;&#34892;&#21160;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36229;&#20986;&#33539;&#22260;&#65292;&#23545;&#20110;&#39564;&#35777;&#26041;&#27861;&#21644;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#34920;&#26126;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#23545;&#25193;&#23637;&#35268;&#21010;&#35821;&#26009;&#24211;&#36827;&#34892;&#24494;&#35843;&#21644;&#38024;&#23545;&#24555;&#36895;&#28508;&#22312;&#35268;&#21010;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#36890;&#36807;&#20005;&#23494;&#30340;&#27604;&#36739;&#26469;&#30830;&#23454;&#22320;&#23637;&#31034;&#24403;&#21069;&#26041;&#27861;&#30340;&#25215;&#35834;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25512;&#21160;&#20102;&#23545;&#21487;&#38752;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
This work pioneers evaluating emergent planning capabilities based on situational awareness in large language models. We contribute (i) novel benchmarks and metrics for standardized assessment; (ii) a unique dataset to spur progress; and (iii) demonstrations that prompting and multi-agent schemes significantly enhance planning performance in context-sensitive planning tasks. Positioning this within a situated agent and automated planning research, we highlight inherent reliability challenges--efficiently mapping world states to actions without environmental guidance remains open despite simulated domain advances. Although out-of-scope, limitations around validation methodology and data availability indicate exciting directions, including fine-tuning on expanded planning corpora and optimizations for triggering fast latent planning. By conclusively demonstrating current methods' promise and limitations via rigorous comparison, we catalyze investigating reliable goal-directed reasoning f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.13544</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20272;&#35745;&#23646;&#20110;&#19968;&#31867;&#29305;&#23450;&#30340;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#21363;&#25152;&#35859;&#30340;&#28201;&#39034;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#20986;&#29616;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65306;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#12289;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#20215;&#20540;&#20989;&#25968;&#25110;&#23567;&#20998;&#23376;&#30340;&#27874;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28201;&#39034;&#20989;&#25968;&#22312;&#20219;&#20309;&#23436;&#20840;&#32500;&#24230;&#30340;&#31435;&#26041;&#20307;&#19978;&#21487;&#29992;&#20998;&#27573;&#22810;&#39033;&#24335;&#26469;&#36924;&#36817;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#28201;&#39034;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#33719;&#24471;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#20511;&#21161;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#26469;&#25903;&#25345;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#23398;&#29983;&#27169;&#22411;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#36733;&#26041;&#38754;&#30340;&#24320;&#38144;&#36739;&#23567;&#65292;&#24182;&#21487;&#20197;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2311.01441</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#33719;&#24471;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#20511;&#21161;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#26469;&#25903;&#25345;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#23398;&#29983;&#27169;&#22411;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#36733;&#26041;&#38754;&#30340;&#24320;&#38144;&#36739;&#23567;&#65292;&#24182;&#21487;&#20197;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#32467;&#21512;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#36827;&#34892;&#33976;&#39311;&#65292;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#33719;&#24471;&#30340;&#40065;&#26834;&#24615;&#22686;&#30410;&#65292;&#20197;&#27492;&#21453;&#39539;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#20250;&#25104;&#20026;&#26356;&#22909;&#30340;&#25945;&#24072;&#30340;&#29468;&#24819;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#65288;Discrete Adversarial Distillation&#65292;DAD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#40065;&#26834;&#30340;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;VQGAN&#23558;&#20854;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#21019;&#36896;&#20986;&#27604;&#26631;&#20934;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23398;&#29983;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#30340;&#40065;&#26834;&#24615;&#21644;&#24178;&#20928;&#20934;&#30830;&#24615;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#31867;&#20284;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#21152;&#20102;&#23569;&#37327;&#30340;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.01378</link><description>&lt;p&gt;
Vision-Language Foundation Models&#20316;&#20026;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#23427;&#20204;&#29702;&#35299;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65288;&#21253;&#25324;&#26426;&#22120;&#20154;&#25805;&#20316;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#35270;&#35273;&#35821;&#35328;&#25805;&#20316;&#26694;&#26550;&#65292;&#21517;&#20026;RoboFlamingo&#65292;&#23427;&#24314;&#31435;&#22312;&#24320;&#28304;&#30340;VLMs&#65292;OpenFlamingo&#20043;&#19978;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;RoboFlamingo&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;VLMs&#36827;&#34892;&#21333;&#27493;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65292;&#20351;&#29992;&#26174;&#24335;&#31574;&#30053;&#22836;&#27169;&#25311;&#39034;&#24207;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#21482;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#20998;&#35299;&#20026;RoboFlamingo&#25552;&#20379;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#36827;&#34892;&#24320;&#29615;&#25511;&#21046;&#21644;&#37096;&#32626;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#22312;&#27979;&#35797;&#22522;&#20934;&#19978;&#22823;&#24133;&#36229;&#36807;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RoboFlamingo&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01256</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#28304;&#30340;&#27861;&#24459;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#24120;&#35265;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#35780;&#20272;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36861;&#27714;&#26368;&#20339;&#24615;&#33021;&#30340;&#31454;&#20105;&#20013;&#65292;&#32463;&#24120;&#24573;&#35270;&#35768;&#22810;&#37325;&#35201;&#22240;&#32032;&#65292;&#32780;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#22240;&#32032;&#24212;&#35813;&#34987;&#20180;&#32454;&#32771;&#34385;&#12290;&#23454;&#38469;&#19978;&#65292;&#26377;&#26102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#32780;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#22240;&#32032;&#24517;&#39035;&#32771;&#34385;&#22312;&#20869;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;NLP&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;LexGLUE&#22522;&#20934;&#19978;&#23545;LLM&#21644;&#20256;&#32479;&#26041;&#27861;&#65288;&#20363;&#22914;SVM&#65289;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#21516;&#26102;&#32771;&#34385;&#24615;&#33021;&#65288;&#26631;&#20934;&#25351;&#26631;&#65289;&#21644;&#20854;&#20182;&#25351;&#26631;&#65292;&#22914;&#26102;&#38388;&#12289;&#32791;&#33021;&#21644;&#25104;&#26412;&#65292;&#24635;&#20043;&#23601;&#26159;&#30899;&#36275;&#36857;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#32771;&#34385;&#20102;&#21407;&#22411;&#35774;&#35745;&#38454;&#27573;&#65288;&#36890;&#36807;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#36845;&#20195;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65289;&#21644;&#29983;&#20135;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they fol
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01223</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01223
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#24050;&#32463;&#20986;&#29616;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#21253;&#25324;&#20316;&#20026;&#36712;&#36857;&#35268;&#21010;&#22120;&#12289;&#34920;&#36798;&#33021;&#21147;&#20016;&#23500;&#30340;&#31574;&#30053;&#31867;&#21035;&#12289;&#25968;&#25454;&#21512;&#25104;&#22120;&#31561;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#21551;&#21457;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#24403;&#21069;RL&#31639;&#27861;&#36935;&#21040;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25193;&#25955;&#27169;&#22411;&#22312;RL&#20013;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#25552;&#20986;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;RL&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#39033;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#37325;&#28857;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.17451</link><description>&lt;p&gt;
&#36890;&#36807;&#29702;&#35299;&#29983;&#25104;&#65306;&#20855;&#26377;&#36923;&#36753;&#31526;&#21495;&#22522;&#30784;&#30340;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings. (arXiv:2310.17451v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#19982;&#24378;&#22823;&#30340;&#31526;&#21495;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#38598;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20027;&#35201;&#25361;&#25112;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#19968;&#20010;&#26159;&#31526;&#21495;&#36171;&#20540;&#65292;&#21363;&#23558;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#22120;&#30340;&#28508;&#22312;&#22240;&#32032;&#19982;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#36827;&#34892;&#32465;&#23450;&#12290;&#21478;&#19968;&#20010;&#26159;&#35268;&#21017;&#23398;&#20064;&#65292;&#21363;&#23398;&#20064;&#26032;&#30340;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#25511;&#21046;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;Abductive Visual Generation (AbdGen)&#65292;&#29992;&#20110;&#22522;&#20110;&#35825;&#23548;&#23398;&#20064;&#26694;&#26550;&#23558;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#24341;&#20837;&#20102;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#32534;&#30721;&#26412;&#20013;&#30340;&#26368;&#36817;&#37051;&#26597;&#25214;&#29983;&#25104;&#35825;&#23548;&#25552;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of neural visual generative models in recent years, integrating them with strong symbolic knowledge reasoning systems remains a challenging task. The main challenges are two-fold: one is symbol assignment, i.e. bonding latent factors of neural visual generators with meaningful symbols from knowledge reasoning systems. Another is rule learning, i.e. learning new rules, which govern the generative process of the data, to augment the knowledge reasoning systems. To deal with these symbol grounding problems, we propose a neural-symbolic learning approach, Abductive Visual Generation (AbdGen), for integrating logic programming systems with neural visual generative models based on the abductive learning framework. To achieve reliable and efficient symbol assignment, the quantized abduction method is introduced for generating abduction proposals by the nearest-neighbor lookups within semantic codebooks. To achieve precise rule learning, the contrastive meta-abduction
&lt;/p&gt;</description></item><item><title>DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15393</link><description>&lt;p&gt;
DoGE: &#20351;&#29992;&#27867;&#21270;&#20272;&#35745;&#36827;&#34892;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15393
&lt;/p&gt;
&lt;p&gt;
DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#35821;&#26009;&#24211;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#32452;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30001;&#21508;&#31181;&#26469;&#28304;&#39046;&#22495;&#65288;&#22914;CommonCrawl&#12289;Wikipedia&#12289;Github&#31561;&#65289;&#25353;&#29031;&#29305;&#23450;&#30340;&#37319;&#26679;&#27010;&#29575;&#65288;&#39046;&#22495;&#26435;&#37325;&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#20248;&#21270;&#39046;&#22495;&#26435;&#37325;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DOmain reweighting with Generalization Estimation&#65288;DoGE&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#37325;&#26032;&#35843;&#25972;&#20102;&#27599;&#20010;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#65292;&#26681;&#25454;&#23427;&#23545;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#36827;&#34892;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#27867;&#21270;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#35757;&#32451;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#33719;&#21462;&#37325;&#26032;&#21152;&#26435;&#30340;&#39046;&#22495;&#26435;&#37325;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#36890;&#36807;&#38236;&#20687;&#19979;&#38477;&#27861;&#26356;&#26032;&#39046;&#22495;&#26435;&#37325;&#20197;&#26368;&#22823;&#21270;&#25972;&#20307;&#30340;&#27867;&#21270;&#22686;&#30410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#33719;&#24471;&#30340;&#39046;&#22495;&#26435;&#37325;&#26469;&#35757;&#32451;&#19968;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;&#23436;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ANPG&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;ANPG&#23454;&#29616;&#20102;&#26679;&#26412;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11677</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes. (arXiv:2310.11677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ANPG&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;ANPG&#23454;&#29616;&#20102;&#26679;&#26412;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#35774;&#35745;&#26679;&#26412;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;ANPG&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#26469;&#33719;&#21462;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#12290;ANPG&#31639;&#27861;&#22312;&#19968;&#33324;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;O(&#949;^{-2})&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;O(&#949;^{-1})&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#949;&#23450;&#20041;&#20102;&#26368;&#20248;&#24615;&#35823;&#24046;&#12290;&#36825;&#23558;&#26679;&#26412;&#22797;&#26434;&#24230;&#25552;&#39640;&#20102;&#19968;&#20010;log(1/&#949;)&#30340;&#22240;&#23376;&#12290;ANPG&#26159;&#19968;&#20010;&#19968;&#38454;&#31639;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#29616;&#26377;&#25991;&#29486;&#20013;&#21487;&#33021;&#26080;&#27861;&#39564;&#35777;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;(IS)&#26435;&#37325;&#26041;&#24046;&#19978;&#30028;&#30340;&#20551;&#35774;&#12290;&#22312;&#26080;Hessian&#21644;&#26080;IS&#31639;&#27861;&#31867;&#20013;&#65292;ANPG&#36229;&#36807;&#20102;&#24050;&#30693;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19968;&#20010;O(&#949;^{-\frac{1}{2}})&#30340;&#22240;&#23376;&#65292;&#24182;&#21516;&#26102;&#36798;&#21040;&#20102;&#23427;&#20204;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\mathcal{O}({\epsilon^{-2}})$ sample complexity and $\mathcal{O}(\epsilon^{-1})$ iteration complexity with general parameterization where $\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\log(\frac{1}{\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\mathcal{O}(\epsilon^{-\frac{1}{2}})$ and simultaneously matches their state-of-the-art it
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09486</link><description>&lt;p&gt;
Mirage: &#22270;&#20998;&#31867;&#30340;&#27169;&#22411;&#26080;&#20851;&#22270;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09486
&lt;/p&gt;
&lt;p&gt;
Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNNs&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#25968;&#25454;&#21644;&#35745;&#31639;&#38656;&#27714;&#37327;&#24456;&#22823;&#12290;&#24613;&#38656;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;GNN&#30340;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#22270;&#33976;&#39311;&#26159;&#20026;&#27492;&#30446;&#30340;&#32780;&#21162;&#21147;&#65292;&#26088;&#22312;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#21021;&#27493;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;(1)&#29616;&#26377;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#26412;&#36523;&#20381;&#36182;&#20110;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#23601;&#30772;&#22351;&#20102;&#22270;&#33976;&#39311;&#30340;&#21069;&#25552;&#12290;(2)&#33976;&#39311;&#36807;&#31243;&#23545;&#30446;&#26631;GNN&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20855;&#26377;&#29305;&#24322;&#24615;&#65292;&#22240;&#27492;&#23545;&#24314;&#27169;&#27969;&#31243;&#30340;&#21464;&#21270;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;Mirage&#30340;&#22270;&#20998;&#31867;&#33976;&#39311;&#31639;&#27861;&#26469;&#36991;&#20813;&#36825;&#20123;&#38480;&#21046;&#12290;Mirage&#24314;&#31435;&#22312;&#19968;&#20010;&#27934;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#21363;&#19968;&#20010;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#23558;&#36755;&#20837;&#22270;&#20998;&#35299;&#20026;&#35745;&#31639;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#35299;transformer&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#35821;&#20041;&#29702;&#35299;&#20013;&#30340;&#38544;&#21547;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.04861</link><description>&lt;p&gt;
&#36890;&#36807;&#21306;&#20998;&#20301;&#32622;&#21644;&#19978;&#19979;&#25991;&#26469;&#25581;&#31034;Transformers&#20013;&#30340;&#38544;&#34255;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Uncovering hidden geometry in Transformers via disentangling position and context. (arXiv:2310.04861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#35299;transformer&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#35821;&#20041;&#29702;&#35299;&#20013;&#30340;&#38544;&#21547;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24191;&#27867;&#29992;&#20110;&#20174;&#36755;&#20837;&#20196;&#29260;&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#35821;&#20041;&#24847;&#20041;&#65292;&#28982;&#32780;&#23427;&#20204;&#36890;&#24120;&#20316;&#20026;&#40657;&#30418;&#27169;&#22411;&#36816;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#20449;&#24687;&#20016;&#23500;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22909;&#30340;transformer&#30340;&#38544;&#34255;&#29366;&#24577;&#65288;&#25110;&#23884;&#20837;&#65289;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#32452;&#20214;&#12290;&#23545;&#20110;&#20219;&#20309;&#23618;&#65292;&#36755;&#20837;&#24207;&#21015;&#26679;&#26412;&#30340;&#23884;&#20837;&#21521;&#37327;&#30001;&#19968;&#20010;&#24352;&#37327;&#34920;&#31034; $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$&#12290;&#32473;&#23450;&#22312;&#24207;&#21015;&#65288;&#25110;&#19978;&#19979;&#25991;&#65289; $c \le C$ &#30340;&#20301;&#32622; $t \le T$ &#22788;&#30340;&#23884;&#20837;&#21521;&#37327; $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$&#65292;&#25552;&#21462;&#22343;&#20540;&#25928;&#26524;&#24471;&#21040;&#20998;&#35299;&#24418;&#24335; \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] &#20854;&#20013; $\boldsymbol{\mu}$ &#26159;&#20840;&#23616;&#22343;&#20540;&#21521;&#37327;&#65292;$\mathbf{pos}_t$ &#21644; $\mathbf{ctx}_c$ &#20998;&#21035;&#26159;&#36328;&#19978;&#19979;&#25991;&#21644;&#36328;&#20301;&#32622;&#30340;&#22343;&#20540;&#21521;&#37327;&#65292;$\mathbf{resid}_{c,t}$ &#26159;&#27531;&#20313;&#21521;&#37327;&#12290;&#38024;&#23545;&#27969;&#34892;&#30340;transformer&#26550;&#26500;&#21644;&#22810;&#26679;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Transformers are widely used to extract complex semantic meanings from input tokens, yet they usually operate as black-box models. In this paper, we present a simple yet informative decomposition of hidden states (or embeddings) of trained transformers into interpretable components. For any layer, embedding vectors of input sequence samples are represented by a tensor $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$. Given embedding vector $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$ at sequence position $t \le T$ in a sequence (or context) $c \le C$, extracting the mean effects yields the decomposition \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] where $\boldsymbol{\mu}$ is the global mean vector, $\mathbf{pos}_t$ and $\mathbf{ctx}_c$ are the mean vectors across contexts and across positions respectively, and $\mathbf{resid}_{c,t}$ is the residual vector. For popular transformer architectures and diverse text datasets, empirica
&lt;/p&gt;</description></item><item><title>Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03052</link><description>&lt;p&gt;
Memoria: &#29992;&#20110;&#31867;&#20154;&#39034;&#24207;&#22788;&#29702;&#30340;&#28023;&#27604;&#23433;&#35760;&#24518;&#20307;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03052
&lt;/p&gt;
&lt;p&gt;
Memoria &#26159;&#19968;&#20010;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#24212;&#29992;&#28023;&#27604;&#23433;&#29702;&#35770;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#65292;Memoria &#22312;&#35832;&#22914; BERT &#21644; GPT &#20043;&#31867;&#30340;&#27969;&#34892; Transformer &#27169;&#22411;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#23481;&#37327;&#65292;Transformer &#24456;&#38590;&#22788;&#29702;&#38271;&#36755;&#20837;&#24207;&#21015;&#12290;&#34429;&#28982;&#22686;&#21152;&#36755;&#20837;&#38271;&#24230;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26080;&#27490;&#22659;&#22320;&#22686;&#21152;&#38271;&#24230;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#27492;&#22806;&#65292;&#19982; Transformer &#19981;&#21516;&#65292;&#20154;&#31867;&#26377;&#36873;&#25321;&#24615;&#22320;&#35760;&#20303;&#21644;&#20351;&#29992;&#20165;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#21040;&#23614;&#22788;&#29702;&#25152;&#26377;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Memoria&#65292;&#19968;&#20010;&#24212;&#29992;&#28023;&#27604;&#23433;&#35760;&#24518;&#24418;&#25104;&#29702;&#35770;&#30340;&#36890;&#29992;&#35760;&#24518;&#32593;&#32476;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#12290;Memoria &#22312;&#24037;&#20316;&#35760;&#24518;&#12289;&#30701;&#26399;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#30340;&#22810;&#20010;&#35760;&#24518;&#23618;&#32423;&#19978;&#23384;&#20648;&#21644;&#26816;&#32034;&#31216;&#20026; engram &#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#26681;&#25454;&#28023;&#24067;&#35268;&#21017;&#21464;&#21270;&#30340;&#36830;&#25509;&#26435;&#37325;&#12290;&#36890;&#36807;&#19982;&#35832;&#22914; BERT &#21644; GPT &#31561;&#27969;&#34892;&#30340;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986; Memoria &#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#32771;&#34385;&#38271;&#26399;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.02299</link><description>&lt;p&gt;
3D&#29289;&#29702;&#31995;&#32479;&#20013;&#23398;&#20064;&#23545;&#31216;&#24615;&#30772;&#32570;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31561;&#20215;&#27169;&#22411;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#23436;&#32654;&#23545;&#31216;&#24615;&#30340;&#20551;&#35774;&#26377;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#19982;&#36825;&#20123;&#23545;&#31216;&#24615;&#19981;&#23436;&#20840;&#19968;&#33268;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#20102;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#21367;&#31215;&#25216;&#26415;&#33021;&#22815;&#22312;&#20445;&#25345;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25581;&#31034;&#30456;&#21464;&#20013;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#65292;&#36824;&#21487;&#20197;&#22312;&#27969;&#20307;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AlignDiff&#65292;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#37327;&#21270;&#20154;&#31867;&#20559;&#22909;&#24182;&#25351;&#23548;&#38646;-shot&#34892;&#20026;&#23450;&#21046;&#30340;&#25193;&#25955;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#23558;&#20195;&#29702;&#34892;&#20026;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02054</link><description>&lt;p&gt;
AlignDiff: &#36890;&#36807;&#21487;&#23450;&#21046;&#34892;&#20026;&#25193;&#25955;&#27169;&#22411;&#23545;&#40784;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model. (arXiv:2310.02054v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AlignDiff&#65292;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#37327;&#21270;&#20154;&#31867;&#20559;&#22909;&#24182;&#25351;&#23548;&#38646;-shot&#34892;&#20026;&#23450;&#21046;&#30340;&#25193;&#25955;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#23558;&#20195;&#29702;&#34892;&#20026;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20195;&#29702;&#34892;&#20026;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#20173;&#28982;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#25277;&#35937;&#24615;&#21644;&#21487;&#21464;&#24615;&#30340;&#20869;&#22312;&#29305;&#24615;&#25152;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AlignDiff&#65292;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#37327;&#21270;&#20154;&#31867;&#20559;&#22909;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#25277;&#35937;&#24615;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20559;&#22909;&#26469;&#24341;&#23548;&#38646;-shot&#34892;&#20026;&#23450;&#21046;&#30340;&#25193;&#25955;&#35268;&#21010;&#65292;&#28085;&#30422;&#20102;&#21487;&#21464;&#24615;&#12290;AlignDiff&#33021;&#22815;&#20934;&#30830;&#21305;&#37197;&#29992;&#25143;&#23450;&#21046;&#30340;&#34892;&#20026;&#24182;&#39640;&#25928;&#22320;&#22312;&#19981;&#21516;&#34892;&#20026;&#20043;&#38388;&#20999;&#25442;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#22810;&#35282;&#24230;&#30340;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#23545;&#19981;&#21516;&#34892;&#20026;&#23646;&#24615;&#36827;&#34892;&#27604;&#36739;&#30340;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#23646;&#24615;&#24378;&#24230;&#27169;&#22411;&#26469;&#39044;&#27979;&#37327;&#21270;&#30340;&#30456;&#23545;&#24378;&#24230;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#24378;&#24230;&#37325;&#26032;&#26631;&#35760;&#34892;&#20026;&#25968;&#25454;&#38598;&#20043;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#35757;&#32451;&#20102;&#19968;&#20010;&#23646;&#24615;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#35268;&#21010;&#22120;&#65292;&#23646;&#24615;&#24378;&#24230;&#27169;&#22411;&#20316;&#20026;&#23548;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RL from Human Feedback (RLHF) to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. AlignDiff can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director fo
&lt;/p&gt;</description></item><item><title>Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00229</link><description>&lt;p&gt;
&#22312;&#35268;&#21010;&#20013;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00229
&lt;/p&gt;
&lt;p&gt;
Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#26377;&#24847;&#35782;&#35268;&#21010;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Skipper&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#25512;&#24191;&#22312;&#26032;&#24773;&#22659;&#20013;&#23398;&#21040;&#30340;&#25216;&#33021;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23427;&#33258;&#21160;&#23558;&#32473;&#23450;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#21487;&#31649;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#36825;&#20381;&#36182;&#20110;&#20174;&#22238;&#28335;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#20026;&#26377;&#21521;&#22270;&#30340;&#25277;&#35937;&#20195;&#29702;&#38382;&#39064;&#30340;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21738;&#20123;&#26041;&#38754;&#26377;&#26395;&#25552;&#20379;&#24110;&#21161;&#12290;&#38024;&#23545;&#27867;&#21270;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.10625</link><description>&lt;p&gt;
&#25506;&#32034;&#23398;&#20064;&#31995;&#32479;&#20013;&#20449;&#24687;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Influence of Information Entropy Change in Learning Systems. (arXiv:2309.10625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#36755;&#20837;/&#38544;&#21547;&#29305;&#24449;&#28155;&#21152;&#22122;&#22768;&#26469;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30340;&#24212;&#29992;&#37325;&#28857;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#21487;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;&#22122;&#22768;&#36890;&#24120;&#34987;&#35270;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#20197;&#21450;&#22270;&#20687;&#20998;&#31867;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#24605;&#32771;&#20256;&#32479;&#21629;&#39064;&#26159;&#21542;&#24635;&#26159;&#25104;&#31435;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#25552;&#21319;&#21508;&#31181;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20449;&#24687;&#29109;&#23450;&#20041;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#20943;&#23569;&#26041;&#38754;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27491;&#22122;&#22768;&#30340;&#22686;&#24378;&#25928;&#26524;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore the influence of entropy change in deep learning systems by adding noise to the inputs/latent features. The applications in this paper focus on deep learning tasks within computer vision, but the proposed theory can be further applied to other fields. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this paper aims to rethink whether the conventional proposition always holds. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the informat
&lt;/p&gt;</description></item><item><title>&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#27169;&#22411;&#21270;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#39044;&#27979;&#20102;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#31561;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10426</link><description>&lt;p&gt;
&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65306;&#36890;&#36807;&#22797;&#21512;&#23545;&#35937;&#21487;&#29992;&#24615;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances. (arXiv:2309.10426v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10426
&lt;/p&gt;
&lt;p&gt;
&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#27169;&#22411;&#21270;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#39044;&#27979;&#20102;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#31561;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#28145;&#20837;&#25506;&#35752;&#20102;&#21333;&#20010;&#25110;&#25104;&#23545;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#22312;&#35843;&#26597;&#30001;&#22797;&#26434;&#24418;&#29366;&#30340;&#20219;&#24847;&#25968;&#37327;&#30340;&#23545;&#35937;&#32452;&#25104;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#65292;&#23427;&#24314;&#27169;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#39044;&#27979;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#29616;&#26377;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#32473;&#23450;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#21253;&#25324;&#22534;&#21472;&#30340;&#29699;&#20307;&#21644;&#26479;&#23376;&#12289;&#26438;&#21644;&#21253;&#22260;&#26438;&#30340;&#29615;&#31561;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning object affordances is an effective tool in the field of robot learning. While the data-driven models delve into the exploration of affordances of single or paired objects, there is a notable gap in the investigation of affordances of compound objects that are composed of an arbitrary number of objects with complex shapes. In this study, we propose Multi-Object Graph Affordance Network (MOGAN) that models compound object affordances and predicts the effect of placing new objects on top of the existing compound. Given different tasks, such as building towers of specific heights or properties, we used a search based planning to find the sequence of stack actions with the objects of suitable affordances. We showed that our system was able to correctly model the affordances of very complex compound objects that include stacked spheres and cups, poles, and rings that enclose the poles. We demonstrated the applicability of our system in both simulated and real-world environments, com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.09357</link><description>&lt;p&gt;
Talk2Care: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20419;&#36827;&#24322;&#27493;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#36828;&#31243;&#21307;&#30103;&#24212;&#29992;&#31243;&#24207;&#26469;&#24110;&#21161;&#23478;&#24237;&#20013;&#30340;&#32769;&#24180;&#20154;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#65292;&#20294;&#22522;&#26412;&#30340;&#28040;&#24687;&#21644;&#30005;&#35805;&#20173;&#28982;&#26159;&#26368;&#24120;&#35265;&#30340;&#36890;&#20449;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#24615;&#12289;&#20449;&#24687;&#20002;&#22833;&#21644;&#27969;&#31243;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#20419;&#36827;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21450;&#20854;&#24378;&#22823;&#30340;&#33258;&#28982;&#23545;&#35805;&#21644;&#25688;&#35201;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLMs&#22312;&#36890;&#20449;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#36824;&#23384;&#22312;&#26377;&#38480;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#39033;&#35775;&#35848;&#30740;&#31350;&#65292;&#20998;&#21035;&#19982;&#32769;&#24180;&#20154;(N=10)&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;(N=9)&#36827;&#34892;&#20102;&#20132;&#27969;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#22312;&#24739;&#32773;-&#21307;&#29983;&#24322;&#27493;&#36890;&#20449;&#20013;&#23545;LLMs&#30340;&#38656;&#27714;&#21644;&#26426;&#20250;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;Talk2Care&#65292;&#24182;&#20026;&#20004;&#20010;&#32676;&#20307;&#35774;&#35745;&#20102;&#20132;&#20114;&#32452;&#20214;: (1) &#23545;&#20110;&#32769;&#24180;&#20154;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#38899;&#21161;&#25163;&#30340;&#20415;&#21033;&#24615;&#21644;&#26131;&#20110;&#33719;&#21462;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06364</link><description>&lt;p&gt;
&#22522;&#20110;&#26694;&#26550;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#30001;&#22238;&#31572;&#30340;&#23450;&#24615;&#20998;&#26512;&#65306;&#31639;&#27861;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#33258;&#30001;&#22238;&#31572;&#38754;&#35797;&#38382;&#39064;&#65292;&#23601;&#20687;&#20256;&#32479;&#19978;&#20351;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#20998;&#26512;&#30340;&#37027;&#26679;&#12290;&#23450;&#24615;&#26041;&#27861;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#28041;&#21450;&#23545;&#24320;&#25918;&#24335;&#35775;&#35848;&#25110;&#33258;&#30001;&#36827;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#30340;&#25163;&#21160;&#20998;&#26512;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#23450;&#24615;&#26041;&#27861;&#23545;LLMs&#29983;&#25104;&#30340;"&#30789;&#21442;&#19982;&#32773;"&#36827;&#34892;&#30740;&#31350;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#32676;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#27010;&#24565;&#26159;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#36825;&#26159;&#30001;Argyle&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24341;&#20837;&#30340;&#19968;&#20010;&#26415;&#35821;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20122;&#32676;&#20307;&#30340;&#20449;&#24565;&#21644;&#24577;&#24230;&#30340;&#31243;&#24230;&#30456;&#21563;&#21512;&#12290;&#26681;&#25454;&#23450;&#20041;&#65292;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#34920;&#26126;&#20174;LLMs&#20013;&#25552;&#21462;&#30340;&#28508;&#22312;&#20449;&#24565;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#65292;&#32780;&#20302;&#31639;&#27861;&#20445;&#30495;&#24230;&#21017;&#20351;&#24471;&#36825;&#26679;&#30340;&#30740;&#31350;&#26080;&#25928;&#12290;&#26412;&#25991;&#20351;&#29992;LLM&#29983;&#25104;&#38754;&#35797;&#38382;&#31572;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Today, using Large-scale generative Language Models (LLMs) it is possible to simulate free responses to interview questions like those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial "silicon participants" generated by LLMs may be productively studied using qualitative methods aiming to produce insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023) capturing the degree to which LLM-generated outputs mirror human sub-populations' beliefs and attitudes. By definition, high algorithmic fidelity suggests latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews wi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#30340;&#26102;&#38388;&#37329;&#23383;&#22612;&#21387;&#32553;&#21644;&#25918;&#22823;&#21464;&#25442;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#20934;&#30830;&#20272;&#35745;&#20154;&#20307;3D&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25193;&#23637;&#26102;&#38388;&#24314;&#27169;&#21644;&#32454;&#21270;&#29305;&#24449;&#20132;&#20114;&#26469;&#35299;&#20915;&#20854;&#20182;&#26041;&#27861;&#20013;&#30340;&#32454;&#33410;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.01365</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#26102;&#38388;&#37329;&#23383;&#22612;&#21387;&#32553;&#21644;&#25918;&#22823;&#21464;&#25442;&#22120;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation. (arXiv:2309.01365v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01365
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#30340;&#26102;&#38388;&#37329;&#23383;&#22612;&#21387;&#32553;&#21644;&#25918;&#22823;&#21464;&#25442;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#20934;&#30830;&#20272;&#35745;&#20154;&#20307;3D&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25193;&#23637;&#26102;&#38388;&#24314;&#27169;&#21644;&#32454;&#21270;&#29305;&#24449;&#20132;&#20114;&#26469;&#35299;&#20915;&#20854;&#20182;&#26041;&#27861;&#20013;&#30340;&#32454;&#33410;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#20934;&#30830;&#20272;&#35745;&#20154;&#20307;&#30340;3D&#23039;&#21183;&#38656;&#35201;&#20934;&#30830;&#24615;&#21644;&#33391;&#22909;&#30340;&#32467;&#26500;&#21270;&#26550;&#26500;&#12290;&#22522;&#20110;transformer&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20248;&#21270;&#30340;&#26102;&#38388;&#37329;&#23383;&#22612;&#21387;&#32553;&#21644;&#25918;&#22823;&#65288;RTPCA&#65289;&#21464;&#25442;&#22120;&#12290;RTPCA&#36890;&#36807;&#20854;&#26102;&#38388;&#37329;&#23383;&#22612;&#21387;&#32553;&#21644;&#25918;&#22823;&#65288;TPCA&#65289;&#32467;&#26500;&#25193;&#23637;&#20102;&#22359;&#20869;&#26102;&#38388;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#23618;&#32454;&#21270;&#65288;XLR&#65289;&#27169;&#22359;&#32454;&#21270;&#22359;&#38388;&#29305;&#24449;&#20132;&#20114;&#12290;&#29305;&#21035;&#22320;&#65292;TPCA&#22359;&#21033;&#29992;&#26102;&#38388;&#37329;&#23383;&#22612;&#33539;&#20363;&#65292;&#22686;&#24378;&#20851;&#38190;&#21644;&#20540;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20174;&#36816;&#21160;&#24207;&#21015;&#20013;&#26080;&#32541;&#25552;&#21462;&#31354;&#38388;&#35821;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;TPCA&#22359;&#19982;XLR&#36830;&#25509;&#36215;&#26469;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#26597;&#35810;&#12289;&#20851;&#38190;&#23383;&#21644;&#20540;&#30340;&#30456;&#20114;&#20316;&#29992;&#20419;&#36827;&#20016;&#23500;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#36825;&#31181;&#31574;&#30053;&#36890;&#36807;&#24403;&#21069;&#27969;&#31243;&#20307;&#29616;&#20102;&#26089;&#26399;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#20854;&#20182;&#22522;&#20110;Transformer&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#32454;&#33410;&#21644;&#31283;&#23450;&#24615;&#19981;&#36275;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#21518;&#30340;&#27169;&#22411;&#22312;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately estimating the 3D pose of humans in video sequences requires both accuracy and a well-structured architecture. With the success of transformers, we introduce the Refined Temporal Pyramidal Compression-and-Amplification (RTPCA) transformer. Exploiting the temporal dimension, RTPCA extends intra-block temporal modeling via its Temporal Pyramidal Compression-and-Amplification (TPCA) structure and refines inter-block feature interaction with a Cross-Layer Refinement (XLR) module. In particular, TPCA block exploits a temporal pyramid paradigm, reinforcing key and value representation capabilities and seamlessly extracting spatial semantics from motion sequences. We stitch these TPCA blocks with XLR that promotes rich semantic representation through continuous interaction of queries, keys, and values. This strategy embodies early-stage information with current flows, addressing typical deficits in detail and stability seen in other transformer-based methods. We demonstrate the eff
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#31454;&#20105;&#36873;&#25321;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#30340;&#25968;&#23398;&#24418;&#24335;&#21644;&#23454;&#29616;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.16262</link><description>&lt;p&gt;
&#26377;&#31454;&#20105;&#36873;&#25321;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Strategic Learning with Competitive Selection. (arXiv:2308.16262v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16262
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#31454;&#20105;&#36873;&#25321;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#30340;&#25968;&#23398;&#24418;&#24335;&#21644;&#23454;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#20915;&#31574;&#32773;&#19979;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20854;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#30001;&#20195;&#29702;&#20154;&#35780;&#20272;&#21644;&#36873;&#25321;&#32452;&#25104;&#30340;&#36873;&#25321;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#20043;&#21069;&#30740;&#31350;&#20013;&#20851;&#27880;&#30340;&#22266;&#23450;&#20195;&#29702;&#20154;&#27744;&#12290;&#24403;&#27599;&#20010;&#20915;&#31574;&#32773;&#36890;&#36807;&#26368;&#22823;&#21270;&#33258;&#36523;&#25928;&#29992;&#26469;&#21333;&#26041;&#38754;&#36873;&#25321;&#20195;&#29702;&#20154;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20339;&#30340;&#36873;&#25321;&#35268;&#21017;&#26159;&#22312;&#36873;&#25321;&#26368;&#20339;&#20195;&#29702;&#20154;&#21644;&#25552;&#20379;&#28608;&#21169;&#20197;&#26368;&#22823;&#21270;&#20195;&#29702;&#20154;&#25913;&#36827;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#20381;&#36182;&#20110;&#20195;&#29702;&#20154;&#32467;&#26524;&#30340;&#38169;&#35823;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20915;&#31574;&#32773;&#30340;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#19981;&#20250;&#23548;&#33268;&#20195;&#29702;&#20154;&#32467;&#26524;&#24694;&#21270;&#65292;&#20063;&#19981;&#20250;&#36896;&#25104;&#19981;&#20844;&#27491;&#30340;&#38477;&#20302;&#20195;&#29702;&#20154;&#36873;&#25321;&#26426;&#20250;&#30340;&#26465;&#20214;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#30340;&#25968;&#23398;&#24418;&#24335;&#21644;&#19968;&#31181;&#23454;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of agent selection in causal strategic learning under multiple decision makers and address two key challenges that come with it. Firstly, while much of prior work focuses on studying a fixed pool of agents that remains static regardless of their evaluations, we consider the impact of selection procedure by which agents are not only evaluated, but also selected. When each decision maker unilaterally selects agents by maximising their own utility, we show that the optimal selection rule is a trade-off between selecting the best agents and providing incentives to maximise the agents' improvement. Furthermore, this optimal selection rule relies on incorrect predictions of agents' outcomes. Hence, we study the conditions under which a decision maker's optimal selection rule will not lead to deterioration of agents' outcome nor cause unjust reduction in agents' selection chance. To that end, we provide an analytical form of the optimal selection rule and a mechanism to r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VIGC&#26694;&#26550;&#65292;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21644;&#32416;&#27491;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#39640;&#36136;&#37327;&#35843;&#25972;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12714</link><description>&lt;p&gt;
VIGC: &#35270;&#35273;&#25351;&#20196;&#29983;&#25104;&#19982;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
VIGC: Visual Instruction Generation and Correction. (arXiv:2308.12714v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VIGC&#26694;&#26550;&#65292;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21644;&#32416;&#27491;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#39640;&#36136;&#37327;&#35843;&#25972;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25972;&#21512;&#25512;&#21160;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#30340;&#31232;&#32570;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#20027;&#23548;&#33539;&#24335;&#65292;&#22914;LLaVA&#65292;&#20381;&#36182;&#20110;&#20165;&#20351;&#29992;&#35821;&#35328;&#30340;GPT-4&#29983;&#25104;&#25968;&#25454;&#65292;&#36825;&#38656;&#35201;&#39044;&#27880;&#37322;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#26816;&#27979;&#21253;&#22260;&#26694;&#65292;&#23548;&#33268;&#23545;&#22270;&#20687;&#32454;&#33410;&#30340;&#29702;&#35299;&#19981;&#36275;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#23454;&#38469;&#26041;&#26696;&#26159;&#21033;&#29992;&#21487;&#29992;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#29983;&#25104;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#21069;&#21487;&#35775;&#38382;&#30340;MLLMs&#19981;&#20687;&#23427;&#20204;&#30340;LLM&#23545;&#24212;&#29289;&#37027;&#26679;&#24378;&#22823;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#20135;&#29983;&#19981;&#36866;&#24403;&#30340;&#22238;&#24212;&#21644;&#29983;&#25104;&#38169;&#35823;&#20449;&#24687;&#12290;&#20316;&#20026;&#35299;&#20915;&#24403;&#21069;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Visual Instruction Generation and Correction&#65288;VIGC&#65289;&#26694;&#26550;&#65292;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#24182;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11129</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#36317;&#31163;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#21387;&#22120;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#35758;&#24456;&#23569;&#28041;&#21450;&#25429;&#25417;&#26356;&#38271;&#36317;&#31163;&#12289;&#23618;&#27425;&#32467;&#26500;&#25110;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#22312;&#20998;&#23376;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#21508;&#31181;&#22270;&#24418;&#20013;&#37117;&#20250;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#36317;&#31163;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22810;&#23618;&#27425;&#12289;&#23618;&#27425;&#21270;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#20197;&#28789;&#27963;&#19982;&#29616;&#26377;&#22270;&#21464;&#21387;&#22120;&#38598;&#25104;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20301;&#32622;&#34920;&#31034;&#21516;&#26102;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;HDSE&#26041;&#27861;&#25104;&#21151;&#25552;&#21319;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#32447;&#21464;&#21387;&#22120;&#65292;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07037</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;BFNs&#20013;&#65292;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#20250;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#24433;&#21709;&#19979;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#20462;&#25913;&#65292;&#28982;&#21518;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#20174;&#31616;&#21333;&#30340;&#20808;&#39564;&#24320;&#22987;&#65292;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#36825;&#20004;&#20010;&#20998;&#24067;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#30340;&#29983;&#25104;&#36807;&#31243;&#65307;&#19981;&#36807;&#65292;&#36825;&#20010;&#36807;&#31243;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#26080;&#38656;&#21069;&#21521;&#36807;&#31243;&#12290;&#23545;&#20110;&#36830;&#32493;&#12289;&#31163;&#25955;&#21270;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#25512;&#23548;&#20986;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#31163;&#25955;&#25968;&#25454;&#65292;&#32593;&#32476;&#30340;&#36755;&#20837;&#20301;&#20110;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#65292;&#22240;&#27492;&#26412;&#36136;&#19978;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#26679;&#26412;&#24341;&#23548;&#21644;&#22312;&#35821;&#35328;&#24314;&#27169;&#31561;&#31163;&#25955;&#39046;&#22495;&#36827;&#34892;&#23569;&#37327;&#27493;&#39588;&#29983;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#20102;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#19988;&#19981;&#25918;&#32622;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#25991;&#31456;&#26088;&#22312;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#20197;&#21450;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05502</link><description>&lt;p&gt;
&#23558;&#39034;&#24207;&#24102;&#20837;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#27861;&#24459;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bringing order into the realm of Transformer-based language models for artificial intelligence and law. (arXiv:2308.05502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#25991;&#31456;&#26088;&#22312;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#20197;&#21450;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;TLM&#65289;&#34987;&#24191;&#27867;&#35748;&#21487;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#25104;&#21151;&#24320;&#21457;&#20986;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#21644;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#25991;&#26412;&#39046;&#22495;&#19968;&#26679;&#65292;TLM&#30830;&#23454;&#25512;&#21160;&#20102;&#27861;&#24459;&#39046;&#22495;&#35768;&#22810;&#24863;&#20852;&#36259;&#20219;&#21153;&#23545;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23613;&#31649;&#31532;&#19968;&#20010;Transformer&#27169;&#22411;&#25552;&#20986;&#20102;&#22823;&#32422;6&#24180;&#26102;&#38388;&#65292;&#20294;&#36825;&#39033;&#25216;&#26415;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#36805;&#29467;&#21457;&#23637;&#65292;BERT&#21644;&#30456;&#20851;&#27169;&#22411;&#25104;&#20026;&#20027;&#35201;&#21442;&#32771;&#65292;&#20063;&#22312;&#27861;&#24459;&#39046;&#22495;&#21344;&#26377;&#37325;&#35201;&#22320;&#20301;&#12290;&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#27010;&#36848;&#20102;TLM&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#31361;&#20986;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#20197;&#20415;&#19968;&#26041;&#38754;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#21462;&#24471;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#26159;&#20160;&#20040;&#65292;&#21478;&#19968;&#26041;&#38754;&#20102;&#35299;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#26159;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20113;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#30340;&#20316;&#26354;&#36741;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.04215</link><description>&lt;p&gt;
&#23454;&#26102;&#20316;&#26354;&#36741;&#21161;&#30340;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04215
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20113;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#30340;&#20316;&#26354;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#22312;&#25552;&#21319;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#25972;&#21512;&#31169;&#20154;&#25968;&#25454;&#21644;&#20943;&#23569;&#24187;&#35273;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#20110;&#38656;&#35201;&#23454;&#26102;&#21709;&#24212;&#30340;&#20219;&#21153;&#65288;&#22914;&#20316;&#26354;&#36741;&#21161;&#65289;&#26102;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#22788;&#29702;&#26102;&#38388;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hybrid Retrieval-Augmented Generation (HybridRAG)&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#23558;&#23458;&#25143;&#31471;&#27169;&#22411;&#21644;&#20113;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#28151;&#21512;&#35774;&#32622;&#12290;HybridRAG&#36890;&#36807;&#24322;&#27493;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20113;&#31471;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#25928;&#30340;&#21709;&#24212;&#65292;&#20174;LLM&#30340;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24322;&#27493;&#20869;&#23384;&#38598;&#25104;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#33021;&#22815;&#23454;&#26102;&#21709;&#24212;&#29992;&#25143;&#35831;&#27714;&#65292;&#26080;&#38656;&#31561;&#24453;&#20113;&#31471;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmented models show promise in enhancing traditional language models by improving their contextual understanding, integrating private data, and reducing hallucination. However, the processing time required for retrieval augmented large language models poses a challenge when applying them to tasks that require real-time responses, such as composition assistance.  To overcome this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework that leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01222</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#65306;&#26368;&#26032;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#21487;&#38752;&#12289;&#40065;&#26834;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#26657;&#20934;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#29702;&#24819;&#30340;&#28145;&#24230;&#27169;&#22411;&#19981;&#20165;&#24212;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#24212;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#20351;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#28145;&#24230;&#27169;&#22411;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#25191;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21407;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#26657;&#20934;&#30340;&#23450;&#20041;&#24320;&#22987;&#65292;&#35299;&#37322;&#20102;&#27169;&#22411;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#26657;&#20934;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20123;&#26657;&#20934;&#26041;&#27861;&#30340;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
&lt;/p&gt;</description></item><item><title>DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.10172</link><description>&lt;p&gt;
DialogStudio&#65306;&#38754;&#21521;&#20250;&#35805; AI &#30340;&#26368;&#20016;&#23500;&#21644;&#26368;&#22810;&#26679;&#21270;&#30340;&#32479;&#19968;&#25968;&#25454;&#38598;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. (arXiv:2307.10172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10172
&lt;/p&gt;
&lt;p&gt;
DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20250;&#35805; AI &#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#20219;&#21153;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#20840;&#38754;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; DialogStudio&#65306;&#26368;&#22823;&#12289;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#20197;&#19968;&#33268;&#30340;&#26684;&#24335;&#32479;&#19968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#21407;&#22987;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#38598;&#21512;&#21253;&#25324;&#26469;&#33258;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12289;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#65292;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#38750;&#24120;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378; DialogStudio &#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30830;&#23450;&#20102;&#35768;&#21487;&#35777;&#65292;&#24182;&#20026;&#36873;&#23450;&#23545;&#35805;&#35774;&#35745;&#20102;&#39046;&#22495;&#24863;&#30693;&#25552;&#31034;&#65292;&#20197;&#20415;&#20419;&#36827;&#25351;&#23548;&#24863;&#30693;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#38598;&#38598;&#21512;&#24320;&#21457;&#20102;&#20250;&#35805; AI &#27169;&#22411;&#65292;&#24182;&#22312;&#38646;&#25688;&#35201;&#29983;&#25104;&#21644;&#20998;&#24067;&#24335;&#25991;&#23383;&#22522;&#20934;&#23545;&#35805;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training. To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-sh
&lt;/p&gt;</description></item><item><title>HAT-CL&#26159;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;&#65292;&#20197;&#25552;&#20379;&#23545;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#25913;&#21892;HAT&#30340;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#65292;&#23454;&#29616;&#20102;&#23545;PyTorch&#27169;&#22359;&#30340;&#33258;&#21160;&#21270;&#26799;&#24230;&#25805;&#20316;&#21644;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.09653</link><description>&lt;p&gt;
HAT-CL: &#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;
&lt;/p&gt;
&lt;p&gt;
HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09653
&lt;/p&gt;
&lt;p&gt;
HAT-CL&#26159;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;&#65292;&#20197;&#25552;&#20379;&#23545;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#25913;&#21892;HAT&#30340;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#65292;&#23454;&#29616;&#20102;&#23545;PyTorch&#27169;&#22359;&#30340;&#33258;&#21160;&#21270;&#26799;&#24230;&#25805;&#20316;&#21644;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20007;&#22833;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#32473;&#20154;&#20204;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#30828;&#27880;&#24847;&#21147;&#20219;&#21153;(HAT)&#26426;&#21046;&#22312;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20854;&#23454;&#38469;&#23454;&#29616;&#21463;&#21040;&#20102;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HAT-CL&#65292;&#36825;&#26159;HAT&#26426;&#21046;&#30340;&#29992;&#25143;&#21451;&#22909;&#12289;&#19982;PyTorch&#20860;&#23481;&#30340;&#37325;&#26032;&#35774;&#35745;&#12290;HAT-CL&#19981;&#20165;&#33258;&#21160;&#21270;&#20102;&#26799;&#24230;&#25805;&#20316;&#65292;&#36824;&#31616;&#21270;&#20102;PyTorch&#27169;&#22359;&#36716;&#21270;&#20026;HAT&#27169;&#22359;&#30340;&#36807;&#31243;&#12290;&#23427;&#36890;&#36807;&#25552;&#20379;&#19968;&#22871;&#20840;&#38754;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#26550;&#26500;&#20013;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#25552;&#20379;&#20102;&#19982;TIMM&#24211;&#24179;&#28369;&#38598;&#25104;&#30340;&#21487;&#29992;&#30340;HAT&#32593;&#32476;&#12290;&#38500;&#20102;&#23545;HAT&#30340;&#37325;&#26032;&#35774;&#35745;&#21644;&#37325;&#26032;&#23454;&#29616;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#29992;&#20110;HAT&#30340;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#21644;&#19987;&#38376;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08430</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#36890;&#36807;&#28176;&#36827;&#25277;&#26679;&#36827;&#34892;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks. (arXiv:2307.08430v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#21644;&#19987;&#38376;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31243;&#20381;&#36182;&#30340;&#21033;&#29992;&#22312;&#21516;&#36136;&#22270;&#20013;&#26377;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#24456;&#23569;&#30740;&#31350;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#39640;&#25104;&#26412;&#21644;&#26377;&#25928;&#20449;&#24687;&#21033;&#29992;&#30340;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#20803;&#36335;&#24452;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#33258;&#21160;&#26694;&#26550;&#65292;&#31216;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#21508;&#31181;&#25968;&#25454;&#38598;&#25110;&#20219;&#21153;&#30340;&#20803;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#25152;&#26377;&#30446;&#26631;&#33410;&#28857;&#30456;&#20851;&#20803;&#36335;&#24452;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#36890;&#36807;&#28176;&#36827;&#25277;&#26679;&#31639;&#27861;&#65292;&#25105;&#20204;&#21160;&#24577;&#22320;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#20197;&#36339;&#25968;&#26080;&#20851;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#39537;&#21160;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#30001;&#24403;&#21069;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#21644;&#20219;&#21153;&#39537;&#21160;&#30340;&#32039;&#20945;&#25628;&#32034;&#31354;&#38388;&#12290;&#21033;&#29992;&#25277;&#26679;&#35780;&#20272;&#31574;&#30053;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19987;&#38376;&#21644;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#12290;&#23545;&#20843;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing long-range dependency, though extensively studied in homogeneous graphs, is rarely studied in large-scale heterogeneous information networks (HINs), whose main challenge is the high costs and the difficulty in utilizing effective information. To this end, we investigate the importance of different meta-paths and propose an automatic framework for utilizing long-range dependency in HINs, called Long-range Meta-path Search through Progressive Sampling (LMSPS). Specifically, to discover meta-paths for various datasets or tasks without prior, we develop a search space with all target-node-related meta-paths. With a progressive sampling algorithm, we dynamically shrink the search space with hop-independent time complexity, leading to a compact search space driven by the current HIN and task. Utilizing a sampling evaluation strategy as the guidance, we conduct a specialized and expressive meta-path selection. Extensive experiments on eight heterogeneous datasets demonstrate that LM
&lt;/p&gt;</description></item><item><title>DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.06924</link><description>&lt;p&gt;
DRAGON: &#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#24102;&#26377;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#30340;&#36741;&#21161;&#23548;&#33322;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06924
&lt;/p&gt;
&lt;p&gt;
DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#29702;&#35299;&#21644;&#23548;&#33322;&#21608;&#22260;&#31354;&#38388;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#23548;&#33322;&#25216;&#26415;&#35201;&#20040;&#21482;&#20851;&#27880;&#23548;&#33322;&#65292;&#35201;&#20040;&#25552;&#20379;&#26377;&#38480;&#30340;&#20851;&#20110;&#29615;&#22659;&#30340;&#27807;&#36890;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#21644;&#35821;&#20041;&#23548;&#33322;&#26041;&#38754;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGON&#65292;&#19968;&#31181;&#30001;&#23545;&#35805;&#31995;&#32479;&#39537;&#21160;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#24182;&#20855;&#26377;&#23558;&#29615;&#22659;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;DRAGON&#33021;&#22815;&#24341;&#23548;&#29992;&#25143;&#21040;&#22320;&#22270;&#19978;&#30340;&#30446;&#26631;&#22320;&#26631;&#65292;&#25551;&#36848;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#23519;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#23545;&#35805;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#33258;&#30001;&#24418;&#24335;&#25551;&#36848;&#19982;&#29615;&#22659;&#20013;&#30340;&#22320;&#26631;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#25552;&#20379;&#35821;&#20041;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#26085;&#24120;&#23460;&#20869;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#30450;&#30446;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DRAGON&#33021;&#22815;&#19982;&#29992;&#25143;&#39034;&#30021;&#22320;&#27807;&#36890;&#65292;
&lt;/p&gt;
&lt;p&gt;
Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FACTOR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20107;&#23454;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#19981;&#27491;&#30830;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#22312;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#26102;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#12290;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#19981;&#24635;&#26159;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2307.06908</link><description>&lt;p&gt;
&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Generating Benchmarks for Factuality Evaluation of Language Models. (arXiv:2307.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06908
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FACTOR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20107;&#23454;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#19981;&#27491;&#30830;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#22312;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#26102;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#12290;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#19981;&#24635;&#26159;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#37096;&#32626;&#21040;&#29305;&#23450;&#39046;&#22495;&#20043;&#21069;&#65292;&#34913;&#37327;&#20854;&#22312;&#35813;&#39046;&#22495;&#20013;&#29983;&#25104;&#20107;&#23454;&#38169;&#35823;&#20449;&#24687;&#30340;&#20542;&#21521;&#24456;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#29983;&#25104;&#35780;&#20272;&#26041;&#27861;&#38598;&#20013;&#20110;&#20174;LM&#33258;&#36523;&#20013;&#37319;&#26679;&#30340;&#20107;&#23454;&#65292;&#22240;&#27492;&#26080;&#27861;&#25511;&#21046;&#35780;&#20272;&#20107;&#23454;&#30340;&#38598;&#21512;&#65292;&#24182;&#19988;&#21487;&#33021;&#20302;&#20272;&#20102;&#32597;&#35265;&#21644;&#19981;&#22826;&#21487;&#33021;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FACTOR&#65306;&#36890;&#36807;&#35821;&#26009;&#24211;&#21464;&#25442;&#36827;&#34892;&#20107;&#23454;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;LM&#30340;&#20107;&#23454;&#24615;&#12290;FACTOR&#20250;&#33258;&#21160;&#23558;&#24863;&#20852;&#36259;&#30340;&#20107;&#23454;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;LM&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#31867;&#20284;&#20294;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#21019;&#24314;&#20102;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;Wiki-FACTOR&#21644;News-FACTOR&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#24182;&#19988;&#24403;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65307;&#65288;ii&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#19982;&#22256;&#24785;&#24230;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#36825;&#20004;&#20010;&#25351;&#26631;&#22312;&#27169;&#22411;&#25490;&#24207;&#19978;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#24403;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#26356;&#33021;&#20934;&#30830;&#21453;&#26144;LM&#30340;&#20107;&#23454;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14275</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#22686;&#24378;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#31616;&#21333;&#30340;&#23545;&#25239;&#35757;&#32451;&#36973;&#21463;&#20102;&#20196;&#20154;&#30031;&#32553;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23548;&#33268;&#40065;&#26834;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#22914;&#39069;&#22806;&#30340;&#35268;&#33539;&#21270;&#12289;&#23545;&#25239;&#26435;&#37325;&#25200;&#21160;&#21644;&#26356;&#22810;&#25968;&#25454;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#40065;&#26834;&#27867;&#21270;&#30340;&#25913;&#36827;&#20173;&#28982;&#36828;&#19981;&#29702;&#24819;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20840;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;--&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#30340;&#31934;&#32454;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#20248;&#21270;&#36712;&#36857;&#22312;&#26102;&#38388;&#19978;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;WOT&#22312;&#21508;&#31181;&#26368;&#26032;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;WOT&#19982;&#29616;&#26377;&#26041;&#27861;&#23436;&#32654;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13588</link><description>&lt;p&gt;
&#31995;&#32479;&#32423;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
System-Level Natural Language Feedback. (arXiv:2306.13588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#29992;&#25143;&#20307;&#39564;&#20449;&#24687;&#12290;&#29616;&#26377;&#30740;&#31350;&#32858;&#28966;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#21453;&#39304;&#29992;&#20110;&#32454;&#21270;&#29305;&#23450;&#20363;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#31995;&#32479;&#33539;&#22260;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#26159;&#36890;&#36807;&#20197;&#19979;&#20004;&#26041;&#38754;&#23454;&#29616;&#30340;&#65306;(i) &#20219;&#21153;&#24230;&#37327;&#35774;&#35745;; (ii) &#29992;&#20110;&#25913;&#36827;&#27169;&#22411;&#21709;&#24212;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#26597;&#35810;&#29983;&#25104;&#21644;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#32452;&#21512;&#24102;&#26469;&#20102;&#36827;&#19968;&#27493;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#23548;&#33268;&#27604;GPT-3.5&#25776;&#20889;&#30340;&#21453;&#39304;&#26356;&#21152;&#25166;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language (NL) feedback contains rich information about the user experience. Existing studies focus on an instance-level approach, where feedback is used to refine specific examples, disregarding its system-wide application. This paper proposes a general framework for unlocking the system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process -- in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query generation and dialog response generation, demonstrating the effectiveness of the use of system-level feedback. We show the combination of system-level feedback and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying
&lt;/p&gt;</description></item><item><title>TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13339</link><description>&lt;p&gt;
TrustGuard: &#22522;&#20110;GNN&#30340;&#21160;&#24577;&#25903;&#25345;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#20449;&#20219;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13339
&lt;/p&gt;
&lt;p&gt;
TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#35780;&#20272;&#35780;&#20272;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#24182;&#20419;&#36827;&#20915;&#31574;&#12290;&#26426;&#22120;&#23398;&#20064;&#30001;&#20110;&#20854;&#23398;&#20064;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#23545;&#20449;&#20219;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#20854;&#29992;&#20110;&#20449;&#20219;&#35780;&#20272;&#65292;&#22240;&#20026;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#21487;&#20197;&#24314;&#27169;&#20026;&#22270;&#24418;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;GNN&#30340;&#24403;&#21069;&#20449;&#20219;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#23436;&#20840;&#28385;&#36275;&#20449;&#20219;&#30340;&#21160;&#24577;&#24615;&#65292;&#24573;&#30053;&#20102;&#25915;&#20987;&#23545;&#20449;&#20219;&#35780;&#20272;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#20449;&#26381;&#30340;&#35780;&#20272;&#32467;&#26524;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustGuard &#65306;&#19968;&#31181;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#12289;&#25239;&#20987;&#40065;&#26834;&#19988;&#36890;&#36807;&#21487;&#35270;&#21270;&#25552;&#20379;&#35299;&#37322;&#30340;&#31934;&#30830;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TrustGuard &#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#21160;&#24577;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#23618;&#12289;&#22270;&#21367;&#31215;&#23618;&#12289;&#27880;&#24847;&#26426;&#21046;&#23618;&#21644;&#20449;&#20219;&#39044;&#27979;&#23618;&#32452;&#25104;&#30340;&#20998;&#23618;&#26550;&#26500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;TrustGuard&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TrustGuard &#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#29983;&#25104;&#24335; AI &#24037;&#20855; ChatGPT &#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23637;&#31034;&#32929;&#31080;&#24066;&#22330;&#30456;&#20851;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20449;&#24687;&#33192;&#32960;&#25351;&#26631;&#24182;&#35777;&#26126;&#20854;&#19982;&#36127;&#38754;&#30340;&#36164;&#26412;&#24066;&#22330;&#21518;&#26524;&#30456;&#20851;&#65292;&#21516;&#26102;&#23637;&#31034;&#20854;&#22312;&#26500;&#24314;&#38024;&#23545;&#24615;&#24635;&#32467;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10224</link><description>&lt;p&gt;
&#33192;&#32960;&#30340;&#25259;&#38706;&#65306;ChatGPT&#26159;&#21542;&#33021;&#24110;&#21161;&#25237;&#36164;&#32773;&#22788;&#29702;&#36130;&#21153;&#20449;&#24687;&#65311;
&lt;/p&gt;
&lt;p&gt;
Bloated Disclosures: Can ChatGPT Help Investors Process Financial Information?. (arXiv:2306.10224v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10224
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#29983;&#25104;&#24335; AI &#24037;&#20855; ChatGPT &#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23637;&#31034;&#32929;&#31080;&#24066;&#22330;&#30456;&#20851;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20449;&#24687;&#33192;&#32960;&#25351;&#26631;&#24182;&#35777;&#26126;&#20854;&#19982;&#36127;&#38754;&#30340;&#36164;&#26412;&#24066;&#22330;&#21518;&#26524;&#30456;&#20851;&#65292;&#21516;&#26102;&#23637;&#31034;&#20854;&#22312;&#26500;&#24314;&#38024;&#23545;&#24615;&#24635;&#32467;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335; AI &#24037;&#20855;&#65288;&#22914; ChatGPT&#65289;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#25237;&#36164;&#32773;&#22788;&#29702;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#32929;&#31080;&#24066;&#22330;&#20316;&#20026;&#23454;&#39564;&#23460;&#65292;&#25506;&#31350;&#36825;&#20123;&#24037;&#20855;&#22312;&#24635;&#32467;&#22797;&#26434;&#30340;&#20844;&#21496;&#25259;&#38706;&#20449;&#24687;&#26102;&#30340;&#32463;&#27982;&#25928;&#29992;&#12290;&#24635;&#32467;&#25688;&#35201;&#26126;&#26174;&#26356;&#30701;&#65292;&#36890;&#24120;&#27604;&#21407;&#22987;&#25991;&#26412;&#32553;&#30701;&#36229;&#36807; 70%&#65292;&#32780;&#20449;&#24687;&#20869;&#23481;&#24471;&#21040;&#22686;&#24378;&#12290;&#24403;&#19968;&#20221;&#25991;&#20214;&#20855;&#26377;&#31215;&#26497;&#65288;&#28040;&#26497;&#65289;&#24773;&#24863;&#26102;&#65292;&#20854;&#24635;&#32467;&#21464;&#24471;&#26356;&#31215;&#26497;&#65288;&#28040;&#26497;&#65289;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24635;&#32467;&#23545;&#35299;&#37322;&#32929;&#24066;&#23545;&#25259;&#38706;&#20449;&#24687;&#30340;&#21453;&#24212;&#26356;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#8220;&#33192;&#32960;&#8221;&#25351;&#26631;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#33192;&#32960;&#30340;&#25259;&#38706;&#19982;&#36127;&#38754;&#30340;&#36164;&#26412;&#24066;&#22330;&#21518;&#26524;&#30456;&#20851;&#65292;&#20363;&#22914;&#26356;&#20302;&#30340;&#20215;&#26684;&#26377;&#25928;&#24615;&#21644;&#26356;&#39640;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#22312;&#26500;&#24314;&#38024;&#23545;&#24615;&#24635;&#32467;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#30830;&#23450;&#20844;&#21496;&#30340;&#65288;&#38750;&#65289;&#36130;&#21153;&#34920;&#29616;&#21644;&#39118;&#38505;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687; ChatGPT &#36825;&#26679;&#30340;&#29983;&#25104;&#24335; AI &#24037;&#20855;&#21487;&#20197;&#26377;&#25928;&#22320;&#24110;&#21161;&#25237;&#36164;&#32773;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#36130;&#21153;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI tools such as ChatGPT can fundamentally change the way investors process information. We probe the economic usefulness of these tools in summarizing complex corporate disclosures using the stock market as a laboratory. The unconstrained summaries are dramatically shorter, often by more than 70% compared to the originals, whereas their information content is amplified. When a document has a positive (negative) sentiment, its summary becomes more positive (negative). More importantly, the summaries are more effective at explaining stock market reactions to the disclosed information. Motivated by these findings, we propose a measure of information "bloat." We show that bloated disclosure is associated with adverse capital markets consequences, such as lower price efficiency and higher information asymmetry. Finally, we show that the model is effective at constructing targeted summaries that identify firms' (non-)financial performance and risks. Collectively, our results indi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$DaL$&#30340;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#24230;&#37197;&#32622;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06651</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#21106;&#23398;&#20064;&#39044;&#27979;&#36719;&#20214;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predicting Software Performance with Divide-and-Learn. (arXiv:2306.06651v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$DaL$&#30340;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#24230;&#37197;&#32622;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#39640;&#24230;&#37197;&#32622;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24615;&#33021;&#26159;&#24615;&#33021;&#27979;&#35797;&#21644;&#36136;&#37327;&#20445;&#35777;&#30340;&#22522;&#30784;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20381;&#38752;&#26426;&#22120;/&#28145;&#24230;&#23398;&#20064;&#26469;&#24314;&#27169;&#36719;&#20214;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#28385;&#36275;&#37197;&#32622;&#26223;&#35266;&#20013;&#32487;&#25215;&#30340;&#31232;&#30095;&#24615;&#65306;&#37197;&#32622;&#36873;&#39033;&#65288;&#29305;&#24449;&#65289;&#30340;&#24433;&#21709;&#21644;&#25968;&#25454;&#26679;&#26412;&#30340;&#20998;&#24067;&#37117;&#38750;&#24120;&#31232;&#30095;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#20998;&#21106;&#23398;&#20064;&#8221;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;$DaL$&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#20026;&#20102;&#22788;&#29702;&#26679;&#26412;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#23558;&#37197;&#32622;&#26223;&#35266;&#20013;&#30340;&#26679;&#26412;&#21010;&#20998;&#20026;&#36828;&#31163;&#30340;&#37096;&#20998;&#65292;&#23545;&#20110;&#27599;&#20010;&#37096;&#20998;&#65292;&#25105;&#20204;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26412;&#22320;&#27169;&#22411;&#26469;&#22788;&#29702;&#29305;&#24449;&#31232;&#30095;&#24615;&#12290;&#28982;&#21518;&#65292;&#26032;&#32473;&#23450;&#30340;&#37197;&#32622;&#23558;&#34987;&#20998;&#37197;&#32473;&#26368;&#32456;&#39044;&#27979;&#30340;&#27491;&#30830;&#27169;&#22411;&#12290;&#20843;&#20010;&#30495;&#23454;&#31995;&#32479;&#21644;&#20116;&#32452;&#35757;&#32451;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Predicting the performance of highly configurable software systems is the foundation for performance testing and quality assurance. To that end, recent work has been relying on machine/deep learning to model software performance. However, a crucial yet unaddressed challenge is how to cater for the sparsity inherited from the configuration landscape: the influence of configuration options (features) and the distribution of data samples are highly sparse.  In this paper, we propose an approach based on the concept of 'divide-and-learn', dubbed $DaL$. The basic idea is that, to handle sample sparsity, we divide the samples from the configuration landscape into distant divisions, for each of which we build a regularized Deep Neural Network as the local model to deal with the feature sparsity. A newly given configuration would then be assigned to the right model of division for the final prediction.  Experiment results from eight real-world systems and five sets of training data reveal that
&lt;/p&gt;</description></item><item><title>STEVE-1 &#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;Minecraft&#20013;&#36319;&#38543;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.00937</link><description>&lt;p&gt;
STEVE-1: &#19968;&#20010;&#29992;&#20110;Minecraft&#20013;&#25991;&#26412;-&#34892;&#20026;&#29983;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. (arXiv:2306.00937v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00937
&lt;/p&gt;
&lt;p&gt;
STEVE-1 &#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;Minecraft&#20013;&#36319;&#38543;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#23545;&#25991;&#26412;&#25351;&#20196;&#20570;&#20986;&#21709;&#24212;&#30340;AI&#27169;&#22411;&#23545;&#20110;&#36830;&#32493;&#24615;&#20915;&#31574;&#20219;&#21153;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;STEVE-1&#30340;Minecraft&#25351;&#20196;&#35843;&#25972;&#22411;&#35270;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;DALL-E 2&#20013;&#20351;&#29992;&#30340;unCLIP&#26041;&#27861;&#20063;&#23545;&#21019;&#24314;&#25351;&#20196;&#36319;&#38543;&#36830;&#32493;&#20915;&#31574;&#20195;&#29702;&#38750;&#24120;&#26377;&#25928;&#12290;STEVE-1&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#35757;&#32451;&#65306;&#39318;&#20808;&#26159;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;VPT&#27169;&#22411;&#36866;&#24212;MineCLIP&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25351;&#20196;&#65292;&#28982;&#21518;&#35757;&#32451;&#19968;&#20010;&#20808;&#39564;&#27169;&#22411;&#20197;&#20174;&#25991;&#26412;&#39044;&#27979;&#28508;&#22312;&#20195;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#22238;&#39038;&#37325;&#26032;&#26631;&#35760;&#26469;&#24494;&#35843;VPT&#65292;&#36991;&#20813;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#25991;&#26412;&#27880;&#37322;&#12290;&#36890;&#36807;&#21033;&#29992;VPT&#21644;MineCLIP&#31561;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;STEVE-1&#30340;&#35757;&#32451;&#25104;&#26412;&#20165;&#20026;60&#32654;&#20803;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Minecraft&#20013;&#36981;&#24490;&#21508;&#31181;&#30701;&#26399;&#24320;&#25918;&#22411;&#25991;&#26412;&#21644;&#35270;&#35273;&#25351;&#20196;&#12290;STEVE-1&#20026;&#24320;&#25918;&#30340;&#25351;&#20196;&#36319;&#38543;&#36830;&#32493;&#20915;&#31574;&#20195;&#29702;&#35774;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces an instruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1, demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effective for creating instruction-following sequential decision-making agents. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, bypassing the need for costly human text annotations. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 costs just $60 to train and can follow a wide range of short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 sets a new bar for open-ended instructi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;</title><link>http://arxiv.org/abs/2305.17326</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;KL&#25955;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Kernel-SSL
&lt;/p&gt;
&lt;p&gt;
Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#23558;&#19968;&#20010;&#27491;&#38170;&#28857;&#26679;&#26412;&#19982;&#35768;&#22810;&#36127;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23436;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#23398;&#20064;&#65292;&#20363;&#22914;BYOL&#12289;SimSiam&#21644;Barlow Twins&#31561;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#36127;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;SSL&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#29616;&#26377;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Kernel-SSL&#65292;&#30452;&#25509;&#20248;&#21270;RKHS&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Kernel-SSL&#22312;&#32447;&#24615;&#35780;&#20272;&#35774;&#32622;&#19979;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36827;&#34892;100&#20010;epoch&#30340;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;SimCLR&#34920;&#29616;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14383</link><description>&lt;p&gt;
&#19968;&#20010;&#38477;&#32500;&#20154;&#31867;&#20998;&#31867;&#30340;&#29702;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Rational Model of Dimension-reduced Human Categorization. (arXiv:2305.14383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14383
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#20013;&#29616;&#26377;&#30340;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#20154;&#31867;&#22312;&#24515;&#29702;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#32423;&#27010;&#25324;&#34892;&#20026;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#31867;&#21035;&#34920;&#31034;&#21487;&#33021;&#20250;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#20154;&#20204;&#19968;&#33324;&#20381;&#36182;&#20110;&#19968;&#32452;&#21487;&#34892;&#20294;&#36275;&#22815;&#30340;&#29305;&#24449;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#27010;&#29575;&#20027;&#25104;&#20998;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#32463;&#27982;&#30340;&#29305;&#24449;&#38598;&#21512;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#20154;&#31867;&#20998;&#31867;&#20013;&#30340;&#32500;&#24230;&#20559;&#24046;&#24182;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20869;&#21033;&#29992;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#39640;&#32500;&#21050;&#28608;&#19979;&#26356;&#22909;&#30340;&#20998;&#31867;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing models in cognitive science typically assume human categorization as graded generalization behavior in a multidimensional psychological space. However, category representations in these models may suffer from the curse of dimensionality in a natural setting. People generally rely on a tractable yet sufficient set of features to understand the complex environment. We propose a rational model of categorization based on a hierarchical mixture of probabilistic principal components, that simultaneously learn category representations and an economical collection of features. The model captures dimensional biases in human categorization and supports zero-shot learning. We further exploit a generative process within a low-dimensional latent space to provide a better account of categorization with high-dimensional stimuli. We validate the model with simulation and behavioral experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#20154;&#21475;&#25968;&#25454;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;</title><link>http://arxiv.org/abs/2305.14195</link><description>&lt;p&gt;
GPT&#31350;&#31455;&#26377;&#22810;&#32769;&#65311;HumBEL&#26694;&#26550;&#36890;&#36807;&#20154;&#32676;&#25968;&#25454;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data. (arXiv:2305.14195v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#20154;&#21475;&#25968;&#25454;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#27169;&#22411;&#30340;&#35821;&#35328;&#20351;&#29992;&#19982;&#29305;&#23450;&#20154;&#32676;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#36825;&#19968;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#27979;&#37327;&#21644;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#19982;&#20154;&#31867;&#23376;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#20511;&#21161;&#35821;&#35328;&#30149;&#29702;&#23398;&#30340;&#20020;&#24202;&#25216;&#26415;&#65292;&#35813;&#23398;&#31185;&#24050;&#32463;&#24314;&#31435;&#20102;&#19981;&#21516;&#65288;&#20154;&#31867;&#65289;&#24180;&#40836;&#38454;&#27573;&#30340;&#35821;&#35328;&#33021;&#21147;&#21457;&#23637;&#35268;&#33539;&#65292;&#23545;&#25216;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#19982;&#39046;&#22495;&#19987;&#23478;&#65288;&#21363;&#25345;&#26377;&#20020;&#24202;&#35768;&#21487;&#35777;&#30340;&#35821;&#35328;&#30149;&#29702;&#23398;&#23478;&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#30340;&#35780;&#20272;&#25216;&#26415;&#20197;&#23454;&#29616;&#35268;&#27169;&#21270;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#30340;&#33021;&#21147;&#22240;&#20219;&#21153;&#32780;&#24322;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;GPT-3.5&#65288;InstructGPT&#65289;&#22312;&#31038;&#20132;&#20132;&#20114;&#20219;&#21153;&#20013;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large pre-trained language models (LMs) find greater use across NLP, existing evaluation protocols do not consider how LM language use aligns with particular human demographic groups, which can be an important consideration in conversational AI applications. To remedy this gap, we consider how LM language skills can be measured and compared to human sub-populations. We suggest clinical techniques from Speech Language Pathology, which has well-established norms for acquisition of language skills, organized by (human) age. We conduct evaluation with a domain expert (i.e., a clinically licensed speech language pathologist), and also propose automated techniques to substitute clinical evaluation at scale. We find LM capability varies widely depending on task with GPT-3.5 mimicking the ability of a typical 6-9 year old at tasks requiring inference about word meanings and simultaneously outperforming a typical 21 year old at memorization. GPT-3.5 (InstructGPT) also has trouble with soc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.12809</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#26469;&#32763;&#36716;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Relabel Minimal Training Subset to Flip a Prediction. (arXiv:2305.12809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Yang&#31561;&#20154;&#21457;&#29616;&#65292;&#20165;&#21024;&#38500;1%&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#32763;&#36716;&#12290;&#37492;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#30340;&#26222;&#36941;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#19968;&#20010;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#21487;&#21542;&#23548;&#33268;&#27979;&#35797;&#32467;&#26524;&#32763;&#36716;&#65311;&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#36825;&#31181;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22987;&#32456;&#33021;&#22815;&#20135;&#29983;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26426;&#21046;&#26377;&#22810;&#37325;&#20316;&#29992;&#65306;&#65288;1&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24674;&#22797;&#21487;&#33021;&#38169;&#35823;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#65307;&#65288;2&#65289;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#26412;&#25991;&#21457;&#29616;&#23376;&#38598;&#30340;&#22823;&#23567;&#19982;&#35757;&#32451;&#38598;&#20013;&#22122;&#22768;&#25968;&#25454;&#30340;&#27604;&#20363;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#65307;&#65288;3&#65289;&#25552;&#20379;&#20102;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#20195;&#34920;&#20102;&#23545;&#35782;&#21035;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#38382;&#39064;&#30340;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yang et al. (2023) discovered that removing a mere 1% of training points can often lead to the flipping of a prediction. Given the prevalence of noisy data in machine learning models, we pose the question: can we also result in the flipping of a test prediction by relabeling a small subset of the training data before the model is trained? In this paper, utilizing the extended influence function, we propose an efficient procedure for identifying and relabeling such a subset, demonstrating consistent success. This mechanism serves multiple purposes: (1) providing a complementary approach to challenge model predictions by recovering potentially mislabeled training points; (2) evaluating model resilience, as our research uncovers a significant relationship between the subset's size and the ratio of noisy data in the training set; and (3) offering insights into bias within the training set. To the best of our knowledge, this work represents the first investigation into the problem of identi
&lt;/p&gt;</description></item><item><title>SongDriver2&#23454;&#29616;&#20102;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#65292;&#24182;&#25552;&#20986;&#20102;&#26580;&#21644;&#36807;&#28193;&#26426;&#21046;&#65292;&#20351;&#38899;&#20048;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#12290;</title><link>http://arxiv.org/abs/2305.08029</link><description>&lt;p&gt;
SongDriver2&#65306;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#19982;&#26580;&#21644;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
SongDriver2: Real-time Emotion-based Music Arrangement with Soft Transition. (arXiv:2305.08029v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08029
&lt;/p&gt;
&lt;p&gt;
SongDriver2&#23454;&#29616;&#20102;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#65292;&#24182;&#25552;&#20986;&#20102;&#26580;&#21644;&#36807;&#28193;&#26426;&#21046;&#65292;&#20351;&#38899;&#20048;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#26088;&#22312;&#23558;&#32473;&#23450;&#30340;&#38899;&#20048;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#33021;&#22815;&#23454;&#26102;&#24341;&#36215;&#29992;&#25143;&#29305;&#23450;&#24773;&#24863;&#20849;&#40483;&#30340;&#38899;&#20048;&#65292;&#22312;&#38899;&#20048;&#30103;&#27861;&#12289;&#28216;&#25103;&#37197;&#20048;&#21644;&#30005;&#24433;&#37197;&#20048;&#31561;&#21508;&#31181;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#26631;&#24773;&#24863;&#30340;&#32454;&#31890;&#24230;&#21644;&#21487;&#21464;&#24615;&#65292;&#24179;&#34913;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#21644;&#26580;&#21644;&#24773;&#24863;&#36716;&#25442;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#29616;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#65292;&#32780;&#26580;&#21644;&#36807;&#28193;&#30340;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#24433;&#21709;&#20102;&#38899;&#20048;&#30340;&#25972;&#20307;&#24773;&#24863;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SongDriver2&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#26368;&#21518;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#38899;&#20048;&#24773;&#32490;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#24403;&#21069;&#26102;&#38388;&#27493;&#30340;&#30446;&#26631;&#36755;&#20837;&#24773;&#32490;&#34701;&#21512;&#12290;&#34701;&#21512;&#30340;&#24773;&#24863;&#38543;&#21518;&#20316;&#20026;SongDriver2&#26681;&#25454;&#36755;&#20837;&#26059;&#24459;&#25968;&#25454;&#29983;&#25104;&#21363;&#23558;&#21040;&#26469;&#30340;&#38899;&#20048;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#35843;&#25972;&#38899;&#20048;&#30456;&#20284;&#24615;&#21644;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#65292;&#20197;&#23454;&#29616;&#20004;&#31181;&#19981;&#21516;&#24773;&#24863;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36719;&#36807;&#28193;&#26426;&#21046;&#65292;&#23558;&#25554;&#20540;&#21644;&#24179;&#28369;&#28388;&#27874;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;SongDriver2&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#30340;&#24773;&#24863;&#38899;&#20048;&#65292;&#36825;&#34920;&#26126;&#20854;&#22312;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time emotion-based music arrangement, which aims to transform a given music piece into another one that evokes specific emotional resonance with the user in real-time, holds significant application value in various scenarios, e.g., music therapy, video game soundtracks, and movie scores. However, balancing emotion real-time fit with soft emotion transition is a challenge due to the fine-grained and mutable nature of the target emotion. Existing studies mainly focus on achieving emotion real-time fit, while the issue of soft transition remains understudied, affecting the overall emotional coherence of the music. In this paper, we propose SongDriver2 to address this balance. Specifically, we first recognize the last timestep's music emotion and then fuse it with the current timestep's target input emotion. The fused emotion then serves as the guidance for SongDriver2 to generate the upcoming music based on the input melody data. To adjust music similarity and emotion real-time fit f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#36827;&#34892;&#20195;&#30721;&#20998;&#31867;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.04228</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification. (arXiv:2305.04228v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#36827;&#34892;&#20195;&#30721;&#20998;&#31867;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#20998;&#31867;&#26159;&#31243;&#24207;&#29702;&#35299;&#21644;&#33258;&#21160;&#32534;&#30721;&#20013;&#30340;&#19968;&#20010;&#38590;&#39064;&#12290;&#30001;&#20110;&#31243;&#24207;&#30340;&#27169;&#31946;&#35821;&#27861;&#21644;&#22797;&#26434;&#35821;&#20041;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25216;&#26415;&#21019;&#24314;&#20195;&#30721;&#34920;&#31034;&#29992;&#20110;&#20195;&#30721;&#20998;&#31867;&#12290;&#36825;&#20123;&#25216;&#26415;&#21033;&#29992;&#20195;&#30721;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#21482;&#32771;&#34385;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;AST&#20013;&#33410;&#28857;&#20043;&#38388;&#24050;&#32463;&#23384;&#22312;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#20195;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#65288;HDHG&#65289;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HDHGN&#65289;&#22788;&#29702;&#22270;&#24418;&#12290;HDHG&#20445;&#30041;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#24182;&#26356;&#20840;&#38754;&#22320;&#32534;&#30721;&#20102;AST&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;HDHGN&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#33410;&#28857;&#30340;&#29305;&#24449;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#22788;&#29702;&#26469;&#23545;AST&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HDHG&#21644;HDHGN&#22312;&#20195;&#30721;&#20998;&#31867;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural network (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order correlations that already exist between nodes in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose to represent AST as a heterogeneous directed hypergraph (HDHG) and process the graph by hetero
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02614</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#21450;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling. (arXiv:2305.02614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#23547;&#25214;&#40657;&#31665;&#20989;&#25968;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#34429;&#28982;&#40657;&#31665;&#20989;&#25968;&#30340;&#35780;&#20272;&#25104;&#26412;&#24448;&#24448;&#24456;&#39640;&#65292;&#20294;&#20943;&#23569;&#26114;&#36149;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#26159;&#29702;&#24819;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;BO&#29615;&#22659;&#19979;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#20854;&#20013;&#65292;&#20851;&#38190;&#22312;&#20110;&#36873;&#25321;&#39564;&#35777;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#25552;&#39640;BO&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#37319;&#26679;&#65292;&#25105;&#20204;&#37319;&#29992;&#40657;&#31665;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#65292;&#23558;&#20854;&#20248;&#21270;&#20026;&#25152;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;&#26356;&#36827;&#19968;&#27493;&#65292;&#36890;&#36807;&#20174;&#21160;&#24577;&#36866;&#24212;&#30340;&#26497;&#20540;&#20998;&#24067;&#20013;&#36873;&#25321;&#26410;&#26631;&#31614;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BO&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;BO&#26041;&#27861;&#22312;&#23398;&#20064;&#21518;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#20351;&#20854;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a powerful tool for seeking the global optimum of black-box functions. While evaluations of the black-box functions can be highly costly, it is desirable to reduce the use of expensive labeled data. For the first time, we introduce a teacher-student model to exploit semi-supervised learning that can make use of large amounts of unlabelled data under the context of BO. Importantly, we show that the selection of the validation and unlabeled data is key to the performance of BO. To optimize the sampling of unlabeled data, we employ a black-box parameterized sampling distribution optimized as part of the employed bi-level optimization framework. Taking one step further, we demonstrate that the performance of BO can be further improved by selecting unlabeled data from a dynamically fitted extreme value distribution. Our BO method operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. The proposed approac
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#38024;&#23545;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#24471;&#21040;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;Presto&#65289;&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14065</link><description>&lt;p&gt;
&#38754;&#21521;&#36965;&#24863;&#26102;&#24207;&#25968;&#25454;&#30340;&#36731;&#37327;&#32423;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
Lightweight, Pre-trained Transformers for Remote Sensing Timeseries. (arXiv:2304.14065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14065
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#38024;&#23545;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#24471;&#21040;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;Presto&#65289;&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#20256;&#24863;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#31038;&#20250;&#30456;&#20851;&#24212;&#29992;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#30340;&#26631;&#31614;&#21487;&#33021;&#24456;&#38590;&#25110;&#19981;&#21487;&#33021;&#33719;&#24471;&#12290;&#36825;&#20010;&#25361;&#25112;&#24050;&#32463;&#25512;&#21160;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#36890;&#36807;&#36965;&#24863;&#25968;&#25454;&#35299;&#38145;&#22312;&#26631;&#35760;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#22320;&#29702;&#20301;&#32622;&#25110;&#24212;&#29992;&#39046;&#22495;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20026;&#36965;&#24863;&#25968;&#25454;&#35774;&#35745;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#35757;&#32451;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#26356;&#23567;&#12289;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Remote Sensing Transformer&#65288;Presto&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#23545;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#21487;&#27604;&#27169;&#22411;&#30456;&#27604;&#65292;Presto&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38656;&#35201;&#25968;&#37327;&#32423;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms for parsing remote sensing data have a wide range of societally relevant applications, but labels used to train these algorithms can be difficult or impossible to acquire. This challenge has spurred research into self-supervised learning for remote sensing data aiming to unlock the use of machine learning in geographies or application domains where labelled datasets are small. Current self-supervised learning approaches for remote sensing data draw significant inspiration from techniques applied to natural images. However, remote sensing data has important differences from natural images -- for example, the temporal dimension is critical for many tasks and data is collected from many complementary sensors. We show that designing models and self-supervised training techniques specifically for remote sensing data results in both smaller and more performant models. We introduce the Pretrained Remote Sensing Transformer (Presto), a transformer-based model pre-tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11082</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19982;&#20154;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#26159;&#23545;&#40784;&#20854;&#34892;&#20026;&#65292;&#20351;&#20854;&#23545;&#20854;&#20154;&#31867;&#29992;&#25143;&#26377;&#29992;&#19988;&#26080;&#23475;&#12290;&#36825;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#25152;&#38656;&#30340;&#34892;&#20026;&#24182;&#25233;&#21046;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;(BEB)&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#20869;&#22312;&#29305;&#24449;&#21644;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#34987;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#26377;&#38480;&#27010;&#29575;&#30340;&#34892;&#20026;&#65292;&#37117;&#23384;&#22312;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#36755;&#20986;&#27492;&#34892;&#20026;&#30340;&#25552;&#31034;&#65292;&#20854;&#27010;&#29575;&#38543;&#25552;&#31034;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#20943;&#24369;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#20294;&#26410;&#23558;&#20854;&#23436;&#20840;&#28040;&#38500;&#30340;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#25269;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#31034;&#20102;&#39046;&#20808;&#30340;
&lt;/p&gt;
&lt;p&gt;
An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#20139;&#20102;&#22312;&#20851;&#38190;&#22330;&#26223;&#19979;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#20026;&#25913;&#36827;&#27169;&#22411;&#25928;&#29575;&#32780;&#21162;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10181</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Operating critical machine learning models in resource constrained regimes. (arXiv:2303.10181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#20139;&#20102;&#22312;&#20851;&#38190;&#22330;&#26223;&#19979;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#20026;&#25913;&#36827;&#27169;&#22411;&#25928;&#29575;&#32780;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#26041;&#38754;&#30340;&#26368;&#26032;&#31361;&#30772;&#65292;&#20351;&#20854;&#24471;&#21040;&#24555;&#36895;&#21457;&#23637;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#65292;&#35745;&#31639;&#21644;&#33021;&#28304;&#25104;&#26412;&#26041;&#38754;&#30340;&#36164;&#28304;&#28040;&#32791;&#26159;&#24040;&#22823;&#30340;&#12290;&#36825;&#20123;&#24040;&#22823;&#30340;&#36164;&#28304;&#25104;&#26412;&#21487;&#33021;&#20250;&#38459;&#30861;&#36825;&#20123;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#21162;&#21147;&#24341;&#20837;&#36164;&#28304;&#25928;&#29575;&#30340;&#27010;&#24565;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#37327;&#21270;&#26469;&#20943;&#36731;&#20869;&#23384;&#28040;&#32791;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#36164;&#28304;&#21033;&#29992;&#65292;&#20294;&#21487;&#33021;&#20250;&#20197;&#24615;&#33021;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#29305;&#21035;&#26159;&#22312;&#35786;&#25152;&#31561;&#20851;&#38190;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerated development of machine learning methods, primarily deep learning, are causal to the recent breakthroughs in medical image analysis and computer aided intervention. The resource consumption of deep learning models in terms of amount of training data, compute and energy costs are known to be massive. These large resource costs can be barriers in deploying these models in clinics, globally. To address this, there are cogent efforts within the machine learning community to introduce notions of resource efficiency. For instance, using quantisation to alleviate memory consumption. While most of these methods are shown to reduce the resource utilisation, they could come at a cost in performance. In this work, we probe into the trade-off between resource consumption and performance, specifically, when dealing with models that are used in critical settings such as in clinics.
&lt;/p&gt;</description></item><item><title>FreDSNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#20840;&#26223;&#22270;&#20687;&#20013;&#23454;&#29616;&#23460;&#20869;&#29615;&#22659;&#30340;&#35821;&#20041;&#19977;&#32500;&#29702;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#39057;&#29575;&#22495;&#20013;&#30340;&#21367;&#31215;&#20174;&#32780;&#33719;&#24471;&#26356;&#23485;&#24191;&#30340;&#24863;&#21463;&#37326;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#32852;&#21512;&#25552;&#20379;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2210.01595</link><description>&lt;p&gt;
FreDSNet: &#21033;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21367;&#31215;&#36827;&#34892;&#21333;&#30446;&#28145;&#24230;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast Fourier Convolutions. (arXiv:2210.01595v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01595
&lt;/p&gt;
&lt;p&gt;
FreDSNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#20840;&#26223;&#22270;&#20687;&#20013;&#23454;&#29616;&#23460;&#20869;&#29615;&#22659;&#30340;&#35821;&#20041;&#19977;&#32500;&#29702;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#39057;&#29575;&#22495;&#20013;&#30340;&#21367;&#31215;&#20174;&#32780;&#33719;&#24471;&#26356;&#23485;&#24191;&#30340;&#24863;&#21463;&#37326;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#32852;&#21512;&#25552;&#20379;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FreDSNet&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#20840;&#26223;&#22270;&#20687;&#20013;&#33719;&#21462;&#23460;&#20869;&#29615;&#22659;&#30340;&#35821;&#20041;&#19977;&#32500;&#29702;&#35299;&#12290;&#20840;&#26223;&#22270;&#20687;&#30001;&#20110;&#25552;&#20379;&#20102;&#25972;&#20010;&#29615;&#22659;&#30340;360&#24230;&#32972;&#26223;&#20449;&#24687;&#65292;&#23545;&#20110;&#35299;&#20915;&#22330;&#26223;&#29702;&#35299;&#38382;&#39064;&#20855;&#26377;&#29305;&#23450;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#20840;&#26223;&#22270;&#20687;&#30340;&#22266;&#26377;&#29305;&#28857;&#32473;&#23545;&#35937;&#30340;&#20934;&#30830;&#26816;&#27979;&#21644;&#20998;&#21106;&#25110;&#32773;&#33391;&#22909;&#30340;&#28145;&#24230;&#20272;&#35745;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#39057;&#29575;&#22495;&#20013;&#30340;&#21367;&#31215;&#65292;&#22312;&#27599;&#20010;&#21367;&#31215;&#23618;&#20013;&#33719;&#24471;&#26356;&#23485;&#24191;&#30340;&#24863;&#21463;&#37326;&#12290;&#36825;&#20123;&#21367;&#31215;&#20801;&#35768;&#20174;&#20840;&#26223;&#22270;&#20687;&#20013;&#21033;&#29992;&#25972;&#20010;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;FreDSNet&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21367;&#31215;&#20174;&#21333;&#20010;&#20840;&#26223;&#22270;&#20687;&#20013;&#32852;&#21512;&#25552;&#20379;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FreDSNet&#30340;&#24615;&#33021;&#19982;&#29305;&#23450;&#29366;&#24577;&#30340;&#26041;&#27861;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present FreDSNet, a deep learning solution which obtains semantic 3D understanding of indoor environments from single panoramas. Omnidirectional images reveal task-specific advantages when addressing scene understanding problems due to the 360-degree contextual information about the entire environment they provide. However, the inherent characteristics of the omnidirectional images add additional problems to obtain an accurate detection and segmentation of objects or a good depth estimation. To overcome these problems, we exploit convolutions in the frequential domain obtaining a wider receptive field in each convolutional layer. These convolutions allow to leverage the whole context information from omnidirectional images. FreDSNet is the first network that jointly provides monocular depth estimation and semantic segmentation from a single panoramic image exploiting fast Fourier convolutions. Our experiments show that FreDSNet has similar performance as specific state 
&lt;/p&gt;</description></item></channel></rss>