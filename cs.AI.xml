<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOLERANCE&#30340;&#26032;&#22411;&#25511;&#21046;&#26550;&#26500;&#65292;&#36890;&#36807;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#35299;&#20915;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#25928;&#31639;&#27861;&#26469;&#25913;&#21892;&#26381;&#21153;&#21487;&#29992;&#24615;&#21644;&#38477;&#20302;&#25805;&#20316;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.01741</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#32423;&#21453;&#39304;&#25511;&#21046;&#23454;&#29616;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;
&lt;/p&gt;
&lt;p&gt;
Intrusion Tolerance for Networked Systems through Two-Level Feedback Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOLERANCE&#30340;&#26032;&#22411;&#25511;&#21046;&#26550;&#26500;&#65292;&#36890;&#36807;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#35299;&#20915;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#25928;&#31639;&#27861;&#26469;&#25913;&#21892;&#26381;&#21153;&#21487;&#29992;&#24615;&#21644;&#38477;&#20302;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#26381;&#21153;&#22797;&#21046;&#21697;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#26412;&#22320;&#32423;&#21035;&#65292;&#33410;&#28857;&#25511;&#21046;&#22120;&#25191;&#34892;&#20837;&#20405;&#24674;&#22797;&#65292;&#22312;&#20840;&#23616;&#32423;&#21035;&#65292;&#31995;&#32479;&#25511;&#21046;&#22120;&#31649;&#29702;&#22797;&#21046;&#22240;&#23376;&#12290;&#26412;&#22320;&#21644;&#20840;&#23616;&#25511;&#21046;&#38382;&#39064;&#21487;&#20197;&#34987;&#21046;&#23450;&#20026;&#36816;&#31609;&#23398;&#20013;&#30340;&#32463;&#20856;&#38382;&#39064;&#65292;&#21363;&#26426;&#22120;&#26356;&#25442;&#38382;&#39064;&#21644;&#24211;&#23384;&#34917;&#32473;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#19968;&#27169;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21517;&#20026;TOLERANCE&#30340;&#20837;&#20405;&#23481;&#24525;&#31995;&#32479;&#25511;&#21046;&#26550;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#23618;&#38754;&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20855;&#26377;&#38408;&#20540;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#29992;&#20110;&#35745;&#31639;&#23427;&#20204;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20223;&#30495;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;TOLERANCE&#65292;&#20854;&#20013;&#36816;&#34892;&#20102;10&#31181;&#32593;&#32476;&#20837;&#20405;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#20837;&#20405;&#23481;&#24525;&#31995;&#32479;&#30456;&#27604;&#65292;TOLERANCE&#33021;&#22815;&#25552;&#39640;&#26381;&#21153;&#21487;&#29992;&#24615;&#24182;&#20943;&#23569;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01741v1 Announce Type: cross  Abstract: We formulate intrusion tolerance for a system with service replicas as a two-level optimal control problem. On the local level node controllers perform intrusion recovery, and on the global level a system controller manages the replication factor. The local and global control problems can be formulated as classical problems in operations research, namely, the machine replacement problem and the inventory replenishment problem. Based on this formulation, we design TOLERANCE, a novel control architecture for intrusion-tolerant systems. We prove that the optimal control strategies on both levels have threshold structure and design efficient algorithms for computing them. We implement and evaluate TOLERANCE in an emulation environment where we run 10 types of network intrusions. The results show that TOLERANCE can improve service availability and reduce operational cost compared with state-of-the-art intrusion-tolerant systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00897</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#40065;&#26834;&#24615;&#65306;&#20837;&#38376;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Robustness: A Primer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#31283;&#20581;&#24615;&#30340;&#22522;&#26412;&#27010;&#24565;&#21450;&#20854;&#22312;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#35752;&#35770;&#22987;&#20110;&#31283;&#20581;&#24615;&#30340;&#35814;&#32454;&#23450;&#20041;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;ML&#27169;&#22411;&#22312;&#21508;&#31181;&#19981;&#21516;&#21644;&#24847;&#22806;&#30340;&#29615;&#22659;&#26465;&#20214;&#19979;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;ML&#40065;&#26834;&#24615;&#36890;&#36807;&#22810;&#20010;&#35270;&#35282;&#36827;&#34892;&#20102;&#21078;&#26512;&#65306;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20114;&#34917;&#24615;&#65307;&#20854;&#20316;&#20026;&#21487;&#20449;AI&#30340;&#35201;&#27714;&#65307;&#20854;&#23545;&#25239;&#24615;&#19982;&#38750;&#23545;&#25239;&#24615;&#26041;&#38754;&#65307;&#20854;&#25968;&#37327;&#21270;&#25351;&#26631;&#65307;&#20197;&#21450;&#20854;&#21487;&#22797;&#29616;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#25351;&#26631;&#12290;&#31456;&#33410;&#28145;&#20837;&#25506;&#35752;&#20102;&#24433;&#21709;&#40065;&#26834;&#24615;&#30340;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#20559;&#24046;&#12289;&#27169;&#22411;&#22797;&#26434;&#24615;&#20197;&#21450;ML&#27969;&#31243;&#19981;&#26126;&#30830;&#30340;&#39118;&#38505;&#12290;&#23427;&#20174;&#24191;&#27867;&#30340;&#35270;&#35282;&#35843;&#26597;&#20102;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21253;&#25324;&#25968;&#23383;&#21644;&#29289;&#29702;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00897v1 Announce Type: cross  Abstract: This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;YOLOOC&#26816;&#27979;&#22120;&#65292;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#35774;&#32622;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#26377;&#25928;&#24212;&#23545;&#26032;&#31867;&#21035;&#26816;&#27979;&#21644;&#22686;&#37327;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00257</link><description>&lt;p&gt;
YOLOOC: &#22522;&#20110;YOLO&#30340;&#24320;&#25918;&#31867;&#21035;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;&#19982;&#26032;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;YOLOOC&#26816;&#27979;&#22120;&#65292;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#35774;&#32622;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#26377;&#25928;&#24212;&#23545;&#26032;&#31867;&#21035;&#26816;&#27979;&#21644;&#22686;&#37327;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#36816;&#29992;&#65292;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65288;OWOD&#65289;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#25361;&#25112;&#22312;&#20110;&#27169;&#22411;&#22914;&#20309;&#26816;&#27979;&#26032;&#31867;&#21035;&#65292;&#28982;&#21518;&#22686;&#37327;&#23398;&#20064;&#23427;&#20204;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#24050;&#30693;&#30340;&#31867;&#21035;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#30417;&#30563;&#25110;&#24369;&#30417;&#30563;&#30340;&#26032;&#31867;&#21035;&#25968;&#25454;&#29992;&#20110;&#26032;&#31867;&#21035;&#26816;&#27979;&#65292;&#36825;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#26032;&#31867;&#21035;&#21482;&#22312;&#25512;&#26029;&#38454;&#27573;&#36935;&#21040;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;&#26032;OWOD&#26816;&#27979;&#22120;YOLOOC&#65292;&#19987;&#38376;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#20197;&#38450;&#27490;&#26816;&#27979;&#22120;&#36807;&#20110;&#33258;&#20449;&#22320;&#23558;&#26032;&#31867;&#21035;&#26144;&#23556;&#21040;&#24050;&#30693;&#31867;&#21035;&#24182;&#21457;&#29616;&#26032;&#31867;&#21035;&#12290;&#22312;&#25105;&#20204;&#26356;&#21152;&#29616;&#23454;&#30340;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#26032;&#22522;&#20934;&#19979;&#21457;&#29616;&#26032;&#31867;&#21035;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00257v1 Announce Type: cross  Abstract: Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#21040;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#30340;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#65292;&#29983;&#25104;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.18985</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#40657;&#30418;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#21644;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#21040;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#30340;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#65292;&#29983;&#25104;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#38024;&#23545;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#65288;1D&#65289;&#12289;&#22270;&#20687;&#20998;&#31867;&#65288;2D&#65289;&#21040;&#35270;&#39057;&#20998;&#31867;&#65288;3D&#65289;&#31561;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#20248;&#21270;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#21644;&#21508;&#31181;&#25197;&#26354;&#31867;&#22411;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#12290;&#26032;&#39062;&#30340;RL&#26041;&#27861;&#22312;&#25152;&#26377;&#19977;&#20010;&#24212;&#29992;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;RL&#26041;&#27861;&#29983;&#25104;&#20102;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#22686;&#24378;&#20102;&#22270;&#20687;&#20998;&#31867;&#21644;&#24515;&#30005;&#22270;&#20998;&#26512;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#20110;&#24515;&#30005;&#22270;&#20998;&#26512;&#31561;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#24179;&#21488;&#31361;&#20986;&#20102;&#20020;&#24202;&#21307;&#29983;&#20851;&#27880;&#30340;&#20851;&#38190;&#24515;&#30005;&#22270;&#29255;&#27573;&#65292;&#21516;&#26102;&#30830;&#20445;&#23545;&#27969;&#34892;&#25197;&#26354;&#30340;&#38887;&#24615;&#12290;&#36825;&#19968;&#20840;&#38754;&#30340;&#24037;&#20855;&#26088;&#22312;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#36879;&#26126;&#24230;&#25552;&#39640;&#21508;&#31181;&#24212;&#29992;&#21644;&#25968;&#25454;&#31867;&#22411;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18985v1 Announce Type: cross  Abstract: We present a generic Reinforcement Learning (RL) framework optimized for crafting adversarial attacks on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D). The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types. The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency. Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models. For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions. This comprehensive tool aims to bolster both resilience with adversarial training and transparency across varied applications and data types.
&lt;/p&gt;</description></item><item><title>VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16973</link><description>&lt;p&gt;
VoiceCraft&#65306;&#37326;&#22806;&#38646;-shot&#35821;&#38899;&#32534;&#36753;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16973
&lt;/p&gt;
&lt;p&gt;
VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;VoiceCraft&#65292;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#22768;&#20070;&#12289;&#20114;&#32852;&#32593;&#35270;&#39057;&#21644;&#25773;&#23458;&#19978;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;VoiceCraft&#37319;&#29992;Transformer&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#35760;&#37325;&#25490;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#21644;&#24310;&#36831;&#22534;&#21472;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#26377;&#24207;&#21015;&#20869;&#30340;&#29983;&#25104;&#12290;&#22312;&#35821;&#38899;&#32534;&#36753;&#20219;&#21153;&#19978;&#65292;VoiceCraft&#29983;&#25104;&#30340;&#32534;&#36753;&#35821;&#38899;&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#20960;&#20046;&#19982;&#26410;&#32534;&#36753;&#30340;&#24405;&#38899;&#38590;&#20197;&#21306;&#20998;&#65292;&#32463;&#20154;&#31867;&#35780;&#20272;&#65307;&#23545;&#20110;&#38646;-shot TTS&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21253;&#25324;VALLE&#21644;&#27969;&#34892;&#30340;&#21830;&#19994;&#27169;&#22411;XTTS-v2&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#26679;&#21475;&#38899;&#12289;&#35821;&#38899;&#39118;&#26684;&#12289;&#24405;&#21046;&#26465;&#20214;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#38899;&#20048;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30495;&#23454;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#22987;&#32456;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16973v1 Announce Type: cross  Abstract: We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models a
&lt;/p&gt;</description></item><item><title>&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#23545;&#35805;&#24335;&#25945;&#23398;&#22330;&#26223;&#20013;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#65292;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.16687</link><description>&lt;p&gt;
&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16687
&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#23545;&#35805;&#24335;&#25945;&#23398;&#22330;&#26223;&#20013;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#65292;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#20986;&#29616;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#21069;&#26223;&#12290; LLM&#20855;&#26377;&#35299;&#37322;&#30693;&#35782;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#32771;&#34385;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#20026;&#23398;&#29983;&#25552;&#20379;&#23545;&#35805;&#24335;&#25945;&#23398;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#26816;&#39564;LLM&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#23545;&#35805;&#25945;&#23398;&#22330;&#26223;&#20013;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#25945;&#32946;&#32773;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290; &#26412;&#30740;&#31350;&#25307;&#21215;&#20102;34&#21517;&#26412;&#31185;&#29983;&#20316;&#20026;&#21442;&#19982;&#32773;&#65292;&#38543;&#26426;&#20998;&#20026;&#20004;&#32452;&#12290; &#23454;&#39564;&#32452;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23545;&#35805;&#24335;&#25945;&#23398;&#65292;&#32780;&#25511;&#21046;&#32452;&#19982;&#20154;&#31867;&#25945;&#24072;&#20114;&#21160;&#12290; &#20004;&#32452;&#37117;&#23398;&#20064;&#20102;&#20449;&#24687;&#30456;&#20851;&#35838;&#31243;&#8220;&#25968;&#23383;&#22270;&#20687;&#8221;&#30340;&#30452;&#26041;&#22270;&#22343;&#34913;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16687v1 Announce Type: cross  Abstract: In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course "Digital Ima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20998;&#20026;&#33145;&#20391;&#27969;&#21644;&#32972;&#20391;&#27969;&#20004;&#37096;&#20998;&#65292;&#20197;&#23454;&#29616;&#23545;&#28966;&#21644;&#22788;&#29702;&#22270;&#20687;patch&#30340;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.15977</link><description>&lt;p&gt;
&#26397;&#21521;&#22522;&#20110;&#21452;&#27969;&#30524;&#24213;&#32858;&#28966;&#30340;&#20027;&#21160;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Two-Stream Foveation-based Active Vision Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20998;&#20026;&#33145;&#20391;&#27969;&#21644;&#32972;&#20391;&#27969;&#20004;&#37096;&#20998;&#65292;&#20197;&#23454;&#29616;&#23545;&#28966;&#21644;&#22788;&#29702;&#22270;&#20687;patch&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#26426;&#22120;&#24863;&#30693;&#26694;&#26550;&#20197;&#19968;&#27425;&#24615;&#26041;&#24335;&#22788;&#29702;&#25972;&#20010;&#36755;&#20837;&#65292;&#20197;&#25552;&#20379;&#8220;&#34987;&#35266;&#23519;&#21040;&#30340;&#29289;&#20307;&#26159;&#20160;&#20040;&#8221;&#21644;&#8220;&#23427;&#20301;&#20110;&#21738;&#37324;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#35299;&#37322;&#20102;&#20154;&#31867;&#35270;&#35273;&#30382;&#23618;&#20013;&#30340;&#31070;&#32463;&#22788;&#29702;&#65292;&#34920;&#26126;&#20854;&#20316;&#20026;&#19968;&#20010;&#21033;&#29992;&#22823;&#33041;&#30340;&#20004;&#20010;&#19981;&#21516;&#21306;&#22495;&#26469;&#22238;&#31572;&#8220;&#26159;&#20160;&#20040;&#8221;&#21644;&#8220;&#22312;&#21738;&#37324;&#8221;&#30340;&#35270;&#35273;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#25152;&#24102;&#26469;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#20197;&#19979;&#26426;&#21046;&#65306;1&#65289;&#33145;&#20391;&#65288;&#26159;&#20160;&#20040;&#65289;&#27969;&#32858;&#28966;&#20110;&#30524;&#29699;&#65288;&#30524;&#24213;&#65289;&#30340;&#35270;&#37326;&#37096;&#20998;&#65292;2&#65289;&#32972;&#20391;&#65288;&#22312;&#21738;&#37324;&#65289;&#27969;&#25552;&#20379;&#35270;&#35273;&#24341;&#23548;&#65292;3&#65289;&#20004;&#20010;&#27969;&#30340;&#36845;&#20195;&#22788;&#29702;&#20197;&#26657;&#20934;&#35270;&#35273;&#28966;&#28857;&#24182;&#22788;&#29702;&#19968;&#31995;&#21015;&#32858;&#28966;&#30340;&#22270;&#20687;&#22359;&#12290;&#35813;&#26694;&#26550;&#30340;&#35757;&#32451;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15977v1 Announce Type: cross  Abstract: Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both "what object is being observed" and "where it is located". In contrast, the "two-stream hypothesis" from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions. In this work, we propose a machine learning framework inspired by the "two-stream hypothesis" and explore the potential benefits that it offers. Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches. The training of the pr
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07718</link><description>&lt;p&gt;
WorkArena&#65306;Web&#20195;&#29702;&#22312;&#35299;&#20915;&#24120;&#35265;&#30693;&#35782;&#24037;&#20316;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#19982;&#36719;&#20214;&#36890;&#36807;web&#27983;&#35272;&#22120;&#20132;&#20114;&#30340;&#24212;&#29992;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#34913;&#37327;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#21033;&#29992;&#20225;&#19994;&#36719;&#20214;&#31995;&#32479;&#30340;&#30693;&#35782;&#24037;&#20316;&#32773;&#30340;&#20856;&#22411;&#26085;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WorkArena&#65292;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;ServiceNow&#24179;&#21488;&#30340;29&#20010;&#20219;&#21153;&#30340;&#36828;&#31243;&#20027;&#26426;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BrowserGym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36825;&#20123;&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#20195;&#29702;&#22312;WorkArena&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#35201;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#20043;&#38388;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;&#25506;&#32034;&#21644;&#21457;&#23637;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#21644;&#25193;&#23637;&#39057;&#29575;&#23545;&#27604;&#27491;&#35268;&#21270;&#65292;&#20197;&#35299;&#20915;&#26174;&#24494;&#38236;&#21435;&#27169;&#31946;&#20013;&#30340;&#38271;&#36317;&#31163;&#20132;&#20114;&#21644;&#29305;&#24449;&#19981;&#36275;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02611</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26174;&#24494;&#38236;&#28966;&#22806;&#27169;&#31946;&#21435;&#38500;&#26694;&#26550;: &#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02611
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#21644;&#25193;&#23637;&#39057;&#29575;&#23545;&#27604;&#27491;&#35268;&#21270;&#65292;&#20197;&#35299;&#20915;&#26174;&#24494;&#38236;&#21435;&#27169;&#31946;&#20013;&#30340;&#38271;&#36317;&#31163;&#20132;&#20114;&#21644;&#29305;&#24449;&#19981;&#36275;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#28966;&#27169;&#31946;&#26159;&#26174;&#24494;&#38236;&#25104;&#20687;&#20013;&#30340;&#25345;&#32493;&#38382;&#39064;&#65292;&#23545;&#30149;&#29702;&#35299;&#37322;&#21644;&#32454;&#32990;&#26174;&#24494;&#38236;&#21644;&#26174;&#24494;&#25163;&#26415;&#20013;&#30340;&#21307;&#30103;&#24178;&#39044;&#36896;&#25104;&#20260;&#23475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#65288;MPT&#65289;&#21644;&#25193;&#23637;&#39057;&#29575;&#23545;&#27604;&#27491;&#35268;&#21270;&#65288;EFCR&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26174;&#24494;&#38236;&#21435;&#27169;&#31946;&#20013;&#30340;&#20004;&#20010;&#31361;&#20986;&#25361;&#25112;&#65306;&#36739;&#38271;&#30340;&#27880;&#24847;&#21147;&#36328;&#24230;&#21644;&#29305;&#24449;&#19981;&#36275;&#12290;MPT&#22312;&#27599;&#20010;&#32593;&#32476;&#38454;&#27573;&#20351;&#29992;&#26174;&#24335;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;&#38598;&#25104;&#20102;&#36328;&#23610;&#24230;&#31383;&#21475;&#27880;&#24847;&#21147;&#65288;CSWA&#65289;&#12289;&#20869;&#23610;&#24230;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;ISCA&#65289;&#21644;&#29305;&#24449;&#22686;&#24378;&#21069;&#21521;&#32593;&#32476;&#65288;FEFN&#65289;&#65292;&#20197;&#25429;&#33719;&#38271;&#36317;&#31163;&#36328;&#23610;&#24230;&#31354;&#38388;&#20132;&#20114;&#21644;&#20840;&#23616;&#36890;&#36947;&#19978;&#19979;&#25991;&#12290;EFCR&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#39057;&#27573;&#30340;&#28508;&#22312;&#21435;&#27169;&#31946;&#20449;&#21495;&#26469;&#35299;&#20915;&#29305;&#24449;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23427;&#36824;&#20351;&#21435;&#27169;&#31946;&#30693;&#35782;&#20256;&#36755;&#65292;&#20174;&#39069;&#22806;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#22495;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02611v1 Announce Type: cross  Abstract: Defocus blur is a persistent problem in microscope imaging that poses harm to pathology interpretation and medical intervention in cell microscopy and microscope surgery. To address this problem, a unified framework including multi-pyramid transformer (MPT) and extended frequency contrastive regularization (EFCR) is proposed to tackle two outstanding challenges in microscopy deblur: longer attention span and feature deficiency. The MPT employs an explicit pyramid structure at each network stage that integrates the cross-scale window attention (CSWA), the intra-scale channel attention (ISCA), and the feature-enhancing feed-forward network (FEFN) to capture long-range cross-scale spatial interaction and global channel context. The EFCR addresses the feature deficiency problem by exploring latent deblur signals from different frequency bands. It also enables deblur knowledge transfer to learn cross-domain information from extra data, impr
&lt;/p&gt;</description></item><item><title>HanDiffuser&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#30340;&#25163;&#37096;&#22806;&#35266;&#65292;&#21253;&#25324;Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#21644;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.01693</link><description>&lt;p&gt;
HanDiffuser: &#20855;&#26377;&#36924;&#30495;&#25163;&#37096;&#22806;&#35266;&#30340;&#25991;&#26412;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01693
&lt;/p&gt;
&lt;p&gt;
HanDiffuser&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#30340;&#25163;&#37096;&#22806;&#35266;&#65292;&#21253;&#25324;Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#21644;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01693v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25991;&#25688;: &#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#24418;&#35937;&#65292;&#20294;&#22312;&#29983;&#25104;&#25163;&#37096;&#26102;&#20250;&#22833;&#21435;&#36924;&#30495;&#24230;&#12290;&#24120;&#35265;&#30340;&#32570;&#38519;&#21253;&#25324;&#19981;&#35268;&#21017;&#30340;&#25163;&#37096;&#23039;&#21183;&#12289;&#24418;&#29366;&#12289;&#38169;&#35823;&#30340;&#25163;&#25351;&#25968;&#37327;&#20197;&#21450;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#25163;&#25351;&#26041;&#21521;&#12290;&#20026;&#20102;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#25163;&#37096;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26032;&#39062;&#26550;&#26500;&#65292;&#31216;&#20026;HanDiffuser&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#24230;&#12290;HanDiffuser&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;:Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#36755;&#20837;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;SMPL-&#36523;&#20307;&#21644;MANO-&#25163;&#37096;&#21442;&#25968;&#65292;&#20197;&#21450;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19978;&#19968;&#37096;&#20214;&#29983;&#25104;&#30340;&#25552;&#31034;&#21644;&#25163;&#37096;&#21442;&#25968;&#19978;&#36827;&#34892;&#35843;&#33410;&#26469;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#21512;&#24182;&#20102;&#25163;&#37096;&#34920;&#31034;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;3D&#24418;&#29366;&#21644;&#20851;&#33410;&#32423;&#25163;&#25351;&#20301;&#32622;&#12289;&#26041;&#21521;&#21644;&#20851;&#33410;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#23398;&#20064;&#21644;&#21487;&#38752;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01693v1 Announce Type: cross  Abstract: Text-to-image generative models can generate high-quality humans, but realism is lost when generating hands. Common artifacts include irregular hand poses, shapes, incorrect numbers of fingers, and physically implausible finger orientations. To generate images with realistic hands, we propose a novel diffusion-based architecture called HanDiffuser that achieves realism by injecting hand embeddings in the generative process. HanDiffuser consists of two components: a Text-to-Hand-Params diffusion model to generate SMPL-Body and MANO-Hand parameters from input text prompts, and a Text-Guided Hand-Params-to-Image diffusion model to synthesize images by conditioning on the prompts and hand parameters generated by the previous component. We incorporate multiple aspects of hand representation, including 3D shapes and joint-level finger positions, orientations and articulations, for robust learning and reliable performance during inference. We
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#27880;&#24847;&#21147;&#24314;&#27169;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#22270;&#20687;&#22788;&#29702;&#12289;&#35270;&#39057;&#22788;&#29702;&#12289;&#35270;&#35273;&#19982;&#35821;&#35328;&#24212;&#29992;&#21644;&#35821;&#35328;&#24314;&#27169;&#31561;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18673</link><description>&lt;p&gt;
&#20154;&#31867;&#27880;&#24847;&#21147;&#24314;&#27169;&#30340;&#36235;&#21183;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Trends, Applications, and Challenges in Human Attention Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18673
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27880;&#24847;&#21147;&#24314;&#27169;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#22270;&#20687;&#22788;&#29702;&#12289;&#35270;&#39057;&#22788;&#29702;&#12289;&#35270;&#35273;&#19982;&#35821;&#35328;&#24212;&#29992;&#21644;&#35821;&#35328;&#24314;&#27169;&#31561;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#31867;&#27880;&#24847;&#21147;&#24314;&#27169;&#19981;&#20165;&#22312;&#29702;&#35299;&#35270;&#35273;&#25506;&#32034;&#32972;&#21518;&#30340;&#35748;&#30693;&#36807;&#31243;&#26041;&#38754;&#29305;&#21035;&#26377;&#29992;&#65292;&#32780;&#19988;&#22312;&#20026;&#26088;&#22312;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#38382;&#39064;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#22788;&#29702;&#12289;&#35270;&#35273;&#19982;&#35821;&#35328;&#24212;&#29992;&#20197;&#21450;&#35821;&#35328;&#24314;&#27169;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#32508;&#36848;&#26368;&#26032;&#21162;&#21147;&#23558;&#20154;&#31867;&#27880;&#24847;&#26426;&#21046;&#25972;&#21512;&#21040;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#29702;&#30001;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;&#26377;&#20851;&#27491;&#22312;&#36827;&#34892;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#35831;&#21442;&#38405;&#25105;&#20204;&#22312; https://github.com/aimagelab/awesome-human-visual-attention &#19978;&#25552;&#20379;&#30340;&#19987;&#29992;&#23384;&#20648;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18673v1 Announce Type: cross  Abstract: Human attention modelling has proven, in recent years, to be particularly useful not only for understanding the cognitive processes underlying visual exploration, but also for providing support to artificial intelligence models that aim to solve problems in various domains, including image and video processing, vision-and-language applications, and language modelling. This survey offers a reasoned overview of recent efforts to integrate human attention mechanisms into contemporary deep learning models and discusses future research directions and challenges. For a comprehensive overview on the ongoing research refer to our dedicated repository available at https://github.com/aimagelab/awesome-human-visual-attention.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15163</link><description>&lt;p&gt;
&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#20013;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Studying the Impact of Stochasticity on the Evaluation of Deep Neural Networks for Forest-Fire Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#29992;&#20110;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#37326;&#28779;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#20004;&#31867;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#20998;&#31867;&#30340;&#25351;&#26631;&#65292;&#35780;&#20272;&#23545;&#35266;&#23519;&#22320;&#38754;&#30495;&#30456;&#65288;GT&#65289;&#30340;&#24544;&#23454;&#24230;&#65292;&#20197;&#21450;&#36866;&#24403;&#30340;&#24471;&#20998;&#35268;&#21017;&#65292;&#27979;&#35797;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26862;&#26519;&#28779;&#28798;&#25968;&#25454;&#65292;&#31361;&#26174;&#20102;&#20256;&#32479;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#21487;&#35299;&#37322;&#30340;&#36866;&#29992;&#20110;&#38543;&#26426;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15163v1 Announce Type: cross  Abstract: This paper presents the first systematic study of the evaluation of Deep Neural Networks (DNNs) for discrete dynamical systems under stochastic assumptions, with a focus on wildfire prediction. We develop a framework to study the impact of stochasticity on two classes of evaluation metrics: classification-based metrics, which assess fidelity to observed ground truth (GT), and proper scoring rules, which test fidelity-to-statistic. Our findings reveal that evaluating for fidelity-to-statistic is a reliable alternative in highly stochastic scenarios. We extend our analysis to real-world wildfire data, highlighting limitations in traditional wildfire prediction evaluation methods, and suggest interpretable stochasticity-compatible alternatives.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#25903;&#25345;&#19979;&#65292;&#33521;&#35821;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#30340;&#20934;&#30830;&#12289;&#26377;&#29992;&#22238;&#24212;&#65292;&#20294;&#23384;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.14778</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-lingual transfer in instruction tuning of large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#25903;&#25345;&#19979;&#65292;&#33521;&#35821;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#30340;&#20934;&#30830;&#12289;&#26377;&#29992;&#22238;&#24212;&#65292;&#20294;&#23384;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#65288;IT&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25945;&#23548;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#20219;&#24847;&#25351;&#20196;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;IT&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#24403;LLM&#22312;&#20165;&#33521;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#28982;&#21518;&#22312;&#20854;&#20182;&#35821;&#35328;&#29992;&#25143;&#25552;&#31034;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27169;&#22411;&#37197;&#32622;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#35780;&#20272;&#31574;&#30053;&#29992;&#20110;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#27169;&#22411;&#35757;&#32451;&#30340;&#25152;&#26377;&#38454;&#27573;&#37117;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#22312;IT&#20013;&#20063;&#20250;&#25104;&#21151;&#21457;&#29983;&#65292;&#20294;&#21482;&#26377;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#32771;&#34385;&#21040;&#22810;&#35821;&#35328;&#24615;&#20197;&#21450;&#26377;&#36275;&#22815;&#22823;&#30340;IT&#25968;&#25454;&#26102;&#25165;&#20250;&#21457;&#29983;&#12290;&#32463;&#36807;&#33521;&#35821;&#35757;&#32451;&#30340;LLMs&#33021;&#22815;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#29983;&#25104;&#20934;&#30830;&#12289;&#20840;&#38754;&#19988;&#26377;&#24110;&#21161;&#30340;&#22238;&#24212;&#65292;&#20294;&#32570;&#20047;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20598;&#23572;&#21487;&#33021;&#23384;&#22312;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14778v1 Announce Type: cross  Abstract: Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings. In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages. We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual instruction following. We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#36753;&#26032;&#20107;&#20214;&#21040;LLMs&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2402.13093</link><description>&lt;p&gt;
&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Event-level Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13093
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#36753;&#26032;&#20107;&#20214;&#21040;LLMs&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#65292;&#20197;&#38450;&#27490;&#23427;&#20204;&#36807;&#26102;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#20107;&#23454;&#30693;&#35782;&#19977;&#20803;&#32452;&#30340;&#32423;&#21035;&#19978;&#32534;&#36753;LLMs&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#30693;&#35782;&#26356;&#26032;&#26469;&#33258;&#26032;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#26356;&#25913;&#20107;&#23454;&#19977;&#20803;&#32452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#30452;&#25509;&#23558;&#26032;&#20107;&#20214;&#32534;&#36753;&#21040;LLMs&#20013;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;(1)&#25928;&#29575;&#12290;&#21333;&#20010;&#20107;&#20214;&#32534;&#36753;&#20250;&#23548;&#33268;&#22810;&#20010;&#25512;&#26029;&#30693;&#35782;&#19977;&#20803;&#32452;&#30340;&#26356;&#26032;&#12290;(2)&#23436;&#25972;&#24615;&#12290;&#38500;&#20102;&#26356;&#26032;&#20107;&#23454;&#30693;&#35782;&#22806;&#65292;&#20107;&#20214;&#32423;&#21035;&#30340;&#32534;&#36753;&#36824;&#38656;&#35201;&#32771;&#34385;&#20107;&#20214;&#24433;&#21709;&#65292;&#26356;&#26032;LLMs&#20851;&#20110;&#26410;&#26469;&#36235;&#21183;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#32423;&#21035;&#32534;&#36753;&#22522;&#20934;ELKEN&#65292;&#21253;&#25324;1,515&#20010;&#20107;&#20214;&#32534;&#36753;&#65292;6,449&#20010;&#20851;&#20110;&#20107;&#23454;&#30693;&#35782;&#30340;&#38382;&#39064;&#21644;10,150&#20010;&#20851;&#20110;&#26410;&#26469;&#21457;&#23637;&#36235;&#21183;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13093v1 Announce Type: cross  Abstract: Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24494;&#35843;&#23398;&#20064;&#29575;&#21487;&#20197;&#32531;&#35299;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20013;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#26159;&#24456;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;mBART&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#31867;&#20284;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#12290;</title><link>https://arxiv.org/abs/2402.12279</link><description>&lt;p&gt;
&#26377;&#25928;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12279
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#23398;&#20064;&#29575;&#21487;&#20197;&#32531;&#35299;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20013;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#26159;&#24456;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;mBART&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#31867;&#20284;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#24847;&#21619;&#30528;&#22312;&#19968;&#20010;&#35821;&#35328;&#19978;&#24494;&#35843;&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#36827;&#34892;&#27492;&#20219;&#21153;&#30340;&#39044;&#27979;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#19968;&#20010;&#32463;&#24120;&#20986;&#29616;&#30340;&#38382;&#39064;&#26159;&#20197;&#38169;&#35823;&#30340;&#35821;&#35328;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;mT5&#20316;&#20026;&#20027;&#24178;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#20013;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#36824;&#21253;&#25324;&#26367;&#20195;&#24615;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#21363;mBART&#21644;NLLB-200&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#24494;&#35843;&#25152;&#20351;&#29992;&#30340;&#23398;&#20064;&#29575;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#22823;&#22823;&#32531;&#35299;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#32454;&#33268;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#20316;&#20026;&#38750;&#24120;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;&#26367;&#20195;&#26041;&#27861;&#21482;&#24102;&#26469;&#24494;&#23567;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;mBART&#19982;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#34920;&#29616;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12279v1 Announce Type: cross  Abstract: Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.11753</link><description>&lt;p&gt;
ArtPrompt: &#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#23545;&#40784;LLMs&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11753
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#22914;&#25968;&#25454;&#36807;&#28388;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21152;&#24378;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24050;&#30693;&#30340;&#25216;&#26415;&#20551;&#35774;&#29992;&#20110;&#23545;&#40784;LLMs&#23433;&#20840;&#24615;&#30340;&#35821;&#26009;&#24211;&#20165;&#30001;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#19981;&#25104;&#31435;&#65292;&#23548;&#33268;LLMs&#23384;&#22312;&#20005;&#37325;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;Vision-in-Text Challenge&#65288;ViTC&#65289;&#26469;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#19981;&#33021;&#20165;&#36890;&#36807;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#30340;&#25552;&#31034;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20116;&#20010;SOTA LLMs&#65288;GPT-3.5&#12289;GPT-4&#12289;Gemini&#12289;Claude&#21644;Llama2&#65289;&#22312;&#35782;&#21035;&#20197;ASCII&#33402;&#26415;&#24418;&#24335;&#25552;&#20379;&#30340;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
&lt;/p&gt;</description></item><item><title>MultiCorrupt&#26159;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#22312;&#21313;&#31181;&#19981;&#21516;&#25968;&#25454;&#25439;&#22351;&#31867;&#22411;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.11677</link><description>&lt;p&gt;
MultiCorrupt&#65306;&#19968;&#31181;&#29992;&#20110;3D&#29289;&#20307;&#26816;&#27979;&#30340;LiDAR-&#30456;&#26426;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#40065;&#26834;&#24615;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11677
&lt;/p&gt;
&lt;p&gt;
MultiCorrupt&#26159;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#22312;&#21313;&#31181;&#19981;&#21516;&#25968;&#25454;&#25439;&#22351;&#31867;&#22411;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#25968;&#25454;&#38598;&#22914;nuScenes&#19978;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#23545;&#23494;&#38598;&#37319;&#26679;&#30340;LiDAR&#28857;&#20113;&#21644;&#31934;&#24515;&#26657;&#20934;&#30340;&#20256;&#24863;&#22120;&#38453;&#21015;&#20381;&#36182;&#24615;&#20351;&#24471;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MultiCorrupt&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#23545;&#21313;&#31181;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#25439;&#22351;&#30340;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11677v1 Announce Type: cross  Abstract: Multi-modal 3D object detection models for automated driving have demonstrated exceptional performance on computer vision benchmarks like nuScenes. However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications. Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference. To address this challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to evaluate the robustness of multi-modal 3D object detectors against ten distinct types of corruptions. We evaluate five state-of-the-art multi-modal detectors on MultiCorrupt and analyze their performance in terms of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FGeo-HyperGNet&#65292;&#23558;&#24418;&#24335;&#31526;&#21495;&#31995;&#32479;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#21160;&#25191;&#34892;&#20154;&#31867;&#21270;&#30340;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.11461</link><description>&lt;p&gt;
FGeo-HyperGNet: &#20960;&#20309;&#38382;&#39064;&#27714;&#35299;&#20013;&#38598;&#25104;&#24418;&#24335;&#31526;&#21495;&#31995;&#32479;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic System and Hypergraph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FGeo-HyperGNet&#65292;&#23558;&#24418;&#24335;&#31526;&#21495;&#31995;&#32479;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#21160;&#25191;&#34892;&#20154;&#31867;&#21270;&#30340;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#38382;&#39064;&#30340;&#27714;&#35299;&#19968;&#30452;&#26159;&#33258;&#21160;&#25512;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#36825;&#26159;&#25105;&#20204;&#31995;&#21015;&#20316;&#21697;&#20013;&#30340;&#31532;&#20116;&#31687;&#25991;&#31456;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#25191;&#34892;&#31867;&#20284;&#20154;&#31867;&#30340;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#12290;&#31526;&#21495;&#37096;&#20998;&#26159;&#24314;&#31435;&#22312;FormalGeo&#19978;&#30340;&#24418;&#24335;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#25191;&#34892;&#20960;&#20309;&#20851;&#31995;&#25512;&#29702;&#21644;&#20195;&#25968;&#35745;&#31639;&#65292;&#24182;&#23558;&#27714;&#35299;&#36807;&#31243;&#32452;&#32455;&#25104;&#19968;&#20010;&#24102;&#26377;&#26465;&#20214;&#20316;&#20026;&#36229;&#33410;&#28857;&#21644;&#23450;&#29702;&#20316;&#20026;&#36229;&#36793;&#30340;&#35299;&#20915;&#26041;&#26696;&#36229;&#26641;&#12290;&#31070;&#32463;&#37096;&#20998;&#31216;&#20026;HyperGNet&#65292;&#26159;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#19968;&#20010;&#32534;&#30721;&#22120;&#26469;&#26377;&#25928;&#32534;&#30721;&#36229;&#26641;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#38382;&#39064;&#27714;&#35299;&#25351;&#23548;&#12290;&#31070;&#32463;&#37096;&#20998;&#26681;&#25454;&#36229;&#26641;&#39044;&#27979;&#23450;&#29702;&#65292;&#32780;&#31526;&#21495;&#37096;&#20998;&#24212;&#29992;&#23450;&#29702;&#24182;&#26356;&#26032;&#36229;&#26641;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#39044;&#27979;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11461v1 Announce Type: new  Abstract: Geometry problem solving has always been a long-standing challenge in the fields of automated reasoning and artificial intelligence. This is the fifth article in a series of our works, we built a neural-symbolic system to automatically perform human-like geometric deductive reasoning. The symbolic part is a formal system built on FormalGeo, which can automatically perform geomertic relational reasoning and algebraic calculations and organize the solving process into a solution hypertree with conditions as hypernodes and theorems as hyperedges. The neural part, called HyperGNet, is a hypergraph neural network based on the attention mechanism, including a encoder to effectively encode the structural and semantic information of the hypertree, and a solver to provide problem-solving guidance. The neural part predicts theorems according to the hypertree, and the symbolic part applies theorems and updates the hypertree, thus forming a Predict-
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#36890;&#36807;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#30340;&#29420;&#29305;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;LLMs&#30340;&#25361;&#25112;&#21644;&#20154;&#31867;&#31867;&#20284;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#25512;&#21160;LLMs&#35299;&#35868;&#33021;&#21147;&#21644;&#36129;&#29486;&#20110;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11291</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#35299;&#20915;&#38590;&#39064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Puzzle Solving using Reasoning of Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#36890;&#36807;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#30340;&#29420;&#29305;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;LLMs&#30340;&#25361;&#25112;&#21644;&#20154;&#31867;&#31867;&#20284;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#25512;&#21160;LLMs&#35299;&#35868;&#33021;&#21147;&#21644;&#36129;&#29486;&#20110;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#38590;&#39064;&#20013;&#30340;&#33021;&#21147;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#26631;&#24535;&#30528;&#29702;&#35299;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#26412;&#35843;&#26597;&#21033;&#29992;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;LLMs&#65292;&#21253;&#25324;&#25552;&#31034;&#25216;&#26415;&#12289;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#25209;&#21028;&#24615;&#23457;&#26597;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#22797;&#26434;&#38590;&#39064;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#35782;&#21035;&#20986;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;&#30340;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#33021;&#21147;&#21450;&#31867;&#20154;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#39640;&#32423;&#36923;&#36753;&#25512;&#26029;&#30340;&#24773;&#20917;&#19979;&#12290;&#35843;&#26597;&#24378;&#35843;&#20102;&#38656;&#35201;&#26032;&#39062;&#31574;&#30053;&#21644;&#26356;&#20016;&#23500;&#25968;&#25454;&#38598;&#26469;&#25552;&#21319;LLMs&#30340;&#35299;&#35868;&#33021;&#21147;&#24182;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11291v1 Announce Type: cross  Abstract: Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's
&lt;/p&gt;</description></item><item><title>Jack of All Trades (JAT)&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26435;&#37325;&#38598;&#65292;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#22343;&#33021;&#21462;&#24471;&#24378;&#22823;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#23427;&#26159;&#39318;&#20010;&#23454;&#29616;&#35813;&#30446;&#26631;&#30340;&#24320;&#25918;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09844</link><description>&lt;p&gt;
&#35832;&#22810;&#25165;&#33402;&#65292;&#20854;&#20013;&#19968;&#20123;&#26159;&#22823;&#24072;&#65306;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#36716;&#25442;&#22120;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09844
&lt;/p&gt;
&lt;p&gt;
Jack of All Trades (JAT)&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26435;&#37325;&#38598;&#65292;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#22343;&#33021;&#21462;&#24471;&#24378;&#22823;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#23427;&#26159;&#39318;&#20010;&#23454;&#29616;&#35813;&#30446;&#26631;&#30340;&#24320;&#25918;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#23547;&#25214;&#19968;&#20010;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#26080;&#32541;&#36816;&#20316;&#30340;&#36890;&#29992;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#20027;&#27969;&#30340;&#26041;&#27861;&#24448;&#24448;&#23558;&#27169;&#22411;&#38480;&#21046;&#22312;&#21333;&#19968;&#20219;&#21153;&#21644;&#21333;&#27169;&#24577;&#26694;&#26550;&#20013;&#65292;&#36825;&#19968;&#38480;&#21046;&#19982;&#36890;&#29992;&#30340;&#12289;&#22810;&#39046;&#22495;&#27169;&#22411;&#30340;&#24191;&#38420;&#24895;&#26223;&#30456;&#30683;&#30462;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Jack of All Trades (JAT) &#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20854;&#29420;&#29305;&#35774;&#35745;&#20248;&#21270;&#20102;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#31867;&#22411;&#30340;&#33021;&#21147;&#12290;JAT&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26435;&#37325;&#38598;&#65292;&#22312;&#38750;&#24120;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#29616;&#20102;&#20854;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;JAT&#27169;&#22411;&#26159;&#26397;&#30528;&#26356;&#36890;&#29992;&#12289;&#36328;&#39046;&#22495;&#30340;AI&#27169;&#22411;&#35774;&#35745;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#24182;&#19988;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#26159;&#39318;&#20010;&#23436;&#20840;&#24320;&#25918;&#30340;&#36825;&#19968;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09844v1 Announce Type: new  Abstract: The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. In this paper, we present Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#23450;&#20041;&#30340;&#25351;&#26631;&#30340;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2402.08772</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#21644;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#26368;&#20248;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Optimal Task Assignment and Path Planning using Conflict-Based Search with Precedence and Temporal Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#23450;&#20041;&#30340;&#25351;&#26631;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#38382;&#39064;&#28041;&#21450;&#20026;&#19968;&#32452;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#24341;&#23548;&#23427;&#20204;&#20174;&#36215;&#28857;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;MAPF&#27809;&#26377;&#32771;&#34385;&#20960;&#20010;&#23454;&#38469;&#30340;&#20219;&#21153;&#30456;&#20851;&#32422;&#26463;&#12290;&#20363;&#22914;&#65292;&#26234;&#33021;&#20307;&#21487;&#33021;&#38656;&#35201;&#22312;&#30446;&#26631;&#20301;&#32622;&#25191;&#34892;&#20855;&#26377;&#29305;&#23450;&#25191;&#34892;&#26102;&#38388;&#30340;&#21160;&#20316;&#65292;&#36981;&#24490;&#39044;&#23450;&#30340;&#39034;&#24207;&#21644;&#26102;&#38388;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#30446;&#26631;&#20998;&#37197;&#21487;&#33021;&#19981;&#26159;&#39044;&#20808;&#23450;&#20041;&#30340;&#65292;&#20248;&#21270;&#30446;&#26631;&#21487;&#33021;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#12290;&#20026;&#20102;&#23558;&#20219;&#21153;&#20998;&#37197;&#12289;&#36335;&#24452;&#35268;&#21010;&#21644;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#32467;&#21512;&#25104;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#65288;TAPF-PTC&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#22686;&#21152;&#20102;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#65288;CBS&#65289;&#20197;&#21516;&#26102;&#29983;&#25104;&#36981;&#23432;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#24182;&#26368;&#22823;&#21270;&#22522;&#20110;&#29992;&#25143;&#23450;&#20041;&#30340;&#25351;&#26631;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08772v1 Announce Type: new Abstract: The Multi-Agent Path Finding (MAPF) problem entails finding collision-free paths for a set of agents, guiding them from their start to goal locations. However, MAPF does not account for several practical task-related constraints. For example, agents may need to perform actions at goal locations with specific execution times, adhering to predetermined orders and timeframes. Moreover, goal assignments may not be predefined for agents, and the optimization objective may lack an explicit definition. To incorporate task assignment, path planning, and a user-defined objective into a coherent framework, this paper examines the Task Assignment and Path Finding with Precedence and Temporal Constraints (TAPF-PTC) problem. We augment Conflict-Based Search (CBS) to simultaneously generate task assignments and collision-free paths that adhere to precedence and temporal constraints, maximizing an objective quantified by the return from a user-defined r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01830</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#21516;&#34892;&#35780;&#23457;&#26041;&#27861;&#65306;&#24320;&#25918;&#29615;&#22659;&#19979;LLMs&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#38598;&#20013;&#20110;&#22312;&#19968;&#20123;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#23553;&#38381;&#29615;&#22659;&#21644;&#29305;&#23450;&#39046;&#22495;&#22522;&#20934;&#19978;&#27979;&#35797;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#33258;&#21160;&#34913;&#37327;LLMs&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;LLMs&#22788;&#20110;&#21516;&#19968;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#22238;&#31572;&#26410;&#26631;&#35760;&#30340;&#38382;&#39064;&#24182;&#20114;&#30456;&#35780;&#20272;&#65292;&#27599;&#20010;LLM&#30340;&#21709;&#24212;&#24471;&#20998;&#30001;&#20854;&#20182;&#21311;&#21517;&#30340;LLMs&#20849;&#21516;&#20915;&#23450;&#12290;&#20026;&#20102;&#33719;&#21462;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#26469;&#35843;&#25972;&#26368;&#32456;&#25490;&#24207;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27599;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32972;&#21518;&#30340;&#20851;&#38190;&#20551;&#35774;&#26159;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#27604;&#20302;&#23618;&#27425;&#30340;LLM&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#32780;&#39640;&#23618;&#27425;&#30340;LLM&#20063;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.01201</link><description>&lt;p&gt;
PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PAC Privacy Preserving Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01201
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#27491;&#22312;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#26377;&#21487;&#33021;&#29983;&#25104;&#26082;&#20855;&#26377;&#39640;&#38544;&#31169;&#24615;&#21448;&#20855;&#26377;&#33391;&#22909;&#35270;&#35273;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#30830;&#20445;&#22312;&#31169;&#26377;&#21270;&#29305;&#23450;&#25968;&#25454;&#23646;&#24615;&#26102;&#30340;&#24378;&#22823;&#20445;&#25252;&#65292;&#24403;&#21069;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#38754;&#32463;&#24120;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21407;&#29702;&#24182;&#30830;&#20445;&#8220;&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#65288;PAC&#65289;&#8221;&#38544;&#31169;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;Langevin&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#27169;&#22411;&#38544;&#31169;&#24615;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#36825;&#20010;&#26032;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;&#30697;&#38453;&#35745;&#31639;&#25903;&#25345;PAC&#30028;&#38480;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#38544;&#31169;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01201v2 Announce Type: replace-cross  Abstract: Data privacy protection is garnering increased attention among researchers. Diffusion models (DMs), particularly with strict differential privacy, can potentially produce images with both high privacy and visual quality. However, challenges arise such as in ensuring robust protection in privatizing specific data attributes, areas where current models often fall short. To address these challenges, we introduce the PAC Privacy Preserving Diffusion Model, a model leverages diffusion principles and ensure Probably Approximately Correct (PAC) privacy. We enhance privacy protection by integrating a private classifier guidance into the Langevin Sampling Process. Additionally, recognizing the gap in measuring the privacy of models, we have developed a novel metric to gauge privacy levels. Our model, assessed with this new metric and supported by Gaussian matrix computations for the PAC bound, has shown superior performance in privacy p
&lt;/p&gt;</description></item><item><title>TransNeXt&#25552;&#20986;&#20102;Aggregated Attention&#65292;&#19968;&#31181;&#20223;&#29983;&#35774;&#35745;&#30340;&#20196;&#29260;&#28151;&#21512;&#22120;&#65292;&#36890;&#36807;&#27169;&#25311;&#29983;&#29289;&#35270;&#35273;&#21644;&#36830;&#32493;&#30524;&#21160;&#65292;&#20351;&#24471;&#27599;&#20010;&#29305;&#24449;&#22270;&#19978;&#30340;&#20196;&#29260;&#20855;&#26377;&#20840;&#23616;&#24863;&#30693;&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#21472;&#21152;&#23618;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#28145;&#24230;&#36864;&#21270;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#30340;&#35270;&#30693;&#35273;&#12290;</title><link>https://arxiv.org/abs/2311.17132</link><description>&lt;p&gt;
TransNeXt&#65306;Vision Transformers&#30340;&#40065;&#26834;&#20239;&#33033;&#35270;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
TransNeXt: Robust Foveal Visual Perception for Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17132
&lt;/p&gt;
&lt;p&gt;
TransNeXt&#25552;&#20986;&#20102;Aggregated Attention&#65292;&#19968;&#31181;&#20223;&#29983;&#35774;&#35745;&#30340;&#20196;&#29260;&#28151;&#21512;&#22120;&#65292;&#36890;&#36807;&#27169;&#25311;&#29983;&#29289;&#35270;&#35273;&#21644;&#36830;&#32493;&#30524;&#21160;&#65292;&#20351;&#24471;&#27599;&#20010;&#29305;&#24449;&#22270;&#19978;&#30340;&#20196;&#29260;&#20855;&#26377;&#20840;&#23616;&#24863;&#30693;&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#21472;&#21152;&#23618;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#28145;&#24230;&#36864;&#21270;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#30340;&#35270;&#30693;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27531;&#24046;&#36830;&#25509;&#20013;&#30340;&#28145;&#24230;&#36864;&#21270;&#25928;&#24212;&#65292;&#35768;&#22810;&#20381;&#36182;&#20110;&#21472;&#21152;&#23618;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#30340;&#39640;&#25928;Vision Transformers&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#24418;&#25104;&#36275;&#22815;&#30340;&#20449;&#24687;&#28151;&#21512;&#65292;&#23548;&#33268;&#35270;&#30693;&#35273;&#19981;&#33258;&#28982;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32858;&#21512;&#27880;&#24847;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20223;&#29983;&#35774;&#35745;&#30340;&#20196;&#29260;&#28151;&#21512;&#22120;&#65292;&#27169;&#25311;&#29983;&#29289;&#35270;&#35273;&#21644;&#36830;&#32493;&#30524;&#21160;&#65292;&#21516;&#26102;&#20351;&#29305;&#24449;&#22270;&#19978;&#30340;&#27599;&#20010;&#20196;&#29260;&#20855;&#26377;&#20840;&#23616;&#24863;&#30693;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#21487;&#23398;&#20064;&#30340;&#20196;&#29260;&#32435;&#20837;&#24120;&#35268;&#26597;&#35810;&#21644;&#23494;&#38053;&#20013;&#65292;&#36827;&#19968;&#27493;&#20351;&#20146;&#21644;&#30697;&#38453;&#30340;&#29983;&#25104;&#22810;&#26679;&#21270;&#65292;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#26597;&#35810;&#21644;&#23494;&#38053;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#21472;&#21152;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#65292;&#22240;&#27492;&#26377;&#25928;&#36991;&#20813;&#20102;&#28145;&#24230;&#36864;&#21270;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#28982;&#30340;&#35270;&#30693;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Convolutional GLU&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#25645;&#36215;&#20102;&#35270;&#30693;&#35273;&#20013;&#30340;&#26029;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17132v2 Announce Type: replace-cross  Abstract: Due to the depth degradation effect in residual connections, many efficient Vision Transformers models that rely on stacking layers for information exchange often fail to form sufficient information mixing, leading to unnatural visual perception. To address this issue, in this paper, we propose Aggregated Attention, a biomimetic design-based token mixer that simulates biological foveal vision and continuous eye movement while enabling each token on the feature map to have a global perception. Furthermore, we incorporate learnable tokens that interact with conventional queries and keys, which further diversifies the generation of affinity matrices beyond merely relying on the similarity between queries and keys. Our approach does not rely on stacking for information exchange, thus effectively avoiding depth degradation and achieving natural visual perception. Additionally, we propose Convolutional GLU, a channel mixer that bridg
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2310.02226</link><description>&lt;p&gt;
&#35880;&#35328;&#24910;&#34892;&#65306;&#20351;&#29992;&#26242;&#20572;&#26631;&#35760;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Think before you speak: Training Language Models With Pause Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02226
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#31435;&#21363;&#36830;&#32493;&#29983;&#25104;&#19968;&#31995;&#21015;&#26631;&#35760;&#26469;&#29983;&#25104;&#21709;&#24212;: &#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#26159;&#36890;&#36807;&#25805;&#20316;&#27599;&#23618;&#30340;$K$&#20010;&#38544;&#34255;&#21521;&#37327;&#24471;&#21040;&#30340;&#65292;&#27599;&#20010;&#21521;&#37327;&#23545;&#24212;&#19968;&#20010;&#21069;&#38754;&#30340;&#26631;&#35760;&#12290;&#22914;&#26524;&#25105;&#20204;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#20043;&#21069;&#25805;&#20316;&#26356;&#22810;&#30340;&#38544;&#34255;&#21521;&#37327;&#65292;&#27604;&#22914;&#35828;$K+10$&#20010;&#21602;&#65311;&#25105;&#20204;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#65288;&#21487;&#23398;&#20064;&#30340;&#65289;$\textit{pause}$&#26631;&#35760;&#65292;&#36825;&#19968;&#31995;&#21015;&#26631;&#35760;&#38468;&#21152;&#21040;&#36755;&#20837;&#21069;&#32512;&#19978;&#12290;&#28982;&#21518;&#25105;&#20204;&#24310;&#36831;&#25552;&#21462;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#30452;&#21040;&#26368;&#21518;&#19968;&#20010;&#26242;&#20572;&#26631;&#35760;&#34987;&#30475;&#21040;&#65292;&#20174;&#32780;&#20801;&#35768;&#27169;&#22411;&#22312;&#20570;&#20986;&#31572;&#26696;&#20043;&#21069;&#36827;&#34892;&#39069;&#22806;&#30340;&#35745;&#31639;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#25317;&#26377;1B&#21644;130M&#21442;&#25968;&#30340;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;$\textit{pause-training}$&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#22312;C4&#19978;&#36827;&#34892;&#20102;&#22240;&#26524;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#28085;&#30422;&#25512;&#29702;&#12289;&#38382;&#31572;&#12289;&#26222;&#36941;&#29702;&#35299;&#21644;&#20107;&#23454;&#22238;&#24518;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;infer
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02226v2 Announce Type: replace-cross  Abstract: Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that infer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.08898</link><description>&lt;p&gt;
&#26725;&#25509;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#65306;&#29702;&#35299;&#33258;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging State and History Representations: Understanding Self-Predictive RL. (arXiv:2401.08898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#26159;&#25152;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#36866;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#35768;&#22810;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#29702;&#35770;&#26694;&#26550;&#34987;&#24320;&#21457;&#29992;&#20110;&#29702;&#35299;&#20160;&#20040;&#26500;&#25104;&#20102;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20849;&#21516;&#23646;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#30475;&#20284;&#19981;&#21516;&#30340;&#29366;&#24577;&#21644;&#21382;&#21490;&#25277;&#35937;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;&#30446;&#26631;&#21644;&#20248;&#21270;&#65288;&#22914;&#20572;&#26799;&#24230;&#25216;&#26415;&#65289;&#22312;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#20013;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#20849;&#21516;&#20135;&#29983;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29366;&#24577;&#21644;&#21382;&#21490;&#30340;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#26631;&#20934;MDP&#12289;&#24102;&#26377;dist&#30340;MDP&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with dist
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.05631</link><description>&lt;p&gt;
DrawTalking&#65306;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05631
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;DrawTalking&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#12290;&#23427;&#24378;&#35843;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#31867;&#20284;&#32534;&#31243;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;iPad&#19978;&#23454;&#29616;&#20102;&#23427;&#12290;&#19968;&#39033;&#24320;&#25918;&#24335;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#35768;&#22810;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#29992;&#20363;&#30456;&#22865;&#21512;&#21644;&#36866;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#33258;&#28982;&#29992;&#25143;&#20013;&#24515;&#30028;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#36890;&#36807;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;800&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#19977;&#31181;&#27969;&#34892;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.05453</link><description>&lt;p&gt;
&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dimensionality-Aware Outlier Detection: Theoretical and Experimental Analysis. (arXiv:2401.05453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#36890;&#36807;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;800&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#19977;&#31181;&#27969;&#34892;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#20869;&#22312;&#32500;&#24230;&#30340;&#23616;&#37096;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#29702;&#35770;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#23427;&#34987;&#25512;&#23548;&#20026;&#19968;&#20010;&#21253;&#21547;&#26597;&#35810;&#28857;&#21644;&#38543;&#26426;&#36873;&#25321;&#30340;&#36817;&#37051;&#30340;&#28176;&#36817;&#23616;&#37096;&#26399;&#26395;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#12290;DAO&#30340;&#32500;&#24230;&#24863;&#30693;&#34892;&#20026;&#26159;&#30001;&#20110;&#23427;&#20197;&#29702;&#35770;&#19978;&#35777;&#26126;&#30340;&#26041;&#24335;&#20351;&#29992;&#23616;&#37096;LID&#20540;&#30340;&#23616;&#37096;&#20272;&#35745;&#12290;&#36890;&#36807;&#23545;800&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;DAO&#26126;&#26174;&#20248;&#20110;&#19977;&#31181;&#27969;&#34892;&#19988;&#37325;&#35201;&#30340;&#22522;&#20934;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65306;&#23616;&#37096;&#31163;&#32676;&#22240;&#23376;&#65288;LOF&#65289;&#65292;&#31616;&#21270;&#29256;LOF&#21644;kNN&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a nonparametric method for outlier detection that takes full account of local variations in intrinsic dimensionality within the dataset. Using the theory of Local Intrinsic Dimensionality (LID), our 'dimensionality-aware' outlier detection method, DAO, is derived as an estimator of an asymptotic local expected density ratio involving the query point and a close neighbor drawn at random. The dimensionality-aware behavior of DAO is due to its use of local estimation of LID values in a theoretically-justified way. Through comprehensive experimentation on more than 800 synthetic and real datasets, we show that DAO significantly outperforms three popular and important benchmark outlier detection methods: Local Outlier Factor (LOF), Simplified LOF, and kNN.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#19968;&#31181;AI&#39550;&#39542;&#25945;&#32451;&#30340;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#20449;&#24687;&#31867;&#22411;&#21644;&#21576;&#29616;&#26041;&#24335;&#23545;&#34920;&#29616;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.04206</link><description>&lt;p&gt;
&#20174;AI&#25945;&#32451;&#23398;&#20064;&#36187;&#36710;&#65306;&#22810;&#27169;&#24577;&#33258;&#21160;&#39550;&#39542;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Learning Racing From an AI Coach: Effects of Multimodal Autonomous Driving Explanations on Driving Performance, Cognitive Load, Expertise, and Trust. (arXiv:2401.04206v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#19968;&#31181;AI&#39550;&#39542;&#25945;&#32451;&#30340;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#20449;&#24687;&#31867;&#22411;&#21644;&#21576;&#29616;&#26041;&#24335;&#23545;&#34920;&#29616;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#39033;&#21069;&#21518;&#23454;&#39564;&#20013;&#65288;n=41&#65289;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#27169;&#20223;&#20154;&#31867;&#39550;&#39542;&#19987;&#23478;&#30340;&#25351;&#23548;&#35828;&#26126;&#30340;AI&#25945;&#32451;&#30340;&#35299;&#37322;&#27807;&#36890;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#20449;&#24515;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#21442;&#19982;&#32773;&#34987;&#20998;&#20026;&#22235;&#20010;&#32452;&#65292;&#35780;&#20272;&#20102;AI&#25945;&#32451;&#35299;&#37322;&#30340;&#20004;&#20010;&#32500;&#24230;&#65306;&#20449;&#24687;&#31867;&#22411;&#65288;'what'&#21644;'why'-type&#35299;&#37322;&#65289;&#21644;&#21576;&#29616;&#26041;&#24335;&#65288;&#21548;&#35273;&#21644;&#35270;&#35273;&#65289;&#12290;&#36890;&#36807;&#37319;&#35775;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21442;&#19982;&#32773;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#27604;&#36739;&#21508;&#32452;&#20043;&#38388;&#65292;&#25105;&#20204;&#21457;&#29616;&#20449;&#24687;&#30340;&#31867;&#22411;&#21644;&#26041;&#24335;&#23545;&#24615;&#33021;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#24046;&#24322;&#24402;&#22240;&#20110;&#20449;&#24687;&#22914;&#20309;&#24341;&#23548;&#27880;&#24847;&#21147;&#65292;&#20943;&#36731;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24433;&#21709;&#21442;&#19982;&#32773;&#32463;&#21382;&#30340;&#36127;&#33655;&#36807;&#36733;&#12290;&#36825;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#20102;&#20449;&#24515;&#21644;&#20449;&#20219;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a pre-post experiment (n = 41), we test the impact of an AI Coach's explanatory communications modeled after the instructions of human driving experts. Participants were divided into four (4) groups to assess two (2) dimensions of the AI coach's explanations: information type ('what' and 'why'-type explanations) and presentation modality (auditory and visual). We directly compare how AI Coaching sessions employing these techniques impact driving performance, cognitive load, confidence, expertise, and trust in an observation learning context. Through interviews, we delineate the learning process of our participants. Results show that an AI driving coach can be useful for teaching performance driving skills to novices. Comparing between groups, we find the type and modality of information influences performance outcomes. We attribute differences to how information directed attention, mitigated uncertainty, and influenced overload experienced by participants. These, in turn, affected h
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20415;&#21033;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#22810;&#20307;&#21407;&#23376;&#20851;&#31995;&#24314;&#27169;&#21644;&#38190;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02683</link><description>&lt;p&gt;
&#29992;&#20110;3D&#20998;&#23376;&#29983;&#25104;&#30340;&#20960;&#20309;&#20415;&#21033;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation. (arXiv:2401.02683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02683
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20415;&#21033;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#22810;&#20307;&#21407;&#23376;&#20851;&#31995;&#24314;&#27169;&#21644;&#38190;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#38754;&#21521;&#20840;&#26032;3D&#20998;&#23376;&#29983;&#25104;&#30340;&#25193;&#25955;&#22522;&#20110;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#20998;&#23376;&#20013;&#30340;&#22823;&#22810;&#25968;&#37325;&#21407;&#23376;&#36890;&#36807;&#21333;&#38190;&#19982;&#22810;&#20010;&#21407;&#23376;&#30456;&#36830;&#65292;&#20165;&#20351;&#29992;&#25104;&#23545;&#36317;&#31163;&#26469;&#27169;&#25311;&#20998;&#23376;&#20960;&#20309;&#26159;&#19981;&#36275;&#30340;&#12290;&#22240;&#27492;&#65292;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#25552;&#20986;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#22810;&#20307;&#21407;&#23376;&#38388;&#20851;&#31995;&#21644;&#23398;&#20064;&#39640;&#36136;&#37327;&#29305;&#24449;&#30340;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21435;&#22122;&#20869;&#26680;&#12290;&#30001;&#20110;&#22270;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#38754;&#23545;&#20998;&#23376;&#30340;&#20027;&#27969;&#25193;&#25955;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#24182;&#20197;&#38388;&#25509;&#26041;&#24335;&#29983;&#25104;&#36793;&#32536;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#28041;&#21450;&#23558;&#20998;&#23376;&#30340;&#29983;&#25104;&#19982;&#25193;&#25955;&#30456;&#32467;&#21512;&#65292;&#24182;&#20934;&#30830;&#39044;&#27979;&#38190;&#30340;&#23384;&#22312;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#26356;&#26032;&#20998;&#23376;&#26500;&#22411;&#30340;&#36845;&#20195;&#26041;&#24335;&#19982;&#20998;&#23376;&#21160;&#21147;&#23398;&#19968;&#33268;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have shown great potential in multiple research areas. Existing diffusion-based generative methods on de novo 3D molecule generation face two major challenges. Since majority heavy atoms in molecules allow connections to multiple atoms through single bonds, solely using pair-wise distance to model molecule geometries is insufficient. Therefore, the first one involves proposing an effective neural network as the denoising kernel that is capable to capture complex multi-body interatomic relationships and learn high-quality features. Due to the discrete nature of graphs, mainstream diffusion-based methods for molecules heavily rely on predefined rules and generate edges in an indirect manner. The second challenge involves accommodating molecule generation to diffusion and accurately predicting the existence of bonds. In our research, we view the iterative way of updating molecule conformations in diffusion process is consistent with molecular dynamics and introd
&lt;/p&gt;</description></item><item><title>NODEC&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01836</link><description>&lt;p&gt;
NODEC: &#29992;&#20110;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#26368;&#20248;&#25511;&#21046;&#30340;&#31070;&#32463;ODE
&lt;/p&gt;
&lt;p&gt;
NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems. (arXiv:2401.01836v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01836
&lt;/p&gt;
&lt;p&gt;
NODEC&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#36890;&#24120;&#28041;&#21450;&#22312;&#21464;&#21270;&#35745;&#31639;&#26694;&#26550;&#19979;&#26368;&#23567;&#21270;&#20855;&#26377;&#24050;&#30693;&#21160;&#21147;&#23398;&#30340;&#26576;&#20123;&#25511;&#21046;&#30446;&#26631;&#12290;&#23545;&#20110;&#20855;&#26377;&#26410;&#30693;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#65292;&#38656;&#35201;&#39069;&#22806;&#36827;&#34892;&#21160;&#21147;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#20219;&#20309;&#19981;&#20934;&#30830;&#37117;&#20250;&#23548;&#33268;&#32467;&#26524;&#25511;&#21046;&#20989;&#25968;&#30340;&#27425;&#20248;&#24615;&#12290;&#21478;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861; - &#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#34701;&#20837;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#24191;&#27867;&#20132;&#20114;&#26469;&#36817;&#20284;&#20540;&#20989;&#25968;&#25110;&#31574;&#30053;&#26799;&#24230;&#65292;&#20294;&#23427;&#30340;&#25968;&#25454;&#25928;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NODEC&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#20004;&#20010;&#32806;&#21512;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#26377;&#36259;&#30456;&#20114;&#20316;&#29992;&#65292;NODEC&#23398;&#20064;&#20102;&#31995;&#32479;&#21160;&#21147;&#23398;&#20197;&#21450;&#25351;&#23548;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework. For systems with unknown dynamics, an additional step of dynamics modeling is required. However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function. Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency. To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model. Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#26041;&#31243;&#65292;&#27491;&#24335;&#24314;&#31435;&#20102;GNN&#19982;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25551;&#36848;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.08616</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#36890;&#29992;&#31070;&#32463;&#25193;&#25955;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalized Neural Diffusion Framework on Graphs. (arXiv:2312.08616v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#26041;&#31243;&#65292;&#27491;&#24335;&#24314;&#31435;&#20102;GNN&#19982;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25551;&#36848;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#21644;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#28608;&#21457;&#20102;&#35768;&#22810;&#22522;&#20110;&#25193;&#25955;&#30340;GNN&#30340;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20004;&#31181;&#26426;&#21046;&#23494;&#20999;&#30456;&#20851;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#33258;&#28982;&#22320;&#20135;&#29983;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#27491;&#24335;&#32479;&#19968;&#36825;&#20123;GNN&#30340;&#36890;&#29992;&#25193;&#25955;&#26694;&#26550;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#19981;&#20165;&#21487;&#20197;&#21152;&#28145;&#25105;&#20204;&#23545;GNN&#23398;&#20064;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#25171;&#24320;&#19968;&#20010;&#35774;&#35745;&#24191;&#27867;&#26032;&#30340;GNN&#31867;&#21035;&#30340;&#26032;&#22823;&#38376;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#36890;&#29992;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#23427;&#27491;&#24335;&#24314;&#31435;&#20102;&#25193;&#25955;&#36807;&#31243;&#19982;&#26356;&#22810;GNN&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#25193;&#25955;&#32593;&#32476;&#30340;&#19968;&#20010;&#29305;&#24449;&#65292;&#21363;&#24403;&#21069;&#31070;&#32463;&#25193;&#25955;&#36807;&#31243;&#21482;&#23545;&#24212;&#20110;&#19968;&#38454;&#25193;&#25955;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#23454;&#38469;&#19978;&#34920;&#29616;&#20986;&#21333;&#19968;&#24615;&#29305;&#24449;&#65292;&#36825;&#24341;&#21457;&#20102;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies reveal the connection between GNNs and the diffusion process, which motivates many diffusion-based GNNs to be proposed. However, since these two mechanisms are closely related, one fundamental question naturally arises: Is there a general diffusion framework that can formally unify these GNNs? The answer to this question can not only deepen our understanding of the learning process of GNNs, but also may open a new door to design a broad new class of GNNs. In this paper, we propose a general diffusion equation framework with the fidelity term, which formally establishes the relationship between the diffusion process with more GNNs. Meanwhile, with this framework, we identify one characteristic of graph diffusion networks, i.e., the current neural diffusion process only corresponds to the first-order diffusion equation. However, by an experimental investigation, we show that the labels of high-order neighbors actually exhibit monophily property, which induces the similarit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#24072;&#20195;&#29702;&#30340;&#25351;&#23548;&#65292;&#23398;&#29983;&#20195;&#29702;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#19987;&#38376;&#21270;&#20110;&#29305;&#23450;&#30446;&#26631;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2311.13373</link><description>&lt;p&gt;
&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#25945;&#24072;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents. (arXiv:2311.13373v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#24072;&#20195;&#29702;&#30340;&#25351;&#23548;&#65292;&#23398;&#29983;&#20195;&#29702;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#19987;&#38376;&#21270;&#20110;&#29305;&#23450;&#30446;&#26631;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#36807;&#25552;&#20379;&#39640;&#32423;&#25351;&#23548;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32570;&#20047;&#19987;&#38376;&#21270;&#22788;&#29702;&#29305;&#23450;&#30446;&#26631;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#23454;&#26102;&#21160;&#24577;&#29615;&#22659;&#20013;&#12290;&#27492;&#22806;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#37096;&#32626;&#21040;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#35757;&#32451;&#19987;&#38376;&#21270;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#20195;&#29702;&#65292;&#20294;&#24448;&#24448;&#36973;&#21463;&#20302;&#37319;&#26679;&#25928;&#29575;&#21644;&#39640;&#25506;&#32034;&#25104;&#26412;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#25945;&#24072;&#20195;&#29702;&#30340;&#25351;&#23548;&#26469;&#35757;&#32451;&#19968;&#20010;&#36739;&#23567;&#12289;&#19987;&#38376;&#21270;&#30340;&#23398;&#29983;RL&#20195;&#29702;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#25945;&#24072;&#20195;&#29702;&#30340;&#20808;&#21069;&#30693;&#35782;&#34701;&#20837;&#23398;&#29983;&#20195;&#29702;&#20013;&#65292;&#23398;&#29983;&#20195;&#29702;&#33021;&#22815;&#20957;&#32858;LLM&#30340;&#30693;&#35782;&#21040;&#33258;&#24049;&#30340;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#23398;&#29983;&#20195;&#29702;&#21487;&#20197;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent studies have uncovered the potential of Large Language Models (LLMs) in addressing complex sequential decision-making tasks through the provision of high-level instructions. However, LLM-based agents lack specialization in tackling specific target problems, particularly in real-time dynamic environments. Additionally, deploying an LLM-based agent in practical scenarios can be both costly and time-consuming. On the other hand, reinforcement learning (RL) approaches train agents that specialize in the target task but often suffer from low sampling efficiency and high exploration costs. In this paper, we introduce a novel framework that addresses these challenges by training a smaller, specialized student RL agent using instructions from an LLM-based teacher agent. By incorporating the guidance from the teacher agent, the student agent can distill the prior knowledge of the LLM into its own model. Consequently, the student agent can be trained with significantly less data. Moreover
&lt;/p&gt;</description></item><item><title>GPT-4&#36890;&#36807;&#20102;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#30340;41%&#30340;&#28216;&#25103;&#65292;&#22312;&#35821;&#35328;&#39118;&#26684;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#36739;&#20339;&#65292;&#20294;&#20173;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#27700;&#24179;&#12290;&#22270;&#28789;&#27979;&#35797;&#20173;&#28982;&#26159;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#30340;&#30456;&#20851;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20216</link><description>&lt;p&gt;
GPT-4 &#26159;&#21542;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does GPT-4 Pass the Turing Test?. (arXiv:2310.20216v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20216
&lt;/p&gt;
&lt;p&gt;
GPT-4&#36890;&#36807;&#20102;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#30340;41%&#30340;&#28216;&#25103;&#65292;&#22312;&#35821;&#35328;&#39118;&#26684;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#36739;&#20339;&#65292;&#20294;&#20173;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#27700;&#24179;&#12290;&#22270;&#28789;&#27979;&#35797;&#20173;&#28982;&#26159;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#30340;&#30456;&#20851;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102; GPT-4&#12290;&#22312;&#34920;&#29616;&#26368;&#22909;&#30340; GPT-4 &#25552;&#31034;&#20013;&#65292;&#22312; 41% &#30340;&#28216;&#25103;&#20013;&#36890;&#36807;&#20102;&#27979;&#35797;&#65292;&#36229;&#36807;&#20102; ELIZA&#65288;27%&#65289;&#21644; GPT-3.5&#65288;14%&#65289;&#35774;&#23450;&#30340;&#22522;&#20934;&#65292;&#20294;&#36824;&#19981;&#22914;&#20154;&#31867;&#21442;&#19982;&#32773;&#65288;63%&#65289;&#30340;&#26426;&#20250;&#21644;&#22522;&#20934;&#12290;&#21442;&#19982;&#32773;&#30340;&#20915;&#31574;&#20027;&#35201;&#22522;&#20110;&#35821;&#35328;&#39118;&#26684;&#65288;35%&#65289;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#65288;27%&#65289;&#65292;&#25903;&#25345;&#26234;&#33021;&#19981;&#36275;&#20197;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#30340;&#35266;&#28857;&#12290;&#21442;&#19982;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#29305;&#24449;&#65292;&#21253;&#25324;&#25945;&#32946;&#27700;&#24179;&#21644;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29087;&#24713;&#24230;&#65292;&#24182;&#19981;&#33021;&#39044;&#27979;&#34987;&#35782;&#21035;&#29575;&#65292;&#36825;&#34920;&#26126;&#21363;&#20351;&#26159;&#28145;&#20837;&#20102;&#35299;&#31995;&#32479;&#24182;&#39057;&#32321;&#19982;&#20854;&#20132;&#20114;&#30340;&#20154;&#65292;&#20063;&#20250;&#23481;&#26131;&#34987;&#27450;&#39575;&#12290;&#23613;&#31649;&#22270;&#28789;&#27979;&#35797;&#20316;&#20026;&#26234;&#33021;&#30340;&#27979;&#35797;&#20855;&#26377;&#24050;&#30693;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#22312;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#26041;&#38754;&#20173;&#28982;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;&#20855;&#26377;&#20882;&#20805;&#20154;&#31867;&#33021;&#21147;&#30340; AI &#27169;&#22411;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#24191;&#27867;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#31574;&#30053;&#21644;&#26631;&#20934;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants' decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria fo
&lt;/p&gt;</description></item><item><title>HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11102</link><description>&lt;p&gt;
HGCVAE: &#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11102
&lt;/p&gt;
&lt;p&gt;
HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#29983;&#25104;&#24335;SSL&#22312;&#24322;&#26500;&#22270;&#23398;&#20064;&#65288;HGL&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#20851;&#20110;&#24322;&#26500;&#22270;&#30340;SSL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#65292;&#38656;&#35201;&#35774;&#35745;&#22797;&#26434;&#30340;&#35270;&#22270;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;SSL&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;HGL&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGCVAE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;HGL&#25670;&#33073;&#20102;&#22797;&#26434;&#24322;&#36136;&#24615;&#30340;&#36127;&#25285;&#12290;HGCVAE&#19981;&#20877;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#26159;&#20805;&#20998;&#21033;&#29992;&#20102;&#29983;&#25104;&#24335;SSL&#30340;&#28508;&#21147;&#12290;HGCVAE&#21019;&#26032;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29983;&#25104;&#24335;SSL&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#26426;&#21046;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;hard&#26679;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04870</link><description>&lt;p&gt;
Lemur&#65306;&#22312;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#23637;&#31034;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#25277;&#35937;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#39564;&#35777;&#24037;&#20855;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#30340;&#33021;&#21147;&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#27491;&#24335;&#25551;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#35770;&#65292;&#23558;&#20854;&#20316;&#20026;&#25512;&#23548;&#35268;&#21017;&#30340;&#38598;&#21512;&#36827;&#34892;&#35770;&#35777;&#20854;&#23436;&#22791;&#24615;&#12290;&#25105;&#20204;&#23558;&#35745;&#31639;&#26426;&#25512;&#29702;&#24418;&#25104;&#20026;&#19968;&#20010;&#23436;&#22791;&#30340;&#33258;&#21160;&#39564;&#35777;&#36807;&#31243;&#65292;&#36825;&#22312;&#19968;&#32452;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#24102;&#26469;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#20302;&#21442;&#25968;&#25104;&#26412;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08513</link><description>&lt;p&gt;
&#36890;&#36807;&#26174;&#33879;&#36890;&#36947;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#31616;&#21333;&#22522;&#20934;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels. (arXiv:2309.08513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#20302;&#21442;&#25968;&#25104;&#26412;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#31034;&#20248;&#21183;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#24494;&#35843;&#39069;&#22806;&#30340;1%&#21442;&#25968;&#23601;&#33021;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24494;&#35843;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#24573;&#35270;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#31216;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#65288;SCT&#65289;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#19982;&#20219;&#21153;&#22270;&#20687;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#65292;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#65292;&#20351;&#24471;&#25105;&#20204;&#21482;&#38656;&#35201;&#24494;&#35843;&#20854;&#20013;&#30340;1/8&#36890;&#36947;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#21442;&#25968;&#25104;&#26412;&#24182;&#22312;VTAB-1K&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;18&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#12290;&#36825;&#20165;&#22686;&#21152;&#20102;0.11M ViT-B&#21442;&#25968;&#65292;&#30456;&#27604;&#20840;&#38754;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;780&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision transformers have strong representation benefits to various downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT) methods have been proposed, and their experiments demonstrate that tuning only 1% of extra parameters could surpass full fine-tuning in low-data resource scenarios. However, these methods overlook the task-specific information when fine-tuning diverse downstream tasks. In this paper, we propose a simple yet effective method called "Salient Channel Tuning" (SCT) to leverage the task-specific information by forwarding the model with the task images to select partial channels in a feature map that enables us to tune only 1/8 channels leading to significantly lower parameter costs. Experiments outperform full fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only 0.11M parameters of the ViT-B, which is 780$\times$ fewer than its full fine-tuning counterpart. Furthermore, experiments on domain generalization and few-shot le
&lt;/p&gt;</description></item><item><title>C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.07597</link><description>&lt;p&gt;
C-Pack: &#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#25171;&#21253;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07597
&lt;/p&gt;
&lt;p&gt;
C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;C-Pack&#65292;&#36825;&#26159;&#19968;&#22871;&#26174;&#33879;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#12290;C-Pack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#36164;&#28304;&#12290;1&#65289;C-MTEB&#26159;&#19968;&#20010;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;35&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;2&#65289;C-MTP&#26159;&#19968;&#20010;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#27721;&#35821;&#35821;&#26009;&#24211;&#20013;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;3&#65289;C-TEM&#26159;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;C-MTEB&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#25152;&#26377;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#36798;&#21040;&#20102;&#21457;&#24067;&#26102;&#30340;&#26368;&#39640;+10%&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#21644;&#20248;&#21270;&#20102;C-TEM&#30340;&#25972;&#22871;&#35757;&#32451;&#26041;&#27861;&#12290;&#38500;&#20102;&#25105;&#20204;&#20851;&#20110;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#36164;&#28304;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#33521;&#35821;&#27169;&#22411;&#22312;MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#24067;&#30340;&#33521;&#35821;&#25968;&#25454;&#27604;&#27721;&#35821;&#25968;&#25454;&#22823;2&#20493;&#12290;&#25152;&#26377;&#36825;&#20123;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;https://github.com/FlagOpen/FlagEmbedding&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07136</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;Transformer&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#24212;&#29992;&#20013;&#26368;&#37325;&#35201;&#30340;&#35786;&#26029;&#24037;&#20855;&#20043;&#19968;&#12290;&#38543;&#30528;&#20808;&#36827;&#31639;&#27861;&#30340;&#20986;&#29616;&#65292;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;ECG&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#20854;&#22312;ECG&#25968;&#25454;&#19978;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#23454;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#29992;&#30340;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;MTECG&#65292;&#23427;&#23558;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;220,251&#20010;ECG&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35786;&#26029;&#27880;&#37322;&#65292;&#20197;&#25506;&#32034;MTECG&#30340;&#29305;&#24615;&#12290;&#22312;&#25552;&#20986;&#30340;&#35757;&#32451;&#31574;&#30053;&#19979;&#65292;&#19968;&#20010;&#21482;&#26377;5.7M&#21442;&#25968;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#65288;5%-75%&#65289;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#12290;&#28040;&#34701;&#30740;&#31350;&#31361;&#20986;&#20102;&#27874;&#21160;&#37325;&#26500;&#30446;&#26631;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;MTECG&#32791;&#26102;&#36739;&#23569;&#19988;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;&#21508;&#31181;&#24515;&#30005;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) is one of the most important diagnostic tools in clinical applications. With the advent of advanced algorithms, various deep learning models have been adopted for ECG tasks. However, the potential of Transformers for ECG data is not yet realized, despite their widespread success in computer vision and natural language processing. In this work, we present a useful masked Transformer method for ECG classification referred to as MTECG, which expands the application of masked autoencoders to ECG time series. We construct a dataset comprising 220,251 ECG recordings with a broad range of diagnoses annoated by medical experts to explore the properties of MTECG. Under the proposed training strategies, a lightweight model with 5.7M parameters performs stably well on a broad range of masking ratios (5%-75%). The ablation studies highlight the importance of fluctuated reconstruction targets, training schedule length, layer-wise LR decay and DropPath rate. The experiments o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;ViTScore&#26469;&#35780;&#20272;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#22312;&#35821;&#20041;&#36890;&#20449;&#20013;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04891</link><description>&lt;p&gt;
&#22914;&#20309;&#29992;ViTScore&#24230;&#37327;&#22270;&#20687;&#30340;&#35821;&#20041;&#36890;&#20449;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Evaluate Semantic Communications for Images with ViTScore Metric?. (arXiv:2309.04891v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;ViTScore&#26469;&#35780;&#20272;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#22312;&#35821;&#20041;&#36890;&#20449;&#20013;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#34987;&#26399;&#26395;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#25512;&#21160;&#19979;&#19968;&#20195;&#36890;&#20449;&#30340;&#21457;&#23637;&#65292;&#20854;&#20027;&#35201;&#20851;&#27880;&#28857;&#20174;&#31934;&#30830;&#30340;&#27604;&#29305;&#20256;&#36755;&#36716;&#31227;&#21040;&#20102;&#22312;&#36890;&#20449;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#35821;&#20041;&#20449;&#24687;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#20687;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#19981;&#36866;&#29992;&#20110;&#35780;&#20272;SC&#20013;&#30340;&#22270;&#20687;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#29992;&#20110;&#34913;&#37327;&#20004;&#20010;&#22270;&#20687;&#20043;&#38388;&#30456;&#20284;&#24230;&#30340;&#32463;&#20856;&#24230;&#37327;&#26631;&#20934;&#36890;&#24120;&#20381;&#36182;&#20110;&#20687;&#32032;&#32423;&#25110;&#32467;&#26500;&#32423;&#65292;&#20363;&#22914;PSNR&#21644;MS-SSIM&#12290;&#22312;SC&#20013;&#30452;&#25509;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26576;&#20123;&#23450;&#21046;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;LPIPS&#65289;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;NLP&#39046;&#22495;&#30340;BERTScore&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;Vision Transformer Score&#65288;ViTScore&#65289;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ViTScore&#20855;&#26377;&#23545;&#31216;&#24615;&#12289;&#26377;&#30028;&#24615;&#21644;&#24402;&#19968;&#21270;&#31561;&#19977;&#20010;&#37325;&#35201;&#23646;&#24615;&#65292;&#36825;&#20351;&#24471;ViTScore&#22312;&#22270;&#20687;&#34913;&#37327;&#20013;&#26041;&#20415;&#21644;&#30452;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communications (SC) have been expected to be a new paradigm shifting to catalyze the next generation communication, whose main concerns shift from accurate bit transmission to effective semantic information exchange in communications. However, the previous and widely-used metrics for images are not applicable to evaluate the image semantic similarity in SC. Classical metrics to measure the similarity between two images usually rely on the pixel level or the structural level, such as the PSNR and the MS-SSIM. Straightforwardly using some tailored metrics based on deep-learning methods in CV community, such as the LPIPS, is infeasible for SC. To tackle this, inspired by BERTScore in NLP community, we propose a novel metric for evaluating image semantic similarity, named Vision Transformer Score (ViTScore). We prove theoretically that ViTScore has 3 important properties, including symmetry, boundedness, and normalization, which make ViTScore convenient and intuitive for image mea
&lt;/p&gt;</description></item><item><title>Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.16475</link><description>&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Transformer Compression via Subspace Projection. (arXiv:2308.16475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16475
&lt;/p&gt;
&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TCSP&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#30340;&#38544;&#34255;&#22823;&#23567;&#26469;&#21387;&#32553;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;&#36716;&#25442;&#27169;&#22411;&#25237;&#24433;&#21040;&#19968;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#30697;&#38453;&#19982;&#20943;&#23567;&#32500;&#24230;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#20010;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#19981;&#21516;&#23618;&#27425;&#30340;&#37319;&#26679;&#25968;&#25454;&#23454;&#20363;&#30340;&#29305;&#24449;&#30697;&#38453;&#20998;&#35299;&#20026;&#19968;&#20010;&#25237;&#24433;&#30697;&#38453;&#12290;&#20026;&#20102;&#35780;&#20272;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#24212;&#29992;TCSP&#26469;&#21387;&#32553;T5&#21644;BERT&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TCSP&#22312;&#20445;&#35777;&#26368;&#22810;1.6%&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;44%&#30340;&#21387;&#32553;&#27604;&#65292;&#36229;&#36807;&#25110;&#32773;&#36798;&#21040;&#20102;&#20808;&#21069;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;TCSP&#36824;&#19982;&#20854;&#20182;&#30446;&#26631;&#36807;&#28388;&#22120;&#21644;&#27880;&#24847;&#21147;&#22836;&#22823;&#23567;&#21387;&#32553;&#30340;&#26041;&#27861;&#30456;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this subspace, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. For evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44\% with at most 1.6\% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;LLM&#29983;&#25104;&#30340;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#34920;&#31034;&#20026;&#25512;&#29702;&#22270;&#65292;&#20174;&#32780;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.09267</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#39564;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach. (arXiv:2308.09267v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;LLM&#29983;&#25104;&#30340;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#34920;&#31034;&#20026;&#25512;&#29702;&#22270;&#65292;&#20174;&#32780;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#31561;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#21463;&#29305;&#23450;&#35774;&#35745;&#30340;&#25552;&#31034;&#25351;&#23548;&#19979;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#36825;&#19981;&#20165;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36824;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#38382;&#39064;&#27714;&#35299;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#25552;&#21319;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;LLM&#36755;&#20986;&#39564;&#35777;&#22120;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#36825;&#20123;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#24819;&#19968;&#20010;&#25512;&#29702;&#20219;&#21153;&#30340;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#30001;LLM&#29983;&#25104;&#65292;&#21487;&#20197;&#30001;&#25512;&#29702;&#22270;&#34920;&#31034;&#65292;&#22240;&#20026;&#19981;&#21516;&#25512;&#29702;&#36335;&#24452;&#30340;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#23384;&#22312;&#36923;&#36753;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25512;&#29702;&#22270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased impressive reasoning capabilities, particularly when guided by specifically designed prompts in complex reasoning tasks such as math word problems. These models typically solve tasks using a chain-of-thought approach, which not only bolsters their reasoning abilities but also provides valuable insights into their problem-solving process. However, there is still significant room for enhancing the reasoning abilities of LLMs. Some studies suggest that the integration of an LLM output verifier can boost reasoning accuracy without necessitating additional model training. In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs. We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths. Therefore, we propose the Reasoning Graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15051</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#22312;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#24739;&#32773;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;TrialGPT&#65292;&#37319;&#29992;LLMs&#39044;&#27979;&#22522;&#20110;&#26631;&#20934;&#30340;&#21512;&#26684;&#24615;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#26469;&#23545;&#20505;&#36873;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#25490;&#21517;&#21644;&#25490;&#38500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;184&#21517;&#24739;&#32773;&#21644;18,238&#20010;&#27880;&#37322;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#38431;&#21015;&#19978;&#35780;&#20272;&#20102;TrialGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;TrialGPT&#22312;&#26631;&#20934;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#31532;&#20108;&#65292;TrialGPT&#30340;&#32508;&#21512;&#35797;&#39564;&#32423;&#21035;&#35780;&#20998;&#19982;&#19987;&#23478;&#26631;&#27880;&#30340;&#21512;&#26684;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16927</link><description>&lt;p&gt;
&#32447;&#26463;&#33258;&#21160;&#39550;&#39542;&#65306;&#25361;&#25112;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
End-to-end Autonomous Driving: Challenges and Frontiers. (arXiv:2306.16927v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#37319;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#29983;&#25104;&#36710;&#36742;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#35832;&#22914;&#26816;&#27979;&#21644;&#36816;&#21160;&#39044;&#27979;&#31561;&#21333;&#20010;&#20219;&#21153;&#12290;&#19982;&#27169;&#22359;&#21270;&#27969;&#27700;&#32447;&#30456;&#27604;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#26469;&#33719;&#30410;&#12290;&#36825;&#19968;&#39046;&#22495;&#22240;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12289;&#38381;&#29615;&#35780;&#20272;&#20197;&#21450;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#25191;&#34892;&#25152;&#38656;&#30340;&#38656;&#27714;&#32780;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;250&#22810;&#31687;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#21160;&#26426;&#12289;&#36335;&#32447;&#22270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 250 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#25910;&#38598;&#33258;2022&#24180;&#39640;&#25490;&#21517;&#27604;&#36187;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;ShuttleSet22&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25293;&#29699;&#39044;&#27979;&#26041;&#27861;ShuttleNet&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.15664</link><description>&lt;p&gt;
ShuttleSet22: &#29992;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;&#23545;&#20013;&#39118;&#39044;&#27979;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ShuttleSet22: Benchmarking Stroke Forecasting with Stroke-Level Badminton Dataset. (arXiv:2306.15664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#25910;&#38598;&#33258;2022&#24180;&#39640;&#25490;&#21517;&#27604;&#36187;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;ShuttleSet22&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25293;&#29699;&#39044;&#27979;&#26041;&#27861;ShuttleNet&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#21644;&#25968;&#25454;&#37319;&#38598;&#30340;&#25928;&#29575;&#65292;&#32701;&#27611;&#29699;&#20998;&#26512;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#31995;&#21015;&#26377;&#25928;&#30340;&#24212;&#29992;&#26469;&#25913;&#21892;&#21644;&#30740;&#31350;&#36873;&#25163;&#34920;&#29616;&#65292;&#20294;&#21482;&#26377;&#24456;&#23569;&#20960;&#20010;&#21487;&#20197;&#20379;&#32701;&#27611;&#29699;&#39046;&#22495;&#22806;&#30340;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;&#20844;&#24320;&#32701;&#27611;&#29699;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;&#38598;&#20013;&#20110;&#29305;&#23450;&#30340;&#27604;&#36187;&#23545;&#20915;&#65292;&#28982;&#32780;&#23427;&#20204;&#26080;&#27861;&#23545;&#19981;&#21516;&#36873;&#25163;&#21644;&#21508;&#31181;&#27604;&#36187;&#23545;&#20915;&#36827;&#34892;&#32508;&#21512;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;ShuttleSet22&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;2022&#24180;&#39640;&#25490;&#21517;&#27604;&#36187;&#20013;&#25910;&#38598;&#30340;&#12290;ShuttleSet22&#35757;&#32451;&#38598;&#21253;&#25324;30,172&#20010;&#22238;&#21512;&#20013;&#30340;2,888&#20010;&#25293;&#29699;&#65292;&#39564;&#35777;&#38598;&#21253;&#25324;450&#20010;&#22238;&#21512;&#20013;&#30340;1,400&#20010;&#25293;&#29699;&#65292;&#27979;&#35797;&#38598;&#21253;&#25324;654&#20010;&#22238;&#21512;&#20013;&#30340;2,040&#20010;&#25293;&#29699;&#65292;&#24182;&#19988;&#20855;&#26377;&#27599;&#20010;&#22238;&#21512;&#20013;&#35814;&#32454;&#30340;&#25293;&#29699;&#32423;&#20803;&#25968;&#25454;&#12290;&#20026;&#20102;&#19982;ShuttleSet22&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#20351;&#29992;ShuttleNet&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25293;&#29699;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, badminton analytics has drawn attention due to the advancement of artificial intelligence and the efficiency of data collection. While there is a line of effective applications to improve and investigate player performance, there are only a few public badminton datasets that can be used for researchers outside the badminton domain. Existing badminton singles datasets focus on specific matchups; however, they cannot provide comprehensive studies on different players and various matchups. In this paper, we provide a badminton singles dataset, ShuttleSet22, which is collected from high-ranking matches in 2022. ShuttleSet22 consists of 30,172 strokes in 2,888 rallies in the training set, 1,400 strokes in 450 rallies in the validation set, and 2,040 strokes in 654 rallies in the testing set with detailed stroke-level metadata within a rally. To benchmark existing work with ShuttleSet22, we test the state-of-the-art stroke forecasting approach, ShuttleNet, with the correspon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05880</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Neural Representations&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#29992;&#20110;&#25554;&#20540;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;Implicit Neural Representations (INR)&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25110;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#35843;&#21046;INR&#21442;&#25968;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#35265;&#26679;&#26412;&#21644;&#26102;&#38388;&#31383;&#21475;&#31227;&#20301;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#26041;&#38754;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#12290;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;&#21069;&#21521;&#35757;&#32451;&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#26469;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#65292;&#20174;&#32780;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2304.10126</link><description>&lt;p&gt;
&#35299;&#32806;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#31616;&#21333;&#30340;GNN&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decouple Graph Neural Networks: Train Multiple Simple GNNs Simultaneously Instead of One. (arXiv:2304.10126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#12290;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;&#21069;&#21521;&#35757;&#32451;&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#26469;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#65292;&#20174;&#32780;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23384;&#22312;&#20005;&#37325;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#33410;&#28857;&#20381;&#36182;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#20351;&#24471;GNN&#30340;&#35757;&#32451;&#36890;&#24120;&#24456;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#22810;&#23618;GNN&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#30001;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#65288;FT&#65289;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#65288;BT&#65289;&#32452;&#25104;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19979;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;FT&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#19981;&#20250;&#25197;&#26354;&#22270;&#24418;&#20449;&#24687;&#12290;&#20026;&#36991;&#20813;FT&#30340;&#21482;&#21333;&#21521;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#65292;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#12290;&#36825;&#31181;&#21453;&#21521;&#35757;&#32451;&#24341;&#20837;&#20102;&#21453;&#21521;&#20449;&#24687;&#20256;&#36882;&#21040;&#35299;&#32806;&#27169;&#22359;&#20013;&#65292;&#21516;&#26102;&#20063;&#20250;&#26377;&#21069;&#21521;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNN) suffer from severe inefficiency. It is mainly caused by the exponential growth of node dependency with the increase of layers. It extremely limits the application of stochastic optimization algorithms so that the training of GNN is usually time-consuming. To address this problem, we propose to decouple a multi-layer GNN as multiple simple modules for more efficient training, which is comprised of classical forward training (FT)and designed backward training (BT). Under the proposed framework, each module can be trained efficiently in FT by stochastic algorithms without distortion of graph information owing to its simplicity. To avoid the only unidirectional information delivery of FT and sufficiently train shallow modules with the deeper ones, we develop a backward training mechanism that makes the former modules perceive the latter modules. The backward training introduces the reversed information delivery into the decoupled modules as well as the forward i
&lt;/p&gt;</description></item><item><title>CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04391</link><description>&lt;p&gt;
CAFIN: &#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#30340;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs. (arXiv:2304.04391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04391
&lt;/p&gt;
&lt;p&gt;
CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25152;&#23398;&#23884;&#20837;&#30340;&#32039;&#20945;&#24615;&#21644;&#20016;&#23500;&#24615;&#20197;&#21450;&#26410;&#26631;&#35760;&#22270;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#22312;(&#22823;&#22411;)&#22270;&#19978;&#24050;&#32463;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#37325;&#35270;&#12290;&#24403;&#36825;&#20123;&#33410;&#28857;&#34920;&#31034;&#34987;&#37096;&#32626;&#26102;&#65292;&#24517;&#39035;&#20351;&#29992;&#36866;&#24403;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#26465;&#20214;&#29983;&#25104;&#20197;&#20943;&#23569;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#36896;&#25104;&#30340;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24050;&#32463;&#35843;&#26597;&#20102;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#36825;&#20123;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#27809;&#26377;&#32771;&#34385;&#36830;&#25509;&#27169;&#24335;&#22312;&#22270;&#20013;&#23548;&#33268;&#30340;&#19981;&#21516;&#33410;&#28857;&#24433;&#21709;(&#25110;&#20013;&#24515;&#24615;&#33021;&#37327;)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#24402;&#32435;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CAFIN&#65288;Centrality Aware Fairness inducing IN-processing&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#25913;&#36827;GraphSAGE&#34920;&#31034;&#30340;&#36827;&#31243;&#25216;&#26415;&#8212;&#8212;&#26080;&#30417;&#30563;&#22270;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#26694;&#26550;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning on (large) graphs has received significant attention in the research community due to the compactness and richness of the learned embeddings and the abundance of unlabelled graph data. When deployed, these node representations must be generated with appropriate fairness constraints to minimize bias induced by them on downstream tasks. Consequently, group and individual fairness notions for graph learning algorithms have been investigated for specific downstream tasks. One major limitation of these fairness notions is that they do not consider the connectivity patterns in the graph leading to varied node influence (or centrality power). In this paper, we design a centrality-aware fairness framework for inductive graph representation learning algorithms. We propose CAFIN (Centrality Aware Fairness inducing IN-processing), an in-processing technique that leverages graph structure to improve GraphSAGE's representations - a popular framework in the unsup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;&#25490;&#21015;&#30340;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#36816;&#34892;&#26102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#23558;&#32463;&#20856;&#30340;&#20266;&#24067;&#23572;&#22522;&#20934;&#36716;&#21270;&#20026;&#22522;&#20110;&#25490;&#21015;&#38598;&#21512;&#23450;&#20041;&#30340;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#20301;&#20018;&#19981;&#21516;&#65292;&#25490;&#21015;&#21464;&#24322;&#30340;&#22256;&#38590;&#31243;&#24230;&#19981;&#20165;&#21462;&#20915;&#20110;&#27721;&#26126;&#36317;&#31163;&#65292;&#36824;&#19982;&#25490;&#21015;&#30340;&#31934;&#30830;&#24490;&#29615;&#32467;&#26500;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2207.04045</link><description>&lt;p&gt;
&#22522;&#20110;&#25490;&#21015;&#30340;&#36827;&#21270;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Runtime Analysis for Permutation-based Evolutionary Algorithms. (arXiv:2207.04045v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#25490;&#21015;&#30340;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#36816;&#34892;&#26102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#23558;&#32463;&#20856;&#30340;&#20266;&#24067;&#23572;&#22522;&#20934;&#36716;&#21270;&#20026;&#22522;&#20110;&#25490;&#21015;&#38598;&#21512;&#23450;&#20041;&#30340;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#20301;&#20018;&#19981;&#21516;&#65292;&#25490;&#21015;&#21464;&#24322;&#30340;&#22256;&#38590;&#31243;&#24230;&#19981;&#20165;&#21462;&#20915;&#20110;&#27721;&#26126;&#36317;&#31163;&#65292;&#36824;&#19982;&#25490;&#21015;&#30340;&#31934;&#30830;&#24490;&#29615;&#32467;&#26500;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#36807;&#21435;25&#24180;&#38024;&#23545;&#20266;&#24067;&#23572;&#20248;&#21270;&#38382;&#39064;&#30340;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#30340;&#29702;&#35770;&#20998;&#26512;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#22914;&#20309;&#35299;&#20915;&#22522;&#20110;&#25490;&#21015;&#30340;&#38382;&#39064;&#30340;&#29702;&#35770;&#32467;&#26524;&#26041;&#38754;&#21482;&#23384;&#22312;&#38646;&#26143;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#22522;&#20110;&#25490;&#21015;&#30340;&#22522;&#20934;&#38382;&#39064;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32463;&#20856;&#30340;&#20266;&#24067;&#23572;&#22522;&#20934;&#36716;&#21270;&#20026;&#22522;&#20110;&#25490;&#21015;&#38598;&#21512;&#23450;&#20041;&#30340;&#22522;&#20934;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;Scharnow&#65292;Tinnefeld&#21644;Wegener&#65288;2004&#65289;&#25552;&#20986;&#30340;&#22522;&#20110;&#25490;&#21015;&#30340;&#65288;1+1&#65289;EA&#22312;LeadingOnes&#21644;Jump&#22522;&#20934;&#30340;&#31867;&#20284;&#29289;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#36816;&#34892;&#26102;&#20998;&#26512;&#12290;&#21518;&#32773;&#34920;&#26126;&#65292;&#19982;&#20301;&#20018;&#19981;&#21516;&#65292;&#19981;&#20165;&#27721;&#26126;&#36317;&#31163;&#20915;&#23450;&#20102;&#23558;&#19968;&#20010;&#25490;&#21015;$\sigma$&#21464;&#24322;&#20026;&#21478;&#19968;&#20010;&#25490;&#21015;$\tau$&#30340;&#22256;&#38590;&#31243;&#24230;&#65292;&#36824;&#26377;$\sigma\tau^{-1}$&#30340;&#31934;&#30830;&#24490;&#29615;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26356;&#23545;&#31216;&#30340;&#28151;&#28102;&#21464;&#24322;&#31639;&#23376;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#19981;&#20165;&#23548;&#33268;&#20102;&#26356;&#31616;&#21333;&#30340;&#35777;&#26126;&#65292;&#36824;&#23548;&#33268;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the theoretical analysis of evolutionary algorithms (EAs) has made significant progress for pseudo-Boolean optimization problems in the last 25 years, only sporadic theoretical results exist on how EAs solve permutation-based problems.  To overcome the lack of permutation-based benchmark problems, we propose a general way to transfer the classic pseudo-Boolean benchmarks into benchmarks defined on sets of permutations. We then conduct a rigorous runtime analysis of the permutation-based $(1+1)$ EA proposed by Scharnow, Tinnefeld, and Wegener (2004) on the analogues of the LeadingOnes and Jump benchmarks. The latter shows that, different from bit-strings, it is not only the Hamming distance that determines how difficult it is to mutate a permutation $\sigma$ into another one $\tau$, but also the precise cycle structure of $\sigma \tau^{-1}$. For this reason, we also regard the more symmetric scramble mutation operator. We observe that it not only leads to simpler proofs, but also 
&lt;/p&gt;</description></item></channel></rss>