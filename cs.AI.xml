<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25925;&#38556;&#20195;&#29702;&#30340;&#35748;&#30693;&#36923;&#36753;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21333;&#32431;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20801;&#35768;&#19990;&#30028;&#20013;&#21442;&#19982;&#30340;&#20195;&#29702;&#25968;&#37327;&#21487;&#20197;&#21464;&#21270;&#12290;&#36825;&#23545;&#20110;&#23481;&#38169;&#20998;&#24067;&#24335;&#35745;&#31639;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01351</link><description>&lt;p&gt;
&#25925;&#38556;&#20195;&#29702;&#30340;&#35748;&#30693;&#36923;&#36753;&#30340;&#21333;&#32431;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simplicial Models for the Epistemic Logic of Faulty Agents. (arXiv:2311.01351v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25925;&#38556;&#20195;&#29702;&#30340;&#35748;&#30693;&#36923;&#36753;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21333;&#32431;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20801;&#35768;&#19990;&#30028;&#20013;&#21442;&#19982;&#30340;&#20195;&#29702;&#25968;&#37327;&#21487;&#20197;&#21464;&#21270;&#12290;&#36825;&#23545;&#20110;&#23481;&#38169;&#20998;&#24067;&#24335;&#35745;&#31639;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#19968;&#20123;&#20316;&#32773;&#19968;&#30452;&#22312;&#30740;&#31350;&#21333;&#32431;&#27169;&#22411;&#65292;&#36825;&#26159;&#22522;&#20110;&#31216;&#20026;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#39640;&#32500;&#32467;&#26500;&#30340;&#35748;&#30693;&#36923;&#36753;&#27169;&#22411;&#12290;&#22312;&#26368;&#21021;&#30340;&#24418;&#24335;&#20013;&#65292;&#21333;&#32431;&#27169;&#22411;&#22987;&#32456;&#34987;&#20551;&#35774;&#20026;&#32431;&#31929;&#30340;&#65292;&#24847;&#21619;&#30528;&#25152;&#26377;&#19990;&#30028;&#20855;&#26377;&#30456;&#21516;&#30340;&#32500;&#24230;&#12290;&#36825;&#30456;&#24403;&#20110;&#22522;&#20110;&#20811;&#37324;&#26222;&#20811;&#27169;&#22411;&#30340;&#26631;&#20934;S5n&#35821;&#20041;&#30340;&#35748;&#30693;&#36923;&#36753;&#12290;&#36890;&#36807;&#31227;&#38500;&#27169;&#22411;&#24517;&#39035;&#26159;&#32431;&#31929;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#21487;&#20197;&#36229;&#36234;&#24120;&#35268;&#30340;&#20811;&#37324;&#26222;&#20811;&#35821;&#20041;&#65292;&#24182;&#30740;&#31350;&#21442;&#19982;&#19968;&#20010;&#19990;&#30028;&#30340;&#20195;&#29702;&#25968;&#37327;&#21487;&#20197;&#21464;&#21270;&#30340;&#35748;&#30693;&#36923;&#36753;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#22312;&#35768;&#22810;&#35770;&#25991;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#20854;&#20013;&#24212;&#29992;&#20110;&#23481;&#38169;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#20854;&#20013;&#22312;&#31995;&#32479;&#25191;&#34892;&#26399;&#38388;&#21487;&#33021;&#20250;&#21457;&#29983;&#36807;&#31243;&#23849;&#28291;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#22312;&#23450;&#20041;&#19981;&#32431;&#30340;&#21333;&#32431;&#27169;&#22411;&#26102;&#65292;&#24494;&#22937;&#30340;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#25152;&#24471;&#21040;&#30340;&#36923;&#36753;&#30340;&#19981;&#21516;&#20844;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23545;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20844;&#29702;&#21270;&#30456;&#24212;&#30340;&#35748;&#30693;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, several authors have been investigating simplicial models, a model of epistemic logic based on higher-dimensional structures called simplicial complexes. In the original formulation, simplicial models were always assumed to be pure, meaning that all worlds have the same dimension. This is equivalent to the standard S5n semantics of epistemic logic, based on Kripke models. By removing the assumption that models must be pure, we can go beyond the usual Kripke semantics and study epistemic logics where the number of agents participating in a world can vary. This approach has been developed in a number of papers, with applications in fault-tolerant distributed computing where processes may crash during the execution of a system. A difficulty that arises is that subtle design choices in the definition of impure simplicial models can result in different axioms of the resulting logic. In this paper, we classify those design choices systematically, and axiomatize the correspon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DECENT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35299;&#32806;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#25506;&#27979;&#25216;&#26415;&#26469;&#24357;&#34917;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19347</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#36827;&#34892;&#23545;&#25239;&#35299;&#32806;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DECENT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35299;&#32806;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#25506;&#27979;&#25216;&#26415;&#26469;&#24357;&#34917;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20250;&#29983;&#25104;&#19982;&#21407;&#22987;&#25991;&#31456;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#65292;&#34987;&#31216;&#20026;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#12290;&#19982;&#20043;&#21069;&#30340;&#23567;&#22411;&#27169;&#22411;&#65288;&#22914;BART&#65292;T5&#65289;&#19981;&#21516;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#21046;&#36896;&#24858;&#34850;&#38169;&#35823;&#26041;&#38754;&#36739;&#23569;&#65292;&#20294;&#21046;&#36896;&#20102;&#26356;&#22797;&#26434;&#30340;&#38169;&#35823;&#65292;&#20363;&#22914;&#21152;&#20837;&#22240;&#26524;&#20851;&#31995;&#12289;&#28155;&#21152;&#38169;&#35823;&#32454;&#33410;&#21644;&#36807;&#24230;&#27867;&#21270;&#31561;&#12290;&#36825;&#20123;&#24187;&#35273;&#24456;&#38590;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#65292;&#36825;&#32473;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#24102;&#26469;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#35299;&#32806;&#26041;&#27861;&#26469;&#20998;&#31163;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65288;DECENT&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#21442;&#25968;&#39640;&#25928;&#25216;&#26415;&#65292;&#20197;&#24357;&#34917;LLMs&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;LLMs&#23545;&#20110;&#20462;&#39280;&#21644;&#29702;&#35299;&#30340;&#27010;&#24565;&#26356;&#21152;&#28165;&#26224;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as "hallucinations" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, and overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding, thus can execute the instructions more accur
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.17940</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550;&#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#26159;&#23454;&#26102;&#22330;&#26223;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#27604;&#22914;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#12289;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#21644;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#65292;&#20854;&#20013;&#30446;&#26631;&#24207;&#21015;&#22312;&#25509;&#25910;&#28304;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#12290;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#30340;&#20851;&#38190;&#22312;&#20110;&#30830;&#23450;&#29983;&#25104;&#30340;&#26368;&#20339;&#26102;&#26426;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#23545;&#28304;-&#30446;&#26631;&#26144;&#23556;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#65292;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#21508;&#31181;&#21516;&#26102;&#20219;&#21153;&#20013;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#20197;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#26144;&#23556;&#12290;&#22312;&#21516;&#26102;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#22312;&#31561;&#24453;&#28304;&#29255;&#27573;&#21644;&#29983;&#25104;&#30446;&#26631;&#29255;&#27573;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified segment-to-segment framework (Seg2Seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.17784</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Financial Large Language Models. (arXiv:2310.17784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#22914;&#37329;&#34701;&#26102;&#21364;&#36935;&#21040;&#22256;&#38590;&#12290;LLMs&#38590;&#20197;&#25512;&#29702;&#21644;&#25972;&#21512;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;LLMs&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#37329;&#34701;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#19981;&#26159;&#19968;&#27425;&#24615;&#32473;LLM&#36127;&#36733;&#36807;&#22810;&#20449;&#24687;&#65292;&#32780;&#26159;&#26356;&#26377;&#25928;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20219;&#21153;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26469;&#21019;&#24314;&#37329;&#34701;LLM&#65288;FLLM&#65289;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#25163;&#21160;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#24378;&#25512;&#29702;&#65288;AAR&#65289;&#26469;&#20462;&#25913;FLLM&#33258;&#36523;&#36755;&#20986;&#30340;&#20266;&#26631;&#31614;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#21270;FLLM&#19982;AAR&#30456;&#27604;&#65292;&#26174;&#33879;&#20248;&#20110;&#20026;&#21407;&#22987;&#25991;&#26412;&#35774;&#35745;&#30340;&#22522;&#32447;&#37329;&#34701;LLMs&#65292;&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17490</link><description>&lt;p&gt;
&#25552;&#39640;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#23545;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24178;&#25200;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20351;&#24471;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;(ODQA)&#20013;&#23454;&#29616;&#38646;&#26679;&#26412;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#26159;&#30001;&#20110;&#38405;&#35835;&#22120;&#30456;&#23545;&#20110;&#26816;&#32034;&#22120;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19968;&#31181;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#25104;&#26412;&#21644;&#26631;&#27880;&#25968;&#25454;&#38656;&#27714;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#30001;&#20110;&#26816;&#32034;&#21040;&#30340;&#26080;&#20851;&#25991;&#26723;&#20197;&#21450;&#20316;&#20026;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#26102;&#29983;&#25104;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#32780;&#21463;&#21040;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21542;&#23450;&#30340;&#25351;&#20196;&#21644;&#20998;&#25968;&#35843;&#25972;&#30340;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#25991;&#26723;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22788;&#29702;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;&#38754;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#32780;&#22256;&#38590;&#37325;&#37325;&#30340;&#30417;&#30563;&#24335;&#38405;&#35835;&#22120;&#19981;&#21516;&#65292;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#24179;&#34913;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.16600</link><description>&lt;p&gt;
&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;
&lt;/p&gt;
&lt;p&gt;
Balancing central and marginal rejection when combining independent significance tests. (arXiv:2310.16600v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#24179;&#34913;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21407;&#22987;&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#35780;&#20272;&#19968;&#32452;p&#20540;&#30340;&#26174;&#33879;&#24615;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#23427;&#20204;&#19982;&#27719;&#38598;&#20989;&#25968;&#36827;&#34892;&#32452;&#21512;&#12290;&#36825;&#20123;&#27719;&#38598;&#30340;p&#20540;&#23558;p&#20540;&#26679;&#26412;&#36716;&#21270;&#20026;&#19968;&#20010;&#34920;&#29616;&#31867;&#20284;&#20110;&#21333;&#21464;&#37327;p&#20540;&#30340;&#21333;&#19968;&#25968;&#20540;&#12290;&#20026;&#20102;&#26126;&#30830;&#35752;&#35770;&#36825;&#20123;&#20989;&#25968;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20132;&#21449;&#20551;&#35774;&#65292;&#20197;&#20256;&#36798;p&#20540;&#20013;&#38750;&#38646;&#35777;&#25454;&#30340;&#24378;&#24230;&#21644;&#26222;&#36941;&#24615;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#24120;&#35268;&#27719;&#38598;&#20844;&#24335;&#12290;&#22312;&#29305;&#23450;&#20132;&#21449;&#20551;&#35774;&#30340;UMP&#27719;&#38598;p&#20540;&#20013;&#35266;&#23519;&#21040;&#30340;&#27169;&#24335;&#25512;&#21160;&#20102;&#23545;&#20110;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#27700;&#24179;&#22312;&#945;&#22788;&#30340;&#23450;&#20041;&#21644;&#35752;&#35770;&#12290;&#35777;&#26126;&#20102;&#20013;&#24515;&#25298;&#32477;&#24635;&#26159;&#22823;&#20110;&#31561;&#20110;&#36793;&#32536;&#25298;&#32477;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#22312;&#27719;&#38598;&#30340;p&#20540;&#20013;&#24179;&#34913;&#30340;&#21830;&#12290;&#22522;&#20110;&#967;&#178;_&#954;&#20998;&#20301;&#25968;&#21464;&#25442;&#30340;&#32452;&#21512;&#20989;&#25968;&#34987;&#25552;&#20986;&#20197;&#25511;&#21046;&#36825;&#20010;&#21830;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach to evaluating the significance of a collection of $p$-values combines them with a pooling function, in particular when the original data are not available. These pooled $p$-values convert a sample of $p$-values into a single number which behaves like a univariate $p$-value. To clarify discussion of these functions, a telescoping series of alternative hypotheses are introduced that communicate the strength and prevalence of non-null evidence in the $p$-values before general pooling formulae are discussed. A pattern noticed in the UMP pooled $p$-value for a particular alternative motivates the definition and discussion of central and marginal rejection levels at $\alpha$. It is proven that central rejection is always greater than or equal to marginal rejection, motivating a quotient to measure the balance between the two for pooled $p$-values. A combining function based on the $\chi^2_{\kappa}$ quantile transformation is proposed to control this quotient and shown to be
&lt;/p&gt;</description></item><item><title>PreWoMe&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#38382;&#31572;&#20013;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#21453;&#39304;&#21644;&#34892;&#21160;&#65292;&#19981;&#20165;&#33021;&#26377;&#25928;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16147</link><description>&lt;p&gt;
PreWoMe:&#21033;&#29992;&#39044;&#35774;&#20026;&#38271;&#31687;&#38382;&#31572;&#20013;&#30340;&#24037;&#20316;&#35760;&#24518;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering. (arXiv:2310.16147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16147
&lt;/p&gt;
&lt;p&gt;
PreWoMe&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#38382;&#31572;&#20013;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#21453;&#39304;&#21644;&#34892;&#21160;&#65292;&#19981;&#20165;&#33021;&#26377;&#25928;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31687;&#38382;&#31572;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#24120;&#24120;&#30001;&#20110;&#38382;&#39064;&#20013;&#30340;&#27169;&#31946;&#25110;&#38169;&#35823;&#39044;&#35774;&#32780;&#35823;&#23548;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#38024;&#23545;&#30340;&#26159;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24456;&#38590;&#36866;&#24212;&#23454;&#38469;&#24773;&#20917;&#20013;&#19981;&#21487;&#39044;&#27979;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PreWoMe&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#20309;&#31867;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#12290;PreWoMe&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#23545;&#38382;&#39064;&#30340;&#21453;&#39304;&#21644;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PreWoMe&#19981;&#20165;&#22312;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#26041;&#38754;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#26041;&#38754;&#20063;&#24456;&#26377;&#25928;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information-seeking questions in long-form question answering (LFQA) often prove misleading due to ambiguity or false presupposition in the question. While many existing approaches handle misleading questions, they are tailored to limited questions, which are insufficient in a real-world setting with unpredictable input characteristics. In this work, we propose PreWoMe, a unified approach capable of handling any type of information-seeking question. The key idea of PreWoMe involves extracting presuppositions in the question and exploiting them as working memory to generate feedback and action about the question. Our experiment shows that PreWoMe is effective not only in tackling misleading questions but also in handling normal ones, thereby demonstrating the effectiveness of leveraging presuppositions, feedback, and action for real-world QA settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.14421</link><description>&lt;p&gt;
&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#65292;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#38024;&#23545;&#65288;&#23616;&#37096;&#65289;&#21807;&#19968;&#21487;&#36870;&#20998;&#31867;&#22120;&#12289;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#21644;&#29109;AI&#65288;EAI&#65289;&#20855;&#26377;&#26368;&#23567;&#23545;&#25239;&#36335;&#24452;&#65288;MAP&#65289;&#21644;&#26368;&#23567;&#23545;&#25239;&#36317;&#31163;&#65288;MAD&#65289;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#26126;&#30830;&#30340;&#20998;&#26512;&#35745;&#31639;&#30340;&#31616;&#21333;&#21487;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#12290;&#22312;&#24120;&#35265;&#30340;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#12289;&#25552;&#21319;&#38543;&#26426;&#26862;&#26519;&#12289;GLM&#21644;EAI&#31561;&#21508;&#31867;AI&#24037;&#20855;&#36827;&#34892;MAP&#21644;MAD&#30340;&#23454;&#38469;&#35745;&#31639;&#12289;&#27604;&#36739;&#21644;&#35299;&#37322;&#65292;&#21253;&#25324;&#21452;&#21367;&#29366;&#34746;&#26059;&#32447;&#21450;&#20854;&#25193;&#23637;&#20197;&#21450;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38382;&#39064;&#65288;&#29992;&#20110;&#20581;&#24247;&#20445;&#38505;&#29702;&#36180;&#39044;&#27979;&#21644;&#24515;&#33039;&#30149;&#21457;&#20316;&#33268;&#27515;&#29575;&#20998;&#31867;&#65289;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#23637;&#31034;&#20102;MAP&#22914;&#20309;&#22312;&#39044;&#23450;&#20041;&#30340;&#21487;&#35775;&#38382;&#25511;&#21046;&#21464;&#37327;&#23376;&#38598;&#20013;&#25552;&#20379;&#21807;&#19968;&#30340;&#26368;&#23567;&#24739;&#32773;&#29305;&#23450;&#39118;&#38505;&#32531;&#35299;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07838</link><description>&lt;p&gt;
&#25506;&#32034;&#26377;&#38480;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#20174;&#25945;&#24072;&#21040;&#27010;&#29575;&#21270;&#23398;&#29983;&#20998;&#31867;&#22120;&#30340;n&#20010;&#26679;&#26412;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20854;&#20013;&#36755;&#20837;&#31354;&#38388;S&#21644;&#26631;&#31614;A&#20026;&#26377;&#38480;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19977;&#20010;&#28176;&#36827;&#32423;&#21035;&#19978;&#30340;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#21152;&#24555;&#20256;&#36882;&#30340;&#36895;&#24230;&#12290;&#22312;&#31532;&#19968;&#32423;&#21035;&#19978;&#65292;&#21482;&#26377;&#20855;&#26377;&#22256;&#38590;&#26631;&#31614;&#30340;&#26679;&#26412;&#26159;&#24050;&#30693;&#30340;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#33021;&#22815;&#36798;&#21040;&#26368;&#23567;&#21270;&#36895;&#29575;sqrt(|S||A|/n)&#12290;&#31532;&#20108;&#32423;&#21035;&#19978;&#65292;&#38500;&#20102;&#24050;&#30693;&#30340;&#22256;&#38590;&#26631;&#31614;&#26679;&#26412;&#22806;&#65292;&#36824;&#26377;&#37319;&#26679;&#26631;&#31614;&#30340;&#25945;&#24072;&#27010;&#29575;&#21487;&#29992;&#65292;&#36825;&#23558;&#25910;&#25947;&#36895;&#24230;&#30340;&#19979;&#30028;&#25552;&#39640;&#21040;|S||A|/n&#12290;&#28982;&#32780;&#65292;&#22312;&#31532;&#20108;&#20010;&#25968;&#25454;&#37319;&#38598;&#21327;&#35758;&#19979;&#65292;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26420;&#32032;&#36866;&#24212;&#20250;&#23548;&#33268;&#28176;&#36817;&#20559;&#24046;&#30340;&#23398;&#29983;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#39564;&#21464;&#20307;&#30340;&#24179;&#26041;&#35823;&#24046;&#36923;&#36753;&#25439;&#22833;&#26469;&#23454;&#29616;&#20102;&#22522;&#26412;&#38480;&#21046;&#12290;&#31532;&#19977;&#32423;&#21035;&#36827;&#19968;&#27493;&#36171;&#20104;&#23398;&#29983;&#36719;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35825;&#23548;&#36741;&#23548;&#33050;&#26412;&#21644;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#30340;&#26032;&#22411;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#21021;&#27493;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#19982;&#20854;&#20182;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#30456;&#27604;&#65292;&#31995;&#32479;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01420</link><description>&lt;p&gt;
Ruffle&amp;Riley&#65306;&#36208;&#21521;&#33258;&#21160;&#21270;&#30340;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Ruffle&amp;Riley: Towards the Automated Induction of Conversational Tutoring Systems. (arXiv:2310.01420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35825;&#23548;&#36741;&#23548;&#33050;&#26412;&#21644;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#30340;&#26032;&#22411;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#21021;&#27493;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#19982;&#20854;&#20182;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#30456;&#27604;&#65292;&#31995;&#32479;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#65288;CTS&#65289;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#25552;&#20379;&#23398;&#20064;&#20307;&#39564;&#12290;&#23427;&#20204;&#34987;&#35748;&#20026;&#33021;&#22815;&#20419;&#36827;&#39640;&#27700;&#24179;&#30340;&#35748;&#30693;&#21442;&#19982;&#65292;&#24182;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#26377;&#30410;&#20110;&#23398;&#20064;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#25776;&#20889;CTS&#20869;&#23481;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#26159;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#22411;&#30340;CTS&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#39318;&#20808;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#25945;&#23398;&#25991;&#26412;&#33258;&#21160;&#35825;&#23548;&#20986;&#36741;&#23548;&#33050;&#26412;&#12290;&#20854;&#27425;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#65288;Ruffle&amp;Riley&#65289;&#22312;&#23398;&#20197;&#25945;&#23398;&#30340;&#24418;&#24335;&#20013;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#33258;&#30001;&#23545;&#35805;&#65292;&#36981;&#24490;ITS&#20856;&#22411;&#30340;&#22806;&#37096;/&#20869;&#37096;&#24490;&#29615;&#32467;&#26500;&#12290;&#22312;&#19968;&#20010;&#21021;&#27493;&#30340;&#34987;&#35797;&#32773;&#22312;&#32447;&#29992;&#25143;&#30740;&#31350;&#65288;N = 100&#65289;&#20013;&#65292;&#23558;Ruffle&amp;Riley&#19982;&#26356;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational tutoring systems (CTSs) offer learning experiences driven by natural language interaction. They are known to promote high levels of cognitive engagement and benefit learning outcomes, particularly in reasoning tasks. Nonetheless, the time and cost required to author CTS content is a major obstacle to widespread adoption. In this paper, we introduce a novel type of CTS that leverages the recent advances in large language models (LLMs) in two ways: First, the system induces a tutoring script automatically from a lesson text. Second, the system automates the script orchestration via two LLM-based agents (Ruffle&amp;Riley) with the roles of a student and a professor in a learning-by-teaching format. The system allows a free-form conversation that follows the ITS-typical outer-/inner-loop structure. In an initial between-subject online user study (N = 100) comparing Ruffle&amp;Riley to simpler QA chatbots and reading activity, we found no significant differences in post-test scores. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;</title><link>http://arxiv.org/abs/2309.14610</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22270;&#28145;&#24230;&#23398;&#20064;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#28304;&#20110;&#19982;&#27946;&#27700;&#21361;&#38505;&#12289;&#27946;&#27700;&#26292;&#38706;&#20197;&#21450;&#31038;&#20250;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#27946;&#27700;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#34920;&#24449;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#27946;&#27700;&#24179;&#21407;&#22320;&#22270;&#65292;&#20391;&#37325;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#35201;&#32032;&#65292;&#20027;&#35201;&#26159;&#21361;&#38505;&#21644;&#26292;&#38706;&#35201;&#32032;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#31354;&#38388;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#31216;&#20026;FloodRisk-Net&#65289;&#30340;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#12290;FloodRisk-Net&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#27946;&#27700;&#21361;&#38505;&#21644;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#30830;&#23450;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#12290;&#21033;&#29992;&#32654;&#22269;&#22810;&#20010;&#37117;&#24066;&#32479;&#35745;&#21306;&#65288;MSAs&#65289;&#30340;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#27946;&#27700;&#39118;&#38505;&#29305;&#24449;&#21270;&#20026;
&lt;/p&gt;
&lt;p&gt;
Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;TPUs&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#20854;&#20182;&#33455;&#29255;&#26550;&#26500;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TPUs&#21487;&#20197;&#22312;&#20113;&#35745;&#31639;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08918</link><description>&lt;p&gt;
&#25506;&#32034;TPUs&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploration of TPUs for AI Applications. (arXiv:2309.08918v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;TPUs&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#20854;&#20182;&#33455;&#29255;&#26550;&#26500;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TPUs&#21487;&#20197;&#22312;&#20113;&#35745;&#31639;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tensor Processing Units&#65288;TPUs&#65289;&#26159;&#30001;Google&#24320;&#21457;&#30340;&#19987;&#38376;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;TPU&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24615;&#33021;&#21644;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#23454;&#29616;&#12290;&#39318;&#20808;&#27010;&#36848;&#20102;TPU&#30340;&#35774;&#35745;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20854;&#24635;&#20307;&#26550;&#26500;&#12289;&#32534;&#35793;&#25216;&#26415;&#21644;&#25903;&#25345;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#20113;TPU&#21644;&#36793;&#32536;TPU&#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#33455;&#29255;&#26550;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;TPU&#21152;&#36895;AI&#24037;&#20316;&#36127;&#36733;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TPU&#22312;&#20113;&#35745;&#31639;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#22343;&#33021;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#22312;&#36793;&#32536;TPU&#20013;&#37096;&#32626;&#26356;&#22810;&#26550;&#26500;&#30340;&#38656;&#27714;&#65292;&#20197;&#21450;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#36827;&#34892;&#26356;&#21487;&#38752;&#27604;&#36739;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor Processing Units (TPUs) are specialized hardware accelerators for deep learning developed by Google. This paper explores the performance of TPU with a focus on AI and its implementation in edge computing. It first provides an overview of TPUs, specifically their design in relation to neural networks, their general architecture, compilation techniques and supporting frameworks. Furthermore, we provide a comparative analysis of Cloud and Edge TPU performance against other counterpart chip architectures. It is then discussed how TPUs can be used to speed up AI workloads. The results show that TPUs can provide significant performance improvements both in cloud and edge computing. Additionally, we address the need for further research for the deployment of more architectures in the Edge TPU, as well as the need for the development of more robust comparisons in edge computing.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13816</link><description>&lt;p&gt;
&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Homological Convolutional Neural Networks. (arXiv:2308.13816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13816
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#65292;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#27604;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#19988;&#21516;&#26679;&#26377;&#25928;&#12290;&#36825;&#26159;&#22240;&#20026;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#38388;&#30340;&#30456;&#20851;&#24615;&#27604;&#22270;&#20687;&#25110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31354;&#38388;&#25110;&#35821;&#20041;&#20851;&#31995;&#35201;&#24369;&#65292;&#32780;&#19988;&#38656;&#35201;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#23545;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#25152;&#24471;&#27169;&#22411;&#21033;&#29992;&#20102;&#21367;&#31215;&#30340;&#33021;&#21147;&#65292;&#24182;&#22260;&#32469;&#32593;&#32476;&#25299;&#25169;&#20013;&#30340;&#26377;&#38480;&#27010;&#24565;&#26469;&#20445;&#35777;&#65288;i&#65289;&#25968;&#25454;&#19968;&#33268;&#24615;&#12289;&#65288;ii&#65289;&#34920;&#31034;&#26377;&#25928;&#24615;&#12289;&#21644;&#65288;iii&#65289;&#23481;&#37327;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have demonstrated outstanding performances on classification and regression tasks on homogeneous data types (e.g., image, audio, and text data). However, tabular data still poses a challenge with classic machine learning approaches being often computationally cheaper and equally effective than increasingly complex deep learning architectures. The challenge arises from the fact that, in tabular data, the correlation among features is weaker than the one from spatial or semantic relationships in images or natural languages, and the dependency structures need to be modeled without any prior information. In this work, we propose a novel deep learning architecture that exploits the data structural organization through topologically constrained network representations to gain spatial information from sparse tabular data. The resulting model leverages the power of convolutions and is centered on a limited number of concepts from network topology to guarantee (i) a data-c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07336</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#21644;&#24418;&#24335;&#36923;&#36753;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20855;&#20307;&#30340;&#28436;&#32462;&#35268;&#21017;&#26469;&#29983;&#25104;&#28436;&#32462;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#35268;&#21017;&#21463;&#38480;&#25110;&#32773;&#26159;&#20219;&#24847;&#30340;&#12290;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#25152;&#33719;&#24471;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#19968;&#32452;&#33391;&#22909;&#22522;&#30784;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24403;&#36825;&#20123;&#35268;&#21017;&#20197;&#22810;&#27493;&#26041;&#24335;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#20219;&#20309;&#20854;&#20182;&#28436;&#32462;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LMs&#65292;&#21363;$\textbf{FLD}$&#65288;$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction&#65289;&#65292;&#33719;&#24471;&#20102;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28436;&#32462;&#25512;&#29702;&#35821;&#26009;&#24211;&#21487;&#20197;&#22686;&#24378;LMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#38754;&#65292;&#20197;&#21450;&#19981;&#21516;&#26041;&#38754;&#26080;&#27861;&#22686;&#24378;&#30340;&#26041;&#38754;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#28436;&#32462;&#35821;&#26009;&#24211;&#25110;&#20854;&#20182;&#26041;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21069;&#39304;&#22810;&#27169;&#24577;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#30340;&#24314;&#35758;&#26469;&#23454;&#26102;&#27604;&#36739;&#26144;&#23556;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#29992;&#25143;&#22312;&#25511;&#21046;&#26426;&#22120;&#20154;&#33218;&#26102;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;</title><link>http://arxiv.org/abs/2307.02933</link><description>&lt;p&gt;
&#26102;&#38388;&#21644;&#31354;&#38388;&#65306;&#21487;&#29992;&#30340;&#36741;&#21161;&#26426;&#22120;&#20154;&#33218;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
In Time and Space: Towards Usable Adaptive Control for Assistive Robotic Arms. (arXiv:2307.02933v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21069;&#39304;&#22810;&#27169;&#24577;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#30340;&#24314;&#35758;&#26469;&#23454;&#26102;&#27604;&#36739;&#26144;&#23556;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#29992;&#25143;&#22312;&#25511;&#21046;&#26426;&#22120;&#20154;&#33218;&#26102;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#20154;&#33218;&#65292;&#22312;&#19982;&#20154;&#31867;&#36827;&#34892;&#23494;&#20999;&#21512;&#20316;&#30340;&#24773;&#20917;&#19979;&#36234;&#26469;&#36234;&#39057;&#32321;&#22320;&#37096;&#32626;&#65292;&#20363;&#22914;&#22312;&#21046;&#36896;&#25110;&#23478;&#24237;&#25252;&#29702;&#29615;&#22659;&#20013;&#12290;&#36825;&#20123;&#26426;&#22120;&#20154;&#33218;&#38656;&#35201;&#29992;&#25143;&#25511;&#21046;&#22810;&#20010;&#33258;&#30001;&#24230;&#65288;DoFs&#65289;&#26469;&#25191;&#34892;&#20219;&#21153;&#65292;&#20027;&#35201;&#28041;&#21450;&#25235;&#21462;&#21644;&#25805;&#20316;&#29289;&#20307;&#12290;&#26631;&#20934;&#36755;&#20837;&#35774;&#22791;&#20027;&#35201;&#20855;&#26377;&#20004;&#20010;DoFs&#65292;&#38656;&#35201;&#32791;&#26102;&#19988;&#35748;&#30693;&#36127;&#33655;&#22823;&#30340;&#27169;&#24335;&#20999;&#25442;&#26469;&#36873;&#25321;&#21333;&#20010;DoFs&#12290;&#29616;&#20195;&#33258;&#36866;&#24212;DoF&#26144;&#23556;&#25511;&#21046;&#65288;ADMCs&#65289;&#24050;&#32463;&#26174;&#31034;&#21487;&#20197;&#20943;&#23569;&#24517;&#35201;&#30340;&#27169;&#24335;&#20999;&#25442;&#27425;&#25968;&#65292;&#20294;&#23578;&#26410;&#33021;&#26174;&#33879;&#38477;&#20302;&#24863;&#30693;&#24037;&#20316;&#36127;&#33655;&#12290;&#29992;&#25143;&#20173;&#28982;&#25215;&#25285;&#23558;&#25277;&#35937;&#27169;&#24335;&#20999;&#25442;&#32435;&#20837;&#24037;&#20316;&#27969;&#31243;&#30340;&#24515;&#29702;&#36127;&#25285;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#21069;&#39304;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#20351;&#29992;ADMC&#30340;&#26356;&#26032;&#24314;&#35758;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#23454;&#26102;&#35270;&#35273;&#27604;&#36739;&#24403;&#21069;&#21644;&#24314;&#35758;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic solutions, in particular robotic arms, are becoming more frequently deployed for close collaboration with humans, for example in manufacturing or domestic care environments. These robotic arms require the user to control several Degrees-of-Freedom (DoFs) to perform tasks, primarily involving grasping and manipulating objects. Standard input devices predominantly have two DoFs, requiring time-consuming and cognitively demanding mode switches to select individual DoFs. Contemporary Adaptive DoF Mapping Controls (ADMCs) have shown to decrease the necessary number of mode switches but were up to now not able to significantly reduce the perceived workload. Users still bear the mental workload of incorporating abstract mode switching into their workflow. We address this by providing feed-forward multimodal feedback using updated recommendations of ADMC, allowing users to visually compare the current and the suggested mapping in real-time. We contrast the effectiveness of two new appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#21442;&#25968;$g_k(G)$&#29992;&#20110;&#26368;&#22823;$k$-plex&#38382;&#39064;&#65292;&#38024;&#23545;&#20854;&#35774;&#35745;&#20102;&#19968;&#20010;&#26681;&#25454;$g_k(G)$&#21442;&#25968;&#21270;&#30340;&#31934;&#30830;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#21487;&#20197;&#22312;&#23454;&#38469;&#22270;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.13258</link><description>&lt;p&gt;
&#26681;&#25454;&#36864;&#21270;&#38388;&#38553;&#21442;&#25968;&#21270;&#30340;&#24555;&#36895;$k$-Plex&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast Maximum $k$-Plex Algorithm Parameterized by the Degeneracy Gap. (arXiv:2306.13258v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#21442;&#25968;$g_k(G)$&#29992;&#20110;&#26368;&#22823;$k$-plex&#38382;&#39064;&#65292;&#38024;&#23545;&#20854;&#35774;&#35745;&#20102;&#19968;&#20010;&#26681;&#25454;$g_k(G)$&#21442;&#25968;&#21270;&#30340;&#31934;&#30830;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#21487;&#20197;&#22312;&#23454;&#38469;&#22270;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#22270;&#65292;$k$-plex&#26159;&#19968;&#20010;&#39030;&#28857;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#39030;&#28857;&#19982;&#35813;&#38598;&#21512;&#20013;&#26368;&#22810;$k-1$&#20010;&#20854;&#20182;&#39030;&#28857;&#19981;&#30456;&#37051;&#12290;&#26368;&#22823;$k$-plex&#38382;&#39064;&#26159;&#20174;&#32473;&#23450;&#30340;&#22270;&#20013;&#23547;&#25214;&#26368;&#22823;$k$-plex&#65292;&#26159;&#22270;&#25628;&#32034;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#24212;&#29992;&#20013;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#23384;&#22312;&#35768;&#22810;&#32463;&#39564;&#31639;&#27861;&#65292;&#22312;&#25928;&#29575;&#26041;&#38754;&#27809;&#26377;&#36275;&#22815;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#36755;&#20837;&#23454;&#20363;&#30340;&#19968;&#20010;&#26032;&#21442;&#25968;$g_k(G)$&#65292;&#26368;&#22823;$k$-plex&#30340;&#36864;&#21270;&#36793;&#30028;&#21644;&#22823;&#23567;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26681;&#25454;$g_k(G)$&#21442;&#25968;&#21270;&#30340;&#31934;&#30830;&#31639;&#27861;&#65292;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#19982;&#36755;&#20837;&#22270;&#30340;&#22823;&#23567;&#25104;&#27491;&#27604;&#65292;&#25351;&#25968;&#22797;&#26434;&#24230;&#19982;$g_k(G)$&#25104;&#27491;&#27604;&#65292;&#20854;&#20013;$k$&#26159;&#19968;&#20010;&#24120;&#25968;&#12290;&#36890;&#24120;&#65292;&#23454;&#38469;&#22270;&#30340;$g_k(G)$&#24456;&#23567;&#65292;&#34987;$O(\log{(|V|)})$&#38480;&#21046;&#65292;&#36825;&#34920;&#26126;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph, the $k$-plex is a vertex set in which each vertex is not adjacent to at most $k-1$ other vertices in the set. The maximum $k$-plex problem, which asks for the largest $k$-plex from a given graph, is an important but computationally challenging problem in applications like graph search and community detection. So far, there is a number of empirical algorithms without sufficient theoretical explanations on the efficiency. We try to bridge this gap by defining a novel parameter of the input instance, $g_k(G)$, the gap between the degeneracy bound and the size of maximum $k$-plex in the given graph, and presenting an exact algorithm parameterized by $g_k(G)$. In other words, we design an algorithm with running time polynomial in the size of input graph and exponential in $g_k(G)$ where $k$ is a constant. Usually, $g_k(G)$ is small and bounded by $O(\log{(|V|)})$ in real-world graphs, indicating that the algorithm runs in polynomial time. We also carry out massive experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.00183</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#25193;&#25955;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#34920;&#31034;&#24050;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26356;&#21152;&#28145;&#20837;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#26159;&#22914;&#20309;&#34987;&#32534;&#30721;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32473;&#23450;&#23618;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#23637;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#23545;&#20110;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#22823;&#23567;&#30340;&#20219;&#20309;&#38543;&#26426;&#23376;&#38598;&#31070;&#32463;&#20803;&#65292;&#37117;&#19982;&#23436;&#25972;&#23618;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33021;&#22815;&#34920;&#29616;&#20986;&#19982;&#25972;&#20010;&#23618;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#65288;&#21253;&#25324;CNN&#21644;Transformer&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;ImageNet1k&#21644;ImageNet21k&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20887;&#20313;&#26469;&#38477;&#20302;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20262;&#29702;&#35780;&#20215;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21019;&#24314;&#22810;&#27169;&#24577;&#20262;&#29702;&#25968;&#25454;&#24211;&#26469;&#21327;&#21516;&#21028;&#26029;&#31995;&#32479;&#26159;&#21542;&#28385;&#36275;&#20262;&#29702;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13765</link><description>&lt;p&gt;
&#36208;&#21521;&#20262;&#29702;&#22810;&#27169;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards ethical multimodal systems. (arXiv:2304.13765v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20262;&#29702;&#35780;&#20215;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21019;&#24314;&#22810;&#27169;&#24577;&#20262;&#29702;&#25968;&#25454;&#24211;&#26469;&#21327;&#21516;&#21028;&#26029;&#31995;&#32479;&#26159;&#21542;&#28385;&#36275;&#20262;&#29702;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#25105;&#20204;&#31038;&#20250;&#30340;&#24433;&#21709;&#27491;&#22312;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;&#20363;&#22914;&#65292;ChatGPT&#27491;&#22312;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#27835;&#30103;&#24212;&#29992;&#30340;&#27979;&#35797;&#65292;&#22914;Koko&#65292;Stable Diffusion&#29983;&#25104;&#30340;&#33402;&#26415;&#20316;&#21697;&#19982;&#20154;&#31867;&#33402;&#26415;&#23478;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#34892;&#20026;&#21644;&#24212;&#29992;&#30340;&#20262;&#29702;&#38382;&#39064;&#36817;&#24180;&#26469;&#19981;&#26029;&#22686;&#21152;&#65292;AI&#23545;&#40784;&#39046;&#22495;&#8212;&#8212;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#34892;&#20026;&#24341;&#23548;&#21521;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#30340;&#26041;&#21521;&#8212;&#8212;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#23376;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20262;&#29702;&#35780;&#20215;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21516;&#26102;&#20197;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#23436;&#25104;&#20316;&#20026;&#36755;&#20837;&#30340;&#21477;&#23376;&#25110;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#20998;&#20004;&#27493;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65306;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#20262;&#29702;&#25968;&#25454;&#24211;&#30340;&#21019;&#24314;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#24211;&#26469;&#21327;&#21516;&#21028;&#26029;&#31995;&#32479;&#26159;&#21542;&#28385;&#36275;&#20262;&#29702;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impact of artificial intelligence systems on our society is increasing at an unprecedented speed. For instance, ChatGPT is being tested in mental health treatment applications such as Koko, Stable Diffusion generates pieces of art competitive with (or outperforming) human artists, and so on. Ethical concerns regarding the behavior and applications of generative AI systems have been increasing over the past years, and the field of AI alignment - steering the behavior of AI systems towards being aligned with human values - is a rapidly growing subfield of modern AI. In this paper, we address the challenges involved in ethical evaluation of a multimodal artificial intelligence system. The multimodal systems we focus on take both text and an image as input and output text, completing the sentence or answering the question asked as input. We perform the evaluation of these models in two steps: we first discus the creation of a multimodal ethical database and then use this database to co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#27169;&#22359;DWA&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24046;&#24322;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#24182;&#25233;&#21046;&#22122;&#22768;&#12290;&#22312;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#38598;&#25104;DWA&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01994</link><description>&lt;p&gt;
DWA&#65306;&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
DWA: Differential Wavelet Amplifier for Image Super-Resolution. (arXiv:2304.01994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#27169;&#22359;DWA&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24046;&#24322;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#24182;&#25233;&#21046;&#22122;&#22768;&#12290;&#22312;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#38598;&#25104;DWA&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;(DWA)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;(SR)&#27169;&#22359;&#12290;DWA&#20026;&#26368;&#36817;&#25910;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#28151;&#21512;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;(DWT)&#26041;&#27861;&#27880;&#20837;&#27963;&#21147;&#12290;DWT&#33021;&#22815;&#26377;&#25928;&#22320;&#20026;SR&#25552;&#20379;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#31354;&#38388;&#38754;&#31215;&#20943;&#23569;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23567;&#20102;&#27169;&#22411;&#24635;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#25104;&#20026;&#21487;&#25345;&#32493;ML&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DWA&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#65292;&#24378;&#35843;&#23616;&#37096;&#23545;&#27604;&#24230;&#24182;&#25233;&#21046;&#36755;&#20837;&#20449;&#21495;&#20013;&#30340;&#24120;&#35265;&#22122;&#22768;&#12290;&#23558;&#20854;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;SR&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;DWA&#20351;DWSR&#21644;MWCNN&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#36755;&#20837;&#22270;&#20687;&#31354;&#38388;&#65292;&#22240;&#20026;&#23427;&#30465;&#30053;&#20102;DWT&#34920;&#31034;&#30340;&#36890;&#36947;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing the DWT representation channel-wise since it omits 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.06273</link><description>&lt;p&gt;
ChatGPT&#30340;&#19968;&#33268;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the consistency issue of ChatGPT and finds that although it has improved language understanding ability, it frequently fails to generate logically correct predictions. Therefore, further consideration is needed for its real-world applications, especially in terms of risk.
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#31572;&#23545;&#35805;&#31995;&#32479;&#65292;&#33258;&#25512;&#20986;&#20197;&#26469;&#24191;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#23427;&#22312;&#27861;&#24459;&#12289;&#21307;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#19987;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#65292;&#20294;&#20063;&#26377;&#20154;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24230;&#34920;&#31034;&#24576;&#30097;&#12290;&#26412;&#25991;&#38024;&#23545;ChatGPT&#22312;&#36923;&#36753;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;ChatGPT&#20284;&#20046;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;ChatGPT&#26159;&#19968;&#31181;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#21644;&#26377;&#21069;&#36884;&#30340;&#26032;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22914;&#26524;&#27809;&#26377;&#32463;&#36807;&#24443;&#24213;&#30340;&#20154;&#24037;&#26816;&#26597;&#65292;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, a question-and-answer dialogue system based on a large language model, has gained huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, including the law, medical, and finance domains, adding extra support to the claim that AI now can assist and, even, replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. In this paper, we investigate ChatGPT's trustworthiness regarding logically consistent behaviours. Our findings suggest that, although ChatGPT seems to achieve an improved language understanding ability, it still fails to generate logically correct predictions frequently. Hence, while it is true that ChatGPT is an impressive and promising new technique, we conclude that its usage in real-world applications without thorough human inspection requires further consideration, especially for risk
&lt;/p&gt;</description></item><item><title>&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;</title><link>http://arxiv.org/abs/2302.08942</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08942
&lt;/p&gt;
&lt;p&gt;
&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#24320;&#21457;&#20102;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#31532;&#19968;&#20010;&#20851;&#20110;Wasserstein&#36317;&#31163;&#30340;&#32467;&#26524;&#20551;&#35774;&#23454;&#20363;&#31354;&#38388;&#26159;&#26377;&#30028;&#30340;&#65292;&#32780;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#32467;&#26524;&#21033;&#29992;&#20102;&#38477;&#32500;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#33258;&#28982;&#36866;&#29992;&#20110;Wasserstein GAN&#21644;Energy-Based GAN&#65292;&#32780;&#25105;&#20204;&#30340;&#30028;&#38480;&#20026;&#36825;&#20004;&#31181;GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#26159;&#29702;&#35770;&#24615;&#30340;&#65292;&#20294;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;Wasserstein GAN&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.13854</link><description>&lt;p&gt;
ComCLIP: &#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#26041;&#38754;&#30340;&#24456;&#22909;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558; CLIP &#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#38656;&#35201;&#27169;&#22411;&#29702;&#35299;&#32452;&#21512;&#35789;&#27010;&#24565;&#21644;&#35270;&#35273;&#32452;&#20214;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#65306;&#21333;&#20010;&#23454;&#20307;&#30340;&#38169;&#35823;&#35821;&#20041;&#26412;&#36136;&#19978;&#26159;&#23548;&#33268;&#21305;&#37197;&#22833;&#36133;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26080;&#38656;&#35757;&#32451;&#8221;&#30340;&#32452;&#21512; CLIP &#27169;&#22411;&#65288;ComCLIP&#65289;&#12290;ComCLIP&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32452;&#21512; CLIP &#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#22312;&#32452;&#21512;&#25991;&#26412;&#23884;&#20837;&#21644;&#23376;&#22270;&#20687;&#23884;&#20837;&#20043;&#19978;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ComCLIP &#21487;&#20197;&#20943;&#36731;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-lanaguage pretrained models like CLIP to compositional image and text matching -- a more challenging image and text matching task requiring the model understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel \textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action sub-images and composes CLIP's vision encoder and text encoder to perform evolving matching over compositional text embedding and sub-image embeddings. In this way, ComCLIP can mitigate spurio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;HMOE&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#26631;&#31614;&#65292;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.08253</link><description>&lt;p&gt;
HMOE: &#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;HMOE&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#26631;&#31614;&#65292;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel domain generalization method called HMOE, which does not rely on domain labels and is more interpretable. HMOE uses hypernetworks to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. Experimental results show that HMOE can divide mixed data and achieve better performance.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#36825;&#23601;&#26159;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#30340;&#30446;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;DG&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#21487;&#29992;&#30340;&#39046;&#22495;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DG&#26041;&#27861;&#65292;&#31216;&#20026;HMOE&#65306;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#39046;&#22495;&#26631;&#31614;&#65292;&#24182;&#19988;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#12290;MoE&#22312;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#27169;&#24335;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#23545;&#20110;DG&#38382;&#39064;&#65292;&#24322;&#36136;&#24615;&#27491;&#26159;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#32780;&#20135;&#29983;&#30340;&#12290;HMOE&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#36825;&#20351;&#24471;&#19987;&#23478;&#21487;&#20197;&#20849;&#20139;&#26377;&#29992;&#30340;&#20803;&#30693;&#35782;&#65292;&#24182;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#20844;&#24179;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;-DomainBed&#19979;&#23558;HMOE&#19982;&#20854;&#20182;DG&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to domain shift, machine learning systems typically fail to generalize well to domains different from those of training data, which is what domain generalization (DG) aims to address. Although various DG methods have been developed, most of them lack interpretability and require domain labels that are not available in many real-world scenarios. This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does not rely on domain labels and is more interpretable. MoE proves effective in identifying heterogeneous patterns in data. For the DG problem, heterogeneity arises exactly from domain shift. HMOE uses hypernetworks taking vectors as input to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. We compare HMOE with other DG algorithms under a fair and unified benchmark-DomainBed. Our extensive experiments show that HMOE can divide mixe
&lt;/p&gt;</description></item></channel></rss>