<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.14115</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#35299;&#37322;&#65306;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35299;&#37322;&#25104;&#20026;&#20102;&#36890;&#36807;&#36873;&#25321;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#38598;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#20013;&#20027;&#35201;&#21464;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#33258;&#25105;&#35299;&#37322;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#29255;&#27573;&#39640;&#24230;&#20114;&#30456;&#20851;&#32852;&#26102;&#26080;&#27861;&#35782;&#21035;&#30495;&#27491;&#30340;&#35299;&#37322;&#65292;&#22240;&#27492;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#31867;&#20284;&#30340;&#36129;&#29486;&#65292;&#25152;&#35859;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#26032;&#39062;&#22320;&#23558;&#20004;&#20010;&#22240;&#26524;&#26399;&#26395;&#20540;&#65288;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65289;&#24341;&#20837;&#20102;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#35299;&#37322;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#30340;&#22240;&#26524;&#27010;&#29575;&#65292;&#36890;&#36807;&#20854;&#29702;&#35770;&#37492;&#23450;&#65292;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35780;&#35770;&#21644;&#21307;&#30103;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#26469;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#20915;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38750; i.i.d. &#38382;&#39064;</title><link>http://arxiv.org/abs/2306.14114</link><description>&lt;p&gt;
TNPAR: &#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20107;&#20214;&#24207;&#21015;Granger&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences. (arXiv:2306.14114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#26469;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#20915;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38750; i.i.d. &#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20107;&#20214;&#24207;&#21015;&#20013;&#23398;&#20064;Granger&#22240;&#26524;&#20851;&#31995;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#20107;&#20214;&#24207;&#21015;&#29420;&#31435;&#21516;&#20998;&#24067; (i.i.d.) &#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20107;&#20214;&#24207;&#21015;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#19968; i.i.d. &#20551;&#35774;&#32463;&#24120;&#34987;&#36829;&#21453;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#34987;&#24314;&#27169;&#25104;&#19968;&#20010;&#25299;&#25169;&#32593;&#32476;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23558;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#24341;&#20837;Granger&#22240;&#26524;&#21457;&#29616;&#26469;&#35299;&#20915;&#38750; i.i.d. &#38382;&#39064;&#12290;&#36825;&#19968;&#21457;&#29616;&#20419;&#20351;&#25105;&#20204;&#35299;&#20915;&#20004;&#20010;&#38382;&#39064;&#65306;1) &#22914;&#20309;&#22312;&#27169;&#22411;&#20107;&#20214;&#24207;&#21015;&#26102;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#65307;2) &#22914;&#20309;&#23398;&#20064;Granger&#22240;&#26524;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#32479;&#19968;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#22312;&#29983;&#25104;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#31070;&#32463;&#27850;&#26494;&#36807;&#31243;&#30340;&#19968;&#31181;&#21464;&#20307;&#26469;&#24314;&#27169;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#21051;&#65292;&#24182;&#36890;&#36807;&#25299;&#25169;&#20851;&#31995;&#21644;&#29616;&#26377;&#20107;&#20214;&#24207;&#21015;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Granger causality from event sequences is a challenging but essential task across various applications. Most existing methods rely on the assumption that event sequences are independent and identically distributed (i.i.d.). However, this i.i.d. assumption is often violated due to the inherent dependencies among the event sequences. Fortunately, in practice, we find these dependencies can be modeled by a topological network, suggesting a potential solution to the non-i.i.d. problem by introducing the prior topological network into Granger causal discovery. This observation prompts us to tackle two ensuing challenges: 1) how to model the event sequences while incorporating both the prior topological network and the latent Granger causal structure, and 2) how to learn the Granger causal structure. To this end, we devise a two-stage unified topological neural Poisson auto-regressive model. During the generation stage, we employ a variant of the neural Poisson process to model the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.14111</link><description>&lt;p&gt;
RLHF&#26159;&#21542;&#27604;&#26631;&#20934;RL&#26356;&#22256;&#38590;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is RLHF More Difficult than Standard RL?. (arXiv:2306.14111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#20174;&#20559;&#22909;&#20449;&#21495;&#23398;&#20064;&#65292;&#32780;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21017;&#30452;&#25509;&#20174;&#22870;&#21169;&#20449;&#21495;&#23398;&#20064;&#12290;&#20559;&#22909;&#20449;&#21495;&#21487;&#33021;&#21253;&#21547;&#30340;&#20449;&#24687;&#27604;&#22870;&#21169;&#20449;&#21495;&#23569;&#65292;&#36825;&#20351;&#24471;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#20284;&#20046;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#31867;&#65306;&#65288;1&#65289;&#22522;&#20110;&#22870;&#21169;&#27010;&#29575;&#27169;&#22411;&#30340;&#20559;&#22909;&#65292;&#27492;&#26102;&#21487;&#20197;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#23481;&#24525;&#22870;&#21169;&#23567;&#35823;&#24046;&#30340;&#40065;&#26834;&#22870;&#21169;RL&#38382;&#39064;&#65307;&#65288;2&#65289;&#23545;&#20110;&#19968;&#33324;&#30340;&#20219;&#24847;&#20559;&#22909;&#19988;&#30446;&#26631;&#26159;&#25214;&#21040;von Neumann&#33719;&#32988;&#32773;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#22810;&#26234;&#33021;&#20307;&#22870;&#21169;RL&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#22312;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;&#31574;&#30053;&#19979;&#25214;&#21040;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22240;&#23376;&#32435;&#20160;&#24179;&#34913;&#35299;&#12290;&#21518;&#19968;&#31181;&#24773;&#20917;&#21487;&#20197;&#36827;&#19968;&#27493;&#38477;&#20302;&#25104;&#23545;&#20851;&#31995;&#30340;MDP&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduce to adversarial MDP when preferences only depend on the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#24182;&#20171;&#32461;&#20102;&#21322;&#30417;&#30563;&#29289;&#20307;&#26816;&#27979; (SSOD) &#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#20027;&#35201;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20266;&#26631;&#31614;&#65292;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65292;&#22270;&#24418;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#21322;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.14106</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#29289;&#20307;&#26816;&#27979;&#65306;&#26368;&#26032;&#30740;&#31350;&#21644;&#36827;&#23637;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Object Detection: A Survey on Recent Research and Progress. (arXiv:2306.14106v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#24182;&#20171;&#32461;&#20102;&#21322;&#30417;&#30563;&#29289;&#20307;&#26816;&#27979; (SSOD) &#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#20027;&#35201;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20266;&#26631;&#31614;&#65292;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65292;&#22270;&#24418;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#21322;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#29087;&#24212;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#39046;&#22495;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#36235;&#21521;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38656;&#35201;&#39640;&#26114;&#30340;&#20154;&#21147;&#25104;&#26412;&#65292;&#23548;&#33268;&#20302;&#25928;&#29575;&#21644;&#38480;&#21046;&#12290;&#21322;&#30417;&#30563;&#29289;&#20307;&#26816;&#27979; (SSOD) &#22240;&#20854;&#39640;&#30740;&#31350;&#20215;&#20540;&#21644;&#23454;&#29992;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23427;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#20449;&#24687;&#12290;&#26412;&#25991;&#20174;&#20116;&#20010;&#26041;&#38754;&#20840;&#38754;&#32780;&#26368;&#26032;&#22320;&#20171;&#32461;&#20102; SSOD &#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;&#20960;&#31181;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20027;&#27969;&#30340;&#21322;&#30417;&#30563;&#31574;&#30053;&#20998;&#20026;&#20266;&#26631;&#31614;&#65292;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65292;&#22522;&#20110;&#22270;&#24418;&#21644;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#22312;&#26377;&#25361;&#25112;&#30340;&#24773;&#20917;&#19979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#28982;&#21518;&#27010;&#36848;&#20102;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#36817;&#22312; SSOD &#19978;&#30340;&#36827;&#23637;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning technology has been maturely applied in the field of object detection, and most algorithms tend to be supervised learning. However, a large amount of labeled data requires high costs of human resources, which brings about low efficiency and limitations. Semi-supervised object detection (SSOD) has been paid more and more attentions due to its high research value and practicability. It is designed to learn information by using small amounts of labeled data and large amounts of unlabeled data. In this paper, we present a comprehensive and up-to-date survey on the SSOD approaches from five aspects. We first briefly introduce several ways of data augmentation. Then, we dive the mainstream semi-supervised strategies into pseudo labels, consistent regularization, graph based and transfer learning based methods, and introduce some methods in challenging settings. We further present widely-used loss functions, and then we outline the common benchmark datasets and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24369;&#23398;&#20064;&#22120;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;boosting&#31639;&#27861;&#20013;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20248;&#20110;&#20256;&#32479;&#30340;&#26641;&#24418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14101</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#24369;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models are weak learners. (arXiv:2306.14101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24369;&#23398;&#20064;&#22120;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;boosting&#31639;&#27861;&#20013;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20248;&#20110;&#20256;&#32479;&#30340;&#26641;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#21644;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20013;&#24515;&#27010;&#24565;&#26159;&#24369;&#23398;&#20064;&#22120;&#65292;&#21363;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#37117;&#33021;&#21462;&#24471;&#27604;&#38543;&#26426;&#26356;&#22909;&#30340;&#24615;&#33021;&#30340;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#21482;&#26159;&#30053;&#24494;&#22909;&#19968;&#28857;&#12290;&#36825;&#26679;&#30340;&#24369;&#23398;&#20064;&#22120;&#26500;&#25104;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;boosting&#65289;&#30340;&#23454;&#29992;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20316;&#20026;&#19978;&#36848;&#24369;&#23398;&#20064;&#22120;&#36827;&#34892;&#25805;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;boosting&#31639;&#27861;&#20013;&#30340;&#24369;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25552;&#20379;&#65288;&#26681;&#25454;&#24863;&#20852;&#36259;&#30340;&#20998;&#24067;&#36827;&#34892;&#36866;&#24403;&#37319;&#26679;&#30340;&#65289;&#34920;&#26684;&#25968;&#25454;&#26679;&#26412;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;LLM&#21487;&#20197;&#20135;&#29983;&#26679;&#26412;&#30340;&#27719;&#24635;&#65292;&#20316;&#20026;&#20998;&#31867;&#30340;&#27169;&#26495;&#65292;&#24182;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20316;&#20026;&#24369;&#23398;&#20064;&#22120;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#32435;&#20837;boosting&#26041;&#27861;&#20013;&#65292;&#22312;&#26576;&#20123;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21033;&#29992;LLM&#20013;&#30340;&#30693;&#35782;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#26641;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central notion in practical and theoretical machine learning is that of a $\textit{weak learner}$, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.14096</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#23454;&#20307;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#23376;&#20219;&#21153;&#65292;&#30446;&#21069;&#38754;&#20020;&#30528;&#20247;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24320;&#21457;&#26377;&#25928;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#25152;&#38656;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#27169;&#24335;&#21305;&#37197;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;&#29616;&#26377;&#24320;&#28304;LLMs&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#21160;&#30495;&#23454;&#19990;&#30028;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24179;&#28369;&#30340;&#36317;&#31163;&#25968;&#25454;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#21644;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14079</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#65306;&#36890;&#36807;&#25193;&#25955;&#20998;&#25968;&#21305;&#37197;&#23454;&#29616;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching. (arXiv:2306.14079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24179;&#28369;&#30340;&#36317;&#31163;&#25968;&#25454;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#21644;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20248;&#21270;&#33539;&#24335;&#65292;&#20363;&#22914;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25110;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#20801;&#35768;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#20294;&#38656;&#35201;&#20180;&#32454;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#20197;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#20854;&#22312;&#39640;&#32500;&#24230;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#20180;&#32454;&#22320;&#32771;&#34385;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#20114;&#24433;&#21709;&#12290;&#25105;&#20204;&#22768;&#31216;&#65292;&#20026;&#20102;&#35753;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#23427;&#24517;&#39035;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#22320;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24179;&#28369;&#30340;&#25968;&#25454;&#36317;&#31163;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19981;&#20165;&#31283;&#23450;&#22320;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;Lipschitz&#24120;&#25968;&#26469;&#20998;&#26512;&#27169;&#22411;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24179;&#28369;&#30340;&#25968;&#25454;&#36317;&#31163;&#21644;&#25968;&#25454;&#20284;&#28982;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline optimization paradigms such as offline Reinforcement Learning (RL) or Imitation Learning (IL) allow policy search algorithms to make use of offline data, but require careful incorporation of uncertainty in order to circumvent the challenges of distribution shift. Gradient-based policy search methods are a promising direction due to their effectiveness in high dimensions; however, we require a more careful consideration of how these methods interplay with uncertainty estimation. We claim that in order for an uncertainty metric to be amenable for gradient-based optimization, it must be (i) stably convergent to data when uncertainty is minimized with gradients, and (ii) not prone to underestimation of true uncertainty. We investigate smoothed distance to data as a metric, and show that it not only stably converges to data, but also allows us to analyze model bias with Lipschitz constants. Moreover, we establish an equivalence between smoothed distance to data and data likelihood, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20840;&#33258;&#21160;&#21270;&#30340;LLM&#23545;&#35805;&#32447;&#31243;&#31639;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#30340;&#25506;&#32034;&#21644;&#23637;&#24320;&#65292;&#23454;&#29616;&#23545;&#20219;&#21153;&#30340;&#28145;&#24230;&#36880;&#27493;&#25512;&#29702;&#65292;&#24182;&#24212;&#29992;&#20110;&#25512;&#35770;&#39044;&#27979;&#12289;&#22240;&#26524;&#35299;&#37322;&#12289;&#24314;&#35758;&#31561;&#26041;&#26696;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.14077</link><description>&lt;p&gt;
&#24102;And-Or Recursors&#21644;Refiner Oracles&#30340;&#30446;&#26631;&#39537;&#21160;LLM&#23545;&#35805;&#32447;&#31243;&#30340;&#20840;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Full Automation of Goal-driven LLM Dialog Threads with And-Or Recursors and Refiner Oracles. (arXiv:2306.14077v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20840;&#33258;&#21160;&#21270;&#30340;LLM&#23545;&#35805;&#32447;&#31243;&#31639;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#30340;&#25506;&#32034;&#21644;&#23637;&#24320;&#65292;&#23454;&#29616;&#23545;&#20219;&#21153;&#30340;&#28145;&#24230;&#36880;&#27493;&#25512;&#29702;&#65292;&#24182;&#24212;&#29992;&#20110;&#25512;&#35770;&#39044;&#27979;&#12289;&#22240;&#26524;&#35299;&#37322;&#12289;&#24314;&#35758;&#31561;&#26041;&#26696;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#36882;&#24402;&#22320;&#25506;&#32034;&#22791;&#36873;&#39033;(OR&#33410;&#28857;)&#21644;&#23637;&#24320;&#32454;&#33410;(AND&#33410;&#28857;)&#26469;&#33258;&#21160;&#21270;LLM&#23545;&#35805;&#32447;&#31243;&#20013;&#30340;&#28145;&#24230;&#36880;&#27493;&#25512;&#29702;&#65292;&#20197;&#36798;&#21040;&#32473;&#23450;&#28145;&#24230;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#31616;&#27905;&#30340;&#20219;&#21153;&#29305;&#23450;&#21551;&#21160;&#22120;&#24320;&#22987;&#65292;&#36890;&#36807;&#21512;&#25104;&#19968;&#20010;&#24635;&#32467;&#36804;&#20170;&#20026;&#27490;&#28145;&#24230;&#20248;&#20808;&#27493;&#39588;&#30340;&#25552;&#31034;&#65292;&#26469;&#24341;&#23548;&#33258;&#21160;&#23545;&#35805;&#32447;&#31243;&#19987;&#27880;&#20110;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#28304;&#33258;&#19968;&#20010;Horn Clause&#35299;&#37322;&#22120;&#30340;&#31616;&#21333;&#36882;&#24402;&#19979;&#38477;&#23454;&#29616;&#65292;&#20294;&#25105;&#20204;&#23558;&#36923;&#36753;&#24341;&#25806;&#35843;&#25972;&#24471;&#36866;&#24212;LLMs&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#24335;&#12290;&#35821;&#20041;&#30456;&#20284;&#24615;&#19982;&#22522;&#26412;&#20107;&#23454;&#25110;&#26469;&#33258;&#21478;&#19968;&#20010;LLM&#23454;&#20363;&#30340;oracle&#24314;&#35758;&#29992;&#20110;&#38480;&#21046;&#25628;&#32034;&#31354;&#38388;&#24182;&#39564;&#35777;&#20316;&#20026;&#31572;&#26696;&#36820;&#22238;&#30340;&#32467;&#26524;&#30340;&#21512;&#29702;&#21270;&#27493;&#39588;&#12290;&#26368;&#21518;&#65292;&#29983;&#25104;&#30340;Horn Clause&#31243;&#24207;&#30340;&#21807;&#19968;&#26368;&#23567;&#27169;&#22411;&#25910;&#38598;&#25512;&#29702;&#36807;&#31243;&#30340;&#32467;&#26524;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25512;&#35770;&#39044;&#27979;&#12289;&#22240;&#26524;&#35299;&#37322;&#12289;&#24314;&#35758;&#31561;&#26041;&#26696;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We automate deep step-by step reasoning in an LLM dialog thread by recursively exploring alternatives (OR-nodes) and expanding details (AND-nodes) up to a given depth. Starting from a single succinct task-specific initiator we steer the automated dialog thread to stay focussed on the task by synthesizing a prompt that summarizes the depth-first steps taken so far.  Our algorithm is derived from a simple recursive descent implementation of a Horn Clause interpreter, except that we accommodate our logic engine to fit the natural language reasoning patterns LLMs have been trained on. Semantic similarity to ground-truth facts or oracle advice from another LLM instance is used to restrict the search space and validate the traces of justification steps returned as answers. At the end, the unique minimal model of a generated Horn Clause program collects the results of the reasoning process.  As applications, we sketch implementations of consequence predictions, causal explanations, recommenda
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TPP&#24314;&#27169;&#26041;&#27861;&#65292;&#23558;&#36830;&#32493;&#26102;&#38388;&#21367;&#31215;&#20107;&#20214;&#32534;&#30721;&#22120;&#19982;RNN&#38598;&#25104;&#65292;&#20197;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.14072</link><description>&lt;p&gt;
&#26080;&#24378;&#24230;&#21367;&#31215;&#26102;&#31354;&#28857;&#36807;&#31243;: &#34701;&#21512;&#23616;&#37096;&#19982;&#20840;&#23616;&#20107;&#20214;&#35821;&#22659;
&lt;/p&gt;
&lt;p&gt;
Intensity-free Convolutional Temporal Point Process: Incorporating Local and Global Event Contexts. (arXiv:2306.14072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TPP&#24314;&#27169;&#26041;&#27861;&#65292;&#23558;&#36830;&#32493;&#26102;&#38388;&#21367;&#31215;&#20107;&#20214;&#32534;&#30721;&#22120;&#19982;RNN&#38598;&#25104;&#65292;&#20197;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#39046;&#22495;&#30340;&#20107;&#20214;&#39044;&#27979;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#20294;&#30456;&#24403;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#26102;&#38388;&#28857;&#36807;&#31243;(TPP)&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#20248;&#21183;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#20351;&#29992;&#20687;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#25110;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#20043;&#31867;&#30340;&#25216;&#26415;&#26469;&#32534;&#30721;&#20107;&#20214;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#20294;&#26159;&#65292;&#23616;&#37096;&#20107;&#20214;&#19978;&#19979;&#25991;&#23545;&#20107;&#20214;&#30340;&#21457;&#29983;&#20063;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#36825;&#26041;&#38754;&#21364;&#24456;&#23569;&#34987;&#20851;&#27880;&#12290;&#27969;&#34892;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19987;&#20026;&#25429;&#33719;&#23616;&#37096;&#19978;&#19979;&#25991;&#32780;&#35774;&#35745;&#65292;&#20294;&#30001;&#20110;&#26080;&#27861;&#22312;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#21270;&#65292;&#22240;&#27492;&#20174;&#26410;&#24212;&#29992;&#20110;TPP&#24314;&#27169;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;TPP&#24314;&#27169;&#26041;&#27861;&#65292;&#21363;&#23558;&#36830;&#32493;&#26102;&#38388;&#21367;&#31215;&#20107;&#20214;&#32534;&#30721;&#22120;&#19982;RNN&#38598;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#38271;&#24207;&#21015;&#21644;&#22797;&#26434;&#28508;&#22312;&#27169;&#24335;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event prediction in the continuous-time domain is a crucial but rather difficult task. Temporal point process (TPP) learning models have shown great advantages in this area. Existing models mainly focus on encoding global contexts of events using techniques like recurrent neural networks (RNNs) or self-attention mechanisms. However, local event contexts also play an important role in the occurrences of events, which has been largely ignored. Popular convolutional neural networks, which are designated for local context capturing, have never been applied to TPP modelling due to their incapability of modelling in continuous time. In this work, we propose a novel TPP modelling approach that combines local and global contexts by integrating a continuous-time convolutional event encoder with an RNN. The presented framework is flexible and scalable to handle large datasets with long sequences and complex latent patterns. The experimental result shows that the proposed model improves the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20026;&#34920;&#26684;MDPs&#25512;&#23548;&#20986;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#30340;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.14063</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data. (arXiv:2306.14063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20026;&#34920;&#26684;MDPs&#25512;&#23548;&#20986;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#30340;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#31163;&#32447;RL&#26041;&#27861;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#29702;&#35770;&#20445;&#35777;&#26159;&#23454;&#29616;&#25968;&#25454;&#38656;&#27714;&#37327;&#36739;&#22823;&#30340;RL&#31639;&#27861;&#23454;&#38469;&#21487;&#34892;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#32467;&#26524;&#20381;&#36182;&#20110;&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#21363;&#21253;&#25324;&#19968;&#20010;&#30001;&#21333;&#19968;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;i.i.d.&#36712;&#36857;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#65292;&#21363;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#33258;&#36866;&#24212;&#25910;&#38598;&#30340;&#12290;&#25105;&#20204;&#20026;&#34920;&#26684;MDPs&#20013;&#30340;TMIS&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#20272;&#35745;&#22120;&#22312;&#36825;&#20010;&#24191;&#20041;&#35774;&#32622;&#20013;&#24320;&#21457;&#29702;&#35770;&#65292;&#25512;&#23548;&#20854;&#20272;&#35745;&#35823;&#24046;&#30340;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#36793;&#30028;&#12290;&#25105;&#20204;&#36824;&#22238;&#25910;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#27169;&#25311;&#65292;&#20197;&#32463;&#39564;&#20998;&#26512;&#36825;&#20123;&#20272;&#35745;&#22120;&#22312;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#27169;&#24335;&#19979;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing theoretical guarantees on the sample complexity of offline RL methods is an important step towards making data-hungry RL algorithms practically viable. Currently, most results hinge on unrealistic assumptions about the data distribution -- namely that it comprises a set of i.i.d. trajectories collected by a single logging policy. We consider a more general setting where the dataset may have been gathered adaptively. We develop theory for the TMIS Offline Policy Evaluation (OPE) estimator in this generalized setting for tabular MDPs, deriving high-probability, instance-dependent bounds on its estimation error. We also recover minimax-optimal offline learning in the adaptive setting. Finally, we conduct simulations to empirically analyze the behavior of these estimators under adaptive and non-adaptive regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#27169;&#31946;&#30340;&#32593;&#32476;&#25915;&#20987;&#25551;&#36848;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20197;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#30340;BaseLLM&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#25915;&#20987;&#25216;&#26415;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#36229;&#36234;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#20998;&#26512;&#30340;&#24433;&#21709;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14062</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#27169;&#31946;&#30340;&#32593;&#32476;&#25915;&#20987;&#25551;&#36848;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions. (arXiv:2306.14062v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#27169;&#31946;&#30340;&#32593;&#32476;&#25915;&#20987;&#25551;&#36848;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20197;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#30340;BaseLLM&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#25915;&#20987;&#25216;&#26415;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#36229;&#36234;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#20998;&#26512;&#30340;&#24433;&#21709;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28431;&#27934;&#21644;&#25915;&#20987;&#30340;&#25968;&#37327;&#12289;&#31181;&#31867;&#21644;&#36895;&#24230;&#30340;&#21464;&#21270;&#20351;&#24471;&#20154;&#31867;&#19987;&#23478;&#21644;&#32463;&#39564;&#22312;&#20107;&#20214;&#23041;&#32961;&#20998;&#26512;&#20013;&#21464;&#24471;&#22256;&#38590;&#12290;MITRE AT&#65286;CK&#26694;&#26550;&#20351;&#29992;&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTP&#65289;&#25551;&#36848;&#25915;&#20987;&#32773;&#22914;&#20309;&#21644;&#20026;&#20309;&#21033;&#29992;&#28431;&#27934;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#23433;&#20840;&#19987;&#19994;&#20154;&#22763;&#25776;&#20889;&#30340;TTP&#25551;&#36848;&#21487;&#33021;&#34987;&#21478;&#19968;&#20010;&#20154;&#35299;&#37322;&#24471;&#38750;&#24120;&#19981;&#21516;&#65292;&#36825;&#20250;&#23548;&#33268;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#29978;&#33267;&#21830;&#19994;&#12289;&#25919;&#31574;&#21644;&#27861;&#24459;&#20915;&#31574;&#30340;&#28151;&#28102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#24050;&#32463;&#23548;&#33268;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#22312;&#32593;&#32476;&#25805;&#20316;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#22686;&#21152;&#20351;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20852;&#36215;&#65292;&#30001;&#20110;LLM&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;NLP&#20219;&#21153;&#24471;&#21040;&#20102;&#26174;&#30528;&#30340;&#25913;&#21892;&#12290;&#36825;&#35753;&#25105;&#20204;&#36136;&#30097;LLM&#22312;&#22914;&#20309;&#35299;&#37322;TTP&#25110;&#19968;&#33324;&#32593;&#32476;&#25915;&#20987;&#25551;&#36848;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#30452;&#25509;&#20351;&#29992;LLMs&#20197;&#21450;&#35757;&#32451;BaseLLMs&#20197;&#25551;&#36848;TTP&#24182;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20197;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#30340;BaseLLM&#22823;&#22823;&#25913;&#21892;&#20102;TTP&#30340;&#35299;&#37322;&#65292;&#24182;&#21487;&#20197;&#32988;&#36807;&#39046;&#22495;&#19987;&#23478;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#20998;&#26512;&#30340;&#24433;&#21709;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The volume, variety, and velocity of change in vulnerabilities and exploits have made incident threat analysis challenging with human expertise and experience along. The MITRE AT&amp;CK framework employs Tactics, Techniques, and Procedures (TTPs) to describe how and why attackers exploit vulnerabilities. However, a TTP description written by one security professional can be interpreted very differently by another, leading to confusion in cybersecurity operations or even business, policy, and legal decisions. Meanwhile, advancements in AI have led to the increasing use of Natural Language Processing (NLP) algorithms to assist the various tasks in cyber operations. With the rise of Large Language Models (LLMs), NLP tasks have significantly improved because of the LLM's semantic understanding and scalability. This leads us to question how well LLMs can interpret TTP or general cyberattack descriptions. We propose and analyze the direct use of LLMs as well as training BaseLLMs with ATT&amp;CK desc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#22235;&#36275;&#26426;&#22120;&#20154;&#36716;&#21270;&#20026;&#23548;&#33322;&#26426;&#22120;&#20154;&#30340;&#19977;&#20010;&#20851;&#38190;&#20027;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24310;&#36831;&#25405;&#20855;&#8221;&#30340;&#20132;&#20114;&#27169;&#22411;&#20197;&#21450;&#19968;&#31181;&#34892;&#21160;&#23631;&#34109;&#26426;&#21046;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14055</link><description>&lt;p&gt;
&#23558;&#22235;&#36275;&#26426;&#22120;&#20154;&#36716;&#21270;&#20026;&#35270;&#38556;&#20154;&#22763;&#23548;&#33322;&#26426;&#22120;&#20154;&#65306;&#24418;&#24335;&#21270;&#23548;&#33322;&#12289;&#20132;&#20114;&#24314;&#27169;&#21644;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Transforming a Quadruped into a Guide Robot for the Visually Impaired: Formalizing Wayfinding, Interaction Modeling, and Safety Mechanism. (arXiv:2306.14055v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#22235;&#36275;&#26426;&#22120;&#20154;&#36716;&#21270;&#20026;&#23548;&#33322;&#26426;&#22120;&#20154;&#30340;&#19977;&#20010;&#20851;&#38190;&#20027;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24310;&#36831;&#25405;&#20855;&#8221;&#30340;&#20132;&#20114;&#27169;&#22411;&#20197;&#21450;&#19968;&#31181;&#34892;&#21160;&#23631;&#34109;&#26426;&#21046;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#22235;&#36275;&#26426;&#22120;&#20154;&#36716;&#21270;&#20026;&#26381;&#21153;&#20110;&#35270;&#38556;&#20154;&#22763;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#30340;&#21407;&#21017;&#12290;&#23548;&#33322;&#26426;&#22120;&#20154;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#35299;&#20915;&#20165;&#26377; 2% &#21040; 3% &#30340;&#28508;&#22312;&#30450;&#25110;&#35270;&#21147;&#38556;&#30861;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#30340;&#23548;&#30450;&#21160;&#29289;&#30340;&#26377;&#38480;&#29366;&#20917;&#12290;&#20026;&#20102;&#24314;&#36896;&#19968;&#21488;&#25104;&#21151;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#20010;&#20851;&#38190;&#20027;&#39064;&#65306;(1) &#24418;&#24335;&#21270;&#23548;&#30450;&#29356;&#21644;&#20154;&#30340;&#23548;&#33322;&#26426;&#21046;&#65292;(2) &#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#26469;&#25551;&#36848;&#20182;&#20204;&#30340;&#20132;&#20114;&#65292;(3) &#25552;&#39640;&#29992;&#25143;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the principles for transforming a quadrupedal robot into a guide robot for individuals with visual impairments. A guide robot has great potential to resolve the limited availability of guide animals that are accessible to only two to three percent of the potential blind or visually impaired (BVI) users. To build a successful guide robot, our paper explores three key topics: (1) formalizing the navigation mechanism of a guide dog and a human, (2) developing a data-driven model of their interaction, and (3) improving user safety. First, we formalize the wayfinding task of the human-guide robot team using Markov Decision Processes based on the literature and interviews. Then we collect real human-robot interaction data from three visually impaired and six sighted people and develop an interaction model called the ``Delayed Harness'' to effectively simulate the navigation behaviors of the team. Additionally, we introduce an action shielding mechanism to enhance user saf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#31574;&#30053;&#34920;&#31034;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#25552;&#39640;&#20248;&#21270;&#24230;&#24182;&#30830;&#20445;&#31574;&#30053;&#26356;&#26032;&#30340;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#30005;&#21147;&#36127;&#33655;&#39640;&#23792;&#21644;&#20302;&#23792;&#20043;&#38388;&#30340;&#21512;&#29702;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2306.14047</link><description>&lt;p&gt;
&#38754;&#21521;&#38656;&#27714;&#21709;&#24212;&#30340;&#26368;&#20248;&#23450;&#20215;--&#19968;&#31181;&#38750;&#21442;&#25968;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Pricing of Demand Response -- A Nonparametric Constrained Policy Optimization Approach. (arXiv:2306.14047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#31574;&#30053;&#34920;&#31034;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#25552;&#39640;&#20248;&#21270;&#24230;&#24182;&#30830;&#20445;&#31574;&#30053;&#26356;&#26032;&#30340;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#30005;&#21147;&#36127;&#33655;&#39640;&#23792;&#21644;&#20302;&#23792;&#20043;&#38388;&#30340;&#21512;&#29702;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#38477;&#20302;&#30005;&#21147;&#24066;&#22330;&#20379;&#38656;&#20004;&#20391;&#19981;&#30830;&#23450;&#24615;&#21644;&#23792;&#20540;&#36127;&#33655;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;DR&#30740;&#31350;&#32780;&#35328;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#36866;&#24403;&#22320;&#35843;&#25972;&#30005;&#21147;&#20215;&#26684;&#65292;&#20197;&#23558;&#30005;&#21147;&#36127;&#33655;&#20174;&#39640;&#23792;&#36716;&#31227;&#21040;&#20302;&#23792;&#26102;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38750;&#21442;&#25968;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#20248;&#21270;&#24230;&#24182;&#30830;&#20445;&#31574;&#30053;&#26356;&#26032;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demand response (DR) has been demonstrated to be an effective method for reducing peak load and mitigating uncertainties on both the supply and demand sides of the electricity market. One critical question for DR research is how to appropriately adjust electricity prices in order to shift electrical load from peak to off-peak hours. In recent years, reinforcement learning (RL) has been used to address the price-based DR problem because it is a model-free technique that does not necessitate the identification of models for end-use customers. However, the majority of RL methods cannot guarantee the stability and optimality of the learned pricing policy, which is undesirable in safety-critical power systems and may result in high customer bills. In this paper, we propose an innovative nonparametric constrained policy optimization approach that improves optimality while ensuring stability of the policy update, by removing the restrictive assumption on policy representation that the majorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#38543;&#26426;&#24615;&#21487;&#34987;&#25915;&#20987;&#32773;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;&#27969;&#34892;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20854;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#34987;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.14043</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#33258;&#24049;&#30340;&#38543;&#26426;&#26631;&#20934;&#65306;&#38543;&#26426;&#24179;&#28369;&#21644;&#22522;&#20110;PRNG&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks. (arXiv:2306.14043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#38543;&#26426;&#24615;&#21487;&#34987;&#25915;&#20987;&#32773;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;&#27969;&#34892;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20854;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#34987;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24615;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35768;&#22810;&#20851;&#38190;&#21151;&#33021;&#65292;&#21253;&#25324;&#20248;&#21270;&#12289;&#25968;&#25454;&#36873;&#25321;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23558;&#29983;&#25104;&#25110;&#25910;&#38598;&#38543;&#26426;&#24615;&#30340;&#20219;&#21153;&#22806;&#21253;&#32473;&#20102;&#32534;&#35793;&#22120;&#12289;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#25110;&#24037;&#20855;&#38142;&#20013;&#30340;&#20854;&#20182;&#22320;&#26041;&#12290;&#20294;&#26159;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#19981;&#33391;&#38543;&#26426;&#24615;&#29978;&#33267;&#21019;&#24314;&#38543;&#26426;&#24615;&#30340;&#21382;&#21490;&#24736;&#20037;&#65292;&#23601;&#20687;NSA&#25918;&#32622;&#21518;&#38376;&#22312;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#20013;&#20197;&#30772;&#35299;&#21152;&#23494;&#19968;&#26679;&#12290;&#26412;&#25991;&#32771;&#34385;&#26159;&#21542;&#33021;&#22815;&#20165;&#21033;&#29992;&#25915;&#20987;&#32773;&#36890;&#24120;&#20381;&#36182;&#30340;&#38543;&#26426;&#24615;&#26469;&#21361;&#23475;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#38543;&#26426;&#24179;&#28369;&#19978;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#20219;&#24847;&#27169;&#22411;&#30340;&#29305;&#23450;&#36755;&#20837;&#25968;&#25454;&#28857;&#25552;&#20379;&#35748;&#35777;&#12290;&#25105;&#20204;&#36873;&#25321;&#38543;&#26426;&#24179;&#28369;&#26159;&#22240;&#20026;&#23427;&#29992;&#20110;&#23433;&#20840;&#21644;&#23433;&#20840;&#65288;&#29992;&#20110;&#23545;&#25239;&#23545;&#25239;&#24615;&#31034;&#20363;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#65289;&#12290;&#22312;&#24149;&#21518;&#65292;&#23427;&#20381;&#36182;&#20110;&#37319;&#26679;&#39640;&#26031;&#22122;&#22768;&#26469;&#25506;&#32034;&#22260;&#32469;&#25968;&#25454;&#28857;&#30340;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomness supports many critical functions in the field of machine learning (ML) including optimisation, data selection, privacy, and security. ML systems outsource the task of generating or harvesting randomness to the compiler, the cloud service provider or elsewhere in the toolchain. Yet there is a long history of attackers exploiting poor randomness, or even creating it -- as when the NSA put backdoors in random number generators to break cryptography. In this paper we consider whether attackers can compromise an ML system using only the randomness on which they commonly rely. We focus our effort on Randomised Smoothing, a popular approach to train certifiably robust models, and to certify specific input datapoints of an arbitrary model. We choose Randomised Smoothing since it is used for both security and safety -- to counteract adversarial examples and quantify uncertainty respectively. Under the hood, it relies on sampling Gaussian noise to explore the volume around a data poin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PG k-means&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#21010;&#20998;&#26469;&#35299;&#20915;&#31354;&#31751;&#65292;&#25552;&#39640;&#20102;iPQ&#19982;&#37327;&#21270;&#22122;&#22768;&#30340;&#31934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14031</link><description>&lt;p&gt;
&#22522;&#20110;&#21010;&#20998;&#25351;&#23548;&#30340;k-means&#31639;&#27861;&#65306;&#26497;&#31471;&#27169;&#22411;&#21387;&#32553;&#19979;&#30340;&#26497;&#31471;&#31354;&#31751;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Model Compression. (arXiv:2306.14031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PG k-means&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#21010;&#20998;&#26469;&#35299;&#20915;&#31354;&#31751;&#65292;&#25552;&#39640;&#20102;iPQ&#19982;&#37327;&#21270;&#22122;&#22768;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#32039;&#20945;&#24615;&#23545;&#20110;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#26497;&#31471;&#27169;&#22411;&#21387;&#32553;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#37327;&#21270;&#12290;&#26412;&#25991;&#32771;&#34385;&#36845;&#20195;&#20056;&#31215;&#37327;&#21270;&#65288;iPQ&#65289;&#19982;&#37327;&#21270;&#22122;&#22768;&#26159;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20294;&#26159;&#36825;&#31181;&#37327;&#21270;&#26694;&#26550;&#30001;&#20110;&#23384;&#22312;&#31354;&#31751;&#32780;&#23548;&#33268;&#25512;&#29702;&#36136;&#37327;&#19979;&#38477;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#35299;&#20915;&#31354;&#31751;&#26469;&#25552;&#39640;iPQ&#19982;&#37327;&#21270;&#22122;&#22768;&#30340;&#31934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#34987;&#31216;&#20026;&#22522;&#20110;&#21010;&#20998;&#25351;&#23548;&#30340;k-means&#31639;&#27861;&#65288;PG k-means&#65289;&#65292;&#26159;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#30340;&#39640;&#24230;&#22686;&#24378;&#30340;k-means&#23454;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21010;&#20998;&#30340;&#39044;&#20998;&#37197;&#31574;&#30053;&#65292;&#30830;&#20445;&#27809;&#26377;&#21021;&#22987;&#31354;&#31751;&#24182;&#40723;&#21169;&#22343;&#21248;&#30340;&#26435;&#37325;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#20248;&#36234;&#30340;&#31354;&#31751;&#35299;&#20915;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#35880;&#24910;&#22320;&#20998;&#21106;&#22823;&#31751;&#26469;&#25191;&#34892;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#38480;&#21046;&#20102;PG k-means&#30340;&#36845;&#20195;&#24635;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compactness in deep learning can be critical to a model's viability in low-resource applications, and a common approach to extreme model compression is quantization. We consider Iterative Product Quantization (iPQ) with Quant-Noise to be state-of-the-art in this area, but this quantization framework suffers from preventable inference quality degradation due to prevalent empty clusters. In this paper, we propose several novel enhancements aiming to improve the accuracy of iPQ with Quant-Noise by focusing on resolving empty clusters. Our contribution, which we call Partitioning-Guided k-means (PG k-means), is a heavily augmented k-means implementation composed of three main components. First, we propose a partitioning-based pre-assignment strategy that ensures no initial empty clusters and encourages an even weight-to-cluster distribution. Second, we propose an empirically superior empty cluster resolution heuristic executed via cautious partitioning of large clusters. Finally, we constr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;LLMs&#26469;&#29983;&#25104;&#30828;&#20214;&#30340;&#23433;&#20840;&#26029;&#35328;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;SystemVerilog&#26029;&#35328;&#26469;&#26367;&#20195;&#32534;&#20889;&#20855;&#26377;&#25361;&#25112;&#30340;&#23433;&#20840;&#26029;&#35328;&#12290;</title><link>http://arxiv.org/abs/2306.14027</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#30828;&#20214;&#26029;&#35328;&#29983;&#25104;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
LLM-assisted Generation of Hardware Assertions. (arXiv:2306.14027v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;LLMs&#26469;&#29983;&#25104;&#30828;&#20214;&#30340;&#23433;&#20840;&#26029;&#35328;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;SystemVerilog&#26029;&#35328;&#26469;&#26367;&#20195;&#32534;&#20889;&#20855;&#26377;&#25361;&#25112;&#30340;&#23433;&#20840;&#26029;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#36890;&#24120;&#20381;&#36182;&#20110;&#30828;&#20214;&#30340;&#23433;&#20840;&#24615;&#12290;&#30828;&#20214;&#28431;&#27934;&#23545;&#31995;&#32479;&#26377;&#20005;&#37325;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#25216;&#26415;&#25903;&#25345;&#23433;&#20840;&#39564;&#35777;&#27963;&#21160;&#12290;&#26029;&#35328;&#39564;&#35777;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#39564;&#35777;&#25216;&#26415;&#65292;&#23427;&#28041;&#21450;&#22312;&#19968;&#32452;&#26029;&#35328;&#20013;&#25429;&#25417;&#35774;&#35745;&#24847;&#22270;&#65292;&#36825;&#20123;&#26029;&#35328;&#21487;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;&#25110;&#22522;&#20110;&#27979;&#35797;&#30340;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#32534;&#20889;&#20197;&#23433;&#20840;&#20026;&#20013;&#24515;&#30340;&#26029;&#35328;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30828;&#20214;&#26029;&#35328;&#29983;&#25104;&#30340;&#20195;&#30721;&#29983;&#25104;&#25216;&#26415;&#65292;&#20854;&#20013;&#20027;&#35201;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65288;&#20363;&#22914;&#22312;&#26029;&#35328;&#25991;&#20214;&#20013;&#30475;&#21040;&#30340;&#20195;&#30721;&#27880;&#37322;&#65289;&#29983;&#25104;SystemVerilog&#26029;&#35328;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#27969;&#34892;&#30340;LLM&#65292;&#24182;&#23545;&#20854;&#22312;&#32473;&#23450;&#19981;&#21516;&#35814;&#32454;&#32423;&#21035;&#30340;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#32534;&#20889;&#26029;&#35328;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29983;&#25104;&#21508;&#31181;LLM&#36741;&#21161;&#19979;&#20135;&#29983;&#30340;&#31995;&#32479;&#26029;&#35328;&#24418;&#24335;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The security of computer systems typically relies on a hardware root of trust. As vulnerabilities in hardware can have severe implications on a system, there is a need for techniques to support security verification activities. Assertion-based verification is a popular verification technique that involves capturing design intent in a set of assertions that can be used in formal verification or testing-based checking. However, writing security-centric assertions is a challenging task. In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce SystemVerilog assertions. We focus our attention on a popular LLM and characterize its ability to write assertions out of the box, given varying levels of detail in the prompt. We design an evaluation framework that generates a variety of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312; COVID-19 &#33647;&#29289;&#20877;&#21033;&#29992;&#20013;&#21457;&#29616;&#20102;&#20855;&#26377;&#35265;&#22320;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#28508;&#22312;&#26377;&#25928;&#30340;&#33647;&#29289;&#20877;&#21033;&#29992;&#20505;&#36873;&#29289;&#12290;</title><link>http://arxiv.org/abs/2306.13995</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#21644;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340; COVID-19 &#33647;&#29289;&#20877;&#21033;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A clustering and graph deep learning-based framework for COVID-19 drug repurposing. (arXiv:2306.13995v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312; COVID-19 &#33647;&#29289;&#20877;&#21033;&#29992;&#20013;&#21457;&#29616;&#20102;&#20855;&#26377;&#35265;&#22320;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#28508;&#22312;&#26377;&#25928;&#30340;&#33647;&#29289;&#20877;&#21033;&#29992;&#20505;&#36873;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20877;&#21033;&#29992; (&#25110;&#37325;&#26032;&#23450;&#20301;) &#26159;&#23547;&#25214;&#24050;&#32463;&#33719;&#24471;&#33647;&#29289;&#30417;&#31649;&#26426;&#26500; (&#20363;&#22914;&#32654;&#22269;&#39135;&#21697;&#33647;&#21697;&#30417;&#30563;&#31649;&#29702;&#23616;&#21644;&#27835;&#30103;&#21830;&#21697;&#31649;&#29702;&#23616;) &#25209;&#20934;&#29992;&#20110;&#20854;&#20182;&#30142;&#30149;&#30340;&#33647;&#29289;&#30340;&#26032;&#27835;&#30103;&#29992;&#36884;&#30340;&#36807;&#31243;&#12290;&#36825;&#21253;&#25324;&#20998;&#26512;&#19981;&#21516;&#29983;&#29289;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#33647;&#29289;&#38774;&#28857; (&#22522;&#22240;/&#34507;&#30333;&#36136;&#21644;&#29983;&#29289;&#36890;&#36335;) &#21644;&#33647;&#29289;&#23646;&#24615;&#65292;&#20197;&#21457;&#29616;&#26032;&#30340;&#33647;&#29289;&#38774;&#28857;&#25110;&#33647;&#29289;-&#30142;&#30149;&#20851;&#31995;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31561;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#20998;&#26512;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#24322;&#36136;&#25968;&#25454;&#65292;&#24182;&#19988;&#24050;&#34987;&#29992;&#20110;&#33647;&#29289;&#20877;&#21033;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#33258;&#32534;&#30721;&#22120;&#22312;&#24322;&#36136;&#33647;&#29289;&#25968;&#25454;&#19978;&#36827;&#34892;&#22810;&#29305;&#24449;&#31867;&#22411;&#32858;&#31867;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324; 438 &#31181;&#33647;&#29289;&#65292;&#20854;&#20013; 224 &#31181;&#27491;&#22312;&#25509;&#21463; COVID-19 &#20020;&#24202;&#35797;&#39564; (&#31867;&#21035; A)&#12290;&#20854;&#20313;&#33647;&#29289;&#32463;&#36807;&#31995;&#32479;&#36807;&#28388;&#20197;&#30830;&#20445;&#20877;&#21033;&#29992;&#28508;&#22312;&#33647;&#29289;&#20505;&#36873;&#29289;&#30340;&#23433;&#20840;&#24615; (&#31867;&#21035; B)&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312; COVID-19 &#29305;&#23450;&#33647;&#29289;&#20877;&#21033;&#29992;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#33021;&#22815;&#25581;&#31034;&#26377;&#35265;&#22320;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#24182;&#30830;&#23450;&#28508;&#22312;&#26377;&#25928;&#30340;&#33647;&#29289;&#20877;&#21033;&#29992;&#20505;&#36873;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug repurposing (or repositioning) is the process of finding new therapeutic uses for drugs already approved by drug regulatory authorities (e.g., the Food and Drug Administration (FDA) and Therapeutic Goods Administration (TGA)) for other diseases. This involves analyzing the interactions between different biological entities, such as drug targets (genes/proteins and biological pathways) and drug properties, to discover novel drug-target or drug-disease relations. Artificial intelligence methods such as machine learning and deep learning have successfully analyzed complex heterogeneous data in the biomedical domain and have also been used for drug repurposing. This study presents a novel unsupervised machine learning framework that utilizes a graph-based autoencoder for multi-feature type clustering on heterogeneous drug data. The dataset consists of 438 drugs, of which 224 are under clinical trials for COVID-19 (category A). The rest are systematically filtered to ensure the safety 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#20302;&#26679;&#26412;&#37327;&#25968;&#25454;&#20998;&#31867;&#30340;&#31283;&#20581;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;&#33021;&#37327;&#36317;&#31163;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#26080;&#38656;&#35843;&#21442;&#19988;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#23436;&#32654;&#20998;&#31867;&#65292;&#24050;&#22312;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#20013;&#24471;&#21040;&#35777;&#26126;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2306.13985</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#33021;&#37327;&#36317;&#31163;&#30340;&#39640;&#32500;&#25968;&#25454;&#31283;&#20581;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Robust Classification of High-Dimensional Data using Data-Adaptive Energy Distance. (arXiv:2306.13985v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#20302;&#26679;&#26412;&#37327;&#25968;&#25454;&#20998;&#31867;&#30340;&#31283;&#20581;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;&#33021;&#37327;&#36317;&#31163;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#26080;&#38656;&#35843;&#21442;&#19988;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#23436;&#32654;&#20998;&#31867;&#65292;&#24050;&#22312;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#20013;&#24471;&#21040;&#35777;&#26126;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#39640;&#32500;&#20302;&#26679;&#26412;&#37327;&#65288;HDLSS&#65289;&#25968;&#25454;&#30340;&#20998;&#31867;&#38754;&#20020;&#25361;&#25112;&#65292;&#20363;&#22914;&#22522;&#22240;&#34920;&#36798;&#30740;&#31350;&#12289;&#30284;&#30151;&#30740;&#31350;&#21644;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#19987;&#38376;&#20026;HDLSS&#25968;&#25454;&#35774;&#35745;&#30340;&#20998;&#31867;&#22120;&#30340;&#24320;&#21457;&#21644;&#20998;&#26512;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#27809;&#26377;&#35843;&#33410;&#21442;&#25968;&#65292;&#24182;&#19988;&#26159;&#31283;&#20581;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#21463;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20219;&#20309;&#30697;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19968;&#20123;&#30456;&#24403;&#26222;&#36941;&#30340;&#26465;&#20214;&#19979;&#65292;&#23427;&#20204;&#22312;HDLSS&#28176;&#36817;&#21306;&#22495;&#20869;&#21487;&#20197;&#23454;&#29616;&#23436;&#32654;&#20998;&#31867;&#12290;&#36824;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#30340;&#25903;&#25345;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#20998;&#31867;&#25216;&#26415;&#20248;&#20110;&#20960;&#31181;&#24191;&#27867;&#35748;&#21487;&#30340;&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification of high-dimensional low sample size (HDLSS) data poses a challenge in a variety of real-world situations, such as gene expression studies, cancer research, and medical imaging. This article presents the development and analysis of some classifiers that are specifically designed for HDLSS data. These classifiers are free of tuning parameters and are robust, in the sense that they are devoid of any moment conditions of the underlying data distributions. It is shown that they yield perfect classification in the HDLSS asymptotic regime, under some fairly general conditions. The comparative performance of the proposed classifiers is also investigated. Our theoretical results are supported by extensive simulation studies and real data analysis, which demonstrate promising advantages of the proposed classification techniques over several widely recognized methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;mTLDR&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#36229;&#22797;&#31354;&#38388;&#19978;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#21495;&#36827;&#34892;&#26497;&#31471;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20219;&#21153;&#12290;mTLDRgen&#27169;&#22411;&#25104;&#21151;&#23454;&#29616;&#20102;TL;DR&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.13968</link><description>&lt;p&gt;
&#21033;&#29992;&#36229;&#22797;&#31354;&#38388;&#19978;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#21495;&#36827;&#34892;&#26497;&#31471;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents. (arXiv:2306.13968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;mTLDR&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#36229;&#22797;&#31354;&#38388;&#19978;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#21495;&#36827;&#34892;&#26497;&#31471;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20219;&#21153;&#12290;mTLDRgen&#27169;&#22411;&#25104;&#21151;&#23454;&#29616;&#20102;TL;DR&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24050;&#26377;&#27880;&#37322;&#25688;&#35201;&#21644;&#20016;&#23500;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#65292;&#31185;&#23398;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#65288;&#20363;&#22914;&#35270;&#39057;&#21644;&#38899;&#39057;&#65289;&#30340;&#21033;&#29992;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#25506;&#32034;&#12290;&#30446;&#21069;&#65292;&#31185;&#23398;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#24448;&#24448;&#37319;&#29992;&#36739;&#38271;&#30340;&#30446;&#26631;&#25688;&#35201;&#65292;&#22914;&#25688;&#35201;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#26469;&#22788;&#29702;&#26497;&#31471;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#65288;&#21363;TL;DR&#29983;&#25104;&#65289;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;mTLDR&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#21019;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#65292;&#20197;&#21450;&#20316;&#32773;&#25776;&#20889;&#30340;&#25688;&#35201;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#25688;&#35201;&#12290;mTLDR&#25968;&#25454;&#38598;&#25628;&#38598;&#20102;&#26469;&#33258;&#19981;&#21516;&#23398;&#26415;&#20250;&#35758;&#35760;&#24405;&#24635;&#20849;4182&#20010;&#23454;&#20363;&#65292;&#22914;ICLR&#12289;ACL&#21644;CVPR&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;mTLDRgen&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36229;&#22797;&#26434;&#31354;&#38388;&#19978;&#26497;&#31471;&#25277;&#35937;&#25688;&#35201;&#30340;&#32534;&#30721;&#35299;&#30721;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;mTLDR&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#24182;&#25253;&#21578;&#20102;&#39318;&#20010;&#25104;&#21151;&#23454;&#26045;TL;DR&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The realm of scientific text summarization has experienced remarkable progress due to the availability of annotated brief summaries and ample data. However, the utilization of multiple input modalities, such as videos and audio, has yet to be thoroughly explored. At present, scientific multimodal-input-based text summarization systems tend to employ longer target summaries like abstracts, leading to an underwhelming performance in the task of text summarization.  In this paper, we deal with a novel task of extreme abstractive text summarization (aka TL;DR generation) by leveraging multiple input modalities. To this end, we introduce mTLDR, a first-of-its-kind dataset for the aforementioned task, comprising videos, audio, and text, along with both author-composed summaries and expert-annotated summaries. The mTLDR dataset accompanies a total of 4,182 instances collected from various academic conference proceedings, such as ICLR, ACL, and CVPR. Subsequently, we present mTLDRgen, an encod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#27880;&#20837;&#23545;&#25239;&#26679;&#26412;&#65292;&#21487;&#20197;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#25552;&#39640;&#25915;&#20987;&#31934;&#24230;&#21644;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13965</link><description>&lt;p&gt;
&#29992;&#23545;&#25239;&#26679;&#26412;&#25552;&#21319;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Model Inversion Attacks with Adversarial Examples. (arXiv:2306.13965v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#27880;&#20837;&#23545;&#25239;&#26679;&#26412;&#65292;&#21487;&#20197;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#25552;&#39640;&#25915;&#20987;&#31934;&#24230;&#21644;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#25351;&#24674;&#22797;&#30446;&#26631;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#23588;&#20854;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#20250;&#38754;&#20020;&#20302;&#25915;&#20987;&#31934;&#24230;&#30340;&#38382;&#39064;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#36825;&#20123;&#24674;&#22797;&#25968;&#25454;&#30340;&#20998;&#31867;&#31934;&#24230;&#36739;&#20302;&#12290;&#23545;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110; GAN &#30340;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#25915;&#20987;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110; GAN &#30340;&#25915;&#20987;&#21482;&#37325;&#26500;&#27599;&#31867;&#30340;&#31867;&#20195;&#34920;&#24615;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#22522;&#20110;&#23398;&#20064;&#30340;&#25915;&#20987;&#21487;&#20197;&#37325;&#26500;&#19981;&#21516;&#31867;&#21035;&#20013;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#35757;&#32451;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#25915;&#20987;&#31934;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#26469;&#35268;&#33539;&#21270;&#25915;&#20987;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#25915;&#20987;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#23545;&#25239;&#24615;&#25915;&#20987;&#26356;&#21152;&#40065;&#26834;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#31934;&#24230;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model inversion attacks involve reconstructing the training data of a target model, which raises serious privacy concerns for machine learning models. However, these attacks, especially learning-based methods, are likely to suffer from low attack accuracy, i.e., low classification accuracy of these reconstructed data by machine learning classifiers. Recent studies showed an alternative strategy of model inversion attacks, GAN-based optimization, can improve the attack accuracy effectively. However, these series of GAN-based attacks reconstruct only class-representative training data for a class, whereas learning-based attacks can reconstruct diverse data for different training data in each class. Hence, in this paper, we propose a new training paradigm for a learning-based model inversion attack that can achieve higher attack accuracy in a black-box setting. First, we regularize the training process of the attack model with an added semantic loss function and, second, we inject adversa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20914;&#31361;&#35299;&#20915;&#26694;&#26550;&#65292;&#31216;&#20026;C-GMCR&#65292;&#23427;&#23558;&#33539;&#30068;&#35770;&#25972;&#21512;&#21040;&#20256;&#32479;&#30340;&#22270;&#27169;&#22411;&#20013;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#25277;&#35937;&#21644;&#36890;&#29992;&#30340;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#24212;&#29992;&#21040;&#22234;&#24466;&#22256;&#22659;&#21644;&#20854;&#20182;&#26696;&#20363;&#20013;&#65292;&#21457;&#29616;&#20998;&#31867;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#21487;&#33021;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#20914;&#31361;&#35299;&#20915;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.13961</link><description>&lt;p&gt;
&#21033;&#29992;&#33539;&#30068;&#35770;&#25972;&#21512;&#21040;&#20914;&#31361;&#35299;&#20915;&#20013;&#30340;&#22270;&#27169;&#22411;&#30340;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Categorical Approach to Conflict Resolution: Integrating Category Theory into the Graph Model for Conflict Resolution. (arXiv:2306.13961v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20914;&#31361;&#35299;&#20915;&#26694;&#26550;&#65292;&#31216;&#20026;C-GMCR&#65292;&#23427;&#23558;&#33539;&#30068;&#35770;&#25972;&#21512;&#21040;&#20256;&#32479;&#30340;&#22270;&#27169;&#22411;&#20013;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#25277;&#35937;&#21644;&#36890;&#29992;&#30340;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#24212;&#29992;&#21040;&#22234;&#24466;&#22256;&#22659;&#21644;&#20854;&#20182;&#26696;&#20363;&#20013;&#65292;&#21457;&#29616;&#20998;&#31867;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#21487;&#33021;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#20914;&#31361;&#35299;&#20915;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20914;&#31361;&#35299;&#20915;&#20013;&#30340;&#33539;&#30068;&#22270;&#27169;&#22411;&#65288;C-GMCR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#33539;&#30068;&#35770;&#25972;&#21512;&#21040;&#20256;&#32479;&#30340;&#22270;&#27169;&#22411;&#20013;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;C-GMCR&#26694;&#26550;&#25552;&#20379;&#20102;&#26356;&#25277;&#35937;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#26469;&#24314;&#27169;&#21644;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#21457;&#29616;&#26356;&#28145;&#23618;&#27425;&#30340;&#35265;&#35299;&#21644;&#32852;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;C-GMCR&#26694;&#26550;&#30340;&#22522;&#26412;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#21040;&#33879;&#21517;&#30340;&#22234;&#24466;&#22256;&#22659;&#21644;&#20854;&#20182;&#20195;&#34920;&#24615;&#26696;&#20363;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#31867;&#26041;&#27861;&#20026;&#31283;&#23450;&#27010;&#24565;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#26377;&#21487;&#33021;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#20914;&#31361;&#35299;&#20915;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Categorical Graph Model for Conflict Resolution (C-GMCR), a novel framework that integrates category theory into the traditional Graph Model for Conflict Resolution (GMCR). The C-GMCR framework provides a more abstract and general way to model and analyze conflict resolution, enabling researchers to uncover deeper insights and connections. We present the basic concepts, methods, and application of the C-GMCR framework to the well-known Prisoner's Dilemma and other representative cases. The findings suggest that the categorical approach offers new perspectives on stability concepts and can potentially lead to the development of more effective conflict resolution strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#36830;&#32493;SO(3)&#26680;&#21644;&#31354;&#38388;&#26680;&#20197;&#23454;&#29616;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.13960</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis. (arXiv:2306.13960v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#36830;&#32493;SO(3)&#26680;&#21644;&#31354;&#38388;&#26680;&#20197;&#23454;&#29616;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#27491;&#21017;&#32452;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(G-CNN)&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#25552;&#39640;&#23545;&#19981;&#21516;&#20960;&#20309;&#23545;&#31216;&#24615;&#30340;&#31561;&#21464;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;SE(3)&#38382;&#39064;&#65292;&#21363;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#22312;&#20307;&#31215;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#12290;&#20307;&#31215;&#22270;&#20687;&#25968;&#25454;&#22312;&#35768;&#22810;&#21307;&#30103;&#35774;&#32622;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#21463;&#21487;&#20998;&#31163;&#32452;&#21367;&#31215;&#30340;&#26368;&#26032;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;SE(3)&#32676;&#21367;&#31215;&#26680;&#65292;&#23558;&#20854;&#20998;&#35299;&#20026;&#36830;&#32493;&#30340;SO(3)&#65288;&#26059;&#36716;&#65289;&#26680;&#21644;&#31354;&#38388;&#26680;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#26679;&#22343;&#21248;&#30340;SO(3)&#32593;&#26684;&#26469;&#36817;&#20284;&#36830;&#32493;&#35774;&#23450;&#19979;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#30340;&#36830;&#32493;SO(3)&#26680;&#26159;&#36890;&#36807;&#31867;&#20284;&#22343;&#21248;&#32593;&#26684;&#30340;RBF&#25554;&#20540;&#21442;&#25968;&#21270;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;SE(3)&#31561;&#21464;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110;CNN&#21644;&#24120;&#35268;&#31163;&#25955;G-CNN&#65292;&#24182;&#26174;&#31034;&#20986;&#26174;&#30528;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22122;&#22768;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#36798;&#21040;16.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular group convolutional neural networks (G-CNNs) have been shown to increase model performance and improve equivariance to different geometrical symmetries. This work addresses the problem of SE(3), i.e., roto-translation equivariance, on volumetric data. Volumetric image data is prevalent in many medical settings. Motivated by the recent work on separable group convolutions, we devise a SE(3) group convolution kernel separated into a continuous SO(3) (rotation) kernel and a spatial kernel. We approximate equivariance to the continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel is parameterized via RBF interpolation on similarly uniform grids. We demonstrate the advantages of our approach in volumetric medical image analysis. Our SE(3) equivariant models consistently outperform CNNs and regular discrete G-CNNs on challenging medical classification tasks and show significantly improved generalization capabilities. Our approach achieves up to a 16.5% gain in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35782;&#21035;&#35828;&#35805;&#32773;&#24773;&#24863;&#32763;&#36716;&#32972;&#21518;&#30340;&#25512;&#21160;&#32773;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#26550;&#26500;c&#36827;&#34892;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.13959</link><description>&lt;p&gt;
&#22810;&#26041;&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#21453;&#36716;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Emotion Flip Reasoning in Multiparty Conversations. (arXiv:2306.13959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35782;&#21035;&#35828;&#35805;&#32773;&#24773;&#24863;&#32763;&#36716;&#32972;&#21518;&#30340;&#25512;&#21160;&#32773;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#26550;&#26500;c&#36827;&#34892;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#65292;&#35828;&#35805;&#32773;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#23427;&#20204;&#30340;&#21160;&#24577;&#22312;&#29702;&#35299;&#24773;&#24863;&#35805;&#35821;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#26816;&#27979;&#24773;&#24863;&#24182;&#19981;&#36275;&#20197;&#23436;&#20840;&#29702;&#35299;&#20250;&#35805;&#36807;&#31243;&#20013;&#21457;&#29983;&#30340;&#35828;&#35805;&#32773;&#29305;&#23450;&#24773;&#24863;&#21464;&#21270;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#29702;&#35299;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#21160;&#24577;&#65292;&#26377;&#24517;&#35201;&#30830;&#23450;&#23548;&#33268;&#35828;&#35805;&#32773;&#24773;&#24863;&#32763;&#36716;&#30340;&#21407;&#22240;&#25110;&#25512;&#21160;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21517;&#20026;Instigator based Emotion Flip Reasoning (EFR) &#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#20250;&#35805;&#20013;&#35828;&#35805;&#32773;&#24773;&#24863;&#32763;&#36716;&#32972;&#21518;&#30340;&#25512;&#21160;&#32773;&#12290;&#20363;&#22914;&#65292;&#20174;&#24555;&#20048;&#21040;&#24868;&#24594;&#30340;&#24773;&#24863;&#32763;&#36716;&#21487;&#33021;&#26159;&#30001;&#23041;&#32961;&#36825;&#26679;&#30340;&#25512;&#21160;&#32773;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#31526;&#21512;&#24773;&#24863;&#24515;&#29702;&#23398;&#26631;&#20934;&#30340;EFR&#25512;&#21160;&#32773;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;MELD-I&#12290;&#20026;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#26550;&#26500;c&#12290;
&lt;/p&gt;
&lt;p&gt;
In a conversational dialogue, speakers may have different emotional states and their dynamics play an important role in understanding dialogue's emotional discourse. However, simply detecting emotions is not sufficient to entirely comprehend the speaker-specific changes in emotion that occur during a conversation. To understand the emotional dynamics of speakers in an efficient manner, it is imperative to identify the rationale or instigator behind any changes or flips in emotion expressed by the speaker. In this paper, we explore the task called Instigator based Emotion Flip Reasoning (EFR), which aims to identify the instigator behind a speaker's emotion flip within a conversation. For example, an emotion flip from joy to anger could be caused by an instigator like threat. To facilitate this task, we present MELD-I, a dataset that includes ground-truth EFR instigator labels, which are in line with emotional psychology. To evaluate the dataset, we propose a novel neural architecture c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;&#32473;&#23450;&#36335;&#24452;&#35268;&#21010;&#20013;&#29305;&#23450;&#26102;&#38388;&#28857;&#19978;&#30340;&#21333;&#20010;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;(LTL)&#32422;&#26463;&#30340;&#30456;&#20851;&#24615;&#21644;&#29366;&#24577;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#31163;&#25955;&#26102;&#38388;&#12289;&#31163;&#25955;&#31354;&#38388;&#20013;&#25191;&#34892;&#26377;&#38480;&#35745;&#21010;&#30340;&#20195;&#29702;&#20219;&#21153;&#20013;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26102;&#38388;&#28857;&#35299;&#37322;&#21644;&#35268;&#21017;&#21442;&#25968;&#29366;&#24577;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13956</link><description>&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26102;&#38388;&#28857;&#35299;&#37322;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Pointwise-in-Time Explanation for Linear Temporal Logic Rules. (arXiv:2306.13956v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;&#32473;&#23450;&#36335;&#24452;&#35268;&#21010;&#20013;&#29305;&#23450;&#26102;&#38388;&#28857;&#19978;&#30340;&#21333;&#20010;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;(LTL)&#32422;&#26463;&#30340;&#30456;&#20851;&#24615;&#21644;&#29366;&#24577;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#31163;&#25955;&#26102;&#38388;&#12289;&#31163;&#25955;&#31354;&#38388;&#20013;&#25191;&#34892;&#26377;&#38480;&#35745;&#21010;&#30340;&#20195;&#29702;&#20219;&#21153;&#20013;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26102;&#38388;&#28857;&#35299;&#37322;&#21644;&#35268;&#21017;&#21442;&#25968;&#29366;&#24577;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#32473;&#23450;&#36335;&#24452;&#35268;&#21010;&#20013;&#29305;&#23450;&#26102;&#38388;&#28857;&#19978;&#30340;&#21333;&#20010;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;(LTL)&#32422;&#26463;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#20010;&#20219;&#21153;&#34987;&#25105;&#20204;&#31216;&#20026;&#8220;&#26102;&#38388;&#28857;&#35299;&#37322;&#8221;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#29366;&#24577;&#35780;&#20272;&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22312;Kripke&#32467;&#26500;&#21487;&#34920;&#36798;&#30340;&#31163;&#25955;&#26102;&#38388;&#12289;&#31163;&#25955;&#31354;&#38388;&#20013;&#25191;&#34892;&#26377;&#38480;&#35745;&#21010;&#30340;&#20195;&#29702;&#12290;&#22312;&#32473;&#23450;&#30340;&#32467;&#26500;&#19978;&#21644;&#24050;&#30693;&#32422;&#26463;&#20195;&#29702;&#30340;&#19968;&#32452;LTL&#35268;&#21017;&#30340;&#35745;&#21010;&#20013;&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#21709;&#24212;&#22320;&#29983;&#25104;&#35299;&#37322;&#12290;&#23545;&#20110;&#25152;&#36873;&#30340;&#26597;&#35810;&#26102;&#38388;&#65292;&#35299;&#37322;&#35782;&#21035;&#21738;&#20123;&#35268;&#21017;&#26159;&#27963;&#21160;&#30340;&#65292;&#21738;&#20123;&#35268;&#21017;&#21018;&#21018;&#34987;&#28385;&#36275;&#65292;&#21738;&#20123;&#35268;&#21017;&#26159;&#19981;&#27963;&#21160;&#30340;&#65292;&#20854;&#20013;&#26694;&#26550;&#29366;&#24577;&#26631;&#20934;&#26159;&#27491;&#24335;&#21644;&#30452;&#35266;&#22320;&#23450;&#20041;&#30340;&#12290;&#35299;&#37322;&#36824;&#21487;&#20197;&#21253;&#25324;&#21333;&#20010;&#35268;&#21017;&#21442;&#25968;&#30340;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#36827;&#19968;&#27493;&#30340;&#27934;&#23519;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#36825;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#20854;&#23454;&#29616;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a framework to assess the relevance of individual linear temporal logic (LTL) constraints at specific times in a given path plan, a task we refer to as "pointwise-in-time" explanation. We develop this framework, featuring a status assessment algorithm, for agents which execute finite plans in a discrete-time, discrete-space setting expressible via a Kripke structure. Given a plan on this structure and a set of LTL rules which are known to constrain the agent, the algorithm responds to two types of user queries to produce explanation. For the selected query time, explanations identify which rules are active, which have just been satisfied, and which are inactive, where the framework status criteria are formally and intuitively defined. Explanations may also include the status of individual rule arguments to provide further insight. In this paper, we systematically present this novel framework and provide an example of its implementation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#22810;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#33539;&#22260;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#24314;&#27169;&#21644;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#30740;&#31350;&#65292;&#21516;&#26102;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26410;&#26469;&#24320;&#21457;&#26032;&#22411;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.13948</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#65306;&#20171;&#32461;&#26131;&#20110;&#20351;&#29992;&#30340;PurpleAirSF&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset. (arXiv:2306.13948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#22810;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#33539;&#22260;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#24314;&#27169;&#21644;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#30740;&#31350;&#65292;&#21516;&#26102;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26410;&#26469;&#24320;&#21457;&#26032;&#22411;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#27493;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#32473;&#30740;&#31350;&#20154;&#21592;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#27169;&#22411;&#39564;&#35777;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20840;&#38754;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;PurpleAir&#32593;&#32476;&#20013;&#25910;&#38598;&#32780;&#26469;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#21508;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26032;&#22411;&#39044;&#27979;&#27169;&#22411;&#12289;&#30740;&#31350;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#20197;&#21450;&#35843;&#26597;&#20854;&#23545;&#20581;&#24247;&#21644;&#29615;&#22659;&#30340;&#24433;&#21709;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26500;&#24314;PurpleAirSF&#25152;&#37319;&#29992;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#32463;&#20856;&#21644;&#29616;&#20195;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#21046;&#23450;&#24314;&#31435;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air quality forecasting has garnered significant attention recently, with data-driven models taking center stage due to advancements in machine learning and deep learning models. However, researchers face challenges with complex data acquisition and the lack of open-sourced datasets, hindering efficient model validation. This paper introduces PurpleAirSF, a comprehensive and easily accessible dataset collected from the PurpleAir network. With its high temporal resolution, various air quality measures, and diverse geographical coverage, this dataset serves as a useful tool for researchers aiming to develop novel forecasting models, study air pollution patterns, and investigate their impacts on health and the environment. We present a detailed account of the data collection and processing methods employed to build PurpleAirSF. Furthermore, we conduct preliminary experiments using both classic and modern spatio-temporal forecasting models, thereby establishing a benchmark for future air q
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#20351;&#29992;Transformer&#31561;&#24207;&#21015;&#27169;&#22411;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#26368;&#36817;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25353;&#29031;&#22788;&#29702;&#26679;&#26412;&#25928;&#29575;&#12289;&#20449;&#29992;&#20998;&#37197;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#26041;&#24335;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.13945</link><description>&lt;p&gt;
&#22823;&#22411;&#24207;&#21015;&#27169;&#22411;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Sequence Models for Sequential Decision-Making: A Survey. (arXiv:2306.13945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#20351;&#29992;Transformer&#31561;&#24207;&#21015;&#27169;&#22411;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#26368;&#36817;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25353;&#29031;&#22788;&#29702;&#26679;&#26412;&#25928;&#29575;&#12289;&#20449;&#29992;&#20998;&#37197;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#26041;&#24335;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32467;&#26500;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#39044;&#27979;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#36890;&#29992;&#24207;&#21015;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20363;&#22914;GPT-3&#21644;Swin Transformer&#12290;&#34429;&#28982;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#38382;&#39064;&#65292;&#20294;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#35810;&#38382;&#23427;&#20204;&#26159;&#21542;&#36866;&#29992;&#20110;&#36890;&#24120;&#23384;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#20449;&#29992;&#20998;&#37197;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#38382;&#39064;&#30340;&#39034;&#24207;&#20915;&#31574;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;Transformer&#65292;&#21560;&#24341;&#20102;RL&#31038;&#21306;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20135;&#29983;&#20102;&#35768;&#22810;&#20855;&#26377;&#26174;&#30528;&#26377;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#36890;&#36807;&#35752;&#35770;&#39034;&#24207;&#20915;&#31574;&#21644;&#24207;&#21015;&#24314;&#27169;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#22788;&#29702;&#21069;&#36848;&#38382;&#39064;&#30340;&#26041;&#24335;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#65292;&#35299;&#20915;&#20351;&#29992;&#24207;&#21015;&#27169;&#22411;&#65288;&#20363;&#22914;Transformer&#65289;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architectures have facilitated the development of large-scale and general-purpose sequence models for prediction tasks in natural language processing and computer vision, e.g., GPT-3 and Swin Transformer. Although originally designed for prediction problems, it is natural to inquire about their suitability for sequential decision-making and reinforcement learning problems, which are typically beset by long-standing issues involving sample efficiency, credit assignment, and partial observability. In recent years, sequence models, especially the Transformer, have attracted increasing interest in the RL communities, spawning numerous approaches with notable effectiveness and generalizability. This survey presents a comprehensive overview of recent works aimed at solving sequential decision-making tasks with sequence models such as the Transformer, by discussing the connection between sequential decision-making and sequence modeling, and categorizing them based on the way they 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27515;&#23616;&#30340;&#36793;&#30028;&#26469;&#36776;&#21035;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#29366;&#24577;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#21516;&#26102;&#20943;&#23569;&#23545;&#25506;&#32034;&#30340;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#20998;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35757;&#32451;&#20102;&#20004;&#20010;&#31574;&#30053;&#65306;&#19968;&#20010;&#20219;&#21153;&#31574;&#30053;&#65292;&#19987;&#27880;&#20110;&#20219;&#21153;&#34920;&#29616;&#65292;&#20197;&#21450;&#19968;&#20010;&#24674;&#22797;&#31574;&#30053;&#65292;&#26368;&#22823;&#21270;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13944</link><description>&lt;p&gt;
&#20855;&#26377;&#36991;&#24320;&#27515;&#23616;&#21644;&#24674;&#22797;&#33021;&#21147;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery. (arXiv:2306.13944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27515;&#23616;&#30340;&#36793;&#30028;&#26469;&#36776;&#21035;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#29366;&#24577;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#21516;&#26102;&#20943;&#23569;&#23545;&#25506;&#32034;&#30340;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#20998;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35757;&#32451;&#20102;&#20004;&#20010;&#31574;&#30053;&#65306;&#19968;&#20010;&#20219;&#21153;&#31574;&#30053;&#65292;&#19987;&#27880;&#20110;&#20219;&#21153;&#34920;&#29616;&#65292;&#20197;&#21450;&#19968;&#20010;&#24674;&#22797;&#31574;&#30053;&#65292;&#26368;&#22823;&#21270;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#26159;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#29616;&#23454;&#29615;&#22659;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#30830;&#20445;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#20043;&#21518;&#30340;&#23433;&#20840;&#24615;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#36807;&#20110;&#20445;&#23432;&#30340;&#31574;&#30053;&#20197;&#36991;&#20813;&#19981;&#23433;&#20840;&#30340;&#24773;&#20917;&#12290;&#20294;&#26159;&#65292;&#36807;&#20110;&#20445;&#23432;&#30340;&#31574;&#30053;&#20005;&#37325;&#38459;&#30861;&#20102;&#25506;&#32034;&#65292;&#20351;&#31639;&#27861;&#30340;&#22238;&#25253;&#22823;&#22823;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#26500;&#24314;&#19968;&#20010;&#36793;&#30028;&#65292;&#21306;&#20998;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#26500;&#24314;&#30340;&#36793;&#30028;&#31561;&#20215;&#20110;&#21306;&#20998;&#27515;&#23616;&#29366;&#24577;&#65292;&#34920;&#26126;&#23433;&#20840;&#25506;&#32034;&#30340;&#26368;&#22823;&#31243;&#24230;&#65292;&#22240;&#27492;&#22312;&#25506;&#32034;&#26041;&#38754;&#30340;&#38480;&#21046;&#26368;&#23567;&#12290;&#31867;&#20284;&#20110;&#24674;&#22797;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#20998;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#20004;&#20010;&#31574;&#30053;&#65292;(1) &#21482;&#32771;&#34385;&#25913;&#21892;&#20219;&#21153;&#34920;&#29616;&#30340;&#20219;&#21153;&#31574;&#30053;&#65292;&#20197;&#21450; (2) &#26368;&#22823;&#21270;&#23433;&#20840;&#24615;&#30340;&#24674;&#22797;&#31574;&#30053;&#12290;&#24674;&#22797;&#31574;&#30053;&#21644;&#30456;&#24212;&#30340;&#23433;&#20840;&#24615;&#25209;&#21028;&#23478;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#23433;&#20840;&#25209;&#21028;&#23478;&#20250;&#21306;&#20998;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#30340;&#29366;&#24577;&#65292;&#32780;&#24674;&#22797;&#31574;&#30053;&#20250;&#37319;&#21462;&#25514;&#26045;&#20197;&#20174;&#19981;&#23433;&#20840;&#29366;&#24577;&#20013;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is one of the main challenges in applying reinforcement learning to realistic environmental tasks. To ensure safety during and after training process, existing methods tend to adopt overly conservative policy to avoid unsafe situations. However, overly conservative policy severely hinders the exploration, and makes the algorithms substantially less rewarding. In this paper, we propose a method to construct a boundary that discriminates safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pretrained on an offline dataset, in which the safety c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#65292;&#29992;&#20110;&#33719;&#21462;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20154;&#31867;&#30340;&#20171;&#20837;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#26469;&#35780;&#20272;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#26631;&#20934;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#21487;&#27604;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13935</link><description>&lt;p&gt;
&#33391;&#22909;&#30340;&#35299;&#37322;&#32773;&#26263;&#22320;&#37324;&#26159;&#20154;&#31867;-&#20027;&#21160;&#23398;&#20064;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Good Explainers Secretly Human-in-the-Loop Active Learners?. (arXiv:2306.13935v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#65292;&#29992;&#20110;&#33719;&#21462;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20154;&#31867;&#30340;&#20171;&#20837;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#26469;&#35780;&#20272;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#26631;&#20934;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#36817;&#24180;&#26469;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23427;&#22312;&#30740;&#31350;&#27169;&#22411;&#39044;&#27979;&#20197;&#25910;&#38598;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#30456;&#24403;&#20110;&#20027;&#21160;&#23398;&#20064;&#65292;&#20854;&#20013;&#26597;&#35810;&#31574;&#30053;&#28041;&#21450;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20154;&#31867;&#35282;&#33394;&#30340;&#25968;&#23398;&#36817;&#20284;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#24037;&#20316;&#27969;&#30340;&#36890;&#29992;&#24418;&#24335;&#21270;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#27604;&#36739;&#27492;&#29992;&#27861;&#19982;&#26631;&#20934;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#25193;&#23637;&#24037;&#20316;&#27969;&#12290;&#19968;&#20010;&#39069;&#22806;&#30340;&#22909;&#22788;&#26159;&#65292;&#23427;&#20204;&#30340;&#25928;&#29992;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#26469;&#35780;&#20272;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#26114;&#36149;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) techniques have become popular for multiple use-cases in the past few years. Here we consider its use in studying model predictions to gather additional training data. We argue that this is equivalent to Active Learning, where the query strategy involves a human-in-the-loop. We provide a mathematical approximation for the role of the human, and present a general formalization of the end-to-end workflow. This enables us to rigorously compare this use with standard Active Learning algorithms, while allowing for extensions to the workflow. An added benefit is that their utility can be assessed via simulation instead of conducting expensive user-studies. We also present some initial promising results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20256;&#32479;&#39044;&#27979;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#35777;&#26126;Deep AR&#22312;&#32929;&#31080;&#25351;&#25968;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13931</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#32929;&#31080;&#25351;&#25968;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Predicting Stock Index Using Deep Learning Models. (arXiv:2306.13931v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20256;&#32479;&#39044;&#27979;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#35777;&#26126;Deep AR&#22312;&#32929;&#31080;&#25351;&#25968;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#29616;&#24050;&#23581;&#35797;&#20102;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#25216;&#26415;&#20998;&#26512;&#12289;&#31639;&#27861;&#32479;&#35745;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#32435;&#20837;&#21040;&#39044;&#27979;&#22330;&#26223;&#20013;&#65292;&#22914;LSTM&#21644;&#24120;&#35268;RNN&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#22914;ARIMA&#12289;SARIMA&#21644;SARIMAX&#65292;&#20197;&#21450;&#20351;&#29992;RNN&#26500;&#24314;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#22914;DF-RNN&#12289;DSSM&#21644;Deep AR&#12290;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#26631;&#20934;NIFTY-50&#25968;&#25454;&#38598;&#20351;&#29992;MSE&#12289;RMSE&#12289;MAPE&#12289;POCID&#21644;Theil's U&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Deep AR&#22312;&#25152;&#26377;&#20854;&#20182;&#24120;&#35268;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26041;&#27861;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;MAPE&#20026;0.01&#65292;RMSE&#20026;189&#12290;&#27492;&#22806;&#65292;&#24403;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#37327;&#26102;&#65292;Deep AR&#21644;GRU&#30340;&#24615;&#33021;&#19981;&#20250;&#38477;&#20302;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting has seen many methods attempted over the past few decades, including traditional technical analysis, algorithmic statistical models, and more recent machine learning and artificial intelligence approaches. Recently, neural networks have been incorporated into the forecasting scenario, such as the LSTM and conventional RNN approaches, which utilize short-term and long-term dependencies. This study evaluates traditional forecasting methods, such as ARIMA, SARIMA, and SARIMAX, and newer neural network approaches, such as DF-RNN, DSSM, and Deep AR, built using RNNs. The standard NIFTY-50 dataset from Kaggle is used to assess these models using metrics such as MSE, RMSE, MAPE, POCID, and Theil's U. Results show that Deep AR outperformed all other conventional deep learning and traditional approaches, with the lowest MAPE of 0.01 and RMSE of 189. Additionally, the performance of Deep AR and GRU did not degrade when the amount of training data was reduced, suggesting t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GAN&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#25552;&#39640;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;GAN&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.13929</link><description>&lt;p&gt;
&#35780;&#20272;GAN&#29983;&#25104;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29992;&#20110;&#31867;&#21035;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Utility of GAN Generated Synthetic Tabular Data for Class Balancing and Low Resource Settings. (arXiv:2306.13929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GAN&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#25552;&#39640;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;GAN&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;SMOTE&#12289;ADASYN&#21644;GAN&#25216;&#26415;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20197;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#24182;&#25913;&#21892;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#35813;&#30740;&#31350;&#37319;&#29992;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#31867;&#21035;&#24179;&#34913;&#23454;&#39564;&#65292;&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#36827;&#34892;&#20302;&#36164;&#28304;&#35774;&#32622;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25152;&#26377;&#20998;&#31867;&#27169;&#22411;&#30340;&#20027;&#35201;&#35780;&#20272;&#25351;&#26631;&#26159;&#21484;&#22238;&#29575;&#12290;&#31867;&#21035;&#24179;&#34913;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;GAN&#24179;&#34913;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;GLM&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#21516;&#26679;&#22312;&#20302;&#36164;&#28304;&#23454;&#39564;&#20013;&#65292;&#35757;&#32451;&#22312;GAN&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#23637;&#29616;&#20986;&#27604;&#21407;&#22987;&#25968;&#25454;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;&#36825;&#20123;&#21457;&#29616;&#23637;&#31034;&#20102;GAN&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#25361;&#25112;&#21644;&#25913;&#21892;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present study aimed to address the issue of imbalanced data in classification tasks and evaluated the suitability of SMOTE, ADASYN, and GAN techniques in generating synthetic data to address the class imbalance and improve the performance of classification models in low-resource settings. The study employed the Generalised Linear Model (GLM) algorithm for class balancing experiments and the Random Forest (RF) algorithm for low-resource setting experiments to assess model performance under varying training data. The recall metric was the primary evaluation metric for all classification models. The results of the class balancing experiments showed that the GLM model trained on GAN-balanced data achieved the highest recall value. Similarly, in low-resource experiments, models trained on data enhanced with GAN-synthesized data exhibited better recall values than original data. These findings demonstrate the potential of GAN-generated synthetic data for addressing the challenge of imbal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#31574;&#30053;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#20013;&#22823;&#37327;&#20887;&#20313;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13923</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#20013;&#30340;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;
&lt;/p&gt;
&lt;p&gt;
Active Data Acquisition in Autonomous Driving Simulation. (arXiv:2306.13923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#31574;&#30053;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#20013;&#22823;&#37327;&#20887;&#20313;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#39640;&#24230;&#20381;&#36182;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#20887;&#20313;&#20449;&#24687;&#65292;&#32780;&#19988;&#25910;&#38598;&#21644;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#21644;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#31574;&#30053;&#30340;&#27010;&#24565;&#12290;&#23545;&#20110;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#22686;&#21152;&#37319;&#38598;&#23494;&#24230;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#26368;&#32456;&#23454;&#29616;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39564;&#35777;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#28436;&#31034;&#35813;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#29616;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/Th1nkMore&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving algorithms rely heavily on learning-based models, which require large datasets for training. However, there is often a large amount of redundant information in these datasets, while collecting and processing these datasets can be time-consuming and expensive. To address this issue, this paper proposes the concept of an active data-collecting strategy. For high-quality data, increasing the collection density can improve the overall quality of the dataset, ultimately achieving similar or even better results than the original dataset with lower labeling costs and smaller dataset sizes. In this paper, we design experiments to verify the quality of the collected dataset and to demonstrate this strategy can significantly reduce labeling costs and dataset size while improving the overall quality of the dataset, leading to better performance of autonomous driving systems. The source code implementing the proposed approach is publicly available on https://github.com/Th1nkMore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#36712;&#36857;&#36827;&#34892;&#20998;&#26512;&#21644;&#29983;&#25104;&#21512;&#25104;&#35821;&#20041;&#36712;&#36857;&#25968;&#25454;&#65292;&#20026;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#24341;&#25806;&#21644;&#21830;&#19994;&#31574;&#30053;&#31561;&#19968;&#31995;&#21015;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.13905</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#36712;&#36857;&#20998;&#26512;&#30340;&#26102;&#31354;&#21465;&#20107;&#65311;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis. (arXiv:2306.13905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#36712;&#36857;&#36827;&#34892;&#20998;&#26512;&#21644;&#29983;&#25104;&#21512;&#25104;&#35821;&#20041;&#36712;&#36857;&#25968;&#25454;&#65292;&#20026;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#24341;&#25806;&#21644;&#21830;&#19994;&#31574;&#30053;&#31561;&#19968;&#31995;&#21015;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#36712;&#36857;&#36319;&#36394;&#36827;&#34892;&#20998;&#26512;&#21644;&#29983;&#25104;&#21512;&#25104;&#35821;&#20041;&#36712;&#36857;&#25968;&#25454;&#65288;SST&#65289;&#30340;&#24895;&#26223;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#35821;&#20041;&#36712;&#36857;&#65292;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#65292;&#22686;&#24378;&#26426;&#22120;&#23545;&#21160;&#29289;&#12289;&#20154;&#31867;&#12289;&#36135;&#29289;&#31561;&#31227;&#21160;&#24773;&#20917;&#30340;&#29702;&#35299;&#65292;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#20026;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#24341;&#25806;&#21644;&#21830;&#19994;&#31574;&#30053;&#31561;&#19968;&#31995;&#21015;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we lay out a vision for analysing semantic trajectory traces and generating synthetic semantic trajectory data (SSTs) using generative language model. Leveraging the advancements in deep learning, as evident by progress in the field of natural language processing (NLP), computer vision, etc. we intend to create intelligent models that can study the semantic trajectories in various contexts, predicting future trends, increasing machine understanding of the movement of animals, humans, goods, etc. enhancing human-computer interactions, and contributing to an array of applications ranging from urban-planning to personalized recommendation engines and business strategy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#25991;&#26412;&#35821;&#35328;&#21464;&#20307;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;DeBERTa&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#20316;&#32773;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#25512;&#23548;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13899</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#38472;&#36848;&#30340;&#35821;&#35328;&#21464;&#20307;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Math Word Problem Solving by Generating Linguistic Variants of Problem Statements. (arXiv:2306.13899v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#25991;&#26412;&#35821;&#35328;&#21464;&#20307;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;DeBERTa&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#20316;&#32773;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#25512;&#23548;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#33402;&#26415;&#26159;&#26234;&#21147;&#36827;&#23637;&#30340;&#22522;&#26412;&#25903;&#26609;&#65292;&#26159;&#22521;&#20859;&#20154;&#31867;&#29420;&#21019;&#24615;&#30340;&#26680;&#24515;&#20652;&#21270;&#21058;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#21457;&#34920;&#20102;&#22823;&#37327;&#22260;&#32469;&#35299;&#20915;&#25968;&#23398;&#35821;&#35328;&#38382;&#39064;&#65288;MWP&#65289;&#30340;&#20316;&#21697;&#65292;&#36825;&#26159;&#36808;&#21521;&#36890;&#29992;AI&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#36825;&#20123;&#29616;&#26377;&#27169;&#22411;&#23481;&#26131;&#20381;&#36182;&#20110;&#32932;&#27973;&#30340;&#21551;&#21457;&#24335;&#21644;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#26469;&#25512;&#23548;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#19968;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#38382;&#39064;&#25991;&#26412;&#35821;&#35328;&#21464;&#20307;&#30340;MWP&#27714;&#35299;&#22120;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#35299;&#20915;&#27599;&#20010;&#19981;&#21516;&#21464;&#20307;&#30340;&#38382;&#39064;&#24182;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#39044;&#27979;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;DeBERTa&#65288;&#20855;&#26377;&#35299;&#30721;&#22686;&#24378;&#30340;BERT&#21644;&#20998;&#31163;&#27880;&#24847;&#21147;&#65289;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#20197;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#22686;&#24378;&#30340;&#36974;&#32617;&#35299;&#30721;&#22120;&#26469;&#26500;&#36896;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;$\mathrm{P\small{ARA}\normalsize}_\mathrm{gen}$-MWP&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#35299;&#26512;&#21464;&#20307;&#38382;&#39064;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;$\mathrm{P\small{ARA}\normalsize}_\mathrm{gen}$-MWP&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#36890;&#36807;&#29702;&#35299;&#21644;&#25512;&#29702;&#38382;&#39064;&#25991;&#26412;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#21035;&#26469;&#25512;&#23548;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The art of mathematical reasoning stands as a fundamental pillar of intellectual progress and is a central catalyst in cultivating human ingenuity. Researchers have recently published a plethora of works centered around the task of solving Math Word Problems (MWP) $-$ a crucial stride towards general AI. These existing models are susceptible to dependency on shallow heuristics and spurious correlations to derive the solution expressions. In order to ameliorate this issue, in this paper, we propose a framework for MWP solvers based on the generation of linguistic variants of the problem text. The approach involves solving each of the variant problems and electing the predicted expression with the majority of the votes. We use DeBERTa (Decoding-enhanced BERT with disentangled attention) as the encoder to leverage its rich textual representations and enhanced mask decoder to construct the solution expressions. Furthermore, we introduce a challenging dataset, $\mathrm{P\small{ARA}\normalsi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#20114;&#24335;&#21367;&#31215;&#32593;&#32476;&#65288;ICN&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20849;&#20139;&#24494;&#31227;&#21160;&#30340;&#26102;&#31354;&#20986;&#34892;&#38656;&#27714;&#12290;ICN&#27169;&#22411;&#37319;&#29992;&#26032;&#39062;&#30340;&#36890;&#36947;&#25193;&#24352;&#26041;&#27861;&#21644;&#21367;&#31215;&#25805;&#20316;&#21516;&#26102;&#25429;&#25417;&#26102;&#38388;&#21644;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#25552;&#21462;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#26102;&#31354;&#20849;&#20139;&#24494;&#31227;&#21160;&#38656;&#27714;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.13897</link><description>&lt;p&gt;
ICN&#65306;&#20132;&#20114;&#24335;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20849;&#20139;&#24494;&#31227;&#21160;&#20986;&#34892;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ICN: Interactive Convolutional Network for Forecasting Travel Demand of Shared Micromobility. (arXiv:2306.13897v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#20114;&#24335;&#21367;&#31215;&#32593;&#32476;&#65288;ICN&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20849;&#20139;&#24494;&#31227;&#21160;&#30340;&#26102;&#31354;&#20986;&#34892;&#38656;&#27714;&#12290;ICN&#27169;&#22411;&#37319;&#29992;&#26032;&#39062;&#30340;&#36890;&#36947;&#25193;&#24352;&#26041;&#27861;&#21644;&#21367;&#31215;&#25805;&#20316;&#21516;&#26102;&#25429;&#25417;&#26102;&#38388;&#21644;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#25552;&#21462;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#26102;&#31354;&#20849;&#20139;&#24494;&#31227;&#21160;&#38656;&#27714;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20849;&#20139;&#24494;&#31227;&#21160;&#38656;&#27714;&#39044;&#27979;&#23545;&#20110;&#20132;&#36890;&#35268;&#21010;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20026;&#22788;&#29702;&#38656;&#27714;&#39044;&#27979;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#23545;&#20110;&#39640;&#31934;&#24230;&#26102;&#31354;&#20849;&#20139;&#24494;&#31227;&#21160;&#38656;&#27714;&#30340;&#39044;&#27979;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#20114;&#24335;&#21367;&#31215;&#32593;&#32476;&#65288;ICN&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20849;&#20139;&#24494;&#31227;&#21160;&#30340;&#26102;&#31354;&#20986;&#34892;&#38656;&#27714;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#26053;&#34892;&#34892;&#20026;&#30693;&#35782;&#65292;&#22522;&#20110;&#22810;&#32500;&#31354;&#38388;&#20449;&#24687;&#65288;&#21363;&#20154;&#21475;&#32479;&#35745;&#23398;&#65292;&#21151;&#33021;&#21644;&#20132;&#36890;&#20379;&#32473;&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#25193;&#24352;&#26041;&#27861;&#26469;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#21367;&#31215;&#36816;&#31639;&#26469;&#22788;&#29702;&#25193;&#24352;&#30340;&#24352;&#37327;&#65292;&#20174;&#32780;&#21516;&#26102;&#25429;&#25417;&#26102;&#38388;&#21644;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#22522;&#20110;&#20108;&#21449;&#26641;&#32467;&#26500;&#30340;&#26550;&#26500;&#21644;&#20132;&#20114;&#24335;&#21367;&#31215;&#65292;ICN&#27169;&#22411;&#25552;&#21462;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#29983;&#25104;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate shared micromobility demand predictions are essential for transportation planning and management. Although deep learning models provide powerful tools to deal with demand prediction problems, studies on forecasting highly-accurate spatiotemporal shared micromobility demand are still lacking. This paper proposes a deep learning model named Interactive Convolutional Network (ICN) to forecast spatiotemporal travel demand for shared micromobility. The proposed model develops a novel channel dilation method by utilizing multi-dimensional spatial information (i.e., demographics, functionality, and transportation supply) based on travel behavior knowledge for building the deep learning model. We use the convolution operation to process the dilated tensor to simultaneously capture temporal and spatial dependencies. Based on a binary-tree-structured architecture and interactive convolution, the ICN model extracts features at different temporal resolutions, and then generates prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#25955;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#21512;&#20316;&#20998;&#25955;&#28145;&#24230;&#23398;&#20064;&#65292;&#38450;&#27490;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#26102;&#27844;&#38706;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.13892</link><description>&lt;p&gt;
&#20855;&#26377;&#20849;&#35782;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#20998;&#25955;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Decentralized Deep Learning with Consensus Algorithms. (arXiv:2306.13892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#25955;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#21512;&#20316;&#20998;&#25955;&#28145;&#24230;&#23398;&#20064;&#65292;&#38450;&#27490;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#26102;&#27844;&#38706;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#20998;&#25955;&#28145;&#24230;&#23398;&#20064;&#20381;&#36182;&#20110;&#36890;&#20449;&#20195;&#29702;&#20043;&#38388;&#30340;&#30452;&#25509;&#20449;&#24687;&#20132;&#25442;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#21487;&#20197;&#35775;&#38382;&#24212;&#35813;&#20445;&#25345;&#31169;&#26377;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#30446;&#26631;&#26159;&#22312;&#35757;&#32451;&#21518;&#20351;&#24471;&#25152;&#26377;&#20195;&#29702;&#22312;&#27169;&#22411;&#21442;&#25968;&#19978;&#36798;&#25104;&#20849;&#35782;&#12290;&#28982;&#32780;&#65292;&#19982;&#19981;&#21487;&#20449;&#30340;&#37051;&#23621;&#20195;&#29702;&#20849;&#20139;&#21442;&#25968;&#21487;&#33021;&#20250;&#27844;&#38706;&#26377;&#20851;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#21487;&#21033;&#29992;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#20998;&#25955;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#22312;&#21512;&#20316;&#35757;&#32451;&#26399;&#38388;&#21644;&#20043;&#21518;&#20445;&#25252;&#27599;&#20010;&#20195;&#29702;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#24120;&#29992;&#20110;&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#27867;&#21270;&#21040;&#23454;&#29992;&#30340;&#22522;&#20110;&#23376;&#26799;&#24230;&#21644;ADMM&#30340;&#20998;&#25955;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#36866;&#29992;&#20110;&#20219;&#24847;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20998;&#26512;&#20102;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#20854;&#20182;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative decentralized deep learning relies on direct information exchange between communicating agents, each with access to a local dataset which should be kept private. The goal is for all agents to achieve consensus on model parameters after training. However, sharing parameters with untrustworthy neighboring agents could leak exploitable information about local datasets. To combat this, we introduce differentially private decentralized learning that secures each agent's local dataset during and after cooperative training. In our approach, we generalize Differentially Private Stochastic Gradient Descent (DP-SGD) -- a popular differentially private training method for centralized deep learning -- to practical subgradient- and ADMM-based decentralized learning methods. Our algorithms' differential privacy guarantee holds for arbitrary deep learning objective functions, and we analyze the convergence properties for strongly convex objective functions. We compare our algorithms again
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25805;&#32437;&#39118;&#38505;&#65292;&#21363;&#21516;&#19968;&#20915;&#31574;&#25110;&#39044;&#27979;&#21487;&#33021;&#26377;&#22810;&#31181;&#35299;&#37322;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#25915;&#20987;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#24213;&#23618;&#25968;&#25454;&#20197;&#24433;&#21709;&#35299;&#37322;&#19982;&#30452;&#25509;&#21033;&#29992;&#35299;&#37322;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#25552;&#20379;&#32773;&#21487;&#36861;&#27714;&#30340;&#20960;&#20010;&#30446;&#26631;&#21644;&#20855;&#20307;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.13885</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25805;&#32437;&#39118;&#38505;: &#19981;&#19968;&#33268;&#38382;&#39064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Manipulation Risks in Explainable AI: The Implications of the Disagreement Problem. (arXiv:2306.13885v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25805;&#32437;&#39118;&#38505;&#65292;&#21363;&#21516;&#19968;&#20915;&#31574;&#25110;&#39044;&#27979;&#21487;&#33021;&#26377;&#22810;&#31181;&#35299;&#37322;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#25915;&#20987;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#24213;&#23618;&#25968;&#25454;&#20197;&#24433;&#21709;&#35299;&#37322;&#19982;&#30452;&#25509;&#21033;&#29992;&#35299;&#37322;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#25552;&#20379;&#32773;&#21487;&#36861;&#27714;&#30340;&#20960;&#20010;&#30446;&#26631;&#21644;&#20855;&#20307;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25105;&#20204;&#29983;&#27963;&#30340;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#65292;&#36825;&#22686;&#21152;&#20102;&#35299;&#37322;&#36825;&#20123;&#20915;&#31574;&#24182;&#30830;&#20445;&#23427;&#20204;&#19982;&#25105;&#20204;&#24819;&#35201;&#30340;&#20915;&#31574;&#19968;&#33268;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26174;&#29616;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#65292;&#21363;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#21363;&#21516;&#19968;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#25110;&#39044;&#27979;&#21487;&#33021;&#26377;&#22810;&#31181;&#35299;&#37322;&#12290;&#34429;&#28982;&#24050;&#32463;&#35748;&#35782;&#21040;&#20102;&#19981;&#19968;&#33268;&#38382;&#39064;&#30340;&#23384;&#22312;&#65292;&#20294;&#19982;&#27492;&#38382;&#39064;&#30456;&#20851;&#30340;&#28508;&#22312;&#24433;&#21709;&#23578;&#26410;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#35299;&#37322;&#25552;&#20379;&#32773;&#21487;&#20197;&#37319;&#29992;&#30340;&#19981;&#21516;&#31574;&#30053;&#65292;&#20197;&#20351;&#36820;&#22238;&#30340;&#35299;&#37322;&#31526;&#21512;&#20182;&#20204;&#30340;&#21033;&#30410;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#25915;&#20987;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#24213;&#23618;&#25968;&#25454;&#20197;&#24433;&#21709;&#35299;&#37322;&#30340;&#31574;&#30053;&#21644;&#30452;&#25509;&#21033;&#29992;&#35299;&#37322;&#38454;&#27573;&#30340;&#31574;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35299;&#37322;&#25552;&#20379;&#32773;&#21487;&#36861;&#27714;&#30340;&#20960;&#20010;&#30446;&#26631;&#21644;&#20855;&#20307;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) systems are increasingly used in high-stakes domains of our life, increasing the need to explain these decisions and to make sure that they are aligned with how we want the decision to be made. The field of Explainable AI (XAI) has emerged in response. However, it faces a significant challenge known as the disagreement problem, where multiple explanations are possible for the same AI decision or prediction. While the existence of the disagreement problem is acknowledged, the potential implications associated with this problem have not yet been widely studied. First, we provide an overview of the different strategies explanation providers could deploy to adapt the returned explanation to their benefit. We make a distinction between strategies that attack the machine learning model or underlying data to influence the explanations, and strategies that leverage the explanation phase directly. Next, we analyse several objectives and concrete scenarios the provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#39537;&#21160;&#30340;&#39640;&#25928;&#24207;&#25968;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;L2RCLIP&#65292;&#23427;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#20013;&#30340;&#24207;&#25968;&#20808;&#39564;&#65292;&#21033;&#29992;&#34917;&#20805;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;RankFormer&#22686;&#24378;&#21407;&#22987;&#25490;&#24207;&#25552;&#31034;&#30340;&#25490;&#24207;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#36328;&#27169;&#24577;&#25490;&#24207;&#32422;&#26463;&#25439;&#22833;(CMOCL)&#36827;&#19968;&#27493;&#23558;&#35821;&#35328;&#20808;&#39564;&#34701;&#20837;&#27169;&#22411;&#20013;&#12290;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;L2RCLIP&#37117;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.13856</link><description>&lt;p&gt;
&#23398;&#20064;&#25490;&#24207;&#36935;&#35265;&#35821;&#35328;&#65306;&#22686;&#24378;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#25490;&#24207;&#23545;&#40784;&#20197;&#25903;&#25345;&#24207;&#25968;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification. (arXiv:2306.13856v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#39537;&#21160;&#30340;&#39640;&#25928;&#24207;&#25968;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;L2RCLIP&#65292;&#23427;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#20013;&#30340;&#24207;&#25968;&#20808;&#39564;&#65292;&#21033;&#29992;&#34917;&#20805;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;RankFormer&#22686;&#24378;&#21407;&#22987;&#25490;&#24207;&#25552;&#31034;&#30340;&#25490;&#24207;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#36328;&#27169;&#24577;&#25490;&#24207;&#32422;&#26463;&#25439;&#22833;(CMOCL)&#36827;&#19968;&#27493;&#23558;&#35821;&#35328;&#20808;&#39564;&#34701;&#20837;&#27169;&#22411;&#20013;&#12290;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;L2RCLIP&#37117;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#25490;&#24207;&#23545;&#20934;&#26041;&#27861;&#65292;&#29992;&#20110;&#24207;&#25968;&#20998;&#31867;&#12290;&#22312;&#24207;&#25968;&#20998;&#31867;&#20013;&#65292;&#26631;&#31614;&#21253;&#21547;&#39069;&#22806;&#30340;&#25490;&#24207;&#20851;&#31995;&#65292;&#22914;&#26524;&#20165;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#24456;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#26368;&#36817;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#21551;&#21457;&#25105;&#20204;&#36890;&#36807;&#23558;&#21407;&#22987;&#20219;&#21153;&#36716;&#21270;&#20026;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#26469;&#21033;&#29992;&#20154;&#31867;&#35821;&#35328;&#20013;&#20016;&#23500;&#30340;&#24207;&#25968;&#20808;&#39564;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2RCLIP&#65292;&#23427;&#20174;&#20004;&#20010;&#26041;&#38754;&#20805;&#20998;&#21033;&#29992;&#20102;&#35821;&#35328;&#20808;&#39564;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#34917;&#20805;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;RankFormer&#65292;&#26088;&#22312;&#22686;&#24378;&#21407;&#22987;&#25490;&#24207;&#25552;&#31034;&#30340;&#25490;&#24207;&#20851;&#31995;&#12290;&#23427;&#22312;&#21333;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#26631;&#35760;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;&#39118;&#26684;&#25552;&#31034;&#28151;&#21512;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#34701;&#20837;&#35821;&#35328;&#20808;&#39564;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#39321;&#33609;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#36817;&#20284;&#32465;&#23450;&#20248;&#21270;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#20869;&#36827;&#34892;&#20102;&#37325;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#25490;&#24207;&#32422;&#26463;&#25439;&#22833;&#65288;CMOCL&#65289;&#65292;&#29992;&#20110;&#35268;&#33539;&#20174;&#35821;&#35328;&#20013;&#23548;&#20986;&#30340;&#24207;&#25968;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#27969;&#34892;&#30340;&#24207;&#25968;&#20998;&#31867;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel language-driven ordering alignment method for ordinal classification. The labels in ordinal classification contain additional ordering relations, making them prone to overfitting when relying solely on training data. Recent developments in pre-trained vision-language models inspire us to leverage the rich ordinal priors in human language by converting the original task into a vision-language alignment task. Consequently, we propose L2RCLIP, which fully utilizes the language priors from two perspectives. First, we introduce a complementary prompt tuning technique called RankFormer, designed to enhance the ordering relation of original rank prompts. It employs token-level attention with residual-style prompt blending in the word embedding space. Second, to further incorporate language priors, we revisit the approximate bound optimization of vanilla cross-entropy loss and restructure it within the cross-modal embedding space. Consequently, we propose a cross-modal ordin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24615;&#20445;&#25345;&#30340;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;SP-AGCL&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#25239;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#33410;&#28857;&#29305;&#24449;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13854</link><description>&lt;p&gt;
&#30456;&#20284;&#24615;&#20445;&#25345;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Similarity Preserving Adversarial Graph Contrastive Learning. (arXiv:2306.13854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24615;&#20445;&#25345;&#30340;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;SP-AGCL&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#25239;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#33410;&#28857;&#29305;&#24449;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363;&#23545;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#12290;&#22312;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#22522;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#30340;&#26041;&#27861;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22266;&#26377;&#35774;&#35745;&#39640;&#24230;&#20381;&#36182;&#20110;&#20174;&#21407;&#22987;&#22270;&#27966;&#29983;&#20986;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#28982;&#32780;&#24403;&#22270;&#21463;&#21040;&#25915;&#20987;&#26102;&#65292;&#21407;&#22987;&#22270;&#20013;&#24050;&#32463;&#21253;&#21547;&#20102;&#22122;&#22768;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#36825;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#24212;&#29992;&#20110;GCL&#26694;&#26550;&#65292;&#23558;&#25915;&#20987;&#30340;&#22270;&#20316;&#20026;GCL&#26694;&#26550;&#19979;&#30340;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;GCL&#26041;&#27861;&#22312;&#20445;&#25345;&#33410;&#28857;&#29305;&#24449;&#30456;&#20284;&#24615;&#26041;&#38754;&#20184;&#20986;&#20102;&#20195;&#20215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24615;&#20445;&#25345;&#30340;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;SP-AGCL&#65289;&#26694;&#26550;&#65292;&#23558;&#24178;&#20928;&#30340;&#22270;&#19982;&#20004;&#20010;&#19981;&#21516;&#35270;&#22270;&#30340;&#36741;&#21161;&#22270;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works demonstrate that GNN models are vulnerable to adversarial attacks, which refer to imperceptible perturbation on the graph structure and node features. Among various GNN models, graph contrastive learning (GCL) based methods specifically suffer from adversarial attacks due to their inherent design that highly depends on the self-supervision signals derived from the original graph, which however already contains noise when the graph is attacked. To achieve adversarial robustness against such attacks, existing methods adopt adversarial training (AT) to the GCL framework, which considers the attacked graph as an augmentation under the GCL framework. However, we find that existing adversarially trained GCL methods achieve robustness at the expense of not being able to preserve the node feature similarity. In this paper, we propose a similarity-preserving adversarial graph contrastive learning (SP-AGCL) framework that contrasts the clean graph with two auxiliary views of differe
&lt;/p&gt;</description></item><item><title>&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#38598;&#30340;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#20302;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PT&#65289;&#32988;&#36807;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#12290;&#24403;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#39640;&#26102;&#65292;MAML&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.13841</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30495;&#30340;&#27604;&#20803;&#23398;&#20064;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Pre-training Truly Better Than Meta-Learning?. (arXiv:2306.13841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13841
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#38598;&#30340;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#20302;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PT&#65289;&#32988;&#36807;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#12290;&#24403;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#39640;&#26102;&#65292;MAML&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#30446;&#21069;&#26222;&#36941;&#35748;&#20026;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PT&#65289;&#21152;&#19978;&#22312;&#35780;&#20215;&#26102;&#24494;&#35843;&#26368;&#21518;&#19968;&#23618;&#65292;&#32988;&#36807;&#26631;&#20934;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#21644;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#27604;&#36739;PT&#21644;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#36825;&#20123;&#35828;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#24378;&#35843;&#20351;&#29992;&#30456;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#30456;&#21516;&#30340;&#20248;&#21270;&#22120;&#65292;&#20197;&#21450;&#25152;&#26377;&#27169;&#22411;&#37117;&#35757;&#32451;&#21040;&#25910;&#25947;&#12290;&#20851;&#38190;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26356;&#20005;&#26684;&#30340;&#32479;&#35745;&#24037;&#20855;&#8212;&#8212;&#25928;&#24212;&#37327;&#65288;Cohen's d&#65289;&#8212;&#8212;&#26469;&#30830;&#23450;&#20351;&#29992;PT&#19982;&#20351;&#29992;MAML&#20043;&#38388;&#30340;&#27169;&#22411;&#24046;&#24322;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;&#39044;&#20808;&#25552;&#20986;&#30340;&#24230;&#37327;&#8212;&#8212;&#22810;&#26679;&#24615;&#31995;&#25968;&#8212;&#8212;&#26469;&#35745;&#31639;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#27491;&#24335;&#22810;&#26679;&#24615;&#12290;&#20351;&#29992;&#36825;&#31181;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20197;&#19979;&#20107;&#23454;&#65306;1. &#24403;&#25968;&#25454;&#38598;&#30340;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#20302;&#26102;&#65292;PT&#22312;&#24179;&#22343;&#24847;&#20041;&#19978;&#32988;&#36807;MAML&#65307;2. &#24403;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#39640;&#26102;&#65292;MAML&#32988;&#36807;PT&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of few-shot learning, it is currently believed that a fixed pre-trained (PT) model, along with fine-tuning the final layer during evaluation, outperforms standard meta-learning algorithms. We re-evaluate these claims under an in-depth empirical examination of an extensive set of formally diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike previous work, we emphasize a fair comparison by using: the same architecture, the same optimizer, and all models trained to convergence. Crucially, we use a more rigorous statistical tool -- the effect size (Cohen's d) -- to determine the practical significance of the difference between a model trained with PT vs. a MAML. We then use a previously proposed metric -- the diversity coefficient -- to compute the average formal diversity of a dataset. Using this analysis, we demonstrate the following: 1. when the formal diversity of a data set is low, PT beats MAML on average and 2. when the formal diversity is hi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DEKGCI&#65292;&#19968;&#31181;&#21452;&#38754;&#25512;&#33616;&#27169;&#22411;&#65292;&#22312;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#21516;&#26102;&#20016;&#23500;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#32852;&#21512;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2306.13837</link><description>&lt;p&gt;
DEKGCI&#65306;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#19982;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#30340;&#21452;&#38754;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DEKGCI: A double-sided recommendation model for integrating knowledge graph and user-item interaction graph. (arXiv:2306.13837v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DEKGCI&#65292;&#19968;&#31181;&#21452;&#38754;&#25512;&#33616;&#27169;&#22411;&#65292;&#22312;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#21516;&#26102;&#20016;&#23500;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#32852;&#21512;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#30693;&#35782;&#22270;&#35889;&#21644;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#34987;&#39057;&#32321;&#20351;&#29992;&#26469;&#24314;&#27169;&#29992;&#25143;&#21644;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#21482;&#20851;&#27880;&#20854;&#20013;&#19968;&#31181;&#20449;&#24687;&#28304;&#65288;&#21363;&#30693;&#35782;&#22270;&#35889;&#25110;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#65289;&#65292;&#23548;&#33268;&#26410;&#20805;&#20998;&#21033;&#29992;&#25972;&#21512;&#20004;&#31181;&#20449;&#24687;&#28304;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#38754;&#25512;&#33616;&#27169;&#22411;DEKGCI&#12290;&#22312;DEKGCI&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#30340;&#39640;&#38454;&#21327;&#20316;&#20449;&#21495;&#26469;&#20016;&#23500;&#29992;&#25143;&#34920;&#31034;&#65292;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#30693;&#35782;&#22270;&#35889;&#30340;&#39640;&#38454;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26469;&#20016;&#23500;&#29289;&#21697;&#34920;&#31034;&#12290;DEKGCI&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#32852;&#21512;&#20132;&#20114;&#12290;&#23454;&#39564;&#37319;&#29992;&#20102;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;DEKGCI&#12290;
&lt;/p&gt;
&lt;p&gt;
Both knowledge graphs and user-item interaction graphs are frequently used in recommender systems due to their ability to provide rich information for modeling users and items. However, existing studies often focused on one of these sources (either the knowledge graph or the user-item interaction graph), resulting in underutilization of the benefits that can be obtained by integrating both sources of information. In this paper, we propose DEKGCI, a novel double-sided recommendation model. In DEKGCI, we use the high-order collaborative signals from the user-item interaction graph to enrich the user representations on the user side. Additionally, we utilize the high-order structural and semantic information from the knowledge graph to enrich the item representations on the item side. DEKGCI simultaneously learns the user and item representations to effectively capture the joint interactions between users and items. Three real-world datasets are adopted in the experiments to evaluate DEKG
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;NLP Transformer&#20013;&#20998;&#26512;&#20301;&#32622;&#12289;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#20301;&#32622;&#20449;&#24687;&#36890;&#36807;&#34746;&#26059;&#36335;&#24452;&#22312;&#28145;&#23618;&#20013;&#33258;&#25105;&#20998;&#31163;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20391;&#29983;&#25104;&#35789;&#24615;&#32858;&#31867;&#12290;&#25552;&#20986;&#20102;&#26367;&#20195;&#22312;&#35821;&#20041;&#23884;&#20837;&#20013;&#28155;&#21152;&#20301;&#32622;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13817</link><description>&lt;p&gt;
NLP Transformer&#20013;&#30340;&#21452;&#34746;&#26059;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Double Helix inside the NLP Transformer. (arXiv:2306.13817v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;NLP Transformer&#20013;&#20998;&#26512;&#20301;&#32622;&#12289;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#20301;&#32622;&#20449;&#24687;&#36890;&#36807;&#34746;&#26059;&#36335;&#24452;&#22312;&#28145;&#23618;&#20013;&#33258;&#25105;&#20998;&#31163;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20391;&#29983;&#25104;&#35789;&#24615;&#32858;&#31867;&#12290;&#25552;&#20986;&#20102;&#26367;&#20195;&#22312;&#35821;&#20041;&#23884;&#20837;&#20013;&#28155;&#21152;&#20301;&#32622;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;NLP Transformer&#20013;&#20998;&#26512;&#19981;&#21516;&#20449;&#24687;&#31867;&#22411;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#22235;&#23618;&#20449;&#24687;&#65306;&#20301;&#32622;&#12289;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#65292;&#24120;&#35265;&#30340;&#22312;&#35821;&#20041;&#23884;&#20837;&#20013;&#28155;&#21152;&#20301;&#32622;&#20449;&#24687;&#30340;&#20570;&#27861;&#26159;&#27425;&#20248;&#30340;&#65292;&#25552;&#35758;&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;(Linar-and-Add)&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20301;&#32622;&#20449;&#24687;&#22312;&#28145;&#23618;&#20013;&#30340;&#33258;&#25105;&#20998;&#31163;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#21521;&#37327;&#30340;&#20301;&#32622;&#32452;&#25104;&#37096;&#20998;&#36981;&#24490;&#34746;&#26059;&#30340;&#36335;&#24452;&#65292;&#22312;&#32534;&#30721;&#22120;&#20391;&#21644;&#35299;&#30721;&#22120;&#20391;&#37117;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#21478;&#22806;&#36824;&#23637;&#31034;&#65292;&#32534;&#30721;&#22120;&#20391;&#30340;&#27010;&#24565;&#32500;&#24230;&#29983;&#25104;&#20102;&#35789;&#24615;(PoS)&#30340;&#32858;&#31867;&#12290;&#22312;&#35299;&#30721;&#22120;&#20391;&#65292;&#25105;&#20204;&#23637;&#31034;&#20351;&#29992;&#20108;&#20803;&#35821;&#27861;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#25581;&#31034;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#35789;&#24615;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#38416;&#26126;&#36890;&#36807;NLP Transformer&#30340;&#28145;&#23618;&#20449;&#24687;&#22788;&#29702;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a framework for analyzing various types of information in an NLP Transformer. In this approach, we distinguish four layers of information: positional, syntactic, semantic, and contextual. We also argue that the common practice of adding positional information to semantic embedding is sub-optimal and propose instead a Linear-and-Add approach. Our analysis reveals an autogenetic separation of positional information through the deep layers. We show that the distilled positional components of the embedding vectors follow the path of a helix, both on the encoder side and on the decoder side. We additionally show that on the encoder side, the conceptual dimensions generate Part-of-Speech (PoS) clusters. On the decoder side, we show that a di-gram approach helps to reveal the PoS clusters of the next token. Our approach paves a way to elucidate the processing of information through the deep layers of an NLP Transformer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36947;&#24503;&#25945;&#32946;&#21644;&#21457;&#23637;&#30740;&#31350;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;&#26368;&#36817;&#30340;LLM&#20855;&#26377;&#26032;&#20852;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#21151;&#33021;&#65292;&#21487;&#20197;&#36890;&#36807;&#25512;&#29702;&#21644;&#20462;&#35746;&#26469;&#35299;&#20915;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2306.13805</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36947;&#24503;&#25945;&#32946;&#21644;&#21457;&#23637;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Potential Benefits of Employing Large Language Models in Research in Moral Education and Development. (arXiv:2306.13805v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36947;&#24503;&#25945;&#32946;&#21644;&#21457;&#23637;&#30740;&#31350;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;&#26368;&#36817;&#30340;LLM&#20855;&#26377;&#26032;&#20852;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#21151;&#33021;&#65292;&#21487;&#20197;&#36890;&#36807;&#25512;&#29702;&#21644;&#20462;&#35746;&#26469;&#35299;&#20915;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#21644;&#20154;&#24037;&#24378;&#21270;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290; LLM&#24050;&#25104;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#31934;&#30830;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#24335;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;LLM&#20855;&#26377;&#27169;&#25311;&#22797;&#26434;&#20154;&#31867;&#35748;&#30693;&#30340;&#26032;&#20852;&#21151;&#33021;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#65292;&#36825;&#20123;&#29305;&#24615;&#22312;&#20197;&#21069;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#19981;&#21487;&#29992;&#12290;&#26412;&#25991;&#23558;&#25506;&#35752;LLM&#22914;&#20309;&#21487;&#33021;&#20026;&#36947;&#24503;&#25945;&#32946;&#21644;&#21457;&#23637;&#30740;&#31350;&#20570;&#20986;&#36129;&#29486;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#23558;&#22238;&#39038;&#26368;&#36817;&#21457;&#34920;&#30340;&#20250;&#35758;&#35770;&#25991;&#21644;ArXiv&#39044;&#21360;&#26412;&#65292;&#27010;&#36848;LLM&#20013;&#23454;&#29616;&#30340;&#26032;&#39062;&#21151;&#33021;&#29305;&#24615;&#12290;&#25105;&#36824;&#25171;&#31639;&#20351;&#29992;ChatGPT&#36827;&#34892;&#31616;&#30701;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;LLM&#22788;&#29702;&#36947;&#24503;&#22256;&#22659;&#21644;&#22806;&#37096;&#21453;&#39304;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#21487;&#33021;&#33021;&#22815;&#22522;&#20110;&#25512;&#29702;&#21644;&#20462;&#35746;&#26469;&#35299;&#20915;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, computer scientists have developed large language models (LLMs) by training prediction models with large-scale language corpora and human reinforcements. The LLMs have become one promising way to implement artificial intelligence with accuracy in various fields. Interestingly, recent LLMs possess emergent functional features that emulate sophisticated human cognition, especially in-context learning and the chain of thought, which were unavailable in previous prediction models. In this paper, I will examine how LLMs might contribute to moral education and development research. To achieve this goal, I will review the most recently published conference papers and ArXiv preprints to overview the novel functional features implemented in LLMs. I also intend to conduct brief experiments with ChatGPT to investigate how LLMs behave while addressing ethical dilemmas and external feedback. The results suggest that LLMs might be capable of solving dilemmas based on reasoning and revising
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#20445;&#25252;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#22788;&#29702;&#24182;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#19979;&#65292;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#65292;&#20026;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20808;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.13803</link><description>&lt;p&gt;
&#22823;&#35937;&#19982;&#31639;&#27861;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#22823;&#35937;&#30417;&#27979;&#20013;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#20316;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring. (arXiv:2306.13803v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#20445;&#25252;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#22788;&#29702;&#24182;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#19979;&#65292;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#65292;&#20026;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20808;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20026;&#22686;&#36827;&#23545;&#21160;&#29289;&#34892;&#20026;&#21644;&#20445;&#25252;&#31574;&#30053;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#26426;&#20250;&#12290;&#20197;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#20026;&#28966;&#28857;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#23427;&#20204;&#20445;&#25252;&#20013;&#30340;&#20316;&#29992;&#12290;&#32473;&#23450;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#65288;&#22914;&#25668;&#20687;&#22836;&#12289;&#40614;&#20811;&#39118;&#12289;&#22320;&#38663;&#20202;&#12289;&#26080;&#20154;&#26426;&#21644;&#21355;&#26143;&#65289;&#25910;&#38598;&#21040;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#65292;&#25361;&#25112;&#22312;&#20110;&#31649;&#29702;&#21644;&#35299;&#35835;&#36825;&#20123;&#24222;&#22823;&#30340;&#25968;&#25454;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24110;&#21161;&#25105;&#20204;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;AI&#39537;&#21160;&#30417;&#27979;&#26041;&#27861;&#21450;&#20854;&#22312;&#25913;&#21892;&#22823;&#35937;&#20445;&#25252;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#21033;&#29992;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#20197;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#30340;&#20851;&#38190;&#25152;&#22312;&#65292;&#20026;&#35768;&#22810;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20102;&#20808;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) and machine learning (ML) present revolutionary opportunities to enhance our understanding of animal behavior and conservation strategies. Using elephants, a crucial species in Africa's protected areas, as our focal point, we delve into the role of AI and ML in their conservation. Given the increasing amounts of data gathered from a variety of sensors like cameras, microphones, geophones, drones, and satellites, the challenge lies in managing and interpreting this vast data. New AI and ML techniques offer solutions to streamline this process, helping us extract vital information that might otherwise be overlooked. This paper focuses on the different AI-driven monitoring methods and their potential for improving elephant conservation. Collaborative efforts between AI experts and ecological researchers are essential in leveraging these innovative technologies for enhanced wildlife conservation, setting a precedent for numerous other species.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#25628;&#32034;&#28385;&#36275;&#29992;&#25143;&#25351;&#23450;&#24615;&#33021;&#26631;&#20934;&#30340;&#22810;&#26679;&#21270;&#32467;&#26524;&#38598;&#26469;&#25214;&#21040;&#19968;&#32452;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#26356;&#22909;&#22320;&#23637;&#31034;&#20986;&#20248;&#31168;&#35299;&#31354;&#38388;&#21644;&#25552;&#20379;&#20915;&#31574;&#32773;&#20248;&#31168;&#35774;&#35745;&#20915;&#31574;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.13780</link><description>&lt;p&gt;
&#23454;&#29616;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#26679;&#26412;&#26377;&#25928;&#25628;&#32034;&#20013;&#30340;&#30446;&#26631;&#31354;&#38388;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Achieving Diversity in Objective Space for Sample-efficient Search of Multiobjective Optimization Problems. (arXiv:2306.13780v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#25628;&#32034;&#28385;&#36275;&#29992;&#25143;&#25351;&#23450;&#24615;&#33021;&#26631;&#20934;&#30340;&#22810;&#26679;&#21270;&#32467;&#26524;&#38598;&#26469;&#25214;&#21040;&#19968;&#32452;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#26356;&#22909;&#22320;&#23637;&#31034;&#20986;&#20248;&#31168;&#35299;&#31354;&#38388;&#21644;&#25552;&#20379;&#20915;&#31574;&#32773;&#20248;&#31168;&#35774;&#35745;&#20915;&#31574;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#22914;&#26448;&#26009;&#35774;&#35745;&#31561;&#37325;&#35201;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#30340;&#27169;&#25311;&#20248;&#21270;&#65292;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#36825;&#20123;&#24212;&#29992;&#25152;&#28041;&#21450;&#30340;&#36153;&#29992;&#26114;&#36149;&#65292;&#38656;&#35201;&#38024;&#23545;&#26679;&#26412;&#26377;&#25928;&#30340;&#12289;&#33021;&#22815;&#26377;&#25928;&#25506;&#32034;Pareto&#21069;&#27839;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#19968;&#32452;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#19981;&#20351;&#29992;&#26174;&#24335;&#20248;&#21270;&#26469;&#35782;&#21035;Pareto&#21069;&#27839;&#65292;&#32780;&#26159;&#24314;&#35758;&#25628;&#32034;&#28385;&#36275;&#29992;&#25143;&#25351;&#23450;&#24615;&#33021;&#26631;&#20934;&#30340;&#22810;&#26679;&#21270;&#32467;&#26524;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#20915;&#31574;&#30340;&#24378;&#22823;&#27744;&#65292;&#24110;&#21161;&#20915;&#31574;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#20248;&#31168;&#35299;&#31354;&#38388;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28385;&#36275;&#24230;&#37327;&#26399;&#26395; &#65288;LMS&#65289;&#25910;&#30410;&#20989;&#25968;&#65292;&#20998;&#26512;&#20102;&#20854;&#34892;&#20026;&#21644;&#29305;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently solving multi-objective optimization problems for simulation optimization of important scientific and engineering applications such as materials design is becoming an increasingly important research topic. This is due largely to the expensive costs associated with said applications, and the resulting need for sample-efficient, multiobjective optimization methods that efficiently explore the Pareto frontier to expose a promising set of design solutions. We propose moving away from using explicit optimization to identify the Pareto frontier and instead suggest searching for a diverse set of outcomes that satisfy user-specified performance criteria. This method presents decision makers with a robust pool of promising design decisions and helps them better understand the space of good solutions. To achieve this outcome, we introduce the Likelihood of Metric Satisfaction (LMS) acquisition function, analyze its behavior and properties, and demonstrate its viability on various pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CeBed&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13761</link><description>&lt;p&gt;
CeBed: &#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#25968;&#25454;&#39537;&#21160;&#30340;OFDM&#20449;&#36947;&#20272;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation. (arXiv:2306.13761v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CeBed&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20449;&#36947;&#20272;&#35745;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20844;&#27491;&#21644;&#29616;&#23454;&#30340;&#27604;&#36739;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#22522;&#20110;&#32463;&#39564;&#20998;&#26512;&#36827;&#34892;&#27604;&#36739;&#12290;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#24037;&#20855;&#65288;&#20363;&#22914;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#24211;&#65289;&#38459;&#30861;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#20449;&#36947;&#20272;&#35745;&#21644;&#26080;&#32447;&#36890;&#20449;&#31561;&#39046;&#22495;&#30340;&#21457;&#23637;&#21644;&#36827;&#27493;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24314;&#31435;&#22522;&#20934;&#27979;&#35797;&#30340;&#20513;&#35758;&#65292;&#32479;&#19968;&#20102;&#20960;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CeBed&#65288;&#20449;&#36947;&#20272;&#35745;&#27979;&#35797;&#24179;&#21488;&#65289;&#65292;&#21253;&#25324;&#28085;&#30422;&#21508;&#31181;&#31995;&#32479;&#27169;&#22411;&#21644;&#20256;&#25773;&#26465;&#20214;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#21313;&#20010;&#28145;&#24230;&#21644;&#20256;&#32479;&#30340;&#22522;&#32447;&#23454;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#65292;&#35299;&#20915;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been extensively used in wireless communication problems, including channel estimation. Although several data-driven approaches exist, a fair and realistic comparison between them is difficult due to inconsistencies in the experimental conditions and the lack of a standardized experimental design. In addition, the performance of data-driven approaches is often compared based on empirical analysis. The lack of reproducibility and availability of standardized evaluation tools (e.g., datasets, codebases) hinder the development and progress of data-driven methods for channel estimation and wireless communication in general. In this work, we introduce an initiative to build benchmarks that unify several data-driven OFDM channel estimation approaches. Specifically, we present CeBed (a testbed for channel estimation) including different datasets covering various systems models and propagation conditions along with the implementation of ten deep and traditional baselines. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#23618;&#20851;&#31995;&#29289;&#20307;&#23548;&#33322;&#20219;&#21153;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#20316;&#20026;&#29615;&#22659;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26377;&#25928;&#22320;&#25506;&#32034;&#20855;&#26377;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#22823;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.13760</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#30340;&#22270;&#27880;&#24847;&#21147;&#22312;&#20998;&#23618;&#20851;&#31995;&#29289;&#20307;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Task-Driven Graph Attention for Hierarchical Relational Object Navigation. (arXiv:2306.13760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#23618;&#20851;&#31995;&#29289;&#20307;&#23548;&#33322;&#20219;&#21153;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#20316;&#20026;&#29615;&#22659;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26377;&#25928;&#22320;&#25506;&#32034;&#20855;&#26377;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#22823;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22330;&#26223;&#20013;&#30340;Embodied AI agents&#32463;&#24120;&#38656;&#35201;&#23548;&#33322;&#20197;&#23547;&#25214;&#29289;&#20307;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33258;&#28982;&#20986;&#29616;&#30340;&#29289;&#20307;&#23548;&#33322;&#20219;&#21153;&#21464;&#20307;&#65292;&#21363;&#20998;&#23618;&#20851;&#31995;&#29289;&#20307;&#23548;&#33322;&#65288;HRON&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#23547;&#25214;&#30001;&#36923;&#36753;&#35859;&#35789;&#25351;&#23450;&#30340;&#29289;&#20307;&#65292;&#36825;&#20123;&#35859;&#35789;&#20197;&#20998;&#23618;&#32467;&#26500;&#32452;&#32455;&#65292;&#20854;&#20013;&#20851;&#20110;&#23478;&#20855;&#21644;&#25151;&#38388;&#30340;&#23545;&#35937;&#30456;&#20851;&#65292;&#20363;&#22914;&#22312;&#21416;&#25151;&#30340;&#26700;&#23376;&#19978;&#25214;&#21040;&#19968;&#20010;&#33529;&#26524;&#12290;&#35299;&#20915;&#36825;&#26679;&#30340;&#20219;&#21153;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#34920;&#31034;&#26041;&#27861;&#26469;&#25512;&#29702;&#23545;&#35937;&#20851;&#31995;&#24182;&#23558;&#29615;&#22659;&#21644;&#20219;&#21153;&#30446;&#26631;&#20013;&#30340;&#20851;&#31995;&#30456;&#20851;&#32852;&#12290;HRON&#22312;&#22823;&#22330;&#26223;&#65288;&#20363;&#22914;&#23478;&#24237;&#65289;&#20013;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#21644;&#38271;&#26102;&#38388;&#36328;&#24230;&#65292;&#36825;&#38656;&#35201;&#21487;&#20197;&#32039;&#20945;&#22320;&#23384;&#20648;&#36807;&#21435;&#30340;&#20449;&#24687;&#24182;&#26377;&#25928;&#22320;&#25506;&#32034;&#22330;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#22270;&#20687;&#25110;2D&#22320;&#22270;&#31561;&#20256;&#32479;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22330;&#26223;&#22270;&#26159;&#26368;&#36866;&#21512;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20351;&#29992;&#22330;&#26223;&#22270;&#20316;&#20026;&#34920;&#31034;&#24182;&#20351;&#29992;&#20219;&#21153;&#39537;&#21160;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI agents in large scenes often need to navigate to find objects. In this work, we study a naturally emerging variant of the object navigation task, hierarchical relational object navigation (HRON), where the goal is to find objects specified by logical predicates organized in a hierarchical structure objects related to furniture and then to rooms - such as finding an apple on top of a table in the kitchen. Solving such a task requires an efficient representation to reason about object relations and correlate the relations in the environment and in the task goal. HRON in large scenes (e.g. homes) is particularly challenging due to its partial observability and long horizon, which invites solutions that can compactly store the past information while effectively exploring the scene. We demonstrate experimentally that scene graphs are the best-suited representation compared to conventional representations such as images or 2D maps. We propose a solution that uses scene graphs a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22495;&#32763;&#35793;&#26469;&#25552;&#39640;&#20840;&#26223;&#20998;&#21106;&#22312;&#22812;&#38388;&#25110;&#20302;&#29031;&#24230;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13725</link><description>&lt;p&gt;
&#25913;&#21892;&#22812;&#38388;&#25110;&#20302;&#29031;&#24230;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#30340;&#20840;&#26223;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Improving Panoptic Segmentation for Nighttime or Low-Illumination Urban Driving Scenes. (arXiv:2306.13725v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22495;&#32763;&#35793;&#26469;&#25552;&#39640;&#20840;&#26223;&#20998;&#21106;&#22312;&#22812;&#38388;&#25110;&#20302;&#29031;&#24230;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#21644;&#39550;&#39542;&#31995;&#32479;&#20351;&#29992;&#22330;&#26223;&#35299;&#26512;&#20316;&#20026;&#20102;&#35299;&#29615;&#22659;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#20840;&#26223;&#20998;&#21106;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34987;&#35777;&#26126;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26550;&#26500;&#24050;&#34987;&#29992;&#20110;&#20840;&#26223;&#20998;&#21106;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#40657;&#26263;&#12289;&#20809;&#29031;&#19981;&#36275;&#25110;&#22812;&#38388;&#22270;&#20687;&#31561;&#24694;&#21155;&#26465;&#20214;&#26102;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#27604;&#30333;&#22825;&#22270;&#20687;&#30340;&#34920;&#29616;&#24046;&#12290;&#36896;&#25104;&#32467;&#26524;&#19981;&#20339;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#26159;&#32570;&#20047;&#36275;&#22815;&#21644;&#20934;&#30830;&#26631;&#27880;&#30340;&#22478;&#24066;&#39550;&#39542;&#22812;&#38388;&#22270;&#20687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#22495;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#39640;&#20840;&#26223;&#20998;&#21106;&#22312;&#22812;&#38388;&#25110;&#20302;&#29031;&#24230;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;CycleGAN&#65288;Zhu et al.&#65292;2017&#65289;&#23558;&#26377;&#29616;&#26377;&#20840;&#26223;&#27880;&#37322;&#30340;&#30333;&#22825;&#22270;&#20687;&#36716;&#25442;&#20026;&#20854;&#22812;&#38388;&#29256;&#26412;&#12290;&#28982;&#21518;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#22270;&#20687;&#35757;&#32451;&#20840;&#26223;&#20998;&#21106;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#22312;&#20302;&#29031;&#24230;&#22330;&#26223;&#19979;&#20998;&#21106;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles and driving systems use scene parsing as an essential tool to understand the surrounding environment. Panoptic segmentation is a state-of-the-art technique which proves to be pivotal in this use case. Deep learning-based architectures have been utilized for effective and efficient Panoptic Segmentation in recent times. However, when it comes to adverse conditions like dark scenes with poor illumination or nighttime images, existing methods perform poorly in comparison to daytime images. One of the main factors for poor results is the lack of sufficient and accurately annotated nighttime images for urban driving scenes. In this work, we propose two new methods, first to improve the performance, and second to improve the robustness of panoptic segmentation in nighttime or poor illumination urban driving scenes using a domain translation approach. The proposed approach makes use of CycleGAN (Zhu et al., 2017) to translate daytime images with existing panoptic annotatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#31038;&#20132;AI&#25552;&#39640;&#38598;&#20307;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#35299;&#20915;&#26032;&#30340;&#25216;&#26415;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13723</link><description>&lt;p&gt;
&#31038;&#20132;AI&#19982;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social AI and the Challenges of the Human-AI Ecosystem. (arXiv:2306.13723v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#31038;&#20132;AI&#25552;&#39640;&#38598;&#20307;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#35299;&#20915;&#26032;&#30340;&#25216;&#26415;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#65292;&#20154;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30456;&#20114;&#20316;&#29992;&#30340;&#23835;&#36215;&#65288;&#21253;&#25324;&#21161;&#25163;&#21644;&#25512;&#33616;&#31995;&#32479;&#65289;&#22686;&#21152;&#20102;&#20986;&#29616;&#38598;&#20307;&#29616;&#35937;&#21644;&#20020;&#30028;&#28857;&#30340;&#26426;&#20250;&#65292;&#24182;&#21487;&#33021;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#12289;&#21487;&#33021;&#26159;&#24847;&#22806;&#30340;&#21518;&#26524;&#12290;&#20363;&#22914;&#65292;&#23548;&#33322;&#31995;&#32479;&#30340;&#24314;&#35758;&#21487;&#33021;&#20250;&#22312;&#22826;&#22810;&#30340;&#21496;&#26426;&#34987;&#24341;&#23548;&#21040;&#21516;&#19968;&#36335;&#32447;&#26102;&#36896;&#25104;&#28151;&#20081;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#21487;&#33021;&#20250;&#25918;&#22823;&#26497;&#21270;&#12289;&#36807;&#28388;&#27668;&#27873;&#21644;&#28608;&#36827;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#22521;&#32946;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#21644;&#38598;&#20307;&#34892;&#21160;&#25928;&#24212;&#65292;&#20197;&#24212;&#23545;&#31038;&#20250;&#21644;&#29615;&#22659;&#25361;&#25112;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#23545;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#30340;&#24433;&#21709;&#24182;&#35774;&#35745;&#19982;&#20154;&#31867;&#21512;&#20316;&#20197;&#24110;&#21161;&#20811;&#26381;&#31038;&#20250;&#38382;&#39064;&#30340;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#22797;&#26434;&#31995;&#32479;&#12289;&#32593;&#32476;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#21449;&#28857;&#19978;&#24314;&#31435;&#31038;&#20132;AI&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#35282;&#24230;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#24320;&#22987;&#35752;&#35770;&#31038;&#20132;AI&#25552;&#39640;&#38598;&#20307;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#31038;&#20132;AI&#21487;&#20197;&#20026;&#31038;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#21033;&#30410;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#26032;&#30340;&#25216;&#26415;&#21644;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of large-scale socio-technical systems in which humans interact with artificial intelligence (AI) systems (including assistants and recommenders, in short AIs) multiplies the opportunity for the emergence of collective phenomena and tipping points, with unexpected, possibly unintended, consequences. For example, navigation systems' suggestions may create chaos if too many drivers are directed on the same route, and personalised recommendations on social media may amplify polarisation, filter bubbles, and radicalisation. On the other hand, we may learn how to foster the "wisdom of crowds" and collective action effects to face social and environmental challenges. In order to understand the impact of AI on socio-technical systems and design next-generation AIs that team with humans to help overcome societal problems rather than exacerbate them, we propose to build the foundations of Social AI at the intersection of Complex Systems, Network Science and AI. In this perspective pape
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20363;&#25991;&#26723;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;"&#29992;&#20363;&#21345;"&#65292;&#26088;&#22312;&#38544;&#24335;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#39118;&#38505;&#27700;&#24179;&#24182;&#23450;&#20041;&#30456;&#20851;&#35201;&#27714;&#12290;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;UML&#30340;&#27169;&#26495;&#21644;&#25903;&#25345;UML&#30340;&#22270;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.13701</link><description>&lt;p&gt;
&#29992;&#20363;&#21345;&#65306;&#19968;&#31181;&#21463;&#27431;&#27954;AI&#27861;&#26696;&#21551;&#21457;&#30340;&#29992;&#20363;&#25253;&#21578;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Use case cards: a use case reporting framework inspired by the European AI Act. (arXiv:2306.13701v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20363;&#25991;&#26723;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;"&#29992;&#20363;&#21345;"&#65292;&#26088;&#22312;&#38544;&#24335;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#39118;&#38505;&#27700;&#24179;&#24182;&#23450;&#20041;&#30456;&#20851;&#35201;&#27714;&#12290;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;UML&#30340;&#27169;&#26495;&#21644;&#25903;&#25345;UML&#30340;&#22270;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;AI&#31038;&#21306;&#26368;&#36817;&#21162;&#21147;&#26397;&#30528;&#26631;&#20934;&#21270;&#31243;&#24207;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#20197;&#35760;&#24405;&#27169;&#22411;&#12289;&#26041;&#27861;&#12289;&#31995;&#32479;&#25110;&#25968;&#25454;&#38598;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#26041;&#27861;&#19987;&#27880;&#20110;&#19982;&#27431;&#27954;AI&#27861;&#26696;&#65288;AI&#27861;&#26696;&#65289;&#30340;&#22522;&#20110;&#39118;&#38505;&#30340;&#26041;&#27861;&#23545;&#40784;&#30340;&#29992;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#32479;&#19968;&#26631;&#35760;&#35821;&#35328;&#65288;UML&#65289;&#26631;&#20934;&#20013;&#21253;&#25324;&#30340;&#29992;&#20363;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20363;&#25991;&#26723;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;"&#29992;&#20363;&#21345;"&#12290;&#19982;&#20854;&#20182;&#25991;&#26723;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;AI&#31995;&#32479;&#30340;&#39044;&#26399;&#30446;&#30340;&#21644;&#25805;&#20316;&#29992;&#36884;&#12290;&#23427;&#30001;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#32452;&#25104;&#12290;&#31532;&#19968;&#37096;&#20998;&#26159;&#22522;&#20110;UML&#30340;&#27169;&#26495;&#65292;&#26088;&#22312;&#38544;&#24335;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#39118;&#38505;&#27700;&#24179;&#24182;&#23450;&#20041;&#30456;&#20851;&#35201;&#27714;&#12290;&#31532;&#20108;&#20010;&#37096;&#20998;&#26159;&#19968;&#20010;&#25903;&#25345;UML&#30340;&#22270;&#65292;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;&#31995;&#32479;&#29992;&#25143;&#20132;&#20114;&#21644;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#28041;&#21450;&#27431;&#30431;&#25919;&#31574;&#19987;&#23478;&#21644;&#23398;&#32773;&#30340;&#21327;&#21516;&#35774;&#35745;&#36807;&#31243;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent efforts by the Artificial Intelligence (AI) community to move towards standardised procedures for documenting models, methods, systems or datasets, there is currently no methodology focused on use cases aligned with the risk-based approach of the European AI Act (AI Act). In this paper, we propose a new framework for the documentation of use cases, that we call "use case cards", based on the use case modelling included in the Unified Markup Language (UML) standard. Unlike other documentation methodologies, we focus on the intended purpose and operational use of an AI system. It consists of two main parts. Firstly, a UML-based template, tailored to allow implicitly assessing the risk level of the AI system and defining relevant requirements. Secondly, a supporting UML diagram designed to provide information about the system-user interactions and relationships. The proposed framework is the result of a co-design process involving a relevant team of EU policy experts and sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65292;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#26088;&#22312;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2306.13686</link><description>&lt;p&gt;
&#25299;&#23637;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35270;&#35282;&#65306; &#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#32508;&#21512;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems. (arXiv:2306.13686v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65292;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#26088;&#22312;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22686;&#21152;&#20351;&#29992;&#23548;&#33268;&#20102;&#22810;&#26041;&#38754;&#30340;&#31038;&#20250;&#12289;&#29615;&#22659;&#21644;&#32463;&#27982;&#21518;&#26524;&#65292;&#21253;&#25324;&#38750;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12289;&#27495;&#35270;&#12289;&#19981;&#24179;&#31561;&#21152;&#21095;&#12289;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#20197;&#21450;&#32463;&#27982;&#23454;&#21147;&#30340;&#38598;&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#22810;&#26041;&#38754;&#24615;&#65292;&#20026;&#8220;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#8221;&#30340;&#29702;&#24565;&#25552;&#20379;&#20102;&#23454;&#36136;&#24615;&#30340;&#25903;&#25345;&#12290;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65288;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;&#25351;&#26631;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#36825;&#20123;&#26631;&#20934;&#21644;&#25351;&#26631;&#22522;&#20110;&#25209;&#21028;&#24615;&#23457;&#26597;&#21644;&#19987;&#23478;&#30740;&#35752;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#25972;&#20307;&#24615;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#26694;&#26550;&#65292;&#20026;AI&#31995;&#32479;&#30340;&#21518;&#32493;&#21457;&#23637;&#21644;&#35780;&#20272;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased use of AI systems is associated with multi-faceted societal, environmental, and economic consequences. These include non-transparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in AI model development and application, and an increasing concentration of economic power. By considering the multi-dimensionality of sustainability, this paper takes steps towards substantiating the call for an overarching perspective on "sustainable AI". It presents the SCAIS Framework (Sustainability Criteria and Indicators for Artificial Intelligence Systems) which contains a set 19 sustainability criteria for sustainable AI and 67 indicators that is based on the results of a critical review and expert workshops. This interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable AI. Further, it provides a concrete framework that lays the foundation for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#23478;&#26063;&#30340;&#24605;&#32500;&#26041;&#24335;&#65292;&#23558;&#30740;&#31350;&#20219;&#21153;&#20998;&#35299;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#36739;&#23567;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24314;&#27169;&#30446;&#30340;&#12289;&#31995;&#32479;&#33539;&#22260;&#21644;&#26144;&#23556;&#22240;&#26524;&#20851;&#31995;&#12290;&#20197;COVID-19&#20026;&#20363;&#65292;&#36825;&#31181;&#31574;&#30053;&#24102;&#26469;&#35768;&#22810;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.13683</link><description>&lt;p&gt;
&#22810;&#20934;&#21017;&#20915;&#31574;&#25903;&#25345;&#30340;&#27169;&#22411;&#23478;&#26063;&#65306;&#20197;COVID-19&#20026;&#20363;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Families for Multi-Criteria Decision Support: A COVID-19 Case Study. (arXiv:2306.13683v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#23478;&#26063;&#30340;&#24605;&#32500;&#26041;&#24335;&#65292;&#23558;&#30740;&#31350;&#20219;&#21153;&#20998;&#35299;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#36739;&#23567;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24314;&#27169;&#30446;&#30340;&#12289;&#31995;&#32479;&#33539;&#22260;&#21644;&#26144;&#23556;&#22240;&#26524;&#20851;&#31995;&#12290;&#20197;COVID-19&#20026;&#20363;&#65292;&#36825;&#31181;&#31574;&#30053;&#24102;&#26469;&#35768;&#22810;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#20915;&#31574;&#25903;&#25345;&#22312;&#38271;&#26399;&#39033;&#30446;&#20013;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#38656;&#35201;&#23450;&#26399;&#37325;&#26032;&#35780;&#20272;&#12289;&#24314;&#27169;&#21644;&#23454;&#29616;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24314;&#27169;&#30446;&#30340;&#12289;&#31995;&#32479;&#33539;&#22260;&#21644;&#26144;&#23556;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;20&#19990;&#32426;90&#24180;&#20195;&#25552;&#20986;&#30340;&#27169;&#22411;&#23478;&#26063;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#19968;&#31181;&#21019;&#36896;&#22823;&#22411;&#30740;&#31350;&#39033;&#30446;&#20915;&#31574;&#25903;&#25345;&#26694;&#26550;&#30340;&#24605;&#32500;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26159;&#23558;&#30740;&#31350;&#20219;&#21153;&#20998;&#35299;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#36739;&#23567;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#19987;&#38376;&#23545;&#24212;&#20110;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;COVID-19&#20915;&#31574;&#25903;&#25345;&#27169;&#22411;&#23478;&#26063;&#30340;&#31034;&#20363;&#26469;&#35828;&#26126;&#36825;&#31181;&#31574;&#30053;&#24102;&#26469;&#30340;&#35768;&#22810;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continued model-based decision support is associated with particular challenges, especially in long-term projects. Due to the regularly changing questions and the often changing understanding of the underlying system, the models used must be regularly re-evaluated, -modelled and -implemented with respect to changing modelling purpose, system boundaries and mapped causalities. Usually, this leads to models with continuously growing complexity and volume. In this work we aim to reevaluate the idea of the model family, dating back to the 1990s, and use it to promote this as a mindset in the creation of decision support frameworks in large research projects. The idea is to generally not develop and enhance a single standalone model, but to divide the research tasks into interacting smaller models which specifically correspond to the research question. This strategy comes with many advantages, which we explain using the example of a family of models for decision support in the COVID-19 cris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#23545;&#8220;&#40657;&#21283;&#23376;&#8221;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24544;&#23454;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#30830;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#24635;&#20307;&#25935;&#24863;&#24615;&#65292;&#24182;&#36890;&#36807;&#20363;&#23376;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26041;&#27861;&#27604;&#36739;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20010;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13682</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#24635;&#20307;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the overall sensitivity of saliency-based explanation methods. (arXiv:2306.13682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#23545;&#8220;&#40657;&#21283;&#23376;&#8221;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24544;&#23454;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#30830;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#24635;&#20307;&#25935;&#24863;&#24615;&#65292;&#24182;&#36890;&#36807;&#20363;&#23376;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26041;&#27861;&#27604;&#36739;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20010;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30528;&#30524;&#20110;&#29983;&#25104;&#23545;"&#40657;&#21283;&#23376;"&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24544;&#23454;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#27979;&#35797;&#26469;&#30830;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#24544;&#23454;&#31243;&#24230;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#36328;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#21644;&#20005;&#35880;&#30340;&#26041;&#27861;&#35770;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#19981;&#20381;&#36182;&#29305;&#23450;&#27169;&#22411;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#21512;&#27604;&#36739;&#22810;&#20010;&#35299;&#37322;&#26041;&#27861;&#30340;&#24544;&#23454;&#31243;&#24230;&#30340;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#25351;&#23450;&#27491;&#24335;&#30340;&#38408;&#20540;&#21644;&#24314;&#31435;&#26631;&#20934;&#26469;&#25193;&#23637;&#23427;&#65292;&#20197;&#30830;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#24635;&#20307;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20363;&#23376;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#25193;&#23637;&#26041;&#27861;&#26469;&#27604;&#36739;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20010;&#35299;&#37322;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25935;&#24863;&#24615;&#21644;&#24544;&#23454;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#32771;&#34385;&#22914;&#20309;&#35843;&#25972;&#27979;&#35797;&#26469;&#35780;&#20272;&#20854;&#20182;&#39046;&#22495;&#30340;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the need to generate faithful explanations of "black box" Deep Learning models. Several tests have been proposed to determine aspects of faithfulness of explanation methods, but they lack cross-domain applicability and a rigorous methodology. Hence, we select an existing test that is model agnostic and is well-suited for comparing one aspect of faithfulness (i.e., sensitivity) of multiple explanation methods, and extend it by specifying formal thresh-olds and building criteria to determine the over-all sensitivity of the explanation method. We present examples of how multiple explanation methods for Convolutional Neural Networks can be compared using this extended methodology. Finally, we discuss the relationship between sensitivity and faithfulness and consider how the test can be adapted to assess different explanation methods in other domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#20851;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#20223;&#30495;&#20013;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#20851;&#27880;&#22235;&#20010;&#24314;&#27169;&#21644;&#20223;&#30495;&#20219;&#21153;&#30340;LLMs&#30340;&#39044;&#26399;&#30410;&#22788;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#23454;&#38469;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.13679</link><description>&lt;p&gt;
&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#36935;&#35265;&#20223;&#30495;&#65306;&#22914;&#20309;&#26377;&#25928;&#22320;&#24212;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20110;&#20223;&#30495;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks. (arXiv:2306.13679v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#20223;&#30495;&#20013;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#20851;&#27880;&#22235;&#20010;&#24314;&#27169;&#21644;&#20223;&#30495;&#20219;&#21153;&#30340;LLMs&#30340;&#39044;&#26399;&#30410;&#22788;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#23454;&#38469;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#25110;GPT-4&#25152;&#25552;&#20379;&#30340;&#39072;&#35206;&#24615;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#36890;&#24120;&#24378;&#35843;&#39640;&#27700;&#24179;&#30340;&#26426;&#20250;&#21644;&#25285;&#24551;&#12290;&#26412;&#25991;&#26159;&#20851;&#20110;LLMs&#22312;&#31185;&#23398;&#20223;&#30495;&#20013;&#24212;&#29992;&#30340;&#31532;&#19968;&#31687;&#30740;&#31350;&#12290;&#25105;&#20204;&#20851;&#27880;&#22235;&#20010;&#24314;&#27169;&#21644;&#20223;&#30495;&#20219;&#21153;&#65292;&#27599;&#27425;&#35780;&#20272;LLMs&#30340;&#39044;&#26399;&#30410;&#22788;&#21644;&#38480;&#21046;&#65292;&#21516;&#26102;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#25552;&#20379;&#23454;&#38469;&#25351;&#23548;&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#26088;&#22312;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#30340;&#32467;&#26500;&#65292;&#20197;&#20419;&#36827;&#21442;&#19982;&#32773;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#21442;&#19982;&#12290;&#31532;&#20108;&#20010;&#20219;&#21153;&#19987;&#27880;&#20110;&#27719;&#24635;&#20223;&#30495;&#36755;&#20986;&#65292;&#20197;&#20415;&#27169;&#22411;&#29992;&#25143;&#33021;&#22815;&#35782;&#21035;&#20986;&#20248;&#36873;&#22330;&#26223;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#20256;&#36798;&#23545;&#20223;&#30495;&#21487;&#35270;&#21270;&#30340;&#35265;&#35299;&#65292;&#20197;&#25193;&#22823;&#20223;&#30495;&#24179;&#21488;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#26368;&#21518;&#65292;&#26368;&#21518;&#19968;&#20010;&#20219;&#21153;&#24341;&#20986;&#20102;&#20351;&#29992;LLMs&#35299;&#37322;&#20223;&#30495;&#38169;&#35823;&#21644;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#20415;&#27169;&#22411;&#24320;&#21457;&#32773;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The disruptive technology provided by large-scale pre-trained language models (LLMs) such as ChatGPT or GPT-4 has received significant attention in several application domains, often with an emphasis on high-level opportunities and concerns. This paper is the first examination regarding the use of LLMs for scientific simulations. We focus on four modeling and simulation tasks, each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved. The first task is devoted to explaining the structure of a conceptual model to promote the engagement of participants in the modeling process. The second task focuses on summarizing simulation outputs, so that model users can identify a preferred scenario. The third task seeks to broaden accessibility to simulation platforms by conveying the insights of simulation visualizations via text. Finally, the last task evokes the possibility of explaining simulation errors and pr
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#24895;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#32463;&#39564;&#12289;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;&#21644;&#36807;&#21435;&#30340;ICT&#25216;&#26415;&#32463;&#39564;&#30340;&#24433;&#21709;&#12290;&#20154;&#24037;&#26234;&#33021;&#32463;&#39564;&#21644;ICT&#32463;&#39564;&#20197;&#20004;&#31181;&#26041;&#24335;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#25509;&#21463;&#24847;&#24895;&#65292;&#20197;&#30452;&#25509;&#36884;&#24452;&#21644;&#38388;&#25509;&#36884;&#24452;&#65292;&#32780;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;&#20540;&#21017;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#25509;&#21463;&#24847;&#24895;&#12290;</title><link>http://arxiv.org/abs/2306.13670</link><description>&lt;p&gt;
&#20160;&#20040;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25509;&#21463;&#65311;&#26399;&#26395;&#21644;&#32463;&#39564;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
What drives the acceptance of AI technology?: the role of expectations and experiences. (arXiv:2306.13670v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13670
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#24895;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#32463;&#39564;&#12289;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;&#21644;&#36807;&#21435;&#30340;ICT&#25216;&#26415;&#32463;&#39564;&#30340;&#24433;&#21709;&#12290;&#20154;&#24037;&#26234;&#33021;&#32463;&#39564;&#21644;ICT&#32463;&#39564;&#20197;&#20004;&#31181;&#26041;&#24335;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#25509;&#21463;&#24847;&#24895;&#65292;&#20197;&#30452;&#25509;&#36884;&#24452;&#21644;&#38388;&#25509;&#36884;&#24452;&#65292;&#32780;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;&#20540;&#21017;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#25509;&#21463;&#24847;&#24895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21644;&#26381;&#21153;&#24050;&#20316;&#20026;&#35797;&#28857;&#21521;&#28508;&#22312;&#29992;&#25143;&#25552;&#20379;&#12290;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#32463;&#39564;&#12289;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;&#12289;&#20197;&#21450;&#36807;&#21435;&#30340;ICT&#25216;&#26415;&#32463;&#39564;&#37117;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#30528;&#20154;&#20204;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#24895;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#25509;&#21463;&#24847;&#24895;&#30340;&#22240;&#32032;&#65292;&#24182;&#29702;&#35299;&#20854;&#24418;&#25104;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#32463;&#39564;&#21644;&#36807;&#21435;&#30340;ICT&#32463;&#39564;&#20197;&#20004;&#31181;&#26041;&#24335;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#25509;&#21463;&#24847;&#24895;&#12290;&#36890;&#36807;&#30452;&#25509;&#36884;&#24452;&#65292;&#26356;&#39640;&#30340;&#20154;&#24037;&#26234;&#33021;&#32463;&#39564;&#21644;ICT&#32463;&#39564;&#19982;&#26356;&#22823;&#30340;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#24895;&#26377;&#20851;&#12290;&#27492;&#22806;&#65292;&#36824;&#23384;&#22312;&#19968;&#31181;&#38388;&#25509;&#36884;&#24452;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#32463;&#39564;&#21644;ICT&#32463;&#39564;&#26377;&#21161;&#20110;&#22686;&#21152;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;&#20540;&#65292;&#32780;&#36825;&#20123;&#26399;&#26395;&#20540;&#21453;&#36807;&#26469;&#25552;&#39640;&#20102;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#24895;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#20026;&#35745;&#21010;&#23454;&#26045;&#20154;&#24037;&#26234;&#33021;&#30340;&#20844;&#21496;&#21644;&#20844;&#20849;&#32452;&#32455;&#25552;&#20986;&#20102;&#20960;&#39033;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Artificial intelligence products and services have been offered potential users as pilots. The acceptance intention towards artificial intelligence is greatly influenced by the experience with current AI products and services, expectations for AI, and past experiences with ICT technology. This study aims to explore the factors that impact AI acceptance intention and understand the process of its formation. The analysis results of this study reveal that AI experience and past ICT experience affect AI acceptance intention in two ways. Through the direct path, higher AI experience and ICT experience are associated with a greater intention to accept AI. Additionally, there is an indirect path where AI experience and ICT experience contribute to increased expectations for AI, and these expectations, in turn, elevate acceptance intention. Based on the findings, several recommendations are suggested for companies and public organizations planning to implement artificial intel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#26469;&#34920;&#31034;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#19987;&#23478;&#37117;&#26377;&#25351;&#23548;&#20316;&#29992;&#65292;&#21516;&#26102;&#20063;&#23545;&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;&#21644;&#31070;&#32463;&#31526;&#21495;AI&#39046;&#22495;&#26377;&#37325;&#35201;&#30340;&#29702;&#35770;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.13660</link><description>&lt;p&gt;
&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;&#21644;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#38454;&#36923;&#36753;&#25552;&#20379;&#20102;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Statistical relational learning and neuro-symbolic AI: what does first-order logic offer?. (arXiv:2306.13660v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#26469;&#34920;&#31034;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#19987;&#23478;&#37117;&#26377;&#25351;&#23548;&#20316;&#29992;&#65292;&#21516;&#26102;&#20063;&#23545;&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;&#21644;&#31070;&#32463;&#31526;&#21495;AI&#39046;&#22495;&#26377;&#37325;&#35201;&#30340;&#29702;&#35770;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#20197;&#38750;&#25216;&#26415;&#30340;&#26041;&#24335;&#31616;&#35201;&#27010;&#36848;&#21644;&#38416;&#36848;&#20351;&#29992;&#65288;&#19968;&#38454;&#65289;&#36923;&#36753;&#26469;&#34920;&#31034;&#65288;&#27010;&#29575;&#65289;&#30693;&#35782;&#30340;&#36923;&#36753;&#21644;&#21746;&#23398;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#26377;&#19977;&#20010;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#65292;&#20102;&#35299;&#20026;&#20160;&#20040;&#30740;&#31350;&#31038;&#21306;&#20851;&#27880;&#20851;&#31995;&#34920;&#31034;&#30340;&#25991;&#31456;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#28201;&#21644;&#30340;&#20171;&#32461;&#12290;&#20854;&#27425;&#65292;&#23545;&#20110;&#36923;&#36753;&#19987;&#23478;&#26159;&#26032;&#26469;&#30340;&#23398;&#20064;&#39046;&#22495;&#65292;&#36825;&#26679;&#19968;&#31687;&#25991;&#31456;&#21487;&#20197;&#24110;&#21161;&#23548;&#33322;&#26377;&#38480;&#19982;&#26080;&#38480;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20027;&#35266;&#27010;&#29575;&#19982;&#38543;&#26426;&#19990;&#30028;&#35821;&#20041;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;&#21644;&#31070;&#32463;&#31526;&#21495;AI&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#20182;&#20204;&#36890;&#24120;&#23884;&#20837;&#22312;&#20855;&#26377;&#20027;&#35266;&#27010;&#29575;&#30340;&#26377;&#38480;&#19990;&#30028;&#20013;&#65292;&#27427;&#36175;&#26080;&#38480;&#39046;&#22495;&#21644;&#38543;&#26426;&#19990;&#30028;&#35821;&#20041;&#25152;&#24102;&#26469;&#30340;&#29702;&#35770;&#20215;&#20540;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, our aim is to briefly survey and articulate the logical and philosophical foundations of using (first-order) logic to represent (probabilistic) knowledge in a non-technical fashion. Our motivation is three fold. First, for machine learning researchers unaware of why the research community cares about relational representations, this article can serve as a gentle introduction. Second, for logical experts who are newcomers to the learning area, such an article can help in navigating the differences between finite vs infinite, and subjective probabilities vs random-world semantics. Finally, for researchers from statistical relational learning and neuro-symbolic AI, who are usually embedded in finite worlds with subjective probabilities, appreciating what infinite domains and random-world semantics brings to the table is of utmost theoretical import.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20844;&#24179;&#24615;&#23450;&#20041;&#36827;&#34892;&#24418;&#24335;&#21270;&#37325;&#26500;&#65292;&#23558;&#20854;&#24212;&#29992;&#22522;&#20110;&#35748;&#30693;&#29615;&#22659;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#23454;&#29616;&#20844;&#24179;&#21644;&#20559;&#35265;&#29702;&#35770;&#65292;&#21253;&#25324;&#19977;&#20010;&#27010;&#24565;&#65306;&#36890;&#36807;&#26080;&#24847;&#35782;&#26469;&#23454;&#29616;&#20844;&#24179;&#12289;&#20154;&#21475;&#23398;&#24179;&#31561;&#21644;&#21453;&#20107;&#23454;&#20844;&#27491;&#12290;</title><link>http://arxiv.org/abs/2306.13659</link><description>&lt;p&gt;
&#23454;&#29616;&#20844;&#24179;&#21644;&#20559;&#35265;&#30340;&#36923;&#36753;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toward A Logical Theory Of Fairness and Bias. (arXiv:2306.13659v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20844;&#24179;&#24615;&#23450;&#20041;&#36827;&#34892;&#24418;&#24335;&#21270;&#37325;&#26500;&#65292;&#23558;&#20854;&#24212;&#29992;&#22522;&#20110;&#35748;&#30693;&#29615;&#22659;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#23454;&#29616;&#20844;&#24179;&#21644;&#20559;&#35265;&#29702;&#35770;&#65292;&#21253;&#25324;&#19977;&#20010;&#27010;&#24565;&#65306;&#36890;&#36807;&#26080;&#24847;&#35782;&#26469;&#23454;&#29616;&#20844;&#24179;&#12289;&#20154;&#21475;&#23398;&#24179;&#31561;&#21644;&#21453;&#20107;&#23454;&#20844;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#22240;&#31639;&#27861;&#22312;&#21382;&#21490;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#32780;&#25918;&#22823;&#21644;&#24310;&#32493;&#21382;&#21490;&#20559;&#24046;&#65292;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20844;&#24179;&#24615;&#23450;&#20041;&#30340;&#24418;&#24335;&#37325;&#26500;&#65292;&#19981;&#26159;&#20026;&#20102;&#21462;&#20195;&#29616;&#26377;&#30340;&#23450;&#20041;&#65292;&#32780;&#26159;&#20026;&#20102;&#23558;&#23427;&#20204;&#30340;&#24212;&#29992;&#22522;&#20110;&#35748;&#30693;&#29615;&#22659;&#27169;&#22411;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#20016;&#23500;&#30340;&#29615;&#22659;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#20010;&#27010;&#24565;&#65306;&#36890;&#36807;&#26080;&#24847;&#35782;&#26469;&#23454;&#29616;&#20844;&#24179;&#12289;&#20154;&#21475;&#23398;&#24179;&#31561;&#21644;&#21453;&#20107;&#23454;&#20844;&#27491;&#65292;&#24182;&#23558;&#23427;&#20204;&#22312;&#35748;&#30693;&#24773;&#20917;&#28436;&#31639;&#20013;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in machine learning is of considerable interest in recent years owing to the propensity of algorithms trained on historical data to amplify and perpetuate historical biases. In this paper, we argue for a formal reconstruction of fairness definitions, not so much to replace existing definitions but to ground their application in an epistemic setting and allow for rich environmental modelling. Consequently we look into three notions: fairness through unawareness, demographic parity and counterfactual fairness, and formalise these in the epistemic situation calculus.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#24847;&#22270;&#20248;&#20808;&#30340;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#25903;&#25345;&#35745;&#31639;&#26426;&#20195;&#29702;&#20154;&#20043;&#38388;&#20849;&#20139;&#24847;&#22270;&#30340;&#22522;&#26412;&#26426;&#21046;&#25152;&#24517;&#39035;&#20855;&#22791;&#30340;&#29305;&#24449;&#65292;&#24182;&#25506;&#32034;&#20102;&#36825;&#20123;&#26426;&#21046;&#22914;&#20309;&#36866;&#29992;&#20110;&#20154;&#31867;&#20197;&#25552;&#20379;&#23545;&#20154;&#31867;&#29702;&#24615;&#21644;&#24847;&#35782;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.13657</link><description>&lt;p&gt;
&#35770;&#20849;&#20139;&#24847;&#22270;&#30340;&#35745;&#31639;&#26426;&#21046;&#20197;&#21450;&#26377;&#20851;&#29702;&#24615;&#21644;&#24847;&#35782;&#30340;&#25512;&#27979;
&lt;/p&gt;
&lt;p&gt;
On Computational Mechanisms for Shared Intentionality, and Speculation on Rationality and Consciousness. (arXiv:2306.13657v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#24847;&#22270;&#20248;&#20808;&#30340;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#25903;&#25345;&#35745;&#31639;&#26426;&#20195;&#29702;&#20154;&#20043;&#38388;&#20849;&#20139;&#24847;&#22270;&#30340;&#22522;&#26412;&#26426;&#21046;&#25152;&#24517;&#39035;&#20855;&#22791;&#30340;&#29305;&#24449;&#65292;&#24182;&#25506;&#32034;&#20102;&#36825;&#20123;&#26426;&#21046;&#22914;&#20309;&#36866;&#29992;&#20110;&#20154;&#31867;&#20197;&#25552;&#20379;&#23545;&#20154;&#31867;&#29702;&#24615;&#21644;&#24847;&#35782;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29420;&#29305;&#30340;&#29305;&#24449;&#20043;&#19968;&#26159;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#26032;&#39062;&#30340;&#12289;&#21512;&#20316;&#30340;&#34892;&#20026;&#25110;&#22242;&#38431;&#21512;&#20316;&#12290;&#36825;&#38656;&#35201;&#25105;&#20204;&#33021;&#22815;&#22312;&#20010;&#20307;&#20043;&#38388;&#20256;&#36798;&#30446;&#26631;&#12289;&#35745;&#21010;&#21644;&#24605;&#24819;&#65292;&#20197;&#21019;&#24314;&#20849;&#20139;&#24847;&#22270;&#12290;&#26412;&#25991;&#21033;&#29992;David Marr&#30340;&#20449;&#24687;&#22788;&#29702;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#25903;&#25345;&#35745;&#31639;&#26426;&#20195;&#29702;&#20154;&#20043;&#38388;&#20849;&#20139;&#24847;&#22270;&#30340;&#22522;&#26412;&#26426;&#21046;&#25152;&#24517;&#39035;&#20855;&#22791;&#30340;&#29305;&#24449;&#65292;&#24182;&#25351;&#20986;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#22312;&#29616;&#26377;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#20013;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#36825;&#20010;&#24605;&#32500;&#23454;&#39564;&#25152;&#24471;&#21040;&#30340;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#20154;&#31867;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19982;&#35266;&#23519;&#30456;&#31526;&#30340;&#20851;&#20110;&#20154;&#31867;&#29702;&#24615;&#12289;&#24847;&#21521;&#21644;&#24863;&#30693;&#24847;&#35782;&#30340;&#35299;&#37322;&#12290;&#36825;&#26679;&#23601;&#24418;&#25104;&#20102;&#20316;&#32773;&#25152;&#31216;&#30340;&#20849;&#20139;&#24847;&#22270;&#20248;&#20808;&#29702;&#35770;&#65288;SIFT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
A singular attribute of humankind is our ability to undertake novel, cooperative behavior, or teamwork. This requires that we can communicate goals, plans, and ideas between the brains of individuals to create shared intentionality. Using the information processing model of David Marr, I derive necessary characteristics of basic mechanisms to enable shared intentionality between computational agents and indicate how these could be implemented in present-day AI-based robots.  More speculatively, I suggest the mechanisms derived by this thought experiment apply to humans and extend to provide explanations for human rationality and aspects of intentional and phenomenal consciousness that accord with observation. This yields what I call the Shared Intentionality First Theory (SIFT) for rationality and consciousness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21098;&#26525;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#26525;&#35780;&#20998;&#26041;&#27861;DSS&#65292;&#35813;&#26041;&#27861;&#19981;&#26159;&#20026;&#20102;&#20445;&#25345;&#28304;&#20934;&#30830;&#24615;&#65292;&#32780;&#26159;&#30452;&#25509;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#21363;&#20415;&#21482;&#24341;&#20837;&#23569;&#37327;&#31232;&#30095;&#20063;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13237</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pruning for Better Domain Generalizability. (arXiv:2306.13237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21098;&#26525;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#26525;&#35780;&#20998;&#26041;&#27861;DSS&#65292;&#35813;&#26041;&#27861;&#19981;&#26159;&#20026;&#20102;&#20445;&#25345;&#28304;&#20934;&#30830;&#24615;&#65292;&#32780;&#26159;&#30452;&#25509;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#21363;&#20415;&#21482;&#24341;&#20837;&#23569;&#37327;&#31232;&#30095;&#20063;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#22914;L2&#24050;&#32463;&#21487;&#20197;&#22312;&#30446;&#26631;&#22495;&#24615;&#33021;&#19978;&#25552;&#20379;&#23567;&#24133;&#24230;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#26525;&#35780;&#20998;&#26041;&#27861;&#65292;&#31216;&#20026;DSS&#65292;&#35774;&#35745;&#19981;&#26159;&#20026;&#20102;&#20445;&#25345;&#28304;&#20934;&#30830;&#24615;&#32780;&#26159;&#30452;&#25509;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#29978;&#33267;&#21487;&#20197;&#19982;MIRO(Cha&#31561;&#20154;&#65292;2022&#24180;)&#31561;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;MNIST&#21040;MNIST-M&#19978;&#65292;&#36890;&#36807;&#23558;60%&#36890;&#36947;&#31232;&#30095;&#24341;&#20837;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#22522;&#32447;&#24615;&#33021;&#25552;&#39640;5&#20010;&#30334;&#20998;&#28857;&#20197;&#19978;&#12290;&#22312;DomainBed&#22522;&#20934;&#21644;&#26368;&#20808;&#36827;&#30340;MIRO&#19978;&#65292;&#20165;&#36890;&#36807;&#23558;10%&#31232;&#30095;&#24341;&#20837;&#27169;&#22411;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate whether we could use pruning as a reliable method to boost the generalization ability of the model. We found that existing pruning method like L2 can already offer small improvement on the target domain performance. We further propose a novel pruning scoring method, called DSS, designed not to maintain source accuracy as typical pruning work, but to directly enhance the robustness of the model. We conduct empirical experiments to validate our method and demonstrate that it can be even combined with state-of-the-art generalization work like MIRO(Cha et al., 2022) to further boost the performance. On MNIST to MNIST-M, we could improve the baseline performance by over 5 points by introducing 60% channel sparsity into the model. On DomainBed benchmark and state-of-the-art MIRO, we can further boost its performance by 1 point only by introducing 10% sparsity into the model. Code can be found at: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13004</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;?
&lt;/p&gt;
&lt;p&gt;
Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20154;&#30340;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#26694;&#26550;&#20351;&#29992;&#40657;&#30418;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#29366;&#24577;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;Cartpole&#12289;&#35270;&#35273;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#21644;Atari&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DDT&#23398;&#20064;&#21487;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26641;&#24418;&#32467;&#26500;&#26377;&#21161;&#20110;&#30830;&#23450;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#31243;&#24230;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;DDT&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26641;&#30340;&#31163;&#25955;&#24615;&#20250;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#29983;&#25104;&#23545;&#24212;&#30340;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#39046;&#20808;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#40065;&#26834;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11915</link><description>&lt;p&gt;
&#22270;&#20998;&#31867;&#38382;&#39064;&#20013;&#32467;&#26500;&#24863;&#30693;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Structure-Aware Robustness Certificates for Graph Classification. (arXiv:2306.11915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11915
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#29983;&#25104;&#23545;&#24212;&#30340;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#39046;&#20808;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#40065;&#26834;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20445;&#35777;&#23433;&#20840;&#24615;&#30340;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#29992;&#20110;&#22270;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35777;&#26126;&#20445;&#35777;&#19982;&#33410;&#28857;&#23545;&#32763;&#36716;&#65288;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#32536;&#65289;&#30340;&#24635;&#25968;&#26377;&#20851;&#65292;&#36825;&#30456;&#24403;&#20110;&#20197;&#37051;&#25509;&#30697;&#38453;&#20026;&#20013;&#24515;&#30340;l0&#29699;&#12290;&#23613;&#31649;&#20174;&#29702;&#35770;&#19978;&#30475;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#36825;&#31181;&#21508;&#21521;&#21516;&#24615;&#30340;&#32467;&#26500;&#22122;&#22768;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#65292;&#22240;&#20026;&#26377;&#20123;&#33410;&#28857;&#23545;&#20110;&#30830;&#23450;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#26356;&#20026;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35777;&#20070;&#32473;&#20986;&#20102;&#23545;&#22270;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24754;&#35266;&#25551;&#36848;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#21508;&#21521;&#21516;&#24615;&#30340;&#22122;&#22768;&#20998;&#24067;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36807;&#31243;&#20026;&#20998;&#31867;&#22120;&#29983;&#25104;&#20102;&#32467;&#26500;&#24863;&#30693;&#30340;&#35777;&#20070;&#65292;&#22240;&#27492;&#40065;&#26834;&#24615;&#35777;&#20070;&#30340;&#22823;&#23567;&#21487;&#20197;&#22312;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#20043;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certifying the robustness of a graph-based machine learning model poses a critical challenge for safety. Current robustness certificates for graph classifiers guarantee output invariance with respect to the total number of node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$ ball centred on the adjacency matrix. Although theoretically attractive, this type of isotropic structural noise can be too restrictive in practical scenarios where some node pairs are more critical than others in determining the classifier's output. The certificate, in this case, gives a pessimistic depiction of the robustness of the graph model. To tackle this issue, we develop a randomised smoothing method based on adding an anisotropic noise distribution to the input graph structure. We show that our process generates structural-aware certificates for our classifiers, whereby the magnitude of robustness certificates can vary across different pre-defined structures of the graph. We demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;All-Positive (AP)&#28608;&#27963;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#35777;&#25454;&#28608;&#27963;&#20989;&#25968;&#20013;&#30340;&#38646;&#35777;&#25454;&#21306;&#22495;&#65292;&#21516;&#26102;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#36798;&#36127;&#35777;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.11113</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#20013;&#32047;&#31215;&#35777;&#25454;&#65306;&#29702;&#35770;&#19982;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Learn to Accumulate Evidence from All Training Samples: Theory and Practice. (arXiv:2306.11113v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;All-Positive (AP)&#28608;&#27963;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#35777;&#25454;&#28608;&#27963;&#20989;&#25968;&#20013;&#30340;&#38646;&#35777;&#25454;&#21306;&#22495;&#65292;&#21516;&#26102;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#36798;&#36127;&#35777;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#29702;&#35770;&#21644;&#20027;&#35266;&#36923;&#36753;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30830;&#23450;&#24615;&#31070;&#32463;&#32593;&#32476;&#21464;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340;&#35777;&#25454;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#35777;&#25454;&#37327;&#21270;&#32454;&#31890;&#24230;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#35777;&#25454;&#27169;&#22411;&#65292;&#35777;&#25454;&#38656;&#35201;&#26159;&#38750;&#36127;&#30340;&#65292;&#36825;&#38656;&#35201;&#20351;&#29992;&#29305;&#27530;&#30340;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#36825;&#20010;&#38480;&#21046;&#36890;&#24120;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#19981;&#22914;&#26631;&#20934;&#30340;softmax&#27169;&#22411;&#65292;&#20351;&#24471;&#23558;&#23427;&#20204;&#25193;&#23637;&#21040;&#35768;&#22810;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#31181;&#19981;&#33391;&#34892;&#20026;&#30340;&#30495;&#27491;&#21407;&#22240;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#35777;&#25454;&#27169;&#22411;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#26681;&#26412;&#38480;&#21046;&#65292;&#23427;&#35299;&#37322;&#20102;&#19981;&#33391;&#24615;&#33021;&#65306;&#29616;&#26377;&#30340;&#35777;&#26126;&#28608;&#27963;&#20989;&#25968;&#21019;&#24314;&#20102;&#38646;&#35777;&#25454;&#21306;&#22495;&#65292;&#36825;&#38459;&#27490;&#20102;&#27169;&#22411;&#20174;&#33853;&#20837;&#36825;&#20123;&#21306;&#22495;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#20064;&#12290;&#23545;&#35777;&#25454;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#34987;&#35270;&#20026;&#23558;&#21407;&#22987;&#36755;&#20837;&#36716;&#25442;&#20026;&#35777;&#25454;&#37327;&#65292;&#28982;&#21518;&#20351;&#29992;&#20449;&#24565;&#26356;&#26032;&#35268;&#21017;&#36827;&#34892;&#32858;&#21512;&#30340;&#36716;&#25442;&#22120;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#31216;&#20026;&#20840;&#27491;&#28608;&#27963;&#65288;All-Positive&#65292;AP&#65289;&#65292;&#23427;&#36890;&#36807;&#26500;&#36896;&#36991;&#20813;&#38646;&#35777;&#25454;&#21306;&#22495;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35777;&#25454;&#28608;&#27963;&#20989;&#25968;&#22312;&#35768;&#22810;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;AP&#28608;&#27963;&#20943;&#23569;&#20102;ReLU&#28608;&#27963;&#65292;&#20174;&#32780;&#24674;&#22797;&#20102;ReLU&#30340;&#33391;&#22909;&#27491;&#23450;&#24615;&#36136;&#65292;&#24182;&#19988;&#23427;&#30340;&#27867;&#21270;&#20801;&#35768;&#36127;&#35777;&#25454;&#37327;&#65292;&#22240;&#27492;&#27604;&#29616;&#26377;&#30340;&#35777;&#25454;&#28608;&#27963;&#26356;&#20855;&#34920;&#29616;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#36807;&#22810;&#26631;&#35760;&#26679;&#26412;&#12290;&#36890;&#36807;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10191</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#29992;&#20110;&#23567;&#26679;&#26412;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Priming for Sample-Efficient Adaptation. (arXiv:2306.10191v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#36807;&#22810;&#26631;&#35760;&#26679;&#26412;&#12290;&#36890;&#36807;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#26410;&#32463;&#36807;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#32473;&#23450;&#31867;&#21517;&#25110;&#26080;&#26631;&#31614;&#27979;&#35797;&#26679;&#26412;&#26102;&#65292;&#31070;&#32463;&#21551;&#21160;&#21487;&#20197;&#20351;&#27169;&#22411;&#22238;&#24518;&#36215;&#39044;&#35757;&#32451;&#26399;&#38388;&#30475;&#21040;&#30340;&#30456;&#20851;&#25968;&#25454;&#24182;&#20197;&#27492;&#20026;&#22522;&#30784;&#26465;&#20214;&#21270;&#20854;&#21442;&#25968;&#65292;&#20174;&#32780;&#20351;&#20854;&#38024;&#23545;&#27979;&#35797;&#20998;&#24067;&#20570;&#22909;&#20934;&#22791;&#12290;&#31070;&#32463;&#21551;&#21160;&#36824;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#65292;&#21363;&#20351;&#26159;&#38024;&#23545;&#22914;LAION-2B&#36825;&#26679;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#22312;&#21508;&#31181;&#20998;&#24067;&#21464;&#21270;&#21644;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23545;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30475;&#21040;ImageNet&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2.45&#65285;&#65292;&#22312;&#26631;&#20934;&#30340;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;3.81&#65285;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#31070;&#32463;&#21551;&#21160;&#26469;&#36866;&#24212;&#20998;&#24067;&#21464;&#21270;&#65292;&#25105;&#20204;&#30475;&#21040;ImageNetV2&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;1.41&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#31070;&#32463;&#21551;&#21160;&#22312;&#22788;&#29702;&#23567;&#26679;&#26412;&#33258;&#36866;&#24212;&#25361;&#25112;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Neural Priming, a technique for adapting large pretrained models to distribution shifts and downstream tasks given few or no labeled examples. Presented with class names or unlabeled test samples, Neural Priming enables the model to recall and conditions its parameters on relevant data seen throughout pretraining, thereby priming it for the test distribution. Neural Priming can be performed at test time, even for pretraining datasets as large as LAION-2B. Performing lightweight updates on the recalled data significantly improves accuracy across a variety of distribution shift and transfer learning benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in accuracy on ImageNet and 3.81% accuracy improvement on average across standard transfer learning benchmarks. Further, using Neural Priming at inference to adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2. These results demonstrate the effectiveness of Neural Priming in addr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09869</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#35821;&#20041;&#38169;&#20301;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#27599;&#20010;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#21046;&#23450;&#28508;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;EBM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#33719;&#24471;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#23545;&#25968;&#21518;&#39564;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#26032;&#21644;&#36716;&#31227;&#21040;&#21518;&#32493;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#23884;&#22871;&#23618;&#27425;&#30340;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#28508;&#22312;EBMs&#36824;&#20801;&#35768;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#65292;&#21363;&#36890;&#36807;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#26412;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#38169;&#20301;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09364</link><description>&lt;p&gt;
TSMixer: &#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09364
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22240;&#20854;&#33021;&#22815;&#25429;&#25417;&#38271;&#24207;&#21015;&#20132;&#20114;&#32780;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20854;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#23545;&#38271;&#26399;&#39044;&#27979;&#26500;&#25104;&#20102;&#20005;&#37325;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TSMixer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#26550;&#26500;&#65292;&#19987;&#20026;&#22810;&#20803;&#39044;&#27979;&#21644;&#34917;&#19969;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#26159;Transformers&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20511;&#37492;&#20102;MLP-Mixer&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25104;&#21151;&#32463;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#35270;&#35273;MLP-Mixer&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#32452;&#20214;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#21363;&#23558;&#22312;&#32447;&#21327;&#35843;&#22836;&#38468;&#21152;&#21040;MLP-Mixer&#39592;&#24178;&#19978;&#65292;&#20197;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#23646;&#24615;&#65292;&#22914;&#23618;&#27425;&#32467;&#26500;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36890;&#36947;&#24314;&#27169;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;&#32534;&#30721;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#36890;&#36947;&#21644;&#20445;&#30041;&#21333;&#20010;&#36890;&#36947;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TSMixer&#22312;&#19968;&#20803;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#22522;&#20110;Transformers&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32763;&#35793;&#20102;MIT&#25968;&#23398;&#21644;EECS&#35838;&#31243;&#20013;&#30340;4550&#20010;&#39064;&#30446;&#65292;&#24320;&#21457;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#35780;&#20998;&#30340;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#35838;&#31243;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.08997</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;MIT&#25968;&#23398;&#21644;EECS&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. (arXiv:2306.08997v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08997
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32763;&#35793;&#20102;MIT&#25968;&#23398;&#21644;EECS&#35838;&#31243;&#20013;&#30340;4550&#20010;&#39064;&#30446;&#65292;&#24320;&#21457;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#35780;&#20998;&#30340;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#35838;&#31243;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20102;&#33719;&#21462;&#23398;&#20301;&#25152;&#38656;&#30340;&#25152;&#26377;MIT&#25968;&#23398;&#21644;&#30005;&#27668;&#24037;&#31243;&#21450;&#35745;&#31639;&#26426;&#31185;&#23398;&#65288;EECS&#65289;&#35838;&#31243;&#30340;&#39064;&#30446;&#38598;&#12289;&#26399;&#20013;&#32771;&#35797;&#21644;&#26399;&#26411;&#32771;&#35797;&#20013;&#30340;4550&#20010;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20219;&#20309;MIT&#25968;&#23398;&#21644;EECS&#19987;&#19994;&#27605;&#19994;&#35201;&#27714;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#25104;&#21151;&#35299;&#20915;&#20102;&#25972;&#20010;MIT&#35838;&#31243;&#30340;&#19977;&#20998;&#20043;&#19968;&#65292;&#32780;GPT-4&#22312;&#39064;&#30446;&#20013;&#19981;&#21253;&#21547;&#22270;&#20687;&#30340;&#27979;&#35797;&#38598;&#19978;&#32463;&#36807;&#25552;&#31034;&#24037;&#31243;&#21518;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#35299;&#20915;&#29575;&#12290;&#25105;&#20204;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#37319;&#29992;GPT-4&#33258;&#21160;&#35780;&#20998;&#65292;&#25552;&#20379;&#20102;&#35838;&#31243;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#31867;&#22411;&#30340;&#35814;&#32454;&#24615;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#23884;&#20837;&#20302;&#32500;&#31354;&#38388;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38382;&#39064;&#12289;&#20027;&#39064;&#21644;&#35838;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#21738;&#20123;&#38382;&#39064;&#21644;&#35838;&#31243;&#26159;&#35299;&#20915;&#20854;&#20182;&#38382;&#39064;&#21644;&#35838;&#31243;&#25152;&#24517;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We curate a comprehensive dataset of 4,550 questions and solutions from problem sets, midterm exams, and final exams across all MIT Mathematics and Electrical Engineering and Computer Science (EECS) courses required for obtaining a degree. We evaluate the ability of large language models to fulfill the graduation requirements for any MIT major in Mathematics and EECS. Our results demonstrate that GPT-3.5 successfully solves a third of the entire MIT curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate on a test set excluding questions based on images. We fine-tune an open-source large language model on this dataset. We employ GPT-4 to automatically grade model responses, providing a detailed performance breakdown by course, question, and answer type. By embedding questions in a low-dimensional space, we explore the relationships between questions, topics, and classes and discover which questions and classes are required for solving other questions and classes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.06202</link><description>&lt;p&gt;
NeuroGraph:&#38754;&#21521;&#33041;&#36830;&#25509;&#32452;&#23398;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#20998;&#26512;&#39640;&#32500;&#21151;&#33021;&#24615;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#39044;&#27979;&#21508;&#31181;&#31070;&#32463;&#30142;&#30149;&#12289;&#31934;&#31070;&#38556;&#30861;&#21644;&#35748;&#30693;&#27169;&#24335;&#26377;&#25928;&#12290;&#22312;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#30740;&#31350;&#20013;&#65292;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#12290;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#35777;&#23454;&#65292;&#26631;&#24535;&#30528;&#25968;&#25454;&#35299;&#37322;&#21644;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#36716;&#21464;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#22270;&#24418;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#24191;&#27867;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#21644;&#22823;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#20013;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#36716;&#25442;&#20173;&#28982;&#21463;&#21040;&#24847;&#22806;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeuroGraph(&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#38598;)&#65292;&#23427;&#28085;&#30422;&#20102;&#22810;&#20010;&#34892;&#20026;&#21644;&#35748;&#30693;&#29305;&#24449;&#31867;&#21035;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05716</link><description>&lt;p&gt;
&#20026;&#25235;&#20303;&#20219;&#20309;&#29289;&#21697;&#38138;&#24179;&#36947;&#36335;&#65306;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#36890;&#29992;&#25235;&#21462;&#25918;&#32622;&#26426;&#22120;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots. (arXiv:2306.05716v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#19968;&#30452;&#26159;&#30740;&#31350;&#31038;&#21306;&#38271;&#26399;&#36861;&#27714;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25910;&#38598;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25968;&#25454;&#65292;&#22914; RT-1 &#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#26032;&#23545;&#35937;&#21644;&#22810;&#26679;&#32972;&#26223;&#30340;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20197;&#35299;&#20915;&#26085;&#24120;&#22330;&#26223;&#20013;&#24191;&#27867;&#30340;&#25342;&#25918;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#25513;&#27169;&#20256;&#36798;&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#20960;&#20309;&#24418;&#29366;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#31574;&#30053;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24863;&#30693;&#20934;&#30830;&#30340;&#29289;&#20307;&#23039;&#24577;&#24182;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;&#26032;&#23545;&#35937;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#21487;&#20197;&#23454;&#29616;&#22312;&#35757;&#32451;&#26102;&#35266;&#23519;&#21040;&#30456;&#20284;&#24418;&#29366;&#30340;&#26032;&#29289;&#20307;&#30340;&#25235;&#21462;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the generalization capabilities of general-purpose robotic agents has long been a significant challenge actively pursued by research communities. Existing approaches often rely on collecting large-scale real-world robotic data, such as the RT-1 dataset. However, these approaches typically suffer from low efficiency, limiting their capability in open-domain scenarios with new objects, and diverse backgrounds. In this paper, we propose a novel paradigm that effectively leverages language-grounded segmentation masks generated by state-of-the-art foundation models, to address a wide range of pick-and-place robot manipulation tasks in everyday scenarios. By integrating precise semantics and geometries conveyed from masks into our multi-view policy model, our approach can perceive accurate object poses and enable sample-efficient learning. Besides, such design facilitates effective generalization for grasping new objects with similar shapes observed during training. Our approach co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;FHE&#21152;&#23494;&#25216;&#26415;&#65292;&#26082;&#21487;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#30340;&#38544;&#31169;&#65292;&#21448;&#21487;&#20197;&#38450;&#27490;&#24694;&#24847;&#29992;&#25143;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05112</link><description>&lt;p&gt;
FheFL&#65306;&#25903;&#25345;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#19982;&#25308;&#21344;&#24237;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;
FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users. (arXiv:2306.05112v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;FHE&#21152;&#23494;&#25216;&#26415;&#65292;&#26082;&#21487;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#30340;&#38544;&#31169;&#65292;&#21448;&#21487;&#20197;&#38450;&#27490;&#24694;&#24847;&#29992;&#25143;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25216;&#26415;&#26368;&#21021;&#26159;&#20026;&#20102;&#32531;&#35299;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#12290;&#23613;&#31649;FL&#30830;&#20445;&#29992;&#25143;&#30340;&#25968;&#25454;&#22987;&#32456;&#20445;&#30041;&#22312;&#29992;&#25143;&#25163;&#20013;&#65292;&#20294;&#23616;&#37096;&#35757;&#32451;&#27169;&#22411;&#30340;&#26799;&#24230;&#24517;&#39035;&#19982;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#36890;&#20449;&#20197;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#65292;&#20351;&#24471;&#26381;&#21153;&#22120;&#21487;&#20197;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#25512;&#26029;&#20986;&#29992;&#25143;&#25968;&#25454;&#30340;&#31169;&#23494;&#20449;&#24687;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#32570;&#38519;&#65292;&#19979;&#19968;&#20195;FL&#26550;&#26500;&#25552;&#20986;&#20102;&#21152;&#23494;&#21644;&#21311;&#21517;&#21270;&#25216;&#26415;&#65292;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#20813;&#21463;&#26381;&#21153;&#22120;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#24102;&#26469;&#20854;&#20182;&#25361;&#25112;&#65292;&#20363;&#22914;&#24694;&#24847;&#29992;&#25143;&#21487;&#33021;&#36890;&#36807;&#20849;&#20139;&#34394;&#20551;&#26799;&#24230;&#26469;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#26799;&#24230;&#34987;&#21152;&#23494;&#65292;&#26381;&#21153;&#22120;&#26080;&#27861;&#35782;&#21035;&#21644;&#25490;&#38500;&#19981;&#33391;&#29992;&#25143;&#20197;&#20445;&#25252;&#20840;&#23616;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#31181;&#25915;&#20987;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#65288;FHE&#65289;&#30340;&#26032;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The federated learning (FL) technique was initially developed to mitigate data privacy issues that can arise in the traditional machine learning paradigm. While FL ensures that a user's data always remain with the user, the gradients of the locally trained models must be communicated with the centralized server to build the global model. This results in privacy leakage, where the server can infer private information of the users' data from the shared gradients. To mitigate this flaw, the next-generation FL architectures proposed encryption and anonymization techniques to protect the model updates from the server. However, this approach creates other challenges, such as a malicious user might sabotage the global model by sharing false gradients. Since the gradients are encrypted, the server is unable to identify and eliminate rogue users which would protect the global model. Therefore, to mitigate both attacks, this paper proposes a novel fully homomorphic encryption (FHE) based scheme 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03763</link><description>&lt;p&gt;
ChatGPT&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20174;&#26102;&#38388;&#25991;&#26412;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#36130;&#32463;&#26032;&#38395;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#32467;&#26500;&#30340;&#28508;&#21147;&#20173;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#22270;&#25512;&#26029;&#33021;&#21147;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#32467;&#26500;&#34701;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36827;&#34892;&#21518;&#32493;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#30340;&#20135;&#20986;&#26500;&#24314;&#30340;&#32452;&#21512;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#24180;&#21270;&#32047;&#35745;&#22238;&#25253;&#12289;&#26356;&#20302;&#30340;&#27874;&#21160;&#24615;&#21644;&#26368;&#22823;&#22238;&#25764;&#12290;&#36825;&#31181;&#21331;&#36234;&#34920;&#29616;&#31361;&#26174;&#20102;ChatGPT&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00739</link><description>&lt;p&gt;
SQL-PaLM&#65306;&#38024;&#23545;Text-to-SQL&#30340;&#25913;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#21151;&#33021;&#26159;&#29983;&#25104;&#20195;&#30721;&#65292;&#21253;&#25324;&#29992;&#20110;&#25968;&#25454;&#24211;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#12290;&#23545;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#36716;&#25442;&#20026;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#21363;Text-to-SQL&#65292;LLMs&#30340;&#36866;&#24212;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20351;&#29992;&#30340;&#36866;&#24212;&#24615;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#21033;&#29992;&#20102;PaLM-2&#65292;&#25512;&#21160;&#20102;&#20004;&#31181;&#35774;&#32622;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;Few-shot SQL-PaLM&#22522;&#20110;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#22312;Spider&#19978;&#23454;&#29616;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#26174;&#30528;&#36739;&#22823;&#30340;&#24494;&#35843;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;SQL-PALM&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;1%&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;SQL-PaLM&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#20854;&#23545;&#20854;&#20182;&#25361;&#25112;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18758</link><description>&lt;p&gt;
&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#27599;&#31867;&#20855;&#26377;&#36275;&#22815;&#26631;&#35760;&#33410;&#28857;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#19981;&#26159;&#25152;&#26377;&#31867;&#37117;&#26377;&#35768;&#22810;&#26631;&#35760;&#33410;&#28857;&#65292;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#20998;&#31867;&#26032;&#31867;&#21035;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#35760;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;GNN&#38656;&#35201;&#33021;&#22815;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#33410;&#28857;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#65292;&#31216;&#20026;Few-shot&#33410;&#28857;&#20998;&#31867;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#21095;&#38598;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;Few-shot&#33410;&#28857;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#20165;&#26377;&#22810;&#26679;&#30340;&#35757;&#32451;&#20803;&#20219;&#21153;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;Few-shot&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;TEG&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23558;&#26723;&#26696;&#21387;&#32553;&#20026;&#21333;&#20010;&#23545;&#20110;&#31574;&#30053;&#21442;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;13&#20493;&#30340;&#21387;&#32553;&#27604;&#29575;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;98%&#30340;&#21407;&#22987;&#22238;&#25253;&#21644;89%&#30340;&#21407;&#22987;&#35206;&#30422;&#29575;&#65292;&#24182;&#20801;&#35768;&#28789;&#27963;&#36873;&#25321;&#21644;&#25490;&#24207;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.18738</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#34892;&#20026;&#22810;&#26679;&#21270;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Generating Behaviorally Diverse Policies with Latent Diffusion Models. (arXiv:2305.18738v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23558;&#26723;&#26696;&#21387;&#32553;&#20026;&#21333;&#20010;&#23545;&#20110;&#31574;&#30053;&#21442;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;13&#20493;&#30340;&#21387;&#32553;&#27604;&#29575;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;98%&#30340;&#21407;&#22987;&#22238;&#25253;&#21644;89%&#30340;&#21407;&#22987;&#35206;&#30422;&#29575;&#65292;&#24182;&#20801;&#35768;&#28789;&#27963;&#36873;&#25321;&#21644;&#25490;&#24207;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#21697;&#36136;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;QD-RL&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20351;&#24471;&#23398;&#20064;&#34892;&#20026;&#22810;&#26679;&#21270;&#12289;&#39640;&#24615;&#33021;&#30340;&#31574;&#30053;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#23384;&#20648;&#25968;&#21315;&#20010;&#31574;&#30053;&#65292;&#23548;&#33268;&#31354;&#38388;&#22797;&#26434;&#24230;&#39640;&#19988;&#38590;&#20197;&#36866;&#24212;&#26356;&#22810;&#34892;&#20026;&#30340;&#25193;&#23637;&#12290;&#23558;&#26723;&#26696;&#21387;&#32553;&#20026;&#21333;&#20010;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#31574;&#30053;&#38598;&#30340;&#24615;&#33021;&#21644;&#35206;&#30422;&#29575;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23558;&#24402;&#26723;&#21387;&#32553;&#20026;&#21333;&#20010;&#23545;&#20110;&#31574;&#30053;&#21442;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;13&#20493;&#30340;&#21387;&#32553;&#27604;&#29575;&#65292;&#21516;&#26102;&#24674;&#22797;&#20102;98%&#30340;&#21407;&#22987;&#22238;&#25253;&#21644;89%&#30340;&#21407;&#22987;&#35206;&#30422;&#29575;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#35843;&#33410;&#26426;&#21046;&#36824;&#20801;&#35768;&#28789;&#27963;&#36873;&#25321;&#21644;&#25490;&#24207;&#34892;&#20026;&#65292;&#21253;&#25324;&#20351;&#29992;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of the original collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original coverage. Further, the conditioning mechanism of diffusion models allows for flexibly selecting and sequencing behaviors, including using language. Project website: https://sites.google.com/view/policydiffusion/home
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#26469;&#22788;&#29702;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;NTS&#25968;&#25454;&#20013;&#30340;&#22270;&#21160;&#24577;&#21644;&#32570;&#22833;&#36793;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18612</link><description>&lt;p&gt;
&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders. (arXiv:2305.18612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#26469;&#22788;&#29702;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;NTS&#25968;&#25454;&#20013;&#30340;&#22270;&#21160;&#24577;&#21644;&#32570;&#22833;&#36793;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26159;&#36817;&#24180;&#26469;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#22823;&#31867;&#65292;&#21253;&#25324;&#65288;1&#65289;&#20027;&#35201;&#20851;&#27880;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#30340;&#28145;&#24230;&#36882;&#24402;&#25110;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;MTS&#22266;&#26377;&#22270;&#32467;&#26500;&#30340;&#25299;&#25169;&#20449;&#24687;&#20316;&#20026;&#25554;&#34917;&#30340;&#20851;&#31995;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#20102;&#25299;&#25169;&#20449;&#24687;&#65292;&#35201;&#20040;&#20551;&#23450;&#22270;&#32467;&#26500;&#22266;&#23450;&#19988;&#20934;&#30830;&#24050;&#30693;&#12290;&#22240;&#27492;&#65292;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;NTS&#65289;&#25968;&#25454;&#20013;&#65292;&#23427;&#20204;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22270;&#21160;&#24577;&#36827;&#34892;&#31934;&#30830;&#30340;&#25554;&#34917;&#65292;&#20854;&#20013;&#24213;&#23618;&#22270;&#19981;&#26029;&#21464;&#21270;&#24182;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#36793;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21253;&#21547;&#33410;&#28857;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#20013;&#32570;&#22833;&#20540;&#30340;NTS&#25554;&#34917;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PGE-VAE&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#23450;&#20301;&#32534;&#30721;&#25216;&#26415;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#21512;&#24182;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#22270;&#20013;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#21644;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22270;&#29983;&#25104;&#32593;&#32476;&#26469;&#23398;&#20064;&#22270;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#30340;&#36793;&#24182;&#36866;&#24212;&#22270;&#21160;&#24577;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#65288;&#31216;&#20026;Cauchy&#27493;&#24577;&#65289;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#26222;&#36941;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15559</link><description>&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#30340;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#26222;&#36941;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse square Levy walk emerging universally in goal-oriented tasks. (arXiv:2305.15559v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#65288;&#31216;&#20026;Cauchy&#27493;&#24577;&#65289;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#26222;&#36941;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Levy&#27493;&#24577;&#20013;&#65292;&#27493;&#38271;&#20986;&#29616;&#39057;&#29575;&#36981;&#24490;&#24130;&#24459;&#20998;&#24067;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#29983;&#29289;&#30340;&#36801;&#31227;&#34892;&#20026;&#20013;&#35266;&#23519;&#21040;&#12290;&#35266;&#23519;&#21040;&#20102;&#25509;&#36817;&#20110;2&#30340;&#24130;&#25351;&#25968;&#30340;Levy&#27493;&#24577;&#65292;&#20294;&#20854;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26222;&#36941;&#20135;&#29983;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#65288;&#31216;&#20026;Cauchy&#27493;&#24577;&#65289;&#30340;&#27169;&#22411;&#65292;&#24182;&#30830;&#23450;&#20986;Cauchy&#27493;&#24577;&#20986;&#29616;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#30446;&#26631;&#23548;&#21521;&#30340;&#20219;&#21153;&#20013;&#65292;Cauchy&#27493;&#24577;&#26222;&#36941;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#26415;&#35821;&#8220;&#30446;&#26631;&#23548;&#21521;&#8221;&#65292;&#24403;&#30446;&#26631;&#26126;&#30830;&#26102;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#24335;&#23454;&#29616;&#65292;&#32780;&#26080;&#27861;&#30830;&#23450;&#21807;&#19968;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#65292;&#19968;&#20010;&#20195;&#29702;&#35266;&#23519;&#21040;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#20174;&#27010;&#29575;&#20998;&#24067;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#24182;&#36830;&#32493;&#20272;&#35745;&#35813;&#27010;&#29575;&#20998;&#24067;&#30340;&#20013;&#24515;&#22352;&#26631;&#12290;&#20195;&#29702;&#26377;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#24182;&#21487;&#20197;&#20462;&#25913;&#35813;&#27169;&#22411;&#65292;&#20197;&#20351;&#20854;&#26356;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Levy walk in which the frequency of occurrence of step lengths follows a power-law distribution, can be observed in the migratory behavior of organisms at various levels. Levy walks with power exponents close to 2 are observed, and the reasons are unclear. This study aims to propose a model that universally generates inverse square Levy walks (called Cauchy walks) and to identify the conditions under which Cauchy walks appear. We demonstrate that Cauchy walks emerge universally in goal-oriented tasks. We use the term "goal-oriented" when the goal is clear, but this can be achieved in different ways, which cannot be uniquely determined. We performed a simulation in which an agent observed the data generated from a probability distribution in a two-dimensional space and successively estimated the central coordinates of that probability distribution. The agent has a model of probability distribution as a hypothesis for data-generating distribution and can modify the model such that ea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.06446</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;: &#24322;&#27493;&#36890;&#20449;&#21644;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#35774;&#32622;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#20197;&#21512;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#21516;&#26102;&#30830;&#20445;&#21512;&#20316;&#20248;&#21183;&#19988;&#36890;&#20449;&#24320;&#38144;&#20302;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377; $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ &#30340;&#36951;&#25022;&#20540;&#21644; $\tilde{\mathcal{O}}(dHM^2)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013; $d$ &#26159;&#29305;&#24449;&#32500;&#25968;&#65292;$H$ &#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$M$ &#26159;&#26234;&#33021;&#20307;&#24635;&#25968;&#65292;$K$ &#26159;&#24635;&#24773;&#33410;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#38480;&#35777;&#26126;&#65292;&#34920;&#26126;&#36890;&#36807;&#21327;&#20316;&#33267;&#23569;&#38656;&#35201; $\Omega(dM)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#25165;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;</title><link>http://arxiv.org/abs/2305.02279</link><description>&lt;p&gt;
Learngene: &#20174;&#31062;&#20808;&#27169;&#22411;&#20013;&#32487;&#25215;&#21387;&#32553;&#30693;&#35782;&#21040;&#21518;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#29983;&#29289;&#30340;&#36830;&#32493;&#36827;&#21270;&#36807;&#31243;&#20013;&#65292;&#23427;&#30340;&#22522;&#22240;&#31215;&#32047;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#21644;&#30693;&#35782;&#65292;&#20351;&#26032;&#29983;&#21518;&#20195;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#20854;&#29305;&#23450;&#29615;&#22659;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#21363; Learngene&#65292;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#34701;&#21512;&#22522;&#22240;&#30340;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#12290; (i) &#31215;&#32047;&#65306;&#30693;&#35782;&#22312;&#31062;&#20808;&#27169;&#22411;&#30340;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#31215;&#32047;&#12290; (ii) &#21387;&#32553;&#65306;&#23558;&#31215;&#32047;&#30340;&#35814;&#23613;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#65292;&#21363; Learngene&#12290; (iii) &#32487;&#25215;&#65306;&#23558;&#21387;&#32553;&#30340; Learngene &#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;&#12290;&#30001;&#20110;&#31215;&#32047;&#24050;&#22312;&#19968;&#20123;&#25104;&#29087;&#30340;&#33539;&#24335;&#20013;&#24471;&#21040;&#30740;&#31350;&#65292;&#22914;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#32456;&#36523;&#23398;&#20064;&#65292;&#22240;&#27492;&#25105;&#20204;&#19987;&#27880;&#20110;&#21387;&#32553;&#21644;&#32487;&#25215;&#65292;&#36825;&#24341;&#21457;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#20026;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm \textit{Learngene} to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an \textbf{ancestry model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into a much more compact information piece, \ie \textbf{learngene}. (iii): Inheriting: the condensed \textbf{learngene} is inherited to make it easier for \textbf{descendant models} to adapt to new environments. Since accumulating has been studied in some well-developed paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these is
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;&#20219;&#21153;&#21046;&#23450;&#12289;&#25968;&#25454;&#29983;&#25104;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#26368;&#32456;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.01505</link><description>&lt;p&gt;
&#36229;&#36234;&#20998;&#31867;&#65306;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36130;&#21153;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. (arXiv:2305.01505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;&#20219;&#21153;&#21046;&#23450;&#12289;&#25968;&#25454;&#29983;&#25104;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#26368;&#32456;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;1000&#20159;&#21450;&#20197;&#19978;&#30340;&#21442;&#25968;&#32452;&#25104;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36890;&#29992;&#30340;&#36827;&#23637;&#24212;&#29992;&#22312;&#24456;&#23569;&#39046;&#22495;&#20013;&#65292;&#20363;&#22914;&#20020;&#24202;&#25110;&#27861;&#24459;&#39046;&#22495;&#65292;&#32780;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#22522;&#26412;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;LLMs&#35299;&#20915;&#36130;&#21153;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#20174;&#26410;&#34987;&#30740;&#31350;&#36807;&#65292;&#24182;&#19988;&#23427;&#26159;&#21542;&#21487;&#20197;&#22312;&#20219;&#20309;&#35268;&#27169;&#19978;&#23436;&#25104;&#20173;&#26410;&#30693;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23545;LLMs&#22312;&#36130;&#21153;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#35843;&#26597;&#21253;&#25324;&#23545;&#19968;&#31995;&#21015;&#20027;&#39064;&#30340;&#35814;&#32454;&#25506;&#35752;&#65292;&#21253;&#25324;&#20219;&#21153;&#21046;&#23450;&#65292;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26377;&#26080;&#25351;&#23548;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), consisting of 100 billion or more parameters, have demonstrated remarkable ability in complex multi-step reasoning tasks. However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored. To the best of our knowledge, the ability of LLMs to solve financial reasoning problems has never been dealt with, and whether it can be performed at any scale remains unknown. To address this knowledge gap, this research presents a comprehensive investigation into the potential application of LLMs in the financial domain. The investigation includes a detailed exploration of a range of subjects, including task formulation, synthetic data generation, prompting methods, and evaluation capability. Furthermore, the study benchmarks various GPT variants with parameter scales ranging from 2.8B to 13B, with and without instruction tuning, on diverse dataset sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00070</link><description>&lt;p&gt;
&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#31216;&#20026;&#22312;&#32447;Platt&#32553;&#25918;(OPS)&#65292;&#23427;&#23558;Platt&#32553;&#25918;&#25216;&#26415;&#19982;&#22312;&#32447;&#36923;&#36753;&#22238;&#24402;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OPS&#22914;&#20309;&#22312;&#20998;&#24067;&#28418;&#31227;&#30340;i.i.d.&#21644;&#38750;i.i.d.&#24773;&#20917;&#19979;&#24179;&#31283;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#24403;&#26368;&#20339;&#30340;Platt&#32553;&#25918;&#27169;&#22411;&#26412;&#36523;&#34987;&#38169;&#35823;&#26657;&#20934;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26368;&#36817;&#24320;&#21457;&#30340;&#31216;&#20026;calibeating&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;OPS&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;OPS+calibeating&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#26159;&#20445;&#35777;&#26657;&#20934;&#30340;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#23427;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;OPS&#24605;&#24819;&#25193;&#23637;&#21040;beta&#32553;&#25918;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
&lt;/p&gt;</description></item><item><title>TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.14226</link><description>&lt;p&gt;
TorchBench: &#29992;&#39640;API&#34920;&#38754;&#35206;&#30422;&#29575;&#35780;&#20272;PyTorch&#24615;&#33021;&#30340;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14226
&lt;/p&gt;
&lt;p&gt;
TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;&#12290;&#20026;&#20102;&#26041;&#20415;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;PyTorch&#26159;&#26368;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;PyTorch&#36719;&#20214;&#26632;&#30340;&#29983;&#24577;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33410;&#30465;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#24182;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TorchBench&#65292;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#30740;&#31350;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19981;&#21516;&#65292;TorchBench&#21253;&#21547;&#20102;&#35768;&#22810;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;&#22823;&#37327;PyTorch API&#34920;&#38754;&#12290;TorchBench&#33021;&#22815;&#20840;&#38754;&#22320;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TorchBench&#30340;&#20004;&#20010;&#23454;&#38469;&#29992;&#20363;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#23545;TorchBench&#36827;&#34892;&#24615;&#33021;&#21078;&#26512;&#65292;&#20197;&#35782;&#21035;PyTorch&#30340;GPU&#24615;&#33021;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#35768;&#22810;&#24615;&#33021;&#25925;&#38556;&#24182;&#21521;&#19978;&#28216;&#25552;&#20132;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hint-Aug&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#21069;&#39044;&#35757;&#32451;&#30340;FViTs&#23398;&#21040;&#30340;&#39640;&#24230;&#20195;&#34920;&#24615;&#29305;&#24449;&#26469;&#22686;&#24378;&#35843;&#21442;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;FViTs&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#8220;&#39269;&#39295;&#8221;&#29305;&#24615;&#65292;&#24182;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;FViT&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#35843;&#21442;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12520</link><description>&lt;p&gt;
Hint-Aug: &#20174;&#22522;&#30784;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#33719;&#21462;&#25552;&#31034;&#65292;&#23454;&#29616;&#22686;&#24378;&#30340;&#23569;&#26679;&#26412;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Hint-Aug: Drawing Hints from Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning. (arXiv:2304.12520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12520
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hint-Aug&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#21069;&#39044;&#35757;&#32451;&#30340;FViTs&#23398;&#21040;&#30340;&#39640;&#24230;&#20195;&#34920;&#24615;&#29305;&#24449;&#26469;&#22686;&#24378;&#35843;&#21442;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;FViTs&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#8220;&#39269;&#39295;&#8221;&#29305;&#24615;&#65292;&#24182;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;FViT&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#35843;&#21442;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36234;&#26469;&#36234;&#38656;&#35201;&#35843;&#20248;&#22522;&#30784;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;FViT&#65289;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#23569;&#26679;&#26412;&#35843;&#20248;&#65289;&#65292;&#20805;&#20998;&#21457;&#25381;FViTs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;FViTs&#30340;&#25968;&#25454;&#29305;&#24615;&#26159;&#39269;&#39295;&#30340;&#12290;&#30001;&#20110;&#23569;&#31034;&#20363;&#35843;&#21442;&#25968;&#25454;&#21253;&#21547;&#26377;&#38480;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#24120;&#35265;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#27492;&#24773;&#20917;&#19979;&#26080;&#27861;&#21457;&#25381;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;FViTs&#22312;&#23569;&#26679;&#26412;&#35843;&#20248;&#26041;&#38754;&#30340;&#26426;&#20250;&#65306;&#39044;&#20808;&#35757;&#32451;&#30340;FViTs&#24050;&#32463;&#20174;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#21040;&#20102;&#39640;&#24230;&#20195;&#34920;&#24615;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#36807;&#31243;&#20013;&#23436;&#20840;&#20445;&#30041;&#12290;&#25105;&#20204;&#22240;&#27492;&#20551;&#35774;&#21033;&#29992;&#36825;&#20123;&#24050;&#23398;&#20064;&#30340;&#29305;&#24449;&#26469;&#22686;&#24378;&#35843;&#21442;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#23569;&#26679;&#26412;FViT&#35843;&#20248;&#30340;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Hint-based Data Augmentation (Hint-Aug) &#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#35843;&#25972;&#26679;&#26412;&#30340;&#36807;&#24230;&#25311;&#21512;&#37096;&#20998;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;FViTs&#30340;&#23398;&#20064;&#29305;&#24449;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;FViT&#30340;&#23569;&#26679;&#26412;&#35843;&#20248;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Hint-Aug&#26174;&#30528;&#25552;&#39640;&#20102;FViT&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20165;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#21442;&#65292;&#20351;FViTs&#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the growing demand for tuning foundation vision transformers (FViTs) on downstream tasks, fully unleashing FViTs' potential under data-limited scenarios (e.g., few-shot tuning) remains a challenge due to FViTs' data-hungry nature. Common data augmentation techniques fall short in this context due to the limited features contained in the few-shot tuning data. To tackle this challenge, we first identify an opportunity for FViTs in few-shot tuning: pretrained FViTs themselves have already learned highly representative features from large-scale pretraining data, which are fully preserved during widely used parameter-efficient tuning. We thus hypothesize that leveraging those learned features to augment the tuning data can boost the effectiveness of few-shot FViT tuning. To this end, we propose a framework called Hint-based Data Augmentation (Hint-Aug), which aims to boost FViT in few-shot tuning by augmenting the over-fitted parts of tuning samples with the learned features of pret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08453</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#25913;&#36827;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Autoregressive NLP Tasks via Modular Linearized Attention. (arXiv:2304.08453v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#30340;&#27169;&#22411;&#24517;&#39035;&#22312;&#26368;&#32456;&#24212;&#29992;&#20110;&#36793;&#32536;&#25110;&#20854;&#20182;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#23567;&#22411;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#65292;&#20294;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#33258;&#22238;&#24402;&#20219;&#21153;&#32780;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#20010;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21253;&#25324;cosFormer&#65292;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#26174;&#30528;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various natural language processing (NLP) tasks necessitate models that are efficient and small based on their ultimate application at the edge or in other resource-constrained environments. While prior research has reduced the size of these models, increasing computational efficiency without considerable performance impacts remains difficult, especially for autoregressive tasks. This paper proposes {modular linearized attention (MLA), which combines multiple efficient attention mechanisms, including cosFormer, to maximize inference quality while achieving notable speedups. We validate this approach on several autoregressive NLP tasks, including speech-to-text neural machine translation (S2T NMT), speech-to-text simultaneous translation (SimulST), and autoregressive text-to-spectrogram, noting efficiency gains on TTS and competitive performance for NMT and SimulST during training and inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.06377</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#31526;&#21495;&#30340;&#20986;&#29616;&#19982;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#21019;&#36896;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#65292;&#24182;&#29087;&#32451;&#22320;&#23558;&#23427;&#20204;&#29992;&#20110;&#26356;&#39640;&#30340;&#35748;&#30693;&#21151;&#33021;&#65292;&#22914;&#20132;&#27969;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#31561;&#65292;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#37325;&#35201;&#21644;&#29420;&#29305;&#20043;&#22788;&#12290; &#30446;&#21069;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#21019;&#36896;&#31526;&#21495;&#36827;&#34892;&#36825;&#20123;&#39640;&#32423;&#35748;&#30693;&#21151;&#33021;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31526;&#21495;&#21019;&#36896;&#12289;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#33021;&#21147;&#12290;SEA-net&#29983;&#25104;&#21160;&#24577;&#37197;&#32622;&#32593;&#32476;&#20197;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#31526;&#21495;&#12290;&#36825;&#20123;&#31526;&#21495;&#25429;&#25417;&#20102;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#36890;&#36807;&#32431;&#31526;&#21495;&#25805;&#20316;&#25110;&#20132;&#27969;&#33719;&#24471;&#26032;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#30340;&#31526;&#21495;&#21576;&#29616;&#20986;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#34920;&#26126;&#22312;&#20154;&#31867;&#22823;&#33041;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#29983;&#25104;&#21644;&#29702;&#35299;&#31526;&#21495;&#30340;&#20849;&#21516;&#26694;&#26550;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#23558;&#25104;&#20026;&#23558;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#30340;&#21161;&#25512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to create meaningful symbols and proficiently use them for higher cognitive functions such as communication, reasoning, planning, etc., is essential and unique for human intelligence. Current deep neural networks are still far behind human's ability to create symbols for such higher cognitive functions. Here we propose a solution, named SEA-net, to endow neural networks with ability of symbol creation, semantic understanding and communication. SEA-net generates symbols that dynamically configure the network to perform specific tasks. These symbols capture compositional semantic information that enables the system to acquire new functions purely by symbolic manipulation or communication. In addition, we found that these self-generated symbols exhibit an intrinsic structure resembling that of natural language, suggesting a common framework underlying the generation and understanding of symbols in both human brains and artificial neural networks. We hope that it will be instrum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01295</link><description>&lt;p&gt;
&#26377;&#25928;&#22320;&#23545;&#40784;&#36328;&#35821;&#35328;&#20250;&#35805;&#20219;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20250;&#35805;&#20219;&#21153;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;XSGD&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;Schema-Guided Dialogue&#65288;SGD&#65289;&#32763;&#35793;&#25104;105&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#27979;&#35797;&#20102;&#23545;&#40784;&#25552;&#31034;&#25152;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23545;&#35805;&#20219;&#21153;&#65288;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12785</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;&#65306;&#25910;&#25947;&#24615;&#19982;&#20840;&#23616;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality. (arXiv:2303.12785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#20195;&#29702;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#38500;&#20102;&#32047;&#35745;&#22870;&#21169;&#22806;&#30340;&#29109;&#22870;&#21169;&#12290;MPG&#19982;&#26631;&#20934;PG&#30340;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#23427;&#35757;&#32451;&#19968;&#31995;&#21015;&#31574;&#30053;&#21516;&#26102;&#23398;&#20064;&#26377;&#38480;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#21333;&#19968;&#30340;&#26631;&#20934;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31574;&#30053;&#12290;&#23545;&#20110;softmax&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPG&#30340;&#25910;&#25947;&#24615;&#21644;&#26497;&#38480;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#36890;&#36807;&#35777;&#26126;MPG&#30446;&#26631;&#30340;&#21807;&#19968;&#20020;&#30028;&#28857;&#26159;&#26368;&#20248;&#31574;&#30053;&#65307;&#21363;&#20351;&#22312;&#36830;&#32493;&#32039;&#33268;&#29366;&#24577;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#12290;MPG&#30452;&#35266;&#12289;&#29702;&#35770;&#19978;Sound&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26631;&#20934;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#26368;&#20248;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;MPG&#26694;&#26550;&#30340;&#26368;&#20248;&#31574;&#30053;&#36827;&#34892;&#20219;&#24847;&#31934;&#24230;&#30340;&#36924;&#36817;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#31574;&#30053;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;MPG&#38750;&#24120;&#36866;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient (MPG), is introduced and studied, in the context of max-entropy reinforcement learning, where an agent aims at maximising entropy bonuses additional to its cumulative rewards. MPG differs from standard PG in that it trains a sequence of policies to learn finite horizon tasks simultaneously, instead of a single policy for the single standard objective. For softmax policies, we prove convergence of MPG and global optimality of the limit by showing that the only critical point of the MPG objective is the optimal policy; these results hold true even in the case of continuous compact state space. MPG is intuitive, theoretically sound and we furthermore show that the optimal policy of the standard max-entropy objective can be approximated arbitrarily well by the optimal policy of the MPG framework. Finally, we justify that MPG is well suited when the policies are parametrized with neural networks and we provide an simp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; RegionLight&#65292;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#23558;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#27599;&#20010;&#21306;&#22495;&#20013;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#25193;&#23637;&#20102;BDQ&#26041;&#27861;&#20026;DBDQ&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11899</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22823;&#35268;&#27169;&#26684;&#32593;&#20132;&#36890;&#32593;&#32476;&#21306;&#22495;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network. (arXiv:2303.11899v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; RegionLight&#65292;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#23558;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#27599;&#20010;&#21306;&#22495;&#20013;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#25193;&#23637;&#20102;BDQ&#26041;&#27861;&#20026;DBDQ&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26159;&#24403;&#21069;&#38750;&#24120;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#21333;&#20010;&#36335;&#21475;&#65292;&#36825;&#20123;&#26041;&#27861;&#20391;&#37325;&#20110;&#36335;&#21475;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;MARL&#30340;&#38750;&#31283;&#24577;&#24615;&#36136;&#38543;&#30528;&#20132;&#36890;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20173;&#28982;&#38480;&#21046;&#30528;&#19978;&#36848;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#22949;&#21327;&#30340;&#31574;&#30053;&#26159;&#23558;&#19968;&#21517;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#19968;&#32452;&#36335;&#21475;&#20013;&#65292;&#20197;&#20943;&#23569;&#26234;&#33021;&#20307;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65292;&#19968;&#20010;&#26159;&#22914;&#20309;&#23558;&#20132;&#36890;&#32593;&#32476;&#21010;&#20998;&#25104;&#23567;&#21306;&#22495;&#65292;&#21478;&#19968;&#20010;&#26159;&#22914;&#20309;&#25628;&#32034;&#21306;&#22495;&#20869;&#30340;&#26368;&#20248;&#32852;&#21512;&#21160;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;RegionLight&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#21306;&#22495;&#21010;&#20998;&#35268;&#21017;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#65292;&#24182;&#25193;&#23637;&#20102;Branching Dueling Q-Network(BDQ)&#12290;&#35813;&#26041;&#27861;&#23558;BDQ&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;Dynamic Branching Dueling Q-Network(DBDQ)&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive traffic signal control with Multi-agent Reinforcement Learning(MARL) is a very popular topic nowadays. In most existing novel methods, one agent controls single intersections and these methods focus on the cooperation between intersections. However, the non-stationary property of MARL still limits the performance of the above methods as the size of traffic networks grows. One compromised strategy is to assign one agent with a region of intersections to reduce the number of agents. There are two challenges in this strategy, one is how to partition a traffic network into small regions and the other is how to search for the optimal joint actions for a region of intersections. In this paper, we propose a novel training framework RegionLight where our region partition rule is based on the adjacency between the intersection and extended Branching Dueling Q-Network(BDQ) to Dynamic Branching Dueling Q-Network(DBDQ) to bound the growth of the size of joint action space and alleviate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SQE&#30340;&#24207;&#21015;&#26597;&#35810;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#32534;&#30721;&#20026;&#19968;&#20010;&#24207;&#21015;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#21644;&#24378;&#22823;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2302.13114</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19978;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#30340;&#24207;&#21015;&#26597;&#35810;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sequential Query Encoding For Complex Query Answering on Knowledge Graphs. (arXiv:2302.13114v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SQE&#30340;&#24207;&#21015;&#26597;&#35810;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#32534;&#30721;&#20026;&#19968;&#20010;&#24207;&#21015;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#21644;&#24378;&#22823;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#26159;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#37325;&#35201;&#21644;&#22522;&#30784;&#20219;&#21153;&#12290;&#26597;&#35810;&#32534;&#30721;&#34987;&#25552;&#20986;&#20316;&#20026;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#30340;&#24555;&#36895;&#32780;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;QE&#26041;&#27861;&#39318;&#20808;&#23558;&#36923;&#36753;&#26597;&#35810;&#35299;&#26512;&#20026;&#21487;&#25191;&#34892;&#30340;&#35745;&#31639;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#65292;&#28982;&#21518;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25805;&#20316;&#31526;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#26368;&#21518;&#36882;&#24402;&#25191;&#34892;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#21270;&#30340;&#25805;&#20316;&#31526;&#12290;&#28982;&#32780;&#65292;&#21442;&#25968;&#21270;&#21644;&#25191;&#34892;&#33539;&#24335;&#21487;&#33021;&#20250;&#28508;&#22312;&#22320;&#36807;&#20110;&#22797;&#26434;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#36827;&#34892;&#32467;&#26500;&#31616;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20687;LSTM&#21644;Transformer&#36825;&#26679;&#30340;&#24207;&#21015;&#32534;&#30721;&#22120;&#34987;&#35777;&#26126;&#23545;&#20110;&#32534;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#35821;&#20041;&#22270;&#38750;&#24120;&#26377;&#25928;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#26597;&#35810;&#32534;&#30721;(SQE)&#20316;&#20026;&#32534;&#30721;CQA&#26597;&#35810;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;SQE&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#31639;&#27861;&#23558;&#35745;&#31639;&#22270;&#32447;&#24615;&#21270;&#20026;&#19968;&#31995;&#21015;&#26631;&#35760;&#65292;&#28982;&#21518;&#20351;&#29992;&#24207;&#21015;&#32534;&#30721;&#22120;&#23558;&#36825;&#20123;&#26631;&#35760;&#32534;&#30721;&#20026;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex Query Answering (CQA) is an important and fundamental task for knowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and robust solution to CQA. In the encoding process, most existing QE methods first parse the logical query into an executable computational direct-acyclic graph (DAG), then use neural networks to parameterize the operators, and finally, recursively execute these neuralized operators. However, the parameterization-and-execution paradigm may be potentially over-complicated, as it can be structurally simplified by a single neural network encoder. Meanwhile, sequence encoders, like LSTM and Transformer, proved to be effective for encoding semantic graphs in related tasks. Motivated by this, we propose sequential query encoding (SQE) as an alternative to encode queries for CQA. Instead of parameterizing and executing the computational graph, SQE first uses a search-based algorithm to linearize the computational graph to a sequence of tokens and th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11996</link><description>&lt;p&gt;
K-SHAP: &#19968;&#31181;&#29992;&#20110;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#23427;&#20204;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#25105;&#20204;&#35299;&#37322;&#23427;&#20204;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#19968;&#31181;&#29305;&#23450;&#30340;&#24773;&#20917;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#37027;&#23601;&#26159;&#26234;&#33021;&#20307;&#36523;&#20221;&#20445;&#25345;&#21311;&#21517;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#26159;&#19987;&#26377;&#30340;&#65292;&#20165;&#20844;&#24320;&#22810;&#20010;&#24066;&#22330;&#21442;&#19982;&#32773;&#20132;&#20114;&#32780;&#20135;&#29983;&#30340;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#34892;&#21160;&#24207;&#21015;&#19981;&#21487;&#35266;&#27979;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;K-SHAP&#65292;&#23427;&#23398;&#20064;&#26681;&#25454;&#26234;&#33021;&#20307;&#31574;&#30053;&#23545;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;(IL)&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;w...
&lt;/p&gt;
&lt;p&gt;
Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.08883</link><description>&lt;p&gt;
&#36817;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20005;&#37325;&#20381;&#36182;&#20110;&#20266;&#26631;&#31614;&#36873;&#25321;&#65288;PLS&#65289;&#12290;&#36873;&#25321;&#36890;&#24120;&#21462;&#20915;&#20110;&#21021;&#22987;&#27169;&#22411;&#25311;&#21512;&#26631;&#35760;&#25968;&#25454;&#30340;&#31243;&#24230;&#12290;&#36807;&#26089;&#30340;&#36807;&#25311;&#21512;&#21487;&#33021;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#30340;&#23454;&#20363;&#65288;&#36890;&#24120;&#31216;&#20026;&#30830;&#35748;&#20559;&#24046;&#65289;&#32780;&#20256;&#25773;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#26159;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65306;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#20998;&#26512;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#33719;&#24471;&#20102;&#36825;&#31181;&#36873;&#25321;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#20811;&#26381;&#35745;&#31639;&#38590;&#39064;&#12290;&#23427;&#19982;&#36793;&#38469;&#20284;&#28982;&#30340;&#20851;&#31995;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#21644;&#39640;&#26031;&#31215;&#20998;&#30340;&#36924;&#36817;&#12290;&#25105;&#20204;&#38024;&#23545;&#21442;&#25968;&#24191;&#20041;&#32447;&#24615;&#21644;&#38750;&#21442;&#25968;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#23545;BPLS&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;164&#31687;&#20027;&#39064;&#24615;&#22238;&#39038;&#65292;&#24635;&#32467;&#20102;&#20154;&#24615;&#21270;&#12289;&#36947;&#24503;&#21270;&#21644;&#36127;&#36131;&#20219;AI&#39046;&#22495;&#30340;&#24213;&#23618;&#26144;&#23556;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;HCER-AI&#30740;&#31350;&#37325;&#28857;&#26159;&#27835;&#29702;&#12289;&#20844;&#27491;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;AIES&#12289;FAccT&#12289;CHI&#21644;CSCW&#31561;&#20250;&#35758;&#19987;&#27880;&#20110;&#29305;&#23450;&#20027;&#39064;&#65292;&#32570;&#20047;&#23545;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20154;&#31867;&#32321;&#33635;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2302.05284</link><description>&lt;p&gt;
&#20154;&#24615;&#21270;&#12289;&#36947;&#24503;&#21270;&#21644;&#36127;&#36131;&#20219;AI&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Literature Review of Human-Centered, Ethical, and Responsible AI. (arXiv:2302.05284v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;164&#31687;&#20027;&#39064;&#24615;&#22238;&#39038;&#65292;&#24635;&#32467;&#20102;&#20154;&#24615;&#21270;&#12289;&#36947;&#24503;&#21270;&#21644;&#36127;&#36131;&#20219;AI&#39046;&#22495;&#30340;&#24213;&#23618;&#26144;&#23556;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;HCER-AI&#30740;&#31350;&#37325;&#28857;&#26159;&#27835;&#29702;&#12289;&#20844;&#27491;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;AIES&#12289;FAccT&#12289;CHI&#21644;CSCW&#31561;&#20250;&#35758;&#19987;&#27880;&#20110;&#29305;&#23450;&#20027;&#39064;&#65292;&#32570;&#20047;&#23545;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20154;&#31867;&#32321;&#33635;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#32771;&#34385;AI&#30340;&#36947;&#24503;&#21644;&#31038;&#20250;&#24433;&#21709;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;AIES&#12289;CHI&#12289;CSCW&#21644;FAccT&#31561;&#20027;&#35201;&#20250;&#35758;&#20013;164&#31687;&#35770;&#25991;&#30340;&#20027;&#39064;&#24615;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#30446;&#21069;&#20154;&#24615;&#21270;AI&#12289;&#36947;&#24503;&#21270;AI&#21644;&#36127;&#36131;&#20219;AI&#65288;HCER-AI&#65289;&#20132;&#21449;&#30740;&#31350;&#39046;&#22495;&#30340;&#24213;&#23618;&#26144;&#23556;&#12290;HCER-AI&#39046;&#22495;&#30340;&#30740;&#31350;&#24378;&#35843;&#27835;&#29702;&#12289;&#20844;&#27491;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#20250;&#35758;&#19987;&#27880;&#20110;&#29305;&#23450;&#20027;&#39064;&#65292;&#32780;AIES&#22312;HCER-AI&#26041;&#38754;&#35770;&#25991;&#36739;&#23569;&#65292;&#24378;&#35843;&#27835;&#29702;&#65292;&#24456;&#23569;&#21457;&#34920;&#26377;&#20851;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20154;&#31867;&#32321;&#33635;&#30340;&#35770;&#25991;&#12290;FAccT&#22312;&#27835;&#29702;&#26041;&#38754;&#21457;&#34920;&#30340;&#35770;&#25991;&#26356;&#22810;&#65292;&#20294;&#32570;&#20047;&#20851;&#20110;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20154;&#31867;&#32321;&#33635;&#30340;&#35770;&#25991;&#12290;CHI&#21644;CSCW&#20316;&#20026;&#26356;&#25104;&#29087;&#30340;&#20250;&#35758;&#65292;&#25317;&#26377;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Artificial Intelligence (AI) continues to advance rapidly, it becomes increasingly important to consider AI's ethical and societal implications. In this paper, we present a bottom-up mapping of the current state of research at the intersection of Human-Centered AI, Ethical, and Responsible AI (HCER-AI) by thematically reviewing and analyzing 164 research papers from leading conferences in ethical, social, and human factors of AI: AIES, CHI, CSCW, and FAccT. The ongoing research in HCER-AI places emphasis on governance, fairness, and explainability. These conferences, however, concentrate on specific themes rather than encompassing all aspects. While AIES has fewer papers on HCER-AI, it emphasizes governance and rarely publishes papers about privacy, security, and human flourishing. FAccT publishes more on governance and lacks papers on privacy, security, and human flourishing. CHI and CSCW, as more established conferences, have a broader research portfolio. We find that the current 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;QR-CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#20301;&#32622;&#21644;&#26102;&#38388;&#25512;&#29702;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#32422;10%&#21644;130%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.00952</link><description>&lt;p&gt;
QR-CLIP: &#24341;&#20837;&#26174;&#24335;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#20301;&#32622;&#21644;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning. (arXiv:2302.00952v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;QR-CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#20301;&#32622;&#21644;&#26102;&#38388;&#25512;&#29702;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#32422;10%&#21644;130%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#30340;&#22270;&#20687;&#21487;&#33021;&#20256;&#36798;&#38656;&#35201;&#25105;&#20204;&#20174;&#20013;&#35760;&#24518;&#21644;&#25512;&#26029;&#20986;&#28145;&#21051;&#20449;&#24687;&#30340;&#25277;&#35937;&#21547;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25945;&#20250;&#26426;&#22120;&#39044;&#27979;&#22270;&#29255;&#25293;&#25668;&#30340;&#22320;&#28857;&#21644;&#26102;&#38388;&#65292;&#32780;&#19981;&#26159;&#25191;&#34892;&#20256;&#32479;&#30340;&#20998;&#21106;&#25110;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#40723;&#21169;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#12290;&#21463;&#21040;Horn&#30340;QR&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#30340;&#26032;&#22411;QR-CLIP&#27169;&#22411;: 1) &#25968;&#37327;&#27169;&#22359;&#39318;&#20808;&#22238;&#39038;&#26356;&#22810;&#30340;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#20316;&#20026;&#20505;&#36873;&#30340;&#35821;&#35328;&#36755;&#20837;; 2) &#30456;&#20851;&#24615;&#27169;&#22359;&#20180;&#32454;&#20272;&#35745;&#35270;&#35273;&#21644;&#35821;&#35328;&#32447;&#32034;&#65292;&#24182;&#25512;&#26029;&#20986;&#20301;&#32622;&#21644;&#26102;&#38388;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;QR-CLIP&#21313;&#20998;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#27599;&#20010;&#20219;&#21153;&#19978;&#37117;&#27604;&#20043;&#21069;&#30340;&#26368;&#39640;&#27700;&#24179;&#34920;&#29616;&#24179;&#22343;&#25552;&#21319;&#20102;&#32422;10%&#21644;130%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;&#26412;&#30740;&#31350;&#20026;&#20301;&#32622;&#21644;&#26102;&#38388;&#25512;&#29702;&#22880;&#23450;&#20102;&#25216;&#26415;&#22522;&#30784;&#65292;&#24182;&#34920;&#26126;&#26377;&#25928;&#24341;&#20837;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#26159;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Daily images may convey abstract meanings that require us to memorize and infer profound information from them. To encourage such human-like reasoning, in this work, we teach machines to predict where and when it was taken rather than performing basic tasks like traditional segmentation or classification. Inspired by Horn's QR theory, we designed a novel QR-CLIP model consisting of two components: 1) the Quantity module first retrospects more open-world knowledge as the candidate language inputs; 2) the Relevance module carefully estimates vision and language cues and infers the location and time. Experiments show our QR-CLIP's effectiveness, and it outperforms the previous SOTA on each task by an average of about 10% and 130% relative lift in terms of location and time reasoning. This study lays a technical foundation for location and time reasoning and suggests that effectively introducing open-world knowledge is one of the panaceas for the tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21738;&#20123;&#26679;&#26412;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20174;&#32780;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12896</link><description>&lt;p&gt;
&#37492;&#23450;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26679;&#26412;&#21644;&#24378;&#38887;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Identifying Adversarially Attackable and Robust Samples. (arXiv:2301.12896v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21738;&#20123;&#26679;&#26412;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20174;&#32780;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#23558;&#24494;&#23567;&#30340;&#65292;&#38590;&#20197;&#24863;&#30693;&#30340;&#25200;&#21160;&#25554;&#20837;&#36755;&#20837;&#26679;&#26412;&#65292;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#21457;&#29983;&#22823;&#37327;&#19981;&#26399;&#26395;&#30340;&#21464;&#21270;&#12290;&#34429;&#28982;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#29983;&#25104;&#21644;&#38450;&#24481;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20174;&#36755;&#20837;&#25968;&#25454;&#35282;&#24230;&#29702;&#35299;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26679;&#26412;&#25915;&#20987;&#24615;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26679;&#26412;&#65288;&#25915;&#20987;&#24615;&#26679;&#26412;&#65289;&#65292;&#20174;&#32780;&#21453;&#36807;&#26469;&#30830;&#23450;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#26679;&#26412;&#65288;&#24378;&#38887;&#26679;&#26412;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#38024;&#23545;&#26410;&#30693;&#30446;&#26631;&#27169;&#22411;&#30340;&#26410;&#35265;&#25968;&#25454;&#38598;&#20013;&#65292;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#24378;&#38887;&#24615;&#26679;&#26412;&#12290;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#28145;&#24230;&#25915;&#20987;&#24615;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#22522;&#20110;&#31616;&#21333;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#25514;&#26045;&#30456;&#27604;&#65292;&#28145;&#24230;&#25915;&#20987;&#24615;&#26816;&#27979;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks insert small, imperceptible perturbations to input samples that cause large, undesired changes to the output of deep learning models. Despite extensive research on generating adversarial attacks and building defense systems, there has been limited research on understanding adversarial attacks from an input-data perspective. This work introduces the notion of sample attackability, where we aim to identify samples that are most susceptible to adversarial attacks (attackable samples) and conversely also identify the least susceptible samples (robust samples). We propose a deep-learning-based method to detect the adversarially attackable and robust samples in an unseen dataset for an unseen target model. Experiments on standard image classification datasets enables us to assess the portability of the deep attackability detector across a range of architectures. We find that the deep attackability detector performs better than simple model uncertainty-based measures for i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#30830;&#23450;&#24615;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#20110;2&#12289;3&#12289;5&#12289;10&#21644;20&#20010;&#35828;&#35805;&#32773;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.10752</link><description>&lt;p&gt;
&#20998;&#31163;&#19982;&#25193;&#25955;&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#25552;&#39640;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Separate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation. (arXiv:2301.10752v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#30830;&#23450;&#24615;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#20110;2&#12289;3&#12289;5&#12289;10&#21644;20&#20010;&#35828;&#35805;&#32773;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20998;&#31163;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#40481;&#23614;&#37202;&#20250;&#38382;&#39064;&#65292;&#25351;&#30340;&#26159;&#20174;&#22810;&#20010;&#28151;&#21512;&#30340;&#35821;&#38899;&#20449;&#21495;&#20013;&#38548;&#31163;&#20986;&#21333;&#20010;&#35821;&#38899;&#20449;&#21495;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#28304;&#20998;&#31163;&#24037;&#20316;&#22312;&#20154;&#31867;&#35821;&#38899;&#39046;&#22495;&#25512;&#23548;&#20986;&#20102;&#28304;&#20998;&#31163;&#20219;&#21153;&#30340;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#26159;&#38024;&#23545;&#30830;&#23450;&#24615;&#27169;&#22411;&#32780;&#25512;&#23548;&#30340;&#12290;&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#25361;&#25112;&#20102;&#35813;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#19978;&#38480;&#25512;&#24191;&#21040;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#12290;&#23558;&#39044;&#20808;&#35757;&#32451;&#23545;&#21333;&#19968;&#35828;&#35805;&#32773;&#22768;&#38899;&#36827;&#34892;&#24314;&#27169;&#30340;&#25193;&#25955;&#27169;&#22411; Vocoder &#24212;&#29992;&#20110;&#30830;&#23450;&#24615;&#20998;&#31163;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20998;&#31163;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#23558;&#20998;&#31163;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#32452;&#21512;&#65292;&#20351;&#29992;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#25512;&#26029;&#20986;&#30340;&#26435;&#37325;&#65292;&#22312;&#39057;&#29575;&#22495;&#20869;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#23545;&#20110;2&#12289;3&#12289;5&#12289;10&#21644;20&#20010;&#35828;&#35805;&#32773;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of speech separation, also known as the cocktail party problem, refers to the task of isolating a single speech signal from a mixture of speech signals. Previous work on source separation derived an upper bound for the source separation task in the domain of human speech. This bound is derived for deterministic models. Recent advancements in generative models challenge this bound. We show how the upper bound can be generalized to the case of random generative models. Applying a diffusion model Vocoder that was pretrained to model single-speaker voices on the output of a deterministic separation model leads to state-of-the-art separation results. It is shown that this requires one to combine the output of the separation model with that of the diffusion model. In our method, a linear combination is performed, in the frequency domain, using weights that are inferred by a learned model. We show state-of-the-art results on 2, 3, 5, 10, and 20 speakers on multiple benchmarks. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#30693;&#35782;&#24211;&#38382;&#31572;&#20013;&#30340;&#21487;&#22238;&#31572;&#24615;&#38382;&#39064;&#65292;&#20351;&#29992;&#26032;&#30340;GrailQAbility&#22522;&#20934;KBQA&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;KBQA&#27169;&#22411;&#22312;&#22788;&#29702;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#23545;&#26080;&#27861;&#22238;&#31572;&#30340;&#26816;&#27979;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#20351;KBQA&#31995;&#32479;&#23545;&#26080;&#27861;&#22238;&#31572;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10189</link><description>&lt;p&gt;
&#25105;&#26377;&#36275;&#22815;&#30340;&#30693;&#35782;&#22238;&#31572;&#21527;&#65311;&#25506;&#31350;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#21487;&#22238;&#31572;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions. (arXiv:2212.10189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#30693;&#35782;&#24211;&#38382;&#31572;&#20013;&#30340;&#21487;&#22238;&#31572;&#24615;&#38382;&#39064;&#65292;&#20351;&#29992;&#26032;&#30340;GrailQAbility&#22522;&#20934;KBQA&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;KBQA&#27169;&#22411;&#22312;&#22788;&#29702;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#23545;&#26080;&#27861;&#22238;&#31572;&#30340;&#26816;&#27979;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#20351;KBQA&#31995;&#32479;&#23545;&#26080;&#27861;&#22238;&#31572;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#26102;&#65292;&#32570;&#22833;&#30340;&#20107;&#23454;&#12289;&#19981;&#23436;&#25972;&#30340;&#27169;&#24335;&#21644;&#26377;&#38480;&#30340;&#33539;&#22260;&#33258;&#28982;&#22320;&#23548;&#33268;&#35768;&#22810;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#12290;&#34429;&#28982;&#22312;&#20854;&#20182;&#38382;&#31572;&#29615;&#22659;&#20013;&#24050;&#32463;&#25506;&#35752;&#20102;&#21487;&#22238;&#31572;&#24615;&#65292;&#20294;&#23545;&#20110;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#30693;&#35782;&#24211;&#19981;&#23436;&#25972;&#24615;&#65292;&#20351;&#24471;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#65292;&#24182;&#36890;&#36807;&#26377;&#31995;&#32479;&#22320;&#35843;&#25972;GrailQA&#65288;&#19968;&#20010;&#20165;&#21253;&#21547;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#27969;&#34892;KBQA&#25968;&#25454;&#38598;&#65289;&#26469;&#21019;&#24314;&#20855;&#26377;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#26032;&#30340;GrailQAbility&#22522;&#20934;KBQA&#25968;&#25454;&#38598;&#12290;&#22312;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#36866;&#24403;&#35843;&#25972;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#21518;&#65292;&#25152;&#26377;&#19977;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#22240;&#38169;&#35823;&#30340;&#21407;&#22240;&#26816;&#27979;&#20986;&#26080;&#27861;&#22238;&#31572;&#65292;&#24182;&#21457;&#29616;&#29305;&#23450;&#24418;&#24335;&#30340;&#26080;&#27861;&#22238;&#31572;&#23588;&#20854;&#38590;&#20197;&#22788;&#29702;&#12290;&#36825;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#20351;KBQA&#31995;&#32479;&#23545;&#26080;&#27861;&#22238;&#31572;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset with only answerable questions). Experimenting with three state-of-the-art KBQA models, we find that all three models suffer a drop in performance even after suitable adaptation for unanswerable questions. In addition, these often detect unanswerability for wrong reasons and find specific forms of unanswerability particularly difficult to handle. This underscores the need for further research in making KBQA systems robust to unanswerability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#26102;&#36816;&#34892;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#29992;&#20110;&#31070;&#32463;&#28210;&#26579;&#12290;</title><link>http://arxiv.org/abs/2212.08057</link><description>&lt;p&gt;
&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#31070;&#32463;&#20809;&#22330;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
Real-Time Neural Light Field on Mobile Devices. (arXiv:2212.08057v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#26102;&#36816;&#34892;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#29992;&#20110;&#31070;&#32463;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#31070;&#32463;&#28210;&#26579;&#22330;&#65288;NeRF&#65289;&#22312;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;3D&#22330;&#26223;&#65292;&#24182;&#23454;&#29616;&#26032;&#35270;&#35282;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#30001;&#20110;&#20307;&#31215;&#28210;&#26579;&#30340;&#36807;&#31243;&#65292;NeRF&#30340;&#25512;&#29702;&#36895;&#24230;&#38750;&#24120;&#32531;&#24930;&#65292;&#38480;&#21046;&#20102;&#22312;&#31227;&#21160;&#35774;&#22791;&#31561;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#21033;&#29992;NeRF&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#26377;&#35768;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#20943;&#23569;&#36816;&#34892;NeRF&#27169;&#22411;&#30340;&#24310;&#36831;&#12290;&#20294;&#26159;&#65292;&#20182;&#20204;&#22823;&#22810;&#20173;&#38656;&#35201;&#39640;&#31471;GPU&#36827;&#34892;&#21152;&#36895;&#25110;&#39069;&#22806;&#30340;&#23384;&#20648;&#20869;&#23384;&#65292;&#36825;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37117;&#19981;&#21487;&#29992;&#12290;&#21478;&#19968;&#20010;&#26032;&#20986;&#29616;&#30340;&#26041;&#21521;&#21017;&#21033;&#29992;&#31070;&#32463;&#20809;&#22330;&#65288;NeLF&#65289;&#26469;&#36827;&#34892;&#21152;&#36895;&#65292;&#22240;&#20026;&#19968;&#26465;&#23556;&#32447;&#19978;&#21482;&#38656;&#36827;&#34892;&#19968;&#27425;&#21521;&#21069;&#20256;&#36882;&#21363;&#21487;&#39044;&#27979;&#20687;&#32032;&#39068;&#33394;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#19982;NeRF&#31867;&#20284;&#30340;&#28210;&#26579;&#36136;&#37327;&#65292;NeLF&#20013;&#30340;&#32593;&#32476;&#35774;&#35745;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#65292;&#36825;&#23545;&#31227;&#21160;&#35774;&#22791;&#24182;&#19981;&#21451;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#26102;&#36816;&#34892;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#29992;&#20110;&#31070;&#32463;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts in Neural Rendering Fields (NeRF) have shown impressive results on novel view synthesis by utilizing implicit neural representation to represent 3D scenes. Due to the process of volumetric rendering, the inference speed for NeRF is extremely slow, limiting the application scenarios of utilizing NeRF on resource-constrained hardware, such as mobile devices. Many works have been conducted to reduce the latency of running NeRF models. However, most of them still require high-end GPU for acceleration or extra storage memory, which is all unavailable on mobile devices. Another emerging direction utilizes the neural light field (NeLF) for speedup, as only one forward pass is performed on a ray to predict the pixel color. Nevertheless, to reach a similar rendering quality as NeRF, the network in NeLF is designed with intensive computation, which is not mobile-friendly. In this work, we propose an efficient network that runs in real-time on mobile devices for neural rendering. W
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CTCO&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#65292;&#30456;&#27604;&#20256;&#32479;RL&#21644;&#26102;&#38388;&#25277;&#35937;RL&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2212.04407</link><description>&lt;p&gt;
&#21487;&#21464;&#21270;&#20915;&#31574;&#39057;&#29575;&#30340;&#36873;&#39033;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
Variable Decision-Frequency Option Critic. (arXiv:2212.04407v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04407
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CTCO&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#65292;&#30456;&#27604;&#20256;&#32479;RL&#21644;&#26102;&#38388;&#25277;&#35937;RL&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20195;&#29702;&#22312;&#31163;&#25955;&#21644;&#22266;&#23450;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#20915;&#31574;&#20043;&#38388;&#30340;&#25345;&#32493;&#26102;&#38388;&#21464;&#25104;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#65292;&#22240;&#20026;&#35774;&#32622;&#24471;&#22826;&#30701;&#21487;&#33021;&#20250;&#22686;&#21152;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#38656;&#35201;&#20195;&#29702;&#36827;&#34892;&#22810;&#27425;&#20915;&#31574;&#25165;&#33021;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#32780;&#35774;&#32622;&#24471;&#22826;&#38271;&#20250;&#23548;&#33268;&#20195;&#29702;&#22833;&#21435;&#23545;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#31995;&#32479;&#19981;&#19968;&#23450;&#38656;&#35201;&#24658;&#23450;&#30340;&#25511;&#21046;&#39057;&#29575;&#65292;&#23545;&#20110;&#23398;&#20064;&#20195;&#29702;&#26469;&#35828;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24403;&#38656;&#35201;&#26102;&#20197;&#39640;&#39057;&#29575;&#36816;&#34892;&#65292;&#32780;&#22312;&#21487;&#33021;&#26102;&#20197;&#20302;&#39057;&#29575;&#36816;&#34892;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#36873;&#39033; (CTCO) &#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20123;&#36873;&#39033;&#26159;&#26102;&#38388;&#36830;&#32493;&#30340;&#65292;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24615;&#33021;&#19982;&#20256;&#32479; RL &#21644;&#26102;&#38388;&#25277;&#35937; RL &#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102; CTCO &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the difficulty of the problem by requiring the agent to make numerous decisions to achieve its goal, while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23545;&#20110;&#20272;&#35745;&#27010;&#29575;&#35789;&#35821;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#20351;&#29992;UNLI&#25968;&#25454;&#38598;&#21644;&#26500;&#24314;WEP&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;WEP&#30340;&#23384;&#22312;&#26102;&#24456;&#26377;&#25928;&#65292;&#20294;&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;&#19982;&#27599;&#20010;&#35789;&#30456;&#20851;&#32852;&#30340;&#20849;&#35782;&#27010;&#29575;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.03358</link><description>&lt;p&gt;
&#25506;&#31350;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#23545;&#20272;&#35745;&#27010;&#29575;&#35789;&#35821;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Probing neural language models for understanding of words of estimative probability. (arXiv:2211.03358v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23545;&#20110;&#20272;&#35745;&#27010;&#29575;&#35789;&#35821;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#20351;&#29992;UNLI&#25968;&#25454;&#38598;&#21644;&#26500;&#24314;WEP&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;WEP&#30340;&#23384;&#22312;&#26102;&#24456;&#26377;&#25928;&#65292;&#20294;&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;&#19982;&#27599;&#20010;&#35789;&#30456;&#20851;&#32852;&#30340;&#20849;&#35782;&#27010;&#29575;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#27010;&#29575;&#35789;&#35821;(WEP)&#26159;&#38472;&#36848;&#30340;&#21487;&#20449;&#24230;&#34920;&#36798;&#65288;&#20363;&#22914;&#65292;&#21487;&#33021;&#65292;&#25110;&#35768;&#65292;&#24456;&#26377;&#21487;&#33021;&#65292;&#24576;&#30097;&#65292;&#19981;&#21487;&#33021;&#31561;&#65289;&#12290;&#22810;&#39033;&#35843;&#26597;&#34920;&#26126;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#20026;WEP&#20998;&#37197;&#25968;&#20540;&#27010;&#29575;&#27700;&#24179;&#26102;&#23384;&#22312;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292; Fagen-Ulmschneider(2015)&#30340;&#35843;&#26597;&#20013;&#65292;&#8220;&#24456;&#26377;&#21487;&#33021;&#8221;&#23545;&#24212;&#30528;&#20013;&#20301;&#25968;0.90+-0.08&#30340;&#20960;&#29575;&#12290;&#26412;&#30740;&#31350;&#27979;&#37327;&#20102;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25429;&#25417;&#19982;&#27599;&#20010;WEP&#30456;&#20851;&#32852;&#30340;&#20849;&#35782;&#27010;&#29575;&#27700;&#24179;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;UNLI&#25968;&#25454;&#38598;(&#38472;&#31561;&#20154;&#65292;2020)&#65292;&#23558;&#21069;&#25552;&#21644;&#20551;&#35774;&#19982;&#20854;&#24863;&#30693;&#30340;&#32852;&#21512;&#27010;&#29575;p&#30456;&#32852;&#31995;&#65292;&#26500;&#24314;&#25552;&#31034;&#65292;&#20363;&#22914;&#8220;[&#21069;&#25552;]&#12290;[WEP]&#65292;[&#20551;&#35774;]&#12290;&#8221;&#24182;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;WEP&#20849;&#35782;&#27010;&#29575;&#27700;&#24179;&#26159;&#21542;&#25509;&#36817;&#20110;p&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;WEP&#30340;&#27010;&#29575;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;WEP&#32452;&#21512;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#25552;&#31034;&#8220;[&#20107;&#20214;A]&#24456;&#26377;&#21487;&#33021;&#12290;[&#20107;&#20214;B]&#19981;&#21487;&#33021;&#12290;[&#32467;&#26524;]&#20250;&#21457;&#29983;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65311; "&#26102;&#65292;&#20154;&#20204;&#21487;&#20197;&#26681;&#25454;WEP&#35789;&#27719;&#30340;&#21487;&#20449;&#24230;&#20272;&#31639;&#27010;&#29575;&#27700;&#24179;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#29616;&#26377;&#30340;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35266;&#23519;&#21040;&#34429;&#28982;&#23427;&#20204;&#22312;&#39044;&#27979;WEP&#30340;&#23384;&#22312;&#26102;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;&#19982;&#27599;&#20010;&#35789;&#30456;&#20851;&#32852;&#30340;&#20849;&#35782;&#27010;&#29575;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Words of estimative probability (WEP) are expressions of a statement's plausibility (probably, maybe, likely, doubt, likely, unlikely, impossible...). Multiple surveys demonstrate the agreement of human evaluators when assigning numerical probability levels to WEP. For example, highly likely corresponds to a median chance of 0.90+-0.08 in Fagen-Ulmschneider (2015)'s survey. In this work, we measure the ability of neural language processing models to capture the consensual probability level associated to each WEP. Firstly, we use the UNLI dataset (Chen et al., 2020) which associates premises and hypotheses with their perceived joint probability p, to construct prompts, e.g. "[PREMISE]. [WEP], [HYPOTHESIS]." and assess whether language models can predict whether the WEP consensual probability level is close to p. Secondly, we construct a dataset of WEP-based probabilistic reasoning, to test whether language models can reason with WEP compositions. When prompted "[EVENTA] is likely. [EVEN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#21644;&#22330;&#26223;&#22270;&#23398;&#20064;&#30340;&#24322;&#26500;&#36947;&#36335;&#20195;&#29702;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24322;&#26500;&#39118;&#38505;&#22270;&#21644;&#20998;&#23618;&#22330;&#26223;&#22270;&#36827;&#34892;&#20132;&#20114;&#24314;&#27169;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.00848</link><description>&lt;p&gt;
&#22522;&#20110;&#39118;&#38505;&#21644;&#22330;&#26223;&#22270;&#23398;&#20064;&#30340;&#24322;&#26500;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Trajectory Forecasting via Risk and Scene Graph Learning. (arXiv:2211.00848v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#21644;&#22330;&#26223;&#22270;&#23398;&#20064;&#30340;&#24322;&#26500;&#36947;&#36335;&#20195;&#29702;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24322;&#26500;&#39118;&#38505;&#22270;&#21644;&#20998;&#23618;&#22330;&#26223;&#22270;&#36827;&#34892;&#20132;&#20114;&#24314;&#27169;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#36712;&#36857;&#39044;&#27979;&#23545;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#24314;&#27169;&#24322;&#26500;&#36947;&#36335;&#20195;&#29702;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#20851;&#31995;&#21450;&#20854;&#20195;&#29702;-&#29615;&#22659;&#32422;&#26463;&#30340;&#38590;&#24230;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#21644;&#22330;&#26223;&#22270;&#23398;&#20064;&#30340;&#24322;&#26500;&#36947;&#36335;&#20195;&#29702;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#23427;&#30001;&#24322;&#26500;&#39118;&#38505;&#22270;&#21644;&#20998;&#23618;&#22330;&#26223;&#22270;&#32452;&#25104;&#65292;&#20174;&#20195;&#29702;&#31867;&#21035;&#21644;&#23427;&#20204;&#21487;&#31227;&#21160;&#30340;&#35821;&#20041;&#21306;&#22495;&#20004;&#20010;&#26041;&#38754;&#26469;&#32771;&#34385;&#12290;&#24322;&#26500;&#39118;&#38505;&#22270;&#23558;&#27599;&#31181;&#36947;&#36335;&#20195;&#29702;&#20998;&#32452;&#65292;&#24182;&#22522;&#20110;&#26377;&#25928;&#30340;&#30896;&#25758;&#39118;&#38505;&#24230;&#37327;&#35745;&#31639;&#23427;&#20204;&#30340;&#20132;&#20114;&#37051;&#25509;&#30697;&#38453;&#12290;&#39550;&#39542;&#22330;&#26223;&#30340;&#20998;&#23618;&#22330;&#26223;&#22270;&#36890;&#36807;&#25512;&#26029;&#36947;&#36335;&#20195;&#29702;&#21644;&#30001;&#36947;&#36335;&#22330;&#26223;&#35821;&#27861;&#23545;&#40784;&#30340;&#36947;&#36335;&#35821;&#20041;&#24067;&#23616;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#24314;&#27169;&#12290;&#22522;&#20110;&#36825;&#31181;&#24418;&#24335;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#39550;&#39542;&#24773;&#20917;&#19979;&#33719;&#24471;&#26377;&#25928;&#30340;&#36712;&#36857;&#39044;&#27979;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous trajectory forecasting is critical for intelligent transportation systems, but it is challenging because of the difficulty of modeling the complex interaction relations among the heterogeneous road agents as well as their agent-environment constraints. In this work, we propose a risk and scene graph learning method for trajectory forecasting of heterogeneous road agents, which consists of a Heterogeneous Risk Graph (HRG) and a Hierarchical Scene Graph (HSG) from the aspects of agent category and their movable semantic regions. HRG groups each kind of road agent and calculates their interaction adjacency matrix based on an effective collision risk metric. HSG of the driving scene is modeled by inferring the relationship between road agents and road semantic layout aligned by the road scene grammar. Based on this formulation, we can obtain effective trajectory forecasting in driving situations, and superior performance to other state-of-the-art approaches is demonstrated by
&lt;/p&gt;</description></item><item><title>GFlowOut&#26159;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;Dropout&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#22797;&#26434;&#30340;&#21518;&#39564;&#20998;&#24067;&#21644;&#26679;&#26412;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#33391;&#22909;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2210.12928</link><description>&lt;p&gt;
GFlowOut&#65306;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;Dropout
&lt;/p&gt;
&lt;p&gt;
GFlowOut: Dropout with Generative Flow Networks. (arXiv:2210.12928v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12928
&lt;/p&gt;
&lt;p&gt;
GFlowOut&#26159;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;Dropout&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#22797;&#26434;&#30340;&#21518;&#39564;&#20998;&#24067;&#21644;&#26679;&#26412;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#33391;&#22909;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#29702;&#20026;&#35299;&#20915;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#35768;&#22810;&#20851;&#38190;&#38382;&#39064;&#65288;&#20363;&#22914;&#19981;&#33391;&#26657;&#20934;&#21644;&#27867;&#21270;&#65292;&#20197;&#21450;&#25968;&#25454;&#25928;&#29575;&#65289;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23558;&#36125;&#21494;&#26031;&#25512;&#29702;&#25193;&#23637;&#21040;&#22823;&#22411;&#26550;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24182;&#19988;&#38656;&#35201;&#20005;&#26684;&#30340;&#36817;&#20284;&#12290;Monte Carlo Dropout&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#36817;&#20284;&#25512;&#29702;&#30340;&#30456;&#23545;&#20415;&#23452;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;dropout&#25513;&#30721;&#26159;&#20174;&#22266;&#23450;&#20998;&#24067;&#20013;&#29420;&#31435;&#37319;&#26679;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;dropout&#25513;&#30721;&#21487;&#20197;&#34987;&#35270;&#20026;&#28508;&#22312;&#21464;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#36827;&#34892;&#25512;&#26029;&#12290;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65306;&#65288;a&#65289;&#25513;&#30721;&#30340;&#21518;&#39564;&#20998;&#24067;&#21487;&#33021;&#39640;&#24230;&#22810;&#27169;&#24577;&#65292;&#24456;&#38590;&#29992;&#26631;&#20934;&#21464;&#20998;&#25512;&#29702;&#36827;&#34892;&#36817;&#20284;&#65307;&#65288;b&#65289;&#20805;&#20998;&#21033;&#29992;dropout&#25513;&#30721;&#20043;&#38388;&#30340;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#21644;&#30456;&#20851;&#24615;&#20197;&#25913;&#21892;&#21518;&#39564;&#20272;&#35745;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GFlowOut&#65292;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#27169;&#25311;dropout&#25513;&#30721;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#27169;&#25311;&#22797;&#26434;&#30340;&#25513;&#30721;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#25513;&#30721;&#20043;&#38388;&#30340;&#26679;&#26412;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GFlowOut&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. However, scaling Bayesian inference to large architectures is challenging and requires restrictive approximations. Monte Carlo Dropout has been widely used as a relatively cheap way for approximate Inference and to estimate uncertainty with deep neural networks. Traditionally, the dropout mask is sampled independently from a fixed distribution. Recent works show that the dropout mask can be viewed as a latent variable, which can be inferred with variational inference. These methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. In this work, we propose GF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;DiffAN&#25299;&#25169;&#25490;&#24207;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#25628;&#32034;&#31354;&#38388;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.06201</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25490;&#24207;&#30340;&#22240;&#26524;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Causal Discovery via Topological Ordering. (arXiv:2210.06201v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;DiffAN&#25299;&#25169;&#25490;&#24207;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#25628;&#32034;&#31354;&#38388;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#21040;&#23558;&#21151;&#33021;&#20851;&#31995;&#32422;&#26463;&#20026;&#38750;&#32447;&#24615;&#24102;&#26377;&#21152;&#24615;&#22122;&#22768;&#65288;ANM&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#25104;&#20026;&#21487;&#33021;&#12290;&#21363;&#20351;&#24102;&#26377;&#24378;&#22823;&#30340;&#20551;&#35774;&#65292;&#22240;&#26524;&#21457;&#29616;&#20063;&#28041;&#21450;&#21040;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#31354;&#38388;&#20013;&#36827;&#34892;&#26114;&#36149;&#30340;&#25628;&#32034;&#38382;&#39064;&#12290;&#25299;&#25169;&#25490;&#24207;&#26041;&#27861;&#36890;&#36807;&#22312;&#25490;&#21015;&#31354;&#38388;&#20013;&#25628;&#32034;&#32780;&#19981;&#26159;&#22270;&#24418;&#31354;&#38388;&#20013;&#25628;&#32034;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#22240;&#26524;&#21457;&#29616;&#20248;&#21270;&#31354;&#38388;&#12290;&#23545;&#20110;ANMs&#65292;&#21487;&#20197;&#20351;&#29992;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#30340;Hessian&#26469;&#25214;&#21040;&#22240;&#26524;&#22270;&#20013;&#30340;&#21494;&#33410;&#28857;&#65292;&#20174;&#32780;&#20801;&#35768;&#23427;&#30340;&#25299;&#25169;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#33719;&#21462;Hessian&#30340;&#35745;&#31639;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#25193;&#23637;&#20026;&#21464;&#37327;&#25968;&#37327;&#21644;&#26679;&#26412;&#25968;&#37327;&#22686;&#21152;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#26368;&#36817;&#21019;&#26032;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffAN&#30340;&#25299;&#25169;&#25490;&#24207;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal relations from observational data becomes possible with additional assumptions such as considering the functional relations to be constrained as nonlinear with additive noise (ANM). Even with strong assumptions, causal discovery involves an expensive search problem over the space of directed acyclic graphs (DAGs). \emph{Topological ordering} approaches reduce the optimisation space of causal discovery by searching over a permutation rather than graph space. For ANMs, the \emph{Hessian} of the data log-likelihood can be used for finding leaf nodes in a causal graph, allowing its topological ordering. However, existing computational methods for obtaining the Hessian still do not scale as the number of variables and the number of samples increase. Therefore, inspired by recent innovations in diffusion probabilistic models (DPMs), we propose \emph{DiffAN}\footnote{Implementation is available at \url{https://github.com/vios-s/DiffAN} .}, a topological ordering algorithm t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#24378;&#20551;&#30456;&#20851;&#30340;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#23454;&#29616;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#30340;&#31639;&#27861; DCWP&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05247</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Debiased Subnetworks with Contrastive Weight Pruning. (arXiv:2210.05247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#24378;&#20551;&#30456;&#20851;&#30340;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#23454;&#29616;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#30340;&#31639;&#27861; DCWP&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#20559;&#32622;&#24615;&#65292;&#23548;&#33268;&#25552;&#20379;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#32479;&#35745;&#35777;&#25454;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#22312;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#21151;&#33021;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#25506;&#32034;&#20855;&#26377;&#24378;&#20551;&#30456;&#20851;&#24615;&#30340;&#26080;&#20559;&#23376;&#32593;&#32476;&#23384;&#22312;&#38480;&#21046;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#20559;&#24046;&#20914;&#31361;&#26679;&#26412;&#23545;&#32467;&#26500;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#30340;&#65288;&#20266;&#65289;&#26080;&#20559;&#26679;&#26412;&#21644;&#36873;&#25321;&#24615;&#20559;&#24046;&#20914;&#31361;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#21435;&#20559;&#32622;&#23545;&#27604;&#21098;&#26525;&#65288;DCWP&#65289;&#31639;&#27861;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102; DCWP &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are often biased to spuriously correlated features that provide misleading statistical evidence that does not generalize. This raises an interesting question: ``Does an optimal unbiased functional subnetwork exist in a severely biased network? If so, how to extract such subnetwork?" While empirical evidence has been accumulated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then further elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbi
&lt;/p&gt;</description></item><item><title>KSAT&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#24341;&#20837;&#30693;&#35782;&#24341;&#23548;&#20559;&#35265;&#26469;&#25972;&#21512;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#33258;&#25105;&#20851;&#27880;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2210.04307</link><description>&lt;p&gt;
KSAT&#65306;&#30693;&#35782;&#27880;&#20837;&#30340;&#33258;&#25105;&#20851;&#27880;&#21464;&#21387;&#22120;&#8212;&#8212;&#25972;&#21512;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
KSAT: Knowledge-infused Self Attention Transformer -- Integrating Multiple Domain-Specific Contexts. (arXiv:2210.04307v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04307
&lt;/p&gt;
&lt;p&gt;
KSAT&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#24341;&#20837;&#30693;&#35782;&#24341;&#23548;&#20559;&#35265;&#26469;&#25972;&#21512;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#33258;&#25105;&#20851;&#27880;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#29702;&#35299;&#38656;&#35201;&#25972;&#21512;&#22810;&#20010;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#8220;&#25105;&#25343;&#30528;&#19968;&#25226;&#26538;&#65292;&#23545;&#25105;&#30340;&#29983;&#27963;&#24863;&#21040;&#24456;&#31967;&#31957;&#65292;&#22914;&#26524;&#26126;&#22825;&#25105;&#19981;&#37266;&#26469;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#26368;&#31967;&#31957;&#30340;&#20107;&#24773;&#8221;&#65292;&#20854;&#20013;&#21253;&#21547;&#33258;&#26432;&#21644;&#25233;&#37057;&#30151;&#30456;&#20851;&#30340;&#34892;&#20026;&#65288;&#22810;&#20010;&#19978;&#19979;&#25991;&#65289;&#12290;&#33258;&#27880;&#24847;&#21147;&#32467;&#26500;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#24615;&#36890;&#36807;&#22312;&#30456;&#20851;&#39046;&#22495;&#29305;&#23450;&#36164;&#28304;&#30340;&#25688;&#24405;&#19978;&#36827;&#34892;&#24494;&#35843;&#65288;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#30693;&#35782;-&#19982;&#33258;&#26432;&#21644;&#25233;&#37057;&#30151;&#30456;&#20851;&#30340;&#24515;&#29702;&#20581;&#24247;&#35786;&#26029;&#30340;&#21307;&#23398;&#25945;&#31185;&#20070;&#31456;&#33410;&#65289;&#26469;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#33258;&#25105;&#20851;&#27880;&#32467;&#26500;KSAT&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#23454;&#29616;&#20102;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#25972;&#21512;&#12290;KSAT&#22312;&#27599;&#20010;&#30693;&#35782;&#28304;&#30340;&#19987;&#38376;&#33258;&#25105;&#20851;&#27880;&#23618;&#20013;&#24341;&#20837;&#30693;&#35782;&#24341;&#23548;&#20559;&#35265;&#26469;&#23436;&#25104;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;KSAT&#25552;&#20379;&#20102;&#25511;&#21046;&#23398;&#20064;&#21644;&#30693;&#35782;&#21033;&#29992;&#20043;&#38388;&#26435;&#34913;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-specific language understanding requires integrating multiple pieces of relevant contextual information. For example, we see both suicide and depression-related behavior (multiple contexts) in the text ``I have a gun and feel pretty bad about my life, and it wouldn't be the worst thing if I didn't wake up tomorrow''. Domain specificity in self-attention architectures is handled by fine-tuning on excerpts from relevant domain specific resources (datasets and external knowledge - medical textbook chapters on mental health diagnosis related to suicide and depression). We propose a modified self-attention architecture Knowledge-infused Self Attention Transformer (KSAT) that achieves the integration of multiple domain-specific contexts through the use of external knowledge sources. KSAT introduces knowledge-guided biases in dedicated self-attention layers for each knowledge source to accomplish this. In addition, KSAT provides mechanics for controlling the trade-off between learning 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;OOD&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#25915;&#20987;&#31867;&#22411;&#35270;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#24212;&#29992;&#39118;&#38505;&#22806;&#25512;&#26041;&#27861;&#23454;&#29616;&#23545;&#21508;&#25915;&#20987;&#30340;&#30456;&#20284;&#40065;&#26834;&#24615;&#27700;&#24179;&#65292;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#30340;&#26356;&#39640;&#24615;&#33021;&#65292;&#26159;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#20013;&#30340;&#21019;&#26032;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2210.03150</link><description>&lt;p&gt;
&#36808;&#21521;&#38754;&#21521;OOD&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Out-of-Distribution Adversarial Robustness. (arXiv:2210.03150v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03150
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;OOD&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#25915;&#20987;&#31867;&#22411;&#35270;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#24212;&#29992;&#39118;&#38505;&#22806;&#25512;&#26041;&#27861;&#23454;&#29616;&#23545;&#21508;&#25915;&#20987;&#30340;&#30456;&#20284;&#40065;&#26834;&#24615;&#27700;&#24179;&#65292;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#30340;&#26356;&#39640;&#24615;&#33021;&#65292;&#26159;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#20013;&#30340;&#21019;&#26032;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#23545;&#19968;&#31181;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24448;&#24448;&#19981;&#33021;&#36716;&#31227;&#21040;&#20854;&#20182;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#37319;&#29992;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#24120;&#29992;&#25915;&#20987;&#20013;&#25913;&#21892;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#27599;&#31181;&#25915;&#20987;&#35270;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#24182;&#24212;&#29992;&#39118;&#38505;&#22806;&#25512;&#26041;&#27861;&#65288;REx&#65289;&#65292;&#20419;&#36827;&#23545;&#25152;&#26377;&#35757;&#32451;&#25915;&#20987;&#30340;&#30456;&#20284;&#40065;&#26834;&#27700;&#24179;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#30475;&#21040;&#30340;&#25915;&#20987;&#19978;&#65292;&#25105;&#20204;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#39640;&#32423;&#21035;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#23478;&#26063;&#25110;&#27979;&#35797;&#26102;&#21482;&#36935;&#21040;&#30340;&#25915;&#20987;&#30340;&#35843;&#25972;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#22312;&#25915;&#20987;&#38598;&#21512;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;MNIST&#30340;&#26368;&#20339;&#29616;&#26377;&#22522;&#32447;&#30340;&#20934;&#30830;&#24615;&#20174;3.4%&#25552;&#39640;&#21040;25.9&#65285;&#65292;&#22312;CIFAR10&#19978;&#20174;16.9&#65285;&#25552;&#39640;&#21040;23.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness continues to be a major challenge for deep learning. A core issue is that robustness to one type of attack often fails to transfer to other attacks. While prior work establishes a theoretical trade-off in robustness against different $L_p$ norms, we show that there is potential for improvement against many commonly used attacks by adopting a domain generalisation approach. Concretely, we treat each type of attack as a domain, and apply the Risk Extrapolation method (REx), which promotes similar levels of robustness against all training attacks. Compared to existing methods, we obtain similar or superior worst-case adversarial robustness on attacks seen during training. Moreover, we achieve superior performance on families or tunings of attacks only encountered at test time. On ensembles of attacks, our approach improves the accuracy from 3.4% with the best existing baseline to 25.9% on MNIST, and from 16.9% to 23.5% on CIFAR10.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#20934;&#21017; RankMe&#65292;&#36890;&#36807;&#35780;&#20272;&#26377;&#25928;&#25490;&#21517;&#65292;&#21487;&#20197;&#25351;&#31034;&#23398;&#20064;JE-SSL&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2210.02885</link><description>&lt;p&gt;
&#36890;&#36807;&#25490;&#21517;&#35780;&#20272;&#39044;&#35757;&#32451;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#19979;&#28216;&#24615;&#33021;&#65306;RankMe
&lt;/p&gt;
&lt;p&gt;
RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank. (arXiv:2210.02885v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#20934;&#21017; RankMe&#65292;&#36890;&#36807;&#35780;&#20272;&#26377;&#25928;&#25490;&#21517;&#65292;&#21487;&#20197;&#25351;&#31034;&#23398;&#20064;JE-SSL&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;JE-SSL&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20351;&#24471;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#21464;&#21270;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#21407;&#21017;&#24615;&#25351;&#23548;&#26041;&#38024;&#65292;&#33021;&#22815;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#25104;&#21151;&#22320;&#37096;&#32626;&#23427;&#20204;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#20934;&#21017;&#65292;&#21363;&#25928;&#26524;&#25490;&#21517;&#65292;&#21487;&#20197;&#25351;&#31034;&#23398;&#20064;JE-SSL&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#32780;&#19988;&#35745;&#31639;&#21451;&#22909;&#65292;&#29978;&#33267;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;JE-SSL&#34920;&#31034;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations but only few principled guidelines that would help practitioners to successfully deploy them. The main reason for that pitfall comes from JE-SSL's core principle of not employing any input reconstruction therefore lacking visual cues of unsuccessful training. Adding non informative loss values to that, it becomes difficult to deploy SSL on a new dataset for which no labels can help to judge the quality of the learned representation. In this study, we develop a simple unsupervised criterion that is indicative of the quality of the learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method -- coined RankMe -- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels. A further benefit of RankMe is that it does not have any training or hyper-p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19968;&#31995;&#21015;&#20998;&#23376;&#34920;&#24449;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#22522;&#20110;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.13492</link><description>&lt;p&gt;
&#26242;&#20572;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#65306;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taking a Respite from Representation Learning for Molecular Property Prediction. (arXiv:2209.13492v3 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19968;&#31995;&#21015;&#20998;&#23376;&#34920;&#24449;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#22522;&#20110;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20854;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#23601;&#26159;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#34429;&#28982;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#30340;&#25216;&#26415;&#22914;&#27492;&#21457;&#36798;&#65292;&#20294;&#20854;&#32972;&#21518;&#30340;&#22522;&#30784;&#38382;&#39064;&#21364;&#26410;&#34987;&#35748;&#30495;&#25506;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#20998;&#23376;&#34920;&#24449;&#23545;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;MoleculeNet&#22522;&#20934;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;ChEMBL&#25968;&#25454;&#24211;&#21644;&#25991;&#29486;&#20013;&#25910;&#38598;&#20102;&#19968;&#22871;&#19982;&#38463;&#29255;&#31867;&#29289;&#36136;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#20004;&#20010;&#39069;&#22806;&#30340;&#27963;&#24615;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#32452;&#35013;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#21516;&#35268;&#27169;&#30340;&#25551;&#36848;&#31526;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;62,820&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;50,220&#20010;&#20351;&#29992;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#12289;4,200&#20010;&#20351;&#29992;SMILES&#24207;&#21015;&#30340;&#27169;&#22411;&#21644;8,400&#20010;&#20351;&#29992;&#20998;&#23376;&#22270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#25968;&#25454;&#38598;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#20102;&#38463;&#29255;&#31867;&#29289;&#36136;&#20013;&#30340;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has been widely applied in drug discovery with a major task as molecular property prediction. Despite booming techniques in molecular representation learning, fundamentals underlying molecular property prediction haven't been carefully examined yet. In this study, we conducted a systematic evaluation on a collection of representative models using various molecular representations. In addition to the commonly used MoleculeNet benchmark datasets, we also assembled a suite of opioids-related datasets from ChEMBL and two additional activity datasets from literature. To interrogate the basic predictive power, we also assembled a series of descriptors datasets with varying sizes to evaluate the models' performance. In total, we trained 62,820 models, including 50,220 models on fixed representations, 4,200 models on SMILES sequences and 8,400 models on molecular graphs. We first conducted dataset profiling and highlighted the activity-cliffs issue in the opioids-r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#12289;&#20855;&#26377;&#33258;&#27965;&#24615;&#30340;&#30446;&#26631;&#65292;&#23427;&#20849;&#21516;&#20248;&#21270;&#20102;&#38544;&#31354;&#38388;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#39640;&#22238;&#25253;&#65292;&#20174;&#32780;&#31616;&#21270;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.08466</link><description>&lt;p&gt;
&#31616;&#21270;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#36890;&#36807;&#19968;&#20010;&#30446;&#26631;&#23398;&#20064;&#34920;&#31034;&#12289;&#38544;&#31354;&#38388;&#27169;&#22411;&#21644;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective. (arXiv:2209.08466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08466
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#12289;&#20855;&#26377;&#33258;&#27965;&#24615;&#30340;&#30446;&#26631;&#65292;&#23427;&#20849;&#21516;&#20248;&#21270;&#20102;&#38544;&#31354;&#38388;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#39640;&#22238;&#25253;&#65292;&#20174;&#32780;&#31616;&#21270;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23398;&#20064;&#29615;&#22659;&#20869;&#37096;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#21487;&#33021;&#27604;&#20854;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25163;&#26356;&#20855;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#23398;&#20064;&#20174;&#39640;&#32500;&#20256;&#24863;&#22120;&#20013;&#27169;&#25311;&#21407;&#22987;&#35266;&#23519;&#32467;&#26524;&#30340;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) methods that learn an internal model of the environment have the potential to be more sample efficient than their model-free counterparts, learning to model raw observations from high dimensional sensors can be challenging. Prior work has addressed this challenge by learning low-dimensional representation of observations through auxiliary objectives, such as reconstruction or value prediction. However, the alignment between these auxiliary objectives and the RL objective is often unclear. In this work, we propose a single objective which jointly optimizes a latent-space model and policy to achieve high returns while remaining self-consistent. This objective is a lower bound on expected returns. Unlike prior bounds for model-based RL on policy exploration or model guarantees, our bound is directly on the overall RL objective. We demonstrate that the resulting algorithm matches or improves the sample-efficiency of the best prior model-based and model-fre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2209.05732</link><description>&lt;p&gt;
R\'{e}nyi&#25955;&#24230;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#8212;&#8212;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#65288;DML&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#36825;&#31181;&#20570;&#27861;&#26356;&#21152;&#28789;&#27963;&#12289;&#21487;&#35843;&#65292;&#20197;&#25913;&#21892;vanilla DML&#12290;&#36825;&#31181;&#20462;&#25913;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#38468;&#21152;&#22797;&#26434;&#24615;&#19979;&#19981;&#26029;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#33539;&#20363;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#19988;&#34920;&#26126;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#38750;&#20984;&#20248;&#21270;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#25910;&#25947;&#30340;&#20559;&#24046;&#20026;$\mathcal{O}(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Transformer&#22312;U.S Patent Phrase to Phrase Matching Dataset&#19978;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.11716</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#35821;&#26009;&#24211;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#30340;&#35748;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Transformer&#22312;U.S Patent Phrase to Phrase Matching Dataset&#19978;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#21644;&#24314;&#27169;&#26159;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35768;&#22810;&#20808;&#39537;&#24212;&#29992;&#20013;&#22522;&#26412;&#35748;&#21487;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#39034;&#24207;&#27169;&#24335;&#35782;&#21035;&#30340;&#24863;&#30693;&#65292;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;RNN&#21644;LSTM&#65289;&#22312;&#35821;&#20041;&#30456;&#20284;&#24230;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#26080;&#27861;&#20197;&#38750;&#39034;&#24207;&#26041;&#24335;&#22788;&#29702;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#34987;&#35748;&#20026;&#25928;&#29575;&#20302;&#19979;&#65292;&#20174;&#32780;&#23548;&#33268;&#19978;&#19979;&#25991;&#25552;&#21462;&#19981;&#24403;&#12290;Transformer&#22240;&#20854;&#38750;&#39034;&#24207;&#25968;&#25454;&#22788;&#29702;&#21644;&#33258;&#25105;&#20851;&#27880;&#31561;&#20248;&#21183;&#32780;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#12290;&#26412;&#25991;&#20351;&#29992;&#20256;&#32479;&#21644;&#22522;&#20110;transformer&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#19987;&#21033;&#30701;&#35821;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#19981;&#21516;&#29256;&#26412;&#30340;&#35299;&#30721;&#22686;&#24378;BERT-DeBERTa&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;K&#25240;&#20132;&#21449;&#39564;&#35777;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity analysis and modeling is a fundamentally acclaimed task in many pioneering applications of natural language processing today. Owing to the sensation of sequential pattern recognition, many neural networks like RNNs and LSTMs have achieved satisfactory results in semantic similarity modeling. However, these solutions are considered inefficient due to their inability to process information in a non-sequential manner, thus leading to the improper extraction of context. Transformers function as the state-of-the-art architecture due to their advantages like non-sequential data processing and self-attention. In this paper, we perform semantic similarity analysis and modeling on the U.S Patent Phrase to Phrase Matching Dataset using both traditional and transformer-based techniques. We experiment upon four different variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by performing K-Fold Cross-Validation. The experimental results demonstrate our me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.12617</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23884;&#20837;&#26159;&#36890;&#36807;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#21040;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#26469;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#32467;&#26500;&#25110;&#22522;&#20110;&#25551;&#36848;&#12290;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#20013;&#26377;&#38480;&#32467;&#26500;&#20449;&#24687;&#19979;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#12290;&#22522;&#20110;&#25551;&#36848;&#30340;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#20808;&#21069;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#36229;&#36234;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23384;&#22312;&#26114;&#36149;&#30340;&#36127;&#37319;&#26679;&#21644;&#38480;&#21046;&#24615;&#25551;&#36848;&#38656;&#27714;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMKE&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#65292;&#26088;&#22312;&#20016;&#23500;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#34920;&#36848;&#22522;&#20110;&#25551;&#36848;&#30340;&#30693;&#35782;&#23884;&#20837;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#35780;&#20215;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMKE&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#20808;&#21069;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#23454;&#20307;&#23884;&#20837;&#21644;&#36923;&#36753;&#35268;&#21017;&#25366;&#25496;&#30456;&#32467;&#21512;&#30340;&#26032;&#27169;&#22411;DegreEmbed&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DegreEmbed&#22312;&#38142;&#25509;&#39044;&#27979;&#21644;&#35268;&#21017;&#25552;&#21462;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.09933</link><description>&lt;p&gt;
DegreEmbed: &#23558;&#23454;&#20307;&#23884;&#20837;&#34701;&#20837;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DegreEmbed: incorporating entity embedding into logic rule learning for knowledge graph reasoning. (arXiv:2112.09933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#23454;&#20307;&#23884;&#20837;&#21644;&#36923;&#36753;&#35268;&#21017;&#25366;&#25496;&#30456;&#32467;&#21512;&#30340;&#26032;&#27169;&#22411;DegreEmbed&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DegreEmbed&#22312;&#38142;&#25509;&#39044;&#27979;&#21644;&#35268;&#21017;&#25552;&#21462;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#34920;&#31034;&#65292;&#23427;&#20204;&#26159;&#26234;&#33021;&#25968;&#25454;&#24211;&#65292;&#23558;&#20154;&#31867;&#30693;&#35782;&#34701;&#20837;&#20854;&#20013;&#65292;&#24110;&#21161;&#26426;&#22120;&#27169;&#20223;&#20154;&#31867;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#38750;&#24120;&#24222;&#22823;&#65292;&#32780;&#19988;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#38382;&#31572;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#24212;&#29992;&#12290;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#36890;&#36807;&#22522;&#20110;&#29616;&#26377;&#30693;&#35782;&#25512;&#29702;&#26469;&#23436;&#25104;&#32570;&#22833;&#20107;&#23454;&#30340;&#20219;&#21153;&#12290;&#20027;&#35201;&#26377;&#20004;&#31181;&#30740;&#31350;&#27969;&#27966;&#65306;&#19968;&#31181;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20302;&#32500;&#24230;&#23884;&#20837;&#65292;&#21487;&#20197;&#25506;&#32034;&#28508;&#22312;&#27169;&#24335;&#65307;&#21478;&#19968;&#31181;&#36890;&#36807;&#25366;&#25496;&#36923;&#36753;&#35268;&#21017;&#33719;&#24471;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28041;&#21450;&#21508;&#31181;&#31867;&#22411;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#29616;&#20195;&#30693;&#35782;&#22270;&#35889;&#30340;&#24322;&#36136;&#24615;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DegreEmbed&#65292;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#23558;&#22522;&#20110;&#23884;&#20837;&#30340;&#23398;&#20064;&#21644;&#36923;&#36753;&#35268;&#21017;&#25366;&#25496;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;DegreEmbed&#23558;&#23454;&#20307;&#23884;&#20837;&#34701;&#20837;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#20013;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#23454;&#20307;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DegreEmbed&#22312;&#38142;&#25509;&#39044;&#27979;&#21644;&#35268;&#21017;&#25552;&#21462;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs), as structured representations of real world facts, are intelligent databases incorporating human knowledge that can help machine imitate the way of human problem solving. However, KGs are usually huge and there are inevitably missing facts in KGs, thus undermining applications such as question answering and recommender systems that are based on knowledge graph reasoning. Link prediction for knowledge graphs is the task aiming to complete missing facts by reasoning based on the existing knowledge. Two main streams of research are widely studied: one learns low-dimensional embeddings for entities and relations that can explore latent patterns, and the other gains good interpretability by mining logical rules. Unfortunately, the heterogeneity of modern KGs that involve entities and relations of various types is not well considered in the previous studies. In this paper, we propose DegreEmbed, a model that combines embedding-based learning and logic rule mining for 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2110.13398</link><description>&lt;p&gt;
&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis. (arXiv:2110.13398v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26088;&#22312;&#30830;&#23450;&#23545;&#26576;&#20010;&#26041;&#38754;&#30340;&#24773;&#24863;&#20542;&#21521;&#12290;&#30001;&#20110;&#26114;&#36149;&#19988;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#39044;&#35757;&#32451;&#31574;&#30053;&#24050;&#25104;&#20026;ABSA&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#24635;&#26159;&#23384;&#22312;&#20005;&#37325;&#30340;&#39046;&#22495;&#20559;&#31227;&#65292;&#30452;&#25509;&#24494;&#35843;&#26102;&#30340;&#30693;&#35782;&#36716;&#31227;&#25928;&#26524;&#19981;&#20339;&#65292;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#20122;&#20248;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#39046;&#22495;&#20559;&#31227;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#30693;&#35782;&#23618;&#38754;&#30340;&#23545;&#40784;&#65292;&#23558;&#20854;&#34701;&#20837;&#21040;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27969;&#31243;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#38454;&#27573;&#26816;&#32034;&#37319;&#26679;&#26041;&#27861;&#65292;&#20174;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#39046;&#22495;&#23454;&#20363;&#30340;&#23545;&#40784;&#65288;&#31532;&#19968;&#38454;&#27573;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#25351;&#23548;&#30340;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#26725;&#25509;&#30693;&#35782;&#23618;&#38754;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;ABSA&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#36229;&#36234;&#24378;&#22522;&#32447;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;ABSA&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;SemEval 2014&#20219;&#21153;4&#12289;SemEval 2015&#20219;&#21153;12&#21644;SemEval 2016&#20219;&#21153;5&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment polarity towards an aspect. Because of the expensive and limited labelled data, the pretraining strategy has become the de-facto standard for ABSA. However, there always exists severe domain shift between the pretraining and downstream ABSA datasets, hindering the effective knowledge transfer when directly finetuning and making the downstream task performs sub-optimal. To mitigate such domain shift, we introduce a unified alignment pretraining framework into the vanilla pretrain-finetune pipeline with both instance- and knowledge-level alignments. Specifically, we first devise a novel coarse-to-fine retrieval sampling approach to select target domain-related instances from the large-scale pretraining dataset, thus aligning the instances between pretraining and target domains (First Stage). Then, we introduce a knowledge guidance-based strategy to further bridge the domain gap at the knowledge level. In practice, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#26679;&#26412;&#21512;&#25104;&#21644;&#35757;&#32451;&#65288;CSST&#65289;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#24403;&#21069;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#30340;&#35821;&#35328;&#20559;&#24046;&#38382;&#39064;&#65292;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29702;&#24819;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#21644;&#38382;&#39064;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.01013</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#35270;&#35273;&#38382;&#31572;&#38656;&#35201;&#21512;&#25104;&#23545;&#25239;&#26679;&#26412;&#26469;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#26679;&#26412;&#21512;&#25104;&#21644;&#35757;&#32451;&#65288;CSST&#65289;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#24403;&#21069;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#30340;&#35821;&#35328;&#20559;&#24046;&#38382;&#39064;&#65292;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29702;&#24819;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#21644;&#38382;&#39064;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#24448;&#24448;&#21482;&#25429;&#25417;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#34920;&#38754;&#35821;&#35328;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20855;&#26377;&#19981;&#21516;&#38382;&#31572;&#20998;&#24067;&#30340;&#27979;&#35797;&#38598;&#20013;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#35821;&#35328;&#20559;&#24046;&#65292;&#26368;&#36817;&#30340;&#35270;&#35273;&#38382;&#31572;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#30340;&#20165;&#38382;&#39064;&#27169;&#22411;&#26469;&#35268;&#33539;&#26377;&#38024;&#23545;&#24615;&#30340;VQA&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#35786;&#26029;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20027;&#23548;&#22320;&#20301;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#27169;&#22411;&#35774;&#35745;&#65292;&#36825;&#20123;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#26080;&#27861;&#20855;&#22791;&#29702;&#24819;&#30340;VQA&#27169;&#22411;&#30340;&#20004;&#20010;&#19981;&#21487;&#25110;&#32570;&#30340;&#29305;&#24449;&#65306;1&#65289;&#21487;&#35270;&#21270;&#35299;&#37322;&#65306;&#27169;&#22411;&#22312;&#20570;&#20915;&#31574;&#26102;&#24212;&#20381;&#36182;&#20110;&#27491;&#30830;&#30340;&#35270;&#35273;&#21306;&#22495;&#12290;2&#65289;&#38382;&#39064;&#25935;&#24863;&#65306;&#27169;&#22411;&#23545;&#38382;&#39064;&#20013;&#30340;&#35821;&#35328;&#21464;&#21270;&#24212;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#26679;&#26412;&#21512;&#25104;&#21644;&#35757;&#32451;&#65288;CSST&#65289;&#31574;&#30053;&#12290;&#32463;&#36807;CSST&#35757;&#32451;&#21518;&#65292;VQA&#27169;&#22411;&#34987;&#36843;&#20851;&#27880;&#25152;&#26377;&#20851;&#38190;&#23545;&#35937;&#21644;&#21333;&#35789;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#35270;&#35273;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's VQA models still tend to capture superficial linguistic correlations in the training set and fail to generalize to the test set with different QA distributions. To reduce these language biases, recent VQA works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on diagnostic benchmarks for out-of-distribution testing. However, due to complex model design, these ensemble-based methods are unable to equip themselves with two indispensable characteristics of an ideal VQA model: 1) Visual-explainable: The model should rely on the right visual regions when making decisions. 2) Question-sensitive: The model should be sensitive to the linguistic variations in questions. To this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy. After training with CSST, VQA models are forced to focus on all critical objects and words, which significantly improves both visu
&lt;/p&gt;</description></item><item><title>&#36817;&#31471;ID&#31639;&#27861;&#26159;&#19968;&#31181;&#23558;&#20202;&#22120;&#21464;&#37327;&#21644;&#20195;&#29702;&#30456;&#32467;&#21512;&#30340;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21033;&#29992;&#24184;&#36816;&#30340;&#22806;&#37096;&#36741;&#21161;&#25163;&#27573;&#30340;&#21516;&#26102;&#65292;&#35843;&#25972;&#26410;&#35266;&#27979;&#22240;&#32032;&#65292;&#23545;&#22810;&#20803;&#31995;&#32479;&#36827;&#34892;&#38750;&#21442;&#25968;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2108.06818</link><description>&lt;p&gt;
&#36817;&#31471;ID&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Proximal ID Algorithm. (arXiv:2108.06818v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06818
&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;ID&#31639;&#27861;&#26159;&#19968;&#31181;&#23558;&#20202;&#22120;&#21464;&#37327;&#21644;&#20195;&#29702;&#30456;&#32467;&#21512;&#30340;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21033;&#29992;&#24184;&#36816;&#30340;&#22806;&#37096;&#36741;&#21161;&#25163;&#27573;&#30340;&#21516;&#26102;&#65292;&#35843;&#25972;&#26410;&#35266;&#27979;&#22240;&#32032;&#65292;&#23545;&#22810;&#20803;&#31995;&#32479;&#36827;&#34892;&#38750;&#21442;&#25968;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#26434;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#24314;&#31435;&#26377;&#25928;&#22240;&#26524;&#32467;&#35770;&#30340;&#22522;&#26412;&#38556;&#30861;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38556;&#30861;&#65292;&#21457;&#23637;&#20102;&#20004;&#31181;&#20114;&#34917;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#24184;&#36816;&#30340;&#22806;&#37096;&#36741;&#21161;&#25163;&#27573;&#65288;&#20363;&#22914;&#20202;&#22120;&#21464;&#37327;&#25110;&#20195;&#29702;&#65289;&#65292;&#33719;&#24471;&#35782;&#21035;&#65307;&#25110;&#32773;&#36890;&#36807;ID&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#24418;&#22240;&#26524;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#23436;&#25972;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#39532;&#23572;&#21487;&#22827;&#38480;&#21046;&#26469;&#36827;&#34892;&#35782;&#21035;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#21069;&#20004;&#32773;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#22810;&#20803;&#31995;&#32479;&#20013;&#24403;&#21069;&#24050;&#30693;&#30340;&#26368;&#26222;&#36866;&#35782;&#21035;&#31639;&#27861;&#8212;&#8212;&#36817;&#31471;ID&#31639;&#27861;&#12290;&#38500;&#20102;&#33021;&#22815;&#22312;ID&#31639;&#27861;&#25104;&#21151;&#30340;&#25152;&#26377;&#24773;&#20917;&#19979;&#33719;&#24471;&#38750;&#21442;&#25968;&#35782;&#21035;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20801;&#35768;&#25105;&#20204;&#31995;&#32479;&#22320;&#21033;&#29992;&#20195;&#29702;&#26469;&#35843;&#25972;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#26434;&#22240;&#32032;&#30340;&#23384;&#22312;&#65292;&#21542;&#21017;&#36825;&#20123;&#22240;&#32032;&#20250;&#38459;&#27490;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#19968;&#31867;&#22240;&#26524;&#25512;&#26029;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unobserved confounding is a fundamental obstacle to establishing valid causal conclusions from observational data. Two complementary types of approaches have been developed to address this obstacle: obtaining identification using fortuitous external aids, such as instrumental variables or proxies, or by means of the ID algorithm, using Markov restrictions on the full data distribution encoded in graphical causal models. In this paper we aim to develop a synthesis of the former and latter approaches to identification in causal inference to yield the most general identification algorithm in multivariate systems currently known -- the proximal ID algorithm. In addition to being able to obtain nonparametric identification in all cases where the ID algorithm succeeds, our approach allows us to systematically exploit proxies to adjust for the presence of unobserved confounders that would have otherwise prevented identification. In addition, we outline a class of estimation strategies for cau
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#24615;&#29289;&#29702;&#26041;&#27861;&#26469;&#37327;&#21270;&#24320;&#25918;&#19990;&#30028;&#29289;&#29702;&#39046;&#22495;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#24868;&#24594;&#30340;&#23567;&#40479;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#35745;&#31639;&#30340;&#26816;&#27979;&#22256;&#38590;&#24230;&#19982;&#20154;&#31867;&#29992;&#25143;&#30456;&#31526;&#12290;</title><link>http://arxiv.org/abs/2106.08670</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#29289;&#29702;&#39046;&#22495;&#20013;&#26032;&#22855;&#24615;&#26816;&#27979;&#30340;&#22256;&#38590;&#24615;&#65306;&#20197;&#24868;&#24594;&#30340;&#23567;&#40479;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
The Difficulty of Novelty Detection in Open-World Physical Domains: An Application to Angry Birds. (arXiv:2106.08670v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#24615;&#29289;&#29702;&#26041;&#27861;&#26469;&#37327;&#21270;&#24320;&#25918;&#19990;&#30028;&#29289;&#29702;&#39046;&#22495;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#24868;&#24594;&#30340;&#23567;&#40479;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#35745;&#31639;&#30340;&#26816;&#27979;&#22256;&#38590;&#24230;&#19982;&#20154;&#31867;&#29992;&#25143;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#35782;&#21035;&#21644;&#21709;&#24212;&#26032;&#22855;&#24773;&#20917;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#19968;&#20010;&#20851;&#38190;&#33021;&#21147;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38754;&#20020;&#30340;&#25345;&#20037;&#24615;&#38382;&#39064;&#12290;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#65292;&#26032;&#22855;&#24615;&#21487;&#20197;&#21576;&#29616;&#22810;&#31181;&#24418;&#24335;&#65292;&#20063;&#21487;&#33021;&#24456;&#23481;&#26131;&#25110;&#24456;&#38590;&#34987;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26032;&#22855;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#19981;&#21516;&#31867;&#22411;&#26032;&#22855;&#24615;&#30340;&#26816;&#27979;&#38590;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#24615;&#29289;&#29702;&#26041;&#27861;&#26469;&#37327;&#21270;&#24320;&#25918;&#19990;&#30028;&#29289;&#29702;&#39046;&#22495;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#22256;&#38590;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#27969;&#34892;&#30340;&#29289;&#29702;&#27169;&#25311;&#28216;&#25103;&#24868;&#24594;&#30340;&#23567;&#40479;&#65292;&#24182;&#36827;&#34892;&#20102;&#38024;&#23545;&#19981;&#21516;&#26032;&#22855;&#24615;&#30340;&#29992;&#25143;&#30740;&#31350;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#35745;&#31639;&#30340;&#26816;&#27979;&#22256;&#38590;&#24230;&#19982;&#20154;&#31867;&#29992;&#25143;&#30340;&#30456;&#31526;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting and responding to novel situations in open-world environments is a key capability of human cognition and is a persistent problem for AI systems. In an open-world, novelties can appear in many different forms and may be easy or hard to detect. Therefore, to accurately evaluate the novelty detection capability of AI systems, it is necessary to investigate how difficult it may be to detect different types of novelty. In this paper, we propose a qualitative physics-based method to quantify the difficulty of novelty detection focusing on open-world physical domains. We apply our method in the popular physics simulation game Angry Birds, and conduct a user study across different novelties to validate our method. Results indicate that our calculated detection difficulties are in line with those of human users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#36890;&#36807;&#21452;&#22240;&#32032;&#25200;&#21160;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20316;&#32773;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20004;&#22240;&#32032;&#25200;&#21160;&#26465;&#20214;&#19979;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#40065;&#26834;&#30340;DL&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2103.03102</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#22240;&#32032;&#25200;&#21160;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#36890;&#36807;&#21452;&#22240;&#32032;&#25200;&#21160;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20316;&#32773;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20004;&#22240;&#32032;&#25200;&#21160;&#26465;&#20214;&#19979;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#40065;&#26834;&#30340;DL&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26469;&#35780;&#20272;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22235;&#35937;&#38480;&#32479;&#35745;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#21253;&#25324;&#26368;&#23567;&#20934;&#30830;&#24615;&#12289;&#26368;&#22823;&#20934;&#30830;&#24615;&#12289;&#24179;&#22343;&#20934;&#30830;&#24615;&#21644;&#21464;&#24322;&#31995;&#25968;&#65292;&#29992;&#20110;&#35780;&#20272;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;69&#20010;&#22522;&#20934;&#27979;&#35797;&#22270;&#20687;&#38598;&#65292;&#21253;&#25324;&#19968;&#20010;&#24178;&#20928;&#30340;&#38598;&#21512;&#12289;&#21333;&#22240;&#32032;&#25200;&#21160;&#30340;&#38598;&#21512;&#21644;&#21452;&#22240;&#32032;&#25200;&#21160;&#26465;&#20214;&#30340;&#38598;&#21512;&#65292;&#20197;&#34913;&#37327;&#40065;&#26834;&#30340;DL&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#25910;&#38598;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#39318;&#20808;&#25253;&#21578;&#20351;&#29992;&#21452;&#22240;&#32032;&#25200;&#21160;&#22270;&#20687;&#21487;&#20197;&#25552;&#39640;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#21452;&#22240;&#32032;&#25200;&#21160;&#21253;&#25324;&#65288;1&#65289;&#22312;&#20004;&#20010;&#24207;&#21015;&#20013;&#37117;&#24212;&#29992;&#30340;&#20004;&#20010;&#25968;&#23383;&#25200;&#21160;&#65288;&#26898;&#30416;&#22122;&#22768;&#21644;&#39640;&#26031;&#22122;&#22768;&#65289;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#25968;&#23383;&#25200;&#21160;&#65288;&#26898;&#30416;&#22122;&#22768;&#65289;&#21644;&#19968;&#20010;&#20960;&#20309;&#25200;&#21160;&#65288;&#26059;&#36716;&#65289;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#23545;&#20004;&#22240;&#32032;&#25200;&#21160;&#26377;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#34892;&#20026;&#65292;&#26292;&#38706;&#20102;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#19981;&#22343;&#21248;&#24615;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#24615;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20004;&#22240;&#32032;&#25200;&#21160;&#26465;&#20214;&#19979;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#40065;&#26834;&#30340;DL&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper adds to the fundamental body of work on benchmarking the robustness of deep learning (DL) classifiers. We innovate a new benchmarking methodology to evaluate robustness of DL classifiers. Also, we introduce a new four-quadrant statistical visualization tool, including minimum accuracy, maximum accuracy, mean accuracy, and coefficient of variation, for benchmarking robustness of DL classifiers. To measure robust DL classifiers, we created a comprehensive 69 benchmarking image set, including a clean set, sets with single factor perturbations, and sets with two-factor perturbation conditions. After collecting experimental results, we first report that using two-factor perturbed images improves both robustness and accuracy of DL classifiers. The two-factor perturbation includes (1) two digital perturbations (salt &amp; pepper noise and Gaussian noise) applied in both sequences, and (2) one digital perturbation (salt &amp; pepper noise) and a geometric perturbation (rotation) applied in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELL&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21327;&#21516;&#38598;&#25104;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RELL&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;&#21453;&#21521;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2004.12908</link><description>&lt;p&gt;
&#20195;&#34920;&#24615;&#38598;&#25104;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#21327;&#21516;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v16 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.12908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELL&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21327;&#21516;&#38598;&#25104;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RELL&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;&#21453;&#21521;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#24403;&#21069;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#20043;&#21069;&#21644;&#23578;&#26410;&#36935;&#21040;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21017;&#20174;&#31354;&#30333;&#29366;&#24577;&#24320;&#22987;&#65292;&#20165;&#38024;&#23545;&#21333;&#20010;&#20219;&#21153;&#20351;&#29992;&#25968;&#25454;&#12290;&#34429;&#28982;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#26410;&#26469;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#21518;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65288;&#31216;&#20026;&#36951;&#24536;&#65289;&#12290;&#36817;&#26399;&#38024;&#23545;&#36830;&#32493;&#25110;&#32456;&#36523;&#23398;&#20064;&#30340;&#35768;&#22810;&#26041;&#27861;&#37117;&#35797;&#22270;&#22312;&#32473;&#23450;&#26032;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#20165;&#21162;&#21147;&#36991;&#20813;&#24536;&#35760;&#23558;&#30446;&#26631;&#23450;&#24471;&#36807;&#20302;&#12290;&#32456;&#36523;&#23398;&#20064;&#30340;&#30446;&#26631;&#19981;&#20165;&#24212;&#35813;&#26159;&#25552;&#39640;&#26410;&#26469;&#20219;&#21153;&#65288;&#21069;&#21521;&#20256;&#36882;&#65289;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#24212;&#35813;&#26159;&#29992;&#20219;&#20309;&#26032;&#25968;&#25454;&#25552;&#39640;&#36807;&#21435;&#20219;&#21153;&#65288;&#21453;&#21521;&#20256;&#36882;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#21327;&#21516;&#38598;&#25104;&#20998;&#21035;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#38598;&#25104;&#65288;RELL&#65289;&#8221;&#65292;&#23427;&#38598;&#25104;&#20102;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#19981;&#21516;&#34920;&#31034;&#20013;&#21253;&#21547;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RELL&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#22909;&#30340;&#21453;&#21521;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
In lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. In contrast, classical machine learning, which we define as, starts from a blank slate, or tabula rasa and uses data only for the single task at hand. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be not only to improve performance on future tasks (forward transfer) but also on past tasks (backward transfer) with any new data. Our key insight is that we can synergistically ensemble representations -- that were learned independently on disparate tasks -- to enable both forward and bac
&lt;/p&gt;</description></item></channel></rss>