<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22823;&#25968;&#25454;&#26102;&#20195;&#24102;&#26469;&#20102;&#20844;&#20849;&#32039;&#24613;&#20107;&#20214;&#19979;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#30340;&#26426;&#36935;&#65292;&#20294;&#24212;&#29992;&#39046;&#22495;&#38480;&#21046;&#20102;&#36328;&#23398;&#31185;&#39044;&#27979;&#26041;&#27861;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#24635;&#32467;&#20102;&#36825;&#19968;&#39046;&#22495;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#21069;&#27839;&#30740;&#31350;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2404.01319</link><description>&lt;p&gt;
&#20844;&#20849;&#32039;&#24613;&#20107;&#20214;&#19979;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Information Cascade Prediction under Public Emergencies: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01319
&lt;/p&gt;
&lt;p&gt;
&#22823;&#25968;&#25454;&#26102;&#20195;&#24102;&#26469;&#20102;&#20844;&#20849;&#32039;&#24613;&#20107;&#20214;&#19979;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#30340;&#26426;&#36935;&#65292;&#20294;&#24212;&#29992;&#39046;&#22495;&#38480;&#21046;&#20102;&#36328;&#23398;&#31185;&#39044;&#27979;&#26041;&#27861;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#24635;&#32467;&#20102;&#36825;&#19968;&#39046;&#22495;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#21069;&#27839;&#30740;&#31350;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#28023;&#37327;&#20449;&#24687;&#12289;&#19987;&#23478;&#32463;&#39564;&#21644;&#39640;&#20934;&#30830;&#24230;&#27169;&#22411;&#20026;&#20844;&#20849;&#32039;&#24613;&#20107;&#20214;&#19979;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#24102;&#26469;&#20102;&#24040;&#22823;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#21508;&#20010;&#23398;&#31185;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#21442;&#19982;&#23548;&#33268;&#20102;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#22320;&#38663;&#12289;&#27946;&#28798;&#12289;&#20256;&#26579;&#30149;&#65289;&#19978;&#12290;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#27979;&#26694;&#26550;&#23545;&#36328;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#30340;&#20998;&#31867;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#25552;&#20379;&#20102;&#20449;&#24687;&#32423;&#32852;&#24314;&#27169;&#12289;&#39044;&#27979;&#21644;&#24212;&#29992;&#30340;&#31995;&#32479;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35782;&#21035;&#21069;&#27839;&#30740;&#31350;&#65292;&#24182;&#29702;&#35299;&#20844;&#20849;&#32039;&#24613;&#20107;&#20214;&#19979;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;&#36890;&#36807;&#24635;&#32467;&#24453;&#35299;&#20915;&#38382;&#39064;&#24182;&#27010;&#36848;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#30340;&#21442;&#32771;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01319v1 Announce Type: cross  Abstract: With the advent of the era of big data, massive information, expert experience, and high-accuracy models bring great opportunities to the information cascade prediction of public emergencies. However, the involvement of specialist knowledge from various disciplines has resulted in a primarily application-specific focus (e.g., earthquakes, floods, infectious diseases) for information cascade prediction of public emergencies. The lack of a unified prediction framework poses a challenge for classifying intersectional prediction methods across different application fields. This survey paper offers a systematic classification and summary of information cascade modeling, prediction, and application. We aim to help researchers identify cutting-edge research and comprehend models and methods of information cascade prediction under public emergencies. By summarizing open issues and outlining future directions in this field, this paper has the p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.19021</link><description>&lt;p&gt;
&#26397;&#21521;LLM-RecSys&#23545;&#40784;&#19982;&#25991;&#26412;ID&#23398;&#20064;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-RecSys Alignment with Textual ID Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19021
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#24050;&#32463;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#25512;&#33616;&#26041;&#24335;&#36716;&#21464;&#20026;&#25991;&#26412;&#29983;&#25104;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#19982;&#22266;&#26377;&#25805;&#20316;&#20154;&#31867;&#35789;&#27719;&#30340;&#26631;&#20934;NLP&#20219;&#21153;&#30456;&#21453;&#65292;&#30446;&#21069;&#29983;&#25104;&#24335;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#22312;&#22914;&#20309;&#22312;&#25991;&#26412;&#29983;&#25104;&#33539;&#24335;&#20013;&#20197;&#31616;&#27905;&#32780;&#26377;&#24847;&#20041;&#30340;ID&#34920;&#31034;&#26377;&#25928;&#32534;&#30721;&#25512;&#33616;&#39033;&#30446;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23545;&#40784;LLMs&#19982;&#25512;&#33616;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDGen&#65292;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#26631;&#35760;&#23558;&#27599;&#20010;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#12289;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#25991;&#26412;ID&#12290;&#36825;&#36890;&#36807;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#26049;&#35757;&#32451;&#25991;&#26412;ID&#29983;&#25104;&#22120;&#26469;&#23454;&#29616;&#65292;&#20351;&#20010;&#24615;&#21270;&#25512;&#33616;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#24182;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#35299;&#32806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26426;&#22120;&#27169;&#22411;&#65292;&#25903;&#25345;&#20102;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#36825;&#19968;&#35770;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.17101</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65306;&#19968;&#20010;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
AI Consciousness is Inevitable: A Theoretical Computer Science Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26426;&#22120;&#27169;&#22411;&#65292;&#25903;&#25345;&#20102;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#36825;&#19968;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#26469;&#23457;&#35270;&#24847;&#35782;&#65292;&#36825;&#26159;&#25968;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#30740;&#31350;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#30340;&#35745;&#31639;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#20026;&#24847;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#26426;&#22120;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#21463;&#21040;&#20102;&#33406;&#20262;&#183;&#22270;&#28789;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#35745;&#31639;&#27169;&#22411;&#21644;&#20271;&#32435;&#24503;&#183;&#24052;&#23572;&#26031;&#24847;&#35782;&#21095;&#22330;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#23613;&#31649;&#38750;&#24120;&#31616;&#21333;&#65292;&#36825;&#20010;&#27169;&#22411;&#22312;&#39640;&#23618;&#27425;&#19978;&#19982;&#35768;&#22810;&#20851;&#20110;&#20154;&#31867;&#21644;&#21160;&#29289;&#24847;&#35782;&#30340;&#20027;&#35201;&#31185;&#23398;&#29702;&#35770;&#30456;&#19968;&#33268;&#65292;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#26029;&#65306;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17101v1 Announce Type: new  Abstract: We look at consciousness through the lens of Theoretical Computer Science, a branch of mathematics that studies computation under resource limitations. From this perspective, we develop a formal machine model for consciousness. The model is inspired by Alan Turing's simple yet powerful model of computation and Bernard Baars' theater model of consciousness. Though extremely simple, the model aligns at a high level with many of the major scientific theories of human and animal consciousness, supporting our claim that machine consciousness is inevitable.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.15523</link><description>&lt;p&gt;
&#37319;&#29992;&#22122;&#22768;&#26631;&#35760;&#30340;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#30740;&#31350;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards auditory attention decoding with noise-tagging: A pilot study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15523
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;(AAD)&#26088;&#22312;&#20174;&#22823;&#33041;&#27963;&#21160;&#20013;&#25552;&#21462;&#34987;&#20851;&#27880;&#30340;&#35828;&#35805;&#32773;&#65292;&#25552;&#20379;&#20102;&#31070;&#32463;&#23548;&#21521;&#21548;&#35273;&#35774;&#22791;&#21644;&#33041;&#26426;&#25509;&#21475;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#26412;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;AAD&#65292;&#35813;&#21327;&#35758;&#24341;&#21457;&#20102;&#21487;&#38752;&#30340;&#32534;&#30721;&#35843;&#21046;&#35825;&#21457;&#30005;&#20301;&#65292;&#20294;&#22312;&#21548;&#35273;&#27169;&#24335;&#19979;&#30340;&#25506;&#32034;&#36824;&#24456;&#26377;&#38480;&#12290;&#30740;&#31350;&#21442;&#19982;&#32773;&#20381;&#27425;&#21576;&#29616;&#20004;&#20010;&#33655;&#20848;&#35821;&#35328;&#35821;&#38899;&#21050;&#28608;&#65292;&#36825;&#20123;&#21050;&#28608;&#34987;&#24133;&#24230;&#35843;&#21046;&#20026;&#20855;&#26377;&#21807;&#19968;&#20108;&#36827;&#21046;&#20266;&#38543;&#26426;&#22122;&#22768;&#30721;&#65292;&#26377;&#25928;&#22320;&#20026;&#20854;&#26631;&#35760;&#20102;&#38468;&#21152;&#21487;&#35299;&#30721;&#20449;&#24687;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#26410;&#35843;&#21046;&#38899;&#39057;&#19982;&#20351;&#29992;&#19981;&#21516;&#35843;&#21046;&#28145;&#24230;&#35843;&#21046;&#30340;&#38899;&#39057;&#30340;&#35299;&#30721;&#65292;&#20197;&#21450;&#20256;&#32479;AAD&#26041;&#27861;&#19982;&#26631;&#20934;&#35299;&#30721;&#22122;&#22768;&#30721;&#26041;&#27861;&#30340;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#35797;&#28857;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#26410;&#35843;&#21046;&#38899;&#39057;&#30456;&#27604;&#65292;70&#33267;100%&#30340;&#35843;&#21046;&#28145;&#24230;&#30340;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15523v1 Announce Type: cross  Abstract: Auditory attention decoding (AAD) aims to extract from brain activity the attended speaker amidst candidate speakers, offering promising applications for neuro-steered hearing devices and brain-computer interfacing. This pilot study makes a first step towards AAD using the noise-tagging stimulus protocol, which evokes reliable code-modulated evoked potentials, but is minimally explored in the auditory modality. Participants were sequentially presented with two Dutch speech stimuli that were amplitude modulated with a unique binary pseudo-random noise-code, effectively tagging these with additional decodable information. We compared the decoding of unmodulated audio against audio modulated with various modulation depths, and a conventional AAD method against a standard method to decode noise-codes. Our pilot study revealed higher performances for the conventional method with 70 to 100 percent modulation depths compared to unmodulated au
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26679;&#24335;&#28508;&#22312;&#27969;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#22312;&#29983;&#25104;&#35270;&#39057;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#26469;&#26816;&#27979;&#20551;&#35270;&#39057;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;StyleGRU&#27169;&#22359;&#21644;&#26679;&#24335;&#27880;&#24847;&#27169;&#22359;&#30340;&#32452;&#21512;&#65292;&#33021;&#26377;&#25928;&#26816;&#27979;&#35270;&#35273;&#21644;&#26102;&#38388;&#24322;&#24120;&#12290;</title><link>https://arxiv.org/abs/2403.06592</link><description>&lt;p&gt;
&#21033;&#29992;&#26679;&#24335;&#28508;&#22312;&#27969;&#26469;&#25512;&#24191;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26679;&#24335;&#28508;&#22312;&#27969;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#22312;&#29983;&#25104;&#35270;&#39057;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#26469;&#26816;&#27979;&#20551;&#35270;&#39057;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;StyleGRU&#27169;&#22359;&#21644;&#26679;&#24335;&#27880;&#24847;&#27169;&#22359;&#30340;&#32452;&#21512;&#65292;&#33021;&#26377;&#25928;&#26816;&#27979;&#35270;&#35273;&#21644;&#26102;&#38388;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#21450;&#20854;&#22312;&#29983;&#25104;&#35270;&#39057;&#30340;&#26102;&#38388;&#21464;&#21270;&#20013;&#24322;&#24120;&#34892;&#20026;&#30340;&#26816;&#27979;&#20551;&#35270;&#39057;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#30340;&#38754;&#37096;&#35270;&#39057;&#22312;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#30340;&#26102;&#38388;&#21464;&#21270;&#20013;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#29420;&#29305;&#24615;&#65292;&#36825;&#22312;&#29983;&#25104;&#20855;&#26377;&#21508;&#31181;&#38754;&#37096;&#34920;&#24773;&#21644;&#20960;&#20309;&#21464;&#25442;&#30340;&#26102;&#38388;&#31283;&#23450;&#35270;&#39057;&#26102;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;StyleGRU&#27169;&#22359;&#26469;&#34920;&#31034;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26679;&#24335;&#27880;&#24847;&#27169;&#22359;&#65292;&#23558;StyleGRU&#29983;&#25104;&#30340;&#29305;&#24449;&#19982;&#22522;&#20110;&#20869;&#23481;&#30340;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#23545;&#35270;&#35273;&#21644;&#26102;&#38388;&#24322;&#24120;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#21508;&#31181;&#22522;&#20934;&#24773;&#26223;&#19979;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36328;&#25968;&#25454;&#38598;&#21644;&#36328;&#25805;&#20316;&#24773;&#26223;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06592v1 Announce Type: cross  Abstract: This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations. Our framework utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with content-based features, enabling the detection of visual and temporal artifacts. We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios. Through f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-Selector&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#35299;&#37322;&#65292;&#39044;&#27979;&#19981;&#21516;&#35299;&#37322;&#32452;&#21512;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.18016</link><description>&lt;p&gt;
&#21160;&#24577;&#35299;&#37322;&#36873;&#25321;&#65306;&#23454;&#29616;&#21487;&#35299;&#37322;AI&#30340;&#25104;&#21151;&#29992;&#25143;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Dynamic Explanation Selection Towards Successful User-Decision Support with Explainable AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-Selector&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#35299;&#37322;&#65292;&#39044;&#27979;&#19981;&#21516;&#35299;&#37322;&#32452;&#21512;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22914;&#20309;&#20026;&#22522;&#20110;&#21487;&#35299;&#37322;AI&#30340;&#26234;&#33021;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;(IDSSs)&#36873;&#25321;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;IDSSs&#36890;&#36807;&#21487;&#35299;&#37322;AI&#29983;&#25104;&#30340;&#35299;&#37322;&#20197;&#21450;AI&#39044;&#27979;&#23637;&#31034;&#20102;&#25552;&#39640;&#29992;&#25143;&#20915;&#31574;&#30340;&#28508;&#21147;&#12290;&#30001;&#20110;&#21487;&#35299;&#37322;AI&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21508;&#31181;&#35299;&#37322;&#65292;&#25105;&#20204;&#35748;&#20026;&#22914;&#26524;&#33021;&#22815;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#25351;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#20915;&#31574;&#30340;&#35299;&#37322;&#65292;IDSSs &#30340;&#24615;&#33021;&#23558;&#24471;&#21040;&#26497;&#22823;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;X-Selector&#65292;&#19968;&#31181;&#21160;&#24577;&#36873;&#25321;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;X-Selector&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#39044;&#27979;&#19981;&#21516;&#35299;&#37322;&#32452;&#21512;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;X-Selector&#30340;&#24615;&#33021;&#19982;&#20004;&#31181;&#26420;&#32032;&#31574;&#30053;&#65288;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#21644;&#20165;&#38024;&#23545;&#26368;&#21487;&#33021;&#39044;&#27979;&#30340;&#35299;&#37322;&#65289;&#12289;&#20197;&#21450;&#20004;&#31181;&#22522;&#32447;&#26041;&#27861;&#65288;&#26080;&#35299;&#37322;&#21644;&#26080;AI&#25903;&#25345;&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;X-Selector&#26377;&#28508;&#21147;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#25512;&#33616;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18016v1 Announce Type: cross  Abstract: This paper addresses the problem of how to select explanations for XAI (Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have shown promise in improving user decisions through XAI-generated explanations along with AI predictions. As the development of XAI made various explanations available, we believe that IDSSs can be greatly improved if they can strategically select explanations that guide users to better decisions. This paper proposes X-Selector, a method for dynamically selecting explanations. X-Selector aims to guide users to better decisions by predicting the impact of different combinations of explanations on user decisions. We compared X-Selector's performance with two naive strategies (all possible explanations and explanations only for the most likely prediction) and two baselines (no explanation and no AI support). The results suggest the potential of X-Selector to guide users to recommended decisio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15189</link><description>&lt;p&gt;
&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Biomedical Entity Linking as Multiple Choice Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15189
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65288;BioEL&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#32454;&#31890;&#24230;&#21644;&#38271;&#23614;&#23454;&#20307;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BioELQA&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;BioELQA&#39318;&#20808;&#21033;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23558;&#25552;&#21450;&#21644;&#20505;&#36873;&#23454;&#20307;&#20849;&#21516;&#21576;&#29616;&#32473;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#36755;&#20986;&#19982;&#20854;&#36873;&#23450;&#23454;&#20307;&#30456;&#20851;&#30340;&#39044;&#27979;&#31526;&#21495;&#12290;&#36825;&#31181;&#20844;&#24335;&#20351;&#24471;&#19981;&#21516;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#26126;&#30830;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#25429;&#25417;&#20102;&#25552;&#21450;&#21644;&#23454;&#20307;&#20043;&#38388;&#20197;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#31934;&#32454;&#20132;&#20114;&#12290;&#20026;&#20102;&#25913;&#21892;&#38271;&#23614;&#23454;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#26816;&#32034;&#30456;&#20284;&#30340;&#24050;&#26631;&#35760;&#35757;&#32451;&#23454;&#20363;&#20316;&#20026;&#32447;&#32034;&#65292;&#24182;&#23558;&#36755;&#20837;&#19982;&#26816;&#32034;&#23454;&#20363;&#36830;&#25509;&#21040;&#29983;&#25104;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BioELQA&#30340;&#34920;&#29616;&#20248;&#20110;&#32479;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;LLM&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13457</link><description>&lt;p&gt;
LLM&#36234;&#29425;&#25915;&#20987;&#19982;&#38450;&#24481;&#25216;&#26415;&#8212;&#19968;&#39033;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;LLM&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#25104;&#20026;&#20135;&#29983;&#20855;&#26377;&#28508;&#22312;&#31038;&#20250;&#24433;&#21709;&#20869;&#23481;&#30340;&#26680;&#24515;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#29983;&#25104;&#21487;&#33021;&#34987;&#35270;&#20026;&#26377;&#23475;&#30340;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#65292;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#20102;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#20351;&#27169;&#22411;&#36755;&#20986;&#19982;&#31038;&#20250;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#36943;&#21046;&#23545;&#24694;&#24847;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#8220;&#36234;&#29425;&#8221;&#29616;&#35937;&#65292;&#21363;&#31934;&#24515;&#21046;&#20316;&#30340;&#25552;&#31034;&#24341;&#21457;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#22238;&#24212;&#30340;&#24773;&#20917;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#23545;&#29616;&#26377;&#20851;&#20110;&#36234;&#29425;LLMs&#21450;&#20854;&#38450;&#24481;&#25216;&#26415;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#20061;&#31181;&#25915;&#20987;&#25216;&#26415;&#21644;&#19971;&#31181;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#32454;&#33268;&#35843;&#26597;&#65306;Vicuna&#12289;LLama&#21644;GPT-3.5 Turbo&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#36825;&#20123;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#30333;&#30418;&#25915;&#20987;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13457v1 Announce Type: cross  Abstract: Large Language Models (LLMS) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of "jailbreaking", where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks u
&lt;/p&gt;</description></item><item><title>GeoEval&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;MMs&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;WizardMath&#27169;&#22411;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.10104</link><description>&lt;p&gt;
GeoEval&#65306;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;&#22810;&#27169;&#22411;&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10104
&lt;/p&gt;
&lt;p&gt;
GeoEval&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;MMs&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;WizardMath&#27169;&#22411;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22810;&#27169;&#22411;&#65288;MMs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#20960;&#20309;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#21363;&#38656;&#35201;&#32508;&#21512;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;GeoEval&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38598;&#21512;&#65292;&#21253;&#25324;&#19968;&#20010;&#20027;&#35201;&#23376;&#38598;&#21512;&#30340;2000&#20010;&#38382;&#39064;&#65292;&#19968;&#20010;&#37325;&#28857;&#20851;&#27880;&#21453;&#25512;&#29702;&#30340;750&#20010;&#38382;&#39064;&#23376;&#38598;&#21512;&#65292;&#19968;&#20010;&#22686;&#24378;&#23376;&#38598;&#21512;&#30340;2000&#20010;&#38382;&#39064;&#20197;&#21450;&#19968;&#20010;&#38590;&#39064;&#23376;&#38598;&#21512;&#30340;300&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;LLMs&#21644;MMs&#22312;&#35299;&#20915;&#20960;&#20309;&#25968;&#23398;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#21313;&#20010;LLMs&#21644;MMs&#22312;&#36825;&#20123;&#19981;&#21516;&#23376;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;WizardMath&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#36798;&#21040;55.67%&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#21482;&#26377;6.00%&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#31361;&#20986;&#20102;&#20851;&#38190;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10104v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\% accuracy rate on the main subset but only a 6.00\% accuracy on the challenging subset. This highlights the critical ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#20840;&#26223;&#35270;&#22270;&#65292;&#24182;&#25351;&#20986;&#20102;&#27602;&#24615;&#12289;&#20559;&#35265;&#21644;&#20854;&#20182;&#38382;&#39064;&#65292;&#21516;&#26102;&#24378;&#35843;&#20154;&#31867;&#22823;&#33041;&#24182;&#19981;&#29305;&#27530;&#65292;&#20154;&#31867;&#26234;&#33021;&#21482;&#26159;&#19968;&#20010;&#23610;&#24230;&#19978;&#30340;&#26032;&#20852;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.07166</link><description>&lt;p&gt;
&#21457;&#34920;&#25991;&#26412;&#30340;&#31038;&#20250;&#28436;&#21270;&#19982;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20986;&#29616;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#27602;&#24615;&#21644;&#20559;&#35265;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#20840;&#26223;&#35270;&#22270;&#65292;&#24182;&#25351;&#20986;&#20102;&#27602;&#24615;&#12289;&#20559;&#35265;&#21644;&#20854;&#20182;&#38382;&#39064;&#65292;&#21516;&#26102;&#24378;&#35843;&#20154;&#31867;&#22823;&#33041;&#24182;&#19981;&#29305;&#27530;&#65292;&#20154;&#31867;&#26234;&#33021;&#21482;&#26159;&#19968;&#20010;&#23610;&#24230;&#19978;&#30340;&#26032;&#20852;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#20840;&#26223;&#35270;&#22270;&#65292;&#36825;&#23548;&#33268;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31361;&#30772;&#24615;&#20986;&#29616;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#23558;&#25152;&#26377;&#36825;&#20123;&#21457;&#23637;&#32622;&#20110;&#23454;&#29992;&#30340;&#24191;&#27867;&#21382;&#21490;&#31038;&#20250;&#35270;&#35282;&#20013;&#65292;&#26082;&#19981;&#22840;&#22823;&#20854;&#25928;&#26524;&#65292;&#21448;&#19981;&#20135;&#29983;1970&#24180;&#20195;&#21040;1990&#24180;&#20195;&#20154;&#24037;&#26234;&#33021;&#20908;&#23395;&#30340;&#24754;&#35266;&#24773;&#32490;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#23384;&#22312;&#27602;&#24615;&#12289;&#20559;&#35265;&#12289;&#35760;&#24518;&#12289;&#35844;&#23194;&#12289;&#36923;&#36753;&#19981;&#19968;&#33268;&#21644;&#24187;&#35273;&#31561;&#38382;&#39064;&#65292;&#21482;&#26159;&#20316;&#20026;&#23545;&#36807;&#20110;&#20048;&#35266;&#30340;&#35686;&#31034;&#12290;&#25105;&#20204;&#22312;&#27492;&#25351;&#20986;&#65292;&#27491;&#22914;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#20284;&#20046;&#21457;&#29983;&#22312;&#31070;&#32463;&#36830;&#25509;&#25110;&#26435;&#37325;&#25968;&#37327;&#30340;&#20020;&#30028;&#28857;&#19978;&#65292;&#20154;&#31867;&#22823;&#33041;&#23588;&#20854;&#26159;&#30382;&#36136;&#21306;&#22495;&#24182;&#27809;&#26377;&#20160;&#20040;&#29305;&#27530;&#25110;&#36229;&#20961;&#65292;&#21482;&#26159;&#28789;&#38271;&#31867;&#22823;&#33041;&#30340;&#19968;&#20010;&#25918;&#22823;&#29256;&#65292;&#29978;&#33267;&#20154;&#31867;&#26234;&#33021;&#20284;&#20046;&#21482;&#26159;&#19968;&#20010;&#23610;&#24230;&#19978;&#30340;&#26032;&#20852;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in Large Language Models. The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s. We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic. We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#20018;&#25200;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#24182;&#34892;&#25351;&#20196;&#20043;&#38388;&#30340;&#20018;&#25200;&#21487;&#33021;&#20250;&#30772;&#22351;&#37327;&#23376;&#29366;&#24577;&#24182;&#23548;&#33268;&#31243;&#24207;&#25191;&#34892;&#38169;&#35823;</title><link>https://arxiv.org/abs/2402.06952</link><description>&lt;p&gt;
&#29992;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#20272;&#35745;&#20018;&#25200;&#35823;&#24046;&#23545;&#30005;&#36335;&#20445;&#30495;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#20018;&#25200;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#24182;&#34892;&#25351;&#20196;&#20043;&#38388;&#30340;&#20018;&#25200;&#21487;&#33021;&#20250;&#30772;&#22351;&#37327;&#23376;&#29366;&#24577;&#24182;&#23548;&#33268;&#31243;&#24207;&#25191;&#34892;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#37327;&#23376;&#35745;&#31639;&#31038;&#21306;&#20851;&#27880;&#20110;&#25506;&#32034;&#36817;&#26399;&#35774;&#22791;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#35774;&#22791;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#35745;&#31639;&#33021;&#21147;&#36229;&#36807;&#20102;&#32463;&#20856;&#35745;&#31639;&#26426;&#12290;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#36825;&#20123;&#35774;&#22791;&#20013;&#22266;&#26377;&#22122;&#22768;&#26159;&#21542;&#21487;&#20197;&#34987;&#20811;&#26381;&#65292;&#25110;&#32773;&#20219;&#20309;&#28508;&#22312;&#30340;&#37327;&#23376;&#20248;&#21183;&#26159;&#21542;&#21463;&#21040;&#38480;&#21046;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;&#20018;&#25200;&#26159;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#20027;&#35201;&#26469;&#28304;&#20043;&#19968;&#65292;&#23427;&#23545;&#30828;&#20214;&#35774;&#35745;&#26500;&#25104;&#20102;&#26681;&#26412;&#24615;&#25361;&#25112;&#12290;&#24182;&#34892;&#25351;&#20196;&#20043;&#38388;&#30340;&#20018;&#25200;&#21487;&#20197;&#30772;&#22351;&#37327;&#23376;&#29366;&#24577;&#24182;&#23548;&#33268;&#31243;&#24207;&#25191;&#34892;&#38169;&#35823;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20018;&#25200;&#35823;&#24046;&#23545;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#31616;&#21333;&#21644;&#23454;&#29992;&#65292;&#21487;&#29992;&#20110;&#34920;&#24449;&#22810;&#27604;&#29305;&#35774;&#22791;&#30340;&#20018;&#25200;&#35823;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#38543;&#26426;&#21270;&#22522;&#20934;&#27979;&#35797;&#21644;&#21516;&#26102;&#38543;&#26426;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Current advancements in technology have focused the attention of the quantum computing community toward exploring the potential of near-term devices whose computing power surpasses that of classical computers in practical applications. An unresolved central question revolves around whether the inherent noise in these devices can be overcome or whether any potential quantum advantage would be limited. There is no doubt that crosstalk is one of the main sources of noise in noisy intermediate-scale quantum (NISQ) systems, and it poses a fundamental challenge to hardware designs. Crosstalk between parallel instructions can corrupt quantum states and cause incorrect program execution. In this study, we present a comprehensive analysis of the crosstalk error effect on NISQ computers. Our approach is extremely straightforward and practical for characterizing the crosstalk error of various multi-qubit devices. In particular, we combine the randomized benchmarking (RB) and simultaneous randomiz
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;ViT-SAM&#26159;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#24182;&#26367;&#25442;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;48.9&#20493;&#30340;&#21152;&#36895;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05008</link><description>&lt;p&gt;
&#39640;&#25928;ViT-SAM: &#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05008
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;ViT-SAM&#26159;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#24182;&#26367;&#25442;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;48.9&#20493;&#30340;&#21152;&#36895;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#8212;&#8212;&#39640;&#25928;ViT-SAM&#12290;&#25105;&#20204;&#20445;&#30041;&#20102;SAM&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#29992;&#39640;&#25928;ViT&#26367;&#25442;&#20102;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;&#22312;&#35757;&#32451;&#26041;&#38754;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;SAM-ViT-H&#22270;&#20687;&#32534;&#30721;&#22120;&#21040;&#39640;&#25928;ViT&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;SA-1B&#25968;&#25454;&#38598;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#30001;&#20110;&#39640;&#25928;ViT&#30340;&#25928;&#29575;&#21644;&#23481;&#37327;&#65292;&#39640;&#25928;ViT-SAM&#22312;A100 GPU&#19978;&#30456;&#27604;SAM-ViT-H&#23454;&#29616;&#20102;48.9&#20493;&#30340;TensorRT&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#29306;&#29298;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#22312;https://github.com/mit-han-lab/efficientvit&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.
&lt;/p&gt;</description></item><item><title>CapHuman&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#32534;&#30721;&#28982;&#21518;&#23398;&#20064;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#30340;&#22836;&#37096;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#29305;&#23450;&#20010;&#20307;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.00627</link><description>&lt;p&gt;
CapHuman: &#22312;&#24179;&#34892;&#23431;&#23449;&#20013;&#25429;&#25417;&#20320;&#30340;&#30636;&#38388;
&lt;/p&gt;
&lt;p&gt;
CapHuman: Capture Your Moments in Parallel Universes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00627
&lt;/p&gt;
&lt;p&gt;
CapHuman&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#32534;&#30721;&#28982;&#21518;&#23398;&#20064;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#30340;&#22836;&#37096;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#29305;&#23450;&#20010;&#20307;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#65292;&#21363;&#20165;&#32473;&#23450;&#19968;&#20010;&#21442;&#32771;&#38754;&#37096;&#29031;&#29255;&#65292;&#26399;&#26395;&#33021;&#22815;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#30340;&#22836;&#37096;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#29305;&#23450;&#20010;&#20307;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#20855;&#22791;&#20197;&#19979;&#26377;&#21033;&#29305;&#24449;&#65306;&#65288;1&#65289;&#23545;&#19990;&#30028;&#21644;&#20154;&#31867;&#31038;&#20250;&#26377;&#24378;&#22823;&#30340;&#35270;&#35273;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#29992;&#20110;&#22522;&#26412;&#29289;&#20307;&#21644;&#20154;&#31867;&#22270;&#20687;&#30340;&#29983;&#25104;&#65307;&#65288;2&#65289;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65307;&#65288;3&#65289;&#28789;&#27963;&#32454;&#31890;&#24230;&#30340;&#22836;&#37096;&#25511;&#21046;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#22522;&#30784;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19978;&#36848;&#20004;&#31181;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CapHuman&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#8220;&#32534;&#30721;&#28982;&#21518;&#23398;&#20064;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#65292;&#20026;&#26032;&#20010;&#20307;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align" paradigm, which enables generalizable identity preservation for new individuals without cum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2401.17583</link><description>&lt;p&gt;
&#25935;&#25463;&#20294;&#23433;&#20840;&#65306;&#23398;&#20064;&#26080;&#30896;&#25758;&#39640;&#36895;&#22235;&#36275;&#26426;&#22120;&#20154;&#34892;&#36208;
&lt;/p&gt;
&lt;p&gt;
Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#34892;&#36208;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#24517;&#39035;&#26082;&#25935;&#25463;&#20197;&#25552;&#39640;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#65292;&#21448;&#35201;&#30830;&#20445;&#23433;&#20840;&#65292;&#36991;&#20813;&#19982;&#38556;&#30861;&#29289;&#25110;&#20154;&#30896;&#25758;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#24320;&#21457;&#20445;&#23432;&#30340;&#25511;&#21046;&#22120;&#65288;&#36895;&#24230;&#23567;&#20110;1.0 m/s&#65289;&#20197;&#30830;&#20445;&#23433;&#20840;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#25935;&#25463;&#24615;&#32780;&#26410;&#32771;&#34385;&#28508;&#22312;&#33268;&#21629;&#30340;&#30896;&#25758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23454;&#29616;&#20102;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#30340;&#34892;&#36208;&#12290;ABS&#21253;&#25324;&#19968;&#20010;&#25935;&#25463;&#31574;&#30053;&#26469;&#22312;&#38556;&#30861;&#29289;&#20013;&#25191;&#34892;&#28789;&#27963;&#30340;&#21160;&#20316;&#25216;&#33021;&#65292;&#24182;&#19988;&#26377;&#19968;&#20010;&#24674;&#22797;&#31574;&#30053;&#26469;&#36991;&#20813;&#22833;&#36133;&#65292;&#20849;&#21516;&#23454;&#29616;&#39640;&#36895;&#19988;&#26080;&#30896;&#25758;&#30340;&#23548;&#33322;&#12290;ABS&#20013;&#30340;&#31574;&#30053;&#20999;&#25442;&#30001;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#25511;&#21046;&#65292;&#35813;&#32593;&#32476;&#20063;&#25351;&#23548;&#24674;&#22797;&#31574;&#30053;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#38381;&#29615;&#20013;&#20445;&#25252;&#26426;&#22120;&#20154;&#12290;&#35757;&#32451;&#36807;&#31243;&#28041;&#21450;&#25935;&#25463;&#31574;&#30053;&#12289;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#12289;&#24674;&#22797;&#31574;&#30053;&#21644;&#22806;&#24863;&#30693;&#34920;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (&lt; 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception repre
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;?
&lt;/p&gt;
&lt;p&gt;
Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07927
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#33021;&#22815;&#25552;&#20379;&#20854;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#23545;&#20844;&#20247;&#26159;&#30452;&#25509;&#21487;&#35775;&#38382;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#36825;&#26679;&#30340;&#39118;&#38505;&#65292;&#21363;&#20196;&#20154;&#20449;&#26381;&#20294;&#38169;&#35823;&#30340;&#35299;&#37322;&#21487;&#33021;&#23548;&#33268;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25903;&#25745;&#30340;&#33258;&#20449;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#26159;AI&#23433;&#20840;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#35780;&#20272;&#33258;&#25105;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#27880;&#37322;&#20160;&#20040;&#26159;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#21487;&#38752;&#24615;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35828;&#26576;&#32452;&#35789;&#23545;&#20110;&#20570;&#20986;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#22312;&#27809;&#26377;&#36825;&#20123;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#24212;&#35813;&#26080;&#27861;&#20570;&#20986;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#33258;&#27965;&#24615;&#26816;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21487;&#38752;&#24615;&#26041;&#27861;&#65292;&#20294;&#20043;&#21069;&#23578;&#26410;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#23558;&#33258;&#27965;&#24615;&#26816;&#27979;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
&lt;/p&gt;</description></item><item><title>AdaptiX&#26159;&#19968;&#20010;&#36807;&#28193;&#24615;&#30340;XR&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#21161;&#21160;&#21147;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#20223;&#30495;&#29615;&#22659;&#65292;&#24182;&#32467;&#21512;&#20102;&#29992;&#25143;&#33258;&#20027;&#24615;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.15887</link><description>&lt;p&gt;
AdaptiX - &#19968;&#20010;&#29992;&#20110;&#21161;&#21160;&#21147;&#26426;&#22120;&#20154;&#20013;&#24320;&#21457;&#21644;&#35780;&#20272;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#30340;&#36807;&#28193;&#24615;XR&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AdaptiX -- A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics. (arXiv:2310.15887v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15887
&lt;/p&gt;
&lt;p&gt;
AdaptiX&#26159;&#19968;&#20010;&#36807;&#28193;&#24615;&#30340;XR&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#21161;&#21160;&#21147;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#20223;&#30495;&#29615;&#22659;&#65292;&#24182;&#32467;&#21512;&#20102;&#29992;&#25143;&#33258;&#20027;&#24615;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#25480;&#26435;&#34892;&#21160;&#21463;&#38480;&#21644;&#25216;&#26415;&#25509;&#21463;&#24230;&#30340;&#25552;&#39640;&#65292;&#22914;&#21512;&#20316;&#26426;&#22120;&#33218;&#31561;&#21161;&#21160;&#25216;&#26415;&#27491;&#22312;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26222;&#21450;&#25104;&#21151;&#21463;&#21040;&#21487;&#29992;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#29992;&#25143;&#36755;&#20837;&#19982;&#36719;&#20214;&#25511;&#21046;&#22312;&#33258;&#20027;&#24615;&#36830;&#32493;&#24615;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20849;&#21516;&#25511;&#21046;&#27010;&#24565;&#25552;&#20379;&#20102;&#23558;&#26377;&#38024;&#23545;&#24615;&#22320;&#22686;&#21152;&#29992;&#25143;&#33258;&#20027;&#24615;&#19982;&#19968;&#23450;&#31243;&#24230;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#30456;&#32467;&#21512;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;AdaptiX&#30340;&#20813;&#36153;&#24320;&#28304;XR&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#20998;&#36776;&#29575;&#20223;&#30495;&#29615;&#22659;&#20013;&#24320;&#21457;&#21644;&#35780;&#20272;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#12290;&#21021;&#22987;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#20013;&#30340;&#31034;&#20363;&#24773;&#26223;&#19979;&#30340;&#27169;&#25311;&#26426;&#22120;&#33218;&#12289;&#22810;&#31181;&#26631;&#20934;&#25511;&#21046;&#25509;&#21475;&#21644;&#19968;&#20010;&#19987;&#38376;&#30340;&#35760;&#24405;/&#22238;&#25918;&#31995;&#32479;&#12290;AdaptiX&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#30740;&#31350;&#38656;&#27714;&#65292;&#20801;&#35768;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#65288;HRI&#65289;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ongoing efforts to empower people with mobility impairments and the increase in technological acceptance by the general public, assistive technologies, such as collaborative robotic arms, are gaining popularity. Yet, their widespread success is limited by usability issues, specifically the disparity between user input and software control along the autonomy continuum. To address this, shared control concepts provide opportunities to combine the targeted increase of user autonomy with a certain level of computer assistance. This paper presents the free and open-source AdaptiX XR framework for developing and evaluating shared control applications in a high-resolution simulation environment. The initial framework consists of a simulated robotic arm with an example scenario in Virtual Reality (VR), multiple standard control interfaces, and a specialized recording/replay system. AdaptiX can easily be extended for specific research needs, allowing Human-Robot Interaction (HRI) resea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#35843;&#25972;&#38598;&#21512;&#20197;&#20272;&#35745;&#24635;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.14691</link><description>&lt;p&gt;
&#26102;&#24207;&#22240;&#26524;&#22270;&#30340;&#25277;&#35937;&#20013;&#24635;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifiability of total effects from abstractions of time series causal graphs. (arXiv:2310.14691v2 [math.ST] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#35843;&#25972;&#38598;&#21512;&#20197;&#20272;&#35745;&#24635;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20165;&#22522;&#20110;&#31995;&#32479;&#30340;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25277;&#35937;&#65306;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#23558;&#25152;&#26377;&#28382;&#21518;&#22240;&#26524;&#20851;&#31995;&#28151;&#28102;&#22312;&#19968;&#36215;&#65292;&#20294;&#21306;&#20998;&#28382;&#21518;&#21644;&#30636;&#26102;&#20851;&#31995;&#65307;&#32780;&#25688;&#35201;&#22240;&#26524;&#22270;&#21017;&#19981;&#25552;&#20379;&#20219;&#20309;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#28382;&#21518;&#30340;&#25351;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#25152;&#24517;&#38656;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#22312;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#20272;&#35745;&#24635;&#25928;&#24212;&#30340;&#35843;&#25972;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of identifiability of the total effect of an intervention from observational time series only given an abstraction of the causal graph of the system. Specifically, we consider two types of abstractions: the extended summary causal graph which conflates all lagged causal relations but distinguishes between lagged and instantaneous relations; and the summary causal graph which does not give any indication about the lag between causal relations. We show that the total effect is always identifiable in extended summary causal graphs and we provide necessary and sufficient graphical conditions for identifiability in summary causal graphs. Furthermore, we provide adjustment sets allowing to estimate the total effect whenever it is identifiable.
&lt;/p&gt;</description></item><item><title>EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.08949</link><description>&lt;p&gt;
Easier Multimodal Generation: Diffusion Models Meet LLMs
&lt;/p&gt;
&lt;p&gt;
Making Multimodal Generation Easier: When Diffusion Models Meet LLMs. (arXiv:2310.08949v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08949
&lt;/p&gt;
&lt;p&gt;
EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EasyGen&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20027;&#35201;&#20381;&#36182;&#20110;&#32534;&#30721;&#22120;&#22914;CLIP&#25110;ImageBind&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#26469;&#26725;&#25509;&#27169;&#24577;&#20043;&#38388;&#24046;&#36317;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;EasyGen&#22522;&#20110;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#65292;&#20419;&#36827;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#12290;EasyGen&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#23618;&#23558;BiDiffuser&#21644;LLM&#36827;&#34892;&#38598;&#25104;&#65292;&#22788;&#29702;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38480;&#20110;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#19981;&#21516;&#65292;EasyGen&#36824;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;LLM&#21019;&#24314;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#30001;BiDiffuser&#35299;&#37322;&#29983;&#25104;&#36866;&#24403;&#30340;&#35270;&#35273;&#22238;&#22797;&#26469;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#20102;EasyGen&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#35757;&#32451;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual responses. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15817</link><description>&lt;p&gt;
&#20351;&#29992;LM&#27169;&#25311;&#27801;&#30418;&#35782;&#21035;LM&#20195;&#29702;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15817
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20195;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#20363;&#22914;ChatGPT&#25554;&#20214;&#65292;&#20351;&#24471;&#20195;&#29702;&#20855;&#22791;&#20102;&#20016;&#23500;&#30340;&#21151;&#33021;&#65292;&#20294;&#20063;&#25918;&#22823;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#22914;&#27844;&#38706;&#31169;&#20154;&#25968;&#25454;&#25110;&#24341;&#21457;&#36130;&#21153;&#25439;&#22833;&#12290;&#35782;&#21035;&#36825;&#20123;&#39118;&#38505;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#23454;&#26045;&#24037;&#20855;&#65292;&#25163;&#21160;&#35774;&#32622;&#27599;&#20010;&#27979;&#35797;&#22330;&#26223;&#30340;&#29615;&#22659;&#65292;&#24182;&#25214;&#21040;&#39118;&#38505;&#26696;&#20363;&#12290;&#38543;&#30528;&#24037;&#20855;&#21644;&#20195;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#27979;&#35797;&#36825;&#20123;&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#23558;&#20351;&#23547;&#25214;&#39640;&#39118;&#38505;&#12289;&#38271;&#23614;&#39118;&#38505;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToolEmu&#65306;&#19968;&#20010;&#20351;&#29992;LM&#26469;&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#23454;&#20363;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;LM&#20195;&#29702;&#36827;&#34892;&#21508;&#31181;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#27979;&#35797;&#12290;&#38500;&#20102;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#26816;&#26597;&#20195;&#29702;&#30340;&#22833;&#36133;&#24182;&#37327;&#21270;&#30456;&#20851;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#27979;&#35797;&#20102;&#24037;&#20855;&#27169;&#25311;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#24182;&#21457;&#29616;&#20102;6&#20010;...
&lt;/p&gt;
&lt;p&gt;
Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 6
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REB&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#39046;&#22495;&#20559;&#24046;&#21644;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#21450;&#20351;&#29992;&#26412;&#22320;&#23494;&#24230;K&#26368;&#36817;&#37051;&#26041;&#27861;&#65292;&#20174;&#32780;&#38477;&#20302;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12577</link><description>&lt;p&gt;
REB&#65306;&#38477;&#20302;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
REB: Reducing Biases in Representation for Industrial Anomaly Detection. (arXiv:2308.12577v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REB&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#39046;&#22495;&#20559;&#24046;&#21644;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#21450;&#20351;&#29992;&#26412;&#22320;&#23494;&#24230;K&#26368;&#36817;&#37051;&#26041;&#27861;&#65292;&#20174;&#32780;&#38477;&#20302;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#26816;&#32034;&#26041;&#27861;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CNN&#27169;&#22411;&#33719;&#21462;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#25191;&#34892;&#36317;&#31163;&#24230;&#37327;&#36827;&#34892;&#32570;&#38519;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24573;&#30053;&#20102;&#39046;&#22495;&#20559;&#24046;&#21644;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#23616;&#37096;&#23494;&#24230;&#24046;&#24322;&#65292;&#36825;&#20123;&#29305;&#24449;&#27809;&#26377;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#38480;&#21046;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39046;&#22495;&#20559;&#24046;&#21644;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39046;&#22495;&#36866;&#24212;&#24615;&#30340;&#26041;&#27861;&#8212;&#8212;Reducing Biases&#65288;REB&#65289;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#27169;&#20223;&#33258;&#28982;&#32570;&#38519;&#30340;&#32570;&#38519;&#29983;&#25104;&#31574;&#30053;&#65288;DefectMaker&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#23494;&#24230;K&#26368;&#36817;&#37051;&#65288;LDKNN&#65289;&#26041;&#27861;&#26469;&#20943;&#23569;&#23616;&#37096;&#23494;&#24230;&#20559;&#24046;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;MVTec AD&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;99.5&#65285;&#30340;AUROC&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MVTec LOCO AD&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;88.0&#65285;&#30340;AUROC&#65292;&#24182;&#23558;AUROC&#25552;&#39640;&#20102;4.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing K-nearest neighbor (KNN) retrieval-based methods usually conduct industrial anomaly detection in two stages: obtain feature representations with a pre-trained CNN model and perform distance measures for defect detection. However, the features are not fully exploited as they ignore domain bias and the difference of local density in feature space, which limits the detection performance. In this paper, we propose Reducing Biases (REB) in representation by considering the domain bias of the pre-trained model and building a self-supervised learning task for better domain adaption with a defect generation strategy (DefectMaker) imitating the natural defects. Additionally, we propose a local density KNN (LDKNN) to reduce the local density bias and obtain effective anomaly detection. We achieve a promising result of 99.5\% AUROC on the widely used MVTec AD benchmark. We also achieve 88.0\% AUROC on the challenging MVTec LOCO AD dataset and bring an improvement of 4.7\% AUROC to the st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#19968;&#31181;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05994</link><description>&lt;p&gt;
ANALOGYKB&#65306;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#24320;&#21551;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base. (arXiv:2305.05994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#19968;&#31181;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#30340;&#19968;&#39033;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#27169;&#22411;&#35757;&#32451;&#36164;&#28304;&#65292;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#36825;&#26159;&#19968;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;&#31867;&#27604;&#30693;&#35782;&#24211;&#65292;&#23427;&#30001;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23548;&#20986;&#12290;ANALOGYKB&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#31867;&#27604;&#65306;1&#65289;&#30456;&#21516;&#20851;&#31995;&#30340;&#31867;&#27604;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#65307;2&#65289;&#31867;&#20284;&#20851;&#31995;&#30340;&#31867;&#27604;&#65292;&#21017;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;InstructGPT&#65289;&#21551;&#29992;&#30340;&#36873;&#25321;&#21644;&#36807;&#28388;&#31649;&#36947;&#36827;&#34892;&#35782;&#21035;&#65292;&#20877;&#32463;&#36807;&#23569;&#37327;&#20154;&#24037;&#36136;&#37327;&#25511;&#21046;&#12290;&#22312;&#20004;&#20010;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#65288;&#31867;&#27604;&#35782;&#21035;&#21644;&#29983;&#25104;&#65289;&#30340;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;ANALOGYKB&#25104;&#21151;&#22320;&#20351;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#35270;&#35273;Transformer&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26725;&#22359;&#37325;&#26500;&#30340;&#28151;&#21512;&#35270;&#35273;Transformer&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20854;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12557</link><description>&lt;p&gt;
Q-HyViT: &#24102;&#26725;&#22359;&#37325;&#26500;&#30340;&#28151;&#21512;&#35270;&#35273;Transformer&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction. (arXiv:2303.12557v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#35270;&#35273;Transformer&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26725;&#22359;&#37325;&#26500;&#30340;&#28151;&#21512;&#35270;&#35273;Transformer&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20854;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;Transformer &#65288;ViT&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#20195;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292; ViT &#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#28151;&#21512;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#32467;&#21512;&#21367;&#31215;&#21644;&#21464;&#21387;&#22120;&#23618;&#65292;&#24182;&#20248;&#21270;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#20351;&#32447;&#24615;&#22797;&#26434;&#24230;&#36798;&#21040;&#26368;&#22823;&#12290;&#27492;&#22806;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#34987;&#25552;&#20986;&#20316;&#20026;&#32531;&#35299;&#35745;&#31639;&#35201;&#27714;&#30340;&#19968;&#31181;&#25163;&#27573;&#12290;&#23558;&#37327;&#21270;&#25216;&#26415;&#21644;&#39640;&#25928;&#30340;&#28151;&#21512;&#21464;&#21387;&#22120;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#23545;&#20110;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#21152;&#36895;&#35270;&#35273;transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#27809;&#26377;&#30740;&#31350;&#23558;&#37327;&#21270;&#24212;&#29992;&#20110;&#39640;&#25928;&#30340;&#28151;&#21512;&#21464;&#21387;&#22120;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#23558;&#29616;&#26377;&#30340;ViT PTQ&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#39640;&#25928;&#30340;&#28151;&#21512;transformer&#26550;&#26500;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#31934;&#24230;&#19979;&#38477;&#65292;&#30001;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;Q-HyViT&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, vision transformers (ViT) have replaced convolutional neural network models in numerous tasks, including classification, detection, and segmentation. However, the high computational requirements of ViTs hinder their widespread implementation. To address this issue, researchers have proposed efficient hybrid transformer architectures that combine convolutional and transformer layers and optimize attention computation for linear complexity. Additionally, post-training quantization has been proposed as a means of mitigating computational demands. Combining quantization techniques and efficient hybrid transformer structures is crucial to maximize the acceleration of vision transformers on mobile devices. However, no prior investigation has applied quantization to efficient hybrid transformers. In this paper, at first, we discover that the straightforward manner to apply the existing PTQ methods for ViT to efficient hybrid transformers results in a drastic accuracy drop due to the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26354;&#29575;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.12307</link><description>&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#30340;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification. (arXiv:2303.12307v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26354;&#29575;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#38271;&#23614;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20559;&#24046;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20551;&#35774;&#26679;&#26412;&#36739;&#23569;&#30340;&#31867;&#26159;&#24369;&#31867;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23614;&#37096;&#31867;&#21035;&#24182;&#19981;&#24635;&#26159;&#38590;&#20197;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#26679;&#26412;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#35266;&#23519;&#21040;&#20102;&#27169;&#22411;&#20559;&#24046;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#20854;&#20182;&#24433;&#21709;&#27169;&#22411;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#21644;&#23398;&#20064;&#22914;&#20309;&#22609;&#36896;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#30340;&#24433;&#21709;&#12290;&#19968;&#20010;&#24847;&#22806;&#30340;&#21457;&#29616;&#26159;&#65306;&#31867;&#21035;&#20934;&#30830;&#24230;&#21644;&#24863;&#30693;&#27969;&#24418;&#30340;&#20998;&#31163;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#65292;&#32780;&#19982;&#26354;&#29575;&#30340;&#36127;&#30456;&#20851;&#24615;&#36880;&#28176;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;&#26354;&#29575;&#19981;&#24179;&#34913;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the challenges of long-tailed classification, researchers have proposed several approaches to reduce model bias, most of which assume that classes with few samples are weak classes. However, recent studies have shown that tail classes are not always hard to learn, and model bias has been observed on sample-balanced datasets, suggesting the existence of other factors that affect model bias. In this work, we systematically propose a series of geometric measurements for perceptual manifolds in deep neural networks, and then explore the effect of the geometric characteristics of perceptual manifolds on classification difficulty and how learning shapes the geometric characteristics of perceptual manifolds. An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds gradually decreases during training, while the negative correlation with the curvature gradually increases, implying that curvature imbalance leads to m
&lt;/p&gt;</description></item></channel></rss>