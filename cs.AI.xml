<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.20150</link><description>&lt;p&gt;
TFB&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#20840;&#38754;&#19988;&#20844;&#24179;&#30340;&#22522;&#20934;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20150
&lt;/p&gt;
&lt;p&gt;
TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20250;&#22312;&#32463;&#27982;&#12289;&#20132;&#36890;&#12289;&#20581;&#24247;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#39046;&#22495;&#20013;&#20135;&#29983;&#65292;&#23545;&#26410;&#26469;&#25968;&#20540;&#30340;&#39044;&#27979;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#35768;&#22810;&#39044;&#27979;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#20026;&#20102;&#30830;&#20445;&#36827;&#23637;&#65292;&#26377;&#24517;&#35201;&#33021;&#22815;&#20197;&#20840;&#38754;&#19988;&#21487;&#38752;&#30340;&#26041;&#24335;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TFB&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#12290;TFB&#36890;&#36807;&#35299;&#20915;&#19982;&#25968;&#25454;&#38598;&#12289;&#27604;&#36739;&#26041;&#27861;&#21644;&#35780;&#20272;&#31649;&#36947;&#30456;&#20851;&#30340;&#32570;&#28857;&#65292;&#25512;&#21160;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#65306;1&#65289;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#65292;2&#65289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;3&#65289;&#19981;&#19968;&#33268;&#21644;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#39046;&#22495;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#26469;&#33258;10&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65306;&#20132;&#36890;&#12289;&#30005;&#21147;&#12289;&#33021;&#28304;&#12289;&#29615;&#22659;&#12289;&#33258;&#28982;&#12289;&#32463;&#27982;&#12289;&#32929;&#31080;&#24066;&#22330;&#12289;&#38134;&#34892;&#12289;&#20581;&#24247;&#21644;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
&lt;/p&gt;</description></item><item><title>MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19888</link><description>&lt;p&gt;
MambaMixer&#65306;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#21644;&#36890;&#36947;&#36873;&#25321;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19888
&lt;/p&gt;
&lt;p&gt;
MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;Transformers&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#23637;&#29616;&#20986;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#23581;&#35797;&#20026;&#22810;&#32500;&#25968;&#25454;&#35774;&#35745;&#39640;&#25928;&#26377;&#25928;&#30340;&#26550;&#26500;&#20027;&#24178;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#26159;&#25968;&#25454;&#29420;&#31435;&#30340;&#65292;&#35201;&#20040;&#26080;&#27861;&#20801;&#35768;&#36328;&#32500;&#24230;&#21644;&#20869;&#37096;&#32500;&#24230;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#23454;&#29616;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#20102;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#21463;&#21040;SSMs&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MambaMixer&#65292;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#36328;&#26631;&#35760;&#21644;&#36890;&#36947;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19050</link><description>&lt;p&gt;
&#36890;&#36807;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#26816;&#27979;&#29983;&#25104;&#24615;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Detecting Generative Parroting through Overfitting Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#25968;&#23383;&#20869;&#23481;&#21019;&#24314;&#30340;&#26041;&#24335;&#65292;&#28982;&#32780;&#30001;&#20110;&#29983;&#25104;&#24615;&#27169;&#20223;&#38382;&#39064;&#65292;&#27169;&#22411;&#36807;&#20110;&#27169;&#20223;&#20854;&#35757;&#32451;&#25968;&#25454;&#32780;&#32473;&#29256;&#26435;&#23436;&#25972;&#24615;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#19968;&#20010;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#36825;&#31181;&#27169;&#20223;&#26679;&#26412;&#12290;&#25105;&#20204;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#25439;&#22833;&#24314;&#31435;&#19968;&#20010;&#26816;&#27979;&#38408;&#20540;&#65292;&#20174;&#32780;&#31934;&#30830;&#23450;&#20301;&#20462;&#25913;&#21518;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#20223;&#20869;&#23481;&#12290;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#24182;&#21152;&#24378;&#27861;&#24459;&#21512;&#35268;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19050v1 Announce Type: cross  Abstract: The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.17209</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65306;&#25968;&#23383;&#23402;&#29983;&#21644;&#35821;&#20041;&#33410;&#28857;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17209
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#21161;&#22312;&#24037;&#19994;4.0&#32972;&#26223;&#19979;&#20026;&#25968;&#23383;&#23402;&#29983;&#24314;&#27169;&#21019;&#24314;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65288;AAS&#65289;&#23454;&#20363;&#65292;&#26088;&#22312;&#22686;&#24378;&#26234;&#33021;&#21046;&#36896;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#20943;&#23569;&#25163;&#21160;&#24037;&#20316;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#25968;&#25454;&#32467;&#26500;&#26469;&#25429;&#25417;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#35201;&#20041;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#24182;&#20174;&#25991;&#26412;&#25216;&#26415;&#25968;&#25454;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25928;&#29983;&#25104;&#29575;&#20026;62-79%&#65292;&#34920;&#26126;&#30456;&#24403;&#27604;&#20363;&#30340;&#25163;&#21160;&#21019;&#24314;&#24037;&#20316;&#21487;&#20197;&#36716;&#25442;&#20026;&#26356;&#23481;&#26131;&#30340;&#39564;&#35777;&#24037;&#20316;&#65292;&#20174;&#32780;&#20943;&#23569;&#21019;&#24314;AAS&#23454;&#20363;&#27169;&#22411;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#23545;&#19981;&#21516;LLM&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26426;&#21046;&#30340;&#28145;&#20837;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;LLM&#26377;&#25928;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17209v1 Announce Type: new  Abstract: This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a "semantic node" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process "semantic node" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;MatchSeg&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#21644;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#22686;&#24378;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.15901</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#32771;&#22270;&#20687;&#21305;&#37197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#21106;&#65306;MatchSeg
&lt;/p&gt;
&lt;p&gt;
MatchSeg: Towards Better Segmentation via Reference Image Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15901
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;MatchSeg&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#21644;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#22686;&#24378;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;Few-shot learning&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#25903;&#25345;&#38598;&#65289;&#26469;&#25351;&#23548;&#39044;&#27979;&#26032;&#30340;&#12289;&#26410;&#26631;&#35760;&#22270;&#20687;&#65288;&#31216;&#20026;&#26597;&#35810;&#38598;&#65289;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#20811;&#26381;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#21463;&#21040;&#36825;&#19968;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MatchSeg&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25112;&#30053;&#24615;&#21442;&#32771;&#22270;&#20687;&#21305;&#37197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#22312;&#23450;&#20041;&#25903;&#25345;&#38598;&#26102;&#36873;&#25321;&#39640;&#24230;&#30456;&#20851;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#21152;&#24378;&#25903;&#25345;&#21644;&#26597;&#35810;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15901v1 Announce Type: new  Abstract: Recently, automated medical image segmentation methods based on deep learning have achieved great success. However, they heavily rely on large annotated datasets, which are costly and time-consuming to acquire. Few-shot learning aims to overcome the need for annotated data by using a small labeled dataset, known as a support set, to guide predicting labels for new, unlabeled images, known as the query set. Inspired by this paradigm, we introduce MatchSeg, a novel framework that enhances medical image segmentation through strategic reference image matching. We leverage contrastive language-image pre-training (CLIP) to select highly relevant samples when defining the support set. Additionally, we design a joint attention module to strengthen the interaction between support and query features, facilitating a more effective knowledge transfer between support and query sets. We validated our method across four public datasets. Experimental re
&lt;/p&gt;</description></item><item><title>TrustSQL&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#26102;&#30340;&#21487;&#38752;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;SQL&#39044;&#27979;&#21644;&#25918;&#24323;&#39044;&#27979;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15879</link><description>&lt;p&gt;
TrustSQL: &#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15879
&lt;/p&gt;
&lt;p&gt;
TrustSQL&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#26102;&#30340;&#21487;&#38752;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;SQL&#39044;&#27979;&#21644;&#25918;&#24323;&#39044;&#27979;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32763;&#35793;&#25104;SQL&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;SQL&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#20154;&#20102;&#35299;&#36825;&#20123;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#33021;&#21542;&#21487;&#38752;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25506;&#35752;&#36825;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustSQL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#21333;&#19968;&#25968;&#25454;&#24211;&#21644;&#36328;&#25968;&#25454;&#24211;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;&#22522;&#20934;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#25552;&#20379;&#20004;&#31181;&#32467;&#26524;&#20043;&#19968;&#65306;1&#65289;SQL&#39044;&#27979;&#65307;&#25110;2&#65289;&#22312;&#29983;&#25104;&#30340;SQL&#20013;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#25110;&#38754;&#20020;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#26102;&#25918;&#24323;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19987;&#38376;&#20026;&#36825;&#19968;&#20219;&#21153;&#35774;&#35745;&#30340;&#21508;&#31181;&#24314;&#27169;&#26041;&#27861;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20026;&#21487;&#22238;&#31572;&#24615;&#20248;&#21270;&#21333;&#29420;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15879v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have led to significant improvements in translating natural language questions into SQL queries. While achieving high accuracy in SQL generation is crucial, little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones. To explore this aspect, we present TrustSQL, a new benchmark designed to assess the reliability of text-to-SQL models in both single-database and cross-database settings. The benchmark tasks models with providing one of two outcomes: 1) SQL prediction; or 2) abstention from making a prediction, either when there is a potential error in the generated SQL or when faced with unanswerable questions. For model evaluation, we explore various modeling approaches specifically designed for this task. These include: 1) optimizing separate models for answerability d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15412</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#27979;&#37327;&#21644;&#24314;&#27169;&#8220;&#25991;&#21270;&#8221;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring and Modeling "Culture" in LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15412
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#23545;39&#31687;&#26368;&#26032;&#35770;&#25991;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27809;&#26377;&#19968;&#31687;&#30740;&#31350;&#23450;&#20041;&#8220;&#25991;&#21270;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#12289;&#22810;&#23618;&#38754;&#30340;&#27010;&#24565;&#65307;&#30456;&#21453;&#65292;&#23427;&#20204;&#22312;&#19968;&#20123;&#29305;&#21035;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#38754;&#31216;&#20026;&#25991;&#21270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#22312;&#20154;&#21475;&#32479;&#35745;&#12289;&#35821;&#20041;&#21644;&#35821;&#35328;&#25991;&#21270;&#20132;&#20114;&#20195;&#29702;&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#12290;&#25105;&#20204;&#36824;&#23545;&#37319;&#29992;&#30340;&#25506;&#26597;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#26377;&#8220;&#25991;&#21270;&#8221;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#22914;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#65292;&#34987;&#30740;&#31350;&#20102;&#65292;&#30041;&#19979;&#20102;&#20960;&#20010;&#20854;&#20182;&#26377;&#36259;&#19988;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22823;&#37327;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#65288;Hershcovich&#31561;&#20154;&#65292;2022&#65289;&#30340;&#26410;&#34987;&#25506;&#31350;&#12290;&#21478;&#22806;&#20004;&#20010;&#20851;&#38190;&#30340;&#31354;&#30333;&#26159;&#30446;&#21069;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24773;&#22659;&#24615;&#30340;&#32570;&#20047;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30740;&#31350;&#38754;&#20020;&#30528;&#20844;&#24179;&#27604;&#36739;&#12289;&#36873;&#25321;&#23646;&#24615;&#20998;&#26512;&#32570;&#20047;&#20197;&#21450;&#36807;&#24230;&#20851;&#27880;&#23792;&#20540;&#24615;&#33021;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12660</link><description>&lt;p&gt;
ERASE&#65306;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12660
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30740;&#31350;&#38754;&#20020;&#30528;&#20844;&#24179;&#27604;&#36739;&#12289;&#36873;&#25321;&#23646;&#24615;&#20998;&#26512;&#32570;&#20047;&#20197;&#21450;&#36807;&#24230;&#20851;&#27880;&#23792;&#20540;&#24615;&#33021;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;(DRS)&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#22823;&#37327;&#29305;&#24449;&#23383;&#27573;&#26469;&#25552;&#20379;&#26356;&#31934;&#20934;&#30340;&#25512;&#33616;&#12290;&#26377;&#25928;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22240;&#27492;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#20248;&#21270;&#23384;&#20648;&#25928;&#29575;&#65292;&#20197;&#28385;&#36275;&#37096;&#32626;&#38656;&#27714;&#12290;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;DRS&#30340;&#32972;&#26223;&#19979;&#65292;&#23578;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;&#38754;&#20020;&#19977;&#20010;&#26680;&#24515;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#30740;&#31350;&#35770;&#25991;&#20043;&#38388;&#23454;&#39564;&#35774;&#32622;&#30340;&#24046;&#24322;&#24448;&#24448;&#23548;&#33268;&#19981;&#20844;&#24179;&#27604;&#36739;&#65292;&#36974;&#34109;&#20102;&#23454;&#36341;&#35265;&#35299;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#23646;&#24615;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#36873;&#25321;&#25216;&#26415;&#21644;DRS&#39592;&#24178;&#20043;&#38388;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#38480;&#21046;&#24615;&#25991;&#31456;&#30340;&#36890;&#29992;&#24615;&#30740;&#31350;&#21644;&#37096;&#32626;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#24448;&#24448;&#19987;&#27880;&#20110;&#27604;&#36739;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21487;&#36798;&#21040;&#30340;&#23792;&#20540;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#22312;&#35745;&#31639;&#26041;&#38754;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12660v1 Announce Type: cross  Abstract: Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#22312;&#21512;&#20316;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#26174;&#31034;&#20986;&#21512;&#20316;&#20542;&#21521;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#21512;&#20316;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#20581;&#22766;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.11381</link><description>&lt;p&gt;
LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#21512;&#20316;&#21527;&#65311;&#36890;&#36807;&#34701;&#21512;&#30406;&#35780;&#20272;&#23427;&#20204;&#30340;&#21512;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#22312;&#21512;&#20316;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#26174;&#31034;&#20986;&#21512;&#20316;&#20542;&#21521;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#21512;&#20316;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#20581;&#22766;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#32500;&#24230;&#26159;&#21457;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#28508;&#21147;&#22686;&#24378;&#22810;&#20195;&#29702;&#20154;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#33879;&#21517;&#30340;Meltin Pot&#29615;&#22659;&#20197;&#21450;&#21442;&#32771;&#27169;&#22411;&#22914;GPT4&#21644;GPT3.5&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#33258;&#20027;&#20195;&#29702;(LAAs)&#30340;&#21512;&#20316;&#33021;&#21147;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#20195;&#29702;&#20154;&#34920;&#29616;&#20986;&#21512;&#20316;&#30340;&#20542;&#21521;&#65292;&#20294;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#24378;&#35843;&#20102;&#26356;&#24378;&#22823;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#36866;&#24212;LLM&#30340;Melting Pot&#28216;&#25103;&#22330;&#26223;&#30340;&#25277;&#35937;&#21270;&#23618;&#65292;&#19968;&#20010;&#29992;&#20110;LLM&#20013;&#20171;&#20195;&#29702;&#24320;&#21457;&#30340;&#21487;&#37325;&#29992;&#26550;&#26500;-&#21253;&#25324;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#20197;&#21450;&#19981;&#21516;&#30340;&#35748;&#30693;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#26041;&#27861;&#35780;&#20272;&#21512;&#20316;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11381v1 Announce Type: new  Abstract: As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5. Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;HumanoidBench&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.10506</link><description>&lt;p&gt;
HumanoidBench&#65306;&#29992;&#20110;&#20840;&#36523;&#36816;&#21160;&#21644;&#25805;&#20316;&#30340;&#20223;&#30495;&#20154;&#22411;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;HumanoidBench&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#22411;&#26426;&#22120;&#20154;&#22312;&#21327;&#21161;&#20154;&#31867;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#31867;&#20154;&#24418;&#24577;&#12290;&#28982;&#32780;&#65292;&#20154;&#22411;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#24120;&#24120;&#21463;&#21040;&#26114;&#36149;&#19988;&#26131;&#25439;&#30340;&#30828;&#20214;&#35774;&#32622;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#21152;&#36895;&#20154;&#22411;&#26426;&#22120;&#20154;&#31639;&#27861;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;HumanoidBench&#65292;&#35813;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#37197;&#22791;&#28789;&#24039;&#25163;&#37096;&#21644;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20840;&#36523;&#25805;&#20316;&#21644;&#36816;&#21160;&#20219;&#21153;&#30340;&#20154;&#22411;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#30340;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#22312;&#34892;&#36208;&#25110;&#21040;&#36798;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#20511;&#21161;HumanoidBench&#65292;&#25105;&#20204;&#20026;&#26426;&#22120;&#20154;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#29992;&#20110;&#35782;&#21035;&#35299;&#20915;&#20154;&#22411;&#26426;&#22120;&#20154;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20419;&#36827;&#31639;&#27861;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10506v1 Announce Type: cross  Abstract: Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitatin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#29305;&#24449;&#32858;&#21512;&#21644;&#23884;&#20837;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.09502</link><description>&lt;p&gt;
EquiAV: &#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09502
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#29305;&#24449;&#32858;&#21512;&#21644;&#23884;&#20837;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#38899;&#39057;-&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26368;&#36817;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23637;&#31034;&#20986;&#25429;&#25417;&#20016;&#23500;&#32508;&#21512;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#22312;&#35768;&#22810;&#23398;&#20064;&#26041;&#27861;&#20013;&#24050;&#32463;&#24471;&#21040;&#39564;&#35777;&#65292;&#38899;&#39057;-&#35270;&#35273;&#23398;&#20064;&#20173;&#28982;&#24456;&#38590;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20248;&#21183;&#65292;&#22240;&#20026;&#22686;&#24378;&#21487;&#33021;&#20250;&#36731;&#26131;&#30772;&#22351;&#36755;&#20837;&#23545;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EquiAV&#65292;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#25193;&#23637;&#31561;&#21464;&#24615;&#24320;&#22987;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23398;&#20064;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#20419;&#36827;&#12290;&#23427;&#20351;&#24471;&#26469;&#33258;&#19981;&#21516;&#22686;&#24378;&#30340;&#29305;&#24449;&#33021;&#22815;&#32858;&#21512;&#21040;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#23884;&#20837;&#20013;&#65292;&#25552;&#20379;&#24378;&#22823;&#30340;&#30417;&#30563;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26159;&#22312;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30340;&#12290;&#22823;&#37327;&#28040;&#34701;&#30740;&#31350;&#21644;&#23450;&#24615;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09502v1 Announce Type: cross  Abstract: Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#21435;&#22122;&#25193;&#25955;&#26469;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.08974</link><description>&lt;p&gt;
&#29992;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#31070;&#32463;&#22330;&#34920;&#31034;&#35299;&#21078;&#26641;
&lt;/p&gt;
&lt;p&gt;
Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08974
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#21435;&#22122;&#25193;&#25955;&#26469;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#21078;&#26641;&#22312;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35299;&#21078;&#26641;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#20960;&#20309;&#24418;&#29366;&#22810;&#26679;&#19988;&#22797;&#26434;&#65292;&#20934;&#30830;&#34920;&#31034;&#35299;&#21078;&#26641;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#26469;&#34920;&#31034;&#35299;&#21078;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;INR&#31354;&#38388;&#20013;&#36827;&#34892;&#21435;&#22122;&#25193;&#25955;&#26469;&#25429;&#25417;&#19968;&#32452;&#26641;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#21487;&#20197;&#22312;&#20219;&#20309;&#25152;&#38656;&#20998;&#36776;&#29575;&#19979;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#30340;&#22797;&#26434;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08974v1 Announce Type: cross  Abstract: Anatomical trees play a central role in clinical diagnosis and treatment planning. However, accurately representing anatomical trees is challenging due to their varying and complex topology and geometry. Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency. Recently, implicit neural representations (INRs) have emerged as a powerful tool for representing shapes accurately and efficiently. We propose a novel approach for representing anatomical trees using INR, while also capturing the distribution of a set of trees via denoising diffusion in the space of INRs. We accurately capture the intricate geometries and topologies of anatomical trees at any desired resolution. Through extensive qualitative and quantitative evaluation, we demonstrate high-fidelity tree reco
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.07322</link><description>&lt;p&gt;
&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07322
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#22312;&#36890;&#36807;&#20998;&#26512;&#23398;&#29983;&#21382;&#21490;&#23398;&#20064;&#36807;&#31243;&#26469;&#39044;&#27979;&#20854;&#26410;&#26469;&#34920;&#29616;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#25311;&#30693;&#35782;&#36861;&#36394;&#36807;&#31243;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#23558;&#38382;&#39064;&#30340;&#20010;&#20307;&#20449;&#24687;&#34701;&#20837;&#24314;&#27169;&#20013;&#12290;&#36825;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#23613;&#31649;&#38382;&#39064;&#20849;&#20139;&#30456;&#21516;&#30340;&#30693;&#35782;&#32452;&#20214;&#65288;KC&#65289;&#65292;&#20294;&#23398;&#29983;&#23545;&#21516;&#36136;&#38382;&#39064;&#30340;&#30693;&#35782;&#20064;&#24471;&#21487;&#20197;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#22312;&#20110;&#35299;&#37322;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#34429;&#28982;&#21487;&#33021;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#20851;&#38190;&#26159;&#20197;&#32769;&#24072;&#33021;&#29702;&#35299;&#30340;&#26041;&#24335;&#21576;&#29616;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06398</link><description>&lt;p&gt;
&#20851;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#23485;&#24230;&#36882;&#20943;&#22238;&#25253;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Diminishing Returns of Width for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06398
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#25353;&#39034;&#24207;&#35757;&#32451;&#26032;&#20219;&#21153;&#26102;&#32463;&#24120;&#20986;&#29616;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#12290; &#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#20943;&#23569;&#65292;&#20294;&#23578;&#26410;&#20934;&#30830;&#21051;&#30011;&#23485;&#24230;&#21644;&#25345;&#32493;&#23398;&#20064;&#20043;&#38388;&#30340;&#30830;&#20999;&#20851;&#31995;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#26089;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#29702;&#35770;&#65292;&#24182;&#35777;&#26126;&#23485;&#24230;&#19982;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#20013;&#30340;&#36951;&#24536;&#30452;&#25509;&#30456;&#20851;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#19978;&#32463;&#39564;&#24615;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#65292;&#32467;&#26524;&#26174;&#31034;&#36882;&#20943;&#22238;&#25253;&#22914;&#25105;&#20204;&#30340;&#29702;&#35770;&#25152;&#39044;&#27979;&#30340;&#37027;&#26679;&#28165;&#26224;&#21487;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06398v1 Announce Type: cross  Abstract: While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#31639;&#27861;FedPIT&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#35757;&#32451;&#26469;&#38450;&#27490;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.06131</link><description>&lt;p&gt;
FedPIT&#65306;&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#21644;&#23569;&#26679;&#26412;&#32852;&#37030;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#31639;&#27861;FedPIT&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#35757;&#32451;&#26469;&#38450;&#27490;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#21709;&#24212;&#26041;&#38754;&#30340;&#24615;&#33021;&#24050;&#34987;&#35777;&#26126;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#25910;&#38598;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#38544;&#31169;&#30340;&#39046;&#22495;&#12290;&#32852;&#37030;&#25351;&#20196;&#35843;&#25972;&#65288;FedIT&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25351;&#20196;&#25968;&#25454;&#26377;&#38480;&#20197;&#21450;&#23481;&#26131;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#23427;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;FedPIT&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#30340;&#29305;&#23450;&#20219;&#21153;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#35757;&#32451;&#26469;&#32500;&#25252;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20840;&#23616;&#21442;&#25968;&#21644;&#22312;&#22686;&#24378;&#26412;&#22320;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;&#21442;&#25968;&#65292;&#26377;&#25928;&#22320;&#38450;&#27490;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06131v1 Announce Type: cross  Abstract: Instruction tuning has proven essential for enhancing the performance of large language models (LLMs) in generating human-aligned responses. However, collecting diverse, high-quality instruction data for tuning poses challenges, particularly in privacy-sensitive domains. Federated instruction tuning (FedIT) has emerged as a solution, leveraging federated learning from multiple data owners while preserving privacy. Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks. To address these issues, we propose a novel federated algorithm, FedPIT, which utilizes LLMs' in-context learning capability to self-generate task-specific synthetic data for training autonomously. Our method employs parameter-isolated training to maintain global parameters trained on synthetic data and local parameters trained on augmented local data, effectively thwarting data extraction attacks. Extensive exper
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.05750</link><description>&lt;p&gt;
&#35299;&#35835;AI&#31508;: &#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#24443;&#24213;&#39072;&#35206;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#28145;&#20837;&#23457;&#26597;&#12289;&#20262;&#29702;&#23457;&#26597;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#20316;&#20026;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#35780;&#20272;&#20102;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#39046;&#22495;&#38480;&#21046;&#30340;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02966</link><description>&lt;p&gt;
&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#29992;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;-shot&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#24615;&#33021;&#65292;&#28982;&#32780;&#32467;&#26500;&#21270;&#30340;KG&#24418;&#24335;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#19977;&#20803;&#32452;&#24418;&#24335;&#25110;&#19977;&#20803;&#32452;&#20107;&#23454;&#30340;&#33258;&#30001;&#25991;&#26412;&#36716;&#25442;&#65292;&#36935;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#30001;&#20110;&#37325;&#22797;&#23454;&#20307;&#25110;&#20851;&#31995;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#23494;&#24230;&#38477;&#20302;&#65292;&#20197;&#21450;&#30001;&#20110;&#26080;&#27861;&#24378;&#35843;&#20851;&#38190;&#35777;&#25454;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#28165;&#26224;&#24230;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EFSum&#65292;&#19968;&#20010;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LLMs&#22686;&#24378;QA&#12290;&#25105;&#20204;&#36890;&#36807;&#33976;&#39311;&#21644;&#20559;&#22909;&#23545;&#40784;&#26469;&#20248;&#21270;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#20316;&#20026;&#20107;&#23454;&#25688;&#35201;&#22120;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;EFSum&#25552;&#39640;&#20102;LLM&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#30830;&#20445;&#25688;&#35201;&#30340;&#21516;&#26102;&#26377;&#30410;&#21644;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00863</link><description>&lt;p&gt;
LLM-Ensemble: &#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00863
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00863v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495;&#25688;&#35201;: &#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#24403;&#20195;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#25552;&#20379;&#31934;&#30830;&#30340;&#20135;&#21697;&#23646;&#24615;&#20540;&#22312;&#30830;&#20445;&#39640;&#36136;&#37327;&#25512;&#33616;&#21644;&#25552;&#21319;&#23458;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#23646;&#24615;&#25552;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#65292;&#19981;&#21516;LLMs&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#36825;&#31181;&#21464;&#21270;&#20351;&#23427;&#20204;&#24444;&#27492;&#20114;&#34917;&#65292;&#27809;&#26377;&#21738;&#20010;LLM&#33021;&#23436;&#20840;&#21387;&#20498;&#20854;&#20182;LLM&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#22810;&#26679;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24320;&#21457;&#19968;&#31181;&#21033;&#29992;&#23427;&#20204;&#20114;&#34917;&#28508;&#21147;&#30340;&#38598;&#25104;&#26041;&#27861;&#21464;&#24471;&#24517;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00863v1 Announce Type: cross  Abstract: Product attribute value extraction is a pivotal component in Natural Language Processing (NLP) and the contemporary e-commerce industry. The provision of precise product attribute values is fundamental in ensuring high-quality recommendations and enhancing customer satisfaction. The recently emerging Large Language Models (LLMs) have demonstrated state-of-the-art performance in numerous attribute extraction tasks, without the need for domain-specific training data. Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters. This variation makes them complementary to each other, with no single LLM dominating all others. Considering the diverse strengths and weaknesses of LLMs, it becomes necessary to develop an ensemble method that leverages their complementary potentials. In this paper, we propose a novel algorithm called LLM-ensemble to ensemble diffe
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.18439</link><description>&lt;p&gt;
&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#65306;LLM&#21033;&#29992;&#26367;&#20195;&#26684;&#24335;&#36827;&#34892;&#22686;&#24378;&#25512;&#29702;&#21644;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18439
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#27807;&#36890;&#30340;&#20027;&#35201;&#26684;&#24335;&#65292;&#22240;&#27492;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#20013;&#21516;&#26679;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;NL&#20043;&#22806;&#65292;LLMs&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#20102;&#21508;&#31181;&#38750;NL&#26684;&#24335;&#65292;&#22914;&#20195;&#30721;&#21644;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;NL&#20316;&#20026;LLMs&#30340;&#26368;&#20339;&#26684;&#24335;&#65292;&#22312;&#21333;&#19968;LLM&#25512;&#29702;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#30340;&#22320;&#20301;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#23457;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;NL&#65292;&#36890;&#36807;&#25506;&#32034;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#38750;NL&#26684;&#24335;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#20801;&#35768;LLMs&#22312;&#25512;&#29702;&#25110;&#27807;&#36890;&#20043;&#21069;&#33258;&#20027;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26684;&#24335;&#65292;&#21487;&#23548;&#33268;&#19981;&#21516;LLMs&#25512;&#29702;&#25928;&#29575;&#25552;&#39640;3.3&#33267;5.7&#65285;&#65292;&#24182;&#19988;&#22312;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#26368;&#22810;&#21487;&#20943;&#23569;72.7&#65285;&#30340;&#26631;&#35760;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27807;&#36890;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;LLMs&#21487;&#20197;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#26631;&#20934; DP &#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#26041;&#27861;&#26469;&#25511;&#21046;&#24402;&#32435;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.18129</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#20844;&#24179;&#23398;&#20064;&#31639;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18129
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#26631;&#20934; DP &#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#26041;&#27861;&#26469;&#25511;&#21046;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;&#30417;&#30563;&#24335;&#23398;&#20064;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#22791;&#21463;&#20851;&#27880;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20998;&#37197;&#26631;&#31614;&#26102;&#24456;&#23569;&#20381;&#36182;&#25935;&#24863;&#23646;&#24615;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#26631;&#20934;DP&#65288;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#65289;&#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#38750;&#22343;&#21248;&#20998;&#24067;&#25935;&#24863;&#23646;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20998;&#31867;&#35268;&#21017;&#20559;&#21521;&#21344;&#25454;&#22823;&#22810;&#25968;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#23646;&#24615;&#32467;&#26524;&#12290;&#20026;&#20102;&#25511;&#21046;DP-based&#20844;&#24179;&#23398;&#20064;&#20013;&#30340;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#65288;SA&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18129v1 Announce Type: cross  Abstract: Fair supervised learning algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community. While the demographic parity (DP) notion has been frequently used to measure a model's fairness in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms. In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute. Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data. To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15537</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of ChatGPT for Spam Email Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#37038;&#20214;&#32487;&#32493;&#26159;&#19987;&#19994;&#21644;&#21830;&#19994;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#36890;&#20449;&#23186;&#20171;&#12290;&#28982;&#32780;&#65292;&#22403;&#22334;&#37038;&#20214;&#30340;&#26222;&#21450;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#25200;&#20081;&#20102;&#20182;&#20204;&#30340;&#26085;&#24120;&#24037;&#20316;&#24182;&#38477;&#20302;&#20102;&#29983;&#20135;&#29575;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20869;&#23481;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#36807;&#28388;&#22403;&#22334;&#37038;&#20214;&#23545;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#65292;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#35780;&#20272;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;ChatGPT&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#38656;&#35201;&#25552;&#31034;&#35828;&#26126;&#21644;&#23569;&#37327;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.15441</link><description>&lt;p&gt;
&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Active Few-Shot Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#23569;&#26679;&#26412;&#24494;&#35843;&#26159;&#20256;&#32479;&#20027;&#21160;&#23398;&#20064;&#21644;&#36716;&#23548;&#20027;&#21160;&#23398;&#20064;&#30340;&#27867;&#21270;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#22522;&#20110;&#36716;&#23548;&#23398;&#20064;&#65288;ITL&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#37319;&#26679;&#20197;&#26368;&#22823;&#21270;&#33719;&#24471;&#23545;&#25351;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;ITL&#22343;&#21248;&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#33719;&#21462;&#30340;&#26368;&#23567;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#25209;&#25512;&#23548;&#20986;&#36825;&#31181;&#27867;&#21270;&#30028;&#38480;&#30340;&#20154;&#65292;&#36825;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#23558;ITL&#24212;&#29992;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;ITL&#26126;&#26174;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14897</link><description>&lt;p&gt;
Chain-of-Thought&#19981;&#24544;&#35802;&#20316;&#20026;&#20266;&#35013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Unfaithfulness as Disguised Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14897
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought (CoT)&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;LLM&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;CoT&#24544;&#23454;&#24230;&#30340;&#20195;&#29702;&#65292;arXiv:2307.13702&#25552;&#20986;&#20102;&#19968;&#20010;&#24230;&#37327;&#27169;&#22411;&#20381;&#36182;&#20854;CoT&#29983;&#25104;&#31572;&#26696;&#30340;&#25351;&#26631;&#12290;&#22312;&#19968;&#20010;&#19987;&#26377;&#27169;&#22411;&#31995;&#21015;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;LLM&#34920;&#29616;&#20986;&#27169;&#22411;&#22823;&#23567;&#19982;&#20854;&#24544;&#23454;&#24230;&#27979;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;-&#21453;&#21521;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#19988;130&#20159;&#21442;&#25968;&#27169;&#22411;&#30456;&#27604;&#20110;&#23610;&#23544;&#20171;&#20110;8.1&#20159;&#21040;1750&#20159;&#21442;&#25968;&#20043;&#38388;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#22686;&#21152;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#20316;&#20026;&#25152;&#26377;LLM&#30340;&#29305;&#24615;&#27867;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#31995;&#21015;&#30340;&#27169;&#22411;&#22797;&#21046;&#20182;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20182;&#20204;&#25253;&#21578;&#30340;CoT&#24544;&#23454;&#24230;&#30340;&#32553;&#25918;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#25913;&#21464;&#35774;&#23450;&#20250;&#23548;&#33268;&#36825;&#20123;&#27169;&#24335;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#28040;&#24687;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#23545;&#25269;&#25239;&#36234;&#29425;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#65292;&#19988;&#36234;&#29425;&#21487;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14857</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#28040;&#24687;&#23545;&#36234;&#29425;&#26159;&#21542;&#30495;&#30340;&#24456;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the System Message Really Important to Jailbreaks in Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14857
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#28040;&#24687;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#23545;&#25269;&#25239;&#36234;&#29425;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#65292;&#19988;&#36234;&#29425;&#21487;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#19981;&#21487;&#25110;&#32570;&#12290;&#23613;&#31649;&#36890;&#24120;&#20250;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#22312;&#21457;&#24067;&#21069;&#23558;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#29616;&#35937;&#65292;&#34987;&#31216;&#20026;"&#36234;&#29425;"&#12290;&#36825;&#20010;&#26415;&#35821;&#25351;&#30340;&#26159;&#24403;LLMs&#21463;&#21040;&#24694;&#24847;&#38382;&#39064;&#25552;&#31034;&#26102;&#20135;&#29983;&#24847;&#22806;&#19988;&#21487;&#33021;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#65306;&#31995;&#32479;&#28040;&#24687;&#23545;LLMs&#20013;&#30340;&#36234;&#29425;&#26159;&#21542;&#30495;&#30340;&#24456;&#37325;&#35201;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#31283;&#23450;&#30340;GPT&#29256;&#26412;gpt-3.5-turbo-0613&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#29983;&#25104;&#20102;&#20855;&#26377;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#30340;&#36234;&#29425;&#25552;&#31034;&#65306;&#30701;&#65292;&#38271;&#21644;&#26080;&#28040;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#31995;&#32479;&#28040;&#24687;&#36890;&#36807;&#23454;&#39564;&#20855;&#26377;&#19981;&#21516;&#30340;&#25269;&#25239;&#36234;&#29425;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36234;&#29425;&#22312;LLMs&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#31995;&#32479;&#28040;&#24687;&#22312;&#38450;&#27490;LLMs&#36234;&#29425;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "jailbreak." This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#26469;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;</title><link>https://arxiv.org/abs/2402.14834</link><description>&lt;p&gt;
MSynFD: &#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MSynFD: Multi-hop Syntax aware Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14834
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#26469;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#24191;&#27867;&#20256;&#25773;&#21161;&#38271;&#20102;&#20551;&#26032;&#38395;&#30340;&#24555;&#36895;&#20256;&#25773;&#65292;&#23545;&#25105;&#20204;&#30340;&#29616;&#23454;&#31038;&#20250;&#26500;&#25104;&#23041;&#32961;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#65292;&#36890;&#36807;&#20998;&#26512;&#26032;&#38395;&#20869;&#23481;&#21644;/&#25110;&#20854;&#31038;&#20250;&#32972;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#20102;&#22522;&#26412;&#30340;&#25991;&#26412;&#26032;&#38395;&#20869;&#23481;&#65288;&#25991;&#31456;&#65289;&#65292;&#24182;&#19988;&#36807;&#20998;&#20381;&#36182;&#24207;&#21015;&#24314;&#27169;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#26469;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#22797;&#26434;&#12289;&#24494;&#22937;&#30340;&#36716;&#25240;&#65292;&#27604;&#22914;&#21477;&#27861;-&#35821;&#20041;&#19981;&#21305;&#37197;&#21644;&#20808;&#39564;&#20559;&#24046;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#20302;&#65292;&#24182;&#22312;&#32570;&#22833;&#27169;&#24577;&#25110;&#31038;&#20250;&#32972;&#26223;&#26102;&#21487;&#33021;&#22833;&#36133;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#37325;&#35201;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65288;MSynFD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#65292;&#20197;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14834v1 Announce Type: cross  Abstract: The proliferation of social media platforms has fueled the rapid dissemination of fake news, posing threats to our real-life society. Existing methods use multimodal data or contextual information to enhance the detection of fake news by analyzing news content and/or its social context. However, these methods often overlook essential textual news content (articles) and heavily rely on sequential modeling and global attention to extract semantic information. These existing methods fail to handle the complex, subtle twists in news articles, such as syntax-semantics mismatches and prior biases, leading to lower performance and potential failure when modalities or social context are missing. To bridge these significant gaps, we propose a novel multi-hop syntax aware fake news detection (MSynFD) method, which incorporates complementary syntax information to deal with subtle twists in fake news. Specifically, we introduce a syntactical depen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;FinBen&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#33021;&#21147;&#30340;&#20840;&#38754;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12659</link><description>&lt;p&gt;
FinBen&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#36130;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
The FinBen: An Holistic Financial Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;FinBen&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#33021;&#21147;&#30340;&#20840;&#38754;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#24443;&#24213;&#30340;&#35780;&#20272;&#21644;&#37329;&#34701;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FinBen&#65292;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;FinBen&#21253;&#25324;23&#31181;&#37329;&#34701;&#20219;&#21153;&#30340;35&#20010;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#20219;&#21153;&#26681;&#25454;&#21345;&#29305;&#23572;-&#38669;&#24681;-&#21345;&#32599;&#23572;&#29702;&#35770;&#30340;&#28789;&#24863;&#32452;&#32455;&#25104;&#19977;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#35889;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#24402;&#32435;&#25512;&#29702;&#12289;&#32852;&#24819;&#35760;&#24518;&#12289;&#25968;&#37327;&#25512;&#29702;&#12289;&#26230;&#20307;&#26234;&#21147;&#31561;&#26041;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#65288;&#21253;&#25324;GPT-4&#12289;ChatGPT&#21644;&#26368;&#26032;&#30340;Gemini&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12659v1 Announce Type: cross  Abstract: LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations withi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;</title><link>https://arxiv.org/abs/2402.11903</link><description>&lt;p&gt;
SoLA: &#20026;&#20102;&#26356;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32780;&#23545;LLM&#36827;&#34892;&#27714;&#35299;&#23618;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#19978;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#35797;&#22270;&#36890;&#36807;&#24037;&#20855;&#23398;&#20064;&#26469;&#25913;&#21464;&#38382;&#39064;&#27714;&#35299;&#12290;&#34429;&#28982;&#22312;&#23567;&#35268;&#27169;&#38382;&#39064;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35268;&#27169;&#24222;&#22823;&#19988;&#34920;&#36798;&#22797;&#26434;&#65292;&#35299;&#20915;&#24037;&#19994;&#26696;&#20363;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;LLM&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27714;&#35299;&#22120;&#20316;&#20026;&#26032;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;&#12290;&#22312;SoLA&#20013;&#65292;LLM&#26088;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#35782;&#21035;&#26368;&#39640;&#36136;&#37327;&#30340;&#23616;&#37096;&#35299;&#65292;&#32780;&#27714;&#35299;&#22120;&#23618;&#21017;&#19987;&#27880;&#20110;&#21021;&#22987;&#35299;&#19981;&#28385;&#36275;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#20511;&#21161;MaxSAT&#20316;&#20026;&#26725;&#26753;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#26799;&#24230;&#65292;&#20351;&#26368;&#32456;&#27169;&#22411;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#28385;&#36275;&#30340;&#35299;&#25110;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#12290;&#21518;&#38376;&#29702;&#35770;&#30830;&#20445;SoLA&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;COIECD&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20914;&#31361;&#19978;&#19979;&#25991;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#22312;&#38750;&#20914;&#31361;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11893</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#35299;&#30721;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;-&#29109;&#32422;&#26463;&#26469;&#35782;&#21035;&#21644;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11893
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;COIECD&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20914;&#31361;&#19978;&#19979;&#25991;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#22312;&#38750;&#20914;&#31361;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20869;&#37096;&#21270;&#20102;&#22823;&#37327;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29616;&#23454;&#24212;&#29992;&#38656;&#35201;&#22806;&#37096;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#24110;&#21161;&#27169;&#22411;&#23436;&#25104;&#22522;&#26412;&#20219;&#21153;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#30693;&#35782;&#20914;&#31361;&#30340;&#20851;&#38190;&#22256;&#22659;&#65292;&#21363;&#19978;&#19979;&#25991;&#30693;&#35782;&#19982;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30456;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#30721;&#26041;&#27861;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#65292;&#21487;&#33021;&#20250;&#22312;&#27809;&#26377;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#26080;&#24847;&#20013;&#38477;&#20302;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#20449;&#24687;-&#29109;&#32422;&#26463;&#35299;&#30721;&#65288;COIECD&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#20914;&#31361;&#24182;&#35299;&#20915;&#23427;&#20204;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20914;&#31361;&#19978;&#19979;&#25991;&#30340;&#24544;&#23454;&#24230;&#65292;&#21516;&#26102;&#22312;&#38750;&#20914;&#31361;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;COIECD&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#20013;&#23637;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;&#25552;&#20379;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11893v1 Announce Type: new  Abstract: Large language models internalize enormous parametric knowledge during pre-training. Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks. This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets. Code is available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11804</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#25552;&#31034;&#22120;&#65306;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#24402;&#32435;&#25512;&#29702;&#26088;&#22312;&#25512;&#26029;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;KG&#20013;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;KG&#24402;&#32435;&#25512;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22788;&#29702;&#22312;&#25991;&#26412;&#21644;&#32467;&#26500;&#26041;&#38754;&#37117;&#31232;&#32570;&#30340;&#20302;&#36164;&#28304;&#22330;&#26223;&#12290;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#20174;&#32780;&#20026;KG&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#24102;&#26469;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#26041;&#27861;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;ProLINK&#65292;&#26088;&#22312;&#22312;&#20219;&#24847;KG&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;36&#20010;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
&lt;/p&gt;</description></item><item><title>Prospector heads&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#24418;&#24577;&#30340;&#23454;&#39564;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11729</link><description>&lt;p&gt;
Prospector Heads:&#22823;&#35268;&#27169;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24191;&#20041;&#29305;&#24449;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Prospector Heads: Generalized Feature Attribution for Large Models &amp; Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11729
&lt;/p&gt;
&lt;p&gt;
Prospector heads&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#24418;&#24577;&#30340;&#23454;&#39564;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#19968;&#31181;&#23450;&#20301;&#36755;&#20837;&#25968;&#25454;&#20013;&#19982;&#20998;&#31867;&#30456;&#20851;&#30340;&#21306;&#22495;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35328;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#33021;&#21147;&#12290;&#24403;&#21069;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20381;&#36182;&#20110;&#8220;&#35299;&#37322;&#8221;&#31471;&#21040;&#31471;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#65292;&#23384;&#22312;&#29305;&#24449;&#23450;&#20301;&#19981;&#31934;&#30830;&#20197;&#21450;&#30001;&#20110;&#35745;&#31639;&#25361;&#25112;&#32780;&#26080;&#27861;&#22312;&#23567;&#26679;&#26412;&#23610;&#23544;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25506;&#23547;&#32773;&#22836;&#37096;&#65288;prospector heads&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#12290;&#36890;&#36807;&#23545;&#24207;&#21015;&#65288;&#25991;&#26412;&#65289;&#12289;&#22270;&#20687;&#65288;&#30149;&#29702;&#23398;&#65289;&#21644;&#22270;&#65288;&#34507;&#30333;&#36136;&#32467;&#26500;&#65289;&#30340;&#23454;&#39564;&#65292;&#25506;&#23547;&#32773;&#22836;&#37096;&#22312;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#27010;&#25324;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#24402;&#22240;&#26041;&#27861;&#65292;&#24179;&#22343;&#23616;&#37096;&#21270;AUPRC&#24471;&#20998;&#25552;&#21319;&#20102;&#39640;&#36798;49&#28857;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25506;&#23547;&#32773;&#22836;&#37096;&#22914;&#20309;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11729v1 Announce Type: cross  Abstract: Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains. Current methods for feature attribution, which rely on "explaining" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpr
&lt;/p&gt;</description></item><item><title>Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11463</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21560;&#24341;&#23376;&#35760;&#24518;&#65306;&#28151;&#27788;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11463
&lt;/p&gt;
&lt;p&gt;
Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24573;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#28304;&#33258;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#23548;&#33268;&#32570;&#20047;&#22806;&#25512;&#21644;&#28436;&#21270;&#33021;&#21147;&#12290; &#37492;&#21035;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\textbf{\textit{Attraos}}&#23558;&#28151;&#27788;&#29702;&#35770;&#34701;&#20837;&#21040;LTSF&#20013;&#65292;&#23558;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26410;&#30693;&#39640;&#32500;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#12290; &#22312;&#21560;&#24341;&#23376;&#19981;&#21464;&#24615;&#30340;&#27010;&#24565;&#19979;&#65292;Attraos&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#26469;&#35760;&#24518;&#21382;&#21490;&#21160;&#24577;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#22686;&#24378;&#30340;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290; &#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#20016;&#23500;&#30340;&#32463;&#39564;&#35777;&#25454;&#19968;&#33268;&#34920;&#26126;&#65292;Attraos&#22312;&#20027;&#27969;LTSF&#25968;&#25454;&#38598;&#21644;&#28151;&#27788;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21508;&#31181;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11089</link><description>&lt;p&gt;
&#30007;&#24615;CEO&#21644;&#22899;&#24615;&#21161;&#29702;&#65306;&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#25506;&#31350;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#65288;&#22914;DALLE-3&#65289;&#23637;&#31034;&#20102;&#22312;&#26032;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21333;&#20154;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;T2I&#27169;&#22411;&#24212;&#29992;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#25551;&#32472;&#20004;&#20010;&#25110;&#26356;&#22810;&#20154;&#12290;&#35813;&#35774;&#23450;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#20173;&#26410;&#34987;&#25506;&#31350;&#65292;&#23548;&#33268;&#20351;&#29992;&#20013;&#30340;&#20844;&#24179;&#30456;&#20851;&#39118;&#38505;&#12290;&#20026;&#20102;&#30740;&#31350;T2I&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#20559;&#35265;&#35780;&#20272;&#26694;&#26550;&#12290;PST&#20419;&#20351;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#22270;&#20687;&#20013;&#30340;&#20004;&#20010;&#20010;&#20307;&#65292;&#29992;&#19982;&#30456;&#21453;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30456;&#20851;&#32852;&#30340;&#20004;&#20010;&#31038;&#20250;&#36523;&#20221;&#26469;&#25551;&#36848;&#20182;&#20204;&#12290;&#36890;&#36807;&#29983;&#25104;&#30340;&#22270;&#20687;&#36981;&#20174;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#31243;&#24230;&#26469;&#34913;&#37327;&#20559;&#35265;&#12290;&#21033;&#29992;PST&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#35780;&#20272;DALLE-3&#65306;&#24615;&#21035;&#32844;&#19994;&#20013;&#30340;&#20559;&#35265;&#21644;&#32452;&#32455;&#26435;&#21147;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11089v1 Announce Type: cross  Abstract: Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate great potential in new applications, but also face unprecedented fairness challenges. Prior studies revealed gender biases in single-person image generation, but T2I model applications might require portraying two or more people simultaneously. Potential biases in this setting remain unexplored, leading to fairness-related risks in usage. To study these underlying facets of gender biases in T2I models, we propose a novel Paired Stereotype Test (PST) bias evaluation framework. PST prompts the model to generate two individuals in the same image. They are described with two social identities that are stereotypically associated with the opposite gender. Biases can then be measured by the level of conformation to gender stereotypes in generated images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered occupation and biases in organizational power.
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#20102;&#25968;&#25454;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#29289;&#29702;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#24182;&#25552;&#39640;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07514</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26680;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Physics-informed machine learning as a kernel method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07514
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#20102;&#25968;&#25454;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#29289;&#29702;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#24182;&#25552;&#39640;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#23558;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#29289;&#29702;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26222;&#36890;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#32463;&#39564;&#39118;&#38505;&#30001;&#19968;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#31243;&#37327;&#21270;&#20102;&#29289;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#32447;&#24615;&#24494;&#20998;&#20808;&#39564;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#26680;&#22238;&#24402;&#20219;&#21153;&#12290;&#21033;&#29992;&#26680;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#27491;&#21017;&#21270;&#39118;&#38505;&#30340;&#26368;&#23567;&#21270;&#22120;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#34920;&#26126;&#23427;&#33267;&#23569;&#20197;Sobolev&#26368;&#23567;&#21270;&#36895;&#24230;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#29289;&#29702;&#35823;&#24046;&#30340;&#19981;&#21516;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#19968;&#32500;&#31034;&#20363;&#26469;&#35828;&#26126;&#36825;&#20010;&#21407;&#29702;&#65292;&#25903;&#25345;&#19968;&#20010;&#35770;&#28857;&#65306;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#26469;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#23545;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#33021;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models. In this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency. We prove that for linear differential priors, the problem can be formulated as a kernel regression task. Taking advantage of kernel theory, we derive convergence rates for the minimizer of the regularized risk and show that it converges at least at the Sobolev minimax rate. However, faster rates can be achieved, depending on the physical error. This principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators.
&lt;/p&gt;</description></item><item><title>VisLingInstruct&#36890;&#36807;&#33258;&#20027;&#20248;&#21270;&#25351;&#23548;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;TextVQA&#21644;HatefulMemes&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#25552;&#39640;&#20102;13.1%&#21644;9%&#12290;</title><link>https://arxiv.org/abs/2402.07398</link><description>&lt;p&gt;
VisLingInstruct: &#36890;&#36807;&#33258;&#20027;&#25351;&#23548;&#20248;&#21270;&#25552;&#21319;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07398
&lt;/p&gt;
&lt;p&gt;
VisLingInstruct&#36890;&#36807;&#33258;&#20027;&#20248;&#21270;&#25351;&#23548;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;TextVQA&#21644;HatefulMemes&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#25552;&#39640;&#20102;13.1%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;VisLingInstruct&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#36827;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25351;&#23548;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;VisLingInstruct&#36890;&#36807;&#33258;&#20027;&#35780;&#20272;&#21644;&#20248;&#21270;&#25351;&#23548;&#25991;&#26412;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25913;&#36827;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#35270;&#35273;&#24863;&#30693;&#21644;&#35821;&#35328;&#34920;&#36798;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#38500;&#20102;&#25351;&#23548;&#25991;&#26412;&#30340;&#25913;&#36827;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#23545;&#25991;&#26412;&#25552;&#31034;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#22522;&#20110;FlanT5&#21644;Vicuna&#30340;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;VisLingInstruct&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#35270;&#35273;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#22312;TextVQA&#21644;HatefulMemes&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;13.1%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets.
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05271</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#24341;&#21457;&#20102;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#26435;&#37325;&#19982;&#32463;&#39564;NTK&#20043;&#38388;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05271
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#65292;&#22312;&#19968;&#33324;&#32467;&#26500;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#20010;&#35828;&#27861;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#30456;&#20851;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24341;&#20837;&#30340;NFA&#26159;&#30001;&#38548;&#31163;&#36825;&#31181;&#23545;&#40784;&#30340;&#20013;&#24515;&#21270;NFA&#39537;&#21160;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#21644;&#20934;&#30830;&#24615;&#20316;&#20026;&#22870;&#21169;&#65292;&#26368;&#32456;&#30340;&#31639;&#27861;Mutant-UCB&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.05144</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
A Bandit Approach with Evolutionary Operators for Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#21644;&#20934;&#30830;&#24615;&#20316;&#20026;&#22870;&#21169;&#65292;&#26368;&#32456;&#30340;&#31639;&#27861;Mutant-UCB&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#27169;&#22411;&#26159;&#33218;&#65292;&#36873;&#25321;&#19968;&#20010;&#33218;&#23545;&#24212;&#37096;&#20998;&#35757;&#32451;&#27169;&#22411;&#65288;&#36164;&#28304;&#20998;&#37197;&#65289;&#12290;&#22870;&#21169;&#26159;&#36873;&#25321;&#27169;&#22411;&#22312;&#37096;&#20998;&#35757;&#32451;&#21518;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#20010;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#65292;&#36951;&#25022;&#26159;&#26368;&#20248;&#27169;&#22411;&#30340;&#39044;&#26399;&#20934;&#30830;&#24615;&#19982;&#26368;&#32456;&#36873;&#25321;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;UCB-E&#22312;&#38543;&#26426;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#30340;&#30452;&#25509;&#25512;&#24191;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#26399;&#26395;&#36951;&#25022;&#30340;&#39034;&#24207;&#26159;$T^{-\alpha}$&#65292;&#20854;&#20013;$\alpha \in (0,1/5)$&#65292;$T$&#26159;&#35201;&#20998;&#37197;&#30340;&#36164;&#28304;&#25968;&#37327;&#12290;&#20174;&#36825;&#20010;&#22522;&#26412;&#31639;&#27861;&#20986;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;Mutant-UCB&#65292;&#23427;&#32467;&#21512;&#20102;&#36827;&#21270;&#31639;&#27861;&#30340;&#25805;&#20316;&#31526;&#12290;&#22312;&#19977;&#20010;&#24320;&#28304;&#22270;&#29255;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#20102;&#36825;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#22269;&#38469;&#39046;&#20808;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper formulates model selection as an infinite-armed bandit problem. The models are arms, and picking an arm corresponds to a partial training of the model (resource allocation). The reward is the accuracy of the selected model after its partial training. In this best arm identification problem, regret is the gap between the expected accuracy of the optimal model and that of the model finally chosen. We first consider a straightforward generalization of UCB-E to the stochastic infinite-armed bandit problem and show that, under basic assumptions, the expected regret order is $T^{-\alpha}$ for some $\alpha \in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla algorithm, we introduce the algorithm Mutant-UCB that incorporates operators from evolutionary algorithms. Tests carried out on three open source image classification data sets attest to the relevance of this novel combining approach, which outperforms the state-of-the-art for a fixed budget.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#20010;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#24433;&#21709;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#24050;&#26377;&#26041;&#27861;&#30340;&#23616;&#37096;&#22343;&#34913;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.04971</link><description>&lt;p&gt;
&#22810;&#21457;&#20449;&#32773;&#35828;&#26381; - &#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-Sender Persuasion -- A Computational Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#20010;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#24433;&#21709;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#24050;&#26377;&#26041;&#27861;&#30340;&#23616;&#37096;&#22343;&#34913;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21040;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#22810;&#20010;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#20351;&#20854;&#37319;&#21462;&#26576;&#20123;&#34892;&#21160;&#12290;&#36825;&#20123;&#35774;&#32622;&#26159;&#35745;&#31639;&#32463;&#27982;&#23398;&#65292;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#21644;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#26680;&#24515;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#26159;&#21457;&#20449;&#32773;&#20449;&#21495;&#31574;&#30053;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#19968;&#33324;&#24773;&#20917;&#19979;&#25214;&#21040;&#19968;&#20010;&#22343;&#34913;&#26159;PPAD-Hard&#30340;;&#23454;&#38469;&#19978;&#65292;&#35745;&#31639;&#19968;&#20010;&#21457;&#20449;&#32773;&#30340;&#26368;&#20339;&#21709;&#24212;&#29978;&#33267;&#26159;NP-Hard&#30340;&#12290;&#37492;&#20110;&#36825;&#20123;&#22266;&#26377;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36716;&#32780;&#23547;&#25214;&#23616;&#37096;&#32435;&#20160;&#22343;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#35813;&#28216;&#25103;&#30340;&#38750;&#32447;&#24615;&#21644;&#19981;&#36830;&#32493;&#25928;&#29992;&#12290;&#32467;&#21512;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#23436;&#20840;&#23637;&#31034;&#22343;&#34913;&#21644;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#30340;&#23616;&#37096;&#22343;&#34913;&#12290;&#24191;&#20041;&#19978;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#36129;&#29486;&#23545;&#24191;&#27867;&#30340;&#31867;&#21035;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#25552;&#21319;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04644</link><description>&lt;p&gt;
LEVI:&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#25552;&#21319;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#22312;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#24494;&#35843;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#24494;&#35843;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#65288;&#21363;&#65292;&#36229;&#20986;&#20998;&#24067;&#65307;OOD&#65289;&#19978;&#30340;&#27867;&#21270;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#25913;&#21892;OOB&#27867;&#21270;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#24494;&#35843;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#24182;&#35843;&#25972;&#24494;&#35843;&#20197;&#20445;&#30041;&#33258;&#39044;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#21040;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#38480;&#21046;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36807;&#24230;&#20381;&#36182;&#39044;&#35757;&#32451;&#34920;&#31034;&#21487;&#33021;&#20250;&#38459;&#30861;&#24494;&#35843;&#23398;&#20064;&#19979;&#28216;&#20219;&#21153;&#30340;&#37325;&#35201;&#34920;&#31034;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;OOB&#27867;&#21270;&#12290;&#24403;&#26032;&#20219;&#21153;&#26469;&#33258;&#20110;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#65288;&#23376;&#65289;&#39046;&#22495;&#26102;&#65292;&#36825;&#21487;&#33021;&#23588;&#20026;&#28798;&#38590;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#23637;&#29616;&#23454;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#20351;&#29992;LLMs&#21487;&#20197;&#23454;&#29616;&#25193;&#23637;&#29616;&#23454;&#30340;&#26356;&#21253;&#23481;&#21644;&#21442;&#19982;&#65292;&#24182;&#26377;&#26395;&#25512;&#21160;&#20854;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03907</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#23637;&#29616;&#23454;&#20013;&#65306;&#21253;&#23481;&#24615;&#12289;&#21442;&#19982;&#24230;&#21644;&#38544;&#31169;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#23637;&#29616;&#23454;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#20351;&#29992;LLMs&#21487;&#20197;&#23454;&#29616;&#25193;&#23637;&#29616;&#23454;&#30340;&#26356;&#21253;&#23481;&#21644;&#21442;&#19982;&#65292;&#24182;&#26377;&#26395;&#25512;&#21160;&#20854;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#12289;&#30828;&#20214;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#26368;&#26032;&#21457;&#23637;&#21487;&#33021;&#23548;&#33268;&#25193;&#23637;&#29616;&#23454;&#35774;&#22791;&#30340;&#26222;&#21450;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23884;&#20837;&#25193;&#23637;&#29616;&#23454;&#20013;&#65292;&#36890;&#36807;&#23558;&#23427;&#20204;&#23884;&#20837;&#34394;&#25311;&#35282;&#33394;&#25110;&#20316;&#20026;&#21465;&#20107;&#26041;&#24335;&#65292;&#26469;&#20419;&#36827;&#26356;&#21253;&#23481;&#30340;&#20307;&#39564;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#21253;&#23481;&#24615;&#23558;&#26377;&#21161;&#20110;&#25193;&#23637;&#29616;&#23454;&#30340;&#22810;&#26679;&#24615;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30456;&#20449;LLMs&#30340;&#22810;&#21151;&#33021;&#23545;&#35805;&#33021;&#21147;&#23558;&#22686;&#21152;&#29992;&#25143;&#19982;&#25193;&#23637;&#29616;&#23454;&#29615;&#22659;&#30340;&#21442;&#19982;&#24230;&#65292;&#20174;&#32780;&#24110;&#21161;&#25193;&#23637;&#29616;&#23454;&#26356;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive. While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques. In this paper, we argue for using large language models (LLMs) in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through prompt engineering according to user profiles and fine-tuning the LLMs for particular purposes. We argue that such inclusion will facilitate diversity for XR use. In addition, we believe that with the versatile conversational capabilities of LLMs, users will engage more with XR environments, which might help XR be more used in everyday life. Lastly, we speculate that combin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#22810;&#20154;&#28216;&#25103;&#20013;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#28216;&#25103;&#25968;&#25454;&#21644;&#35757;&#32451;AI&#20195;&#29702;&#26469;&#27604;&#36739;&#21644;&#23545;&#27604;&#20154;&#31867;&#21644;AI&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#20197;&#35782;&#21035;&#39640;&#32423;&#34892;&#20026;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2402.03575</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#22810;&#20154;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
Toward Human-AI Alignment in Large-Scale Multi-Player Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#22810;&#20154;&#28216;&#25103;&#20013;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#28216;&#25103;&#25968;&#25454;&#21644;&#35757;&#32451;AI&#20195;&#29702;&#26469;&#27604;&#36739;&#21644;&#23545;&#27604;&#20154;&#31867;&#21644;AI&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#20197;&#35782;&#21035;&#39640;&#32423;&#34892;&#20026;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#21516;&#23545;&#20110;&#21019;&#24314;&#22686;&#24378;&#28216;&#25103;&#20307;&#39564;&#30340;&#21487;&#20449;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#31181;&#21327;&#21516;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#38598;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#39640;&#32423;&#34892;&#20026;&#20219;&#21153;&#32780;&#38750;&#20302;&#32423;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26469;&#33258;Xbox&#30340;Bleeding Edge&#65288;10&#19975;+&#28216;&#25103;&#65289;&#30340;&#22823;&#37327;&#20154;&#31867;&#28216;&#25103;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#22797;&#26434;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#36825;&#20010;&#20219;&#21153;&#31354;&#38388;&#20316;&#20026;&#34892;&#20026;&#27969;&#24418;&#30340;&#22522;&#30784;&#38598;&#21512;&#65292;&#25429;&#25417;&#21487;&#35299;&#37322;&#30340;&#36724;&#65306;&#25112;&#26007;-&#36867;&#36305;&#12289;&#25506;&#32034;-&#21033;&#29992;&#20197;&#21450;&#21333;&#20154;-&#22810;&#20154;&#26234;&#33021;&#20307;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#22240;&#26524;&#21464;&#25442;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#29609;Bleeding Edge&#65292;&#24182;&#27979;&#37327;&#20854;&#34892;&#20026;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#28216;&#25103;&#26144;&#23556;&#21040;&#25552;&#20986;&#30340;&#34892;&#20026;&#27969;&#24418;&#20013;&#36827;&#34892;&#27604;&#36739;&#21644;&#23545;&#27604;&#12290;&#36825;&#26679;&#21487;&#20197;&#35299;&#37322;&#31574;&#30053;&#30340;&#24046;&#24322;&#65292;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#29609;&#23478;&#22312;&#25112;&#26007;-&#36867;&#36305;&#26041;&#38754;&#34920;&#29616;&#21464;&#21270;&#22810;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00447</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Data-Efficient Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#31038;&#20132;&#32593;&#32476;&#21040;&#29983;&#29289;&#21270;&#23398;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#26159;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#36825;&#31181;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#26631;&#27880;&#36164;&#28304;&#26377;&#38480;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;(DEGL)&#30340;&#30740;&#31350;&#21069;&#27839;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;DEGL&#24403;&#21069;&#36827;&#23637;&#30340;&#39318;&#27425;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#25105;&#20204;&#23545;DEGL&#30340;&#25506;&#32034;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#19968;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#35780;&#20215;&#20013;&#30340;&#20844;&#24179;&#20851;&#27880;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#24320;&#21457;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#20844;&#24179;&#24615;&#35780;&#35770;&#65292;&#24182;&#35782;&#21035;&#20986;&#32422;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;</title><link>https://arxiv.org/abs/2401.08097</link><description>&lt;p&gt;
AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#35780;&#20215;&#30340;&#20844;&#24179;&#20851;&#27880;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Fairness Concerns in AI-based Mobile App Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#35780;&#20215;&#20013;&#30340;&#20844;&#24179;&#20851;&#27880;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#24320;&#21457;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#20844;&#24179;&#24615;&#35780;&#35770;&#65292;&#24182;&#35782;&#21035;&#20986;&#32422;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26159;AI&#31995;&#32479;&#20013;&#24517;&#39035;&#35299;&#20915;&#30340;&#31038;&#20250;&#25216;&#26415;&#38382;&#39064;&#20043;&#19968;&#12290;&#19981;&#20844;&#24179;&#30340;AI&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#19981;&#20844;&#24179;&#30340;AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#65292;&#21487;&#33021;&#32473;&#20840;&#29699;&#24456;&#22823;&#19968;&#37096;&#20998;&#20154;&#21475;&#24102;&#26469;&#22256;&#38590;&#12290;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;AI&#22522;&#20110;&#24212;&#29992;&#35780;&#20215;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20844;&#24179;&#24615;&#21644;&#38750;&#20844;&#24179;&#24615;&#35780;&#35770;&#30340;&#32479;&#35745;&#26679;&#26412;&#12290;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#32452;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21306;&#20998;&#20844;&#24179;&#24615;&#35780;&#35770;&#21644;&#38750;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#26368;&#20339;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#20197;94%&#30340;&#31934;&#30830;&#24230;&#26816;&#27979;&#21040;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26368;&#20339;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#20174;108&#20010;AI&#22522;&#20110;&#24212;&#29992;&#25910;&#38598;&#30340;&#32422;950&#19975;&#26465;&#35780;&#35770;&#65292;&#35782;&#21035;&#20986;&#32422;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;K-means&#32858;&#31867;&#25216;&#26415;&#24212;&#29992;&#20110;&#36825;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08097v2 Announce Type: replace-cross Abstract: Fairness is one of the socio-technical concerns that must be addressed in AI-based systems. Unfair AI-based systems, particularly unfair AI-based mobile apps, can pose difficulties for a significant proportion of the global population. This paper aims to analyze fairness concerns in AI-based app reviews.We first manually constructed a ground-truth dataset, including a statistical sample of fairness and non-fairness reviews. Leveraging the ground-truth dataset, we developed and evaluated a set of machine learning and deep learning classifiers that distinguish fairness reviews from non-fairness reviews. Our experiments show that our best-performing classifier can detect fairness reviews with a precision of 94%. We then applied the best-performing classifier on approximately 9.5M reviews collected from 108 AI-based apps and identified around 92K fairness reviews. Next, applying the K-means clustering technique to the 92K fairness r
&lt;/p&gt;</description></item><item><title>MapGPT&#24341;&#20837;&#20102;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#65292;&#24110;&#21161;GPT&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#65292;&#25552;&#20986;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#20197;&#21327;&#21161;&#20195;&#29702;&#25191;&#34892;&#22810;&#27493;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2401.07314</link><description>&lt;p&gt;
MapGPT&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#36335;&#24452;&#35268;&#21010;&#30340;&#22320;&#22270;&#24341;&#23548;&#25552;&#31034;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07314
&lt;/p&gt;
&lt;p&gt;
MapGPT&#24341;&#20837;&#20102;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#65292;&#24110;&#21161;GPT&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#65292;&#25552;&#20986;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#20197;&#21327;&#21161;&#20195;&#29702;&#25191;&#34892;&#22810;&#27493;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;GPT&#20316;&#20026;&#22823;&#33041;&#30340;&#20307;&#39564;&#20195;&#29702;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#20915;&#31574;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#38646;-shot&#20195;&#29702;&#21482;&#20419;&#20351;GPT-4&#22312;&#23616;&#37096;&#29615;&#22659;&#20013;&#36873;&#25321;&#28508;&#22312;&#20301;&#32622;&#65292;&#32780;&#27809;&#26377;&#20026;&#20195;&#29702;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#8220;&#20840;&#23616;&#35270;&#22270;&#8221;&#26469;&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22320;&#22270;&#24341;&#23548;&#30340;&#22522;&#20110;GPT&#30340;&#20195;&#29702;&#65292;&#21517;&#20026;MapGPT&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#26469;&#40723;&#21169;&#20840;&#23616;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#32447;&#22320;&#22270;&#65292;&#24182;&#23558;&#20854;&#21512;&#24182;&#21040;&#21253;&#21547;&#33410;&#28857;&#20449;&#24687;&#21644;&#25299;&#25169;&#20851;&#31995;&#30340;&#25552;&#31034;&#20013;&#65292;&#20197;&#24110;&#21161;GPT&#29702;&#35299;&#31354;&#38388;&#29615;&#22659;&#12290;&#20174;&#36825;&#19968;&#35774;&#35745;&#20013;&#33719;&#30410;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#65292;&#20197;&#24110;&#21161;&#20195;&#29702;&#26681;&#25454;&#22320;&#22270;&#25191;&#34892;&#22810;&#27493;&#35268;&#21010;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#22810;&#20010;&#20505;&#36873;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07314v2 Announce Type: replace  Abstract: Embodied agents equipped with GPT as their brain have exhibited extraordinary decision-making and generalization abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT-4 to select potential locations within localized environments, without constructing an effective "global-view" for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based agent, dubbed MapGPT, which introduces an online linguistic-formed map to encourage the global exploration. Specifically, we build an online map and incorporate it into the prompts that include node information and topological relationships, to help GPT understand the spatial environment. Benefiting from this design, we further propose an adaptive planning mechanism to assist the agent in performing multi-step path planning based on a map, systematically exploring multiple candidate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.13327</link><description>&lt;p&gt;
&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#30340;&#24773;&#22659;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Reinforcement Learning for Variable Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#20808;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#22810;&#24773;&#33410;&#35757;&#32451;&#30340;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#21487;&#20197;&#22312;&#24773;&#22659;&#20013;&#27867;&#21270;&#21040;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#24341;&#20837;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#36890;&#24120;&#38656;&#35201;&#25968;&#25454;&#37325;&#26032;&#25910;&#38598;&#21644;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21482;&#35757;&#32451;&#19968;&#27425;&#30340;Headless-AD&#27169;&#22411;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20855;&#26377;&#21487;&#21464;&#22823;&#23567;&#12289;&#35821;&#20041;&#20869;&#23481;&#21644;&#39034;&#24207;&#30340;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#22312;&#20271;&#21162;&#21033;&#21644;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#20197;&#21450;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Headless-AD&#22312;&#20174;&#26410;&#36935;&#21040;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#20960;&#20010;&#29615;&#22659;&#37197;&#32622;&#19978;&#32988;&#36807;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25233;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.11560</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#21457;&#29616;&#65306;&#20851;&#20110;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25233;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#65292;&#33258;&#21457;&#29616;&#21463;&#21040;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#20551;&#35774;&#65292;&#21363;&#22312;&#35268;&#27169;&#25193;&#22823;&#30340;&#36807;&#31243;&#20013;&#39640;&#24230;&#20419;&#36827;&#24615;&#33021;&#30340;&#22240;&#32032;&#65306;&#20943;&#23569;&#21482;&#33021;&#19982;&#29305;&#23450;&#29305;&#24449;&#24418;&#25104;&#19968;&#23545;&#19968;&#20851;&#31995;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#12290;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#24448;&#24448;&#26356;&#31232;&#30095;&#65292;&#24182;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#24605;&#36335;&#26469;&#35782;&#21035;&#21644;&#25233;&#21046;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#26159;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#27809;&#26377;&#32479;&#19968;&#30340;&#23450;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#31616;&#21333;&#22320;&#31105;&#27490;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#24182;&#19981;&#33021;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#24847;&#24605;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#33258;&#21457;&#29616;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#24320;&#20102;&#20851;&#20110;&#31215;&#26497;&#25233;&#21046;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11560v2 Announce Type: replace-cross  Abstract: Recently, emergence has received widespread attention from the research community along with the success of large language models. Different from the literature, we hypothesize a key factor that highly promotes the performance during the increase of scale: the reduction of monosemantic neurons that can only form one-to-one correlations with specific features. Monosemantic neurons tend to be sparser and have negative impacts on the performance in large models. Inspired by this insight, we propose an intuitive idea to identify monosemantic neurons and inhibit them. However, achieving this goal is a non-trivial task as there is no unified quantitative evaluation metric and simply banning monosemantic neurons does not promote polysemanticity in neural networks. Therefore, we propose to learn from emergence and present a study on proactively inhibiting the monosemantic neurons in this paper. More specifically, we first propose a new
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24207;&#36143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#21382;&#21490;&#65292;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2312.04772</link><description>&lt;p&gt;
&#22312;&#24207;&#36143;&#20915;&#31574;&#20013;&#35760;&#20303;&#20844;&#24179;&#65306;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24207;&#36143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#21382;&#21490;&#65292;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;&#20915;&#31574;&#21046;&#23450;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38024;&#23545;&#21333;&#19968;&#20915;&#31574;&#36827;&#34892;&#30740;&#31350;&#30340;&#12290;&#26412;&#25991;&#22312;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#33021;&#21463;&#21040;&#20915;&#31574;&#32467;&#26524;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#20844;&#24179;&#27010;&#24565;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#30340;&#21382;&#21490;&#65292;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35762;&#65292;&#23427;&#26159;&#22266;&#26377;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#20844;&#24179;&#36890;&#24120;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#26576;&#20010;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#36807;&#31243;&#32467;&#26463;&#26102;&#12290;&#20026;&#20102;&#25512;&#36827;&#25105;&#20204;&#23545;&#36825;&#31867;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#39034;&#24207;&#20915;&#31574;&#32972;&#26223;&#19979;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#30340;&#23646;&#24615;&#65292;&#21253;&#25324;&#38271;&#26399;&#20844;&#24179;&#24615;&#12289;&#20219;&#24847;&#26102;&#21051;&#20844;&#24179;&#24615;&#12289;&#21608;&#26399;&#24615;&#20844;&#24179;&#24615;&#21644;&#26377;&#30028;&#20844;&#24179;&#24615;&#31561;&#27010;&#24565;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#21644;&#35760;&#24518;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#36825;&#22914;&#20309;&#25903;&#25345;&#21046;&#23450;&#20844;&#24179;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04772v3 Announce Type: replace  Abstract: Fair decision making has largely been studied with respect to a single decision. In this paper we investigate the notion of fairness in the context of sequential decision making where multiple stakeholders can be affected by the outcomes of decisions. We observe that fairness often depends on the history of the sequential decision-making process, and in this sense that it is inherently non-Markovian. We further observe that fairness often needs to be assessed at time points within the process, not just at the end of the process. To advance our understanding of this class of fairness problems, we explore the notion of non-Markovian fairness in the context of sequential decision making. We identify properties of non-Markovian fairness, including notions of long-term, anytime, periodic, and bounded fairness. We further explore the interplay between non-Markovian fairness and memory, and how this can support construction of fair policies
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16755</link><description>&lt;p&gt;
&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion model for relational inference. (arXiv:2401.16755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#21160;&#24577;&#34892;&#20026;&#65292;&#21253;&#25324;&#22823;&#33041;&#27963;&#21160;&#12289;&#37329;&#34701;&#20215;&#26684;&#27874;&#21160;&#21644;&#29289;&#29702;&#38598;&#20307;&#29616;&#35937;&#65292;&#19982;&#31995;&#32479;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30456;&#20851;&#12290;&#21033;&#29992;&#21487;&#35266;&#27979;&#30340;&#21160;&#24577;&#26469;&#21457;&#29616;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#20851;&#31995;&#34987;&#31216;&#20026;&#20851;&#31995;&#25512;&#29702;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#23427;&#20511;&#37492;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#25554;&#20540;&#26041;&#27861;&#12290;DiffRI&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#12290;&#23545;&#20110;&#27169;&#25311;&#21644;&#20934;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DiffRI&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamical behaviors of complex interacting systems, including brain activities, financial price movements, and physical collective phenomena, are associated with underlying interactions between the system's components. The issue of uncovering interaction relations in such systems using observable dynamics is called relational inference. In this study, we propose a Diffusion model for Relational Inference (DiffRI), inspired by a self-supervised method for probabilistic time series imputation. DiffRI learns to infer the probability of the presence of connections between components through conditional diffusion modeling. Experiments on both simulated and quasi-real datasets show that DiffRI is highly competent compared with other state-of-the-art models in discovering ground truth interactions in an unsupervised manner. Our code will be made public soon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10471</link><description>&lt;p&gt;
DeepEdit: &#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#24335;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10471
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#35270;&#20026;&#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeepEdit&#65288;&#22522;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#28176;&#36827;&#24335;&#35299;&#30721;&#30693;&#35782;&#32534;&#36753;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12289;&#38382;&#39064;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#26469;&#25913;&#36827;&#30693;&#35782;&#32534;&#36753;&#12290;DeepEdit&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#25152;&#26377;&#40657;&#30418;LLMs&#65306;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#34920;&#31034;&#25110;&#36755;&#20986;&#35789;&#27719;&#20998;&#24067;&#12290;DeepEdit&#36880;&#27493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#20462;&#25913;LLMs&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#36755;&#20986;&#23545;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#12290;&#22312;&#30693;&#35782;&#32534;&#36753;&#26041;&#38754;&#65292;DeepEdit&#22312;&#25511;&#21046;LLMs&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;MQuaKE&#19978;&#65292;DeepEdit&#22312;&#23450;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
&lt;/p&gt;</description></item><item><title>MISS&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25105;&#20204;&#25226;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20316;&#20026;&#19968;&#20010;&#29983;&#25104;&#24335;&#20219;&#21153;&#22788;&#29702;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#23454;&#29616;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.05163</link><description>&lt;p&gt;
MISS&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05163
&lt;/p&gt;
&lt;p&gt;
MISS&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25105;&#20204;&#25226;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20316;&#20026;&#19968;&#20010;&#29983;&#25104;&#24335;&#20219;&#21153;&#22788;&#29702;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#23454;&#29616;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20854;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22810;&#25968;&#26041;&#27861;&#23558;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#35270;&#20026;&#19968;&#20010;&#38590;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#30340;&#31572;&#26696;&#20998;&#31867;&#20219;&#21153;&#12290;&#21478;&#22806;&#65292;&#30001;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#31169;&#24615;&#21644;&#26114;&#36149;&#30340;&#27880;&#37322;&#36807;&#31243;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#21307;&#23398;&#22270;&#25991;&#23545;&#25968;&#25454;&#38598;&#20005;&#37325;&#32570;&#20047;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;MISS&#65289;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#35270;&#20026;&#19968;&#39033;&#29983;&#25104;&#24335;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision fiel
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#35266;&#23519;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2312.15643</link><description>&lt;p&gt;
&#36890;&#36807;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#25512;&#36827;&#35825;&#23548;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation. (arXiv:2312.15643v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15643
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#35266;&#23519;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35825;&#23548;&#25512;&#29702;&#26159;&#36890;&#36807;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#26469;&#35299;&#37322;&#35266;&#23519;&#32467;&#26524;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#20351;&#29992;&#30693;&#35782;&#36827;&#34892;&#35299;&#37322;&#65292;&#20294;&#23558;&#35825;&#23548;&#25512;&#29702;&#19982;&#32467;&#26500;&#21270;&#30693;&#35782;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#32467;&#21512;&#20351;&#29992;&#30340;&#26041;&#27861;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#22797;&#26434;&#30340;&#36923;&#36753;&#20551;&#35774;&#65292;&#20197;&#35299;&#37322;&#19968;&#32452;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32463;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#35266;&#23519;&#32467;&#26524;&#26102;&#65292;&#36825;&#31181;&#35757;&#32451;&#30446;&#26631;&#24182;&#19981;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#20551;&#35774;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusio
&lt;/p&gt;</description></item><item><title>CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08753</link><description>&lt;p&gt;
CompA: &#35299;&#20915;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08753
&lt;/p&gt;
&lt;p&gt;
CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#30340;&#22522;&#26412;&#29305;&#24615;&#26159;&#20854;&#32452;&#21512;&#24615;&#12290;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;CLAP&#65289;&#35757;&#32451;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#33021;&#22815;&#23398;&#20064;&#38899;&#39057;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12289;&#38899;&#39057;&#26816;&#32034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25191;&#34892;&#32452;&#21512;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#36824;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CompA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#30495;&#23454;&#19990;&#30028;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CompA-order&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#25110;&#21457;&#29983;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#32780;CompA-attribute&#35780;&#20272;&#22768;&#38899;&#20107;&#20214;&#30340;&#23646;&#24615;&#32465;&#23450;&#12290;&#27599;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21253;&#21547;&#20004;&#20010;&#38899;&#39057;-&#26631;&#39064;&#23545;&#65292;&#20854;&#20013;&#20004;&#20010;&#38899;&#39057;&#20855;&#26377;&#30456;&#21516;&#30340;&#22768;&#38899;&#20107;&#20214;&#65292;&#20294;&#32452;&#21512;&#26041;&#24335;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07059</link><description>&lt;p&gt;
DKEC: &#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#30005;&#23376;&#30149;&#21382;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32463;&#24120;&#38754;&#20020;&#38271;&#23614;&#26631;&#31614;&#20998;&#24067;&#65292;&#21363;&#32597;&#35265;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#23569;&#20110;&#39057;&#32321;&#31867;&#21035;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#23618;&#27425;&#21270;&#26631;&#31614;&#32467;&#26500;&#26469;&#25214;&#21040;&#37325;&#35201;&#29305;&#24449;&#65292;&#20294;&#22823;&#22810;&#25968;&#24573;&#30053;&#20102;&#20174;&#21307;&#23398;&#25351;&#21335;&#20013;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#30340;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21019;&#26032;&#28857;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22522;&#20110;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#32467;&#21512;&#24322;&#26500;&#22270;&#21644;&#39046;&#22495;&#26412;&#20307;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#65288;2&#65289;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#30456;&#20284;&#24615;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DKEC&#65306;RAA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#24613;&#25937;&#26381;&#21153;&#65288;EMS&#65289;&#20107;&#20214;&#30340;4,417&#20010;&#24739;&#32773;&#25252;&#29702;&#25253;&#21578;&#30340;&#25910;&#38598;&#65292;&#21644;&#26469;&#33258;53898&#25253;&#21578;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05227</link><description>&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#38761;&#21629;&#31185;&#23398;&#33539;&#24335;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#36807;&#31243;&#30340;&#27700;&#25991;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05227
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#23545;&#20110;&#35299;&#20915;&#27700;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#21160;&#24577;&#24433;&#21709;&#19979;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#27700;&#25991;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#29420;&#31435;&#30340;&#33539;&#24335;&#23384;&#22312;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20197;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#31181;&#35748;&#30693;&#38556;&#30861;&#65292;&#24182;&#38761;&#26032;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#35770;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#31038;&#21306;&#65288;PaML&#65289;&#65292;&#23558;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#25110;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#27169;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20174;&#29289;&#29702;&#25968;&#25454;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#20449;&#24687;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#24863;&#30693;&#28151;&#21512;&#23398;&#20064;&#22235;&#20010;&#26041;&#38754;&#20998;&#26512;&#20102;&#36825;&#20123;PaML&#26041;&#27861;&#12290;PaML&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20551;&#35774;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05128</link><description>&lt;p&gt;
&#23454;&#20363;&#21644;&#26631;&#31614;: &#38024;&#23545;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification. (arXiv:2310.05128v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;HMTC&#65289;&#26088;&#22312;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#36817;&#26399;&#20851;&#20110;HMTC&#30340;&#26041;&#27861;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#20197;&#21322;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#25991;&#26412;&#21644;&#26631;&#31614;&#23884;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#26045;&#21152;&#36807;&#24230;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#30340;&#29983;&#25104;&#24448;&#24448;&#24341;&#20837;&#22122;&#22768;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#21516;&#19968;&#25209;&#27425;&#20013;&#30456;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26041;&#27861;&#26159;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#26631;&#31614;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\textbf{HJCL}$&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22635;&#34917;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;HMTC&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#20180;&#32454;&#26500;&#36896;&#25209;&#27425;&#26469;&#28385;&#36275;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GPT-4&#22312;&#26032;&#25968;&#25454;&#19978;&#25191;&#34892;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#22797;&#21046;&#30340;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#20854;&#25581;&#31034;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#26041;&#27861;&#20013;&#20551;&#35774;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.01727</link><description>&lt;p&gt;
GPT-4&#33021;&#21542;&#22797;&#21046;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GPT-4 Replicate Empirical Software Engineering Research?. (arXiv:2310.01727v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GPT-4&#22312;&#26032;&#25968;&#25454;&#19978;&#25191;&#34892;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#22797;&#21046;&#30340;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#20854;&#25581;&#31034;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#26041;&#27861;&#20013;&#20551;&#35774;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29983;&#20135;&#31995;&#32479;&#30340;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24102;&#26469;&#20102;&#23545;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#29983;&#20135;&#31995;&#32479;&#30340;&#19968;&#23567;&#37096;&#20998;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#38480;&#21046;&#20102;&#35813;&#30740;&#31350;&#30340;&#24433;&#21709;&#21147;&#12290;&#34429;&#28982;&#36719;&#20214;&#24037;&#31243;&#20174;&#19994;&#32773;&#21487;&#20197;&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;&#26469;&#33719;&#24471;&#20805;&#23454;&#33258;&#24049;&#25968;&#25454;&#30340;&#22909;&#22788;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#22240;&#20026;&#25191;&#34892;&#22797;&#21046;&#38656;&#35201;&#23545;&#30740;&#31350;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#31243;&#25968;&#25454;&#20013;&#30340;&#24494;&#22937;&#32454;&#33410;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#22312;&#22788;&#29702;&#36719;&#20214;&#24037;&#31243;&#21644;&#31185;&#23398;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#25512;&#24191;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#26032;&#25968;&#25454;&#19978;&#25191;&#34892;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#22797;&#21046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#23427;&#20204;&#25581;&#31034;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#26041;&#27861;&#20013;&#25152;&#20570;&#30340;&#20551;&#35774;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help democratize empirical software engineering research.  In this paper, we examine LLMs' abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodolog
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.15857</link><description>&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15857
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#25104;&#20026;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#23548;&#33268;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#20215;&#20540;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#21457;&#23637;&#37324;&#31243;&#30865;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#23427;&#20204;&#30340;&#21457;&#23637;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#22522;&#20110;&#23427;&#20204;&#34987;&#24341;&#20837;&#30340;&#26102;&#38388;&#21644;&#23545;&#23398;&#31185;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20219;&#21153;&#22312;&#23398;&#26415;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#26222;&#21450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#19982;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30456;&#20851;&#30340;&#20219;&#21153;&#21010;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#31867;&#22411;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#20173;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amidst the evolving landscape of artificial intelligence, the convergence of visual and textual information has surfaced as a crucial frontier, leading to the advent of image-text multimodal models. This paper provides a comprehensive review of the evolution and current state of image-text multimodal models, exploring their application value, challenges, and potential research trajectories. Initially, we revisit the basic concepts and developmental milestones of these models, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline. Furthermore, based on the tasks' significance and prevalence in the academic landscape, we propose a categorization of the tasks associated with image-text multimodal models into five major types, elucidating the recent progress and key technologies within each category. Despite the remarkable accomplishments of these models, numerous challenges a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2309.14780</link><description>&lt;p&gt;
&#36716;&#31227;&#27668;&#20505;&#21464;&#21270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14780
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27668;&#20505;&#39044;&#27979;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#20110;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#22312;&#23545;&#23567;&#23610;&#24230;&#29289;&#29702;&#36807;&#31243;&#65288;&#20363;&#22914;&#20113;&#65289;&#30340;&#34920;&#31034;&#20013;&#26412;&#36136;&#19978;&#36827;&#34892;&#20102;&#36817;&#20284;&#65292;&#36825;&#26159;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#23545;&#22686;&#21152;&#30340;&#28201;&#23460;&#27668;&#20307;&#27987;&#24230;&#30340;&#21709;&#24212;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26681;&#28304;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#21382;&#21490;&#35266;&#27979;&#32422;&#26463;&#26410;&#26469;&#39044;&#27979;&#65292;&#24182;&#20943;&#23569;&#27668;&#20505;&#39044;&#27979;&#21644;&#27668;&#20505;&#21453;&#39304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#27668;&#20505;&#31995;&#32479;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#21644;&#25972;&#21512;&#20174;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
&lt;/p&gt;</description></item><item><title>CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11143</link><description>&lt;p&gt;
CoT-BERT: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11143
&lt;/p&gt;
&lt;p&gt;
CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#36755;&#20837;&#21477;&#23376;&#36716;&#21270;&#20026;&#23500;&#21547;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#30340;&#22266;&#23450;&#38271;&#24230;&#21521;&#37327;&#65292;&#21516;&#26102;&#28040;&#38500;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#25512;&#21160;&#19979;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#36712;&#36857;&#20013;&#65292;&#20173;&#28982;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#20026;&#20102;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20013;&#30340;&#28508;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21477;&#23376;&#34920;&#31034;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#29702;&#35299;&#21644;&#25688;&#35201;&#12290;&#38543;&#21518;&#65292;&#21518;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#34987;&#21033;&#29992;&#20026;&#36755;&#20837;&#21477;&#23376;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#21644;&#27169;&#26495;&#21435;&#22122;&#25216;&#26415;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CoT-BERT&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2308.07706</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25506;&#32034;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25972;&#21512;&#25991;&#26412;&#25351;&#23548;&#20197;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#20173;&#28982;&#26159;&#19968;&#20010;&#36827;&#23637;&#26377;&#38480;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#30340;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#22312;&#24320;&#25918;&#39046;&#22495;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#30452;&#25509;&#24212;&#29992;&#30340;&#38590;&#39064;&#65292;&#38656;&#35201;&#25163;&#21160;&#20171;&#20837;&#25110;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20174;&#22270;&#20687;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;&#33021;&#22815;&#23545;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20197;&#35780;&#20272;&#20854;&#20174;&#24320;&#25918;&#39046;&#22495;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#24341;&#20837;&#20102;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#32771;&#34385;&#19987;&#21033;&#20449;&#24687;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#19987;&#21033;&#20998;&#31867;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#32508;&#21512;&#20102;&#19987;&#21033;&#30340;&#25991;&#26412;&#25551;&#36848;&#12289;&#26435;&#21033;&#20154;&#20449;&#24687;&#21644;IPC&#20195;&#30721;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20026;&#19987;&#21033;&#20998;&#31867;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.05385</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20998;&#31867;&#23398;&#20064;&#21644;&#21382;&#21490;&#27169;&#24335;&#24314;&#27169;&#29992;&#20110;&#19987;&#21033;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adaptive Taxonomy Learning and Historical Patterns Modelling for Patent Classification. (arXiv:2308.05385v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#32771;&#34385;&#19987;&#21033;&#20449;&#24687;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#19987;&#21033;&#20998;&#31867;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#32508;&#21512;&#20102;&#19987;&#21033;&#30340;&#25991;&#26412;&#25551;&#36848;&#12289;&#26435;&#21033;&#20154;&#20449;&#24687;&#21644;IPC&#20195;&#30721;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20026;&#19987;&#21033;&#20998;&#31867;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#21033;&#20998;&#31867;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#19987;&#21033;&#20998;&#37197;&#22810;&#20010;&#22269;&#38469;&#19987;&#21033;&#20998;&#31867;&#65288;IPC&#65289;&#20195;&#30721;&#12290;&#26368;&#36817;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#19987;&#21033;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#20998;&#26512;&#19987;&#21033;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#25991;&#26412;&#20043;&#22806;&#65292;&#27599;&#20010;&#19987;&#21033;&#36824;&#19982;&#19968;&#20123;&#26435;&#21033;&#20154;&#30456;&#20851;&#32852;&#65292;&#20102;&#35299;&#20182;&#20204;&#30003;&#35831;&#30340;&#19987;&#21033;&#23545;&#20110;&#20998;&#31867;&#24448;&#24448;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#27492;&#22806;&#65292;IPC&#31995;&#32479;&#21046;&#23450;&#30340;&#23618;&#27425;&#21270;&#20998;&#31867;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#19988;&#20351;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;IPC&#20195;&#30721;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#32508;&#21512;&#32771;&#34385;&#19978;&#36848;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#32771;&#34385;&#19987;&#21033;&#20449;&#24687;&#30340;&#25972;&#21512;&#26694;&#26550;&#26469;&#36827;&#34892;&#19987;&#21033;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;IPC&#20195;&#30721;&#30456;&#20851;&#24615;&#23398;&#20064;&#27169;&#22359;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23618;&#32423;&#21644;&#19981;&#21516;&#23618;&#32423;&#20043;&#38388;&#33258;&#36866;&#24212;&#22320;&#20256;&#36882;&#21644;&#32858;&#21512;&#28040;&#24687;&#65292;&#20174;&#32780;&#24471;&#21040;&#23427;&#20204;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patent classification aims to assign multiple International Patent Classification (IPC) codes to a given patent. Recent methods for automatically classifying patents mainly focus on analyzing the text descriptions of patents. However, apart from the texts, each patent is also associated with some assignees, and the knowledge of their applied patents is often valuable for classification. Furthermore, the hierarchical taxonomy formulated by the IPC system provides important contextual information and enables models to leverage the correlations between IPC codes for more accurate classification. However, existing methods fail to incorporate the above aspects. In this paper, we propose an integrated framework that comprehensively considers the information on patents for patent classification. To be specific, we first present an IPC codes correlations learning module to derive their semantic representations via adaptively passing and aggregating messages within the same level and across dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#22320;&#22270;&#34920;&#31034;&#65292;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#26469;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04792</link><description>&lt;p&gt;
&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#34892;&#26143;&#25506;&#27979;&#36710;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast and Optimal Learning-based Path Planning Method for Planetary Rovers. (arXiv:2308.04792v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#22320;&#22270;&#34920;&#31034;&#65292;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#26469;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#23545;&#20110;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#31243;&#22270;&#20013;&#24555;&#36895;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#65292;&#31216;&#20026;NNPP&#12290;NNPP&#27169;&#22411;&#20174;&#22823;&#37327;&#39044;&#27880;&#37322;&#30340;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#23398;&#20064;&#36215;&#22987;&#21644;&#30446;&#26631;&#20301;&#32622;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#22320;&#22270;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#27599;&#20010;&#20687;&#32032;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#34920;&#31034;&#20854;&#23646;&#20110;&#22320;&#22270;&#19978;&#26368;&#20248;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20174;DEM&#33719;&#21462;&#30340;&#22369;&#24230;&#12289;&#31895;&#31961;&#24230;&#21644;&#39640;&#24230;&#24046;&#35745;&#31639;&#27599;&#20010;&#32593;&#26684;&#21333;&#20803;&#30340;&#36941;&#21382;&#25104;&#26412;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#23545;&#36215;&#22987;&#21644;&#30446;&#26631;&#20301;&#32622;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20998;&#26512;&#19981;&#21516;&#20301;&#32622;&#32534;&#30721;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;NNPP&#27169;&#22411;&#33021;&#22815;&#22312;&#26032;&#30340;&#22320;&#22270;&#19978;&#25191;&#34892;&#36335;&#24452;&#35268;&#21010;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;NNPP&#29983;&#25104;&#30340;&#24341;&#23548;&#22330;&#33021;&#22815;&#20934;&#30830;&#25351;&#23548;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent autonomous path planning is crucial to improve the exploration efficiency of planetary rovers. In this paper, we propose a learning-based method to quickly search for optimal paths in an elevation map, which is called NNPP. The NNPP model learns semantic information about start and goal locations, as well as map representations, from numerous pre-annotated optimal path demonstrations, and produces a probabilistic distribution over each pixel representing the likelihood of it belonging to an optimal path on the map. More specifically, the paper computes the traversal cost for each grid cell from the slope, roughness and elevation difference obtained from the DEM. Subsequently, the start and goal locations are encoded using a Gaussian distribution and different location encoding parameters are analyzed for their effect on model performance. After training, the NNPP model is able to perform path planning on novel maps. Experiments show that the guidance field generated by the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.01646</link><description>&lt;p&gt;
SwinGNN:&#37325;&#26032;&#24605;&#32771;&#22312;&#22270;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32622;&#25442;&#31561;&#21464;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#32622;&#25442;&#19981;&#21464;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#38750;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#19981;&#21464;&#27169;&#22411;&#36935;&#21040;&#20102;&#26356;&#22823;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20204;&#30340;&#30446;&#26631;&#20998;&#24067;&#26356;&#20855;&#27169;&#24577;&#24615;&#65307;2&#65289;&#23427;&#20204;&#30340;&#26368;&#20248;&#19968;&#27493;&#21435;&#22122;&#24471;&#20998;&#26159;&#20855;&#26377;&#26356;&#22810;&#25104;&#20998;&#30340;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#19981;&#21464;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;SwinGNN&#8221;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#21040;&#36793;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;SwinTransformers&#20013;&#30340;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#21644;&#21078;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#31181;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#65292;&#21363;&#38543;&#26426;&#32622;&#25442;&#29983;&#25104;&#30340;&#22270;&#65292;&#21487;&#20197;&#35777;&#26126;&#23558;&#20219;&#20309;&#22270;&#36716;&#25442;&#25104;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;AI&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#21147;&#27874;&#25506;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28151;&#21512;&#33192;&#32960;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#24341;&#21147;&#27874;&#20449;&#21495;&#30340;&#26102;&#31354;&#20851;&#32852;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35757;&#32451;&#22810;&#20010;AI&#27169;&#22411;&#65292;&#24182;&#22312;&#30701;&#26102;&#38388;&#20869;&#33719;&#24471;&#26368;&#20339;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15728</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;AI&#38598;&#21512;&#29992;&#20110;&#24341;&#21147;&#27874;&#25506;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physics-inspired spatiotemporal-graph AI ensemble for gravitational wave detection. (arXiv:2306.15728v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;AI&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#21147;&#27874;&#25506;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28151;&#21512;&#33192;&#32960;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#24341;&#21147;&#27874;&#20449;&#21495;&#30340;&#26102;&#31354;&#20851;&#32852;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35757;&#32451;&#22810;&#20010;AI&#27169;&#22411;&#65292;&#24182;&#22312;&#30701;&#26102;&#38388;&#20869;&#33719;&#24471;&#26368;&#20339;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25506;&#27979;&#24341;&#21147;&#27874;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#65306;1&#65289;&#28151;&#21512;&#33192;&#32960;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#24341;&#21147;&#27874;&#20449;&#21495;&#30340;&#30701;&#26102;&#21644;&#38271;&#26102;&#24207;&#21015;&#20449;&#24687;&#65307;&#20197;&#21450;2&#65289;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25429;&#25417;&#24341;&#21147;&#27874;&#22825;&#25991;&#35266;&#27979;&#31449;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#32852;&#65292;&#20197;&#19968;&#33268;&#22320;&#25551;&#36848;&#21644;&#35782;&#21035;&#25506;&#27979;&#22120;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#23384;&#22312;&#12290;&#36825;&#20123;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;AI&#27169;&#22411;&#32463;&#36807;&#27979;&#35797;&#65292;&#29992;&#20110;&#25506;&#27979;&#36817;&#22278;&#38750;&#33258;&#26059;&#21644;&#36817;&#22278;&#33258;&#26059;&#38750;&#36827;&#21160;&#30340;&#20108;&#36827;&#21046;&#40657;&#27934;&#21512;&#24182;&#20135;&#29983;&#30340;&#24341;&#21147;&#27874;&#20449;&#21495;&#12290;&#23545;&#20110;&#21518;&#19968;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#21253;&#21547;120&#19975;&#20010;&#27169;&#25311;&#27874;&#24418;&#30340;&#25968;&#25454;&#38598;&#26469;&#23494;&#38598;&#37319;&#26679;&#36825;&#20010;&#20449;&#21495;&#27969;&#24418;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#38463;&#36129;&#22269;&#23478;&#23454;&#39564;&#23460;&#40857;&#22836;&#36229;&#32423;&#35745;&#31639;&#26426;Polaris&#19978;&#20351;&#29992;256&#20010;NVIDIA A100 GPU&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#22312;1.7&#23567;&#26102;&#20869;&#23454;&#29616;&#20102;&#35757;&#32451;&#26102;&#38388;&#21040;&#35299;&#20915;&#26041;&#26696;&#30340;&#20943;&#23567;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel method for gravitational wave detection that combines: 1) hybrid dilated convolution neural networks to accurately model both shortand long-range temporal sequential information of gravitational wave signals; and 2) graph neural networks to capture spatial correlations among gravitational wave observatories to consistently describe and identify the presence of a signal in a detector network. These spatiotemporal-graph AI models are tested for signal detection of gravitational waves emitted by quasi-circular, non-spinning and quasi-circular, spinning, non-precessing binary black hole mergers. For the latter case, we needed a dataset of 1.2 million modeled waveforms to densely sample this signal manifold. Thus, we reduced time-to-solution by training several AI models in the Polaris supercomputer at the Argonne Leadership Supercomputing Facility within 1.7 hours by distributing the training over 256 NVIDIA A100 GPUs, achieving optimal classification performance. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#65292;&#33021;&#22815;&#20351;&#29992;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#34920;&#36798;&#32422;&#26463;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#24182;&#20445;&#35777;&#32422;&#26463;&#26465;&#20214;&#30340;&#27010;&#29575;&#36275;&#22815;&#39640;&#12290;&#21516;&#26102;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#22810;&#26234;&#33021;&#20307;&#35774;&#32622;&#36827;&#34892;&#26368;&#20248;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.14736</link><description>&lt;p&gt;
&#36923;&#36753;&#32422;&#26463;&#19979;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Optimal Control of Logically Constrained Partially Observable and Multi-Agent Markov Decision Processes. (arXiv:2305.14736v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#65292;&#33021;&#22815;&#20351;&#29992;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#34920;&#36798;&#32422;&#26463;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#24182;&#20445;&#35777;&#32422;&#26463;&#26465;&#20214;&#30340;&#27010;&#29575;&#36275;&#22815;&#39640;&#12290;&#21516;&#26102;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#22810;&#26234;&#33021;&#20307;&#35774;&#32622;&#36827;&#34892;&#26368;&#20248;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#31995;&#32479;&#36890;&#24120;&#20250;&#20135;&#29983;&#36923;&#36753;&#32422;&#26463;&#65292;&#20363;&#22914;&#26469;&#33258;&#23433;&#20840;&#12289;&#25805;&#20316;&#25110;&#27861;&#35268;&#35201;&#27714;&#65292;&#21487;&#20197;&#29992;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#34920;&#36798;&#36825;&#20123;&#32422;&#26463;&#12290;&#31995;&#32479;&#29366;&#24577;&#36890;&#24120;&#26159;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#65292;&#21487;&#33021;&#21253;&#21547;&#20855;&#26377;&#20849;&#21516;&#30446;&#26631;&#20294;&#19981;&#21516;&#20449;&#24687;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#22810;&#20010;&#26234;&#33021;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#65292;&#29992;&#20110;&#20855;&#26377;&#26377;&#38480;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#32422;&#26463;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#28385;&#36275;&#26102;&#38388;&#36923;&#36753;&#32422;&#26463;&#30340;&#27010;&#29575;&#36275;&#22815;&#39640;&#26102;&#26368;&#22823;&#21270;&#32047;&#31215;&#22238;&#25253;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20851;&#20110;&#36817;&#20284;&#22870;&#21169;&#26368;&#20248;&#24615;&#21644;&#32422;&#26463;&#28385;&#36275;&#30340;&#20445;&#35777;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#35774;&#32622;&#36827;&#34892;&#26368;&#20248;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#35813;&#26041;&#27861;&#24182;&#32473;&#20986;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous systems often have logical constraints arising, for example, from safety, operational, or regulatory requirements. Such constraints can be expressed using temporal logic specifications. The system state is often partially observable. Moreover, it could encompass a team of multiple agents with a common objective but disparate information structures and constraints. In this paper, we first introduce an optimal control theory for partially observable Markov decision processes (POMDPs) with finite linear temporal logic constraints. We provide a structured methodology for synthesizing policies that maximize a cumulative reward while ensuring that the probability of satisfying a temporal logic constraint is sufficiently high. Our approach comes with guarantees on approximate reward optimality and constraint satisfaction. We then build on this approach to design an optimal control framework for logically constrained multi-agent settings with information asymmetry. We illustrate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#8212;&#8212;&#35299;&#37322;&#21512;&#29702;&#24615;&#12290;&#34429;&#28982;&#19968;&#30452;&#34987;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#65292;&#20294;&#26159;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.17707</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#21487;&#35299;&#37322;&#24615;&#19982;&#21512;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking AI Explainability and Plausibility. (arXiv:2303.17707v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#8212;&#8212;&#35299;&#37322;&#21512;&#29702;&#24615;&#12290;&#34429;&#28982;&#19968;&#30452;&#34987;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#65292;&#20294;&#26159;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31639;&#27861;&#31526;&#21512;&#20154;&#31867;&#20132;&#27969;&#35268;&#33539;&#65292;&#25903;&#25345;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#28385;&#36275;&#20154;&#31867;&#23545;&#20110;AI&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#35774;&#23450;&#36866;&#24403;&#30340;&#35780;&#20272;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#37322;&#21512;&#29702;&#24615;&#65292;&#36825;&#26159;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#12290;&#21512;&#29702;&#24615;&#34913;&#37327;&#26426;&#22120;&#35299;&#37322;&#19982;&#20154;&#31867;&#35299;&#37322;&#30456;&#27604;&#30340;&#21512;&#29702;&#31243;&#24230;&#12290;&#21512;&#29702;&#24615;&#19968;&#30452;&#34987;&#20256;&#32479;&#22320;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#12290;&#25105;&#20204;&#21453;&#23545;&#36825;&#20010;&#24819;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20248;&#21270;&#21644;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35780;&#20272;XAI&#31639;&#27861;&#30340;&#21512;&#29702;&#24615;&#20250;&#35268;&#33539;&#26426;&#22120;&#35299;&#37322;&#65292;&#20197;&#34920;&#36798;&#19982;&#20154;&#31867;&#35299;&#37322;&#23436;&#20840;&#30456;&#21516;&#30340;&#20869;&#23481;&#65292;&#36825;&#20559;&#31163;&#20102;&#20154;&#31867;&#35299;&#37322;&#30340;&#22522;&#26412;&#21160;&#26426;&#65306;&#34920;&#36798;&#33258;&#24049;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Setting proper evaluation objectives for explainable artificial intelligence (XAI) is vital for making XAI algorithms follow human communication norms, support human reasoning processes, and fulfill human needs for AI explanations. In this article, we examine explanation plausibility, which is the most pervasive human-grounded concept in XAI evaluation. Plausibility measures how reasonable the machine explanation is compared to the human explanation. Plausibility has been conventionally formulated as an important evaluation objective for AI explainability tasks. We argue against this idea, and show how optimizing and evaluating XAI for plausibility is sometimes harmful, and always ineffective to achieve model understandability, transparency, and trustworthiness. Specifically, evaluating XAI algorithms for plausibility regularizes the machine explanation to express exactly the same content as human explanation, which deviates from the fundamental motivation for humans to explain: expres
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#26500;&#36896;&#24207;&#21015;&#25968;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20844;&#24335;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.13186</link><description>&lt;p&gt;
&#26500;&#36896;&#25968;&#65306;&#22914;&#20309;&#24314;&#31435;&#19968;&#20010;&#22270;&#24418;&#65311;
&lt;/p&gt;
&lt;p&gt;
Construction numbers: How to build a graph?. (arXiv:2302.13186v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13186
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#26500;&#36896;&#24207;&#21015;&#25968;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20844;&#24335;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;50&#24180;&#21069;&#65292;&#26031;&#22374;&#21033;&#32771;&#34385;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#12290;&#23545;&#20110;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25105;&#20204;&#31216;&#36825;&#26679;&#30340;&#32447;&#24615;&#25193;&#23637;&#20026;&#22270;&#24418;&#30340;&#8220;&#26500;&#36896;&#24207;&#21015;&#8221;&#65292;&#22240;&#20026;&#27599;&#20010;&#36793;&#37117;&#36981;&#24490;&#20854;&#20004;&#20010;&#31471;&#28857;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#27492;&#31867;&#24207;&#21015;&#25968;&#37327;&#12290;&#23545;&#20110;&#36335;&#24452;&#65292;&#25105;&#20204;&#35748;&#21516;&#26031;&#22374;&#21033;&#30340;&#24819;&#27861;&#65288;&#20999;&#32447;&#25968;&#65289;&#65292;&#24182;&#24471;&#21040;&#20102;&#20854;&#20182;&#31867;&#22411;&#30340;&#20844;&#24335;&#12290;&#27492;&#22806;&#36824;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counting the number of linear extensions of a partial order was considered by Stanley about 50 years ago. For the partial order on the vertices and edges of a graph determined by inclusion, we call such linear extensions {\it construction sequences} for the graph as each edge follows both of its endpoints. The number of such sequences for paths, cycles, stars, double-stars, and complete graphs is found. For paths, we agree with Stanley (the Tangent numbers) and get formulas for the other classes. Structure and applications are also studied.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#21512;&#29702;&#29289;&#21697;&#38598;&#21512;&#30340;&#22522;&#26412;&#36923;&#36753;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#31616;&#21333;&#34920;&#36848;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08176</link><description>&lt;p&gt;
&#21512;&#29702;&#29289;&#21697;&#38598;&#21512;&#32972;&#21518;&#30340;&#36923;&#36753;&#21450;&#20854;&#36807;&#28388;&#22120;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
The logic behind desirable sets of things, and its filter representation. (arXiv:2302.08176v2 [math.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#21512;&#29702;&#29289;&#21697;&#38598;&#21512;&#30340;&#22522;&#26412;&#36923;&#36753;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#31616;&#21333;&#34920;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30830;&#23450;&#20102;&#26368;&#36817;&#19968;&#32452;&#21512;&#29702;&#19988;&#20196;&#20154;&#21521;&#24448;&#30340;&#29289;&#21697;&#38598;&#21512;&#29702;&#35770;&#30340;&#36923;&#36753;&#65292;&#36825;&#20010;&#29702;&#35770;&#25512;&#24191;&#20102;&#20196;&#20154;&#21521;&#24448;&#30340;&#36172;&#27880;&#21644;&#19968;&#33268;&#30340;&#36873;&#25321;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#35782;&#21035;&#20801;&#35768;&#25105;&#20204;&#24314;&#31435;&#21508;&#31181;&#34920;&#31034;&#32467;&#26524;&#65292;&#29992;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;&#36825;&#31181;&#19968;&#33268;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We identify the logic behind the recent theory of coherent sets of desirable (sets of) things, which generalise desirable (sets of) gambles and coherent choice functions, and show that this identification allows us to establish various representation results for such coherent models in terms of simpler ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12901</link><description>&lt;p&gt;
&#27169;&#25311;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#34701;&#20837;&#29616;&#26377;&#20132;&#36890;&#31995;&#32479;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Simulating the Integration of Urban Air Mobility into Existing Transportation Systems: A Survey. (arXiv:2301.12901v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys the current state of research on urban air mobility (UAM) in metropolitan-scale traffic using simulation techniques, identifying key challenges and opportunities for integrating UAM into urban transportation systems, including impacts on existing traffic patterns and congestion, safety analysis and risk assessment, potential economic and environmental benefits, and the development of shared infrastructure and routes for UAM and ground-based transportation. The potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas, are also discussed.
&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#22823;&#37117;&#24066;&#22320;&#21306;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#32531;&#35299;&#25317;&#22581;&#65292;&#25552;&#39640;&#21487;&#36798;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;UAM&#34701;&#20837;&#29616;&#26377;&#20132;&#36890;&#31995;&#32479;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#20854;&#23545;&#20132;&#36890;&#27969;&#37327;&#21644;&#23481;&#37327;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#20351;&#29992;&#27169;&#25311;&#25216;&#26415;&#35843;&#26597;&#20102;UAM&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Urban air mobility (UAM) has the potential to revolutionize transportation in metropolitan areas, providing a new mode of transportation that could alleviate congestion and improve accessibility. However, the integration of UAM into existing transportation systems is a complex task that requires a thorough understanding of its impact on traffic flow and capacity. In this paper, we conduct a survey to investigate the current state of research on UAM in metropolitan-scale traffic using simulation techniques. We identify key challenges and opportunities for the integration of UAM into urban transportation systems, including impacts on existing traffic patterns and congestion; safety analysis and risk assessment; potential economic and environmental benefits; and the development of shared infrastructure and routes for UAM and ground-based transportation. We also discuss the potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas. Our survey 
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.14041</link><description>&lt;p&gt;
RFold&#65306;&#22522;&#20110;&#35299;&#32806;&#20248;&#21270;&#26041;&#27861;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14041
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#31958;&#26680;&#37240;&#65288;RNA&#65289;&#30340;&#20108;&#32423;&#32467;&#26500;&#27604;&#19977;&#32423;&#32467;&#26500;&#26356;&#31283;&#23450;&#21644;&#26356;&#26131;&#20110;&#22312;&#32454;&#32990;&#20013;&#35775;&#38382;&#65292;&#22240;&#27492;&#23545;&#20110;&#21151;&#33021;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#24615;&#24046;&#21644;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;RFold&#12290;RFold&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#23558;&#20256;&#32479;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#34892;&#21644;&#36880;&#21015;&#20248;&#21270;&#65292;&#31616;&#21270;&#20102;&#27714;&#35299;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;RFold&#37319;&#29992;&#27880;&#24847;&#21147;&#22320;&#22270;&#20316;&#20026;&#20449;&#24687;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#35774;&#35745;&#25163;&#24037;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RFold&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#32422;8&#20493;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#21644;Colab&#28436;&#31034;&#21487;&#22312;\href{this http URL}{this http UR}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;700,000&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#19988;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2206.02909</link><description>&lt;p&gt;
&#20351;&#29992;70&#19975;&#20154;&#26085;&#30340;&#21487;&#31359;&#25140;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data. (arXiv:2206.02909v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;700,000&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#19988;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#38754;&#30340;&#36827;&#23637;&#30456;&#23545;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;UK-Biobank&#27963;&#21160;&#36861;&#36394;&#22120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;70&#19975;&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#22312;&#19971;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;F1&#30456;&#23545;&#25913;&#21892;2.5%-100%&#65288;&#20013;&#20301;&#25968;18.4%&#65289;&#65292;&#25913;&#36827;&#26368;&#22823;&#30340;&#26159;&#35268;&#27169;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#27867;&#21270;&#21040;&#22806;&#37096;&#25968;&#25454;&#38598;&#12289;&#35774;&#22791;&#21644;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#24320;&#28304;&#27169;&#22411;&#23558;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#26500;&#24314;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in deep learning for human activity recognition have been relatively limited due to the lack of large labelled datasets. In this study, we leverage self-supervised learning techniques on the UK-Biobank activity tracker dataset--the largest of its kind to date--containing more than 700,000 person-days of unlabelled wearable sensor data. Our resulting activity recognition model consistently outperformed strong baselines across seven benchmark datasets, with an F1 relative improvement of 2.5%-100% (median 18.4%), the largest improvements occurring in the smaller datasets. In contrast to previous studies, our results generalise across external datasets, devices, and environments. Our open-source model will help researchers and developers to build customisable and generalisable activity classifiers with high performance.
&lt;/p&gt;</description></item></channel></rss>