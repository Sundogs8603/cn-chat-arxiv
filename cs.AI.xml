<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13649</link><description>&lt;p&gt;
GKD&#65306;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#33976;&#39311;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#35757;&#32451;&#26399;&#38388;&#36755;&#20986;&#24207;&#21015;&#21644;&#37096;&#32626;&#26102;&#30001;&#23398;&#29983;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20043;&#38388;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#65288;2&#65289;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#33021;&#19981;&#22815;&#34920;&#36798;&#32769;&#24072;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#12290;GKD&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;GKD&#36890;&#36807;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26469;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36825;&#20123;&#31163;&#25955;&#24230;&#38598;&#20013;&#20110;&#29983;&#25104;&#21487;&#33021;&#31526;&#21512;&#32769;&#24072;&#20998;&#24067;&#30340;&#23398;&#29983;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;GKD&#20248;&#20110;&#24120;&#29992;&#30340;LLM&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#22270;&#65288;OSG&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#27010;&#25324;&#20986;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13630</link><description>&lt;p&gt;
&#31163;&#32447;&#25216;&#33021;&#22270;&#65288;OSG&#65289;&#65306;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25216;&#33021;&#36827;&#34892;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Skill Graph (OSG): A Framework for Learning and Planning using Offline Reinforcement Learning Skills. (arXiv:2306.13630v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#22270;&#65288;OSG&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#27010;&#25324;&#20986;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22240;&#20854;&#22312;&#31454;&#25216;&#28216;&#25103;&#20013;&#30340;&#25104;&#21151;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26085;&#24120;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#21463;&#21040;&#38480;&#21046;&#65288;&#20363;&#22914;&#24037;&#19994;&#12289;&#23478;&#24237;&#12289;&#21307;&#30103;&#31561;&#65289;&#12290;&#26412;&#25991;&#36890;&#36807;&#20171;&#32461;&#19968;&#20010;&#35268;&#21010;&#31163;&#32447;&#25216;&#33021;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65292;&#20849;&#21516;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#27010;&#25324;&#20986;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26426;&#26800;&#33218;&#36827;&#34892;&#27979;&#35797;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning has received wide interest due to its success in competitive games. Yet, its adoption in everyday applications is limited (e.g. industrial, home, healthcare, etc.). In this paper, we address this limitation by presenting a framework for planning over offline skills and solving complex tasks in real-world environments. Our framework is comprised of three modules that together enable the agent to learn from previously collected data and generalize over it to solve long-horizon tasks. We demonstrate our approach by testing it on a robotic arm that is required to solve complex tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13588</link><description>&lt;p&gt;
&#31995;&#32479;&#32423;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
System-Level Natural Language Feedback. (arXiv:2306.13588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#29992;&#25143;&#20307;&#39564;&#20449;&#24687;&#12290;&#29616;&#26377;&#30740;&#31350;&#32858;&#28966;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#21453;&#39304;&#29992;&#20110;&#32454;&#21270;&#29305;&#23450;&#20363;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#31995;&#32479;&#33539;&#22260;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#26159;&#36890;&#36807;&#20197;&#19979;&#20004;&#26041;&#38754;&#23454;&#29616;&#30340;&#65306;(i) &#20219;&#21153;&#24230;&#37327;&#35774;&#35745;; (ii) &#29992;&#20110;&#25913;&#36827;&#27169;&#22411;&#21709;&#24212;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#26597;&#35810;&#29983;&#25104;&#21644;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#32452;&#21512;&#24102;&#26469;&#20102;&#36827;&#19968;&#27493;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#23548;&#33268;&#27604;GPT-3.5&#25776;&#20889;&#30340;&#21453;&#39304;&#26356;&#21152;&#25166;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language (NL) feedback contains rich information about the user experience. Existing studies focus on an instance-level approach, where feedback is used to refine specific examples, disregarding its system-wide application. This paper proposes a general framework for unlocking the system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process -- in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query generation and dialog response generation, demonstrating the effectiveness of the use of system-level feedback. We show the combination of system-level feedback and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#21046;&#36896;&#23545;&#24694;&#24847;&#36719;&#20214;&#26377;&#25928;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#21151;&#33021;&#20445;&#25252;&#20462;&#25913;&#65292;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;53.84%&#30340;&#25104;&#21151;&#36867;&#36991;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13587</link><description>&lt;p&gt;
&#21046;&#36896;&#23545;&#24694;&#24847;&#36719;&#20214;&#26377;&#25928;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Creating Valid Adversarial Examples of Malware. (arXiv:2306.13587v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#21046;&#36896;&#23545;&#24694;&#24847;&#36719;&#20214;&#26377;&#25928;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#21151;&#33021;&#20445;&#25252;&#20462;&#25913;&#65292;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;53.84%&#30340;&#25104;&#21151;&#36867;&#36991;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#25104;&#20026;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26432;&#27602;&#36719;&#20214;&#24320;&#21457;&#32773;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32435;&#20837;&#20854;&#20135;&#21697;&#20013;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#25913;&#36827;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20063;&#24102;&#26377;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#21155;&#21183;&#12290;&#25105;&#20204;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#24694;&#24847;&#20195;&#30721;&#31034;&#20363;&#29983;&#25104;&#22120;&#65292;&#20854;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#19968;&#32452;&#21151;&#33021;&#20445;&#25252;&#20462;&#25913;&#26469;&#21019;&#24314;&#26377;&#25928;&#30340;&#23545;&#25239;&#31034;&#20363;&#12290;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#38024;&#23545;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#27169;&#22411;&#23454;&#29616;&#20102;53.84&#65285;&#30340;&#36867;&#36991;&#29575;&#12290;&#20043;&#21069;&#38024;&#23545;GBDT&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#30340;PPO&#20195;&#29702;&#24471;&#20998;&#20026;11.41&#65285;&#30340;&#36867;&#36991;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is becoming increasingly popular as a go-to approach for many tasks due to its world-class results. As a result, antivirus developers are incorporating machine learning models into their products. While these models improve malware detection capabilities, they also carry the disadvantage of being susceptible to adversarial attacks. Although this vulnerability has been demonstrated for many models in white-box settings, a black-box attack is more applicable in practice for the domain of malware detection. We present a generator of adversarial malware examples using reinforcement learning algorithms. The reinforcement learning agents utilize a set of functionality-preserving modifications, thus creating valid adversarial examples. Using the proximal policy optimization (PPO) algorithm, we achieved an evasion rate of 53.84% against the gradient-boosted decision tree (GBDT) model. The PPO agent previously trained against the GBDT classifier scored an evasion rate of 11.41%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#8220;&#24314;&#31569;&#8221;&#36825;&#20010;&#35789;&#20174;&#20854;&#24076;&#33098;&#21382;&#21490;&#21644;&#24212;&#29992;&#20110;&#24314;&#31569;&#21644;&#35745;&#31639;&#26426;&#30340;&#21547;&#20041;&#21457;&#23637;&#21040;&#22914;&#20170;&#30340;&#24605;&#32500;&#26041;&#38754;&#30340;&#36335;&#24452;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#36234;&#25152;&#26377;&#19977;&#20010;&#38454;&#27573;&#30340;&#8220;&#24314;&#31569;&#8221;&#23450;&#20041;&#20197;&#21450;&#37325;&#26032;&#24605;&#32771;&#19982;&#35748;&#30693;&#20307;&#31995;&#32467;&#26500;&#26377;&#20851;&#30340;&#19977;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13572</link><description>&lt;p&gt;
&#23545;&#20110;&#8220;&#24314;&#31569;&#8221;&#30340;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Thoughts on Architecture. (arXiv:2306.13572v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#8220;&#24314;&#31569;&#8221;&#36825;&#20010;&#35789;&#20174;&#20854;&#24076;&#33098;&#21382;&#21490;&#21644;&#24212;&#29992;&#20110;&#24314;&#31569;&#21644;&#35745;&#31639;&#26426;&#30340;&#21547;&#20041;&#21457;&#23637;&#21040;&#22914;&#20170;&#30340;&#24605;&#32500;&#26041;&#38754;&#30340;&#36335;&#24452;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#36234;&#25152;&#26377;&#19977;&#20010;&#38454;&#27573;&#30340;&#8220;&#24314;&#31569;&#8221;&#23450;&#20041;&#20197;&#21450;&#37325;&#26032;&#24605;&#32771;&#19982;&#35748;&#30693;&#20307;&#31995;&#32467;&#26500;&#26377;&#20851;&#30340;&#19977;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#24314;&#31569;&#8221;&#36825;&#20010;&#35789;&#20174;&#20854;&#24076;&#33098;&#21382;&#21490;&#21644;&#24212;&#29992;&#20110;&#24314;&#31569;&#21644;&#35745;&#31639;&#26426;&#30340;&#21547;&#20041;&#21457;&#23637;&#21040;&#22914;&#20170;&#30340;&#24605;&#32500;&#26041;&#38754;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36825;&#19968;&#21382;&#21490;&#30340;&#25945;&#35757;&#65292;&#25552;&#20986;&#20102;&#22312;&#27599;&#20010;&#38454;&#27573;&#24341;&#20837;&#30340;&#19968;&#32452;&#30456;&#20851;&#21306;&#20998;&#21644;&#36328;&#36234;&#25152;&#26377;&#19977;&#20010;&#38454;&#27573;&#30340;&#8220;&#24314;&#31569;&#8221;&#23450;&#20041;&#65292;&#20197;&#21450;&#37325;&#26032;&#24605;&#32771;&#19982;&#35748;&#30693;&#20307;&#31995;&#32467;&#26500;&#26377;&#20851;&#30340;&#19977;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term architecture has evolved considerably from its original Greek roots and its application to buildings and computers to its more recent manifestation for minds. This article considers lessons from this history, in terms of a set of relevant distinctions introduced at each of these stages and a definition of architecture that spans all three, and a reconsideration of three key issues from cognitive architectures for architectures in general and cognitive architectures more particularly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#20027;&#21160;&#25512;&#29702;&#27169;&#22411;&#65292;&#22312;&#22810;&#25151;&#38388;&#36855;&#23467;&#29615;&#22659;&#20013;&#25512;&#26029;&#20986;&#19990;&#30028;&#32467;&#26500;&#65292;&#26377;&#25928;&#23454;&#29616;&#25506;&#32034;&#21644;&#23548;&#33322;&#65292;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13546</link><description>&lt;p&gt;
&#25512;&#26029;&#22810;&#25151;&#38388;&#36855;&#23467;&#29615;&#22659;&#20013;&#30340;&#20998;&#23618;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Inferring Hierarchical Structure in Multi-Room Maze Environments. (arXiv:2306.13546v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#20027;&#21160;&#25512;&#29702;&#27169;&#22411;&#65292;&#22312;&#22810;&#25151;&#38388;&#36855;&#23467;&#29615;&#22659;&#20013;&#25512;&#26029;&#20986;&#19990;&#30028;&#32467;&#26500;&#65292;&#26377;&#25928;&#23454;&#29616;&#25506;&#32034;&#21644;&#23548;&#33322;&#65292;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#22320;&#22270;&#36890;&#36807;&#34920;&#31034;&#29615;&#22659;&#20013;&#30340;&#31354;&#38388;&#21644;&#27010;&#24565;&#20851;&#31995;&#22312;&#20419;&#36827;&#28789;&#27963;&#34892;&#20026;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23398;&#20064;&#21644;&#25512;&#26029;&#29615;&#22659;&#30340;&#22522;&#30784;&#32467;&#26500;&#23545;&#20110;&#26377;&#25928;&#30340;&#25506;&#32034;&#21644;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#20027;&#21160;&#25512;&#29702;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#22522;&#20110;&#20687;&#32032;&#30340;&#35266;&#23519;&#20013;&#25512;&#26029;&#19990;&#30028;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#20998;&#23618;&#27169;&#22411;&#65292;&#21253;&#25324;&#35748;&#30693;&#22320;&#22270;&#12289;&#23458;&#35266;&#20013;&#24515;&#12289;&#33258;&#25105;&#20013;&#24515;&#19990;&#30028;&#27169;&#22411;&#65292;&#22312;&#19978;&#19979;&#25991;&#12289;&#22320;&#28857;&#21040;&#36816;&#21160;&#30340;&#19981;&#21516;&#25512;&#29702;&#23618;&#38754;&#32467;&#21512;&#20102;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#25506;&#32034;&#21644;&#30446;&#30340;&#39537;&#21160;&#30340;&#34892;&#20026;&#12290;&#36825;&#20801;&#35768;&#22312;&#25151;&#38388;&#32467;&#26500;&#21270;&#30340;&#23567;&#32593;&#26684;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#21644;&#30446;&#26631;&#23548;&#21521;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive maps play a crucial role in facilitating flexible behaviour by representing spatial and conceptual relationships within an environment. The ability to learn and infer the underlying structure of the environment is crucial for effective exploration and navigation. This paper introduces a hierarchical active inference model addressing the challenge of inferring structure in the world from pixel-based observations. We propose a three-layer hierarchical model consisting of a cognitive map, an allocentric, and an egocentric world model, combining curiosity-driven exploration with goal-oriented behaviour at the different levels of reasoning from context to place to motion. This allows for efficient exploration and goal-directed search in room-structured mini-grid environments.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TorGNN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#36816;&#29992;&#35299;&#26512;&#25197;&#26354;&#24230;&#37327;&#21270;&#22270;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#24182;&#22312;16&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.13541</link><description>&lt;p&gt;
&#25197;&#26354;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Torsion Graph Neural Networks. (arXiv:2306.13541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13541
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TorGNN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#36816;&#29992;&#35299;&#26512;&#25197;&#26354;&#24230;&#37327;&#21270;&#22270;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#24182;&#22312;16&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20102;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#20998;&#26512;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#23427;&#20204;&#34987;&#24320;&#21457;&#20986;&#26469;&#23558;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#20449;&#24687;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#12290;&#21463;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#31163;&#25955;&#40654;&#26364;&#26354;&#29575;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TorGNN&#65292;&#19968;&#31181;&#22686;&#24378;&#20102;&#35299;&#26512;&#25197;&#26354;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20854;&#22522;&#26412;&#24605;&#24819;&#26159;&#29992;&#22522;&#20110;&#35299;&#26512;&#25197;&#26354;&#24230;&#30340;&#26435;&#37325;&#20844;&#24335;&#26469;&#34920;&#24449;&#22270;&#30340;&#23616;&#37096;&#32467;&#26500;&#12290;&#25968;&#23398;&#19978;&#65292;&#35299;&#26512;&#25197;&#26354;&#24230;&#26159;&#19968;&#31181;&#25299;&#25169;&#19981;&#21464;&#37327;&#65292;&#21487;&#20197;&#21306;&#20998;&#21516;&#20262;&#20294;&#19981;&#21516;&#32986;&#30340;&#31354;&#38388;&#12290;&#22312;&#25105;&#20204;&#30340;TorGNN&#20013;&#65292;&#23545;&#20110;&#27599;&#20010;&#36793;&#65292;&#25105;&#20204;&#37117;&#21487;&#20197;&#25214;&#21040;&#30456;&#24212;&#30340;&#23616;&#37096;&#21333;&#32431;&#22797;&#21512;&#20307;&#65292;&#28982;&#21518;&#35745;&#31639;&#20854;&#35299;&#26512;&#25197;&#26354;&#24230;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#20854;&#29992;&#20316;&#20449;&#24687;&#20256;&#36882;&#36807;&#31243;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;TorGNN&#27169;&#22411;&#24050;&#22312;&#26469;&#33258;16&#31181;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning (GDL) models have demonstrated a great potential for the analysis of non-Euclidian data. They are developed to incorporate the geometric and topological information of non-Euclidian data into the end-to-end deep learning architectures. Motivated by the recent success of discrete Ricci curvature in graph neural network (GNNs), we propose TorGNN, an analytic Torsion enhanced Graph Neural Network model. The essential idea is to characterize graph local structures with an analytic torsion based weight formula. Mathematically, analytic torsion is a topological invariant that can distinguish spaces which are homotopy equivalent but not homeomorphic. In our TorGNN, for each edge, a corresponding local simplicial complex is identified, then the analytic torsion (for this local simplicial complex) is calculated, and further used as a weight (for this edge) in message-passing process. Our TorGNN model is validated on link prediction tasks from sixteen different types of n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#21040;&#26426;&#26800;&#33218;&#30340;&#20849;&#20139;&#25511;&#21046;&#33539;&#24335;&#20013;&#65292;&#20197;&#24110;&#21161;&#36816;&#21160;&#21463;&#25439;&#20154;&#22763;&#23454;&#29616;&#26356;&#39640;&#31243;&#24230;&#30340;&#20010;&#20154;&#33258;&#27835;&#12290;</title><link>http://arxiv.org/abs/2306.13509</link><description>&lt;p&gt;
&#25506;&#32034;AI&#22686;&#24378;&#30340;&#21327;&#20316;&#25511;&#21046;&#23545;&#20110;&#36741;&#21161;&#26426;&#26800;&#33218;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring AI-enhanced Shared Control for an Assistive Robotic Arm. (arXiv:2306.13509v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#21040;&#26426;&#26800;&#33218;&#30340;&#20849;&#20139;&#25511;&#21046;&#33539;&#24335;&#20013;&#65292;&#20197;&#24110;&#21161;&#36816;&#21160;&#21463;&#25439;&#20154;&#22763;&#23454;&#29616;&#26356;&#39640;&#31243;&#24230;&#30340;&#20010;&#20154;&#33258;&#27835;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#36741;&#21161;&#26426;&#26800;&#33218;&#24050;&#32463;&#25104;&#20026;&#24110;&#21161;&#36816;&#21160;&#21463;&#25439;&#20154;&#22763;&#23454;&#29616;&#33258;&#20027;&#29983;&#27963;&#30340;&#21487;&#33021;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#36825;&#26679;&#30340;&#31995;&#32479;&#24050;&#32463;&#38754;&#21521;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#65292;&#20363;&#22914;Kinova Jaco&#26426;&#26800;&#33218;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22823;&#22810;&#38656;&#35201;&#22797;&#26434;&#30340;&#25163;&#21160;&#25511;&#21046;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#29992;&#25143;&#19981;&#22570;&#37325;&#36127;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#35753;&#36825;&#20123;&#26426;&#22120;&#20154;&#33258;&#20027;&#34892;&#21160;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#33267;&#23569;&#23545;&#20110;&#36825;&#20010;&#29305;&#23450;&#30340;&#29992;&#25143;&#32676;&#20307;&#26469;&#35828;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24466;&#21171;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#29992;&#25143;&#24076;&#26395;&#20445;&#25345;&#25511;&#21046;&#26435;&#20197;&#23454;&#29616;&#26356;&#39640;&#31243;&#24230;&#30340;&#20010;&#20154;&#33258;&#27835;&#65292;&#20294;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#19982;&#27492;&#30456;&#21453;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#38598;&#25104;&#21040;&#20849;&#20139;&#25511;&#21046;&#33539;&#24335;&#20013;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30028;&#38754;&#30340;&#24517;&#35201;&#35201;&#27714;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#26174;&#33879;&#20943;&#23569;&#24515;&#29702;&#36127;&#25285;&#21644;&#25152;&#38656;&#30340;&#26426;&#21160;&#33021;&#21147;&#30340;&#21516;&#26102;&#20445;&#25345;&#20154;&#31867;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assistive technologies and in particular assistive robotic arms have the potential to enable people with motor impairments to live a self-determined life. More and more of these systems have become available for end users in recent years, such as the Kinova Jaco robotic arm. However, they mostly require complex manual control, which can overwhelm users. As a result, researchers have explored ways to let such robots act autonomously. However, at least for this specific group of users, such an approach has shown to be futile. Here, users want to stay in control to achieve a higher level of personal autonomy, to which an autonomous robot runs counter. In our research, we explore how Artifical Intelligence (AI) can be integrated into a shared control paradigm. In particular, we focus on the consequential requirements for the interface between human and robot and how we can keep humans in the loop while still significantly reducing the mental load and required motor skills.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#29992;&#20110;&#27169;&#25311;&#30005;&#36335;&#39564;&#35777;&#65292;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#20223;&#30495;&#27425;&#25968;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#30005;&#36335;&#21709;&#24212;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#29616;&#26368;&#22351;&#24773;&#20917;&#21644;&#25925;&#38556;&#12290;</title><link>http://arxiv.org/abs/2306.13484</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#30005;&#36335;&#39564;&#35777;&#30340;&#33258;&#36866;&#24212;&#35268;&#21010;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Planning Search Algorithm for Analog Circuit Verification. (arXiv:2306.13484v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13484
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#29992;&#20110;&#27169;&#25311;&#30005;&#36335;&#39564;&#35777;&#65292;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#20223;&#30495;&#27425;&#25968;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#30005;&#36335;&#21709;&#24212;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#29616;&#26368;&#22351;&#24773;&#20917;&#21644;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#30005;&#36335;&#39564;&#35777;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#36825;&#20123;&#30005;&#36335;&#27599;&#24180;&#37117;&#22312;&#19981;&#26029;&#22686;&#38271;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;Si&#21069;&#39564;&#35777;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#27491;&#24120;&#21151;&#33021;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20943;&#23569;&#25163;&#21160;&#39564;&#35777;IC&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36739;&#23569;&#30340;&#20223;&#30495;&#27425;&#25968;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#25805;&#20316;&#26465;&#20214;&#37197;&#32622;&#65288;OCC&#65289;&#65292;&#20197;&#35757;&#32451;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26367;&#20195;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20986;&#36827;&#19968;&#27493;&#26356;&#22256;&#38590;&#30340;OCC&#12290;&#23545;&#20110;&#20960;&#20010;&#36845;&#20195;&#37325;&#22797;&#36825;&#20010;&#36807;&#31243;&#24050;&#32463;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30005;&#36335;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;GP&#20272;&#35745;&#30005;&#36335;&#21709;&#24212;&#65292;&#25552;&#39640;&#20102;&#25214;&#21040;&#26576;&#20123;&#30005;&#36335;&#21709;&#24212;&#30340;&#26368;&#22351;&#24773;&#20917;&#29978;&#33267;&#20351;&#20043;&#22833;&#36133;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20026;&#25152;&#26377;&#30005;&#36335;&#25552;&#20379;&#26356;&#25509;&#36817;&#35268;&#26684;&#30340;OCC&#65292;&#24182;&#30830;&#23450;&#19968;&#31181;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrated circuit verification has gathered considerable interest in recent times. Since these circuits keep growing in complexity year by year, pre-Silicon (pre-SI) verification becomes ever more important, in order to ensure proper functionality. Thus, in order to reduce the time needed for manually verifying ICs, we propose a machine learning (ML) approach, which uses less simulations. This method relies on an initial evaluation set of operating condition configurations (OCCs), in order to train Gaussian process (GP) surrogate models. By using surrogate models, we can propose further, more difficult OCCs. Repeating this procedure for several iterations has shown better GP estimation of the circuit's responses, on both synthetic and real circuits, resulting in a better chance of finding the worst case, or even failures, for certain circuit responses. Thus, we show that the proposed approach is able to provide OCCs closer to the specifications for all circuits and identify a failure 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;LeakDistill&#65292;&#23427;&#20351;&#29992;&#32467;&#26500;&#36866;&#37197;&#22120;&#23558;&#22270;&#24418;&#20449;&#24687;&#26126;&#30830;&#24182;&#20837;&#21040;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;AMR&#35299;&#26512;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#21333;&#35789;&#21040;&#33410;&#28857;&#23545;&#40784;&#23558;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#23884;&#20837;&#32534;&#30721;&#22120;&#20013;&#65292;&#21363;&#20351;&#19981;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;AMR&#35299;&#26512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13467</link><description>&lt;p&gt;
&#23558;&#22270;&#34920;&#20449;&#24687;&#34701;&#20837;&#22522;&#20110;Transformer&#30340;AMR&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Incorporating Graph Information in Transformer-based AMR Parsing. (arXiv:2306.13467v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;LeakDistill&#65292;&#23427;&#20351;&#29992;&#32467;&#26500;&#36866;&#37197;&#22120;&#23558;&#22270;&#24418;&#20449;&#24687;&#26126;&#30830;&#24182;&#20837;&#21040;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;AMR&#35299;&#26512;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#21333;&#35789;&#21040;&#33410;&#28857;&#23545;&#40784;&#23558;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#23884;&#20837;&#32534;&#30721;&#22120;&#20013;&#65292;&#21363;&#20351;&#19981;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;AMR&#35299;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#24847;&#20041;&#34920;&#36798;&#65288;AMR&#65289;&#26159;&#19968;&#31181;&#35821;&#20041;&#35299;&#26512;&#24418;&#24335;&#20027;&#20041;&#65292;&#26088;&#22312;&#25552;&#20379;&#34920;&#31034;&#32473;&#23450;&#25991;&#26412;&#30340;&#35821;&#20041;&#22270;&#34920;&#25277;&#35937;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;BART&#25110;T5&#65292;&#36890;&#36807;Teacher Forcing&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20174;&#21477;&#23376;&#24471;&#21040;&#32447;&#24615;&#21270;&#29256;&#26412;&#30340;AMR&#22270;&#34920;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LeakDistill&#65292;&#19968;&#31181;&#25506;&#32034;&#36716;&#25442;&#22120;&#26550;&#26500;&#20462;&#25913;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#20351;&#29992;&#32467;&#26500;&#36866;&#37197;&#22120;&#26126;&#30830;&#23558;&#22270;&#24418;&#20449;&#24687;&#24182;&#20837;&#21040;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#65292;&#20197;&#25552;&#39640;AMR&#35299;&#26512;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#21333;&#35789;&#21040;&#33410;&#28857;&#23545;&#40784;&#23558;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#23884;&#20837;&#32534;&#30721;&#22120;&#20013;&#65292;&#21363;&#20351;&#19981;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;AMR&#35299;&#26512;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;\url{this http URL}&#19978;&#21457;&#24067;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at \url{this http URL}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ResNet-18&#20998;&#31867;&#22120;&#29983;&#25104;&#31867;&#28608;&#27963;&#22270;&#36827;&#34892;&#26893;&#29289;&#21494;&#29255;&#30149;&#25439;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;0.45&#30340;&#30149;&#25439;&#20301;&#32622;&#25104;&#21151;&#29575;&#65292;&#28040;&#38500;&#20102;&#30149;&#25439;&#27880;&#37322;&#36807;&#31243;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.13366</link><description>&lt;p&gt;
&#21033;&#29992;&#31867;&#28608;&#27963;&#22270;&#36827;&#34892;&#26893;&#29289;&#21494;&#29255;&#30149;&#25439;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Lesion Detection on Leaves using Class Activation Maps. (arXiv:2306.13366v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ResNet-18&#20998;&#31867;&#22120;&#29983;&#25104;&#31867;&#28608;&#27963;&#22270;&#36827;&#34892;&#26893;&#29289;&#21494;&#29255;&#30149;&#25439;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;0.45&#30340;&#30149;&#25439;&#20301;&#32622;&#25104;&#21151;&#29575;&#65292;&#28040;&#38500;&#20102;&#30149;&#25439;&#27880;&#37322;&#36807;&#31243;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26893;&#29289;&#21494;&#29255;&#30149;&#25439;&#26816;&#27979;&#26159;&#26893;&#29289;&#30149;&#29702;&#23398;&#21644;&#20892;&#19994;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#35782;&#21035;&#30149;&#25439;&#26377;&#21161;&#20110;&#35780;&#20272;&#26893;&#29289;&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#24182;&#21046;&#23450;&#20851;&#20110;&#30142;&#30149;&#25511;&#21046;&#25514;&#26045;&#21644;&#27835;&#30103;&#31574;&#30053;&#30340;&#26126;&#26234;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ResNet-18&#20998;&#31867;&#22120;&#29983;&#25104;&#31867;&#28608;&#27963;&#22270;&#36827;&#34892;&#26893;&#29289;&#21494;&#29255;&#30149;&#25439;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#22312;&#27979;&#35797;&#38598;&#20013;&#65292;&#25105;&#20204;&#25104;&#21151;&#39044;&#27979;&#20102;0.45&#30340;&#30149;&#25439;&#20301;&#32622;&#25104;&#21151;&#29575;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;ResNet&#20998;&#31867;&#22120;&#29983;&#25104;&#30340;CAMs&#65292;&#28040;&#38500;&#20102;&#30149;&#25439;&#27880;&#37322;&#36807;&#31243;&#30340;&#38656;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26893;&#29289;&#21494;&#29255;&#30149;&#25439;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lesion detection on plant leaves is a critical task in plant pathology and agricultural research. Identifying lesions enables assessing the severity of plant diseases and making informed decisions regarding disease control measures and treatment strategies. To detect lesions, there are studies that propose well-known object detectors. However, training object detectors to detect small objects such as lesions can be problematic. In this study, we propose a method for lesion detection on plant leaves utilizing class activation maps generated by a ResNet-18 classifier. In the test set, we achieved a 0.45 success rate in predicting the locations of lesions in leaves. Our study presents a novel approach for lesion detection on plant leaves by utilizing CAMs generated by a ResNet classifier while eliminating the need for a lesion annotation process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#29076;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#22686;&#24378;&#20854;&#20934;&#30830;&#24615;&#21644;&#25511;&#21046;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13345</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30693;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;&#35745;&#31639;&#29076;&#28857;&#24182;&#25511;&#21046;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#20248;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
A physics-informed AI method for calculating melting points with uncertainty control and optimal sampling. (arXiv:2306.13345v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#29076;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#22686;&#24378;&#20854;&#20934;&#30830;&#24615;&#21644;&#25511;&#21046;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;NPT&#38598;&#21512;&#20013;&#30340;&#20849;&#23384;&#27169;&#25311;&#33258;&#21160;&#35745;&#31639;&#29076;&#28857;&#12290;&#32473;&#23450;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20915;&#23450;&#36827;&#34892;&#27169;&#25311;&#30340;&#21407;&#23376;&#25968;&#37327;&#21644;&#28201;&#24230;&#65292;&#24182;&#22522;&#20110;&#25910;&#38598;&#30340;&#25968;&#25454;&#39044;&#27979;&#29076;&#28857;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#26356;&#22810;&#30340;&#25968;&#25454;&#31995;&#32479;&#22320;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#22266;&#28082;&#20849;&#23384;&#28436;&#21270;&#30340;&#29289;&#29702;&#27169;&#22411;&#32435;&#20837;AI&#26041;&#27861;&#65292;&#22686;&#24378;&#20854;&#20934;&#30830;&#24615;&#65292;&#24182;&#23454;&#29616;&#26377;&#25928;&#38477;&#20302;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20248;&#20915;&#31574;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#22823;&#32422;20&#20010;&#25991;&#29486;&#20013;&#30340;&#29076;&#28857;&#35745;&#31639;&#36827;&#34892;&#27604;&#36739;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#32422;&#19977;&#20998;&#20043;&#19968;&#30340;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#26174;&#33879;&#20559;&#24046;&#65292;&#31361;&#26174;&#20986;&#26448;&#26009;&#24615;&#36136;&#35745;&#31639;&#38656;&#35201;&#20934;&#30830;&#21487;&#38752;&#30340;&#22522;&#20110;AI&#30340;&#31639;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an artificial intelligence (AI) method for automatically computing the melting point based on coexistence simulations in the NPT ensemble. Given the interatomic interaction model, the method makes decisions regarding the number of atoms and temperature at which to conduct simulations, and based on the collected data predicts the melting point along with the uncertainty, which can be systematically improved with more data. We demonstrate how incorporating physical models of the solid-liquid coexistence evolution enhances the AI method's accuracy and enables optimal decision-making to effectively reduce predictive uncertainty. To validate our approach, we compare our results with approximately 20 melting point calculations from the literature. Remarkably, we observe significant deviations in about one-third of the cases, underscoring the need for accurate and reliable AI-based algorithms for materials property calculations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#26368;&#23567;&#21270;&#21487;&#25511;&#22240;&#32032;&#30340;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#21464;&#39118;&#37327;&#24320;&#25918;&#24335;&#21150;&#20844;&#23460; HVAC &#31995;&#32479;&#30340;&#33021;&#28304;&#20248;&#21270;&#65292;&#19982;&#20256;&#32479;&#31995;&#32479;&#30456;&#27604;&#33021;&#28304;&#28040;&#32791;&#20943;&#23569;&#20102; 37%&#65292;&#19988;&#28201;&#24230;&#33539;&#22260;&#36829;&#35268;&#29575;&#26497;&#20302; (&lt;1%)&#12290;</title><link>http://arxiv.org/abs/2306.13333</link><description>&lt;p&gt;
&#22810;&#21464;&#39118;&#37327;&#24320;&#25918;&#24335;&#21150;&#20844;&#23460;&#20013; HVAC &#31995;&#32479;&#33021;&#28304;&#20248;&#21270;&#65306;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Energy Optimization for HVAC Systems in Multi-VAV Open Offices: A Deep Reinforcement Learning Approach. (arXiv:2306.13333v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#26368;&#23567;&#21270;&#21487;&#25511;&#22240;&#32032;&#30340;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#21464;&#39118;&#37327;&#24320;&#25918;&#24335;&#21150;&#20844;&#23460; HVAC &#31995;&#32479;&#30340;&#33021;&#28304;&#20248;&#21270;&#65292;&#19982;&#20256;&#32479;&#31995;&#32479;&#30456;&#27604;&#33021;&#28304;&#28040;&#32791;&#20943;&#23569;&#20102; 37%&#65292;&#19988;&#28201;&#24230;&#33539;&#22260;&#36829;&#35268;&#29575;&#26497;&#20302; (&lt;1%)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#36229;&#36807; 32% &#30340;&#33021;&#28304;&#29992;&#20110;&#21830;&#19994;&#21644;&#20303;&#23429;&#24314;&#31569;&#65292;&#36843;&#20999;&#38656;&#35201;&#37325;&#26032;&#23457;&#35270;&#20256;&#32479;&#30340;&#24314;&#31569;&#33021;&#28304;&#31649;&#29702;&#26041;&#27861;&#12290;&#30001;&#20110; HVAC &#31995;&#32479;&#21344;&#21830;&#19994;&#37096;&#38376;&#24635;&#33021;&#32791;&#30340;&#32422; 40%&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#22797;&#26434;&#24230; DRL &#27169;&#22411;&#30340;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#24320;&#25918;&#24335;&#21150;&#20844;&#23460; HVAC &#33021;&#28304;&#20248;&#21270;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#21487;&#25511;&#21644;&#21487;&#35775;&#38382;&#22240;&#32032;&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#24314;&#31569;&#20013;&#22522;&#20110;&#29616;&#26377; HVAC &#35745;&#21010;&#30340;&#22522;&#32447;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#25972;&#20307;&#33021;&#28304;&#28040;&#32791;&#21644;&#28909;&#33298;&#36866;&#24230;&#27700;&#24179;&#12290;&#35813;&#27604;&#36739;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24037;&#20316;&#26102;&#38388;&#20869;&#23454;&#29616;&#20102; 37% &#30340;&#33021;&#28304;&#28040;&#32791;&#33410;&#32422;&#65292;&#36829;&#35268;&#28201;&#24230;&#33539;&#22260;&#21344;&#27604;&#26368;&#20302; (&lt;1%)&#12290;&#35757;&#32451;&#19968;&#20010;&#24615;&#33021;&#20248;&#36234;&#30340;&#35206;&#30422; d
&lt;/p&gt;
&lt;p&gt;
With more than 32% of the global energy used by commercial and residential buildings, there is an urgent need to revisit traditional approaches to Building Energy Management (BEM). With HVAC systems accounting for about 40% of the total energy cost in the commercial sector, we propose a low-complexity DRL-based model with multi-input multi-output architecture for the HVAC energy optimization of open-plan offices, which uses only a handful of controllable and accessible factors. The efficacy of our solution is evaluated through extensive analysis of the overall energy consumption and thermal comfort levels compared to a baseline system based on the existing HVAC schedule in a real building. This comparison shows that our method achieves 37% savings in energy consumption with minimum violation (&lt;1%) of the desired temperature range during work hours. It takes only a total of 40 minutes for 5 epochs (about 7.75 minutes per epoch) to train a network with superior performance and covering d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#25216;&#26415;&#65288;&#21253;&#25324;LSTM&#12289;T5&#12289;Pegasus&#12289;BART&#21644;BART-Large&#27169;&#22411;&#65289;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#31616;&#21382;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#24494;&#35843;&#21518;&#30340;BART-Large&#27169;&#22411;&#25928;&#26524;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.13315</link><description>&lt;p&gt;
&#21033;&#29992;&#20808;&#36827;&#30340;NLP&#21464;&#24418;&#22120;&#21644;LSTM&#36827;&#34892;&#31616;&#21382;&#30340;&#25552;&#21462;&#24615;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Abstractive Text Summarization for Resumes With Cutting Edge NLP Transformers and LSTM. (arXiv:2306.13315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#25216;&#26415;&#65288;&#21253;&#25324;LSTM&#12289;T5&#12289;Pegasus&#12289;BART&#21644;BART-Large&#27169;&#22411;&#65289;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#31616;&#21382;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#24494;&#35843;&#21518;&#30340;BART-Large&#27169;&#22411;&#25928;&#26524;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#22823;&#37327;&#30340;&#25991;&#26412;&#20449;&#24687;&#21387;&#32553;&#25104;&#31616;&#27905;&#36830;&#36143;&#30340;&#25688;&#35201;&#12290;&#38543;&#30528;&#20869;&#23481;&#30340;&#25351;&#25968;&#22686;&#38271;&#21644;&#39640;&#25928;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#30340;&#38656;&#27714;&#65292;&#25991;&#26412;&#25688;&#35201;&#22312;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#35780;&#20272;&#20102;LSTM&#21644;&#39044;&#35757;&#32451;&#30340;T5&#12289;Pegasus&#12289;BART&#21644;BART-Large&#27169;&#22411;&#22312;&#24320;&#28304;&#25968;&#25454;&#38598;&#65288;Xsum&#12289;CNN/Daily Mail&#12289;&#20122;&#39532;&#36874;&#31934;&#32654;&#39135;&#21697;&#35780;&#35770;&#21644;&#26032;&#38395;&#25688;&#35201;&#65289;&#21644;&#20934;&#22791;&#30340;&#31616;&#21382;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#31616;&#21382;&#25968;&#25454;&#38598;&#21253;&#25324;&#35768;&#22810;&#20449;&#24687;&#65292;&#22914;&#35821;&#35328;&#12289;&#25945;&#32946;&#12289;&#32463;&#39564;&#12289;&#20010;&#20154;&#20449;&#24687;&#12289;&#25216;&#33021;&#31561;&#65292;&#25968;&#25454;&#38598;&#20013;&#21253;&#25324;&#20102;75&#20221;&#31616;&#21382;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#31616;&#21382;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#20351;&#29992;&#31616;&#21382;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;LSTM&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#12290;&#20351;&#29992;&#31616;&#21382;&#25968;&#25454;&#38598;&#24494;&#35843;&#30340;BART-Large&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization is a fundamental task in natural language processing that aims to condense large amounts of textual information into concise and coherent summaries. With the exponential growth of content and the need to extract key information efficiently, text summarization has gained significant attention in recent years. In this study, LSTM and pre-trained T5, Pegasus, BART and BART-Large model performances were evaluated on the open source dataset (Xsum, CNN/Daily Mail, Amazon Fine Food Review and News Summary) and the prepared resume dataset. This resume dataset consists of many information such as language, education, experience, personal information, skills, and this data includes 75 resumes. The primary objective of this research was to classify resume text. Various techniques such as LSTM, pre-trained models, and fine-tuned models were assessed using a dataset of resumes. The BART-Large model fine-tuned with the resume dataset gave the best performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#30456;&#20114;&#25351;&#23548;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#36827;&#34892;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#22495;&#23569;&#26679;&#26412;&#19977;&#20803;&#32452;&#25552;&#21462;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#26377;&#31454;&#20105;&#21147;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13310</link><description>&lt;p&gt;
&#30456;&#20114;&#25351;&#23548;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mutually Guided Few-shot Learning for Relational Triple Extraction. (arXiv:2306.13310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#30456;&#20114;&#25351;&#23548;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#36827;&#34892;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#22495;&#23569;&#26679;&#26412;&#19977;&#20803;&#32452;&#25552;&#21462;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#26377;&#31454;&#20105;&#21147;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#21253;&#21547;&#35768;&#22810;&#23454;&#20307;-&#20851;&#31995;-&#23454;&#20307;&#19977;&#20803;&#32452;&#65292;&#20026;&#19979;&#28216;&#24212;&#29992;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#19977;&#20803;&#32452;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#23454;&#20363;&#12290;&#24403;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#24615;&#33021;&#23558;&#24613;&#21095;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20114;&#25351;&#23548;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#36827;&#34892;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#65288;MG-FTE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#19968;&#20010;&#20197;&#23454;&#20307;&#20026;&#23548;&#21521;&#30340;&#20851;&#31995;&#21407;&#22411;&#35299;&#30721;&#22120;&#65292;&#39318;&#20808;&#23545;&#20851;&#31995;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#21450;&#19968;&#20010;&#20197;&#20851;&#31995;&#20026;&#23548;&#21521;&#30340;&#23454;&#20307;&#21407;&#22411;&#35299;&#30721;&#22120;&#65292;&#26681;&#25454;&#20998;&#31867;&#30340;&#20851;&#31995;&#25552;&#21462;&#23454;&#20307;&#12290;&#20026;&#20102;&#36830;&#32467;&#23454;&#20307;&#19982;&#20851;&#31995;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21407;&#22411;&#23618;&#34701;&#21512;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#23454;&#20307;&#25552;&#21462;&#21644;&#20851;&#31995;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#22495;&#23569;&#26679;&#26412;&#19977;&#20803;&#32452;&#25552;&#21462;&#20219;&#21153;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#19977;&#20803;&#32452;&#25552;&#21462;&#20219;&#21153;&#20013;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#20063;&#33021;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs), containing many entity-relation-entity triples, provide rich information for downstream applications. Although extracting triples from unstructured texts has been widely explored, most of them require a large number of labeled instances. The performance will drop dramatically when only few labeled data are available. To tackle this problem, we propose the Mutually Guided Few-shot learning framework for Relational Triple Extraction (MG-FTE). Specifically, our method consists of an entity-guided relation proto-decoder to classify the relations firstly and a relation-guided entity proto-decoder to extract entities based on the classified relations. To draw the connection between entity and relation, we design a proto-level fusion module to boost the performance of both entity extraction and relation classification. Moreover, a new cross-domain few-shot triple extraction task is introduced. Extensive experiments show that our method outperforms many state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;LLMs&#22312;&#35780;&#35770;&#20998;&#31867;&#26041;&#38754;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#20004;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#19978;&#19981;&#22826;&#19968;&#33268;&#65292;&#23545;&#20110;&#20998;&#31867;&#36798;&#25104;&#19968;&#33268;&#20165;&#32422;&#21344;&#20116;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2306.13298</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#25506;&#32034;&#36136;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Qualitative Research Using LLMs. (arXiv:2306.13298v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;LLMs&#22312;&#35780;&#35770;&#20998;&#31867;&#26041;&#38754;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#20004;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#19978;&#19981;&#22826;&#19968;&#33268;&#65292;&#23545;&#20110;&#20998;&#31867;&#36798;&#25104;&#19968;&#33268;&#20165;&#32422;&#21344;&#20116;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#39537;&#21160;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#26377;&#20851;&#23427;&#20204;&#22312;&#36136;&#24615;&#30740;&#31350;&#20013;&#20316;&#29992;&#30340;&#35752;&#35770;&#12290;&#19968;&#20123;&#20154;&#35748;&#20026;&#36825;&#20123;&#26159;&#20016;&#23500;&#20154;&#31867;&#29702;&#35299;&#30340;&#24037;&#20855;&#65292;&#32780;&#21478;&#19968;&#20123;&#20154;&#21017;&#35748;&#20026;&#23427;&#20204;&#23041;&#32961;&#21040;&#35813;&#23398;&#31185;&#30340;&#26680;&#24515;&#20215;&#20540;&#35266;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20154;&#31867;&#21644;LLMs&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#23567;&#22411;Alexa&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#26368;&#21021;&#30001;&#20154;&#31867;&#20998;&#26512;&#24072;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#21518;&#35201;&#27714;LLMs&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#27599;&#20010;&#20998;&#31867;&#32972;&#21518;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#20154;&#31867;&#20998;&#31867;&#21644;&#25512;&#29702;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19977;&#20998;&#20043;&#19968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#20998;&#31867;&#21644;ChatGPT 3.5&#20998;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#23545;&#40784;&#65292;&#36229;&#36807;&#22235;&#20998;&#20043;&#19968;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982; GPT4&#30340;&#23545;&#40784;&#30053;&#20302;&#12290;&#20004;&#20010;AI&#27169;&#22411;&#22312;&#36229;&#36807;&#19968;&#21322;&#30340;&#23454;&#20363;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#23545;&#40784;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#25152;&#26377;&#19977;&#31181;&#26041;&#27861;&#20013;&#36798;&#25104;&#19968;&#33268;&#30340;&#20165;&#32422;&#21344;&#20998;&#31867;&#30340;&#20116;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of AI driven large language models (LLMs) have stirred discussions about their role in qualitative research. Some view these as tools to enrich human understanding, while others perceive them as threats to the core values of the discipline. This study aimed to compare and contrast the comprehension capabilities of humans and LLMs. We conducted an experiment with small sample of Alexa app reviews, initially classified by a human analyst. LLMs were then asked to classify these reviews and provide the reasoning behind each classification. We compared the results with human classification and reasoning. The research indicated a significant alignment between human and ChatGPT 3.5 classifications in one third of cases, and a slightly lower alignment with GPT4 in over a quarter of cases. The two AI models showed a higher alignment, observed in more than half of the instances. However, a consensus across all three methods was seen only in about one fifth of the classifications. In t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#25913;&#21892;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13292</link><description>&lt;p&gt;
&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#25913;&#36827;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Variance-Covariance Regularization Improves Representation Learning. (arXiv:2306.13292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#25913;&#21892;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#20174;&#19968;&#20010;&#39046;&#22495;&#33719;&#24471;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20851;&#20110;&#36825;&#20123;&#21518;&#32493;&#20219;&#21153;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24378;&#26377;&#21147;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#35201;&#27714;&#22312;&#21021;&#22987;&#39044;&#35757;&#32451;&#38454;&#27573;&#25429;&#33719;&#21508;&#31181;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#36275;&#22815;&#30340;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#38598;&#20013;&#20110;&#20027;&#35201;&#20943;&#23569;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#36235;&#21183;&#21487;&#33021;&#23548;&#33268;&#19981;&#20805;&#20998;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#21463;&#25439;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#65288;VCR&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#12290;&#20511;&#37492;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#34920;&#29616;&#20986;&#39640;&#26041;&#24046;&#21644;&#39640;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has emerged as a key approach in the machine learning domain, enabling the application of knowledge derived from one domain to improve performance on subsequent tasks. Given the often limited information about these subsequent tasks, a strong transfer learning approach calls for the model to capture a diverse range of features during the initial pretraining stage. However, recent research suggests that, without sufficient regularization, the network tends to concentrate on features that primarily reduce the pretraining loss function. This tendency can result in inadequate feature learning and impaired generalization capability for target tasks. To address this issue, we propose Variance-Covariance Regularization (VCR), a regularization technique aimed at fostering diversity in the learned network features. Drawing inspiration from recent advancements in the self-supervised learning approach, our approach promotes learned representations that exhibit high variance and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#20013;&#25240;&#25187;&#22240;&#23376;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36991;&#20813;&#20102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13284</link><description>&lt;p&gt;
&#25913;&#27491;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#20013;&#25240;&#25187;&#22240;&#23376;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Correcting discount-factor mismatch in on-policy policy gradient methods. (arXiv:2306.13284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#20013;&#25240;&#25187;&#22240;&#23376;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36991;&#20813;&#20102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#31574;&#30053;&#26799;&#24230;&#24418;&#24335;&#65292;&#21253;&#25324;&#19977;&#20010;&#22240;&#32032;: &#21160;&#20316;&#20540;&#12289;&#21160;&#20316;&#20284;&#28982;&#26799;&#24230;&#21644;&#25240;&#25187;&#21033;&#28070;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#30340;&#24120;&#29992;&#30340;&#36827;&#31574;&#30053;&#26041;&#27861;&#24573;&#30053;&#20102;&#29366;&#24577;&#20998;&#24067;&#20013;&#30340;&#25240;&#25187;&#22240;&#23376;&#65292;&#36825;&#26159;&#25216;&#26415;&#19978;&#30340;&#38169;&#35823;&#65292;&#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#29978;&#33267;&#21487;&#33021;&#24341;&#21457;&#36864;&#21270;&#30340;&#23398;&#20064;&#34892;&#20026;&#12290;&#26082;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#22312;&#26799;&#24230;&#20272;&#35745;&#20013;&#20351;&#29992; $\gamma^t$ &#20316;&#20026;&#22240;&#23376;&#26469;&#32416;&#27491;&#27492; discrepency&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24182;&#19981;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#24182;&#19988;&#22312;&#21518;&#32493;&#29366;&#24577;&#31867;&#20284;&#20110;&#21069;&#38754;&#29366;&#24577;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#25240;&#25187;&#31283;&#24577;&#20998;&#24067;&#38382;&#39064;&#65292;&#21487;&#20197;&#25554;&#20837;&#21040;&#35768;&#22810;&#29616;&#26377;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#20013;&#12290;&#25105;&#20204;&#30340;&#26657;&#27491;&#26041;&#27861;&#22312;&#26041;&#24046;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#36991;&#20813;&#20102;&#19982; $\gamma^t$ &#26657;&#27491;&#30456;&#20851;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
The policy gradient theorem gives a convenient form of the policy gradient in terms of three factors: an action value, a gradient of the action likelihood, and a state distribution involving discounting called the \emph{discounted stationary distribution}. But commonly used on-policy methods based on the policy gradient theorem ignores the discount factor in the state distribution, which is technically incorrect and may even cause degenerate learning behavior in some environments. An existing solution corrects this discrepancy by using $\gamma^t$ as a factor in the gradient estimate. However, this solution is not widely adopted and does not work well in tasks where the later states are similar to earlier states. We introduce a novel distribution correction to account for the discounted stationary distribution that can be plugged into many existing gradient estimators. Our correction circumvents the performance degradation associated with the $\gamma^t$ correction with a lower variance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13264</link><description>&lt;p&gt;
FedSelect: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#25968;&#33258;&#23450;&#20041;&#36873;&#25321;&#30340;&#32454;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#24494;&#35843;&#23458;&#25143;&#31471;&#21442;&#25968;&#25110;&#38024;&#23545;&#26412;&#22320;&#20219;&#21153;&#20010;&#24615;&#21270;&#26550;&#26500;&#26469;&#25552;&#39640;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#22312;&#29306;&#29298;&#37325;&#35201;&#30340;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#35201;&#20040;&#22312;&#39044;&#20808;&#30830;&#23450;&#32593;&#32476;&#23618;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#20840;&#23616;&#30693;&#35782;&#20648;&#23384;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedSelect&#65292;&#36890;&#36807;&#21516;&#26102;&#25628;&#32034;&#24182;&#33719;&#24471;&#20010;&#24615;&#21270;&#26368;&#20339;&#21442;&#25968;&#21644;&#29992;&#20110;&#20840;&#23616;&#32858;&#21512;&#30340;&#20854;&#20313;&#21442;&#25968;&#65292;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#21442;&#25968;$g_k(G)$&#29992;&#20110;&#26368;&#22823;$k$-plex&#38382;&#39064;&#65292;&#38024;&#23545;&#20854;&#35774;&#35745;&#20102;&#19968;&#20010;&#26681;&#25454;$g_k(G)$&#21442;&#25968;&#21270;&#30340;&#31934;&#30830;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#21487;&#20197;&#22312;&#23454;&#38469;&#22270;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.13258</link><description>&lt;p&gt;
&#26681;&#25454;&#36864;&#21270;&#38388;&#38553;&#21442;&#25968;&#21270;&#30340;&#24555;&#36895;$k$-Plex&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast Maximum $k$-Plex Algorithm Parameterized by the Degeneracy Gap. (arXiv:2306.13258v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#21442;&#25968;$g_k(G)$&#29992;&#20110;&#26368;&#22823;$k$-plex&#38382;&#39064;&#65292;&#38024;&#23545;&#20854;&#35774;&#35745;&#20102;&#19968;&#20010;&#26681;&#25454;$g_k(G)$&#21442;&#25968;&#21270;&#30340;&#31934;&#30830;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#21487;&#20197;&#22312;&#23454;&#38469;&#22270;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#22270;&#65292;$k$-plex&#26159;&#19968;&#20010;&#39030;&#28857;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#39030;&#28857;&#19982;&#35813;&#38598;&#21512;&#20013;&#26368;&#22810;$k-1$&#20010;&#20854;&#20182;&#39030;&#28857;&#19981;&#30456;&#37051;&#12290;&#26368;&#22823;$k$-plex&#38382;&#39064;&#26159;&#20174;&#32473;&#23450;&#30340;&#22270;&#20013;&#23547;&#25214;&#26368;&#22823;$k$-plex&#65292;&#26159;&#22270;&#25628;&#32034;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#24212;&#29992;&#20013;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#23384;&#22312;&#35768;&#22810;&#32463;&#39564;&#31639;&#27861;&#65292;&#22312;&#25928;&#29575;&#26041;&#38754;&#27809;&#26377;&#36275;&#22815;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#36755;&#20837;&#23454;&#20363;&#30340;&#19968;&#20010;&#26032;&#21442;&#25968;$g_k(G)$&#65292;&#26368;&#22823;$k$-plex&#30340;&#36864;&#21270;&#36793;&#30028;&#21644;&#22823;&#23567;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26681;&#25454;$g_k(G)$&#21442;&#25968;&#21270;&#30340;&#31934;&#30830;&#31639;&#27861;&#65292;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#19982;&#36755;&#20837;&#22270;&#30340;&#22823;&#23567;&#25104;&#27491;&#27604;&#65292;&#25351;&#25968;&#22797;&#26434;&#24230;&#19982;$g_k(G)$&#25104;&#27491;&#27604;&#65292;&#20854;&#20013;$k$&#26159;&#19968;&#20010;&#24120;&#25968;&#12290;&#36890;&#24120;&#65292;&#23454;&#38469;&#22270;&#30340;$g_k(G)$&#24456;&#23567;&#65292;&#34987;$O(\log{(|V|)})$&#38480;&#21046;&#65292;&#36825;&#34920;&#26126;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph, the $k$-plex is a vertex set in which each vertex is not adjacent to at most $k-1$ other vertices in the set. The maximum $k$-plex problem, which asks for the largest $k$-plex from a given graph, is an important but computationally challenging problem in applications like graph search and community detection. So far, there is a number of empirical algorithms without sufficient theoretical explanations on the efficiency. We try to bridge this gap by defining a novel parameter of the input instance, $g_k(G)$, the gap between the degeneracy bound and the size of maximum $k$-plex in the given graph, and presenting an exact algorithm parameterized by $g_k(G)$. In other words, we design an algorithm with running time polynomial in the size of input graph and exponential in $g_k(G)$ where $k$ is a constant. Usually, $g_k(G)$ is small and bounded by $O(\log{(|V|)})$ in real-world graphs, indicating that the algorithm runs in polynomial time. We also carry out massive experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#24369;&#28151;&#28102;&#19979;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#39033;&#24335;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2306.13242</link><description>&lt;p&gt;
&#24369;&#28151;&#28102;&#19979;&#30340;&#36817;&#20284;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Approximate Causal Effect Identification under Weak Confounding. (arXiv:2306.13242v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#24369;&#28151;&#28102;&#19979;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#39033;&#24335;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#12290;&#38024;&#23545;&#21487;&#35782;&#21035;&#22240;&#26524;&#26597;&#35810;&#30340;&#28857;&#20272;&#35745;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#27491;&#30830;&#23436;&#22791;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#26597;&#35810;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#22810;&#39033;&#24335;&#31243;&#24207;&#65292;&#20197;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#32039;&#23494;&#30028;&#38480;&#12290;&#20294;&#23545;&#20110;&#25903;&#25345;&#22823;&#23567;&#36739;&#22823;&#30340;&#21464;&#37327;&#65292;&#20248;&#21270;&#36825;&#20123;&#22810;&#39033;&#24335;&#31243;&#24207;&#22312;&#35745;&#31639;&#19978;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#8220;&#24369;&#28151;&#28102;&#8221;&#23545;&#22240;&#26524;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#29109;&#24456;&#23567;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32447;&#24615;&#35268;&#21010;&#26041;&#27861;&#26469;&#23548;&#20986;&#22240;&#26524;&#25928;&#24212;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#19968;&#33268;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#29109;&#36235;&#36817;&#20110;&#38646;&#26102;&#65292;&#19978;&#38480;&#21644;&#19979;&#38480;&#20043;&#38388;&#30340;&#24046;&#24322;&#20250;&#28040;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#27169;&#25311;&#65292;&#20197;&#27604;&#36739;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#39033;&#24335;&#31243;&#24207;&#24471;&#21040;&#30340;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#26356;&#21152;&#39640;&#25928;&#65292;&#24615;&#33021;&#20063;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal effect estimation has been studied by many researchers when only observational data is available. Sound and complete algorithms have been developed for pointwise estimation of identifiable causal queries. For non-identifiable causal queries, researchers developed polynomial programs to estimate tight bounds on causal effect. However, these are computationally difficult to optimize for variables with large support sizes. In this paper, we analyze the effect of "weak confounding" on causal estimands. More specifically, under the assumption that the unobserved confounders that render a query non-identifiable have small entropy, we propose an efficient linear program to derive the upper and lower bounds of the causal effect. We show that our bounds are consistent in the sense that as the entropy of unobserved confounders goes to zero, the gap between the upper and lower bound vanishes. Finally, we conduct synthetic and real data simulations to compare our bounds with the bounds obta
&lt;/p&gt;</description></item><item><title>DiversiGATE&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#27719;&#38598;&#20102;&#22810;&#31181;LLM&#39564;&#35777;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;&#35813;&#26694;&#26550;&#30340;&#26032;&#27169;&#22411;&#8220;SelfLearner&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#20248;&#21270;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;GSM8K&#22522;&#20934;&#27979;&#35797;&#19978;&#25552;&#39640;&#20102;7%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13230</link><description>&lt;p&gt;
DiversiGATE: &#19968;&#20010;&#21487;&#38752;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20840;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiversiGATE: A Comprehensive Framework for Reliable Large Language Models. (arXiv:2306.13230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13230
&lt;/p&gt;
&lt;p&gt;
DiversiGATE&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#27719;&#38598;&#20102;&#22810;&#31181;LLM&#39564;&#35777;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;&#35813;&#26694;&#26550;&#30340;&#26032;&#27169;&#22411;&#8220;SelfLearner&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#20248;&#21270;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;GSM8K&#22522;&#20934;&#27979;&#35797;&#19978;&#25552;&#39640;&#20102;7%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiversiGATE&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#27719;&#38598;LLM&#39564;&#35777;&#30340;&#22810;&#31181;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#22810;&#26679;&#21270;&#21644;&#32858;&#21512;&#65292;&#22312;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#19978;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#20363;&#22914;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;SelfLearner&#8221;&#27169;&#22411;&#65292;&#31526;&#21512;DiversiGATE&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#19981;&#26029;&#23436;&#21892;&#20854;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;SelfLearner&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#26415;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;GSM8K&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;LLMs&#65292;&#22312;GSM8K&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#21487;&#35266;&#30340;54.8%-&gt;61.8%&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce DiversiGATE, a unified framework that consolidates diverse methodologies for LLM verification. The proposed framework comprises two main components: Diversification and Aggregation which provide a holistic perspective on existing verification approaches, such as Self-Consistency, Math Prompter and WebGPT. Furthermore, we propose a novel `SelfLearner' model that conforms to the DiversiGATE framework which can learn from its own outputs and refine its performance over time, leading to improved accuracy. To evaluate the effectiveness of SelfLearner, we conducted a rigorous series of experiments, including tests on synthetic data as well as on popular arithmetic reasoning benchmarks such as GSM8K. Our results demonstrate that our approach outperforms traditional LLMs, achieving a considerable 54.8% -&gt; 61.8% improvement on the GSM8K benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TACO&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#25552;&#39640;&#20195;&#29702;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13229</link><description>&lt;p&gt;
TACO&#65306;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning. (arXiv:2306.13229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TACO&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#25552;&#39640;&#20195;&#29702;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#21019;&#24314;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#29366;&#24577;&#39044;&#27979;&#20016;&#23500;&#20195;&#29702;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#25511;&#21046;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30446;&#26631;&#36890;&#24120;&#19981;&#36275;&#20197;&#23398;&#20064;&#33021;&#22815;&#34920;&#31034;&#26368;&#20248;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#23427;&#20204;&#36890;&#24120;&#32771;&#34385;&#20855;&#26377;&#23567;&#30340;&#25277;&#35937;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#24573;&#35270;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#20013;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TACO&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#65292;&#20195;&#29702;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#12290;TACO&#36890;&#36807;&#20248;&#21270;&#37325;&#26032;&#33719;&#24471;&#35266;&#23519;&#19982;&#26368;&#36817;&#30340;&#22810;&#20010;&#20808;&#21069;&#35266;&#23519;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#19982;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven Contrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27010;&#24565;&#65292;&#21487;&#20197;&#34920;&#36798;&#23545;&#21333;&#20010;&#20219;&#21153;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;Pareto&#26368;&#20248;&#30340;&#35268;&#21010;&#26041;&#26696;&#30340;&#39640;&#25928;&#35268;&#21010;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.13222</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#20010;&#26102;&#38388;&#20219;&#21153;&#30340;&#26368;&#20248;&#25104;&#26412;&#20559;&#22909;&#26435;&#34913;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Optimal Cost-Preference Trade-off Planning with Multiple Temporal Tasks. (arXiv:2306.13222v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27010;&#24565;&#65292;&#21487;&#20197;&#34920;&#36798;&#23545;&#21333;&#20010;&#20219;&#21153;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;Pareto&#26368;&#20248;&#30340;&#35268;&#21010;&#26041;&#26696;&#30340;&#39640;&#25928;&#35268;&#21010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#22810;&#20010;&#22797;&#26434;&#20219;&#21153;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#20123;&#24773;&#22659;&#20013;&#65292;&#21487;&#33021;&#23384;&#22312;&#19968;&#31181;&#23436;&#25104;&#25152;&#26377;&#20219;&#21153;&#30340;&#39318;&#36873;&#26041;&#24335;&#65292;&#20294;&#23427;&#32463;&#24120;&#19982;&#26368;&#20248;&#25191;&#34892;&#23384;&#22312;&#20914;&#31361;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#23558;&#22522;&#20110;&#20559;&#22909;&#30340;&#35268;&#21010;&#35270;&#20026;&#30740;&#31350;&#37325;&#28857;&#65292;&#20294;&#26159;&#23427;&#20204;&#23578;&#26410;&#23558;&#20559;&#22909;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#26426;&#22120;&#20154;&#23545;&#27599;&#20010;&#20219;&#21153;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27010;&#24565;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#24191;&#20041;&#26694;&#26550;&#26469;&#34920;&#36798;&#23545;&#21333;&#20010;&#20219;&#21153;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#20559;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;A*&#25628;&#32034;&#26469;&#36827;&#34892;&#19968;&#31181;&#36981;&#24490;&#29992;&#25143;&#20559;&#22909;&#21644;&#36164;&#28304;&#26368;&#20248;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#26368;&#20248;&#25104;&#26412;&#20559;&#22909;&#26435;&#34913;&#65288;Pareto&#65289;&#20998;&#26512;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#30446;&#26631;A*&#31639;&#27861;&#30340;&#25913;&#36827;&#26469;&#35745;&#31639;&#25972;&#20010;Pareto&#21069;&#27839;&#65288;&#25152;&#26377;&#26368;&#20248;&#25104;&#26412;&#26435;&#34913;&#30340;&#38598;&#21512;&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35268;&#21010;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#29983;&#25104;Pareto&#26368;&#20248;&#30340;&#35268;&#21010;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots are increasingly utilized in realistic scenarios with multiple complex tasks. In these scenarios, there may be a preferred way of completing all of the given tasks, but it is often in conflict with optimal execution. Recent work studies preference-based planning, however, they have yet to extend the notion of preference to the behavior of the robot with respect to each task. In this work, we introduce a novel notion of preference that provides a generalized framework to express preferences over individual tasks as well as their relations. Then, we perform an optimal trade-off (Pareto) analysis between behaviors that adhere to the user's preference and the ones that are resource optimal. We introduce an efficient planning framework that generates Pareto-optimal plans given user's preference by extending A* search. Further, we show a method of computing the entire Pareto front (the set of all optimal trade-offs) via an adaptation of a multi-objective A* algorithm. We al
&lt;/p&gt;</description></item><item><title>&#22312;Gradient-based Attribution Methods&#20013;&#65292;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#25110;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#30340;&#36873;&#25321;&#26377;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.13197</link><description>&lt;p&gt;
Gradient-based Attribution Methods&#20013;Pre&#25110;Post-Softmax Scores&#65292;&#21738;&#20010;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?. (arXiv:2306.13197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13197
&lt;/p&gt;
&lt;p&gt;
&#22312;Gradient-based Attribution Methods&#20013;&#65292;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#25110;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#30340;&#36873;&#25321;&#26377;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24037;&#20316;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24402;&#22240;&#26041;&#27861;&#20351;&#29992;&#32593;&#32476;&#20998;&#25968;&#30340;&#26799;&#24230;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#21644;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#20043;&#38388;&#30340;&#23454;&#38469;&#24046;&#24322;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient based attribution methods for neural networks working as classifiers use gradients of network scores. Here we discuss the practical differences between using gradients of pre-softmax scores versus post-softmax scores, and their respective advantages and disadvantages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2306.13196</link><description>&lt;p&gt;
DiMSam:&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#19982;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability. (arXiv:2306.13196v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#22320;&#35745;&#21010;&#38271;&#21608;&#26399;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#35268;&#21010;&#27169;&#22411;&#65292;&#22240;&#27492;&#22312;&#29615;&#22659;&#21644;&#20854;&#21160;&#24577;&#19981;&#23436;&#20840;&#20102;&#35299;&#30340;&#39046;&#22495;&#20013;&#24212;&#29992;&#23427;&#20204;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23398;&#20064;&#25429;&#33719;&#35268;&#21010;&#27169;&#22411;&#20013;&#38590;&#20197;&#35774;&#35745;&#30340;&#32422;&#26463;&#21644;&#37319;&#26679;&#22120;&#12290;&#36825;&#20123;&#23398;&#20064;&#37319;&#26679;&#22120;&#22312;TAMP&#27714;&#35299;&#22120;&#20013;&#32452;&#21512;&#21644;&#21512;&#24182;&#65292;&#20197;&#32852;&#21512;&#25214;&#21040;&#28385;&#36275;&#35268;&#21010;&#20013;&#32422;&#26463;&#30340;&#34892;&#21160;&#21442;&#25968;&#20540;&#12290;&#20026;&#20102;&#20415;&#20110;&#23545;&#29615;&#22659;&#20013;&#26410;&#30693;&#23545;&#35937;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#37319;&#26679;&#22120;&#23450;&#20041;&#20026;&#23398;&#20064;&#30340;&#20302;&#32500;&#28508;&#21464;&#37327;&#23884;&#20837;&#30340;&#21487;&#21464;&#23545;&#35937;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20851;&#33410;&#24335;&#29289;&#20307;&#25805;&#20316;&#39046;&#22495;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#32463;&#20856;TAMP&#12289;&#29983;&#25104;&#23398;&#20064;&#21644;&#28508;&#22312;&#23884;&#20837;&#30340;&#32452;&#21512;&#22914;&#20309;&#20351;&#24471;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#36827;&#34892;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#32972;&#26223;&#21435;&#38500;&#25216;&#26415;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#25552;&#39640;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13178</link><description>&lt;p&gt;
&#30446;&#26631;&#32972;&#26223;&#21435;&#38500;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Targeted Background Removal Creates Interpretable Feature Visualizations. (arXiv:2306.13178v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13178
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#32972;&#26223;&#21435;&#38500;&#25216;&#26415;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#25552;&#39640;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21487;&#35270;&#21270;&#29992;&#20110;&#21487;&#35270;&#21270;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25506;&#35752;&#20102;&#19968;&#31181;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#20197;&#25552;&#39640;&#21487;&#35270;&#21270;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36890;&#36807;&#20351;&#29992;&#32972;&#26223;&#21435;&#38500;&#25216;&#26415;&#20316;&#20026;&#40065;&#26834;&#24615;&#35757;&#32451;&#30340;&#24418;&#24335;&#65292;&#32593;&#32476;&#34987;&#36843;&#23398;&#20064;&#26356;&#22810;&#20154;&#31867;&#21487;&#35782;&#21035;&#30340;&#29305;&#24449;&#65292;&#21363;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#20027;&#35201;&#30446;&#26631;&#19978;&#32780;&#27809;&#26377;&#32972;&#26223;&#30340;&#24178;&#25200;&#12290;&#22235;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#12290;&#31532;&#19968;&#31181;&#20351;&#29992;&#26410;&#20462;&#25913;&#30340;&#22270;&#29255;&#12290;&#31532;&#20108;&#31181;&#20351;&#29992;&#40657;&#33394;&#32972;&#26223;&#12290;&#31532;&#19977;&#31181;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#32972;&#26223;&#12290;&#31532;&#22235;&#31181;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#21435;&#38500;&#32972;&#26223;&#22270;&#20687;&#21644;&#26410;&#20462;&#25913;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32972;&#26223;&#21435;&#38500;&#22270;&#20687;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#26032;&#32467;&#26524;&#20174;&#21508;&#33258;&#30340;&#31867;&#21035;&#20013;&#23637;&#31034;&#20986;&#26131;&#20110;&#35782;&#21035;&#30340;&#29305;&#24449;&#65292;&#32780;&#26410;&#20462;&#25913;&#25968;&#25454;&#30340;&#27169;&#22411;&#35757;&#32451;&#21017;&#19981;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature visualization is used to visualize learned features for black box machine learning models. Our approach explores an altered training process to improve interpretability of the visualizations. We argue that by using background removal techniques as a form of robust training, a network is forced to learn more human recognizable features, namely, by focusing on the main object of interest without any distractions from the background. Four different training methods were used to verify this hypothesis. The first used unmodified pictures. The second used a black background. The third utilized Gaussian noise as the background. The fourth approach employed a mix of background removed images and unmodified images. The feature visualization results show that the background removed images reveal a significant improvement over the baseline model. These new results displayed easily recognizable features from their respective classes, unlike the model trained on unmodified data.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Amorphous Fortress&#30340;&#31995;&#32479;&#65292;&#22312;&#36825;&#20010;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807; FSMS &#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#65292;&#24182;&#24212;&#29992;&#36827;&#21270;&#25628;&#32034;&#31639;&#27861;&#65292;&#25506;&#32034;&#20102;&#38544;&#21547;&#22312;&#25991;&#20214;&#20013;&#30340; Emergent AI &#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.13169</link><description>&lt;p&gt;
&#38750;&#26230;&#22561;&#22418;&#65306;&#22810;&#26234;&#33021;&#20307;FSMs&#20013;&#35266;&#23519;&#20986;&#30340; emergent &#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Amorphous Fortress: Observing Emergent Behavior in Multi-Agent FSMs. (arXiv:2306.13169v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13169
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Amorphous Fortress&#30340;&#31995;&#32479;&#65292;&#22312;&#36825;&#20010;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807; FSMS &#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#65292;&#24182;&#24212;&#29992;&#36827;&#21270;&#25628;&#32034;&#31639;&#27861;&#65292;&#25506;&#32034;&#20102;&#38544;&#21547;&#22312;&#25991;&#20214;&#20013;&#30340; Emergent AI &#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Amorphous Fortress&#30340;&#31995;&#32479;&#8212;&#8212;&#19968;&#20010;&#25277;&#35937;&#20294;&#26159;&#20855;&#26377;&#31354;&#38388;&#30340;&#24320;&#25918;&#24335;&#20154;&#24037;&#29983;&#21629;&#27169;&#25311;&#31995;&#32479;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#34987;&#34920;&#31034;&#20026;&#26377;&#38480;&#29366;&#24577;&#26426;&#65288;FSMs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#31354;&#38388;&#20869;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;&#36825;&#20123;&#20195;&#29702;&#26159;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#21644;&#28436;&#21270;FSMs&#26469;&#21019;&#24314;&#30340;&#65292;&#20174;&#39044;&#23450;&#20041;&#30340;&#29366;&#24577;&#21644;&#36716;&#31227;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;&#35813;&#29615;&#22659;&#26088;&#22312;&#25506;&#32034;&#22312; Dwarf Fortress &#25110; The Sims &#31561;&#27169;&#25311;&#28216;&#25103;&#20013;&#38544;&#21547;&#30340; emergent AI &#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#23665;&#19992;&#29228;&#34892;&#32773;&#36827;&#21270;&#25628;&#32034;&#31639;&#27861;&#24212;&#29992;&#20110;&#35813;&#29615;&#22659;&#20013;&#65292;&#25506;&#32034;&#20174;&#29983;&#25104;&#30340;FSMs&#20013;&#24471;&#21040;&#30340;&#21508;&#31181;&#28145;&#24230;&#21644;&#20132;&#20114;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a system called Amorphous Fortress -- an abstract, yet spatial, open-ended artificial life simulation. In this environment, the agents are represented as finite-state machines (FSMs) which allow for multi-agent interaction within a constrained space. These agents are created by randomly generating and evolving the FSMs; sampling from pre-defined states and transitions. This environment was designed to explore the emergent AI behaviors found implicitly in simulation games such as Dwarf Fortress or The Sims. We apply the hill-climber evolutionary search algorithm to this environment to explore the various levels of depth and interaction from the generated FSMs.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35752;&#35770;&#20102;&#24320;&#25918;&#29615;&#22659;&#20013;AI&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#39044;&#21028;&#24615;&#24605;&#32500;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#38754;&#21521;&#26356;&#24378;&#22823;&#39118;&#38505;&#31649;&#29702;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13157</link><description>&lt;p&gt;
&#24320;&#25918;&#24615;&#29615;&#22659;&#20013;&#30340;&#39044;&#21028;&#24615;&#24605;&#32500;&#25361;&#25112;&#65306;&#39118;&#38505;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Anticipatory Thinking Challenges in Open Worlds: Risk Management. (arXiv:2306.13157v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13157
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35752;&#35770;&#20102;&#24320;&#25918;&#29615;&#22659;&#20013;AI&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#39044;&#21028;&#24615;&#24605;&#32500;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#38754;&#21521;&#26356;&#24378;&#22823;&#39118;&#38505;&#31649;&#29702;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#21028;&#24615;&#24605;&#32500;&#33021;&#22815;&#25512;&#21160;&#25105;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#31649;&#29702;&#39118;&#38505;&#8212;&#8212;&#35782;&#21035;&#21644;&#32531;&#35299;&#39118;&#38505;&#65292;&#20174;&#24102;&#38632;&#20254;&#21040;&#36141;&#20080;&#27773;&#36710;&#20445;&#38505;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25104;&#20026;&#26085;&#24120;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#23427;&#20204;&#20063;&#24320;&#22987;&#31649;&#29702;&#39118;&#38505;&#12290;&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#34892;&#39542;&#20102;&#25968;&#30334;&#19975;&#33521;&#37324;&#65292;&#26143;&#38469;&#20105;&#38712;&#21644;&#22260;&#26827;&#20195;&#29702;&#20855;&#26377;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#33021;&#21147;&#65292;&#38544;&#21547;&#22320;&#31649;&#29702;&#30528;&#23545;&#25163;&#25552;&#20986;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36229;&#20998;&#24067;&#24335;&#35780;&#20272;&#21487;&#20197;&#25551;&#36848;&#27169;&#22411;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#31181;&#39118;&#38505;&#31649;&#29702;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#35782;&#21035;&#21644;&#32531;&#35299;&#20302;&#39057;&#39640;&#24433;&#21709;&#30340;&#39118;&#38505;&#19982;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#35266;&#23519;&#20559;&#24046;&#26159;&#30456;&#20114;&#20914;&#31361;&#30340;&#12290;&#26143;&#38469;&#20105;&#38712;&#21644;&#22260;&#26827;&#26159;&#23553;&#38381;&#30340;&#39046;&#22495;&#65292;&#20854;&#39118;&#38505;&#24050;&#30693;&#24182;&#19988;&#32531;&#35299;&#26041;&#26696;&#26377;&#33391;&#22909;&#30340;&#35760;&#24405;&#65292;&#36866;&#21512;&#36890;&#36807;&#37325;&#22797;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;&#23545;&#25239;&#31579;&#36873;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22256;&#38590;&#30340;&#20363;&#23376;&#65292;&#20294;&#25968;&#25454;&#38598;&#30340;&#31579;&#36873;&#21644;&#24314;&#31435;&#26159;&#36153;&#26102;&#30340;&#65292;&#20063;&#19981;&#20855;&#22791;&#21160;&#24577;&#24615;&#65292;&#36825;&#37117;&#26159;&#23454;&#38469;&#39118;&#38505;&#31649;&#29702;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipatory thinking drives our ability to manage risk - identification and mitigation - in everyday life, from bringing an umbrella when it might rain to buying car insurance. As AI systems become part of everyday life, they too have begun to manage risk. Autonomous vehicles log millions of miles, StarCraft and Go agents have similar capabilities to humans, implicitly managing risks presented by their opponents. To further increase performance in these tasks, out-of-distribution evaluation can characterize a model's bias, what we view as a type of risk management. However, learning to identify and mitigate low-frequency, high-impact risks is at odds with the observational bias required to train machine learning models. StarCraft and Go are closed-world domains whose risks are known and mitigations well documented, ideal for learning through repetition. Adversarial filtering datasets provide difficult examples but are laborious to curate and static, both barriers to real-world risk ma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#29289;&#29702;&#31526;&#21495;&#31995;&#32479;&#20551;&#35828;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#20551;&#35828;&#65292;&#20197;&#24357;&#21512;&#31526;&#21495;&#21644;&#31070;&#32463;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.13150</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#29289;&#29702;&#31526;&#21495;&#31995;&#32479;&#20551;&#35828;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Physical Symbol Systems Hypothesis. (arXiv:2306.13150v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13150
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#29289;&#29702;&#31526;&#21495;&#31995;&#32479;&#20551;&#35828;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#20551;&#35828;&#65292;&#20197;&#24357;&#21512;&#31526;&#21495;&#21644;&#31070;&#32463;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36317;&#31163;&#29289;&#29702;&#31526;&#21495;&#31995;&#32479;&#20551;&#35774;&#65288;PSSH&#65289;&#31532;&#19968;&#27425;&#34987;&#25552;&#20986;&#24050;&#32463;&#36229;&#36807;&#21322;&#20010;&#19990;&#32426;&#20102;&#12290;&#26368;&#36817;&#36890;&#36807;&#19982;&#31070;&#32463;&#32593;&#32476;&#21644;&#35748;&#30693;&#26550;&#26500;&#30340;&#24037;&#20316;&#30340;&#35777;&#25454;&#21066;&#24369;&#20102;&#35813;&#20551;&#35774;&#65292;&#20294;&#23427;&#23578;&#26410;&#20197;&#20219;&#20309;&#20196;&#20154;&#28385;&#24847;&#30340;&#26041;&#24335;&#34987;&#26367;&#25442;&#12290;&#22522;&#20110;&#23545;&#35745;&#31639;&#31526;&#21495;&#8212;&#8212;&#20316;&#20026;&#21407;&#23376;&#25110;&#21344;&#20301;&#31526;&#8212;&#8212;&#24615;&#36136;&#30340;&#37325;&#26032;&#24605;&#32771;&#65292;&#24182;&#22240;&#27492;&#20063;&#23545;&#23427;&#20204;&#21442;&#19982;&#30340;&#31995;&#32479;&#30340;&#37325;&#26032;&#24605;&#32771;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#24357;&#21512;&#31526;&#21495;&#21644;&#31070;&#32463;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20004;&#20010;&#26032;&#30340;&#20551;&#35774;&#65292;&#19968;&#20010;&#29992;&#26469;&#21462;&#20195;PSSH&#65292;&#21478;&#19968;&#20010;&#26356;&#30452;&#25509;&#22320;&#20851;&#27880;&#35748;&#30693;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is now more than a half-century since the Physical Symbol Systems Hypothesis (PSSH) was first articulated as an empirical hypothesis. More recent evidence from work with neural networks and cognitive architectures has weakened it, but it has not yet been replaced in any satisfactory manner. Based on a rethinking of the nature of computational symbols -- as atoms or placeholders -- and thus also of the systems in which they participate, a hybrid approach is introduced that responds to these challenges while also helping to bridge the gap between symbolic and neural approaches, resulting in two new hypotheses, one that is to replace the PSSH and other focused more directly on cognitive architectures.
&lt;/p&gt;</description></item><item><title>TRECVID&#26159;&#19968;&#31181;TREC&#39118;&#26684;&#30340;&#35270;&#39057;&#20998;&#26512;&#21644;&#26816;&#32034;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#20419;&#36827;&#25968;&#23383;&#35270;&#39057;&#20013;&#22522;&#20110;&#20869;&#23481;&#30340;&#24320;&#21457;&#21644;&#26816;&#32034;&#20449;&#24687;&#12290;TRECVID 2022&#35745;&#21010;&#24320;&#23637;&#20845;&#20010;&#20219;&#21153;&#65292;&#26377;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#30340;35&#20010;&#30740;&#31350;&#32452;&#32455;&#21442;&#21152;&#12290;</title><link>http://arxiv.org/abs/2306.13118</link><description>&lt;p&gt;
TRECVID 2022 &#20013;&#35780;&#20272;&#35270;&#39057;&#26816;&#32034;&#20219;&#21153;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
An overview on the evaluated video retrieval tasks at TRECVID 2022. (arXiv:2306.13118v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13118
&lt;/p&gt;
&lt;p&gt;
TRECVID&#26159;&#19968;&#31181;TREC&#39118;&#26684;&#30340;&#35270;&#39057;&#20998;&#26512;&#21644;&#26816;&#32034;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#20419;&#36827;&#25968;&#23383;&#35270;&#39057;&#20013;&#22522;&#20110;&#20869;&#23481;&#30340;&#24320;&#21457;&#21644;&#26816;&#32034;&#20449;&#24687;&#12290;TRECVID 2022&#35745;&#21010;&#24320;&#23637;&#20845;&#20010;&#20219;&#21153;&#65292;&#26377;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#30340;35&#20010;&#30740;&#31350;&#32452;&#32455;&#21442;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TREC &#35270;&#39057;&#26816;&#32034;&#35780;&#20272;&#65288;TRECVID&#65289;&#26159;&#19968;&#31181; TREC &#39118;&#26684;&#30340;&#35270;&#39057;&#20998;&#26512;&#21644;&#26816;&#32034;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24320;&#25918;&#12289;&#20219;&#21153;&#39537;&#21160;&#30340;&#35780;&#20272;&#21644;&#27979;&#37327;&#26469;&#20419;&#36827;&#25968;&#23383;&#35270;&#39057;&#20013;&#22522;&#20110;&#20869;&#23481;&#30340;&#24320;&#21457;&#21644;&#26816;&#32034;&#20449;&#24687;&#12290;&#22810;&#24180;&#26469;&#65292;&#35813;&#35780;&#20272;&#26041;&#27861;&#24050;&#32463;&#22312;&#22914;&#20309;&#26377;&#25928;&#22320;&#23436;&#25104;&#22788;&#29702;&#21644;&#22914;&#20309;&#21487;&#38752;&#22320;&#23545;&#31995;&#32479;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;TRECVID &#30001;&#32654;&#22269;&#22269;&#23478;&#26631;&#20934;&#25216;&#26415;&#30740;&#31350;&#25152;&#65288;NIST&#65289;&#21644;&#20854;&#20182;&#32654;&#22269;&#25919;&#24220;&#26426;&#26500;&#36164;&#21161;&#65292;&#20197;&#21450;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#30340;&#35768;&#22810;&#32452;&#32455;&#21644;&#20010;&#20154;&#36129;&#29486;&#20102;&#37325;&#35201;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290; TRECVID 2022&#35745;&#21010;&#24320;&#23637;&#20197;&#19979;&#20845;&#20010;&#20219;&#21153;&#65306;&#33258;&#36866;&#24212;&#35270;&#39057;&#25628;&#32034;&#12289;&#35270;&#39057;&#25991;&#26412;&#23383;&#24149;&#12289;&#28798;&#38590;&#22330;&#26223;&#25551;&#36848;&#21644;&#32034;&#24341;&#12289;&#25193;&#23637;&#35270;&#39057;&#20013;&#30340;&#27963;&#21160;&#12289;&#28145;&#24230;&#35270;&#39057;&#29702;&#35299;&#21644;&#30005;&#24433;&#25688;&#35201;&#12290;&#24635;&#20849;&#65292;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#30340;35&#20010;&#30740;&#31350;&#32452;&#32455;&#25253;&#21517;&#21442;&#21152;&#20102;TRECVID 2022&#12290;
&lt;/p&gt;
&lt;p&gt;
The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis and retrieval evaluation with the goal of promoting progress in research and development of content-based exploitation and retrieval of information from digital video via open, tasks-based evaluation supported by metrology. Over the last twenty-one years this effort has yielded a better understanding of how systems can effectively accomplish such processing and how one can reliably benchmark their performance. TRECVID has been funded by NIST (National Institute of Standards and Technology) and other US government agencies. In addition, many organizations and individuals worldwide contribute significant time and effort. TRECVID 2022 planned for the following six tasks: Ad-hoc video search, Video to text captioning, Disaster scene description and indexing, Activity in extended videos, deep video understanding, and movie summarization. In total, 35 teams from various research organizations worldwide signed up to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#31649;&#36947;&#20869;&#22721;&#30340;&#27668;&#20307;&#21387;&#21147;&#65292;&#20026;&#31649;&#36947;&#31995;&#32479;&#30417;&#25511;&#25552;&#20379;&#20102;&#31532;&#19968;&#27493;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#20445;&#30495;&#24230;&#19988;&#20248;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13116</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#27682;&#33030;&#30340;&#26426;&#22120;&#23398;&#20064;&#21387;&#21147;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Pressure Emulator for Hydrogen Embrittlement. (arXiv:2306.13116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#31649;&#36947;&#20869;&#22721;&#30340;&#27668;&#20307;&#21387;&#21147;&#65292;&#20026;&#31649;&#36947;&#31995;&#32479;&#30417;&#25511;&#25552;&#20379;&#20102;&#31532;&#19968;&#27493;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#20445;&#30495;&#24230;&#19988;&#20248;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27682;&#19982;&#22825;&#28982;&#27668;&#28151;&#21512;&#21518;&#20316;&#20026;&#27682;&#30340;&#26367;&#20195;&#21697;&#34987;&#29992;&#20110;&#27682;&#36816;&#36755;&#12290;&#20294;&#26159;&#65292;&#29289;&#36136;&#30340;&#27682;&#33030;&#24615;&#26159;&#31185;&#23398;&#23478;&#21644;&#22825;&#28982;&#27668;&#23433;&#35013;&#35774;&#35745;&#24072;&#38656;&#35201;&#36991;&#20813;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#22788;&#29702;&#20013;&#30340;&#22833;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#31649;&#36947;&#20869;&#22721;&#30340;&#27668;&#20307;&#21387;&#21147;&#12290;&#23613;&#31649;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#27169;&#25311;&#22120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#31649;&#36947;&#20869;&#22721;&#30340;&#21387;&#21147;&#65292;&#36825;&#26159;&#31649;&#36947;&#31995;&#32479;&#30417;&#25511;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#20248;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#28385;&#36275;&#27668;&#27969;&#31995;&#32479;&#30340;&#29289;&#29702;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent alternative for hydrogen transportation as a mixture with natural gas is blending it into natural gas pipelines. However, hydrogen embrittlement of material is a major concern for scientists and gas installation designers to avoid process failures. In this paper, we propose a physics-informed machine learning model to predict the gas pressure on the pipes' inner wall. Despite its high-fidelity results, the current PDE-based simulators are time- and computationally-demanding. Using simulation data, we train an ML model to predict the pressure on the pipelines' inner walls, which is a first step for pipeline system surveillance. We found that the physics-based method outperformed the purely data-driven method and satisfy the physical constraints of the gas flow system.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#28436;&#21069;&#21521;&#27169;&#22411;&#12289;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#31561;&#25163;&#27573;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.13104</link><description>&lt;p&gt;
&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses. (arXiv:2306.13104v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#28436;&#21069;&#21521;&#27169;&#22411;&#12289;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#31561;&#25163;&#27573;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21069;&#21015;&#33146;&#22312;&#24674;&#22797;&#22833;&#21435;&#30340;&#24863;&#23448;&#21151;&#33021;&#21644;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#35774;&#22791;&#20135;&#29983;&#30340;&#24863;&#35273;&#36890;&#24120;&#20284;&#20046;&#19981;&#33258;&#28982;&#25110;&#25197;&#26354;&#12290;&#26893;&#20837;&#22120;&#30340;&#30830;&#20999;&#20301;&#32622;&#21644;&#20010;&#20307;&#24863;&#30693;&#30340;&#24046;&#24322;&#23548;&#33268;&#21050;&#28608;&#21709;&#24212;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#20351;&#20010;&#24615;&#21270;&#21050;&#28608;&#20248;&#21270;&#25104;&#20026;&#20851;&#38190;&#25361;&#25112;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#21487;&#29992;&#20110;&#20248;&#21270;&#20855;&#26377;&#26377;&#38480;&#22122;&#22768;&#35266;&#23519;&#25968;&#25454;&#30340;&#24739;&#32773;&#19987;&#23646;&#21050;&#28608;&#21442;&#25968;&#65292;&#20294;&#23545;&#20110;&#39640;&#32500;&#21050;&#28608;&#19981;&#21487;&#34892;&#12290;&#32780;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#21050;&#28608;&#32534;&#30721;&#31574;&#30053;&#65292;&#20294;&#36890;&#24120;&#20551;&#35774;&#26377;&#20851;&#24739;&#32773;&#29305;&#23450;&#21464;&#21270;&#30340;&#23436;&#32654;&#30693;&#35782;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#23454;&#38469;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#20004;&#20010;&#22522;&#26412;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21453;&#28436;&#23558;&#30005;&#21050;&#28608;&#26144;&#23556;&#21040;&#35270;&#35273;&#24863;&#30693;&#30340;&#21069;&#21521;&#27169;&#22411;&#65292;&#35757;&#32451;&#28145;&#24230;&#32534;&#30721;&#22120;&#32593;&#32476;&#20197;&#20026;&#20219;&#20309;&#20010;&#20307;&#24739;&#32773;&#20135;&#29983;&#26368;&#20339;&#21050;&#28608;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#36873;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#65292;&#25104;&#21151;&#20351;&#30693;&#35273;&#21050;&#28608;&#26356;&#21152;&#36924;&#30495;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#26041;&#27861;&#22312;&#21160;&#29289;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuroprostheses show potential in restoring lost sensory function and enhancing human capabilities, but the sensations produced by current devices often seem unnatural or distorted. Exact placement of implants and differences in individual perception lead to significant variations in stimulus response, making personalized stimulus optimization a key challenge. Bayesian optimization could be used to optimize patient-specific stimulation parameters with limited noisy observations, but is not feasible for high-dimensional stimuli. Alternatively, deep learning models can optimize stimulus encoding strategies, but typically assume perfect knowledge of patient-specific variations. Here we propose a novel, practically feasible approach that overcomes both of these fundamental limitations. First, a deep encoder network is trained to produce optimal stimuli for any individual patient by inverting a forward model mapping electrical stimuli to visual percepts. Second, a preferential Bayesian opti
&lt;/p&gt;</description></item><item><title>MBrain&#26159;&#19968;&#31181;&#38024;&#23545;&#33041;&#20449;&#21495;&#30340;&#22810;&#36890;&#36947;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#35299;&#20915;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#39640;&#25104;&#26412;&#20020;&#24202;&#26631;&#31614;&#21644;&#19981;&#21516;&#27979;&#37327;&#26041;&#27861;&#20043;&#38388;&#20020;&#24202;&#27169;&#24335;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13102</link><description>&lt;p&gt;
MBrain&#65306;&#19968;&#31181;&#29992;&#20110;&#33041;&#20449;&#21495;&#30340;&#22810;&#36890;&#36947;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MBrain: A Multi-channel Self-Supervised Learning Framework for Brain Signals. (arXiv:2306.13102v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13102
&lt;/p&gt;
&lt;p&gt;
MBrain&#26159;&#19968;&#31181;&#38024;&#23545;&#33041;&#20449;&#21495;&#30340;&#22810;&#36890;&#36947;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#35299;&#20915;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#39640;&#25104;&#26412;&#20020;&#24202;&#26631;&#31614;&#21644;&#19981;&#21516;&#27979;&#37327;&#26041;&#27861;&#20043;&#38388;&#20020;&#24202;&#27169;&#24335;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#20449;&#21495;&#26159;&#20102;&#35299;&#20154;&#31867;&#22823;&#33041;&#30340;&#29983;&#29702;&#27963;&#21160;&#21644;&#30142;&#30149;&#30340;&#37325;&#35201;&#23450;&#37327;&#25968;&#25454;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#20851;&#27880;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39640;&#25104;&#26412;&#30340;&#20020;&#24202;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#20405;&#20837;&#24335;&#65288;&#20363;&#22914;SEEG&#65289;&#21644;&#38750;&#20405;&#20837;&#24335;&#65288;&#20363;&#22914;EEG&#65289;&#26041;&#27861;&#27979;&#37327;&#30340;&#33041;&#20449;&#21495;&#30340;&#20020;&#24202;&#27169;&#24335;&#20043;&#38388;&#24040;&#22823;&#30340;&#24046;&#24322;&#23548;&#33268;&#32570;&#20047;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22788;&#29702;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30740;&#31350;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;SEEG&#25110;EEG&#25968;&#25454;&#12290;&#30452;&#35266;&#22320;&#65292;&#30001;&#31070;&#32463;&#20803;&#30340;&#21457;&#23556;&#20135;&#29983;&#30340;&#33041;&#20449;&#21495;&#22312;&#20154;&#31867;&#22823;&#33041;&#20013;&#20256;&#36755;&#21040;&#19981;&#21516;&#30340;&#36830;&#25509;&#32467;&#26500;&#20043;&#38388;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MBrain&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#36890;&#36947;&#65288;&#21363;&#30005;&#26497;&#30340;&#25509;&#35302;&#28857;&#65292;&#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#33041;&#21306;&#65289;&#20043;&#38388;&#30340;&#38544;&#24335;&#31354;&#38388;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20316;&#20026;&#32479;&#19968;&#24314;&#27169;&#19981;&#21516;&#31867;&#22411;&#33041;&#20449;&#21495;&#30340;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain signals are important quantitative data for understanding physiological activities and diseases of human brain. Most existing studies pay attention to supervised learning methods, which, however, require high-cost clinical labels. In addition, the huge difference in the clinical patterns of brain signals measured by invasive (e.g., SEEG) and non-invasive (e.g., EEG) methods leads to the lack of a unified method. To handle the above issues, we propose to study the self-supervised learning (SSL) framework for brain signals that can be applied to pre-train either SEEG or EEG data. Intuitively, brain signals, generated by the firing of neurons, are transmitted among different connecting structures in human brain. Inspired by this, we propose MBrain to learn implicit spatial and temporal correlations between different channels (i.e., contacts of the electrode, corresponding to different brain areas) as the cornerstone for uniformly modeling different types of brain signals. Specifical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;SEEG&#25968;&#25454;&#38598;&#36827;&#34892;&#30315;&#30187;&#27874;&#26816;&#27979;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#25216;&#26415;&#65292;&#21487;&#20197;&#26816;&#27979;&#24182;&#20998;&#26512;&#25193;&#25955;&#36335;&#24452;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20020;&#24202;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.13101</link><description>&lt;p&gt;
BrainNet&#65306;&#22522;&#20110;&#20998;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;SEEG&#30315;&#30187;&#27874;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BrainNet: Epileptic Wave Detection from SEEG with Hierarchical Graph Diffusion Learning. (arXiv:2306.13101v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;SEEG&#25968;&#25454;&#38598;&#36827;&#34892;&#30315;&#30187;&#27874;&#26816;&#27979;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#25216;&#26415;&#65292;&#21487;&#20197;&#26816;&#27979;&#24182;&#20998;&#26512;&#25193;&#25955;&#36335;&#24452;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20020;&#24202;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#24433;&#21709;&#30528;&#20840;&#29699;1-2&#65285;&#30340;&#20154;&#21475;&#12290;&#30315;&#30187;&#30340;&#35786;&#26029;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23545;&#30315;&#30187;&#27874;&#30340;&#35782;&#21035;&#65292;&#21363;&#24739;&#32773;&#22823;&#33041;&#20013;&#30340;&#26080;&#24207;&#30005;&#33041;&#27963;&#21160;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#30005;&#26497;&#38142;&#35760;&#24405;&#30382;&#23618;&#33041;&#30005;&#22270;(EEG)&#26816;&#27979;&#30315;&#30187;&#27874;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#31435;&#20307;&#30005;&#26497;&#33041;&#30005;&#22270;(SEEG)&#26041;&#27861;&#25552;&#20379;&#30340;&#31435;&#20307;&#20449;&#24687;&#27604;&#20256;&#32479;&#30340;EEG&#26356;&#31934;&#30830;&#65292;&#24182;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20020;&#24202;&#23454;&#36341;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;SEEG&#25968;&#25454;&#38598;&#26469;&#26816;&#27979;&#30315;&#30187;&#27874;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#12290;&#22312;&#25552;&#20379;&#26032;&#26426;&#20250;&#30340;&#21516;&#26102;&#65292;SEEG&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#30315;&#30187;&#27874;&#27963;&#21160;&#34987;&#35748;&#20026;&#22312;&#22823;&#33041;&#30340;&#19981;&#21516;&#21306;&#22495;&#38388;&#20256;&#25773;&#12290;&#36825;&#20123;&#20256;&#25773;&#36335;&#24452;&#65292;&#20063;&#31216;&#20026;&#30315;&#30187;&#21457;&#29983;&#32593;&#32476;&#65292;&#22312;&#30315;&#30187;&#25163;&#26415;&#30340;&#32972;&#26223;&#19979;&#34987;&#35748;&#20026;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is one of the most serious neurological diseases, affecting 1-2% of the world's population. The diagnosis of epilepsy depends heavily on the recognition of epileptic waves, i.e., disordered electrical brainwave activity in the patient's brain. Existing works have begun to employ machine learning models to detect epileptic waves via cortical electroencephalogram (EEG). However, the recently developed stereoelectrocorticography (SEEG) method provides information in stereo that is more precise than conventional EEG, and has been broadly applied in clinical practice. Therefore, we propose the first data-driven study to detect epileptic waves in a real-world SEEG dataset. While offering new opportunities, SEEG also poses several challenges. In clinical practice, epileptic wave activities are considered to propagate between different regions in the brain. These propagation paths, also known as the epileptogenic network, are deemed to be a key factor in the context of epilepsy surger
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12802</link><description>&lt;p&gt;
Otter-Knowledge&#65306;&#19981;&#21516;&#26469;&#28304;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#21033;&#29992;&#22823;&#37327;&#30340;&#34507;&#30333;&#36136;&#25110;&#20998;&#23376;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#33719;&#24471;&#33647;&#29289;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#22914;&#39044;&#27979;&#33647;&#29289;&#21644;&#38774;&#34507;&#30333;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#27169;&#24577;&#30340;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#21040;&#24207;&#21015;&#25110;SMILES&#34920;&#31034;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20016;&#23500;&#34920;&#31034;&#65292;&#24182;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26469;&#33258;7&#20010;&#20844;&#20849;&#26469;&#28304;&#30340;&#39044;&#22788;&#29702;&#21644;&#25972;&#21512;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;30M&#20010;&#19977;&#20803;&#32452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;Therapeutic Data Commons (TDC)&#22522;&#20934;&#27979;&#35797;&#20013;&#24615;&#33021;&#25253;&#21578;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;&#20013;&#20998;&#25903;&#22240;&#23376;&#30340;&#26368;&#20248;&#21462;&#20540;&#38382;&#39064;&#65292;&#22522;&#20110;&#38548;&#31163;&#25928;&#29575;&#25552;&#20986;&#21019;&#26032;&#31639;&#27861;OptIForest&#65292;&#35813;&#31639;&#27861;&#32467;&#26500;&#31616;&#27905;&#12289;&#26816;&#27979;&#24615;&#33021;&#20248;&#31168;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.12703</link><description>&lt;p&gt;
OptIForest: &#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26368;&#20248;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
OptIForest: Optimal Isolation Forest for Anomaly Detection. (arXiv:2306.12703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;&#20013;&#20998;&#25903;&#22240;&#23376;&#30340;&#26368;&#20248;&#21462;&#20540;&#38382;&#39064;&#65292;&#22522;&#20110;&#38548;&#31163;&#25928;&#29575;&#25552;&#20986;&#21019;&#26032;&#31639;&#27861;OptIForest&#65292;&#35813;&#31639;&#27861;&#32467;&#26500;&#31616;&#27905;&#12289;&#26816;&#27979;&#24615;&#33021;&#20248;&#31168;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35832;&#22810;&#39046;&#22495;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#35832;&#22914;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#20837;&#20405;&#26816;&#27979;&#12289;&#37329;&#34701;&#39118;&#38505;&#30417;&#25511;&#12289;&#20154;&#31867;&#20581;&#24247;&#30417;&#27979;&#31561;&#12290;&#26681;&#25454;&#38548;&#31163;&#26862;&#26519;&#26426;&#21046;&#25552;&#20986;&#30340;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20854;&#31616;&#27905;&#12289;&#26377;&#25928;&#12289;&#39640;&#25928;&#32780;&#22791;&#21463;&#38738;&#30544;&#65292;&#20363;&#22914;&#38024;&#23545;&#23454;&#38469;&#37096;&#32626;&#65292;iForest&#26159;&#26368;&#24120;&#29992;&#30340;&#26816;&#27979;&#22120;&#20043;&#19968;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#38548;&#31163;&#26862;&#26519;&#37319;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#65292;&#20294;&#26694;&#26550;LSHiForest&#24050;&#32463;&#35777;&#26126;&#20102;&#22810;&#21449;&#38548;&#31163;&#26641;&#32467;&#26500;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23578;&#26080;&#29702;&#35770;&#24037;&#20316;&#22238;&#31572;&#20851;&#20110;&#38548;&#31163;&#26862;&#26519;&#30340;&#26368;&#20248;&#26641;&#32467;&#26500;&#30340;&#26681;&#26412;&#21644;&#23454;&#36341;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#20309;&#31181;&#20998;&#25903;&#22240;&#23376;&#30340;&#38548;&#31163;&#26641;&#32467;&#26500;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#38548;&#31163;&#25928;&#29575;&#29702;&#35770;&#26469;&#35299;&#31572;&#35813;&#38382;&#39064;&#65292;&#36827;&#32780;&#30830;&#23450;&#20102;&#19968;&#20010;&#38548;&#31163;&#26641;&#30340;&#26368;&#20248;&#20998;&#25903;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection plays an increasingly important role in various fields for critical tasks such as intrusion detection in cybersecurity, financial risk detection, and human health monitoring. A variety of anomaly detection methods have been proposed, and a category based on the isolation forest mechanism stands out due to its simplicity, effectiveness, and efficiency, e.g., iForest is often employed as a state-of-the-art detector for real deployment. While the majority of isolation forests use the binary structure, a framework LSHiForest has demonstrated that the multi-fork isolation tree structure can lead to better detection performance. However, there is no theoretical work answering the fundamentally and practically important question on the optimal tree structure for an isolation forest with respect to the branching factor. In this paper, we establish a theory on isolation efficiency to answer the question and determine the optimal branching factor for an isolation tree. Based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#26045;&#24037;&#26426;&#22120;&#20154;&#20219;&#21153;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20856;&#22411;&#26045;&#24037;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.11897</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#28082;&#21387;&#24314;&#31569;&#26426;&#22120;&#20154;&#36828;&#31243;&#25805;&#20316;&#34394;&#25311;&#35013;&#32622;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-based Virtual Fixtures for Teleoperation of Hydraulic Construction Machine. (arXiv:2306.11897v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#26045;&#24037;&#26426;&#22120;&#20154;&#20219;&#21153;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20856;&#22411;&#26045;&#24037;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#25805;&#32437;&#26045;&#24037;&#35774;&#22791;&#26159;&#24314;&#31569;&#34892;&#19994;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#33021;&#22815;&#23454;&#29616;&#36828;&#36317;&#31163;&#23433;&#20840;&#25805;&#32437;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25163;&#26564;&#36827;&#34892;&#20851;&#33410;&#32423;&#21035;&#30340;&#36828;&#31243;&#25805;&#20316;&#38656;&#35201;&#32463;&#36807;&#38271;&#26102;&#38388;&#30340;&#22521;&#35757;&#25165;&#33021;&#29087;&#32451;&#25484;&#25569;&#65292;&#21516;&#26102;&#65292;&#30001;&#20110;&#26426;&#22120;&#30340;&#22810;&#33258;&#30001;&#24230;&#29305;&#24615;&#65292;&#26426;&#22120;&#30340;&#36816;&#21160;&#21482;&#33021;&#22312;&#25191;&#34892;&#21518;&#36827;&#34892;&#39564;&#35777;&#65292;&#20351;&#24471;&#20248;&#21270;&#25511;&#21046;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#36890;&#36807;&#23398;&#20064;&#33719;&#24471;&#30340;&#25511;&#21046;&#31574;&#30053;&#29992;&#20110;&#25552;&#20379;&#20851;&#33410;&#30340;&#39640;&#25928;&#25511;&#21046;&#21644;&#21327;&#35843;&#30340;&#20219;&#21153;&#25351;&#20196;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;Brokk 170&#26045;&#24037;&#26426;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20854;&#22312;&#20856;&#22411;&#26045;&#24037;&#20219;&#21153;&#65288;&#23558;&#20991;&#23376;&#25554;&#20837;&#38075;&#23380;&#20013;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of teleoperation is a crucial aspect of the construction industry, as it enables operators to control machines safely from a distance. However, remote operation of these machines at a joint level using individual joysticks necessitates extensive training for operators to achieve proficiency due to their multiple degrees of freedom. Additionally, verifying the machine resulting motion is only possible after execution, making optimal control challenging. In addressing this issue, this study proposes a reinforcement learning-based approach to optimize task performance. The control policy acquired through learning is used to provide instructions on efficiently controlling and coordinating multiple joints. To evaluate the effectiveness of the proposed framework, a user study is conducted with a Brokk 170 construction machine by assessing its performance in a typical construction task involving inserting a chisel into a borehole. The effectiveness of the proposed framework is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10946</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#30456;&#23545;&#25104;&#29087;&#38454;&#27573;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#33616;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#20363;&#22914;&#22312;&#26053;&#28216;&#39046;&#22495;&#65292;&#36873;&#25321;&#36866;&#21512;&#30340;&#26053;&#28216;&#26223;&#28857;&#23646;&#24615;&#27969;&#31243;&#20316;&#20026;&#25512;&#33616;&#22522;&#30784;&#36739;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(Att-KGCN)&#65292;&#33258;&#21160;&#35821;&#20041;&#22320;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#23558;&#30456;&#23545;&#30456;&#20284;&#30340;&#20301;&#32622;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#26053;&#23458;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#32034;&#31185;&#29305;&#25289;&#23707;-&#20063;&#38376;&#30340;&#26053;&#28216;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#26053;&#28216;&#39046;&#22495;&#30340;&#26223;&#28857;&#25512;&#33616;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;</title><link>http://arxiv.org/abs/2306.10359</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#39537;&#21160;Foley&#38899;&#25928;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Foley&#38899;&#25928;&#29983;&#25104;&#26088;&#22312;&#20026;&#22810;&#23186;&#20307;&#20869;&#23481;&#29983;&#25104;&#32972;&#26223;&#38899;&#25928;&#12290;&#20808;&#21069;&#30340;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#24320;&#21457;&#38598;&#20316;&#20026;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;&#25968;&#23383;&#25110;one-hot&#21521;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#37197;&#23545;&#65288;CLAP&#65289;&#25216;&#26415;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26469;&#23545;&#35813;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#23884;&#20837;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#20043;&#21518;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#38899;&#39057;&#29255;&#27573;&#24182;&#36873;&#25321;&#26368;&#20339;&#29255;&#27573;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#29983;&#25104;&#30340;&#27874;&#24418;&#65292;&#26368;&#20339;&#29255;&#27573;&#26159;&#26681;&#25454;&#23884;&#20837;&#20043;&#38388;&#30456;&#20284;&#24615;&#24471;&#20998;&#30830;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#30340;&#26041;&#24335;&#65292;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25506;&#31350;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#26469;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#32463;&#27982;&#29702;&#35770;&#30340;CAMLOP&#21487;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.07932</link><description>&lt;p&gt;
&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop through Chain-of-Thought. (arXiv:2306.07932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07932
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#30340;&#26041;&#24335;&#65292;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25506;&#31350;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#26469;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#32463;&#27982;&#29702;&#35770;&#30340;CAMLOP&#21487;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#20986;&#29616;&#20351;&#33258;&#21160;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#26377;&#26102;&#22312;&#38271;&#26399;&#25110;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#20854;&#24369;&#28857;&#12290;&#20363;&#22914;&#65292;&#29992;&#25143;&#22312;&#27809;&#26377;&#20154;&#31867;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#19981;&#24635;&#33021;&#24471;&#21040;&#22797;&#26434;&#25968;&#23398;&#38382;&#39064;&#30340;&#29702;&#24819;&#31572;&#26696;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#65288;MCS&#65289;&#8212;&#8212;&#19968;&#20010;&#36890;&#36807;&#24605;&#32500;&#38142;&#25552;&#31034;&#22686;&#24378;&#30340;&#20154;&#24037;&#21442;&#19982;&#31995;&#32479;&#65292;&#25506;&#31350;&#20102;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#22914;&#20309;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#26356;&#36827;&#19968;&#27493;&#32771;&#34385;&#21040;&#26377;&#20154;&#21442;&#19982;&#30340;&#31995;&#32479;&#19981;&#20165;&#35201;&#25552;&#39640;&#24615;&#33021;&#65292;&#36824;&#35201;&#25511;&#21046;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21476;&#20856;&#32463;&#27982;&#29702;&#35770;&#30340;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#25104;&#26412;&#25928;&#29992;&#20998;&#26512;&#27169;&#22411;&#65288;CAMLOP&#65289;&#26469;&#20998;&#26512;&#12289;&#37327;&#21270;&#21644;&#24179;&#34913;&#25928;&#29992;&#21644;&#30456;&#24212;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;12&#20010;&#25968;&#25454;&#38598;&#23545;MCS&#21644;CAMLOP&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) -- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06077</link><description>&lt;p&gt;
&#35270;&#35273;&#35789;&#27719;&#25551;&#36848;&#25552;&#21319;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Visually-Grounded Descriptions Improve Zero-Shot Image Classification. (arXiv:2306.06077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#22914;CLIP&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#20219;&#21153;&#65288;&#20363;&#22914;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;ZSIC&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#20307;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#31867;&#21035;&#25551;&#36848;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#31890;&#24230;&#21644;&#26631;&#31614;&#27495;&#20041;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;V-GLOSS&#65306;Visual Glosses&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#26469;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;ZSIC&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;ImageNet&#21644;STL-10&#65289;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#26469;&#23637;&#31034;V-GLOSS&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;V-GLOSS&#29983;&#25104;&#30340;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20854;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-vision models like CLIP have made significant progress in zero-shot vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive class descriptions remains a major challenge. Existing approaches suffer from granularity and label ambiguity issues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel method leveraging modern language models and semantic knowledge bases to produce visually-grounded class descriptions. We demonstrate V-GLOSS's effectiveness by achieving state-of-the-art results on benchmark ZSIC datasets including ImageNet and STL-10. In addition, we introduce a silver dataset with class descriptions generated by V-GLOSS, and show its usefulness for vision tasks. We make available our code and dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20986;&#22312;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#27169;&#22411;&#65292;&#20197;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.01110</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#20013;&#22122;&#22768;&#24433;&#21709;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparative Study on the Effects of Noise in ML-Based Anxiety Detection. (arXiv:2306.01110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20986;&#22312;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#27169;&#22411;&#65292;&#20197;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31359;&#25140;&#24335;&#20581;&#24247;&#35774;&#22791;&#27491;&#22312;&#24341;&#39046;&#19968;&#31181;&#26032;&#26102;&#20195;&#30340;&#36830;&#32493;&#21644;&#38750;&#20405;&#20837;&#24615;&#36828;&#31243;&#30417;&#27979;&#12290;&#20854;&#20013;&#19968;&#39033;&#24212;&#29992;&#26159;&#29992;&#20110;&#28966;&#34385;&#26816;&#27979;&#12290;&#35768;&#22810;&#28966;&#34385;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#21457;&#29983;&#22312;&#21463;&#25511;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#20294;&#22122;&#22768;&#38459;&#27490;&#20102;&#36825;&#20123;&#36827;&#23637;&#25512;&#24191;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26465;&#20214;&#19979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#24182;&#24320;&#21457;&#23545;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#26085;&#24120;&#29983;&#27963;&#20013;&#28151;&#20081;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#23581;&#35797;&#30740;&#31350;&#20808;&#21069;&#30340;&#26041;&#27861;&#20026;&#20309;&#22833;&#36133;&#65292;&#24182;&#20351;&#29992;&#21487;&#31359;&#25140;&#36127;&#33655;&#19982;&#24773;&#24863;&#26816;&#27979;&#65288;WESAD&#65289;&#25968;&#25454;&#38598;&#65292;&#22312;&#19977;&#31867;&#20998;&#31867;&#38382;&#39064;&#65288;&#22522;&#20934;&#20540; vs. &#21387;&#21147; vs. &#24841;&#24742;&#65289;&#20013;&#27604;&#36739;&#19981;&#21516;&#24378;&#24230;&#22122;&#22768;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#29983;&#29702;&#21796;&#37266;&#31561;&#32423;&#30340;&#24433;&#21709;&#12290;&#22312;&#24341;&#20837;&#22122;&#22768;&#20043;&#21069;&#65292;&#25105;&#20204;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;98.7&#65285;&#65292;&#32780;Schmidt 2018&#24180;&#30340;&#27169;&#22411;&#20165;&#36798;&#21040;&#20102;80.3&#65285;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable health devices are ushering in a new age of continuous and noninvasive remote monitoring. One application of this technology is in anxiety detection. Many advancements in anxiety detection have happened in controlled lab settings, but noise prevents these advancements from generalizing to real-world conditions. We seek to progress the field by studying how noise impacts model performance and developing models that are robust to noisy, real-world conditions and, hence, attuned to the commotion of everyday life. In this study we look to investigate why and how previous methods have failed. Using the wearable stress and affect detection (WESAD) dataset, we compare the effect of various intensities of noise on machine learning models classifying levels of physiological arousal in the three-class classification problem: baseline vs. stress vs. amusement. Before introducing noise, our baseline model performance reaches 98.7%, compared to Schmidt 2018's 80.3%. We discuss potential so
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.01430</link><description>&lt;p&gt;
&#20998;&#31163;&#30340;&#20851;&#27880;&#21147;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#20998;&#31163;&#27133;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots. (arXiv:2304.01430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#22522;&#30784;&#30495;&#20540;&#25110;&#30417;&#30563;&#12290;&#23427;&#30001;&#22522;&#20110;&#27133;&#20851;&#27880;&#30340;&#23545;&#25239;&#26465;&#20214;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#32452;&#25104;&#65292;&#20462;&#25913;&#20026;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#35299;&#30721;&#20809;&#27969;&#65292;&#32780;&#19981;&#26159;&#23581;&#35797;&#37325;&#26500;&#22270;&#20687;&#26412;&#36523;&#12290;&#22312;&#32467;&#26524;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#20013;&#65292;&#19968;&#31181;&#27169;&#24335;&#65288;&#27969;&#65289;&#23558;&#39304;&#36865;&#32473;&#32534;&#30721;&#22120;&#20197;&#20135;&#29983;&#21333;&#29420;&#30340;&#28508;&#22312;&#20195;&#30721;&#65288;&#27133;&#65289;&#65292;&#32780;&#21478;&#19968;&#31181;&#27169;&#24335;&#65288;&#22270;&#20687;&#65289;&#23558;&#20915;&#23450;&#35299;&#30721;&#22120;&#20174;&#27133;&#29983;&#25104;&#31532;&#19968;&#20010;&#27169;&#24335;&#65288;&#27969;&#65289;&#12290;&#30001;&#20110;&#24815;&#24120;&#30340;&#33258;&#32534;&#30721;&#22522;&#20110;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#24182;&#19981;&#33021;&#38450;&#27490;&#25972;&#20010;&#27969;&#34987;&#32534;&#30721;&#21040;&#19968;&#20010;&#27133;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#25439;&#22833;&#20462;&#25913;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20998;&#31163;&#30340;&#23545;&#25239;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to segment the visual field into independently moving regions, trained with no ground truth or supervision. It consists of an adversarial conditional encoder-decoder architecture based on Slot Attention, modified to use the image as context to decode optical flow without attempting to reconstruct the image itself. In the resulting multi-modal representation, one modality (flow) feeds the encoder to produce separate latent codes (slots), whereas the other modality (image) conditions the decoder to generate the first (flow) from the slots. This design frees the representation from having to encode complex nuisance variability in the image due to, for instance, illumination and reflectance properties of the scene. Since customary autoencoding based on minimizing the reconstruction error does not preclude the entire flow from being encoded into a single slot, we modify the loss to an adversarial criterion based on Contextual Information Separation. The resulting min-m
&lt;/p&gt;</description></item><item><title>OTOV2&#26159;&#19968;&#31181;&#33258;&#21160;&#12289;&#36890;&#29992;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#19968;&#27425;&#21363;&#21487;&#29983;&#25104;&#24615;&#33021;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26356;&#32039;&#20945;&#27169;&#22411;&#65292;&#26080;&#38656;&#24494;&#35843;&#65292;&#26377;&#25928;&#31616;&#21270;&#20102;&#27169;&#22411;&#21387;&#32553;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.06862</link><description>&lt;p&gt;
OTOV2: &#33258;&#21160;&#21270;&#12289;&#36890;&#29992;&#21270;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#36890;&#29992;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
OTOV2: Automatic, Generic, User-Friendly. (arXiv:2303.06862v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06862
&lt;/p&gt;
&lt;p&gt;
OTOV2&#26159;&#19968;&#31181;&#33258;&#21160;&#12289;&#36890;&#29992;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#19968;&#27425;&#21363;&#21487;&#29983;&#25104;&#24615;&#33021;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26356;&#32039;&#20945;&#27169;&#22411;&#65292;&#26080;&#38656;&#24494;&#35843;&#65292;&#26377;&#25928;&#31616;&#21270;&#20102;&#27169;&#22411;&#21387;&#32553;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36890;&#36807;&#32467;&#26500;&#21270;&#21098;&#26525;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#36807;&#31243;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#26356;&#24191;&#27867;&#30340;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#20108;&#20195; Only-Train-Once (OTOv2) &#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#35757;&#32451;&#21644;&#21387;&#32553;&#36890;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29983;&#25104;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#26356;&#32039;&#20945;&#27169;&#22411;&#65292;&#26080;&#38656;&#24494;&#35843;&#12290;OTOv2 &#33258;&#21160;&#19988;&#21487;&#25554;&#20837;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#29992;&#25143;&#20960;&#20046;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#24037;&#31243;&#21270;&#24037;&#20316;&#12290;&#26041;&#27861;&#19978;&#65292;OTOv2 &#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#25913;&#36827;: (i) &#33258;&#20027;&#24615;&#65306;&#33258;&#21160;&#21033;&#29992;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#36182;&#24615;&#65292;&#23558;&#21487;&#35757;&#32451;&#30340;&#21464;&#37327;&#20998;&#25104;&#38646;&#19981;&#21464;&#32452; (ZIGs)&#65292;&#24182;&#26500;&#24314;&#21387;&#32553;&#27169;&#22411;; (ii) &#21452;&#21322;&#31354;&#38388;&#25237;&#24433;&#26799;&#24230; (DHSPG)&#65306;&#19968;&#31181;&#29992;&#20110;&#26356;&#21487;&#38752;&#22320;&#35299;&#20915;&#32467;&#26500;&#31232;&#30095;&#38382;&#39064;&#30340;&#26032;&#22411;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing model compression methods via structured pruning typically require complicated multi-stage procedures. Each individual stage necessitates numerous engineering efforts and domain-knowledge from the end-users which prevent their wider applications onto broader scenarios. We propose the second generation of Only-Train-Once (OTOv2), which first automatically trains and compresses a general DNN only once from scratch to produce a more compact model with competitive performance without fine-tuning. OTOv2 is automatic and pluggable into various deep learning applications, and requires almost minimal engineering efforts from the users. Methodologically, OTOv2 proposes two major improvements: (i) Autonomy: automatically exploits the dependency of general DNNs, partitions the trainable variables into Zero-Invariant Groups (ZIGs), and constructs the compressed model; and (ii) Dual Half-Space Projected Gradient (DHSPG): a novel optimizer to more reliably solve structured-sparsity prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BLOOM-zh&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#25193;&#23637;&#20102;BLOOM&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#20855;&#26377;&#25913;&#36827;&#30340;&#32321;&#20307;&#20013;&#25991;&#25903;&#25345;&#12290;BLOOM-zh&#22312;&#32321;&#20307;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#21069;&#36523;&#12290;</title><link>http://arxiv.org/abs/2303.04715</link><description>&lt;p&gt;
BLOOM&#30340;&#39044;&#35757;&#32451;&#25193;&#23637;&#20197;&#25913;&#21892;&#23545;&#32321;&#20307;&#20013;&#25991;&#30340;&#25903;&#25345;&#65306;&#27169;&#22411;&#12289;&#26041;&#27861;&#21644;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Extending the Pre-Training of BLOOM for Improved Support of Traditional Chinese: Models, Methods and Results. (arXiv:2303.04715v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BLOOM-zh&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#25193;&#23637;&#20102;BLOOM&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#20855;&#26377;&#25913;&#36827;&#30340;&#32321;&#20307;&#20013;&#25991;&#25903;&#25345;&#12290;BLOOM-zh&#22312;&#32321;&#20307;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#21069;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BLOOM-zh&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#25913;&#36827;&#30340;&#32321;&#20307;&#20013;&#25991;&#25903;&#25345;&#12290;BLOOM-zh&#36215;&#28304;&#20110;&#30001;BigScience&#20110;2022&#24180;&#25512;&#20986;&#30340;&#24320;&#28304;BLOOM&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#24050;&#21457;&#24067;&#30340;&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;74&#20159;&#20010;&#39069;&#22806;&#30340;&#32321;&#20307;&#20013;&#25991;&#21644;&#33521;&#25991;&#26631;&#35760;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#35206;&#30422;&#20102;&#21508;&#31181;&#39046;&#22495;&#65292;&#22914;&#26032;&#38395;&#25991;&#31456;&#12289;&#20070;&#31821;&#12289;&#30334;&#31185;&#20840;&#20070;&#12289;&#25945;&#32946;&#26448;&#26009;&#20197;&#21450;&#21475;&#35821;&#35821;&#35328;&#12290;&#20026;&#20102;&#23637;&#31034;BLOOM-zh&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#21644;&#26032;&#21019;&#24314;&#30340;&#22522;&#20934;&#22330;&#26223;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#22312;&#22823;&#22810;&#25968;&#30340;&#32321;&#20307;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;BLOOM-zh&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#21069;&#36523;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20854;&#33521;&#25991;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#27169;&#22411;&#21457;&#24067;&#32473;&#30740;&#31350;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the multilingual language model BLOOM-zh that features enhanced support for Traditional Chinese. BLOOM-zh has its origins in the open-source BLOOM models presented by BigScience in 2022. Starting from released models, we extended the pre-training of BLOOM by additional 7.4 billion tokens in Traditional Chinese and English covering a variety of domains such as news articles, books, encyclopedias, educational materials as well as spoken language. In order to show the properties of BLOOM-zh, both existing and newly created benchmark scenarios are used for evaluating the performance. BLOOM-zh outperforms its predecessor on most Traditional Chinese benchmarks while maintaining its English capability. We release all our models to the research community.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#35299;&#37322;&#20102;&#20960;&#31181;&#26368;&#26032;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21450;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08560</link><description>&lt;p&gt;
&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#65306;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#32479;&#19968;&#21644;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dual RL: Unification and New Methods for Reinforcement and Imitation Learning. (arXiv:2302.08560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#35299;&#37322;&#20102;&#20960;&#31181;&#26368;&#26032;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21450;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26399;&#26395;&#32047;&#31215;&#22238;&#25253;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#24615;&#32422;&#26463;&#19979;&#20248;&#21270;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#34920;&#31034;&#12290;&#36825;&#20010;&#34920;&#36848;&#30340;&#23545;&#20598;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#65292;&#26159;&#26080;&#32422;&#26463;&#30340;&#24182;&#19988;&#26356;&#23481;&#26131;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#27169;&#20223;&#23398;&#20064;&#21487;&#20197;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#34987;&#35270;&#20026;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#32479;&#19968;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#22522;&#30784;&#65292;&#21487;&#20197;&#30740;&#31350;&#21644;&#35782;&#21035;&#36825;&#20123;&#26041;&#27861;&#25104;&#21151;&#30340;&#26500;&#25104;&#37096;&#20998;&#65292;&#24182;&#25581;&#31034;&#36825;&#20123;&#26041;&#27861;&#30340;&#20849;&#21516;&#32570;&#28857;&#21644;&#25913;&#36827;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20197;&#21069;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#19981;&#29616;&#23454;&#30340;&#35206;&#30422;&#29575;&#20551;&#35774;&#65292;&#24182;&#26368;&#23567;&#21270;&#20102;&#23398;&#20064;&#20195;&#29702;&#21644;&#19987;&#23478;&#35775;&#38382;&#20998;&#24067;&#20043;&#38388;&#30340;&#29305;&#23450;f-&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#22312;&#21516;&#26679;&#30340;&#21452;&#37325;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#23545;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of reinforcement learning (RL) is to maximize the expected cumulative return. It has been shown that this objective can be represented by an optimization problem of the state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as dual RL, is unconstrained and easier to optimize. We show that several state-of-the-art off-policy deep reinforcement learning (RL) algorithms, under both online and offline, RL and imitation learning (IL) settings, can be viewed as dual RL approaches in a unified framework. This unification provides a common ground to study and identify the components that contribute to the success of these methods and also reveals the common shortcomings across methods with new insights for improvement. Our analysis shows that prior off-policy imitation learning methods are based on an unrealistic coverage assumption and are minimizing a particular f-divergence between the visitation distributions of the l
&lt;/p&gt;</description></item><item><title>MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.05981</link><description>&lt;p&gt;
MarioGPT: &#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25991;&#26412;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05981
&lt;/p&gt;
&lt;p&gt;
MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#22797;&#26434;&#25968;&#19968;&#33268;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#21453;&#26144;&#29305;&#23450;&#24847;&#22270;&#21644;&#38480;&#21046;&#30340;&#26377;&#24847;&#20041;&#20869;&#23481;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#32570;&#20047;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#35757;&#32451;&#26377;&#32032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#65292;&#37325;&#22797;&#20351;&#29992;&#20449;&#24687;&#24182;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#22521;&#35757;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MarioGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;GPT2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#29943;&#30742;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#25105;&#20204;&#20197;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#30340;&#20851;&#21345;&#20026;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MarioGPT&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#25511;&#21046;&#20851;&#21345;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;PCG&#25216;&#26415;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#20851;&#21345;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.01246</link><description>&lt;p&gt;
&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20248;&#21270;&#26234;&#33021;&#20307;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Optimizing Agent Collaboration through Heuristic Multi-Agent Planning. (arXiv:2301.01246v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01246
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28041;&#21450;&#21040;&#19981;&#21516;&#31867;&#22411;&#24863;&#30693;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#35299;&#20915;QDec-POMDP&#30340;SOTA&#31639;&#27861;QDec-FP&#21644;QDec-FPS&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#35201;&#27714;&#26234;&#33021;&#20307;&#37319;&#21462;&#30456;&#21516;&#30340;&#35745;&#21010;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#27604;QDec-FP&#21644;QDec-FPS&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SOTA algorithms for addressing QDec-POMDP issues, QDec-FP and QDec-FPS, are unable to effectively tackle problems that involve different types of sensing agents. We propose a new algorithm that addresses this issue by requiring agents to adopt the same plan if one agent is unable to take a sensing action but the other can. Our algorithm performs significantly better than both QDec-FP and QDec-FPS in these types of situations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19977;&#32500;&#20154;&#31867;&#25968;&#25454;&#38598; UltraStage&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807; 2,000 &#20010;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#36164;&#20135;&#65292;&#36825;&#20123;&#20154;&#31867;&#36164;&#20135;&#37117;&#26159;&#22312;&#22810;&#35270;&#35282;&#21644;&#22810;&#29031;&#26126;&#35774;&#32622;&#19979;&#25429;&#33719;&#30340;&#12290;&#26368;&#26032;&#36827;&#23637;&#30340;&#31070;&#32463;&#34920;&#31034;&#27861;&#23558;&#27599;&#20010;&#31034;&#20363;&#35299;&#37322;&#20026;&#31070;&#32463;&#20154;&#31867;&#36164;&#20135;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#19979;&#20855;&#26377;&#28789;&#27963;&#24615;&#22320;&#20219;&#24847;&#37325;&#29031;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#24615;&#39640;&#30340;&#26082;&#36866;&#29992;&#20110;&#20154;&#31867;&#24314;&#27169;&#21448;&#36866;&#29992;&#20110;&#20877;&#29031;&#26126;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2212.07648</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#28176;&#21464;&#29031;&#26126;&#19979;&#30340;&#31070;&#32463;&#20154;&#31867;&#27169;&#22411;&#21450;&#20854;&#20877;&#29031;&#26126;
&lt;/p&gt;
&lt;p&gt;
Relightable Neural Human Assets from Multi-view Gradient Illuminations. (arXiv:2212.07648v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19977;&#32500;&#20154;&#31867;&#25968;&#25454;&#38598; UltraStage&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807; 2,000 &#20010;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#36164;&#20135;&#65292;&#36825;&#20123;&#20154;&#31867;&#36164;&#20135;&#37117;&#26159;&#22312;&#22810;&#35270;&#35282;&#21644;&#22810;&#29031;&#26126;&#35774;&#32622;&#19979;&#25429;&#33719;&#30340;&#12290;&#26368;&#26032;&#36827;&#23637;&#30340;&#31070;&#32463;&#34920;&#31034;&#27861;&#23558;&#27599;&#20010;&#31034;&#20363;&#35299;&#37322;&#20026;&#31070;&#32463;&#20154;&#31867;&#36164;&#20135;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#19979;&#20855;&#26377;&#28789;&#27963;&#24615;&#22320;&#20219;&#24847;&#37325;&#29031;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#24615;&#39640;&#30340;&#26082;&#36866;&#29992;&#20110;&#20154;&#31867;&#24314;&#27169;&#21448;&#36866;&#29992;&#20110;&#20877;&#29031;&#26126;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24314;&#27169;&#21644;&#20877;&#29031;&#26126;&#39046;&#22495;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#20004;&#20010;&#22522;&#30784;&#24615;&#38590;&#39064;&#12290;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#26497;&#22823;&#22320;&#20419;&#36827;&#30456;&#20851;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20154;&#31867;&#25968;&#25454;&#38598;&#21482;&#25552;&#20379;&#22312;&#30456;&#21516;&#29031;&#26126;&#19979;&#25429;&#33719;&#30340;&#22810;&#35282;&#24230;&#22270;&#20687;&#65292;&#34429;&#28982;&#23545;&#20110;&#24314;&#27169;&#20219;&#21153;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;&#20877;&#29031;&#26126;&#38382;&#39064;&#12290;&#20026;&#20102;&#25512;&#21160;&#20004;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; UltraStage&#65292;&#19968;&#20010;&#26032;&#30340;&#19977;&#32500;&#20154;&#31867;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807; 2,000 &#20010;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#36164;&#20135;&#65292;&#36825;&#20123;&#20154;&#31867;&#36164;&#20135;&#37117;&#26159;&#22312;&#22810;&#35270;&#35282;&#21644;&#22810;&#29031;&#26126;&#35774;&#32622;&#19979;&#25429;&#33719;&#30340;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#27599;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;32&#20010;&#21608;&#22260;&#35270;&#35282;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#30333;&#20809;&#29031;&#26126;&#65292;&#21478;&#22806;&#20004;&#20010;&#26159;&#28176;&#21464;&#29031;&#26126;&#12290;&#38500;&#20102;&#24120;&#35268;&#30340;&#22810;&#35270;&#35282;&#22270;&#20687;&#22806;&#65292;&#28176;&#21464;&#29031;&#26126;&#36824;&#26377;&#21161;&#20110;&#24674;&#22797;&#35814;&#32454;&#30340;&#34920;&#38754;&#27861;&#32447;&#21644;&#31354;&#38388;&#21464;&#21270;&#26448;&#36136;&#36148;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#21508;&#31181;&#20877;&#29031;&#26126;&#24212;&#29992;&#12290;&#21463;&#21040;&#31070;&#32463;&#34920;&#31034;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#27599;&#20010;&#31034;&#20363;&#35299;&#37322;&#20026;&#31070;&#32463;&#20154;&#31867;&#36164;&#20135;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#19979;&#20855;&#26377;&#28789;&#27963;&#24615;&#22320;&#20219;&#24847;&#37325;&#29031;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#24615;&#39640;&#30340;&#26082;&#36866;&#29992;&#20110;&#20154;&#31867;&#24314;&#27169;&#21448;&#36866;&#29992;&#20110;&#20877;&#29031;&#26126;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human modeling and relighting are two fundamental problems in computer vision and graphics, where high-quality datasets can largely facilitate related research. However, most existing human datasets only provide multi-view human images captured under the same illumination. Although valuable for modeling tasks, they are not readily used in relighting problems. To promote research in both fields, in this paper, we present UltraStage, a new 3D human dataset that contains more than 2,000 high-quality human assets captured under both multi-view and multi-illumination settings. Specifically, for each example, we provide 32 surrounding views illuminated with one white light and two gradient illuminations. In addition to regular multi-view images, gradient illuminations help recover detailed surface normal and spatially-varying material maps, enabling various relighting applications. Inspired by recent advances in neural representation, we further interpret each example into a neural human ass
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#31995;&#32479;&#30340;&#34920;&#22411;&#25628;&#32034;&#36712;&#36857;&#65292;&#30830;&#23450;&#26356;&#22797;&#26434;&#34920;&#22411;&#30340;&#22522;&#22240;&#20016;&#24230;&#36739;&#23569;&#65292;&#26356;&#38590;&#34987;&#21457;&#29616;&#65292;&#36739;&#31616;&#21333;&#34920;&#22411;&#21017;&#34987;&#36807;&#24230;&#20195;&#34920;&#23481;&#26131;&#36827;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.08516</link><description>&lt;p&gt;
&#32447;&#24615;&#36951;&#20256;&#32534;&#31243;&#30340;&#34920;&#22411;&#25628;&#32034;&#36712;&#36857;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Phenotype Search Trajectory Networks for Linear Genetic Programming. (arXiv:2211.08516v2 [q-bio.PE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08516
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#31995;&#32479;&#30340;&#34920;&#22411;&#25628;&#32034;&#36712;&#36857;&#65292;&#30830;&#23450;&#26356;&#22797;&#26434;&#34920;&#22411;&#30340;&#22522;&#22240;&#20016;&#24230;&#36739;&#23569;&#65292;&#26356;&#38590;&#34987;&#21457;&#29616;&#65292;&#36739;&#31616;&#21333;&#34920;&#22411;&#21017;&#34987;&#36807;&#24230;&#20195;&#34920;&#23481;&#26131;&#36827;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#22411;&#21040;&#34920;&#22411;&#26144;&#23556;&#23558;&#22522;&#22240;&#22411;&#21464;&#24322;&#36716;&#21270;&#25104;&#34920;&#22411;&#21464;&#24322;&#12290;&#20013;&#24615;&#26159;&#19968;&#20123;&#21464;&#24322;&#19981;&#20250;&#23548;&#33268;&#34920;&#22411;&#21464;&#21270;&#30340;&#29616;&#35937;&#12290;&#30740;&#31350;&#22522;&#22240;&#22411;&#21644;&#34920;&#22411;&#31354;&#38388;&#20013;&#30340;&#25628;&#32034;&#36712;&#36857;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20013;&#24615;&#21464;&#24322;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#28436;&#21270;&#30340;&#36827;&#31243;&#21450;&#20854;&#31639;&#27861;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#22270;&#24418;&#30340;&#27169;&#22411;&#29983;&#25104;&#36951;&#20256;&#32534;&#31243;&#31995;&#32479;&#30340;&#25628;&#32034;&#36712;&#36857;&#65292;&#20854;&#20013;&#33410;&#28857;&#20195;&#34920;&#22522;&#22240;&#22411;/&#34920;&#22411;&#65292;&#36793;&#20195;&#34920;&#23427;&#20204;&#30340;&#21464;&#24322;&#36716;&#25442;&#12290;&#25105;&#20204;&#36824;&#23450;&#37327;&#22320;&#27979;&#37327;&#34920;&#22411;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#22522;&#22240;&#22411;&#20016;&#24230;&#65288;&#23545;&#20013;&#24615;&#30340;&#35201;&#27714;&#65289;&#21644; Kolmogorov &#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#23450;&#37327;&#25351;&#26631;&#19982;&#25628;&#32034;&#36712;&#36857;&#21487;&#35270;&#21270;&#30456;&#32467;&#21512;&#65292;&#21457;&#29616;&#26356;&#22797;&#26434;&#30340;&#34920;&#22411;&#34987;&#26356;&#23569;&#30340;&#22522;&#22240;&#22411;&#25152;&#20195;&#34920;&#65292;&#36827;&#21270;&#26356;&#38590;&#20197;&#21457;&#29616;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36739;&#31616;&#21333;&#30340;&#34920;&#22411;&#21017;&#34987;&#36807;&#24230;&#20195;&#34920;&#20102;&#65292;&#26356;&#26131;&#20110;&#36827;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#25506;&#32034;&#22522;&#22240;&#22411;&#12289;&#34920;&#22411;&#21644;&#36827;&#21270;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#20851;&#31995;&#30340;&#26032;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Genotype-to-phenotype mappings translate genotypic variations such as mutations into phenotypic changes. Neutrality is the observation that some mutations do not lead to phenotypic changes. Studying the search trajectories in genotypic and phenotypic spaces, especially through neutral mutations, helps us to better understand the progression of evolution and its algorithmic behaviour. In this study, we visualise the search trajectories of a genetic programming system as graph-based models, where nodes are genotypes/phenotypes and edges represent their mutational transitions. We also quantitatively measure the characteristics of phenotypes including their genotypic abundance (the requirement for neutrality) and Kolmogorov complexity. We connect these quantified metrics with search trajectory visualisations, and find that more complex phenotypes are under-represented by fewer genotypes and are harder for evolution to discover. Less complex phenotypes, on the other hand, are over-represent
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21450;&#35821;&#38899;&#19982;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36890;&#36807;&#25429;&#33719;&#20854;&#26465;&#20214;&#20381;&#36182;&#24615;&#24471;&#20986;&#39044;&#27979;&#32467;&#26524;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#25233;&#37057;&#21644;&#20854;&#30456;&#20851;&#30151;&#29366;&#12290;</title><link>http://arxiv.org/abs/2211.04924</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#22312;&#20351;&#29992;&#35821;&#38899;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#39044;&#27979;&#25233;&#37057;&#21450;&#20854;&#30151;&#29366;&#30340;&#40065;&#26834;&#19988;&#26080;&#20559;&#39044;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data. (arXiv:2211.04924v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04924
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21450;&#35821;&#38899;&#19982;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36890;&#36807;&#25429;&#33719;&#20854;&#26465;&#20214;&#20381;&#36182;&#24615;&#24471;&#20986;&#39044;&#27979;&#32467;&#26524;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#25233;&#37057;&#21644;&#20854;&#30456;&#20851;&#30151;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#34892;&#20026;&#21644;&#35748;&#30693;&#20449;&#21495;&#39044;&#27979;&#37325;&#24230;&#25233;&#37057;&#38556;&#30861;&#65288;MDD&#65289;&#30340;&#23384;&#22312;&#26159;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;MDD&#30340;&#24322;&#36136;&#24615;&#20020;&#24202;&#29305;&#24449;&#24847;&#21619;&#30528;&#20219;&#20309;&#32473;&#23450;&#30340;&#35821;&#38899;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;/&#25110;&#35266;&#23519;&#21040;&#30340;&#35748;&#30693;&#27169;&#24335;&#37117;&#21487;&#33021;&#19982;&#25233;&#37057;&#30151;&#29366;&#30340;&#29420;&#29305;&#32452;&#21512;&#30456;&#20851;&#32852;&#12290;&#20256;&#32479;&#30340;&#21028;&#21035;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#32570;&#20047;&#31283;&#20581;&#22320;&#24314;&#27169;&#36825;&#31181;&#24322;&#36136;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#36125;&#21494;&#26031;&#32593;&#32476;&#21487;&#33021;&#26356;&#36866;&#21512;&#36825;&#31181;&#24773;&#20917;&#12290;&#36825;&#20123;&#32593;&#32476;&#26159;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#26126;&#30830;&#25429;&#33719;&#38543;&#26426;&#21464;&#37327;&#30340;&#26465;&#20214;&#20381;&#36182;&#24615;&#65292;&#39640;&#25928;&#22320;&#25551;&#36848;&#20102;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20010;&#26694;&#26550;&#30456;&#27604;&#20110;&#26631;&#20934;&#30340;&#21028;&#21035;&#24615;&#24314;&#27169;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22312;&#27169;&#22411;&#32467;&#26500;&#20013;&#34701;&#21512;&#19987;&#23478;&#24847;&#35265;&#65292;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#65292;&#20102;&#35299;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#33258;&#28982;&#22320;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#38899;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#39044;&#27979;&#25233;&#37057;&#30151;&#21450;&#20854;&#30456;&#20851;&#30151;&#29366;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#30001;&#26377;&#21644;&#27809;&#26377;MDD&#30340;&#20010;&#20307;&#37319;&#38598;&#30340;&#35821;&#38899;&#21644;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#19982;&#20256;&#32479;&#30340;&#21028;&#21035;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36125;&#21494;&#26031;&#32593;&#32476;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#32593;&#32476;&#32467;&#26500;&#25552;&#20379;&#20102;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#30456;&#20114;&#20316;&#29992;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;MDD&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the presence of major depressive disorder (MDD) using behavioural and cognitive signals is a highly non-trivial task. The heterogeneous clinical profile of MDD means that any given speech, facial expression and/or observed cognitive pattern may be associated with a unique combination of depressive symptoms. Conventional discriminative machine learning models potentially lack the complexity to robustly model this heterogeneity. Bayesian networks, however, may instead be well-suited to such a scenario. These networks are probabilistic graphical models that efficiently describe the joint probability distribution over a set of random variables by explicitly capturing their conditional dependencies. This framework provides further advantages over standard discriminative modelling by offering the possibility to incorporate expert opinion in the graphical structure of the models, generating explainable model predictions, informing about the uncertainty of predictions, and naturally
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#24182;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2211.01407</link><description>&lt;p&gt;
&#20851;&#20110;&#30417;&#30563;&#20449;&#21495;&#30340;&#20449;&#24687;&#37327;
&lt;/p&gt;
&lt;p&gt;
On the Informativeness of Supervision Signals. (arXiv:2211.01407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#24182;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#20391;&#37325;&#20110;&#20174;&#20154;&#31867;&#26631;&#27880;&#30340;&#35757;&#32451;&#31034;&#20363;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#20016;&#23500;&#30340;&#27880;&#37322;&#65288;&#22914;&#36719;&#26631;&#31614;&#65289;&#27604;&#31232;&#30095;&#30340;&#27880;&#37322;&#65288;&#22914;&#30828;&#26631;&#31614;&#65289;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#30340;&#25910;&#38598;&#25104;&#26412;&#20063;&#26356;&#39640;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#20110;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#33021;&#21147;&#22914;&#20309;&#21463;&#21040;&#26631;&#31614;&#25968;&#12289;&#31867;&#21035;&#12289;&#32500;&#24230;&#21644;&#22122;&#22768;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning typically focuses on learning transferable representations from training examples annotated by humans. While rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. For example, while hard labels only provide information about the closest class an object belongs to (e.g., "this is a dog"), soft labels provide information about the object's relationship with multiple classes (e.g., "this is most likely a dog, but it could also be a wolf or a coyote"). We use information theory to compare how a number of commonly-used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. Our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#35268;&#21010;&#26694;&#26550;PASTA&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#38271;&#26399;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#30340;&#26377;&#25928;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2210.15751</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20013;&#30340;&#26102;&#31354;&#25277;&#35937;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning with Spatial-Temporal Abstraction from Point Clouds for Deformable Object Manipulation. (arXiv:2210.15751v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#35268;&#21010;&#26694;&#26550;PASTA&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#38271;&#26399;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#30340;&#26377;&#25928;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#35270;&#37326;&#19979;&#26377;&#25928;&#22320;&#35268;&#21010;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#38656;&#35201;&#36866;&#24403;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#19987;&#27880;&#20110;&#30701;&#26399;&#20219;&#21153;&#65292;&#35201;&#20040;&#20570;&#20986;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#35748;&#20026;&#23436;&#25972;&#30340;&#29366;&#24577;&#20449;&#24687;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#38459;&#27490;&#20102;&#23427;&#20204;&#22312;&#21464;&#24418;&#23545;&#35937;&#19978;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PASTA&#30340;&#22522;&#20110;&#26102;&#31354;&#25277;&#35937;&#30340;&#35268;&#21010;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#31354;&#38388;&#25277;&#35937;&#65288;&#23545;&#29289;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#25512;&#29702;&#65289;&#21644;&#26102;&#38388;&#25277;&#35937;&#65288;&#23545;&#25216;&#33021;&#32780;&#19981;&#26159;&#20302;&#32423;&#34892;&#20026;&#30340;&#25512;&#29702;&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#39640;&#32500;&#30340;3D&#35266;&#27979;&#65292;&#22914;&#28857;&#20113;&#65292;&#26144;&#23556;&#21040;&#19968;&#32452;&#28508;&#22312;&#21521;&#37327;&#65292;&#24182;&#22312;&#28508;&#22312;&#38598;&#34920;&#31034;&#30340;&#22522;&#30784;&#19978;&#35268;&#21010;&#25216;&#33021;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39034;&#24207;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#65292;&#36825;&#38656;&#35201;&#32467;&#21512;&#22810;&#20010;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#65292;&#22914;&#29992;&#20992;&#20999;&#21106;&#12289;&#29992;&#25512;&#23376;&#25512;&#21160;&#21644;&#38138;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective planning of long-horizon deformable object manipulation requires suitable abstractions at both the spatial and temporal levels. Previous methods typically either focus on short-horizon tasks or make strong assumptions that full-state information is available, which prevents their use on deformable objects. In this paper, we propose PlAnning with Spatial-Temporal Abstraction (PASTA), which incorporates both spatial abstraction (reasoning about objects and their relations to each other) and temporal abstraction (reasoning over skills instead of low-level actions). Our framework maps high-dimension 3D observations such as point clouds into a set of latent vectors and plans over skill sequences on top of the latent set representation. We show that our method can effectively perform challenging sequential deformable object manipulation tasks in the real world, which require combining multiple tool-use skills such as cutting with a knife, pushing with a pusher, and spreading the do
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#21306;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#22823;&#27493;&#31070;&#32463;&#32593;&#32476;&#65292;&#23398;&#20064;&#36763;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#28436;&#21270;&#65292;&#26377;&#25928;&#25233;&#21046;&#32047;&#31215;&#35823;&#24046;&#65292;&#24182;&#25104;&#21151;&#20445;&#25345;Jacobi&#31215;&#20998;&#30340;&#23432;&#24658;&#12290;</title><link>http://arxiv.org/abs/2208.14148</link><description>&lt;p&gt;
&#22823;&#27493;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26469;&#33258;&#20998;&#21306;&#25968;&#25454;&#30340;&#36763;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-step neural network for learning the symplectic evolution from partitioned data. (arXiv:2208.14148v2 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#21306;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#22823;&#27493;&#31070;&#32463;&#32593;&#32476;&#65292;&#23398;&#20064;&#36763;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#28436;&#21270;&#65292;&#26377;&#25928;&#25233;&#21046;&#32047;&#31215;&#35823;&#24046;&#65292;&#24182;&#25104;&#21151;&#20445;&#25345;Jacobi&#31215;&#20998;&#30340;&#23432;&#24658;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#38656;&#35201;&#39044;&#27979;&#36763;&#26144;&#23556;&#29983;&#25104;&#30340;&#22352;&#26631;&#65288;q&#65289;&#21644;&#21160;&#37327;&#65288;p&#65289;&#21464;&#37327;&#12290;&#22522;&#20110;Chen&#65286;Tao&#65288;2021&#65289;&#30340;&#30740;&#31350;&#65292;&#36763;&#26144;&#23556;&#30001;&#29983;&#25104;&#20989;&#25968;&#34920;&#31034;&#12290;&#20026;&#20102;&#24310;&#38271;&#39044;&#27979;&#26102;&#38388;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#24207;&#21015;&#65288;q_i&#12289;p_i&#65289;&#20998;&#25104;&#20960;&#20010;&#21306;&#38388;&#65292;&#24182;&#29992;&#22823;&#27493;&#31070;&#32463;&#32593;&#32476;&#65288;LSNN&#65289;&#26469;&#36924;&#36817;&#31532;&#19968;&#21306;&#38388;&#65288;&#21363;&#21021;&#22987;&#26465;&#20214;&#65289;&#21644;&#20854;&#20313;&#21508;&#20010;&#21306;&#38388;&#20043;&#38388;&#30340;&#29983;&#25104;&#20989;&#25968;&#12290;&#36825;&#31181;&#20998;&#21306;&#26041;&#27861;&#20351;&#25105;&#20204;&#30340;LSNN&#22312;&#39044;&#27979;&#31995;&#32479;&#28436;&#21270;&#26102;&#33021;&#26377;&#25928;&#25233;&#21046;&#32047;&#31215;&#35823;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;LSNN&#23398;&#20064;25000&#24180;&#30340;2&#65306;3&#20849;&#25391;&#26607;&#20234;&#20271;&#24102;&#23545;&#35937;&#30340;&#36816;&#21160;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25105;&#20204;&#20808;&#21069;&#24037;&#20316;&#20013;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;Li&#31561;&#65292;2022&#65289;&#22522;&#30784;&#19978;&#65292;&#26377;&#20004;&#20010;&#26174;&#33879;&#30340;&#25913;&#36827;&#65306;&#65288;1&#65289;Jacobi&#31215;&#20998;&#30340;&#23432;&#24658;&#65292;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on learning Hamiltonian systems, which involves predicting the coordinate (q) and momentum (p) variables generated by a symplectic mapping. Based on Chen &amp; Tao (2021), the symplectic mapping is represented by a generating function. To extend the prediction time period, we develop a new learning scheme by splitting the time series (q_i, p_i) into several partitions. We then train a large-step neural network (LSNN) to approximate the generating function between the first partition (i.e. the initial condition) and each one of the remaining partitions. This partition approach makes our LSNN effectively suppress the accumulative error when predicting the system evolution. Then we train the LSNN to learn the motions of the 2:3 resonant Kuiper belt objects for a long time period of 25000 yr. The results show that there are two significant improvements over the neural network constructed in our previous work (Li et al. 2022): (1) the conservation of the Jacobi integral,
&lt;/p&gt;</description></item><item><title>DeepGraviLens&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.00701</link><description>&lt;p&gt;
DeepGraviLens&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data. (arXiv:2205.00701v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00701
&lt;/p&gt;
&lt;p&gt;
DeepGraviLens&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#36879;&#38236;&#26159;&#30001;&#22823;&#36136;&#37327;&#29289;&#20307;&#20135;&#29983;&#30340;&#30456;&#23545;&#35770;&#25928;&#24212;&#65292;&#20250;&#24367;&#26354;&#20854;&#21608;&#22260;&#30340;&#26102;&#31354;&#12290;&#36825;&#26159;&#22825;&#20307;&#29289;&#29702;&#23398;&#20013;&#19968;&#20010;&#28145;&#20837;&#30740;&#31350;&#30340;&#35838;&#39064;&#65292;&#20801;&#35768;&#39564;&#35777;&#29702;&#35770;&#30456;&#23545;&#35770;&#32467;&#26524;&#24182;&#30740;&#31350;&#19968;&#20123;&#21542;&#21017;&#19981;&#21487;&#35265;&#30340;&#24494;&#24369;&#22825;&#20307;&#29289;&#20307;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24341;&#21147;&#36879;&#38236;&#29616;&#35937;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#20142;&#24230;&#21464;&#21270;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#36879;&#38236;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20165;&#32771;&#34385;&#22270;&#20687;&#32780;&#24573;&#30053;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#35201;&#20040;&#22312;&#26368;&#22256;&#38590;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30456;&#23545;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; DeepGraviLens&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19968;&#20010;&#38750;&#36879;&#38236;&#31995;&#32479;&#31867;&#22411;&#21644;&#19977;&#20010;&#36879;&#38236;&#31995;&#32479;&#31867;&#22411;&#30340;&#26102;&#31354;&#25968;&#25454;&#12290;&#23427;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#24403;&#21069;&#30340; state-of-art &#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#32422; 19% &#21040; 43%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#25152;&#32771;&#34385;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gravitational lensing is the relativistic effect generated by massive bodies, which bend the space-time surrounding them. It is a deeply investigated topic in astrophysics and allows validating theoretical relativistic results and studying faint astrophysical objects that would not be visible otherwise. In recent years Machine Learning methods have been applied to support the analysis of the gravitational lensing phenomena by detecting lensing effects in data sets consisting of images associated with brightness variation time series. However, the state-of-art approaches either consider only images and neglect time-series data or achieve relatively low accuracy on the most difficult data sets. This paper introduces DeepGraviLens, a novel multi-modal network that classifies spatio-temporal data belonging to one non-lensed system type and three lensed system types. It surpasses the current state of the art accuracy results by $\approx$ 19% to $\approx$ 43%, depending on the considered dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#20026;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2106.16046</link><description>&lt;p&gt;
&#25506;&#32034;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65306;&#22522;&#20934;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Exploring the Context Generalizability in Spatiotemporal Crowd Flow Prediction: Benchmark and Guideline. (arXiv:2106.16046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.16046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#20026;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#29305;&#24449;&#26159;&#26500;&#24314;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#65288;STCFP&#65289;&#27169;&#22411;&#30340;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#19978;&#19979;&#25991;&#30340;&#22256;&#38590;&#22312;&#20110;&#19981;&#21516;&#22330;&#26223;&#20013;&#19978;&#19979;&#25991;&#29305;&#24449;&#65288;&#20363;&#22914;&#22825;&#27668;&#12289;&#20551;&#26085;&#21644;&#20852;&#36259;&#28857;&#65289;&#21644;&#19978;&#19979;&#25991;&#24314;&#27169;&#25216;&#26415;&#30340;&#26410;&#30693;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#30001;&#22823;&#35268;&#27169;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#25968;&#25454;&#12289;&#19978;&#19979;&#25991;&#25968;&#25454;&#21644;&#26368;&#20808;&#36827;&#30340;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#32452;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22478;&#24066;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#23450;&#37327;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#29305;&#24449;&#21644;&#24314;&#27169;&#25216;&#26415;&#30340;&#27867;&#21270;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#27969;&#34892;&#30740;&#31350;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#24320;&#21457;&#20102;&#19978;&#19979;&#25991;&#24314;&#27169;&#25216;&#26415;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25968;&#30334;&#19975;&#26465;&#35760;&#24405;&#21644;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#25968;&#30334;&#31181;&#27169;&#22411;&#20197;&#25429;&#25417;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;STCFP&#20013;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual features are important data sources for building spatiotemporal crowd flow prediction (STCFP) models. However, the difficulty of applying context lies in the unknown generalizability of both contextual features (e.g., weather, holiday, and points of interests) and context modeling techniques across different scenarios. In this paper, we build a benchmark composed of large-scale spatiotemporal crowd flow data, contextual data, and state-of-the-art spatiotemporal prediction models. We conduct a comprehensive experimental study to quantitatively investigate the generalizability of different contextual features and modeling techniques in several urban crowd flow prediction scenarios (including bike flow, metro passenger flow, electric vehicle charging demand and so on). In particular, we develop a general taxonomy of context modeling techniques based on extensive investigations in prevailing research. With millions of records and rich context data, we have trained and tested hun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#24320;&#25918;&#24335;&#22810;&#27169;&#24577;&#20851;&#31995;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#20132;&#20114;&#22238;&#31572;&#35270;&#39057;&#22330;&#26223;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2012.00822</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#22810;&#27169;&#24577;&#20851;&#31995;&#25512;&#29702;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Open-Ended Multi-Modal Relational Reasoning for Video Question Answering. (arXiv:2012.00822v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.00822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#24320;&#25918;&#24335;&#22810;&#27169;&#24577;&#20851;&#31995;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#20132;&#20114;&#22238;&#31572;&#35270;&#39057;&#22330;&#26223;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#29992;&#20110;&#20998;&#26512;&#22806;&#37096;&#29615;&#22659;&#24182;&#22238;&#31572;&#21442;&#19982;&#32773;&#30340;&#38382;&#39064;&#12290;&#35813;&#20195;&#29702;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#22312;&#22522;&#20110;&#35270;&#39057;&#22330;&#26223;&#30340;&#35821;&#35328;&#20132;&#20114;&#20013;&#21327;&#21161;&#20010;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#35270;&#39057;&#35782;&#21035;&#25216;&#26415;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25972;&#21512;&#21040;&#26426;&#22120;&#20154;&#20195;&#29702;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#21442;&#19982;&#32773;&#21644;&#26426;&#22120;&#20154;&#20195;&#29702;&#20043;&#38388;&#20986;&#29616;&#30340;&#30456;&#20851;&#38382;&#39064;&#65292;&#25506;&#35752;&#24433;&#21709;&#20154;&#26426;&#20132;&#20114;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26041;&#27861;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20449;&#20219;&#21644;&#20132;&#20114;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#31215;&#26497;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;2%&#33267;3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a robotic agent specifically designed to analyze external environments and address participants' questions. The primary focus of this agent is to assist individuals using language-based interactions within video-based scenes. Our proposed method integrates video recognition technology and natural language processing models within the robotic agent. We investigate the crucial factors affecting human-robot interactions by examining pertinent issues arising between participants and robot agents. Methodologically, our experimental findings reveal a positive relationship between trust and interaction efficiency. Furthermore, our model demonstrates a 2\% to 3\% performance enhancement in comparison to other benchmark methods.
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#31995;&#32479;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#36827;&#21270;&#35843;&#25972;&#36890;&#36807;&#20998;&#23618;&#26550;&#26500;&#23454;&#29616;&#30340;&#20449;&#24687;&#26159;&#26500;&#24314;AGI&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/1703.02245</link><description>&lt;p&gt;
&#20154;&#24037;&#35774;&#35745;&#65306;&#20174;&#26222;&#36866;&#26234;&#33021;&#30340;&#29983;&#29289;&#26681;&#28304;&#20013;&#27762;&#21462;&#30340;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
Design of the Artificial: lessons from the biological roots of general intelligence. (arXiv:1703.02245v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1703.02245
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#31995;&#32479;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#36827;&#21270;&#35843;&#25972;&#36890;&#36807;&#20998;&#23618;&#26550;&#26500;&#23454;&#29616;&#30340;&#20449;&#24687;&#26159;&#26500;&#24314;AGI&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#26234;&#33021;&#26426;&#22120;&#30340;&#36855;&#24651;&#21487;&#20197;&#36861;&#28335;&#21040;&#21476;&#20195;&#26102;&#26399;&#65292;&#31070;&#35805;&#20154;&#29289;&#22612;&#27931;&#26031;&#12289;&#20122;&#37324;&#22763;&#22810;&#24503;&#30340;&#26426;&#26800;&#24605;&#24819;&#21644;&#20122;&#21382;&#23665;&#22823;&#30340;&#26426;&#26800;&#26426;&#22120;&#12290;&#28982;&#32780;&#65292;&#36861;&#27714;&#20154;&#24037;&#26222;&#36866;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#25506;&#32034;&#19968;&#30452;&#22791;&#21463;&#25387;&#25240;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#21521;&#29983;&#29289;&#21551;&#21457;&#24335;&#36719;&#20214;&#21644;&#30828;&#20214;&#30340;&#36716;&#21464;&#65292;&#20294;&#23427;&#20204;&#21333;&#19968;&#30340;&#35774;&#35745;&#37325;&#24515;&#20351;&#24471;&#20854;&#22312;&#23454;&#29616;AGI&#19978;&#25928;&#29575;&#20302;&#19979;&#12290;AGI&#30340;&#35774;&#35745;&#38656;&#35201;&#28385;&#36275;&#21738;&#20123;&#35201;&#27714;&#65311;&#20154;&#24037;&#35774;&#35745;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#65311;&#23545;&#29983;&#29289;&#31995;&#32479;&#35745;&#31639;&#30340;&#20180;&#32454;&#23457;&#26597;&#34920;&#26126;&#65292;&#36890;&#36807;&#20998;&#23618;&#26550;&#26500;&#23454;&#29616;&#30340;&#20449;&#24687;&#30340;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#36827;&#21270;&#35843;&#25972;&#26159;&#26500;&#24314;AGI&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our fascination with intelligent machines goes back to ancient times with the mythical automaton Talos, Aristotle's mode of mechanical thought (syllogism) and Heron of Alexandria's mechanical machines. However, the quest for Artificial General Intelligence (AGI) has been troubled with repeated failures. Recently, there has been a shift towards bio-inspired software and hardware, but their singular design focus makes them inefficient in achieving AGI. Which set of requirements have to be met in the design of AGI? What are the limits in the design of the artificial? A careful examination of computation in biological systems suggests that evolutionary tinkering of contextual processing of information enabled by a hierarchical architecture is key to building AGI.
&lt;/p&gt;</description></item></channel></rss>