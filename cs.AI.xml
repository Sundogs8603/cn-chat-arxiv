<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"DOST"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#35268;&#21017;&#30340;&#32435;&#20837;&#65292;&#21033;&#29992;&#39046;&#22495;&#36981;&#20174;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20851;&#38190;&#25351;&#26631;&#65292;&#20943;&#23567;&#20102;&#27880;&#37322;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.05101</link><description>&lt;p&gt;
DOST - &#26080;&#22122;&#22768;&#26631;&#31614;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#39046;&#22495;&#36981;&#20174;&#33258;&#30417;&#30563;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DOST -- Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels. (arXiv:2308.05101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"DOST"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#35268;&#21017;&#30340;&#32435;&#20837;&#65292;&#21033;&#29992;&#39046;&#22495;&#36981;&#20174;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20851;&#38190;&#25351;&#26631;&#65292;&#20943;&#23567;&#20102;&#27880;&#37322;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#24040;&#22823;&#38656;&#27714;&#24341;&#21457;&#20102;&#27880;&#37322;&#22122;&#22768;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20010;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24191;&#27867;&#35752;&#35770;&#65292;&#20294;&#22312;"&#22810;&#26631;&#31614;&#20998;&#31867;"&#65288;MLC&#65289;&#20219;&#21153;&#30340;&#22797;&#26434;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;&#21364;&#30456;&#23545;&#26410;&#32463;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#24403;&#25152;&#28041;&#21450;&#30340;&#39046;&#22495;&#20855;&#26377;&#26576;&#20123;&#36923;&#36753;&#32422;&#26463;&#26102;&#65292;&#22122;&#22768;&#26631;&#27880;&#24120;&#24120;&#21152;&#21095;&#36829;&#35268;&#24773;&#20917;&#65292;&#20351;&#24471;&#35813;&#31995;&#32479;&#34987;&#19987;&#23478;&#35748;&#20026;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26631;&#31614;&#22122;&#22768;&#23545;MLC&#20219;&#21153;&#20013;&#39046;&#22495;&#35268;&#21017;&#36829;&#35268;&#20107;&#20214;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#39046;&#22495;&#35268;&#21017;&#32435;&#20837;&#25105;&#20204;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"Domain Obedient Self-supervised Training"&#65288;DOST&#65289;&#33539;&#24335;&#65292;&#19981;&#20165;&#21487;&#20197;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26356;&#31526;&#21512;&#39046;&#22495;&#35268;&#21017;&#65292;&#36824;&#21487;&#20197;&#22312;&#20851;&#38190;&#25351;&#26631;&#19978;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#26368;&#23567;&#21270;&#27880;&#37322;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20351;&#29992;&#39046;&#22495;&#25351;&#23548;&#35757;&#32451;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enormous demand for annotated data brought forth by deep learning techniques has been accompanied by the problem of annotation noise. Although this issue has been widely discussed in machine learning literature, it has been relatively unexplored in the context of "multi-label classification" (MLC) tasks which feature more complicated kinds of noise. Additionally, when the domain in question has certain logical constraints, noisy annotations often exacerbate their violations, making such a system unacceptable to an expert. This paper studies the effect of label noise on domain rule violation incidents in the MLC task, and incorporates domain rules into our learning algorithm to mitigate the effect of noise. We propose the Domain Obedient Self-supervised Training (DOST) paradigm which not only makes deep learning models more aligned to domain rules, but also improves learning performance in key metrics and minimizes the effect of annotation noise. This novel approach uses domain guid
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LayoutLLM-T2I&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;LLM&#20013;&#33719;&#21462;&#24067;&#23616;&#25351;&#23548;&#20197;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#30001;&#31895;&#21040;&#32454;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#32473;&#23450;&#25991;&#26412;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#21512;&#25104;&#19982;&#25991;&#26412;&#35821;&#20041;&#23545;&#40784;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.05095</link><description>&lt;p&gt;
LayoutLLM-T2I&#65306;&#20174;LLM&#20013;&#33719;&#21462;&#24067;&#23616;&#25351;&#23548;&#20197;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation. (arXiv:2308.05095v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LayoutLLM-T2I&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;LLM&#20013;&#33719;&#21462;&#24067;&#23616;&#25351;&#23548;&#20197;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#30001;&#31895;&#21040;&#32454;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#32473;&#23450;&#25991;&#26412;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#21512;&#25104;&#19982;&#25991;&#26412;&#35821;&#20041;&#23545;&#40784;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#65292;&#36817;&#26399;&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#30340;&#26174;&#33879;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#21508;&#31181;&#26032;&#39062;&#30340;&#36924;&#30495;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#22797;&#26434;&#33258;&#28982;&#22330;&#26223;&#20013;&#20173;&#28982;&#38754;&#20020;&#30528;&#38169;&#20301;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#21644;&#25968;&#23383;&#21270;&#22833;&#36133;&#65289;&#65292;&#36825;&#38459;&#30861;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#32463;&#36827;&#34892;&#20102;&#19968;&#20123;&#25913;&#36827;&#65292;&#36890;&#36807;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#25351;&#23548;&#65288;&#20363;&#22914;&#65292;&#33609;&#22270;&#21644;&#28034;&#40486;&#65289;&#26469;&#25913;&#21892;&#21487;&#25511;&#24615;&#65292;&#20294;&#26159;&#30001;&#20110;&#29992;&#25143;&#24517;&#39035;&#25163;&#21160;&#25552;&#20379;&#36825;&#20123;&#25351;&#23548;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#20010;&#38382;&#39064;&#23578;&#26410;&#26681;&#26412;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21162;&#21147;&#21512;&#25104;&#19982;&#32473;&#23450;&#25991;&#26412;&#25552;&#31034;&#35821;&#20041;&#23545;&#40784;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#25351;&#23548;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31895;&#21040;&#32454;&#30340;&#33539;&#20363;&#65292;&#29992;&#20110;&#24067;&#23616;&#35268;&#21010;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#26465;&#20214;&#19979;&#29983;&#25104;&#31895;&#31890;&#24230;&#30340;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the text-to-image generation field, recent remarkable progress in Stable Diffusion makes it possible to generate rich kinds of novel photorealistic images. However, current models still face misalignment issues (e.g., problematic spatial relation understanding and numeration failure) in complex natural scenes, which impedes the high-faithfulness text-to-image generation. Although recent efforts have been made to improve controllability by giving fine-grained guidance (e.g., sketch and scribbles), this issue has not been fundamentally tackled since users have to provide such guidance information manually. In this work, we strive to synthesize high-fidelity images that are semantically aligned with a given textual prompt without any guidance. Toward this end, we propose a coarse-to-fine paradigm to achieve layout planning and image generation. Concretely, we first generate the coarse-grained layout conditioned on a given textual prompt via in-context learning based on Large Language M
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#36828;&#31243;&#24037;&#20316;&#29615;&#22659;&#20013;&#32452;&#32455;&#22823;&#35268;&#27169;&#37038;&#20214;&#31995;&#32479;&#30340;&#20316;&#29992;&#21644;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35813;&#31995;&#32479;&#23384;&#22312;&#38382;&#39064;&#65292;&#21253;&#25324;&#25509;&#25910;&#32773;&#26080;&#27861;&#20445;&#30041;&#28040;&#24687;&#12289;&#25509;&#25910;&#32773;&#21644;&#21457;&#36865;&#32773;&#23545;&#37325;&#35201;&#28040;&#24687;&#26377;&#19981;&#21516;&#30475;&#27861;&#20197;&#21450;&#27807;&#36890;&#32773;&#32570;&#20047;&#25216;&#26415;&#25903;&#25345;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#35813;&#31995;&#32479;&#30340;&#35758;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.05085</link><description>&lt;p&gt;
&#32452;&#32455;&#22823;&#35268;&#27169;&#37038;&#20214;&#31995;&#32479;&#65306;&#22312;&#36828;&#31243;&#24037;&#20316;&#20013;&#30340;&#20316;&#29992;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Organizational Bulk Email Systems: Their Role and Performance in Remote Work. (arXiv:2308.05085v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05085
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#36828;&#31243;&#24037;&#20316;&#29615;&#22659;&#20013;&#32452;&#32455;&#22823;&#35268;&#27169;&#37038;&#20214;&#31995;&#32479;&#30340;&#20316;&#29992;&#21644;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35813;&#31995;&#32479;&#23384;&#22312;&#38382;&#39064;&#65292;&#21253;&#25324;&#25509;&#25910;&#32773;&#26080;&#27861;&#20445;&#30041;&#28040;&#24687;&#12289;&#25509;&#25910;&#32773;&#21644;&#21457;&#36865;&#32773;&#23545;&#37325;&#35201;&#28040;&#24687;&#26377;&#19981;&#21516;&#30475;&#27861;&#20197;&#21450;&#27807;&#36890;&#32773;&#32570;&#20047;&#25216;&#26415;&#25903;&#25345;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#35813;&#31995;&#32479;&#30340;&#35758;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#36843;&#20351;&#35768;&#22810;&#21592;&#24037;&#22312;&#23478;&#24037;&#20316;&#12290;&#32452;&#32455;&#22823;&#35268;&#27169;&#37038;&#20214;&#29616;&#22312;&#22312;&#36825;&#31181;&#36828;&#31243;&#24037;&#20316;&#29615;&#22659;&#20013;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#65292;&#20197;&#20415;&#21521;&#21592;&#24037;&#20256;&#36798;&#20013;&#22830;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25105;&#20204;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30693;&#36947;&#32452;&#32455;&#22823;&#35268;&#27169;&#37038;&#20214;&#23384;&#22312;&#38382;&#39064;&#65306;&#25509;&#25910;&#32773;&#26080;&#27861;&#20445;&#30041;&#20182;&#20204;&#20174;&#32452;&#32455;&#25509;&#25910;&#21040;&#30340;&#22823;&#35268;&#27169;&#28040;&#24687;&#65307;&#25509;&#25910;&#32773;&#21644;&#21457;&#36865;&#32773;&#23545;&#21738;&#20123;&#22823;&#35268;&#27169;&#28040;&#24687;&#37325;&#35201;&#26377;&#19981;&#21516;&#30340;&#24847;&#35265;&#65307;&#20197;&#21450;&#27807;&#36890;&#32773;&#32570;&#20047;&#25216;&#26415;&#25903;&#25345;&#26469;&#26356;&#22909;&#22320;&#23450;&#20301;&#21644;&#35774;&#35745;&#28040;&#24687;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#22238;&#39038;&#20102;&#35780;&#20272;&#12289;&#35774;&#35745;&#21644;&#21407;&#22411;&#21270;&#32452;&#32455;&#27807;&#36890;&#31995;&#32479;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#25105;&#20204;&#26368;&#36817;&#30340;&#21457;&#29616;&#21644;&#19968;&#20123;&#25105;&#20204;&#21457;&#29616;&#23545;&#30740;&#31350;&#32452;&#32455;&#27807;&#36890;&#26377;&#29992;&#30340;&#30740;&#31350;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#35758;&#31243;&#65292;&#30740;&#31350;&#36828;&#31243;&#24037;&#20316;&#29615;&#22659;&#20013;&#30340;&#32452;&#32455;&#27807;&#36890;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has forced many employees to work from home. Organizational bulk emails now play a critical role to reach employees with central information in this work-from-home environment. However, we know from our own recent work that organizational bulk email has problems: recipients fail to retain the bulk messages they received from the organization; recipients and senders have different opinions on which bulk messages were important; and communicators lack technology support to better target and design messages. In this position paper, first we review the prior work on evaluating, designing, and prototyping organizational communication systems. Second we review our recent findings and some research techniques we found useful in studying organizational communication. Last we propose a research agenda to study organizational communications in remote work environment and suggest some key questions and potential study directions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#26080;&#20154;&#26426;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23454;&#29616;&#33258;&#21160;&#21270;&#21644;&#22823;&#35268;&#27169;&#24773;&#20917;&#35780;&#20272;&#65292;&#21516;&#26102;&#28436;&#31034;&#20102;&#26426;&#36733;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#22312;&#26080;&#20154;&#26426;&#25937;&#25588;&#25237;&#36865;&#20013;&#30340;&#38598;&#25104;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#12289;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#26512;&#21644;&#25552;&#39640;&#25937;&#25588;&#25237;&#36865;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05074</link><description>&lt;p&gt;
Drones4Good&#65306;&#36890;&#36807;&#36965;&#24863;&#21644;&#20154;&#24037;&#26234;&#33021;&#25903;&#25345;&#25937;&#28798;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
Drones4Good: Supporting Disaster Relief Through Remote Sensing and AI. (arXiv:2308.05074v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05074
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#26080;&#20154;&#26426;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23454;&#29616;&#33258;&#21160;&#21270;&#21644;&#22823;&#35268;&#27169;&#24773;&#20917;&#35780;&#20272;&#65292;&#21516;&#26102;&#28436;&#31034;&#20102;&#26426;&#36733;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#22312;&#26080;&#20154;&#26426;&#25937;&#25588;&#25237;&#36865;&#20013;&#30340;&#38598;&#25104;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#12289;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#26512;&#21644;&#25552;&#39640;&#25937;&#25588;&#25237;&#36865;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#28798;&#21518;&#24773;&#20917;&#65292;&#32039;&#24613;&#26381;&#21153;&#21644;&#25937;&#25588;&#32452;&#32455;&#38656;&#35201;&#21450;&#26102;&#20934;&#30830;&#22320;&#33719;&#21462;&#21463;&#28798;&#21306;&#22495;&#30340;&#20449;&#24687;&#12290;&#36965;&#24863;&#25216;&#26415;&#26377;&#26395;&#36890;&#36807;&#24555;&#36895;&#21208;&#23519;&#22823;&#33539;&#22260;&#21306;&#22495;&#26469;&#26174;&#33879;&#20943;&#23569;&#25910;&#38598;&#27492;&#31867;&#20449;&#24687;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#20174;&#36965;&#24863;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20154;&#26426;&#25968;&#25454;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#32467;&#21512;&#22914;&#20309;&#23454;&#29616;&#33258;&#21160;&#21270;&#21644;&#22823;&#35268;&#27169;&#24773;&#20917;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26426;&#36733;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#23454;&#29616;&#26080;&#20154;&#26426;&#33258;&#20027;&#25237;&#36865;&#25937;&#25588;&#29289;&#36164;&#30340;&#38598;&#25104;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29616;&#22330;&#36827;&#34892;&#24555;&#36895;&#21644;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#26512;&#26159;&#21487;&#34892;&#30340;&#65292;&#32780;&#26426;&#36733;&#22270;&#20687;&#22788;&#29702;&#21487;&#20197;&#25552;&#39640;&#26080;&#20154;&#26426;&#25937;&#25588;&#25237;&#36865;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to respond effectively in the aftermath of a disaster, emergency services and relief organizations rely on timely and accurate information about the affected areas. Remote sensing has the potential to significantly reduce the time and effort required to collect such information by enabling a rapid survey of large areas. To achieve this, the main challenge is the automatic extraction of relevant information from remotely sensed data. In this work, we show how the combination of drone-based data with deep learning methods enables automated and large-scale situation assessment. In addition, we demonstrate the integration of onboard image processing techniques for the deployment of autonomous drone-based aid delivery. The results show the feasibility of a rapid and large-scale image analysis in the field, and that onboard image processing can increase the safety of drone-based aid deliveries.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;AI&#39046;&#22495;&#20013;&#30340;&#27714;&#35299;&#22120;&#31454;&#36187;&#32467;&#26524;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#37325;&#37319;&#26679;&#25216;&#26415;&#21457;&#29616;&#65292;&#31454;&#36187;&#32467;&#26524;&#30340;&#25490;&#21517;&#23545;&#22522;&#20934;&#23454;&#20363;&#38598;&#30340;&#24494;&#23567;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#33021;&#26399;&#26395;&#36825;&#20123;&#25490;&#21517;&#33021;&#22815;&#36866;&#29992;&#20110;&#20854;&#20182;&#26679;&#26412;&#12290;&#22240;&#27492;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05062</link><description>&lt;p&gt;
AI&#20013;&#30340;&#31454;&#36187;--&#20351;&#29992;&#32479;&#35745;&#37325;&#37319;&#26679;&#31283;&#23450;&#22320;&#25490;&#21517;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Competitions in AI -- Robustly Ranking Solvers Using Statistical Resampling. (arXiv:2308.05062v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;AI&#39046;&#22495;&#20013;&#30340;&#27714;&#35299;&#22120;&#31454;&#36187;&#32467;&#26524;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#37325;&#37319;&#26679;&#25216;&#26415;&#21457;&#29616;&#65292;&#31454;&#36187;&#32467;&#26524;&#30340;&#25490;&#21517;&#23545;&#22522;&#20934;&#23454;&#20363;&#38598;&#30340;&#24494;&#23567;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#33021;&#26399;&#26395;&#36825;&#20123;&#25490;&#21517;&#33021;&#22815;&#36866;&#29992;&#20110;&#20854;&#20182;&#26679;&#26412;&#12290;&#22240;&#27492;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27714;&#35299;&#22120;&#31454;&#36187;&#22312;&#35780;&#20272;&#21644;&#25512;&#36827;&#35299;&#20915;AI&#21644;&#20854;&#20182;&#38382;&#39064;&#30340;&#26368;&#26032;&#25216;&#26415;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#29305;&#23450;&#31454;&#36187;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#33021;&#22815;&#25512;&#24191;&#21040;&#19982;&#22522;&#20934;&#23454;&#20363;&#38598;&#19981;&#21516;&#30340;&#38382;&#39064;&#38598;&#21512;&#21527;&#65311;&#26412;&#30740;&#31350;&#20351;&#29992;&#32479;&#35745;&#37325;&#37319;&#26679;&#25216;&#26415;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#31454;&#36187;&#32467;&#26524;&#30340;&#26631;&#20934;&#35299;&#37322;&#25152;&#20135;&#29983;&#30340;&#25490;&#21517;&#23545;&#22522;&#20934;&#23454;&#20363;&#38598;&#30340;&#24494;&#23567;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#33021;&#26399;&#26395;&#36825;&#20123;&#25490;&#21517;&#33021;&#22815;&#36866;&#29992;&#20110;&#21516;&#19968;&#22522;&#30784;&#23454;&#20363;&#20998;&#24067;&#30340;&#20854;&#20182;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#35745;&#20998;&#26512;&#31454;&#36187;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solver competitions play a prominent role in assessing and advancing the state of the art for solving many problems in AI and beyond. Notably, in many areas of AI, competitions have had substantial impact in guiding research and applications for many years, and for a solver to be ranked highly in a competition carries considerable weight. But to which extent can we expect competition results to generalise to sets of problem instances different from those used in a particular competition? This is the question we investigate here, using statistical resampling techniques. We show that the rankings resulting from the standard interpretation of competition results can be very sensitive to even minor changes in the benchmark instance set used as the basis for assessment and can therefore not be expected to carry over to other samples from the same underlying instance distribution. To address this problem, we introduce a novel approach to statistically meaningful analysis of competition resul
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;AudioSep&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.05037</link><description>&lt;p&gt;
&#23558;&#20219;&#20309;&#20320;&#25551;&#36848;&#30340;&#20107;&#29289;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Separate Anything You Describe. (arXiv:2308.05037v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05037
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;AudioSep&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26597;&#35810;&#38899;&#39057;&#28304;&#20998;&#31163;&#65288;LASS&#65289;&#26159;&#35745;&#31639;&#21548;&#35273;&#22330;&#26223;&#20998;&#26512;&#65288;CASA&#65289;&#20013;&#30340;&#19968;&#31181;&#26032;&#33539; Paradigm&#12290;LASS&#26088;&#22312;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20174;&#38899;&#39057;&#28151;&#21512;&#29289;&#20013;&#20998;&#31163;&#30446;&#26631;&#22768;&#38899;&#65292;&#20026;&#25968;&#23383;&#38899;&#39057;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#19988;&#21487;&#25193;&#23637;&#30340;&#30028;&#38754;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;LASS&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20998;&#31163;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#20048;&#22120;&#65292;&#26377;&#38480;&#31867;&#21035;&#30340;&#38899;&#39057;&#20107;&#20214;&#65289;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#22312;&#24320;&#25918;&#22495;&#20013;&#20998;&#31163;&#38899;&#39057;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AudioSep&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#35757;&#32451;AudioSep&#65292;&#24182;&#23545;&#20854;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;&#38899;&#39057;&#20107;&#20214;&#20998;&#31163;&#65292;&#20048;&#22120;&#20998;&#31163;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;AudioSep&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#38899;&#39057;&#26631;&#39064;&#25110;&#25991;&#23383;&#26631;&#31614;&#20316;&#20026;&#26597;&#35810;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-queried audio source separation (LASS) is a new paradigm for computational auditory scene analysis (CASA). LASS aims to separate a target sound from an audio mixture given a natural language query, which provides a natural and scalable interface for digital audio applications. Recent works on LASS, despite attaining promising separation performance on specific sources (e.g., musical instruments, limited classes of audio events), are unable to separate audio concepts in the open domain. In this work, we introduce AudioSep, a foundation model for open-domain audio source separation with natural language queries. We train AudioSep on large-scale multimodal datasets and extensively evaluate its capabilities on numerous tasks including audio event separation, musical instrument separation, and speech enhancement. AudioSep demonstrates strong separation performance and impressive zero-shot generalization ability using audio captions or text labels as queries, substantially outperfor
&lt;/p&gt;</description></item><item><title>&#22312;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#30340;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#20013;&#65292;&#20026;&#20102;&#30830;&#20445;&#26368;&#23567;&#38169;&#35823;&#65292;&#38656;&#35201;&#26681;&#25454;&#27169;&#22411;&#32622;&#20449;&#24230;&#35774;&#23450;&#25805;&#20316;&#28857;&#36827;&#34892;&#20915;&#31574;&#22996;&#25176;&#32473;&#19987;&#23478;&#12290;&#20026;&#20102;&#25552;&#39640;&#31995;&#32479;&#25928;&#29992;&#65292;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#20165;&#23545;&#20934;&#30830;&#26679;&#26412;&#20855;&#26377;&#33258;&#20449;&#30340;&#29305;&#28857;&#20197;&#21450;&#23613;&#37327;&#20943;&#23569;&#22996;&#25176;&#32473;&#19987;&#23478;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32622;&#20449;&#24230;&#25805;&#20316;&#29305;&#24615;&#65288;COC&#65289;&#26354;&#32447;&#26469;&#34920;&#31034;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#19987;&#23478;&#22788;&#29702;&#26679;&#26412;&#25968;&#37327;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#36825;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#31561;&#21487;&#29992;&#19987;&#23478;&#26102;&#38388;&#26377;&#38480;&#19988;&#26114;&#36149;&#30340;&#39046;&#22495;&#23588;&#20026;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2308.05035</link><description>&lt;p&gt;
&#19987;&#23478;&#36127;&#36733;&#24456;&#37325;&#35201;&#65306;&#20197;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#20154;&#24037;&#24037;&#20316;&#37327;&#36816;&#34892;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Expert load matters: operating networks at high accuracy and low manual effort. (arXiv:2308.05035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05035
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#30340;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#20013;&#65292;&#20026;&#20102;&#30830;&#20445;&#26368;&#23567;&#38169;&#35823;&#65292;&#38656;&#35201;&#26681;&#25454;&#27169;&#22411;&#32622;&#20449;&#24230;&#35774;&#23450;&#25805;&#20316;&#28857;&#36827;&#34892;&#20915;&#31574;&#22996;&#25176;&#32473;&#19987;&#23478;&#12290;&#20026;&#20102;&#25552;&#39640;&#31995;&#32479;&#25928;&#29992;&#65292;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#20165;&#23545;&#20934;&#30830;&#26679;&#26412;&#20855;&#26377;&#33258;&#20449;&#30340;&#29305;&#28857;&#20197;&#21450;&#23613;&#37327;&#20943;&#23569;&#22996;&#25176;&#32473;&#19987;&#23478;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32622;&#20449;&#24230;&#25805;&#20316;&#29305;&#24615;&#65288;COC&#65289;&#26354;&#32447;&#26469;&#34920;&#31034;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#19987;&#23478;&#22788;&#29702;&#26679;&#26412;&#25968;&#37327;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#36825;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#31561;&#21487;&#29992;&#19987;&#23478;&#26102;&#38388;&#26377;&#38480;&#19988;&#26114;&#36149;&#30340;&#39046;&#22495;&#23588;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#31995;&#32479;&#30340;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#20026;&#20102;&#30830;&#20445;&#26368;&#23567;&#38169;&#35823;&#65292;&#29992;&#25143;&#24212;&#26681;&#25454;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#35774;&#23450;&#25805;&#20316;&#28857;&#65292;&#30830;&#23450;&#20309;&#26102;&#23558;&#20915;&#31574;&#22996;&#25176;&#32473;&#20154;&#31867;&#19987;&#23478;&#12290;&#27169;&#22411;&#32622;&#20449;&#24230;&#20302;&#20110;&#25805;&#20316;&#28857;&#30340;&#26679;&#26412;&#23558;&#30001;&#19987;&#23478;&#25163;&#21160;&#20998;&#26512;&#65292;&#20197;&#36991;&#20813;&#38169;&#35823;&#12290;&#21482;&#26377;&#22312;&#32771;&#34385;&#21040;&#20004;&#20010;&#26041;&#38754;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26679;&#30340;&#31995;&#32479;&#25165;&#33021;&#30495;&#27491;&#26377;&#29992;&#65306;&#27169;&#22411;&#21482;&#23545;&#20934;&#30830;&#30340;&#26679;&#26412;&#20855;&#26377;&#33258;&#20449;&#65292;&#24182;&#19988;&#23613;&#37327;&#20943;&#23569;&#22996;&#25176;&#32473;&#19987;&#23478;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#21518;&#32773;&#23545;&#20110;&#21487;&#29992;&#19987;&#23478;&#26102;&#38388;&#26377;&#38480;&#19988;&#26114;&#36149;&#30340;&#24212;&#29992;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#23588;&#20026;&#20851;&#38190;&#12290;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#22996;&#25176;&#32473;&#19987;&#23478;&#26679;&#26412;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#21487;&#20197;&#29992;&#31867;&#20284;ROC&#26354;&#32447;&#30340;&#26354;&#32447;&#34920;&#31034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32622;&#20449;&#24230;&#25805;&#20316;&#29305;&#24615;&#65288;COC&#65289;&#26354;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#36890;&#36807;&#32771;&#34385;&#21040;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#19987;&#23478;&#22788;&#29702;&#30340;&#26679;&#26412;&#25968;&#37327;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
In human-AI collaboration systems for critical applications, in order to ensure minimal error, users should set an operating point based on model confidence to determine when the decision should be delegated to human experts. Samples for which model confidence is lower than the operating point would be manually analysed by experts to avoid mistakes. Such systems can become truly useful only if they consider two aspects: models should be confident only for samples for which they are accurate, and the number of samples delegated to experts should be minimized. The latter aspect is especially crucial for applications where available expert time is limited and expensive, such as healthcare. The trade-off between the model accuracy and the number of samples delegated to experts can be represented by a curve that is similar to an ROC curve, which we refer to as confidence operating characteristic (COC) curve. In this paper, we argue that deep neural networks should be trained by taking into 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20256;&#32479;&#20132;&#36890;CRM&#21453;&#39304;&#24320;&#21457;&#21644;&#24212;&#29992;&#20132;&#36890;&#20027;&#39064;&#24863;&#30693;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#24320;&#25918;&#24615;&#25991;&#26412;&#21453;&#39304;&#20998;&#31867;&#21040;&#30456;&#20851;&#20132;&#36890;&#20027;&#39064;&#20013;&#12290;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RoBERTa&#26550;&#26500;&#35757;&#32451;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.05012</link><description>&lt;p&gt;
MetRoBERTa&#65306;&#21033;&#29992;&#20256;&#32479;&#30340;&#23458;&#25143;&#20851;&#31995;&#31649;&#29702;&#25968;&#25454;&#24320;&#21457;&#20132;&#36890;&#20027;&#39064;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MetRoBERTa: Leveraging Traditional Customer Relationship Management Data to Develop a Transit-Topic-Aware Language Model. (arXiv:2308.05012v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20256;&#32479;&#20132;&#36890;CRM&#21453;&#39304;&#24320;&#21457;&#21644;&#24212;&#29992;&#20132;&#36890;&#20027;&#39064;&#24863;&#30693;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#24320;&#25918;&#24615;&#25991;&#26412;&#21453;&#39304;&#20998;&#31867;&#21040;&#30456;&#20851;&#20132;&#36890;&#20027;&#39064;&#20013;&#12290;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RoBERTa&#26550;&#26500;&#35757;&#32451;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20056;&#23458;&#22312;&#20056;&#36710;&#35843;&#26597;&#12289;&#23458;&#25143;&#20851;&#31995;&#31649;&#29702;&#65288;CRM&#65289;&#28192;&#36947;&#20197;&#21450;&#26368;&#36817;&#30340;&#31038;&#20132;&#23186;&#20307;&#19978;&#25552;&#20379;&#30340;&#21453;&#39304;&#26159;&#20132;&#36890;&#26426;&#26500;&#26356;&#22909;&#22320;&#35780;&#20272;&#20854;&#26381;&#21153;&#21644;&#35745;&#21010;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#12290;&#36890;&#36807;&#36825;&#20123;&#24037;&#20855;&#20849;&#20139;&#30340;&#21453;&#39304;&#26469;&#33719;&#24471;&#23545;&#20056;&#23458;&#20307;&#39564;&#30340;&#25972;&#20307;&#29702;&#35299;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#25991;&#26412;&#21453;&#39304;&#30340;&#24320;&#25918;&#24615;&#21644;&#38750;&#32467;&#26500;&#21270;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20256;&#32479;&#30340;&#20132;&#36890;CRM&#21453;&#39304;&#26469;&#24320;&#21457;&#21644;&#24212;&#29992;&#33021;&#22815;&#23558;&#24320;&#25918;&#24615;&#25991;&#26412;&#21453;&#39304;&#20998;&#31867;&#21040;&#30456;&#20851;&#30340;&#20132;&#36890;&#29305;&#23450;&#20027;&#39064;&#20013;&#30340;&#20132;&#36890;&#20027;&#39064;&#24863;&#30693;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#21326;&#30427;&#39039;&#37117;&#20250;&#21306;&#20132;&#36890;&#23616;&#65288;WMATA&#65289;&#30340;6&#24180;&#39038;&#23458;&#21453;&#39304;&#35821;&#26009;&#24211;&#20013;&#26816;&#27979;&#21040;&#30340;11&#20010;&#24191;&#27867;&#20132;&#36890;&#20027;&#39064;&#26500;&#24314;&#19968;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#24182;&#20840;&#38754;&#35780;&#20272;&#22522;&#20110;RoBERTa&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transit riders' feedback provided in ridership surveys, customer relationship management (CRM) channels, and in more recent times, through social media is key for transit agencies to better gauge the efficacy of their services and initiatives. Getting a holistic understanding of riders' experience through the feedback shared in those instruments is often challenging, mostly due to the open-ended, unstructured nature of text feedback. In this paper, we propose leveraging traditional transit CRM feedback to develop and deploy a transit-topic-aware large language model (LLM) capable of classifying open-ended text feedback to relevant transit-specific topics. First, we utilize semi-supervised learning to engineer a training dataset of 11 broad transit topics detected in a corpus of 6 years of customer feedback provided to the Washington Metropolitan Area Transit Authority (WMATA). We then use this dataset to train and thoroughly evaluate a language model based on the RoBERTa architecture. 
&lt;/p&gt;</description></item><item><title>AspectMMKG&#26159;&#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#21305;&#37197;&#22270;&#20687;&#21644;&#19981;&#21516;&#23454;&#20307;&#26041;&#38754;&#65292;&#23427;&#25552;&#20379;&#20102;&#20174;&#22810;&#20010;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04992</link><description>&lt;p&gt;
AspectMMKG: &#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities. (arXiv:2308.04992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04992
&lt;/p&gt;
&lt;p&gt;
AspectMMKG&#26159;&#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#21305;&#37197;&#22270;&#20687;&#21644;&#19981;&#21516;&#23454;&#20307;&#26041;&#38754;&#65292;&#23427;&#25552;&#20379;&#20102;&#20174;&#22810;&#20010;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MMKG&#65289;&#32467;&#21512;&#19981;&#21516;&#30340;&#27169;&#24577;&#25968;&#25454;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#65292;&#20197;&#20840;&#38754;&#29702;&#35299;&#23454;&#20307;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;MMKG&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;MMKG&#24573;&#35270;&#20102;&#23454;&#20307;&#30340;&#22810;&#26041;&#38754;&#24615;&#36136;&#65292;&#38480;&#21046;&#20102;&#20174;&#21508;&#31181;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;AspectMMKG&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#30340;MMKG&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#19982;&#19981;&#21516;&#30340;&#23454;&#20307;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#30693;&#35782;&#24211;&#20013;&#25910;&#38598;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#25552;&#21462;&#30693;&#35782;&#24211;&#20013;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#21477;&#23376;&#20316;&#20026;&#26597;&#35810;&#65292;&#20197;&#26816;&#32034;&#22823;&#37327;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;AspectMMKG&#21253;&#21547;2380&#20010;&#23454;&#20307;&#65292;18139&#20010;&#23454;&#20307;&#26041;&#38754;&#21644;645383&#20010;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AspectMMKG&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#65288;EAL&#65289;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#35777;&#26126;&#22312;AspectMMKG&#30340;&#24110;&#21161;&#19979;&#65292;&#20808;&#21069;&#30340;EAL&#27169;&#22411;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text and image) for a comprehensive understanding of entities. Despite the recent progress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature of entities, limiting the ability to comprehend entities from various perspectives. In this paper, we construct AspectMMKG, the first MMKG with aspect-related images by matching images to different entity aspects. Specifically, we collect aspect-related images from a knowledge base, and further extract aspect-related sentences from the knowledge base as queries to retrieve a large number of aspect-related images via an online image search engine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and 645,383 aspect-related images. We demonstrate the usability of AspectMMKG in entity aspect linking (EAL) downstream task and show that previous EAL models achieve a new state-of-the-art performance with the help of AspectMMKG. To facilitate the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#25552;&#28860;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#24378;&#24230;&#21644;&#36328;&#26550;&#26500;&#27867;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#33391;&#22909;&#65292;&#24182;&#32771;&#34385;&#20102;&#25968;&#25454;&#25688;&#35201;&#30340;&#35821;&#35328;&#29305;&#23450;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04982</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#35821;&#35328;&#25991;&#26412;&#25968;&#25454;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Exploring Multilingual Text Data Distillation. (arXiv:2308.04982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#25552;&#28860;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#24378;&#24230;&#21644;&#36328;&#26550;&#26500;&#27867;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#33391;&#22909;&#65292;&#24182;&#32771;&#34385;&#20102;&#25968;&#25454;&#25688;&#35201;&#30340;&#35821;&#35328;&#29305;&#23450;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20852;&#36215;&#65292;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22797;&#26434;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#24120;&#35265;&#30340;&#38656;&#27714;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25968;&#25454;&#25552;&#28860;&#25216;&#26415;&#24212;&#36816;&#32780;&#29983;&#65292;&#33021;&#22815;&#29992;&#26356;&#20302;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#35201;&#27714;&#24555;&#36895;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25968;&#25454;&#25552;&#28860;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#22240;&#20026;&#20854;&#31163;&#25955;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#25552;&#28860;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#27867;&#21270;&#21040;&#26032;&#30340;&#26550;&#26500;&#19978;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#25552;&#28860;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#20854;&#22312;&#20998;&#31867;&#24378;&#24230;&#21644;&#36328;&#26550;&#26500;&#27867;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#25688;&#35201;&#30340;&#35821;&#35328;&#29305;&#23450;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#29616;&#26377;&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#65292;&#22686;&#24378;&#20102;&#25991;&#26412;&#25968;&#25454;&#25552;&#28860;&#20013;&#30340;&#36328;&#26550;&#26500;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of deep learning, large datasets and complex models have become common, requiring significant computing power. To address this, data distillation has emerged as a technique to quickly train models with lower memory and time requirements. However, data distillation on text-based datasets hasn't been explored much because of the challenges rising due to its discrete nature. Additionally, existing dataset distillation methods often struggle to generalize to new architectures. In the paper, we propose several data distillation techniques for multilingual text classification datasets using language-model-based learning methods. We conduct experiments to analyze their performance in terms of classification strength, and cross-architecture generalization. Furthermore, we investigate the language-specific fairness of the data summaries generated by these methods. Our approach builds upon existing techniques, enhancing cross-architecture generalization in the text data distillatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36895;&#24230;&#21644;&#22402;&#30452;&#26426;&#21160;&#25163;&#27573;&#65292;&#22312;&#39640;&#23494;&#24230;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#33258;&#20027;&#30340;&#33258;&#25105;&#20998;&#31163;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#20027;&#20998;&#31163;&#20445;&#38556;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.04958</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#33258;&#20027;&#20998;&#31163;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks. (arXiv:2308.04958v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36895;&#24230;&#21644;&#22402;&#30452;&#26426;&#21160;&#25163;&#27573;&#65292;&#22312;&#39640;&#23494;&#24230;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#33258;&#20027;&#30340;&#33258;&#25105;&#20998;&#31163;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#20027;&#20998;&#31163;&#20445;&#38556;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#31354;&#20013;&#31227;&#21160;&#65288;AAM&#65289;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#39640;&#25928;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#21033;&#29992;&#36710;&#36742;&#33258;&#20027;&#24615;&#21644;&#30005;&#21160;&#39134;&#26426;&#65292;&#22312;&#20043;&#21069;&#26410;&#24471;&#21040;&#20805;&#20998;&#26381;&#21153;&#30340;&#24066;&#22330;&#20043;&#38388;&#25552;&#20379;&#36234;&#26469;&#36234;&#33258;&#20027;&#30340;&#20132;&#36890;&#12290;&#36890;&#36807;&#39640;&#23494;&#24230;&#29615;&#22659;&#20013;&#20302;&#31354;&#39134;&#34892;&#22120;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#23548;&#33322;&#65292;&#38656;&#35201;&#25972;&#21512;&#22823;&#37327;&#22797;&#26434;&#35266;&#27979;&#25968;&#25454;&#65292;&#22914;&#30417;&#35270;&#65292;&#36710;&#36742;&#21160;&#21147;&#23398;&#30693;&#35782;&#21644;&#22825;&#27668;&#12290;&#22312;&#22788;&#29702;&#21644;&#25512;&#29702;&#36825;&#20123;&#35266;&#27979;&#25968;&#25454;&#26102;&#65292;&#38754;&#20020;&#30528;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#30340;&#22810;&#20010;&#26469;&#28304;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#35201;&#30830;&#20445;&#19982;&#31354;&#22495;&#20013;&#21487;&#21464;&#25968;&#37327;&#30340;&#39134;&#26426;&#21512;&#20316;&#12290;&#36825;&#20123;&#25361;&#25112;&#21152;&#19978;&#38656;&#35201;&#23454;&#26102;&#20570;&#20986;&#23433;&#20840;&#20851;&#38190;&#20915;&#31574;&#30340;&#35201;&#27714;&#65292;&#20351;&#24471;&#20256;&#32479;&#30340;&#20998;&#31163;&#20445;&#38556;&#25216;&#26415;&#26080;&#27861;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;AAM&#36208;&#24266;&#20869;&#20351;&#29992;&#36895;&#24230;&#21644;&#22402;&#30452;&#26426;&#21160;&#25163;&#27573;&#25552;&#20379;&#33258;&#20027;&#30340;&#33258;&#25105;&#20998;&#31163;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced Air Mobility (AAM) introduces a new, efficient mode of transportation with the use of vehicle autonomy and electrified aircraft to provide increasingly autonomous transportation between previously underserved markets. Safe and efficient navigation of low altitude aircraft through highly dense environments requires the integration of a multitude of complex observations, such as surveillance, knowledge of vehicle dynamics, and weather. The processing and reasoning on these observations pose challenges due to the various sources of uncertainty in the information while ensuring cooperation with a variable number of aircraft in the airspace. These challenges coupled with the requirement to make safety-critical decisions in real-time rule out the use of conventional separation assurance techniques. We present a decentralized reinforcement learning framework to provide autonomous self-separation capabilities within AAM corridors with the use of speed and vertical maneuvers. The probl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#32447;&#20379;&#30005;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#37319;&#38598;-&#24863;&#30693;-&#35757;&#32451;-&#20256;&#36755;&#21327;&#35758;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21151;&#29575;&#20256;&#36755;&#12289;&#20256;&#36755;&#21151;&#29575;&#20998;&#37197;&#12289;&#25968;&#25454;&#24863;&#30693;&#21644;&#27169;&#22411;&#35757;&#32451;&#21442;&#25968;&#31561;&#26041;&#24335;&#26469;&#26368;&#23567;&#21270;&#24635;&#23436;&#25104;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.04953</link><description>&lt;p&gt;
&#26080;&#32447;&#20379;&#30005;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#65306;&#32852;&#21512;&#21151;&#29575;&#20256;&#36755;&#12289;&#25968;&#25454;&#24863;&#30693;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Wirelessly Powered Federated Learning Networks: Joint Power Transfer, Data Sensing, Model Training, and Resource Allocation. (arXiv:2308.04953v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#32447;&#20379;&#30005;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#37319;&#38598;-&#24863;&#30693;-&#35757;&#32451;-&#20256;&#36755;&#21327;&#35758;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21151;&#29575;&#20256;&#36755;&#12289;&#20256;&#36755;&#21151;&#29575;&#20998;&#37197;&#12289;&#25968;&#25454;&#24863;&#30693;&#21644;&#27169;&#22411;&#35757;&#32451;&#21442;&#25968;&#31561;&#26041;&#24335;&#26469;&#26368;&#23567;&#21270;&#24635;&#23436;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#28982;&#32780;&#65292;FL&#30340;&#23454;&#26045;&#21463;&#21040;&#31227;&#21160;&#35774;&#22791;&#65288;MDs&#65289;&#30340;&#33021;&#28304;&#38480;&#21046;&#21644;MDs&#19978;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#12290;&#22914;&#20309;&#23558;&#26080;&#32447;&#33021;&#37327;&#20256;&#36755;&#21644;&#31227;&#21160;&#20247;&#21253;&#24863;&#30693;&#25972;&#21512;&#21040;&#21487;&#25345;&#32493;&#30340;FL&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#26159;&#24320;&#25918;&#25991;&#29486;&#20013;&#23436;&#20840;&#32570;&#22833;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#21327;&#20316;&#24863;&#30693;&#36741;&#21161;&#21487;&#25345;&#32493;FL&#65288;S2FL&#65289;&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#24635;&#23436;&#25104;&#26102;&#38388;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#37319;&#38598;-&#24863;&#30693;-&#35757;&#32451;-&#20256;&#36755;&#21327;&#35758;&#65292;&#22312;&#36825;&#20010;&#21327;&#35758;&#20013;&#65292;&#33021;&#37327;&#26377;&#38480;&#30340;MDs&#39318;&#20808;&#20174;&#23556;&#39057;&#20449;&#21495;&#20013;&#25910;&#38598;&#33021;&#37327;&#65292;&#29992;&#26469;&#22870;&#21169;&#29992;&#25143;&#21442;&#19982;&#65292;&#20174;&#29615;&#22659;&#20013;&#24863;&#30693;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;MDs&#19978;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#24182;&#23558;&#27169;&#22411;&#26356;&#26032;&#20256;&#36755;&#32473;&#26381;&#21153;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21151;&#29575;&#20256;&#36755;&#12289;&#20256;&#36755;&#21151;&#29575;&#20998;&#37197;&#12289;&#25968;&#25454;&#24863;&#30693;&#65292;&#20197;&#21450;&#27169;&#22411;&#35757;&#32451;&#21442;&#25968;&#31561;&#26041;&#24335;&#26469;&#26368;&#23567;&#21270;&#24635;&#23436;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has found many successes in wireless networks; however, the implementation of FL has been hindered by the energy limitation of mobile devices (MDs) and the availability of training data at MDs. How to integrate wireless power transfer and mobile crowdsensing towards sustainable FL solutions is a research topic entirely missing from the open literature. This work for the first time investigates a resource allocation problem in collaborative sensing-assisted sustainable FL (S2FL) networks with the goal of minimizing the total completion time. We investigate a practical harvesting-sensing-training-transmitting protocol in which energy-limited MDs first harvest energy from RF signals, use it to gain a reward for user participation, sense the training data from the environment, train the local models at MDs, and transmit the model updates to the server. The total completion time minimization problem of jointly optimizing power transfer, transmit power allocation, dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20856;&#22411;&#26680;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;&#65292;&#35299;&#20915;&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#34920;&#31034;&#20998;&#21106;&#21644;&#23884;&#20837;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20998;&#21106;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#26680;&#20197;&#21450;&#20856;&#22411;&#23398;&#20064;&#21644;&#21069;&#26223;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04952</link><description>&lt;p&gt;
&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#20856;&#22411;&#26680;&#23398;&#20064;&#19982;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation. (arXiv:2308.04952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20856;&#22411;&#26680;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;&#65292;&#35299;&#20915;&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#34920;&#31034;&#20998;&#21106;&#21644;&#23884;&#20837;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20998;&#21106;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#26680;&#20197;&#21450;&#20856;&#22411;&#23398;&#20064;&#21644;&#21069;&#26223;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#65288;GFSS&#65289;&#23558;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#65288;FSS&#65289;&#25193;&#23637;&#21040;&#35780;&#20272;&#36807;&#31243;&#20013;&#21516;&#26102;&#20998;&#21106;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#24050;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#39069;&#22806;&#30340;&#20998;&#25903;&#25110;&#20856;&#22411;&#32858;&#21512;&#26469;&#28040;&#38500;FSS&#30340;&#32422;&#26463;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#34920;&#31034;&#20998;&#21106;&#21644;&#23884;&#20837;&#20559;&#35265;&#65292;&#20005;&#37325;&#24433;&#21709;GFSS&#30340;&#24615;&#33021;&#65292;&#23578;&#26410;&#32508;&#21512;&#32771;&#34385;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20856;&#22411;&#26680;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#26680;&#26469;&#23545;&#27599;&#20010;&#31867;&#21035;&#36827;&#34892;&#20998;&#21106;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20856;&#22411;&#23398;&#20064;&#19982;&#22522;&#31867;&#26680;&#30340;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#36825;&#19982;&#23569;&#26679;&#26412;&#26032;&#31867;&#21035;&#30340;&#21407;&#22411;&#30693;&#35782;&#32858;&#21512;&#30456;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#19982;&#26465;&#20214;&#20559;&#24046;&#22522;&#20110;&#25512;&#29702;&#30340;&#21069;&#26223;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22359;&#65292;&#29992;&#20110;&#25191;&#34892;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot Semantic Segmentation (FSS) to simultaneously segment unseen classes and seen classes during evaluation. Previous works leverage additional branch or prototypical aggregation to eliminate the constrained setting of FSS. However, representation division and embedding prejudice, which heavily results in poor performance of GFSS, have not been synthetical considered. We address the aforementioned problems by jointing the prototypical kernel learning and open-set foreground perception. Specifically, a group of learnable kernels is proposed to perform segmentation with each kernel in charge of a stuff class. Then, we explore to merge the prototypical learning to the update of base-class kernels, which is consistent with the prototype knowledge aggregation of few-shot novel classes. In addition, a foreground contextual perception module cooperating with conditional bias based inference is adopted to perform class-agnostic as 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#32780;&#20840;&#38754;&#22320;&#25551;&#36848;&#20102;&#20174;&#19981;&#21516;&#26469;&#28304;&#33719;&#24471;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#24110;&#21161;&#29702;&#35299;&#32929;&#31080;&#24066;&#22330;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04947</link><description>&lt;p&gt;
&#33719;&#24471;&#21644;&#25972;&#21512;&#30693;&#35782;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Methods for Acquiring and Incorporating Knowledge into Stock Price Prediction: A Survey. (arXiv:2308.04947v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04947
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#32780;&#20840;&#38754;&#22320;&#25551;&#36848;&#20102;&#20174;&#19981;&#21516;&#26469;&#28304;&#33719;&#24471;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#24110;&#21161;&#29702;&#35299;&#32929;&#31080;&#24066;&#22330;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#22240;&#20026;&#32929;&#31080;&#24066;&#22330;&#30340;&#22266;&#26377;&#27874;&#21160;&#24615;&#21644;&#38750;&#32447;&#24615;&#24615;&#36136;&#12290;&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#29702;&#35299;&#32929;&#31080;&#24066;&#22330;&#30340;&#30693;&#35782;&#22686;&#24378;&#22411;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#22312;&#20174;&#22806;&#37096;&#30693;&#35782;&#31867;&#22411;&#30340;&#35282;&#24230;&#31995;&#32479;&#22320;&#32508;&#21512;&#20197;&#24448;&#30740;&#31350;&#26041;&#38754;&#65292;&#23398;&#26415;&#20316;&#21697;&#30340;&#31232;&#32570;&#24615;&#23384;&#22312;&#19968;&#23450;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22806;&#37096;&#30693;&#35782;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#25968;&#25454;&#32467;&#26500;&#24314;&#27169;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#38750;&#22270;&#24418;&#21270;&#26684;&#24335;&#21644;&#22270;&#24418;&#21270;&#26684;&#24335;&#20004;&#31867;&#65306;1) &#38750;&#22270;&#24418;&#21270;&#30693;&#35782;&#25429;&#33719;&#19982;&#20010;&#21035;&#32929;&#31080;&#23494;&#20999;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#22810;&#23186;&#20307;&#25551;&#36848;&#65307;2) &#22270;&#24418;&#21270;&#30693;&#35782;&#25429;&#33719;&#32929;&#31080;&#24066;&#22330;&#20013;&#30456;&#20114;&#20851;&#32852;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20449;&#24687;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#26088;&#22312;&#31995;&#32479;&#32780;&#20840;&#38754;&#22320;&#25551;&#36848;&#20174;&#19981;&#21516;&#26469;&#28304;&#33719;&#21462;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting stock prices presents a challenging research problem due to the inherent volatility and non-linear nature of the stock market. In recent years, knowledge-enhanced stock price prediction methods have shown groundbreaking results by utilizing external knowledge to understand the stock market. Despite the importance of these methods, there is a scarcity of scholarly works that systematically synthesize previous studies from the perspective of external knowledge types. Specifically, the external knowledge can be modeled in different data structures, which we group into non-graph-based formats and graph-based formats: 1) non-graph-based knowledge captures contextual information and multimedia descriptions specifically associated with an individual stock; 2) graph-based knowledge captures interconnected and interdependent information in the stock market. This survey paper aims to provide a systematic and comprehensive description of methods for acquiring external knowledge from va
&lt;/p&gt;</description></item><item><title>LLMeBench&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21487;&#20197;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#65292;&#25903;&#25345;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#12290;&#24050;&#32463;&#22312;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#35745;&#21010;&#23558;&#26694;&#26550;&#24320;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.04945</link><description>&lt;p&gt;
LLMeBench&#65306;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#30340;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking. (arXiv:2308.04945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04945
&lt;/p&gt;
&lt;p&gt;
LLMeBench&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21487;&#20197;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#65292;&#25903;&#25345;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#12290;&#24050;&#32463;&#22312;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#35745;&#21010;&#23558;&#26694;&#26550;&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#25104;&#21151;&#20351;&#24471;&#38656;&#35201;&#35780;&#20272;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#24182;&#20844;&#24320;&#20102;&#20960;&#20010;&#26694;&#26550;&#65292;&#20294;&#23545;&#20110;&#19981;&#21516;&#29992;&#25143;&#26469;&#35828;&#65292;&#23427;&#20204;&#23545;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#23450;&#21046;&#33021;&#21147;&#36890;&#24120;&#24456;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMeBench&#26694;&#26550;&#12290;&#26368;&#21021;&#26159;&#20026;&#20102;&#20351;&#29992;OpenAI&#30340;GPT&#21644;BLOOM&#27169;&#22411;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#32780;&#24320;&#21457;&#30340;&#65307;&#23427;&#21487;&#20197;&#26080;&#32541;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#22914;&#20309;&#12290;&#35813;&#26694;&#26550;&#36824;&#20855;&#26377;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#12290;&#21487;&#20197;&#22312;&#19981;&#21040;10&#20998;&#38047;&#20869;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#24049;&#30340;&#27169;&#22411;API&#23494;&#38053;&#26469;&#35780;&#20272;&#24403;&#21069;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#24050;&#32463;&#22312;90&#20010;&#23454;&#39564;&#35774;&#32622;&#20013;&#20351;&#29992;53&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#23545;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28041;&#21450;&#22823;&#32422;296K&#20010;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#35745;&#21010;&#23558;&#35813;&#26694;&#26550;&#24320;&#28304;&#20379;&#31038;&#21306;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework. Initially developed to evaluate Arabic NLP tasks using OpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task and model, regardless of language. The framework also features zero- and few-shot learning settings. A new custom dataset can be added in less than 10 minutes, and users can use their own model API keys to evaluate the task at hand. The developed framework has been already tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We plan to open-source the framework for the community
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#31574;&#30053;&#30340;&#29305;&#24449;&#20540;&#20998;&#37327;&#36873;&#25321;&#65292;&#23547;&#25214;&#26368;&#20248;&#23376;&#38598;&#20197;&#25552;&#39640;&#24615;&#33021;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.04944</link><description>&lt;p&gt;
&#20351;&#29992;&#36138;&#23146;&#29305;&#24449;&#20540;&#20998;&#37327;&#36873;&#25321;&#30340;&#39640;&#26031;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Gaussian Image Anomaly Detection with Greedy Eigencomponent Selection. (arXiv:2308.04944v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#31574;&#30053;&#30340;&#29305;&#24449;&#20540;&#20998;&#37327;&#36873;&#25321;&#65292;&#23547;&#25214;&#26368;&#20248;&#23376;&#38598;&#20197;&#25552;&#39640;&#24615;&#33021;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#27491;&#24120;&#24773;&#20917;&#26126;&#26174;&#20559;&#31163;&#30340;&#22320;&#26041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#37319;&#29992;EfficientNet&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29305;&#24449;&#20540;&#20998;&#37327;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#37319;&#29992;&#36138;&#23146;&#31574;&#30053;&#30340;&#26641;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#20540;&#20998;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#30340;&#23454;&#39564;&#26469;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#31532;&#19968;&#20010;&#23454;&#39564;&#25506;&#32034;&#20102;&#27979;&#35797;&#38598;&#24615;&#33021;&#23545;&#29305;&#24449;&#20540;&#20998;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#31532;&#20108;&#20010;&#23454;&#39564;&#30740;&#31350;&#20102;&#24403;&#25105;&#20204;&#22312;&#19968;&#20010;&#24322;&#24120;&#31867;&#22411;&#19978;&#36827;&#34892;&#35757;&#32451;&#24182;&#22312;&#25152;&#26377;&#20854;&#20182;&#31867;&#22411;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#30340;&#24615;&#33021;&#65292;&#31532;&#19977;&#20010;&#23454;&#39564;&#35843;&#26597;&#20102;&#20351;&#29992;&#26368;&#23569;&#25968;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#24182;&#26681;&#25454;&#24322;&#24120;&#31867;&#22411;&#36873;&#25321;&#36825;&#20123;&#22270;&#20687;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25214;&#21040;&#26368;&#20248;&#30340;&#20998;&#37327;&#23376;&#38598;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#30340;&#24615;&#33021;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) in images, identifying significant deviations from normality, is a critical issue in computer vision. This paper introduces a novel approach to dimensionality reduction for AD using pre-trained convolutional neural network (CNN) that incorporate EfficientNet models. We investigate the importance of component selection and propose two types of tree search approaches, both employing a greedy strategy, for optimal eigencomponent selection. Our study conducts three main experiments to evaluate the effectiveness of our approach. The first experiment explores the influence of test set performance on component choice, the second experiment examines the performance when we train on one anomaly type and evaluate on all other types, and the third experiment investigates the impact of using a minimum number of images for training and selecting them based on anomaly types. Our approach aims to find the optimal subset of components that deliver the highest performance score, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#21644;&#35821;&#20041;&#27807;&#36890;&#65288;SemCom&#65289;&#65292;&#20197;&#20135;&#29983;&#26377;&#24847;&#20041;&#21644;&#25928;&#26524;&#30340;&#20869;&#23481;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#37319;&#29992;AIGC&#25216;&#26415;&#20316;&#20026;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#35821;&#20041;&#25552;&#21462;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04942</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#35821;&#20041;&#27807;&#36890;&#23545;&#20110;&#26377;&#25928;&#30340;&#20869;&#23481;&#21019;&#36896;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Semantic Communications for Artificial Intelligence Generated Content (AIGC) Toward Effective Content Creation. (arXiv:2308.04942v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#21644;&#35821;&#20041;&#27807;&#36890;&#65288;SemCom&#65289;&#65292;&#20197;&#20135;&#29983;&#26377;&#24847;&#20041;&#21644;&#25928;&#26524;&#30340;&#20869;&#23481;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#37319;&#29992;AIGC&#25216;&#26415;&#20316;&#20026;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#35821;&#20041;&#25552;&#21462;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26381;&#21153;&#22312;&#25968;&#23383;&#20869;&#23481;&#21019;&#36896;&#39046;&#22495;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;AIGC&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#26368;&#23567;&#36755;&#20837;&#30340;&#20869;&#23481;&#29983;&#25104;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#35821;&#20041;&#27807;&#36890;&#65288;SemCom&#65289;&#30456;&#32467;&#21512;&#26102;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;AIGC&#21644;SemCom&#30340;&#38598;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#35821;&#20041;&#23618;&#20043;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#20869;&#23481;&#29983;&#25104;&#23618;&#65292;&#28165;&#26224;&#22320;&#27010;&#36848;&#20102;AIGC&#21644;SemCom&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#20197;&#20135;&#29983;&#26377;&#24847;&#20041;&#21644;&#26377;&#25928;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#37319;&#29992;AIGC&#25216;&#26415;&#20316;&#20026;&#35821;&#20041;&#20449;&#24687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#26032;&#26694;&#26550;&#65292;&#32771;&#34385;&#21040;&#38024;&#23545;AIGC&#26381;&#21153;&#23450;&#21046;&#30340;&#35821;&#20041;&#25552;&#21462;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#32852;&#21512;&#20248;&#21270;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#20869;&#23481;&#12289;&#25152;&#38656;&#30340;&#36136;&#37327;&#21644;&#25152;&#21033;&#29992;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36890;&#36807;&#37319;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#65292;&#23545;&#20855;&#20307;&#26696;&#20363;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Generated Content (AIGC) Services have significant potential in digital content creation. The distinctive abilities of AIGC, such as content generation based on minimal input, hold huge potential, especially when integrating with semantic communication (SemCom). In this paper, a novel comprehensive conceptual model for the integration of AIGC and SemCom is developed. Particularly, a content generation level is introduced on top of the semantic level that provides a clear outline of how AIGC and SemCom interact with each other to produce meaningful and effective content. Moreover, a novel framework that employs AIGC technology is proposed as an encoder and decoder for semantic information, considering the joint optimization of semantic extraction and evaluation metrics tailored to AIGC services. The framework can adapt to different types of content generated, the required quality, and the semantic information utilized. By employing a Deep Q Network (DQN), a case 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#20351;&#29992;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#36827;&#34892;&#36890;&#20449;&#23398;&#20064;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#20808;&#36827;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04938</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#36827;&#34892;&#36890;&#20449;&#23398;&#20064;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An In-Depth Analysis of Discretization Methods for Communication Learning using Backpropagation with Multi-Agent Reinforcement Learning. (arXiv:2308.04938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#20351;&#29992;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#36827;&#34892;&#36890;&#20449;&#23398;&#20064;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#20808;&#36827;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26234;&#33021;&#20307;&#26080;&#27861;&#35266;&#23519;&#21040;&#23436;&#25972;&#30340;&#29615;&#22659;&#29366;&#24577;&#26102;&#65292;&#36890;&#20449;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20801;&#35768;&#26234;&#33021;&#20307;&#20043;&#38388;&#23398;&#20064;&#36890;&#20449;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#36890;&#20449;&#36890;&#36947;&#65292;&#20197;&#20415;&#26799;&#24230;&#33021;&#22815;&#20316;&#20026;&#21453;&#39304;&#27969;&#21160;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#24819;&#35201;&#20351;&#29992;&#31163;&#25955;&#28040;&#24687;&#26469;&#20943;&#23567;&#28040;&#24687;&#30340;&#22823;&#23567;&#26102;&#65292;&#36825;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#26799;&#24230;&#19981;&#33021;&#36890;&#36807;&#31163;&#25955;&#30340;&#36890;&#20449;&#36890;&#36947;&#20256;&#25773;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#36890;&#20449;&#23398;&#20064;&#26550;&#26500;&#21644;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20351;&#24471;&#24456;&#38590;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#26799;&#24230;&#36827;&#34892;&#36890;&#20449;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;COMA-DIAL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is crucial in multi-agent reinforcement learning when agents are not able to observe the full state of the environment. The most common approach to allow learned communication between agents is the use of a differentiable communication channel that allows gradients to flow between agents as a form of feedback. However, this is challenging when we want to use discrete messages to reduce the message size, since gradients cannot flow through a discrete communication channel. Previous work proposed methods to deal with this problem. However, these methods are tested in different communication learning architectures and environments, making it hard to compare them. In this paper, we compare several state-of-the-art discretization methods as well as a novel approach. We do this comparison in the context of communication learning using gradients from other agents and perform tests on several environments. In addition, we present COMA-DIAL, a communication learning approach based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32511;&#33394;Metaverse&#20013;&#30340;&#26381;&#21153;&#39044;&#35746;&#21644;&#23450;&#20215;&#38382;&#39064;&#65292;&#36890;&#36807;Stackelberg&#21338;&#24328;&#26041;&#27861;&#35299;&#20915;&#20102;MSP&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#32463;&#27982;&#20914;&#31361;&#65292;&#23454;&#29616;&#20102;&#33021;&#25928;&#30340;&#26381;&#21153;&#25552;&#20379;&#12290;</title><link>http://arxiv.org/abs/2308.04914</link><description>&lt;p&gt;
&#32511;&#33394;Metaverses&#30340;&#26381;&#21153;&#39044;&#35746;&#21644;&#23450;&#20215;&#65306;&#19968;&#31181;Stackelberg&#21338;&#24328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Service Reservation and Pricing for Green Metaverses: A Stackelberg Game Approach. (arXiv:2308.04914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32511;&#33394;Metaverse&#20013;&#30340;&#26381;&#21153;&#39044;&#35746;&#21644;&#23450;&#20215;&#38382;&#39064;&#65292;&#36890;&#36807;Stackelberg&#21338;&#24328;&#26041;&#27861;&#35299;&#20915;&#20102;MSP&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#32463;&#27982;&#20914;&#31361;&#65292;&#23454;&#29616;&#20102;&#33021;&#25928;&#30340;&#26381;&#21153;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metaverse&#36890;&#36807;&#25968;&#23383;&#21270;&#30340;&#34394;&#25311;&#35282;&#33394;&#20351;&#29992;&#25143;&#33021;&#22815;&#30456;&#20114;&#20132;&#27969;&#12289;&#21512;&#20316;&#21644;&#31038;&#20132;&#12290;&#30001;&#20110;&#26102;&#31354;&#29305;&#24615;&#65292;&#36890;&#36807;&#21327;&#21516;&#26041;&#24335;&#25191;&#34892;&#36719;&#20214;&#32452;&#20214;&#21487;&#20197;&#20026;&#20849;&#21516;&#23450;&#20301;&#30340;&#29992;&#25143;&#25552;&#20379;&#33391;&#22909;&#30340;&#26381;&#21153;&#65292;&#20174;&#32780;&#20943;&#23569;&#20887;&#20313;&#30340;&#25968;&#25454;&#20256;&#36755;&#21644;&#22788;&#29702;&#65292;&#26368;&#32456;&#20943;&#23569;&#24635;&#33021;&#32791;&#12290;&#33021;&#25928;&#30340;&#26381;&#21153;&#25552;&#20379;&#23545;&#20110;&#23454;&#29616;&#32511;&#33394;&#21644;&#21487;&#25345;&#32493;&#30340;Metaverse&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20197;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#24212;&#29992;&#31243;&#24207;&#20026;&#20363;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#32463;&#27982;&#38382;&#39064;&#65292;&#21363;&#29992;&#25143;&#22914;&#20309;&#39044;&#35746;MSP&#30340;&#21368;&#36733;&#26381;&#21153;&#65292;&#20197;&#21450;MSP&#22914;&#20309;&#30830;&#23450;&#26368;&#20248;&#25910;&#36153;&#20215;&#26684;&#65292;&#22240;&#20026;&#27599;&#20010;&#29992;&#25143;&#37117;&#20250;&#26681;&#25454;&#36135;&#24065;&#25104;&#26412;&#26469;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#21368;&#36733;&#26381;&#21153;&#12290;&#22312;MSP&#21644;&#29992;&#25143;&#20043;&#38388;&#24314;&#31435;&#20102;&#21333;&#20027;&#23548;&#22810;&#20174;&#23646;&#30340;Stackelberg&#21338;&#24328;&#65292;&#32780;&#27599;&#20010;&#29992;&#25143;&#37117;&#22312;&#20248;&#21270;&#21368;&#36733;&#26381;&#21153;&#26102;&#21516;&#26102;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaverse enables users to communicate, collaborate and socialize with each other through their digital avatars. Due to the spatio-temporal characteristics, co-located users are served well by performing their software components in a collaborative manner such that a Metaverse service provider (MSP) eliminates redundant data transmission and processing, ultimately reducing the total energy consumption. The energyefficient service provision is crucial for enabling the green and sustainable Metaverse. In this article, we take an augmented reality (AR) application as an example to achieve this goal. Moreover, we study an economic issue on how the users reserve offloading services from the MSP and how the MSP determines an optimal charging price since each user is rational to decide whether to accept the offloading service by taking into account the monetary cost. A single-leader multi-follower Stackelberg game is formulated between the MSP and users while each user optimizes an offloading
&lt;/p&gt;</description></item><item><title>LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2308.04913</link><description>&lt;p&gt;
LLaMA-E&#65306;&#22810;&#26041;&#38754;&#25351;&#23548;&#19979;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#22686;&#24378;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following. (arXiv:2308.04913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04913
&lt;/p&gt;
&lt;p&gt;
LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#28041;&#21450;&#21019;&#24314;&#21560;&#24341;&#20154;&#12289;&#20016;&#23500;&#19988;&#26377;&#38024;&#23545;&#24615;&#30340;&#20419;&#38144;&#20869;&#23481;&#65292;&#20197;&#25512;&#21160;&#20135;&#21697;&#38144;&#21806;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#20363;&#65292;&#20026;&#35299;&#20915;&#36825;&#31181;&#24773;&#26223;&#20013;&#30340;&#21508;&#31181;&#21019;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36890;&#29992;&#35821;&#26009;&#24211;&#21644;&#24120;&#35782;&#30693;&#35782;&#35757;&#32451;&#30340;&#20027;&#27969;LLM&#22312;&#36866;&#24212;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#21644;&#23458;&#25143;&#29420;&#29305;&#30340;&#22797;&#26434;&#21644;&#20010;&#24615;&#21270;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#20687;GPT-3.5&#36825;&#26679;&#30340;LLM&#38656;&#35201;&#36827;&#34892;&#36828;&#31243;&#35775;&#38382;&#65292;&#24341;&#21457;&#20102;&#22312;&#20256;&#36755;&#36807;&#31243;&#20013;&#20445;&#25252;&#22823;&#37327;&#23458;&#25143;&#38544;&#31169;&#25968;&#25454;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-E&#65292;&#38024;&#23545;&#22810;&#26679;&#21270;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39046;&#22495;&#19987;&#23478;&#20174;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#21019;&#24314;&#20102;&#31181;&#23376;&#25351;&#23548;&#38598;&#21512;&#12290;&#36825;&#20123;&#20219;&#21153;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
E-commerce authoring involves creating attractive, abundant, and targeted promotional content to drive product sales. The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario. However, mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission. This paper proposes the LLaMA-E, the unified and customized instruction-following language models focusing on diverse e-commerce authoring tasks. Specifically, the domain experts create the seed instruction set from the tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general Q&amp;A. These tasks enabl
&lt;/p&gt;</description></item><item><title>&#22312;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;SLPT&#65292;&#23558;&#36873;&#25321;&#24615;&#26631;&#27880;&#21644;&#25552;&#31034;&#35843;&#25972;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.04911</link><description>&lt;p&gt;
SLPT&#65306;&#36873;&#25321;&#24615;&#26631;&#27880;&#19982;&#25552;&#31034;&#35843;&#25972;&#22312;&#26377;&#38480;&#26631;&#27880;&#30340;&#30149;&#21464;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation. (arXiv:2308.04911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04911
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;SLPT&#65292;&#23558;&#36873;&#25321;&#24615;&#26631;&#27880;&#21644;&#25552;&#31034;&#35843;&#25972;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#24120;&#24120;&#38754;&#20020;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#21644;&#39640;&#26114;&#30340;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;&#22312;&#26377;&#38480;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#23545;&#25972;&#20010;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19981;&#20339;&#12290;&#26368;&#36817;&#65292;&#25552;&#31034;&#35843;&#25972;&#20316;&#20026;&#19968;&#31181;&#26356;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#20986;&#29616;&#65292;&#23427;&#21521;&#29420;&#31435;&#20110;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20123;&#39069;&#22806;&#30340;&#21487;&#35843;&#21442;&#25968;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#20165;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#26356;&#26032;&#36825;&#20123;&#21442;&#25968;&#65292;&#32780;&#20445;&#25345;&#39044;&#35757;&#32451;&#27169;&#22411;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#36873;&#25321;&#24615;&#26631;&#27880;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#23427;&#26088;&#22312;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#19979;&#28216;&#26679;&#26412;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21644;&#26368;&#23567;&#30340;&#26631;&#27880;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#36873;&#25321;&#24615;&#26631;&#27880;&#19982;&#25552;&#31034;&#35843;&#25972;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;(SLPT)&#65292;&#20197;&#25552;&#21319;&#26377;&#38480;&#26631;&#31614;&#19979;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#29305;&#24449;&#30340;&#25552;&#31034;&#26356;&#26032;&#22120;&#26469;&#24341;&#23548;&#25552;&#31034;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;TandEm Selective LAbeling&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image analysis using deep learning is often challenged by limited labeled data and high annotation costs. Fine-tuning the entire network in label-limited scenarios can lead to overfitting and suboptimal performance. Recently, prompt tuning has emerged as a more promising technique that introduces a few additional tunable parameters as prompts to a task-agnostic pre-trained model, and updates only these parameters using supervision from limited labeled data while keeping the pre-trained model unchanged. However, previous work has overlooked the importance of selective labeling in downstream tasks, which aims to select the most valuable downstream samples for annotation to achieve the best performance with minimum annotation cost. To address this, we propose a framework that combines selective labeling with prompt tuning (SLPT) to boost performance in limited labels. Specifically, we introduce a feature-aware prompt updater to guide prompt tuning and a TandEm Selective LAbeling (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#35757;&#32451;&#26356;&#21152;&#40065;&#26834;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#31639;&#27861;&#30340;&#24046;&#24322;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#22240;&#26524;&#25915;&#20987;&#35797;&#22270;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#26377;&#24207;&#30340;&#22240;&#26524;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2308.04909</link><description>&lt;p&gt;
&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#23545;&#25239;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Deep Reinforcement Learning for Cyber Security in Software Defined Networks. (arXiv:2308.04909v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#35757;&#32451;&#26356;&#21152;&#40065;&#26834;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#31639;&#27861;&#30340;&#24046;&#24322;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#22240;&#26524;&#25915;&#20987;&#35797;&#22270;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#26377;&#24207;&#30340;&#22240;&#26524;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;Software Defined Networks&#65292;SDN&#65289;&#20013;&#65292;&#21033;&#29992;&#33258;&#20027;&#25915;&#20987;&#26041;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;Deep Reinforcement Learning&#65292;DRL&#65289;&#20013;&#35757;&#32451;&#26356;&#21152;&#40065;&#26834;&#30340;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;&#23558;&#23545;&#25239;&#24615;&#23398;&#20064;&#24212;&#29992;&#20110;DRL&#30340;&#33258;&#20027;&#23433;&#20840;&#24615;&#12290;&#27604;&#36739;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;Double Deep Q-Networks&#65288;DDQN&#65289;&#21644;Neural Episodic Control to Deep Q-Network&#65288;NEC2DQN&#25110;N2D&#65289;&#12290;&#25915;&#20987;&#32773;&#23545;&#29615;&#22659;&#20855;&#26377;&#23436;&#20840;&#30340;&#21487;&#35265;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;&#29366;&#24577;&#25805;&#20316;&#36827;&#34892;&#22240;&#26524;&#25915;&#20987;&#65292;&#35797;&#22270;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;&#25915;&#20987;&#23454;&#26045;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#36827;&#34892;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#38450;&#24481;&#32773;&#30340;&#27169;&#22411;&#21644;&#32463;&#39564;&#12290;&#36827;&#34892;&#20102;&#20004;&#36718;&#28216;&#25103;&#65306;&#31532;&#19968;&#36718;&#28216;&#25103;&#20013;&#65292;DDQN&#26159;&#38450;&#24481;&#32773;&#65292;N2D&#26159;&#25915;&#20987;&#32773;&#65307;&#31532;&#20108;&#36718;&#28216;&#25103;&#20013;&#65292;&#35282;&#33394;&#20114;&#25442;&#12290;&#20004;&#36718;&#28216;&#25103;&#20998;&#21035;&#36827;&#34892;&#20102;&#20004;&#27425;&#65292;&#31532;&#19968;&#27425;&#27809;&#26377;&#20027;&#21160;&#30340;&#22240;&#26524;&#25915;&#20987;&#65292;&#31532;&#20108;&#27425;&#36827;&#34892;&#20102;&#26377;&#24207;&#30340;&#22240;&#26524;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the impact of leveraging autonomous offensive approaches in Deep Reinforcement Learning (DRL) to train more robust agents by exploring the impact of applying adversarial learning to DRL for autonomous security in Software Defined Networks (SDN). Two algorithms, Double Deep Q-Networks (DDQN) and Neural Episodic Control to Deep Q-Network (NEC2DQN or N2D), are compared. NEC2DQN was proposed in 2018 and is a new member of the deep q-network (DQN) family of algorithms. The attacker has full observability of the environment and access to a causative attack that uses state manipulation in an attempt to poison the learning process. The implementation of the attack is done under a white-box setting, in which the attacker has access to the defender's model and experiences. Two games are played; in the first game, DDQN is a defender and N2D is an attacker, and in second game, the roles are reversed. The games are played twice; first, without an active causative attack and se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCC&#30340;&#23454;&#29992;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#30340;&#25317;&#22622;&#25511;&#21046;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#24555;&#36895;&#21644;&#31361;&#28982;&#30340;&#32593;&#32476;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04905</link><description>&lt;p&gt;
GraphCC: &#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;&#30340;&#23454;&#29992;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GraphCC: A Practical Graph Learning-based Approach to Congestion Control in Datacenters. (arXiv:2308.04905v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCC&#30340;&#23454;&#29992;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#30340;&#25317;&#22622;&#25511;&#21046;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#24555;&#36895;&#21644;&#31361;&#28982;&#30340;&#32593;&#32476;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#22622;&#25511;&#21046;&#65288;CC&#65289;&#22312;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#32593;&#32476;&#65288;DCN&#65289;&#20013;&#30340;&#27969;&#37327;&#26041;&#38754;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#12290;&#30446;&#21069;&#65292;DCN&#20027;&#35201;&#23454;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;CC&#21327;&#35758;&#65306;DCTCP&#21644;DCQCN&#12290;&#36825;&#20004;&#20010;&#21327;&#35758;&#21450;&#20854;&#20027;&#35201;&#21464;&#20307;&#37117;&#22522;&#20110;&#26174;&#24335;&#25317;&#22622;&#36890;&#30693;&#65288;ECN&#65289;&#65292;&#20854;&#20013;&#20013;&#38388;&#20132;&#25442;&#26426;&#22312;&#26816;&#27979;&#21040;&#25317;&#22622;&#26102;&#26631;&#35760;&#25968;&#25454;&#21253;&#12290;ECN&#37197;&#32622;&#22240;&#27492;&#25104;&#20026;CC&#21327;&#35758;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#29616;&#22312;&#65292;&#32593;&#32476;&#19987;&#23478;&#35774;&#23450;&#38745;&#24577;ECN&#21442;&#25968;&#65292;&#31934;&#24515;&#36873;&#25321;&#20197;&#20248;&#21270;&#32593;&#32476;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#20170;&#30340;&#39640;&#36895;DCN&#32463;&#21382;&#24555;&#36895;&#21644;&#31361;&#28982;&#30340;&#21464;&#21270;&#65292;&#20005;&#37325;&#25913;&#21464;&#20102;&#32593;&#32476;&#29366;&#24577;&#65288;&#20363;&#22914;&#65292;&#21160;&#24577;&#27969;&#37327;&#24037;&#20316;&#36127;&#36733;&#65292;incast&#20107;&#20214;&#65292;&#25925;&#38556;&#65289;&#12290;&#36825;&#23548;&#33268;&#20102;&#20302;&#25928;&#21033;&#29992;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCC&#30340;&#26032;&#39062;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20869;CC&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Congestion Control (CC) plays a fundamental role in optimizing traffic in Data Center Networks (DCN). Currently, DCNs mainly implement two main CC protocols: DCTCP and DCQCN. Both protocols -- and their main variants -- are based on Explicit Congestion Notification (ECN), where intermediate switches mark packets when they detect congestion. The ECN configuration is thus a crucial aspect on the performance of CC protocols. Nowadays, network experts set static ECN parameters carefully selected to optimize the average network performance. However, today's high-speed DCNs experience quick and abrupt changes that severely change the network state (e.g., dynamic traffic workloads, incast events, failures). This leads to under-utilization and sub-optimal performance. This paper presents GraphCC, a novel Machine Learning-based framework for in-network CC optimization. Our distributed solution relies on a novel combination of Multi-agent Reinforcement Learning (MARL) and Graph Neural Networks (
&lt;/p&gt;</description></item><item><title>&#35813;&#25253;&#21578;&#20851;&#27880;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#20026;&#20381;&#25454;&#32534;&#21046;&#20102;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#21015;&#34920;&#12290;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04889</link><description>&lt;p&gt;
NLLG&#23395;&#24230;arXiv&#25253;&#21578; 06/23&#65306;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#26159;&#20160;&#20040;&#65311;&#65288;arXiv:2308.04889v1 [cs.CY]&#65289;
&lt;/p&gt;
&lt;p&gt;
NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?. (arXiv:2308.04889v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25253;&#21578;&#20851;&#27880;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#20026;&#20381;&#25454;&#32534;&#21046;&#20102;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#21015;&#34920;&#12290;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;Generative Artificial Intelligence&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;Natural Language Processing&#65292;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;Machine Learning&#65292;ML&#65289;&#65289;&#20449;&#24687;&#30340;&#24555;&#36895;&#22686;&#38271;&#32473;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#20182;&#20204;&#38590;&#20197;&#36319;&#19978;&#26368;&#26032;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#20449;&#24687;&#36807;&#36733;&#30340;&#38382;&#39064;&#65292;Bielefeld&#22823;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#32452;&#22312;&#26412;&#25253;&#21578;&#20013;&#19987;&#27880;&#20110;&#35782;&#21035;arXiv&#19978;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#65292;&#29305;&#21035;&#20851;&#27880;NLP&#21644;ML&#12290;&#20854;&#30446;&#26631;&#26159;&#20026;&#26368;&#30456;&#20851;&#19988;&#34987;&#24191;&#27867;&#35752;&#35770;&#30340;&#30740;&#31350;&#25552;&#20379;&#24555;&#36895;&#25351;&#21335;&#65292;&#20197;&#24110;&#21161;&#26032;&#26469;&#32773;&#21644;&#24050;&#26377;&#30740;&#31350;&#20154;&#21592;&#36319;&#19978;&#24403;&#21069;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;2023&#24180;&#19978;&#21322;&#24180;&#30340;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#32534;&#21046;&#20102;&#19968;&#20010;&#30001;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#32452;&#25104;&#30340;&#21015;&#34920;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;ChatGPT&#26174;&#31034;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#22411;&#31526;&#21495;&#35268;&#21010;&#21160;&#20316;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#23454;&#20307;&#23618;&#27425;&#32467;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#30456;&#20284;&#34892;&#20026;&#26469;&#23454;&#29616;&#36890;&#29992;&#21270;&#12290;&#22312;&#27169;&#25311;&#30340;&#21416;&#25151;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04867</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#29992;&#22411;&#31526;&#21495;&#35268;&#21010;&#30340;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Type-Generalized Actions for Symbolic Planning. (arXiv:2308.04867v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#22411;&#31526;&#21495;&#35268;&#21010;&#21160;&#20316;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#23454;&#20307;&#23618;&#27425;&#32467;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#30456;&#20284;&#34892;&#20026;&#26469;&#23454;&#29616;&#36890;&#29992;&#21270;&#12290;&#22312;&#27169;&#25311;&#30340;&#21416;&#25151;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#35268;&#21010;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#38656;&#35201;&#38271;&#24207;&#21015;&#21160;&#20316;&#24182;&#35013;&#22791;&#26234;&#33021;&#20307;&#20855;&#26377;&#22797;&#26434;&#34892;&#20026;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#26159;&#38656;&#35201;&#21512;&#36866;&#30340;&#31526;&#21495;&#34920;&#31034;&#26469;&#25551;&#36848;&#29615;&#22659;&#30340;&#29366;&#24577;&#20197;&#21450;&#33021;&#22815;&#25913;&#21464;&#29366;&#24577;&#30340;&#21160;&#20316;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#34920;&#31034;&#26159;&#30001;&#19987;&#23478;&#20026;&#19981;&#21516;&#30340;&#38382;&#39064;&#22495;&#31934;&#24515;&#35774;&#35745;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#38382;&#39064;&#21644;&#29615;&#22659;&#22797;&#26434;&#24615;&#19978;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#32473;&#23450;&#30340;&#23454;&#20307;&#23618;&#27425;&#32467;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#30456;&#20284;&#34892;&#20026;&#26469;&#36890;&#29992;&#21270;&#31526;&#21495;&#21160;&#20316;&#12290;&#22312;&#19968;&#20010;&#27169;&#25311;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#21416;&#25151;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#23569;&#37327;&#35266;&#23519;&#20013;&#23398;&#20064;&#21040;&#30340;&#36890;&#29992;&#22411;&#21160;&#20316;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#30340;&#24773;&#20917;&#12290;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;&#21363;&#26102;&#36890;&#29992;&#21270;&#26426;&#21046;&#65292;&#21487;&#20197;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32452;&#21512;&#12289;&#36739;&#38271;&#24207;&#21015;&#12289;&#26032;&#23454;&#20307;&#21644;&#24847;&#22806;&#29615;&#22659;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic planning is a powerful technique to solve complex tasks that require long sequences of actions and can equip an intelligent agent with complex behavior. The downside of this approach is the necessity for suitable symbolic representations describing the state of the environment as well as the actions that can change it. Traditionally such representations are carefully hand-designed by experts for distinct problem domains, which limits their transferability to different problems and environment complexities. In this paper, we propose a novel concept to generalize symbolic actions using a given entity hierarchy and observed similar behavior. In a simulated grid-based kitchen environment, we show that type-generalized actions can be learned from few observations and generalize to novel situations. Incorporating an additional on-the-fly generalization mechanism during planning, unseen task combinations, involving longer sequences, novel entities and unexpected environment behavior,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20102;&#22312;&#22686;&#21152;&#28040;&#24687;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#21644;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#28040;&#24687;&#32534;&#30721;&#26041;&#27861;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22343;&#20540;&#28040;&#24687;&#32534;&#30721;&#22120;&#27604;&#27880;&#24847;&#21147;&#28040;&#24687;&#32534;&#30721;&#22120;&#26356;&#20248;&#65292;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#21327;&#35758;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04844</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#36830;&#32493;&#36890;&#20449;&#30340;&#28040;&#24687;&#32534;&#30721;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scalability of Message Encoding Techniques for Continuous Communication Learned with Multi-Agent Reinforcement Learning. (arXiv:2308.04844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20102;&#22312;&#22686;&#21152;&#28040;&#24687;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#21644;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#28040;&#24687;&#32534;&#30721;&#26041;&#27861;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22343;&#20540;&#28040;&#24687;&#32534;&#30721;&#22120;&#27604;&#27880;&#24847;&#21147;&#28040;&#24687;&#32534;&#30721;&#22120;&#26356;&#20248;&#65292;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#21327;&#35758;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#38656;&#35201;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#20197;&#30830;&#20445;&#27491;&#30830;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#36890;&#20449;&#21327;&#35758;&#21644;&#21160;&#20316;&#21327;&#35758;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#28789;&#27963;&#20915;&#23450;&#24212;&#35813;&#20849;&#20139;&#21738;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#25105;&#20204;&#38656;&#35201;&#21019;&#24314;&#21253;&#21547;&#22312;&#36825;&#20123;&#28040;&#24687;&#20013;&#30340;&#20449;&#24687;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22686;&#21152;&#28040;&#24687;&#20013;&#24212;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#21644;&#22686;&#21152;&#26234;&#33021;&#20307;&#25968;&#37327;&#23545;&#20004;&#31181;&#19981;&#21516;&#28040;&#24687;&#32534;&#30721;&#26041;&#27861;&#65288;&#22343;&#20540;&#28040;&#24687;&#32534;&#30721;&#22120;&#21644;&#27880;&#24847;&#21147;&#28040;&#24687;&#32534;&#30721;&#22120;&#65289;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#19968;&#20010;&#30697;&#38453;&#29615;&#22659;&#20013;&#23545;&#36825;&#20123;&#24433;&#21709;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22343;&#20540;&#28040;&#24687;&#32534;&#30721;&#22120;&#22987;&#32456;&#20248;&#20110;&#27880;&#24847;&#21147;&#28040;&#24687;&#32534;&#30721;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#22343;&#20540;&#28040;&#24687;&#32534;&#30721;&#22120;&#30340;&#26234;&#33021;&#20307;&#20351;&#29992;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#24182;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;
&lt;/p&gt;
&lt;p&gt;
Many multi-agent systems require inter-agent communication to properly achieve their goal. By learning the communication protocol alongside the action protocol using multi-agent reinforcement learning techniques, the agents gain the flexibility to determine which information should be shared. However, when the number of agents increases we need to create an encoding of the information contained in these messages. In this paper, we investigate the effect of increasing the amount of information that should be contained in a message and increasing the number of agents. We evaluate these effects on two different message encoding methods, the mean message encoder and the attention message encoder. We perform our experiments on a matrix environment. Surprisingly, our results show that the mean message encoder consistently outperforms the attention message encoder. Therefore, we analyse the communication protocol used by the agents that use the mean message encoder and can conclude that the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;RDF&#21644;&#25551;&#36848;&#36923;&#36753;&#25512;&#29702;&#26426;&#30340;&#29616;&#29366;&#21644;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#25512;&#29702;&#22312;RDF(S)&#12289;EL&#21644;ALC&#25551;&#36848;&#36923;&#36753;&#20197;&#21450;OWL 2 RL&#39046;&#22495;&#30340;&#24050;&#26377;&#30740;&#31350;&#25991;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.04814</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;RDF&#21644;&#25551;&#36848;&#36923;&#36753;&#25512;&#29702;&#26426;&#65306;&#29616;&#29366;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic RDF and Description Logic Reasoners: The State-Of-The-Art and Challenges. (arXiv:2308.04814v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;RDF&#21644;&#25551;&#36848;&#36923;&#36753;&#25512;&#29702;&#26426;&#30340;&#29616;&#29366;&#21644;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#25512;&#29702;&#22312;RDF(S)&#12289;EL&#21644;ALC&#25551;&#36848;&#36923;&#36753;&#20197;&#21450;OWL 2 RL&#39046;&#22495;&#30340;&#24050;&#26377;&#30740;&#31350;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;RDF&#21644;&#25551;&#36848;&#36923;&#36753;&#25512;&#29702;&#26426;&#30340;&#29616;&#29366;&#21644;&#25361;&#25112;&#12290;&#38543;&#30528;&#26412;&#20307;&#35770;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;RDF&#21644;OWL&#25104;&#20026;&#26412;&#20307;&#24320;&#21457;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;&#34429;&#28982;RDF&#31616;&#21333;&#28789;&#27963;&#65292;OWL&#33021;&#22815;&#35814;&#32454;&#34920;&#31034;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26412;&#20307;&#30340;&#25193;&#22823;&#21644;&#34920;&#36798;&#30340;&#22686;&#21152;&#65292;&#25512;&#29702;&#22797;&#26434;&#24230;&#22686;&#21152;&#65292;&#20256;&#32479;&#25512;&#29702;&#26426;&#30340;&#25928;&#29575;&#21463;&#21040;&#25361;&#25112;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#31526;&#21495;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#25512;&#29702;&#22312;RDF(S)&#12289;EL&#21644;ALC&#25551;&#36848;&#36923;&#36753;&#20197;&#21450;OWL 2 RL&#39046;&#22495;&#30340;&#24050;&#26377;&#30740;&#31350;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies are used in various domains, with RDF and OWL being prominent standards for ontology development. RDF is favored for its simplicity and flexibility, while OWL enables detailed domain knowledge representation. However, as ontologies grow larger and more expressive, reasoning complexity increases, and traditional reasoners struggle to perform efficiently. Despite optimization efforts, scalability remains an issue. Additionally, advancements in automated knowledge base construction have created large and expressive ontologies that are often noisy and inconsistent, posing further challenges for conventional reasoners. To address these challenges, researchers have explored neuro-symbolic approaches that combine neural networks' learning capabilities with symbolic systems' reasoning abilities. In this chapter,we provide an overview of the existing literature in the field of neuro-symbolic deductive reasoning supported by RDF(S), the description logics EL and ALC, and OWL 2 RL, dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#22320;&#22270;&#34920;&#31034;&#65292;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#26469;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04792</link><description>&lt;p&gt;
&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#34892;&#26143;&#25506;&#27979;&#36710;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast and Optimal Learning-based Path Planning Method for Planetary Rovers. (arXiv:2308.04792v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#22320;&#22270;&#34920;&#31034;&#65292;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#26469;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#23545;&#20110;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#31243;&#22270;&#20013;&#24555;&#36895;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#65292;&#31216;&#20026;NNPP&#12290;NNPP&#27169;&#22411;&#20174;&#22823;&#37327;&#39044;&#27880;&#37322;&#30340;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#23398;&#20064;&#36215;&#22987;&#21644;&#30446;&#26631;&#20301;&#32622;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#22320;&#22270;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#27599;&#20010;&#20687;&#32032;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#34920;&#31034;&#20854;&#23646;&#20110;&#22320;&#22270;&#19978;&#26368;&#20248;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20174;DEM&#33719;&#21462;&#30340;&#22369;&#24230;&#12289;&#31895;&#31961;&#24230;&#21644;&#39640;&#24230;&#24046;&#35745;&#31639;&#27599;&#20010;&#32593;&#26684;&#21333;&#20803;&#30340;&#36941;&#21382;&#25104;&#26412;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#23545;&#36215;&#22987;&#21644;&#30446;&#26631;&#20301;&#32622;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20998;&#26512;&#19981;&#21516;&#20301;&#32622;&#32534;&#30721;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;NNPP&#27169;&#22411;&#33021;&#22815;&#22312;&#26032;&#30340;&#22320;&#22270;&#19978;&#25191;&#34892;&#36335;&#24452;&#35268;&#21010;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;NNPP&#29983;&#25104;&#30340;&#24341;&#23548;&#22330;&#33021;&#22815;&#20934;&#30830;&#25351;&#23548;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent autonomous path planning is crucial to improve the exploration efficiency of planetary rovers. In this paper, we propose a learning-based method to quickly search for optimal paths in an elevation map, which is called NNPP. The NNPP model learns semantic information about start and goal locations, as well as map representations, from numerous pre-annotated optimal path demonstrations, and produces a probabilistic distribution over each pixel representing the likelihood of it belonging to an optimal path on the map. More specifically, the paper computes the traversal cost for each grid cell from the slope, roughness and elevation difference obtained from the DEM. Subsequently, the start and goal locations are encoded using a Gaussian distribution and different location encoding parameters are analyzed for their effect on model performance. After training, the NNPP model is able to perform path planning on novel maps. Experiments show that the guidance field generated by the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35270;&#35282;&#34701;&#21512;&#21644;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;3D&#22320;&#19979;&#38647;&#36798;&#30340;&#36335;&#22522;&#25439;&#22351;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#35282;&#20449;&#24687;&#21644;&#26500;&#24314;&#30495;&#23454;&#22810;&#35270;&#35282;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04779</link><description>&lt;p&gt;
&#22522;&#20110;3D&#22320;&#19979;&#38647;&#36798;&#30340;&#36335;&#22522;&#25439;&#22351;&#26816;&#27979;&#30340;&#22810;&#35270;&#35282;&#34701;&#21512;&#19982;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-View Fusion and Distillation for Subgrade Distresses Detection based on 3D-GPR. (arXiv:2308.04779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35270;&#35282;&#34701;&#21512;&#21644;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;3D&#22320;&#19979;&#38647;&#36798;&#30340;&#36335;&#22522;&#25439;&#22351;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#35282;&#20449;&#24687;&#21644;&#26500;&#24314;&#30495;&#23454;&#22810;&#35270;&#35282;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#22320;&#19979;&#38647;&#36798;&#65288;3D-GPR&#65289;&#22312;&#36335;&#22522;&#25439;&#22351;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24191;&#27867;&#27969;&#34892;&#36215;&#26469;&#12290;&#20026;&#20102;&#25552;&#39640;&#26816;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#20808;&#23548;&#24615;&#30740;&#31350;&#23581;&#35797;&#37319;&#29992;&#33258;&#21160;&#26816;&#27979;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;1D A&#25195;&#25551;&#12289;2D B&#25195;&#25551;&#25110;3D C&#25195;&#25551;&#25968;&#25454;&#65292;&#23548;&#33268;&#31354;&#38388;&#20449;&#24687;&#19981;&#36275;&#25110;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;3D-GPR&#25968;&#25454;&#30340;&#36335;&#22522;&#25439;&#22351;&#26816;&#27979;&#20219;&#21153;&#65292;&#21033;&#29992;&#22810;&#35270;&#35282;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#21407;&#22987;3D-GPR&#25968;&#25454;&#23548;&#20986;&#30340;&#30495;&#23454;&#22810;&#35270;&#35282;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#20219;&#21153;&#65292;&#30456;&#27604;A&#25195;&#25551;&#21644;B&#25195;&#25551;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#19982;C&#25195;&#25551;&#25968;&#25454;&#30456;&#27604;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#34701;&#21512;&#21644;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The application of 3D ground-penetrating radar (3D-GPR) for subgrade distress detection has gained widespread popularity. To enhance the efficiency and accuracy of detection, pioneering studies have attempted to adopt automatic detection techniques, particularly deep learning. However, existing works typically rely on traditional 1D A-scan, 2D B-scan or 3D C-scan data of the GPR, resulting in either insufficient spatial information or high computational complexity. To address these challenges, we introduce a novel methodology for the subgrade distress detection task by leveraging the multi-view information from 3D-GPR data. Moreover, we construct a real multi-view image dataset derived from the original 3D-GPR data for the detection task, which provides richer spatial information compared to A-scan and B-scan data, while reducing computational complexity compared to C-scan data. Subsequently, we develop a novel \textbf{M}ulti-\textbf{V}iew \textbf{V}usion and \textbf{D}istillation fram
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#20316;&#22810;&#20010;&#26412;&#22320;NMF&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04778</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Multi-view Clustering based on Non-negative Matrix Factorization. (arXiv:2308.04778v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#20316;&#22810;&#20010;&#26412;&#22320;NMF&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#30456;&#20851;&#23545;&#35937;&#32452;&#21512;&#36215;&#26469;&#65292;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26088;&#22312;&#25581;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#12290;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#20803;&#32032;&#38750;&#36127;&#24615;&#26045;&#21152;&#38480;&#21046;&#65292;&#23558;&#25968;&#25454;&#30697;&#38453;&#20998;&#35299;&#20026;&#20004;&#20010;&#30697;&#38453;&#65306;&#19968;&#20010;&#34920;&#31034;&#25968;&#25454;&#20998;&#21306;&#65292;&#21478;&#19968;&#20010;&#34920;&#31034;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#21407;&#22411;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#22312;&#25991;&#26412;&#25366;&#25496;&#12289;&#32858;&#31867;&#12289;&#35821;&#35328;&#24314;&#27169;&#12289;&#38899;&#20048;&#36716;&#24405;&#21644;&#31070;&#32463;&#31185;&#23398;&#65288;&#22522;&#22240;&#20998;&#31163;&#65289;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#29983;&#25104;&#30697;&#38453;&#30340;&#35299;&#37322;&#30001;&#20110;&#19981;&#23384;&#22312;&#36127;&#20540;&#32780;&#21464;&#24471;&#26356;&#31616;&#21333;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#22810;&#27169;&#24577;&#32858;&#31867;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65292;&#20854;&#20013;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#20010;&#26412;&#22320;NMF&#27169;&#22411;&#30340;&#21327;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
By combining related objects, unsupervised machine learning techniques aim to reveal the underlying patterns in a data set. Non-negative Matrix Factorization (NMF) is a data mining technique that splits data matrices by imposing restrictions on the elements' non-negativity into two matrices: one representing the data partitions and the other to represent the cluster prototypes of the data set. This method has attracted a lot of attention and is used in a wide range of applications, including text mining, clustering, language modeling, music transcription, and neuroscience (gene separation). The interpretation of the generated matrices is made simpler by the absence of negative values. In this article, we propose a study on multi-modal clustering algorithms and present a novel method called multi-modal multi-view non-negative matrix factorization, in which we analyze the collaboration of several local NMF models. The experimental results show the value of the proposed approach, which wa
&lt;/p&gt;</description></item><item><title>E3-UAV&#26159;&#19968;&#31181;&#38754;&#21521;&#36793;&#32536;&#30340;&#33021;&#37327;&#39640;&#25928;&#26080;&#20154;&#26426;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#39134;&#34892;&#21442;&#25968;&#21644;&#26816;&#27979;&#31639;&#27861;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2308.04774</link><description>&lt;p&gt;
E3-UAV:&#19968;&#31181;&#38754;&#21521;&#36793;&#32536;&#30340;&#33021;&#37327;&#39640;&#25928;&#26080;&#20154;&#26426;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
E3-UAV: An Edge-based Energy-Efficient Object Detection System for Unmanned Aerial Vehicles. (arXiv:2308.04774v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04774
&lt;/p&gt;
&lt;p&gt;
E3-UAV&#26159;&#19968;&#31181;&#38754;&#21521;&#36793;&#32536;&#30340;&#33021;&#37327;&#39640;&#25928;&#26080;&#20154;&#26426;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#39134;&#34892;&#21442;&#25968;&#21644;&#26816;&#27979;&#31639;&#27861;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#30446;&#26631;&#26816;&#27979;&#22312;&#36710;&#36742;&#35745;&#25968;&#12289;&#28779;&#28798;&#26816;&#27979;&#21644;&#22478;&#24066;&#30417;&#27979;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#21482;&#20851;&#27880;&#26080;&#20154;&#26426;&#30446;&#26631;&#26816;&#27979;&#20013;&#26576;&#20123;&#25361;&#25112;&#30340;&#23376;&#38598;&#65292;&#32570;&#20047;&#22312;&#21508;&#20010;&#26041;&#38754;&#24179;&#34913;&#26469;&#35774;&#35745;&#19968;&#20010;&#23454;&#38469;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#30340;&#31995;&#32479;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#36793;&#32536;&#30340;&#33021;&#37327;&#39640;&#25928;&#26080;&#20154;&#26426;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;E3-UAV&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20915;&#23450;&#28385;&#36275;&#20219;&#21153;&#26816;&#27979;&#35201;&#27714;&#30340;&#26368;&#33410;&#33021;&#39134;&#34892;&#21442;&#25968;&#65288;&#21253;&#25324;&#39134;&#34892;&#39640;&#24230;&#12289;&#39134;&#34892;&#36895;&#24230;&#12289;&#26816;&#27979;&#31639;&#27861;&#21644;&#37319;&#26679;&#29575;&#65289;&#65292;&#20174;&#32780;&#21160;&#24577;&#25903;&#25345;&#21508;&#31181;&#26080;&#20154;&#26426;&#35774;&#22791;&#12289;&#36793;&#32536;&#35774;&#22791;&#21644;&#26816;&#27979;&#31639;&#27861;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#23454;&#38469;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#36879;&#26126;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Motivated by the advances in deep learning techniques, the application of Unmanned Aerial Vehicle (UAV)-based object detection has proliferated across a range of fields, including vehicle counting, fire detection, and city monitoring. While most existing research studies only a subset of the challenges inherent to UAV-based object detection, there are few studies that balance various aspects to design a practical system for energy consumption reduction. In response, we present the E3-UAV, an edge-based energy-efficient object detection system for UAVs. The system is designed to dynamically support various UAV devices, edge devices, and detection algorithms, with the aim of minimizing energy consumption by deciding the most energy-efficient flight parameters (including flight altitude, flight speed, detection algorithm, and sampling rate) required to fulfill the detection requirements of the task. We first present an effective evaluation metric for actual tasks and construct a transpare
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Induction Network&#26469;&#35299;&#20915;&#33258;&#30417;&#30563;&#38899;&#28304;&#23450;&#20301;&#20013;&#30340;&#35270;&#21548;&#27169;&#24577;&#24046;&#36317;&#38382;&#39064;&#12290;&#36890;&#36807;&#35299;&#32806;&#26799;&#24230;&#24182;&#24341;&#20837;&#33258;&#20030;&#23398;&#20064;&#65292;&#20351;&#22768;&#28304;&#30340;&#21028;&#21035;&#24615;&#35270;&#35273;&#34920;&#31034;&#33021;&#22815;&#19982;&#38899;&#39057;&#27169;&#24577;&#20445;&#25345;&#19968;&#33268;&#12290;&#36824;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#36873;&#25321;&#31574;&#30053;&#26469;&#22686;&#24378;&#20854;&#24341;&#23548;&#33021;&#21147;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04767</link><description>&lt;p&gt;
Induction Network:&#33258;&#30417;&#30563;&#38899;&#28304;&#23450;&#20301;&#30340;&#35270;&#21548;&#27169;&#24577;&#24046;&#36317;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization. (arXiv:2308.04767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Induction Network&#26469;&#35299;&#20915;&#33258;&#30417;&#30563;&#38899;&#28304;&#23450;&#20301;&#20013;&#30340;&#35270;&#21548;&#27169;&#24577;&#24046;&#36317;&#38382;&#39064;&#12290;&#36890;&#36807;&#35299;&#32806;&#26799;&#24230;&#24182;&#24341;&#20837;&#33258;&#20030;&#23398;&#20064;&#65292;&#20351;&#22768;&#28304;&#30340;&#21028;&#21035;&#24615;&#35270;&#35273;&#34920;&#31034;&#33021;&#22815;&#19982;&#38899;&#39057;&#27169;&#24577;&#20445;&#25345;&#19968;&#33268;&#12290;&#36824;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#36873;&#25321;&#31574;&#30053;&#26469;&#22686;&#24378;&#20854;&#24341;&#23548;&#33021;&#21147;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#38899;&#28304;&#23450;&#20301;&#36890;&#24120;&#21463;&#21040;&#27169;&#24577;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31574;&#30053;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#24314;&#31435;&#38899;&#39057;&#21644;&#22768;&#28304;&#20043;&#38388;&#19968;&#33268;&#23545;&#24212;&#30340;&#33391;&#22909;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20013;&#30340;&#24322;&#36136;&#24615;&#24433;&#21709;&#65292;&#36825;&#20010;&#26041;&#26696;&#30340;&#20851;&#27880;&#24230;&#19981;&#36275;&#20173;&#28982;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#65292;&#36825;&#20063;&#25104;&#20026;&#25105;&#20204;&#24037;&#20316;&#30340;&#21160;&#26426;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Induction Network&#26469;&#26356;&#26377;&#25928;&#22320;&#24357;&#21512;&#27169;&#24577;&#24046;&#36317;&#12290;&#36890;&#36807;&#35299;&#32806;&#35270;&#35273;&#21644;&#38899;&#39057;&#27169;&#24577;&#30340;&#26799;&#24230;&#65292;&#21487;&#20197;&#20197;&#24341;&#23548;&#21521;&#37327;&#35774;&#35745;&#30340;&#33258;&#20030;&#26041;&#24335;&#23398;&#20064;&#22768;&#28304;&#30340;&#21028;&#21035;&#24615;&#35270;&#35273;&#34920;&#31034;&#65292;&#36825;&#20063;&#20351;&#38899;&#39057;&#27169;&#24577;&#33021;&#22815;&#19982;&#35270;&#35273;&#27169;&#24577;&#20445;&#25345;&#19968;&#33268;&#12290;&#38500;&#20102;&#35270;&#35273;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#36873;&#25321;&#31574;&#30053;&#26469;&#22686;&#24378;&#24341;&#23548;&#33021;&#21147;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised sound source localization is usually challenged by the modality inconsistency. In recent studies, contrastive learning based strategies have shown promising to establish such a consistent correspondence between audio and sound sources in visual scenarios. Unfortunately, the insufficient attention to the heterogeneity influence in the different modality features still limits this scheme to be further improved, which also becomes the motivation of our work. In this study, an Induction Network is proposed to bridge the modality gap more effectively. By decoupling the gradients of visual and audio modalities, the discriminative visual representations of sound sources can be learned with the designed Induction Vector in a bootstrap manner, which also enables the audio modality to be aligned with the visual modality consistently. In addition to a visual weighted contrastive loss, an adaptive threshold selection strategy is introduced to enhance the robustness of the Induction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#30340;&#29305;&#24449;&#21305;&#37197;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#25273;&#21435;&#30495;&#23454;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.04761</link><description>&lt;p&gt;
&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#30340;&#29305;&#24449;&#21305;&#37197;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Feature Matching Data Synthesis for Non-IID Federated Learning. (arXiv:2308.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#30340;&#29305;&#24449;&#21305;&#37197;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#25273;&#21435;&#30495;&#23454;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#33539;&#24335;&#65292;&#23427;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32780;&#19981;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#25910;&#38598;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#22788;&#29702;&#35774;&#22791;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#30528;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#29305;&#24449;&#21305;&#37197;&#25968;&#25454;&#21512;&#25104;&#65288;HFMDS&#65289;&#26041;&#27861;&#65292;&#38500;&#20102;&#26412;&#22320;&#27169;&#22411;&#22806;&#65292;&#36824;&#20849;&#20139;&#36741;&#21161;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23398;&#20064;&#30495;&#23454;&#26679;&#26412;&#30340;&#22522;&#26412;&#31867;&#30456;&#20851;&#29305;&#24449;&#24182;&#20002;&#24323;&#22810;&#20313;&#30340;&#29305;&#24449;&#65292;&#29983;&#25104;&#20102;&#21512;&#25104;&#25968;&#25454;&#65292;&#36825;&#26377;&#21161;&#20110;&#26377;&#25928;&#22788;&#29702;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20445;&#25252;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65292;&#23558;&#30495;&#23454;&#29305;&#24449;&#36716;&#31227;&#21040;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#65292;&#20174;&#32780;&#21512;&#25104;&#25968;&#25454;&#19981;&#20165;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19988;&#25273;&#21435;&#20102;&#30495;&#23454;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#23558;&#25552;&#20986;&#30340;HFMDS&#26041;&#27861;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a privacy-preserving paradigm that trains neural networks on edge devices without collecting data at a central server. However, FL encounters an inherent challenge in dealing with non-independent and identically distributed (non-IID) data among devices. To address this challenge, this paper proposes a hard feature matching data synthesis (HFMDS) method to share auxiliary data besides local models. Specifically, synthetic data are generated by learning the essential class-relevant features of real samples and discarding the redundant features, which helps to effectively tackle the non-IID issue. For better privacy preservation, we propose a hard feature augmentation method to transfer real features towards the decision boundary, with which the synthetic data not only improve the model generalization but also erase the information of real features. By integrating the proposed HFMDS method with FL, we present a novel FL framework with data augmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#65292;&#20351;&#29992;&#39044;&#23450;&#20041;&#21442;&#25968;&#26469;&#35780;&#20272;&#21487;&#33021;&#21457;&#29983;&#20107;&#25925;&#30340;&#39118;&#38505;&#65292;&#24182;&#38598;&#25104;&#20262;&#29702;&#20542;&#21521;&#29702;&#35770;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#29615;&#22659;&#21644;&#20915;&#31574;&#32972;&#26223;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#23450;&#20041;&#36710;&#36742;&#34892;&#20026;&#30340;&#36947;&#24503;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.04760</link><description>&lt;p&gt;
&#26080;&#20262;&#29702;&#30340;&#33258;&#21160;&#39550;&#39542;&#65306;&#24847;&#20041;&#12289;&#35774;&#35745;&#21644;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Automated Driving Without Ethics: Meaning, Design and Real-World Implementation. (arXiv:2308.04760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#65292;&#20351;&#29992;&#39044;&#23450;&#20041;&#21442;&#25968;&#26469;&#35780;&#20272;&#21487;&#33021;&#21457;&#29983;&#20107;&#25925;&#30340;&#39118;&#38505;&#65292;&#24182;&#38598;&#25104;&#20262;&#29702;&#20542;&#21521;&#29702;&#35770;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#29615;&#22659;&#21644;&#20915;&#31574;&#32972;&#26223;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#23450;&#20041;&#36710;&#36742;&#34892;&#20026;&#30340;&#36947;&#24503;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#36947;&#24503;&#38382;&#39064;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#21040;&#20154;&#31867;&#20260;&#23475;&#30340;&#20107;&#25925;&#24773;&#20917;&#19979;&#30340;&#20915;&#31574;&#25919;&#31574;&#26041;&#38754;&#12290;&#22312;&#35752;&#35770;&#8220;&#20154;&#24037;&#36947;&#24503;&#20195;&#29702;&#8221;&#19968;&#35789;&#26159;&#21542;&#36866;&#29992;&#20110;&#25551;&#36848;&#33021;&#22815;&#20570;&#20986;&#36825;&#20123;&#20915;&#31574;&#30340;AV&#20043;&#21518;&#65292;&#24182;&#22522;&#20110;&#19968;&#31181;&#20551;&#35774;&#65292;&#21363;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#36991;&#20813;&#20154;&#31867;&#20260;&#23475;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;AV&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#65292;&#21482;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#21442;&#25968;&#26469;&#34920;&#24449;&#21487;&#33021;&#21457;&#29983;&#20107;&#25925;&#30340;&#39118;&#38505;&#65292;&#24182;&#23558;&#20262;&#29702;&#20542;&#21521;&#29702;&#35770;&#32435;&#20837;&#22810;&#31181;&#21487;&#33021;&#30340;&#20915;&#31574;&#35268;&#21017;&#20013;&#65292;&#20197;&#30830;&#23450;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#20915;&#31574;&#32972;&#26223;&#19979;&#26368;&#21512;&#36866;&#30340;&#34892;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#30446;&#26631;&#19981;&#26159;&#23450;&#20041;&#36947;&#24503;&#29702;&#35770;&#35201;&#27714;&#36710;&#36742;&#22914;&#20309;&#34892;&#20026;&#65292;&#32780;&#26159;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ethics of automated vehicles (AV) has received a great amount of attention in recent years, specifically in regard to their decisional policies in accident situations in which human harm is a likely consequence. After a discussion about the pertinence and cogency of the term 'artificial moral agent' to describe AVs that would accomplish these sorts of decisions, and starting from the assumption that human harm is unavoidable in some situations, a strategy for AV decision making is proposed using only pre-defined parameters to characterize the risk of possible accidents and also integrating the Ethical Valence Theory, which paints AV decision-making as a type of claim mitigation, into multiple possible decision rules to determine the most suitable action given the specific environment and decision context. The goal of this approach is not to define how moral theory requires vehicles to behave, but rather to provide a computational approach that is flexible enough to accommodate a nu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40479;&#30640;&#22330;&#26223;&#22270;&#65288;BSG&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#40479;&#30640;&#34920;&#31034;&#26469;&#32534;&#30721;&#22330;&#26223;&#24067;&#23616;&#21644;&#20960;&#20309;&#32447;&#32034;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#24403;&#21069;&#20840;&#26223;&#35266;&#23519;&#30340;&#23548;&#33322;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.04758</link><description>&lt;p&gt;
&#40479;&#30640;&#22330;&#26223;&#22270;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Bird's-Eye-View Scene Graph for Vision-Language Navigation. (arXiv:2308.04758v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04758
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40479;&#30640;&#22330;&#26223;&#22270;&#65288;BSG&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#40479;&#30640;&#34920;&#31034;&#26469;&#32534;&#30721;&#22330;&#26223;&#24067;&#23616;&#21644;&#20960;&#20309;&#32447;&#32034;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#24403;&#21069;&#20840;&#26223;&#35266;&#23519;&#30340;&#23548;&#33322;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#38656;&#35201;&#19968;&#20010;&#20195;&#29702;&#26681;&#25454;&#20154;&#31867;&#25351;&#31034;&#22312;3D&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20195;&#29702;&#22522;&#20110;&#20840;&#26223;&#35266;&#23519;&#26500;&#24314;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#24863;&#30693;3D&#22330;&#26223;&#20960;&#20309;&#21644;&#23481;&#26131;&#23548;&#33268;&#20840;&#26223;&#35270;&#22270;&#30340;&#27169;&#31946;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40479;&#30640;&#22330;&#26223;&#22270;&#65288;BSG&#65289;&#65292;&#23427;&#21033;&#29992;&#22810;&#27493;&#40479;&#30640;&#34920;&#31034;&#26469;&#32534;&#30721;&#23460;&#20869;&#29615;&#22659;&#30340;&#22330;&#26223;&#24067;&#23616;&#21644;&#20960;&#20309;&#32447;&#32034;&#65292;&#22312;3D&#26816;&#27979;&#30340;&#30417;&#30563;&#19979;&#12290;&#22312;&#23548;&#33322;&#36807;&#31243;&#20013;&#65292;BSG&#22312;&#27599;&#20010;&#27493;&#39588;&#26500;&#24314;&#19968;&#20010;&#26412;&#22320;&#40479;&#30640;&#34920;&#31034;&#65292;&#24182;&#32500;&#25252;&#19968;&#20010;&#22522;&#20110;&#40479;&#30640;&#30340;&#20840;&#23616;&#22330;&#26223;&#22320;&#22270;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#25299;&#25169;&#20851;&#31995;&#23384;&#20648;&#21644;&#32452;&#32455;&#25152;&#26377;&#22312;&#32447;&#25910;&#38598;&#30340;&#26412;&#22320;&#40479;&#30640;&#34920;&#31034;&#12290;&#22522;&#20110;BSG&#65292;&#20195;&#29702;&#39044;&#27979;&#26412;&#22320;&#40479;&#30640;&#32593;&#26684;&#32423;&#20915;&#31574;&#24471;&#20998;&#21644;&#20840;&#23616;&#22270;&#24418;&#32423;&#20915;&#31574;&#24471;&#20998;&#65292;&#32467;&#21512;&#20840;&#26223;&#35270;&#22270;&#30340;&#23376;&#35270;&#22270;&#36873;&#25321;&#24471;&#20998;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vision-language navigation (VLN), which entails an agent to navigate 3D environments following human instructions, has shown great advances. However, current agents are built upon panoramic observations, which hinders their ability to perceive 3D scene geometry and easily leads to ambiguous selection of panoramic view. To address these limitations, we present a BEV Scene Graph (BSG), which leverages multi-step BEV representations to encode scene layouts and geometric cues of indoor environment under the supervision of 3D detection. During navigation, BSG builds a local BEV representation at each step and maintains a BEV-based global scene map, which stores and organizes all the online collected local BEV representations according to their topological relations. Based on BSG, the agent predicts a local BEV grid-level decision score and a global graph-level decision score, combined with a sub-view selection score on panoramic views, for more accurate action prediction. Our approach signi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21160;&#24577;&#32467;&#26500;&#21457;&#23637;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#39640;&#25928;&#36830;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#21160;&#24577;&#22686;&#38271;&#21644;&#20462;&#21098;&#31070;&#32463;&#20803;&#26469;&#25552;&#39640;&#35760;&#24518;&#23481;&#37327;&#21644;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#21516;&#26102;&#21033;&#29992;&#37325;&#21472;&#30340;&#20849;&#20139;&#32467;&#26500;&#24555;&#36895;&#24212;&#29992;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#21040;&#26032;&#20219;&#21153;&#19978;&#12290;</title><link>http://arxiv.org/abs/2308.04749</link><description>&lt;p&gt;
&#25552;&#21319;&#24102;&#26377;&#21160;&#24577;&#32467;&#26500;&#21457;&#23637;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Efficient Continual Learning with Dynamic Structure Development of Spiking Neural Networks. (arXiv:2308.04749v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04749
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#32467;&#26500;&#21457;&#23637;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#39640;&#25928;&#36830;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#21160;&#24577;&#22686;&#38271;&#21644;&#20462;&#21098;&#31070;&#32463;&#20803;&#26469;&#25552;&#39640;&#35760;&#24518;&#23481;&#37327;&#21644;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#21516;&#26102;&#21033;&#29992;&#37325;&#21472;&#30340;&#20849;&#20139;&#32467;&#26500;&#24555;&#36895;&#24212;&#29992;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#21040;&#26032;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23401;&#31461;&#20855;&#22791;&#39034;&#24207;&#23398;&#20064;&#22810;&#20010;&#35748;&#30693;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#38271;&#36828;&#30446;&#26631;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#36890;&#24120;&#36866;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#20294;&#23545;&#26356;&#21152;&#33041;&#21551;&#21457;&#12289;&#33021;&#25928;&#26356;&#39640;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#32570;&#20047;&#25506;&#32034;&#12290;&#20511;&#37492;&#23401;&#31461;&#25104;&#38271;&#21644;&#21457;&#23637;&#36807;&#31243;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#26426;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#32467;&#26500;&#21457;&#23637;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;DSD-SNN&#65289;&#65292;&#29992;&#20110;&#39640;&#25928;&#21644;&#33258;&#36866;&#24212;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#26102;&#65292;DSD-SNN&#20250;&#21160;&#24577;&#22320;&#20998;&#37197;&#24182;&#22686;&#38271;&#26032;&#30340;&#31070;&#32463;&#20803;&#26469;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#24182;&#20462;&#21098;&#22810;&#20313;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22686;&#21152;&#35760;&#24518;&#23481;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#37325;&#21472;&#30340;&#20849;&#20139;&#32467;&#26500;&#26377;&#21161;&#20110;&#24555;&#36895;&#23558;&#25152;&#26377;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#24212;&#29992;&#21040;&#26032;&#20219;&#21153;&#19978;&#65292;&#20351;&#21333;&#20010;&#32593;&#32476;&#33021;&#22815;&#25903;&#25345;&#22810;&#20010;&#22686;&#37327;&#20219;&#21153;&#65288;&#32780;&#26080;&#38656;&#20026;&#27599;&#20010;&#20219;&#21153;&#21333;&#29420;&#21019;&#24314;&#23376;&#32593;&#32476;&#30340;&#25513;&#30721;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children possess the ability to learn multiple cognitive tasks sequentially, which is a major challenge toward the long-term goal of artificial general intelligence. Existing continual learning frameworks are usually applicable to Deep Neural Networks (DNNs) and lack the exploration on more brain-inspired, energy-efficient Spiking Neural Networks (SNNs). Drawing on continual learning mechanisms during child growth and development, we propose Dynamic Structure Development of Spiking Neural Networks (DSD-SNN) for efficient and adaptive continual learning. When learning a sequence of tasks, the DSD-SNN dynamically assigns and grows new neurons to new tasks and prunes redundant neurons, thereby increasing memory capacity and reducing computational overhead. In addition, the overlapping shared structure helps to quickly leverage all acquired knowledge to new tasks, empowering a single network capable of supporting multiple incremental tasks (without the separate sub-network mask for each ta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#19987;&#27880;&#20110;&#31227;&#21160;&#24320;&#21457;&#30340;&#22242;&#38431;&#20013;&#20351;&#29992;AI&#36741;&#21161;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#36827;&#34892;&#25216;&#26415;&#20837;&#32844;&#21644;&#25216;&#26415;&#22534;&#26632;&#20999;&#25442;&#38454;&#27573;&#30340;&#38382;&#39064;&#27714;&#35299;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;AI-Code&#29983;&#25104;&#22120;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#38388;&#12289;&#27491;&#30830;&#24615;&#21644;&#25216;&#26415;&#38598;&#25104;&#31561;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#65292;&#20197;&#30830;&#23450;&#20351;&#29992;AI&#36741;&#21161;&#32534;&#31243;&#24037;&#20855;&#26159;&#21542;&#23545;&#24320;&#21457;&#20154;&#21592;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04736</link><description>&lt;p&gt;
&#26696;&#20363;&#30740;&#31350;&#65306;&#22312;&#31227;&#21160;&#22242;&#38431;&#20013;&#20351;&#29992;AI&#36741;&#21161;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Case Study: Using AI-Assisted Code Generation In Mobile Teams. (arXiv:2308.04736v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#19987;&#27880;&#20110;&#31227;&#21160;&#24320;&#21457;&#30340;&#22242;&#38431;&#20013;&#20351;&#29992;AI&#36741;&#21161;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#36827;&#34892;&#25216;&#26415;&#20837;&#32844;&#21644;&#25216;&#26415;&#22534;&#26632;&#20999;&#25442;&#38454;&#27573;&#30340;&#38382;&#39064;&#27714;&#35299;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;AI-Code&#29983;&#25104;&#22120;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#38388;&#12289;&#27491;&#30830;&#24615;&#21644;&#25216;&#26415;&#38598;&#25104;&#31561;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#65292;&#20197;&#30830;&#23450;&#20351;&#29992;AI&#36741;&#21161;&#32534;&#31243;&#24037;&#20855;&#26159;&#21542;&#23545;&#24320;&#21457;&#20154;&#21592;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22312;&#19987;&#27880;&#20110;Kotlin&#21644;Swift&#31561;&#21407;&#29983;&#31227;&#21160;&#35821;&#35328;&#30340;&#23454;&#38469;&#31227;&#21160;&#24320;&#21457;&#22242;&#38431;&#20013;&#20351;&#29992;AI&#36741;&#21161;&#32534;&#31243;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#24191;&#27867;&#30340;&#26696;&#20363;&#30740;&#31350;&#28041;&#21450;16&#21517;&#21442;&#19982;&#32773;&#21644;2&#21517;&#25216;&#26415;&#35780;&#23457;&#20154;&#21592;&#65292;&#26469;&#33258;&#19968;&#20010;&#36719;&#20214;&#24320;&#21457;&#37096;&#38376;&#65292;&#26088;&#22312;&#20102;&#35299;&#22312;&#22242;&#38431;&#30340;&#29305;&#23450;&#38454;&#27573;&#20013;&#20351;&#29992;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35757;&#32451;&#30340;LLMs&#30340;&#24433;&#21709;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#25216;&#26415;&#20837;&#32844;&#21644;&#25216;&#26415;&#22534;&#26632;&#20999;&#25442;&#12290;&#30740;&#31350;&#20351;&#29992;&#38024;&#23545;&#27599;&#20010;&#38454;&#27573;&#30340;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#35201;&#27714;&#21442;&#19982;&#32773;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;AI-Code&#29983;&#25104;&#22120;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;ReviewerScore&#36825;&#19968;&#29305;&#23450;&#20110;&#26412;&#35770;&#25991;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#20174;&#23454;&#38469;&#34892;&#19994;&#26631;&#20934;&#65288;&#21512;&#24182;&#35831;&#27714;&#30340;&#20195;&#30721;&#35780;&#23457;&#20154;&#21592;&#65289;&#20013;&#25552;&#21462;&#30340;&#24230;&#37327;&#26102;&#38388;&#12289;&#27491;&#30830;&#24615;&#21644;&#25216;&#26415;&#38598;&#25104;&#12290;&#36755;&#20986;&#19982;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#19968;&#36215;&#36716;&#25442;&#21644;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20351;&#29992;AI&#36741;&#21161;&#32534;&#31243;&#24037;&#20855;&#26159;&#21542;&#23545;&#33719;&#24471;&#24320;&#21457;&#20154;&#21592;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to evaluate the performance of AI-assisted programming in actual mobile development teams that are focused on native mobile languages like Kotlin and Swift. The extensive case study involves 16 participants and 2 technical reviewers, from a software development department designed to understand the impact of using LLMs trained for code generation in specific phases of the team, more specifically, technical onboarding and technical stack switch. The study uses technical problems dedicated to each phase and requests solutions from the participants with and without using AI-Code generators. It measures time, correctness, and technical integration using ReviewerScore, a metric specific to the paper and extracted from actual industry standards, the code reviewers of merge requests. The output is converted and analyzed together with feedback from the participants in an attempt to determine if using AI-assisted programming tools will have an impact on getting develope
&lt;/p&gt;</description></item><item><title>JEN-1&#26159;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#21644;&#24310;&#32493;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04729</link><description>&lt;p&gt;
JEN-1&#65306;&#20855;&#26377;&#20840;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#24341;&#23548;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models. (arXiv:2308.04729v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04729
&lt;/p&gt;
&lt;p&gt;
JEN-1&#26159;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#21644;&#24310;&#32493;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#38899;&#20048;&#29983;&#25104;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#38899;&#20048;&#65288;&#21363;&#25991;&#26412;&#21040;&#38899;&#20048;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#38899;&#20048;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#21644;&#39640;&#37319;&#26679;&#29575;&#30340;&#35201;&#27714;&#12290;&#23613;&#31649;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#38899;&#20048;&#36136;&#37327;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;JEN-1&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#30340;&#36890;&#29992;&#39640;&#20445;&#30495;&#27169;&#22411;&#12290;JEN-1&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;JEN-1&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#20197;&#21450;&#24310;&#32493;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;JEN-1&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#21487;&#22312;&#27492;&#32593;&#22336;&#33719;&#21462;&#65306;http://URL
&lt;/p&gt;
&lt;p&gt;
Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task's significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through in-context learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1's superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at this http URL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#38750;&#20256;&#36882;&#24615;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#27743;&#20426;&#31639;&#27861;&#65292;&#32467;&#21512;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65292;&#20197;&#36924;&#36817;&#32435;&#20160;&#22343;&#34913;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#35937;&#26827;&#23545;&#24328;&#20013;&#30340;&#32988;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04719</link><description>&lt;p&gt;
JiangJun&#65306;&#36890;&#36807;&#24212;&#23545;&#20108;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#38750;&#20256;&#36882;&#24615;&#26469;&#25484;&#25569;&#35937;&#26827;
&lt;/p&gt;
&lt;p&gt;
JiangJun: Mastering Xiangqi by Tackling Non-Transitivity in Two-Player Zero-Sum Games. (arXiv:2308.04719v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#38750;&#20256;&#36882;&#24615;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#27743;&#20426;&#31639;&#27861;&#65292;&#32467;&#21512;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65292;&#20197;&#36924;&#36817;&#32435;&#20160;&#22343;&#34913;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#35937;&#26827;&#23545;&#24328;&#20013;&#30340;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#35937;&#26827;&#31561;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#30340;&#38750;&#20256;&#36882;&#24615;&#36827;&#34892;&#32463;&#39564;&#24615;&#25506;&#32034;&#65292;&#20998;&#26512;&#20102;&#36229;&#36807;10,000&#20010;&#20154;&#31867;&#35937;&#26827;&#23545;&#23616;&#35760;&#24405;&#65292;&#21457;&#29616;&#20102;&#28216;&#25103;&#25112;&#30053;&#32467;&#26500;&#20013;&#23384;&#22312;&#20256;&#36882;&#24615;&#21644;&#38750;&#20256;&#36882;&#24615;&#20803;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#20256;&#36882;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27743;&#20426;&#31639;&#27861;&#65292;&#36825;&#26159;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65288;PSRO&#65289;&#30340;&#21019;&#26032;&#32452;&#21512;&#65292;&#26088;&#22312;&#36924;&#36817;&#32435;&#20160;&#22343;&#34913;&#12290;&#25105;&#20204;&#20351;&#29992;&#24494;&#20449;&#23567;&#31243;&#24207;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#29609;&#23478;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;&#22823;&#24072;&#32423;&#21035;&#65292;&#32988;&#29575;&#36798;&#21040;99.41&#65285;&#12290;&#22823;&#37327;&#25351;&#26631;&#65292;&#22914;&#30456;&#23545;&#20154;&#21475;&#34920;&#29616;&#21644;&#21487;&#35270;&#21270;&#32467;&#26524;&#65292;&#35777;&#23454;&#20102;&#35813;&#31639;&#27861;&#20811;&#26381;&#38750;&#20256;&#36882;&#24615;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#31449;&#21487;&#22312;\url{https://sites.google.com/view/jiangjun-site/}&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an empirical exploration of non-transitivity in perfect-information games, specifically focusing on Xiangqi, a traditional Chinese board game comparable in game-tree complexity to chess and shogi. By analyzing over 10,000 records of human Xiangqi play, we highlight the existence of both transitive and non-transitive elements within the game's strategic structure. To address non-transitivity, we introduce the JiangJun algorithm, an innovative combination of Monte-Carlo Tree Search (MCTS) and Policy Space Response Oracles (PSRO) designed to approximate a Nash equilibrium. We evaluate the algorithm empirically using a WeChat mini program and achieve a Master level with a 99.41\% win rate against human players. The algorithm's effectiveness in overcoming non-transitivity is confirmed by a plethora of metrics, such as relative population performance and visualization results. Our project site is available at \url{https://sites.google.com/view/jiangjun-site/}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#24322;&#24120;&#24402;&#22240;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#25200;&#21160;&#26469;&#35745;&#31639;&#27599;&#20010;&#36755;&#20837;&#21464;&#37327;&#30340;&#24402;&#22240;&#24471;&#20998;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#24471;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04708</link><description>&lt;p&gt;
&#29983;&#25104;&#25200;&#21160;&#20998;&#26512;&#29992;&#20110;&#27010;&#29575;&#40657;&#30418;&#24322;&#24120;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution. (arXiv:2308.04708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#24322;&#24120;&#24402;&#22240;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#25200;&#21160;&#26469;&#35745;&#31639;&#27599;&#20010;&#36755;&#20837;&#21464;&#37327;&#30340;&#24402;&#22240;&#24471;&#20998;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#24471;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#40657;&#30418;&#22238;&#24402;&#27169;&#22411;&#35774;&#32622;&#20013;&#30340;&#27010;&#29575;&#24322;&#24120;&#24402;&#22240;&#20219;&#21153;&#65292;&#26088;&#22312;&#35745;&#31639;&#27599;&#20010;&#36755;&#20837;&#21464;&#37327;&#30340;&#24402;&#22240;&#24471;&#20998;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#32473;&#23450;&#19968;&#20010;&#35266;&#23519;&#21040;&#30340;&#24322;&#24120;&#12290;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#21487;&#29992;&#12290;&#19982;&#26631;&#20934;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24773;&#26223;&#19981;&#21516;&#65292;&#36825;&#20010;&#20219;&#21153;&#24076;&#26395;&#35299;&#37322;&#19982;&#40657;&#30418;&#39044;&#27979;&#30340;&#24322;&#24120;&#20559;&#24046;&#65292;&#32780;&#19981;&#26159;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#26412;&#36523;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#20027;&#27969;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26041;&#27861;&#65292;&#22914;Shapley&#20540;&#65292;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#8220;&#20559;&#24046;&#26080;&#20851;&#23646;&#24615;&#8221;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#24322;&#24120;&#24402;&#22240;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#35745;&#31639;&#24402;&#22240;&#24471;&#20998;&#20316;&#20026;&#39044;&#27979;&#22343;&#20540;&#65292;&#36824;&#21487;&#20197;&#37327;&#21270;&#36825;&#20123;&#24471;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#29983;&#25104;&#36807;&#31243;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#23545;&#25200;&#21160;&#36827;&#34892;&#21453;&#20107;&#23454;&#22320;&#23558;&#35266;&#27979;&#21040;&#30340;&#24322;&#24120;&#35266;&#23519;&#24674;&#22797;&#21040;&#27491;&#24120;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the task of probabilistic anomaly attribution in the black-box regression setting, where the goal is to compute the probability distribution of the attribution score of each input variable, given an observed anomaly. The training dataset is assumed to be unavailable. This task differs from the standard XAI (explainable AI) scenario, since we wish to explain the anomalous deviation from a black-box prediction rather than the black-box model itself.  We begin by showing that mainstream model-agnostic explanation methods, such as the Shapley values, are not suitable for this task because of their ``deviation-agnostic property.'' We then propose a novel framework for probabilistic anomaly attribution that allows us to not only compute attribution scores as the predictive mean but also quantify the uncertainty of those scores. This is done by considering a generative process for perturbations that counter-factually bring the observed anomalous observation back to normalcy. We int
&lt;/p&gt;</description></item><item><title>&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#39592;&#31185;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#20063;&#24102;&#26469;&#26426;&#36935;&#12290;&#20026;&#20102;&#23454;&#29616;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#38656;&#35201;&#24320;&#21457;&#27880;&#37325;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#31639;&#27861;&#65292;&#24182;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.04696</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#39592;&#31185;&#20013;&#30340;&#24212;&#29992;: &#25361;&#25112;&#12289;&#26426;&#36935;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Explainable AI in Orthopedics: Challenges, Opportunities, and Prospects. (arXiv:2308.04696v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04696
&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#39592;&#31185;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#20063;&#24102;&#26469;&#26426;&#36935;&#12290;&#20026;&#20102;&#23454;&#29616;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#38656;&#35201;&#24320;&#21457;&#27880;&#37325;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#31639;&#27861;&#65292;&#24182;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#24212;&#29992;&#65292;&#20294;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#21364;&#30053;&#26174;&#28382;&#21518;&#12290;&#20854;&#20013;&#19968;&#20123;&#22240;&#32032;&#21253;&#25324;&#30417;&#31649;&#26694;&#26550;&#12289;&#24739;&#32773;&#38544;&#31169;&#38382;&#39064;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#31561;&#24433;&#21709;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#22312;&#39592;&#31185;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#38459;&#30861;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#26045;&#12290;&#35299;&#20915;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#39592;&#31185;&#20013;&#30340;&#38382;&#39064;&#38656;&#35201;&#24320;&#21457;&#27880;&#37325;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#20351;&#20020;&#24202;&#21307;&#29983;&#12289;&#22806;&#31185;&#21307;&#29983;&#21644;&#24739;&#32773;&#33021;&#22815;&#29702;&#35299;&#20219;&#20309;&#20381;&#36182;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#25110;&#25551;&#36848;&#27169;&#22411;&#30340;&#22240;&#32032;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#39592;&#31185;&#23454;&#36341;&#20013;&#30340;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#20043;&#38388;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
While artificial intelligence (AI) has made many successful applications in various domains, its adoption in healthcare lags a little bit behind other high-stakes settings. Several factors contribute to this slower uptake, including regulatory frameworks, patient privacy concerns, and data heterogeneity. However, one significant challenge that impedes the implementation of AI in healthcare, particularly in orthopedics, is the lack of explainability and interpretability around AI models. Addressing the challenge of explainable AI (XAI) in orthopedics requires developing AI models and algorithms that prioritize transparency and interpretability, allowing clinicians, surgeons, and patients to understand the contributing factors behind any AI-powered predictive or descriptive models. The current contribution outlines several key challenges and opportunities that manifest in XAI in orthopedic practice. This work emphasizes the need for interdisciplinary collaborations between AI practitione
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#23427;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04690</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Finite Element Operator Network for Solving Parametric PDEs. (arXiv:2308.04690v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#23427;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#25105;&#20204;&#29702;&#35299;&#21644;&#39044;&#27979;&#29289;&#29702;&#12289;&#24037;&#31243;&#21644;&#37329;&#34701;&#31561;&#20247;&#22810;&#39046;&#22495;&#33258;&#28982;&#29616;&#35937;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#21442;&#25968;PDE&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26377;&#38480;&#20803;&#27861;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#37197;&#23545;&#30340;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#34920;&#26126;&#23427;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;FEONet&#26694;&#26550;&#22312;&#27169;&#25311;&#20855;&#26377;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#21644;&#22797;&#26434;&#22495;&#30340;&#21508;&#31181;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) underlie our understanding and prediction of natural phenomena across numerous fields, including physics, engineering, and finance. However, solving parametric PDEs is a complex task that necessitates efficient numerical methods. In this paper, we propose a novel approach for solving parametric PDEs using a Finite Element Operator Network (FEONet). Our proposed method leverages the power of deep learning in conjunction with traditional numerical methods, specifically the finite element method, to solve parametric PDEs in the absence of any paired input-output training data. We demonstrate the effectiveness of our approach on several benchmark problems and show that it outperforms existing state-of-the-art methods in terms of accuracy, generalization, and computational flexibility. Our FEONet framework shows potential for application in various fields where PDEs play a crucial role in modeling complex domains with diverse boundary conditions and sin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#32593;&#32476;&#29228;&#34411;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#30830;&#23450;&#32593;&#39029;&#25490;&#21517;&#20197;&#21450;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#32593;&#39029;&#12290;&#24182;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#21644;robot.txt&#25991;&#20214;&#30340;&#22522;&#26412;&#26684;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.04689</link><description>&lt;p&gt;
&#32593;&#32476;&#29228;&#34411;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
web crawler strategies for web pages under robot.txt restriction. (arXiv:2308.04689v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#32593;&#32476;&#29228;&#34411;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#30830;&#23450;&#32593;&#39029;&#25490;&#21517;&#20197;&#21450;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#32593;&#39029;&#12290;&#24182;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#21644;robot.txt&#25991;&#20214;&#30340;&#22522;&#26412;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#65292;&#25152;&#26377;&#20154;&#37117;&#20102;&#35299;&#20114;&#32852;&#32593;&#24182;&#27599;&#22825;&#22312;&#20114;&#32852;&#32593;&#19978;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20026;&#29992;&#25143;&#36755;&#20837;&#30340;&#20851;&#38190;&#23383;&#36827;&#34892;&#25628;&#32034;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#20026;&#19978;&#32593;&#32773;&#25552;&#20379;&#26041;&#20415;&#30340;&#32467;&#26524;&#12290;&#19978;&#32593;&#32773;&#36873;&#25321;&#25490;&#21517;&#38752;&#21069;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#20294;&#26159;&#32593;&#39029;&#30340;&#25490;&#21517;&#26159;&#22914;&#20309;&#30001;&#25628;&#32034;&#24341;&#25806;&#30830;&#23450;&#30340;&#65311;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#25152;&#26377;&#32593;&#39029;&#65311;&#26412;&#25991;&#32473;&#20986;&#20102;&#25152;&#26377;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#20026;&#25628;&#32034;&#24341;&#25806;&#24037;&#20316;&#30340;&#32593;&#32476;&#29228;&#34411;&#21644;&#32593;&#32476;&#29228;&#34411;&#30340;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#12290;&#32593;&#31449;&#31649;&#29702;&#21592;&#20351;&#29992;robot.txt&#25991;&#20214;&#20013;&#30340;&#19981;&#21516;&#38480;&#21046;&#35268;&#21017;&#25351;&#23548;&#32593;&#32476;&#29228;&#34411;&#65292;&#26412;&#25991;&#36824;&#25552;&#21040;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;robot.txt&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present time, all know about World Wide Web and work over the Internet daily. In this paper, we introduce the search engines working for keywords that are entered by users to find something. The search engine uses different search algorithms for convenient results for providing to the net surfer. Net surfers go with the top search results but how did the results of web pages get higher ranks over search engines? how the search engine got that all the web pages in the database? This paper gives the answers to all these kinds of basic questions. Web crawlers working for search engines and robot exclusion protocol rules for web crawlers are also addressed in this research paper. Webmaster uses different restriction facts in robot.txt file to instruct web crawler, some basic formats of robot.txt are also mentioned in this paper.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#24555;&#36895;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#20013;&#29983;&#25104;&#27880;&#37322;&#25968;&#25454;&#30340;&#19987;&#19994;&#30693;&#35782;&#12289;&#26102;&#38388;&#21644;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#24369;&#30417;&#30563;&#21644;&#24378;&#30417;&#30563;&#30446;&#26631;&#23450;&#20301;&#27169;&#22411;&#19978;&#22343;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04687</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#24555;&#36895;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#20197;&#36827;&#34892;&#20998;&#31867;&#21644;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Rapid Training Data Creation by Synthesizing Medical Images for Classification and Localization. (arXiv:2308.04687v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04687
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#24555;&#36895;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#20013;&#29983;&#25104;&#27880;&#37322;&#25968;&#25454;&#30340;&#19987;&#19994;&#30693;&#35782;&#12289;&#26102;&#38388;&#21644;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#24369;&#30417;&#30563;&#21644;&#24378;&#30417;&#30563;&#30446;&#26631;&#23450;&#20301;&#27169;&#22411;&#19978;&#22343;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#21307;&#23398;&#39046;&#22495;&#29983;&#25104;&#27880;&#37322;&#25968;&#25454;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#12289;&#26102;&#38388;&#21644;&#25104;&#26412;&#26174;&#33879;&#39640;&#12290;&#24378;&#30417;&#30563;&#30446;&#26631;&#23450;&#20301;&#27169;&#22411;&#38656;&#35201;&#35814;&#23613;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#35782;&#21035;&#22270;&#20687;&#20013;&#25152;&#26377;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#32780;&#35328;&#65292;&#36825;&#26159;&#38590;&#20197;&#23454;&#29616;&#21644;&#39564;&#35777;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30495;&#23454;&#25968;&#25454;&#36716;&#21270;&#20026;&#35757;&#32451;&#20219;&#20309;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24369;&#30417;&#30563;&#23450;&#20301;&#27169;&#22411;&#21644;&#24378;&#30417;&#30563;&#23450;&#20301;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#26126;&#26174;&#25552;&#39640;&#20102;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;&#23545;&#20110;&#24378;&#30417;&#30563;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#35814;&#23613;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#22312;&#21518;&#32773;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
While the use of artificial intelligence (AI) for medical image analysis is gaining wide acceptance, the expertise, time and cost required to generate annotated data in the medical field are significantly high, due to limited availability of both data and expert annotation. Strongly supervised object localization models require data that is exhaustively annotated, meaning all objects of interest in an image are identified. This is difficult to achieve and verify for medical images. We present a method for the transformation of real data to train any Deep Neural Network to solve the above problems. We show the efficacy of this approach on both a weakly supervised localization model and a strongly supervised localization model. For the weakly supervised model, we show that the localization accuracy increases significantly using the generated data. For the strongly supervised model, this approach overcomes the need for exhaustive annotation on real images. In the latter model, we show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;Sci-CoT&#26694;&#26550;&#65292;&#20998;&#31163;&#20102;&#29983;&#25104;&#29702;&#30001;&#21644;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.04679</link><description>&lt;p&gt;
Sci-CoT: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#23567;&#22411;&#31185;&#23398;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA. (arXiv:2308.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;Sci-CoT&#26694;&#26550;&#65292;&#20998;&#31163;&#20102;&#29983;&#25104;&#29702;&#30001;&#21644;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24191;&#27867;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#33021;&#21147;&#24402;&#21151;&#20110;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#35268;&#27169;&#21644;&#23545;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;LLMs&#23637;&#29616;&#20986;&#22686;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#23545;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#36825;&#24402;&#21151;&#20110;&#19968;&#31181;&#21517;&#20026;"&#24605;&#32500;&#38142; (CoT)&#25552;&#31034;"&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#24341;&#23548;&#26368;&#32456;&#31572;&#26696;&#25512;&#29702;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#24378;&#35843;&#30340;&#26159;&#65292;&#36825;&#20123;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#20284;&#20046;&#21482;&#22312;&#20855;&#26377;&#33267;&#23569;100&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sci-CoT&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#20998;&#31163;&#20102;&#29983;&#25104;&#29702;&#30001;&#21644;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown outstanding performance across wide range of downstream tasks. This competency is attributed to their substantial parameter size and pre-training on extensive corpus. Moreover, LLMs have exhibited enhanced reasoning capabilities in tackling complex reasoning tasks, owing to the utilization of a method named ``Chain-of-Thought (CoT) prompting''. This method is designed to generate intermediate reasoning steps that guide the inference of the final answer. However, it is essential to highlight that these advanced reasoning abilities appear to emerge in models with a minimum of 10 billion parameters, thereby limiting its efficacy in situations where computational resources are constrained. In this paper, we investigate the possibility of transferring the reasoning capabilities of LLMs to smaller models via knowledge distillation. Specifically, we propose Sci-CoT, a two-stage framework that separates the processes of generating rationales and inferrin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31181;&#26063;&#20998;&#24067;&#30340;&#23376;&#37319;&#26679;&#35757;&#32451;&#38598;&#65292;&#24182;&#35780;&#20272;&#27169;&#25311;&#20013;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#23039;&#21183;&#38754;&#37096;&#30340;&#36739;&#23567;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#20294;&#22312;&#20855;&#26377;&#26356;&#22823;&#38754;&#37096;&#21464;&#24322;&#30340;&#36739;&#22823;&#25968;&#25454;&#38598;&#20013;&#65292;&#31181;&#26063;&#24179;&#34913;&#20173;&#28982;&#26080;&#27861;&#23454;&#29616;&#19981;&#21516;&#31181;&#26063;&#32676;&#20307;&#20043;&#38388;&#30340;&#27979;&#35797;&#24615;&#33021;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04674</link><description>&lt;p&gt;
&#35299;&#20915;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Racial Bias in Facial Emotion Recognition. (arXiv:2308.04674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31181;&#26063;&#20998;&#24067;&#30340;&#23376;&#37319;&#26679;&#35757;&#32451;&#38598;&#65292;&#24182;&#35780;&#20272;&#27169;&#25311;&#20013;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#23039;&#21183;&#38754;&#37096;&#30340;&#36739;&#23567;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#20294;&#22312;&#20855;&#26377;&#26356;&#22823;&#38754;&#37096;&#21464;&#24322;&#30340;&#36739;&#22823;&#25968;&#25454;&#38598;&#20013;&#65292;&#31181;&#26063;&#24179;&#34913;&#20173;&#28982;&#26080;&#27861;&#23454;&#29616;&#19981;&#21516;&#31181;&#26063;&#32676;&#20307;&#20043;&#38388;&#30340;&#27979;&#35797;&#24615;&#33021;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35757;&#32451;&#38598;&#20855;&#26377;&#39640;&#32500;&#24230;&#36755;&#20837;&#21644;&#20027;&#35266;&#26631;&#31614;&#30340;&#20844;&#24179;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#30740;&#31350;&#19981;&#36275;&#30340;&#39046;&#22495;&#12290;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#24120;&#24120;&#23384;&#22312;&#31181;&#26063;&#19981;&#24179;&#34913;&#30340;&#39046;&#22495;&#65292;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#19981;&#21516;&#31181;&#26063;&#32676;&#20307;&#20043;&#38388;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#20998;&#26512;&#36890;&#36807;&#23545;&#20855;&#26377;&#19981;&#21516;&#31181;&#26063;&#20998;&#24067;&#30340;&#35757;&#32451;&#38598;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#24182;&#35780;&#20272;&#36825;&#20123;&#27169;&#25311;&#20013;&#30340;&#27979;&#35797;&#24615;&#33021;&#26469;&#35299;&#20915;&#31181;&#26063;&#20559;&#35265;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#27169;&#25311;&#25509;&#36817;&#31181;&#26063;&#24179;&#34913;&#65292;&#37319;&#29992;&#36739;&#23567;&#30340;&#20855;&#26377;&#23039;&#21183;&#38754;&#37096;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#21462;&#24471;&#25913;&#21892;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;F1&#24471;&#20998;&#24179;&#22343;&#25552;&#39640;&#20102;27.2&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#19988;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#24615;&#24179;&#22343;&#25552;&#39640;&#20102;15.7&#20010;&#30334;&#20998;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#26356;&#22823;&#38754;&#37096;&#21464;&#24322;&#30340;&#36739;&#22823;&#25968;&#25454;&#38598;&#20013;&#65292;&#20844;&#24179;&#24615;&#25351;&#26631;&#36890;&#24120;&#20445;&#25345;&#19981;&#21464;&#65292;&#36825;&#34920;&#26126;&#20165;&#20165;&#31181;&#26063;&#24179;&#34913;&#26159;&#26080;&#27861;&#23454;&#29616;&#22312;&#19981;&#21516;&#31181;&#26063;&#32676;&#20307;&#20043;&#38388;&#27979;&#35797;&#24615;&#33021;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in deep learning models trained with high-dimensional inputs and subjective labels remains a complex and understudied area. Facial emotion recognition, a domain where datasets are often racially imbalanced, can lead to models that yield disparate outcomes across racial groups. This study focuses on analyzing racial bias by sub-sampling training sets with varied racial distributions and assessing test performance across these simulations. Our findings indicate that smaller datasets with posed faces improve on both fairness and performance metrics as the simulations approach racial balance. Notably, the F1-score increases by $27.2\%$ points, and demographic parity increases by $15.7\%$ points on average across the simulations. However, in larger datasets with greater facial variation, fairness metrics generally remain constant, suggesting that racial balance by itself is insufficient to achieve parity in test performance across different racial groups.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SSL-Auth&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36873;&#25321;&#30340;&#20851;&#38190;&#26679;&#26412;&#20316;&#20026;&#27700;&#21360;&#20449;&#24687;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#39564;&#35777;&#32593;&#32476;&#26469;&#37325;&#26500;&#27700;&#21360;&#20449;&#24687;&#65292;&#20174;&#32780;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.04673</link><description>&lt;p&gt;
SSL-Auth:&#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SSL-Auth: An Authentication Framework by Fragile Watermarking for Pre-trained Encoders in Self-supervised Learning. (arXiv:2308.04673v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SSL-Auth&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36873;&#25321;&#30340;&#20851;&#38190;&#26679;&#26412;&#20316;&#20026;&#27700;&#21360;&#20449;&#24687;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#39564;&#35777;&#32593;&#32476;&#26469;&#37325;&#26500;&#27700;&#21360;&#20449;&#24687;&#65292;&#20174;&#32780;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20026;&#39044;&#35757;&#32451;&#30340;&#24378;&#22823;&#32534;&#30721;&#22120;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#36825;&#20123;&#32534;&#30721;&#22120;&#24120;&#34987;&#29992;&#20316;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20854;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22312;&#21830;&#19994;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#65292;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32534;&#30721;&#22120;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12289;&#23545;&#25239;&#25915;&#20987;&#31561;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#39564;&#35777;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#23436;&#25972;&#24615;&#30340;&#26041;&#26696;&#26469;&#20445;&#25252;&#29992;&#25143;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SSL-Auth&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36873;&#25321;&#30340;&#20851;&#38190;&#26679;&#26412;&#20316;&#20026;&#27700;&#21360;&#20449;&#24687;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#39564;&#35777;&#32593;&#32476;&#26469;&#37325;&#26500;&#27700;&#21360;&#20449;&#24687;&#65292;&#20174;&#32780;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) which leverages unlabeled datasets for pre-training powerful encoders has achieved significant success in recent years. These encoders are commonly used as feature extractors for various downstream tasks, requiring substantial data and computing resources for their training process. With the deployment of pre-trained encoders in commercial use, protecting the intellectual property of model owners and ensuring the trustworthiness of the models becomes crucial. Recent research has shown that encoders are threatened by backdoor attacks, adversarial attacks, etc. Therefore, a scheme to verify the integrity of pre-trained encoders is needed to protect users. In this paper, we propose SSL-Auth, the first fragile watermarking scheme for verifying the integrity of encoders without compromising model performance. Our method utilizes selected key samples as watermark information and trains a verification network to reconstruct the watermark information, thereby ver
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#21463;&#38480;&#19979;&#36890;&#36807;&#26497;&#23567;&#21270;&#26368;&#22823;&#21270;&#20248;&#21270;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04672</link><description>&lt;p&gt;
&#36164;&#28304;&#21463;&#38480;&#19979;&#36890;&#36807;&#26497;&#23567;&#21270;&#26368;&#22823;&#21270;&#20248;&#21270;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Resource Constrained Model Compression via Minimax Optimization for Spiking Neural Networks. (arXiv:2308.04672v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04672
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#21463;&#38480;&#19979;&#36890;&#36807;&#26497;&#23567;&#21270;&#26368;&#22823;&#21270;&#20248;&#21270;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#37096;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#21644;&#39640;&#33021;&#25928;&#30340;&#29305;&#28857;&#65292;&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#22312;&#31070;&#32463;&#20803;&#20223;&#30495;&#33455;&#29255;&#31561;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26102;&#26377;&#25152;&#19981;&#21516;&#12290;&#22823;&#37096;&#20998;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;SNNs&#35757;&#32451;&#31574;&#30053;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#24341;&#20837;&#26356;&#22823;&#26356;&#28145;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#20294;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30452;&#25509;&#37096;&#32626;&#36825;&#20123;&#22797;&#26434;&#32593;&#32476;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#65292;&#20154;&#20204;&#36890;&#36807;&#35880;&#24910;&#22320;&#23545;SNNs&#36827;&#34892;&#21387;&#32553;&#65292;&#20197;&#24179;&#34913;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#29616;&#26377;&#30340;&#21387;&#32553;&#26041;&#27861;&#35201;&#20040;&#36890;&#36807;&#26435;&#37325;&#33539;&#25968;&#22823;&#23567;&#36845;&#20195;&#22320;&#21098;&#26525;SNNs&#65292;&#35201;&#20040;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#31232;&#30095;&#23398;&#20064;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31471;&#21040;&#31471;&#26497;&#23567;&#21270;&#26368;&#22823;&#21270;&#20248;&#21270;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;SNNs&#19978;&#32852;&#21512;&#24212;&#29992;&#21387;&#32553;&#21644;&#24494;&#35843;&#20248;&#20110;&#39034;&#24207;&#24212;&#29992;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired Spiking Neural Networks (SNNs) have the characteristics of event-driven and high energy-efficient, which are different from traditional Artificial Neural Networks (ANNs) when deployed on edge devices such as neuromorphic chips. Most previous work focuses on SNNs training strategies to improve model performance and brings larger and deeper network architectures. It is difficult to deploy these complex networks on resource-limited edge devices directly. To meet such demand, people compress SNNs very cautiously to balance the performance and the computation efficiency. Existing compression methods either iteratively pruned SNNs using weights norm magnitude or formulated the problem as a sparse learning optimization. We propose an improved end-to-end Minimax optimization method for this sparse learning problem to better balance the model performance and the computation efficiency. We also demonstrate that jointly applying compression and finetuning on SNNs is better than seq
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#30772;&#22351;&#21644;&#20462;&#22797;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#26497;&#22823;&#35268;&#27169;&#30340;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#36882;&#24402;&#20462;&#22797;&#21644;&#21387;&#32553;&#36755;&#20837;&#23454;&#20363;&#30340;&#23618;&#27425;&#21270;&#25628;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#39640;&#31454;&#20105;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.04639</link><description>&lt;p&gt;
&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#30772;&#22351;&#21644;&#20462;&#22797;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#26497;&#22823;&#35268;&#27169;&#30340;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Destroy and Repair Approach for Solving Very Large-Scale Travelling Salesman Problem. (arXiv:2308.04639v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04639
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#30772;&#22351;&#21644;&#20462;&#22797;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#26497;&#22823;&#35268;&#27169;&#30340;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#36882;&#24402;&#20462;&#22797;&#21644;&#21387;&#32553;&#36755;&#20837;&#23454;&#20363;&#30340;&#23618;&#27425;&#21270;&#25628;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#39640;&#31454;&#20105;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35268;&#27169;&#24222;&#22823;&#30340;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;(TSP)&#65292;&#29616;&#26377;&#31639;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26041;&#38754;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#30772;&#22351;&#21644;&#20462;&#22797;(HDR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#30772;&#22351;&#21644;&#20462;&#22797;&#25805;&#20316;&#26469;&#25913;&#36827;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21019;&#26032;&#27010;&#24565;&#26159;&#23618;&#27425;&#21270;&#25628;&#32034;&#26694;&#26550;&#65292;&#23427;&#36882;&#24402;&#20462;&#22797;&#37096;&#20998;&#36793;&#65292;&#24182;&#23558;&#36755;&#20837;&#23454;&#20363;&#21387;&#32553;&#20026;&#20855;&#26377;&#26576;&#31181;&#31561;&#20215;&#20445;&#35777;&#30340;&#23567;&#35268;&#27169;TSP&#12290;&#36825;&#20010;&#31616;&#21333;&#26126;&#20102;&#30340;&#25628;&#32034;&#26694;&#26550;&#33021;&#22815;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#39640;&#31454;&#20105;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#21313;&#20061;&#20010;&#33879;&#21517;&#30340;&#22823;&#35268;&#27169;&#23454;&#20363;(&#21253;&#21547;10,000&#21040;10,000,000&#20010;&#22478;&#24066;)&#30340;&#20844;&#24179;&#27604;&#36739;&#34920;&#26126;&#65292;HDR&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26041;&#38754;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;TSP&#31639;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20855;&#26377;3,162,278&#21644;10,000,000&#20010;&#22478;&#24066;&#30340;&#20004;&#20010;&#22823;&#35268;&#27169;&#23454;&#20363;&#19978;&#65292;HDR&#25171;&#30772;&#20102;&#19990;&#30028;&#32426;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
For prohibitively large-scale Travelling Salesman Problems (TSPs), existing algorithms face big challenges in terms of both computational efficiency and solution quality. To address this issue, we propose a hierarchical destroy-and-repair (HDR) approach, which attempts to improve an initial solution by applying a series of carefully designed destroy-and-repair operations. A key innovative concept is the hierarchical search framework, which recursively fixes partial edges and compresses the input instance into a small-scale TSP under some equivalence guarantee. This neat search framework is able to deliver highly competitive solutions within a reasonable time. Fair comparisons based on nineteen famous large-scale instances (with 10,000 to 10,000,000 cities) show that HDR is highly competitive against existing state-of-the-art TSP algorithms, in terms of both efficiency and solution quality. Notably, on two large instances with 3,162,278 and 10,000,000 cities, HDR breaks the world record
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#31232;&#30095;&#20108;&#20540;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#21333;&#27493;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;&#23494;&#38598;&#28014;&#28857;&#21464;&#21387;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20004;&#20010;&#20462;&#25913;&#38477;&#20302;&#20102;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04637</link><description>&lt;p&gt;
&#31232;&#30095;&#20108;&#20540;&#21464;&#21387;&#22120;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sparse Binary Transformers for Multivariate Time Series Modeling. (arXiv:2308.04637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31232;&#30095;&#20108;&#20540;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#21333;&#27493;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;&#23494;&#38598;&#28014;&#28857;&#21464;&#21387;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20004;&#20010;&#20462;&#25913;&#38477;&#20302;&#20102;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#26377;&#21487;&#33021;&#23454;&#29616;&#22312;&#26032;&#24212;&#29992;&#21644;&#36739;&#23567;&#30340;&#35745;&#31639;&#29615;&#22659;&#20013;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#36825;&#31181;&#27169;&#22411;&#22312;&#21738;&#20123;&#23398;&#20064;&#20219;&#21153;&#20013;&#33021;&#22815;&#25104;&#21151;&#30340;&#20102;&#35299;&#19981;&#22810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31232;&#30095;&#21644;&#20108;&#20540;&#26435;&#37325;&#30340;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#19982;&#30456;&#21516;&#32467;&#26500;&#30340;&#23494;&#38598;&#28014;&#28857;&#21464;&#21387;&#22120;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#22909;&#30340;&#32467;&#26524;&#65306;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#21333;&#27493;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38477;&#20302;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#20462;&#25913;&#65292;&#36825;&#20004;&#20010;&#20462;&#25913;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#20960;&#20046;&#27809;&#26377;&#19979;&#38477;&#65306;1) &#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23545;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#28608;&#27963;&#24212;&#29992;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#25513;&#30721;&#65307;2) &#23545;&#20110;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#36825;&#20123;&#20219;&#21153;&#37117;&#20381;&#36182;&#20110;&#23545;&#21333;&#20010;&#26102;&#38388;&#28857;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attentio
&lt;/p&gt;</description></item><item><title>AI&#29983;&#25104;&#24335;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#20855;&#26377;&#28508;&#22312;&#36131;&#20219;&#39118;&#38505;&#30340;&#26377;&#23475;&#35328;&#35770;&#12290;&#35299;&#20915;&#27169;&#22411;&#21019;&#24314;&#32773;&#21644;&#37096;&#32626;&#32773;&#30340;&#27861;&#24459;&#36131;&#20219;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#31639;&#27861;&#35774;&#35745;&#30340;&#25216;&#26415;&#32454;&#33410;&#12290;&#38656;&#35201;&#36827;&#34892;&#28145;&#20837;&#30340;Section 230&#20813;&#36131;&#20998;&#26512;&#20197;&#21450;&#19979;&#28216;&#36131;&#20219;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.04635</link><description>&lt;p&gt;
AI&#26377;&#23475;&#35328;&#35770;&#30340;&#36131;&#20219;&#22312;&#21738;&#37324;&#65311;
&lt;/p&gt;
&lt;p&gt;
Where's the Liability in Harmful AI Speech?. (arXiv:2308.04635v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04635
&lt;/p&gt;
&lt;p&gt;
AI&#29983;&#25104;&#24335;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#20855;&#26377;&#28508;&#22312;&#36131;&#20219;&#39118;&#38505;&#30340;&#26377;&#23475;&#35328;&#35770;&#12290;&#35299;&#20915;&#27169;&#22411;&#21019;&#24314;&#32773;&#21644;&#37096;&#32626;&#32773;&#30340;&#27861;&#24459;&#36131;&#20219;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#31639;&#27861;&#35774;&#35745;&#30340;&#25216;&#26415;&#32454;&#33410;&#12290;&#38656;&#35201;&#36827;&#34892;&#28145;&#20837;&#30340;Section 230&#20813;&#36131;&#20998;&#26512;&#20197;&#21450;&#19979;&#28216;&#36131;&#20219;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;&#25991;&#26412;&#30340;&#8220;&#22522;&#30784;&#27169;&#22411;&#8221;&#65289;&#21487;&#20197;&#29983;&#25104;&#21487;&#33021;&#22312;&#24191;&#27867;&#30340;&#36131;&#20219;&#21046;&#24230;&#19979;&#24341;&#21457;&#38382;&#39064;&#30340;&#35328;&#35770;&#12290;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#32463;&#24120;&#23545;&#27169;&#22411;&#36827;&#34892;&#8220;&#32418;&#38431;&#8221;&#27979;&#35797;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#27492;&#31867;&#38382;&#39064;&#35328;&#35770;&#65292;&#20174;&#38169;&#35823;&#25351;&#36131;&#20005;&#37325;&#19981;&#31471;&#34892;&#20026;&#30340;&#8220;&#24187;&#35273;&#8221;&#21040;&#26500;&#36896;&#21407;&#23376;&#24377;&#30340;&#39135;&#35889;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#36825;&#20123;&#32418;&#38431;&#27979;&#35797;&#34892;&#20026;&#26159;&#21542;&#30495;&#30340;&#23545;&#27169;&#22411;&#21019;&#24314;&#32773;&#21644;&#37096;&#32626;&#32773;&#26500;&#25104;&#20219;&#20309;&#27861;&#24459;&#36131;&#20219;&#39118;&#38505;&#65292;&#20174;&#32780;&#28608;&#21169;&#25237;&#36164;&#20110;&#23433;&#20840;&#26426;&#21046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#36131;&#20219;&#21046;&#24230;&#65292;&#24182;&#23558;&#20854;&#19982;&#32418;&#38431;&#27979;&#35797;&#27169;&#22411;&#34892;&#20026;&#30340;&#24120;&#35265;&#20363;&#23376;&#32852;&#31995;&#36215;&#26469;&#65306;&#35837;&#35876;&#12289;&#26500;&#25104;&#29359;&#32618;&#34892;&#20026;&#30340;&#35328;&#35770;&#21644;&#38169;&#35823;&#33268;&#27515;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20219;&#20309;Section 230&#20813;&#36131;&#20998;&#26512;&#25110;&#19979;&#28216;&#36131;&#20219;&#20998;&#26512;&#37117;&#19982;&#31639;&#27861;&#35774;&#35745;&#30340;&#25216;&#26415;&#32454;&#33410;&#23494;&#20999;&#30456;&#20851;&#12290;&#32780;&#35201;&#30495;&#27491;&#25214;&#21040;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI, in particular text-based "foundation models" (large models trained on a huge variety of information including the internet), can generate speech that could be problematic under a wide range of liability regimes. Machine learning practitioners regularly "red team" models to identify and mitigate such problematic speech: from "hallucinations" falsely accusing people of serious misconduct to recipes for constructing an atomic bomb. A key question is whether these red-teamed behaviors actually present any liability risk for model creators and deployers under U.S. law, incentivizing investments in safety mechanisms. We examine three liability regimes, tying them to common examples of red-teamed model behaviors: defamation, speech integral to criminal conduct, and wrongful death. We find that any Section 230 immunity analysis or downstream liability analysis is intimately wrapped up in the technical details of algorithm design. And there are many roadblocks to truly finding mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#36817;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#26512;&#35821;&#20041;&#21464;&#21270;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25991;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.04625</link><description>&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#22312;&#35780;&#20272;&#35821;&#20041;&#21464;&#21270;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation. (arXiv:2308.04625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#36817;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#26512;&#35821;&#20041;&#21464;&#21270;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25991;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#38271;&#31687;&#30495;&#23454;&#19990;&#30028;&#25991;&#26412;&#65288;&#22914;&#20070;&#31821;&#25110;&#35760;&#24405;&#65289;&#20013;&#35821;&#20041;&#21464;&#21270;&#30340;&#27169;&#24335;&#22312;&#25991;&#20307;&#12289;&#35748;&#30693;&#21644;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#19978;&#24456;&#26377;&#36259;&#12290;&#23427;&#20063;&#26377;&#21161;&#20110;&#35832;&#22914;&#25991;&#26412;&#20998;&#27573;&#12289;&#25991;&#26723;&#25688;&#35201;&#21644;&#35821;&#20041;&#26032;&#39062;&#24615;&#26816;&#27979;&#31561;&#24212;&#29992;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#20960;&#31181;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#20351;&#24471;&#36825;&#31181;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#19981;&#21516;&#26041;&#27861;&#20135;&#29983;&#30340;&#35821;&#20041;&#34920;&#31034;&#22312;&#33258;&#36523;&#19978;&#26159;&#21542;&#19968;&#33268;&#21644;&#26377;&#24847;&#20041;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#21477;&#23376;&#35821;&#20041;&#30456;&#20284;&#24230;&#21644;&#22810;&#26412;&#25991;&#23398;&#20316;&#21697;&#30340;&#21477;&#23376;&#23545;&#35821;&#20041;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#27604;&#36739;&#20960;&#31181;&#26368;&#36817;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#12290;&#19982;&#20197;&#21069;&#20351;&#29992;&#30446;&#26631;&#20219;&#21153;&#21644;&#31574;&#21010;&#25968;&#25454;&#38598;&#26469;&#27604;&#36739;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;"&#37326;&#22806;"&#25552;&#20379;&#20102;&#23545;&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#32771;&#34385;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#30830;&#23454;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing the pattern of semantic variation in long real-world texts such as books or transcripts is interesting from the stylistic, cognitive, and linguistic perspectives. It is also useful for applications such as text segmentation, document summarization, and detection of semantic novelty. The recent emergence of several vector-space methods for sentence embedding has made such analysis feasible. However, this raises the issue of how consistent and meaningful the semantic representations produced by various methods are in themselves. In this paper, we compare several recent sentence embedding methods via time-series of semantic similarity between successive sentences and matrices of pairwise sentence similarity for multiple books of literature. In contrast to previous work using target tasks and curated datasets to compare sentence embedding methods, our approach provides an evaluation of the methods 'in the wild'. We find that most of the sentence embedding methods considered do in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;E2E&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#25351;&#26631;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04624</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#25216;&#26415;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#26041;&#27861;&#21644;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Benchmarking LLM powered Chatbots: Methods and Metrics. (arXiv:2308.04624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;E2E&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#25351;&#26631;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#23545;&#35805;&#20195;&#29702;&#65292;&#21363;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#27491;&#25104;&#20026;&#20225;&#19994;&#20026;&#23458;&#25143;&#21644;&#21512;&#20316;&#20249;&#20276;&#25552;&#20379;&#25903;&#25345;&#30340;&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#26426;&#21046;&#12290;&#20026;&#20102;&#35780;&#20272;&#29305;&#21035;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#38656;&#35201;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#36825;&#23601;&#26159;&#32842;&#22825;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#25152;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;E2E&#65288;&#31471;&#21040;&#31471;&#65289;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;E2E&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#30001;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;E2E&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#20854;&#20182;&#24120;&#29992;&#30340;&#29616;&#26377;&#25351;&#26631;&#35780;&#20272;&#20102;&#19968;&#20010;&#31034;&#20363;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19981;&#21516;&#22797;&#26434;&#31243;&#24230;&#65292;&#24182;&#35266;&#23519;&#21040;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#20854;&#20182;&#25351;&#26631;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#25351;&#26631;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#39044;&#27979;&#30340;&#65292;&#32780;&#19982;E2E&#22522;&#20934;&#27979;&#35797;&#30456;&#20851;&#30340;&#25351;&#26631;&#20351;&#29992;&#20102;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous conversational agents, i.e. chatbots, are becoming an increasingly common mechanism for enterprises to provide support to customers and partners. In order to rate chatbots, especially ones powered by Generative AI tools like Large Language Models (LLMs) we need to be able to accurately assess their performance. This is where chatbot benchmarking becomes important. In this paper, we propose the use of a novel benchmark that we call the E2E (End to End) benchmark, and show how the E2E benchmark can be used to evaluate accuracy and usefulness of the answers provided by chatbots, especially ones powered by LLMs. We evaluate an example chatbot at different levels of sophistication based on both our E2E benchmark, as well as other available metrics commonly used in the state of art, and observe that the proposed benchmark show better results compared to others. In addition, while some metrics proved to be unpredictable, the metric associated with the E2E benchmark, which uses cosi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20998;&#38454;&#27573;&#25512;&#27979;&#35299;&#30721;&#65292;&#29992;&#20110;&#21152;&#36895;&#22312;&#23567;&#25209;&#37327;&#12289;&#35774;&#22791;&#19978;&#36827;&#34892;LLM&#25512;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#26641;&#24418;&#32467;&#26500;&#30340;&#25209;&#27425;&#37325;&#32452;&#21644;&#22686;&#21152;&#31532;&#20108;&#38454;&#27573;&#30340;&#25512;&#27979;&#35299;&#30721;&#65292;&#23558;&#21333;&#25209;&#35299;&#30721;&#24310;&#36831;&#38477;&#20302;&#20102;3.16&#20493;&#65292;&#32780;&#36755;&#20986;&#36136;&#37327;&#20445;&#25345;&#23436;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.04623</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#38454;&#27573;&#25512;&#27979;&#35299;&#30721;&#21152;&#36895;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Accelerating LLM Inference with Staged Speculative Decoding. (arXiv:2308.04623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20998;&#38454;&#27573;&#25512;&#27979;&#35299;&#30721;&#65292;&#29992;&#20110;&#21152;&#36895;&#22312;&#23567;&#25209;&#37327;&#12289;&#35774;&#22791;&#19978;&#36827;&#34892;LLM&#25512;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#26641;&#24418;&#32467;&#26500;&#30340;&#25209;&#27425;&#37325;&#32452;&#21644;&#22686;&#21152;&#31532;&#20108;&#38454;&#27573;&#30340;&#25512;&#27979;&#35299;&#30721;&#65292;&#23558;&#21333;&#25209;&#35299;&#30721;&#24310;&#36831;&#38477;&#20302;&#20102;3.16&#20493;&#65292;&#32780;&#36755;&#20986;&#36136;&#37327;&#20445;&#25345;&#23436;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;LLM&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22810;&#26679;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21363;&#20998;&#38454;&#27573;&#25512;&#27979;&#35299;&#30721;&#65292;&#26469;&#21152;&#36895;&#22312;&#23567;&#25209;&#37327;&#12289;&#35774;&#22791;&#19978;&#36827;&#34892;LLM&#25512;&#29702;&#12290;&#36890;&#36807;&#25913;&#36827;&#20808;&#21069;&#30340;&#25512;&#27979;&#35299;&#30721;&#24037;&#20316;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23567;&#25209;&#37327;&#25512;&#29702;&#30340;&#20302;&#31639;&#26415;&#24378;&#24230;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25512;&#27979;&#25209;&#27425;&#37325;&#26032;&#32452;&#32455;&#25104;&#26641;&#24418;&#32467;&#26500;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#29983;&#25104;&#25104;&#26412;&#65292;&#24182;&#22686;&#21152;&#20102;&#27599;&#25209;&#39044;&#26399;&#30340;&#26631;&#35760;&#25968;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#31532;&#20108;&#38454;&#27573;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;&#32508;&#21512;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20445;&#25345;&#36755;&#20986;&#36136;&#37327;&#23436;&#32654;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#21333;&#25209;&#35299;&#30721;&#24310;&#36831;&#38477;&#20302;&#20102;3.16&#20493;&#65292;&#20351;&#29992;&#20102;762M&#21442;&#25968;&#30340;GPT-2-L&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances with large language models (LLM) illustrate their diverse capabilities. We propose a novel algorithm, staged speculative decoding, to accelerate LLM inference in small-batch, on-device scenarios. We address the low arithmetic intensity of small-batch inference by improving upon previous work in speculative decoding. First, we restructure the speculative batch as a tree, which reduces generation costs and increases the expected tokens per batch. Second, we add a second stage of speculative decoding. Taken together, we reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L model while perfectly preserving output quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25104;&#29087;&#26234;&#33021;&#29366;&#24577;&#19979;&#25805;&#20316;&#32047;&#31215;&#30340;&#30693;&#35782;&#65292;&#24182;&#20381;&#36182;&#36866;&#24403;&#30340;&#24847;&#24895;&#36827;&#34892;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.04600</link><description>&lt;p&gt;
&#27169;&#22411;&#30340;&#27169;&#22411;--&#31532;&#19968;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Model of models -- Part 1. (arXiv:2308.04600v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25104;&#29087;&#26234;&#33021;&#29366;&#24577;&#19979;&#25805;&#20316;&#32047;&#31215;&#30340;&#30693;&#35782;&#65292;&#24182;&#20381;&#36182;&#36866;&#24403;&#30340;&#24847;&#24895;&#36827;&#34892;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#27169;&#22411;&#65292;&#20316;&#20026;AGI&#20195;&#29702;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#35813;&#27169;&#22411;&#26159;&#22312;&#20854;&#25104;&#29087;&#30340;&#26234;&#33021;&#29366;&#24577;&#19979;&#24341;&#20837;&#30340;&#65292;&#26159;DENN&#21644;&#29305;&#21035;&#26159;AKREM&#20043;&#21069;&#27169;&#22411;&#30340;&#24310;&#20280;&#65292;&#21253;&#25324;&#25805;&#20316;&#27169;&#22411;&#65288;&#26694;&#26550;/&#31867;&#21035;&#65289;&#21644;&#24847;&#24895;&#12290;&#35813;&#27169;&#22411;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#35748;&#30693;&#26159;&#22312;&#36866;&#24403;&#30340;&#24847;&#24895;&#24341;&#23548;&#19979;&#23545;&#32047;&#31215;&#30340;&#30693;&#35782;&#36827;&#34892;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#25104;&#29087;&#30340;&#26234;&#33021;&#29366;&#24577;&#20043;&#21069;&#30340;&#28436;&#21270;&#38454;&#27573;&#65292;&#34892;&#20026;&#65288;&#30693;&#35782;&#30340;&#19968;&#37096;&#20998;&#65289;&#26159;&#23398;&#20064;&#19982;&#24847;&#24895;&#23545;&#40784;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#27599;&#20010;&#24050;&#30693;&#26234;&#33021;&#26041;&#38754;&#30340;&#20108;&#20803;&#24615;&#21407;&#21017;&#65292;&#20363;&#22914;&#19978;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#27867;&#21270;&#19982;&#29305;&#21270;&#65292;&#31561;&#31561;&#12290;&#27492;&#22806;&#65292;&#20513;&#23548;&#19968;&#31181;&#25972;&#20307;&#30340;AGI&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#35748;&#30693;&#25110;&#25928;&#29575;&#38382;&#39064;&#65292;&#20197;&#21487;&#37325;&#29992;&#24615;&#21644;&#31616;&#27905;&#24615;&#30340;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23454;&#29616;&#30693;&#35782;&#21644;&#35268;&#21017;&#20043;&#38388;&#30340;&#21452;&#37325;&#21472;&#21152;&#26041;&#24335;&#26469;&#25551;&#36848;&#36798;&#21040;&#36825;&#31181;&#25104;&#29087;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new cognitive model, acting as the main component of an AGI agent. The model is introduced in its mature intelligence state, and as an extension of previous models, DENN, and especially AKREM, by including operational models (frames/classes) and will. This model's core assumption is that cognition is about operating on accumulated knowledge, with the guidance of an appropriate will. Also, we assume that the actions, part of knowledge, are learning to be aligned with will, during the evolution phase that precedes the mature intelligence state. In addition, this model is mainly based on the duality principle in every known intelligent aspect, such as exhibiting both top-down and bottom-up model learning, generalization verse specialization, and more. Furthermore, a holistic approach is advocated for AGI designing, and cognition under constraints or efficiency is proposed, in the form of reusability and simplicity. Finally, reaching this mature state is described via
&lt;/p&gt;</description></item><item><title>Shepherd&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#35780;&#35770;&#21644;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#21644;&#20462;&#22797;&#19981;&#21516;&#30340;&#38169;&#35823;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;Shepherd&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.04592</link><description>&lt;p&gt;
"Shepherd: &#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35780;&#35770;&#32773;"
&lt;/p&gt;
&lt;p&gt;
Shepherd: A Critic for Language Model Generation. (arXiv:2308.04592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04592
&lt;/p&gt;
&lt;p&gt;
Shepherd&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#35780;&#35770;&#21644;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#21644;&#20462;&#22797;&#19981;&#21516;&#30340;&#38169;&#35823;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;Shepherd&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25216;&#26415;&#24320;&#22987;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#20248;&#21270;&#20854;&#36755;&#20986;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Shepherd&#65292;&#19968;&#31181;&#29305;&#23450;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35780;&#35770;&#22238;&#22797;&#24182;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#65292;&#36229;&#36234;&#20102;&#26410;&#35843;&#25972;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#35782;&#21035;&#19981;&#21516;&#30340;&#38169;&#35823;&#24182;&#25552;&#20379;&#24314;&#35758;&#26469;&#20462;&#22797;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#31038;&#21306;&#21453;&#39304;&#21644;&#20154;&#24037;&#27880;&#37322;&#20013;&#31574;&#21010;&#25972;&#29702;&#32780;&#25104;&#12290;&#23613;&#31649;Shepherd&#35268;&#27169;&#36739;&#23567;&#65288;7B&#20010;&#21442;&#25968;&#65289;&#65292;&#20294;&#20854;&#35780;&#35770;&#35201;&#20040;&#19982;ChatGPT&#31561;&#24050;&#24314;&#31435;&#30340;&#27169;&#22411;&#31561;&#25928;&#65292;&#35201;&#20040;&#26356;&#20248;&#12290;&#36890;&#36807;&#20351;&#29992;GPT-4&#36827;&#34892;&#35780;&#20272;&#65292;Shepherd&#30456;&#23545;&#20110;&#31454;&#20105;&#23545;&#25163;&#24179;&#22343;&#20855;&#26377;53-87%&#30340;&#32988;&#29575;&#12290;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;Shepherd&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#19988;&#24179;&#22343;&#19982;ChatGPT&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models improve, there is increasing interest in techniques that leverage these models' capabilities to refine their own outputs. In this work, we introduce Shepherd, a language model specifically tuned to critique responses and suggest refinements, extending beyond the capabilities of an untuned model to identify diverse errors and provide suggestions to remedy them. At the core of our approach is a high quality feedback dataset, which we curate from community feedback and human annotations. Even though Shepherd is small (7B parameters), its critiques are either equivalent or preferred to those from established models including ChatGPT. Using GPT-4 for evaluation, Shepherd reaches an average win-rate of 53-87% compared to competitive alternatives. In human evaluation, Shepherd strictly outperforms other models and on average closely ties with ChatGPT.
&lt;/p&gt;</description></item><item><title>Temporal DINO&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#35270;&#39057;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#21644;&#19968;&#20010;&#25945;&#24072;&#27169;&#22411;&#65292;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#36807;&#21435;&#24103;&#26469;&#23398;&#20064;&#26410;&#26469;&#24103;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#22686;&#24378;&#21160;&#20316;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#34892;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#22312;ROAD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.04589</link><description>&lt;p&gt;
Temporal DINO:&#19968;&#31181;&#22686;&#24378;&#21160;&#20316;&#39044;&#27979;&#30340;&#33258;&#30417;&#30563;&#35270;&#39057;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction. (arXiv:2308.04589v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04589
&lt;/p&gt;
&lt;p&gt;
Temporal DINO&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#35270;&#39057;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#21644;&#19968;&#20010;&#25945;&#24072;&#27169;&#22411;&#65292;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#36807;&#21435;&#24103;&#26469;&#23398;&#20064;&#26410;&#26469;&#24103;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#22686;&#24378;&#21160;&#20316;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#34892;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#22312;ROAD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#39044;&#27979;&#30340;&#26032;&#20852;&#39046;&#22495;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#27963;&#21160;&#20998;&#26512;&#21644;&#20154;&#26426;&#20132;&#20114;&#12290;&#23613;&#31649;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35270;&#39057;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39640;&#32500;&#24230;&#12289;&#22797;&#26434;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#21160;&#20316;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#30417;&#30563;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;DINO&#65288;&#26080;&#26631;&#31614;&#30340;&#33258;&#33976;&#39311;&#65289;&#21551;&#21457;&#30340;&#22686;&#24378;&#21160;&#20316;&#39044;&#27979;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#35270;&#39057;&#31574;&#30053;&#65292;&#31216;&#20026;Temporal-DINO&#12290;&#35813;&#31574;&#30053;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#23398;&#29983;&#8221;&#22788;&#29702;&#36807;&#21435;&#30340;&#24103;&#65292;&#19968;&#20010;&#8220;&#25945;&#24072;&#8221;&#21516;&#26102;&#22788;&#29702;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#24103;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25945;&#24072;&#36890;&#36807;&#20165;&#35266;&#23519;&#36807;&#21435;&#30340;&#24103;&#26469;&#25351;&#23548;&#23398;&#29983;&#23398;&#20064;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#12290;&#35813;&#31574;&#30053;&#22312;ROAD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#34892;&#21160;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emerging field of action prediction plays a vital role in various computer vision applications such as autonomous driving, activity analysis and human-computer interaction. Despite significant advancements, accurately predicting future actions remains a challenging problem due to high dimensionality, complex dynamics and uncertainties inherent in video data. Traditional supervised approaches require large amounts of labelled data, which is expensive and time-consuming to obtain. This paper introduces a novel self-supervised video strategy for enhancing action prediction inspired by DINO (self-distillation with no labels). The Temporal-DINO approach employs two models; a 'student' processing past frames; and a 'teacher' processing both past and future frames, enabling a broader temporal context. During training, the teacher guides the student to learn future context by only observing past frames. The strategy is evaluated on ROAD dataset for the action prediction downstream task usi
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#26412;&#22320;&#35823;&#24046;&#20449;&#21495;&#23454;&#29616;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.04539</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#29289;&#21551;&#21457;&#30340;&#26550;&#26500;&#25552;&#39640;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures. (arXiv:2308.04539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#26412;&#22320;&#35823;&#24046;&#20449;&#21495;&#23454;&#29616;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#20174;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#20013;&#26080;&#38388;&#26029;&#22320;&#23398;&#20064;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21450;&#20854;&#21464;&#20307;&#65292;&#36825;&#20123;&#26041;&#27861;&#37319;&#29992;&#20840;&#23616;&#35823;&#24046;&#26356;&#26032;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#31574;&#30053;&#65292;&#22914;&#20869;&#23384;&#32531;&#20914;&#21306;&#25110;&#22238;&#25918;&#65292;&#20197;&#35268;&#36991;&#20854;&#31283;&#23450;&#24615;&#12289;&#36138;&#23146;&#21644;&#30701;&#26399;&#35760;&#24518;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21253;&#25324;&#31361;&#35302;&#21487;&#22609;&#24615;&#26426;&#21046;&#21644;&#31070;&#32463;&#35843;&#33410;&#65292;&#24182;&#36890;&#36807;&#26412;&#22320;&#35823;&#24046;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#32780;&#26080;&#38656;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Split-MNIST&#12289;Split-CIFAR-10&#21644;Split-CIFAR-100&#25968;&#25454;&#38598;&#19978;&#27604;&#20854;&#20182;&#20869;&#23384;&#21463;&#38480;&#30340;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#20869;&#23384;&#23494;&#38598;&#22411;&#22238;&#25918;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn continuously from an incoming data stream without catastrophic forgetting is critical to designing intelligent systems. Many approaches to continual learning rely on stochastic gradient descent and its variants that employ global error updates, and hence need to adopt strategies such as memory buffers or replay to circumvent its stability, greed, and short-term memory limitations. To address this limitation, we have developed a biologically inspired lightweight neural network architecture that incorporates synaptic plasticity mechanisms and neuromodulation and hence learns through local error signals to enable online continual learning without stochastic gradient descent.  Our approach leads to superior online continual learning performance on Split-MNIST, Split-CIFAR-10, and Split-CIFAR-100 datasets compared to other memory-constrained learning approaches and matches that of the state-of-the-art memory-intensive replay-based approaches. We further demonstrate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#29983;&#25104;&#29616;&#20195;&#27874;&#26031;&#22320;&#27631;&#22320;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;DNN&#39118;&#26684;&#36801;&#31227;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;Style-Swap&#26041;&#27861;&#21019;&#24314;&#21021;&#22987;&#22320;&#22270;&#65292;&#24182;&#20351;&#29992;Clip-Styler&#12289;Gatys&#21644;Style-Swap&#26041;&#27861;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;&#23545;&#22320;&#22270;&#36827;&#34892;&#30528;&#33394;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04529</link><description>&lt;p&gt;
&#36890;&#36807;&#39118;&#26684;&#36801;&#31227;&#29983;&#25104;&#29616;&#20195;&#27874;&#26031;&#22320;&#27631;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Generating Modern Persian Carpet Map by Style-transfer. (arXiv:2308.04529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#29983;&#25104;&#29616;&#20195;&#27874;&#26031;&#22320;&#27631;&#22320;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;DNN&#39118;&#26684;&#36801;&#31227;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;Style-Swap&#26041;&#27861;&#21019;&#24314;&#21021;&#22987;&#22320;&#22270;&#65292;&#24182;&#20351;&#29992;Clip-Styler&#12289;Gatys&#21644;Style-Swap&#26041;&#27861;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;&#23545;&#22320;&#22270;&#36827;&#34892;&#30528;&#33394;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#26368;&#26377;&#21560;&#24341;&#21147;&#30340;&#24212;&#29992;&#20043;&#19968;&#26159;&#29983;&#25104;&#33402;&#26415;&#35774;&#35745;&#12290;&#20316;&#20026;&#19968;&#31181;&#34987;&#35748;&#20026;&#26159;&#33402;&#26415;&#21697;&#30340;&#22320;&#27631;&#26159;&#25151;&#23376;&#20013;&#26368;&#37325;&#35201;&#30340;&#29289;&#21697;&#20043;&#19968;&#65292;&#25317;&#26377;&#20840;&#29699;&#21508;&#22320;&#30340;&#20247;&#22810;&#29233;&#22909;&#32773;&#12290;&#29983;&#25104;&#22320;&#27631;&#30340;&#31532;&#19968;&#27493;&#26159;&#20934;&#22791;&#23427;&#30340;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#39033;&#22256;&#38590;&#12289;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#20351;&#29992;DNN&#26469;&#29983;&#25104;&#29616;&#20195;&#27874;&#26031;&#22320;&#27631;&#22320;&#22270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;DNN&#39118;&#26684;&#36801;&#31227;&#26041;&#27861;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;Style-Swap&#26041;&#27861;&#21019;&#24314;&#21021;&#22987;&#22320;&#27631;&#22320;&#22270;&#65292;&#24182;&#22312;&#25509;&#19979;&#26469;&#20351;&#29992;Clip-Styler&#12289;Gatys&#21644;Style-Swap&#26041;&#27861;&#20998;&#21035;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#19968;&#20123;&#30528;&#33394;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20171;&#32461;&#65292;&#20197;&#32473;&#29983;&#25104;&#30340;&#22320;&#27631;&#22320;&#22270;&#19978;&#33394;&#12290;&#36890;&#36807;&#22635;&#20889;&#38382;&#21367;&#30340;&#32467;&#26524;&#26469;&#35780;&#20272;&#35774;&#35745;&#30340;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, the great performance of Deep Neural Networks(DNN) has been proven in various fields. One of its most attractive applications is to produce artistic designs. A carpet that is known as a piece of art is one of the most important items in a house, which has many enthusiasts all over the world. The first stage of producing a carpet is to prepare its map, which is a difficult, time-consuming, and expensive task. In this research work, our purpose is to use DNN for generating a Modern Persian Carpet Map. To reach this aim, three different DNN style transfer methods are proposed and compared against each other. In the proposed methods, the Style-Swap method is utilized to create the initial carpet map, and in the following, to generate more diverse designs, methods Clip-Styler, Gatys, and Style-Swap are used separately. In addition, some methods are examined and introduced for coloring the produced carpet maps. The designed maps are evaluated via the results of filled questionnaires w
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.04522</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#38544;&#20889;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#26159;&#20449;&#24687;&#23433;&#20840;&#39046;&#22495;&#30340;&#20004;&#20010;&#30456;&#20851;&#26041;&#38754;&#12290;&#38544;&#20889;&#26415;&#26088;&#22312;&#38544;&#34255;&#36890;&#20449;&#65292;&#32780;&#38544;&#20889;&#20998;&#26512;&#21017;&#26088;&#22312;&#25214;&#21040;&#36825;&#20123;&#38544;&#34255;&#20449;&#24687;&#65292;&#29978;&#33267;&#23581;&#35797;&#24674;&#22797;&#20854;&#25152;&#21253;&#21547;&#30340;&#25968;&#25454;&#12290;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#21463;&#21040;&#25191;&#27861;&#37096;&#38376;&#30340;&#20851;&#27880;&#12290;&#38544;&#20889;&#26415;&#24120;&#34987;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#29978;&#33267;&#24656;&#24598;&#20998;&#23376;&#29992;&#26469;&#36991;&#20813;&#22312;&#25317;&#26377;&#35777;&#25454;&#26102;&#34987;&#25429;&#65292;&#21363;&#20351;&#21152;&#23494;&#20063;&#19968;&#26679;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#22269;&#23478;&#31105;&#27490;&#25110;&#38480;&#21046;&#20351;&#29992;&#23494;&#30721;&#23398;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#25581;&#31034;&#38544;&#34255;&#20449;&#24687;&#30340;&#23574;&#31471;&#25216;&#26415;&#23545;&#25581;&#38706;&#38750;&#27861;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#35768;&#22810;&#24378;&#22823;&#21487;&#38752;&#30340;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper cove
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#32452;&#21512;&#20998;&#24067;&#24335;&#24847;&#20041;&#27169;&#22411;&#20013;&#35299;&#26512;Donkey&#21477;&#23376;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#22411;&#36923;&#36753;&#35821;&#27861;&#21644;&#20851;&#31995;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.04519</link><description>&lt;p&gt;
DisCoCat&#29992;&#20110;Donkey&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
DisCoCat for Donkey Sentences. (arXiv:2308.04519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04519
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#32452;&#21512;&#20998;&#24067;&#24335;&#24847;&#20041;&#27169;&#22411;&#20013;&#35299;&#26512;Donkey&#21477;&#23376;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#22411;&#36923;&#36753;&#35821;&#27861;&#21644;&#20851;&#31995;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#32452;&#21512;&#20998;&#24067;&#24335;&#24847;&#20041;&#27169;&#22411;&#20013;&#35299;&#26512;Geach&#30340;Donkey&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;&#20043;&#21069;&#20851;&#20110;DisCoCat&#65288;&#20998;&#24067;&#24335;&#32452;&#21512;&#33539;&#30068;&#65289;&#26694;&#26550;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#36827;&#34892;&#25193;&#23637;&#65292;&#21253;&#25324;&#23545;&#35805;&#35821;&#12289;&#38480;&#23450;&#35789;&#21644;&#20851;&#31995;&#20195;&#35789;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#22411;&#36923;&#36753;&#35821;&#27861;&#26469;&#35299;&#26512;donkey&#21477;&#23376;&#65292;&#21516;&#26102;&#23450;&#20041;&#20102;&#20851;&#31995;&#21644;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how to parse Geach's Donkey sentences in a compositional distributional model of meaning. We build on previous work on the DisCoCat (Distributional Compositional Categorical) framework, including extensions that model discourse, determiners, and relative pronouns. We present a type-logical syntax for parsing donkey sentences, for which we define both relational and vector space semantics.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT-IceNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21271;&#26497;&#28023;&#20912;&#27987;&#24230;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31354;&#38388;&#21644;&#22810;&#26102;&#24207;&#32467;&#26500;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#36339;&#36291;&#36830;&#25509;&#22788;&#29702;&#22810;&#26102;&#24207;&#36755;&#20837;&#27969;&#65292;&#24182;&#21487;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#27493;&#39588;&#30340;&#31354;&#38388;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2308.04511</link><description>&lt;p&gt;
MT-IceNet - &#19968;&#31181;&#29992;&#20110;&#21271;&#26497;&#28023;&#20912;&#39044;&#27979;&#30340;&#31354;&#38388;&#21644;&#22810;&#26102;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MT-IceNet -- A Spatial and Multi-Temporal Deep Learning Model for Arctic Sea Ice Forecasting. (arXiv:2308.04511v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04511
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT-IceNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21271;&#26497;&#28023;&#20912;&#27987;&#24230;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31354;&#38388;&#21644;&#22810;&#26102;&#24207;&#32467;&#26500;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#36339;&#36291;&#36830;&#25509;&#22788;&#29702;&#22810;&#26102;&#24207;&#36755;&#20837;&#27969;&#65292;&#24182;&#21487;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#27493;&#39588;&#30340;&#31354;&#38388;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#26497;&#25918;&#22823;&#24050;&#32463;&#25913;&#21464;&#20102;&#21306;&#22495;&#21644;&#20840;&#29699;&#30340;&#27668;&#20505;&#27169;&#24335;&#65292;&#23548;&#33268;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#12290;&#21271;&#26497;&#25918;&#22823;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#21355;&#26143;&#35266;&#27979;&#25152;&#35777;&#23454;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#28023;&#20912;&#28040;&#22833;&#12290;&#20934;&#30830;&#22320;&#20174;&#27425;&#23395;&#33410;&#21040;&#23395;&#33410;&#24615;&#23610;&#24230;&#39044;&#27979;&#21271;&#26497;&#28023;&#20912;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#23384;&#22312;&#26681;&#26412;&#24615;&#25361;&#25112;&#12290;&#38500;&#20102;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65292;&#30740;&#31350;&#32773;&#19968;&#30452;&#24212;&#29992;&#22810;&#31181;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#28023;&#20912;&#39044;&#27979;&#12290;&#22238;&#39038;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30740;&#31350;&#28023;&#20912;&#21464;&#21270;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MT-IceNet - &#19968;&#31181;&#22522;&#20110;UNet&#30340;&#31354;&#38388;&#21644;&#22810;&#26102;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21271;&#26497;&#28023;&#20912;&#27987;&#24230;&#65288;SIC&#65289;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#22788;&#29702;&#22810;&#26102;&#24207;&#36755;&#20837;&#27969;&#20197;&#22312;&#26410;&#26469;&#30340;&#26102;&#38388;&#27493;&#39588;&#20013;&#37325;&#26032;&#29983;&#25104;&#31354;&#38388;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arctic amplification has altered the climate patterns both regionally and globally, resulting in more frequent and more intense extreme weather events in the past few decades. The essential part of Arctic amplification is the unprecedented sea ice loss as demonstrated by satellite observations. Accurately forecasting Arctic sea ice from sub-seasonal to seasonal scales has been a major research question with fundamental challenges at play. In addition to physics-based Earth system models, researchers have been applying multiple statistical and machine learning models for sea ice forecasting. Looking at the potential of data-driven approaches to study sea ice variations, we propose MT-IceNet - a UNet based spatial and multi-temporal (MT) deep learning model for forecasting Arctic sea ice concentration (SIC). The model uses an encoder-decoder architecture with skip connections and processes multi-temporal input streams to regenerate spatial maps at future timesteps. Using bi-monthly and m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#65292;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#26657;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#25552;&#31034;&#26041;&#27861;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20013;&#65292;GPT-4&#33021;&#22815;&#36798;&#21040;65.49&#30340;F1&#24471;&#20998;&#65292;&#27604;&#25105;&#20204;&#24050;&#26377;&#30340;&#22522;&#20934;&#25552;&#39640;&#20102;&#22823;&#32422;5&#20010;&#28857;&#12290;&#36825;&#34920;&#26126;LLMs&#22312;&#36164;&#28304;&#21294;&#20047;&#29615;&#22659;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04492</link><description>&lt;p&gt;
&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;ChatGPT
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Arabic Grammatical Error Correction. (arXiv:2308.04492v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#65292;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#26657;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#25552;&#31034;&#26041;&#27861;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20013;&#65292;GPT-4&#33021;&#22815;&#36798;&#21040;65.49&#30340;F1&#24471;&#20998;&#65292;&#27604;&#25105;&#20204;&#24050;&#26377;&#30340;&#22522;&#20934;&#25552;&#39640;&#20102;&#22823;&#32422;5&#20010;&#28857;&#12290;&#36825;&#34920;&#26126;LLMs&#22312;&#36164;&#28304;&#21294;&#20047;&#29615;&#22659;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#34987;&#35843;&#25972;&#20197;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33521;&#25991;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#20219;&#21153;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#33521;&#25991;&#35821;&#35328;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#28982;&#30456;&#24403;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#20196;&#35843;&#26657;&#30340;LLM&#22312;&#38463;&#25289;&#20271;&#35821;GEC&#20013;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#30001;&#20110;&#38463;&#25289;&#20271;&#35821;&#30340;&#20016;&#23500;&#24418;&#24577;&#32780;&#21464;&#24471;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#65288;&#19978;&#19979;&#25991;&#65289;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#23637;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#25928;&#26524;&#65292;GPT-4&#22312;&#19987;&#23478;&#25552;&#31034;&#19979;&#36798;&#21040;&#20102;65.49&#30340;F1&#24471;&#20998;&#65288;&#27604;&#25105;&#20204;&#24050;&#26377;&#30340;&#22522;&#20934;&#25552;&#39640;&#20102;&#22823;&#32422;5&#20010;&#28857;&#65289;&#12290;&#36825;&#31361;&#26174;&#20102;LLM&#22312;&#36164;&#28304;&#21294;&#20047;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#65292;&#20026;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#26377;&#29992;&#30340;&#21512;&#25104;&#25968;&#25454;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#35770;&#35268;&#27169;&#22914;&#20309;&#65292;&#25351;&#20196;&#35843;&#26657;&#27169;&#22411;&#30340;&#24615;&#33021;&#26174;&#33879;&#20302;&#20110;&#27604;&#36739;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) fine-tuned to follow human instruction have exhibited significant capabilities in various English NLP tasks. However, their performance in grammatical error correction (GEC) tasks, particularly in non-English languages, remains significantly unexplored. In this paper, we delve into abilities of instruction fine-tuned LLMs in Arabic GEC, a task made complex due to Arabic's rich morphology. Our findings suggest that various prompting methods, coupled with (in-context) few-shot learning, demonstrate considerable effectiveness, with GPT-4 achieving up to $65.49$ F\textsubscript{1} score under expert prompting (approximately $5$ points higher than our established baseline). This highlights the potential of LLMs in low-resource settings, offering a viable approach for generating useful synthetic data for model training. Despite these positive results, we find that instruction fine-tuned models, regardless of their size, significantly underperform compar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;ChatGPT 3.5&#22312;10&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#29983;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#30340;&#24847;&#22806;&#34892;&#20026;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.04477</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT 3.5&#29983;&#25104;&#20195;&#30721;&#22312;10&#31181;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Code Generation using ChatGPT 3.5 across 10 Programming Languages. (arXiv:2308.04477v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;ChatGPT 3.5&#22312;10&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#29983;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#30340;&#24847;&#22806;&#34892;&#20026;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#32463;&#36807;&#22823;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#35757;&#32451;&#65292;&#20197;&#29702;&#35299;&#21644;&#20135;&#29983;&#19982;&#20154;&#31867;&#35821;&#35328;&#30456;&#20284;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#20855;&#22791;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#22810;&#20010;&#23398;&#31185;&#30340;&#22823;&#23398;&#32771;&#35797;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#22788;&#29702;&#26032;&#38382;&#39064;&#30340;&#21151;&#33021;&#20195;&#30721;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#30001;OpenAI&#20110;2022&#24180;11&#26376;&#21457;&#24067;&#30340;ChatGPT 3.5&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#35813;&#27169;&#22411;&#22240;&#20854;&#20986;&#33394;&#30340;&#25991;&#26412;&#29983;&#25104;&#21644;&#20195;&#30721;&#21019;&#24314;&#33021;&#21147;&#32780;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#12290;&#35813;&#27169;&#22411;&#22312;10&#31181;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#21644;4&#20010;&#19981;&#21516;&#30340;&#36719;&#20214;&#39046;&#22495;&#20013;&#30340;&#20195;&#30721;&#29255;&#27573;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#27169;&#22411;&#30340;&#37325;&#35201;&#24847;&#22806;&#34892;&#20026;&#21644;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#21457;&#23637;&#30340;&#28508;&#22312;&#39046;&#22495;&#65292;&#24182;&#30740;&#31350;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are advanced Artificial Intelligence (AI) systems that have undergone extensive training using large datasets in order to understand and produce language that closely resembles that of humans. These models have reached a level of proficiency where they are capable of successfully completing university exams across several disciplines and generating functional code to handle novel problems. This research investigates the coding proficiency of ChatGPT 3.5, a LLM released by OpenAI in November 2022, which has gained significant recognition for its impressive text generating and code creation capabilities. The skill of the model in creating code snippets is evaluated across 10 various programming languages and 4 different software domains. Based on the findings derived from this research, major unexpected behaviors and limitations of the model have been identified. This study aims to identify potential areas for development and examine the ramifications of auto
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#22238;&#24402;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#26816;&#27979;&#21307;&#30103;&#20445;&#38505;&#32034;&#36180;&#20013;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#21306;&#20998;&#27450;&#35784;&#21644;&#38750;&#27450;&#35784;&#32034;&#36180;&#12290;</title><link>http://arxiv.org/abs/2308.04469</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#21307;&#30103;&#20445;&#38505;&#32034;&#36180;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Correlating Medi- Claim Service by Deep Learning Neural Networks. (arXiv:2308.04469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#22238;&#24402;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#26816;&#27979;&#21307;&#30103;&#20445;&#38505;&#32034;&#36180;&#20013;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#21306;&#20998;&#27450;&#35784;&#21644;&#38750;&#27450;&#35784;&#32034;&#36180;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20445;&#38505;&#32034;&#36180;&#20013;&#23384;&#22312;&#19982;&#24739;&#32773;&#12289;&#21307;&#29983;&#12289;&#35786;&#26029;&#20013;&#24515;&#21644;&#20445;&#38505;&#25552;&#20379;&#21830;&#30456;&#20851;&#30340;&#26377;&#32452;&#32455;&#29359;&#32618;&#65292;&#24418;&#25104;&#20102;&#24517;&#39035;&#19981;&#26029;&#30417;&#27979;&#30340;&#36830;&#38145;&#21453;&#24212;&#12290;&#36825;&#20123;&#27450;&#35784;&#34892;&#20026;&#24433;&#21709;&#34987;&#20445;&#38505;&#20154;&#21644;&#20581;&#24247;&#20445;&#38505;&#20844;&#21496;&#30340;&#36130;&#21153;&#22686;&#38271;&#12290;&#36890;&#36807;&#23545;&#22238;&#24402;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#26816;&#27979;&#27450;&#35784;&#32034;&#36180;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26816;&#27979;&#26469;&#33258;&#19981;&#21516;&#25552;&#20379;&#21830;&#30340;&#19981;&#21516;&#32034;&#36180;&#30340;&#27927;&#38065;&#34892;&#20026;&#12290;&#20351;&#29992;&#20102;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#27450;&#35784;&#21644;&#38750;&#27450;&#35784;&#32034;&#36180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical insurance claims are of organized crimes related to patients, physicians, diagnostic centers, and insurance providers, forming a chain reaction that must be monitored constantly. These kinds of frauds affect the financial growth of both insured people and health insurance companies. The Convolution Neural Network architecture is used to detect fraudulent claims through a correlation study of regression models, which helps to detect money laundering on different claims given by different providers. Supervised and unsupervised classifiers are used to detect fraud and non-fraud claims.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#65292;&#20197;&#24212;&#23545;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2308.04455</link><description>&lt;p&gt;
&#21311;&#21517;&#21270;&#35821;&#38899;&#65306;&#35780;&#20272;&#21644;&#35774;&#35745;&#35828;&#35805;&#20154;&#21311;&#21517;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques. (arXiv:2308.04455v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#65292;&#20197;&#24212;&#23545;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#29992;&#25143;&#30028;&#38754;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#35821;&#38899;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#23384;&#20648;&#20063;&#22823;&#22823;&#22686;&#21152;&#12290;&#34429;&#28982;&#25968;&#25454;&#25910;&#38598;&#21487;&#20197;&#20026;&#22823;&#22810;&#25968;&#35821;&#38899;&#26381;&#21153;&#25552;&#20379;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20063;&#32473;&#29992;&#25143;&#30340;&#38544;&#31169;&#36896;&#25104;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38598;&#20013;&#23384;&#20648;&#20351;&#20010;&#20154;&#30340;&#35821;&#38899;&#25968;&#25454;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#23041;&#32961;&#30340;&#20405;&#23475;&#12290;&#38543;&#30528;&#20122;&#39532;&#36874;&#30340;Alexa&#65292;&#35895;&#27468;&#30340;Home&#21644;&#33529;&#26524;&#30340;Siri&#31561;&#22522;&#20110;&#35821;&#38899;&#30340;&#25968;&#23383;&#21161;&#25163;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#20197;&#21450;&#20010;&#20154;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#65292;&#22768;&#38899;&#20811;&#38534;&#21644;&#35828;&#35805;&#20154;/&#24615;&#21035;/&#30149;&#29702;&#31561;&#35782;&#21035;&#30340;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#20063;&#22686;&#21152;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21311;&#21517;&#21270;&#26159;&#25351;&#20351;&#20010;&#20154;&#35821;&#38899;&#25968;&#25454;&#19982;&#36523;&#20221;&#26080;&#27861;&#20851;&#32852;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#38899;&#20449;&#21495;&#30340;&#23454;&#29992;&#24615;&#65288;&#20363;&#22914;&#65292;&#35775;&#38382;&#35821;&#35328;&#20869;&#23481;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#20960;&#20010;&#35780;&#20272;&#21327;&#35758;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing use of voice user interfaces has led to a surge in the collection and storage of speech data. While data collection allows for the development of efficient tools powering most speech services, it also poses serious privacy issues for users as centralized storage makes private personal speech data vulnerable to cyber threats. With the increasing use of voice-based digital assistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the increasing ease with which personal speech data can be collected, the risk of malicious use of voice-cloning and speaker/gender/pathological/etc. recognition has increased.  This thesis proposes solutions for anonymizing speech and evaluating the degree of the anonymization. In this work, anonymization refers to making personal speech data unlinkable to an identity while maintaining the usefulness (utility) of the speech signal (e.g., access to linguistic content). We start by identifying several challenges that evaluation protoco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#21363;&#27880;&#20837;&#24694;&#24847;&#26679;&#26412;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#25968;&#25454;&#34987;&#27602;&#21270;&#65292;&#32780;&#19988;&#19981;&#24433;&#21709;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#20351;&#20854;&#38590;&#20197;&#34987;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.04451</link><description>&lt;p&gt;
AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#28431;&#27934;&#65306;&#25506;&#32034;&#38024;&#23545;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks. (arXiv:2308.04451v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#21363;&#27880;&#20837;&#24694;&#24847;&#26679;&#26412;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#25968;&#25454;&#34987;&#27602;&#21270;&#65292;&#32780;&#19988;&#19981;&#24433;&#21709;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#20351;&#20854;&#38590;&#20197;&#34987;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#27602;&#21270;&#35780;&#20272;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#21363;&#36890;&#36807;&#23558;&#24694;&#24847;&#26679;&#26412;&#27880;&#20837;&#35757;&#32451;&#25968;&#25454;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#36890;&#36807;&#27880;&#20837;&#21253;&#21547;&#23433;&#20840;&#28431;&#27934;&#30340;&#20195;&#30721;&#26469;&#27602;&#21270;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#23545;&#25915;&#20987;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#25968;&#25454;&#27602;&#21270;&#65292;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#35813;&#25915;&#20987;&#19981;&#20250;&#24433;&#21709;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#20351;&#20854;&#38590;&#20197;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we assess the security of AI code generators via data poisoning, i.e., an attack that injects malicious samples into the training data to generate vulnerable code. We poison the training data by injecting increasing amounts of code containing security vulnerabilities and assess the attack's success on different state-of-the-art models for code generation. Our analysis shows that AI code generators are vulnerable to even a small amount of data poisoning. Moreover, the attack does not impact the correctness of code generated by pre-trained models, making it hard to detect.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;AI&#39046;&#22495;&#30340;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#38598;&#20013;&#30417;&#31649;&#19982;&#20247;&#21253;&#23433;&#20840;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#21452;&#37325;&#27835;&#29702;&#26041;&#24335;&#12290;&#38598;&#20013;&#30417;&#31649;&#23384;&#22312;&#32570;&#20047;&#26126;&#30830;&#24615;&#21644;&#32479;&#19968;&#24615;&#31561;&#38382;&#39064;&#65292;&#32780;&#20247;&#21253;&#23433;&#20840;&#26426;&#21046;&#21017;&#23384;&#22312;&#32570;&#20047;&#32479;&#19968;&#24615;&#21644;&#21512;&#35268;&#24615;&#30340;&#19981;&#36275;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20419;&#36827;&#29983;&#25104;AI&#39046;&#22495;&#30340;&#36127;&#36131;&#20219;&#21644;&#36947;&#24503;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.04448</link><description>&lt;p&gt;
&#21452;&#37325;&#27835;&#29702;&#65306;&#38598;&#20013;&#30417;&#31649;&#19982;&#20247;&#21253;&#23433;&#20840;&#26426;&#21046;&#22312;&#29983;&#25104;AI&#20013;&#30340;&#20132;&#38598;
&lt;/p&gt;
&lt;p&gt;
Dual Governance: The intersection of centralized regulation and crowdsourced safety mechanisms for Generative AI. (arXiv:2308.04448v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;AI&#39046;&#22495;&#30340;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#38598;&#20013;&#30417;&#31649;&#19982;&#20247;&#21253;&#23433;&#20840;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#21452;&#37325;&#27835;&#29702;&#26041;&#24335;&#12290;&#38598;&#20013;&#30417;&#31649;&#23384;&#22312;&#32570;&#20047;&#26126;&#30830;&#24615;&#21644;&#32479;&#19968;&#24615;&#31561;&#38382;&#39064;&#65292;&#32780;&#20247;&#21253;&#23433;&#20840;&#26426;&#21046;&#21017;&#23384;&#22312;&#32570;&#20047;&#32479;&#19968;&#24615;&#21644;&#21512;&#35268;&#24615;&#30340;&#19981;&#36275;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20419;&#36827;&#29983;&#25104;AI&#39046;&#22495;&#30340;&#36127;&#36131;&#20219;&#21644;&#36947;&#24503;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26368;&#36817;&#22312;&#28040;&#36153;&#32773;&#38754;&#21521;&#30340;&#12289;&#24320;&#25918;&#24335;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#26679;&#30340;&#31995;&#32479;&#24341;&#21457;&#20102;&#37325;&#35201;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#21253;&#25324;&#38544;&#31169;&#20405;&#29359;&#12289;&#34394;&#20551;&#20449;&#24687;&#21644;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#12290;&#29983;&#25104;AI&#21487;&#33021;&#21462;&#20195;&#20154;&#31867;&#30340;&#21019;&#36896;&#21147;&#21644;&#35851;&#29983;&#26041;&#24335;&#20063;&#21463;&#21040;&#20102;&#20005;&#26684;&#23457;&#26597;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#65292;&#22312;&#29983;&#25104;AI&#39046;&#22495;&#38656;&#35201;&#19968;&#31181;&#36127;&#36131;&#20219;&#21644;&#36947;&#24503;&#30340;&#21457;&#23637;&#25919;&#31574;&#21644;&#30417;&#31649;&#26426;&#21046;&#12290;&#29616;&#26377;&#21644;&#25552;&#35758;&#30340;&#25919;&#24220;&#38598;&#20013;&#30417;&#31649;AI&#30340;&#35268;&#23450;&#38754;&#20020;&#35832;&#22810;&#25209;&#35780;&#65292;&#20363;&#22914;&#32570;&#20047;&#36275;&#22815;&#30340;&#26126;&#30830;&#24615;&#21644;&#32479;&#19968;&#24615;&#65292;&#22312;&#19981;&#21516;&#21496;&#27861;&#36758;&#21306;&#20043;&#38388;&#32570;&#20047;&#20114;&#25805;&#20316;&#24615;&#65292;&#38480;&#21046;&#21019;&#26032;&#65292;&#38459;&#30861;&#33258;&#30001;&#24066;&#22330;&#31454;&#20105;&#12290;&#20998;&#25955;&#30340;&#20247;&#21253;&#23433;&#20840;&#24037;&#20855;&#21644;&#26426;&#21046;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#32570;&#20047;&#32479;&#19968;&#24615;&#12289;&#21512;&#35268;&#24615;&#21644;&#36879;&#26126;&#24615;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) has seen mainstream adoption lately, especially in the form of consumer-facing, open-ended, text and image generating models. However, the use of such systems raises significant ethical and safety concerns, including privacy violations, misinformation and intellectual property theft. The potential for generative AI to displace human creativity and livelihoods has also been under intense scrutiny. To mitigate these risks, there is an urgent need of policies and regulations responsible and ethical development in the field of generative AI. Existing and proposed centralized regulations by governments to rein in AI face criticisms such as not having sufficient clarity or uniformity, lack of interoperability across lines of jurisdictions, restricting innovation, and hindering free market competition. Decentralized protections via crowdsourced safety tools and mechanisms are a potential alternative. However, they have clear deficiencies in terms of lac
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#26410;&#26469;&#30340;AI&#65292;&#38656;&#35201;&#20174;&#29983;&#25104;&#24335;AI&#36716;&#21521;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#36890;&#36807;&#22521;&#20859;&#22522;&#20110;&#26126;&#30830;&#30693;&#35782;&#21644;&#32463;&#39564;&#35268;&#21017;&#30340;AI&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#30340;&#38480;&#21046;&#24182;&#23454;&#29616;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#36182;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04445</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#24335;AI&#36208;&#21521;&#21487;&#20449;&#36182;&#30340;AI&#65306;LLM&#21487;&#20197;&#20174;Cyc&#20013;&#23398;&#21040;&#20160;&#20040;
&lt;/p&gt;
&lt;p&gt;
Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc. (arXiv:2308.04445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04445
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26410;&#26469;&#30340;AI&#65292;&#38656;&#35201;&#20174;&#29983;&#25104;&#24335;AI&#36716;&#21521;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#36890;&#36807;&#22521;&#20859;&#22522;&#20110;&#26126;&#30830;&#30693;&#35782;&#21644;&#32463;&#39564;&#35268;&#21017;&#30340;AI&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#30340;&#38480;&#21046;&#24182;&#23454;&#29616;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#36182;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#26159;&#30446;&#21069;&#26368;&#27969;&#34892;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#30001;&#35757;&#32451;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32452;&#25104;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#20449;&#65292;&#20294;&#19981;&#19968;&#23450;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#24120;&#24120;&#20196;&#20154;&#24778;&#21497;&#65292;&#20294;&#22312;&#25512;&#29702;&#26041;&#38754;&#23427;&#20204;&#23384;&#22312;&#32570;&#38519;&#65292;&#23548;&#33268;LLMs&#19981;&#23436;&#20840;&#21487;&#20449;&#36182;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#32467;&#26524;&#24448;&#24448;&#19981;&#21487;&#39044;&#27979;&#21644;&#19981;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#30340;16&#20010;&#26399;&#26395;&#65292;&#24182;&#35752;&#35770;&#20102;AI&#30340;&#21478;&#19968;&#31181;&#21487;&#33021;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#35768;&#22810;&#38480;&#21046;&#30340;&#26041;&#27861;&#65306;&#22521;&#20859;&#22522;&#20110;&#26126;&#30830;&#30693;&#35782;&#21644;&#32463;&#39564;&#35268;&#21017;&#30340;AI&#65292;&#20351;&#25512;&#29702;&#24341;&#25806;&#33021;&#22815;&#33258;&#21160;&#25512;&#23548;&#20986;&#25152;&#26377;&#30693;&#35782;&#30340;&#36923;&#36753;&#34164;&#21547;&#12290;&#21363;&#20351;&#26159;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#20135;&#29983;&#30340;&#38271;&#35770;&#35777;&#20063;&#21487;&#20197;&#26159;&#21487;&#20449;&#19988;&#21487;&#35299;&#37322;&#30340;&#65292;&#22240;&#20026;&#23436;&#25972;&#30340;&#36880;&#27493;&#25512;&#29702;&#36807;&#31243;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#65292;&#24182;&#19988;&#21487;&#20197;&#35760;&#24405;&#21644;&#23457;&#35745;&#27599;&#20010;&#27493;&#39588;&#20351;&#29992;&#30340;&#30693;&#35782;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI, the most popular current approach to AI, consists of large language models (LLMs) that are trained to produce outputs that are plausible, but not necessarily correct. Although their abilities are often uncanny, they are lacking in aspects of reasoning, leading LLMs to be less than completely trustworthy. Furthermore, their results tend to be both unpredictable and uninterpretable.  We lay out 16 desiderata for future AI, and discuss an alternative approach to AI which could theoretically address many of the limitations associated with current approaches: AI educated with curated pieces of explicit knowledge and rules of thumb, enabling an inference engine to automatically deduce the logical entailments of all that knowledge. Even long arguments produced this way can be both trustworthy and interpretable, since the full step-by-step line of reasoning is always available, and for each step the provenance of the knowledge used can be documented and audited. There is however
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20248;&#21270;&#23458;&#25143;&#36873;&#25321;&#21644;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21333;&#28857;&#25925;&#38556;&#25915;&#20987;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2308.04442</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20248;&#21270;&#23458;&#25143;&#36873;&#25321;&#21644;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Blockchain-based Optimized Client Selection and Privacy Preserved Framework for Federated Learning. (arXiv:2308.04442v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20248;&#21270;&#23458;&#25143;&#36873;&#25321;&#21644;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21333;&#28857;&#25925;&#38556;&#25915;&#20987;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#21046;&#65292;&#36890;&#36807;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#21442;&#19982;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25968;&#25454;&#20173;&#28982;&#20445;&#30041;&#22312;&#20854;&#35774;&#22791;&#19978;&#65292;&#21482;&#20998;&#20139;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#12290;&#36825;&#20010;&#29305;&#28857;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#32467;&#26500;&#20381;&#36182;&#20110;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#65292;&#36825;&#23548;&#33268;&#20102;&#21333;&#28857;&#25925;&#38556;&#25915;&#20987;&#65292;&#38543;&#26426;&#36873;&#25321;&#23458;&#25143;&#31471;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#20063;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#25163;&#36824;&#35797;&#22270;&#36827;&#34892;&#25512;&#29702;&#25915;&#20987;&#65292;&#21363;&#23545;&#38544;&#31169;&#30340;&#25915;&#20987;&#20250;&#23548;&#33268;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20248;&#21270;&#23458;&#25143;&#36873;&#25321;&#21644;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#26234;&#33021;&#21512;&#32422;&#65292;&#21253;&#25324;1) &#23458;&#25143;&#31471;&#27880;&#20876;&#65292;2) &#27491;&#21521;&#31454;&#20215;&#36873;&#25321;&#20248;&#21270;&#23458;&#25143;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65292;3) &#20184;&#27454;&#32467;&#31639;&#21644;&#22870;&#21169;&#26234;&#33021;&#21512;&#32422;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;Cheon, Kim, Kim&#21644;Son&#30340;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed mechanism that trained large-scale neural network models with the participation of multiple clients and data remains on their devices, only sharing the local model updates. With this feature, federated learning is considered a secure solution for data privacy issues. However, the typical FL structure relies on the client-server, which leads to the single-point-of-failure (SPoF) attack, and the random selection of clients for model training compromised the model accuracy. Furthermore, adversaries try for inference attacks i.e., attack on privacy leads to gradient leakage attacks. We proposed the blockchain-based optimized client selection and privacy-preserved framework in this context. We designed the three kinds of smart contracts such as 1) registration of clients 2) forward bidding to select optimized clients for FL model training 3) payment settlement and reward smart contracts. Moreover, fully homomorphic encryption with Cheon, Kim, Kim, and Son
&lt;/p&gt;</description></item><item><title>&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#22312;&#19968;&#31687;&#31038;&#35770;&#20013;&#21628;&#21505;&#25105;&#20204;&#8220;&#20572;&#27490;&#35848;&#35770;&#26126;&#22825;&#30340;AI&#26411;&#26085;&#65292;&#32780;AI&#20170;&#22825;&#23601;&#23384;&#22312;&#39118;&#38505;&#12290;&#8221;&#36825;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#21028;&#26029;&#22833;&#35823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#21160;&#32773;&#26469;&#35828;&#65292;&#22240;&#20026;&#25105;&#20204;&#26399;&#26395;&#20182;&#20204;&#33021;&#22815;&#32771;&#34385;&#21040;&#38169;&#35823;&#30340;&#21518;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04440</link><description>&lt;p&gt;
&#33258;&#28982;&#21644;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nature and the Machines. (arXiv:2308.04440v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04440
&lt;/p&gt;
&lt;p&gt;
&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#22312;&#19968;&#31687;&#31038;&#35770;&#20013;&#21628;&#21505;&#25105;&#20204;&#8220;&#20572;&#27490;&#35848;&#35770;&#26126;&#22825;&#30340;AI&#26411;&#26085;&#65292;&#32780;AI&#20170;&#22825;&#23601;&#23384;&#22312;&#39118;&#38505;&#12290;&#8221;&#36825;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#21028;&#26029;&#22833;&#35823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#21160;&#32773;&#26469;&#35828;&#65292;&#22240;&#20026;&#25105;&#20204;&#26399;&#26395;&#20182;&#20204;&#33021;&#22815;&#32771;&#34385;&#21040;&#38169;&#35823;&#30340;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#26159;&#21542;&#23545;&#20154;&#31867;&#26500;&#25104;&#23384;&#22312;&#21361;&#38505;&#65311;&#19968;&#20123;&#25209;&#35780;&#23478;&#35748;&#20026;&#36825;&#20010;&#38382;&#39064;&#27491;&#22312;&#21463;&#21040;&#36807;&#22810;&#30340;&#20851;&#27880;&#65292;&#20182;&#20204;&#24076;&#26395;&#23558;&#20854;&#25512;&#21040;&#19968;&#36793;&#65292;&#36716;&#32780;&#35752;&#35770;AI&#30340;&#21363;&#26102;&#39118;&#38505;&#12290;&#36825;&#20123;&#25209;&#35780;&#23478;&#29616;&#22312;&#21253;&#25324;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#65292;&#22312;&#26368;&#36817;&#30340;&#19968;&#31687;&#31038;&#35770;&#20013;&#25958;&#20419;&#25105;&#20204;&#8220;&#20572;&#27490;&#35848;&#35770;&#26126;&#22825;&#30340;AI&#26411;&#26085;&#65292;&#32780;AI&#20170;&#22825;&#23601;&#23384;&#22312;&#39118;&#38505;&#12290;&#8221;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#21028;&#26029;&#22833;&#35823;&#65292;&#23545;&#20110;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#26469;&#35828;&#12290;&#22312;&#31185;&#23398;&#39046;&#22495;&#65292;&#23601;&#20687;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#19968;&#26679;&#65292;&#25105;&#20204;&#24076;&#26395;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#20026;&#32773;&#33021;&#22815;&#32771;&#34385;&#38169;&#35823;&#30340;&#21518;&#26524;&#12290;&#20316;&#20026;&#19990;&#30028;&#39046;&#20808;&#30340;&#31185;&#23398;&#26399;&#21002;&#65292;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#26080;&#30097;&#26159;&#19968;&#20010;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#20026;&#32773;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#20047;&#20581;&#20840;&#20840;&#29699;AI&#30417;&#31649;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#36825;&#20010;&#26696;&#20363;&#20013;&#26126;&#26174;&#26410;&#33021;&#32771;&#34385;&#21040;&#38169;&#35823;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does artificial intelligence (AI) pose existential risks to humanity? Some critics feel this question is getting too much attention, and want to push it aside in favour of conversations about the immediate risks of AI. These critics now include the journal Nature, where a recent editorial urges us to 'stop talking about tomorrow's AI doomsday when AI poses risks today.' We argue that this is a serious failure of judgement, on Nature's part. In science, as in everyday life, we expect influential actors to consider the consequences of error. As the world's leading scientific journal, Nature is certainly an influential actor, especially so in the absence of robust global regulation of AI. Yet it has manifestly failed to consider the cost of error in this case.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#32511;&#33394;&#31227;&#21160;&#35745;&#31639;&#39046;&#22495;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#24182;&#25351;&#20986;&#20154;&#24037;&#26234;&#33021;&#22312;&#31227;&#21160;&#35774;&#22791;&#20013;&#26082;&#26159;&#20851;&#38190;&#30340;&#21151;&#33021;&#23454;&#29616;&#32773;&#65292;&#21448;&#26159;&#33021;&#28304;&#28040;&#32791;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.04436</link><description>&lt;p&gt;
AI&#22312;&#32511;&#33394;&#31227;&#21160;&#35745;&#31639;&#20013;&#30340;&#20004;&#38754;&#24615;: &#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Two Faces of AI in Green Mobile Computing: A Literature Review. (arXiv:2308.04436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04436
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#32511;&#33394;&#31227;&#21160;&#35745;&#31639;&#39046;&#22495;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#24182;&#25351;&#20986;&#20154;&#24037;&#26234;&#33021;&#22312;&#31227;&#21160;&#35774;&#22791;&#20013;&#26082;&#26159;&#20851;&#38190;&#30340;&#21151;&#33021;&#23454;&#29616;&#32773;&#65292;&#21448;&#26159;&#33021;&#28304;&#28040;&#32791;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20026;&#31227;&#21160;&#35774;&#22791;&#24102;&#26469;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#21151;&#33021;&#65292;&#22914;&#30456;&#26426;&#21644;&#35821;&#38899;&#21161;&#25163;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#65292;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#28982;&#32780;&#65292;&#36816;&#34892;&#20154;&#24037;&#26234;&#33021;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#20063;&#34987;&#29992;&#20110;&#23454;&#29616;&#26356;&#33410;&#33021;&#30340;&#31227;&#21160;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#20174;&#36825;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#20154;&#24037;&#26234;&#33021;&#26377;&#20004;&#20010;&#38754;&#21521;&#65292;&#23427;&#26082;&#26159;&#23454;&#29616;&#29702;&#24819;&#65288;&#39640;&#25928;&#65289;&#31227;&#21160;&#21151;&#33021;&#30340;&#20851;&#38190;&#65292;&#20063;&#26159;&#36825;&#20123;&#35774;&#22791;&#30340;&#20027;&#35201;&#33021;&#28304;&#28040;&#32791;&#32773;&#65292;&#26082;&#26159;&#38382;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20063;&#26159;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36807;&#21435;&#21313;&#24180;&#22312;&#32511;&#33394;&#31227;&#21160;&#35745;&#31639;&#39046;&#22495;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#36890;&#36807;&#20998;&#26512;34&#31687;&#35770;&#25991;&#65292;&#25105;&#20204;&#24635;&#32467;&#20986;13&#20010;&#20027;&#35201;&#20027;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#39046;&#22495;&#36817;&#24180;&#26469;&#30340;&#21457;&#23637;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is bringing ever new functionalities to the realm of mobile devices that are now considered essential (e.g., camera and voice assistants, recommender systems). Yet, operating artificial intelligence takes up a substantial amount of energy. However, artificial intelligence is also being used to enable more energy-efficient solutions for mobile systems. Hence, artificial intelligence has two faces in that regard, it is both a key enabler of desired (efficient) mobile functionalities and a major power draw on these devices, playing a part in both the solution and the problem. In this paper, we present a review of the literature of the past decade on the usage of artificial intelligence within the realm of green mobile computing. From the analysis of 34 papers, we highlight the emerging patterns and map the field into 13 main topics that are summarized in details.  Our results showcase that the field is slowly increasing in the past years, more specifically, since 2
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#21644;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#65292;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2308.04396</link><description>&lt;p&gt;
&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining. (arXiv:2308.04396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#21644;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#65292;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#25366;&#25496;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#20174;&#20449;&#24687;&#31995;&#32479;&#30340;&#20107;&#20214;&#26085;&#24535;&#20013;&#21457;&#29616;&#27969;&#31243;&#27169;&#22411;&#12290;&#27969;&#31243;&#25366;&#25496;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#38754;&#21521;&#27969;&#31243;&#30340;&#20225;&#19994;&#31995;&#32479;&#65292;&#20294;&#23545;&#20110;&#38754;&#21521;&#36890;&#20449;&#21644;&#25991;&#26723;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#65288;ECS&#65289;&#26469;&#35828;&#19981;&#22826;&#36866;&#29992;&#12290;ECS&#20107;&#20214;&#26085;&#24535;&#38750;&#24120;&#32454;&#31890;&#24230;&#65292;&#23545;&#20854;&#26085;&#24535;&#24212;&#29992;&#27969;&#31243;&#25366;&#25496;&#20250;&#23548;&#33268;&#28151;&#20081;&#30340;&#27169;&#22411;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20107;&#20214;&#25277;&#35937;&#65292;&#21363;&#22312;&#36816;&#34892;&#21457;&#29616;&#31639;&#27861;&#20043;&#21069;&#23558;&#20302;&#32423;&#21035;&#26085;&#24535;&#36716;&#25442;&#20026;&#26356;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#26082;&#26377;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;ECS&#26085;&#24535;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23450;&#21046;&#30340;ECS&#20107;&#20214;&#25277;&#35937;&#65288;ECSEA&#65289;&#26041;&#27861;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#35760;&#24405;&#30340;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#65288;&#39640;&#32423;&#21035;&#36319;&#36394;&#65289;&#19982;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#65288;&#20174;ECS&#20013;&#25552;&#21462;&#65289;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#33258;&#21160;&#23558;&#26410;&#26469;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
One aim of Process Mining (PM) is the discovery of process models from event logs of information systems. PM has been successfully applied to process-oriented enterprise systems but is less suited for communication- and document-oriented Enterprise Collaboration Systems (ECS). ECS event logs are very fine-granular and PM applied to their logs results in spaghetti models. A common solution for this is event abstraction, i.e., converting low-level logs into more abstract high-level logs before running discovery algorithms. ECS logs come with special characteristics that have so far not been fully addressed by existing event abstraction approaches. We aim to close this gap with a tailored ECS event abstraction (ECSEA) approach that trains a model by comparing recorded actual user activities (high-level traces) with the system-generated low-level traces (extracted from the ECS). The model allows us to automatically convert future low-level traces into an abstracted high-level log that can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#27169;&#25311;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#31616;&#21270;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;24&#28857;&#28216;&#25103;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.04371</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32047;&#31215;&#25512;&#29702;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Cumulative Reasoning With Large Language Models. (arXiv:2308.04371v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#27169;&#25311;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#31616;&#21270;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;24&#28857;&#28216;&#25103;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#24378;&#22823;&#19988;&#22810;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#38656;&#35201;&#28145;&#24605;&#29087;&#34385;&#65292;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27492;&#21482;&#26377;&#26368;&#23567;&#31243;&#24230;&#30340;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#65292;&#23427;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#20351;&#20854;&#26356;&#26131;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#12290;&#23545;&#20110;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;CR&#22312;&#24615;&#33021;&#19978;&#22987;&#32456;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22810;&#36798;9.3&#65285;&#65292;&#24182;&#22312;&#32463;&#36807;&#31574;&#21010;&#30340;FOLIO&#32500;&#22522;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#24778;&#20154;&#30340;98.04&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#22312;24&#28857;&#28216;&#25103;&#30340;&#32972;&#26223;&#19979;&#65292;CR&#23454;&#29616;&#20102;94&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While language models are powerful and versatile, they often fail to address highly complex problems. This is because solving complex problems requires deliberate thinking, which has been only minimally guided during training. In this paper, we propose a new method called Cumulative Reasoning (CR), which employs language models in a cumulative and iterative manner to emulate human thought processes. By decomposing tasks into smaller components, \ournameb streamlines the problem-solving process, rendering it both more manageable and effective. For logical inference tasks, CR consistently outperforms existing methods with an improvement up to 9.3\%, and achieves the astonishing accuracy of 98.04\% on the curated FOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy of 94\%, which signifies a substantial enhancement of 20\% over the previous state-of-the-art method.
&lt;/p&gt;</description></item><item><title>&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#65292;&#19968;&#27454;&#20855;&#26377;&#28151;&#21512;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#21151;&#33021;&#30340;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#65292;&#25317;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#23637;&#31034;&#20329;&#25140;&#32773;&#30340;&#30524;&#30555;&#20197;&#21450;&#25968;&#23383;&#30343;&#20896;&#25353;&#38062;&#30340;&#34701;&#21512;&#21151;&#33021;&#12290;&#36825;&#27454;&#26080;&#32447;&#35774;&#22791;&#21487;&#33021;&#23454;&#29616;&#20102;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04313</link><description>&lt;p&gt;
Apple Vision Pro for Healthcare: &#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#65311;&#65288;arXiv:2308.04313v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Apple Vision Pro for Healthcare: "The Ultimate Display"?. (arXiv:2308.04313v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04313
&lt;/p&gt;
&lt;p&gt;
&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#65292;&#19968;&#27454;&#20855;&#26377;&#28151;&#21512;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#21151;&#33021;&#30340;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#65292;&#25317;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#23637;&#31034;&#20329;&#25140;&#32773;&#30340;&#30524;&#30555;&#20197;&#21450;&#25968;&#23383;&#30343;&#20896;&#25353;&#38062;&#30340;&#34701;&#21512;&#21151;&#33021;&#12290;&#36825;&#27454;&#26080;&#32447;&#35774;&#22791;&#21487;&#33021;&#23454;&#29616;&#20102;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2023&#24180;6&#26376;&#30340;&#20840;&#29699;&#24320;&#21457;&#32773;&#22823;&#20250;&#65288;WWDC&#65289;&#19978;&#65292;&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#12290;Vision Pro&#26159;&#19968;&#27454;&#28151;&#21512;&#29616;&#23454;&#65288;MR&#65289;&#22836;&#30420;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#23427;&#26159;&#19968;&#27454;&#20855;&#26377;&#39069;&#22806;&#35270;&#39057;&#36879;&#35270;&#65288;VST&#65289;&#33021;&#21147;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#35774;&#22791;&#12290;&#36890;&#36807;&#23558;&#30495;&#23454;&#19990;&#30028;&#36890;&#36807;&#25668;&#20687;&#22836;&#20256;&#36755;&#21040;&#29992;&#25143;&#30524;&#21069;&#30340;&#65288;VR&#65289;&#23631;&#24149;&#65292;&#20351;&#24471;Vision Pro&#20063;&#25104;&#20026;&#20102;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#35774;&#22791;&#12290;&#24403;&#28982;&#65292;&#36825;&#24182;&#19981;&#29420;&#29305;&#65292;&#19982;Varjo XR-3&#31561;&#20854;&#20182;&#35774;&#22791;&#31867;&#20284;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;Vision Pro&#20855;&#26377;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#21487;&#20197;&#21521;&#8220;&#22806;&#30028;&#8221;&#26174;&#31034;&#20329;&#25140;&#22836;&#30420;&#32773;&#30340;&#30524;&#30555;&#65292;&#25110;&#32773;&#39030;&#37096;&#30340;&#19968;&#20010;&#25353;&#38062;&#31216;&#20026;&#8220;&#25968;&#23383;&#30343;&#20896;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#26059;&#36716;&#26080;&#32541;&#22320;&#34701;&#21512;&#25968;&#23383;&#20869;&#23481;&#19982;&#29289;&#29702;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;Vision Pro&#26159;&#26080;&#32447;&#30340;&#65292;&#21482;&#26377;&#30005;&#27744;&#30340;&#30005;&#32518;&#36830;&#25509;&#65292;&#36825;&#20351;&#24471;&#22836;&#30420;&#27604;Varjo XR-3&#26356;&#21152;&#28789;&#27963;&#12290;&#36825;&#21487;&#33021;&#26356;&#25509;&#36817;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more specifically it is a Virtual Reality (VR) device with an additional Video See-Through (VST) capability. The VST capability turns the Vision Pro also into an Augmented Reality (AR) device. The AR feature is enabled by streaming the real world via cameras to the (VR) screens in front of the user's eyes. This is of course not unique and similar to other devices, like the Varjo XR-3. Nevertheless, the Vision Pro has some interesting features, like an inside-out screen that can show the headset wearers' eyes to "outsiders" or a button on the top, called "Digital Crown", that allows you to seamlessly blend digital content with your physical space by turning it. In addition, it is untethered, except for the cable to the battery, which makes the headset more agile, compared to the Varjo XR-3. This could actually come closer to the "Ultimate Display",
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#40657;&#30418;&#29305;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03999</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#29702;&#35299;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning. (arXiv:2308.03999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#40657;&#30418;&#29305;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Explainable AI&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20934;&#30830;&#35299;&#37322;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#65306;&#20934;&#30830;&#30340;&#35299;&#37322;&#23558;&#20026;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20869;&#37096;&#26816;&#27979;&#21040;&#30340;&#36755;&#20837;&#30456;&#20851;&#20869;&#23481;&#25552;&#20379;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#40657;&#30418;&#29305;&#24615;&#12290;&#29616;&#26377;&#25216;&#26415;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#38544;&#34255;&#33410;&#28857;&#30340;&#28608;&#27963;&#21487;&#20197;&#34987;&#20154;&#31867;&#29702;&#35299;&#65292;&#20294;&#26159;&#23545;&#38544;&#34255;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#35299;&#37322;&#36827;&#34892;&#20551;&#35774;&#21644;&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#33258;&#21160;&#21270;&#26041;&#27861;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#31181;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#20174;&#32500;&#22522;&#30334;&#31185;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#20013;&#31579;&#36873;&#20986;&#30340;&#32422;200&#19975;&#20010;&#31867;&#21035;&#65292;&#20197;&#21450;&#19968;&#20010;&#31216;&#20026;&#27010;&#24565;&#24402;&#32435;&#30340;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#35821;&#20041;Web&#39046;&#22495;&#30340;&#24212;&#29992;&#32780;&#24320;&#21457;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would provide insights into the question of what a deep learning system has internally detected as relevant on the input, de-mystifying the otherwise black-box character of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. In this paper, we provide such a method and demonstrate that it provides meaningful interpretations. Our approach is based on using large-scale background knowledge approximately 2 million classes curated from the Wikipedia concept hierarchy together with a symbolic reasoning approach called Concept Induction based on description logics, originally developed for applications in the Semantic Web field. Ou
&lt;/p&gt;</description></item><item><title>ALFA&#20351;&#29992;&#21508;&#20010;&#23618;&#27425;&#30340;&#29305;&#24449;&#25277;&#35937;&#26469;&#22686;&#24378;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#22312;&#26410;&#30693;&#21307;&#38498;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22495;&#23545;&#40784;&#23454;&#29616;&#20102;&#23545;&#19981;&#21464;&#29305;&#24449;&#30340;&#25552;&#21462;&#65292;&#20197;&#21450;&#23545;&#21442;&#19982;&#21307;&#38498;&#30340;&#39640;&#24230;&#29305;&#23450;&#29305;&#24449;&#30340;&#34920;&#31034;&#19982;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.03936</link><description>&lt;p&gt;
ALFA - &#21033;&#29992;&#21508;&#20010;&#23618;&#27425;&#30340;&#29305;&#24449;&#25277;&#35937;&#26469;&#22686;&#24378;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#22312;&#26410;&#30693;&#21307;&#38498;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals. (arXiv:2308.03936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03936
&lt;/p&gt;
&lt;p&gt;
ALFA&#20351;&#29992;&#21508;&#20010;&#23618;&#27425;&#30340;&#29305;&#24449;&#25277;&#35937;&#26469;&#22686;&#24378;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#22312;&#26410;&#30693;&#21307;&#38498;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22495;&#23545;&#40784;&#23454;&#29616;&#20102;&#23545;&#19981;&#21464;&#29305;&#24449;&#30340;&#25552;&#21462;&#65292;&#20197;&#21450;&#23545;&#21442;&#19982;&#21307;&#38498;&#30340;&#39640;&#24230;&#29305;&#23450;&#29305;&#24449;&#30340;&#34920;&#31034;&#19982;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21508;&#20010;&#23618;&#27425;&#30340;&#29305;&#24449;&#25277;&#35937;&#65292;&#26088;&#22312;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#22312;&#26410;&#30693;&#21307;&#38498;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30149;&#29702;&#23398;&#22330;&#26223;&#20013;&#30340;&#24120;&#35265;&#20998;&#24067;&#20559;&#31227;&#20316;&#20026;&#21069;&#25552;&#20219;&#21153;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#35757;&#32451;&#22270;&#20687;&#20013;&#25552;&#21462;&#19981;&#21464;&#29305;&#24449;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#26631;&#31614;&#65292;&#20174;&#32780;&#28085;&#30422;&#19981;&#21516;&#30340;&#25277;&#35937;&#23618;&#27425;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#25277;&#35937;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#22495;&#23545;&#40784;&#27169;&#22359;&#26469;&#36827;&#19968;&#27493;&#25552;&#21462;&#22312;&#19981;&#21516;&#35757;&#32451;&#21307;&#38498;&#20013;&#30340;&#19981;&#21464;&#29305;&#24449;&#12290;&#20026;&#20102;&#34920;&#31034;&#21442;&#19982;&#21307;&#38498;&#30340;&#39640;&#24230;&#29305;&#23450;&#29305;&#24449;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#26469;&#23545;&#21307;&#38498;&#26631;&#31614;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#32771;&#34385;&#35786;&#26029;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#27599;&#20010;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#36827;&#34892;&#35299;&#32544;&#20197;&#26368;&#23567;&#21270;&#20887;&#20313;&#24182;&#20998;&#31163;&#29305;&#24449;&#12290;&#36825;&#31181;&#34920;&#31034;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an exhaustive methodology that leverages all levels of feature abstraction, targeting an enhancement in the generalizability of image classification to unobserved hospitals. Our approach incorporates augmentation-based self-supervision with common distribution shifts in histopathology scenarios serving as the pretext task. This enables us to derive invariant features from training images without relying on training labels, thereby covering different abstraction levels. Moving onto the subsequent abstraction level, we employ a domain alignment module to facilitate further extraction of invariant features across varying training hospitals. To represent the highly specific features of participating hospitals, an encoder is trained to classify hospital labels, independent of their diagnostic labels. The features from each of these encoders are subsequently disentangled to minimize redundancy and segregate the features. This representation, which spans a broad spectrum of semanti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#37096;&#32626;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#25552;&#21319;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2308.03855</link><description>&lt;p&gt;
&#31227;&#21160;&#20379;&#24212;&#65306;&#25512;&#33616;&#31995;&#32479;&#30340;&#26368;&#21518;&#19968;&#22359;&#25340;&#22270;
&lt;/p&gt;
&lt;p&gt;
Mobile Supply: The Last Piece of Jigsaw of Recommender System. (arXiv:2308.03855v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#37096;&#32626;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#25552;&#21319;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26159;&#22312;&#32447;&#24179;&#21488;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#38543;&#30528;&#25163;&#26426;&#35745;&#31639;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#23558;&#25512;&#33616;&#31639;&#27861;&#37096;&#32626;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#20998;&#39029;&#26426;&#21046;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36793;&#32536;&#31471;&#31227;&#21160;&#25490;&#21517;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#31227;&#21160;&#25490;&#21517;&#21482;&#33021;&#23545;&#24403;&#21069;&#39029;&#38754;&#19978;&#30340;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#65292;&#25152;&#20197;&#22914;&#26524;&#21482;&#35843;&#29992;&#19968;&#20004;&#27425;&#26159;&#19981;&#36215;&#20316;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#29992;&#25143;&#26597;&#30475;&#20102;&#24403;&#21069;&#39029;&#38754;&#19978;&#30340;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#21518;&#65292;&#29992;&#25143;&#20250;&#21047;&#26032;&#39029;&#38754;&#33719;&#21462;&#26032;&#30340;&#39033;&#30446;&#12290;&#36825;&#20250;&#20351;&#31227;&#21160;&#25490;&#21517;&#27169;&#22411;&#20570;&#24456;&#22810;&#26080;&#29992;&#21151;&#65292;&#24433;&#21709;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#27969;&#27700;&#32447;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation system is a fundamental functionality of online platforms. With the development of computing power of mobile phones, some researchers have deployed recommendation algorithms on users' devices to solve the problems of data transmission delay and pagination mechanism. However, the existing edge-side mobile rankings cannot completely solve the problem of pagination mechanism. The mobile rankings can only sort the items on the current page, so it will not work if it is called once or twice. Besides, after the user has viewed the items of interest to the user on the current page, the user refresh to get a new page of items. This will make the mobile ranking model do a lot of useless work and affect the user's immersive experience. In order to solve the pagination mechanism problem, we propose a completely new module in the pipeline of recommender named Mobile Supply. The pipeline of recommender system is extended to "retrival-&gt;pre-ranking-&gt;ranking-&gt;re-ranking-&gt;Mobile Supply-&gt;
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65288;MDR&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;MDR&#33021;&#22815;&#22312;&#20943;&#23567;&#26041;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03443</link><description>&lt;p&gt;
&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces. (arXiv:2308.03443v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65288;MDR&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;MDR&#33021;&#22815;&#22312;&#20943;&#23567;&#26041;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#32972;&#26223;&#19979;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#25240;&#34935;&#38382;&#39064;&#12290;&#21442;&#25968;&#21270;&#26041;&#27861;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#27491;&#30830;&#30340;&#27169;&#22411;&#32780;&#23548;&#33268;&#20559;&#24046;&#65292;&#32780;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#30001;&#20110;&#26041;&#24046;&#32780;&#20135;&#29983;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21028;&#21035;&#24335;&#30340;&#19981;&#33391;&#34892;&#20026;&#25233;&#21046;&#22120;&#65288;MIPS&#65289;&#26469;&#36890;&#36807;&#23545;&#21160;&#20316;&#30340;&#23884;&#20837;&#26469;&#20943;&#23567;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#20026;&#20102;&#20351;&#20272;&#35745;&#22120;&#26356;&#20934;&#30830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIPS&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#8212;&#8212;&#36793;&#38469;&#21270;&#21452;&#37325;&#31283;&#20581;&#65288;MDR&#65289;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#22312;&#27604;MIPS&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#26159;&#26080;&#20559;&#30340;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;IPS&#30340;&#26041;&#24046;&#20943;&#23567;&#65292;&#36825;&#26159;MIPS&#30340;&#20027;&#35201;&#20248;&#21183;&#12290;&#32463;&#39564;&#23454;&#39564;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38750;&#26497;&#31471;&#29109;&#65292;&#22312;&#20840;&#29699;&#37329;&#34701;&#24066;&#22330;&#20013;&#25104;&#21151;&#26816;&#27979;&#20986;&#20102;&#24322;&#24120;&#24773;&#20917;&#65292;&#21457;&#29616;&#20102;&#22312;&#21361;&#26426;&#26399;&#38388;&#39640;&#24230;&#30456;&#20851;&#30340;&#36164;&#20135;&#30340;&#22797;&#26434;&#32467;&#26500;&#20943;&#23569;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#38750;&#26497;&#31471;&#29109;&#21442;&#25968;&#22312;&#21361;&#26426;&#21069;&#12289;&#26399;&#38388;&#21644;&#21518;&#30340;&#24322;&#24120;&#25968;&#37327;&#23384;&#22312;&#32479;&#35745;&#19978;&#30340;&#24046;&#24322;</title><link>http://arxiv.org/abs/2308.02914</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38750;&#26497;&#31471;&#29109;&#22312;&#20840;&#29699;&#37329;&#34701;&#24066;&#22330;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Global Financial Markets with Graph Neural Networks and Nonextensive Entropy. (arXiv:2308.02914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02914
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38750;&#26497;&#31471;&#29109;&#65292;&#22312;&#20840;&#29699;&#37329;&#34701;&#24066;&#22330;&#20013;&#25104;&#21151;&#26816;&#27979;&#20986;&#20102;&#24322;&#24120;&#24773;&#20917;&#65292;&#21457;&#29616;&#20102;&#22312;&#21361;&#26426;&#26399;&#38388;&#39640;&#24230;&#30456;&#20851;&#30340;&#36164;&#20135;&#30340;&#22797;&#26434;&#32467;&#26500;&#20943;&#23569;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#38750;&#26497;&#31471;&#29109;&#21442;&#25968;&#22312;&#21361;&#26426;&#21069;&#12289;&#26399;&#38388;&#21644;&#21518;&#30340;&#24322;&#24120;&#25968;&#37327;&#23384;&#22312;&#32479;&#35745;&#19978;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#21464;&#37327;&#30340;&#31995;&#32479;&#20013;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24322;&#24120;&#26159;&#19982;&#20998;&#26512;&#25968;&#25454;&#22312;&#32479;&#35745;&#19978;&#19981;&#21516;&#30340;&#31163;&#32676;&#20540;&#65292;&#21487;&#33021;&#26469;&#33258;&#32597;&#35265;&#20107;&#20214;&#12289;&#35774;&#22791;&#25925;&#38556;&#25110;&#31995;&#32479;&#28389;&#29992;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#30001;&#38750;&#26497;&#31471;&#29109;&#34913;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#22312;&#20840;&#29699;&#37329;&#34701;&#24066;&#22330;&#20013;&#26816;&#27979;&#24322;&#24120;&#30340;&#33021;&#21147;&#12290;&#20027;&#35201;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#21361;&#26426;&#26399;&#38388;&#39640;&#24230;&#30456;&#20851;&#30340;&#36164;&#20135;&#30340;&#22797;&#26434;&#32467;&#26500;&#20943;&#23569;&#65292;&#19981;&#21516;&#30340;&#38750;&#26497;&#31471;&#29109;&#21442;&#25968;&#22312;&#21361;&#26426;&#21069;&#12289;&#26399;&#38388;&#21644;&#21518;&#30340;&#24322;&#24120;&#25968;&#37327;&#22312;&#32479;&#35745;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is a challenging task, particularly in systems with many variables. Anomalies are outliers that statistically differ from the analyzed data and can arise from rare events, malfunctions, or system misuse. This study investigated the ability to detect anomalies in global financial markets through Graph Neural Networks (GNN) considering an uncertainty scenario measured by a nonextensive entropy. The main findings show that the complex structure of highly correlated assets decreases in a crisis, and the number of anomalies is statistically different for nonextensive entropy parameters considering before, during, and after crisis.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02582</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#27867;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#29702;&#20986;&#23569;&#37327;&#26679;&#26412;&#65292;&#20197;&#21512;&#25104;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#27979;&#35797;&#26597;&#35810;&#30340;&#36816;&#34892;&#26102;&#25552;&#31034;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#23436;&#20840;&#35206;&#30422;SQL&#23376;&#21477;&#12289;&#36816;&#31639;&#31526;&#21644;&#20989;&#25968;&#65292;&#24182;&#22312;&#20801;&#35768;&#30340;&#20196;&#29260;&#38271;&#24230;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#22823;&#39046;&#22495;&#35206;&#30422;&#12290;&#36825;&#26679;&#21487;&#20197;&#21512;&#25104;&#19968;&#20010;&#22266;&#23450;&#30340;&#36890;&#29992;&#25552;&#31034;&#65288;GP&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;NL&#27979;&#35797;&#26597;&#35810;&#20043;&#38388;&#20849;&#29992;&#30340;&#22810;&#26679;&#21270;&#26679;&#26412;&#38598;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#36824;&#23558;GP&#33258;&#36866;&#24212;&#21040;&#30446;&#26631;&#25968;&#25454;&#24211;&#39046;&#22495;&#65288;DA-GP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#27867;&#21270;&#65307;&#28982;&#21518;&#37319;&#29992;&#20998;&#35299;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#65288;LTMP-DA-GP&#65289;&#26469;&#22788;&#29702;&#36328;&#32452;&#21512;&#27867;&#21270;&#12290;LTMP-DA-GP&#30340;&#21512;&#25104;&#26159;&#31163;&#32447;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
&lt;/p&gt;</description></item><item><title>AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.02182</link><description>&lt;p&gt;
AutoML4ETC: &#33258;&#21160;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23454;&#29616;&#29616;&#23454;&#19990;&#30028;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification. (arXiv:2308.02182v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02182
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;DL&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#38543;&#26102;&#38388;&#19981;&#21487;&#36991;&#20813;&#22320;&#19979;&#38477;&#12290;&#20165;&#20165;&#23545;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#21482;&#33021;&#37096;&#20998;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25163;&#21160;&#35843;&#25972;&#27169;&#22411;&#26550;&#26500;&#20197;&#28385;&#36275;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26399;&#26395;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;AutoML4ETC&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#20197;&#36827;&#34892;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#24378;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#19987;&#38376;&#38024;&#23545;&#20351;&#29992;&#25968;&#25454;&#21253;&#22836;&#23383;&#33410;&#36827;&#34892;&#36817;&#23454;&#26102;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#25628;&#32034;&#31354;&#38388;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AutoML4ETC&#29983;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01011</link><description>&lt;p&gt;
&#20351;&#29992;Floss&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#39057;&#22495;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26159;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23637;&#29616;&#20986;&#37325;&#35201;&#30340;&#21608;&#26399;&#24615;&#25110;&#20934;&#21608;&#26399;&#24615;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#24448;&#24448;&#19981;&#33021;&#34987;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20805;&#20998;&#25429;&#25417;&#21040;&#12290;&#36825;&#23548;&#33268;&#23545;&#24863;&#20852;&#36259;&#30340;&#22522;&#30784;&#21160;&#24577;&#34892;&#20026;&#30340;&#34920;&#31034;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21483;&#20570;Floss&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#22320;&#22312;&#39057;&#22495;&#19978;&#35843;&#25972;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;Floss&#26041;&#27861;&#39318;&#20808;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20027;&#35201;&#21608;&#26399;&#24615;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#21608;&#26399;&#31227;&#20301;&#21644;&#35889;&#23494;&#24230;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;Floss&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#21040;&#26377;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#20808;&#39564;&#30693;&#35782;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#38598;&#25104;&#21040;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#19988;&#27867;&#21270;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.16186</link><description>&lt;p&gt;
ESP:&#21033;&#29992;&#23545;&#31216;&#24615;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#22810;Agent&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning. (arXiv:2307.16186v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#20808;&#39564;&#30693;&#35782;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#38598;&#25104;&#21040;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#19988;&#27867;&#21270;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#35201;&#27714;&#26500;&#24314;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22312;&#24403;&#21069;&#30340;MARL&#26041;&#27861;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#21463;&#22810;Agent&#31995;&#32479;&#20013;&#23545;&#31216;&#29616;&#35937;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#22686;&#24378;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#19968;&#33268;&#24615;&#25439;&#22833;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;MARL&#26041;&#27861;&#20013;&#65292;&#26469;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;MARL&#31639;&#27861;&#12290;&#23545;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#19968;&#20010;&#29289;&#29702;&#22810;&#26426;&#22120;&#20154;&#23454;&#39564;&#24179;&#21488;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) has achieved promising results in recent years. However, most existing reinforcement learning methods require a large amount of data for model training. In addition, data-efficient reinforcement learning requires the construction of strong inductive biases, which are ignored in the current MARL approaches. Inspired by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12306</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#35781;&#21650;(CoD)&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#26469;&#26497;&#24230;&#31246;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#22312;&#35299;&#20915;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#20013;&#38754;&#20020;&#26497;&#22823;&#25361;&#25112;&#65292;&#27491;&#22914;Richard Bellman&#22312;60&#24180;&#21069;&#39318;&#27425;&#25351;&#20986;&#30340;&#37027;&#26679;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#39640;&#32500;&#24230;&#19978;&#25968;&#20540;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#36825;&#26679;&#30340;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#65292;&#32780;&#23558;&#19968;&#33324;&#38750;&#32447;&#24615;PDEs&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#20174;&#26410;&#23454;&#29616;&#36807;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25193;&#23637;&#21040;&#35299;&#20915;&#20219;&#24847;&#39640;&#32500;PDEs&#12290;&#35813;&#26032;&#26041;&#27861;&#31216;&#20026;&#38543;&#26426;&#32500;&#24230;&#26799;&#24230;&#19979;&#38477;(SDGD)&#65292;&#23558;PDE&#30340;&#26799;&#24230;&#20998;&#35299;&#20026;&#19982;&#19981;&#21516;&#32500;&#24230;&#23545;&#24212;&#30340;&#37096;&#20998;&#65292;&#24182;&#22312;&#35757;&#32451;PINNs&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#38543;&#26426;&#36873;&#25321;&#36825;&#20123;&#32500;&#24230;&#37096;&#20998;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.12267</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#35770;&#25991;&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#33021;&#22815;&#22312;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27969;&#30021;&#22238;&#31572;&#12290;&#23613;&#31649;&#25215;&#35748;&#25216;&#26415;&#36827;&#27493;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#25945;&#32946;&#32773;&#20063;&#25285;&#24515;&#23398;&#29983;&#21487;&#33021;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#24182;&#23558;&#20854;&#20551;&#20882;&#20026;&#33258;&#24049;&#30340;&#21407;&#21019;&#20316;&#21697;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;AI&#20869;&#23481;&#26816;&#27979;&#30740;&#31350;&#26159;&#22522;&#20110;&#36825;&#20123;&#25285;&#24551;&#36827;&#34892;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;AI&#20869;&#23481;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20551;&#35774;&#19968;&#20010;&#25991;&#26412;&#35201;&#20040;&#23436;&#20840;&#30001;&#20154;&#31867;&#32534;&#20889;&#65292;&#35201;&#20040;&#23436;&#20840;&#30001;AI&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#22312;&#19968;&#20010;&#23569;&#26377;&#25506;&#32034;&#20294;&#21364;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26816;&#27979;&#30340;&#25991;&#26412;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#28151;&#21512;&#25991;&#26412;&#65289;&#21327;&#20316;&#32534;&#20889;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#26816;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20174;&#32473;&#23450;&#30340;&#28151;&#21512;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#31867;&#32534;&#20889;&#20869;&#23481;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#36716;&#25442;&#28857;&#65288;&#36793;&#30028;&#26816;&#27979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
&lt;/p&gt;</description></item><item><title>MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.10763</link><description>&lt;p&gt;
MSQNet: &#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10763
&lt;/p&gt;
&lt;p&gt;
MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#65292;&#22240;&#20026;&#28436;&#21592;&#20043;&#38388;&#20855;&#26377;&#22266;&#26377;&#30340;&#25299;&#25169;&#21644;&#26174;&#30528;&#24046;&#24322;&#12290;&#36825;&#23601;&#38656;&#35201;&#29305;&#23450;&#28436;&#21592;&#30340;&#23039;&#24577;&#20272;&#35745;&#65288;&#20363;&#22914;&#20154;&#31867;&#19982;&#21160;&#29289;&#65289;&#65292;&#23548;&#33268;&#27169;&#22411;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#39640;&#32500;&#25252;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#21482;&#20851;&#27880;&#23398;&#20064;&#35270;&#35273;&#27169;&#24577;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#21487;&#29992;&#20449;&#24687;&#28304;&#65288;&#20363;&#22914;&#31867;&#21517;&#25991;&#26412;&#65289;&#21644;&#22810;&#20010;&#21160;&#20316;&#30340;&#21516;&#26102;&#21457;&#29983;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#8221;&#65292;&#20026;&#21253;&#25324;&#20154;&#31867;&#21644;&#21160;&#29289;&#22312;&#20869;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#28436;&#21592;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65288;&#20363;&#22914;DETR&#65289;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#26597;&#35810;&#32593;&#32476;&#65288;MSQNet&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#12290;&#28040;&#38500;&#20102;&#28436;&#21592;&#29305;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-spec
&lt;/p&gt;</description></item><item><title>RL-ViGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#21547;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#31867;&#22411;&#65292;&#26088;&#22312;&#25512;&#21160;&#23545;&#20195;&#29702;&#20154;&#35270;&#35273;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.10224</link><description>&lt;p&gt;
RL-ViGen: &#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization. (arXiv:2307.10224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10224
&lt;/p&gt;
&lt;p&gt;
RL-ViGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#21547;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#31867;&#22411;&#65292;&#26088;&#22312;&#25512;&#21160;&#23545;&#20195;&#29702;&#20154;&#35270;&#35273;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;Visual RL&#65289;&#19982;&#39640;&#32500;&#35266;&#23519;&#30456;&#32467;&#21512;&#65292;&#19968;&#30452;&#38754;&#20020;&#30528;&#38271;&#26399;&#23384;&#22312;&#30340;&#27867;&#21270;&#25361;&#25112;&#12290;&#23613;&#31649;&#26377;&#37325;&#28857;&#30740;&#31350;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#27867;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#22522;&#20934;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#23616;&#38480;&#20110;&#23396;&#31435;&#30340;&#20219;&#21153;&#21644;&#27867;&#21270;&#31867;&#21035;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#23545;&#20195;&#29702;&#20154;&#35270;&#35273;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RL-ViGen&#65306;&#19968;&#31181;&#26032;&#22411;&#30340;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#31867;&#22411;&#65292;&#20174;&#32780;&#20419;&#36827;&#24471;&#20986;&#26356;&#21487;&#38752;&#30340;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;RL-ViGen&#23558;&#26368;&#26032;&#30340;&#27867;&#21270;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#29616;&#26377;&#31639;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#26222;&#36941;&#21344;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#24895;&#26223;&#26159;RL-ViGen&#23558;&#22312;&#36825;&#20010;&#39046;&#22495;&#36215;&#21040;&#20652;&#21270;&#21058;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Reinforcement Learning (Visual RL), coupled with high-dimensional observations, has consistently confronted the long-standing challenge of generalization. Despite the focus on algorithms aimed at resolving visual generalization problems, we argue that the devil is in the existing benchmarks as they are restricted to isolated tasks and generalization categories, undermining a comprehensive evaluation of agents' visual generalization capabilities. To bridge this gap, we introduce RL-ViGen: a novel Reinforcement Learning Benchmark for Visual Generalization, which contains diverse tasks and a wide spectrum of generalization types, thereby facilitating the derivation of more reliable conclusions. Furthermore, RL-ViGen incorporates the latest generalization visual RL algorithms into a unified framework, under which the experiment results indicate that no single existing algorithm has prevailed universally across tasks. Our aspiration is that RL-ViGen will serve as a catalyst in this a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08873</link><description>&lt;p&gt;
&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#26367;&#20195;&#65306;&#22522;&#23612;&#31163;&#24046;
&lt;/p&gt;
&lt;p&gt;
An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39118;&#38505;&#21388;&#24694;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#38480;&#21046;&#31574;&#30053;&#22238;&#25253;&#30340;&#26041;&#24046;&#26159;&#19968;&#31181;&#24120;&#35265;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26126;&#30830;&#30340;&#25968;&#23398;&#23450;&#20041;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#38480;&#21046;&#24635;&#22238;&#25253;&#26041;&#24046;&#65292;&#32780;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#27599;&#27493;&#22870;&#21169;&#26041;&#24046;&#20316;&#20026;&#20195;&#29702;&#12290;&#26412;&#25991;&#24443;&#24213;&#30740;&#31350;&#20102;&#36825;&#20123;&#22522;&#20110;&#26041;&#24046;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#25968;&#23383;&#23610;&#24230;&#30340;&#25935;&#24863;&#24615;&#21644;&#38459;&#30861;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#26367;&#20195;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#8212;&#8212;&#22522;&#23612;&#31163;&#24046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#21270;&#22522;&#23612;&#31163;&#24046;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#22312;&#39118;&#38505;&#21388;&#24694;&#21487;&#20197;&#26126;&#30830;&#23450;&#20041;&#30340;&#39046;&#22495;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#22522;&#20110;&#26041;&#24046;&#30340;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#20854;&#20182;&#31574;&#30053;&#26080;&#27861;&#23398;&#21040;&#21512;&#29702;&#31574;&#30053;&#26102;&#23454;&#29616;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#65292;&#20197;&#26041;&#24046;&#21644;&#22522;&#23612;&#31163;&#24046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
&lt;/p&gt;</description></item><item><title>INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08131</link><description>&lt;p&gt;
INFLECT-DGNN: &#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24433;&#21709;&#32773;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks. (arXiv:2307.08131v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08131
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#32593;&#32476;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#12290;&#22312;&#25512;&#33616;&#21644;&#23450;&#21521;&#33829;&#38144;&#39046;&#22495;&#20013;&#65292;&#24433;&#21709;&#32773;&#26816;&#27979;&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#21160;&#24577;&#32593;&#32476;&#34920;&#31034;&#22823;&#22823;&#21463;&#30410;&#30340;&#39046;&#22495;&#65292;&#21407;&#22240;&#26159;&#19981;&#26029;&#21457;&#23637;&#30340;&#23458;&#25143;-&#21697;&#29260;&#20851;&#31995;&#12290;&#20026;&#20102;&#38416;&#36848;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;INFLECT-DGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#20197;&#21450;&#31934;&#24515;&#35774;&#35745;&#30340;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#35780;&#20272;&#39044;&#27979;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#22478;&#24066;&#32593;&#32476;&#30340;&#29420;&#29305;&#20225;&#19994;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#20197;&#21033;&#28070;&#20026;&#39537;&#21160;&#30340;&#24433;&#21709;&#32773;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#20197;&#21450;GNN&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships. To elaborate this idea, we introduce INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural Networks (RNN) with weighted loss functions, the Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted rolling-window strategy. To evaluate predictive performance, we utilize a unique corporate data set with networks of three cities and derive a profit-driven evaluation methodology for influencer prediction. Our results show how using RNN to encode temporal attributes alongside GNNs significantly improves predictive perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#29983;&#25104;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20248;&#21270;&#21407;&#22987;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.07415</link><description>&lt;p&gt;
AutoHint: &#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#19982;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#29983;&#25104;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20248;&#21270;&#21407;&#22987;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#34429;&#28982;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27880;&#37322;&#33021;&#21147;&#65292;&#20294;&#23558;&#27492;&#33021;&#21147;&#24212;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#20851;&#38190;&#22312;&#20110;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#27966;&#29983;&#30340;&#20016;&#23500;&#25351;&#23548;&#32435;&#20837;&#21407;&#22987;&#25552;&#31034;&#65292;&#20197;&#32487;&#25215;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#20016;&#23500;&#31216;&#20026;&#8220;&#25552;&#31034;&#8221;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#19968;&#20010;&#21021;&#22987;&#25552;&#31034;&#24320;&#22987;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#25351;&#23548;LLM&#20174;&#38169;&#35823;&#39044;&#27979;&#20013;&#25512;&#26029;&#20986;&#36873;&#23450;&#26679;&#26412;&#30340;&#26032;&#25552;&#31034;&#65292;&#28982;&#21518;&#20174;&#27599;&#20010;&#26679;&#26412;&#30340;&#25552;&#31034;&#20013;&#36827;&#34892;&#24635;&#32467;&#65292;&#24182;&#23558;&#32467;&#26524;&#28155;&#21152;&#22238;&#21021;&#22987;&#25552;&#31034;&#65292;&#24418;&#25104;&#19968;&#20010;&#26032;&#30340;&#20016;&#23500;&#25351;&#23548;&#12290;&#35813;&#26041;&#27861;&#22312;BIG-Bench&#25351;&#20196;&#25512;&#23548;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents AutoHint, a novel framework for automatic prompt engineering and optimization for Large Language Models (LLM). While LLMs have demonstrated remarkable ability in achieving high-quality annotation in various tasks, the key to applying this ability to specific tasks lies in developing high-quality prompts. Thus we propose a framework to inherit the merits of both in-context learning and zero-shot learning by incorporating enriched instructions derived from input-output demonstrations to optimize original prompt. We refer to the enrichment as the hint and propose a framework to automatically generate the hint from labeled data. More concretely, starting from an initial prompt, our method first instructs a LLM to deduce new hints for selected samples from incorrect predictions, and then summarizes from per-sample hints and adds the results back to the initial prompt to form a new, enriched instruction. The proposed method is evaluated on the BIG-Bench Instruction Induct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.03759</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#25554;&#20540;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection. (arXiv:2307.03759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#26159;&#35760;&#24405;&#21160;&#24577;&#31995;&#32479;&#27979;&#37327;&#32467;&#26524;&#30340;&#20027;&#35201;&#25968;&#25454;&#31867;&#22411;&#65292;&#36890;&#36807;&#29289;&#29702;&#20256;&#24863;&#22120;&#21644;&#22312;&#32447;&#36807;&#31243;&#65288;&#34394;&#25311;&#20256;&#24863;&#22120;&#65289;&#29983;&#25104;&#22823;&#37327;&#25968;&#25454;&#12290;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#25581;&#31034;&#21487;&#29992;&#25968;&#25454;&#20013;&#25152;&#34164;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22522;&#20110;GNN&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#20256;&#32479;&#21644;&#20854;&#20182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21017;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;&#65288;GNN4TS&#65289;&#65292;&#21253;&#25324;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#25105;&#20204;&#26088;&#22312;&#25351;&#23548;&#35774;&#35745;&#24072;&#21644;&#23454;&#36341;&#32773;&#20102;&#35299;&#12289;&#26500;&#24314;&#24212;&#29992;&#21644;&#25512;&#21160;GNN4TS&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;GNN4TS&#20998;&#31867;&#20307;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. Approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: Forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present a
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09417</link><description>&lt;p&gt;
Diff-TTSG: &#21435;&#22122;&#27010;&#29575;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09417
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26391;&#35835;&#35821;&#38899;&#21512;&#25104;&#23454;&#29616;&#39640;&#33258;&#28982;&#24230;&#35780;&#20998;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#21512;&#25104;&#33258;&#28982;&#35328;&#35821;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#38754;&#23545;&#38754;&#30340;&#33258;&#21457;&#23545;&#35805;&#26082;&#26377;&#21475;&#22836;&#30340;&#65292;&#20063;&#26377;&#38750;&#35821;&#35328;&#30340;&#65288;&#20363;&#22914;&#65292;&#20849;&#21516;&#35328;&#35821;&#25163;&#21183;&#65289;&#12290;&#26368;&#36817;&#25165;&#24320;&#22987;&#30740;&#31350;&#32852;&#21512;&#21512;&#25104;&#36825;&#20004;&#31181;&#27169;&#24577;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#31995;&#32479;&#20013;&#30340;&#22909;&#22788;&#12290;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20351;&#29992;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#36807;&#24230;&#24179;&#28369;&#30340;&#20266;&#24433;&#21644;&#27425;&#20248;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026; Diff-TTSG&#65292;&#20849;&#21516;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#32452;&#23567;&#24515;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#20027;&#35266;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#31995;&#32479;&#65292;&#24182;&#29992;&#23427;&#20204;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#21512;&#25104;&#30340;&#26679;&#20363;&#32780;&#35328;&#65292;Diff-TTSG&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
With read-aloud speech synthesis achieving high naturalness scores, there is a growing research interest in synthesising spontaneous speech. However, human spontaneous face-to-face conversation has both spoken and non-verbal aspects (here, co-speech gestures). Only recently has research begun to explore the benefits of jointly synthesising these two modalities in a single system. The previous state of the art used non-probabilistic methods, which fail to capture the variability of human speech and motion, and risk producing oversmoothing artefacts and sub-optimal synthesis quality. We present the first diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to synthesise speech and gestures together. Our method can be trained on small datasets from scratch. Furthermore, we describe a set of careful uni- and multi-modal subjective tests for evaluating integrated speech and gesture synthesis systems, and use them to validate our proposed approach. For synthesised examp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#20998;&#23618;&#26550;&#26500;&#65292;&#21033;&#29992;&#26102;&#38388;&#25277;&#35937;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36830;&#32493;&#25511;&#21046;&#65292;&#20855;&#26377;&#33410;&#33021;&#12289;&#25345;&#32493;&#25506;&#32034;&#12289;&#20943;&#23569;&#20915;&#31574;&#12289;&#20943;&#23569;&#25238;&#21160;&#21644;&#22686;&#21152;&#21160;&#20316;&#37325;&#22797;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.18701</link><description>&lt;p&gt;
&#39640;&#25928;&#36830;&#32493;&#25511;&#21046;&#30340;&#26102;&#38388;&#20998;&#23618;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Temporally Layered Architecture for Efficient Continuous Control. (arXiv:2305.18701v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18701
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#20998;&#23618;&#26550;&#26500;&#65292;&#21033;&#29992;&#26102;&#38388;&#25277;&#35937;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36830;&#32493;&#25511;&#21046;&#65292;&#20855;&#26377;&#33410;&#33021;&#12289;&#25345;&#32493;&#25506;&#32034;&#12289;&#20943;&#23569;&#20915;&#31574;&#12289;&#20943;&#23569;&#25238;&#21160;&#21644;&#22686;&#21152;&#21160;&#20316;&#37325;&#22797;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#20998;&#23618;&#26550;&#26500;&#65288;TLA&#65289;&#65292;&#29992;&#20110;&#20855;&#26377;&#26368;&#23567;&#33021;&#37327;&#28040;&#32791;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#25511;&#21046;&#12290;TLA&#23558;&#24555;&#36895;&#31574;&#30053;&#21644;&#24930;&#36895;&#31574;&#30053;&#23618;&#21472;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#26102;&#38388;&#25277;&#35937;&#65292;&#20351;&#27599;&#20010;&#23618;&#21487;&#20197;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20511;&#37492;&#20102;&#20154;&#33041;&#30340;&#33410;&#33021;&#26426;&#21046;&#65292;&#26681;&#25454;&#29615;&#22659;&#38656;&#27714;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#25191;&#34892;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;TLA&#38500;&#20102;&#33410;&#33021;&#20043;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#35768;&#22810;&#39069;&#22806;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#25345;&#32493;&#25506;&#32034;&#12289;&#20943;&#23569;&#38656;&#27714;&#20915;&#31574;&#12289;&#20943;&#23569;&#25238;&#21160;&#21644;&#22686;&#21152;&#21160;&#20316;&#37325;&#22797;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;TLA&#22312;&#22810;&#20010;&#37325;&#35201;&#25351;&#26631;&#19978;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22810;&#30446;&#26631;&#35780;&#20998;&#26469;&#23450;&#24615;&#35780;&#20272;&#36830;&#32493;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;TLA&#20855;&#26377;&#26174;&#33879;&#26356;&#22909;&#30340;&#35780;&#20998;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#31639;&#27861;&#22312;&#24930;&#36895;&#31574;&#30053;&#21644;
&lt;/p&gt;
&lt;p&gt;
We present a temporally layered architecture (TLA) for temporally adaptive control with minimal energy expenditure. The TLA layers a fast and a slow policy together to achieve temporal abstraction that allows each layer to focus on a different time scale. Our design draws on the energy-saving mechanism of the human brain, which executes actions at different timescales depending on the environment's demands. We demonstrate that beyond energy saving, TLA provides many additional advantages, including persistent exploration, fewer required decisions, reduced jerk, and increased action repetition. We evaluate our method on a suite of continuous control tasks and demonstrate the significant advantages of TLA over existing methods when measured over multiple important metrics. We also introduce a multi-objective score to qualitatively assess continuous control policies and demonstrate a significantly better score for TLA. Our training algorithm uses minimal communication between the slow and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#22270;&#20687;&#26816;&#32034;&#30340;&#36328;&#22495;&#21644;&#35821;&#20041;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20837;&#31616;&#21333;&#19988;&#36731;&#37327;&#30340;&#22495;&#36866;&#37197;&#22120;&#37325;&#26032;&#23398;&#20064;&#33609;&#22270;&#39046;&#22495;&#30340;&#26032;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#23545;&#40784;&#23398;&#20064;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#20197;&#25552;&#39640;&#36328;&#22495;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05144</link><description>&lt;p&gt;
&#25913;&#36827;&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#30340;&#22270;&#20687;&#26816;&#32034;&#30340;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval. (arXiv:2305.05144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#22270;&#20687;&#26816;&#32034;&#30340;&#36328;&#22495;&#21644;&#35821;&#20041;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20837;&#31616;&#21333;&#19988;&#36731;&#37327;&#30340;&#22495;&#36866;&#37197;&#22120;&#37325;&#26032;&#23398;&#20064;&#33609;&#22270;&#39046;&#22495;&#30340;&#26032;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#23545;&#40784;&#23398;&#20064;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#20197;&#25552;&#39640;&#36328;&#22495;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#30340;&#22270;&#20687;&#26816;&#32034;(ZS-SBIR)&#30001;&#20110;&#33609;&#22270;&#21644;&#29031;&#29255;&#20043;&#38388;&#30340;&#36328;&#22495;&#26412;&#36136;&#20197;&#21450;&#24050;&#30693;&#21644;&#26410;&#30693;&#22270;&#20687;&#20998;&#24067;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#21508;&#31181;&#36741;&#21161;&#20449;&#24687;&#21644;&#23398;&#20064;&#31574;&#30053;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#35813;&#31354;&#38388; (\romannumeral 1)&#22312;&#33609;&#22270;&#21644;&#29031;&#29255;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#65292;(\romannumeral 2) &#26725;&#25509;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#22312;&#36866;&#24212;&#39046;&#22495;&#21644;&#20174;&#24050;&#30693;&#31867;&#21035;&#20256;&#36882;&#30693;&#35782;&#26041;&#38754;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#8220;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#8221;&#26041;&#27861;&#26469;&#35299;&#20915;&#20851;&#38190;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25554;&#20837;&#31616;&#21333;&#19988;&#36731;&#37327;&#30340;&#22495;&#36866;&#37197;&#22120;&#65292;&#20197;&#23398;&#20064;&#33609;&#22270;&#39046;&#22495;&#30340;&#26032;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#25552;&#39640;&#36328;&#22495;&#34920;&#31034;&#33021;&#21147;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;(CLIP)&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23558;&#23398;&#20064;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#19982;&#27169;&#22411;&#26126;&#30830;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the cross-domain nature of sketches and photos, as well as the semantic gap between seen and unseen image distributions. Previous methods fine-tune pre-trained models with various side information and learning strategies to learn a compact feature space that (\romannumeral1) is shared between the sketch and photo domains and (\romannumeral2) bridges seen and unseen classes. However, these efforts are inadequate in adapting domains and transferring knowledge from seen to unseen classes. In this paper, we present an effective \emph{``Adapt and Align''} approach to address the key challenges. Specifically, we insert simple and lightweight domain adapters to learn new abstract concepts of the sketch domain and improve cross-domain representation capabilities. Inspired by recent advances in image-text foundation models (\textit{e.g.}, CLIP) on zero-shot scenarios, we explicitly align the learned image embedding with a mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#38477;&#20302;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#36798;&#21040;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02396</link><description>&lt;p&gt;
&#29305;&#24449;&#24037;&#31243;&#33021;&#24110;&#21161;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Feature Engineering Help Quantum Machine Learning for Malware Detection?. (arXiv:2305.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#38477;&#20302;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#36798;&#21040;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#25968;&#37327;&#21644;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#35768;&#22810;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#30340;&#27969;&#34892;ML&#27169;&#22411;&#37117;&#26159;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#20123;&#26377;&#30417;&#30563;&#20998;&#31867;&#22120;&#36890;&#24120;&#23545;&#26032;&#22411;&#24694;&#24847;&#36719;&#20214;&#30340;&#25512;&#24191;&#25928;&#26524;&#19981;&#22909;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#32463;&#24120;&#37325;&#26032;&#35757;&#32451;&#23427;&#20204;&#20197;&#26816;&#27979;&#26032;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#37327;&#23376;ML&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#38477;&#20302;&#25968;&#25454;&#22823;&#23567;&#21644;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;XGBoost&#36873;&#25321;&#30340;&#29305;&#24449;&#30340;VQC&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#33719;&#24471;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;XGBoost&#36873;&#25321;&#30340;&#29305;&#24449;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;IBM 5 qubits&#26426;&#22120;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#20026;74&#65285;&#65288;+-11.35&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing number and sophistication of malware attacks, malware detection systems based on machine learning (ML) grow in importance. At the same time, many popular ML models used in malware classification are supervised solutions. These supervised classifiers often do not generalize well to novel malware. Therefore, they need to be re-trained frequently to detect new malware specimens, which can be time-consuming. Our work addresses this problem in a hybrid framework of theoretical Quantum ML, combined with feature selection strategies to reduce the data size and malware classifier training time. The preliminary results show that VQC with XGBoost selected features can get a 78.91% test accuracy on the simulator. The average accuracy for the model trained using the features selected with XGBoost was 74% (+- 11.35%) on the IBM 5 qubits machines.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;150&#20010;CWE&#30340;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#20316;&#32773;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#25913;&#36827;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#32467;&#21512;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#21069;&#36884;&#30340;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#28431;&#27934;&#20173;&#23384;&#22312;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00409</link><description>&lt;p&gt;
DiverseVul: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#30340;&#26032;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection. (arXiv:2304.00409v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;150&#20010;CWE&#30340;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#20316;&#32773;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#25913;&#36827;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#32467;&#21512;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#21069;&#36884;&#30340;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#28431;&#27934;&#20173;&#23384;&#22312;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#29228;&#21462;&#23433;&#20840;&#38382;&#39064;&#32593;&#31449;&#65292;&#25552;&#21462;&#30456;&#24212;&#39033;&#30446;&#30340;&#28431;&#27934;&#20462;&#22797;&#25552;&#20132;&#21644;&#28304;&#20195;&#30721;&#65292;&#31579;&#36873;&#20986;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;150&#20010;CWE&#65292;26,635&#20010;&#26131;&#21463;&#25915;&#20987;&#30340;&#20989;&#25968;&#21644;352,606&#20010;&#19981;&#26131;&#21463;&#25915;&#20987;&#30340;&#20989;&#25968;&#65292;&#25552;&#21462;&#33258;7,861&#20010;&#25552;&#20132;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35206;&#30422;&#20102;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;11&#20010;&#27169;&#22411;&#26550;&#26500;&#65292;&#23646;&#20110;4&#20010;&#23478;&#26063;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#65292;&#28145;&#24230;&#23398;&#20064;&#20173;&#26410;&#20934;&#22791;&#22909;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;......
&lt;/p&gt;
&lt;p&gt;
We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 150 CWEs, 26,635 vulnerable functions, and 352,606 non-vulnerable functions extracted from 7,861 commits. Our dataset covers 305 more projects than all previous datasets combined. We show that increasing the diversity and volume of training data improves the performance of deep learning models for vulnerability detection.  Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonst
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07122</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#37327;&#21270;&#21271;&#26497;&#25918;&#22823;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#26497;&#21464;&#26262;&#65292;&#20063;&#31216;&#21271;&#26497;&#25918;&#22823;&#65292;&#30001;&#22810;&#31181;&#22823;&#27668;&#21644;&#28023;&#27915;&#22240;&#32032;&#23548;&#33268;&#65292;&#20294;&#20854;&#22522;&#30784;&#28909;&#21147;&#22240;&#32032;&#30340;&#35814;&#32454;&#24773;&#20917;&#20173;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22266;&#23450;&#27835;&#30103;&#25928;&#24212;&#31574;&#30053;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#20250;&#23548;&#33268;&#19981;&#29616;&#23454;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#26102;&#38388;&#21464;&#21270;&#30340;&#28151;&#28102;&#30340;&#24433;&#21709;&#32780;&#24341;&#36215;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TCINet - &#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36830;&#32493;&#27835;&#30103;&#26041;&#24335;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#22914;&#20309;&#22823;&#22823;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#34928;&#31469;&#24341;&#20837;&#21040;&#22266;&#23450;&#30340;&#23618;&#27425;&#24863;&#30693;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23618;&#27425;&#24863;&#30693;&#30340;&#20498;&#25968;&#31532;&#20108;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#22349;&#32553;&#21040;&#26694;&#26550;&#19978;&#65292;&#26469;&#20943;&#23569;&#27169;&#22411;&#39044;&#27979;&#30340;&#38169;&#35823;&#20005;&#37325;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.05689</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#34928;&#31469;&#24341;&#20837;&#21040;&#22266;&#23450;&#30340;&#31561;&#35282;&#32039;&#26694;&#26550;&#20013;&#65292;&#20197;&#20943;&#23569;&#38169;&#35823;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity. (arXiv:2303.05689v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#34928;&#31469;&#24341;&#20837;&#21040;&#22266;&#23450;&#30340;&#23618;&#27425;&#24863;&#30693;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23618;&#27425;&#24863;&#30693;&#30340;&#20498;&#25968;&#31532;&#20108;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#22349;&#32553;&#21040;&#26694;&#26550;&#19978;&#65292;&#26469;&#20943;&#23569;&#27169;&#22411;&#39044;&#27979;&#30340;&#38169;&#35823;&#20005;&#37325;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#29616;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#31070;&#32463;&#34928;&#31469;&#65306;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#30340;&#26411;&#26399;&#38454;&#27573;&#65292;&#25152;&#26377;&#24179;&#22374;&#31867;&#30340;&#20498;&#25968;&#31532;&#20108;&#29305;&#24449;&#22343;&#20540;&#21644;&#30456;&#20851;&#30340;&#20998;&#31867;&#22120;&#21521;&#37327;&#37117;&#20250;&#22349;&#32553;&#21040;&#19968;&#20010;&#31616;&#21333;&#30340;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#30340;&#39030;&#28857;&#19978;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#23581;&#35797;&#21033;&#29992;&#36825;&#19968;&#29616;&#35937;&#65292;&#36890;&#36807;&#23558;&#30456;&#20851;&#30340;&#20998;&#31867;&#22120;&#26435;&#37325;&#22266;&#23450;&#20026;&#39044;&#20808;&#35745;&#31639;&#24471;&#21040;&#30340;ETF&#26469;&#24341;&#21457;&#31070;&#32463;&#34928;&#31469;&#65292;&#24182;&#22312;&#20351;&#29992;&#19981;&#24179;&#34913;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#26368;&#22823;&#21270;&#25152;&#23398;&#29305;&#24449;&#20043;&#38388;&#30340;&#20998;&#31163;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#22266;&#23450;&#20026;&#25353;&#23618;&#27425;&#32467;&#26500;&#35774;&#35745;&#30340;&#26694;&#26550;&#65288;HAFrame&#65289;&#65292;&#32780;&#19981;&#26159;ETF&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#36741;&#21161;&#25439;&#22833;&#26469;&#23398;&#20064;&#22349;&#32553;&#21040;HAFrame&#30340;&#23618;&#27425;&#24863;&#30693;&#30340;&#20498;&#25968;&#31532;&#20108;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#22312;&#22810;&#20010;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;top-1&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#38169;&#35823;&#20005;&#37325;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recently discovered and intriguing phenomenon called Neural Collapse: at the terminal phase of training a deep neural network for classification, the within-class penultimate feature means and the associated classifier vectors of all flat classes collapse to the vertices of a simplex Equiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon by fixing the related classifier weights to a pre-computed ETF to induce neural collapse and maximize the separation of the learned features when training with imbalanced data. In this work, we propose to fix the linear classifier of a deep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF, and use a cosine similarity-based auxiliary loss to learn hierarchy-aware penultimate features that collapse to the HAFrame. We demonstrate that our approach reduces the mistake severity of the model's predictions while maintaining its top-1 accuracy on several datasets of varying scales with hierarchies of he
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#32463;&#27982;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20110;&#25104;&#26412;&#30340;&#20462;&#21098;&#65292;&#25552;&#20986;&#20102;EcoOptiGen&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20351;&#29992;GPT-3.5/GPT-4&#27169;&#22411;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.04673</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference. (arXiv:2303.04673v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#32463;&#27982;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20110;&#25104;&#26412;&#30340;&#20462;&#21098;&#65292;&#25552;&#20986;&#20102;EcoOptiGen&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20351;&#29992;GPT-3.5/GPT-4&#27169;&#22411;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20854;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#21508;&#31181;&#21830;&#19994;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#25104;&#26412;&#39537;&#20351;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#32773;&#22312;&#26377;&#38480;&#30340;&#25512;&#29702;&#39044;&#31639;&#19979;&#26368;&#22823;&#21270;&#29983;&#25104;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#20248;&#21270;&#25512;&#29702;&#36229;&#21442;&#25968;&#65288;&#22914;&#22238;&#22797;&#25968;&#37327;&#12289;&#28201;&#24230;&#21644;&#26368;&#22823;token&#25968;&#65289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26174;&#33879;&#24433;&#21709;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#29992;/&#25104;&#26412;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;EcoOptiGen&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#32463;&#27982;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20110;&#25104;&#26412;&#30340;&#20462;&#21098;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#20351;&#29992;GPT-3.5/GPT-4&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;EcoOptiGen&#24050;&#22312;FLAML&#24211;&#30340;`autogen'&#21253;&#20013;&#23454;&#29616;&#65306;\url{https://aka.ms/autogen}&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have sparked significant interest in their generative capabilities, leading to the development of various commercial applications. The high cost of using the models drives application builders to maximize the value of generation under a limited inference budget. This paper presents a study of optimizing inference hyperparameters such as the number of responses, temperature and max tokens, which significantly affects the utility/cost of text generation. We design a framework named EcoOptiGen which leverages economical hyperparameter optimization and cost-based pruning. Experiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its effectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML library: \url{https://aka.ms/autogen}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KGQAn&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#26080;&#38656;&#38024;&#23545;&#27599;&#20010;&#30446;&#26631;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#23450;&#21046;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#36716;&#25442;&#20026;&#20013;&#38388;&#30340;&#25277;&#35937;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;SPARQL&#26597;&#35810;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.00595</link><description>&lt;p&gt;
&#19968;&#20010;&#36866;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#36890;&#29992;&#38382;&#31572;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
A Universal Question-Answering Platform for Knowledge Graphs. (arXiv:2303.00595v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KGQAn&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#26080;&#38656;&#38024;&#23545;&#27599;&#20010;&#30446;&#26631;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#23450;&#21046;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#36716;&#25442;&#20026;&#20013;&#38388;&#30340;&#25277;&#35937;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;SPARQL&#26597;&#35810;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#20197;RDF&#24341;&#25806;&#23384;&#20648;&#30340;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;SPARQL&#31471;&#28857;&#21487;&#20197;&#22312;Web&#19978;&#35775;&#38382;&#30693;&#35782;&#22270;&#35889;&#12290;&#20026;&#20102;&#27491;&#30830;&#34920;&#36798;&#19968;&#20010;&#31526;&#21512;&#35268;&#33539;&#30340;SPARQL&#26597;&#35810;&#65292;&#38656;&#35201;&#20102;&#35299;&#22270;&#32467;&#26500;&#21644;&#20854;&#32452;&#25104;&#37096;&#20998;&#30340;&#30830;&#20999;URI&#65292;&#36825;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#26469;&#35828;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#38382;&#31572;&#31995;&#32479;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;SPARQL&#26469;&#25552;&#20379;&#24110;&#21161;&#12290;&#29616;&#26377;&#30340;&#38382;&#31572;&#31995;&#32479;&#36890;&#24120;&#22522;&#20110;&#24212;&#29992;&#29305;&#23450;&#30340;&#20154;&#24037;&#31574;&#30053;&#65292;&#25110;&#32773;&#38656;&#35201;&#20808;&#39564;&#20449;&#24687;&#12289;&#26114;&#36149;&#30340;&#39044;&#22788;&#29702;&#21644;&#27169;&#22411;&#35843;&#25972;&#26469;&#36866;&#37197;&#27599;&#20010;&#30446;&#26631;&#30693;&#35782;&#22270;&#35889;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24456;&#38590;&#25512;&#24191;&#21040;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#30693;&#35782;&#22270;&#35889;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KGQAn&#65292;&#19968;&#20010;&#19981;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#30446;&#26631;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#23450;&#21046;&#30340;&#36890;&#29992;&#38382;&#31572;&#31995;&#32479;&#12290;KGQAn&#19981;&#20351;&#29992;&#20154;&#24037;&#31574;&#30053;&#65292;&#32780;&#26159;&#23558;&#38382;&#39064;&#29702;&#35299;&#20316;&#20026;&#19968;&#20010;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#26469;&#36827;&#34892;&#26032;&#39062;&#30340;&#24418;&#24335;&#21270;&#65292;&#36890;&#36807;&#31070;&#32463;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#23558;&#38382;&#39064;&#36716;&#25442;&#20026;&#20013;&#38388;&#30340;&#25277;&#35937;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge from diverse application domains is organized as knowledge graphs (KGs) that are stored in RDF engines accessible in the web via SPARQL endpoints. Expressing a well-formed SPARQL query requires information about the graph structure and the exact URIs of its components, which is impractical for the average user. Question answering (QA) systems assist by translating natural language questions to SPARQL. Existing QA systems are typically based on application-specific human-curated rules, or require prior information, expensive pre-processing and model adaptation for each targeted KG. Therefore, they are hard to generalize to a broad set of applications and KGs.  In this paper, we propose KGQAn, a universal QA system that does not need to be tailored to each target KG. Instead of curated rules, KGQAn introduces a novel formalization of question understanding as a text generation problem to convert a question into an intermediate abstract representation via a neural sequence-to-se
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#21253;&#25324;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#24635;&#32467;&#20102;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.00722</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32508;&#36848;&#65306;&#20174;&#28608;&#27963;&#20989;&#25968;&#21040;Transformer
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Learning: From Activations to Transformers. (arXiv:2302.00722v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00722
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#21253;&#25324;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#24635;&#32467;&#20102;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24471;&#30410;&#20110;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#28044;&#29616;&#12290;&#36825;&#20123;&#21253;&#25324;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#22810;&#31181;&#21464;&#20307;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21521;&#20855;&#26377;&#28145;&#24230;&#23398;&#20064;&#22522;&#26412;&#29702;&#35299;&#30340;&#20154;&#25552;&#20379;&#23545;&#36825;&#20123;&#39046;&#22495;&#20013;&#26368;&#26032;&#37325;&#35201;&#36129;&#29486;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#26399;&#26395;&#26159;&#36890;&#36807;&#23545;&#37325;&#35201;&#26368;&#26032;&#20316;&#21697;&#30340;&#32508;&#21512;&#21644;&#20840;&#38754;&#30340;&#25506;&#35752;&#65292;&#20419;&#36827;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20043;&#38388;&#24418;&#25104;&#26032;&#30340;&#32852;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#35768;&#22810;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#23545;&#26368;&#36817;&#19968;&#20123;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#20363;&#22914;OpenAI&#30340;GPT-4&#21644;Google&#30340;PaLM 2&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed remarkable advancements in deep learning, owing to the emergence of various architectures, layers, objectives, and optimization techniques. These consist of a multitude of variations of attention, normalization, skip connections, transformer, and self-supervised learning methods, among others. Our goal is to furnish a comprehensive survey of significant recent contributions in these domains to individuals with a fundamental grasp of deep learning. Our aspiration is that an integrated and comprehensive approach of influential recent works will facilitate the formation of new connections between different areas of deep learning. In our discussion, we discuss multiple patterns that summarize the key strategies for many of the successful innovations over the last decade. We also include a discussion on recent commercially built, closed-source models such as OpenAI's GPT-4 and Google's PaLM 2.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#32654;&#39135;&#30340;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#22238;&#25910;&#22522;&#20110;&#21516;&#19968;&#22522;&#30784;&#27169;&#22411;&#22312;&#22810;&#26679;&#36741;&#21161;&#20219;&#21153;&#19978;&#30340;&#22810;&#27425;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#31181;&#31574;&#30053;&#26088;&#22312;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#27169;&#22411;&#26435;&#37325;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10445</link><description>&lt;p&gt;
&#27169;&#22411;&#32654;&#39135;&#65306;&#22810;&#26679;&#27169;&#22411;&#30340;&#22238;&#25910;&#21033;&#29992;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization. (arXiv:2212.10445v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#32654;&#39135;&#30340;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#22238;&#25910;&#22522;&#20110;&#21516;&#19968;&#22522;&#30784;&#27169;&#22411;&#22312;&#22810;&#26679;&#36741;&#21161;&#20219;&#21153;&#19978;&#30340;&#22810;&#27425;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#31181;&#31574;&#30053;&#26088;&#22312;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#27169;&#22411;&#26435;&#37325;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#27491;&#22312;&#37325;&#26032;&#23450;&#20041;AI&#31995;&#32479;&#30340;&#26500;&#24314;&#26041;&#24335;&#12290;&#20174;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#24320;&#22987;&#65292;&#20174;&#19994;&#32773;&#29616;&#22312;&#37117;&#36981;&#24490;&#19968;&#20010;&#26631;&#20934;&#30340;&#27969;&#31243;&#26469;&#26500;&#24314;&#20182;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65306;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#24494;&#35843;&#26435;&#37325;&#12290;&#22240;&#27492;&#65292;&#20114;&#32852;&#32593;&#19978;&#20805;&#26021;&#30528;&#35768;&#22810;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;&#65306;&#36825;&#20123;&#21333;&#29420;&#30340;&#24494;&#35843;&#36807;&#31243;&#23384;&#22312;&#23396;&#31435;&#65292;&#27809;&#26377;&#30456;&#20114;&#21463;&#30410;&#12290;&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#36825;&#26159;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#26426;&#20250;&#65292;&#22240;&#20026;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#21253;&#21547;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#32654;&#39135;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#19981;&#21516;&#36741;&#21161;&#20219;&#21153;&#19978;&#22238;&#25910;&#30456;&#21516;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#20010;&#24494;&#35843;&#30340;&#26032;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36741;&#21161;&#26435;&#37325;&#37325;&#26032;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#22810;&#20010;&#24182;&#34892;&#24494;&#35843;&#30340;&#21021;&#22987;&#21270;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#25152;&#26377;&#24494;&#35843;&#21518;&#30340;&#26435;&#37325;&#21462;&#24179;&#22343;&#20540;&#65292;&#24471;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#36825;&#31181;&#22238;&#25910;&#31574;&#30053;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#26469;&#26368;&#22823;&#21270;&#26435;&#37325;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.06108</link><description>&lt;p&gt;
RaLiBEV: &#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#34701;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#28608;&#20809;&#38647;&#36798;&#21644;&#38647;&#36798;&#22312;&#24863;&#30693;&#21608;&#22260;&#29615;&#22659;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28608;&#20809;&#38647;&#36798;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#31354;&#38388;&#24863;&#30693;&#20449;&#24687;&#65292;&#20294;&#22312;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#19979;&#26080;&#27861;&#24037;&#20316;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38647;&#36798;&#20449;&#21495;&#30001;&#20110;&#27874;&#38271;&#30340;&#29305;&#24615;&#22312;&#36935;&#21040;&#38632;&#28404;&#25110;&#38654;&#31890;&#26102;&#20250;&#21457;&#29983;&#34893;&#23556;&#65292;&#20294;&#23427;&#21463;&#21040;&#22823;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#26368;&#36817;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#34701;&#21512;&#21487;&#20197;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#23454;&#29616;&#24378;&#20581;&#30340;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20174;&#27599;&#20010;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#23545;&#40784;&#21644;&#27719;&#32858;&#20004;&#20010;&#20998;&#25903;&#30340;&#29305;&#24449;&#20197;&#39044;&#27979;&#29289;&#20307;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#31614;&#20998;&#37197;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#31616;&#21333;&#35774;&#35745;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#36793;&#30028;&#26694;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#26469;&#33258;&#38647;&#36798;&#30340;&#36317;&#31163;-&#26041;&#20301;&#29305;&#24449;&#34701;&#21512;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment. LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23384;&#22312;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#21521;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25554;&#20837;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#36827;&#25552;&#31034;&#20013;&#65292;&#20174;&#32780;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2211.02408</link><description>&lt;p&gt;
&#25554;&#20837;&#21518;&#38376;&#20803;&#32032;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis. (arXiv:2211.02408v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23384;&#22312;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#21521;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25554;&#20837;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#36827;&#25552;&#31034;&#20013;&#65292;&#20174;&#32780;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#25216;&#26415;&#22312;&#30740;&#31350;&#32773;&#21644;&#20844;&#20247;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20381;&#36182;&#20110;&#22806;&#37096;&#26469;&#28304;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#29992;&#25143;&#30456;&#20449;&#26816;&#32034;&#21040;&#30340;&#27169;&#22411;&#20250;&#20687;&#25215;&#35834;&#30340;&#37027;&#26679;&#36816;&#34892;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#21521;&#38376;&#25915;&#20987;&#25991;&#26412;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21482;&#26159;&#36731;&#24494;&#22320;&#25913;&#21464;&#20102;&#32534;&#30721;&#22120;&#65292;&#20351;&#24471;&#23545;&#20110;&#24102;&#26377;&#24178;&#20928;&#25552;&#31034;&#30340;&#22270;&#20687;&#29983;&#25104;&#27809;&#26377;&#21487;&#30097;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#25554;&#20837;&#25552;&#31034;&#20013;&#65292;&#20363;&#22914;&#19968;&#20010;&#38750;&#25289;&#19969;&#23383;&#31526;&#25110;&#34920;&#24773;&#31526;&#21495;&#65292;&#25915;&#20987;&#32773;&#23601;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;Stable Diffusion&#21644;highligh&#19978;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#25915;&#20987;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highligh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#31070;&#32463;&#22330;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;3D&#23545;&#35937;&#29702;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#21333;&#20010;RGB&#22270;&#20687;&#26500;&#24314;&#32479;&#19968;&#32780;&#32039;&#20945;&#30340;&#22330;&#26223;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#65292;&#22914;&#26032;&#35270;&#35282;&#28210;&#26579;&#12289;3D&#37325;&#24314;&#12289;&#30896;&#25758;&#26816;&#26597;&#21644;&#31283;&#23450;&#25235;&#21462;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20174;&#26032;&#35270;&#35282;&#36827;&#34892;&#28210;&#26579;&#24182;&#39044;&#27979;&#25104;&#21151;&#30340;&#25235;&#21462;&#12290;</title><link>http://arxiv.org/abs/2210.12126</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#31070;&#32463;&#22330;&#29992;&#20110;3D&#23545;&#35937;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
One-Shot Neural Fields for 3D Object Understanding. (arXiv:2210.12126v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#31070;&#32463;&#22330;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;3D&#23545;&#35937;&#29702;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#21333;&#20010;RGB&#22270;&#20687;&#26500;&#24314;&#32479;&#19968;&#32780;&#32039;&#20945;&#30340;&#22330;&#26223;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#65292;&#22914;&#26032;&#35270;&#35282;&#28210;&#26579;&#12289;3D&#37325;&#24314;&#12289;&#30896;&#25758;&#26816;&#26597;&#21644;&#31283;&#23450;&#25235;&#21462;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20174;&#26032;&#35270;&#35282;&#36827;&#34892;&#28210;&#26579;&#24182;&#39044;&#27979;&#25104;&#21151;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#30340;&#32479;&#19968;&#19988;&#32039;&#20945;&#30340;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#20854;&#20013;&#22330;&#26223;&#20013;&#30340;&#27599;&#20010;&#23545;&#35937;&#30001;&#25429;&#25417;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#28508;&#22312;&#20195;&#30721;&#26469;&#25551;&#36848;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#21487;&#20197;&#35299;&#30721;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#20363;&#22914;&#26032;&#35270;&#35282;&#28210;&#26579;&#65292;3D&#37325;&#24314;&#65288;&#20363;&#22914;&#24674;&#22797;&#28145;&#24230;&#65292;&#28857;&#20113;&#25110;&#20307;&#32032;&#22270;&#65289;&#65292;&#30896;&#25758;&#26816;&#26597;&#21644;&#31283;&#23450;&#25235;&#21462;&#39044;&#27979;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#26032;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#22312;&#27979;&#35797;&#26102;&#20174;&#21333;&#20010;RGB&#36755;&#20837;&#22270;&#20687;&#26500;&#24314;&#25105;&#20204;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#22810;&#35270;&#22270;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#31867;&#21035;&#32423;&#20808;&#39564;&#30693;&#35782;&#65292;&#28982;&#21518;&#22312;&#23569;&#25968;&#25110;&#20165;&#19968;&#20010;&#35270;&#22270;&#30340;&#26032;&#23545;&#35937;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;NeRF&#27169;&#22411;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25235;&#21462;&#36755;&#20986;&#65292;&#24182;&#25506;&#32034;&#20102;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#30340;&#26041;&#27861;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#20174;&#20165;&#19968;&#20010;&#35270;&#28857;&#35266;&#23519;&#21040;&#30340;&#21333;&#20010;RGB&#36755;&#20837;&#22270;&#20687;&#26500;&#24314;&#34920;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#24674;&#22797;&#30340;&#34920;&#31034;&#26041;&#27861;&#20801;&#35768;&#20174;&#26032;&#35270;&#35282;&#36827;&#34892;&#28210;&#26579;&#65292;&#21253;&#25324;&#36974;&#25377;&#30340;&#29289;&#20307;&#37096;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#39044;&#27979;&#25104;&#21151;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified and compact scene representation for robotics, where each object in the scene is depicted by a latent code capturing geometry and appearance. This representation can be decoded for various tasks such as novel view rendering, 3D reconstruction (e.g. recovering depth, point clouds, or voxel maps), collision checking, and stable grasp prediction. We build our representation from a single RGB input image at test time by leveraging recent advances in Neural Radiance Fields (NeRF) that learn category-level priors on large multiview datasets, then fine-tune on novel objects from one or few views. We expand the NeRF model for additional grasp outputs and explore ways to leverage this representation for robotics. At test-time, we build the representation from a single RGB input image observing the scene from only one viewpoint. We find that the recovered representation allows rendering from novel views, including of occluded object parts, and also for predicting successful 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#35821;&#35328;&#31867;&#22411;&#21644;&#31639;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#26816;&#27979;&#21644;&#21453;&#24212;&#21512;&#25104;&#31561;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.02307</link><description>&lt;p&gt;
&#23433;&#20840;&#21644;&#21327;&#23433;&#20840;&#35821;&#35328;&#30340;&#19968;&#38454;&#36923;&#36753;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
A first-order logic characterization of safety and co-safety languages. (arXiv:2209.02307v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#35821;&#35328;&#31867;&#22411;&#21644;&#31639;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#26816;&#27979;&#21644;&#21453;&#24212;&#21512;&#25104;&#31561;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#26102;&#38388;&#36923;&#36753;&#20043;&#19968;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;LTL&#30340;&#24378;&#22823;&#22522;&#30784;&#24615;&#36136;&#20043;&#19968;&#26159;&#20854;&#31561;&#20215;&#20110;&#26080;&#35745;&#25968;&#969;-&#33258;&#21160;&#26426;&#12289;&#26143;&#33258;&#30001;&#969;-&#27491;&#21017;&#34920;&#36798;&#24335;&#20197;&#21450;&#65288;&#36890;&#36807;Kamp&#23450;&#29702;&#65289;&#32447;&#24615;&#24207;&#30340;&#19968;&#38454;&#29702;&#35770;&#65288;FO-TLO&#65289;&#12290;&#23433;&#20840;&#35821;&#35328;&#21644;&#21327;&#23433;&#20840;&#35821;&#35328;&#20998;&#21035;&#25351;&#21482;&#38656;&#26377;&#38480;&#21069;&#32512;&#20415;&#21487;&#30830;&#23450;&#35813;&#21333;&#35789;&#23646;&#20110;&#25110;&#19981;&#23646;&#20110;&#35813;&#35821;&#35328;&#30340;&#35821;&#35328;&#31867;&#22411;&#12290;SafetyLTL&#65288;&#21644;coSafetyLTL&#65289;&#26159;LTL&#30340;&#19968;&#20010;&#29255;&#27573;&#65292;&#20854;&#20013;&#20165;&#20801;&#35768;&#20351;&#29992;&#20840;&#23616;&#65288;&#23384;&#22312;&#65289;&#26102;&#38388;&#20462;&#39280;&#35789;&#26469;&#35782;&#21035;&#23433;&#20840;&#65288;&#21327;&#23433;&#20840;&#65289;&#35821;&#35328;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;FO-TLO&#30340;&#29255;&#27573;SafetyFO&#20197;&#21450;&#20854;&#23545;&#20598;&#30340;coSafetyFO&#65292;&#23427;&#20204;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#37117;&#26159;&#23436;&#22791;&#30340;&#65288;except&#19968;&#20123;&#36793;&#30028;&#24773;&#20917;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Temporal Logic (LTL) is one of the most popular temporal logics, that comes into play in a variety of branches of computer science. Among the various reasons of its widespread use there are its strong foundational properties: LTL is equivalent to counter-free omega-automata, to star-free omega-regular expressions, and (by Kamp's theorem) to the First-Order Theory of Linear Orders (FO-TLO). Safety and co-safety languages, where a finite prefix suffices to establish whether a word does not belong or belongs to the language, respectively, play a crucial role in lowering the complexity of problems like model checking and reactive synthesis for LTL. SafetyLTL (resp., coSafetyLTL) is a fragment of LTL where only universal (resp., existential) temporal modalities are allowed, that recognises safety (resp., co-safety) languages only. The main contribution of this paper is the introduction of a fragment of FO-TLO, called SafetyFO, and of its dual coSafetyFO, which are expressively comple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.01937</link><description>&lt;p&gt;
BoMD&#65306;&#36866;&#29992;&#20110;&#22024;&#26434;X&#20809;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#21253;
&lt;/p&gt;
&lt;p&gt;
BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#38382;&#39064;&#30340;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#20855;&#26377;&#28165;&#27905;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#36825;&#31181;&#25163;&#21160;&#27880;&#37322;&#30340;&#39640;&#25104;&#26412;&#65292;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#21487;&#33021;&#38656;&#35201;&#20381;&#36182;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;&#20107;&#23454;&#19978;&#65292;&#35768;&#22810;&#33016;&#37096;X&#20809;&#20998;&#31867;&#22120;&#24050;&#32463;&#20174;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#19981;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#23548;&#33268;&#27425;&#20248;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;CXR&#25968;&#25454;&#38598;&#22823;&#22810;&#26159;&#22810;&#26631;&#35760;&#30340;&#65292;&#22240;&#27492;&#24403;&#21069;&#35774;&#35745;&#29992;&#20110;&#22810;&#31867;&#38382;&#39064;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#19981;&#33021;&#36731;&#26494;&#22320;&#36827;&#34892;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22024;&#26434;&#22810;&#26631;&#31614;CXR&#23398;&#20064;&#65292;&#20854;&#20013;&#26816;&#27979;&#24182;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#24120;&#35265;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#20351;&#29992;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a ba
&lt;/p&gt;</description></item></channel></rss>