<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06182</link><description>&lt;p&gt;
&#36808;&#21521;MoE&#37096;&#32626;&#65306;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#25512;&#29702;&#20013;&#30340;&#20302;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference. (arXiv:2303.06182v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes three optimization techniques to mitigate inefficiencies in Mixture-of-Experts (MoE) models during inference, including dynamic gating, expert buffering, and expert load balancing. These techniques can significantly improve execution time and reduce memory usage.
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24191;&#27867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#23567;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#37096;&#32626;&#36825;&#26679;&#30340;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;MoE&#24037;&#20316;&#36127;&#36733;&#30340;&#29305;&#24449;&#21270;&#65292;&#21363;&#35821;&#35328;&#24314;&#27169;&#65288;LM&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#30340;&#20302;&#25928;&#29575;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#20302;&#25928;&#29575;&#30340;&#26469;&#28304;&#65292;&#21363;&#65288;1&#65289;&#21160;&#24577;&#38376;&#25511;&#65292;&#65288;2&#65289;&#19987;&#23478;&#32531;&#20914;&#21644;&#65288;3&#65289;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21160;&#24577;&#38376;&#25511;&#21487;&#20197;&#20351;LM&#30340;&#25191;&#34892;&#26102;&#38388;&#25552;&#39640;1.25-4&#20493;&#65292;MT&#32534;&#30721;&#22120;&#25552;&#39640;2-5&#20493;&#65292;MT&#35299;&#30721;&#22120;&#25552;&#39640;1.09-1.5&#20493;&#12290;&#23427;&#36824;&#21487;&#20197;&#23558;LM&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.36&#20493;&#65292;MT&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Experts (MoE) models have recently gained steam in achieving the state-of-the-art performance in a wide range of tasks in computer vision and natural language processing. They effectively expand the model capacity while incurring a minimal increase in computation cost during training. However, deploying such models for inference is difficult due to their large model size and complex communication pattern. In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment.  We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show that dynamic gating improves execution time by 1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to 1.1$\times$ for MT. We f
&lt;/p&gt;</description></item><item><title>MOELA&#26159;&#19968;&#20010;&#22810;&#30446;&#26631;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#36827;&#21270;&#25628;&#32034;&#21644;&#23398;&#20064;&#25628;&#32034;&#65292;&#29992;&#20110;&#20248;&#21270;3D NoC&#21551;&#29992;&#30340;&#24322;&#26500;&#22810;&#26680;&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#30446;&#26631;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;MOELA&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#30340;&#26597;&#25214;&#36895;&#24230;&#65292;&#25552;&#39640;Pareto Hypervolume&#65288;PHV&#65289;&#21644;&#33021;&#37327;&#24310;&#36831;&#20056;&#31215;&#65288;EDP&#65289;&#12290;</title><link>http://arxiv.org/abs/2303.06169</link><description>&lt;p&gt;
MOELA&#65306;&#29992;&#20110;3D&#24322;&#26500;&#22810;&#26680;&#24179;&#21488;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;/&#23398;&#20064;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MOELA: A Multi-Objective Evolutionary/Learning Design Space Exploration Framework for 3D Heterogeneous Manycore Platforms. (arXiv:2303.06169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06169
&lt;/p&gt;
&lt;p&gt;
MOELA&#26159;&#19968;&#20010;&#22810;&#30446;&#26631;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#36827;&#21270;&#25628;&#32034;&#21644;&#23398;&#20064;&#25628;&#32034;&#65292;&#29992;&#20110;&#20248;&#21270;3D NoC&#21551;&#29992;&#30340;&#24322;&#26500;&#22810;&#26680;&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#30446;&#26631;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;MOELA&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#30340;&#26597;&#25214;&#36895;&#24230;&#65292;&#25552;&#39640;Pareto Hypervolume&#65288;PHV&#65289;&#21644;&#33021;&#37327;&#24310;&#36831;&#20056;&#31215;&#65288;EDP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
MOELA is a multi-objective design space exploration framework that combines evolutionary-based search with learning-based local search to optimize multiple objectives in 3D NoC enabled heterogeneous manycore systems. Compared to state-of-the-art approaches, MOELA increases the speed of finding solutions, improves Pareto Hypervolume (PHV) and energy-delay-product (EDP).
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#22788;&#29702;&#31561;&#26032;&#20852;&#24212;&#29992;&#65292;&#38656;&#35201;&#33021;&#22815;&#38598;&#25104;&#22810;&#20010;&#22788;&#29702;&#20803;&#20214;&#65288;PE&#65289;&#30340;3D&#32593;&#32476;&#33455;&#29255;&#65288;NoC&#65289;&#21551;&#29992;&#30340;&#24322;&#26500;&#22810;&#26680;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#21644;&#38271;&#26102;&#38388;&#30340;&#35780;&#20272;&#26102;&#38388;&#65292;&#35774;&#35745;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#30340;&#36825;&#31181;&#22797;&#26434;&#31995;&#32479;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#26679;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26694;&#26550;MOELA&#65292;&#23427;&#23558;&#22522;&#20110;&#36827;&#21270;&#30340;&#25628;&#32034;&#30340;&#20248;&#28857;&#19982;&#22522;&#20110;&#23398;&#20064;&#30340;&#23616;&#37096;&#25628;&#32034;&#30456;&#32467;&#21512;&#65292;&#20197;&#24555;&#36895;&#30830;&#23450;PE&#21644;&#36890;&#20449;&#38142;&#36335;&#30340;&#25918;&#32622;&#20301;&#32622;&#65292;&#20197;&#20248;&#21270;3D NoC&#21551;&#29992;&#30340;&#24322;&#26500;&#22810;&#26680;&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#30446;&#26631;&#65288;&#20363;&#22914;&#24310;&#36831;&#65292;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#65289;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;MOELA&#21487;&#20197;&#23558;&#35299;&#20915;&#26041;&#26696;&#30340;&#26597;&#25214;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;128&#20493;&#65292;&#22312;5&#20010;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23558;Pareto Hypervolume&#65288;PHV&#65289;&#25552;&#39640;&#22810;&#36798;12.14&#20493;&#65292;&#24182;&#23558;&#33021;&#37327;&#24310;&#36831;&#20056;&#31215;&#65288;EDP&#65289;&#25552;&#39640;&#22810;&#36798;7.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enable emerging applications such as deep machine learning and graph processing, 3D network-on-chip (NoC) enabled heterogeneous manycore platforms that can integrate many processing elements (PEs) are needed. However, designing such complex systems with multiple objectives can be challenging due to the huge associated design space and long evaluation times. To optimize such systems, we propose a new multi-objective design space exploration framework called MOELA that combines the benefits of evolutionary-based search with a learning-based local search to quickly determine PE and communication link placement to optimize multiple objectives (e.g., latency, throughput, and energy) in 3D NoC enabled heterogeneous manycore systems. Compared to state-of-the-art approaches, MOELA increases the speed of finding solutions by up to 128x, leads to a better Pareto Hypervolume (PHV) by up to 12.14x and improves energy-delay-product (EDP) by up to 7.7% in a 5-objective scenario.
&lt;/p&gt;</description></item></channel></rss>