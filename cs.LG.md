# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Natural Counterfactuals With Necessary Backtracking](https://rss.arxiv.org/abs/2402.01607) | 本研究提出了一种自然反事实框架和方法，通过优化控制回溯的范围，生成与实际世界的数据分布相匹配的自然反事实，从而改进了反事实推理。 |
| [^2] | [CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples](https://arxiv.org/abs/2402.13254) | 本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。 |
| [^3] | [FlashTex: Fast Relightable Mesh Texturing with LightControlNet](https://arxiv.org/abs/2402.13251) | 提出了FlashTex方法，基于LightControlNet实现了快速自动化3D网格纹理生成，实现了照明与表面材质的解耦，使得网格能够在任何照明环境下正确重照和渲染 |
| [^4] | [Federated Causal Discovery from Heterogeneous Data](https://arxiv.org/abs/2402.13241) | 该研究提出了一种新型联邦因果发现方法，旨在适应任意因果模型和异构数据，通过使用替代变量和联邦条件独立性检验来解决数据异质性，并建立了联邦独立变化原则用于确定因果方向。 |
| [^5] | [SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification](https://arxiv.org/abs/2402.13233) | 提出了SMORE，一种新颖的多传感器时间序列分类领域自适应算法，利用高维计算的高效和并行操作，动态定制测试模型以减轻数据分布偏移带来的性能下降。 |
| [^6] | [Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive](https://arxiv.org/abs/2402.13228) | 在这项工作中，我们提出了一种新的损失函数和训练过程DPO-Positive（DPOP），以避免直接偏好优化（DPO）中潜在的失败模式，并发现DPOP明显优于DPO。 |
| [^7] | [Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming](https://arxiv.org/abs/2402.13224) | 本文介绍了一个新的电动汽车充电站模型，通过用户行为建模和随机规划，解决了充电会话不确定性问题，并提出了两种方法来优化成本并提高用户满意度。 |
| [^8] | [CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning](https://arxiv.org/abs/2402.13221) | 图机器学习领域目前主要集中在预测分子和材料的目标特性，而尚未达到生成能力与其他领域的水平。 |
| [^9] | [Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies](https://arxiv.org/abs/2402.13219) | 本研究提出了一种专门强化学习框架，集成了AI决策支持系统，旨在改善控制室操作员的工作效率和情境意识，并为其提供根据系统和人类表现状态量身定制的干预策略。 |
| [^10] | [Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A](https://arxiv.org/abs/2402.13213) | 多项选择问答任务中，基于最大softmax概率（MSPs）的模型预测方法有助于提高大型语言模型（LLMs）的正确性，我们提出了一种根据MSP有选择地弃权的策略以提高性能。 |
| [^11] | [Soft Self-Consistency Improves Language Model Agents](https://arxiv.org/abs/2402.13212) | Soft Self-Consistency (Soft-SC)通过软化评分标准替代自一致性的多数投票方法，提高了在涉及生成多个动作的长期互动任务中的性能和效率 |
| [^12] | [Bayesian Reward Models for LLM Alignment](https://arxiv.org/abs/2402.13210) | 通过训练贝叶斯奖励模型，可以成功缓解奖励的过度优化问题。 |
| [^13] | [SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search](https://arxiv.org/abs/2402.13204) | SONATA提出了一种自适应进化框架，用于硬件感知神经架构搜索，旨在解决神经网络设计参数与硬件感知NAS优化目标之间关系的挑战，并利用评估数据改进优化策略。 |
| [^14] | [Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers](https://arxiv.org/abs/2402.13201) | 将模仿学习问题视为条件序列建模任务，通过训练决策变换器并使用奖励机制，将生成模型压缩以适应资源受限的机器人平台。 |
| [^15] | [Practical Kernel Tests of Conditional Independence](https://arxiv.org/abs/2402.13196) | 提出了一种数据高效、基于核的方法，用于测试条件独立性，并提出了三种偏差控制方法来纠正测试水平。 |
| [^16] | [Testing Calibration in Subquadratic Time](https://arxiv.org/abs/2402.13187) | 该论文通过属性测试算法方面的研究，提出了一种基于近似线性规划的算法，可以在信息理论上最优地解决校准性测试问题（最多一个常数倍数）。 |
| [^17] | [Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness](https://arxiv.org/abs/2402.13182) | 提出的算法在分布式核赌博机问题中实现了最优次序遗憾，并且通信成本对于代理数量和时间都是亚线性的。 |
| [^18] | [DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models](https://arxiv.org/abs/2402.13181) | DINOBot利用Vision Transformers的特征，实现了通过检索和对齐来进行机器人操作，提高了学习效率和泛化能力 |
| [^19] | [Defending Jailbreak Prompts via In-Context Adversarial Game](https://arxiv.org/abs/2402.13148) | 介绍了一种通过上下文对抗游戏(ICAG)防御越狱提示的方法，能够显著降低成功率。 |
| [^20] | [SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations](https://arxiv.org/abs/2402.13147) | 逆向软 Q 学习用于获得次优演示的离线模仿挑战了离线 IL 中有限支持专家演示的问题，并提出了一种解决方案以匹配次优演示集合的占用分布 |
| [^21] | [Neural Network Diffusion](https://arxiv.org/abs/2402.13144) | 扩散模型能够生成表现优异的神经网络参数，生成的模型在性能上与训练网络相媲美甚至更好，且成本极低。 |
| [^22] | [VGMShield: Mitigating Misuse of Video Generative Models](https://arxiv.org/abs/2402.13126) | VGMShield提出了三项简单但开创性的措施，通过检测虚假视频、溯源问题和利用预训练的空间-时间动态模型，防范视频生成模型的误用。 |
| [^23] | [BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes](https://arxiv.org/abs/2402.13114) | BuffGraph通过插入缓冲节点到图中，调节主要类别的影响，以改善次要类别的表示，在类别不平衡的节点分类问题上表现优越。 |
| [^24] | [On the Stability of Gradient Descent for Large Learning Rate](https://arxiv.org/abs/2402.13108) | 本文研究了线性神经网络在二次损失函数下的优化问题，证明了梯度下降映射的非奇异性以及全局最小值点集的光滑流形特性，为理解大学习率下梯度下降的稳定性提供了重要线索。 |
| [^25] | [On Generalization Bounds for Deep Compound Gaussian Neural Networks](https://arxiv.org/abs/2402.13106) | 本文针对受复合高斯先验启发的展开DNN，提出了新颖的泛化误差界限，这些网络在压缩感知和层析成像问题中表现出色。 |
| [^26] | [Multivariate Functional Linear Discriminant Analysis for the Classification of Short Time Series with Missing Data](https://arxiv.org/abs/2402.13103) | 开发了多变量版本的FLDA（MUDRA）以应对缺失数据问题，并提出了一种高效的期望/条件极大化（ECM）算法来推断其参数，在分类性能上较现有技术取得了改进 |
| [^27] | [A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations](https://arxiv.org/abs/2402.13101) | 该研究提出了一种基于微结构的图神经网络代理模型，旨在加速多尺度模拟，将微观和宏观量预测和均质化，实现了在弹塑性材料中保持多尺度性质的目标。 |
| [^28] | [Towards an empirical understanding of MoE design choices](https://arxiv.org/abs/2402.13089) | 本研究系统评估了Mixture of Experts（MoEs）中常见设计选择对验证性能的影响，揭示了路由器的学习与初始化对模型性能的比较、序列级路由与标记级路由在专家专业化方面的不同影响。 |
| [^29] | [How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning](https://arxiv.org/abs/2402.13087) | 本论文探讨了超参数调整中的隐私性问题，发现当前的隐私分析在一般情况下是紧密的，但在特定的超参数调整问题上则不再成立，并通过隐私审计揭示了当前理论隐私界与实证之间的显著差距。 |
| [^30] | [IT Intrusion Detection Using Statistical Learning and Testbed Measurements](https://arxiv.org/abs/2402.13081) | 该研究通过统计学习方法和基础设施连续测量数据，以及在内部测试台上进行攻击模拟，实现了IT基础设施中的自动入侵检测。 |
| [^31] | [Mode Estimation with Partial Feedback](https://arxiv.org/abs/2402.13079) | 本文提出了一种在模态估计中利用部分反馈的方法，通过熵编码实现最优信息获取，开发了粗糙的充分统计用于模态识别，并将赌博算法调整为新设置，最终提出了一个高效的问题解决方案 |
| [^32] | [Mechanistic Neural Networks for Scientific Machine Learning](https://arxiv.org/abs/2402.13077) | 本文提出了一种名为机制神经网络的神经网络设计，通过在标准架构中引入新的机制模块，学习控制微分方程作为表示，提高数据建模的可解释性和效率，并借助一种新颖的松弛线性规划求解器实现可扩展的GPU并行处理。 |
| [^33] | [Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition](https://arxiv.org/abs/2402.13076) | 权重参数在设备上的流式语音识别模型中的功耗影响有所不同，作者提出了基于权重参数敏感性的有针对性压缩方法，将能源使用减少高达47%而维持模型准确性 |
| [^34] | [Text-Guided Molecule Generation with Diffusion Language Model](https://arxiv.org/abs/2402.13040) | 提出了一种名为Text-Guided Molecule Generation with Diffusion Language Model（TGM-DLM）的新方法，通过扩散模型进行文本引导的分子生成，在生成有效分子表示方面表现出显著的效果优于自回归模型MolT5-Base。 |
| [^35] | [Align Your Intents: Offline Imitation Learning via Optimal Transport](https://arxiv.org/abs/2402.13037) | 通过最优传输的离线模仿学习方法AILOT，可以在缺乏明确奖励的情况下，仅通过观察专家学习所需的行为。 |
| [^36] | [Enhancing Real-World Complex Network Representations with Hyperedge Augmentation](https://arxiv.org/abs/2402.13033) | 提出了一种新颖的图增强方法Hyperedge Augmentation (HyperAug)，通过构建直接从原始数据形成的虚拟超边，以解决现实世界复杂网络表示中高阶节点关系的问题 |
| [^37] | [Improving Neural-based Classification with Logical Background Knowledge](https://arxiv.org/abs/2402.13019) | 本文提出了一种新的神经符号技术——在推理过程中的语义调节，该技术仅在推理过程中约束系统，而不影响训练，相对于其他两种常见神经符号技术具有理论和实际优势，在多尺度方法上的评估结果表明其对网络规模的好处。 |
| [^38] | [Improve Cross-Architecture Generalization on Dataset Distillation](https://arxiv.org/abs/2402.13007) | 提出一种新的“模型池”方法，通过在数据蒸馏过程中选择多样的模型，结合知识蒸馏方法，并将其应用于蒸馏数据集的测试过程，从而改进数据集蒸馏的跨架构泛化。 |
| [^39] | [Investigating the Impact of Model Instability on Explanations and Uncertainty](https://arxiv.org/abs/2402.13006) | 模型稳定性对解释和不确定性的影响进行了调查，并发现实际扰动对性能和解释影响较小，但掩盖却有 drastical 影响。 |
| [^40] | [SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms](https://arxiv.org/abs/2402.13005) | 提出了SzCORE框架，用于验证基于脑电图的自动癫痫检测算法，旨在标准化验证方法，包括数据集、文件格式、输入内容、性能度量等。 |
| [^41] | [A unifying primary framework for quantum graph neural networks from quantum graph states](https://arxiv.org/abs/2402.13001) | 量子图神经网络模型可基于图态理解和实现，可用作参数化的量子电路来表示神经网络，或作为构建量子计算机上的图神经网络的基础结构。 |
| [^42] | [An Autonomous Large Language Model Agent for Chemical Literature Data Mining](https://arxiv.org/abs/2402.12993) | 介绍了一个端到端的人工智能代理框架，利用大型语言模型实现从化学文献中高保真提取信息，充当化学助手的角色，自动化数据收集和分析，从而提高工作效率。 |
| [^43] | [TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification](https://arxiv.org/abs/2402.12991) | TRAP提出了一种名为Targeted Random Adversarial Prompt (TRAP)的方法，用于识别特定LLM的使用，并在单次交互后以超过95%的真阳性率和低于0.2%的误报率检测目标LLMs。 |
| [^44] | [Towards Robust Graph Incremental Learning on Evolving Graphs](https://arxiv.org/abs/2402.12987) | 本文关注面向图结构演化的节点式图增量学习问题，并提出了一种名为结构转移的基于正则化的技术 |
| [^45] | [How Temporal Unrolling Supports Neural Physics Simulators](https://arxiv.org/abs/2402.12971) | 在不可微分但展开的培训设置支持下，通过数值求解器支持的神经物理模拟器能够获得比完全可微化预测设置高出4.5倍的改进。 |
| [^46] | [Conditional Logical Message Passing Transformer for Complex Query Answering](https://arxiv.org/abs/2402.12954) | 提出了一种考虑查询图中常量和变量之间差异，能动态测量消息重要性并捕捉隐式逻辑依赖关系的条件逻辑消息传递变压器。 |
| [^47] | [Stochastic Approximation Approach to Federated Machine Learning](https://arxiv.org/abs/2402.12945) | 本文提出了一种基于随机逼近的联邦机器学习方法，通过使用近似样本梯度和缩小步长来定位成本函数的极小值，实现了在联邦学习中对神经网络模型进行协作训练的效果，并在数值模拟中与标准算法进行了比较。 |
| [^48] | [Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space](https://arxiv.org/abs/2402.12939) | 通过利用轨迹聚类和降维技术在神经网络的潜空间中研究深度强化学习策略的行为模式，可以发现并改进其多样的行为模式和次优选择。 |
| [^49] | [GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks](https://arxiv.org/abs/2402.12937) | GRAPHGINI在图神经网络中首次引入了基尼系数作为公平性度量的方法，同时实现个体和群体公平性约束，并保持高预测准确性。 |
| [^50] | [Learning Exceptional Subgroups by End-to-End Maximizing KL-divergence](https://arxiv.org/abs/2402.12930) | 提出一种Syflow方法，通过端到端最大化KL散度，利用正规化流来模拟任意目标分布，引入新颖的神经层，成功找到具有见地描述的高度异常子群。 |
| [^51] | [Right on Time: Revising Time Series Models by Constraining their Explanations](https://arxiv.org/abs/2402.12921) | 引入了准时到位（RioT）方法，通过使模型解释在时间和频率域之间交互，并利用反馈来约束模型，有效地解决了时间序列数据中的混杂因素问题。 |
| [^52] | [Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of Machine Learning Models](https://arxiv.org/abs/2402.12916) | 本文讨论了如何通过集成 AutoML 技术优化数据管道，提升数据流智能性以在机器学习任务中取得更好结果，并揭示了构建高效适应不断变化数据环境的关键策略。 |
| [^53] | [More Discriminative Sentence Embeddings via Semantic Graph Smoothing](https://arxiv.org/abs/2402.12890) | 通过语义图平滑技术增强预训练模型获取的句子嵌入，有效改善文本聚类和分类任务的结果。 |
| [^54] | [A Bound on the Maximal Marginal Degrees of Freedom](https://arxiv.org/abs/2402.12885) | 该论文提出了对于核岭回归的低秩近似和替代方法中，关于低维近似秩的一个下界，从而保证可靠的预测能力，并将有效维度与最大统计杠杆得分联系起来。 |
| [^55] | [Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study](https://arxiv.org/abs/2402.12876) | 这项研究介绍了一种新颖的FMTL-Bench框架，用于系统评估联邦多任务学习（FMTL）范式，填补了FL和MTL综合评估方法的空白。 |
| [^56] | [Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://arxiv.org/abs/2402.12875) | 思维链赋予变压器模型执行固有串行计算的能力，提高了变压器在算术和符号推理任务中的准确性。 |
| [^57] | [Skill or Luck? Return Decomposition via Advantage Functions](https://arxiv.org/abs/2402.12874) | 优势函数的创新在于将回报分解为代理动作引起的部分（技能）和代理无法控制的部分（运气），进而扩展了直接优势估计到离线环境，使得从离线轨迹学习更加高效。 |
| [^58] | [Fast Rates in Online Convex Optimization by Exploiting the Curvature of Feasible Sets](https://arxiv.org/abs/2402.12868) | 该论文提出了一种新的分析方法，通过利用可行集的曲率，在在线凸优化中实现了快速收敛速度。 |
| [^59] | [Towards MLOps: A DevOps Tools Recommender System for Machine Learning System](https://arxiv.org/abs/2402.12867) | 本研究提出了一个用于机器学习系统的DevOps工具推荐系统，旨在建立一个可以自动构建数据集、训练模型、部署模型以及存储模型和数据集的流水线，以提高模型交付的速度和结果准确性。 |
| [^60] | [Backward Lens: Projecting Language Model Gradients into the Vocabulary Space](https://arxiv.org/abs/2402.12865) | 将语言模型梯度投影到词汇空间中，挖掘信息在LMs内部的流动方式，探索新信息如何存储在LMs的神经元中。 |
| [^61] | [Bounding Reconstruction Attack Success of Adversaries Without Data Priors](https://arxiv.org/abs/2402.12861) | 本研究提供了差分隐私训练的机器学习模型在现实对抗设置下重建成功率的正式上限，并通过实证结果支持，有助于更明智地选择隐私参数。 |
| [^62] | [Differentiable Mapper For Topological Optimization Of Data Representation](https://arxiv.org/abs/2402.12854) | 可微分映射器提出了一种新方法，用于优化数据表示的拓扑结构，解决了手动调节Mapper图中许多参数的问题，特别是调节关键的滤波器参数。 |
| [^63] | [CCFC++: Enhancing Federated Clustering through Feature Decorrelation](https://arxiv.org/abs/2402.12852) | 引入了去相关正则化器的CCFC++方法有效地解决了数据异质性对联邦聚类性能的不利影响，取得了优越表现。 |
| [^64] | [Instruction-tuned Language Models are Better Knowledge Learners](https://arxiv.org/abs/2402.12847) | 通过在持续预训练文档之前暴露LLM到问题-答案对，以便从复杂文档中编码知识，可以更好地适应知识访问方式。 |
| [^65] | [PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning](https://arxiv.org/abs/2402.12842) | 提出了PromptKD方法，通过提示调整实现了生成语言模型提取学生友好知识的蒸馏，无需微调整整个教师模型。 |
| [^66] | [SGD with Clipping is Secretly Estimating the Median Gradient](https://arxiv.org/abs/2402.12828) | 本研究提出了一种基于中值估计的稳健梯度估计方法，针对包括重尾噪声在内的多种应用场景进行了探讨，揭示了不同形式的剪切方法实际上是该方法的特例。 |
| [^67] | [Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model](https://arxiv.org/abs/2402.12821) | 该研究提出了针对摘要中事实不一致性的解决方案：通过大型语言模型在正确的范式设计下无需训练即可解决任务，并提出了训练策略以精炼更小型的高准确性的语言模型。 |
| [^68] | [Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?](https://arxiv.org/abs/2402.12819) | 专门模型通常只需少量标记样本（100-1000个）就能与通用模型持平甚至更好，取决于任务的复杂性和结果的变化。 |
| [^69] | [On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices](https://arxiv.org/abs/2402.12817) | 有限标注数据学习对随机性的敏感性，通过系统研究随机因素的影响，揭示了忽略相互作用可能导致的不一致结果。 |
| [^70] | [Scalable Decentralized Algorithms for Online Personalized Mean Estimation](https://arxiv.org/abs/2402.12812) | 本研究提出了一种可扩展的分散算法框架，使代理能够自组织成图，并提出了两种协同均值估计算法，解决了每个代理在学习模型的同时识别具有相似分布客户的挑战。 |
| [^71] | [Learning Generalization and Regularization of Nonhomogeneous Temporal Poisson Processes](https://arxiv.org/abs/2402.12808) | 将NHPPs的估计问题转化为学习泛化问题，提出了正则化学习NHPPs的框架与两种新的自适应和数据驱动的分箱方法，有效解决了数据量有限时过拟合的问题。 |
| [^72] | [Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles](https://arxiv.org/abs/2402.12794) | 通过使用自主仿生四足机器人代理和无人机，本文提出了一种自主的3D现实建模方法，可以用于文化遗产(CH)文物，实现了系统化和可重复的3D RM过程。 |
| [^73] | [From Movements to Metrics: Evaluating Explainable AI Methods in Skeleton-Based Human Activity Recognition](https://arxiv.org/abs/2402.12790) | 本研究测试并比较了基于骨架的人类活动识别中的可解释AI方法，发现在某些情境下“忠实度”可能不可靠，而“稳定性”在轻微数据扰动时更可靠。 |
| [^74] | [Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach](https://arxiv.org/abs/2402.12789) | 在不实施公平训练算法的情况下学习公平分类器，通过抽样具有影响力的数据来逐步转移原始训练数据，从而提高公平性和准确性。 |
| [^75] | [Tackling Byzantine Clients in Federated Learning](https://arxiv.org/abs/2402.12780) | 研究通过引入鲁棒平均规则来抵御联邦学习中的拜占庭式客户，同时强调客户子采样和本地步骤对模型性能的重要影响。 |
| [^76] | [When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting](https://arxiv.org/abs/2402.12767) | 提出了一种名为IDEA的模型，通过学习可识别的潜在状态检测时间序列数据中的分布变迁，并进一步分离平稳和非平稳的潜在状态。 |
| [^77] | [Learning under Singularity: An Information Criterion improving WBIC and sBIC](https://arxiv.org/abs/2402.12762) | LS信息准则旨在增强WBIC和sBIC的功能，有效处理非正则情况，具有稳定性，为奇异情况下的信息准则提供了新的方法 |
| [^78] | [FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework](https://arxiv.org/abs/2402.12761) | 提出了一种有效的联邦图异常检测框架FGAD，通过自我提升的知识蒸馏解决了联邦学习中的非独立同分布问题和高通信成本，推动图异常检测领域的发展。 |
| [^79] | [Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi Fingerprinting: A Discussion from a Data Perspective](https://arxiv.org/abs/2402.12756) | 该论文讨论了由于室内环境中电磁干扰的时变特性，静态数据库和动态数据库对基于Wi-Fi指纹的室内定位的不同影响。 |
| [^80] | [APT-MMF: An advanced persistent threat actor attribution method based on multimodal and multilevel feature fusion](https://arxiv.org/abs/2402.12743) | 提出了一种基于多模态和多级特征融合的高级持续性威胁行为者归因方法，利用异构属性图和多模态特征提取来增强网络威胁情报中的威胁行为者归因能力 |
| [^81] | [Guarantee Regions for Local Explanations](https://arxiv.org/abs/2402.12737) | 提出了一种基于锚点的算法，可以识别出局部解释被明确保证正确的区域，产生一个可解释的特征对齐盒，相较于现有基线，能找到具有更大保证区域的解释。 |
| [^82] | [UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation](https://arxiv.org/abs/2402.12730) | 使用机器翻译和大型语言模型，本文开发了用于非洲和亚洲语言语义文本相关性任务的两种模型，取得了比部分官方基准更好的效果。 |
| [^83] | [Scalable and reliable deep transfer learning for intelligent fault detection via multi-scale neural processes embedded with knowledge](https://arxiv.org/abs/2402.12729) | 提出了一种名为GTNP的神经过程深度迁移学习方法，通过特征传输策略弥合源域和目标域的数据分布差异，解决了数据稀缺和缺乏可靠性分析的问题 |
| [^84] | [Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2402.12728) | 提出了一种模态感知的LLM集成方法（MAIL）用于针对KVQA，通过细致地利用多模态知识来处理图像理解和知识推理。 |
| [^85] | [Diffusion Posterior Sampling is Computationally Intractable](https://arxiv.org/abs/2402.12727) | 我们证明了后验抽样在计算上是难以解决的：在加密学中最基本的假设下——单向函数存在的假设下，存在一些实例，对于这些实例，每个算法都需要超多项式时间，即使无条件抽样可以证明是快速的。 |
| [^86] | [Structural Knowledge Informed Continual Multivariate Time Series Forecasting](https://arxiv.org/abs/2402.12722) | 提出了一种新颖的结构知识指导的持续学习（SKI-CL）框架，用于在不同制度下持续积累的多变量时间序列（MTS）预测中，可以有效解决变量依赖关系的灾难性遗忘问题。 |
| [^87] | [Spurious Correlations in Machine Learning: A Survey](https://arxiv.org/abs/2402.12715) | 机器学习系统对输入中偏见特征与标签之间的虚假相关性敏感，本文回顾了解决这一问题的最新方法，同时总结了数据集、基准和度量标准，并讨论了未来研究挑战。 |
| [^88] | [Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules](https://arxiv.org/abs/2402.12714) | 提出了具有等变性的预训练Transformer(EPT)框架，能够统一多领域分子的几何学习，通过块增强表示和E(3)等变性实现更准确的3D结构表示。 |
| [^89] | [Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee](https://arxiv.org/abs/2402.12711) | 本文引入了统一最后迭代(ULI)保证这一更强的性能度量，同时证明接近最优的ULI保证直接导致了在各种性能度量上接近最优的累积性能。 |
| [^90] | [Integrating Active Learning in Causal Inference with Interference: A Novel Approach in Online Experiments](https://arxiv.org/abs/2402.12710) | 这项研究通过引入主动学习方法ACI，专注于在网络干扰和非随机处理分配情况下估计直接和溢出处理效应。 |
| [^91] | [Quantum Embedding with Transformer for High-dimensional Data](https://arxiv.org/abs/2402.12704) | 这项研究提出了一种结合transformer的量子嵌入架构，显著提升了处理高维数据的能力，验证了其在现代量子机器学习问题中的高度灵活和实用性。 |
| [^92] | [Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling](https://arxiv.org/abs/2402.12694) | 引入可学习分解策略和双注意力模块，同时捕捉跨系列依赖和内部变化，以应对复杂的多变量时间序列预测挑战。 |
| [^93] | [Learning on manifolds without manifold learning](https://arxiv.org/abs/2402.12687) | 提出了一种无需流形学习的在流形上学习方法，通过一次性构造获得最佳误差界限。 |
| [^94] | [TorchCP: A Library for Conformal Prediction based on PyTorch](https://arxiv.org/abs/2402.12683) | TorchCP是一个基于PyTorch的Python工具包，为深度学习模型上的合拟常规预测研究提供了实现后验和训练方法的多种工具，包括分类和回归任务。En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output. |
| [^95] | [Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies](https://arxiv.org/abs/2402.12673) | 研究将RL策略鲁棒性扩展至状态对抗攻击模型，超越仅针对最坏情况攻击，提出基于后悔最小化问题的自适应防御方法。 |
| [^96] | [Randomization Can Reduce Both Bias and Variance: A Case Study in Random Forests](https://arxiv.org/abs/2402.12668) | 随机森林相对于装袋法具有减少偏差的能力，在揭示数据模式和高信噪比情况下表现更好的特点，为随机森林在不同信噪比环境下的成功提供了解释和实用见解。 |
| [^97] | [Discriminant Distance-Aware Representation on Deterministic Uncertainty Quantification Methods](https://arxiv.org/abs/2402.12664) | 提出了一种新颖高效的确定性不确定性估计方法DDAR，通过引入判别距离感知表示和最优可训练原型，克服了特征塌缩问题，证明了其灵活性和架构无关性 |
| [^98] | [SoftQE: Learned Representations of Queries Expanded by LLMs](https://arxiv.org/abs/2402.12663) | SoftQE通过将输入查询的嵌入映射到LLM扩展查询的嵌入，提高了密集检索性能，并在领域外任务上取得了显著的性能改善。 |
| [^99] | [HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts](https://arxiv.org/abs/2402.12656) | HyperMoE通过Hypernetworks框架整合知识传递的概念，解决了在专家选择过程中专家知识稀疏性和可用性之间的矛盾。 |
| [^100] | [Training Artificial Neural Networks by Coordinate Search Algorithm](https://arxiv.org/abs/2402.12646) | 通过提出的高效版本的非梯度坐标搜索（CS）算法，我们可以训练神经网络，解决了需要可微激活函数和同时优化多个非可微损失函数的问题。 |
| [^101] | [FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML](https://arxiv.org/abs/2402.12630) | FAST框架通过快速分段形状函数的优化和新的特征选择算法，使得透明的附加模型的拟合速度比现有方法快2个数量级。 |
| [^102] | [A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective](https://arxiv.org/abs/2402.12627) | 该综述将领域转移和概念漂移重新归为一个单一的研究问题，即数据变化问题，系统地总结了这两个研究领域中最新的方法。 |
| [^103] | [Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors](https://arxiv.org/abs/2402.12626) | 本文研究了针对预训练特征提取器的任意数据毒化攻击，探讨了这种攻击对机器学习模型的安全风险和影响。 |
| [^104] | [Compact NSGA-II for Multi-objective Feature Selection](https://arxiv.org/abs/2402.12625) | 提出了一种紧凑型NSGA-II算法，用于多目标特征选择，旨在提高分类准确度并减少所选特征数量 |
| [^105] | [Reflect-RL: Two-Player Online RL Fine-Tuning for LMs](https://arxiv.org/abs/2402.12621) | 提出Reflect-RL，使用在线强化学习来微调语言模型，在这过程中引入了反射模型协助，并采用了负例生成、单提示动作枚举和课程学习来提高性能。 |
| [^106] | [Generative AI Security: Challenges and Countermeasures](https://arxiv.org/abs/2402.12617) | 生成式人工智能的安全挑战及对策研究。 |
| [^107] | [Multi-objective Binary Coordinate Search for Feature Selection](https://arxiv.org/abs/2402.12616) | 这项研究提出了二进制多目标坐标搜索（MOCS）算法，用于解决大规模特征选择问题，是首个多目标坐标搜索算法。 |
| [^108] | [Analysis of Using Sigmoid Loss for Contrastive Learning](https://arxiv.org/abs/2402.12613) | 提出了双常数嵌入模型（CCEM）以理论分析使用Sigmoid Loss在对比学习中的应用，有可能提供更高效的性能。 |
| [^109] | [Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations](https://arxiv.org/abs/2402.12598) | 本文提出了一种基于图的虚拟传感方法，通过利用相关变量之间的依赖关系，设计了GgNet架构，用于推断未观测信道的值。 |
| [^110] | [Truncated Polynomial Expansion-Based Detection in Massive MIMO: A Model-Driven Deep Learning Approach](https://arxiv.org/abs/2402.12595) | 通过在大规模MIMO系统中应用基于截断多项式展开的深度学习方法，提高了信号检测的效率，并降低了计算复杂性 |
| [^111] | [FairProof : Confidential and Certifiable Fairness for Neural Networks](https://arxiv.org/abs/2402.12572) | FairProof提出了一种使用零知识证明来公开验证神经网络模型公平性的系统，同时保持机密性，并提出了适用于ZKPs的全连接神经网络的公平性认证算法。 |
| [^112] | [Offline Multi-task Transfer RL with Representational Penalization](https://arxiv.org/abs/2402.12570) | 提出了一种计算学习表示不确定性度量的算法，为目标任务学到的策略的次优性建立了数据相关的上界。 |
| [^113] | [GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence](https://arxiv.org/abs/2402.12566) | GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。 |
| [^114] | [Dynamic Pricing and Learning with Long-term Reference Effects](https://arxiv.org/abs/2402.12562) | 在考虑顾客价格期望对当前价格反应的情况下，研究了一种具有长期参考效应的动态定价问题，提出了一种新颖的参考价格机制，展示在该机制下降价政策几乎是最优的，为线性需求模型提供了近似最优降价策略。 |
| [^115] | [Evaluation of Country Dietary Habits Using Machine Learning Techniques in Relation to Deaths from COVID-19](https://arxiv.org/abs/2402.12558) | 该研究利用机器学习技术评估了170个国家的饮食习惯，发现肥胖和高脂肪摄入与COVID-19死亡率较高的国家相关，而谷物消费水平较高的国家则有较低的死亡率。 |
| [^116] | [Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization](https://arxiv.org/abs/2402.12550) | 多线性专家混合（MMoE）层通过因式分解针对视觉模型提供了一种可扩展的专家特化解决方案，避免了离散专家路由和过高推理时间成本。 |
| [^117] | [Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage](https://arxiv.org/abs/2402.12539) | 研究发现，在具有智能储能的建筑中，简单的线性多层感知器模型提供了与最先进模型相当的预测准确性，更具有数据效率和泛化能力。 |
| [^118] | [A Machine Learning Ensemble Model for the Detection of Cyberbullying](https://arxiv.org/abs/2402.12538) | 该研究使用堆叠集成机器学习方法结合多种特征提取技术，成功开发出用于检测网络欺凌的自动化系统，并在性能上表现出色。 |
| [^119] | [Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning](https://arxiv.org/abs/2402.12537) | 该论文提出了基于分层贝叶斯统计框架的算法，用于个性化无监督学习，其中开发了适应性算法来平衡利用有限本地数据和协作信息。 |
| [^120] | [Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics](https://arxiv.org/abs/2402.12535) | 本研究提出了一种针对科学领域中大规模点云处理优化的Transformer模型，通过局部敏感哈希技术实现了近线性复杂度，并提出了基于LSH的高效点变换器HEPT。 |
| [^121] | [Improving Deep Generative Models on Many-To-One Image-to-Image Translation](https://arxiv.org/abs/2402.12531) | 介绍了一种新的非对称框架，可改进现有深度生成模型在多对一图像到图像翻译上的效果，并在 StarGAN V2 上展示了其性能优化。 |
| [^122] | [Parallel Structures in Pre-training Data Yield In-Context Learning](https://arxiv.org/abs/2402.12530) | 本研究发现，语言模型的上下文学习能力取决于预训练数据中的平行结构，通过在相似模板的短语对中学习来提高上下文学习准确度。 |
| [^123] | [The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2402.12527) | 学习的动力学模型被真实且无误差的动力学替代时，现有模型驱动方法将会完全失败，揭示出一个重大误解。 |
| [^124] | [Gaussian Process Neural Additive Models](https://arxiv.org/abs/2402.12518) | 本文提出了一种新的高斯过程神经加性模型（GP-NAM），通过随机傅里叶特征对高斯过程进行单层神经网络构建，可以实现具有凸目标函数和可训练参数数量随特征维度线性增长的优势，同时在性能上不亚于更深的NAM方法。 |
| [^125] | [Induced Model Matching: How Restricted Models Can Help Larger Ones](https://arxiv.org/abs/2402.12513) | 提出了引导模型匹配（IMM）方法，通过使完整模型的性能与受限模型对齐，将受限模型的知识传递给完整模型，具有广泛的应用性。 |
| [^126] | [SDEs for Minimax Optimization](https://arxiv.org/abs/2402.12508) | 本文开创性地使用随机微分方程分析和比较极小化优化器，展示了超参数、隐式正则化和隐式曲率诱导噪声之间的相互作用，并提供了统一简化的分析策略。 |
| [^127] | [PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling](https://arxiv.org/abs/2402.12503) | PARCv2通过引入微分算子扩展了PARC模型，用于模拟不稳定、瞬态和传输主导系统的时空动力学。 |
| [^128] | [Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification](https://arxiv.org/abs/2402.12500) | 将kNN与自然图像上自监督预训练的基础模型结合，独立存储训练数据的嵌入，实现动态数据修改而无需重新训练，提升图像分类的可解释性和适应性 |
| [^129] | [Automated Security Response through Online Learning with Adaptive Conjectures](https://arxiv.org/abs/2402.12499) | 该论文通过自适应猜想的在线学习，提出了一种适用于IT基础设施的自动化安全响应方法，其中游戏参与者通过Bayesian学习调整猜想，并通过推演更新策略，最终实现了最佳拟合，提高了推演在猜想模型下的性能。 |
| [^130] | [Feudal Networks for Visual Navigation](https://arxiv.org/abs/2402.12498) | 使用封建学习的视觉导航，通过高级管理者、中级管理者和工作代理的分层结构，在不同空间和时间尺度上操作，具有独特模块来实现自监督学习记忆代理地图。 |
| [^131] | [Towards Cross-Domain Continual Learning](https://arxiv.org/abs/2402.12490) | 介绍了一种名为跨领域持续学习（CDCL）的新方法，通过整合任务间和任务内的交叉注意机制，在紧凑的卷积网络中延迟数据漂移，实现了无监督的跨领域学习（UDA）。 |
| [^132] | [SECP: A Speech Enhancement-Based Curation Pipeline For Scalable Acquisition Of Clean Speech](https://arxiv.org/abs/2402.12482) | 提出了基于语音增强的策划管道（SECP），可以在规模上获取干净语音并训练语音增强模型，通过两轮迭代观察到增强输出作为基准不会降低模型性能，并通过主观测试证明优化数据在感知上优于原始数据。 |
| [^133] | [In deep reinforcement learning, a pruned network is a good network](https://arxiv.org/abs/2402.12479) | 通过逐渐剪枝，使代理能够最大程度地发挥参数效能，从而产生比传统网络显著性能提升的网络，并展现出一种“缩放定律”。 |
| [^134] | [Diffeomorphism Neural Operator for various domains and parameters of partial differential equations](https://arxiv.org/abs/2402.12475) | 通过微分同胚神经算子学习框架，提出了一种适用于各种和复杂领域的物理系统的领域灵活模型，从而将学习函数映射在不同领域的问题转化为在共享的微分同胚上学习算子的问题。 |
| [^135] | [Neuro-mimetic Task-free Unsupervised Online Learning with Continual Self-Organizing Maps](https://arxiv.org/abs/2402.12465) | 该研究提出了一种具有持续学习能力的神经仿真系统，克服了灾难性遗忘问题，并研究了在无监督架构中的应用，尤其是自组织映射模型。 |
| [^136] | [DBNets: A publicly available deep learning tool to measure the masses of young planets in dusty protoplanetary discs](https://arxiv.org/abs/2402.12448) | 开发了一种深度学习工具DBNets，用于分析原行星盘尘埃辐射中的次结构，快速推断嵌入行星的质量，并能可靠量化质量和相关不确定性 |
| [^137] | [Emulating the interstellar medium chemistry with neural operators](https://arxiv.org/abs/2402.12435) | 该研究使用神经算子取代计算量大、精确度高、计算代价昂贵的普通微分方程求解器，成功模拟了星际介质中的非平衡化学网络演化，对于宇宙学和天体物理模拟具有重要意义。 |
| [^138] | [Attacks on Node Attributes in Graph Neural Networks](https://arxiv.org/abs/2402.12426) | 该研究通过基于特征的对抗攻击，针对图神经网络中的节点属性展开研究，发现使用Projected Gradient Descent的决策时攻击比使用Mean Node Embeddings和Graph Contrastive Learning策略的毒化攻击更加有效。 |
| [^139] | [Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data](https://arxiv.org/abs/2402.12424) | 本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。 |
| [^140] | [On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models](https://arxiv.org/abs/2402.12423) | 本研究在文本转语音模型中探索了冻结模型的潜空间，发现其中包含丰富的语义信息，并提出了一些新方法来找出其中的语义方向，从而实现了不经过额外训练、架构更改或数据需求就能进行音频编辑。 |
| [^141] | [EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs](https://arxiv.org/abs/2402.12419) | 提出了一种有效的块状微调稀疏LLM的框架，通过最小化重建误差并采用反向传播逐块优化解决方案，实验结果表明在各种基准测试中优于其他方法。 |
| [^142] | [Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures](https://arxiv.org/abs/2402.12418) | 引入了一种基于二阶损失景观信息的自动缩放方法，同时扩展和训练transformers，提出了神经架构中的深度异质性概念，并在ImageNet100上实现了准确性和参数效率的提升。 |
| [^143] | [Predicting trucking accidents with truck drivers 'safety climate perception across companies: A transfer learning approach](https://arxiv.org/abs/2402.12417) | 提出了一种预先训练然后微调的迁移学习方法，利用其他公司的数据开发AI模型，更准确地预测卡车事故风险。 |
| [^144] | [Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks](https://arxiv.org/abs/2402.12411) | 通过利用异构结构化知识，提出了一个新的学习框架SKES，用于在异构信息网络中丰富节点表示的信息量，进而建立了一个可解释的节点重要性计算范式。 |
| [^145] | [ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation](https://arxiv.org/abs/2402.12408) | ModelGPT是一个新颖的框架，通过利用LLM的能力，根据用户提供的数据或任务描述生成定制化的AI模型，让用户能够更快速和方便地使用AI模型。 |
| [^146] | [Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation](https://arxiv.org/abs/2402.12406) | 该论文发现现有的无数据知识蒸馏方法对不同的教师模型非常敏感，生成的样本可能出现质量问题。 |
| [^147] | [Estimating the age-conditioned average treatment effects curves: An application for assessing load-management strategies in the NBA](https://arxiv.org/abs/2402.12400) | 引入了一种新框架，通过比赛层次数据估计年龄曲线，提高了性能轨迹分析的细致度，可以发现复杂非线性模式，实现对年龄曲线的因果效应的细致研究。 |
| [^148] | [Turn Waste into Worth: Rectifying Top-$k$ Router of MoE](https://arxiv.org/abs/2402.12399) | 提出了Rectify-Router解决了MoE模型中常用的Top-k路由机制所带来的令牌丢失和填充问题，通过Intra-GPU矫正和Fill-in矫正来实现。 |
| [^149] | [Primary and Secondary Factor Consistency as Domain Knowledge to Guide Happiness Computing in Online Assessment](https://arxiv.org/abs/2402.12398) | 本文尝试通过实证研究角度提供对解释一致性的新见解，并研究如何通过引入领域知识约束来使机器学习模型更加可信。 |
| [^150] | [Multi-class Temporal Logic Neural Networks](https://arxiv.org/abs/2402.12397) | 提出了一种结合神经网络和信号时间逻辑的方法，用于多类别时间序列数据的分类，关键贡献包括引入边界概念和利用STL属性增强结果的可解释性。 |
| [^151] | [Improving Model's Interpretability and Reliability using Biomarkers](https://arxiv.org/abs/2402.12394) | 利用决策树解释基于生物标志物的诊断模型，帮助临床医生提高识别不准确预测的能力，从而增强医学诊断模型的可靠性。 |
| [^152] | [Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data](https://arxiv.org/abs/2402.12391) | 引入了一个名为AI科学家团队（TAIS）的框架，旨在简化科学发现流程，由模拟角色协作，特别关注于识别具有疾病预测价值的基因 |
| [^153] | [Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!](https://arxiv.org/abs/2402.12343) | 安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。 |
| [^154] | [Endowing Pre-trained Graph Models with Provable Fairness](https://arxiv.org/abs/2402.12161) | 提出了一种新的适配器调优框架，赋予预训练图模型具有可证明的公平性 |
| [^155] | [WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More](https://arxiv.org/abs/2402.12065) | 该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。 |
| [^156] | [AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization](https://arxiv.org/abs/2402.11940) | 提出了一种新的对抗攻击策略AICAttack，旨在通过微小的图像扰动来攻击图像字幕模型，在黑盒攻击情景下具有良好的效果。 |
| [^157] | [A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning](https://arxiv.org/abs/2402.11922) | 提出了一种生成式预训练框架 GPDiff，通过在源城市数据优化的模型参数上进行预训练，将STG迁移学习转化为预训练生成式超网络，实现了对不同数据分布和特定城市特征的适应性。 |
| [^158] | [Microstructures and Accuracy of Graph Recall by Large Language Models](https://arxiv.org/abs/2402.11821) | 本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。 |
| [^159] | [MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs](https://arxiv.org/abs/2402.11756) | MARS提出了一种新的评分函数MARS，考虑了生成序列中每个标记的语义贡献，该方法改进了生成式LLMs中的不确定性估计性能。 |
| [^160] | [Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing](https://arxiv.org/abs/2402.11752) | 引入了Diagonalisation Stochastic Gradient Descent（对角化SGD），通过重新参数化和平滑实现非可微模型的快速收敛SGD，在实证评估中表现出简单、快速、稳定，并且取得了数量级的工作规范化方差降低。 |
| [^161] | [GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation](https://arxiv.org/abs/2402.11401) | 提出了一种基于图的知识蒸馏框架，用于在文档图像中识别和定位文档对象，以减少大型模型在资源受限设备上的部署成本 |
| [^162] | [Physics-based material parameters extraction from perovskite experiments via Bayesian optimization](https://arxiv.org/abs/2402.11101) | 使用贝叶斯优化开发了一个分析平台，可以从钙钛矿实验中提取多个基本材料参数，加速材料发现和半导体优化 |
| [^163] | [Optimal feature rescaling in machine learning based on neural networks](https://arxiv.org/abs/2402.10964) | 提出了一种通过遗传算法进行输入特征的最佳重缩放来改善神经网络训练效率和泛化性能的方法。 |
| [^164] | [The Unreasonable Effectiveness of Eccentric Automatic Prompts](https://arxiv.org/abs/2402.10949) | 异类自动提示的不合理有效性研究了大型语言模型在处理各种提示时的表现，结果显示在大多数情况下，包括“积极思考”提示会对模型性能产生积极影响。 |
| [^165] | [ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters](https://arxiv.org/abs/2402.10930) | ConSmax是一种硬件友好型Softmax替代方案，通过引入可学习参数，在不影响性能的情况下实现了对原Softmax关键任务的高效处理。 |
| [^166] | [Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice](https://arxiv.org/abs/2402.10870) | 本文分享了关于在工业环境中使用AED系统面临的挑战，并提供了在这种环境中适当的目标和系统规格的视角，最终开发了一个基于反事实推断的AED框架并在商业环境中进行了测试。 |
| [^167] | [Collaborative Learning with Different Labeling Functions](https://arxiv.org/abs/2402.10445) | 研究了使用不同标注函数的协作学习中，基于经验风险最小化算法在增强假设类上的高效学习方法。 |
| [^168] | [Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective](https://arxiv.org/abs/2402.10184) | 本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。 |
| [^169] | [Best Arm Identification for Prompt Learning under a Limited Budget](https://arxiv.org/abs/2402.09723) | 这项工作提出了一种在提示学习中考虑有限预算约束的方法，通过建立提示学习和多臂赌博机中固定预算最佳臂识别之间的联系，提出了一个通用框架TRIPLE，通过利用聚类和嵌入思想实现了两个增强方法。 |
| [^170] | [A Systematic Review of Data-to-Text NLG](https://arxiv.org/abs/2402.08496) | 这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。 |
| [^171] | [Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming](https://arxiv.org/abs/2402.08491) | 本研究开发了一个基于深度强化学习的计算框架，用于细胞重编程中的重编程策略识别。在控制问题中，引入了伪吸引子的概念和识别方法，并设计了一个用于解决该问题的计算框架。 |
| [^172] | [ChatCell: Facilitating Single-Cell Analysis with Natural Language](https://arxiv.org/abs/2402.08303) | ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。 |
| [^173] | [An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering](https://arxiv.org/abs/2402.07845) | 本研究展示了使用无监督度量模块性优化GNN进行节点聚类的方法，且无需与基准值进行比较。在设计合成实验的过程中，我们发现了这种方法的局限性。 |
| [^174] | [Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting](https://arxiv.org/abs/2402.05956) | 本文提出了一种名为Pathformer的多尺度自适应路径的Transformer模型，用于时间序列预测。通过整合时间分辨率和时间距离进行多尺度建模，并使用自适应路径来优化建模过程，可以提高预测准确性和泛化能力。 |
| [^175] | [On the Convergence of Zeroth-Order Federated Tuning in Large Language Models](https://arxiv.org/abs/2402.05926) | 我们的研究提出了一种名为FedMeZO的方法，以在零阶联邦学习和大规模语言模型之间实现内存高效的调整。我们的理论和实证证据表明FedMeZO不仅收敛速度快于传统的一阶方法，而且在个性化的联邦策略中具有关键的作用。 |
| [^176] | [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/abs/2402.04333) | LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。 |
| [^177] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^178] | [Variational Flow Models: Flowing in Your Style](https://arxiv.org/abs/2402.02977) | 我们引入了一种变分流模型的方法，并提出了一种系统的无需训练的转换方法，使得快速采样成为可能，同时保持了采样的准确性和效率。 |
| [^179] | [Reducing Optimism Bias in Incomplete Cooperative Games](https://arxiv.org/abs/2402.01930) | 本文提出了一个框架，旨在通过优化揭示联盟价值的顺序来减少不完全合作博弈中的乐观偏误。 |
| [^180] | [PRewrite: Prompt Rewriting with Reinforcement Learning](https://arxiv.org/abs/2401.08189) | 本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。 |
| [^181] | [Generalization in Kernel Regression Under Realistic Assumptions](https://arxiv.org/abs/2312.15995) | 本文提供了一个统一的理论，用于对几乎所有常见和现实设置下的核回归的超出风险进行上限约束，并揭示了核分解中存在的自我正则化现象。 |
| [^182] | [Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities](https://arxiv.org/abs/2312.15006) | 三种提示方法对ChatGPT的数学能力并未产生一贯性改进效果，部分方法甚至导致性能下降 |
| [^183] | [Best Arm Identification with Fixed Budget: A Large Deviation Perspective](https://arxiv.org/abs/2312.12137) | 本文通过建立经验比例和经验臂奖励之间的连接，提高了一些现有算法的错误概率上界。 |
| [^184] | [Dynamic Retrieval-Augmented Generation](https://arxiv.org/abs/2312.08976) | 所提出的Dynamic Retrieval-Augmented Generation (DRAG)是一种新颖的方法，通过实体增强生成，将检索到的实体的压缩嵌入注入到生成模型中，从而在代码生成任务中取得较好效果。 |
| [^185] | [Coupled Confusion Correction: Learning from Crowds with Sparse Annotations](https://arxiv.org/abs/2312.07331) | 通过耦合混淆校正算法，能够减轻众包标注中的标签噪声，提高模型性能，并通过双模型相互校正学习到的混淆矩阵，进一步优化结果。 |
| [^186] | [Meta Co-Training: Two Views are Better than One](https://arxiv.org/abs/2311.18083) | 元共训练通过在数据上构建不同的视角，并利用未标记数据进行共同训练，提高了半监督学习的性能。 |
| [^187] | [Procedural Fairness Through Decoupling Objectionable Data Generating Components](https://arxiv.org/abs/2311.14688) | 通过解耦可抗议的数据生成组件，本研究提出了一个框架来防止伪装的程序不公平，并强调了满足程序公平要求的重要性 |
| [^188] | [Touring sampling with pushforward maps](https://arxiv.org/abs/2311.13845) | 该论文从理论角度对生成建模中的多种采样方法进行了审视和组织，帮助克服采样中的一些挑战，比如推理时间长和生成样本缺乏多样性。 |
| [^189] | [Prompt Engineering a Prompt Engineer](https://arxiv.org/abs/2311.05661) | 提示工程任务对于优化大型语言模型在定制任务上的表现至关重要，PE2方法通过详细描述、上下文规范和逐步推理模板的注入，在各种语言任务中展现出出色的适用性和效果。 |
| [^190] | [Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game](https://arxiv.org/abs/2310.18940) | 使用强化学习为基础，提出了一种框架，用于在狼人杀游戏中开发具有灵活语言行为和强大决策能力的战略语言代理 |
| [^191] | [How Graph Neural Networks Learn: Lessons from Training Dynamics](https://arxiv.org/abs/2310.05105) | 图神经网络的优化过程中涉及核-图对齐现象，从优化角度解释了学到的函数何时和为何泛化，有助于理解其在异源图上的限制。 |
| [^192] | [MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and Accelerating Distributed DNN Training](https://arxiv.org/abs/2310.00967) | MiCRO是一种新颖的梯度稀疏化方法，通过对梯度向量进行分区，并由每个工作者选择其分区中的梯度，从而减少了通信流量并避免了梯度堆积。 |
| [^193] | [Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees](https://arxiv.org/abs/2309.09968) | 通过XGBoost技术，本研究提出了一种用于生成和填充混合类型表格数据的新方法，在数据生成任务中优于深度学习方法并在数据填充方面表现竞争力，且可以在CPU上并行训练。 |
| [^194] | [Self-concordant Smoothing for Large-Scale Convex Composite Optimization](https://arxiv.org/abs/2309.01781) | 提出了适用于大规模凸组合优化问题的自共轭平滑方法，通过变量度量和步长规则优化了近端牛顿算法，有效处理了非光滑函数的结构，提出了Prox-N-SCORE和Prox-GGN-SCORE算法，后者通过重要近似程序显著减少了逆Hessian计算开销。 |
| [^195] | [Underwater Acoustic Target Recognition based on Smoothness-inducing Regularization and Spectrogram-based Data Augmentation](https://arxiv.org/abs/2306.06945) | 通过平滑度诱导正则化和基于频谱图的数据增强，提高水下声学目标识别模型的泛化能力和避免性能降级的风险 |
| [^196] | [Incentivizing Exploration with Linear Contexts and Combinatorial Actions](https://arxiv.org/abs/2306.01990) | 研究在激励式赌博机探索中通过线性赌博机模型替代先验独立性条件，提高了高维动作空间下的激励探索效率和最优遗憾，同时改进了半赌博模型中关于初始数据收集的样本复杂度。 |
| [^197] | [Federated Learning in the Presence of Adversarial Client Unavailability](https://arxiv.org/abs/2305.19971) | 放宽了结构性假设，研究了恶意客户端不可用的情况，引入了$\epsilon$-对手丢包分数的概念。 |
| [^198] | [Learning not to Regret](https://arxiv.org/abs/2303.01074) | 提出了一个“学习不后悔”的框架，可加速在类似但不相同游戏分布上的均衡寻找，同时在任何游戏上具有减小后悔的保证。 |
| [^199] | [Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time](https://arxiv.org/abs/2302.02181) | 提出了一种利用GAN生成器拼接网络并实时反转的方法，在图像分类和语义分割网络中表现出与梯度下降方法相当的性能，但处理时间快两个数量级。 |
| [^200] | [Autoregressive Bandits](https://arxiv.org/abs/2212.06251) | 在自回归过程控制的奖励下，提出了自回归赌博机（ARBs）在线学习设置，并设计了AutoRegressive Upper Confidence Bound (AR-UCB)算法，可以方便计算最优策略并具有次线性遗憾。 |
| [^201] | [Graph Filters for Signal Processing and Machine Learning on Graphs](https://arxiv.org/abs/2211.08854) | 图滤波器是为处理和学习网络和其他不规则域数据而设计的，可以增强表示能力以模拟更广泛的信号类、数据模式和关系。 |
| [^202] | [Quantum Vision Transformers](https://arxiv.org/abs/2209.08167) | 本研究设计和分析了量子变压器，引入了三种类型的量子变压器用于训练和推理，在保证量子注意机制具有理论优势的基础上，采用浅量子电路构建，产生不同的分类模型。 |
| [^203] | [Mathematical Framework for Online Social Media Auditing](https://arxiv.org/abs/2209.05550) | 研究提出了一个数学框架，探讨了社交媒体平台上算法过滤的不良影响以及监管复杂性。 |
| [^204] | [Learning inducing points and uncertainty on molecular data by scalable variational Gaussian processes](https://arxiv.org/abs/2207.07654) | 通过引入潜在的引导点变量并选择合适的边缘对数似然目标，可在分子描述符空间中改进预测性能。 |
| [^205] | [Inverse Boundary Value and Optimal Control Problems on Graphs: A Neural and Numerical Synthesis](https://arxiv.org/abs/2206.02911) | 引入了图上确定性系统识别问题的通用设置，提出了一种边界注入消息传递神经网络提高预测准确性的方法，并引入了一种基于图距离的正则化技术来稳定远离边界的节点的预测。 |
| [^206] | [Learning in Mean Field Games: A Survey](https://arxiv.org/abs/2205.12944) | 强化学习和均场博弈的结合有望在很大规模上解决游戏的均衡和社会最优问题。 |
| [^207] | [Preemptive Motion Planning for Human-to-Robot Indirect Placement Handovers](https://arxiv.org/abs/2203.00156) | 提出了一种新颖的预测规划流程，允许机器人主动朝着人类代理的预定放置位置移动 |
| [^208] | [Simplicial Convolutional Filters](https://arxiv.org/abs/2201.11720) | 该论文研究了线性滤波器，提出了简单复合卷积核，定义为较低和较高霍奇拉普拉斯矩阵多项式，具有线性、平移不变、置换和方向等变性，以低计算复杂度在分布式方式实现。 |
| [^209] | [Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint](https://arxiv.org/abs/2009.01947) | 提出了针对大小约束非单调子模函数的优化算法，通过引入改进的子程序ThreshSeq实现更高的逼近比率和更低的复杂度 |
| [^210] | [Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models.](http://arxiv.org/abs/2401.13227) | 本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。 |
| [^211] | [Space and Time Continuous Physics Simulation From Partial Observations.](http://arxiv.org/abs/2401.09198) | 本研究提出了一种新颖的方法，可以从部分观测中进行连续的空间和时间物理模拟，并解决了基于固定支持网格的传统方法的缺点。这种方法通过在稀疏观测上进行训练，利用两个相互关联的动力系统在稀疏位置和连续域上进行预测和插值求解。 |
| [^212] | [Bellman Optimal Step-size Straightening of Flow-Matching Models.](http://arxiv.org/abs/2312.16414) | 本论文介绍了一种名为BOSS的技术，通过优化步长和生成路径，提升了低资源场景下流匹配生成模型的图像质量和资源利用效率。 |
| [^213] | [Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint.](http://arxiv.org/abs/2312.11456) | 该论文研究了在KL约束下的反馈强化学习的理论框架，并提出了有效的算法和实践。实证评估表明，该框架在大型语言模型的对齐实验中表现出良好的效果。 |
| [^214] | [Add and Thin: Diffusion for Temporal Point Processes.](http://arxiv.org/abs/2311.01139) | 本研究提出了一种基于概率去噪扩散模型的时间点过程模型，相比于现有的方法，该模型在预测方面取得了较好的性能，对具有离散和连续成分的数据具有处理能力。 |
| [^215] | [Self-supervised Pre-training for Precipitation Post-processor.](http://arxiv.org/abs/2310.20187) | 该论文提出了一种基于深度学习的降水后处理方法，使用自监督预训练和转移学习来提高数值天气预报模型的准确性。实验结果表明该方法在区域降水校正方面表现优于其他方法。 |
| [^216] | [Interpretable Prototype-based Graph Information Bottleneck.](http://arxiv.org/abs/2310.19906) | 这项工作提出了一种新颖的可解释的GNN框架，通过在信息瓶颈框架中将原型学习与输入图的关键子图相结合，为模型的解释能力和性能提供了改进。 |
| [^217] | [Conformal Normalization in Recurrent Neural Network of Grid Cells.](http://arxiv.org/abs/2310.19192) | 本文提出了一种循环神经网络中的共形归一化方法，用于处理网格细胞在2D物理空间中的自我位置信息。实验结果表明，该方法能够显著减小位置误差。 |
| [^218] | [Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice.](http://arxiv.org/abs/2310.18212) | 这项研究调查了超参数对因果结构学习任务的影响，并通过对不同复杂级别的数据集进行实证评估，发现超参数选择同样对算法选择具有重要的影响。 |
| [^219] | [Contextual directed acyclic graphs.](http://arxiv.org/abs/2310.15627) | 本论文研究了上下文定向无环图的问题，通过神经网络将上下文特征映射到DAG，利用稀疏的加权邻接矩阵表示图结构，并通过新颖的投影层满足无环性的特点。实验证明该方法能够成功恢复出真实的上下文特定图。 |
| [^220] | [Stable Nonconvex-Nonconcave Training via Linear Interpolation.](http://arxiv.org/abs/2310.13459) | 本文通过理论分析指出，优化过程中的不稳定性通常是由损失函数的非单调性引起的，提出了一种稳定（大规模）神经网络训练的方法，即通过线性插值来利用“非扩张算子”的理论来解决。通过构建一种新的优化方案，松弛近似近端点（RAPP），发现了一族Lookahead算法，能够在协调部分单调问题中收敛，即使基本优化器采用梯度下降升级算法。 |
| [^221] | [Impact of multi-armed bandit strategies on deep recurrent reinforcement learning.](http://arxiv.org/abs/2310.08331) | 本研究在自动驾驶场景中使用部分可观测系统，通过部署和测试多种技术来平衡探索和利用的权衡，以预测方向盘操作。 |
| [^222] | [LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection.](http://arxiv.org/abs/2310.05668) | LARA是一种轻量级且抗过拟合的无监督异常检测再训练方法，它将重新训练过程形式化为一个凸问题，并设计了一个反思模块以利用历史数据，同时数学证明了在微调后可以获得更好的性能。 |
| [^223] | [Physics Informed Neural Network Code for 2D Transient Problems (PINN-2DT) Compatible with Google Colab.](http://arxiv.org/abs/2310.03755) | 这个论文提出了一个适用于谷歌Colab的开源物理信息神经网络环境，可用于在二维矩形域上模拟瞬态现象，并提供了多个特性和问题库。 |
| [^224] | [FLAIM: AIM-based Synthetic Data Generation in the Federated Setting.](http://arxiv.org/abs/2310.03447) | FLAIM是一个在联邦设置中基于AIM的合成数据生成方法，该方法解决了差分隐私方向的技术在联邦场景下的适用问题，并提出了FLAIM方法来维持较高的效用和处理异构性。 |
| [^225] | [SemiReward: A General Reward Model for Semi-supervised Learning.](http://arxiv.org/abs/2310.03013) | SemiReward是一个通用奖励模型，通过预测奖励分数来评估和过滤高质量的伪标签，可以应用于各种半监督学习任务，并在实验中取得了显著的成果。 |
| [^226] | [SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping.](http://arxiv.org/abs/2310.01201) | SWoTTeD是一种扩展的张量分解方法，用于发现复杂时间模式下的隐藏表征。在实验中，SWoTTeD不仅能与最新的基于张量分解的方法一样准确地重建数据，还能提取出对临床医生有意义的时间表征。 |
| [^227] | [Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications.](http://arxiv.org/abs/2309.13207) | Evidential deep learning extends parametric deep learning to higher-order distributions, enabling the estimation of both aleatoric and epistemic uncertainty with one model. This study shows that evidential deep learning models achieve predictive accuracy comparable to standard methods while robustly quantifying both sources of uncertainty. |
| [^228] | [A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining.](http://arxiv.org/abs/2309.04761) | 本调研综合审查了在教育数据挖掘中深度学习技术的最新研究进展，包括对知识跟踪、学生不良行为检测、性能预测和个性化推荐等典型教育场景的应用。同时提供了公共数据集和处理工具的综合概述，并指出了未来的研究方向。 |
| [^229] | [Will More Expressive Graph Neural Networks do Better on Generative Tasks?.](http://arxiv.org/abs/2308.11978) | 本论文调查了更具表现力的图神经网络在分子图生成任务中的表现能力，并通过替换图生成模型的基础GNN来进行实验。研究发现，使用更具表现力的GNN可以改善生成任务的性能。 |
| [^230] | [Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees.](http://arxiv.org/abs/2308.09842) | 通过epsilon-ProVe方法，我们提出了一种高效近似的方法来枚举深度神经网络中的安全区域，并提供了可证明概率保证的紧密下估计。 |
| [^231] | [PMET: Precise Model Editing in a Transformer.](http://arxiv.org/abs/2308.08742) | 该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。 |
| [^232] | [Kernel Single Proxy Control for Deterministic Confounding.](http://arxiv.org/abs/2308.04585) | 本研究考虑了具有未观测混淆因素的因果效应估计问题，在结果是确定性生成的情况下，提出了一种使用单一代理变量的内核方法，通过两阶段回归和最大矩约束的方法可以一致估计因果效应，并在合成数据集上成功恢复了因果效应。 |
| [^233] | [Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting.](http://arxiv.org/abs/2308.01421) | 这项工作提出了一种处理神经网络泛化和过拟合的新方法，通过正则化损失函数和提前停止策略来优化网络参数，并通过数值实验验证了该方法的有效性。 |
| [^234] | [WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms.](http://arxiv.org/abs/2307.12449) | 本研究提出了一种名为WEPRO的新方法，通过利用参数权重中的规律趋势加速了混合量子-经典算法的收敛速度，相比标准训练方法，速度提高约2.25倍，准确性提高了2.3%，损失降低了6.1%。 |
| [^235] | [The importance of feature preprocessing for differentially private linear optimization.](http://arxiv.org/abs/2307.11106) | 本论文研究了差分隐私线性优化中特征预处理的重要性。在简单的线性分类情况下，与非隐私优化相比，特征预处理对于差分隐私优化是至关重要的，否则会产生与特征最大范数成比例的隐私错误。我们提出了一种结合特征预处理的算法DPSGD-F。 |
| [^236] | [Choice Models and Permutation Invariance.](http://arxiv.org/abs/2307.07090) | 本文提出了选择模型和置换不变性的基本特征化方法，并展示了如何通过非参数估计器逼近选择函数，以及在实际应用中的灵活性和优越性。 |
| [^237] | [TGRL: An Algorithm for Teacher Guided Reinforcement Learning.](http://arxiv.org/abs/2307.03186) | TGRL是一种用于教师引导强化学习的算法，通过动态和自动平衡何时遵循教师指导和何时使用奖励，教师监督的重要性会根据代理的表现调整。 |
| [^238] | [Hybrid Graph: A Unified Graph Representation with Datasets and Benchmarks for Complex Graphs.](http://arxiv.org/abs/2306.05108) | 本论文介绍了混合图的概念及其在高阶图建模中的应用，同时提出了混合图数据集及全面的评估框架，这为图神经网络在复杂图上的性能提供了全面的解决方案。 |
| [^239] | [A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises.](http://arxiv.org/abs/2306.04802) | 本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。 |
| [^240] | [Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL.](http://arxiv.org/abs/2305.17342) | 本文介绍了另一种常见、现实的多智能体RL攻击设置，提出了一种模拟攻击者对代理$\alpha$控制的更一般化攻击形式。并解决了先前攻击模型中缺乏可证明防御的问题。 |
| [^241] | [Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II.](http://arxiv.org/abs/2305.17282) | 本文研究了k近邻学习规则中的普遍一致性，发现在可分度量空间中，该规则在Nagata维度下的sigma有限维度的空间中是普遍一致的，在非阿基米德度量空间中是强普遍一致的，此规则在具有de Groot有限维度意义下的度量空间和Heisenberg群中也是普遍一致的。 |
| [^242] | [Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders.](http://arxiv.org/abs/2305.16189) | 该论文提出了一种因子高斯混合变分自动编码器，用于多尺度聚类和源分离，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程，并在MRO数据集上展现了更好的性能。 |
| [^243] | [Having Beer after Prayer? Measuring Cultural Bias in Large Language Models.](http://arxiv.org/abs/2305.14456) | 这篇论文研究了大型语言模型在处理和生成阿拉伯文本时出现的文化偏向西方文化的现象，表明语言模型在人名、食品、服装、地点、文学、饮料、宗教和体育等八个文化方面存在偏见。这些发现引发对于当前语言模型文化相关性的担忧。 |
| [^244] | [Generative Sliced MMD Flows with Riesz Kernels.](http://arxiv.org/abs/2305.11463) | 本文使用Riesz核展示了生成式分割MMD流的高效计算方法，实现了在大规模应用中通过神经网络训练生成模型。 |
| [^245] | [Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning.](http://arxiv.org/abs/2305.08014) | 本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。 |
| [^246] | [A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning.](http://arxiv.org/abs/2305.03582) | 本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习，该方法在中间表示上进行了静态与动态信息、模态特异与共同信息的分离，并且在实验中表现出优越性。 |
| [^247] | [Stabilizing the Maximal Entropy Moment Method for Rarefied Gas Dynamics at Single-Precision.](http://arxiv.org/abs/2303.02898) | 本文的创新在于将最大熵时刻法稳定应用于单精度下的稀疏气体动力学，使其能够在现代GPU上模拟非常强的正常激波。 |
| [^248] | [A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization.](http://arxiv.org/abs/2302.08766) | 该论文提出了一种双层经验风险最小化算法，使用的梯度计算次数 $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$，在样本复杂度方面是最优的。 |
| [^249] | [Learning Large Causal Structures from Inverse Covariance Matrix via Matrix Decomposition.](http://arxiv.org/abs/2211.14221) | 本文提出了一种基于逆协方差矩阵的$\mathcal{O}$-ICID方法，该方法是通过连续优化一种矩阵分解来学习因果结构的，适用于变量数量庞大的情况。该方法可以在噪声方差已知时识别真实DAG, 也可以在较弱的先验信息下给出有用的定向图解 |
| [^250] | [Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee.](http://arxiv.org/abs/2206.10477) | Survival Kernets 是一种可扩展且可解释的深度核生存分析模型，能够在大规模数据集上进行模型解释和理论分析。它利用核函数估计个体的生存分布，通过训练集压缩方案进行数据分簇，因此具有较高的可视化能力和预测准确性保证。该模型在特定情况下的预测生存分布误差界限最优，且在测试时具有可扩展性。 |
| [^251] | [Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning.](http://arxiv.org/abs/2109.03445) | 本文提出了一种批量异步随机逼近算法，它可以在内存需求和时间复杂度之间进行权衡，同时提供了可以使用较弱假设证明收敛的一般方法；在强化学习领域，我们使用此方法证明了SARSA算法的批量异步版本的收敛性。 |
| [^252] | [Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation.](http://arxiv.org/abs/2106.03907) | 本论文提出了一种深度代理因果学习（PCL）方法，用于在存在混淆因素的情况下估计治疗对结果的因果效应。通过构建治疗和代理之间的模型，并利用该模型在给定代理的情况下学习治疗对结果的影响，PCL可以保证恢复真实的因果效应。作者还提出了一种名为深度特征代理变量方法（DFPV）的新方法，用于处理高维和非线性复杂关系的情况，并表明DFPV在合成基准测试中的性能优于最先进的PCL方法。 |

# 详细

[^1]: 具有必要回溯的自然反事实

    Natural Counterfactuals With Necessary Backtracking

    [https://rss.arxiv.org/abs/2402.01607](https://rss.arxiv.org/abs/2402.01607)

    本研究提出了一种自然反事实框架和方法，通过优化控制回溯的范围，生成与实际世界的数据分布相匹配的自然反事实，从而改进了反事实推理。

    

    反事实推理对于人类认知非常重要，尤其对于提供解释和做出决策至关重要。尽管Judea Pearl的研究方法在理论上很优雅，但其生成反事实情景往往需要过于脱离实际情景的干预，因此难以实施。为了解决这个问题，我们提出了一种自然反事实的框架和一种根据实际世界数据分布生成自然反事实的方法。我们的方法提供了对反事实推理的改进，允许对因果前置变量进行改变以最小化与实际情景的偏差。为了生成自然反事实，我们引入了一种创新的优化框架，通过自然性准则允许但控制回溯的范围。实证实验表明了我们方法的有效性。

    Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
    
[^2]: CounterCurate: 通过对照例子增强物理和语义视觉-语言组合推理能力

    CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples

    [https://arxiv.org/abs/2402.13254](https://arxiv.org/abs/2402.13254)

    本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。

    

    我们提出CounterCurate，一个框架，全面提升对比和生成式多模态模型的视觉-语言组合推理能力。特别地，我们确定了两个尚未充分探讨的关键问题：忽视了基于物理的推理（计数和位置理解），以及利用高性能文本和图像生成模型进行语义反事实微调的潜力。我们的工作开创了一个解决这些空白的方法。我们首先突出了多模态模型（如CLIP和LLaVA）在基于物理的组合推理中几乎无法胜任的表现。然后，我们应用简单的数据增强，使用基于图像的生成模型GLIGEN生成微调数据，使得性能显著提高：在我们新的策划的Flickr30k-Positions基准测试中，CLIP和LLaVA的性能分别提高了+33%和+37%。此外，我们利用了高性能文本和图像生成模型的能力。

    arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
    
[^3]: FlashTex：具有LightControlNet的快速可重塑网格纹理

    FlashTex: Fast Relightable Mesh Texturing with LightControlNet

    [https://arxiv.org/abs/2402.13251](https://arxiv.org/abs/2402.13251)

    提出了FlashTex方法，基于LightControlNet实现了快速自动化3D网格纹理生成，实现了照明与表面材质的解耦，使得网格能够在任何照明环境下正确重照和渲染

    

    手动为3D网格创建纹理费时费力，即使对于专家视觉内容创建者也是如此。我们提出了一种快速方法，根据用户提供的文本提示自动为输入的3D网格着色。重要的是，我们的方法将照明与表面材质/反射在生成的纹理中解耦，以便网格可以在任何照明环境中正确重照和渲染。我们引入了LightControlNet，这是一种基于ControlNet架构的新文本到图像模型，允许将所需照明规格作为对模型的条件图像。我们的文本到纹理管道然后分两个阶段构建纹理。第一阶段使用LightControlNet生成网格的一组稀疏的视觉一致的参考视图。第二阶段应用基于分数蒸馏采样（SDS）的纹理优化，通过LightControlNet来提高纹理质量同时解耦表面材质

    arXiv:2402.13251v1 Announce Type: cross  Abstract: Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surf
    
[^4]: 来自异构数据的联邦因果发现

    Federated Causal Discovery from Heterogeneous Data

    [https://arxiv.org/abs/2402.13241](https://arxiv.org/abs/2402.13241)

    该研究提出了一种新型联邦因果发现方法，旨在适应任意因果模型和异构数据，通过使用替代变量和联邦条件独立性检验来解决数据异质性，并建立了联邦独立变化原则用于确定因果方向。

    

    传统的因果发现方法依赖于集中式数据，这与许多实际情况下数据的分散性质不一致。这种差异推动了联邦因果发现（FCD）方法的发展。然而，现有的FCD方法可能受到其对可识别功能因果模型或 homogeneous数据分布的潜在限制，从而限制了它们在各种场景中的适用性。在本文中，我们提出了一种尝试适应任意因果模型和异构数据的新型FCD方法。我们首先利用与客户端索引对应的替代变量，以解决不同客户端之间的数据异质性。然后我们开发了一个用于因果骨架发现的联邦条件独立性检验（FCIT），并建立了一个用于确定因果方向的联邦独立变化原则（FICP）。这些方法涉及构建

    arXiv:2402.13241v1 Announce Type: cross  Abstract: Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing
    
[^5]: 基于相似性的高维域自适应算法用于多传感器时间序列分类

    SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification

    [https://arxiv.org/abs/2402.13233](https://arxiv.org/abs/2402.13233)

    提出了SMORE，一种新颖的多传感器时间序列分类领域自适应算法，利用高维计算的高效和并行操作，动态定制测试模型以减轻数据分布偏移带来的性能下降。

    

    许多物联网(IoT)的实际应用利用机器学习(ML)算法分析由相互连接的传感器收集的时间序列信息。然而，在部署在与训练数据不同的数据分布上的模型时，数据驱动的ML中固有的挑战——分布偏移会显著降低模型性能。此外，越来越复杂的深度神经网络(DNNs)需要捕获多传感器时间序列数据中复杂的空间和时间依赖关系，往往超过了今天边缘设备的能力。 在本文中，我们提出了SMORE，一种新颖的资源高效的多传感器时间序列分类领域自适应(DA)算法，利用了超高维计算的高效和并行操作。 SMORE动态地定制测试模型，明确考虑每个样本的领域上下文，以减轻

    arXiv:2402.13233v1 Announce Type: new  Abstract: Many real-world applications of the Internet of Things (IoT) employ machine learning (ML) algorithms to analyze time series information collected by interconnected sensors. However, distribution shift, a fundamental challenge in data-driven ML, arises when a model is deployed on a data distribution different from the training data and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) are required to capture intricate spatial and temporal dependencies in multi-sensor time series data, often exceeding the capabilities of today's edge devices. In this paper, we propose SMORE, a novel resource-efficient domain adaptation (DA) algorithm for multi-sensor time series classification, leveraging the efficient and parallel operations of hyperdimensional computing. SMORE dynamically customizes test-time models with explicit consideration of the domain context of each sample to mitigate
    
[^6]: Smaug：使用DPO-Positive修复偏好优化的失败模式

    Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive

    [https://arxiv.org/abs/2402.13228](https://arxiv.org/abs/2402.13228)

    在这项工作中，我们提出了一种新的损失函数和训练过程DPO-Positive（DPOP），以避免直接偏好优化（DPO）中潜在的失败模式，并发现DPOP明显优于DPO。

    

    直接偏好优化（DPO）在显著改善大型语言模型（LLMs）在推理、总结和对齐等下游任务上的性能方面是有效的。 DPO使用首选和非首选数据对模型选择一个响应而不是另一个的“相对”概率进行建模。在这项工作中，我们首先从理论上表明，只要首选和非首选类别之间的相对概率增加，标准DPO损失就可能导致模型对首选示例的可能性降低。然后，我们在实证上展示了当在常见数据集上微调LLMs时，尤其是在完成之间的编辑距离较短的数据集上，会出现这种现象。利用这些见解，我们设计了DPO-Positive（DPOP），一种新的损失函数和训练过程，避免了这种失败模式。令人惊讶的是，我们还发现DPOP明显优于DPO。

    arXiv:2402.13228v1 Announce Type: cross  Abstract: Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO a
    
[^7]: 通过用户行为建模和随机规划控制大型电动汽车充电站

    Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming

    [https://arxiv.org/abs/2402.13224](https://arxiv.org/abs/2402.13224)

    本文介绍了一个新的电动汽车充电站模型，通过用户行为建模和随机规划，解决了充电会话不确定性问题，并提出了两种方法来优化成本并提高用户满意度。

    

    本文介绍了一个电动汽车充电站（EVCS）模型，该模型融合了真实世界的约束条件，如插槽功率限制、合同阈值超限惩罚以及电动汽车（EVs）的早期断开。我们提出了一个在不确定性下控制EVCS的问题形式，并实施了两种多阶段随机规划方法，利用用户提供的信息，即模型预测控制和二阶段随机规划。该模型解决了充电会话开始和结束时间以及能量需求的不确定性。基于驻留时间依赖随机过程的用户行为模型增强了成本降低的同时保持客户满意度。通过使用真实世界数据集进行的22天模拟展示了两种提出方法相对于两个基线的优势。两阶段方法证明了针对早期断开的鲁棒性，考虑了更多

    arXiv:2402.13224v1 Announce Type: cross  Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more
    
[^8]: CHILI: 用于推进图机器学习的化学信息的大型无机纳米材料数据集

    CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning

    [https://arxiv.org/abs/2402.13221](https://arxiv.org/abs/2402.13221)

    图机器学习领域目前主要集中在预测分子和材料的目标特性，而尚未达到生成能力与其他领域的水平。

    

    图机器学习的进展主要受化学应用的驱动，因为图一直是分子最具表现力的表示形式。虽然早期的图机器学习方法主要集中在小有机分子上，但最近，图机器学习的范围已经扩展到包括无机材料。建模无机晶体材料的周期性和对称性带来独特挑战，现有的图机器学习方法无法解决。转向无机纳米材料会增加复杂性，因为每个图中节点数量的范围可能很广（$10$到$10^5$）。现有图机器学习的主要重点是通过图作为输入来预测目标特性，来表征分子和材料。但是，图机器学习最激动人心的应用将在其生成能力方面，目前与图像或文本等其他领域还不在同一水平。

    arXiv:2402.13221v1 Announce Type: new  Abstract: Advances in graph machine learning (ML) have been driven by applications in chemistry as graphs have remained the most expressive representations of molecules. While early graph ML methods focused primarily on small organic molecules, recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to address. Moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. However, the most exciting applications of graph ML will be in their generative capabilities, which is currently not at par with other domains such as images or text.   We invite the graph ML community to address th
    
[^9]: 分析操作员状态和AI增强决策支持对控制室的影响：一种人在回路中的专门强化学习框架用于干预策略

    Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies

    [https://arxiv.org/abs/2402.13219](https://arxiv.org/abs/2402.13219)

    本研究提出了一种专门强化学习框架，集成了AI决策支持系统，旨在改善控制室操作员的工作效率和情境意识，并为其提供根据系统和人类表现状态量身定制的干预策略。

    

    在复杂的工业和化学过程控制室中，有效的决策对安全性和效率至关重要。本文中的实验评估了集成到改进的人机界面中的AI决策支持系统的影响和应用，使用动态影响图、隐马尔科夫模型和深度强化学习。增强的支持系统旨在减少操作员的工作负荷，提高情境意识，并根据系统和人类表现的当前状态为操作员提供不同的干预策略。这样的系统在信息过载的情况下特别有用，当许多警报和输入同时呈现在同一个时间窗口内，或者在培训期间对初级操作员来说尤其有用。进行了广泛的交叉数据分析，涉及47名参与者和各种数据来源，如智能手表指标、眼动数据等。

    arXiv:2402.13219v1 Announce Type: new  Abstract: In complex industrial and chemical process control rooms, effective decision-making is crucial for safety and effi- ciency. The experiments in this paper evaluate the impact and applications of an AI-based decision support system integrated into an improved human-machine interface, using dynamic influ- ence diagrams, a hidden Markov model, and deep reinforcement learning. The enhanced support system aims to reduce operator workload, improve situational awareness, and provide different intervention strategies to the operator adapted to the current state of both the system and human performance. Such a system can be particularly useful in cases of information overload when many alarms and inputs are presented all within the same time window, or for junior operators during training. A comprehensive cross-data analysis was conducted, involving 47 participants and a diverse range of data sources such as smartwatch metrics, eye- tracking data,
    
[^10]: 软最大概率（大部分时候）在多项选择问答任务中预测大型语言模型的正确性

    Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A

    [https://arxiv.org/abs/2402.13213](https://arxiv.org/abs/2402.13213)

    多项选择问答任务中，基于最大softmax概率（MSPs）的模型预测方法有助于提高大型语言模型（LLMs）的正确性，我们提出了一种根据MSP有选择地弃权的策略以提高性能。

    

    尽管大型语言模型（LLMs）在许多任务上表现出色，但过度自信仍然是一个问题。我们假设在多项选择问答任务中，错误答案将与最大softmax概率（MSPs）较小相关，相比之下正确答案较大。我们在十个开源LLMs和五个数据集上全面评估了这一假设，在表现良好的原始问答任务中发现了对我们假设的强有力证据。对于表现最佳的六个LLMs，从MSP导出的AUROC在59/60个实例中都优于随机机会，p < 10^{-4}。在这六个LLMs中，平均AUROC范围在60%至69%之间。利用这些发现，我们提出了一个带有弃权选项的多项选择问答任务，并展示通过根据初始模型响应的MSP有选择地弃权可以提高性能。我们还用预softmax logits而不是softmax进行了相同的实验。

    arXiv:2402.13213v1 Announce Type: cross  Abstract: Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&A task. For the six LLMs with the best Q&A performance, the AUROC derived from the MSP was better than random chance with p < 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of sof
    
[^11]: 软自一致性改善语言模型代理

    Soft Self-Consistency Improves Language Model Agents

    [https://arxiv.org/abs/2402.13212](https://arxiv.org/abs/2402.13212)

    Soft Self-Consistency (Soft-SC)通过软化评分标准替代自一致性的多数投票方法，提高了在涉及生成多个动作的长期互动任务中的性能和效率

    

    大型语言模型（LLMs）生成可以通过对多个解决方案进行抽样和评分来改进，以选择最终答案。当前的“抽样和选择”方法如自一致性（SC）依赖于多数投票来评分答案。然而，当任务有许多不同且有效的答案时，通过投票进行选择需要大量样本。这使得SC在涉及顺序生成多个动作（答案）的互动任务时成本过高。在确定大多数投票未能为此类任务提供一致的收益之后，我们展示了如何通过软化评分标准来提高成功率。我们引入了软自一致性（Soft-SC），它用模型可能性计算连续分数来取代SC的不连续评分，即使动作分布稀疏，也允许选择。软自一致性在长期互动任务上提高了性能和效率，需要较少的样本和投票。

    arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi
    
[^12]: Bayesian Reward Models for LLM Alignment

    Bayesian Reward Models for LLM Alignment

    [https://arxiv.org/abs/2402.13210](https://arxiv.org/abs/2402.13210)

    通过训练贝叶斯奖励模型，可以成功缓解奖励的过度优化问题。

    

    为了确保大型语言模型（LLM）的回复有益且无毒，通常我们会在人类偏好数据上微调奖励模型。然后我们选择具有高奖励的策略回复（best-of-n抽样），或者进一步优化策略以生成具有高奖励的回复（从人类反馈中进行强化学习）。然而，这个过程容易受到奖励过度优化或攻击的影响，选定的回复由于奖励模型中的错误而具有高奖励，而不是真实偏好。这一问题在提示或回复偏离训练数据时尤为严重。我们通过训练贝叶斯奖励模型来缓解这些问题，这种模型在远离训练数据分布时会产生更高的不确定性。因此，我们使用Laplace-LoRA（Yang等人，2024）训练了贝叶斯奖励模型，发现由此产生的不确定性估计可以成功缓解奖励的过度优化。

    arXiv:2402.13210v1 Announce Type: new  Abstract: To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback). However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimi
    
[^13]: SONATA：面向硬件感知神经架构搜索的自适应进化框架

    SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search

    [https://arxiv.org/abs/2402.13204](https://arxiv.org/abs/2402.13204)

    SONATA提出了一种自适应进化框架，用于硬件感知神经架构搜索，旨在解决神经网络设计参数与硬件感知NAS优化目标之间关系的挑战，并利用评估数据改进优化策略。

    

    人工智能（AI）的最新进展，由神经网络（NN）推动，要求创新的神经架构设计，特别是在物联网（IoT）系统的受限环境中，以平衡性能和效率。硬件感知神经架构搜索（HW-aware NAS）成为一种有吸引力的策略，通过多目标优化方法，如进化算法，自动化设计NN。然而，NN设计参数与HW-aware NAS优化目标之间的复杂关系仍然是一个不足探索的研究领域，忽视了有效利用这一知识来相应地指导搜索过程的机会。此外，搜索过程中产生的大量评估数据具有未开发的潜力，可用于优化策略和改进帕累托前沿的逼近。针对这些问题，我们提出SONATA，一个自适

    arXiv:2402.13204v1 Announce Type: cross  Abstract: Recent advancements in Artificial Intelligence (AI), driven by Neural Networks (NN), demand innovative neural architecture designs, particularly within the constrained environments of Internet of Things (IoT) systems, to balance performance and efficiency. HW-aware Neural Architecture Search (HW-aware NAS) emerges as an attractive strategy to automate the design of NN using multi-objective optimization approaches, such as evolutionary algorithms. However, the intricate relationship between NN design parameters and HW-aware NAS optimization objectives remains an underexplored research area, overlooking opportunities to effectively leverage this knowledge to guide the search process accordingly. Furthermore, the large amount of evaluation data produced during the search holds untapped potential for refining the optimization strategy and improving the approximation of the Pareto front. Addressing these issues, we propose SONATA, a self-ad
    
[^14]: 使用决策变换器进行四足动作的小型强化学习

    Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers

    [https://arxiv.org/abs/2402.13201](https://arxiv.org/abs/2402.13201)

    将模仿学习问题视为条件序列建模任务，通过训练决策变换器并使用奖励机制，将生成模型压缩以适应资源受限的机器人平台。

    

    arXiv:2402.13201v1 公告类型：跨领域 摘要：受资源限制的机器人平台特别适用于需要低成本硬件替代方案的任务，因为存在失去机器人的风险，比如在搜救应用中，或者需要大量设备，比如在群体机器人技术中。因此，关键在于找到机制，使强化学习技术适应这些超低成本机器人平台的低计算能力和较小内存容量所施加的约束。我们试图通过提出一种方法，将模仿学习部署到受资源限制的机器人平台上来满足这种需求。在这里，我们将模仿学习问题视为条件序列建模任务，通过使用附加了自定义奖励的专家演示来训练决策变换器。然后，我们使用软件优化方案，包括量化和修剪，来压缩生成的模型。我们在...

    arXiv:2402.13201v1 Announce Type: cross  Abstract: Resource-constrained robotic platforms are particularly useful for tasks that require low-cost hardware alternatives due to the risk of losing the robot, like in search-and-rescue applications, or the need for a large number of devices, like in swarm robotics. For this reason, it is crucial to find mechanisms for adapting reinforcement learning techniques to the constraints imposed by lower computational power and smaller memory capacities of these ultra low-cost robotic platforms. We try to address this need by proposing a method for making imitation learning deployable onto resource-constrained robotic platforms. Here we cast the imitation learning problem as a conditional sequence modeling task and we train a decision transformer using expert demonstrations augmented with a custom reward. Then, we compress the resulting generative model using software optimization schemes, including quantization and pruning. We test our method in si
    
[^15]: 实用的条件独立性核测试

    Practical Kernel Tests of Conditional Independence

    [https://arxiv.org/abs/2402.13196](https://arxiv.org/abs/2402.13196)

    提出了一种数据高效、基于核的方法，用于测试条件独立性，并提出了三种偏差控制方法来纠正测试水平。

    

    我们描述了一种数据高效、基于核的统计测试方法，用于测试条件独立性。条件独立性测试的一个主要挑战是获得正确的测试水平（指定的错误阳性率上限），同时仍具有竞争力的测试能力。过多的假阳性是由于测试统计量中的偏差引起的，该统计量是使用非参数核岭回归获得的。我们提出了三种偏差控制方法来修正测试水平，基于数据分割、辅助数据，以及（在可能的情况下）更简单的函数类。我们展示了这些组合策略在合成数据和真实世界数据中的有效性。

    arXiv:2402.13196v1 Announce Type: new  Abstract: We describe a data-efficient, kernel-based approach to statistical testing of conditional independence. A major challenge of conditional independence testing, absent in tests of unconditional independence, is to obtain the correct test level (the specified upper bound on the rate of false positives), while still attaining competitive test power. Excess false positives arise due to bias in the test statistic, which is obtained using nonparametric kernel ridge regression. We propose three methods for bias control to correct the test level, based on data splitting, auxiliary data, and (where possible) simpler function classes. We show these combined strategies are effective both for synthetic and real-world data.
    
[^16]: 在次线性时间内测试校准性

    Testing Calibration in Subquadratic Time

    [https://arxiv.org/abs/2402.13187](https://arxiv.org/abs/2402.13187)

    该论文通过属性测试算法方面的研究，提出了一种基于近似线性规划的算法，可以在信息理论上最优地解决校准性测试问题（最多一个常数倍数）。

    

    在最近的机器学习和决策制定文献中，校准性已经成为二元预测模型输出的一个值得期望和广泛研究的统计性质。然而，测量模型校准性的算法方面仍然相对较少被探索。在论文 [BGHN23] 的启发下，该论文提出了一个严格的框架来衡量到校准性的距离，我们通过属性测试的视角引入了校准性研究的算法方面。我们定义了从样本中进行校准性测试的问题，其中从分布 $\mathcal{D}$（预测，二元结果）中给出 $n$ 次抽样，我们的目标是区分 $\mathcal{D}$ 完全校准和 $\mathcal{D}$ 距离校准性为 $\varepsilon$ 的情况。

    arXiv:2402.13187v1 Announce Type: new  Abstract: In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\mathcal{D}$ is perfectly calibrated, and the case where $\mathcal{D}$ is $\varepsilon$-far from calibration.   We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factor
    
[^17]: 使用共享随机性的均匀采样实现分布式核赌博机中的最优次序遗憾

    Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness

    [https://arxiv.org/abs/2402.13182](https://arxiv.org/abs/2402.13182)

    提出的算法在分布式核赌博机问题中实现了最优次序遗憾，并且通信成本对于代理数量和时间都是亚线性的。

    

    我们考虑分布式核赌博机问题，其中$N$个代理旨在协同最大化存在于再生核希尔伯特空间中的未知奖励函数。每个代理依次查询该函数，以在查询点处获得嘈杂的观测值。代理可以通过中央服务器共享信息，目的是最小化随着时间$T$累积并汇总在代理之间的遗憾。我们开发了第一个算法，该算法实现了在通信成本对$N$和$T$均为亚线性的情况下实现了最优遗憾次序（由集中式学习定义）。所提出算法的关键特点是局部代理的均匀探索和与中央服务器的共享随机性。与GP模型的稀疏逼近一起工作，这两个关键组件使得能够以通信的衰减速率保持中央设置的学习速率。

    arXiv:2402.13182v1 Announce Type: new  Abstract: We consider distributed kernel bandits where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents. We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.
    
[^18]: DINOBot：通过视觉基础模型的检索和对齐实现机器人操作

    DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models

    [https://arxiv.org/abs/2402.13181](https://arxiv.org/abs/2402.13181)

    DINOBot利用Vision Transformers的特征，实现了通过检索和对齐来进行机器人操作，提高了学习效率和泛化能力

    

    我们提出了DINOBot，这是一个新颖的模仿学习框架，用于机器人操作，它利用了从使用DINO训练的Vision Transformers提取的特征的图像级和像素级能力。与新对象交互时，DINOBot首先使用这些特征来检索在人类演示中经历过的最相似的对象，然后使用该对象来将其末端执行器与新对象对齐，以实现有效的交互。通过一系列在日常任务中的真实世界实验，我们展示利用视觉基础模型的图像级和像素级属性能够实现前所未有的学习效率和泛化能力。视频和代码可在https://www.robot-learning.uk/dinobot找到。

    arXiv:2402.13181v1 Announce Type: cross  Abstract: We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its end-effector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot.
    
[^19]: 通过上下文对抗游戏防御越狱提示

    Defending Jailbreak Prompts via In-Context Adversarial Game

    [https://arxiv.org/abs/2402.13148](https://arxiv.org/abs/2402.13148)

    介绍了一种通过上下文对抗游戏(ICAG)防御越狱提示的方法，能够显著降低成功率。

    

    大语言模型(LLMs)展现出在不同应用领域中的显著能力。然而，对其安全性的担忧，特别是对越狱攻击的脆弱性，仍然存在。受到深度学习中对抗训练和LLM代理学习过程的启发，我们引入了上下文对抗游戏(ICAG)来防御越狱攻击，无需进行微调。ICAG利用代理学习进行对抗游戏，旨在动态扩展知识以防御越狱攻击。与依赖静态数据集的传统方法不同，ICAG采用迭代过程来增强防御和攻击代理。这一持续改进过程加强了对新生成的越狱提示的防御。我们的实证研究证实了ICAG的有效性，经由ICAG保护的LLMs在各种攻击场景中显著降低了越狱成功率。

    arXiv:2402.13148v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Mo
    
[^20]: SubIQ: 逆向软 Q 学习用于获得次优演示的离线模仿

    SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations

    [https://arxiv.org/abs/2402.13147](https://arxiv.org/abs/2402.13147)

    逆向软 Q 学习用于获得次优演示的离线模仿挑战了离线 IL 中有限支持专家演示的问题，并提出了一种解决方案以匹配次优演示集合的占用分布

    

    我们考虑了离线模仿学习（IL），旨在从专家演示中模仿专家的行为，而无需与环境进行进一步交互。在离线 IL 中的一个主要挑战是处理仅涵盖状态-动作空间的一小部分的专家演示的有限支持。我们考虑离线 IL，其中专家演示受到限制，但是由更大规模的次优演示集合补充。大部分现有的用于此设置的离线 IL 方法基于行为克隆或分布匹配，其目的是将模仿策略的占用分布与专家策略的占用分布匹配。这种方法往往存在过拟合问题，因为专家演示有限，无法准确表示任何占用分布。另一方面，由于次优演示集合规模更大，有很高的可能性

    arXiv:2402.13147v1 Announce Type: cross  Abstract: We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that 
    
[^21]: 神经网络扩散

    Neural Network Diffusion

    [https://arxiv.org/abs/2402.13144](https://arxiv.org/abs/2402.13144)

    扩散模型能够生成表现优异的神经网络参数，生成的模型在性能上与训练网络相媲美甚至更好，且成本极低。

    

    扩散模型在图像和视频生成方面取得了显著成功。在这项工作中，我们展示了扩散模型也可以\textit{生成表现优异的神经网络参数}。我们的方法很简单，利用了自动编码器和标准的潜在扩散模型。自动编码器提取了部分受训网络参数的潜在表示。然后训练了一个扩散模型来从随机噪声中合成这些潜在参数表示。它生成了新的表示，经过自动编码器的解码器，输出准备用作新的网络参数子集。在各种架构和数据集上，我们的扩散过程始终生成性能与经过训练的网络相当或更好的模型，附加成本极小。值得注意的是，我们在实证研究中发现，生成的模型与经过训练的网络表现出差异。

    arXiv:2402.13144v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results en
    
[^22]: VGMShield：缓解视频生成模型的误用

    VGMShield: Mitigating Misuse of Video Generative Models

    [https://arxiv.org/abs/2402.13126](https://arxiv.org/abs/2402.13126)

    VGMShield提出了三项简单但开创性的措施，通过检测虚假视频、溯源问题和利用预训练的空间-时间动态模型，防范视频生成模型的误用。

    

    随着视频生成技术的快速发展，人们可以方便地利用视频生成模型创建符合其特定需求的视频。然而，人们也越来越担心这些技术被用于创作和传播虚假信息。在这项工作中，我们介绍了VGMShield：一套包含三项直接但开创性的措施，用于防范虚假视频生成过程中可能出现的问题。我们首先从“虚假视频检测”开始，尝试理解生成的视频中是否存在独特性，以及我们是否能够区分它们与真实视频的不同；然后，我们探讨“溯源”问题，即将一段虚假视频追溯回生成它的模型。为此，我们提出利用预训练的关注“时空动态”的模型作为骨干，以识别视频中的不一致性。通过对七个最先进的开源模型进行实验，我们证明了...

    arXiv:2402.13126v1 Announce Type: cross  Abstract: With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that
    
[^23]: BuffGraph: 通过缓冲节点增强节点分类的不平衡类别问题

    BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes

    [https://arxiv.org/abs/2402.13114](https://arxiv.org/abs/2402.13114)

    BuffGraph通过插入缓冲节点到图中，调节主要类别的影响，以改善次要类别的表示，在类别不平衡的节点分类问题上表现优越。

    

    图结构数据中的类别不平衡，即次要类别明显代表不足，对图神经网络（GNNs）构成了重大挑战。为了应对这一挑战，现有研究通常生成新的少数节点，并连接新节点到原始图中，使得类别平衡。然而，它们并未解决主要类别通过原始图中的边向次要节点传播信息的问题，从而引入了对主要类别的偏见。为了解决这个问题，我们引入了BuffGraph，将缓冲节点插入图中，调节主要类别的影响，以改善次要类别的表示。我们在各种真实世界数据集上进行了大量实验证明，BuffGraph在自然设置和不平衡设置中的类别不平衡节点分类中优于现有基线方法。代码可在https://anonymous.4open.scien获得。

    arXiv:2402.13114v1 Announce Type: cross  Abstract: Class imbalance in graph-structured data, where minor classes are significantly underrepresented, poses a critical challenge for Graph Neural Networks (GNNs). To address this challenge, existing studies generally generate new minority nodes and edges connecting new nodes to the original graph to make classes balanced. However, they do not solve the problem that majority classes still propagate information to minority nodes by edges in the original graph which introduces bias towards majority classes. To address this, we introduce BuffGraph, which inserts buffer nodes into the graph, modulating the impact of majority classes to improve minor class representation. Our extensive experiments across diverse real-world datasets empirically demonstrate that BuffGraph outperforms existing baseline methods in class-imbalanced node classification in both natural settings and imbalanced settings. Code is available at https://anonymous.4open.scien
    
[^24]: 关于大学习率下梯度下降的稳定性

    On the Stability of Gradient Descent for Large Learning Rate

    [https://arxiv.org/abs/2402.13108](https://arxiv.org/abs/2402.13108)

    本文研究了线性神经网络在二次损失函数下的优化问题，证明了梯度下降映射的非奇异性以及全局最小值点集的光滑流形特性，为理解大学习率下梯度下降的稳定性提供了重要线索。

    

    目前对理解“稳定性边缘（EoS）”现象存在着相当大的兴趣，这一现象在神经网络训练中被观察到，其特点是损失函数在不同纪元间的非单调下降，而损失的陡峭度（Hessian的谱范数）逐渐接近并稳定在2/(学习率)附近。最近有人提出了使用梯度下降训练时出现EoS的原因——沿梯度下降轨迹附近缺乏平坦的极小值点，同时存在紧致的正向不变集。在本文中，我们证明了在二次损失函数下优化的线性神经网络满足第一个假设以及第二个假设的一个必要条件。更具体地，我们证明了梯度下降映射是非奇异的，损失函数的全局最小值点集构成一个光滑流形，并且稳定的极小值构成有界子集。

    arXiv:2402.13108v1 Announce Type: new  Abstract: There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate). Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets. In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption. More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset i
    
[^25]: 关于深度复合高斯神经网络的泛化界限

    On Generalization Bounds for Deep Compound Gaussian Neural Networks

    [https://arxiv.org/abs/2402.13106](https://arxiv.org/abs/2402.13106)

    本文针对受复合高斯先验启发的展开DNN，提出了新颖的泛化误差界限，这些网络在压缩感知和层析成像问题中表现出色。

    

    算法展开或滚动是一种从迭代算法构建深度神经网络（DNN）的技术。展开的DNN在信号估计任务中通常比标准DNN提供更好的可解释性和更优越的经验性能。一个重要的理论问题是最近才引起关注的是为展开的DNN开发泛化误差界限。这些界限提供了理论和实际洞察，说明了DNN在与生成DNN训练数据的概率密度不同但采样自其中的经验数据集上的表现。在本文中，我们为一类受复合高斯先验启发的展开DNN开发了新颖的泛化误差界限。已经显示这些复合高斯网络在压缩感知和层析成像问题中优于比较的标准和展开的深度神经网络。

    arXiv:2402.13106v1 Announce Type: cross  Abstract: Algorithm unfolding or unrolling is the technique of constructing a deep neural network (DNN) from an iterative algorithm. Unrolled DNNs often provide better interpretability and superior empirical performance over standard DNNs in signal estimation tasks. An important theoretical question, which has only recently received attention, is the development of generalization error bounds for unrolled DNNs. These bounds deliver theoretical and practical insights into the performance of a DNN on empirical datasets that are distinct from, but sampled from, the probability density generating the DNN training data. In this paper, we develop novel generalization error bounds for a class of unrolled DNNs that are informed by a compound Gaussian prior. These compound Gaussian networks have been shown to outperform comparative standard and unfolded deep neural networks in compressive sensing and tomographic imaging problems. The generalization error
    
[^26]: 多变量功能线性判别分析用于具有缺失数据的短时间序列分类

    Multivariate Functional Linear Discriminant Analysis for the Classification of Short Time Series with Missing Data

    [https://arxiv.org/abs/2402.13103](https://arxiv.org/abs/2402.13103)

    开发了多变量版本的FLDA（MUDRA）以应对缺失数据问题，并提出了一种高效的期望/条件极大化（ECM）算法来推断其参数，在分类性能上较现有技术取得了改进

    

    功能线性判别分析（FLDA）是一种强大的工具，将LDA实现的多类别分类和降维扩展到单变量时间序列函数。然而，在大规模多变量和不完整数据时代，必须以可计算的方式估计特征之间的统计依赖关系，同时处理缺失数据。我们开发了FLDA的多元版本（MUDRA）来解决这个问题，并描述了一种高效的期望/条件极大化（ECM）算法来推断其参数。我们评估了它在“Articulary Word Recognition”数据集上的预测能力，并展示了其在缺失数据情况下比现有技术的改进。MUDRA允许对具有大比例数据集的分类进行可解释性分析

    arXiv:2402.13103v1 Announce Type: new  Abstract: Functional linear discriminant analysis (FLDA) is a powerful tool that extends LDA-mediated multiclass classification and dimension reduction to univariate time-series functions. However, in the age of large multivariate and incomplete data, statistical dependencies between features must be estimated in a computationally tractable way, while also dealing with missing data. There is a need for a computationally tractable approach that considers the statistical dependencies between features and can handle missing values. We here develop a multivariate version of FLDA (MUDRA) to tackle this issue and describe an efficient expectation/conditional-maximization (ECM) algorithm to infer its parameters. We assess its predictive power on the "Articulary Word Recognition" data set and show its improvement over the state-of-the-art, especially in the case of missing data. MUDRA allows interpretable classification of data sets with large proportions
    
[^27]: 基于微结构的图神经网络用于加速多尺度模拟

    A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations

    [https://arxiv.org/abs/2402.13101](https://arxiv.org/abs/2402.13101)

    该研究提出了一种基于微结构的图神经网络代理模型，旨在加速多尺度模拟，将微观和宏观量预测和均质化，实现了在弹塑性材料中保持多尺度性质的目标。

    

    通过并发多尺度模型进行先进材料的力学响应模拟可以比单尺度模拟更准确。然而，计算成本阻碍了这种方法的实际应用。成本源自必须在每个宏观积分点求解的微观有限元（FE）模型。大量的代理建模策略试图通过学习从宏观应变到宏观应力的预测来减轻这一成本，完全替代微观模型。在这项工作中，我们引入了一种替代代理建模策略，允许保持问题的多尺度性质，可以与有限元求解器交替使用任何时间步长。我们的代理提供所有微观量，然后经均质化得到感兴趣的宏观量。我们通过预测一种弹塑性材料实现了这一点

    arXiv:2402.13101v1 Announce Type: new  Abstract: Simulating the mechanical response of advanced materials can be done more accurately using concurrent multiscale models than with single-scale simulations. However, the computational costs stand in the way of the practical application of this approach. The costs originate from microscale Finite Element (FE) models that must be solved at every macroscopic integration point. A plethora of surrogate modeling strategies attempt to alleviate this cost by learning to predict macroscopic stresses from macroscopic strains, completely replacing the microscale models. In this work, we introduce an alternative surrogate modeling strategy that allows for keeping the multiscale nature of the problem, allowing it to be used interchangeably with an FE solver for any time step. Our surrogate provides all microscopic quantities, which are then homogenized to obtain macroscopic quantities of interest. We achieve this for an elasto-plastic material by pred
    
[^28]: 朝向对MoE设计选择的实证理解

    Towards an empirical understanding of MoE design choices

    [https://arxiv.org/abs/2402.13089](https://arxiv.org/abs/2402.13089)

    本研究系统评估了Mixture of Experts（MoEs）中常见设计选择对验证性能的影响，揭示了路由器的学习与初始化对模型性能的比较、序列级路由与标记级路由在专家专业化方面的不同影响。

    

    在这项研究中，我们系统地评估了Mixture of Experts（MoEs）中常见设计选择对验证性能的影响，揭示了在标记和序列级别上的不同影响。我们还提供经验证据表明，在学习路由器和冻结的随机初始化路由器之间具有可比性的性能，这表明学习路由可能并非必不可少。我们的研究进一步揭示了序列级路由可能导致特定主题的弱专家专业化，与标记级别路由观察到的语法专业化形成对比。

    arXiv:2402.13089v1 Announce Type: cross  Abstract: In this study, we systematically evaluate the impact of common design choices in Mixture of Experts (MoEs) on validation performance, uncovering distinct influences at token and sequence levels. We also present empirical evidence showing comparable performance between a learned router and a frozen, randomly initialized router, suggesting that learned routing may not be essential. Our study further reveals that Sequence-level routing can result in topic-specific weak expert specialization, in contrast to syntax specialization observed with Token-level routing.
    
[^29]: 选择如何泄漏隐私：重新审视私有选择及超参数调整的改进结果

    How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning

    [https://arxiv.org/abs/2402.13087](https://arxiv.org/abs/2402.13087)

    本论文探讨了超参数调整中的隐私性问题，发现当前的隐私分析在一般情况下是紧密的，但在特定的超参数调整问题上则不再成立，并通过隐私审计揭示了当前理论隐私界与实证之间的显著差距。

    

    我们研究了在超参数调整中保证差分隐私(DP)的问题，这是机器学习中一个关键的过程，涉及从几个运行中选择最佳的过程。与许多私有算法（包括普遍存在的DP-SGD）不同，调整的隐私影响仍然不够了解。最近的研究提出了一个通用的私有解决方案用于调整过程，然而一个根本的问题仍然存在：当前解决方案的隐私界是否紧密？本文对这个问题提出了积极和消极的答案。最初，我们提供的研究证实了当前的隐私分析在一般意义上确实是紧密的。然而，当我们专门研究超参数调整问题时，这种紧密性则不再成立。首先，通过对调整过程进行隐私审计来证明了这一点。我们的研究结果突显了当前理论隐私界与实证之间存在重大差距。

    arXiv:2402.13087v1 Announce Type: new  Abstract: We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several. Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood. Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight?   This paper contributes both positive and negative answers to this question. Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense. However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds. This is first demonstrated by applying privacy audit on the tuning process. Our findings underscore a substantial gap between the current theoretical privacy bound and the empirica
    
[^30]: 使用统计学习和测试台测量进行IT入侵检测

    IT Intrusion Detection Using Statistical Learning and Testbed Measurements

    [https://arxiv.org/abs/2402.13081](https://arxiv.org/abs/2402.13081)

    该研究通过统计学习方法和基础设施连续测量数据，以及在内部测试台上进行攻击模拟，实现了IT基础设施中的自动入侵检测。

    

    我们研究了IT基础设施中的自动入侵检测，特别是识别攻击开始、攻击类型以及攻击者采取的动作顺序的问题，基于基础设施的连续测量。我们应用统计学习方法，包括隐马尔可夫模型（HMM）、长短期记忆（LSTM）和随机森林分类器（RFC），将观测序列映射到预测攻击动作序列。与大多数相关研究不同，我们拥有丰富的数据来训练模型并评估其预测能力。数据来自我们在内部测试台上生成的跟踪数据，在这里我们对模拟的IT基础设施进行攻击。我们工作的核心是一个机器学习管道，将来自高维观测空间的测量映射到低维空间或少量观测符号的空间。我们研究离线和在线入侵检测

    arXiv:2402.13081v1 Announce Type: new  Abstract: We study automated intrusion detection in an IT infrastructure, specifically the problem of identifying the start of an attack, the type of attack, and the sequence of actions an attacker takes, based on continuous measurements from the infrastructure. We apply statistical learning methods, including Hidden Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier (RFC) to map sequences of observations to sequences of predicted attack actions. In contrast to most related research, we have abundant data to train the models and evaluate their predictive power. The data comes from traces we generate on an in-house testbed where we run attacks against an emulated IT infrastructure. Central to our work is a machine-learning pipeline that maps measurements from a high-dimensional observation space to a space of low dimensionality or to a small set of observation symbols. Investigating intrusions in offline as well as onli
    
[^31]: 具有部分反馈的模态估计

    Mode Estimation with Partial Feedback

    [https://arxiv.org/abs/2402.13079](https://arxiv.org/abs/2402.13079)

    本文提出了一种在模态估计中利用部分反馈的方法，通过熵编码实现最优信息获取，开发了粗糙的充分统计用于模态识别，并将赌博算法调整为新设置，最终提出了一个高效的问题解决方案

    

    arXiv:2402.13079v1 通告类型: 交叉摘要: 最近人工智能发展中，轻度监督的预训练和在线微调的组合在起着关键作用。这些新的学习流程需要新的理论框架。本文通过一个简单问题，形式化弱监督和主动学习的核心方面：使用部分反馈估计分布的模态。我们展示了如何利用熵编码从部分反馈中实现最优信息获取，为模态识别开发了粗糙的充分统计，并将赌博算法调整为我们的新设置。最后，我们将这些贡献结合起来，提出了一个在统计上和计算上高效的问题解决方案。

    arXiv:2402.13079v1 Announce Type: cross  Abstract: The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.
    
[^32]: 用于科学机器学习的机制神经网络

    Mechanistic Neural Networks for Scientific Machine Learning

    [https://arxiv.org/abs/2402.13077](https://arxiv.org/abs/2402.13077)

    本文提出了一种名为机制神经网络的神经网络设计，通过在标准架构中引入新的机制模块，学习控制微分方程作为表示，提高数据建模的可解释性和效率，并借助一种新颖的松弛线性规划求解器实现可扩展的GPU并行处理。

    

    本文提出了一种名为机制神经网络的神经网络设计，用于科学机器学习应用。它在标准架构中引入了一个新的机制模块，明确地学习控制微分方程作为表示，揭示数据的基本动态，并增强了数据建模中的可解释性和效率。我们方法的核心是一种新颖的松弛线性规划求解器（NeuRLP），受一种将求解线性ODE转化为求解线性规划的技术启发。它与神经网络很好地集成，并超越了传统ODE求解器的局限，实现了可扩展的GPU并行处理。总体而言，机制神经网络展示了它们在科学机器学习应用中的多才多艺，能够灵活处理从方程发现到动态系统建模的任务。我们证明了它们在分析和解释复杂科学问题方面的全面能力。

    arXiv:2402.13077v1 Announce Type: cross  Abstract: This paper presents Mechanistic Neural Networks, a neural network design for machine learning applications in the sciences. It incorporates a new Mechanistic Block in standard architectures to explicitly learn governing differential equations as representations, revealing the underlying dynamics of data and enhancing interpretability and efficiency in data modeling. Central to our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by a technique that reduces solving linear ODEs to solving linear programs. This integrates well with neural networks and surpasses the limitations of traditional ODE solvers enabling scalable GPU parallel processing. Overall, Mechanistic Neural Networks demonstrate their versatility for scientific machine learning applications, adeptly managing tasks from equation discovery to dynamic systems modeling. We prove their comprehensive capabilities in analyzing and interpreting complex scient
    
[^33]: 不是所有的权重都是平等的: 在设备上增强能效的流式语音识别

    Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition

    [https://arxiv.org/abs/2402.13076](https://arxiv.org/abs/2402.13076)

    权重参数在设备上的流式语音识别模型中的功耗影响有所不同，作者提出了基于权重参数敏感性的有针对性压缩方法，将能源使用减少高达47%而维持模型准确性

    

    电力消耗在设备上的流式语音识别中起着重要作用，因为它直接影响用户体验。本研究深入探讨了语音识别模型中的权重参数如何影响这些模型的总体功耗。我们发现权重参数对功耗的影响因多种因素而异，受到调用频率及其在内存中的位置等因素的影响。凭借这一洞察力，我们制定了旨在优化设备上语音识别模型的设计指南。这些指南侧重于在尽量不显著影响准确性的情况下最小化功耗。我们的方法，基于权重参数变化敏感性的有针对性压缩，表现出优越性能，相比最先进的压缩方法，可以实现高达47%的能源使用减少，同时保持类似的模型准确性，并改善实时流

    arXiv:2402.13076v1 Announce Type: cross  Abstract: Power consumption plays an important role in on-device streaming speech recognition, as it has a direct impact on the user experience. This study delves into how weight parameters in speech recognition models influence the overall power consumption of these models. We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory. Armed with this insight, we developed design guidelines aimed at optimizing on-device speech recognition models. These guidelines focus on minimizing power use without substantially affecting accuracy. Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods. It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time f
    
[^34]: 用扩散语言模型进行文本引导的分子生成

    Text-Guided Molecule Generation with Diffusion Language Model

    [https://arxiv.org/abs/2402.13040](https://arxiv.org/abs/2402.13040)

    提出了一种名为Text-Guided Molecule Generation with Diffusion Language Model（TGM-DLM）的新方法，通过扩散模型进行文本引导的分子生成，在生成有效分子表示方面表现出显著的效果优于自回归模型MolT5-Base。

    

    文本引导的分子生成是一个任务，其中生成的分子与特定的文本描述相匹配。最近，大多数现有基于SMILES的分子生成方法依赖于自回归架构。在这项工作中，我们提出了一种名为Text-Guided Molecule Generation with Diffusion Language Model（TGM-DLM）的新方法，它利用扩散模型来解决自回归方法的局限性。TGM-DLM集体和迭代地更新SMILES字符串中的标记嵌入，使用两阶段扩散生成过程。第一阶段通过随机噪声从文本描述中引导来优化嵌入，而第二阶段纠正无效的SMILES字符串以形成有效的分子表示。我们证明，TGM-DLM在不需要额外数据资源的情况下，胜过了自回归模型MolT5-Base。我们的研究结果强调了TGM-DLM在生成连贯性方面的显著有效性。

    arXiv:2402.13040v1 Announce Type: cross  Abstract: Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating cohe
    
[^35]: 对齐您的意图：通过最优传输的离线模仿学习

    Align Your Intents: Offline Imitation Learning via Optimal Transport

    [https://arxiv.org/abs/2402.13037](https://arxiv.org/abs/2402.13037)

    通过最优传输的离线模仿学习方法AILOT，可以在缺乏明确奖励的情况下，仅通过观察专家学习所需的行为。

    

    离线强化学习（RL）通过学习预先收集的数据来解决顺序决策问题，而无需与环境进行交互。我们展示出，即使缺乏明确的奖励或动作标签，模仿代理也可以仅通过观察专家来学习所需的行为。在我们的方法AILOT（通过最优传输对齐模仿学习）中，我们使用意图的特殊状态表示形式，其中包含数据内的两两空间距离。在给定这种表示形式的情况下，我们通过专家和代理轨迹之间的最优传输距离定义内在奖励函数。我们报告称AILOT在D4RL基准测试上优于最先进的离线模仿学习算法。

    arXiv:2402.13037v1 Announce Type: cross  Abstract: Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and im
    
[^36]: 用超边增强改进现实世界复杂网络表示

    Enhancing Real-World Complex Network Representations with Hyperedge Augmentation

    [https://arxiv.org/abs/2402.13033](https://arxiv.org/abs/2402.13033)

    提出了一种新颖的图增强方法Hyperedge Augmentation (HyperAug)，通过构建直接从原始数据形成的虚拟超边，以解决现实世界复杂网络表示中高阶节点关系的问题

    

    arXiv:2402.13033v1 公告类型: 新摘要: 图增强方法在改进图神经网络（GNNs）的性能和增强泛化能力中起着至关重要的作用。现有的图增强方法主要扰动图结构，通常限于成对节点关系。这些方法无法完全解决真实世界大规模网络的复杂性，这些网络通常涉及高阶节点关系，而不仅仅是成对关系。同时，由于缺乏可用于形成高阶边的数据，真实世界图数据集主要被建模为简单图。因此，将高阶边重新配置为图增强策略的一部分是一个有前途的研究路径，可解决前述问题。在本文中，我们提出了超边增强（HyperAug），一种新颖的图增强方法，直接从原始数据构建虚拟超边，并产生辅助节点。

    arXiv:2402.13033v1 Announce Type: new  Abstract: Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs). Existing graph augmentation methods mainly perturb the graph structures and are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues. In this paper, we present Hyperedge Augmentation (HyperAug), a novel graph augmentation method that constructs virtual hyperedges directly form the raw data, and produces auxiliary nod
    
[^37]: 用逻辑背景知识提升基于神经网络的分类

    Improving Neural-based Classification with Logical Background Knowledge

    [https://arxiv.org/abs/2402.13019](https://arxiv.org/abs/2402.13019)

    本文提出了一种新的神经符号技术——在推理过程中的语义调节，该技术仅在推理过程中约束系统，而不影响训练，相对于其他两种常见神经符号技术具有理论和实际优势，在多尺度方法上的评估结果表明其对网络规模的好处。

    

    arXiv:2402.13019v1 公告类型:新 抽象:神经符号人工智能是一个日益发展的研究领域，旨在将神经网络学习能力与符号系统的推理能力相结合。这种混合可以采用多种形式。在本文中，我们提出了一种新的形式化方法，用于具有命题背景知识的监督式多标签分类。我们引入了一种名为在推理过程中的语义调节的新的神经符号技术，该技术仅在推理过程中约束系统，而不影响训练。我们讨论了它相对于另外两种流行的神经符号技术——语义调节和语义正则化的理论和实际优势。我们开发了一种新的多尺度方法，评估神经符号技术的好处随网络规模的变化而发展。然后，我们在几个数据集上实验评估并比较这三种技术的好处。我们的结果表明，语义调节…

    arXiv:2402.13019v1 Announce Type: new  Abstract: Neurosymbolic AI is a growing field of research aiming to combine neural networks learning capabilities with the reasoning abilities of symbolic systems. This hybridization can take many shapes. In this paper, we propose a new formalism for supervised multi-label classification with propositional background knowledge. We introduce a new neurosymbolic technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected. We discuss its theoritical and practical advantages over two other popular neurosymbolic techniques: semantic conditioning and semantic regularization. We develop a new multi-scale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network. We then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets. Our results demonstrate that semantic conditi
    
[^38]: 改进数据集蒸馏上的跨架构泛化

    Improve Cross-Architecture Generalization on Dataset Distillation

    [https://arxiv.org/abs/2402.13007](https://arxiv.org/abs/2402.13007)

    提出一种新的“模型池”方法，通过在数据蒸馏过程中选择多样的模型，结合知识蒸馏方法，并将其应用于蒸馏数据集的测试过程，从而改进数据集蒸馏的跨架构泛化。

    

    数据集蒸馏是机器学习中一种实用的方法，旨在从现有较大的数据集中创建一个较小的合成数据集。然而，现有的蒸馏方法主要采用基于模型的范式，其中合成数据集继承了特定模型的偏见，限制了其对替代模型的泛化能力。为了应对这一限制，我们提出了一种名为“模型池”的新方法。这种方法在数据蒸馏过程中根据特定概率分布从多样的模型池中选择模型。此外，我们将我们的模型池与已建立的知识蒸馏方法相结合，并将知识蒸馏应用于蒸馏数据集的测试过程。我们的实验结果验证了模型池方法在一系列现有模型上的有效性，同时在测试中表现出优于现有方法的性能。

    arXiv:2402.13007v1 Announce Type: new  Abstract: Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed "model pool". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.
    
[^39]: 探究模型不稳定性对解释和不确定性的影响

    Investigating the Impact of Model Instability on Explanations and Uncertainty

    [https://arxiv.org/abs/2402.13006](https://arxiv.org/abs/2402.13006)

    模型稳定性对解释和不确定性的影响进行了调查，并发现实际扰动对性能和解释影响较小，但掩盖却有 drastical 影响。

    

    可解释的AI方法有助于理解模型行为，然而，对输入进行微小、不可察觉的扰动可能会极大地扭曲解释。这些解释通常在模型部署之前被全面评估，因此很难评估特定解释的可信度。一些研究已经尝试为解释创建置信度估计器，但没有人调查不确定性和解释质量之间的现有联系。我们通过在推断时引入噪声来人为模拟文本输入中的认识不确定性。在这项大规模实证研究中，我们插入不同级别的噪声扰动，并测量对预训练语言模型的输出和不同不确定性度量的影响。实际扰动对性能和解释的影响很小，然而掩盖却有 drastical 影响。我们发现高不确定性并不一定意味着解释不佳。

    arXiv:2402.13006v1 Announce Type: cross  Abstract: Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn't necessarily imply low explanation 
    
[^40]: SzCORE：用于验证基于脑电图的自动癫痫检测算法的癫痫社区开源研究评估框架

    SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms

    [https://arxiv.org/abs/2402.13005](https://arxiv.org/abs/2402.13005)

    提出了SzCORE框架，用于验证基于脑电图的自动癫痫检测算法，旨在标准化验证方法，包括数据集、文件格式、输入内容、性能度量等。

    

    随着家庭和长期脑电图监测的增加，基于脑电图的高质量自动癫痫检测算法的需求变得更加迫切。这些算法验证方法的异质性影响了报告的结果，并使全面评估和比较变得具有挑战性。该异质性主要涉及数据集的选择、评估方法和性能度量等方面。本文提出了一个统一的框架，旨在建立EEG基础癫痫检测算法验证的标准化。基于现有指南和建议，该框架引入了一组关于数据集、文件格式、EEG数据输入内容、癫痫注释输入和输出、交叉验证策略以及性能度量的建议和标准。

    arXiv:2402.13005v1 Announce Type: cross  Abstract: The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging. This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics. In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms. Based on existing guidelines and recommendations, the framework introduces a set of recommendations and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics. We also propose the 10-20 seizure detection benchmark, a machine-learning 
    
[^41]: 一个统一的量子图神经网络主要框架来自量子图态

    A unifying primary framework for quantum graph neural networks from quantum graph states

    [https://arxiv.org/abs/2402.13001](https://arxiv.org/abs/2402.13001)

    量子图神经网络模型可基于图态理解和实现，可用作参数化的量子电路来表示神经网络，或作为构建量子计算机上的图神经网络的基础结构。

    

    图态被用来将数学图表示为量子计算机上的量子状态。它们可以通过稳定子码或直接的量子门和量子状态来构建。本文展示了量子图神经网络模型可以基于图态加以理解和实现。我们展示了它们可以被用作参数化的量子电路来表示神经网络，或作为构建量子计算机上的图神经网络的基础结构。

    arXiv:2402.13001v1 Announce Type: cross  Abstract: Graph states are used to represent mathematical graphs as quantum states on quantum computers. They can be formulated through stabilizer codes or directly quantum gates and quantum states. In this paper we show that a quantum graph neural network model can be understood and realized based on graph states. We show that they can be used either as a parameterized quantum circuits to represent neural networks or as an underlying structure to construct graph neural networks on quantum computers.
    
[^42]: 用于化学文献数据挖掘的自主大型语言模型代理

    An Autonomous Large Language Model Agent for Chemical Literature Data Mining

    [https://arxiv.org/abs/2402.12993](https://arxiv.org/abs/2402.12993)

    介绍了一个端到端的人工智能代理框架，利用大型语言模型实现从化学文献中高保真提取信息，充当化学助手的角色，自动化数据收集和分析，从而提高工作效率。

    

    化学合成对于推动材料合成和药物发现至关重要，影响着包括环境科学和医疗保健在内的各个领域。化学领域的技术上升使得产生了大量的化学数据，挑战研究人员去识别模式并细化合成过程。人工智能通过分析数据来优化合成并提高产量。然而，人工智能在处理文献数据方面面临着挑战，因为化学文献的结构不规整，写作风格多样。为了克服这些困难，我们引入了一个端到端的人工智能代理框架，能够从广泛的化学文献中高保真地提取信息。这个人工智能代理采用大型语言模型（LLMs）进行快速生成和迭代优化。它充当化学助手的角色，自动化数据收集和分析，从而节省人力并提高性能。

    arXiv:2402.12993v1 Announce Type: cross  Abstract: Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs large language models (LLMs) for prompt generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework's ef
    
[^43]: TRAP: 面向黑盒身份验证的有针对性随机对抗提示诱饵

    TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification

    [https://arxiv.org/abs/2402.12991](https://arxiv.org/abs/2402.12991)

    TRAP提出了一种名为Targeted Random Adversarial Prompt (TRAP)的方法，用于识别特定LLM的使用，并在单次交互后以超过95%的真阳性率和低于0.2%的误报率检测目标LLMs。

    

    大型语言模型（LLM）服务和模型通常伴随着关于谁可以使用它们以及他们必须如何使用它们的法律规定。评估发布的LLMs的合规性是至关重要的，因为这些规定保护了LLM贡献者的利益并防止了滥用。在这种背景下，我们描述了黑盒身份验证（BBIV）的新问题。其目标是确定第三方应用是否通过其聊天功能使用某个特定的LLM。我们提出了一种名为目标随机对抗提示（TRAP）的方法，用于识别正在使用的具体LLM。我们重新利用了最初用于越狱的对抗性后缀，以从目标LLM获得预定义的答案，而其他模型则给出随机答案。TRAP可以在单次交互后以超过95%的真阳性率和低于0.2%的误报率检测目标LLMs。即使LLM有不会显著改变的细微变化，TRAP仍然有效。

    arXiv:2402.12991v1 Announce Type: cross  Abstract: Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the
    
[^44]: 面向演化图的稳健图增量学习

    Towards Robust Graph Incremental Learning on Evolving Graphs

    [https://arxiv.org/abs/2402.12987](https://arxiv.org/abs/2402.12987)

    本文关注面向图结构演化的节点式图增量学习问题，并提出了一种名为结构转移的基于正则化的技术

    

    增量学习是一种机器学习方法，涉及在一系列任务上训练模型，而不是一次性处理所有任务。这种能够从任务流中逐步学习的能力对许多现实应用至关重要。然而，在图结构数据上进行增量学习是一个具有挑战性的问题，因为许多与图相关的问题涉及到对每个节点的预测任务，被称为节点式图增量学习（NGIL）。这在样本数据生成过程中引入了非独立和非同分布特征，使得在添加新任务时难以保持模型的性能。本文关注归纳式NGIL问题，考虑了由新任务引起的图结构演化（结构转移）。我们提供了该问题的正式形式化和分析，并提出了一个名为结构转移的新型基于正则化的技术。

    arXiv:2402.12987v1 Announce Type: new  Abstract: Incremental learning is a machine learning approach that involves training a model on a sequence of tasks, rather than all tasks at once. This ability to learn incrementally from a stream of tasks is crucial for many real-world applications. However, incremental learning is a challenging problem on graph-structured data, as many graph-related problems involve prediction tasks for each individual node, known as Node-wise Graph Incremental Learning (NGIL). This introduces non-independent and non-identically distributed characteristics in the sample data generation process, making it difficult to maintain the performance of the model as new tasks are added. In this paper, we focus on the inductive NGIL problem, which accounts for the evolution of graph structure (structural shift) induced by emerging tasks. We provide a formal formulation and analysis of the problem, and propose a novel regularization-based technique called Structural-Shift
    
[^45]: 如何通过时间展开支持神经物理模拟器

    How Temporal Unrolling Supports Neural Physics Simulators

    [https://arxiv.org/abs/2402.12971](https://arxiv.org/abs/2402.12971)

    在不可微分但展开的培训设置支持下，通过数值求解器支持的神经物理模拟器能够获得比完全可微化预测设置高出4.5倍的改进。

    

    在时间上展开培训轨迹强烈影响神经网络增强型物理模拟器的推理精度。我们通过研究三种不同于离散GroundTruth轨迹训练神经网络的变体来分析这些影响。除了常用的一步设置和完全可微的展开外，我们还包括第三种不太常用的变体：没有时间梯度的展开。比较使用这三种模式训练的网络使得我们能够分解出展开的两个主要影响，即训练分布的转变和长期梯度。我们在物理系统、网络大小、网络架构、训练设置和测试场景中进行了详细研究。它为我们的主要发现提供了经验基础：通过数值求解器支持的不可微分但展开的培训设置，可以使预测设置的完全可微化带来4.5倍的改进。

    arXiv:2402.12971v1 Announce Type: cross  Abstract: Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators. We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories. In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients. Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training distribution shift and long-term gradients. We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios. It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does no
    
[^46]: 用于复杂查询回答的条件逻辑消息传递变压器

    Conditional Logical Message Passing Transformer for Complex Query Answering

    [https://arxiv.org/abs/2402.12954](https://arxiv.org/abs/2402.12954)

    提出了一种考虑查询图中常量和变量之间差异，能动态测量消息重要性并捕捉隐式逻辑依赖关系的条件逻辑消息传递变压器。

    

    知识图谱（KGs）上的复杂查询回答（CQA）是一项具有挑战性的任务。由于KGs通常是不完整的，提出了神经模型来通过执行多跳逻辑推理来解决CQA。然而，大多数模型不能同时在一跳和多跳查询上表现良好。最近的工作提出了一种基于预训练神经链接预测器的逻辑消息传递机制。虽然在一跳和多跳查询上都有效，但它忽略了查询图中常量和变量节点之间的差异。此外，在节点嵌入更新阶段，该机制不能动态衡量不同消息的重要性，并且它能否捕捉与节点和接收消息相关的隐式逻辑依赖关系仍不清楚。在本文中，我们提出了条件逻辑消息传递变压器（CLMPT），考虑了查询图中常量和变量之间的差异，并且具有动态测量不同消息重要性以及捕捉与节点和接收消息相关的隐式逻辑依赖关系的能力。

    arXiv:2402.12954v1 Announce Type: cross  Abstract: Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the c
    
[^47]: 基于随机逼近的联邦机器学习方法

    Stochastic Approximation Approach to Federated Machine Learning

    [https://arxiv.org/abs/2402.12945](https://arxiv.org/abs/2402.12945)

    本文提出了一种基于随机逼近的联邦机器学习方法，通过使用近似样本梯度和缩小步长来定位成本函数的极小值，实现了在联邦学习中对神经网络模型进行协作训练的效果，并在数值模拟中与标准算法进行了比较。

    

    本文在随机逼近（SA）框架下研究了联邦学习（FL）。 FL是一种协作方式，用于跨不同参与方或客户端训练神经网络模型，而无需将它们的数据集中。 每个客户端将根据各自的数据训练一个模型，并定期将权重发送到服务器进行聚合。 服务器对这些权重进行聚合，然后客户端使用这些权重重新初始化其神经网络并继续训练。 SA是一种使用近似样本梯度和缩小步长来定位成本函数极小值的迭代算法。 本文中，客户端使用随机逼近迭代更新其神经网络的权重。 结果表明，聚合权重跟踪一个自治ODE。 进行了数值模拟，并将结果与FedAvg和FedProx等标准算法进行了比较。

    arXiv:2402.12945v1 Announce Type: new  Abstract: This paper examines Federated learning (FL) in a Stochastic Approximation (SA) framework. FL is a collaborative way to train neural network models across various participants or clients without centralizing their data. Each client will train a model on their respective data and send the weights across to a the server periodically for aggregation. The server aggregates these weights which are then used by the clients to re-initialize their neural network and continue the training. SA is an iterative algorithm that uses approximate sample gradients and tapering step size to locate a minimizer of a cost function. In this paper the clients use a stochastic approximation iterate to update the weights of its neural network. It is shown that the aggregated weights track an autonomous ODE. Numerical simulations are performed and the results are compared with standard algorithms like FedAvg and FedProx. It is observed that the proposed algorithm 
    
[^48]: 利用轨迹聚类在潜空间中发现深度强化学习策略的行为模式

    Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space

    [https://arxiv.org/abs/2402.12939](https://arxiv.org/abs/2402.12939)

    通过利用轨迹聚类和降维技术在神经网络的潜空间中研究深度强化学习策略的行为模式，可以发现并改进其多样的行为模式和次优选择。

    

    深度强化学习（DRL）代理的行为分析对于提高其性能和可靠性至关重要。然而，它们的策略复杂性往往使其难以理解。本文介绍了一种新方法，用于研究DRL策略的行为模式，该方法涉及在神经网络的潜空间中利用降维和轨迹聚类。具体地，我们使用Pairwise Controlled Manifold Approximation Projection（PaCMAP）进行降维和TRACLUS进行轨迹聚类，分析了在Mountain Car控制任务上训练的DRL策略的潜空间。我们的方法有助于识别多样的行为模式和策略的次优选择，从而实现有针对性的改进。我们展示了如何结合领域知识，可以增强策略在状态空间特定区域的性能。

    arXiv:2402.12939v1 Announce Type: cross  Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents is crucial for improving their performance and reliability. However, the complexity of their policies often makes them challenging to understand. In this paper, we introduce a new approach for investigating the behavior modes of DRL policies, which involves utilizing dimensionality reduction and trajectory clustering in the latent space of neural networks. Specifically, we use Pairwise Controlled Manifold Approximation Projection (PaCMAP) for dimensionality reduction and TRACLUS for trajectory clustering to analyze the latent space of a DRL policy trained on the Mountain Car control task. Our methodology helps identify diverse behavior patterns and suboptimal choices by the policy, thus allowing for targeted improvements. We demonstrate how our approach, combined with domain knowledge, can enhance a policy's performance in specific regions of the state space.
    
[^49]: GRAPHGINI：在图神经网络中促进个体和群体公平性

    GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks

    [https://arxiv.org/abs/2402.12937](https://arxiv.org/abs/2402.12937)

    GRAPHGINI在图神经网络中首次引入了基尼系数作为公平性度量的方法，同时实现个体和群体公平性约束，并保持高预测准确性。

    

    我们解决了日益增长的担忧，即在缺乏公平约束的情况下，GNN可能会产生偏见决策，从而不成比例地影响到弱势群体或个人。与先前的工作不同，我们首次引入了一种将基尼系数作为公平性度量的方法，用于在GNN框架内使用。我们的提议，GRAPHGINI，在单个系统中处理个体和群体公平性的两个不同目标，同时保持高预测准确性。GRAPHGINI通过可学习的注意力分数来实施个体公平性，这有助于通过类似节点聚合更多信息。基于启发式的最大纳什社会福利约束确保了最大可能的群体公平。个体公平性约束和群体公平性约束都是以可微分的基尼系数的近似形式陈述的。这种近似是一个贡献

    arXiv:2402.12937v1 Announce Type: new  Abstract: We address the growing apprehension that GNNs, in the absence of fairness constraints, might produce biased decisions that disproportionately affect underprivileged groups or individuals. Departing from previous work, we introduce for the first time a method for incorporating the Gini coefficient as a measure of fairness to be used within the GNN framework. Our proposal, GRAPHGINI, works with the two different goals of individual and group fairness in a single system, while maintaining high prediction accuracy. GRAPHGINI enforces individual fairness through learnable attention scores that help in aggregating more information through similar nodes. A heuristic-based maximum Nash social welfare constraint ensures the maximum possible group fairness. Both the individual fairness constraint and the group fairness constraint are stated in terms of a differentiable approximation of the Gini coefficient. This approximation is a contribution tha
    
[^50]: 通过端到端最大化KL散度学习异常子群

    Learning Exceptional Subgroups by End-to-End Maximizing KL-divergence

    [https://arxiv.org/abs/2402.12930](https://arxiv.org/abs/2402.12930)

    提出一种Syflow方法，通过端到端最大化KL散度，利用正规化流来模拟任意目标分布，引入新颖的神经层，成功找到具有见地描述的高度异常子群。

    

    找到并描述相对于目标属性异常的子群在许多科学领域具有重要应用，从在人口普查数据中识别处于劣势的人口群体到在金纳米粒子中找到有导电性的分子。当前找到这样的子群的方法需要预先离散化的预测变量，不允许非平凡目标分布，不适用于大型数据集，并且难以找到多样性结果。为解决这些限制，我们提出了Syflow，这是一种端到端可优化的方法，在这种方法中，我们利用正规化流来建模任意目标分布，并引入了一种新颖的神经层，可以产生易于解释的子群描述。我们在合成和真实数据上进行了演示，包括一个案例研究，表明Syflow可可靠地找到伴随着有见地的描述的高度异常子群。

    arXiv:2402.12930v1 Announce Type: new  Abstract: Finding and describing sub-populations that are exceptional regarding a target property has important applications in many scientific disciplines, from identifying disadvantaged demographic groups in census data to finding conductive molecules within gold nanoparticles. Current approaches to finding such subgroups require pre-discretized predictive variables, do not permit non-trivial target distributions, do not scale to large datasets, and struggle to find diverse results.   To address these limitations, we propose Syflow, an end-to-end optimizable approach in which we leverage normalizing flows to model arbitrary target distributions, and introduce a novel neural layer that results in easily interpretable subgroup descriptions. We demonstrate on synthetic and real-world data, including a case study, that Syflow reliably finds highly exceptional subgroups accompanied by insightful descriptions.
    
[^51]: 准时到位：通过限制时间序列模型的解释来修订它们

    Right on Time: Revising Time Series Models by Constraining their Explanations

    [https://arxiv.org/abs/2402.12921](https://arxiv.org/abs/2402.12921)

    引入了准时到位（RioT）方法，通过使模型解释在时间和频率域之间交互，并利用反馈来约束模型，有效地解决了时间序列数据中的混杂因素问题。

    

    深度时间序列模型的可靠性经常会受到其依赖混杂因素的倾向的损害，这可能导致误导性结果。我们的新记录的、自然混杂的数据集P2S来自真实的机械生产线，强调了这一点。为了解决时间序列数据中的混杂因素的挑战性问题，我们引入了准时到位（RioT）。我们的方法使模型解释在时间和频率域之间进行交互。然后利用两个域内的解释反馈来约束模型，使其远离标注的混杂因素。在处理时间序列数据集中混杂因素方面，双域交互策略至关重要。我们凭经验证明，RioT能够有效地引导模型远离P2S以及流行的时间序列分类和预测数据集中的错误原因。

    arXiv:2402.12921v1 Announce Type: new  Abstract: The reliability of deep time series models is often compromised by their tendency to rely on confounding factors, which may lead to misleading results. Our newly recorded, naturally confounded dataset named P2S from a real mechanical production line emphasizes this. To tackle the challenging problem of mitigating confounders in time series data, we introduce Right on Time (RioT). Our method enables interactions with model explanations across both the time and frequency domain. Feedback on explanations in both domains is then used to constrain the model, steering it away from the annotated confounding factors. The dual-domain interaction strategy is crucial for effectively addressing confounders in time series datasets. We empirically demonstrate that RioT can effectively guide models away from the wrong reasons in P2S as well as popular time series classification and forecasting datasets.
    
[^52]: 数据管道训练：将 AutoML 集成到优化机器学习模型数据流中

    Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of Machine Learning Models

    [https://arxiv.org/abs/2402.12916](https://arxiv.org/abs/2402.12916)

    本文讨论了如何通过集成 AutoML 技术优化数据管道，提升数据流智能性以在机器学习任务中取得更好结果，并揭示了构建高效适应不断变化数据环境的关键策略。

    

    数据管道在机器学习建模和开发数据产品等任务中扮演着不可或缺的角色。随着数据源日益多样化和复杂化，以及数据量的快速增长，构建高效的数据管道对于提高工作效率和解决复杂问题至关重要。本文重点探讨如何通过集成 AutoML 到数据管道中，优化数据流动的方法。我们将讨论如何利用 AutoML 技术提升数据管道的智能化，从而在机器学习任务中取得更好的结果。通过深入研究数据流的自动化和优化，我们揭示了构建适应不断变化的数据环境的高效数据管道的关键策略。这不仅加快了建模过程，还为复杂问题提供了创新解决方案，实现了更显著的效果。

    arXiv:2402.12916v1 Announce Type: cross  Abstract: Data Pipeline plays an indispensable role in tasks such as modeling machine learning and developing data products. With the increasing diversification and complexity of Data sources, as well as the rapid growth of data volumes, building an efficient Data Pipeline has become crucial for improving work efficiency and solving complex problems. This paper focuses on exploring how to optimize data flow through automated machine learning methods by integrating AutoML with Data Pipeline. We will discuss how to leverage AutoML technology to enhance the intelligence of Data Pipeline, thereby achieving better results in machine learning tasks. By delving into the automation and optimization of Data flows, we uncover key strategies for constructing efficient data pipelines that can adapt to the ever-changing data landscape. This not only accelerates the modeling process but also provides innovative solutions to complex problems, enabling more sig
    
[^53]: 通过语义图平滑实现更具辨别力的句子嵌入

    More Discriminative Sentence Embeddings via Semantic Graph Smoothing

    [https://arxiv.org/abs/2402.12890](https://arxiv.org/abs/2402.12890)

    通过语义图平滑技术增强预训练模型获取的句子嵌入，有效改善文本聚类和分类任务的结果。

    

    这篇论文探讨了一种经验方法，以无监督的方式学习更具辨别性的句子表示。利用语义图平滑，我们增强了从预训练模型中获取的句子嵌入，以提高文本聚类和分类任务的结果。我们的方法在八个基准上得到验证，展示了语义图平滑在改善用于监督和无监督文档分类任务的句子嵌入中的潜力。

    arXiv:2402.12890v1 Announce Type: new  Abstract: This paper explores an empirical approach to learn more discriminantive sentence representations in an unsupervised fashion. Leveraging semantic graph smoothing, we enhance sentence embeddings obtained from pretrained models to improve results for the text clustering and classification tasks. Our method, validated on eight benchmarks, demonstrates consistent improvements, showcasing the potential of semantic graph smoothing in improving sentence embeddings for the supervised and unsupervised document categorization tasks.
    
[^54]: 对最大边际自由度的一个界限

    A Bound on the Maximal Marginal Degrees of Freedom

    [https://arxiv.org/abs/2402.12885](https://arxiv.org/abs/2402.12885)

    该论文提出了对于核岭回归的低秩近似和替代方法中，关于低维近似秩的一个下界，从而保证可靠的预测能力，并将有效维度与最大统计杠杆得分联系起来。

    

    arXiv:2402.12885v1 公告类型: 交叉摘要: 通用核岭回归在内存分配和计算时间上成本高昂。本文研究了核岭回归的低秩近似和替代方法，以应对这些困难。本文的基本贡献在于对低维近似的秩提出了一个下界，要求其保持可靠的预测能力。该界限将有效维度与最大统计杠杆得分联系起来。我们通过涉及核的正则性来表征有效维度及其随正则化参数的增长行为。对于适当选择的核，这种增长被证明是对数渐近的，从而证明了低秩近似作为Nyström方法的合理性。

    arXiv:2402.12885v1 Announce Type: cross  Abstract: Common kernel ridge regression is expensive in memory allocation and computation time. This paper addresses low rank approximations and surrogates for kernel ridge regression, which bridge these difficulties. The fundamental contribution of the paper is a lower bound on the rank of the low dimensional approximation, which is required such that the prediction power remains reliable. The bound relates the effective dimension with the largest statistical leverage score. We characterize the effective dimension and its growth behavior with respect to the regularization parameter by involving the regularity of the kernel. This growth is demonstrated to be asymptotically logarithmic for suitably chosen kernels, justifying low-rank approximations as the Nystr\"om method.
    
[^55]: 非独立同分布数据孤岛上的联邦多任务学习：实验研究

    Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study

    [https://arxiv.org/abs/2402.12876](https://arxiv.org/abs/2402.12876)

    这项研究介绍了一种新颖的FMTL-Bench框架，用于系统评估联邦多任务学习（FMTL）范式，填补了FL和MTL综合评估方法的空白。

    

    这种创新的联邦多任务学习（FMTL）方法整合了联邦学习（FL）和多任务学习（MTL）的优点，能够在多任务学习数据集上进行协作模型训练。然而，目前在该领域缺乏整合FL和MTL独特特性的综合评估方法。本文通过引入一个新颖的框架，FMTL-Bench，填补了这一空白，用于系统评估FMTL范式。这个基准涵盖了数据、模型和优化算法级别的各个方面，并包括七组比较实验，封装了广泛的非独立同分布（Non-IID）数据分区场景。我们提出了对比各种指标的基线的系统过程，并对通信开支、时间和能源消耗进行了案例研究。通过我们的大量实验，我们旨在提供有价值的

    arXiv:2402.12876v1 Announce Type: new  Abstract: The innovative Federated Multi-Task Learning (FMTL) approach consolidates the benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling collaborative model training on multi-task learning datasets. However, a comprehensive evaluation method, integrating the unique features of both FL and MTL, is currently absent in the field. This paper fills this void by introducing a novel framework, FMTL-Bench, for systematic evaluation of the FMTL paradigm. This benchmark covers various aspects at the data, model, and optimization algorithm levels, and comprises seven sets of comparative experiments, encapsulating a wide array of non-independent and identically distributed (Non-IID) data partitioning scenarios. We propose a systematic process for comparing baselines of diverse indicators and conduct a case study on communication expenditure, time, and energy consumption. Through our exhaustive experiments, we aim to provide valuable
    
[^56]: 思维链激发变压器解决固有串行问题的能力

    Chain of Thought Empowers Transformers to Solve Inherently Serial Problems

    [https://arxiv.org/abs/2402.12875](https://arxiv.org/abs/2402.12875)

    思维链赋予变压器模型执行固有串行计算的能力，提高了变压器在算术和符号推理任务中的准确性。

    

    指导模型生成一系列中间步骤，即思维链（CoT），是提高大型语言模型（LLMs）在算术和符号推理任务上准确性的高效方法。然而，CoT背后的机制仍不清楚。这项工作通过表达性的视角提供了对解码器专用变压器的CoT能力的理论理解。在概念上，CoT赋予模型执行固有串行计算的能力，而这种能力在变压器中缺乏，特别是当深度较低时。先前的作品已经表明，在没有CoT的情况下，具有有限精度$\mathsf{poly}(n)$嵌入尺寸的恒定深度变压器只能在$\mathsf{TC}^0$中解决问题。我们首先展示了具有常数位精度的恒定深度变压器的更紧密的表达性上界，它只能解决$\mathsf{AC}^0$中的问题。

    arXiv:2402.12875v1 Announce Type: new  Abstract: Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a 
    
[^57]: 技能还是运气？通过优势函数对回报进行分解

    Skill or Luck? Return Decomposition via Advantage Functions

    [https://arxiv.org/abs/2402.12874](https://arxiv.org/abs/2402.12874)

    优势函数的创新在于将回报分解为代理动作引起的部分（技能）和代理无法控制的部分（运气），进而扩展了直接优势估计到离线环境，使得从离线轨迹学习更加高效。

    

    学习来自离线数据对于高效的强化学习至关重要。在本工作中，我们基于这样的洞察力，即优势函数可以被理解为动作对回报的因果影响，并展示了这使我们能够将轨迹的回报分解为由代理的动作（技能）引起的部分和代理无法控制的部分（运气）。此外，这种分解使我们能够自然地将直接优势估计（DAE）扩展到离线设置（离线DAE）。由此产生的方法可以从离线轨迹中学习，而无需依赖重要性采样技术或截断离线动作。我们建立离线DAE与先前方法之间的联系，以展示它如何加速学习以及当所提出的离线校正何时重要。最后，我们使用MinAtar环境来说明忽略离线校正可能导致子

    arXiv:2402.12874v1 Announce Type: new  Abstract: Learning from off-policy data is essential for sample-efficient reinforcement learning. In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent's actions (skill) and parts outside of the agent's control (luck). Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The resulting method can learn from off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions. We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important. Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to sub
    
[^58]: 通过利用可行集的曲率，在在线凸优化中实现快速收敛速度

    Fast Rates in Online Convex Optimization by Exploiting the Curvature of Feasible Sets

    [https://arxiv.org/abs/2402.12868](https://arxiv.org/abs/2402.12868)

    该论文提出了一种新的分析方法，通过利用可行集的曲率，在在线凸优化中实现了快速收敛速度。

    

    在本文中，我们探讨了在线凸优化（OCO），介绍了一种通过利用可行集的曲率提供快速收敛速度的新分析。我们首先证明，如果最优决策位于可行集的边界上且基础损失函数的梯度非零，则算法在随机环境中可以达到$O(\rho \log T)$的遗憾上界。其中，$\rho > 0$是包含最优决策并围绕可行集的最小球体的半径。

    arXiv:2402.12868v1 Announce Type: new  Abstract: In this paper, we explore online convex optimization (OCO) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets. In online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret. This paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. We first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $O(\rho \log T)$ in stochastic environments. Here, $\rho > 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. Our approach, unlike existing ones, can work directly with co
    
[^59]: 朝着MLOps的方向：面向机器学习系统的DevOps工具推荐系统

    Towards MLOps: A DevOps Tools Recommender System for Machine Learning System

    [https://arxiv.org/abs/2402.12867](https://arxiv.org/abs/2402.12867)

    本研究提出了一个用于机器学习系统的DevOps工具推荐系统，旨在建立一个可以自动构建数据集、训练模型、部署模型以及存储模型和数据集的流水线，以提高模型交付的速度和结果准确性。

    

    将DevOps实践应用于机器学习系统被称为MLOps，机器学习系统会随着新数据的出现而不断发展，与传统系统基于需求的方式不同。MLOps的目标是建立不同开源工具之间的连接，构建一个可以自动执行构建数据集、训练机器学习模型、部署模型到生产环境以及存储不同版本模型和数据集步骤的流水线。MLOps的好处在于确保将新训练好的模型快速交付到生产环境，以获得准确的结果。此外，MLOps实践会影响软件产品的整体质量，并且完全依赖于开源工具，选择相关开源工具被视为一项挑战，因此期望有一种通用方法来选择合适的开源工具。本文提出了一个推荐系统框架，该框架可以...

    arXiv:2402.12867v1 Announce Type: cross  Abstract: Applying DevOps practices to machine learning system is termed as MLOps and machine learning systems evolve on new data unlike traditional systems on requirements. The objective of MLOps is to establish a connection between different open-source tools to construct a pipeline that can automatically perform steps to construct a dataset, train the machine learning model and deploy the model to the production as well as store different versions of model and dataset. Benefits of MLOps is to make sure the fast delivery of the new trained models to the production to have accurate results. Furthermore, MLOps practice impacts the overall quality of the software products and is completely dependent on open-source tools and selection of relevant open-source tools is considered as challenged while a generalized method to select an appropriate open-source tools is desirable. In this paper, we present a framework for recommendation system that proce
    
[^60]: 反向镜头：将语言模型梯度投影到词汇空间中

    Backward Lens: Projecting Language Model Gradients into the Vocabulary Space

    [https://arxiv.org/abs/2402.12865](https://arxiv.org/abs/2402.12865)

    将语言模型梯度投影到词汇空间中，挖掘信息在LMs内部的流动方式，探索新信息如何存储在LMs的神经元中。

    

    了解基于Transformer的语言模型(LMs)如何学习和记忆信息是深度学习社区的一个重要目标。最近的可解释性方法将从前向传播中获得的权重和隐藏状态投影到模型的词汇表中，有助于揭示LMs内部信息流动的方式。在这项工作中，我们将这种方法扩展到LMs的后向传播和梯度。我们首先证明梯度矩阵可以被表示为其前向和后向传播输入的低秩线性组合。然后我们开发方法将这些梯度投影到词汇项中，并探讨新信息如何存储在LMs的神经元中的机制。

    arXiv:2402.12865v1 Announce Type: cross  Abstract: Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs' backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons.
    
[^61]: 在没有数据先验条件下限制对抗者重建攻击成功率

    Bounding Reconstruction Attack Success of Adversaries Without Data Priors

    [https://arxiv.org/abs/2402.12861](https://arxiv.org/abs/2402.12861)

    本研究提供了差分隐私训练的机器学习模型在现实对抗设置下重建成功率的正式上限，并通过实证结果支持，有助于更明智地选择隐私参数。

    

    机器学习模型的重建攻击存在泄漏敏感数据的风险。在特定情境下，对手可以使用模型的梯度几乎完美地重建训练数据样本。在使用差分隐私（DP）训练机器学习模型时，可以提供对这种重建攻击成功率的正式上限。迄今为止，这些上限是在可能不符合高度现实实用性的最坏情况假设下制定的。在本文中，我们针对差分隐私训练的机器学习模型提供了在现实对抗设置下的重建成功率正式上限，并通过实证结果支持这些上限。通过这一点，我们展示了在现实情境中，（a）预期的重建成功率可以在不同背景和不同度量下得到适当的限制，这（b）有助于更明智地选择隐私参数。

    arXiv:2402.12861v1 Announce Type: new  Abstract: Reconstruction attacks on machine learning (ML) models pose a strong risk of leakage of sensitive data. In specific contexts, an adversary can (almost) perfectly reconstruct training data samples from a trained model using the model's gradients. When training ML models with differential privacy (DP), formal upper bounds on the success of such reconstruction attacks can be provided. So far, these bounds have been formulated under worst-case assumptions that might not hold high realistic practicality. In this work, we provide formal upper bounds on reconstruction success under realistic adversarial settings against ML models trained with DP and support these bounds with empirical results. With this, we show that in realistic scenarios, (a) the expected reconstruction success can be bounded appropriately in different contexts and by different metrics, which (b) allows for a more educated choice of a privacy parameter.
    
[^62]: 可微分映射器用于数据表示的拓扑优化

    Differentiable Mapper For Topological Optimization Of Data Representation

    [https://arxiv.org/abs/2402.12854](https://arxiv.org/abs/2402.12854)

    可微分映射器提出了一种新方法，用于优化数据表示的拓扑结构，解决了手动调节Mapper图中许多参数的问题，特别是调节关键的滤波器参数。

    

    无监督数据表示和可视化利用拓扑工具是拓扑数据分析（TDA）和数据科学的一个积极且快速发展的领域。其中最突出的工作线是基于所谓的Mapper图，这是一个组合图，其拓扑结构（连通组件，分支，环）与数据本身的拓扑结构相对应。然而，尽管高度通用和适用，其使用到目前为止一直受到其许多参数的手动调节的阻碍-其中一个至关重要的是所谓的滤波器:它是一个连续函数，其在数据集上的变化是构建Mapper表示和评估其拓扑结构的存在和大小的主要成分。然而，尽管对于其他Mapper参数（即，分辨率，增益，聚类）已经研究了一些参数调整方法，但目前还没有针对调整滤波器本身的方法。

    arXiv:2402.12854v1 Announce Type: new  Abstract: Unsupervised data representation and visualization using tools from topology is an active and growing field of Topological Data Analysis (TDA) and data science. Its most prominent line of work is based on the so-called Mapper graph, which is a combinatorial graph whose topological structures (connected components, branches, loops) are in correspondence with those of the data itself. While highly generic and applicable, its use has been hampered so far by the manual tuning of its many parameters-among these, a crucial one is the so-called filter: it is a continuous function whose variations on the data set are the main ingredient for both building the Mapper representation and assessing the presence and sizes of its topological structures. However, while a few parameter tuning methods have already been investigated for the other Mapper parameters (i.e., resolution, gain, clustering), there is currently no method for tuning the filter itse
    
[^63]: CCFC++：通过特征去相关增强联邦聚类

    CCFC++: Enhancing Federated Clustering through Feature Decorrelation

    [https://arxiv.org/abs/2402.12852](https://arxiv.org/abs/2402.12852)

    引入了去相关正则化器的CCFC++方法有效地解决了数据异质性对联邦聚类性能的不利影响，取得了优越表现。

    

    在联邦聚类中，多个持有数据的客户端在不交换原始数据的情况下协作对数据进行分组。这一领域通过与对比学习相结合取得了显着进展，以Cluster-Contrastive Federated Clustering (CCFC)为例。然而，CCFC受到不同客户端之间的异构数据影响，导致表现不佳且不稳健。我们的研究进行了经验和理论分析，以了解异构数据对CCFC的影响。研究结果表明，增加数据的异质性加剧了CCFC中的维度坍缩，通过增加学习表示的多个维度之间的相关性来证明。为了解决这一问题，我们向CCFC引入了一个去相关正则化器。得益于正则化器，改进的方法有效地缓解了数据异质性的不利影响，并取得了优越的性能，通过NMI分数的显著增加来证明。

    arXiv:2402.12852v1 Announce Type: new  Abstract: In federated clustering, multiple data-holding clients collaboratively group data without exchanging raw data. This field has seen notable advancements through its marriage with contrastive learning, exemplified by Cluster-Contrastive Federated Clustering (CCFC). However, CCFC suffers from heterogeneous data across clients, leading to poor and unrobust performance. Our study conducts both empirical and theoretical analyses to understand the impact of heterogeneous data on CCFC. Findings indicate that increased data heterogeneity exacerbates dimensional collapse in CCFC, evidenced by increased correlations across multiple dimensions of the learned representations. To address this, we introduce a decorrelation regularizer to CCFC. Benefiting from the regularizer, the improved method effectively mitigates the detrimental effects of data heterogeneity, and achieves superior performance, as evidenced by a marked increase in NMI scores, with t
    
[^64]: 调整过的语言模型更好的知识学习者

    Instruction-tuned Language Models are Better Knowledge Learners

    [https://arxiv.org/abs/2402.12847](https://arxiv.org/abs/2402.12847)

    通过在持续预训练文档之前暴露LLM到问题-答案对，以便从复杂文档中编码知识，可以更好地适应知识访问方式。

    

    为了使基于大语言模型（LLM）的助手能够有效地适应不断发展的信息需求，必须能够通过持续在新数据上训练来更新它们的事实知识。传统做法涉及在新文档上持续预培训，然后根据问题-答案（QA）对进行指导调整。

    arXiv:2402.12847v1 Announce Type: cross  Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a met
    
[^65]: PromptKD：通过提示调整为生成语言模型提取学生友好知识的蒸馏方法

    PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning

    [https://arxiv.org/abs/2402.12842](https://arxiv.org/abs/2402.12842)

    提出了PromptKD方法，通过提示调整实现了生成语言模型提取学生友好知识的蒸馏，无需微调整整个教师模型。

    

    近期大型语言模型（LLMs）的发展引起了对推理成本的担忧，进一步增加了对模型压缩研究的需求。尽管知识蒸馏（KD）是一种突出的方法，但是针对LLMs这样的生成语言模型的KD研究相对较少，而提取适合学生的知识的方法，在分类模型的KD中表现出了良好性能，在生成语言模型中尚未被探索。为了探索这种方法，我们提出了PromptKD，一种简单而有效的方法，它利用提示调整 - 在KD中首次出现 - 使生成语言模型能够传递适合学生的知识。与先前分类工作不同，先前那些需要微调整整个教师模型以提取适合学生的知识，PromptKD通过添加少量提示标记，并仅通过学生指导调整提示来达到类似效果。

    arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
    
[^66]: SGD梯度剪切方法暗中估计中值梯度

    SGD with Clipping is Secretly Estimating the Median Gradient

    [https://arxiv.org/abs/2402.12828](https://arxiv.org/abs/2402.12828)

    本研究提出了一种基于中值估计的稳健梯度估计方法，针对包括重尾噪声在内的多种应用场景进行了探讨，揭示了不同形式的剪切方法实际上是该方法的特例。

    

    有几种随机优化的应用场景可以受益于对梯度的稳健估计。例如，在具有损坏节点的分布式学习领域、训练数据中存在大的异常值、在隐私约束下学习，甚至由于算法动态本身的重尾噪声。本文研究了基于中值估计的稳健梯度估计的SGD。首先考虑跨样本计算中值梯度，结果表明即使在重尾、状态相关噪声下，该方法也能收敛。然后我们推导了基于随机近端点方法的迭代方法，用于计算几何中值和其推广形式。最后，我们提出了一种算法，用于估计迭代间的中值梯度，并发现几种众所周知的方法 - 特别是不同形式的剪切 - 是这一框架的特例。

    arXiv:2402.12828v1 Announce Type: cross  Abstract: There are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient. For example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself. Here we study SGD with robust gradient estimators based on estimating the median. We first consider computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise. We then derive iterative methods based on the stochastic proximal point method for computing the geometric median and generalizations thereof. Finally we propose an algorithm estimating the median gradient across iterations, and find that several well known methods - in particular different forms of clipping - are particular cases of this framework.
    
[^67]: 在摘要中识别事实不一致性：朝向大型语言模型的有效利用

    Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model

    [https://arxiv.org/abs/2402.12821](https://arxiv.org/abs/2402.12821)

    该研究提出了针对摘要中事实不一致性的解决方案：通过大型语言模型在正确的范式设计下无需训练即可解决任务，并提出了训练策略以精炼更小型的高准确性的语言模型。

    

    事实上的不一致性对抽象性摘要生成器的商业部署构成重要障碍。本研究围绕两个重要问题展开：如何最好地利用大型语言模型来检测事实不一致性，以及如何精炼一个同时具有高效性和功效性的更小型语言模型？首先提出并评估了三种零样本范式，跨越五个不同数据集：直接推理整个摘要或每个摘要窗口；通过问题生成和回答进行实体验证。实验表明，在适当的范式设计下，语言模型本身能够在无需训练的情况下解决这一任务，平均超过强大的训练基线2.8%。为进一步促进实用性，我们提出针对精炼更小的开源语言模型的训练策略，该模型可以一次性高准确地评分整个摘要，胜过零

    arXiv:2402.12821v1 Announce Type: new  Abstract: Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero
    
[^68]: 微调、提示、上下文学习和指导微调：我们需要多少标记样本？

    Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?

    [https://arxiv.org/abs/2402.12819](https://arxiv.org/abs/2402.12819)

    专门模型通常只需少量标记样本（100-1000个）就能与通用模型持平甚至更好，取决于任务的复杂性和结果的变化。

    

    当解决具有有限标记数据的任务时，研究人员可以选择使用通用的大型语言模型而不进行进一步更新，或者使用少量示例来调整专门的较小模型。 当有足够的标记可用时，专门的模型在许多自然语言处理任务上表现优于通用模型。 在这项工作中，我们旨在调查专门模型需要多少标记样本才能实现这种出色的性能，同时考虑结果的变化。观察提示、上下文学习、微调和指导微调的行为，识别它们在增加不同复杂性任务的标记训练样本数量时的收支平衡点，我们发现专门模型通常只需少量样本（100-1000个）就能与通用模型持平甚至更好。 同时，所需的标记数据量强烈依赖于任务的复杂性和结果的变化。

    arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
    
[^69]: 有限标注数据学习对随机性的敏感性：相互作用和系统选择的影响

    On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices

    [https://arxiv.org/abs/2402.12817](https://arxiv.org/abs/2402.12817)

    有限标注数据学习对随机性的敏感性，通过系统研究随机因素的影响，揭示了忽略相互作用可能导致的不一致结果。

    

    有限标注数据学习可以在标签不足时提高性能，但也对所谓的随机因素（例如数据的变化顺序）引入的无法控制的随机性敏感。我们提出了一种方法，系统地调查随机因素的影响，同时考虑它们之间的相互作用。为了测量单个随机因素的真实影响，我们的方法减轻了其他因素的影响，并观察了性能在多次运行中的变化。将我们的方法应用于7个代表性文本分类任务的上下文学习和微调方法以及3个任务的元学习，我们发现：1）现有作品中忽略随机因素之间的相互作用导致了不一致的研究结果，因为错误地归因于随机因素的影响，比如否定了一些一

    arXiv:2402.12817v1 Announce Type: cross  Abstract: While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consis
    
[^70]: 可扩展的分散算法用于在线个性化均值估计

    Scalable Decentralized Algorithms for Online Personalized Mean Estimation

    [https://arxiv.org/abs/2402.12812](https://arxiv.org/abs/2402.12812)

    本研究提出了一种可扩展的分散算法框架，使代理能够自组织成图，并提出了两种协同均值估计算法，解决了每个代理在学习模型的同时识别具有相似分布客户的挑战。

    

    在许多情况下，代理缺乏足够的数据直接学习模型。与其他代理合作可能有所帮助，但当本地数据分布不同时，会引入偏差-方差权衡。一个关键挑战是每个代理在学习模型的同时识别具有相似分布的客户，这个问题主要仍未解决。本研究着眼于一个简化版本的普遍问题，即每个代理随时间从实值分布中收集样本来估计其均值。现有算法面临着不切实际的空间和时间复杂度（与代理数量A的平方成正比）。为了解决可扩展性挑战，我们提出了一个框架，代理自组织成一个图，使得每个代理只能与选定数量的对等体r进行通信。我们介绍了两种协作均值估计算法：一种灵感来源于信念传播，另一种采用基于共识的方法。

    arXiv:2402.12812v1 Announce Type: new  Abstract: In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based appr
    
[^71]: 学习非齐次时间泊松过程的泛化和正则化

    Learning Generalization and Regularization of Nonhomogeneous Temporal Poisson Processes

    [https://arxiv.org/abs/2402.12808](https://arxiv.org/abs/2402.12808)

    将NHPPs的估计问题转化为学习泛化问题，提出了正则化学习NHPPs的框架与两种新的自适应和数据驱动的分箱方法，有效解决了数据量有限时过拟合的问题。

    

    泊松过程，尤其是非齐次泊松过程(NHPP)，是一种在许多实际应用中非常重要的计数过程。目前，文献中几乎所有的工作都致力于使用非数据驱动的分箱方法对具有无穷数据的NHPP进行估计。本文将有限和有限数据下的NHPP估计问题公式化为一个学习泛化问题。我们在数学上证明，尽管分箱方法对于估计NHPPs很重要，但在数据量有限时会带来过拟合的风险。我们提出了一个正则化学习NHPPs的框架，其中包括两种新的自适应和数据驱动的分箱方法，帮助消除分箱参数的即兴调整。我们在合成和实际数据集上对我们的方法进行了实验证明了其有效性。

    arXiv:2402.12808v1 Announce Type: new  Abstract: The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is an essentially important counting process with numerous real-world applications. Up to date, almost all works in the literature have been on the estimation of NHPPs with infinite data using non-data driven binning methods. In this paper, we formulate the problem of estimation of NHPPs from finite and limited data as a learning generalization problem. We mathematically show that while binning methods are essential for the estimation of NHPPs, they pose a threat of overfitting when the amount of data is limited. We propose a framework for regularized learning of NHPPs with two new adaptive and data-driven binning methods that help to remove the ad-hoc tuning of binning parameters. Our methods are experimentally tested on synthetic and real-world datasets and the results show their effectiveness.
    
[^72]: 利用协作四足机器人和无人机进行文化遗产遗址的自主现实建模

    Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles

    [https://arxiv.org/abs/2402.12794](https://arxiv.org/abs/2402.12794)

    通过使用自主仿生四足机器人代理和无人机，本文提出了一种自主的3D现实建模方法，可以用于文化遗产(CH)文物，实现了系统化和可重复的3D RM过程。

    

    如今，先进传感器的使用，如地面3D激光扫描仪、移动LiDAR和无人机摄影测量，已经成为文化遗产(CH)大型文物的3D现实建模和数字化的主要实践。在实践中，这个过程与调查团队的专业知识密切相关，处理针对每个遗址特定要求和约束的耗时规划和执行3D映射过程。为了最小化人类干预，本文引入了一种新颖的方法，通过利用配备适当传感器的自主仿生四足机器人代理和无人机来实现文化遗产(CH)文物的自主3D现实建模。这些自主机器人代理以系统化且可重复的方式进行3D RM过程。这个自动化过程的结果可能在数字孪生平台中找到应用。

    arXiv:2402.12794v1 Announce Type: cross  Abstract: Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners, mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has become the prevalent practice for 3D Reality Modeling and digitization of large-scale monuments of Cultural Heritage (CH). In practice, this process is heavily related to the expertise of the surveying team, handling the laborious planning and time-consuming execution of the 3D mapping process that is tailored to the specific requirements and constraints of each site. To minimize human intervention, this paper introduces a novel methodology for autonomous 3D Reality Modeling for CH monuments by employing au-tonomous biomimetic quadrupedal robotic agents and UAVs equipped with the appropriate sensors. These autonomous robotic agents carry out the 3D RM process in a systematic and repeatable ap-proach. The outcomes of this automated process may find applications in digital twin platfo
    
[^73]: 从运动到衡量：评估基于骨架的人类活动识别可解释AI方法

    From Movements to Metrics: Evaluating Explainable AI Methods in Skeleton-Based Human Activity Recognition

    [https://arxiv.org/abs/2402.12790](https://arxiv.org/abs/2402.12790)

    本研究测试并比较了基于骨架的人类活动识别中的可解释AI方法，发现在某些情境下“忠实度”可能不可靠，而“稳定性”在轻微数据扰动时更可靠。

    

    3D骨架数据中深度学习在人类活动识别（HAR）中的进展对医疗、安全、体育和人机交互应用至关重要。本文解决了该领域一个众所周知的问题，即在基于骨架的HAR领域中缺乏XAI评估指标的适用性和可靠性测试。我们在类激活映射（CAM）和梯度加权类激活映射（Grad-CAM）上测试了已建立的XAI评估指标，即忠实度和稳定性，以解决这一问题。该研究还引入一种尊重人体生物力学约束的扰动方法，以确保人类运动中的现实变化。我们的研究结果表明，在某些情境下，如EfficientGCN模型中，\textit{忠实度}可能不是一个可靠的指标。相反，当存在轻微输入数据扰动时，稳定性更可靠。CAM和Grad-CAM

    arXiv:2402.12790v1 Announce Type: new  Abstract: The advancement of deep learning in human activity recognition (HAR) using 3D skeleton data is critical for applications in healthcare, security, sports, and human-computer interaction. This paper tackles a well-known gap in the field, which is the lack of testing in the applicability and reliability of XAI evaluation metrics in the skeleton-based HAR domain. We have tested established XAI metrics namely faithfulness and stability on Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) to address this problem. The study also introduces a perturbation method that respects human biomechanical constraints to ensure realistic variations in human movement. Our findings indicate that \textit{faithfulness} may not be a reliable metric in certain contexts, such as with the EfficientGCN model. Conversely, stability emerges as a more dependable metric when there is slight input data perturbations. CAM and Grad-C
    
[^74]: 无需公平训练的公平分类器：一种受影响数据抽样方法

    Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach

    [https://arxiv.org/abs/2402.12789](https://arxiv.org/abs/2402.12789)

    在不实施公平训练算法的情况下学习公平分类器，通过抽样具有影响力的数据来逐步转移原始训练数据，从而提高公平性和准确性。

    

    一个公平的分类器应该确保来自不同群体的人们受益，而群体信息往往是敏感的，不适合模型训练。因此，在训练数据集中学习一个公平的分类器但排除敏感属性是很重要的。本文研究了学习公平分类器而不实现公平训练算法的方法，以避免可能泄露敏感信息。我们的理论分析验证了这种方法的可能性，即在具有适当分布偏移的数据集上进行传统训练可以同时减少公平差距的上限和模型泛化误差，表明公平性和准确性可以同步提高，只需简单地进行传统训练。然后，我们提出了一个可行的解决方案，通过抽样有影响力的数据逐步转移原始训练数据，在训练过程中不访问新数据的敏感属性。

    arXiv:2402.12789v1 Announce Type: cross  Abstract: A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in s
    
[^75]: 处理联邦学习中的拜占庭客户问题

    Tackling Byzantine Clients in Federated Learning

    [https://arxiv.org/abs/2402.12780](https://arxiv.org/abs/2402.12780)

    研究通过引入鲁棒平均规则来抵御联邦学习中的拜占庭式客户，同时强调客户子采样和本地步骤对模型性能的重要影响。

    

    通过替换标准$\mathsf{FedAvg}$算法中服务器端的简单平均操作为\emph{鲁棒平均规则}来使联邦学习(FL)抵御拜占庭式(adversarial)客户的可能性。 先前的研究大部分忽略了\emph{客户子采样}和\emph{本地步骤}对FL特性的影响。我们通过展示深入分析来验证这一观察。

    arXiv:2402.12780v1 Announce Type: new  Abstract: The possibility of adversarial (a.k.a., {\em Byzantine}) clients makes federated learning (FL) prone to arbitrary manipulation. The natural approach to robustify FL against adversarial clients is to replace the simple averaging operation at the server in the standard $\mathsf{FedAvg}$ algorithm by a \emph{robust averaging rule}. While a significant amount of work has been devoted to studying the convergence of federated {\em robust averaging} (which we denote by $\mathsf{FedRo}$), prior work has largely ignored the impact of {\em client subsampling} and {\em local steps}, two fundamental FL characteristics. While client subsampling increases the effective fraction of Byzantine clients, local steps increase the drift between the local updates computed by honest (i.e., non-Byzantine) clients. Consequently, a careless deployment of $\mathsf{FedRo}$ could yield poor performance. We validate this observation by presenting an in-depth analysis
    
[^76]: 在何时以及如何：学习可识别的潜在状态进行非平稳时间序列预测

    When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting

    [https://arxiv.org/abs/2402.12767](https://arxiv.org/abs/2402.12767)

    提出了一种名为IDEA的模型，通过学习可识别的潜在状态检测时间序列数据中的分布变迁，并进一步分离平稳和非平稳的潜在状态。

    

    时间分布的转移在时间序列数据中是普遍存在的。其中一种最流行的方法假定时间分布的转移是均匀发生的，以区分平稳和非平稳的依赖关系。然而，这个假设很难满足，因为我们不知道分布何时发生转移。为了解决这个问题，我们提出了学习可识别的潜在状态（IDEA）来检测分布何时发生转移。除此之外，我们进一步通过充分观察假设来分离平稳和非平稳的潜在状态，学习潜在状态的变化方式。具体来说，我们将因果过程形式化为与环境不相关的稳定变量和与环境相关的非平稳变量。在温和的条件下，我们展示了潜在环境和稳定/非稳定变量是可识别的。基于这些理论，我们设计了IDEA模型，该模型结合了自回归隐马尔科夫模型。

    arXiv:2402.12767v1 Announce Type: new  Abstract: Temporal distribution shifts are ubiquitous in time series data. One of the most popular methods assumes that the temporal distribution shift occurs uniformly to disentangle the stationary and nonstationary dependencies. But this assumption is difficult to meet, as we do not know when the distribution shifts occur. To solve this problem, we propose to learn IDentifiable latEnt stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we further disentangle the stationary and nonstationary latent states via sufficient observation assumption to learn how the latent states change. Specifically, we formalize the causal process with environment-irrelated station- ary and environment-related nonstationary variables. Under mild conditions, we show that latent environments and stationary/nonstationary variables are identifiable. Based on these theories, we devise the IDEA model, which incorporates an autoregressive hidden Markov m
    
[^77]: 在奇异性下的学习：改进WBIC和sBIC的信息准则

    Learning under Singularity: An Information Criterion improving WBIC and sBIC

    [https://arxiv.org/abs/2402.12762](https://arxiv.org/abs/2402.12762)

    LS信息准则旨在增强WBIC和sBIC的功能，有效处理非正则情况，具有稳定性，为奇异情况下的信息准则提供了新的方法

    

    我们介绍了一种新颖的信息准则（IC），称为在奇异性下的学习（LS），旨在增强广泛适用的贝叶斯信息准则（WBIC）和奇异贝叶斯信息准则（sBIC）的功能。 LS在没有正则性约束的情况下是有效的，并表现出稳定性。Watanabe定义了一个统计模型或学习机器为正则，如果从参数到概率分布的映射是一对一的，并且其Fisher信息矩阵是正定的。相反，不符合这些条件的模型被称为奇异。 在过去的十年中，已经提出了几种奇异情况下的信息准则，包括WBIC和sBIC。 WBIC适用于非正则情况，但在样本量很大且已知学习系数估计冗余时面临挑战。 相反，sBIC在广泛应用方面存在限制，因为它依赖于最大似然估计。

    arXiv:2402.12762v1 Announce Type: cross  Abstract: We introduce a novel Information Criterion (IC), termed Learning under Singularity (LS), designed to enhance the functionality of the Widely Applicable Bayes Information Criterion (WBIC) and the Singular Bayesian Information Criterion (sBIC). LS is effective without regularity constraints and demonstrates stability. Watanabe defined a statistical model or a learning machine as regular if the mapping from a parameter to a probability distribution is one-to-one and its Fisher information matrix is positive definite. In contrast, models not meeting these conditions are termed singular. Over the past decade, several information criteria for singular cases have been proposed, including WBIC and sBIC. WBIC is applicable in non-regular scenarios but faces challenges with large sample sizes and redundant estimation of known learning coefficients. Conversely, sBIC is limited in its broader application due to its dependence on maximum likelihood
    
[^78]: FGAD：自我提升的知识蒸馏，用于有效的联邦图异常检测框架

    FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework

    [https://arxiv.org/abs/2402.12761](https://arxiv.org/abs/2402.12761)

    提出了一种有效的联邦图异常检测框架FGAD，通过自我提升的知识蒸馏解决了联邦学习中的非独立同分布问题和高通信成本，推动图异常检测领域的发展。

    

    图异常检测 (GAD) 旨在识别明显偏离其他图的异常图，由于在许多现实场景中存在图结构数据的广泛存在和复杂性，GAD引起了越来越多的关注。然而，现有的GAD方法通常需要在集中训练，这可能导致在某些敏感场合存在隐私泄露风险，从而阻碍了寻求共同开发强大GAD模型的组织之间的合作。尽管联邦学习提供了一个有前途的解决方案，但普遍存在的非独立同分布问题和高通信成本在分布在不同参与者之间的图数据合作中提出了重大挑战。为了解决这些挑战，我们提出了一种有效的联邦图异常检测框架 (FGAD)。我们首先引入一个异常生成器来扰动正常图以成为异常图，并训练一个强大的异常检测

    arXiv:2402.12761v1 Announce Type: new  Abstract: Graph anomaly detection (GAD) aims to identify anomalous graphs that significantly deviate from other ones, which has raised growing attention due to the broad existence and complexity of graph-structured data in many real-world scenarios. However, existing GAD methods usually execute with centralized training, which may lead to privacy leakage risk in some sensitive cases, thereby impeding collaboration among organizations seeking to collectively develop robust GAD models. Although federated learning offers a promising solution, the prevalent non-IID problems and high communication costs present significant challenges, particularly pronounced in collaborations with graph data distributed among different participants. To tackle these challenges, we propose an effective federated graph anomaly detection framework (FGAD). We first introduce an anomaly generator to perturb the normal graphs to be anomalous, and train a powerful anomaly dete
    
[^79]: 室内定位基于Wi-Fi指纹的静态数据库与动态数据库：基于数据视角的讨论

    Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi Fingerprinting: A Discussion from a Data Perspective

    [https://arxiv.org/abs/2402.12756](https://arxiv.org/abs/2402.12756)

    该论文讨论了由于室内环境中电磁干扰的时变特性，静态数据库和动态数据库对基于Wi-Fi指纹的室内定位的不同影响。

    

    Wi-Fi指纹识别已成为室内定位最流行的方法。机器学习算法的使用极大地提高了Wi-Fi指纹识别的定位性能，但其成功取决于由大量RSSI、接入点的MAC地址和其他测量信息组成的指纹数据库的可用性。然而，大多数指纹数据库未能很好地反映复杂现代室内环境中电磁干扰的时变特性。这可能导致训练/验证和测试数据集的统计特征发生显著变化，这些数据集通常是在不同时间构建的，甚至测试数据集的特征可能与在部署后定位系统运行期间由用户提交的数据的特征不同。本文考虑了时间变化的Wi-Fi指纹对定位系统性能的影响。

    arXiv:2402.12756v1 Announce Type: new  Abstract: Wi-Fi fingerprinting has emerged as the most popular approach to indoor localization. The use of ML algorithms has greatly improved the localization performance of Wi-Fi fingerprinting, but its success depends on the availability of fingerprint databases composed of a large number of RSSIs, the MAC addresses of access points, and the other measurement information. However, most fingerprint databases do not reflect well the time varying nature of electromagnetic interferences in complicated modern indoor environment. This could result in significant changes in statistical characteristics of training/validation and testing datasets, which are often constructed at different times, and even the characteristics of the testing datasets could be different from those of the data submitted by users during the operation of localization systems after their deployment. In this paper, we consider the implications of time-varying Wi-Fi fingerprints on
    
[^80]: 基于多模态和多级特征融合的高级持久性威胁行为者归因方法

    APT-MMF: An advanced persistent threat actor attribution method based on multimodal and multilevel feature fusion

    [https://arxiv.org/abs/2402.12743](https://arxiv.org/abs/2402.12743)

    提出了一种基于多模态和多级特征融合的高级持续性威胁行为者归因方法，利用异构属性图和多模态特征提取来增强网络威胁情报中的威胁行为者归因能力

    

    威胁行为者归因是对抗高级持久性威胁（APTs）的重要防御策略。网络威胁情报（CTI）在APTs中起着重要作用，涉及对多源异构数据进行分析。当前的归因方法从不同的CTI视角提取特征，并利用机器学习模型根据威胁行为者对CTI报告进行分类。然而，这些方法通常只提取一种特征，并忽略异构信息，尤其是指示器威胁的属性和关系，这构成了CTI的核心。为解决这些问题，我们提出了一种基于多模态和多级特征融合（APT-MMF）的APT行为者归因方法。首先，我们利用异构属性图来表征APT报告及其IOCs信息。然后，我们提取并融合多模态特征，包括属性类型特征

    arXiv:2402.12743v1 Announce Type: cross  Abstract: Threat actor attribution is a crucial defense strategy for combating advanced persistent threats (APTs). Cyber threat intelligence (CTI), which involves analyzing multisource heterogeneous data from APTs, plays an important role in APT actor attribution. The current attribution methods extract features from different CTI perspectives and employ machine learning models to classify CTI reports according to their threat actors. However, these methods usually extract only one kind of feature and ignore heterogeneous information, especially the attributes and relations of indicators of compromise (IOCs), which form the core of CTI. To address these problems, we propose an APT actor attribution method based on multimodal and multilevel feature fusion (APT-MMF). First, we leverage a heterogeneous attributed graph to characterize APT reports and their IOC information. Then, we extract and fuse multimodal features, including attribute type feat
    
[^81]: 本地解释的保证区域

    Guarantee Regions for Local Explanations

    [https://arxiv.org/abs/2402.12737](https://arxiv.org/abs/2402.12737)

    提出了一种基于锚点的算法，可以识别出局部解释被明确保证正确的区域，产生一个可解释的特征对齐盒，相较于现有基线，能找到具有更大保证区域的解释。

    

    使用局部替代模型（如LIME）的可解释性方法非常擅长描述在感兴趣点的预测模型行为，但它们不能保证对周围局部区域进行外推。然而，对预测模型的局部曲率过拟合和恶意篡改可以显著限制外推。我们提出了一种基于锚点的算法，用于识别局部解释被明确保证正确的区域，从而明确描述那些可以信任输入特征的间隔。我们的方法产生一个可解释的特征对齐盒，其中局部替代模型的预测保证与预测模型匹配。我们展示了我们的算法可以用于找到具有更大保证区域的解释，比现有基线更好地覆盖数据流形。我们还展示了我们的方法如何识别引导

    arXiv:2402.12737v1 Announce Type: new  Abstract: Interpretability methods that utilise local surrogate models (e.g. LIME) are very good at describing the behaviour of the predictive model at a point of interest, but they are not guaranteed to extrapolate to the local region surrounding the point. However, overfitting to the local curvature of the predictive model and malicious tampering can significantly limit extrapolation. We propose an anchor-based algorithm for identifying regions in which local explanations are guaranteed to be correct by explicitly describing those intervals along which the input features can be trusted. Our method produces an interpretable feature-aligned box where the prediction of the local surrogate model is guaranteed to match the predictive model. We demonstrate that our algorithm can be used to find explanations with larger guarantee regions that better cover the data manifold compared to existing baselines. We also show how our method can identify mislead
    
[^82]: UMBCLU在SemEval-2024任务1A和1C中的表现：带有和不带有机器翻译的语义文本相关性

    UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation

    [https://arxiv.org/abs/2402.12730](https://arxiv.org/abs/2402.12730)

    使用机器翻译和大型语言模型，本文开发了用于非洲和亚洲语言语义文本相关性任务的两种模型，取得了比部分官方基准更好的效果。

    

    这篇论文描述了我们为SemEval-2024任务1开发的系统，“非洲和亚洲语言的语义文本相关性”。 该任务的目标是构建一个能够识别目标语言中属于非洲和亚洲语言集合的两个句子之间的语义文本相关性（STR）的模型。 我们参与了子任务A和C，并探索了利用大型语言模型（LLMs）进行监督和跨语言训练。 预训练的大型语言模型已被广泛用于机器翻译和语义相似性。 使用机器翻译和句子嵌入LLMs的组合，我们为子任务A开发了一个统一的STR模型，TranSem，并对STR数据上的T5系列模型进行了微调，用于子任务C的FineSem。 我们在子任务A中7种语言的模型结果比3种语言的官方基准更好，而与其他4种语言的基准相当。

    arXiv:2402.12730v1 Announce Type: cross  Abstract: This paper describes the system we developed for SemEval-2024 Task 1, "Semantic Textual Relatedness for African and Asian Languages." The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two sentences of a target language belonging to a collection of African and Asian languages. We participated in Subtasks A and C and explored supervised and cross-lingual training leveraging large language models (LLMs). Pre-trained large language models have been extensively used for machine translation and semantic similarity. Using a combination of machine translation and sentence embedding LLMs, we developed a unified STR model, TranSem, for subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for use in subtask C. Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages. Our m
    
[^83]: 可扩展可靠的多尺度神经过程嵌入知识用于智能故障检测的深度迁移学习

    Scalable and reliable deep transfer learning for intelligent fault detection via multi-scale neural processes embedded with knowledge

    [https://arxiv.org/abs/2402.12729](https://arxiv.org/abs/2402.12729)

    提出了一种名为GTNP的神经过程深度迁移学习方法，通过特征传输策略弥合源域和目标域的数据分布差异，解决了数据稀缺和缺乏可靠性分析的问题

    

    深度迁移学习（DTL）是智能故障检测（IFD）领域中的一种基本方法，旨在减轻训练集（源域）和测试集（目标域）之间数据分布不一致导致方法性能下降的问题。本文提出了一种名为基于神经过程的图卷积网络（GTNP）的新颖的DTL方法，可以解决数据分布不一致和可靠性问题。

    arXiv:2402.12729v1 Announce Type: cross  Abstract: Deep transfer learning (DTL) is a fundamental method in the field of Intelligent Fault Detection (IFD). It aims to mitigate the degradation of method performance that arises from the discrepancies in data distribution between training set (source domain) and testing set (target domain). Considering the fact that fault data collection is challenging and certain faults are scarce, DTL-based methods face the limitation of available observable data, which reduces the detection performance of the methods in the target domain. Furthermore, DTL-based methods lack comprehensive uncertainty analysis that is essential for building reliable IFD systems. To address the aforementioned problems, this paper proposes a novel DTL-based method known as Neural Processes-based deep transfer learning with graph convolution network (GTNP). Feature-based transfer strategy of GTNP bridges the data distribution discrepancies of source domain and target domain 
    
[^84]: 基于大型语言模型的模态感知集成用于基于知识的视觉问答

    Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering

    [https://arxiv.org/abs/2402.12728](https://arxiv.org/abs/2402.12728)

    提出了一种模态感知的LLM集成方法（MAIL）用于针对KVQA，通过细致地利用多模态知识来处理图像理解和知识推理。

    

    知识驱动的视觉问答（KVQA）已被广泛研究，以利用外部知识如知识图谱（KG）来回答视觉问题。尽管已提出几种尝试利用大型语言模型（LLMs）作为隐含知识源，但由于LLMs可能生成幻觉，因此仍然具有挑战性。此外，多种知识来源，例如图像、知识图谱和LLMs，不能轻易对齐以应对复杂场景。为了解决这些问题，我们提出了一种针对KVQA的新颖的具有模态感知的LLM集成方法（MAIL）。它精心利用多模态知识进行图像理解和知识推理。具体而言，（i）我们提出了一种使用LLMs的两阶段提示策略，将图像密集地融入带有详细视觉特征的场景图中；（ii）我们通过将提到的实体与外部事实联系起来构建一个耦合的概念图；（iii）设计了一个定制的伪孪生图中介融合。

    arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
    
[^85]: 扩散后验抽样在计算上是难以解决的

    Diffusion Posterior Sampling is Computationally Intractable

    [https://arxiv.org/abs/2402.12727](https://arxiv.org/abs/2402.12727)

    我们证明了后验抽样在计算上是难以解决的：在加密学中最基本的假设下——单向函数存在的假设下，存在一些实例，对于这些实例，每个算法都需要超多项式时间，即使无条件抽样可以证明是快速的。

    

    扩散模型是学习和从分布$p(x)$中抽样的一种非常有效的方法。在后验抽样中，人们还会给出一个测量模型$p(y \mid x)$和一个测量$y$，希望从$p(x \mid y)$中抽样。后验抽样对于诸如修补、超分辨率和MRI重建等任务非常有用，因此一些最近的工作已经给出了启发式近似算法；但没有一个已知能在多项式时间内收敛到正确的分布。

    arXiv:2402.12727v1 Announce Type: cross  Abstract: Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x \mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time.   In this paper we show that posterior sampling is \emph{computationally intractable}: under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which \emph{every} algorithm takes superpolynomial time, even though \emph{unconditional} sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption 
    
[^86]: 结构知识指导的持续多变量时间序列预测

    Structural Knowledge Informed Continual Multivariate Time Series Forecasting

    [https://arxiv.org/abs/2402.12722](https://arxiv.org/abs/2402.12722)

    提出了一种新颖的结构知识指导的持续学习（SKI-CL）框架，用于在不同制度下持续积累的多变量时间序列（MTS）预测中，可以有效解决变量依赖关系的灾难性遗忘问题。

    

    近期研究表明，在多变量时间序列（MTS）预测中，明确建模不同时间序列之间的隐藏依赖关系可以产生有前途的预测性能和可靠的解释。然而，当MTS在不同的制度（阶段）下持续积累时，建模变量之间的依赖仍未得到充分探讨。由于潜在的分布和依赖关系差异，基础模型可能会遇到灾难性遗忘问题，即在保持预测性能的同时，记忆和推断不同类型的变量依赖关系是具有挑战性的。为了解决这个问题，我们提出了一种新颖的结构知识指导的持续学习（SKI-CL）框架，以在持续学习范式中执行MTS预测，利用结构知识引导预测模型识别和适应不同制度，并选择repr。

    arXiv:2402.12722v1 Announce Type: new  Abstract: Recent studies in multivariate time series (MTS) forecasting reveal that explicitly modeling the hidden dependencies among different time series can yield promising forecasting performance and reliable explanations. However, modeling variable dependencies remains underexplored when MTS is continuously accumulated under different regimes (stages). Due to the potential distribution and dependency disparities, the underlying model may encounter the catastrophic forgetting problem, i.e., it is challenging to memorize and infer different types of variable dependencies across different regimes while maintaining forecasting performance. To address this issue, we propose a novel Structural Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS forecasting within a continual learning paradigm, which leverages structural knowledge to steer the forecasting model toward identifying and adapting to different regimes, and selects repr
    
[^87]: 机器学习中的虚假相关性：一项调查

    Spurious Correlations in Machine Learning: A Survey

    [https://arxiv.org/abs/2402.12715](https://arxiv.org/abs/2402.12715)

    机器学习系统对输入中偏见特征与标签之间的虚假相关性敏感，本文回顾了解决这一问题的最新方法，同时总结了数据集、基准和度量标准，并讨论了未来研究挑战。

    

    众所周知，机器学习系统对输入中偏见特征（例如背景、纹理和次要对象）与相应标签之间的虚假相关性敏感。这些特征及其与标签的相关性被称为“虚假”，因为它们往往随着真实世界数据分布的变化而改变，这可能对模型的泛化能力和鲁棒性产生负面影响。在这项调查中，我们全面审查了这一问题，提供了一个关于解决机器学习模型中虚假相关性的当前最先进方法的分类法。此外，我们总结了现有的数据集、基准和度量标准，以帮助未来的研究。本文最后讨论了这一领域的最新进展和未来研究挑战，旨在为相关领域的研究人员提供宝贵的见解。

    arXiv:2402.12715v1 Announce Type: new  Abstract: Machine learning systems are known to be sensitive to spurious correlations between biased features of the inputs (e.g., background, texture, and secondary objects) and the corresponding labels. These features and their correlations with the labels are known as "spurious" because they tend to change with shifts in real-world data distributions, which can negatively impact the model's generalization and robustness. In this survey, we provide a comprehensive review of this issue, along with a taxonomy of current state-of-the-art methods for addressing spurious correlations in machine learning models. Additionally, we summarize existing datasets, benchmarks, and metrics to aid future research. The paper concludes with a discussion of the recent advancements and future research challenges in this field, aiming to provide valuable insights for researchers in the related domains.
    
[^88]: 具有等变性的预训练Transformer用于多域3D分子的统一几何学习

    Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules

    [https://arxiv.org/abs/2402.12714](https://arxiv.org/abs/2402.12714)

    提出了具有等变性的预训练Transformer(EPT)框架，能够统一多领域分子的几何学习，通过块增强表示和E(3)等变性实现更准确的3D结构表示。

    

    通过在大量未标记的3D分子上进行预训练已经展示出在各种科学应用中具有优势。然而，先前的努力通常集中在特定领域（蛋白质或小分子）的模型预训练上，错失了利用跨领域知识的机会。为了弥补这一差距，我们引入了等变预训练Transformer（EPT），这是一个新颖的预训练框架，旨在协调小分子和蛋白质的几何学习。具体来说，EPT通过块增强表示统一了多领域分子的几何建模，能够关注每个原子更广泛的上下文。在Transformer框架上，EPT进一步通过E(3)等变性进行增强，以促进准确表示3D结构。EPT的另一个关键创新是其块级预训练任务，这允许在包含小分子和蛋白质的数据集上进行联合预训练。

    arXiv:2402.12714v1 Announce Type: new  Abstract: Pretraining on a large number of unlabeled 3D molecules has showcased superiority in various scientific applications. However, prior efforts typically focus on pretraining models on a specific domain, either proteins or small molecules, missing the opportunity to leverage the cross-domain knowledge. To mitigate this gap, we introduce Equivariant Pretrained Transformer (EPT), a novel pretraining framework designed to harmonize the geometric learning of small molecules and proteins. To be specific, EPT unifies the geometric modeling of multi-domain molecules via the block-enhanced representation that can attend a broader context of each atom. Upon transformer framework, EPT is further enhanced with E(3) equivariance to facilitate the accurate representation of 3D structures. Another key innovation of EPT is its block-level pretraining task, which allows for joint pretraining on datasets comprising both small molecules and proteins. Experim
    
[^89]: 具有统一最后迭代保证的赌博算法实现接近最优遗憾

    Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee

    [https://arxiv.org/abs/2402.12711](https://arxiv.org/abs/2402.12711)

    本文引入了统一最后迭代(ULI)保证这一更强的性能度量，同时证明接近最优的ULI保证直接导致了在各种性能度量上接近最优的累积性能。

    

    现有的赌博算法性能度量，如遗憾、PAC界限或统一PAC(Dann等人，2017)，通常评估累积性能，同时允许在任意有限时间t内玩弱劣的臂。这种行为在高风险应用中可能造成严重损失。本文介绍了一种更强的性能度量，统一最后迭代(ULI)保证，捕捉赌博算法的累积和瞬时性能。具体来说，ULI表征了瞬时性能，因为它确保所玩弱劣臂的每轮遗憾受到一个函数的限制，该函数随着（大）轮次t单调递减，在有足够样本可用时防止重复访问劣质臂。我们证明，接近最优的ULI保证直接意味着在上述性能度量中实现接近最优的累积性能。为了研究ULI在有限臂集上的可达性

    arXiv:2402.12711v1 Announce Type: new  Abstract: Existing performance measures for bandit algorithms such as regret, PAC bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative performance, while allowing the play of an arbitrarily bad arm at any finite time t. Such a behavior can be highly detrimental in high-stakes applications. This paper introduces a stronger performance measure, the uniform last-iterate (ULI) guarantee, capturing both cumulative and instantaneous performance of bandit algorithms. Specifically, ULI characterizes the instantaneous performance since it ensures that the per-round regret of the played arm is bounded by a function, monotonically decreasing w.r.t. (large) round t, preventing revisits to bad arms when sufficient samples are available. We demonstrate that a near-optimal ULI guarantee directly implies near-optimal cumulative performance across aforementioned performance measures. To examine the achievability of ULI in the finite arm se
    
[^90]: 在因果推断中整合干预学习：在线实验中一种新颖方法

    Integrating Active Learning in Causal Inference with Interference: A Novel Approach in Online Experiments

    [https://arxiv.org/abs/2402.12710](https://arxiv.org/abs/2402.12710)

    这项研究通过引入主动学习方法ACI，专注于在网络干扰和非随机处理分配情况下估计直接和溢出处理效应。

    

    在因果推断研究领域中，普遍的潜在结果框架，尤其是鲁宾因果模型（RCM），通常忽视个体干扰并假设独立处理效应。然而，这一假设经常与现实世界场景的复杂现实不符，干扰不仅仅是可能性，而且是常见现象。我们的研究旨在通过专注于在两种假设下估计直接和溢出处理效应来解决这一差异：（1）基于网络的干扰，其中连接网络内邻居的处理会影响一个人的结果，以及（2）受混杂因素影响的非随机处理分配。为了提高估计可能复杂效果函数的效率，我们引入了一种新颖的主动学习方法: 交互干预因果推断中的主动学习（ACI）。这种方法利用高斯过程灵活地mo

    arXiv:2402.12710v1 Announce Type: cross  Abstract: In the domain of causal inference research, the prevalent potential outcomes framework, notably the Rubin Causal Model (RCM), often overlooks individual interference and assumes independent treatment effects. This assumption, however, is frequently misaligned with the intricate realities of real-world scenarios, where interference is not merely a possibility but a common occurrence. Our research endeavors to address this discrepancy by focusing on the estimation of direct and spillover treatment effects under two assumptions: (1) network-based interference, where treatments on neighbors within connected networks affect one's outcomes, and (2) non-random treatment assignments influenced by confounders. To improve the efficiency of estimating potentially complex effects functions, we introduce an novel active learning approach: Active Learning in Causal Inference with Interference (ACI). This approach uses Gaussian process to flexibly mo
    
[^91]: 用Transformer进行量子嵌入以处理高维数据

    Quantum Embedding with Transformer for High-dimensional Data

    [https://arxiv.org/abs/2402.12704](https://arxiv.org/abs/2402.12704)

    这项研究提出了一种结合transformer的量子嵌入架构，显著提升了处理高维数据的能力，验证了其在现代量子机器学习问题中的高度灵活和实用性。

    

    arXiv:2402.12704v1 公告类型：跨界 摘要：量子嵌入与transformers结合是量子机器学习中一种新颖且有前景的架构，在近期设备或模拟器上表现出卓越的能力。该研究将视觉transformer（ViT）引入，显著提升了量子嵌入的能力，并在BirdCLEF-2021上的单量子比特分类器中取得了约3%的中值F1得分，证明了我们基于transformer的架构是处理现代量子机器学习问题的一种高度灵活且实用的方法。

    arXiv:2402.12704v1 Announce Type: cross  Abstract: Quantum embedding with transformers is a novel and promising architecture for quantum machine learning to deliver exceptional capability on near-term devices or simulators. The research incorporated a vision transformer (ViT) to advance quantum significantly embedding ability and results for a single qubit classifier with around 3 percent in the median F1 score on the BirdCLEF-2021, a challenging high-dimensional dataset. The study showcases and analyzes empirical evidence that our transformer-based architecture is a highly versatile and practical approach to modern quantum machine learning problems.
    
[^92]: 复兴多变量时间序列预测：可学习分解与跨系列依赖关系和内部变化建模

    Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling

    [https://arxiv.org/abs/2402.12694](https://arxiv.org/abs/2402.12694)

    引入可学习分解策略和双注意力模块，同时捕捉跨系列依赖和内部变化，以应对复杂的多变量时间序列预测挑战。

    

    预测多变量时间序列是至关重要的，要求精确建模错综复杂模式，包括跨时间序列的依赖关系和内部变动。每个时间序列具有独特的趋势特征带来挑战，现有方法依赖基本的移动平均核可能难以处理现实数据中的非线性结构和复杂趋势。基于此，我们引入了一个可学习的分解策略，更合理地捕捉动态趋势信息。此外，我们提出了一个双注意力模块，专门用于同时捕捉跨系列依赖关系和内部变化，以实现更好的时间序列预测，其中通过通道自注意力和自回归自注意力实现。为了评估我们方法的有效性，我们在八个开源数据集上进行了实验，并将其与最先进的方法进行了比较。通过比较结果，我们的 Leddam...

    arXiv:2402.12694v1 Announce Type: new  Abstract: Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam
    
[^93]: 在流形上学习而无需流形学习

    Learning on manifolds without manifold learning

    [https://arxiv.org/abs/2402.12687](https://arxiv.org/abs/2402.12687)

    提出了一种无需流形学习的在流形上学习方法，通过一次性构造获得最佳误差界限。

    

    从未知分布随机抽样的数据进行函数逼近是机器学习中的一个重要问题。与通过最小化损失函数来解决这个问题的盛行范式相反，我们给出了一种直接的一次性构造方法，并在流形假设下给出了最佳误差界限；即假设数据是从高维欧几里得空间的未知子流形中抽样得到的。 Neural Networks 132:253268, 2020 中，我们提出了一个一次性直接方法来实现函数逼近。

    arXiv:2402.12687v1 Announce Type: new  Abstract: Function approximation based on data drawn randomly from an unknown distribution is an important problem in machine learning. In contrast to the prevalent paradigm of solving this problem by minimizing a loss functional, we have given a direct one-shot construction together with optimal error bounds under the manifold assumption; i.e., one assumes that the data is sampled from an unknown sub-manifold of a high dimensional Euclidean space. A great deal of research deals with obtaining information about this manifold, such as the eigendecomposition of the Laplace-Beltrami operator or coordinate charts, and using this information for function approximation. This two step approach implies some extra errors in the approximation stemming from basic quantities of the data in addition to the errors inherent in function approximation. In Neural Networks, 132:253268, 2020, we have proposed a one-shot direct method to achieve function approximation
    
[^94]: TorchCP：基于PyTorch的一种适用于合拟常规预测的库

    TorchCP: A Library for Conformal Prediction based on PyTorch

    [https://arxiv.org/abs/2402.12683](https://arxiv.org/abs/2402.12683)

    TorchCP是一个基于PyTorch的Python工具包，为深度学习模型上的合拟常规预测研究提供了实现后验和训练方法的多种工具，包括分类和回归任务。En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.

    

    TorchCP是一个用于深度学习模型上的合拟常规预测研究的Python工具包。它包含了用于后验和训练方法的各种实现，用于分类和回归任务（包括多维输出）。TorchCP建立在PyTorch之上，并利用矩阵计算的优势，提供简洁高效的推理实现。该代码采用LGPL许可证，并在$\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$开源。

    arXiv:2402.12683v1 Announce Type: new  Abstract: TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$.
    
[^95]: 超越最坏情况攻击：通过非支配策略实现自适应防御的鲁棒RL

    Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies

    [https://arxiv.org/abs/2402.12673](https://arxiv.org/abs/2402.12673)

    研究将RL策略鲁棒性扩展至状态对抗攻击模型，超越仅针对最坏情况攻击，提出基于后悔最小化问题的自适应防御方法。

    

    随着强化学习（RL）在各种现实世界应用中取得的蓬勃发展，人们开始集中关注确保RL策略在测试时对抗性攻击的鲁棒性。目前的方法主要围绕着解决极端情况问题，以应对潜在的最坏情况。尽管这些方法对强攻击具有一定的效果，但往往在没有攻击或只有弱攻击存在时会牺牲性能。为了解决这个问题，我们研究了在广泛接受的状态对抗攻击模型下的策略鲁棒性，将重点从仅限于最坏情况攻击扩展出来。我们首先将测试时的任务正式化为一个后悔最小化问题，并在基准策略来自于通用连续策略类$\Pi$时，在实现亚线性后悔的困难性问题上做出了阐述。这一发现促使我们在测试之前\textit{优化}基准策略类$\Pi$，致力于

    arXiv:2402.12673v1 Announce Type: new  Abstract: In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond only worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic hardness in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\Pi$. This finding prompts us to \textit{refine} the baseline policy class $\Pi$ prior to test time, aimin
    
[^96]: 随机化既可以减少偏差又可以减少方差：随机森林的案例研究

    Randomization Can Reduce Both Bias and Variance: A Case Study in Random Forests

    [https://arxiv.org/abs/2402.12668](https://arxiv.org/abs/2402.12668)

    随机森林相对于装袋法具有减少偏差的能力，在揭示数据模式和高信噪比情况下表现更好的特点，为随机森林在不同信噪比环境下的成功提供了解释和实用见解。

    

    我们研究了往往被忽视的现象，首次在\cite{breiman2001random}中指出，即随机森林似乎比装袋法减少了偏差。受\cite{mentch2020randomization}一篇有趣的论文的启发，其中作者认为随机森林减少了有效自由度，并且只有在低信噪比（SNR）环境下才能胜过装袋集成，我们探讨了随机森林如何能够揭示被装袋法忽视的数据模式。我们在实证中证明，在存在这种模式的情况下，随机森林不仅可以减小偏差还能减小方差，并且当信噪比高时随机森林的表现愈发好于装袋集成。我们的观察为解释随机森林在各种信噪比情况下的真实世界成功提供了见解，并增进了我们对随机森林与装袋集成在每次分割注入的随机化方面的差异的理解。我们的调查结果还提供了实用见解。

    arXiv:2402.12668v1 Announce Type: cross  Abstract: We study the often overlooked phenomenon, first noted in \cite{breiman2001random}, that random forests appear to reduce bias compared to bagging. Motivated by an interesting paper by \cite{mentch2020randomization}, where the authors argue that random forests reduce effective degrees of freedom and only outperform bagging ensembles in low signal-to-noise ratio (SNR) settings, we explore how random forests can uncover patterns in the data missed by bagging. We empirically demonstrate that in the presence of such patterns, random forests reduce bias along with variance and increasingly outperform bagging ensembles when SNR is high. Our observations offer insights into the real-world success of random forests across a range of SNRs and enhance our understanding of the difference between random forests and bagging ensembles with respect to the randomization injected into each split. Our investigations also yield practical insights into the 
    
[^97]: 判别距离感知表示在确定性不确定性量化方法上的研究

    Discriminant Distance-Aware Representation on Deterministic Uncertainty Quantification Methods

    [https://arxiv.org/abs/2402.12664](https://arxiv.org/abs/2402.12664)

    提出了一种新颖高效的确定性不确定性估计方法DDAR，通过引入判别距离感知表示和最优可训练原型，克服了特征塌缩问题，证明了其灵活性和架构无关性

    

    不确定性估计是在部署可靠的深度学习模型到安全关键系统中的一个关键方面。本研究引入了一种新颖高效的确定性不确定性估计方法，称为判别距离感知表示（DDAR）。我们的方法涉及构建一个深度神经网络模型，在其潜在表示中包含一组原型，使我们能够从输入数据中分析有价值的特征信息。通过在最优可训练原型上应用区分最大化层，DDAR能够学习到判别距离感知表示。我们证明了DDAR通过放宽束缚确定性不确定性方法（DUMs）架构的实用性所阻碍的李普希茨约束，克服了特征塌缩问题。我们的实验表明，DDAR是一种灵活的、与架构无关的方法，可以作为一个可插入的层轻松集成到具有距离敏感度量的模型中。

    arXiv:2402.12664v1 Announce Type: new  Abstract: Uncertainty estimation is a crucial aspect of deploying dependable deep learning models in safety-critical systems. In this study, we introduce a novel and efficient method for deterministic uncertainty estimation called Discriminant Distance-Awareness Representation (DDAR). Our approach involves constructing a DNN model that incorporates a set of prototypes in its latent representations, enabling us to analyze valuable feature information from the input data. By leveraging a distinction maximization layer over optimal trainable prototypes, DDAR can learn a discriminant distance-awareness representation. We demonstrate that DDAR overcomes feature collapse by relaxing the Lipschitz constraint that hinders the practicality of deterministic uncertainty methods (DUMs) architectures. Our experiments show that DDAR is a flexible and architecture-agnostic method that can be easily integrated as a pluggable layer with distance-sensitive metrics,
    
[^98]: SoftQE: LLM扩展的查询学习表示

    SoftQE: Learned Representations of Queries Expanded by LLMs

    [https://arxiv.org/abs/2402.12663](https://arxiv.org/abs/2402.12663)

    SoftQE通过将输入查询的嵌入映射到LLM扩展查询的嵌入，提高了密集检索性能，并在领域外任务上取得了显著的性能改善。

    

    我们研究了将大型语言模型(LLMs)集成到查询编码器中，以改善密集检索，同时避免在推断时依赖LLMs增加延迟和成本。SoftQE通过将输入查询的嵌入映射到LLM扩展查询的嵌入来整合LLMs的知识。虽然对于领域内MS-MARCO指标，SoftQE相对于各种强基准模型的改善有限，但在五个领域外BEIR任务上，SoftQE在平均性能上提高了2.83个绝对百分点。

    arXiv:2402.12663v1 Announce Type: new  Abstract: We investigate the integration of Large Language Models (LLMs) into query encoders to improve dense retrieval without increasing latency and cost, by circumventing the dependency on LLMs at inference time. SoftQE incorporates knowledge from LLMs by mapping embeddings of input queries to those of the LLM-expanded queries. While improvements over various strong baselines on in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83 absolute percentage points on average on five out-of-domain BEIR tasks.
    
[^99]: HyperMoE: 通过专家之间的知识传递实现更好的专家混合

    HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts

    [https://arxiv.org/abs/2402.12656](https://arxiv.org/abs/2402.12656)

    HyperMoE通过Hypernetworks框架整合知识传递的概念，解决了在专家选择过程中专家知识稀疏性和可用性之间的矛盾。

    

    混合专家(MoE)在语言模型中被证明有效地增强了模型的能力，通过动态地将每个输入标记路由到特定的专家子集进行处理。尽管取得了成功，但大多数现有方法在专家知识的稀疏性和可用性之间面临挑战：通过增加对专家知识的使用来增强性能，往往会导致在专家选择过程中稀疏度减少。为了缓解这一矛盾，我们提出了HyperMoE，这是一个建立在Hypernetworks之上的新颖MoE框架。该框架将MoE的计算过程与多任务学习中的知识传递概念进行了集成。基于未选择专家信息生成的特定模块作为补充信息，允许未被选中的专家的知识在保持选择稀疏性的同时被使用。

    arXiv:2402.12656v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multi
    
[^100]: 通过坐标搜索算法训练人工神经网络

    Training Artificial Neural Networks by Coordinate Search Algorithm

    [https://arxiv.org/abs/2402.12646](https://arxiv.org/abs/2402.12646)

    通过提出的高效版本的非梯度坐标搜索（CS）算法，我们可以训练神经网络，解决了需要可微激活函数和同时优化多个非可微损失函数的问题。

    

    训练人工神经网络在机器学习中是一个具有挑战性和关键性的问题。 尽管梯度下降等基于梯度的学习方法在训练神经网络方面有效，但它们也存在一些限制。 例如，它们需要可微激活函数，并且不能基于多个独立的非可微损失函数同时优化模型；例如，在测试期间使用的 F1 分数可以在训练期间使用，当采用无梯度优化算法时。 此外，任何 DNN 中的训练可能只需很少量的训练数据集。 为了解决这些问题，我们提出了非梯度坐标搜索（CS）算法的高效版本，它是通用模式搜索方法的一种实例，用于训练神经网络。

    arXiv:2402.12646v1 Announce Type: cross  Abstract: Training Artificial Neural Networks poses a challenging and critical problem in machine learning. Despite the effectiveness of gradient-based learning methods, such as Stochastic Gradient Descent (SGD), in training neural networks, they do have several limitations. For instance, they require differentiable activation functions, and cannot optimize a model based on several independent non-differentiable loss functions simultaneously; for example, the F1-score, which is used during testing, can be used during training when a gradient-free optimization algorithm is utilized. Furthermore, the training in any DNN can be possible with a small size of the training dataset. To address these concerns, we propose an efficient version of the gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern Search methods, for training neural networks. The proposed algorithm can be used with non-differentiable activation functions and
    
[^101]: FAST: 一种用于快速透明机器学习中快速附加分割的优化框架

    FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML

    [https://arxiv.org/abs/2402.12630](https://arxiv.org/abs/2402.12630)

    FAST框架通过快速分段形状函数的优化和新的特征选择算法，使得透明的附加模型的拟合速度比现有方法快2个数量级。

    

    我们提出了FAST，一种用于快速附加分割的优化框架。FAST为数据集中的每个特征分段常数形状函数，以产生透明的附加模型。该框架利用一种新颖的优化过程适配这些模型，速度比现有最先进的方法，如可解释性增强机器 \citep{nori2019interpretml}，快约2个数量级。我们还在FAST框架中开发了新的特征选择算法，以适配性能良好的简约模型。通过实验证明，FAST提高了附加模型的计算效率和可解释性。

    arXiv:2402.12630v1 Announce Type: cross  Abstract: We present FAST, an optimization framework for fast additive segmentation. FAST segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models. The framework leverages a novel optimization procedure to fit these models $\sim$2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines \citep{nori2019interpretml}. We also develop new feature selection algorithms in the FAST framework to fit parsimonious models that perform well. Through experiments and case studies, we show that FAST improves the computational efficiency and interpretability of additive models.
    
[^102]: 机器学习在数据变化方面的综合评论：跨领域透视

    A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective

    [https://arxiv.org/abs/2402.12627](https://arxiv.org/abs/2402.12627)

    该综述将领域转移和概念漂移重新归为一个单一的研究问题，即数据变化问题，系统地总结了这两个研究领域中最新的方法。

    

    近期的人工智能（AI）技术在各个学术领域和行业都展现了显著的发展，然而，在现实世界中，动态数据导致了部署AI模型的主要挑战。意外的数据变化会导致AI模型性能严重下降。我们确定了两个主要相关研究领域，领域转移和概念漂移，根据数据变化的设定。虽然这两个流行的研究领域的目标是解决分布偏移和非平稳数据流问题，但基本属性仍然相似，这也鼓励采用类似的技术方法。在这篇综述中，我们将领域转移和概念漂移重新组合成一个单一的研究问题，即数据变化问题，并系统地概述了这两个研究领域中最新方法。我们提出了一个三阶段问题分类方案，以将这两个技术领域的关键思想联系起来。

    arXiv:2402.12627v1 Announce Type: cross  Abstract: Recent artificial intelligence (AI) technologies show remarkable evolution in various academic fields and industries. However, in the real world, dynamic data lead to principal challenges for deploying AI models. An unexpected data change brings about severe performance degradation in AI models. We identify two major related research fields, domain shift and concept drift according to the setting of the data change. Although these two popular research fields aim to solve distribution shift and non-stationary data stream problems, the underlying properties remain similar which also encourages similar technical approaches. In this review, we regroup domain shift and concept drift into a single research problem, namely the data change problem, with a systematic overview of state-of-the-art methods in the two research fields. We propose a three-phase problem categorization scheme to link the key ideas in the two technical fields. We thus p
    
[^103]: 针对预训练特征提取器的任意数据毒化攻击研究

    Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors

    [https://arxiv.org/abs/2402.12626](https://arxiv.org/abs/2402.12626)

    本文研究了针对预训练特征提取器的任意数据毒化攻击，探讨了这种攻击对机器学习模型的安全风险和影响。

    

    机器学习模型在监督学习任务中取得了巨大成功，这需要大量标记数据，但这并不总是可行的。最近，许多从业者转向自监督学习方法，利用廉价的未标记数据通过预训练学习一个通用特征提取器，可以简单地通过训练一个额外的线性层并使用有限的标记数据来应用于个性化的下游任务。然而，这一过程也可能引发对数据毒化攻击的担忧。例如，任意数据毒化攻击旨在通过将少量毒化数据注入训练集来降低模型效用，这对机器学习模型构成安全风险，但目前仅在端到端监督学习中进行了研究。本文扩展了对应用预训练的下游任务的任意攻击威胁的探讨。

    arXiv:2402.12626v1 Announce Type: new  Abstract: Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible. Recently, many practitioners have shifted to self-supervised learning methods that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data. However, such a process may also raise concerns regarding data poisoning attacks. For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning. In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-t
    
[^104]: 多目标特征选择的紧凑型NSGA-II

    Compact NSGA-II for Multi-objective Feature Selection

    [https://arxiv.org/abs/2402.12625](https://arxiv.org/abs/2402.12625)

    提出了一种紧凑型NSGA-II算法，用于多目标特征选择，旨在提高分类准确度并减少所选特征数量

    

    特征选择是机器学习和数据挖掘中一项昂贵且具有挑战性的任务，旨在消除无关和冗余特征。这有助于提高分类准确度，以及分类或特征选择后进行的任何其他后处理任务的预算和内存要求。在这方面，我们将特征选择定义为一个多目标二进制优化任务，其目标是最大化分类准确度和最小化选择的特征数量。为了选择最优特征，我们提出了一种二进制紧凑型NSGA-II（CNSGA-II）算法。紧凑型代表将群体表示为概率分布，以增强进化算法不仅更节省内存，还减少评估的数量。在优化过程中，我们的方法利用若干概率向量，而不是保持两个群体。

    arXiv:2402.12625v1 Announce Type: new  Abstract: Feature selection is an expensive challenging task in machine learning and data mining aimed at removing irrelevant and redundant features. This contributes to an improvement in classification accuracy, as well as the budget and memory requirements for classification, or any other post-processing task conducted after feature selection. In this regard, we define feature selection as a multi-objective binary optimization task with the objectives of maximizing classification accuracy and minimizing the number of selected features. In order to select optimal features, we have proposed a binary Compact NSGA-II (CNSGA-II) algorithm. Compactness represents the population as a probability distribution to enhance evolutionary algorithms not only to be more memory-efficient but also to reduce the number of fitness evaluations. Instead of holding two populations during the optimization process, our proposed method uses several Probability Vectors (
    
[^105]: Reflect-RL：两个玩家在线RL微调语言模型

    Reflect-RL: Two-Player Online RL Fine-Tuning for LMs

    [https://arxiv.org/abs/2402.12621](https://arxiv.org/abs/2402.12621)

    提出Reflect-RL，使用在线强化学习来微调语言模型，在这过程中引入了反射模型协助，并采用了负例生成、单提示动作枚举和课程学习来提高性能。

    

    随着语言模型在各个领域展示其能力，将它们应用于需要多轮交互的任务变得越来越受欢迎。这些任务通常具有复杂的动态性，因此仅在有限的离线数据集上进行监督微调（SFT）无法取得良好性能。然而，只有少数研究尝试在交互式决策制定环境内直接对LM进行训练。我们旨在创建一个在这些环境中使用在线强化学习（RL）对LM进行微调的有效机制。我们提出了Reflect-RL，一个两个玩家的系统，使用在线RL对LM进行微调，在此过程中，一个冻结的反射模型辅助策略模型。为了为热身SFT阶段生成数据，我们使用负例生成来增强反射模型的纠错能力。此外，我们设计了单提示动作枚举，并应用了课程学习让策略模型学习更多。

    arXiv:2402.12621v1 Announce Type: cross  Abstract: As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more 
    
[^106]: 生成式人工智能安全：挑战与对策

    Generative AI Security: Challenges and Countermeasures

    [https://arxiv.org/abs/2402.12617](https://arxiv.org/abs/2402.12617)

    生成式人工智能的安全挑战及对策研究。

    

    arXiv:2402.12617v1 公告类型：跨领域 摘要：生成式人工智能在许多行业的不断扩展引发了人们的兴奋和增加的关注。本文深入探讨了生成式人工智能所带来的独特安全挑战，并概述了管理这些风险的潜在研究方向。

    arXiv:2402.12617v1 Announce Type: cross  Abstract: Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.
    
[^107]: 多目标二进制坐标搜索特征选择

    Multi-objective Binary Coordinate Search for Feature Selection

    [https://arxiv.org/abs/2402.12616](https://arxiv.org/abs/2402.12616)

    这项研究提出了二进制多目标坐标搜索（MOCS）算法，用于解决大规模特征选择问题，是首个多目标坐标搜索算法。

    

    一种监督特征选择方法选择适当但简洁的特征集以区分类别，在处理大规模数据集时成本高昂。因此，特征选择应旨在既减少选择的特征数量又最大化分类准确性，或任何其他任务。然而，这一关键任务在许多现实世界数据集上计算量极大，并且需要非常高效的算法才能通过有限数量的适应度评估达到一组最佳特征。为此，我们提出了二进制多目标坐标搜索（MOCS）算法来解决大规模特征选择问题。据我们所知，本文提出的算法是首个多目标坐标搜索算法。在这种方法中，我们通过翻转帕累托前沿候选解的变量来生成新个体。这使我们能够研究

    arXiv:2402.12616v1 Announce Type: new  Abstract: A supervised feature selection method selects an appropriate but concise set of features to differentiate classes, which is highly expensive for large-scale datasets. Therefore, feature selection should aim at both minimizing the number of selected features and maximizing the accuracy of classification, or any other task. However, this crucial task is computationally highly demanding on many real-world datasets and requires a very efficient algorithm to reach a set of optimal features with a limited number of fitness evaluations. For this purpose, we have proposed the binary multi-objective coordinate search (MOCS) algorithm to solve large-scale feature selection problems. To the best of our knowledge, the proposed algorithm in this paper is the first multi-objective coordinate search algorithm. In this method, we generate new individuals by flipping a variable of the candidate solutions on the Pareto front. This enables us to investigat
    
[^108]: 使用Sigmoid Loss进行对比学习的分析

    Analysis of Using Sigmoid Loss for Contrastive Learning

    [https://arxiv.org/abs/2402.12613](https://arxiv.org/abs/2402.12613)

    提出了双常数嵌入模型（CCEM）以理论分析使用Sigmoid Loss在对比学习中的应用，有可能提供更高效的性能。

    

    对比学习已经成为自监督学习中一个重要的分支数年。特别是，将对比学习应用于大量带标题的图片集的CLIP引起了很大关注。最近，提出了SigLIP，CLIP的一种变体，它使用sigmoid loss而不是标准的InfoNCE loss。SigLIP通过消除对全局视图的需求，以更有效的方式达到与CLIP相当的性能。然而，对比学习中使用sigmoid loss的理论理解尚未被充分探讨。在本文中，我们从学习嵌入的几何结构的角度，对在对比学习中使用sigmoid loss进行了理论分析。首先，我们提出了双常数嵌入模型（CCEM），一个通过单个变量来参数化各种众所周知的嵌入结构的框架。有趣的是，所提出的CCEM被证明包含了t

    arXiv:2402.12613v1 Announce Type: new  Abstract: Contrastive learning has emerged as a prominent branch of self-supervised learning for several years. Especially, CLIP, which applies contrastive learning to large sets of captioned images, has garnered significant attention. Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid loss instead of the standard InfoNCE loss. SigLIP achieves the performance comparable to CLIP in a more efficient manner by eliminating the need for a global view. However, theoretical understanding of using the sigmoid loss in contrastive learning is underexplored. In this paper, we provide a theoretical analysis of using the sigmoid loss in contrastive learning, in the perspective of the geometric structure of learned embeddings. First, we propose the double-Constant Embedding Model (CCEM), a framework for parameterizing various well-known embedding structures by a single variable. Interestingly, the proposed CCEM is proven to contain t
    
[^109]: 基于图的虚拟传感：来自稀疏和部分多变量观测的方法

    Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations

    [https://arxiv.org/abs/2402.12598](https://arxiv.org/abs/2402.12598)

    本文提出了一种基于图的虚拟传感方法，通过利用相关变量之间的依赖关系，设计了GgNet架构，用于推断未观测信道的值。

    

    虚拟传感技术允许通过利用来自不同位置的物理传感器的时空测量来推断新位置的信号。然而，由于成本或其他限制导致传感器覆盖范围变得稀疏，无法利用物理接近性支持插值。本文通过利用目标变量与一组相关变量（协变量）之间的依赖关系来克服这一挑战，这些协变量可以经常与感兴趣的每个位置相关联。从这个角度来看，协变量提供了部分可观测性，问题在于通过利用其他位置的观测结果推断未观测信道的值，以了解这些变量的相关性如何。我们引入了一种新颖的基于图的方法来利用这种关系，并设计了一个名为GgNet的图深度学习架构来实现该框架。提出的方法依赖于p

    arXiv:2402.12598v1 Announce Type: cross  Abstract: Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting spatio-temporal measurements coming from physical sensors at different locations. However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to support interpolation. In this paper, we overcome this challenge by leveraging dependencies between the target variable and a set of correlated variables (covariates) that can frequently be associated with each location of interest. From this viewpoint, covariates provide partial observability, and the problem consists of inferring values for unobserved channels by exploiting observations at other locations to learn how such variables can correlate. We introduce a novel graph-based methodology to exploit such relationships and design a graph deep learning architecture, named GgNet, implementing the framework. The proposed approach relies on p
    
[^110]: 基于截断多项式展开的大规模MIMO中的检测：一种基于模型驱动的深度学习方法

    Truncated Polynomial Expansion-Based Detection in Massive MIMO: A Model-Driven Deep Learning Approach

    [https://arxiv.org/abs/2402.12595](https://arxiv.org/abs/2402.12595)

    通过在大规模MIMO系统中应用基于截断多项式展开的深度学习方法，提高了信号检测的效率，并降低了计算复杂性

    

    在本文中，我们提出了一种基于深度学习（DL）的方法，利用截断多项式展开（TPE）高效计算Hermitian矩阵的逆。我们的基于模型驱动的方法涉及在离线训练过程中优化给定数量TPE项的系数。我们将该方法应用于上行大规模多输入多输出（MIMO）系统中的信号检测，其中线性检测器，如零强迫（ZF）和最小均方误差（MMSE）所需的矩阵逆运算，使用TPE进行近似。我们的仿真结果表明，所提出的基于学习的TPE方法在渐近收敛速度方面优于具有最佳系数的传统TPE方法，并降低了在线检测阶段的计算复杂性，尽管以牺牲离线训练阶段为代价。然而，有限数量的可训练参数导致一种迅速

    arXiv:2402.12595v1 Announce Type: cross  Abstract: In this paper, we propose a deep learning (DL)-based approach for efficiently computing the inverse of Hermitian matrices using truncated polynomial expansion (TPE). Our model-driven approach involves optimizing the coefficients of the TPE during an offline training procedure for a given number of TPE terms. We apply this method to signal detection in uplink massive multiple-input multiple-output (MIMO) systems, where the matrix inverse operation required by linear detectors, such as zero-forcing (ZF) and minimum mean square error (MMSE), is approximated using TPE. Our simulation results demonstrate that the proposed learned TPE-based method outperforms the conventional TPE method with optimal coefficients in terms of asymptotic convergence speed and reduces the computational complexity of the online detection stage, albeit at the expense of the offline training stage. However, the limited number of trainable parameters leads to a swif
    
[^111]: FairProof：神经网络的机密和可认证公平性

    FairProof : Confidential and Certifiable Fairness for Neural Networks

    [https://arxiv.org/abs/2402.12572](https://arxiv.org/abs/2402.12572)

    FairProof提出了一种使用零知识证明来公开验证神经网络模型公平性的系统，同时保持机密性，并提出了适用于ZKPs的全连接神经网络的公平性认证算法。

    

    机器学习模型在社会应用中的使用越来越普遍，然而法律和隐私问题要求这些模型往往需要保密。因此，消费者对这些模型的公平性属性越来越不信任，消费者通常是模型预测的接收者。为此，我们提出了FairProof - 一种系统，使用零知识证明（一种密码原语）来公开验证模型的公平性，同时保持机密性。我们还提出了一个适合于ZKPs的全连接神经网络的公平性认证算法，并在该系统中使用。我们在Gnark中实现了FairProof，并通过实证证明了我们的系统是实际可行的。

    arXiv:2402.12572v1 Announce Type: cross  Abstract: Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential. Consequently, there is a growing distrust about the fairness properties of these models in the minds of consumers, who are often at the receiving end of model predictions. To this end, we propose FairProof - a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the fairness of a model, while maintaining confidentiality. We also propose a fairness certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system. We implement FairProof in Gnark and demonstrate empirically that our system is practically feasible.
    
[^112]: 离线多任务转移强化学习与表征惩罚

    Offline Multi-task Transfer RL with Representational Penalization

    [https://arxiv.org/abs/2402.12570](https://arxiv.org/abs/2402.12570)

    提出了一种计算学习表示不确定性度量的算法，为目标任务学到的策略的次优性建立了数据相关的上界。

    

    我们研究了离线强化学习中表示转移的问题，其中学习者可以访问事先收集的多个源任务的序列数据，并旨在学习一个共享表示，以用于为目标任务找到一个良好的策略。我们提出了一种算法来计算学到的表示的逐点不确定性度量，并为目标任务学到的策略的次优性建立了一个数据相关的上界。我们的算法利用源任务的集体探索来减轻少数任务在某些点的覆盖不足，从而克服了需要平均覆盖良好的限制。

    arXiv:2402.12570v1 Announce Type: new  Abstract: We study the problem of representation transfer in offline Reinforcement Learning (RL), where a learner has access to episodic data from a number of source tasks collected a priori, and aims to learn a shared representation to be used in finding a good policy for a target task. Unlike in online RL where the agent interacts with the environment while learning a policy, in the offline setting there cannot be such interactions in either the source tasks or the target task; thus multi-task offline RL can suffer from incomplete coverage.   We propose an algorithm to compute pointwise uncertainty measures for the learnt representation, and establish a data-dependent upper bound for the suboptimality of the learnt policy for the target task. Our algorithm leverages the collective exploration done by source tasks to mitigate poor coverage at some points by a few tasks, thus overcoming the limitation of needing uniformly good coverage for a meani
    
[^113]: GenAudit：利用证据修复语言模型输出中的事实错误

    GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence

    [https://arxiv.org/abs/2402.12566](https://arxiv.org/abs/2402.12566)

    GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。

    

    LLMs即使可以访问参考文档，也可能生成事实不准确的陈述。在高风险应用中（例如基于文档的医疗保健或金融问答），这样的错误可能具有危险性。我们提出了GenAudit -- 一个旨在帮助检查基于文档任务语言模型响应的工具。GenAudit通过修订或删除未被参考文档支持的声明，同时为看似被证据支持的事实提供来自参考文献的证据，来建议修改LLM响应。我们训练模型来执行这些任务，并设计了一个交互界面，向用户呈现建议的修改和证据。通过人工评分员的全面评估显示，GenAudit在总结不同领域文档时能够检测出8种不同的LLM输出中的错误。为确保系统能够标记大多数错误，我们提出了一种方法，可以提高错误召回率，同时最小化对预处理的影响。

    arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
    
[^114]: 具有长期参考效应的动态定价与学习

    Dynamic Pricing and Learning with Long-term Reference Effects

    [https://arxiv.org/abs/2402.12562](https://arxiv.org/abs/2402.12562)

    在考虑顾客价格期望对当前价格反应的情况下，研究了一种具有长期参考效应的动态定价问题，提出了一种新颖的参考价格机制，展示在该机制下降价政策几乎是最优的，为线性需求模型提供了近似最优降价策略。

    

    我们考虑了一个动态定价问题，其中顾客对当前价格的反应受到顾客价格期望，即参考价格的影响。我们研究了一种简单而新颖的参考价格机制，其中参考价格是卖家过去提供的价格的平均值。与更常见的指数平滑机制相反，在我们的参考价格机制中，卖家提供的价格对未来顾客期望有更长期的影响。我们展示，在这种机制下，降价政策几乎是最优的，不受模型参数的影响。这符合一个常见的直觉，即卖家可以通过以较高的价格出发，然后逐渐降低价格，因为顾客会觉得他们正在购买通常更昂贵的物品上的便宜货。对于线性需求模型，我们还提供了近似最优降价策略的详细特征性描述以及一个有效的方法。

    arXiv:2402.12562v1 Announce Type: new  Abstract: We consider a dynamic pricing problem where customer response to the current price is impacted by the customer price expectation, aka reference price. We study a simple and novel reference price mechanism where reference price is the average of the past prices offered by the seller. As opposed to the more commonly studied exponential smoothing mechanism, in our reference price mechanism the prices offered by seller have a longer term effect on the future customer expectations.   We show that under this mechanism, a markdown policy is near-optimal irrespective of the parameters of the model. This matches the common intuition that a seller may be better off by starting with a higher price and then decreasing it, as the customers feel like they are getting bargains on items that are ordinarily more expensive. For linear demand models, we also provide a detailed characterization of the near-optimal markdown policy along with an efficient way
    
[^115]: 利用机器学习技术评估与COVID-19死亡率相关的国家饮食习惯

    Evaluation of Country Dietary Habits Using Machine Learning Techniques in Relation to Deaths from COVID-19

    [https://arxiv.org/abs/2402.12558](https://arxiv.org/abs/2402.12558)

    该研究利用机器学习技术评估了170个国家的饮食习惯，发现肥胖和高脂肪摄入与COVID-19死亡率较高的国家相关，而谷物消费水平较高的国家则有较低的死亡率。

    

    COVID-19疾病已经影响了世界上几乎每个国家。大量感染者和不同国家之间的死亡率差异引发了许多关于使病毒在某些地方如此致命的关键因素的假设。本研究评估了170个国家的饮食习惯，以找出这些习惯与COVID-19引起的死亡率之间的相关性，使用机器学习技术根据23种不同食物的脂肪、能量和蛋白质的不同分布，以及以千克为单位的摄入量，将国家进行分组。结果表明，肥胖和高脂肪摄入在死亡率最高的国家中出现，而死亡率较低的国家则伴随着更高的谷物消费水平以及较低的总体平均千卡摄入量。

    arXiv:2402.12558v1 Announce Type: new  Abstract: COVID-19 disease has affected almost every country in the world. The large number of infected people and the different mortality rates between countries has given rise to many hypotheses about the key points that make the virus so lethal in some places. In this study, the eating habits of 170 countries were evaluated in order to find correlations between these habits and mortality rates caused by COVID-19 using machine learning techniques that group the countries together according to the different distribution of fat, energy, and protein across 23 different types of food, as well as the amount ingested in kilograms. Results shown how obesity and the high consumption of fats appear in countries with the highest death rates, whereas countries with a lower rate have a higher level of cereal consumption accompanied by a lower total average intake of kilocalories.
    
[^116]: 多线性专家混合：通过因式分解实现可扩展的专家特化

    Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization

    [https://arxiv.org/abs/2402.12550](https://arxiv.org/abs/2402.12550)

    多线性专家混合（MMoE）层通过因式分解针对视觉模型提供了一种可扩展的专家特化解决方案，避免了离散专家路由和过高推理时间成本。

    

    专家混合（MoE）范式提供了一种强大的方法，将难以理解的密集层分解为更小、模块化的计算，通常更易于人类解释、调试和编辑。然而，一个主要问题在于扩展专家数量的计算成本，以实现足够精细的专业化。本文提出了多线性专家混合（MMoE）层来解决这个问题，重点放在视觉模型上。MMoE层完全以因式化形式对庞大的权重张量进行隐式计算。因此，MMoEs既避免了在流行的“稀疏”MoE模型中离散专家路由所造成的问题，又不会引起“软”MoE替代方案中过高的推理时间成本。我们通过可视化和反事实干预，提供了定性和定量证据，证明了扩展MMoE层的效果。

    arXiv:2402.12550v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular 'sparse' MoE models, yet (2) do not incur the restrictively high inference-time costs of 'soft' MoE alternatives. We present both qualitative and quantitative evidence (through visualization and counterfactual interventions respectively) that scaling MMoE layers wh
    
[^117]: 数据使用对具有智能储能的建筑中模型预测控制性能的影响

    Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage

    [https://arxiv.org/abs/2402.12539](https://arxiv.org/abs/2402.12539)

    研究发现，在具有智能储能的建筑中，简单的线性多层感知器模型提供了与最先进模型相当的预测准确性，更具有数据效率和泛化能力。

    

    数据是开发用于建筑能源系统中模型预测控制（MPC）方案的预测模型所必需的。然而，数据使用会产生收集和利用方面的成本。确定成本最优数据使用需要了解其带来的预测准确性以及结果 MPC 运行性能的影响。本研究使用历史建筑能源数据在一个多建筑能源系统模拟中，研究了简单和最先进的机器学习预测模型在 MPC 中的性能。对于以下数据效率措施，即重新使用预测模型、减少训练数据量、减少模型数据特征和在线模型训练，量化了数据使用对预测准确性的影响。结果显示，简单的线性多层感知器模型提供了与最先进模型相当的预测准确性，且具有更高的数据效率和普适性。

    arXiv:2402.12539v1 Announce Type: cross  Abstract: Data is required to develop forecasting models for use in Model Predictive Control (MPC) schemes in building energy systems. However, data usage incurs costs from both its collection and exploitation. Determining cost optimal data usage requires understanding of the forecast accuracy and resulting MPC operational performance it enables. This study investigates the performance of both simple and state-of-the-art machine learning prediction models for MPC in a multi-building energy system simulation using historic building energy data. The impact of data usage on forecast accuracy is quantified for the following data efficiency measures: reuse of prediction models, reduction of training data volumes, reduction of model data features, and online model training. A simple linear multi-layer perceptron model is shown to provide equivalent forecast accuracy to state-of-the-art models, with greater data efficiency and generalisability. The use
    
[^118]: 用于检测网络欺凌的机器学习集成模型

    A Machine Learning Ensemble Model for the Detection of Cyberbullying

    [https://arxiv.org/abs/2402.12538](https://arxiv.org/abs/2402.12538)

    该研究使用堆叠集成机器学习方法结合多种特征提取技术，成功开发出用于检测网络欺凌的自动化系统，并在性能上表现出色。

    

    社交媒体平台的广泛使用，如Facebook、Instagram等，显著增强了我们的电子互联性。然而，这些平台现在可以在任何地点随时轻松访问。然而，社交媒体的普及也导致了网络欺凌的增加。有必要解决在社交媒体平台上找到、监控和减轻网络欺凌帖子的需求。受到这种需求的推动，我们提出本文为发展检测攻击性推文的二进制标签的自动化系统做出贡献。我们的研究表明，在相同数据集上与先前实验相比取得了显着的性能。我们采用了堆叠集成机器学习方法，利用四种不同的特征提取技术来优化堆叠集成学习框架内的性能。将五种机器学习算法，决策树、随机森林，

    arXiv:2402.12538v1 Announce Type: cross  Abstract: The pervasive use of social media platforms, such as Facebook, Instagram, and X, has significantly amplified our electronic interconnectedness. Moreover, these platforms are now easily accessible from any location at any given time. However, the increased popularity of social media has also led to cyberbullying.It is imperative to address the need for finding, monitoring, and mitigating cyberbullying posts on social media platforms. Motivated by this necessity, we present this paper to contribute to developing an automated system for detecting binary labels of aggressive tweets.Our study has demonstrated remarkable performance compared to previous experiments on the same dataset. We employed the stacking ensemble machine learning method, utilizing four various feature extraction techniques to optimize performance within the stacking ensemble learning framework. Combining five machine learning algorithms,Decision Trees, Random Forest, L
    
[^119]: 针对个性化联邦无监督学习的分层贝叶斯方法

    Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning

    [https://arxiv.org/abs/2402.12537](https://arxiv.org/abs/2402.12537)

    该论文提出了基于分层贝叶斯统计框架的算法，用于个性化无监督学习，其中开发了适应性算法来平衡利用有限本地数据和协作信息。

    

    客户本地数据的统计异质性是联邦学习中的重要特征，其促使个性化算法针对本地数据统计量进行定制。尽管已经提出了大量针对个性化监督学习的算法，但通过个性化无监督学习发现本地数据的结构却很少被探索。我们通过基于层次贝叶斯统计框架启动了对这种个性化无监督学习的系统研究。我们开发了基于优化标准的算法，这些算法受启发于层次贝叶斯统计框架。我们开发了适应性算法，发现了利用有限本地数据和协作信息之间的平衡。我们在两个无监督学习任务的背景下进行了这项工作：个性化降维和个性化扩散模型。我们为我们的自适应算法开发了收敛分析，这些分析展示了对问题参数（例如，异质性）的依赖性。

    arXiv:2402.12537v1 Announce Type: new  Abstract: Statistical heterogeneity of clients' local data is an important characteristic in federated learning, motivating personalized algorithms tailored to the local data statistics. Though there has been a plethora of algorithms proposed for personalized supervised learning, discovering the structure of local data through personalized unsupervised learning is less explored. We initiate a systematic study of such personalized unsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework. We develop adaptive algorithms that discover the balance between using limited local data and collaborative information. We do this in the context of two unsupervised learning tasks: personalized dimensionality reduction and personalized diffusion models. We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity
    
[^120]: 基于局部敏感哈希的高能物理中应用的高效点变换器

    Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics

    [https://arxiv.org/abs/2402.12535](https://arxiv.org/abs/2402.12535)

    本研究提出了一种针对科学领域中大规模点云处理优化的Transformer模型，通过局部敏感哈希技术实现了近线性复杂度，并提出了基于LSH的高效点变换器HEPT。

    

    这项研究介绍了一种针对大规模点云处理在科学领域（如高能物理和天体物理）进行优化的新型Transformer模型。该模型解决了图神经网络和标准Transformer的局限性，集成了局部归纳偏差，并通过硬件友好的常规操作实现接近线性复杂度。本工作的一个贡献是对各种稀疏化技术进行误差-复杂度权衡的定量分析，以构建高效Transformer。我们的发现突显了在具有局部归纳偏差的大规模点云数据中使用局部敏感哈希（LSH），尤其是OR＆AND构造LSH，在核近似中的优越性。基于这一发现，我们提出了基于LSH的高效点变换器（HEPT），它将E^2LSH与OR＆AND构造相结合，并建立在常规计算之上。

    arXiv:2402.12535v1 Announce Type: new  Abstract: This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR \& AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (\textbf{HEPT}), which combines E$^2$LSH with OR \& AND constructions and is built upon regular computations. HEPT demonstrates remarkabl
    
[^121]: 在多对一图像到图像翻译上改进深度生成模型

    Improving Deep Generative Models on Many-To-One Image-to-Image Translation

    [https://arxiv.org/abs/2402.12531](https://arxiv.org/abs/2402.12531)

    介绍了一种新的非对称框架，可改进现有深度生成模型在多对一图像到图像翻译上的效果，并在 StarGAN V2 上展示了其性能优化。

    

    arXiv:2402.12531v1 通告类型: 跨 针对图像到图像翻译中的多个应用，已应用深度生成模型。 生成对抗网络和扩散模型展示了令人印象深刻的结果，在这些任务上取得了新的最先进结果。 大多数方法在数据集中的不同领域之间具有对称设置。 这些方法假设所有领域都具有多个模态或仅一个模态。 但是，许多数据集存在两个域之间的多对一关系。 在这项工作中，我们首先介绍了一个Colorized MNIST数据集和一个Color-Recall分数，它可以为在多对一翻译上评估模型提供一个简单的基准。 然后，我们引入了一个新的非对称框架，以改进现有的深度生成模型在多对一图像到图像翻译上的表现。 我们将这个框架应用到 StarGAN V2 上，并表明在无监督和半监督设置中，这个新模型的性能得到了提升。

    arXiv:2402.12531v1 Announce Type: cross  Abstract: Deep generative models have been applied to multiple applications in image- to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized MNIST dataset and a Color-Recall score that can provide a simple benchmark for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep generative models on many-to-one image-to- image translation. We apply this framework to StarGAN V2 and show that in both unsupervised and semi-supervised settings, the performance of this new model improves o
    
[^122]: 在预训练数据中的平行结构实现上下文学习

    Parallel Structures in Pre-training Data Yield In-Context Learning

    [https://arxiv.org/abs/2402.12530](https://arxiv.org/abs/2402.12530)

    本研究发现，语言模型的上下文学习能力取决于预训练数据中的平行结构，通过在相似模板的短语对中学习来提高上下文学习准确度。

    

    预训练语言模型（LMs）具备上下文学习（ICL）的能力：它们可以在只给出少量示例的情况下适应任务而无需进行任何参数更新。然而，目前尚不清楚这种能力来自何处，因为预训练文本与ICL提示之间存在明显的分布偏移。在本研究中，我们探讨了预训练数据中的哪些模式有助于ICL。我们发现LMs的ICL能力取决于预训练数据中的“平行结构”——在相同上下文窗口中遵循相似模板的短语对。具体来说，通过检查训练一个短语是否提高了对另一个短语的预测来检测平行结构，并进行消融实验以研究其对ICL的影响。我们展示了从预训练数据中去除平行结构会导致LMs的ICL准确度下降51％（与随机切除的2％相比）。即使排除常见模式如 n-gram

    arXiv:2402.12530v1 Announce Type: cross  Abstract: Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gr
    
[^123]: 离线模型驱动强化学习中的边缘问题

    The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning

    [https://arxiv.org/abs/2402.12527](https://arxiv.org/abs/2402.12527)

    学习的动力学模型被真实且无误差的动力学替代时，现有模型驱动方法将会完全失败，揭示出一个重大误解。

    

    离线强化学习旨在使智能体能够从预先收集的数据集中进行训练，然而，由此带来了一个额外的挑战，即估计数据集中未涵盖的行为的价值。模型驱动方法通过允许智能体通过在学习动力学模型中进行展开进行收集额外的合成数据来提供解决方案。然而，令人惊讶的是，我们发现，如果学习的动力学模型被真实且无误差的动力学替代，现有的模型驱动方法将完全失败。这揭示了一个重大误解。我们的后续调查发现，模型驱动算法中使用的一般过程导致存在一组触发病态值过高的边缘状态。

    arXiv:2402.12527v1 Announce Type: cross  Abstract: Offline reinforcement learning aims to enable agents to be trained from pre-collected datasets, however, this comes with the added challenge of estimating the value of behavior not covered in the dataset. Model-based methods offer a solution by allowing agents to collect additional synthetic data via rollouts in a learned dynamics model. The prevailing theoretical understanding is that this can then be viewed as online reinforcement learning in an approximate dynamics model, and any remaining gap is therefore assumed to be due to the imperfect dynamics model. Surprisingly, however, we find that if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail. This reveals a major misconception. Our subsequent investigation finds that the general procedure used in model-based algorithms results in the existence of a set of edge-of-reach states which trigger pathological value overes
    
[^124]: 高斯过程神经加性模型

    Gaussian Process Neural Additive Models

    [https://arxiv.org/abs/2402.12518](https://arxiv.org/abs/2402.12518)

    本文提出了一种新的高斯过程神经加性模型（GP-NAM），通过随机傅里叶特征对高斯过程进行单层神经网络构建，可以实现具有凸目标函数和可训练参数数量随特征维度线性增长的优势，同时在性能上不亚于更深的NAM方法。

    

    深度神经网络已经在许多领域引起了革命，但它们的黑盒特性有时也阻碍了它们在医疗保健和金融等领域的广泛应用，这些领域需要可解释和可解释的模型。最近发展出的神经加性模型（NAMs）是在面向表格数据集的可解释深度学习方向上迈出的重要一步。在本文中，我们提出了一种新的NAM子类，它使用通过随机傅里叶特征对高斯过程进行单层神经网络构建，我们称之为高斯过程神经加性模型（GP-NAM）。GP-NAM具有凸目标函数和随特征维度线性增长的可训练参数数量的优势。与更深的NAM方法相比，它在性能上没有损失，因为GPs非常适合学习复杂的非参数单变量函数。我们在多个表格数据集上展示了GP-NAM的性能。

    arXiv:2402.12518v1 Announce Type: cross  Abstract: Deep neural networks have revolutionized many fields, but their black-box nature also occasionally prevents their wider adoption in fields such as healthcare and finance, where interpretable and explainable models are required. The recent development of Neural Additive Models (NAMs) is a significant step in the direction of interpretable deep learning for tabular datasets. In this paper, we propose a new subclass of NAMs that use a single-layer neural network construction of the Gaussian process via random Fourier features, which we call Gaussian Process Neural Additive Models (GP-NAM). GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensionality. It suffers no loss in performance compared to deeper NAM approaches because GPs are well-suited for learning complex non-parametric univariate functions. We demonstrate the performance of GP-NAM on several tabular
    
[^125]: 引导模型匹配：受限模型如何帮助更大的模型

    Induced Model Matching: How Restricted Models Can Help Larger Ones

    [https://arxiv.org/abs/2402.12513](https://arxiv.org/abs/2402.12513)

    提出了引导模型匹配（IMM）方法，通过使完整模型的性能与受限模型对齐，将受限模型的知识传递给完整模型，具有广泛的应用性。

    

    我们考虑在训练更大、具有完整特征的模型时，是否可以利用受限特征的非常准确的预测模型。这个受限模型可以被视为“辅助信息”，可以通过来自辅助源数据集的详尽数据或在相同数据集上通过施加限制来获得。我们提出了一种方法，将受限模型的知识传递给完整模型，通过使完整模型的上下文受限性能与受限模型的性能对齐。我们将这种方法称为引导模型匹配（IMM），首先通过以逻辑回归为玩具示例来说明其普适性。然后我们探讨了IMM在语言建模中的应用，这也是最初的灵感来源，IMM在这里提供了明确的基础，与在技术中隐式使用受限模型的方法相对应，例如添加噪声。

    arXiv:2402.12513v1 Announce Type: new  Abstract: We consider scenarios where a very accurate predictive model using restricted features is available at the time of training of a larger, full-featured, model. This restricted model may be thought of as "side-information", derived either from an auxiliary exhaustive dataset or on the same dataset, by forcing the restriction. How can the restricted model be useful to the full model? We propose an approach for transferring the knowledge of the restricted model to the full model, by aligning the full model's context-restricted performance with that of the restricted model's. We call this methodology Induced Model Matching (IMM) and first illustrate its general applicability by using logistic regression as a toy example. We then explore IMM's use in language modeling, the application that initially inspired it, and where it offers an explicit foundation in contrast to the implicit use of restricted models in techniques such as noising. We dem
    
[^126]: 用于极小化优化的随机微分方程

    SDEs for Minimax Optimization

    [https://arxiv.org/abs/2402.12508](https://arxiv.org/abs/2402.12508)

    本文开创性地使用随机微分方程分析和比较极小化优化器，展示了超参数、隐式正则化和隐式曲率诱导噪声之间的相互作用，并提供了统一简化的分析策略。

    

    极小化优化问题在过去几年中吸引了很多关注，应用范围从经济学到机器学习。虽然存在针对这类问题的先进优化方法，但在随机场景中描述它们的动态仍然具有挑战性。本文开创性地使用随机微分方程（SDEs）来分析和比较极小化优化器。我们的SDE模型适用于随机梯度下降-上升、随机外推法和随机哈密尔顿梯度下降，可被证明是它们算法对应物的近似，清晰展示了超参数、隐式正则化和隐式曲率诱导噪声之间的相互作用。这种视角还允许基于伊藤微积分原理进行统一简化分析策略。最后，我们的方法有助于推导收敛条件和闭式解。

    arXiv:2402.12508v1 Announce Type: new  Abstract: Minimax optimization problems have attracted a lot of attention over the past few years, with applications ranging from economics to machine learning. While advanced optimization methods exist for such problems, characterizing their dynamics in stochastic scenarios remains notably challenging. In this paper, we pioneer the use of stochastic differential equations (SDEs) to analyze and compare Minimax optimizers. Our SDE models for Stochastic Gradient Descent-Ascent, Stochastic Extragradient, and Stochastic Hamiltonian Gradient Descent are provable approximations of their algorithmic counterparts, clearly showcasing the interplay between hyperparameters, implicit regularization, and implicit curvature-induced noise. This perspective also allows for a unified and simplified analysis strategy based on the principles of It\^o calculus. Finally, our approach facilitates the derivation of convergence conditions and closed-form solutions for th
    
[^127]: PARCv2：物理感知循环卷积神经网络用于时空动力学建模

    PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling

    [https://arxiv.org/abs/2402.12503](https://arxiv.org/abs/2402.12503)

    PARCv2通过引入微分算子扩展了PARC模型，用于模拟不稳定、瞬态和传输主导系统的时空动力学。

    

    arXiv:2402.12503v1 公告类型：新摘要：对不稳定的、快速瞬态和优势传输主导的物理问题进行建模是物理感知深度学习（PADL）面临的迫切挑战。复杂系统的物理由大型偏微分方程（PDEs）系统和带有非线性结构的辅助本构模型所控制，同时还包括表现出急剧梯度和快速变形材料界面的演化状态场。在这里，我们研究了一种多功能且通用的归纳偏见方法，用于模拟通用的非线性场演变问题。我们的研究聚焦于最近的物理感知循环卷积（PARC），它结合了一种区分-积分器结构，归纳地模拟了通用物理系统的时空动力学。我们扩展了PARC的功能，以模拟不稳定、瞬态和传输主导系统。这个扩展模型被称为PARCv2，配备了微分算子来建模

    arXiv:2402.12503v1 Announce Type: new  Abstract: Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL). The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces. Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems. Our study focuses on the recent physics-aware recurrent convolutions (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems. We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems. The extended model, referred to as PARCv2, is equipped with differential operators to model
    
[^128]: 将kNN与基础模型结合，实现适应性和注重隐私的图像分类

    Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification

    [https://arxiv.org/abs/2402.12500](https://arxiv.org/abs/2402.12500)

    将kNN与自然图像上自监督预训练的基础模型结合，独立存储训练数据的嵌入，实现动态数据修改而无需重新训练，提升图像分类的可解释性和适应性

    

    传统的深度学习模型隐含地编码知识，限制了它们的透明度和适应数据变化的能力。然而，这种适应性对解决用户数据隐私问题至关重要。我们通过独立存储基础训练数据的嵌入来解决这一限制，而不是依赖模型权重，从而实现动态数据修改而无需重新训练。具体来说，我们的方法将k近邻（k-NN）分类器与基于视觉的基础模型集成，该模型在自然图像上进行了自监督预训练，提高了可解释性和适应性。我们共享了之前未公开的基线方法的开源实现，以及我们的性能提升贡献。定量实验证实了在已建立的基准数据集上改善分类效果以及该方法在不同医学图像分类任务中的适用性。此外，我们评估了该方法在cont

    arXiv:2402.12500v1 Announce Type: cross  Abstract: Traditional deep learning models implicity encode knowledge limiting their transparency and ability to adapt to data changes. Yet, this adaptability is vital for addressing user data privacy concerns. We address this limitation by storing embeddings of the underlying training data independently of the model weights, enabling dynamic data modifications without retraining. Specifically, our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a vision-based foundation model, pre-trained self-supervised on natural images, enhancing interpretability and adaptability. We share open-source implementations of a previously unpublished baseline method as well as our performance-improving contributions. Quantitative experiments confirm improved classification across established benchmark datasets and the method's applicability to distinct medical image classification tasks. Additionally, we assess the method's robustness in cont
    
[^129]: 通过自适应猜想的在线学习实现自动化安全响应

    Automated Security Response through Online Learning with Adaptive Conjectures

    [https://arxiv.org/abs/2402.12499](https://arxiv.org/abs/2402.12499)

    该论文通过自适应猜想的在线学习，提出了一种适用于IT基础设施的自动化安全响应方法，其中游戏参与者通过Bayesian学习调整猜想，并通过推演更新策略，最终实现了最佳拟合，提高了推演在猜想模型下的性能。

    

    我们研究了针对IT基础设施的自动化安全响应，并将攻击者和防御者之间的互动形式表述为一个部分观测、非平稳博弈。我们放宽了游戏模型正确规定的标准假设，并考虑每个参与者对模型有一个概率性猜想，可能在某种意义上错误规定，即真实模型的概率为0。这种形式允许我们捕捉关于基础设施和参与者意图的不确定性。为了在线学习有效的游戏策略，我们设计了一种新颖的方法，其中一个参与者通过贝叶斯学习迭代地调整其猜想，并通过推演更新其策略。我们证明了猜想会收敛到最佳拟合，并提供了在具有猜测模型的情况下推演实现性能改进的上限。为了刻画游戏的稳定状态，我们提出了Berk-Nash平衡的一个变种。

    arXiv:2402.12499v1 Announce Type: cross  Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We 
    
[^130]: 封建网络用于视觉导航

    Feudal Networks for Visual Navigation

    [https://arxiv.org/abs/2402.12498](https://arxiv.org/abs/2402.12498)

    使用封建学习的视觉导航，通过高级管理者、中级管理者和工作代理的分层结构，在不同空间和时间尺度上操作，具有独特模块来实现自监督学习记忆代理地图。

    

    视觉导航遵循人类可以在没有详细地图的情况下导航的直觉。一种常见方法是在建立包含可用于规划的图像节点的拓扑图的同时进行交互式探索。最近的变体从被动视频中学习，并可以利用复杂的社交和语义线索进行导航。然而，需要大量的训练视频，利用大型图并且由于使用了里程计，场景不是未知的。我们引入了一种使用封建学习的视觉导航的新方法，该方法采用了由工作代理、中级管理者和高级管理者组成的分层结构。封建学习范式的关键在于，每个级别的代理看到任务的不同方面，并且在不同的空间和时间尺度上运作。在此框架中开发了两个独特的模块。对于高级管理者，我们自监督地学习一个记忆代理地图以记录

    arXiv:2402.12498v1 Announce Type: cross  Abstract: Visual navigation follows the intuition that humans can navigate without detailed maps. A common approach is interactive exploration while building a topological graph with images at nodes that can be used for planning. Recent variations learn from passive videos and can navigate using complex social and semantic cues. However, a significant number of training videos are needed, large graphs are utilized, and scenes are not unseen since odometry is utilized. We introduce a new approach to visual navigation using feudal learning, which employs a hierarchical structure consisting of a worker agent, a mid-level manager, and a high-level manager. Key to the feudal learning paradigm, agents at each level see a different aspect of the task and operate at different spatial and temporal scales. Two unique modules are developed in this framework. For the high- level manager, we learn a memory proxy map in a self supervised manner to record prio
    
[^131]: 跨领域持续学习的研究

    Towards Cross-Domain Continual Learning

    [https://arxiv.org/abs/2402.12490](https://arxiv.org/abs/2402.12490)

    介绍了一种名为跨领域持续学习（CDCL）的新方法，通过整合任务间和任务内的交叉注意机制，在紧凑的卷积网络中延迟数据漂移，实现了无监督的跨领域学习（UDA）。

    

    持续学习是一个过程，涉及训练学习代理以顺序地掌握一系列任务或类别，而不需要重新回顾过去的数据。挑战在于利用先前获得的知识有效地学习新任务，同时避免灾难性遗忘。现有方法主要集中在单一领域，限制了它们在特定问题上的适用性。在这项工作中，我们介绍了一种名为跨领域持续学习（CDCL）的新方法，它解决了被限制在单一监督领域的局限性。我们的方法在紧凑的卷积网络中结合了任务间和任务内的交叉注意机制。这种整合使得模型能够与先前任务的特征保持对齐，从而延迟可能发生在任务之间的数据漂移，同时在相关领域之间进行无监督的跨领域学习（UDA）。通过利用任务内具体的伪标签

    arXiv:2402.12490v1 Announce Type: cross  Abstract: Continual learning is a process that involves training learning agents to sequentially master a stream of tasks or classes without revisiting past data. The challenge lies in leveraging previously acquired knowledge to learn new tasks efficiently, while avoiding catastrophic forgetting. Existing methods primarily focus on single domains, restricting their applicability to specific problems.   In this work, we introduce a novel approach called Cross-Domain Continual Learning (CDCL) that addresses the limitations of being limited to single supervised domains. Our method combines inter- and intra-task cross-attention mechanisms within a compact convolutional network. This integration enables the model to maintain alignment with features from previous tasks, thereby delaying the data drift that may occur between tasks, while performing unsupervised cross-domain (UDA) between related domains. By leveraging an intra-task-specific pseudo-labe
    
[^132]: SECP：基于语音增强的策划管道，用于可扩展获取干净语音

    SECP: A Speech Enhancement-Based Curation Pipeline For Scalable Acquisition Of Clean Speech

    [https://arxiv.org/abs/2402.12482](https://arxiv.org/abs/2402.12482)

    提出了基于语音增强的策划管道（SECP），可以在规模上获取干净语音并训练语音增强模型，通过两轮迭代观察到增强输出作为基准不会降低模型性能，并通过主观测试证明优化数据在感知上优于原始数据。

    

    随着越来越多的语音技术依赖于以干净语音为基准的监督式深度学习方法，需要一种方法来在规模上接入这些语音。然而，这种方法需要最大程度地减少对人类听觉和注释的依赖，只在需要时需要人类介入。本文通过概述基于语音增强的策划管道（SECP）来解决这个问题，作为一个框架用来接入干净语音。这些干净语音可以用来训练语音增强模型，进一步改进原始数据集，从而关闭迭代循环。我们通过两轮迭代的实验观察到，作为基准的增强输出不会使模型性能根据本文使用的 $\Delta_{PESQ}$ 指标下降。我们还通过基于比较均值意见评分（CMOS）的主观测试表明，优化数据的最高和最低边界在感知上优于原始数据。

    arXiv:2402.12482v1 Announce Type: cross  Abstract: As more speech technologies rely on a supervised deep learning approach with clean speech as the ground truth, a methodology to onboard said speech at scale is needed. However, this approach needs to minimize the dependency on human listening and annotation, only requiring a human-in-the-loop when needed. In this paper, we address this issue by outlining Speech Enhancement-based Curation Pipeline (SECP) which serves as a framework to onboard clean speech. This clean speech can then train a speech enhancement model, which can further refine the original dataset and thus close the iterative loop. By running two iterative rounds, we observe that enhanced output used as ground truth does not degrade model performance according to $\Delta_{PESQ}$, a metric used in this paper. We also show through comparative mean opinion score (CMOS) based subjective tests that the highest and lowest bound of refined data is perceptually better than the ori
    
[^133]: 在深度强化学习中，修剪网络是一个好网络

    In deep reinforcement learning, a pruned network is a good network

    [https://arxiv.org/abs/2402.12479](https://arxiv.org/abs/2402.12479)

    通过逐渐剪枝，使代理能够最大程度地发挥参数效能，从而产生比传统网络显著性能提升的网络，并展现出一种“缩放定律”。

    

    最近的研究表明，深度强化学习代理在有效利用其网络参数方面存在困难。我们利用对稀疏训练技术优势的先前见解，并证明逐渐剪枝使代理能够最大程度地发挥参数效能。这导致网络比传统网络产生显著的性能改进，并表现出一种“缩放定律”，仅使用完整网络参数的一小部分。

    arXiv:2402.12479v1 Announce Type: cross  Abstract: Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of "scaling law", using only a small fraction of the full network parameters.
    
[^134]: 不同领域和参数的微分同胚神经算子

    Diffeomorphism Neural Operator for various domains and parameters of partial differential equations

    [https://arxiv.org/abs/2402.12475](https://arxiv.org/abs/2402.12475)

    通过微分同胚神经算子学习框架，提出了一种适用于各种和复杂领域的物理系统的领域灵活模型，从而将学习函数映射在不同领域的问题转化为在共享的微分同胚上学习算子的问题。

    

    许多科学和工程应用需要对传统上使用资源密集型数值求解器计算的偏微分方程（PDE）进行评估。神经算子模型通过直接从数据中学习控制物理定律，提供了一种有效的替代方案，适用于具有不同参数的PDE类别，但在固定边界（领域）内受限。许多应用，例如设计和制造，在大规模研究时将受益于具有灵活领域的神经算子。在这里，我们提出了一种微分同胚神经算子学习框架，旨在为具有各种和复杂领域的物理系统开发领域灵活模型。具体来说，提出了一个在由微分同胚从各领域映射而来的共享领域中训练的神经算子，该方法将在不同领域（空间）学习函数映射的问题转化为在共享的微分同胚上学习算子的问题。

    arXiv:2402.12475v1 Announce Type: cross  Abstract: Many science and engineering applications demand partial differential equations (PDE) evaluations that are traditionally computed with resource-intensive numerical solvers. Neural operator models provide an efficient alternative by learning the governing physical laws directly from data in a class of PDEs with different parameters, but constrained in a fixed boundary (domain). Many applications, such as design and manufacturing, would benefit from neural operators with flexible domains when studied at scale. Here we present a diffeomorphism neural operator learning framework towards developing domain-flexible models for physical systems with various and complex domains. Specifically, a neural operator trained in a shared domain mapped from various domains of fields by diffeomorphism is proposed, which transformed the problem of learning function mappings in varying domains (spaces) into the problem of learning operators on a shared dif
    
[^135]: 具有持续自组织映射的神经仿真无任务无监督在线学习

    Neuro-mimetic Task-free Unsupervised Online Learning with Continual Self-Organizing Maps

    [https://arxiv.org/abs/2402.12465](https://arxiv.org/abs/2402.12465)

    该研究提出了一种具有持续学习能力的神经仿真系统，克服了灾难性遗忘问题，并研究了在无监督架构中的应用，尤其是自组织映射模型。

    

    一种具有持续学习能力的智能系统能够从潜在的无限长的模式向量流中提取知识。这种系统面临的主要挑战是灾难性遗忘，即当学习新样本时，如基于人工神经网络（ANNs）的代理无法保留先前获得的知识。此外，并没有补充任务边界信息的输入会使为以前的任务保留知识变得更具挑战性。尽管在人工神经网络的背景下对遗忘进行了广泛研究，但在无监督架构方面的研究工作还远远不够，比如著名的自组织映射（SOM），这是一种常用于聚类和降维的神经模型。尽管SOM的内部机制原则上可能产生改进记忆的稀疏表示

    arXiv:2402.12465v1 Announce Type: new  Abstract: An intelligent system capable of continual learning is one that can process and extract knowledge from potentially infinitely long streams of pattern vectors. The major challenge that makes crafting such a system difficult is known as catastrophic forgetting - an agent, such as one based on artificial neural networks (ANNs), struggles to retain previously acquired knowledge when learning from new samples. Furthermore, ensuring that knowledge is preserved for previous tasks becomes more challenging when input is not supplemented with task boundary information. Although forgetting in the context of ANNs has been studied extensively, there still exists far less work investigating it in terms of unsupervised architectures such as the venerable self-organizing map (SOM), a neural model often used in clustering and dimensionality reduction. While the internal mechanisms of SOMs could, in principle, yield sparse representations that improve mem
    
[^136]: DBNets：一种公开可用的深度学习工具，用于测量尘埃原行星盘中年轻行星的质量

    DBNets: A publicly available deep learning tool to measure the masses of young planets in dusty protoplanetary discs

    [https://arxiv.org/abs/2402.12448](https://arxiv.org/abs/2402.12448)

    开发了一种深度学习工具DBNets，用于分析原行星盘尘埃辐射中的次结构，快速推断嵌入行星的质量，并能可靠量化质量和相关不确定性

    

    当前用于表征原行星盘观测中嵌入行星的方法在完全考虑观察到的复杂物理过程方面要么存在严重限制，要么在计算和时间成本上受到限制。为了解决这一缺点，我们开发了DBNets：一种基于卷积神经网络的深度学习工具，分析原行星盘尘埃连续辐射中观察到的次结构，快速推断据称嵌入行星的质量。我们专注于开发一种可靠地量化不仅行星质量，而且我们建模和采用技术引入的相关不确定性的方法。我们的测试取得了令人满意的结果，与在相同数据上拟合的解析公式相比，实现了对log Mp均方误差的87%降低（DBNets指标：lmse 0.016，r2分数 97%）。

    arXiv:2402.12448v1 Announce Type: cross  Abstract: Current methods to characterize embedded planets in protoplanetary disc observations are severely limited either in their ability to fully account for the observed complex physics or in their computational and time costs. To address this shortcoming, we developed DBNets: a deep learning tool, based on convolutional neural networks, that analyses substructures observed in the dust continuum emission of protoplanetary discs to quickly infer the mass of allegedly embedded planets. We focussed on developing a method to reliably quantify not only the planet mass, but also the associated uncertainty introduced by our modelling and adopted techniques. Our tests gave promising results achieving an 87% reduction of the log Mp mean squared error with respect to an analytical formula fitted on the same data (DBNets metrics: lmse 0.016, r2-score 97%). With the goal of providing the final user of DBNets with all the tools needed to interpret their 
    
[^137]: 使用神经算子模拟星际介质化学

    Emulating the interstellar medium chemistry with neural operators

    [https://arxiv.org/abs/2402.12435](https://arxiv.org/abs/2402.12435)

    该研究使用神经算子取代计算量大、精确度高、计算代价昂贵的普通微分方程求解器，成功模拟了星际介质中的非平衡化学网络演化，对于宇宙学和天体物理模拟具有重要意义。

    

    银河系的形成和进化在很大程度上取决于理解主导星际介质（ISM）演化和热力学的复杂光化学过程。在计算上，解决化学问题是宇宙学和天体物理模拟中最繁重的任务之一。这项研究旨在用基于神经算子的快速、预先训练的仿真器替代这些程序求解器。我们通过采用DeepONet形式主义模拟非平衡化学网络直至H$_2$形成（9种物种，52个反应），将初始条件和时间演化映射为两个神经网络的张量乘积来分解映射运算符。我们使用$\texttt{KROME}$生成一个训练集，涵盖$-2\leq \log(n/\mathrm{cm}^{-3}) \leq 3.5$，$\log(

    arXiv:2402.12435v1 Announce Type: cross  Abstract: Galaxy formation and evolution critically depend on understanding the complex photo-chemical processes that govern the evolution and thermodynamics of the InterStellar Medium (ISM). Computationally, solving chemistry is among the most heavy tasks in cosmological and astrophysical simulations. The evolution of such non-equilibrium photo-chemical network relies on implicit, precise, computationally costly, ordinary differential equations (ODE) solvers. Here, we aim at substituting such procedural solvers with fast, pre-trained, emulators based on neural operators. We emulate a non-equilibrium chemical network up to H$_2$ formation (9 species, 52 reactions) by adopting the DeepONet formalism, i.e. by splitting the ODE solver operator that maps the initial conditions and time evolution into a tensor product of two neural networks. We use $\texttt{KROME}$ to generate a training set spanning $-2\leq \log(n/\mathrm{cm}^{-3}) \leq 3.5$, $\log(
    
[^138]: 图神经网络中节点属性的攻击

    Attacks on Node Attributes in Graph Neural Networks

    [https://arxiv.org/abs/2402.12426](https://arxiv.org/abs/2402.12426)

    该研究通过基于特征的对抗攻击，针对图神经网络中的节点属性展开研究，发现使用Projected Gradient Descent的决策时攻击比使用Mean Node Embeddings和Graph Contrastive Learning策略的毒化攻击更加有效。

    

    图经常用来模型化现代社交媒体和文献应用中的复杂网络。我们的研究通过基于特征的对抗攻击，重点关注决策时攻击和毒化攻击，探究这些图的脆弱性。与Net Attack和Meta Attack等最先进模型针对节点属性和图结构不同，我们的研究专门针对节点属性。我们利用Hellaswag文本数据集以及图数据集Cora和CiteSeer进行分析，为评估提供了多样的基础。我们的发现表明，使用Projected Gradient Descent (PGD)的决策时攻击比采用Mean Node Embeddings和Graph Contrastive Learning策略的毒化攻击更具威力。这为图数据安全提供了见解，指出了图基模型最脆弱的地方，从而为开发工作提供信息。

    arXiv:2402.12426v1 Announce Type: cross  Abstract: Graphs are commonly used to model complex networks prevalent in modern social media and literacy applications. Our research investigates the vulnerability of these graphs through the application of feature based adversarial attacks, focusing on both decision-time attacks and poisoning attacks. In contrast to state-of-the-art models like Net Attack and Meta Attack, which target node attributes and graph structure, our study specifically targets node attributes. For our analysis, we utilized the text dataset Hellaswag and graph datasets Cora and CiteSeer, providing a diverse basis for evaluation. Our findings indicate that decision-time attacks using Projected Gradient Descent (PGD) are more potent compared to poisoning attacks that employ Mean Node Embeddings and Graph Contrastive Learning strategies. This provides insights for graph data security, pinpointing where graph-based models are most vulnerable and thereby informing the develo
    
[^139]: 表格作为图片？探讨LLM在多模态表格数据表示上的优势和局限性

    Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data

    [https://arxiv.org/abs/2402.12424](https://arxiv.org/abs/2402.12424)

    本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。

    

    在本文中，我们通过不同的提示策略和数据格式研究了各种LLM在解释表格数据方面的有效性。我们的分析涵盖了六个针对与表格相关任务的基准，如问答和事实核查。我们首次介绍了LLM在基于图像的表格表示上的表现评估。具体地，我们比较了五种基于文本和三种基于图像的表格表示，展示了表示和提示对LLM性能的影响。我们的研究为在表格相关任务上有效使用LLM提供了见解。

    arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
    
[^140]: 关于基于扩散的文本转语音模型的语义潜空间

    On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models

    [https://arxiv.org/abs/2402.12423](https://arxiv.org/abs/2402.12423)

    本研究在文本转语音模型中探索了冻结模型的潜空间，发现其中包含丰富的语义信息，并提出了一些新方法来找出其中的语义方向，从而实现了不经过额外训练、架构更改或数据需求就能进行音频编辑。

    

    在文本转语音（TTS）领域，Denoising Diffusion Models (DDMs) 的引入日益增多，为合成高质量语音提供了巨大价值。尽管它们展示出令人印象深刻的音频质量，但它们的语义能力程度尚不明确，并且控制合成语音的声音特性仍然是一个挑战。受图像合成最新进展的启发，我们探索了冻结的TTS模型的潜空间，该空间由DDM去噪器的潜空间激活组成。我们发现这个空间包含丰富的语义信息，并概述了若干查找其中语义方向的新方法，包括监督和无监督方法。然后，我们演示了如何利用这些方法进行现成音频编辑，无需进一步训练、架构更改或数据需求。我们呈现了编辑后音频的语义和声学特质的证据，并提供了补充样本。

    arXiv:2402.12423v1 Announce Type: cross  Abstract: The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech (TTS) domain is rising, providing great value in synthesizing high quality speech. Although they exhibit impressive audio quality, the extent of their semantic capabilities is unknown, and controlling their synthesized speech's vocal properties remains a challenge. Inspired by recent advances in image synthesis, we explore the latent space of frozen TTS models, which is composed of the latent bottleneck activations of the DDM's denoiser. We identify that this space contains rich semantic information, and outline several novel methods for finding semantic directions within it, both supervised and unsupervised. We then demonstrate how these enable off-the-shelf audio editing, without any further training, architectural changes or data requirements. We present evidence of the semantic and acoustic qualities of the edited audio, and provide supplemental samples: h
    
[^141]: EBFT：稀疏LLM的有效和块状微调

    EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs

    [https://arxiv.org/abs/2402.12419](https://arxiv.org/abs/2402.12419)

    提出了一种有效的块状微调稀疏LLM的框架，通过最小化重建误差并采用反向传播逐块优化解决方案，实验结果表明在各种基准测试中优于其他方法。

    

    现有的稀疏LLM微调方法通常需要资源密集型的要求和高昂的重新训练成本。此外，许多微调方法往往依赖于近似或启发式优化策略，这可能导致次优解。为了解决这些问题，我们提出了一种基于最小化重建误差的高效快速微调稀疏LLM的框架。我们的方法涉及对一个小数据集进行采样以进行校准，并利用反向传播逐块地优化块状重建误差，致力于寻求最佳解决方案。对各种基准测试的广泛实验证明，我们的方法在多个基线上始终表现卓越。例如，在Wikitext2数据集上，LLamaV1-7B在70%稀疏度下，我们提出的EBFT取得了16.88的困惑度，超过了75.14的DSnoT的最先进水平。

    arXiv:2402.12419v1 Announce Type: cross  Abstract: Existing methods for fine-tuning sparse LLMs often suffer from resource-intensive requirements and high retraining costs. Additionally, many fine-tuning methods often rely on approximations or heuristic optimization strategies, which may lead to suboptimal solutions. To address these issues, we propose an efficient and fast framework for fine-tuning sparse LLMs based on minimizing reconstruction error. Our approach involves sampling a small dataset for calibration and utilizing backpropagation to iteratively optimize block-wise reconstruction error, on a block-by-block basis, aiming for optimal solutions. Extensive experiments on various benchmarks consistently demonstrate the superiority of our method over other baselines. For instance, on the Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of 75.14. Moreover, with a structured
    
[^142]: 超越统一缩放：探索神经架构中的深度异质性

    Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures

    [https://arxiv.org/abs/2402.12418](https://arxiv.org/abs/2402.12418)

    引入了一种基于二阶损失景观信息的自动缩放方法，同时扩展和训练transformers，提出了神经架构中的深度异质性概念，并在ImageNet100上实现了准确性和参数效率的提升。

    

    传统的神经网络缩放通常涉及设计基本网络，并通过一些预定义的缩放因子增加不同维度（如宽度、深度等）。我们引入了一种利用二阶损失景观信息的自动缩放方法。我们的方法对现代视觉transformers中的跳过连接具有灵活性。我们的训练感知方法同时扩展和训练transformers，而无需额外的训练迭代。受到并非所有神经元都需要统一深度复杂性的假设启发，我们的方法采用深度异质性。对DeiT-S在ImageNet100上进行的广泛评估显示比传统缩放提高了2.5％的准确性并提高了10％的参数效率。在从头开始训练小规模数据集时，缩放的网络表现出色。我们引入了视觉transformers的第一个完整缩放机制，这是朝向高效模型场景的一步。

    arXiv:2402.12418v1 Announce Type: cross  Abstract: Conventional scaling of neural networks typically involves designing a base network and growing different dimensions like width, depth, etc. of the same by some predefined scaling factors. We introduce an automated scaling approach leveraging second-order loss landscape information. Our method is flexible towards skip connections a mainstay in modern vision transformers. Our training-aware method jointly scales and trains transformers without additional training iterations. Motivated by the hypothesis that not all neurons need uniform depth complexity, our approach embraces depth heterogeneity. Extensive evaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10% parameter efficiency improvement over conventional scaling. Scaled networks demonstrate superior performance upon training small scale datasets from scratch. We introduce the first intact scaling mechanism for vision transformers, a step towards efficient model sc
    
[^143]: 用跨公司的卡车司机安全氛围感知来预测卡车事故：一种迁移学习方法

    Predicting trucking accidents with truck drivers 'safety climate perception across companies: A transfer learning approach

    [https://arxiv.org/abs/2402.12417](https://arxiv.org/abs/2402.12417)

    提出了一种预先训练然后微调的迁移学习方法，利用其他公司的数据开发AI模型，更准确地预测卡车事故风险。

    

    人工智能（AI）驱动的安全分析在预测卡车行业事故方面越来越受到关注。然而，公司可能面临一个实际挑战，即没有足够的数据来开发良好的安全分析模型。为了填补这一空白，我们提出了一种预先训练然后微调的迁移学习方法，以帮助任何公司利用其他公司的数据开发AI模型，更准确地预测事故风险。我们还开发了SafeNet，一种适用于事故预测的分类任务的深度神经网络算法。通过来自七家数据规模各不相同的卡车公司的安全氛围调查数据，我们展示了我们提出的方法在结果上比传统方法更好。

    arXiv:2402.12417v1 Announce Type: cross  Abstract: There is a rising interest in using artificial intelligence (AI)-powered safety analytics to predict accidents in the trucking industry. Companies may face the practical challenge, however, of not having enough data to develop good safety analytics models. Although pretrained models may offer a solution for such companies, existing safety research using transfer learning has mostly focused on computer vision and natural language processing, rather than accident analytics. To fill the above gap, we propose a pretrain-then-fine-tune transfer learning approach to help any company leverage other companies' data to develop AI models for a more accurate prediction of accident risk. We also develop SafeNet, a deep neural network algorithm for classification tasks suitable for accident prediction. Using the safety climate survey data from seven trucking companies with different data sizes, we show that our proposed approach results in better m
    
[^144]: 基于异构信息网络的节点重要性值估计的深度结构知识利用与协同作用

    Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks

    [https://arxiv.org/abs/2402.12411](https://arxiv.org/abs/2402.12411)

    通过利用异构结构化知识，提出了一个新的学习框架SKES，用于在异构信息网络中丰富节点表示的信息量，进而建立了一个可解释的节点重要性计算范式。

    

    节点重要性估计问题在传统上是通过同质网络拓扑分析来研究的。为了处理网络的异质性，最近一些方法采用图神经模型来自动学习多样的信息来源。然而，他们的全自适应学习过程可能导致信息探索不足，从而将问题制定为对孤立节点的值预测，表现不佳且可解释性较差。在本研究中，我们提出了一个新颖的学习框架：SKES。与以前的自动学习设计不同，SKES利用异构结构化知识来丰富节点表示的信息量。基于一个足够不具信息的参考，SKES通过量化输入节点与参考之间的差异来估计任何输入节点的重要性值。这建立了一个可解释的节点重要性计算范式。

    arXiv:2402.12411v1 Announce Type: cross  Abstract: Node importance estimation problem has been studied conventionally with homogeneous network topology analysis. To deal with network heterogeneity, a few recent methods employ graph neural models to automatically learn diverse sources of information. However, the major concern revolves around that their full adaptive learning process may lead to insufficient information exploration, thereby formulating the problem as the isolated node value prediction with underperformance and less interpretability. In this work, we propose a novel learning framework: SKES. Different from previous automatic learning designs, SKES exploits heterogeneous structural knowledge to enrich the informativeness of node representations. Based on a sufficiently uninformative reference, SKES estimates the importance value for any input node, by quantifying its disparity against the reference. This establishes an interpretable node importance computation paradigm. F
    
[^145]: ModelGPT：释放LLM的能力，为定制模型生成铺平道路

    ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation

    [https://arxiv.org/abs/2402.12408](https://arxiv.org/abs/2402.12408)

    ModelGPT是一个新颖的框架，通过利用LLM的能力，根据用户提供的数据或任务描述生成定制化的AI模型，让用户能够更快速和方便地使用AI模型。

    

    大型语言模型（LLM）的快速发展通过自动化例行任务，标志着迈向人工通用智能（AGI）的实现迈出了一步，革新了各个行业。然而，它们仍然难以满足用户的多样化和特定需求，也难以简化AI模型对普通用户的利用。因此，我们提出了ModelGPT，这是一个新颖的框架，旨在根据用户提供的数据或任务描述来确定和生成特定定制的AI模型，利用了LLM的能力。ModelGPT能够根据用户需求提供的模型，比之前的范式（例如全参数或LoRA微调）快至多270倍。在NLP、CV和表格数据集上进行的全面实验证明了我们的框架在使AI模型更易访问和用户友好方面的效果。我们的代码可在 https://github.com/IshiKura-a/ModelGPT 找到。

    arXiv:2402.12408v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has revolutionized various sectors by automating routine tasks, marking a step toward the realization of Artificial General Intelligence (AGI). However, they still struggle to accommodate the diverse and specific needs of users and simplify the utilization of AI models for the average user. In response, we propose ModelGPT, a novel framework designed to determine and generate AI models specifically tailored to the data or task descriptions provided by the user, leveraging the capabilities of LLMs. Given user requirements, ModelGPT is able to provide tailored models at most 270x faster than the previous paradigms (e.g. all-parameter or LoRA finetuning). Comprehensive experiments on NLP, CV, and Tabular datasets attest to the effectiveness of our framework in making AI models more accessible and user-friendly. Our code is available at https://github.com/IshiKura-a/ModelGPT.
    
[^146]: 教师作为宽容的专家：不依赖于教师的无数据知识蒸馏

    Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation

    [https://arxiv.org/abs/2402.12406](https://arxiv.org/abs/2402.12406)

    该论文发现现有的无数据知识蒸馏方法对不同的教师模型非常敏感，生成的样本可能出现质量问题。

    

    无数据知识蒸馏（DFKD）旨在在不使用原始数据的情况下，借助生成器将预训练知识蒸馏给学生模型。在这种无数据情况下，由于验证数据不可用，实现DFKD的稳定性是必不可少的。不幸的是，本文发现现有的DFKD方法对不同的教师模型非常敏感，有时即使使用训练良好的教师模型也会出现蒸馏的灾难性失败。我们的观察是DFKD中的生成器并不总是保证使用现有的旨在最小化类先验和对抗损失的代表性策略产生精确而多样化的样本。通过我们的实证研究，我们关注的事实是类先验不仅减少了生成样本的多样性，还不能完全解决根据教师模型生成意外低质量样本的问题。

    arXiv:2402.12406v1 Announce Type: cross  Abstract: Data-free knowledge distillation (DFKD) aims to distill pretrained knowledge to a student model with the help of a generator without using original data. In such data-free scenarios, achieving stable performance of DFKD is essential due to the unavailability of validation data. Unfortunately, this paper has discovered that existing DFKD methods are quite sensitive to different teacher models, occasionally showing catastrophic failures of distillation, even when using well-trained teacher models. Our observation is that the generator in DFKD is not always guaranteed to produce precise yet diverse samples using the existing representative strategy of minimizing both class-prior and adversarial losses. Through our empirical study, we focus on the fact that class-prior not only decreases the diversity of generated samples, but also cannot completely address the problem of generating unexpectedly low-quality samples depending on teacher mod
    
[^147]: 估计年龄条件下的平均治疗效应曲线：评估NBA中的负荷管理策略应用

    Estimating the age-conditioned average treatment effects curves: An application for assessing load-management strategies in the NBA

    [https://arxiv.org/abs/2402.12400](https://arxiv.org/abs/2402.12400)

    引入了一种新框架，通过比赛层次数据估计年龄曲线，提高了性能轨迹分析的细致度，可以发现复杂非线性模式，实现对年龄曲线的因果效应的细致研究。

    

    在竞技体育领域，了解运动员表现动态（通过年龄曲线展示进展、巅峰和衰退）至关重要。我们的研究引入了一种新颖的框架来量化年龄特定的治疗效应，增强了性能轨迹分析的细致度。首先，我们提出了一种使用比赛层次数据估计年龄曲线的方法，与传统的赛季级数据方法有所不同，通过利用先进的机器学习模型的元学习框架来解决其固有的复杂性。该方法揭示了现有方法忽略的复杂非线性模式。其次，我们的框架使得能够确定因果效应，可以详细研究不同条件下的年龄曲线。通过定义年龄条件下的治疗效应（ACTE），我们促进了对治疗影响因果关系的探讨。

    arXiv:2402.12400v1 Announce Type: cross  Abstract: In the realm of competitive sports, understanding the performance dynamics of athletes, represented by the age curve (showing progression, peak, and decline), is vital. Our research introduces a novel framework for quantifying age-specific treatment effects, enhancing the granularity of performance trajectory analysis. Firstly, we propose a methodology for estimating the age curve using game-level data, diverging from traditional season-level data approaches, and tackling its inherent complexities with a meta-learner framework that leverages advanced machine learning models. This approach uncovers intricate non-linear patterns missed by existing methods. Secondly, our framework enables the identification of causal effects, allowing for a detailed examination of age curves under various conditions. By defining the Age-Conditioned Treatment Effect (ACTE), we facilitate the exploration of causal relationships regarding treatment impacts a
    
[^148]: 将废料变废为宝：矫正MoE的Top-k路由器

    Turn Waste into Worth: Rectifying Top-$k$ Router of MoE

    [https://arxiv.org/abs/2402.12399](https://arxiv.org/abs/2402.12399)

    提出了Rectify-Router解决了MoE模型中常用的Top-k路由机制所带来的令牌丢失和填充问题，通过Intra-GPU矫正和Fill-in矫正来实现。

    

    稀疏混合专家（MoE）模型因其计算效率而受到欢迎，用于训练大型语言模型。然而，常用的Top-k路由机制由于不平衡的路由导致冗余计算和内存成本过高。一些专家会溢出，其中超出的令牌会被丢弃。而一些专家是空闲的，这些专家会填充为零，负面影响了模型性能。为了解决丢弃令牌和填充问题，我们提出了Rectify-Router，包括Intra-GPU矫正和Fill-in矫正。Intra-GPU矫正处理丢弃的令牌，将它们有效地路由到GPU内的专家，避免跨GPU通信。Fill-in矫正通过用具有高路由分数的令牌替换填充令牌来解决填充问题。我们的实验结果表明，Intra-GPU矫正和Fill-in矫正

    arXiv:2402.12399v1 Announce Type: cross  Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectificati
    
[^149]: 在线评估中利用主次要因素一致性作为领域知识指导幸福计算

    Primary and Secondary Factor Consistency as Domain Knowledge to Guide Happiness Computing in Online Assessment

    [https://arxiv.org/abs/2402.12398](https://arxiv.org/abs/2402.12398)

    本文尝试通过实证研究角度提供对解释一致性的新见解，并研究如何通过引入领域知识约束来使机器学习模型更加可信。

    

    基于大规模在线网络数据和机器学习方法的幸福计算是一个新兴的研究课题，支持个人成长和社会稳定等一系列问题。许多带有解释的先进机器学习（ML）模型被用于计算在线评估幸福，同时保持高准确性的结果。然而，这些模型缺乏主次幸福因素关系等领域知识约束，这限制了计算结果与发生的原因之间的关联。本文试图从经验研究的角度提供对解释一致性的新见解。然后我们研究如何表示和引入领域知识约束以使ML模型更加可信。我们通过以下方式实现这一目标：（1）证明具有附加因素归因的多个预测模型将具有良好的性质

    arXiv:2402.12398v1 Announce Type: new  Abstract: Happiness computing based on large-scale online web data and machine learning methods is an emerging research topic that underpins a range of issues, from personal growth to social stability. Many advanced Machine Learning (ML) models with explanations are used to compute the happiness online assessment while maintaining high accuracy of results. However, domain knowledge constraints, such as the primary and secondary relations of happiness factors, are absent from these models, which limits the association between computing results and the right reasons for why they occurred. This article attempts to provide new insights into the explanation consistency from an empirical study perspective. Then we study how to represent and introduce domain knowledge constraints to make ML models more trustworthy. We achieve this through: (1) proving that multiple prediction models with additive factor attributions will have the desirable property of pr
    
[^150]: 多类别时间逻辑神经网络

    Multi-class Temporal Logic Neural Networks

    [https://arxiv.org/abs/2402.12397](https://arxiv.org/abs/2402.12397)

    提出了一种结合神经网络和信号时间逻辑的方法，用于多类别时间序列数据的分类，关键贡献包括引入边界概念和利用STL属性增强结果的可解释性。

    

    时间序列数据可以代表无人系统（如无人机和自动驾驶汽车）的行为。在这一领域，二元和多类别分类问题受到了广泛关注。神经网络是一种流行的分类数据的方法；然而，它们缺乏可解释性，这在从中提取有意义的信息方面构成了重要挑战。信号时间逻辑（STL）是一种描述定时行为属性的形式化语言。我们提出了一种将所有这些元素结合在一起的方法：使用表示STL规范的神经网络进行时间序列数据的多类别分类。我们提供了两个关键贡献：1）我们引入了多类别分类的边界概念，2）我们引入了基于STL的属性来增强结果的可解释性。我们在两个数据集上评估了我们的方法，并与最先进的基准进行了比较。

    arXiv:2402.12397v1 Announce Type: cross  Abstract: Time-series data can represent the behaviors of autonomous systems, such as drones and self-driving cars. The problem of binary and multi-class classification has received a lot of attention in this field. Neural networks represent a popular approach to classifying data; However, they lack interpretability, which poses a significant challenge in extracting meaningful information from them. Signal Temporal Logic (STL) is a formalism to describe the properties of timed behaviors. We propose a method that combines all of the above: neural networks that represent STL specifications for multi-class classification of time-series data. We offer two key contributions: 1) We introduce a notion of margin for multi-class classification, and 2) we introduce the use of STL-based attributes for enhancing the interpretability of the results. We evaluate our method on two datasets and compare with state-of-the-art baselines.
    
[^151]: 利用生物标志物提高模型的解释性和可靠性

    Improving Model's Interpretability and Reliability using Biomarkers

    [https://arxiv.org/abs/2402.12394](https://arxiv.org/abs/2402.12394)

    利用决策树解释基于生物标志物的诊断模型，帮助临床医生提高识别不准确预测的能力，从而增强医学诊断模型的可靠性。

    

    准确且具有解释性的诊断模型在医学这个安全关键领域至关重要。我们研究了我们提出的基于生物标志物的肺部超声诊断流程的可解释性，以增强临床医生的诊断能力。本研究的目标是评估决策树分类器利用生物标志物提供的解释是否能够改善用户识别模型不准确预测能力，与传统的显著性图相比。我们的研究发现表明，基于临床建立的生物标志物的决策树解释能够帮助临床医生检测到假阳性，从而提高医学诊断模型的可靠性。

    arXiv:2402.12394v1 Announce Type: cross  Abstract: Accurate and interpretable diagnostic models are crucial in the safety-critical field of medicine. We investigate the interpretability of our proposed biomarker-based lung ultrasound diagnostic pipeline to enhance clinicians' diagnostic capabilities. The objective of this study is to assess whether explanations from a decision tree classifier, utilizing biomarkers, can improve users' ability to identify inaccurate model predictions compared to conventional saliency maps. Our findings demonstrate that decision tree explanations, based on clinically established biomarkers, can assist clinicians in detecting false positives, thus improving the reliability of diagnostic models in medicine.
    
[^152]: 实现基因表达数据科学发现的AI科学家团队

    Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data

    [https://arxiv.org/abs/2402.12391](https://arxiv.org/abs/2402.12391)

    引入了一个名为AI科学家团队（TAIS）的框架，旨在简化科学发现流程，由模拟角色协作，特别关注于识别具有疾病预测价值的基因

    

    机器学习已成为科学发现的强大工具，使研究人员能够从复杂数据集中提取有意义的见解。我们引入了一个新颖的框架，名为AI科学家团队（TAIS），旨在简化科学发现流程。TAIS包括模拟角色，包括项目经理、数据工程师和领域专家，每个角色由大型语言模型（LLM）代表。这些角色协作以复制数据科学家通常执行的任务，特别关注于识别具有疾病预测价值的基因。

    arXiv:2402.12391v1 Announce Type: cross  Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset t
    
[^153]: 模拟失调: 大型语言模型的安全对齐可能会适得其反！

    Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!

    [https://arxiv.org/abs/2402.12343](https://arxiv.org/abs/2402.12343)

    安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。

    

    大型语言模型（LLMs）需要进行安全对齐，以确保与人类进行安全的对话。然而，在这项工作中，我们引入了一种推理时攻击框架，表明安全对齐也可能在对抗性操纵下无意中促成有害结果。这个框架被命名为模拟失调（ED），在输出空间中不良地组合了一对开源预训练和安全对齐的语言模型，产生了一个有害的语言模型而无需任何训练。我们对ED在三个数据集和四个模型系列（Llama-1、Llama-2、Mistral和Alpaca）上的实验表明，ED使预训练模型的有害性增加了一倍，并胜过强基线，以较大优势在48个评估子集中的43个中实现了最高的有害率。至关重要的是，我们的研究结果凸显了即使在安全对齐后，重新评估开源语言模型实践的重要性。

    arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
    
[^154]: 赋予预训练图模型具有可证明的公平性

    Endowing Pre-trained Graph Models with Provable Fairness

    [https://arxiv.org/abs/2402.12161](https://arxiv.org/abs/2402.12161)

    提出了一种新的适配器调优框架，赋予预训练图模型具有可证明的公平性

    

    预训练图模型（PGMs）旨在捕捉可转移的固有结构属性，并将其应用于不同的下游任务。类似于预训练语言模型，PGMs也会继承人类社会中的偏见，导致在下游应用中出现歧视行为。现有公平方法的去偏见过程通常与GNNs的参数优化相结合。然而，不同的下游任务在现实中可能与不同的敏感属性相关联，直接采用现有方法改善PGMs的公平性是不灵活且低效的。此外，大多数方法缺乏理论保证，即对模型预测公平性的可证明下限，这直接提供了实际场景下的保证。为了克服这些限制，我们提出了一种新的适配器调优框架，赋予预训练\textbf{图}模型具有\textbf{可证明}的\textbf{公}平\textbf{性}（称为

    arXiv:2402.12161v1 Announce Type: cross  Abstract: Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \textbf{Graph} models with \textbf{P}rovable f\textbf{A}i\textbf{R}ness (called
    
[^155]: WKVQuant：量化大型语言模型的参数权重和键值缓存以提高性能

    WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More

    [https://arxiv.org/abs/2402.12065](https://arxiv.org/abs/2402.12065)

    该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。

    

    大型语言模型（LLMs）面临着部署挑战，主要是由于其巨大的内存需求和自回归文本生成过程的计算需求。本文通过关注LLMs的量化来解决这些挑战，量化是一种通过将模型参数和激活转换为低比特整数来减少内存消耗的技术。我们批判性地分析了现有的量化方法，识别出它们在平衡量化LLMs的准确性和效率方面的局限性。为了超越这些局限性，我们提出了WKVQuant，这是一个专为量化LLMs的参数权重和键值（KV）缓存而设计的PTQ框架。具体而言，我们引入了仅考虑过去的量化以改善注意力计算。此外，我们还介绍了二维量化策略来处理KV缓存的分布，以及一种跨块重建正则化方法以帮助模型压缩。

    arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
    
[^156]: AICAttack：基于注意力优化的对抗性图像字幕攻击

    AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization

    [https://arxiv.org/abs/2402.11940](https://arxiv.org/abs/2402.11940)

    提出了一种新的对抗攻击策略AICAttack，旨在通过微小的图像扰动来攻击图像字幕模型，在黑盒攻击情景下具有良好的效果。

    

    最近深度学习研究取得了在计算机视觉（CV）和自然语言处理（NLP）等许多任务上显著的成就。CV和NLP交叉点上的图像字幕问题中，相关模型对抗攻击的稳健性尚未得到充分研究。本文提出了一种新颖的对抗攻击策略，称为AICAttack（基于注意力的图像字幕攻击），旨在通过对图像进行微小扰动来攻击图像字幕模型。在黑盒攻击环境中运行，我们的算法不需要访问目标模型的架构、参数或梯度信息。我们引入了基于注意力的候选选择机制，可识别最佳像素进行攻击，然后采用差分进化（DE）来扰乱像素的RGB值。通过对基准上的广泛实验，我们证明了AICAttack的有效性。

    arXiv:2402.11940v1 Announce Type: cross  Abstract: Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchma
    
[^157]: 一种用于时空图迁移学习的生成式预训练框架

    A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning

    [https://arxiv.org/abs/2402.11922](https://arxiv.org/abs/2402.11922)

    提出了一种生成式预训练框架 GPDiff，通过在源城市数据优化的模型参数上进行预训练，将STG迁移学习转化为预训练生成式超网络，实现了对不同数据分布和特定城市特征的适应性。

    

    时空图（STG）学习对于智慧城市应用至关重要，然而在许多城市和地区往往存在数据稀缺问题。为了弥补这一差距，我们提出了一种新颖的生成式预训练框架 GPDiff，用于STG迁移学习。与传统方法不同，我们的解决方案采用了一种新颖的方法，通过在经过源城市数据优化的一系列模型参数上进行生成式预训练来执行STG迁移学习。

    arXiv:2402.11922v1 Announce Type: new  Abstract: Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning. Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the
    
[^158]: 大型语言模型对图形召回的微结构和准确性

    Microstructures and Accuracy of Graph Recall by Large Language Models

    [https://arxiv.org/abs/2402.11821](https://arxiv.org/abs/2402.11821)

    本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。

    

    图形数据对许多应用至关重要，其中很多数据以文本格式描述关系。因此，准确地召回和编码先前文本中描述的图形是大型语言模型(LLMs)需要展示的基本但关键能力，以执行涉及图形结构信息的推理任务。人类在图形召回方面的表现已被认知科学家研究了几十年，发现其经常呈现与人类处理社会关系一致的某些结构性偏见模式。然而，迄今为止，我们很少了解LLMs在类似图形召回任务中的行为：它们召回的图形是否也呈现某些偏见模式，如果是，它们与人类的表现有何不同并如何影响其他图形推理任务？在这项研究中，我们进行了第一次对LLMs进行图形召回的系统研究，研究其准确性和偏见微结构（局部结构）。

    arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
    
[^159]: MARS：用于生成式LLMs中不确定性估计的意义感知响应评分

    MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs

    [https://arxiv.org/abs/2402.11756](https://arxiv.org/abs/2402.11756)

    MARS提出了一种新的评分函数MARS，考虑了生成序列中每个标记的语义贡献，该方法改进了生成式LLMs中的不确定性估计性能。

    

    生成式大型语言模型（LLMs）因在各种任务中的卓越表现而被广泛利用。然而，它们产生不准确或误导性输出的倾向可能带来潜在风险，尤其是在高风险环境中。因此，估计生成式LLM输出的正确性是增强可靠性的重要任务。生成式LLMs中的不确定性估计（UE）是一个不断发展的领域，其中SOTA基于概率的方法通常采用长度标准化评分。在这项工作中，我们提出了一种名为意义感知响应评分（MARS）的替代长度标准化评分的UE方法。MARS是一种考虑在问题的上下文中生成序列中每个标记的语义贡献的新型评分函数。我们证明将MARS整合到UE方法中会在UE性能上带来普遍和显著的改进。我们使用三种不同的闭卷式问答来进行实验

    arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
    
[^160]: 对角化SGD：通过重新参数化和平滑实现非可微模型的快速收敛SGD

    Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing

    [https://arxiv.org/abs/2402.11752](https://arxiv.org/abs/2402.11752)

    引入了Diagonalisation Stochastic Gradient Descent（对角化SGD），通过重新参数化和平滑实现非可微模型的快速收敛SGD，在实证评估中表现出简单、快速、稳定，并且取得了数量级的工作规范化方差降低。

    

    众所周知，对于非可微模型，展现出较低方差的重新参数化梯度估计器在实践中存在偏差。这可能危及基于梯度的优化方法（如随机梯度下降SGD）的正确性。我们引入了一个简单的语法框架来分块地定义非可微函数，并提出了一种系统方法，以获得使重新参数化梯度估计器无偏的平滑。我们的主要贡献是一种新颖的SGD变体，对角化随机梯度下降，它在优化过程中逐步提高平滑近似的准确性，并证明收敛到未平滑（原始）目标的稳定点。我们的实证评估显示，与现有技术相比，我们的方法简单、快速、稳定，并且在工作规范化方差上实现了数量级的降低。

    arXiv:2402.11752v1 Announce Type: cross  Abstract: It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.
    
[^161]: GraphKD：探索知识蒸馏在文档目标检测中的应用，并通过结构化图创建实现

    GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation

    [https://arxiv.org/abs/2402.11401](https://arxiv.org/abs/2402.11401)

    提出了一种基于图的知识蒸馏框架，用于在文档图像中识别和定位文档对象，以减少大型模型在资源受限设备上的部署成本

    

    在文档中进行目标检测是自动化识别数字或扫描文档中的结构元素的关键步骤，通过理解不同元素之间的分层结构和关系。大型和复杂的模型虽然可以实现高准确性，但在计算上昂贵且占用内存，使其在资源受限设备上部署变得不切实际。知识蒸馏允许我们创建小型且更高效的模型，这些模型保留了大型模型的大部分性能。在这里，我们提出了一种基于图的知识蒸馏框架，以正确识别并定位文档图像中的文档对象。在这里，我们设计了一个具有建议级特征的结构化图，边表示不同建议区域之间的关系。此外，为了减少文本偏见，设计了一种自适应节点抽样策略来修剪权重。

    arXiv:2402.11401v1 Announce Type: cross  Abstract: Object detection in documents is a key step to automate the structural elements identification process in a digital or scanned document through understanding the hierarchical structure and relationships between different elements. Large and complex models, while achieving high accuracy, can be computationally expensive and memory-intensive, making them impractical for deployment on resource constrained devices. Knowledge distillation allows us to create small and more efficient models that retain much of the performance of their larger counterparts. Here we present a graph-based knowledge distillation framework to correctly identify and localize the document objects in a document image. Here, we design a structured graph with nodes containing proposal-level features and edges representing the relationship between the different proposal regions. Also, to reduce text bias an adaptive node sampling strategy is designed to prune the weight
    
[^162]: 通过贝叶斯优化从钙钛矿实验中提取基于物理的材料参数

    Physics-based material parameters extraction from perovskite experiments via Bayesian optimization

    [https://arxiv.org/abs/2402.11101](https://arxiv.org/abs/2402.11101)

    使用贝叶斯优化开发了一个分析平台，可以从钙钛矿实验中提取多个基本材料参数，加速材料发现和半导体优化

    

    从定量实验分析中提取材料参数的能力对于合理设计和理论进步至关重要。然而，随着理论模型的复杂性和材料参数数量的增加，这种分析的难度显着增加。在这里，我们使用贝叶斯优化开发了一个分析平台，可以从瞬态光致发光实验中提取一个有机金属钙钛矿半导体的8个基本材料参数，基于一个包括载流子漂移扩散和动态缺陷占据的复杂全物理模型。热降解的一个示例研究表明，掺杂浓度和载流子迁移率的变化主导，而缺陷能级几乎保持不变。这个平台可以方便地应用于其他实验或实验组合，加速材料发现和半导体优化。

    arXiv:2402.11101v1 Announce Type: cross  Abstract: The ability to extract material parameters from quantitative experimental analysis is essential for rational design and theory advancement. However, the difficulty of this analysis increases significantly with the complexity of the theoretical model and the number of material parameters. Here we use Bayesian optimization to develop an analysis platform that can extract up to 8 fundamental material parameters of an organometallic perovskite semiconductor from a transient photoluminescence experiment, based on a complex full physics model that includes drift-diffusion of carriers and dynamic defect occupation. An example study of thermal degradation reveals that changes in doping concentration and carrier mobility dominate, while the defect energy level remains nearly unchanged. This platform can be conveniently applied to other experiments or to combinations of experiments, accelerating materials discovery and optimization of semiconduc
    
[^163]: 基于神经网络的机器学习中的最佳特征重缩放

    Optimal feature rescaling in machine learning based on neural networks

    [https://arxiv.org/abs/2402.10964](https://arxiv.org/abs/2402.10964)

    提出了一种通过遗传算法进行输入特征的最佳重缩放来改善神经网络训练效率和泛化性能的方法。

    

    这篇论文提出了一种新的方法，通过遗传算法（GA）进行输入特征的最佳重缩放（OFR）来改善前馈神经网络（FFNNs）的训练效率和泛化性能。OFR重新塑造了输入空间，改善了用于训练的基于梯度的算法的条件。此外，GA进行的比例因子探索和选择对应于每次训练尝试中第一层权重的不同初始化，从而实现了一个多起点全局搜索算法（尽管仅限于少量权重），从而促进了全局最小值的实现。该方法已在模拟实际工业过程（无心磨削）结果的FFNN上进行了测试。

    arXiv:2402.10964v1 Announce Type: new  Abstract: This paper proposes a novel approach to improve the training efficiency and the generalization performance of Feed Forward Neural Networks (FFNNs) resorting to an optimal rescaling of input features (OFR) carried out by a Genetic Algorithm (GA). The OFR reshapes the input space improving the conditioning of the gradient-based algorithm used for the training. Moreover, the scale factors exploration entailed by GA trials and selection corresponds to different initialization of the first layer weights at each training attempt, thus realizing a multi-start global search algorithm (even though restrained to few weights only) which fosters the achievement of a global minimum. The approach has been tested on a FFNN modeling the outcome of a real industrial process (centerless grinding).
    
[^164]: 异类自动提示的不合理有效性

    The Unreasonable Effectiveness of Eccentric Automatic Prompts

    [https://arxiv.org/abs/2402.10949](https://arxiv.org/abs/2402.10949)

    异类自动提示的不合理有效性研究了大型语言模型在处理各种提示时的表现，结果显示在大多数情况下，包括“积极思考”提示会对模型性能产生积极影响。

    

    大型语言模型（LLMs）展示了出色的问题解决和基本数学能力。然而，它们的功效高度依赖于提示的制定。本研究旨在量化将“积极思考”纳入系统提示消息的影响，然后将其与系统化提示优化进行比较。我们评估了60种系统消息片段的性能，分别使用和不使用Chain of Thought提示，跨三个参数范围从70亿到70亿个变量的模型，在GSM8K数据集上进行测试。我们的发现表明，结果并不在所有模型中普遍适用。在大多数情况下，包括“积极思考”提示会积极影响模型性能。然而，值得注意的是，Llama2-70B在不使用Chain of Thought时是个例外，因为发现最佳系统消息实际上是没有消息。考虑到组合复杂性，以及其导至的加# Truncated due to exceeding character limit.

    arXiv:2402.10949v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus com
    
[^165]: ConSmax: 具有可学习参数的硬件友好型Softmax替代方案

    ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters

    [https://arxiv.org/abs/2402.10930](https://arxiv.org/abs/2402.10930)

    ConSmax是一种硬件友好型Softmax替代方案，通过引入可学习参数，在不影响性能的情况下实现了对原Softmax关键任务的高效处理。

    

    自注意机制将基于transformer的大型语言模型（LLM）与卷积和循环神经网络区分开来。尽管性能有所提升，但由于自注意中广泛使用Softmax，在硅上实现实时LLM推断仍具挑战性。为了解决这一挑战，我们提出了Constant Softmax（ConSmax），这是一种高效的Softmax替代方案，采用可微的规范化参数来消除Softmax中的最大搜索和分母求和，实现了大规模并行化。

    arXiv:2402.10930v1 Announce Type: cross  Abstract: The self-attention mechanism sets transformer-based large language model (LLM) apart from the convolutional and recurrent neural networks. Despite the performance improvement, achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context. To address this challenge, we propose Constant Softmax (ConSmax), a software-hardware co-design as an efficient Softmax alternative. ConSmax employs differentiable normalization parameters to remove the maximum searching and denominator summation in Softmax. It allows for massive parallelization while performing the critical tasks of Softmax. In addition, a scalable ConSmax hardware utilizing a bitwidth-split look-up table (LUT) can produce lossless non-linear operation and 
    
[^166]: 三界之最：实践中的数字营销自适应实验

    Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice

    [https://arxiv.org/abs/2402.10870](https://arxiv.org/abs/2402.10870)

    本文分享了关于在工业环境中使用AED系统面临的挑战，并提供了在这种环境中适当的目标和系统规格的视角，最终开发了一个基于反事实推断的AED框架并在商业环境中进行了测试。

    

    自适应实验设计（AED）方法越来越多地被工业界用作一种工具，以提高测试吞吐量或减少与传统A/B/N测试方法相比的实验成本。然而，这些方法的行为和保证在理想化的稳态设置之外并不为人熟知。本文分享了有关在工业环境中天真地使用AED系统面临的挑战，以及在这种环境中适当的目标和系统规格的视角。我们根据这些经验开发了一个基于反事实推断的AED框架，并在商业环境中进行了测试。

    arXiv:2402.10870v1 Announce Type: new  Abstract: Adaptive experimental design (AED) methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods. However, the behavior and guarantees of such methods are not well-understood beyond idealized stationary settings. This paper shares lessons learned regarding the challenges of naively using AED systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in such settings. We developed an AED framework for counterfactual inference based on these experiences, and tested it in a commercial environment.
    
[^167]: 使用不同标注函数的协作学习

    Collaborative Learning with Different Labeling Functions

    [https://arxiv.org/abs/2402.10445](https://arxiv.org/abs/2402.10445)

    研究了使用不同标注函数的协作学习中，基于经验风险最小化算法在增强假设类上的高效学习方法。

    

    我们研究了一种 Collaborative PAC Learning 的变体，在这种情况下，我们旨在学习每个$n$个数据分布的准确分类器，同时最小化从它们总共抽取的样本数量。与通常的协作学习设置不同，不假设存在一个同时对所有分布准确的单一分类器。我们表明，当数据分布满足较弱的可实现性假设时，仍然可以实现高效的学习。我们给出了一种基于经验风险最小化(ERM)的学习算法，应用于假设类的一个自然增强，分析依赖于对该增强类的VC维的上界。

    arXiv:2402.10445v1 Announce Type: new  Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.   In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, 
    
[^168]: 重塑RLHF中的信息结构：基于图论的奖励泛化视角

    Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective

    [https://arxiv.org/abs/2402.10184](https://arxiv.org/abs/2402.10184)

    本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。

    

    在强化学习从人类反馈中（RLHF）存在一个三难问题：高度多样的环境、低标注成本和可靠的对齐性能之间的不兼容性。本文旨在通过设计奖励建模过程中的数据集信息结构来缓解这种不兼容性。具体而言，我们重新审视了RLHF过程，并提出了一个理论框架将其描绘为文本分布上的自动编码过程。我们的框架形式化了RLHF目标，即确保人类偏好与大型语言模型（LLM）行为之间的分布一致性。基于这个框架，我们系统地研究了RLHF奖励建模阶段中信息结构的性能影响。为了进一步理解奖励建模阶段中的奖励泛化，我们引入了一种基于随机图论的方法来建模语义空间中的泛化。其中的关键见解是...

    arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
    
[^169]: 有限预算下的迅速学习最佳臂识别

    Best Arm Identification for Prompt Learning under a Limited Budget

    [https://arxiv.org/abs/2402.09723](https://arxiv.org/abs/2402.09723)

    这项工作提出了一种在提示学习中考虑有限预算约束的方法，通过建立提示学习和多臂赌博机中固定预算最佳臂识别之间的联系，提出了一个通用框架TRIPLE，通过利用聚类和嵌入思想实现了两个增强方法。

    

    大型语言模型（LLMs）的显著指令跟随能力引发了对自动学习合适提示的兴趣。然而，虽然提出了许多有效的方法，但在学习过程中产生的成本（例如访问LLM和评估响应）尚未得到考虑。为克服这个限制，本工作在提示学习中明确引入了有限预算约束。为了开发有原则的解决方案，本研究在提示学习和多臂赌博机的固定预算最佳臂识别（BAI-FB）之间建立了一种新的联系。基于这种联系，提出了一个通用框架TRIPLE（用于提示学习的最佳臂识别），以系统地利用BAI-FB在提示学习中的力量。提示学习的独特特点进一步通过利用聚类和嵌入思想提出了TRIPLE的两个基于嵌入的增强方法。

    arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
    
[^170]: 数据到文本自然语言生成研究的系统性回顾

    A Systematic Review of Data-to-Text NLG

    [https://arxiv.org/abs/2402.08496](https://arxiv.org/abs/2402.08496)

    这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。

    

    这篇系统性回顾旨在全面分析数据到文本生成研究的现状，重点是确定研究空白，提供未来方向，并解决回顾中发现的挑战。我们对文献进行了全面的检查，包括方法、数据集、评估指标、应用、多语言性和幻觉缓解措施。我们的回顾为这个快速发展的领域的未来研究提供了路线图。

    This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
    
[^171]: 深度强化学习在细胞重编程的布尔模型吸引子景观中的控制遍历中的应用研究

    Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming

    [https://arxiv.org/abs/2402.08491](https://arxiv.org/abs/2402.08491)

    本研究开发了一个基于深度强化学习的计算框架，用于细胞重编程中的重编程策略识别。在控制问题中，引入了伪吸引子的概念和识别方法，并设计了一个用于解决该问题的计算框架。

    

    细胞重编程可用于预防和治疗不同疾病。然而，通过传统湿实验发现重编程策略的效率受到时间和成本的限制。在本研究中，我们基于深度强化学习开发了一个新颖的计算框架，以便帮助识别重编程策略。为此，我们在细胞重编程框架的BNs和PBNs以及异步更新模式下制定了一个控制问题。此外，我们引入了伪吸引子的概念和训练过程中伪吸引子状态的识别方法。最后，我们设计了一个用于解决控制问题的计算框架，并在多个不同模型上进行了测试。

    Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.
    
[^172]: ChatCell: 利用自然语言促进单细胞分析

    ChatCell: Facilitating Single-Cell Analysis with Natural Language

    [https://arxiv.org/abs/2402.08303](https://arxiv.org/abs/2402.08303)

    ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。

    

    随着大型语言模型(LLMs)的快速发展，它们在科学中的影响日益突出。LLMs在任务泛化和自由对话方面的新兴能力可以极大地推进化学和生物学等领域。然而，单细胞生物学这个构成生物体基础构件的领域仍面临一些挑战。当前方法在知识门槛和可扩展性方面存在限制，阻碍了LLMs在掌握单细胞数据方面的充分利用，影响了直接可访问和快速迭代的能力。为此，我们引入了ChatCell，通过利用词汇适应和统一序列生成，它在单细胞生物学领域获得了深厚的专业知识和适应各种分析任务的能力，标志着一种范式转变。

    As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
    
[^173]: 使用无监督度量优化GNN进行节点聚类的研究

    An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering

    [https://arxiv.org/abs/2402.07845](https://arxiv.org/abs/2402.07845)

    本研究展示了使用无监督度量模块性优化GNN进行节点聚类的方法，且无需与基准值进行比较。在设计合成实验的过程中，我们发现了这种方法的局限性。

    

    图神经网络（GNN）可以通过学习特征和连接信息的二元性来训练以检测图中的社区。目前，优化GNN的常见方法是使用与基准值的比较来进行超参数调整和模型选择。本研究表明，仅通过优化模块性，可以使用GNN将节点聚类成社区，而无需与基准值进行比较。尽管模块性是一种图分区质量度量，我们证明这也可以用于优化同时编码特征的GNN，并且不会降低性能。我们进一步研究无监督度量性能是否能够预测基准值的性能。为了探究为什么可以使用模块性优化GNN，我们设计了一些合成实验来展示这种方法的局限性。这些合成图表明其在不同、随机和零信息空间分区中的当前能力。

    Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information. Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection. In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth. Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance. We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance. To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach. The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in att
    
[^174]: Pathformer: 多尺度自适应路径的时间序列预测模型

    Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting

    [https://arxiv.org/abs/2402.05956](https://arxiv.org/abs/2402.05956)

    本文提出了一种名为Pathformer的多尺度自适应路径的Transformer模型，用于时间序列预测。通过整合时间分辨率和时间距离进行多尺度建模，并使用自适应路径来优化建模过程，可以提高预测准确性和泛化能力。

    

    基于Transformer的模型在时间序列预测中取得了一些成功。现有的方法主要从有限或固定尺度对时间序列进行建模，这使得捕捉跨多个尺度的不同特征变得具有挑战性。本文提出了一种多尺度自适应路径（Pathformer）的Transformer模型。该模型同时整合了时间分辨率和时间距离进行多尺度建模。多尺度划分运用不同大小的数据块将时间序列分割成不同的时间分辨率。基于每个尺度的划分，对这些数据块进行双重注意力机制，以捕捉全局相关性和局部细节作为时间依赖关系。我们进一步通过自适应路径来丰富多尺度Transformer，该路径可以根据输入时间序列中不断变化的时间动态调整多尺度建模过程，提高Pathformer的预测准确性和泛化能力。在11个真实数据集上进行了大量实验。

    Transformer-based models have achieved some success in time series forecasting. Existing methods mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. In this paper, we propose multi-scale transformers with adaptive pathways (Pathformer). The proposed Transformer integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics in the input time series, improving the prediction accuracy and generalization of Pathformer. Extensive experiments on eleven rea
    
[^175]: 关于大规模语言模型中零阶联邦调整的收敛性

    On the Convergence of Zeroth-Order Federated Tuning in Large Language Models

    [https://arxiv.org/abs/2402.05926](https://arxiv.org/abs/2402.05926)

    我们的研究提出了一种名为FedMeZO的方法，以在零阶联邦学习和大规模语言模型之间实现内存高效的调整。我们的理论和实证证据表明FedMeZO不仅收敛速度快于传统的一阶方法，而且在个性化的联邦策略中具有关键的作用。

    

    联邦学习（FL）和大规模语言模型（LLM）的融合为隐私保护的自然语言处理带来了新时代。然而，精调LLM所需的强大内存要求在部署到边缘设备时会面临重大挑战，因为这些设备的计算资源有限。为了解决这个问题，我们在联邦环境中探索了内存高效的零阶优化的全新整合，我们称之为FedMeZO。我们的研究是第一个在LLM背景下考察FedMeZO的理论基础的研究，涉及到大参数空间对优化行为的影响、收敛性的建立以及为个性化的联邦策略确定关键参数的问题。我们广泛的实证证据支持了这个理论，表明FedMeZO不仅比传统的一阶方法（如SGD）收敛更快，而且明显...

    The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also signific
    
[^176]: LESS：用于目标指导调整的选择有影响力的数据

    LESS: Selecting Influential Data for Targeted Instruction Tuning

    [https://arxiv.org/abs/2402.04333](https://arxiv.org/abs/2402.04333)

    LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。

    

    指令调整已经在大型语言模型中释放出强大的能力，有效地使用组合数据集来开发通用聊天机器人。然而，实际应用往往需要一套专门的技能（例如推理）。挑战在于从这些广泛的数据集中识别出最相关的数据，以有效开发特定的能力，我们将这种情况称为目标指导调整。我们提出了LESS，一种优化感知且实际高效的算法，以有效估计数据影响并执行适用于指令数据选择的低秩梯度相似性搜索。关键在于LESS将现有的影响公式调整为与Adam优化器和可变长度指令数据一起工作。LESS首先构建了一个具有低维梯度特征的高度可重用和可传递的梯度数据存储库，然后根据它们与具有特定能力的少样本示例的相似度选择示例。实验证明，t

    Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
    
[^177]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^178]: 变分流模型：以你的风格流动

    Variational Flow Models: Flowing in Your Style

    [https://arxiv.org/abs/2402.02977](https://arxiv.org/abs/2402.02977)

    我们引入了一种变分流模型的方法，并提出了一种系统的无需训练的转换方法，使得快速采样成为可能，同时保持了采样的准确性和效率。

    

    我们引入了一种对"后验流"模型进行变分推理解释的方法——用以将"概率流"推广到更广泛的随机过程类别，不必局限于扩散过程。我们将这种结果称为"变分流模型"。此外，我们提出了一种无需训练的系统方法，将由方程Xt = at * X0 + st * X1所描述的"线性"随机过程的后验流转化为直线恒速(SC)流，类似于矫正流。这种转化使得可以快速沿着原始的后验流进行采样，而无需训练一个新的SC流模型。我们的方法的灵活性使我们能够将转换扩展到两个不同"线性"随机过程的后验流之间进行互相转化。此外，我们还可以将高阶数值解法轻松集成到转换后的SC流中，进一步提高采样的准确性和效率。我们进行了严格的理论分析和大量实验结果的验证。

    We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
    
[^179]: 减少不完全合作博弈中的乐观偏误

    Reducing Optimism Bias in Incomplete Cooperative Games

    [https://arxiv.org/abs/2402.01930](https://arxiv.org/abs/2402.01930)

    本文提出了一个框架，旨在通过优化揭示联盟价值的顺序来减少不完全合作博弈中的乐观偏误。

    

    合作博弈理论在当代人工智能中具有广泛的应用，包括解释性机器学习、资源分配和协同决策等领域。然而，指定一个合作博弈需要为指数多个联盟分配价值，并且在实践中获得一个联盟价值可能会消耗大量资源。然而，简单地不公开某些联盟的价值会引入关于个体对集体大联盟的贡献的模糊性。这种模糊性经常导致玩家持有过于乐观的期望，其源于内在偏见或战略考虑，进而常常导致集体要求超过实际的大联盟价值。本文提出了一个框架，旨在优化揭示联盟价值的顺序，以实现有效地缩小合作博弈中玩家期望与可实现结果之间的差距。

    Cooperative game theory has diverse applications in contemporary artificial intelligence, including domains like interpretable machine learning, resource allocation, and collaborative decision-making. However, specifying a cooperative game entails assigning values to exponentially many coalitions, and obtaining even a single value can be resource-intensive in practice. Yet simply leaving certain coalition values undisclosed introduces ambiguity regarding individual contributions to the collective grand coalition. This ambiguity often leads to players holding overly optimistic expectations, stemming from either inherent biases or strategic considerations, frequently resulting in collective claims exceeding the actual grand coalition value. In this paper, we present a framework aimed at optimizing the sequence for revealing coalition values, with the overarching goal of efficiently closing the gap between players' expectations and achievable outcomes in cooperative games. Our contributio
    
[^180]: PRewrite: 使用强化学习的提示重写

    PRewrite: Prompt Rewriting with Reinforcement Learning

    [https://arxiv.org/abs/2401.08189](https://arxiv.org/abs/2401.08189)

    本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。

    

    arXiv:2401.08189v2 公告类型: 替换 摘要: 提示工程对于基于LLM的应用程序的开发至关重要。然而，通常以“试错”的方式手动完成。这种手动程序可能耗时，效果不佳，并且在许多情况下生成的提示都是次优的。即使对那些看似运作良好的提示，始终存在一个悬而未决的问题：是否可以通过进一步修改使提示变得更好呢？为了解决这些问题，在本文中，我们研究了提示工程自动化。我们考虑了一个特定的使用情景，即开发者/用户已经起草了初始提示，但缺乏时间/专业知识来优化它们。我们提出了PRewrite，一个自动化工具，可重写这些草案，并生成高效的新提示。PRewrite基于强化学习（RL）框架，允许端到端优化，我们的设计允许RL搜索在大动作空间中进行。

    arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
    
[^181]: 在现实假设下的核回归中的泛化

    Generalization in Kernel Regression Under Realistic Assumptions

    [https://arxiv.org/abs/2312.15995](https://arxiv.org/abs/2312.15995)

    本文提供了一个统一的理论，用于对几乎所有常见和现实设置下的核回归的超出风险进行上限约束，并揭示了核分解中存在的自我正则化现象。

    

    现在已经确立的事实是，现代过度参数化模型似乎能够逃避偏差-方差权衡，在过度拟合噪音的情况下泛化良好。许多最近的研究尝试分析这一现象在核回归相对易处理的设置中。然而，正如我们详细讨论的那样，大多数关于这个主题的过去的研究要么做出了不切实际的假设，要么专注于一个狭窄的问题设置。本文旨在提供一个统一的理论来限制几乎所有常见和现实设置下核回归的超出风险。具体来说，我们提供了对于常见核函数以及任意的正则化量、噪声、任意输入维度和任意样本数都成立的严格界限。此外，我们还为核矩阵的特征值提供了相对扰动界限，这可能具有独立的重要性。这些界限揭示了一种自我正则化现象，即核分解的特征值中存在重尾现象。

    arXiv:2312.15995v2 Announce Type: replace-cross  Abstract: It is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise. Many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression. However, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup. This work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings. Specifically, we provide rigorous bounds that hold for common kernels and for any amount of regularization, noise, any input dimension, and any number of samples. Furthermore, we provide relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest. These reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the ker
    
[^182]: 评估提示方法对ChatGPT的数学能力的影响

    Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities

    [https://arxiv.org/abs/2312.15006](https://arxiv.org/abs/2312.15006)

    三种提示方法对ChatGPT的数学能力并未产生一贯性改进效果，部分方法甚至导致性能下降

    

    本研究批判性地评估了提示方法在提升大型语言模型（LLMs）的数学推理能力方面的功效。该研究使用了三种规定性提示方法 - 简单提示、个人提示和对话提示 - 这些方法以提升LLMs语言任务效果而闻名。我们在OpenAI的LLM闲聊机器人ChatGPT-3.5上进行此分析，涵盖了来自MATH、GSM8K和MMLU数据集的广泛问题集合，这些问题涵盖了各种数学挑战。针对每个数据集调整的评分脚本用于确定这些提示干预在增强模型数学分析能力方面的效果。与预期相反，我们的实证分析显示，所检验的方法均未在持续改进ChatGPT-3.5基准表现上，部分方法甚至导致明显的退化。我们的发现表明，提示策略未必能提高模型的数学分析能力。

    arXiv:2312.15006v2 Announce Type: replace  Abstract: This study critically evaluates the efficacy of prompting methods in enhancing the mathematical reasoning capability of large language models (LLMs). The investigation uses three prescriptive prompting methods - simple, persona, and conversational prompting - known for their effectiveness in enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and MMLU datasets, encompassing a broad spectrum of mathematical challenges. A grading script adapted to each dataset is used to determine the effectiveness of these prompting interventions in enhancing the model's mathematical analysis power. Contrary to expectations, our empirical analysis reveals that none of the investigated methods consistently improves over ChatGPT-3.5's baseline performance, with some causing significant degradation. Our findings suggest that prompting strategies do not nece
    
[^183]: 带有固定预算的最佳臂识别：大偏差视角

    Best Arm Identification with Fixed Budget: A Large Deviation Perspective

    [https://arxiv.org/abs/2312.12137](https://arxiv.org/abs/2312.12137)

    本文通过建立经验比例和经验臂奖励之间的连接，提高了一些现有算法的错误概率上界。

    

    我们考虑使用固定抽样预算在随机多臂老虎机(MABs)中识别最佳臂的问题。表征该问题的最小特定实例误差概率构成MABs中一直存在的重要开放问题之一。当使用静态抽样策略选择臂时，错误概率随着样本数呈指数衰减，其速率可以通过大偏差技术明确推导。然而，分析具有自适应抽样策略的算法的性能要困难得多。在本文中，我们建立了通过经验比例满足的大偏差原理(LDP)和通过经验臂奖励满足的LDP之间的连接。这种连接适用于任何自适应算法，并被利用来( i ) 提高某些现有算法的错误概率上界，例如著名的\sr (Successive Re

    arXiv:2312.12137v2 Announce Type: replace  Abstract: We consider the problem of identifying the best arm in stochastic Multi-Armed Bandits (MABs) using a fixed sampling budget. Characterizing the minimal instance-specific error probability for this problem constitutes one of the important remaining open problems in MABs. When arms are selected using a static sampling strategy, the error probability decays exponentially with the number of samples at a rate that can be explicitly derived via Large Deviation techniques. Analyzing the performance of algorithms with adaptive sampling strategies is however much more challenging. In this paper, we establish a connection between the Large Deviation Principle (LDP) satisfied by the empirical proportions of arm draws and that satisfied by the empirical arm rewards. This connection holds for any adaptive algorithm, and is leveraged (i) to improve error probability upper bounds of some existing algorithms, such as the celebrated \sr (Successive Re
    
[^184]: 动态检索增强生成

    Dynamic Retrieval-Augmented Generation

    [https://arxiv.org/abs/2312.08976](https://arxiv.org/abs/2312.08976)

    所提出的Dynamic Retrieval-Augmented Generation (DRAG)是一种新颖的方法，通过实体增强生成，将检索到的实体的压缩嵌入注入到生成模型中，从而在代码生成任务中取得较好效果。

    

    当前最先进的大型语言模型在生成高质量文本和封装广泛世界知识方面非常有效。然而，这些模型往往会产生幻觉并缺乏局部相关事实数据。检索增强方法被引入以克服这些问题并提供更准确的响应。通常，检索到的信息被简单地附加到主请求中，限制了模型的上下文窗口大小。我们提出了一种基于实体增强生成的动态检索增强生成（DRAG）的新方法，将检索到的实体的压缩嵌入注入到生成模型中。所提出的流程是为代码生成任务而开发的，但也可以转移到一些自然语言处理领域。为了训练模型，我们收集并发布了一个新的项目级代码生成数据集。我们将其用于评估。

    arXiv:2312.08976v2 Announce Type: replace-cross  Abstract: Current state-of-the-art large language models are effective in generating high-quality text and encapsulating a broad spectrum of world knowledge. These models, however, often hallucinate and lack locally relevant factual data. Retrieval-augmented approaches were introduced to overcome these problems and provide more accurate responses. Typically, the retrieved information is simply appended to the main request, restricting the context window size of the model. We propose a novel approach for the Dynamic Retrieval-Augmented Generation (DRAG), based on the entity-augmented generation, which injects compressed embeddings of the retrieved entities into the generative model. The proposed pipeline was developed for code-generation tasks, yet can be transferred to some domains of natural language processing. To train the model, we collect and publish a new project-level code generation dataset. We use it for the evaluation along wit
    
[^185]: 耦合混淆校正：从稀疏标注的群体学习

    Coupled Confusion Correction: Learning from Crowds with Sparse Annotations

    [https://arxiv.org/abs/2312.07331](https://arxiv.org/abs/2312.07331)

    通过耦合混淆校正算法，能够减轻众包标注中的标签噪声，提高模型性能，并通过双模型相互校正学习到的混淆矩阵，进一步优化结果。

    

    随着数据集规模的增大，准确标注这些数据集变得越来越不切实际，因为在时间和经济上都变得更加昂贵。因此，众包已被广泛采用以减轻收集标签的成本，但这也不可避免地引入了标签噪声，最终降低了模型的性能。为了从众包标注中学习，对每个标注者的专业知识进行建模是一种常见但具有挑战性的范式，因为通过众包收集的标注通常是高度稀疏的。为了缓解这个问题，我们提出了耦合混淆校正（CCC），其中同时训练两个模型来校正彼此学习的混淆矩阵。通过双层优化，一个模型学习的混淆矩阵可以通过另一个模型的精炼数据进行校正。此外，我们对“标注者组”进行聚类，这些组共享相似的专业知识，从而提高他们的混淆矩阵。

    arXiv:2312.07331v3 Announce Type: replace  Abstract: As the size of the datasets getting larger, accurately annotating such datasets is becoming more impractical due to the expensiveness on both time and economy. Therefore, crowd-sourcing has been widely adopted to alleviate the cost of collecting labels, which also inevitably introduces label noise and eventually degrades the performance of the model. To learn from crowd-sourcing annotations, modeling the expertise of each annotator is a common but challenging paradigm, because the annotations collected by crowd-sourcing are usually highly-sparse. To alleviate this problem, we propose Coupled Confusion Correction (CCC), where two models are simultaneously trained to correct the confusion matrices learned by each other. Via bi-level optimization, the confusion matrices learned by one model can be corrected by the distilled data from the other. Moreover, we cluster the ``annotator groups'' who share similar expertise so that their confu
    
[^186]: 元共训练：两种视角优于一种

    Meta Co-Training: Two Views are Better than One

    [https://arxiv.org/abs/2311.18083](https://arxiv.org/abs/2311.18083)

    元共训练通过在数据上构建不同的视角，并利用未标记数据进行共同训练，提高了半监督学习的性能。

    

    在许多实际的计算机视觉场景中，未标记的数据很多，但标签却稀缺且难以获得。因此，半监督学习利用未标记的数据提升监督分类器的性能已经在最近的文献中引起了重要的关注。其中一种主要的半监督算法是共训练。在共训练中，两种不同的模型利用数据的不同独立和足够的“视角”来共同进行更好的预测。在共训练过程中，每个模型在未标记的数据点上创建伪标签，用于改进另一个模型的性能。我们展示了在常见情况下，当独立视角不可用时，我们可以使用预训练模型来廉价地构建这些视角。在构建的视角上进行共训练可以提高性能，优于我们构建的任何单个视角，并且与半监督学习中的最新方法性能相当，但具有一些不可取之处。

    In many practical computer vision scenarios unlabeled data is plentiful, but labels are scarce and difficult to obtain. As a result, semi-supervised learning which leverages unlabeled data to boost the performance of supervised classifiers have received significant attention in recent literature. One major class of semi-supervised algorithms is co-training. In co-training two different models leverage different independent and sufficient "views" of the data to jointly make better predictions. During co-training each model creates pseudo labels on unlabeled points which are used to improve the other model. We show that in the common case when independent views are not available we can construct such views inexpensively using pre-trained models. Co-training on the constructed views yields a performance improvement over any of the individual views we construct and performance comparable with recent approaches in semi-supervised learning, but has some undesirable properties. To alleviate t
    
[^187]: 通过解耦可抗议的数据生成组件来实现程序公平

    Procedural Fairness Through Decoupling Objectionable Data Generating Components

    [https://arxiv.org/abs/2311.14688](https://arxiv.org/abs/2311.14688)

    通过解耦可抗议的数据生成组件，本研究提出了一个框架来防止伪装的程序不公平，并强调了满足程序公平要求的重要性

    

    我们揭示并解决了经常被忽视但重要的问题，即伪装的程序不公平，即对数据生成过程中的中立（即不成问题的）方面的可能无意的改变，和/或对最不利利益个体的实现没有程序保证。受约翰·罗尔斯对纯程序公正的倡导启发，我们将自动决策视为社会制度的缩影，并考虑数据生成过程本身如何满足程序公平的要求。我们提出了一个框架，通过利用参考点和相关的价值实例化规则，将可抗议的数据生成组件与中立的数据生成组件解耦。我们的发现强调了防止伪装的程序不公平的必要性，不仅引起了我们力图缓解的可抗议的数据生成组件的注意

    arXiv:2311.14688v2 Announce Type: replace-cross  Abstract: We reveal and address the frequently overlooked yet important issue of disguised procedural unfairness, namely, the potentially inadvertent alterations on the behavior of neutral (i.e., not problematic) aspects of data generating process, and/or the lack of procedural assurance of the greatest benefit of the least advantaged individuals. Inspired by John Rawls's advocacy for pure procedural justice, we view automated decision-making as a microcosm of social institutions, and consider how the data generating process itself can satisfy the requirements of procedural fairness. We propose a framework that decouples the objectionable data generating components from the neutral ones by utilizing reference points and the associated value instantiation rule. Our findings highlight the necessity of preventing disguised procedural unfairness, drawing attention not only to the objectionable data generating components that we aim to mitiga
    
[^188]: 使用推前映射进行各地采样

    Touring sampling with pushforward maps

    [https://arxiv.org/abs/2311.13845](https://arxiv.org/abs/2311.13845)

    该论文从理论角度对生成建模中的多种采样方法进行了审视和组织，帮助克服采样中的一些挑战，比如推理时间长和生成样本缺乏多样性。

    

    对于一个希望将强大的机器学习方法应用于特定问题的从业者来说，采样方法的数量可能令人生畏。本文从理论角度出发，对在“生成建模”设置中许多采样方法进行了审视和组织，其中希望生成与一些训练样本类似的新数据。通过揭示现有方法之间的联系，可能有助于克服与扩散模型采样相关的一些当前挑战，比如由于扩散模拟而导致的长推理时间，或者生成样本缺乏多样性。

    arXiv:2311.13845v2 Announce Type: replace-cross  Abstract: The number of sampling methods could be daunting for a practitioner looking to cast powerful machine learning methods to their specific problem. This paper takes a theoretical stance to review and organize many sampling approaches in the ``generative modeling'' setting, where one wants to generate new data that are similar to some training examples. By revealing links between existing methods, it might prove useful to overcome some of the current challenges in sampling with diffusion models, such as long inference time due to diffusion simulation, or the lack of diversity in generated samples.
    
[^189]: Prompt Engineering a Prompt Engineer

    Prompt Engineering a Prompt Engineer

    [https://arxiv.org/abs/2311.05661](https://arxiv.org/abs/2311.05661)

    提示工程任务对于优化大型语言模型在定制任务上的表现至关重要，PE2方法通过详细描述、上下文规范和逐步推理模板的注入，在各种语言任务中展现出出色的适用性和效果。

    

    提示工程是优化大型语言模型在定制任务上表现的一项具有挑战性但至关重要的任务。为了检查模型的错误，假设当前提示中缺少或误导了什么，并清晰地传达任务，需要复杂的推理。尽管最近的研究表明，大型语言模型可以被元提示来执行自动提示工程，但我们认为由于元提示中缺乏复杂推理的充分指导，它们的潜力受到限制。我们通过将详细描述、上下文规范和逐步推理模板注入到元提示中来填补这一空白。所得到的方法称为PE2，展示了在不同语言任务中出色的适用性。它找到的提示在MultiArith上比“按步骤思考”高出6.3%，在GSM8K上高出3.1%，并在对立任务上优于竞争基线

    arXiv:2311.05661v2 Announce Type: replace-cross  Abstract: Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, showcases remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks 
    
[^190]: 使用强化学习的语言代理在狼人杀游戏中进行战略对战

    Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game

    [https://arxiv.org/abs/2310.18940](https://arxiv.org/abs/2310.18940)

    使用强化学习为基础，提出了一种框架，用于在狼人杀游戏中开发具有灵活语言行为和强大决策能力的战略语言代理

    

    基于大型语言模型（LLMs）构建的代理在各领域展现出巨大潜力。然而，在复杂决策任务中，纯LLM代理往往表现出固有偏见，这些偏见来源于模型的训练数据，导致性能不佳。为了开发具有灵活语言行为和强大决策能力的战略语言代理，我们提出了一种新颖的框架，通过强化学习（RL）提升LLM代理的能力。我们选择狼人杀作为具有多样沟通和战略游戏玩法的挑战测试平台。为了减轻语言行为中的固有偏见，我们的代理使用LLM进行演绎推理并生成多样行为候选集。然后，经过训练以优化决策能力的RL策略从候选集中选择行为。

    arXiv:2310.18940v3 Announce Type: replace  Abstract: Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop strategic language agents, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidat
    
[^191]: 图神经网络是如何学习的：来自训练动态的启示

    How Graph Neural Networks Learn: Lessons from Training Dynamics

    [https://arxiv.org/abs/2310.05105](https://arxiv.org/abs/2310.05105)

    图神经网络的优化过程中涉及核-图对齐现象，从优化角度解释了学到的函数何时和为何泛化，有助于理解其在异源图上的限制。

    

    在深度学习中，一个长期以来的目标是以更易解释的方式表征黑盒模型的学习行为。对于图神经网络（GNNs），在正式化它们可以表示的函数方面已经取得了相当大的进展，但在优化过程中GNNs是否会学习到期望的函数仍不太清楚。为了填补这一空白，我们研究了它们在函数空间中的训练动态。特别是，我们发现通过梯度下降优化GNNs隐式利用图结构来更新学到的函数。这种现象被称为核-图对齐，已经经验性和理论上得到了验证。这种来自优化角度的新分析框架能够解释了何时以及为什么学习到的GNN函数泛化，这对于它们在异源图上的限制具有相关性。从实用的角度看，它也提供了对于GNNs如何学习函数的洞察。

    arXiv:2310.05105v2 Announce Type: replace  Abstract: A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, but whether GNNs will learn desired functions during the optimization process remains less clear. To fill this gap, we study their training dynamics in function space. In particular, we find that the optimization of GNNs through gradient descent implicitly leverages the graph structure to update the learned function. This phenomenon is dubbed as kernel-graph alignment, which has been empirically and theoretically corroborated. This new analytical framework from the optimization perspective enables interpretable explanations of when and why the learned GNN functions generalize, which are relevant to their limitations on heterophilic graphs. From a practical standpoint, it also prov
    
[^192]: MiCRO：用于扩展和加速分布式DNN训练的近零成本梯度稀疏化技术

    MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and Accelerating Distributed DNN Training

    [https://arxiv.org/abs/2310.00967](https://arxiv.org/abs/2310.00967)

    MiCRO是一种新颖的梯度稀疏化方法，通过对梯度向量进行分区，并由每个工作者选择其分区中的梯度，从而减少了通信流量并避免了梯度堆积。

    

    梯度稀疏化是一种用于扩展和加速分布式深度神经网络（DNN）训练的通信优化技术，它减少了梯度聚合的通信流量增加。然而，现有的稀疏化方法由于梯度选择的高计算成本和/或通信流量增加而具有较差的可扩展性。为了解决这些挑战，我们提出了一种名为MiCRO的新型梯度稀疏化方法。

    arXiv:2310.00967v3 Announce Type: replace  Abstract: Gradient sparsification is a communication optimisation technique for scaling and accelerating distributed deep neural network (DNN) training. It reduces the increasing communication traffic for gradient aggregation. However, existing sparsifiers have poor scalability because of the high computational cost of gradient selection and/or increase in communication traffic. In particular, an increase in communication traffic is caused by gradient build-up and inappropriate threshold for gradient selection.   To address these challenges, we propose a novel gradient sparsification method called MiCRO. In MiCRO, the gradient vector is partitioned, and each partition is assigned to the corresponding worker. Each worker then selects gradients from its partition, and the aggregated gradients are free from gradient build-up. Moreover, MiCRO estimates the accurate threshold to maintain the communication traffic as per user requirement by minimisi
    
[^193]: 通过扩散和基于梯度提升树的流生成和填充表格数据

    Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees

    [https://arxiv.org/abs/2309.09968](https://arxiv.org/abs/2309.09968)

    通过XGBoost技术，本研究提出了一种用于生成和填充混合类型表格数据的新方法，在数据生成任务中优于深度学习方法并在数据填充方面表现竞争力，且可以在CPU上并行训练。

    

    表格数据很难获取，且容易存在缺失值。本文引入了一种新颖的方法，利用基于分数的扩散和条件流匹配来生成和填补混合类型（连续和分类）表格数据。与依赖神经网络学习分数函数或向量场的先前方法不同，我们采用了XGBoost，一种广泛使用的梯度提升树（GBT）技术。为了测试我们的方法，我们构建了一个包含27个不同数据集和9个度量标准的最广泛的表格数据生成和填充基准。通过对基准的实证评估，我们证明了我们的方法在数据生成任务中优于深度学习生成方法，并在数据填充方面保持竞争力。值得注意的是，它可以在CPU上并行训练，无需GPU。我们的Python和R代码可以在https://github.com/SamsungSAILMontreal/ForestDiffusio找到。

    arXiv:2309.09968v3 Announce Type: replace  Abstract: Tabular data is hard to acquire and is subject to missing values. This paper introduces a novel approach for generating and imputing mixed-type (continuous and categorical) tabular data utilizing score-based diffusion and conditional flow matching. In contrast to prior methods that rely on neural networks to learn the score function or the vector field, we adopt XGBoost, a widely used Gradient-Boosted Tree (GBT) technique. To test our method, we build one of the most extensive benchmarks for tabular data generation and imputation, containing 27 diverse datasets and 9 metrics. Through empirical evaluation across the benchmark, we demonstrate that our approach outperforms deep-learning generation methods in data generation tasks and remains competitive in data imputation. Notably, it can be trained in parallel using CPUs without requiring a GPU. Our Python and R code is available at https://github.com/SamsungSAILMontreal/ForestDiffusio
    
[^194]: 大规模凸组合优化的自共轭平滑方法

    Self-concordant Smoothing for Large-Scale Convex Composite Optimization

    [https://arxiv.org/abs/2309.01781](https://arxiv.org/abs/2309.01781)

    提出了适用于大规模凸组合优化问题的自共轭平滑方法，通过变量度量和步长规则优化了近端牛顿算法，有效处理了非光滑函数的结构，提出了Prox-N-SCORE和Prox-GGN-SCORE算法，后者通过重要近似程序显著减少了逆Hessian计算开销。

    

    我们引入了一种自共轭平滑的概念，用于最小化两个凸函数的和，其中一个是光滑的，另一个可能是非光滑的。我们方法的关键亮点在于所得问题结构的自然特性，为我们提供了一种变量度量选择方法和一个特别适用于近端牛顿类型算法的步长选择规则。此外，我们高效处理了非光滑函数推动的具体结构，如$\ell_1$正则化和分组Lasso惩罚。我们证明了两个算法的收敛性：Prox-N-SCORE，一种近端牛顿算法，和Prox-GGN-SCORE，一种近端广义高斯-牛顿算法。Prox-GGN-SCORE算法突出了一种重要的近似程序，有助于显著减少逆Hessian相关的大部分计算开销。这种近似在...

    arXiv:2309.01781v2 Announce Type: replace-cross  Abstract: We introduce a notion of self-concordant smoothing for minimizing the sum of two convex functions, one of which is smooth and the other may be nonsmooth. The key highlight of our approach is in a natural property of the resulting problem's structure which provides us with a variable-metric selection method and a step-length selection rule particularly suitable for proximal Newton-type algorithms. In addition, we efficiently handle specific structures promoted by the nonsmooth function, such as $\ell_1$-regularization and group-lasso penalties. We prove the convergence of two resulting algorithms: Prox-N-SCORE, a proximal Newton algorithm and Prox-GGN-SCORE, a proximal generalized Gauss-Newton algorithm. The Prox-GGN-SCORE algorithm highlights an important approximation procedure which helps to significantly reduce most of the computational overhead associated with the inverse Hessian. This approximation is essentially useful fo
    
[^195]: 基于平滑度诱导正则化和基于频谱图的数据增强的水下声学目标识别

    Underwater Acoustic Target Recognition based on Smoothness-inducing Regularization and Spectrogram-based Data Augmentation

    [https://arxiv.org/abs/2306.06945](https://arxiv.org/abs/2306.06945)

    通过平滑度诱导正则化和基于频谱图的数据增强，提高水下声学目标识别模型的泛化能力和避免性能降级的风险

    

    水下声学目标识别是一项具有挑战性的任务，这归因于复杂的水下环境和有限的数据可用性。数据不足可能会阻碍识别系统支持复杂建模的能力，从而阻碍其发展。为了提高识别模型的泛化能力，已经采用了诸如数据增强之类的技术来模拟水下信号并丰富数据分布。然而，水下环境的复杂性可能导致模拟信号偏离真实场景，导致受到非真实数据误导的偏见模型。在本研究中，我们提出了两种策略，可以在数据有限的情况下增强模型的泛化能力，同时避免性能降级的风险。首先，作为替代传统数据增强的方法，我们利用平滑度诱导正则化，该方法仅纳入模拟信号

    arXiv:2306.06945v2 Announce Type: replace-cross  Abstract: Underwater acoustic target recognition is a challenging task owing to the intricate underwater environments and limited data availability. Insufficient data can hinder the ability of recognition systems to support complex modeling, thus impeding their advancement. To improve the generalization capacity of recognition models, techniques such as data augmentation have been employed to simulate underwater signals and diversify data distribution. However, the complexity of underwater environments can cause the simulated signals to deviate from real scenarios, resulting in biased models that are misguided by non-true data. In this study, we propose two strategies to enhance the generalization ability of models in the case of limited data while avoiding the risk of performance degradation. First, as an alternative to traditional data augmentation, we utilize smoothness-inducing regularization, which only incorporates simulated signal
    
[^196]: 通过线性上下文和组合动作激励探索

    Incentivizing Exploration with Linear Contexts and Combinatorial Actions

    [https://arxiv.org/abs/2306.01990](https://arxiv.org/abs/2306.01990)

    研究在激励式赌博机探索中通过线性赌博机模型替代先验独立性条件，提高了高维动作空间下的激励探索效率和最优遗憾，同时改进了半赌博模型中关于初始数据收集的样本复杂度。

    

    我们推进了激励式赌博机探索的研究，其中手臂选择被视为推荐，并且要求是贝叶斯激励兼容的。最近的工作表明，在满足一定独立性假设后，经过足够的初始样本收集，流行的汤普森抽样算法变得激励兼容。我们为线性赌博机提供了这个结果的类比，其中先验的独立性被自然的凸性条件取代。这打开了在高维动作空间中高效和遗憾最优的激励探索的可能性。在半赌博模型中，我们还改进了用于初始数据收集的前汤普森抽样阶段的样本复杂度。

    arXiv:2306.01990v2 Announce Type: replace-cross  Abstract: We advance the study of incentivized bandit exploration, in which arm choices are viewed as recommendations and are required to be Bayesian incentive compatible. Recent work has shown under certain independence assumptions that after collecting enough initial samples, the popular Thompson sampling algorithm becomes incentive compatible. We give an analog of this result for linear bandits, where the independence of the prior is replaced by a natural convexity condition. This opens up the possibility of efficient and regret-optimal incentivized exploration in high-dimensional action spaces. In the semibandit model, we also improve the sample complexity for the pre-Thompson sampling phase of initial data collection.
    
[^197]: 在存在恶意客户不可用的情况下的联邦学习

    Federated Learning in the Presence of Adversarial Client Unavailability

    [https://arxiv.org/abs/2305.19971](https://arxiv.org/abs/2305.19971)

    放宽了结构性假设，研究了恶意客户端不可用的情况，引入了$\epsilon$-对手丢包分数的概念。

    

    联邦学习是一种分散式机器学习框架，可以在不泄露原始数据的情况下进行协作模型训练。由于各种硬件和软件限制，客户端可能并不总是可以响应参数服务器的计算请求。新兴的研究方向致力于解决任意客户端不可用问题。然而，现有工作仍对不可用模式施加结构性假设，限制了它们在不受参数服务器控制的具有挑战性情景中的适用性。此外，在像战场这样恶劣的环境中，对手可以选择性地和自适应地使特定客户端沉默。在本文中，我们放宽了结构性假设，并考虑了恶意客户端不可用性。为了量化客户端不可用的程度，我们使用了$\epsilon$-对手丢包分数的概念。

    arXiv:2305.19971v2 Announce Type: replace  Abstract: Federated learning is a decentralized machine learning framework that enables collaborative model training without revealing raw data. Due to the diverse hardware and software limitations, a client may not always be available for the computation requests from the parameter server. An emerging line of research is devoted to tackling arbitrary client unavailability. However, existing work still imposes structural assumptions on the unavailability patterns, impeding their applicability in challenging scenarios wherein the unavailability patterns are beyond the control of the parameter server. Moreover, in harsh environments like battlefields, adversaries can selectively and adaptively silence specific clients. In this paper, we relax the structural assumptions and consider adversarial client unavailability. To quantify the degrees of client unavailability, we use the notion of $\epsilon$-adversary dropout fraction. We show that simple v
    
[^198]: 学习不后悔

    Learning not to Regret

    [https://arxiv.org/abs/2303.01074](https://arxiv.org/abs/2303.01074)

    提出了一个“学习不后悔”的框架，可加速在类似但不相同游戏分布上的均衡寻找，同时在任何游戏上具有减小后悔的保证。

    

    博弈论均衡研究文献主要集中在单个游戏或其重复游戏上。然而，许多现实世界的场景涉及玩一个取自类似但不相同游戏分布的游戏，比如用不同公共牌玩扑克或在股票市场上交易相关资产。由于这些相似游戏具有相似的均衡，我们研究了在这种分布上加速寻找均衡的方法。我们提出了一个新颖的“学会不后悔”的框架，使我们能够元学习针对特定分布量身定制的减小后悔的算法。我们的主要贡献，即神经预测后悔匹配，是独特地元学习，能够在所选择的游戏分布上快速收敛，同时在任何游戏上具有减小后悔的保证。我们验证了算法在一组公共牌扑克游戏分布上更快地收敛。我们的实验表明，元学习的...

    arXiv:2303.01074v2 Announce Type: replace-cross  Abstract: The literature on game-theoretic equilibrium finding predominantly focuses on single games or their repeated play. Nevertheless, numerous real-world scenarios feature playing a game sampled from a distribution of similar, but not identical games, such as playing poker with different public cards or trading correlated assets on the stock market. As these similar games feature similar equilibra, we investigate a way to accelerate equilibrium finding on such a distribution. We present a novel "learning not to regret" framework, enabling us to meta-learn a regret minimizer tailored to a specific distribution. Our key contribution, Neural Predictive Regret Matching, is uniquely meta-learned to converge rapidly for the chosen distribution of games, while having regret minimization guarantees on any game. We validated our algorithms' faster convergence on a distribution of river poker games. Our experiments show that the meta-learned 
    
[^199]: 模型拼接与可视化：如何利用GAN生成器实时反转网络

    Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time

    [https://arxiv.org/abs/2302.02181](https://arxiv.org/abs/2302.02181)

    提出了一种利用GAN生成器拼接网络并实时反转的方法，在图像分类和语义分割网络中表现出与梯度下降方法相当的性能，但处理时间快两个数量级。

    

    在这项工作中，我们提出了一种快速准确的方法，通过将分类和语义分割网络与利用1x1卷积的GAN生成器拼接起来重建激活。我们在AFHQ野生数据集、ImageNet1K的动物图像以及染色组织样本的真实数字病理扫描图像上测试了我们的方法。我们的结果表明，与已建立的梯度下降方法相比，我们的方法性能相当，但处理时间快两个数量级，使得这种方法在实际应用中具有前景。

    arXiv:2302.02181v2 Announce Type: replace-cross  Abstract: In this work, we propose a fast and accurate method to reconstruct activations of classification and semantic segmentation networks by stitching them with a GAN generator utilizing a 1x1 convolution. We test our approach on images of animals from the AFHQ wild dataset, ImageNet1K, and real-world digital pathology scans of stained tissue samples. Our results show comparable performance to established gradient descent methods but with a processing time that is two orders of magnitude faster, making this approach promising for practical applications.
    
[^200]: 自回归赌博机

    Autoregressive Bandits

    [https://arxiv.org/abs/2212.06251](https://arxiv.org/abs/2212.06251)

    在自回归过程控制的奖励下，提出了自回归赌博机（ARBs）在线学习设置，并设计了AutoRegressive Upper Confidence Bound (AR-UCB)算法，可以方便计算最优策略并具有次线性遗憾。

    

    自回归过程在股票市场、销售预测、天气预测、广告和定价等各种实际场景中自然而然地出现。在面对这样的序贯决策问题时，应该正确考虑连续观测之间的时间依赖性，以保证收敛到最优策略。在这项工作中，我们提出了一种新颖的在线学习设置，即自回归赌博机（ARBs），其中观测到的奖励由一个阶数为$k$的自回归过程控制，其参数取决于选择的动作。我们证明，在对奖励过程进行温和假设的情况下，最优策略可以方便地计算出来。然后，我们设计了一种新的乐观遗憾最小化算法，即自回归上置信界（AR-UCB），其遗憾呈现出次线性的阶数$\widetilde{\mathcal{O}} \left( \frac{(k+1)^{3/2}\sqrt{nT}}{(1-\Gamma)^

    arXiv:2212.06251v2 Announce Type: replace  Abstract: Autoregressive processes naturally arise in a large variety of real-world scenarios, including stock markets, sales forecasting, weather prediction, advertising, and pricing. When facing a sequential decision-making problem in such a context, the temporal dependence between consecutive observations should be properly accounted for guaranteeing convergence to the optimal policy. In this work, we propose a novel online learning setting, namely, Autoregressive Bandits (ARBs), in which the observed reward is governed by an autoregressive process of order $k$, whose parameters depend on the chosen action. We show that, under mild assumptions on the reward process, the optimal policy can be conveniently computed. Then, we devise a new optimistic regret minimization algorithm, namely, AutoRegressive Upper Confidence Bound (AR-UCB), that suffers sublinear regret of order $\widetilde{\mathcal{O}} \left( \frac{(k+1)^{3/2}\sqrt{nT}}{(1-\Gamma)^
    
[^201]: 图滤波器在图信号处理和机器学习中的应用

    Graph Filters for Signal Processing and Machine Learning on Graphs

    [https://arxiv.org/abs/2211.08854](https://arxiv.org/abs/2211.08854)

    图滤波器是为处理和学习网络和其他不规则域数据而设计的，可以增强表示能力以模拟更广泛的信号类、数据模式和关系。

    

    滤波器在从数据中提取信息中起着基础作用。对于存在于欧氏域上的时间序列和图像数据，滤波器是许多信号处理和机器学习技术的关键，包括卷积神经网络。越来越多的现代数据存在于网络和其他不规则域，其结构更适合用图表示。为了处理和学习这些数据，图滤波器考虑了底层数据域的结构。在本文中，我们全面介绍了图滤波器，包括不同的滤波器类别，每种类型的设计策略，以及不同类型图滤波器之间的权衡。我们讨论如何将图滤波器扩展到滤波器组和图神经网络，以增强表示能力；也就是说，模拟更广泛的信号类、数据模式和关系。我们还展示了图滤波器在提高表征能力方面的基本作用

    arXiv:2211.08854v2 Announce Type: replace-cross  Abstract: Filters are fundamental in extracting information from data. For time series and image data that reside on Euclidean domains, filters are the crux of many signal processing and machine learning techniques, including convolutional neural networks. Increasingly, modern data also reside on networks and other irregular domains whose structure is better captured by a graph. To process and learn from such data, graph filters account for the structure of the underlying data domain. In this article, we provide a comprehensive overview of graph filters, including the different filtering categories, design strategies for each type, and trade-offs between different types of graph filters. We discuss how to extend graph filters into filter banks and graph neural networks to enhance the representational power; that is, to model a broader variety of signal classes, data patterns, and relationships. We also showcase the fundamental role of gr
    
[^202]: 量子视觉变压器

    Quantum Vision Transformers

    [https://arxiv.org/abs/2209.08167](https://arxiv.org/abs/2209.08167)

    本研究设计和分析了量子变压器，引入了三种类型的量子变压器用于训练和推理，在保证量子注意机制具有理论优势的基础上，采用浅量子电路构建，产生不同的分类模型。

    

    本文通过扩展已知在自然语言处理和图像分析中非常高效的最新经典变压器神经网络架构，设计并详细分析了量子变压器。在之前使用参数化量子电路进行数据加载和正交神经层的基础上，我们介绍了三种类型的量子变压器用于训练和推理，包括基于复合矩阵的量子变压器，它确保了量子注意机制在渐近运行时间和模型参数数量方面相较于它们的经典对应物存在理论优势。这些量子架构可以使用浅量子电路构建，并产生定性不同的分类模型。

    arXiv:2209.08167v2 Announce Type: replace-cross  Abstract: In this work, quantum transformers are designed and analysed in detail by extending the state-of-the-art classical transformer neural network architectures known to be very performant in natural language processing and image analysis. Building upon the previous work, which uses parametrised quantum circuits for data loading and orthogonal neural layers, we introduce three types of quantum transformers for training and inference, including a quantum transformer based on compound matrices, which guarantees a theoretical advantage of the quantum attention mechanism compared to their classical counterpart both in terms of asymptotic run time and the number of model parameters. These quantum architectures can be built using shallow quantum circuits and produce qualitatively different classification models. The three proposed quantum attention layers vary on the spectrum between closely following the classical transformers and exhibi
    
[^203]: 在线社交媒体审计的数学框架

    Mathematical Framework for Online Social Media Auditing

    [https://arxiv.org/abs/2209.05550](https://arxiv.org/abs/2209.05550)

    研究提出了一个数学框架，探讨了社交媒体平台上算法过滤的不良影响以及监管复杂性。

    

    社交媒体平台（SMPs）利用算法过滤（AF）来选择构成用户信息流的内容，旨在最大化奖励。有选择地选择要显示在用户信息流中的内容可能会在一定程度上，无论是较小还是较大，对用户的决策产生影响，与在自然/公平内容选择下会产生的影响相比。正如我们在过去的十年中所见，算法过滤可能导致有害的副作用，从偏见个人决定到塑造整个社会的决定，例如，将用户注意力从是否接种COVID-19疫苗转移或诱使公众选择总统候选人。政府常常试图监管AF的不良影响，但往往因官僚主义、法律事务和财务考虑而变得复杂。另一方面，SMPs寻求监测他们自己的算

    arXiv:2209.05550v2 Announce Type: replace  Abstract: Social media platforms (SMPs) leverage algorithmic filtering (AF) as a means of selecting the content that constitutes a user's feed with the aim of maximizing their rewards. Selectively choosing the contents to be shown on the user's feed may yield a certain extent of influence, either minor or major, on the user's decision-making, compared to what it would have been under a natural/fair content selection. As we have witnessed over the past decade, algorithmic filtering can cause detrimental side effects, ranging from biasing individual decisions to shaping those of society as a whole, for example, diverting users' attention from whether to get the COVID-19 vaccine or inducing the public to choose a presidential candidate. The government's constant attempts to regulate the adverse effects of AF are often complicated, due to bureaucracy, legal affairs, and financial considerations. On the other hand SMPs seek to monitor their own alg
    
[^204]: 通过可扩展的变分高斯过程在分子数据上学习引导点和不确定性

    Learning inducing points and uncertainty on molecular data by scalable variational Gaussian processes

    [https://arxiv.org/abs/2207.07654](https://arxiv.org/abs/2207.07654)

    通过引入潜在的引导点变量并选择合适的边缘对数似然目标，可在分子描述符空间中改进预测性能。

    

    不确定性控制和可扩展性是在材料科学和化学中部署高斯过程（GP）模型到自主机器学习预测管道中的两个主要问题。引入潜在的引导点变量并选择合适的近似以获得边缘对数似然目标是解决这两个问题的一种方法。本文在分子描述符空间中经验性地展示了通过变分学习引导点可以提高两个分子动力学数据集上能量和原子力的预测性能。首先，我们表明变分高斯过程可以学习表示初始化集合中不存在的不同类型分子的构型。我们提供了不同训练目标和变分分布的比较。在评估了几种近似边缘对数似然的训练目标和变分分布后

    arXiv:2207.07654v3 Announce Type: replace-cross  Abstract: Uncertainty control and scalability to large datasets are the two main issues for the deployment of Gaussian process (GP) models within the autonomous machine learning-based prediction pipelines in material science and chemistry. One way to address both of these issues is by introducing the latent inducing point variables and choosing the right approximation for the marginal log-likelihood objective. Here, we empirically show that variational learning of the inducing points in a molecular descriptor space improves the prediction of energies and atomic forces on two molecular dynamics datasets. First, we show that variational GPs can learn to represent the configurations of the molecules of different types that were not present within the initialization set of configurations. We provide a comparison of alternative log-likelihood training objectives and variational distributions. Among several evaluated approximate marginal log-l
    
[^205]: 图上的逆边界值和最优控制问题：神经和数值综合

    Inverse Boundary Value and Optimal Control Problems on Graphs: A Neural and Numerical Synthesis

    [https://arxiv.org/abs/2206.02911](https://arxiv.org/abs/2206.02911)

    引入了图上确定性系统识别问题的通用设置，提出了一种边界注入消息传递神经网络提高预测准确性的方法，并引入了一种基于图距离的正则化技术来稳定远离边界的节点的预测。

    

    引入了一个关于图上带有Dirichlet和Neumann边界条件的确定性系统识别问题的通用设置。当边界上有控制节点时，我们应用了一种离散化然后优化的方法来估计最优控制。当前体系结构中的一个关键部分是我们的边界注入消息传递神经网络。这将产生更准确的预测，在边界附近更加稳定。此外，引入了一种基于图距离的正则化技术，有助于稳定远离边界的节点处的预测。

    arXiv:2206.02911v2 Announce Type: replace  Abstract: A general setup for deterministic system identification problems on graphs with Dirichlet and Neumann boundary conditions is introduced. When control nodes are available along the boundary, we apply a discretize-then-optimize method to estimate an optimal control. A key piece in the present architecture is our boundary injected message passing neural network. This will produce more accurate predictions that are considerably more stable in proximity of the boundary. Also, a regularization technique based on graphical distance is introduced that helps with stabilizing the predictions at nodes far from the boundary.
    
[^206]: 在均场博弈中的学习：一项调查

    Learning in Mean Field Games: A Survey

    [https://arxiv.org/abs/2205.12944](https://arxiv.org/abs/2205.12944)

    强化学习和均场博弈的结合有望在很大规模上解决游戏的均衡和社会最优问题。

    

    非合作和合作游戏在拥有大量玩家时有许多应用，但随着玩家数量的增加，通常变得难以解决。均场博弈(Mean Field Games, MFGs)由Lasry和Lions以及Huang，Caines和Malham\'e引入，依靠均场近似允许玩家数量增长到无穷大。传统解决这些游戏的方法通常依赖于解决带有对模型的完全了解的偏微分方程或随机微分方程。最近，强化学习(Reinforcement Learning, RL)出现在解决规模复杂问题上表现出了很大的潜力。RL和MFGs的结合有望解决在人口规模和环境复杂性方面非常庞大的游戏。在这项调查中，我们回顾了最近迅速增长的关于RL方法在MFGs中学习均衡和社交最优的文献。我们首先确定了M中最常见的设置(静态、稳态和进化的)。

    arXiv:2205.12944v3 Announce Type: replace-cross  Abstract: Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malham\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of M
    
[^207]: 用于人与机器人间接放置交接的主动运动规划

    Preemptive Motion Planning for Human-to-Robot Indirect Placement Handovers

    [https://arxiv.org/abs/2203.00156](https://arxiv.org/abs/2203.00156)

    提出了一种新颖的预测规划流程，允许机器人主动朝着人类代理的预定放置位置移动

    

    随着技术的进步，安全、高效和协作的人机团队的需求变得越来越重要。在任何环境中最基本的协作任务之一是物体交接。人与机器人之间的交接可以采用两种方式：（1）直接手对手或（2）间接手对放置再抓取。后一种方式确保了人与机器人之间的最小接触，但也可能导致由于需要等待物体首先放置在表面上而增加闲置时间。为了最小化这种闲置时间，机器人必须预测人类意图，即物体将被放置在何处。此外，为了使机器人能够预先以任何一种有生产力的方式行动，预测和运动规划必须实时发生。我们引入了一种新颖的预测规划流程，允许机器人主动朝着人类代理的预定放置位置移动。

    arXiv:2203.00156v3 Announce Type: replace-cross  Abstract: As technology advances, the need for safe, efficient, and collaborative human-robot-teams has become increasingly important. One of the most fundamental collaborative tasks in any setting is the object handover. Human-to-robot handovers can take either of two approaches: (1) direct hand-to-hand or (2) indirect hand-to-placement-to-pick-up. The latter approach ensures minimal contact between the human and robot but can also result in increased idle time due to having to wait for the object to first be placed down on a surface. To minimize such idle time, the robot must preemptively predict the human intent of where the object will be placed. Furthermore, for the robot to preemptively act in any sort of productive manner, predictions and motion planning must occur in real-time. We introduce a novel prediction-planning pipeline that allows the robot to preemptively move towards the human agent's intended placement location using g
    
[^208]: 简单复合卷积核

    Simplicial Convolutional Filters

    [https://arxiv.org/abs/2201.11720](https://arxiv.org/abs/2201.11720)

    该论文研究了线性滤波器，提出了简单复合卷积核，定义为较低和较高霍奇拉普拉斯矩阵多项式，具有线性、平移不变、置换和方向等变性，以低计算复杂度在分布式方式实现。

    

    我们研究了用于处理支持在抽象拓扑空间上的信号的线性滤波器，这些拓扑空间被建模为单纯复形，可以被解释为图的推广，考虑到节点、边、三角形面等。为了处理这样的信号，我们开发了定义为较低和较高霍奇拉普拉斯矩阵多项式的单纯复合卷积核。

    arXiv:2201.11720v3 Announce Type: replace-cross  Abstract: We study linear filters for processing signals supported on abstract topological spaces modeled as simplicial complexes, which may be interpreted as generalizations of graphs that account for nodes, edges, triangular faces etc. To process such signals, we develop simplicial convolutional filters defined as matrix polynomials of the lower and upper Hodge Laplacians. First, we study the properties of these filters and show that they are linear and shift-invariant, as well as permutation and orientation equivariant. These filters can also be implemented in a distributed fashion with a low computational complexity, as they involve only (multiple rounds of) simplicial shifting between upper and lower adjacent simplices. Second, focusing on edge-flows, we study the frequency responses of these filters and examine how we can use the Hodge-decomposition to delineate gradient, curl and harmonic frequencies. We discuss how these frequenc
    
[^209]: 实用且可并行化的非单调子模最大化算法与大小约束

    Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint

    [https://arxiv.org/abs/2009.01947](https://arxiv.org/abs/2009.01947)

    提出了针对大小约束非单调子模函数的优化算法，通过引入改进的子程序ThreshSeq实现更高的逼近比率和更低的复杂度

    

    我们提出了一种组合和可并行化的算法，用于针对大小约束非单调的子模函数进行最大化。我们改进了一个具有最佳适应性和近乎最佳查询复杂度的算法所达到的最佳逼近比率为$0.193 - \varepsilon$。这项工作的会议版本错误地使用了一个对非单调子模函数无效的子程序。在这个版本中，我们提出了一个固定并改进的子程序，用于添加具有高平均边际增益的集合ThreshSeq，该子程序通过$O( \log(n) )$自适应轮次返回一个高概率的解。此外，我们提供了两种近似算法。第一种近似比率为$1/6 - \varepsilon$，适应性为$O( \log(n) )$，查询复杂度为$O( n \log(k) )$；第二种近似比率为$0.193 - \varepsilon$，适应性为$O( \log^2(n) )$，查询复杂度为$O(n \log$

    arXiv:2009.01947v5 Announce Type: replace-cross  Abstract: We present combinatorial and parallelizable algorithms for maximization of a submodular function, not necessarily monotone, with respect to a size constraint. We improve the best approximation factor achieved by an algorithm that has optimal adaptivity and nearly optimal query complexity to $0.193 - \varepsilon$. The conference version of this work mistakenly employed a subroutine that does not work for non-monotone, submodular functions. In this version, we propose a fixed and improved subroutine to add a set with high average marginal gain, ThreshSeq, which returns a solution in $O( \log(n) )$ adaptive rounds with high probability. Moreover, we provide two approximation algorithms. The first has approximation ratio $1/6 - \varepsilon$, adaptivity $O( \log (n) )$, and query complexity $O( n \log (k) )$, while the second has approximation ratio $0.193 - \varepsilon$, adaptivity $O( \log^2 (n) )$, and query complexity $O(n \log 
    
[^210]: 大规模异构图上基于大型语言模型的链接预测的可扩展性研究

    Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])

    [http://arxiv.org/abs/2401.13227](http://arxiv.org/abs/2401.13227)

    本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。

    

    探索将大规模语言模型应用于图学习是一项新颖的努力。然而，大图中蕴含的大量信息给这一过程带来了重大挑战。本文侧重于链接预测任务，并介绍了LPNL（Link Prediction via Natural Language），这是一个基于大型语言模型的框架，用于大规模异构图上的可扩展链接预测。我们设计了能以自然语言表达图细节的创新提示语。我们提出了一个两阶段的采样流程，从大规模异构图中提取关键信息，并采用分而治之的策略来控制输入令牌数量在预定限制内，解决了信息过载的挑战。我们还通过自监督学习设计了一个用于链接预测的T5模型进行微调。在大型公共异构图上进行的广泛实验表明，LPNL的性能超过了各种先进的基准模型。

    Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
    
[^211]: 从部分观测中进行空间和时间连续的物理模拟

    Space and Time Continuous Physics Simulation From Partial Observations. (arXiv:2401.09198v1 [cs.LG])

    [http://arxiv.org/abs/2401.09198](http://arxiv.org/abs/2401.09198)

    本研究提出了一种新颖的方法，可以从部分观测中进行连续的空间和时间物理模拟，并解决了基于固定支持网格的传统方法的缺点。这种方法通过在稀疏观测上进行训练，利用两个相互关联的动力系统在稀疏位置和连续域上进行预测和插值求解。

    

    现代物理模拟技术依赖于数值方案和网格细化方法来解决精度和复杂性之间的权衡，但这些手工解决方案繁琐且需要高计算能力。基于大规模机器学习的数据驱动方法通过更直接和高效地集成长距离依赖来实现高适应性。在这项工作中，我们主要关注流体动力学，并解决了大部分文献中存在的问题，即计算和预测形式为常规或非规则网格的固定支持。我们提出了一种新颖的设置，可以在连续的空间和时间域中进行预测，同时在稀疏观测上进行训练。我们将这个任务形式化为双观测问题，并提出了一个解决方案，其中在稀疏位置和连续域上定义了两个相互关联的动力系统，可以从初始条件进行预测和插值求解。

    Modern techniques for physical simulations rely on numerical schemes and mesh-refinement methods to address trade-offs between precision and complexity, but these handcrafted solutions are tedious and require high computational power. Data-driven methods based on large-scale machine learning promise high adaptivity by integrating long-range dependencies more directly and efficiently. In this work, we focus on fluid dynamics and address the shortcomings of a large part of the literature, which are based on fixed support for computations and predictions in the form of regular or irregular grids. We propose a novel setup to perform predictions in a continuous spatial and temporal domain while being trained on sparse observations. We formulate the task as a double observation problem and propose a solution with two interlinked dynamical systems defined on, respectively, the sparse positions and the continuous domain, which allows to forecast and interpolate a solution from the initial cond
    
[^212]: Bellman最佳步长直线化流匹配模型

    Bellman Optimal Step-size Straightening of Flow-Matching Models. (arXiv:2312.16414v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.16414](http://arxiv.org/abs/2312.16414)

    本论文介绍了一种名为BOSS的技术，通过优化步长和生成路径，提升了低资源场景下流匹配生成模型的图像质量和资源利用效率。

    

    流匹配是一个强大的框架，用于在各种应用中生成高质量的样本，尤其是图像合成。然而，这些模型的强大计算需求，尤其在微调过程和采样过程中，给低资源场景带来了重大挑战。本文引入了Bellman最佳步长直线化（BOSS）技术来提炼流匹配生成模型：它针对的是在计算预算约束下进行少数步骤的高效图像采样。首先，该技术涉及一个动态规划算法，优化预训练网络的步长。然后，它通过优化速度网络以匹配最佳步长来改进生成路径。广泛的实验评估表明，BOSS在资源利用和图像质量方面都具有显著的优势。我们的结果显示，BOSS在图像生成任务中取得了实质性的收益。

    Flow matching is a powerful framework for generating high-quality samples in various applications, especially image synthesis. However, the intensive computational demands of these models, especially during the fine-tuning process and sampling processes, pose significant challenges for low-resource scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS) technique for distilling flow-matching generative models: it aims specifically for a few-step efficient image sampling while adhering to a computational budget constraint. First, this technique involves a dynamic programming algorithm that optimizes the step sizes of the pretrained network. Then, it refines the velocity network to match the optimal step sizes, aiming to straighten the generation paths. Extensive experimental evaluations across image generation tasks demonstrate the efficacy of BOSS in terms of both resource utilization and image quality. Our results reveal that BOSS achieves substantial gains in 
    
[^213]: 人类反馈的迭代偏好学习：在KL约束下将理论与实践联系起来的RLHF

    Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11456](http://arxiv.org/abs/2312.11456)

    该论文研究了在KL约束下的反馈强化学习的理论框架，并提出了有效的算法和实践。实证评估表明，该框架在大型语言模型的对齐实验中表现出良好的效果。

    

    本文研究了生成模型与强化学习从人类反馈中的对齐过程的理论框架。我们考虑了一个标准的数学表达式，即反向KL正则化的上下文多臂赌博机用于RLHF。尽管它被广泛应用于实际应用，但对这个公式的严格理论分析仍然很开放。我们研究了它在离线、在线和混合三种不同场景下的行为，并提出了具有有限样本理论保证的高效算法。朝着实际应用的方向，我们的框架通过对信息理论策略改进预言的稳健近似，自然地产生了几种新颖的RLHF算法。这包括在线场景中的迭代版本的直接偏好优化(DPO)算法，以及离线情景下的多步拒绝抽样策略。我们对大型语言模型的真实对齐实验进行了实证评估。

    This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate t
    
[^214]: 添加和稀疏：一种用于时间点过程的扩散方法

    Add and Thin: Diffusion for Temporal Point Processes. (arXiv:2311.01139v1 [cs.LG])

    [http://arxiv.org/abs/2311.01139](http://arxiv.org/abs/2311.01139)

    本研究提出了一种基于概率去噪扩散模型的时间点过程模型，相比于现有的方法，该模型在预测方面取得了较好的性能，对具有离散和连续成分的数据具有处理能力。

    

    在时间点过程（TPP）框架内，自回归神经网络已成为建模连续时间事件数据的标准。尽管这些模型可以以一步预测的方式精确地捕捉事件序列，但由于其顺序性质引起的误差积累，它们在长期预测应用中具有固有的局限性。为了克服这些限制，我们推导出ADD-THIN，一种面向整个事件序列工作的基于概率去噪扩散模型，它自然地处理具有离散和连续成分的数据。在合成和真实数据集的实验中，我们的模型在密度估计方面与最先进的TPP模型相匹配，并在预测方面表现出色。

    Autoregressive neural networks within the temporal point process (TPP) framework have become the standard for modeling continuous-time event data. Even though these models can expressively capture event sequences in a one-step-ahead fashion, they are inherently limited for long-term forecasting applications due to the accumulation of errors caused by their sequential nature. To overcome these limitations, we derive ADD-THIN, a principled probabilistic denoising diffusion model for TPPs that operates on entire event sequences. Unlike existing diffusion approaches, ADD-THIN naturally handles data with discrete and continuous components. In experiments on synthetic and real-world datasets, our model matches the state-of-the-art TPP models in density estimation and strongly outperforms them in forecasting.
    
[^215]: 自监督预训练用于降水后处理

    Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])

    [http://arxiv.org/abs/2310.20187](http://arxiv.org/abs/2310.20187)

    该论文提出了一种基于深度学习的降水后处理方法，使用自监督预训练和转移学习来提高数值天气预报模型的准确性。实验结果表明该方法在区域降水校正方面表现优于其他方法。

    

    为了预防危险天气事件，确保充足的局地降水预报提前时间至关重要。然而，全球变暖引起的气候变化增加了准确预测严重降水事件（如暴雨）的挑战。本工作提出了一种基于深度学习的降水后处理方法，用于数值天气预报（NWP）模型。降水后处理包括（i）自监督预训练，其中编码器的参数在大气物理领域的遮蔽变量重构上进行预训练，以及（ii）从预训练的编码器中转移学习到降水分割任务（目标领域）。我们还引入了一种启发式标记方法，以有效地训练类别不平衡的数据集。我们在区域NWP中的降水校正实验结果表明，所提出的方法优于其他方法。

    Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
    
[^216]: 可解释的基于原型的图信息瓶颈

    Interpretable Prototype-based Graph Information Bottleneck. (arXiv:2310.19906v1 [cs.LG])

    [http://arxiv.org/abs/2310.19906](http://arxiv.org/abs/2310.19906)

    这项工作提出了一种新颖的可解释的GNN框架，通过在信息瓶颈框架中将原型学习与输入图的关键子图相结合，为模型的解释能力和性能提供了改进。

    

    图神经网络（GNN）的成功导致了对其决策过程的理解和对其预测的解释的需求，这催生了可解释的人工智能（XAI），为黑盒模型提供透明的解释。最近，原型的使用成功提高了模型的可解释性，通过学习原型来暗示影响预测的训练图。然而，这些方法往往会给原型提供来自整个图的过多信息，导致关键子结构的排除或无关子结构的包含，这可以限制模型在下游任务中的解释能力和性能。在这项工作中，我们提出了一种新颖的可解释的GNN框架，称为解释性的基于原型的图信息瓶颈 (PGIB)，将原型学习纳入信息瓶颈框架，为原型提供输入图的关键子图。

    The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph
    
[^217]: 网格细胞中的循环神经网络中的共形归一化

    Conformal Normalization in Recurrent Neural Network of Grid Cells. (arXiv:2310.19192v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.19192](http://arxiv.org/abs/2310.19192)

    本文提出了一种循环神经网络中的共形归一化方法，用于处理网格细胞在2D物理空间中的自我位置信息。实验结果表明，该方法能够显著减小位置误差。

    

    哺乳动物大脑中颞叶皮层的网格细胞在2D开放环境中以惊人的六角形发射模式展示出反应图。网格细胞群体的反应在高维神经活动空间中形成一个向量，这个向量表示代理在2D物理空间中的自我位置。当代理移动时，这个向量被一个循环神经网络转换，该网络将代理的速度作为输入。本文中，我们提出了对循环神经网络输入速度进行简单而通用的共形归一化，使得高维神经空间中位置向量的局部位移与2D物理空间中代理的局部位移成比例，无论输入速度的方向如何。我们在最简单的线性和非线性循环网络上进行了数值实验，结果显示共形归一化导致数量级较小的位置误差。

    Grid cells in the entorhinal cortex of the mammalian brain exhibit striking hexagon firing patterns in their response maps as the animal (e.g., a rat) navigates in a 2D open environment. The responses of the population of grid cells collectively form a vector in a high-dimensional neural activity space, and this vector represents the self-position of the agent in the 2D physical space. As the agent moves, the vector is transformed by a recurrent neural network that takes the velocity of the agent as input. In this paper, we propose a simple and general conformal normalization of the input velocity for the recurrent neural network, so that the local displacement of the position vector in the high-dimensional neural space is proportional to the local displacement of the agent in the 2D physical space, regardless of the direction of the input velocity. Our numerical experiments on the minimally simple linear and non-linear recurrent networks show that conformal normalization leads to the 
    
[^218]: 用于因果结构学习的算法对超参数选择的稳健性

    Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice. (arXiv:2310.18212v1 [cs.LG])

    [http://arxiv.org/abs/2310.18212](http://arxiv.org/abs/2310.18212)

    这项研究调查了超参数对因果结构学习任务的影响，并通过对不同复杂级别的数据集进行实证评估，发现超参数选择同样对算法选择具有重要的影响。

    

    超参数在机器学习中起着关键作用。由于结构学习的无监督特性，超参数调整可以在任何算法中产生全球领先和糟糕的预测表现之间的差异。因此，为了使用特定算法的默认值，人们经常忽视超参数调整。虽然已经有大量关于因果发现算法性能评估的研究，但超参数如何影响单个算法以及选择最佳算法解决特定问题的问题尚未得到深入研究。本研究通过调查超参数对因果结构学习任务的影响来弥补这一空白。具体而言，我们对不同复杂级别的数据集上的一些开创性学习算法进行了超参数选择的实证评估。我们发现，尽管算法的选择仍然至关重要，但超参数的选择同样对因果结构学习任务有重要影响。

    Hyperparameters play a critical role in machine learning. Hyperparameter tuning can make the difference between state-of-the-art and poor prediction performance for any algorithm, but it is particularly challenging for structure learning due to its unsupervised nature. As a result, hyperparameter tuning is often neglected in favour of using the default values provided by a particular implementation of an algorithm. While there have been numerous studies on performance evaluation of causal discovery algorithms, how hyperparameters affect individual algorithms, as well as the choice of the best algorithm for a specific problem, has not been studied in depth before. This work addresses this gap by investigating the influence of hyperparameters on causal structure learning tasks. Specifically, we perform an empirical evaluation of hyperparameter selection for some seminal learning algorithms on datasets of varying levels of complexity. We find that, while the choice of algorithm remains cr
    
[^219]: 上下文定向无环图

    Contextual directed acyclic graphs. (arXiv:2310.15627v1 [stat.ML])

    [http://arxiv.org/abs/2310.15627](http://arxiv.org/abs/2310.15627)

    本论文研究了上下文定向无环图的问题，通过神经网络将上下文特征映射到DAG，利用稀疏的加权邻接矩阵表示图结构，并通过新颖的投影层满足无环性的特点。实验证明该方法能够成功恢复出真实的上下文特定图。

    

    从观测数据中估计定向无环图（DAG）的结构仍然是机器学习中的一个重大挑战。这个领域的大部分研究集中在为整个人口学习单个DAG上。本文考虑了一个替代性的设置，其中图结构基于可用的“上下文”特征而因人而异。我们通过一个将上下文特征映射到DAG的神经网络来解决这个上下文DAG问题，DAG以加权邻接矩阵表示。神经网络配备了一个新颖的投影层，确保输出矩阵是稀疏的，并满足最近发展的无环性的特点。我们设计了一个可扩展的计算框架来学习上下文DAG，并提供了收敛保证和通过投影层反向传播的分析梯度。我们的实验证明，这种新方法可以恢复出真实的上下文特定图，而现有方法则失败。

    Estimating the structure of directed acyclic graphs (DAGs) from observational data remains a significant challenge in machine learning. Most research in this area concentrates on learning a single DAG for the entire population. This paper considers an alternative setting where the graph structure varies across individuals based on available "contextual" features. We tackle this contextual DAG problem via a neural network that maps the contextual features to a DAG, represented as a weighted adjacency matrix. The neural network is equipped with a novel projection layer that ensures the output matrices are sparse and satisfy a recently developed characterization of acyclicity. We devise a scalable computational framework for learning contextual DAGs and provide a convergence guarantee and an analytical gradient for backpropagating through the projection layer. Our experiments suggest that the new approach can recover the true context-specific graph where existing approaches fail.
    
[^220]: 稳定的非凸-非凹训练通过线性插值

    Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])

    [http://arxiv.org/abs/2310.13459](http://arxiv.org/abs/2310.13459)

    本文通过理论分析指出，优化过程中的不稳定性通常是由损失函数的非单调性引起的，提出了一种稳定（大规模）神经网络训练的方法，即通过线性插值来利用“非扩张算子”的理论来解决。通过构建一种新的优化方案，松弛近似近端点（RAPP），发现了一族Lookahead算法，能够在协调部分单调问题中收敛，即使基本优化器采用梯度下降升级算法。

    

    本文提出了一种关于线性插值的理论分析，作为一种稳定（大规模）神经网络训练的方法。我们认为优化过程中的不稳定性通常是由损失函数的非单调性引起的，并展示了线性插值如何通过利用“非扩张算子”的理论来帮助解决这个问题。我们构建了一种新的优化方案，称为松弛近似近端点（RAPP），这是第一个明确的方法，能够实现完整范围内的协调部分单调问题的最后迭代收敛速率。该构造可扩展到约束和正则化设置。通过替换RAPP中的内部优化器，我们重新发现了Lookahead算法族，我们证明了这些算法在协调部分单调问题中的收敛性，即使基本优化器采用梯度下降升级算法。通过利用Lookahead继承性质，我们进一步扩展了Lookahead在协调部分单调问题中收敛的范围。

    This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of 
    
[^221]: 多臂赌博策略对深度循环强化学习的影响

    Impact of multi-armed bandit strategies on deep recurrent reinforcement learning. (arXiv:2310.08331v1 [stat.ML])

    [http://arxiv.org/abs/2310.08331](http://arxiv.org/abs/2310.08331)

    本研究在自动驾驶场景中使用部分可观测系统，通过部署和测试多种技术来平衡探索和利用的权衡，以预测方向盘操作。

    

    对环境的不完全了解导致智能体在不确定性下做出决策。强化学习中一个重要的困境是，在做出决策时，智能体需要在利用当前环境知识最大化累积奖励和探索行动以提高环境知识的之间进行权衡（探索-利用的平衡）。同时，另一个相关问题是状态的完全可观测性，不是所有应用都能假定。例如，当只将2D图像作为输入用于在3D模拟环境中找到最佳行动时，就存在这个问题。在本研究中，我们通过部署和测试多种技术来解决部分可观测系统中探索和利用的平衡问题，以预测自动驾驶场景中的方向盘操作。

    Incomplete knowledge of the environment leads an agent to make decisions under uncertainty. One of the major dilemmas in Reinforcement Learning (RL) where an autonomous agent has to balance two contrasting needs in making its decisions is: exploiting the current knowledge of the environment to maximize the cumulative reward as well as exploring actions that allow improving the knowledge of the environment, hopefully leading to higher reward values (exploration-exploitation trade-off). Concurrently, another relevant issue regards the full observability of the states, which may not be assumed in all applications. Such as when only 2D images are considered as input in a RL approach used for finding the optimal action within a 3D simulation environment. In this work, we address these issues by deploying and testing several techniques to balance exploration and exploitation trade-off on partially observable systems for predicting steering wheels in autonomous driving scenario. More precisel
    
[^222]: LARA：一种轻量级且抗过拟合的无监督异常检测再训练方法

    LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection. (arXiv:2310.05668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05668](http://arxiv.org/abs/2310.05668)

    LARA是一种轻量级且抗过拟合的无监督异常检测再训练方法，它将重新训练过程形式化为一个凸问题，并设计了一个反思模块以利用历史数据，同时数学证明了在微调后可以获得更好的性能。

    

    当前大部分异常检测模型都假设正常模式始终保持不变。然而，Web服务的正常模式经常发生剧烈变化。在这种变化之后，使用旧分布数据训练的模型已经过时。每次都重新训练整个模型是昂贵的。此外，在正常模式变化开始时，新分布的观察数据不足。用有限的数据对大型神经网络模型进行重新训练容易过拟合。因此，我们提出了一种轻量级且抗过拟合的再训练方法（LARA），用于基于深度变分自编码器的时间序列异常检测方法（VAEs）。本工作旨在提出三个新颖的贡献：1）将重新训练过程形式化为一个凸问题，并能够以快速收敛以及防止过拟合；2）设计了一个反思模块，可以利用历史数据而无需储存它们；3）数学证明了在微调后可以获得更好的性能。

    Most of current anomaly detection models assume that the normal pattern remains same all the time. However, the normal patterns of Web services change dramatically and frequently. The model trained on old-distribution data is outdated after such changes. Retraining the whole model every time is expensive. Besides, at the beginning of normal pattern changes, there is not enough observation data from the new distribution. Retraining a large neural network model with limited data is vulnerable to overfitting. Thus, we propose a Light and Anti-overfitting Retraining Approach (LARA) for deep variational auto-encoder based time series anomaly detection methods (VAEs). This work aims to make three novel contributions: 1) the retraining process is formulated as a convex problem and can converge at a fast rate as well as prevent overfitting; 2) designing a ruminate block, which leverages the historical data without the need to store them; 3) mathematically proving that when fine-tuning the late
    
[^223]: 适用于谷歌Colab的二维瞬态问题中的物理信息神经网络代码（PINN-2DT）

    Physics Informed Neural Network Code for 2D Transient Problems (PINN-2DT) Compatible with Google Colab. (arXiv:2310.03755v1 [cs.CE])

    [http://arxiv.org/abs/2310.03755](http://arxiv.org/abs/2310.03755)

    这个论文提出了一个适用于谷歌Colab的开源物理信息神经网络环境，可用于在二维矩形域上模拟瞬态现象，并提供了多个特性和问题库。

    

    我们提出了一个开源的物理信息神经网络环境，用于在二维矩形域上模拟瞬态现象，具有以下特点：（1）与谷歌Colab兼容，可以在云环境中进行自动执行；（2）支持二维时变PDE；（3）提供简单的界面来定义残留损失、边界条件和初始损失，以及它们的权重；（4）支持诺依曼和迪利克雷边界条件；（5）允许自定义层数和每层的神经元数量，以及任意激活函数；（6）学习率和迭代次数可以作为参数调节；（7）对空间和时间变量的PINN进行自动求导；（8）提供了绘制收敛性（具有滑动平均）、学习到的初始条件、模拟的二维和三维快照以及视频的常规函数；（9）包含了一个问题库。

    We present an open-source Physics Informed Neural Network environment for simulations of transient phenomena on two-dimensional rectangular domains, with the following features: (1) it is compatible with Google Colab which allows automatic execution on cloud environment; (2) it supports two dimensional time-dependent PDEs; (3) it provides simple interface for definition of the residual loss, boundary condition and initial loss, together with their weights; (4) it support Neumann and Dirichlet boundary conditions; (5) it allows for customizing the number of layers and neurons per layer, as well as for arbitrary activation function; (6) the learning rate and number of epochs are available as parameters; (7) it automatically differentiates PINN with respect to spatial and temporal variables; (8) it provides routines for plotting the convergence (with running average), initial conditions learnt, 2D and 3D snapshots from the simulation and movies (9) it includes a library of problems: (a) n
    
[^224]: FLAIM: 在联邦设置中基于AIM的合成数据生成

    FLAIM: AIM-based Synthetic Data Generation in the Federated Setting. (arXiv:2310.03447v1 [cs.CR])

    [http://arxiv.org/abs/2310.03447](http://arxiv.org/abs/2310.03447)

    FLAIM是一个在联邦设置中基于AIM的合成数据生成方法，该方法解决了差分隐私方向的技术在联邦场景下的适用问题，并提出了FLAIM方法来维持较高的效用和处理异构性。

    

    保护个人隐私同时实现协同数据共享对组织至关重要。合成数据生成是一种解决方案，它产生与私有数据的统计特性相似的人工数据。虽然在差分隐私下已经设计出了许多技术，但它们主要假设数据是集中的。然而，数据往往以联邦方式分布在多个客户端上。在这项工作中，我们开始研究联邦合成表数据生成。在AIM这个先进的中心方法的基础上，我们提出了DistAIM和FLAIM。我们展示了分发AIM是简单的，扩展了基于安全多方计算的最新方法，但需要额外的开销，使其在联邦场景中不太适用。然后，我们证明了简单地联邦AIM可能导致在异构性存在的情况下效用严重下降。为了解决这两个问题，我们提出了一种增强的FLAIM方法，该方法可以维持较高的效用，并且可以处理联邦设置中的异构性。

    Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We show it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that mai
    
[^225]: SemiReward: 半监督学习的通用奖励模型

    SemiReward: A General Reward Model for Semi-supervised Learning. (arXiv:2310.03013v1 [cs.LG])

    [http://arxiv.org/abs/2310.03013](http://arxiv.org/abs/2310.03013)

    SemiReward是一个通用奖励模型，通过预测奖励分数来评估和过滤高质量的伪标签，可以应用于各种半监督学习任务，并在实验中取得了显著的成果。

    

    半监督学习在自训练框架和伪标签上取得了显著进展。主要挑战是如何区分高质量的伪标签，避免确证偏见。然而，现有的伪标签选择策略限制于预定义的方案或复杂的手工制作策略，无法同时实现高质量标签、快速收敛和任务多样性。为此，我们提出了一种半监督奖励框架（SemiReward），用于预测奖励分数以评估和过滤高质量的伪标签，可以在各种任务类型和场景下与主流的半监督学习方法相结合使用。为了减少确证偏见，在两个阶段通过生成模型和子抽样策略进行在线训练。通过在三种模态的13个标准半监督学习基准上进行分类和回归任务的广泛实验验证，表明SemiReward取得了显著的成果。

    Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks of three modalities, extensive experiments verify that SemiReward achieves sig
    
[^226]: SWoTTeD:张量分解在时间表征中的扩展

    SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping. (arXiv:2310.01201v1 [cs.LG])

    [http://arxiv.org/abs/2310.01201](http://arxiv.org/abs/2310.01201)

    SWoTTeD是一种扩展的张量分解方法，用于发现复杂时间模式下的隐藏表征。在实验中，SWoTTeD不仅能与最新的基于张量分解的方法一样准确地重建数据，还能提取出对临床医生有意义的时间表征。

    

    张量分解最近在机器学习领域对于个体追踪数据的分析，如电子健康记录(EHR)，引起了人们的关注。然而，当数据遵循复杂的时间模式时，这个任务变得更加困难。本文引入了时间表征的概念，即一组随时间变化的特征，并提出了SWoTTeD (Sliding Window for Temporal Tensor Decomposition)方法，一种发现隐藏时间模式的新方法。SWoTTeD集成了多种约束和正则化方法，以增强提取到的表征的可解释性。我们使用合成数据集和真实世界数据集进行验证，并提供了使用巴黎大学医院的数据的原始用例。结果表明，SWoTTeD能够至少与最新的基于张量分解的模型一样准确地重建数据，并提取到对临床医生有意义的时间表征。

    Tensor decomposition has recently been gaining attention in the machine learning community for the analysis of individual traces, such as Electronic Health Records (EHR). However, this task becomes significantly more difficult when the data follows complex temporal patterns. This paper introduces the notion of a temporal phenotype as an arrangement of features over time and it proposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel method to discover hidden temporal patterns. SWoTTeD integrates several constraints and regularizations to enhance the interpretability of the extracted phenotypes. We validate our proposal using both synthetic and real-world datasets, and we present an original usecase using data from the Greater Paris University Hospital. The results show that SWoTTeD achieves at least as accurate reconstruction as recent state-of-the-art tensor decomposition models, and extracts temporal phenotypes that are meaningful for clinicians.
    
[^227]: Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications. (arXiv:2309.13207v1 [cs.LG])

    Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications. (arXiv:2309.13207v1 [cs.LG])

    [http://arxiv.org/abs/2309.13207](http://arxiv.org/abs/2309.13207)

    Evidential deep learning extends parametric deep learning to higher-order distributions, enabling the estimation of both aleatoric and epistemic uncertainty with one model. This study shows that evidential deep learning models achieve predictive accuracy comparable to standard methods while robustly quantifying both sources of uncertainty.

    

    预测不确定性的可靠量化对于了解天气和气候结果的驱动因素至关重要。集合提供预测不确定性估计，并且可以进行物理分解，但物理和机器学习集合都需要计算量很大。参数化深度学习可以通过预测概率分布的参数来估计不确定性，但不考虑认识不确定性。证据深度学习是一种将参数化深度学习扩展到高阶分布的技术，可以通过一个模型同时考虑两种不确定性：随机误差和认识误差。本研究比较了从证据神经网络和集合中得到的不确定性。通过对冬季降水类型的分类和地表层通量的回归应用，我们展示了证据深度学习模型达到了与标准方法相媲美的预测准确性，同时可靠地量化这两种来源的不确定性。

    Robust quantification of predictive uncertainty is critical for understanding factors that drive weather and climate outcomes. Ensembles provide predictive uncertainty estimates and can be decomposed physically, but both physics and machine learning ensembles are computationally expensive. Parametric deep learning can estimate uncertainty with one model by predicting the parameters of a probability distribution but do not account for epistemic uncertainty.. Evidential deep learning, a technique that extends parametric deep learning to higher-order distributions, can account for both aleatoric and epistemic uncertainty with one model. This study compares the uncertainty derived from evidential neural networks to those obtained from ensembles. Through applications of classification of winter precipitation type and regression of surface layer fluxes, we show evidential deep learning models attaining predictive accuracy rivaling standard methods, while robustly quantifying both sources of 
    
[^228]: 在教育数据挖掘中深度学习技术的综合调研

    A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])

    [http://arxiv.org/abs/2309.04761](http://arxiv.org/abs/2309.04761)

    本调研综合审查了在教育数据挖掘中深度学习技术的最新研究进展，包括对知识跟踪、学生不良行为检测、性能预测和个性化推荐等典型教育场景的应用。同时提供了公共数据集和处理工具的综合概述，并指出了未来的研究方向。

    

    教育数据挖掘(EDM)作为研究的重要领域，利用计算技术来分析教育数据。随着教育数据的复杂性和多样性增加，深度学习技术在解决分析和建模这些数据所面临的挑战方面表现出了显著的优势。本调研旨在系统地审查深度学习在EDM领域的最新研究进展。我们首先提供了关于EDM和深度学习的简要介绍，强调了它们在现代教育环境中的重要性。接下来，我们详细回顾了在四个典型教育场景中应用的深度学习技术，包括知识跟踪、学生不良行为检测、性能预测和个性化推荐。此外，我们还提供了EDM的公共数据集和处理工具的综合概述。最后，我们指出了该研究领域的新兴趋势和未来方向。

    Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
    
[^229]: 更具表现力的图神经网络在生成任务中是否更好？

    Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])

    [http://arxiv.org/abs/2308.11978](http://arxiv.org/abs/2308.11978)

    本论文调查了更具表现力的图神经网络在分子图生成任务中的表现能力，并通过替换图生成模型的基础GNN来进行实验。研究发现，使用更具表现力的GNN可以改善生成任务的性能。

    

    图生成是一个重要的挑战，它涉及根据给定的标签预测一个完整的具有多个节点和边的图。这个任务对许多实际应用非常重要，包括药物和分子设计。近年来，在图生成领域出现了几种成功的方法。然而，这些方法存在两个重大问题：(1) 这些方法中使用的基础图神经网络（GNN）架构往往未经深入探索；(2) 这些方法往往只在有限的指标上进行评估。为填补这个空白，我们通过将图生成模型的基础GNN替换为更具表现力的GNN，研究了GNN在分子图生成任务中的表现能力。具体而言，我们分析了两种不同生成框架（GCPN和GraphAF）中六种GNN在六个不同的分子生成目标上的性能。

    Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
    
[^230]: 用可证明概率保证在深度神经网络中枚举安全区域

    Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees. (arXiv:2308.09842v1 [cs.LG])

    [http://arxiv.org/abs/2308.09842](http://arxiv.org/abs/2308.09842)

    通过epsilon-ProVe方法，我们提出了一种高效近似的方法来枚举深度神经网络中的安全区域，并提供了可证明概率保证的紧密下估计。

    

    识别安全区域是保证基于深度神经网络（DNNs）系统的信任的关键点。为此，我们引入了AllDNN-Verification问题：给定一个安全属性和一个DNN，枚举属性输入域的所有安全区域，即属性成立的区域。由于问题的#P难度，我们提出了一种高效的近似方法叫做epsilon-ProVe。我们的方法通过统计预测容限限制获得可控低估的输出可达集，并能够提供一个具有可证明概率保证的安全区域的紧密下估计。我们在不同的标准基准测试上进行的实证评估显示了我们方法的可扩展性和有效性，为这种新型的DNN验证提供了有价值的见解。

    Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
    
[^231]: PMET: 在Transformer中的精确模型编辑

    PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])

    [http://arxiv.org/abs/2308.08742](http://arxiv.org/abs/2308.08742)

    该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。

    

    模型编辑技术可以以较低的成本修改大型语言模型中的少量知识，并且已经取得了显著的成功。现有方法假设Transformer层隐藏状态是前馈网络的键值内存的值。它们通常优化Transformer层隐藏状态来记忆目标知识，并将其用于更新大型语言模型中前馈网络的权重。然而，Transformer层隐藏状态的信息流来自三个部分：多头自注意力、前馈网络和残差连接。现有方法忽视了Transformer层隐藏状态包含了前馈网络特别需要的信息这一事实。因此，模型编辑的性能下降。为了实现更精确的模型编辑，我们分析了多头自注意力和前馈网络的隐藏状态，发现多头自注意力编码了某些通用知识提取模式。这意味着当引入新知识时，多头自注意力的权重不需要更新。

    Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
    
[^232]: 决定性混淆下的内核单一代理控制

    Kernel Single Proxy Control for Deterministic Confounding. (arXiv:2308.04585v1 [stat.ML])

    [http://arxiv.org/abs/2308.04585](http://arxiv.org/abs/2308.04585)

    本研究考虑了具有未观测混淆因素的因果效应估计问题，在结果是确定性生成的情况下，提出了一种使用单一代理变量的内核方法，通过两阶段回归和最大矩约束的方法可以一致估计因果效应，并在合成数据集上成功恢复了因果效应。

    

    本文考虑具有未观测混淆因素的因果效应估计问题，其中我们观测到与混淆因素相关的代理变量。尽管代理因果学习（PCL）使用两个代理变量来恢复真实的因果效应，我们证明如果结果是确定性生成的，则使用单个代理变量就足以进行因果估计，并概括了控制结果校准法（COCA）。我们提出了两种基于内核的方法：一种基于两阶段回归方法，另一种基于最大矩约束方法。我们证明了这两种方法都可以一致地估计因果效应，并通过合成数据集的实证实验成功地恢复了因果效应。

    We consider the problem of causal effect estimation with an unobserved confounder, where we observe a proxy variable that is associated with the confounder. Although Proxy Causal Learning (PCL) uses two proxy variables to recover the true causal effect, we show that a single proxy variable is sufficient for causal estimation if the outcome is generated deterministically, generalizing Control Outcome Calibration Approach (COCA). We propose two kernel-based methods for this setting: the first based on the two-stage regression approach, and the second based on a maximum moment restriction approach. We prove that both approaches can consistently estimate the causal effect, and we empirically demonstrate that we can successfully recover the causal effect on a synthetic dataset.
    
[^233]: 正则化、提前停止和梦想：一种处理泛化和过拟合的类Hopfield设置

    Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting. (arXiv:2308.01421v1 [cs.LG])

    [http://arxiv.org/abs/2308.01421](http://arxiv.org/abs/2308.01421)

    这项工作提出了一种处理神经网络泛化和过拟合的新方法，通过正则化损失函数和提前停止策略来优化网络参数，并通过数值实验验证了该方法的有效性。

    

    在这项工作中，我们从机器学习的角度来处理吸引子神经网络：通过对正则化损失函数进行梯度下降来寻找最优网络参数。在这个框架中，最优的神经元交互矩阵被证明是一类通过迭代应用某些取消学习协议修订的Hebbian核矩阵。值得注意的是，取消学习步骤的数量被证明与损失函数的正则化超参数和训练时间有关。因此，我们可以设计避免过拟合的策略，这些策略可以用交互矩阵的代数性质来描述，或者等价地用正则化调整和提前停止策略来描述。还研究了这些吸引子网络的泛化能力：针对随机合成数据集获得了分析结果，随后用数值实验来验证了所得到的整体情况。

    In this work we approach attractor neural networks from a machine learning perspective: we look for optimal network parameters by applying a gradient descent over a regularized loss function. Within this framework, the optimal neuron-interaction matrices turn out to be a class of matrices which correspond to Hebbian kernels revised by iteratively applying some unlearning protocols. Remarkably, the number of unlearning steps is proved to be related to the regularization hyperparameters of the loss function and to the training time. Thus, we can design strategies to avoid overfitting that are formulated in terms of the algebraic properties of the interaction matrix, or, equivalently, in terms of regularization tuning and early-stopping strategies. The generalization capabilities of these attractor networks are also investigated: analytical results are obtained for random synthetic datasets, next, the emerging picture is corroborated by numerical experiments that highlight the existence o
    
[^234]: WEPRO: 用于混合量子-经典算法高效优化的权重预测方法

    WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms. (arXiv:2307.12449v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2307.12449](http://arxiv.org/abs/2307.12449)

    本研究提出了一种名为WEPRO的新方法，通过利用参数权重中的规律趋势加速了混合量子-经典算法的收敛速度，相比标准训练方法，速度提高约2.25倍，准确性提高了2.3%，损失降低了6.1%。

    

    量子模拟器在经典计算机上的指数级运行时间、真实量子设备的长队列深度和高成本给变分量子算法(VQA)如量子神经网络(QNNs)、变分量子本征求解器(VQE)和量子近似优化算法(QAOA)的有效训练带来了巨大挑战。为了解决这些限制，我们提出了一种新方法WEPRO(权重预测)，通过利用参数权重中的规律趋势来加快VQA的收敛速度。我们引入了两种优化预测性能的技术，即Naive Prediction(NaP)和Adaptive Prediction(AdaP)。通过对各种数据集上多个QNN模型的广泛实验和训练，我们证明WEPRO相对于标准训练方法加快了大约2.25倍的速度，同时在存储和计算开销较低的情况下提供了更高的准确性(高达2.3%)和更低的损失(高达6.1%)。

    The exponential run time of quantum simulators on classical machines and long queue depths and high costs of real quantum devices present significant challenges in the effective training of Variational Quantum Algorithms (VQAs) like Quantum Neural Networks (QNNs), Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA). To address these limitations, we propose a new approach, WEPRO (Weight Prediction), which accelerates the convergence of VQAs by exploiting regular trends in the parameter weights. We introduce two techniques for optimal prediction performance namely, Naive Prediction (NaP) and Adaptive Prediction (AdaP). Through extensive experimentation and training of multiple QNN models on various datasets, we demonstrate that WEPRO offers a speedup of approximately $2.25\times$ compared to standard training methods, while also providing improved accuracy (up to $2.3\%$ higher) and loss (up to $6.1\%$ lower) with low storage and computational over
    
[^235]: 特征预处理对差分隐私线性优化的重要性

    The importance of feature preprocessing for differentially private linear optimization. (arXiv:2307.11106v1 [cs.LG])

    [http://arxiv.org/abs/2307.11106](http://arxiv.org/abs/2307.11106)

    本论文研究了差分隐私线性优化中特征预处理的重要性。在简单的线性分类情况下，与非隐私优化相比，特征预处理对于差分隐私优化是至关重要的，否则会产生与特征最大范数成比例的隐私错误。我们提出了一种结合特征预处理的算法DPSGD-F。

    

    在最近几年中，使用差分隐私（DP）训练机器学习模型引起了越来越多的关注。其中最流行的用于训练差分隐私模型的算法之一是差分隐私随机梯度下降（DPSGD）及其变种，在每个步骤中，梯度被剪裁并与一些噪音结合。鉴于DPSGD的广泛使用，我们提出一个问题：在隐私约束下，仅仅使用DPSGD是否足以找到每个数据集的良好极小值点？作为回答这个问题的第一步，我们展示了即使对于简单的线性分类情况，与非隐私优化相比，（私有）特征预处理对于差分隐私优化是至关重要的。具体而言，我们首先从理论上证明了存在一种例子，在没有特征预处理的情况下，DPSGD会产生与所有样本上的特征的最大范数成比例的隐私错误。然后，我们提出了一种名为DPSGD-F的算法，将DPSGD与特征预处理结合起来。

    Training machine learning models with differential privacy (DP) has received increasing interest in recent years. One of the most popular algorithms for training differentially private models is differentially private stochastic gradient descent (DPSGD) and its variants, where at each step gradients are clipped and combined with some noise. Given the increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints? As a first step towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization. In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs a privacy error proportional to the maximum norm of features over all samples. We then propose an algorithm called DPSGD-F, which combines DPSGD with feature
    
[^236]: 选择模型和置换不变性

    Choice Models and Permutation Invariance. (arXiv:2307.07090v1 [econ.EM])

    [http://arxiv.org/abs/2307.07090](http://arxiv.org/abs/2307.07090)

    本文提出了选择模型和置换不变性的基本特征化方法，并展示了如何通过非参数估计器逼近选择函数，以及在实际应用中的灵活性和优越性。

    

    选择建模是许多经济学、运营和营销问题的核心。在本文中，我们提出了一种对选择函数进行基本特征化的方法，涵盖了各种现有的选择模型。我们展示了非参数估计器如神经网络如何能够轻松逼近此类函数，并克服在非参数估计选择函数中固有的维数灾难。通过大量模拟，我们证明了我们提出的函数可以以完全数据驱动的方式灵活捕捉潜在的消费者行为，并胜过传统的参数模型。因为需求设置常常具有内生特征，我们将我们的框架扩展到包括在内生特征下的估计。此外，我们还描述了一个形式推理程序，以构建关于价格弹性等感兴趣对象的有效置信区间。最后，为了评估我们估计器的实际适用性，我们利用了一个真实世界的实证数据集。

    Choice Modeling is at the core of many economics, operations, and marketing problems. In this paper, we propose a fundamental characterization of choice functions that encompasses a wide variety of extant choice models. We demonstrate how nonparametric estimators like neural nets can easily approximate such functionals and overcome the curse of dimensionality that is inherent in the non-parametric estimation of choice functions. We demonstrate through extensive simulations that our proposed functionals can flexibly capture underlying consumer behavior in a completely data-driven fashion and outperform traditional parametric models. As demand settings often exhibit endogenous features, we extend our framework to incorporate estimation under endogenous features. Further, we also describe a formal inference procedure to construct valid confidence intervals on objects of interest like price elasticity. Finally, to assess the practical applicability of our estimator, we utilize a real-world
    
[^237]: TGRL:一种用于教师引导强化学习的算法

    TGRL: An Algorithm for Teacher Guided Reinforcement Learning. (arXiv:2307.03186v1 [cs.LG])

    [http://arxiv.org/abs/2307.03186](http://arxiv.org/abs/2307.03186)

    TGRL是一种用于教师引导强化学习的算法，通过动态和自动平衡何时遵循教师指导和何时使用奖励，教师监督的重要性会根据代理的表现调整。

    

    学习奖励(即强化学习或RL)和学习模仿教师(即教师-学生学习)是解决顺序决策问题的两种成熟方法。为了结合这些不同形式学习的优点，通常会训练一个策略来最大化强化学习和教师-学生学习目标的组合。然而，如果没有一个有原则的方法来平衡这些目标，之前的工作使用启发式方法和问题特定的超参数搜索来平衡两个目标。我们提出了一种"有原则"的方法，并提出了一种近似实现"动态"和"自动"平衡何时遵循教师和何时使用奖励。主要思想是通过比较代理的性能与没有教师监督并只从奖励中学习的对照情景来调整教师监督的重要性。如果使用教师监督改善了代理的性能，那么教师监督的重要性就会增加。

    Learning from rewards (i.e., reinforcement learning or RL) and learning to imitate a teacher (i.e., teacher-student learning) are two established approaches for solving sequential decision-making problems. To combine the benefits of these different forms of learning, it is common to train a policy to maximize a combination of reinforcement and teacher-student learning objectives. However, without a principled method to balance these objectives, prior work used heuristics and problem-specific hyperparameter searches to balance the two objectives. We present a $\textit{principled}$ approach, along with an approximate implementation for $\textit{dynamically}$ and $\textit{automatically}$ balancing when to follow the teacher and when to use rewards. The main idea is to adjust the importance of teacher supervision by comparing the agent's performance to the counterfactual scenario of the agent learning without teacher supervision and only from rewards. If using teacher supervision improves 
    
[^238]: 混合图：一种用于复杂图的统一图表示及数据集和基准的方法

    Hybrid Graph: A Unified Graph Representation with Datasets and Benchmarks for Complex Graphs. (arXiv:2306.05108v1 [cs.LG])

    [http://arxiv.org/abs/2306.05108](http://arxiv.org/abs/2306.05108)

    本论文介绍了混合图的概念及其在高阶图建模中的应用，同时提出了混合图数据集及全面的评估框架，这为图神经网络在复杂图上的性能提供了全面的解决方案。

    

    图被广泛用于封装各种数据格式，但是现实世界中的网络通常涉及复杂的节点关系，不仅仅是成对的关系。虽然已经开发和使用了超图和分层图来解决复杂的节点关系，但它们在实践中无法完全表示这些复杂性。此外，尽管已经提出了许多用于更高阶图的表示学习的图神经网络（GNN），但它们通常只在简单的图数据集上进行评估。因此，需要一个统一的更高阶图建模方法，并且需要一组包含全面数据集的可访问的评估框架，以完全了解这些算法在复杂图上的性能。在本文中，我们引入了混合图的概念，这是一个更高阶图的统一定义，并提出了混合图贝奇马克（HGB）。HGB包含各个领域的23个真实混合图数据集（如生物学、社交媒体和交通），并为GNN在复杂图上提供了全面的评估框架。

    Graphs are widely used to encapsulate a variety of data formats, but real-world networks often involve complex node relations beyond only being pairwise. While hypergraphs and hierarchical graphs have been developed and employed to account for the complex node relations, they cannot fully represent these complexities in practice. Additionally, though many Graph Neural Networks (GNNs) have been proposed for representation learning on higher-order graphs, they are usually only evaluated on simple graph datasets. Therefore, there is a need for a unified modelling of higher-order graphs, and a collection of comprehensive datasets with an accessible evaluation framework to fully understand the performance of these algorithms on complex graphs. In this paper, we introduce the concept of hybrid graphs, a unified definition for higher-order graphs, and present the Hybrid Graph Benchmark (HGB). HGB contains 23 real-world hybrid graph datasets across various domains such as biology, social media
    
[^239]: 医疗知识图谱综述：资源、应用和前景

    A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])

    [http://arxiv.org/abs/2306.04802](http://arxiv.org/abs/2306.04802)

    本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。

    

    医疗知识图谱(HKGs)已成为组织医学知识的有结构且可解释的有为工具，提供了医学概念及其关系的全面视图。然而，数据异质性和覆盖范围有限等挑战仍然存在，强调了在HKG领域需要进一步研究的必要性。本综述是HKG的第一份综合概述。我们总结了HKG构建的流程和关键技术（即从头开始和通过集成），以及常见的利用方法（即基于模型和非基于模型）。为了为研究人员提供有价值的资源，我们根据它们捕获的数据类型和应用领域（该资源存储于https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase）组织了现有的HKG，并提供了相关的统计信息。在应用部分，我们深入探讨了HKG在各种医疗领域的变革性影响。

    Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
    
[^240]: 重新思考对抗策略：多智能体强化学习中的广义攻击形式和可证明的防御

    Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])

    [http://arxiv.org/abs/2305.17342](http://arxiv.org/abs/2305.17342)

    本文介绍了另一种常见、现实的多智能体RL攻击设置，提出了一种模拟攻击者对代理$\alpha$控制的更一般化攻击形式。并解决了先前攻击模型中缺乏可证明防御的问题。

    

    大多数现有的研究研究直接扰动受害者的状态/动作或基础转移动态以展示强化学习智能体在对抗攻击下的脆弱性。然而，这样的直接操纵在实践中并不总是可行的。在本文中，我们考虑另一种常见且现实的攻击设置：在经过训练的多智能体RL的设置中，在部署期间，受害代理$\nu$被攻击者控制另一个代理$\alpha$以敌对方式行动，使用“对抗策略”对受害代理进行攻击。尽管之前的攻击模型考虑了这种设置，但他们没有考虑到攻击者可以遇到抵抗，因此只能部分控制代理$\alpha$，同时引入可察觉的“异常”行为，这些行为很容易被检测到。并且缺乏针对这些对抗策略的可证明的防御。为了解决这些问题，我们引入了一个更一般化的攻击形式，模拟了攻击者在何种程度上可以控制代理$\alpha$。

    Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\nu$ is exploited by an attacker who controls another agent $\alpha$ to act adversarially against the victim using an \textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a
    
[^241]: 度量空间和Nagata维度中k-NN规则的普遍一致性(II)

    Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II. (arXiv:2305.17282v1 [cs.LG])

    [http://arxiv.org/abs/2305.17282](http://arxiv.org/abs/2305.17282)

    本文研究了k近邻学习规则中的普遍一致性，发现在可分度量空间中，该规则在Nagata维度下的sigma有限维度的空间中是普遍一致的，在非阿基米德度量空间中是强普遍一致的，此规则在具有de Groot有限维度意义下的度量空间和Heisenberg群中也是普遍一致的。

    

    我们继续在可分度量空间中研究k近邻学习规则。由于C\'erou和Guyader(2006)以及Preiss(1983)的结果，已知该规则在每个Nagata意义下的sigma有限维度的度量空间X中是普遍一致的。在此，我们展示了在无平局情况下此规则在这样的空间中是强普遍一致的。在Devroye，Gy\"{o}rfi，Krzy\.{z}ak和Lugosi（1994）在欧几里得设置中应用的打破平局策略下，我们设法在非阿基米德度量空间（即Nagata维度为零的空间）中展示了强普遍一致性。结合C\'erou和Guyader的定理和Assouad和Quentin de Gromard (2006)的结果，可以推出$k$-NN规则在具有de Groot有限维度意义下的度量空间中是普遍一致的。特别地，$k$-NN规则在Heisenberg群中是普遍一致的，而该群并非sigma有限维度的。

    We continue to investigate the $k$ nearest neighbour learning rule in separable metric spaces. Thanks to the results of C\'erou and Guyader (2006) and Preiss (1983), this rule is known to be universally consistent in every metric space $X$ that is sigma-finite dimensional in the sense of Nagata. Here we show that the rule is strongly universally consistent in such spaces in the absence of ties. Under the tie-breaking strategy applied by Devroye, Gy\"{o}rfi, Krzy\.{z}ak, and Lugosi (1994) in the Euclidean setting, we manage to show the strong universal consistency in non-Archimedian metric spaces (that is, those of Nagata dimension zero). Combining the theorem of C\'erou and Guyader with results of Assouad and Quentin de Gromard (2006), one deduces that the $k$-NN rule is universally consistent in metric spaces having finite dimension in the sense of de Groot. In particular, the $k$-NN rule is universally consistent in the Heisenberg group which is not sigma-finite dimensional in the se
    
[^242]: 火星时间序列分解：一种多尺度嵌套方法中的因子变分自编码器

    Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v1 [cs.LG])

    [http://arxiv.org/abs/2305.16189](http://arxiv.org/abs/2305.16189)

    该论文提出了一种因子高斯混合变分自动编码器，用于多尺度聚类和源分离，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程，并在MRO数据集上展现了更好的性能。

    

    无监督的源分离涉及通过混合操作记录的未知源信号的分解，其中对源的先验知识有限，仅可以访问信号混合数据集。这个问题本质上是不适用的，并且进一步受到时间序列数据中源展现出的多种时间尺度的挑战。为了解决这个问题，我们提出了一种无监督的多尺度聚类和源分离框架，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程。在这个表示空间中，我们开发了一个因子高斯混合变分自动编码器，它被训练用于(1)概率地对不同时间尺度上的源进行聚类和逐层非监督源分离，(2)在每个时间尺度上提取低维表示，(3)学习源信号的因子表示，(4)在表示空间中进行采样，以生成未知源信号。我们在MRO上的三个频道的可见数据集上进行了评估，结果表明所提出的方法比目前最先进的技术具有更好的性能。

    Unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. This problem is inherently ill-posed and is further challenged by the variety of time-scales exhibited by sources in time series data. Existing methods typically rely on a preselected window size that limits their capacity to handle multi-scale sources. To address this issue, instead of operating in the time domain, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering covariances that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-Gaussian stochastic processes. Nested within this representation space, we develop a factorial Gaussian-mixture variational autoencoder that is trained to (1) probabilistically cluster sources at different time-scales a
    
[^243]: 在祈祷之后喝啤酒？测量大型语言模型中的文化偏见。

    Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])

    [http://arxiv.org/abs/2305.14456](http://arxiv.org/abs/2305.14456)

    这篇论文研究了大型语言模型在处理和生成阿拉伯文本时出现的文化偏向西方文化的现象，表明语言模型在人名、食品、服装、地点、文学、饮料、宗教和体育等八个文化方面存在偏见。这些发现引发对于当前语言模型文化相关性的担忧。

    

    语言模型是否存在文化偏见？语言模型符合所服务社区的文化因素很重要。然而，本文表明在处理和生成阿拉伯文本时，语言模型存在显著的偏向西方文化的偏见，倾向于产生西方文化相关内容而非阿拉伯文化相关内容。我们通过使用从在线社交媒体上收集的自然出现的上下文和基于可能性评分的指标来量化这种偏见。我们的实验显示，阿拉伯语单语和多语模型在八个不同的文化方面存在西方文化偏见，包括人名、食品、服装、地点、文学、饮料、宗教和体育。当输入的阿拉伯语句子越接近英语时，模型也更容易表现出偏见。这些发现引发人们对当前语言模型文化相关性的担忧。我们的分析表明，在模型设计中应更多考虑文化因素和多样性。

    Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
    
[^244]: 利用Riesz核的生成式分割MMD流

    Generative Sliced MMD Flows with Riesz Kernels. (arXiv:2305.11463v1 [cs.LG])

    [http://arxiv.org/abs/2305.11463](http://arxiv.org/abs/2305.11463)

    本文使用Riesz核展示了生成式分割MMD流的高效计算方法，实现了在大规模应用中通过神经网络训练生成模型。

    

    在大规模计算中，最大平均差异度(MMD)流的计算成本很高。在本文中，我们展示了使用Riesz核$K(x,y)=-\|x-y\|^r$，$r \in (0,2)$的MMD流具有杰出的性质，可允许其进行高效计算。首先，Riesz核的MMD与其分割版本的MMD重合。因此，可以在一维设置中进行MMD梯度的计算。在此处，对于$r=1$，可以应用简单的排序算法将两个经验度量的复杂度从$O(MN+N^2)$降低到$O((M+N)\log(M+N))$，其中$M$和$N$是支持点。对于实现，我们通过仅使用有限数量的$P$个切片来近似分割MMD的梯度。我们展示了由此产生的误差具有$O(\sqrt{d/P})$的复杂度，其中$d$是数据维度。这些结果使我们能够通过神经网络近似MMD梯度流来训练生成模型，甚至用于大规模应用。

    Maximum mean discrepancy (MMD) flows suffer from high computational costs in large scale computations. In this paper, we show that MMD flows with Riesz kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which allow for their efficient computation. First, the MMD of Riesz kernels coincides with the MMD of their sliced version. As a consequence, the computation of gradients of MMDs can be performed in the one-dimensional setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two empirical measures with $M$ and $N$ support points. For the implementations we approximate the gradient of the sliced MMD by using only a finite number $P$ of slices. We show that the resulting error has complexity $O(\sqrt{d/P})$, where $d$ is the data dimension. These results enable us to train generative models by approximating MMD gradient flows by neural networks even for large scale applications. We demo
    
[^245]: 基于表面肌电图像的轻量级全卷积神经网络和迁移学习的跨场景手势识别

    Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])

    [http://arxiv.org/abs/2305.08014](http://arxiv.org/abs/2305.08014)

    本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。

    

    利用低分辨率瞬时高清肌电图像进行手势识别可以开辟发展更流畅、更自然的肌肉-计算机界面的新途径。然而，跨场景数据的变异性存在极大的挑战。现有的方法采用非常大且复杂的深度卷积神经网络或基于2SRNN的领域适应方法，来逼近由这些跨场景数据变异性引起的分布偏移。因此，这些方法也需要在预训练和适应阶段中在数百万个训练参数和大规模预训练数据集上进行学习。结果，这使得在实时应用中进行高端资源约束和计算非常昂贵的部署。为了解决这个问题，我们提出了一种轻量级的全卷积神经网络+迁移学习模型，利用轻量级全卷积神经网络和迁移学习(TL)来增强跨场景手势识别。

    Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
    
[^246]: 一种用于音视频语音表示学习的多模态动态变分自编码器

    A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning. (arXiv:2305.03582v1 [cs.SD])

    [http://arxiv.org/abs/2305.03582](http://arxiv.org/abs/2305.03582)

    本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习，该方法在中间表示上进行了静态与动态信息、模态特异与共同信息的分离，并且在实验中表现出优越性。

    

    本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习。潜在空间被构造为将在各个模态之间共享的潜在动态因素与每个模态特定的因素区分开来。同时，引入一个静态潜变量来编码音视频语音序列中随时间恒定的信息。模型在一个音视频情感语音数据集上进行无监督训练分两个阶段。在第一阶段，对于每个模态，首先独立学习一个向量量化自编码器（VQ-VAE），而没有时间建模。第二阶段则在向量量化自编码器（VQ-VAEs）的中间表示上学习MDVAE模型。该方法的实验结果表明，在语音表示学习方面，提出的方法优于现有方法。

    In this paper, we present a multimodal \textit{and} dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how a
    
[^247]: 将最大熵时刻法稳定应用于单精度下的稀疏气体动力学

    Stabilizing the Maximal Entropy Moment Method for Rarefied Gas Dynamics at Single-Precision. (arXiv:2303.02898v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2303.02898](http://arxiv.org/abs/2303.02898)

    本文的创新在于将最大熵时刻法稳定应用于单精度下的稀疏气体动力学，使其能够在现代GPU上模拟非常强的正常激波。

    

    发展适用于密集和稀疏气体的扩展流体力学方程仍然是一个巨大的挑战。对于这个挑战的一个系统解决方案是利用气体分子速度分布的矩法描述密集和稀疏气体行为。在众多的矩法中，最大熵时刻法（MEM）因其具有良好的解决性和稳定性而突出，它利用了熵最大化的速度分布。然而，寻找这样的分布需要解决一个条件病态且计算需求较大的优化问题。这个问题在数值精度不足时会导致数值溢出和崩溃，尤其是对于高速激波等流动现象。它还阻碍了现代GPU利用其巨大的单精度计算能力来加速优化。本文旨在稳定MEM，使其在现代GPU上以单精度实用于模拟非常强的正常激波。

    Developing extended hydrodynamics equations valid for both dense and rarefied gases remains a great challenge. A systematical solution for this challenge is the moment method describing both dense and rarefied gas behaviors with moments of gas molecule velocity distributions. Among moment methods, the maximal entropy moment method (MEM) stands out for its well-posedness and stability, which utilizes velocity distributions with maximized entropy. However, finding such distributions requires solving an ill-conditioned and computation-demanding optimization problem. This problem causes numerical overflow and breakdown when the numerical precision is insufficient, especially for flows like high-speed shock waves. It also prevents modern GPUs from accelerating optimization with their enormous single floating-point precision computation power. This paper aims to stabilize MEM, making it practical for simulating very strong normal shock waves on modern GPUs at single precision. We propose the
    
[^248]: 一种双层经验风险最小化算法的下界和近似最优算法

    A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization. (arXiv:2302.08766v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08766](http://arxiv.org/abs/2302.08766)

    该论文提出了一种双层经验风险最小化算法，使用的梯度计算次数 $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$，在样本复杂度方面是最优的。

    

    双层最优化问题越来越多地应用于机器学习中。在许多实际情况下，上层和下层目标对应于经验风险最小化问题，并因此具有总和结构。在这个背景下，我们提出了一个著名的SARAH算法的双层扩展。我们证明了该算法需要$\mathcal {O}((n+m)^{\frac{1}{2}}\varepsilon ^{-1})$次梯度计算才能实现$\varepsilon$稳定性，其中$n+m$是样本总数，这比先前所有的双层算法都要好。此外，我们提供了一个下界，用于得到双层问题的目标函数的近似稳定点所需的oracle调用次数。这个下界正是我们的算法所达到的，因此在样本复杂度方面是最优的。

    Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $\mathcal{O}((n+m)^{\frac12}\varepsilon^{-1})$ gradient computations to achieve $\varepsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, which is therefore optimal in terms of sample complexity.
    
[^249]: 通过矩阵分解从逆协方差矩阵中学习大型因果结构

    Learning Large Causal Structures from Inverse Covariance Matrix via Matrix Decomposition. (arXiv:2211.14221v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14221](http://arxiv.org/abs/2211.14221)

    本文提出了一种基于逆协方差矩阵的$\mathcal{O}$-ICID方法，该方法是通过连续优化一种矩阵分解来学习因果结构的，适用于变量数量庞大的情况。该方法可以在噪声方差已知时识别真实DAG, 也可以在较弱的先验信息下给出有用的定向图解

    

    当变量数量庞大时，从观测数据中学习因果结构是一个基本但高度复杂的问题。本文从线性结构方程模型(SEMs)出发，研究了从逆协方差矩阵中学习因果结构的方法。所提出的方法称为$\mathcal{O}$-ICID(来自Oracle 逆协方差矩阵的独立保持分解)，基于一种矩阵分解的连续优化，该分解保留了逆协方差矩阵的非零模式。我们证明了在噪声方差已知的情况下，$\mathcal{O}$-ICID为识别真实有向无环图(DAG)提供了一种高效的方式。在较弱的先验信息下，所提出的方法可以给出有用的定向图解，用于进行更精细的因果发现。当真实DAG具有有限的节点度数时，所提出的方法具有低复杂度，在实验中表现出良好的时间效率。

    Learning causal structures from observational data is a fundamental yet highly complex problem when the number of variables is large. In this paper, we start from linear structural equation models (SEMs) and investigate ways of learning causal structures from the inverse covariance matrix. The proposed method, called $\mathcal{O}$-ICID (for {\it Independence-preserving} Decomposition from Oracle Inverse Covariance matrix), is based on continuous optimization of a type of matrix decomposition that preserves the nonzero patterns of the inverse covariance matrix. We show that $\mathcal{O}$-ICID provides an efficient way for identifying the true directed acyclic graph (DAG) under the knowledge of noise variances. With weaker prior information, the proposed method gives directed graph solutions that are useful for making more refined causal discovery. The proposed method enjoys a low complexity when the true DAG has bounded node degrees, as reflected by its time efficiency in experiments in
    
[^250]: Survival Kernets: 可扩展且可解释的深度核生存分析模型，并具有准确性保证

    Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee. (arXiv:2206.10477v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10477](http://arxiv.org/abs/2206.10477)

    Survival Kernets 是一种可扩展且可解释的深度核生存分析模型，能够在大规模数据集上进行模型解释和理论分析。它利用核函数估计个体的生存分布，通过训练集压缩方案进行数据分簇，因此具有较高的可视化能力和预测准确性保证。该模型在特定情况下的预测生存分布误差界限最优，且在测试时具有可扩展性。

    

    核生存分析模型通过核函数来估计个体的生存分布，核函数度量任意两个数据点之间的相似性。我们提出了一种新的深度核生存模型——生存kernet，该模型可以适用于大规模数据集，并且易于解释和进行理论分析。具体而言，训练数据根据一种最近发展的用于分类和回归的训练集压缩方案（称为核群）进行分簇。在测试时，每个数据点被表示为这些簇的加权组合，每个簇可以进行可视化展示。对于生存kernet的一个特殊情况，我们建立了一个有限样本误差界限，预测的生存分布在该界限下是最优的（除去一个对数因子）。在测试时具有可扩展性。

    Kernel survival analysis models estimate individual survival distributions with the help of a kernel function, which measures the similarity between any two data points. Such a kernel function can be learned using deep kernel survival models. In this paper, we present a new deep kernel survival model called a survival kernet, which scales to large datasets in a manner that is amenable to model interpretation and also theoretical analysis. Specifically, the training data are partitioned into clusters based on a recently developed training set compression scheme for classification and regression called kernel netting that we extend to the survival analysis setting. At test time, each data point is represented as a weighted combination of these clusters, and each such cluster can be visualized. For a special case of survival kernets, we establish a finite-sample error bound on predicted survival distributions that is, up to a log factor, optimal. Whereas scalability at test time is achiev
    
[^251]: 批量异步随机逼近的收敛性及在强化学习中的应用

    Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.03445](http://arxiv.org/abs/2109.03445)

    本文提出了一种批量异步随机逼近算法，它可以在内存需求和时间复杂度之间进行权衡，同时提供了可以使用较弱假设证明收敛的一般方法；在强化学习领域，我们使用此方法证明了SARSA算法的批量异步版本的收敛性。

    

    随机逼近（SA）算法是一种广泛使用的概率方法，用于在仅可用函数的有噪测量情况下找到零点或固定点。目前的文献中，区分“同步”更新和“异步”更新，在“同步”更新中，每个猜测的组件都会在每个时间更新，而在“异步”更新中，仅更新一个组件。本文研究了一种中间情况，称为“批量异步随机逼近”（BASA），在这种情况下，每个时间点仅更新“当前估计解”的一些但不是全部的组件。BASA允许用户在内存需求和时间复杂度之间进行权衡。我们开发了一种通用方法，证明此类算法收敛于所研究映射的固定点。这些收敛证明使用比现有结果更弱的假设。具体而言，现有的收敛证明要求步长参数以适当的速率下降。相反，我们仅要求每个组件具有足够的更新频率。我们在强化学习领域展示了我们方法的有用性，证明了广泛使用的SARSA算法的批量异步版本的收敛性。

    The stochastic approximation (SA) algorithm is a widely used probabilistic method for finding a zero or a fixed point of a vector-valued funtion, when only noisy measurements of the function are available. In the literature to date, one makes a distinction between ``synchronous'' updating, whereby every component of the current guess is updated at each time, and ``asynchronous'' updating, whereby only one component is updated. In this paper, we study an intermediate situation that we call ``batch asynchronous stochastic approximation'' (BASA), in which, at each time instant, \textit{some but not all} components of the current estimated solution are updated. BASA allows the user to trade off memory requirements against time complexity. We develop a general methodology for proving that such algorithms converge to the fixed point of the map under study. These convergence proofs make use of weaker hypotheses than existing results. Specifically, existing convergence proofs require that the 
    
[^252]: 深度代理因果学习及其在混淆赌博策略评估中的应用

    Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation. (arXiv:2106.03907v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.03907](http://arxiv.org/abs/2106.03907)

    本论文提出了一种深度代理因果学习（PCL）方法，用于在存在混淆因素的情况下估计治疗对结果的因果效应。通过构建治疗和代理之间的模型，并利用该模型在给定代理的情况下学习治疗对结果的影响，PCL可以保证恢复真实的因果效应。作者还提出了一种名为深度特征代理变量方法（DFPV）的新方法，用于处理高维和非线性复杂关系的情况，并表明DFPV在合成基准测试中的性能优于最先进的PCL方法。

    

    代理因果学习（PCL）是一种在存在未观察到的混淆因素时，利用代理（结构化侧面信息）估计治疗对结果的因果效应的方法。这是通过两阶段回归实现的：在第一阶段，我们建模治疗和代理之间的关系；在第二阶段，我们利用这个模型来学习在给定代理提供的上下文下，治疗对结果的影响。PCL在可识别条件下保证恢复真实的因果效应。我们提出了一种新的PCL方法，深度特征代理变量方法（DFPV），以解决代理、治疗和结果为高维且具有非线性复杂关系的情况，如深度神经网络特征表示。我们表明DFPV在具有挑战性的合成基准测试中优于最近的最先进的PCL方法，包括涉及高维图像数据的设置。此外，我们还展示了PCL的应用...

    Proxy causal learning (PCL) is a method for estimating the causal effect of treatments on outcomes in the presence of unobserved confounding, using proxies (structured side information) for the confounder. This is achieved via two-stage regression: in the first stage, we model relations among the treatment and proxies; in the second stage, we use this model to learn the effect of treatment on the outcome, given the context provided by the proxies. PCL guarantees recovery of the true causal effect, subject to identifiability conditions. We propose a novel method for PCL, the deep feature proxy variable method (DFPV), to address the case where the proxies, treatments, and outcomes are high-dimensional and have nonlinear complex relationships, as represented by deep neural network features. We show that DFPV outperforms recent state-of-the-art PCL methods on challenging synthetic benchmarks, including settings involving high dimensional image data. Furthermore, we show that PCL can be app
    

