# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation.](http://arxiv.org/abs/2309.13042) | MosaicFusion是一种用于大词汇实例分割的数据增强方法，通过将扩散模型作为数据集生成器，能够生成大量合成标记数据。在实验中，我们的方法在准确率和泛化能力方面取得了显著的提升。 |
| [^2] | [Robotic Offline RL from Internet Videos via Value-Function Pre-Training.](http://arxiv.org/abs/2309.13041) | 本文提出了一种通过时间差分学习从大规模视频数据集中利用机器人离线强化学习的系统，该系统被称为V-PTR。与其他学习视频数据方法相比，通过价值学习可以更好地表示机器人离线强化学习的数据。 |
| [^3] | [Memory-augmented conformer for improved end-to-end long-form ASR.](http://arxiv.org/abs/2309.13029) | 该论文提出了一种将记忆增强神经网络添加到Conformer模型中以解决长话语情况下性能下降的问题。实验结果表明，该系统在长话语上优于基准Conformer模型。 |
| [^4] | [Graph Neural Network for Stress Predictions in Stiffened Panels Under Uniform Loading.](http://arxiv.org/abs/2309.13022) | 该研究提出了一种用于高效表示三维加固板的图嵌入技术，并采用图神经网络方法预测不同几何形状下加固板的应力分布。通过对比有限元-顶点图表示，证明了所提方法的有效性。 |
| [^5] | [A Hybrid Deep Learning-based Approach for Optimal Genotype by Environment Selection.](http://arxiv.org/abs/2309.13021) | 该论文提出了一种基于深度学习的混合方法，用于优化基于环境的基因型选择。通过整合不同作物品种的天气数据，特别是在不同气候条件下，准确地预测作物产量对于理解其适应性至关重要。论文中开发了两种新颖的卷积神经网络架构，并利用广义集成方法确定了最优的基因型与环境选择模型。 |
| [^6] | [Understanding Deep Gradient Leakage via Inversion Influence Functions.](http://arxiv.org/abs/2309.13016) | 本文提出了一种新的方法I²F，可以有效近似深度梯度泄露攻击，并建立了恢复图像和私有梯度之间的连接。通过这个方法，我们能够更好地理解和应对深度梯度泄露攻击。 |
| [^7] | [Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design.](http://arxiv.org/abs/2309.13015) | 本文提出了一种使用算法，架构和数据流协同设计的高效N:M稀疏DNN训练方案，该方案利用双向权重修剪方法优化计算成本，并通过稀疏加速器硬件支持实现高稀疏比的DNN训练。 |
| [^8] | [ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs.](http://arxiv.org/abs/2309.13007) | ReConcile是一个通过多轮讨论和投票机制来增强LLM推理能力的多模型多代理框架。 |
| [^9] | [Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains.](http://arxiv.org/abs/2309.13005) | 通过顺序自编码器实现反事实公平性追求的创新框架将环境信息和敏感属性与分类特征的嵌入表示分开，以提高模型在不同和陌生领域中的泛化能力，并解决公平问题。 |
| [^10] | [Expressive variational quantum circuits provide inherent privacy in federated learning.](http://arxiv.org/abs/2309.13002) | 表达性变分量子电路模型在联邦学习中提供固有隐私保护，同时通过使用过度参数化保证模型可训练性。通过解决高次多元切比雪夫多项式方程的复杂性，实现对梯度反转攻击的困难性。 |
| [^11] | [Point Cloud Network: An Order of Magnitude Improvement in Linear Layer Parameter Count.](http://arxiv.org/abs/2309.12996) | 本文提出了点云网络（PCN）架构，通过对线性层的改进，在保持与原始架构相当的测试准确性的同时，大幅度减少了参数数量。 |
| [^12] | [Deep learning probability flows and entropy production rates in active matter.](http://arxiv.org/abs/2309.12991) | 本论文通过深度学习方法，结合生成模型，提出了一个估计活性物质系统熵产生速率和概率流大小的框架，解决了传统计算方法中高维概率密度的问题，并给出了直接计算这些物理量的方法。 |
| [^13] | [Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes.](http://arxiv.org/abs/2309.12971) | 本文提出了基于花瓣拉普拉斯的高阶图卷积网络，通过利用简单复合体来建模高阶交互，在不同拓扑尺度上识别内在特征，并使用可学习的图滤波器来量化高阶交互强度。 |
| [^14] | [On Separate Normalization in Self-supervised Transformers.](http://arxiv.org/abs/2309.12931) | 在自监督变形器中，通过为标记和[CLS]符号分别使用归一化层，可以更好地捕捉它们各自的特点并提高下游任务的性能。 |
| [^15] | [BayesDLL: Bayesian Deep Learning Library.](http://arxiv.org/abs/2309.12928) | BayesDLL是一个用于PyTorch的贝叶斯深度学习库，与其他现有库相比，它可以处理非常大规模的深度网络，无需修改用户代码，并且可以使用预训练模型权重作为先验均值，适用于贝叶斯推断。 |
| [^16] | [A matter of attitude: Focusing on positive and active gradients to boost saliency maps.](http://arxiv.org/abs/2309.12913) | 本文研究了在显著图中考虑梯度符号和影响的作用，揭示了更好地识别卷积神经网络关注的图像像素的方法，并阐明了遮挡或改变这些像素会如何影响结果。 |
| [^17] | [Building explainable graph neural network by sparse learning for the drug-protein binding prediction.](http://arxiv.org/abs/2309.12906) | 本研究提出了一种稀疏学习的图神经网络（SLGNN），通过使用基于化学亚结构的图来表示药物分子，并结合广义融合套索和消息传递算法以识别对药物-蛋白质结合预测至关重要的连接亚图。 |
| [^18] | [FairComp: Workshop on Fairness and Robustness in Machine Learning for Ubiquitous Computing.](http://arxiv.org/abs/2309.12877) | 本研讨会旨在讨论普适计算研究中的公平性及其社会、技术和法律影响，通过社会角度、技术角度和法律角度来探究公正的途径，并为该领域的未来研究规划出明确的路径。 |
| [^19] | [AnglE-Optimized Text Embeddings.](http://arxiv.org/abs/2309.12871) | 本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。 |
| [^20] | [Associative Transformer Is A Sparse Representation Learner.](http://arxiv.org/abs/2309.12862) | 关联变换器（AiT）是一种采用低秩显式记忆和关联记忆的稀疏表示学习器，通过联合端到端训练实现模块特化和注意力瓶颈的形成。 |
| [^21] | [Robotic Handling of Compliant Food Objects by Robust Learning from Demonstration.](http://arxiv.org/abs/2309.12856) | 本文介绍了一种机器人从示教中学习处理软性食品对象的稳健学习策略，通过合并RGB-D图像和触觉数据进行抓取。 |
| [^22] | [Cross-Modal Translation and Alignment for Survival Analysis.](http://arxiv.org/abs/2309.12855) | 本论文提出了一种跨模态翻译与对齐 (CMTA) 框架，用于将病理图像和基因组特征结合起来进行生存分析，通过探索内在的跨模态相关性和传递互补信息，提高了生存分析预测的准确性和效果。 |
| [^23] | [DeepOPF-U: A Unified Deep Neural Network to Solve AC Optimal Power Flow in Multiple Networks.](http://arxiv.org/abs/2309.12849) | DeepOPF-U 是一种统一的深度神经网络，用于解决不同电力网络中的交流最优功率流问题，具有较强的泛化性和性能优势。 |
| [^24] | [Reward Function Design for Crowd Simulation via Reinforcement Learning.](http://arxiv.org/abs/2309.12841) | 本论文研究了基于强化学习的人群模拟中的奖励函数设计。通过理论分析和经验评估，我们发现直接最小化能源使用并配合适当的引导势是一种有效的策略，并且不同奖励组成对模拟人群行为具有影响。这些发现对于新的人群模拟技术的开发具有指导意义。 |
| [^25] | [AxOCS: Scaling FPGA-based Approximate Operators using Configuration Supersampling.](http://arxiv.org/abs/2309.12830) | 近似计算作为低成本机器学习实现的解决方案在嵌入式系统中得到广泛研究。其中，设计特定于平台的近似算术运算符成为主要问题，现有方法主要使用基于机器学习的代理函数预测性能和行为影响，缺乏更先进的方法。 |
| [^26] | [Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography.](http://arxiv.org/abs/2309.12829) | 本研究探讨了使用合成数据集来增强超声心动图分割的视觉-语言分割模型（VLSM），结果显示合成数据集可以提高分割模型的指标和训练速度。 |
| [^27] | [Doubly Robust Proximal Causal Learning for Continuous Treatments.](http://arxiv.org/abs/2309.12819) | 本文提出了一种基于核函数的双重稳健近端因果学习方法，用于处理连续治疗，并提出了一种高效求解干扰函数的新方法。 |
| [^28] | [Improving Generalization in Game Agents with Data Augmentation in Imitation Learning.](http://arxiv.org/abs/2309.12815) | 本文提出了一种改进模仿学习中游戏智能体泛化能力的方法，通过数据增强技术使训练数据更好地代表真实场景中的状态和行动分布，并在多个3D环境中进行了性能测试，结果表明数据增强可以显著提升模仿学习智能体的泛化能力。 |
| [^29] | [Automatically Testing Functional Properties of Code Translation Models.](http://arxiv.org/abs/2309.12813) | 本研究介绍了一种自动、功能性的代码翻译模型测试方法，能够捕捉各种属性从纯语法到纯语义的相关信息，并在实验中证明其有效性。 |
| [^30] | [Deepfake audio as a data augmentation technique for training automatic speech to text transcription models.](http://arxiv.org/abs/2309.12802) | 本论文提出了一种基于深度伪造音频的数据增强框架，用于训练自动语音转文字模型。通过使用已有的深度伪造和转录模型进行实验，验证了该框架的有效性。 |
| [^31] | [An Intelligent Approach to Detecting Novel Fault Classes for Centrifugal Pumps Based on Deep CNNs and Unsupervised Methods.](http://arxiv.org/abs/2309.12765) | 本文提出了一种基于深度卷积神经网络和无监督方法的智能检测离心泵新型故障类别的方法，通过部分知识训练神经网络并使用t-SNE方法和聚类技术检测新故障，并通过新数据增强网络，实验证明该方法能够高准确性地检测新故障。 |
| [^32] | [Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where.](http://arxiv.org/abs/2309.12757) | 该论文研究了如何将遮盖操作引入卷积神经网络的对比学习框架中，以提高自监督学习的效果。同时，研究还发现遮盖操作可能存在一些副作用，作者提出了解决方案来应对这些问题。 |
| [^33] | [Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation.](http://arxiv.org/abs/2309.12742) | 本文提出了一种名为"不变一致性学习"（ICON）的方法，通过给予源域和目标域相等的地位，学习一个不变的分类器，从而消除了在目标域中的虚假相关性不一致。 |
| [^34] | [Optimal Dynamic Fees for Blockchain Resources.](http://arxiv.org/abs/2309.12735) | 我们开发了一个通用且实用的框架来解决多个区块链资源的动态费用机制的最优设计问题，并通过最优策略正确处理了资源需求中的交叉效应。我们还展示了如何利用这些交叉效应来指导资源设计，并演示了如何使用这个框架来完善或指导启发式费用更新规则。 |
| [^35] | [H2O+: An Improved Framework for Hybrid Offline-and-Online RL with Dynamics Gaps.](http://arxiv.org/abs/2309.12716) | H2O+是一种改进的混合离线和在线强化学习框架，通过综合考虑真实和模拟环境的动力学差距，同时利用有限的离线数据和不完美的模拟器进行策略学习，并在广泛的仿真和实际机器人实验中展示了卓越的性能和灵活性。 |
| [^36] | [Unsupervised Representations Improve Supervised Learning in Speech Emotion Recognition.](http://arxiv.org/abs/2309.12714) | 这项研究提出了一种新颖的方法，利用无监督特征提取和监督分类相结合的方式，在小样本音频片段中进行情感识别。通过使用自监督特征提取器和基于CNN的模型，该方法在情感识别上表现出优于传统方法的结果。 |
| [^37] | [Big model only for hard audios: Sample dependent Whisper model selection for efficient inferences.](http://arxiv.org/abs/2309.12712) | 提出了一个决策模块来选择最小的足够模型用于音频转录，可以实现大幅度的计算节约。 |
| [^38] | [PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion.](http://arxiv.org/abs/2309.12708) | PointSSC是第一个为了语义场景补全而引入的车辆基础设施点云合作基准，具备长距离感知和最小遮挡。通过使用Segment Anything进行自动化注释，我们提出了一种基于激光雷达的模型，结合补全和分割的合作模块，来推动语义点云补全在真实世界导航中的发展。 |
| [^39] | [Multi-Label Noise Transition Matrix Estimation with Label Correlations: Theory and Algorithm.](http://arxiv.org/abs/2309.12706) | 本论文提出了一种解决多标签噪声转移矩阵估计问题的方法，通过研究可辨识性，并结合标签相关性，提出了一种新的估计器，可以在噪声多标签学习中实现统计一致性算法。 |
| [^40] | [Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming.](http://arxiv.org/abs/2309.12701) | 本文提出了一种使用动态规划找到最优决策树的方法，可以得到多个可解释性-性能权衡的最优决策树，使用户可以根据自己的需求选择最适合的树。 |
| [^41] | [Semantic similarity prediction is better than other semantic similarity measures.](http://arxiv.org/abs/2309.12697) | 本文提出了一种使用经过微调的模型直接预测语义相似性的方法，并将其与其他方法进行比较，结果表明所得到的相似性更加符合我们对鲁棒的语义相似性度量的预期。 |
| [^42] | [Recurrent Temporal Revision Graph Networks.](http://arxiv.org/abs/2309.12694) | 该论文提出了一种循环时间修订图网络的新框架，通过使用循环神经网络和逐节点的隐藏状态，将所有历史邻居的信息整合起来，以获得每个节点的完整邻居信息。该框架在理论表现能力和实际应用中都取得了优越的性能。 |
| [^43] | [AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer.](http://arxiv.org/abs/2309.12689) | AMPLIFY提出了一种基于注意力机制的Mixup方法，用于减少原始样本中的噪音和异常值对于模型的影响，并在文本分类任务中表现出更好的性能。 |
| [^44] | [On Sparse Modern Hopfield Model.](http://arxiv.org/abs/2309.12673) | 本文介绍了稀疏的现代 Hopfield 模型，通过引入稀疏能量函数和稀疏记忆检索动力学，实现了对稀疏注意机制的一步近似。相比密集模型，稀疏模型的记忆检索误差上界更紧凑，具有明确的稀疏优势条件。同时，稀疏的现代 Hopfield 模型还保持了其密集对应物的稳健理论性质。 |
| [^45] | [How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization.](http://arxiv.org/abs/2309.12671) | 本文提出了一个统一模型偏移和模型偏差的优化目标，并通过微调过程实现了自适应的模型更新，以提供性能改进保证和避免模型过拟合。 |
| [^46] | [OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling.](http://arxiv.org/abs/2309.12659) | OneNet是一个在线的时间序列预测模型，通过在线集成两个模型来适应概念漂移问题，其中一个模型用于建模时间维度上的依赖关系，另一个模型用于跨变量的依赖关系。实验证明，OneNet在适应概念漂移方面比其他方法更快且更有效。 |
| [^47] | [Neural Operator Variational Inference based on Regularized Stein Discrepancy for Deep Gaussian Processes.](http://arxiv.org/abs/2309.12658) | 基于正则化Stein差异的神经算子变分推断用于深度高斯过程，通过使用神经生成器获得取样器以及使用蒙特卡罗估计和子采样随机优化技术解决极小极大问题，提高了深度高斯过程模型的表达能力和推断效果。 |
| [^48] | [FP-PET: Large Model, Multiple Loss And Focused Practice.](http://arxiv.org/abs/2309.12650) | 本研究提出了FP-PET，使用STUNet-large、SwinUNETR和VNet等多种机器学习模型在CT和PET图像分割领域取得了最先进的结果。研究引入了综合评估指标，综合考虑了Dice分数、假阳性体积和假阴性体积，并讨论了相关的计算挑战和解决方案。研究还探索了预处理和后处理技术来进一步优化分割输出，为医学图像分割领域提供了宝贵的见解。 |
| [^49] | [Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?.](http://arxiv.org/abs/2309.12632) | 深度学习在CT扫描分类中的结果往往只关注准确性，而忽视了公正性和解释性，导致模型不可信和不适用于真实场景。 |
| [^50] | [Sequential Action-Induced Invariant Representation for Reinforcement Learning.](http://arxiv.org/abs/2309.12628) | 本文提出了一种称为Sequential Action--Induced Invariant Representation (SAR)的方法，通过将包含任务关键信号的动作序列纳入表示学习，解决了从视觉干扰的高维观测中准确学习与任务相关的状态表示的问题。 |
| [^51] | [Data-driven Preference Learning Methods for Multiple Criteria Sorting with Temporal Criteria.](http://arxiv.org/abs/2309.12620) | 本研究提出了一种新的基于偏好学习的方法，用于解决存在时间准则的多准则排序问题。首先，提出了一个凸二次规划模型，并引入正则化框架。其次，设计了一种集成学习算法，用于合并多个优化器的输出。为了提高可扩展性和适应可学习的时间折扣因子，引入了一种新颖的单调循环神经网络(mRNN)。这些方法有效地处理了时间序列数据，并保持了多准则排序问题的关键属性。 |
| [^52] | [Zero-Regret Performative Prediction Under Inequality Constraints.](http://arxiv.org/abs/2309.12618) | 本文研究了一种零后悔的不平等约束下的演化性预测框架，该框架在实际学习问题中具有重要应用。通过开发稳健的原始-对偶框架，可以在考虑数据分布演化性的情况下找到最优解。 |
| [^53] | [Multiply Robust Federated Estimation of Targeted Average Treatment Effects.](http://arxiv.org/abs/2309.12600) | 本研究提出了一种多样性鲁棒性联邦方法，用于通过多中心数据进行合理的因果推断，解决了数据隐私保护和个体协变量分布异质性的挑战，并展示了在效率和鲁棒性方面相对于现有方法的有限样本优势。 |
| [^54] | [Improving Machine Learning Robustness via Adversarial Training.](http://arxiv.org/abs/2309.12593) | 本文通过对抗训练在集中式和分布式环境中研究了机器学习的鲁棒性，取得了较现有研究更好的效果。 |
| [^55] | [Sampling-Frequency-Independent Universal Sound Separation.](http://arxiv.org/abs/2309.12581) | 本文提出了一种采样频率无关的通用声音分离方法，能够处理未经训练的采样频率，为实现普适的源分离器提供了关键技术。 |
| [^56] | [SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling.](http://arxiv.org/abs/2309.12578) | 本文提出了一种新颖的Transformer稀疏化方案，通过集成卷积滤波器和泛滥填充方法，高效地实现了注意力操作的逐层稀疏模式，减少了计算复杂度和内存占用。 |
| [^57] | [Classification of Alzheimers Disease with Deep Learning on Eye-tracking Data.](http://arxiv.org/abs/2309.12574) | 该论文研究了使用深度学习模型在原始眼动数据上进行阿尔茨海默病分类的方法，称为VTNet。研究表明，VTNet在AD分类任务中的性能优于现有方法，显示了该模型从眼动数据进行预测的潜力。 |
| [^58] | [Interpretable 3D Multi-Modal Residual Convolutional Neural Network for Mild Traumatic Brain Injury Diagnosis.](http://arxiv.org/abs/2309.12572) | 这项研究提出了一种可解释的3D多模态残差卷积神经网络（MRCNN）用于轻度颅脑损伤（mTBI）诊断，结合遮蔽敏感度图（OSM）进行增强。与传统的CT图像诊断相比，MRCNN模型在特异度和准确率上都有显著提升。 |
| [^59] | [Invariant Learning via Probability of Sufficient and Necessary Causes.](http://arxiv.org/abs/2309.12559) | 本研究通过引入充分因素和必要因素的概率（PNS）来改善在未知测试分布上的泛化问题，以解决现有方法主要关注因果性的不变性属性而忽视充分性和必要性条件的问题。 |
| [^60] | [Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation.](http://arxiv.org/abs/2309.12545) | 本文提出了一种名为PROPLACE的方法，通过鲁棒优化技术为神经网络提供可证明的鲁棒和可信的反事实解释，解决了现有方法在保持鲁棒性的同时生成不合理解释的问题。 |
| [^61] | [Trip Planning for Autonomous Vehicles with Wireless Data Transfer Needs Using Reinforcement Learning.](http://arxiv.org/abs/2309.12534) | 本文研究了无线数据传输需求的自动驾驶车辆行程规划问题，在城市区域中考虑了驾驶时间和数据传输需求，并使用强化学习方法进行解决。 |
| [^62] | [Confidence Calibration for Systems with Cascaded Predictive Modules.](http://arxiv.org/abs/2309.12510) | 本研究提出了一种针对级联预测模块系统的置信度校准方法，通过利用模块级别的验证数据，解决了现有算法在多模块系统中无法提供可靠预测的问题。 |
| [^63] | [A Diffusion-Model of Joint Interactive Navigation.](http://arxiv.org/abs/2309.12508) | 本文提出了一种基于扩散模型的方法 DJINN，用于生成交通场景。通过联合扩散所有代理的轨迹，并以灵活的状态观察为条件，我们在轨迹预测上取得了最先进的性能。此外，DJINN还能灵活地从多种有价值的条件分布中进行测试时抽样。 |
| [^64] | [Knowledge Graph Embedding: An Overview.](http://arxiv.org/abs/2309.12501) | 该论文综述了知识图谱嵌入的研究状态，介绍了两个主要分支：基于距离和基于语义匹配的方法。还讨论了CompoundE和CompoundE3D模型，并揭示了一个潜在的研究趋势。 |
| [^65] | [User-Level Differential Privacy With Few Examples Per User.](http://arxiv.org/abs/2309.12500) | 本论文研究了用户级差分隐私在少量示例的情况下的应用。对于近似差分隐私，提供了一种转换方法来将项级差分隐私算法转换为用户级差分隐私算法，可以在保持相同效用的前提下减少所需的用户数量。对于纯差分隐私，提出了一种简单的技术来适应指数机制。 |
| [^66] | [Evidential uncertainties on rich labels for active learning.](http://arxiv.org/abs/2309.12494) | 本文提出了两种策略来应对主动学习中的不确定性问题，即采用Klir不确定性采样和证据学派不确定性采样，在考虑到标签中已存在的不确定性的基础上，对模型的不确定性进行分解和处理。 |
| [^67] | [Sharpness-Aware Minimization and the Edge of Stability.](http://arxiv.org/abs/2309.12488) | 本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。 |
| [^68] | [Studying and improving reasoning in humans and machines.](http://arxiv.org/abs/2309.12485) | 本研究通过对大型语言模型（LLM）和人类的推理能力进行比较研究，发现LLM在推理中存在类似于人类启发式推理的错误，但与人类推理有重要差异，最新的LLM版本几乎消除了模型的限制。此外，人类和机器对相同的提示方案的反应不同。这些结果对我们的认识论有重大影响。 |
| [^69] | [Robust Energy Consumption Prediction with a Missing Value-Resilient Metaheuristic-based Neural Network in Mobile App Development.](http://arxiv.org/abs/2309.12484) | 本研究提出了一种基于元启发式方法增强的神经网络框架，旨在实现在移动应用开发中稳健的能耗预测。 |
| [^70] | [State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding.](http://arxiv.org/abs/2309.12482) | 本论文致力于开发一种基于概念的解释方法，旨在提高非AI专家对AI决策的理解。通过定义顺序决策设置中的“概念”以及探索基于概念的解释对RL agent学习效果和最终用户对agent决策理解的双重好处，我们提出了一个统一的框架。 |
| [^71] | [Impact of architecture on robustness and interpretability of multispectral deep neural networks.](http://arxiv.org/abs/2309.12463) | 这项工作研究了不同融合策略对多光谱深度学习模型性能，依赖性和稳健性的影响。 |
| [^72] | [Multimodal Deep Learning for Scientific Imaging Interpretation.](http://arxiv.org/abs/2309.12460) | 本研究提出了一种多模态深度学习框架，通过模拟人类与扫描电子显微镜图像的交互，利用文本和视觉数据进行精细数据合成和评估。该模型（GlassLLaVA）能够准确解释、识别关键特征和检测以前未见的SEM图像中的缺陷，同时引入了适用于多种科学成像应用的灵活评估指标。 |
| [^73] | [A Theory of Multimodal Learning.](http://arxiv.org/abs/2309.12458) | 这篇论文提供了一个理论框架来解释多模态学习中的一个有趣发现，即在单模态任务上，训练在多个模态上的模型可以胜过经过精细调节的单模态模型。 |
| [^74] | [LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation.](http://arxiv.org/abs/2309.12455) | LongDocFACTScore是一种评估长文档生成摘要实证性的评估框架，可以解决传统自动评估度量标准无法评估长文档摘要事实一致性的问题。 |
| [^75] | [A Convex Framework for Confounding Robust Inference.](http://arxiv.org/abs/2309.12450) | 本文提出了一个支撑鲁棒推断的凸框架，通过利用凸规划提供策略价值的精确下界。此外，该方法还可以进行多种扩展，并且具有强理论保证。 |
| [^76] | [Ensemble Neural Networks for Remaining Useful Life (RUL) Prediction.](http://arxiv.org/abs/2309.12445) | 提出了一种使用集成神经网络的方法进行概率性剩余寿命预测，该方法解耦了来自系统和模型参数的不确定性，并且可以准确地建模和解释预测的信心。 |
| [^77] | [Change Management using Generative Modeling on Digital Twins.](http://arxiv.org/abs/2309.12421) | 本论文展示了如何利用生成建模在数字孪生中进行变更管理，为小型和中型企业提供了安全地管理软件更新和变更的解决方案。 |
| [^78] | [Speeding up Resnet Architecture with Layers Targeted Low Rank Decomposition.](http://arxiv.org/abs/2309.12412) | 本研究通过针对网络进行低秩分解的压缩方法，加速了Resnet架构的训练和推断，通过对ResNet50的研究案例，证明了硬件目标压缩的优势。 |
| [^79] | [Memory Efficient Mixed-Precision Optimizers.](http://arxiv.org/abs/2309.12381) | 本论文提出了一种内存高效的混合精度优化算法，通过消除参数的浮点副本和去除梯度值的方法，实现了高效降低内存使用和加快训练速度，同时保持模型准确性。 |
| [^80] | [Methods for generating and evaluating synthetic longitudinal patient data: a systematic review.](http://arxiv.org/abs/2309.12380) | 本文对生成和评估合成纵向患者数据的方法进行了系统综述，以解决医学领域中数据使用和隐私保护的问题。 |
| [^81] | [Shedding Light on the Ageing of Extra Virgin Olive Oil: Probing the Impact of Temperature with Fluorescence Spectroscopy and Machine Learning Techniques.](http://arxiv.org/abs/2309.12377) | 本研究使用荧光光谱和机器学习技术探究了温度对特级初榨橄榄油的老化影响，并提出了一种基于机器学习的方法来评估油质量。 |
| [^82] | [Fairness Hub Technical Briefs: AUC Gap.](http://arxiv.org/abs/2309.12371) | 本论文介绍了一种称为AUC Gap的指标，它可以测量AI/ML模型在不同子群体中的性能差异，从而实现非二元的公平评估，为实现共同目标提供了基准和策略分享的基础。 |
| [^83] | [Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis.](http://arxiv.org/abs/2309.12368) | 本研究探索了在复杂医疗决策中重新思考人工智能与人类合作的设计要求，以脓毒症诊断为例。研究发现，在人工智能系统中，支持临床专家在决策过程的中间阶段发挥作用（如生成假设或收集数据）是至关重要的，而不仅仅关注最终决策。 |
| [^84] | [Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors.](http://arxiv.org/abs/2309.12367) | 本文研究了在基于GPT的智能辅导系统中将领域知识库与语言模型集成，以提高回答的可靠性。通过设计可扩展的知识库和评估实验，我们展示了该系统的有效性。学生和领域专家对于智能辅导系统的回答进行了验证和排名。 |
| [^85] | [Efficient Social Choice via NLP and Sampling.](http://arxiv.org/abs/2309.12360) | 本文通过结合自然语言处理和抽样技术，提出了一种高效的注意力感知社会选择系统，该系统使用训练有素的NLP模型估计了提案通过的概率，并通过采样多数来决定提案。 |
| [^86] | [Antagonising explanation and revealing bias directly through sequencing and multimodal inference.](http://arxiv.org/abs/2309.12345) | 本研究通过测序和多模态推理，对抗解释并直接揭示偏见，探讨了利用生成模型进行未来计算的潜力和应用，特别是在电影和视听艺术领域。研究发现，通过承认记录的存在，将过去与未来紧密联系起来，可以更好地实现生成模型的效果。 |
| [^87] | [Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions.](http://arxiv.org/abs/2309.12342) | 本研究提出了一种使用霍夫斯泰德文化维度框架来量化大型语言模型与不同文化之间的对齐程度的文化对齐测试（CAT）。通过在不同文化国家应用该方法，我们发现LLMs在解释性文化维度上存在差异，并能量化其与特定国家的文化对齐情况。 |
| [^88] | [Deep Knowledge Tracing is an implicit dynamic multidimensional item response theory model.](http://arxiv.org/abs/2309.12334) | 本文将深度知识追踪视为一种编码器-解码器结构，通过在多个数据集上进行实验发现，一个更简单的解码器可以比DKT预测学生表现更好。 |
| [^89] | [Onchain Sports Betting using UBET Automated Market Maker.](http://arxiv.org/abs/2309.12333) | 本文介绍了一种使用UBET自动市场制造者进行链上体育博彩的方法，通过此方法可以解决传统中心化平台的缺点，确保透明度、安全性和较低的费用。这种方法利用智能合约和算法来定价体育赔率，提供流动性并实现全球可访问性。 |
| [^90] | [Mono/Multi-material Characterization Using Hyperspectral Images and Multi-Block Non-Negative Matrix Factorization.](http://arxiv.org/abs/2309.12329) | 使用高光谱图像和多块非负矩阵分解方法进行单一/多物质的表征，在塑料分类中具有重要的应用价值。 |
| [^91] | [FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare.](http://arxiv.org/abs/2309.12325) | FUTURE-AI是第一个国际共识框架，为医疗保健领域的可信AI工具开发和部署提供指导原则和最佳实践。 |
| [^92] | [Aviation Safety Risk Analysis and Flight Technology Assessment Issues.](http://arxiv.org/abs/2309.12324) | 该论文研究了中国民航业中飞行安全的重要性，并提出了解决超额事件分析不充分原因的方法。通过数据处理、可靠性评估、神经网络飞行控制、数据分析、机器学习飞行人员评估和实时警报等手段，旨在提升航空安全、人员评估和警报机制。 |
| [^93] | [Evaluating the diversity and utility of materials proposed by generative models.](http://arxiv.org/abs/2309.12323) | 本研究评估了物理引导的晶体生成模型（PGCGM）的多样性和实用性，并提出了改进生成模型以实现更好逆向设计的建议。 |
| [^94] | [The Topology and Geometry of Neural Representations.](http://arxiv.org/abs/2309.11028) | 本文探索了从几何结构到拓扑结构的抽象步骤，并提出了一种拓扑表征相似分析方法（tRSA），通过一系列地理拓扑摘要统计量对大脑表征进行表征。 |
| [^95] | [Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification.](http://arxiv.org/abs/2309.09976) | Des-q是一种量子算法，用于在回归和二分类任务中构建和重新训练决策树。它显著减少了树重新训练所需的时间复杂度，并且能够处理新样本的加载时间。该算法通过 k 分段线性树分裂来构建决策树，将数据划分为不同的子空间。 |
| [^96] | [FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning.](http://arxiv.org/abs/2309.08420) | 提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。 |
| [^97] | [A Real-time Faint Space Debris Detector With Learning-based LCM.](http://arxiv.org/abs/2309.08244) | 本文提出了一种基于学习的方法来解决低强度和高角速度下的微弱空间碎片探测问题。该方法使用局部对比和最大似然估计相结合的方式，能够高效地检测信噪比为2.0的空间物体。 |
| [^98] | [Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition.](http://arxiv.org/abs/2309.07988) | 本论文提出了一种名为折叠注意力的技术，在基于Transformer的流式语音识别模型中，通过减少线性投影层的数量，显著减小了模型大小，提高了内存和功耗效率，实验证明可以将模型大小减小24%、功耗减小23%。 |
| [^99] | [Virchow: A Million-Slide Digital Pathology Foundation Model.](http://arxiv.org/abs/2309.07778) | Virchow是一个数百万参数的深度神经网络基础模型，通过在数百万张全数字病理学切片图像上进行自监督学习训练，有效解决了计算病理学任务中数据不足的问题，并在多个下游任务上超越了最先进的系统。 |
| [^100] | [Online Infinite-Dimensional Regression: Learning Linear Operators.](http://arxiv.org/abs/2309.06548) | 在这篇论文中，我们研究了在线设置下学习无限维线性算子的问题。我们证明了在一定的条件下，线性算子是可以在线学习的，而在另一些条件下则不可以。我们还证明了在线均一收敛和学习能力之间的分离，并在PAC设置下得到了相同的结果。 |
| [^101] | [Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models.](http://arxiv.org/abs/2309.06375) | 建模推荐系统生态系统需要考虑参与者激励、行为以及策略引发的相互作用，通过强化学习进行长期优化，使用社会选择方法进行权衡，并减少信息不对称。 |
| [^102] | [Computation and Communication Efficient Federated Learning over Wireless Networks.](http://arxiv.org/abs/2309.01816) | 提出了一种计算和通信高效的无线网络联合学习框架，通过模型剪枝和个性化，在分布式学习中减少计算和通信延迟，并提高非独立同分布数据设备的学习准确度。 |
| [^103] | [Contextual Biasing of Named-Entities with Large Language Models.](http://arxiv.org/abs/2309.00723) | 本文研究了使用大型语言模型进行上下文偏倚的方法，通过在第二次打分时提供额外的上下文信息，以提高自动语音识别性能。我们利用提示信息对大型语言模型进行boosting，并采用多任务训练以预测实体类别和下一个标记。此外，我们提出了动态提示方法来提高效率。 |
| [^104] | [Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm.](http://arxiv.org/abs/2308.16775) | 这项研究提出了一种新的方法，通过深度学习进行零样本架构搜索，通过使用可学习的傅里叶正弦和求和编码来构建计算的前馈图，从而解决了基于预测的神经架构搜索中性能指标泛化的限制。 |
| [^105] | [3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking.](http://arxiv.org/abs/2308.15316) | 3D-MuPPET是一个用于估计和跟踪多只鸽子三维姿势的框架，通过多视角实时推测2D关键点并将其三角化到3D空间，同时使用动态匹配和2D跟踪器维持对应关系。相比最先进的3D姿势估计器，具有可比的准确性。该框架还能在使用单只鸽子数据训练的情况下应用于多只鸽子数据，简化领域转换。 |
| [^106] | [Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective.](http://arxiv.org/abs/2308.13985) | 本论文重新审视了多任务学习中的标量化方法，并从理论的角度探讨了标量化是否能够充分探索帕累托前沿。结果显示，与最近的研究声称的经验优势相反，标量化本质上无法进行全面探索，特别是对于那些平衡了paren |
| [^107] | [BridgeData V2: A Dataset for Robot Learning at Scale.](http://arxiv.org/abs/2308.12952) | BridgeData V2是一个大规模且多样化的机器人操作行为数据集，广泛应用于机器人学习研究。该数据集具备任务和环境变异性，并且兼容多种学习方法，通过实验表明，使用此数据集可以提高模型性能。 |
| [^108] | [Easy attention: A simple self-attention mechanism for Transformers.](http://arxiv.org/abs/2308.12874) | 本论文提出了一种名为简易注意力的注意力机制，用于提高Transformer神经网络在混沌系统时间动态预测中的鲁棒性。该方法不依赖于键、查询和softmax，直接将注意力得分作为可学习参数。实验结果表明，该方法在重构和预测混沌系统的时间动态方面比传统的自注意机制和长短期记忆方法更具鲁棒性和简化性。 |
| [^109] | [Improving Generative Model-based Unfolding with Schr\"{o}dinger Bridges.](http://arxiv.org/abs/2308.12351) | 本研究提出了一种使用Schrödinger桥和扩散模型创建的展开方法SBUnfold，它将判别模型和生成模型的优势结合起来。与最先进方法相比，在合成的Z+jets数据集上获得了卓越的性能。 |
| [^110] | [Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference.](http://arxiv.org/abs/2308.12066) | 预门控MoE系统通过算法和系统的共同设计，有效解决了传统MoE架构的计算和存储挑战。 |
| [^111] | [Deep learning-based flow disaggregation for hydropower plant management.](http://arxiv.org/abs/2308.11631) | 本研究提出了一种基于深度学习的时间序列分解模型，用于将每日流量分解为每小时流量，并在挪威某流量测站的数据上进行了测试，初步结果显示了该模型的一些有希望的方面。 |
| [^112] | [ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations.](http://arxiv.org/abs/2308.10457) | ALI-DPFL是一种进行差分隐私联邦学习的算法，通过自适应本地迭代来优化性能，并在实验中展示了显著的改进。 |
| [^113] | [Causal Interpretable Progression Trajectory Analysis of Chronic Disease.](http://arxiv.org/abs/2308.09735) | 提出了一种名为因果轨迹预测（CTP）的新模型，通过将轨迹预测和因果发现相结合，准确预测慢性疾病的进展轨迹并揭示特征间的因果关系。 |
| [^114] | [Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models.](http://arxiv.org/abs/2308.07706) | 本论文提出使用视觉-语言模型进行医学图像分割的迁移学习，并评估了其在医学领域的可迁移性。通过捕捉语义信息和引入新的图像描述变化，实现了对多样化医学图像的分割。 |
| [^115] | [Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting.](http://arxiv.org/abs/2307.15299) | 本研究使用差分进化算法选择Transformer神经网络模型的优化超参数，以提高负荷预测的准确性。 |
| [^116] | [MiVOLO: Multi-input Transformer for Age and Gender Estimation.](http://arxiv.org/abs/2307.04616) | 本论文提出了一种简单的方法MiVOLO，使用最新的视觉变换器进行年龄和性别估计。该方法将面部信息和人物图像数据集成到一个统一的模型中，提高了模型的泛化能力，并在图像中人脸不可见的情况下仍能提供令人满意的结果。实验证明了该方法在四个基准测试上达到了最先进的性能，并具有实时处理能力。 |
| [^117] | [FITS: Modeling Time Series with $10k$ Parameters.](http://arxiv.org/abs/2307.03756) | FITS是一种轻量而强大的时间序列分析模型，通过在复杂频率域中进行插值操作，丢弃对时间序列数据影响微小的高频分量，实现了与最先进模型相当的性能，并且具有较小的模型参数数量，适用于边缘设备。 |
| [^118] | [Elastic Decision Transformer.](http://arxiv.org/abs/2307.02484) | 弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。 |
| [^119] | [Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning.](http://arxiv.org/abs/2307.01849) | 本论文提出了一种名为Crossway Diffusion的方法，通过使用自监督学习目标增强了基于扩散的视觉机器人策略学习，实验证明了其在各种机器人任务中的有效性和优势。 |
| [^120] | [Environmental effects on emergent strategy in micro-scale multi-agent reinforcement learning.](http://arxiv.org/abs/2307.00994) | 本研究使用基于粒子的动力学模拟研究了微观环境中温度对多智能体强化学习系统中策略形成和有效性的影响。实验结果显示，在较高温度下，强化学习智能体能够发展出新的策略，为解决微观尺度控制问题提供了洞见。 |
| [^121] | [The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement.](http://arxiv.org/abs/2306.09633) | 谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。 |
| [^122] | [TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models.](http://arxiv.org/abs/2306.08013) | 本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。 |
| [^123] | [Urban Spatiotemporal Data Synthesis via Neural Disaggregation.](http://arxiv.org/abs/2306.07292) | 本研究提出了一种基于神经网络的城市时空数据合成方法，旨在通过分解粗糙的低分辨率地理单元的聚合城市数据来合成细粒度，高分辨率的城市数据，以增加高度聚合的城市数据的可用性和实现价值。 |
| [^124] | [ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer.](http://arxiv.org/abs/2306.06446) | ShiftAddViT通过使用位移和加法等多种乘法原语对ViT进行重新参数化，实现了减少乘法操作的高效视觉变换器模型，可以在GPU上实现端到端的推理加速，无需从头训练。 |
| [^125] | [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.](http://arxiv.org/abs/2306.03341) | 本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。 |
| [^126] | [Disentanglement via Latent Quantization.](http://arxiv.org/abs/2305.18378) | 本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。 |
| [^127] | [Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation.](http://arxiv.org/abs/2305.17558) | 本论文提出了两种基于虚拟粒子随机逼近的可证速限制变种的SVGD算法，具有可证速的有限粒子收敛率。 |
| [^128] | [Incentivizing Honesty among Competitors in Collaborative Learning and Optimization.](http://arxiv.org/abs/2305.16272) | 这项研究提出了一个模型来描述在协作学习中竞争对手的不诚实行为，提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。 |
| [^129] | [Strategic Data Sharing between Competitors.](http://arxiv.org/abs/2305.16052) | 这项研究介绍了竞争对手之间战略数据共享的问题，提出了一个分析框架，研究了市场条件对数据共享激励的影响。 |
| [^130] | [Learning high-level visual representations from a child's perspective without strong inductive biases.](http://arxiv.org/abs/2305.15372) | 通过儿童的视觉经验，我们在没有引入强归纳偏差的情况下训练了最先进的神经网络模型，并成功学习了广泛的语义类别和对象定位能力。嵌入模型表现为ImageNet模型的70%水平，尽管训练数据存在差异。 |
| [^131] | [SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models.](http://arxiv.org/abs/2305.11281) | 本文提出了一种名为SlotDiffusion的对象中心潜在扩散模型，它具有强大的建模能力，能够提高物体中心槽到图像解码的质量，超越了先前的槽模型。 |
| [^132] | [How to Index Item IDs for Recommendation Foundation Models.](http://arxiv.org/abs/2305.06569) | 本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。 |
| [^133] | [Few-shot Link Prediction on N-ary Facts.](http://arxiv.org/abs/2305.06104) | 本文提出了一个新任务——少样本N-元事实链接预测，并提出了一个名为FLEN的模型来实现。FLEN由三个模块组成，可以从有限的标记实例中预测N-元事实中的缺失实体。 |
| [^134] | [Maximizing Model Generalization for Manufacturing with Self-Supervised Learning and Federated Learning.](http://arxiv.org/abs/2304.14398) | 本研究提出了一种利用自我监督学习和联邦学习提高制造业模型的泛化能力，在处理未标记的和有限的状况监测数据以及领域移位时具有较好的效果，同时通过联邦学习保护了数据隐私和提高了计算效率。 |
| [^135] | [Differentiable graph-structured models for inverse design of lattice materials.](http://arxiv.org/abs/2304.05422) | 本文提出了一种使用图形表示结构和属性的晶格材料的计算方法，使用可微分传递算法计算机械属性以实现反向设计，进而实现了可扩展的、具有前所未有的结构和功能多样性的晶格材料设计。 |
| [^136] | [Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing.](http://arxiv.org/abs/2303.15585) | 本文通过对IMWUT期刊上过去五年发表的论文进行系统回顾，发现UbiComp社区在算法公平方面的进展滞后，存在敏感属性偏差导致的歧视性结果，需要探索报告数据集的信息以解决这些偏差。 |
| [^137] | [Optimization Dynamics of Equivariant and Augmented Neural Networks.](http://arxiv.org/abs/2303.13458) | 本论文研究了在对称数据上优化多层感知机的方法，比较了等变和增强两种策略的优缺点，证明了在自然假设下等变稳定点的集合和等变层的集合具有不变性，但增强模型的稳定点可能是不稳定的。 |
| [^138] | [Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement.](http://arxiv.org/abs/2303.08983) | 提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性、鲁棒性和校准性。例如，使用ImageDataNet+训练的ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。 |
| [^139] | [Synthetic Experience Replay.](http://arxiv.org/abs/2303.06614) | 本文提出了合成经验回放方法解决深度强化学习中数据匮乏问题，通过巧妙应用生成建模技术来扩充数据效果显著。 |
| [^140] | [Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks.](http://arxiv.org/abs/2303.01682) | Neural-BO是一种使用神经网络模型的黑盒优化算法，避免了高斯过程中的缩放和维数问题，具有高效收敛的特性。 |
| [^141] | [Deep Imbalanced Time-series Forecasting via Local Discrepancy Density.](http://arxiv.org/abs/2302.13563) | 该论文提出了一种基于局部差异密度的重新加权框架，用于解决时间序列预测中的深度不平衡问题。这个框架通过降低突发变化引起的损失，增加正常状态引起的损失，使模型能够学习可推广的模式。 |
| [^142] | [Red Teaming Deep Neural Networks with Feature Synthesis Tools.](http://arxiv.org/abs/2302.10894) | 本文提出了一个用于评估可解释性工具的基准，通过训练模型以对特定触发器产生特定输出的方式，可以解决传统可解释性方法无法分析未知特征行为的问题。 |
| [^143] | [Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved Confounders.](http://arxiv.org/abs/2302.00662) | 提出了一种在顺序外源未观察到的混淆因素下的强健策略评估和优化方法，使用正交化的强健Fitted-Q迭代，并添加了分位数估计的偏差校正。 |
| [^144] | [Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics.](http://arxiv.org/abs/2302.00600) | 本文通过训练扩散生成模型来学习粗粒化分子动力学的力场，无需使用任何力场输入，从而实现了对生物过程的精确模拟，并展示了在多个蛋白质模拟中具有优越性能。 |
| [^145] | [Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence.](http://arxiv.org/abs/2301.11476) | 这篇论文通过研究广义的Tsallis KL散度，扩展了Munchausen强化学习算法，并提供了一种将KL正则化纳入实际算法的方法。对于Tsallis KL，当$q > 1$时，可以获得新的策略优化选项。 |
| [^146] | [A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference.](http://arxiv.org/abs/2212.12393) | 本文介绍了一种名为A-NeSI的新颖PNL框架，它使用神经网络实现了近似推理，能够保证概率逻辑语义的同时解决了PNL的可扩展性问题，能够在安全关键应用中保证逻辑约束的满足。 |
| [^147] | [Mean Shift Mask Transformer for Unseen Object Instance Segmentation.](http://arxiv.org/abs/2211.11679) | 本文提出了一种新的均值漂移掩模变换器，用于联合训练和推断特征提取器和聚类器，可用于未知物体实例分割，在COCO数据集上表现出竞争性的性能，并在罕见和未知物体类别上具有显着优势。 |
| [^148] | ["Task-relevant autoencoding" enhances machine learning for human neuroscience.](http://arxiv.org/abs/2208.08478) | "任务相关的自编码"（TRACE）是一种用于人类神经科学的机器学习方法，可以从人类神经影像数据中提取与受试者行为相关的低维表示，并在分类准确率和发现能力上超过了其他常用模型。 |
| [^149] | [Self-Supervised Training with Autoencoders for Visual Anomaly Detection.](http://arxiv.org/abs/2206.11723) | 本文提出了一种自监督学习方法，通过修改重构误差的方式集中在数据流形上，从而解决深度卷积自编码器在视觉异常检测中容易出现的重构异常信号而导致检测效果不佳的问题。 |
| [^150] | [Quantitative Gaussian Approximation of Randomly Initialized Deep Neural Networks.](http://arxiv.org/abs/2203.07379) | 本论文通过量化高斯逼近的方法，研究了随机初始化的深度神经网络的输出分布与高斯过程的二次Wasserstein距离的上界，揭示了隐藏层和输出层大小对网络高斯行为的影响，并定量地恢复了在宽限制下的分布收敛结果。 |
| [^151] | [Pixyz: a Python library for developing deep generative models.](http://arxiv.org/abs/2107.13109) | Pixyz是一个用于开发深度生成模型的Python库，通过封装深度神经网络为概率分布以及基于目标函数的模型设计和学习，可以更简洁直观地实现各种DGMs。此外，该库还引入了记忆化技术加快计算速度，并在实验证明在训练DGMs时比现有的概率编程语言更快。 |
| [^152] | [Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation.](http://arxiv.org/abs/2012.02420) | 该论文构建了一个名为MedLane的新数据集，支持自动临床语言简化方法的开发和评估。提出了一种叫做DECLARE的新模型，与八种基准模型相比取得了最先进的性能，并提出了三个特定的评估指标。 |
| [^153] | [Learning neutrino effects in Cosmology with Convolutional Neural Networks.](http://arxiv.org/abs/1910.04255) | 该论文使用基于深度学习网络的新方法快速生成包含大质量中微子的模拟数据，并且通过计算多个相关统计量验证了其准确性。 |
| [^154] | [Probabilistic Contraction Analysis of Iterated Random Operators.](http://arxiv.org/abs/1804.01195) | 本文提出了一种新的概率收缩分析方法，用于证明由迭代随机算子产生的马尔可夫链在特定条件下的收敛性，为理解各种蒙特卡洛方法的收敛性提供了一个通用框架。 |

# 详细

[^1]: MosaicFusion: 将扩散模型作为大词汇实例分割的数据增强器

    MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation. (arXiv:2309.13042v1 [cs.CV])

    [http://arxiv.org/abs/2309.13042](http://arxiv.org/abs/2309.13042)

    MosaicFusion是一种用于大词汇实例分割的数据增强方法，通过将扩散模型作为数据集生成器，能够生成大量合成标记数据。在实验中，我们的方法在准确率和泛化能力方面取得了显著的提升。

    

    我们提出了MosaicFusion，一种简单而有效的基于扩散的数据增强方法，用于大词汇实例分割。我们的方法无需训练，也不依赖于任何标签监督。两个关键设计使我们能够将现成的文本到图像扩散模型作为有用的数据集生成器，用于对象实例和蒙版注释。首先，我们将图像画布分为几个区域，并执行一轮扩散过程，同时基于不同的文本提示生成多个实例。其次，我们通过聚合与对象提示相关联的跨注意力图在层和扩散时间步上，然后进行简单的阈值处理和边缘感知的细化处理，得到相应的实例蒙版。我们的MosaicFusion可以为稀缺和新颖类别产生大量的合成标记数据，而无需复杂的处理。在具有挑战性的LVIS长尾和开放词汇基准上进行的实验结果表明，我们的方法在准确率和泛化能力方面均取得了显著的提升。

    We present MosaicFusion, a simple yet effective diffusion-based data augmentation approach for large vocabulary instance segmentation. Our method is training-free and does not rely on any label supervision. Two key designs enable us to employ an off-the-shelf text-to-image diffusion model as a useful dataset generator for object instances and mask annotations. First, we divide an image canvas into several regions and perform a single round of diffusion process to generate multiple instances simultaneously, conditioning on different text prompts. Second, we obtain corresponding instance masks by aggregating cross-attention maps associated with object prompts across layers and diffusion time steps, followed by simple thresholding and edge-aware refinement processing. Without bells and whistles, our MosaicFusion can produce a significant amount of synthetic labeled data for both rare and novel categories. Experimental results on the challenging LVIS long-tailed and open-vocabulary benchma
    
[^2]: 通过价值函数预训练从网络视频中进行机器人离线强化学习

    Robotic Offline RL from Internet Videos via Value-Function Pre-Training. (arXiv:2309.13041v1 [cs.RO])

    [http://arxiv.org/abs/2309.13041](http://arxiv.org/abs/2309.13041)

    本文提出了一种通过时间差分学习从大规模视频数据集中利用机器人离线强化学习的系统，该系统被称为V-PTR。与其他学习视频数据方法相比，通过价值学习可以更好地表示机器人离线强化学习的数据。

    

    在许多现代机器学习系统中，通过在互联网数据上进行预训练已被证明是广泛推广的关键因素。那么，在机器人强化学习中启用这样的能力需要什么？离线强化学习方法利用机器人经验数据集进行学习，这是一种利用先前数据进入机器人学习流程的方式。然而，这些方法与视频数据（如Ego4D）存在"类型不匹配"，而视频数据是机器人学习的最大先前数据集，因为视频提供仅有观察经验，没有强化学习方法所需的动作或奖励注释。在本文中，我们开发了一个系统，通过时间差分学习完全基于学习价值函数，从大规模人类视频数据集中利用机器人离线强化学习。我们展示了与其他从视频数据学习的方法相比，价值学习对于下游机器人离线强化学习更有利的表示学习。我们的系统被称为V-PTR，将预训练的好处与离线强化学习相结合。

    Pre-training on Internet data has proven to be a key ingredient for broad generalization in many modern ML systems. What would it take to enable such capabilities in robotic reinforcement learning (RL)? Offline RL methods, which learn from datasets of robot experience, offer one way to leverage prior data into the robotic learning pipeline. However, these methods have a "type mismatch" with video data (such as Ego4D), the largest prior datasets available for robotics, since video offers observation-only experience without the action or reward annotations needed for RL methods. In this paper, we develop a system for leveraging large-scale human video datasets in robotic offline RL, based entirely on learning value functions via temporal-difference learning. We show that value learning on video datasets learns representations that are more conducive to downstream robotic offline RL than other approaches for learning from video data. Our system, called V-PTR, combines the benefits of pre-
    
[^3]: 增强记忆的Conformer用于改进端到端长篇ASR

    Memory-augmented conformer for improved end-to-end long-form ASR. (arXiv:2309.13029v1 [eess.AS])

    [http://arxiv.org/abs/2309.13029](http://arxiv.org/abs/2309.13029)

    该论文提出了一种将记忆增强神经网络添加到Conformer模型中以解决长话语情况下性能下降的问题。实验结果表明，该系统在长话语上优于基准Conformer模型。

    

    Conformer最近被提出作为自动语音识别（ASR）中一种有前景的建模方法，优于循环神经网络和Transformer。然而，总体而言，这些端到端模型（尤其是注意力机制的模型）在长话语的情况下性能明显下降。为了解决这个局限性，我们建议在Conformer的编码器和解码器之间添加一个完全可微分的记忆增强神经网络。这个外部记忆可以增强对长话语的泛化能力，因为它允许系统循环地存储和检索更多的信息。值得注意的是，我们探索了神经图灵机（NTM），从而提出了我们的Conformer-NTM模型架构用于ASR。使用Librispeech的train-clean-100和train-960数据集进行的实验结果表明，对于长话语，所提出的系统优于没有记忆的基准Conformer。

    Conformers have recently been proposed as a promising modelling approach for automatic speech recognition (ASR), outperforming recurrent neural network-based approaches and transformers. Nevertheless, in general, the performance of these end-to-end models, especially attention-based models, is particularly degraded in the case of long utterances. To address this limitation, we propose adding a fully-differentiable memory-augmented neural network between the encoder and decoder of a conformer. This external memory can enrich the generalization for longer utterances since it allows the system to store and retrieve more information recurrently. Notably, we explore the neural Turing machine (NTM) that results in our proposed Conformer-NTM model architecture for ASR. Experimental results using Librispeech train-clean-100 and train-960 sets show that the proposed system outperforms the baseline conformer without memory for long utterances.
    
[^4]: 基于图神经网络的均匀加载下加固板应力预测

    Graph Neural Network for Stress Predictions in Stiffened Panels Under Uniform Loading. (arXiv:2309.13022v1 [cs.LG])

    [http://arxiv.org/abs/2309.13022](http://arxiv.org/abs/2309.13022)

    该研究提出了一种用于高效表示三维加固板的图嵌入技术，并采用图神经网络方法预测不同几何形状下加固板的应力分布。通过对比有限元-顶点图表示，证明了所提方法的有效性。

    

    机器学习和深度学习技术已经成为代价较高的结构分析方法的降阶模型，在计算上获得了显著的关注，比如有限元分析。图神经网络是一种特殊类型的神经网络，它可以处理可以表示为图的数据。这使得在结构或产品的概念设计过程中，可以有效地表示复杂的几何形状变化。本研究提出了一种用于高效表示三维加固板的新型图嵌入技术，通过将单独的板域作为顶点进行考虑。采用图采样和聚合 (GraphSAGE) 方法预测不同几何形状下加固板的应力分布，并进行了有限元-顶点图表示的对比以证明所提方法的有效性。进行了全面的参数研究，以了解参数对预测结果的影响。

    Machine learning (ML) and deep learning (DL) techniques have gained significant attention as reduced order models (ROMs) to computationally expensive structural analysis methods, such as finite element analysis (FEA). Graph neural network (GNN) is a particular type of neural network which processes data that can be represented as graphs. This allows for efficient representation of complex geometries that can change during conceptual design of a structure or a product. In this study, we propose a novel graph embedding technique for efficient representation of 3D stiffened panels by considering separate plate domains as vertices. This approach is considered using Graph Sampling and Aggregation (GraphSAGE) to predict stress distributions in stiffened panels with varying geometries. A comparison between a finite-element-vertex graph representation is conducted to demonstrate the effectiveness of the proposed approach. A comprehensive parametric study is performed to examine the effect of s
    
[^5]: 基于深度学习的混合方法用于优化基于环境的基因型选择

    A Hybrid Deep Learning-based Approach for Optimal Genotype by Environment Selection. (arXiv:2309.13021v1 [cs.LG])

    [http://arxiv.org/abs/2309.13021](http://arxiv.org/abs/2309.13021)

    该论文提出了一种基于深度学习的混合方法，用于优化基于环境的基因型选择。通过整合不同作物品种的天气数据，特别是在不同气候条件下，准确地预测作物产量对于理解其适应性至关重要。论文中开发了两种新颖的卷积神经网络架构，并利用广义集成方法确定了最优的基因型与环境选择模型。

    

    准确的作物产量预测对于改善农业实践和确保作物在不同气候条件下的韧性至关重要。在MLCAS2021作物产量预测挑战中，我们利用包含93,028个训练记录的数据集，预测了10,337个测试记录的产量，在13年（2003-2015年）的时间范围内覆盖了美国28个州和加拿大省份的159个地点。该数据集包括5,838个不同基因型的详细信息和为期214天的生长季节的每日天气数据，使得综合分析成为可能。作为获胜团队之一，我们开发了两种新颖的卷积神经网络（CNN）架构：CNN-DNN模型，结合了CNN和全连接网络；以及CNN-LSTM-DNN模型，加入了LSTM层用于天气变量。利用广义集成方法（GEM），我们确定了最优的基因型与环境选择模型。

    Precise crop yield prediction is essential for improving agricultural practices and ensuring crop resilience in varying climates. Integrating weather data across the growing season, especially for different crop varieties, is crucial for understanding their adaptability in the face of climate change. In the MLCAS2021 Crop Yield Prediction Challenge, we utilized a dataset comprising 93,028 training records to forecast yields for 10,337 test records, covering 159 locations across 28 U.S. states and Canadian provinces over 13 years (2003-2015). This dataset included details on 5,838 distinct genotypes and daily weather data for a 214-day growing season, enabling comprehensive analysis. As one of the winning teams, we developed two novel convolutional neural network (CNN) architectures: the CNN-DNN model, combining CNN and fully-connected networks, and the CNN-LSTM-DNN model, with an added LSTM layer for weather variables. Leveraging the Generalized Ensemble Method (GEM), we determined opt
    
[^6]: 通过反演影响函数理解深度梯度泄露

    Understanding Deep Gradient Leakage via Inversion Influence Functions. (arXiv:2309.13016v1 [cs.LG])

    [http://arxiv.org/abs/2309.13016](http://arxiv.org/abs/2309.13016)

    本文提出了一种新的方法I²F，可以有效近似深度梯度泄露攻击，并建立了恢复图像和私有梯度之间的连接。通过这个方法，我们能够更好地理解和应对深度梯度泄露攻击。

    

    深度梯度泄露（DGL）是一种非常有效的攻击方法，可以从梯度向量中恢复私有训练图像。这种攻击对于具有敏感数据的客户端分布式学习提出了重要的隐私挑战，其中客户端需要共享梯度。防御此类攻击需要但缺乏对隐私泄露发生的时间和方式的理解，主要是因为深度网络的黑盒特性。在本文中，我们提出了一种新颖的反演影响函数（I²F），通过隐式解决DGL问题，建立了恢复图像和私有梯度之间的闭式连接。与直接解决DGL相比，I²F在分析深度网络时具有可扩展性，仅需要梯度和雅可比向量乘积的预言访问。我们通过实验证明，I²F在不同的模型架构、数据集、攻击实现和基于噪声的防御中都能有效近似DGL。我们通过这种新颖的工具，能够更好地了解深度梯度泄露的机理和应对方法。

    Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products. We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, attack implementations, and noise-based defenses. With this novel tool, we 
    
[^7]: 高效的N:M稀疏DNN训练使用算法，架构和数据流协同设计

    Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design. (arXiv:2309.13015v1 [cs.LG])

    [http://arxiv.org/abs/2309.13015](http://arxiv.org/abs/2309.13015)

    本文提出了一种使用算法，架构和数据流协同设计的高效N:M稀疏DNN训练方案，该方案利用双向权重修剪方法优化计算成本，并通过稀疏加速器硬件支持实现高稀疏比的DNN训练。

    

    稀疏训练是减少DNN计算成本同时保持高准确性的一种有前景的技术。特别是N:M细粒度结构稀疏，其中只有连续M个元素中的N个可以是非零值，因其对硬件友好的模式和达到高稀疏比的能力而受到关注。然而，加速N:M稀疏DNN训练的潜力尚未充分利用，并且缺乏支持N:M稀疏训练的高效硬件。为了解决这些挑战，本文提出了一种使用算法，架构和数据流协同设计的计算高效的N:M稀疏DNN训练方案。在算法层面上，提出了一种双向权重修剪方法（BDWP），利用DNN训练的前向和后向传播过程中权重的N:M稀疏性，可以显著降低计算成本同时保持模型准确性。在架构层面上，提出了一种用于DNN稀疏加速的架构，该架构基于数据流协议，利用稀疏权重的结构模式优化计算性能。

    Sparse training is one of the promising techniques to reduce the computational cost of DNNs while retaining high accuracy. In particular, N:M fine-grained structured sparsity, where only N out of consecutive M elements can be nonzero, has attracted attention due to its hardware-friendly pattern and capability of achieving a high sparse ratio. However, the potential to accelerate N:M sparse DNN training has not been fully exploited, and there is a lack of efficient hardware supporting N:M sparse training. To tackle these challenges, this paper presents a computation-efficient training scheme for N:M sparse DNNs using algorithm, architecture, and dataflow co-design. At the algorithm level, a bidirectional weight pruning method, dubbed BDWP, is proposed to leverage the N:M sparsity of weights during both forward and backward passes of DNN training, which can significantly reduce the computational cost while maintaining model accuracy. At the architecture level, a sparse accelerator for DN
    
[^8]: ReConcile：圆桌会议通过多元LLM的共识改进推理能力

    ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])

    [http://arxiv.org/abs/2309.13007](http://arxiv.org/abs/2309.13007)

    ReConcile是一个通过多轮讨论和投票机制来增强LLM推理能力的多模型多代理框架。

    

    大型语言模型（LLM）仍然在复杂的推理任务上遇到困难。受到心智社会理论（Minsky, 1988）的启发，我们提出了ReConcile，这是一个多模型多代理的框架，旨在通过多样的LLM代理人之间的圆桌会议来促进多样的思想和讨论，从而改进一致性。ReConcile通过进行多轮讨论、学习说服其他代理人改进答案以及采用置信度加权投票机制来增强LLM的推理能力。在每一轮中，ReConcile通过“讨论提示”来启动代理人间的讨论，其中包括上一轮每个代理人生成的答案和解释的分组、它们的不确定性以及用于说服其他代理人的答案修正人类解释的演示。这个讨论提示使每个代理人能够根据其他代理人的见解修订自己的回答。一旦达成一致并结束讨论，ReConcile执行一次全体投票以确定最终答案。

    Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
    
[^9]: 通过跨域序列自编码器实现反事实公平性追求

    Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains. (arXiv:2309.13005v1 [cs.LG])

    [http://arxiv.org/abs/2309.13005](http://arxiv.org/abs/2309.13005)

    通过顺序自编码器实现反事实公平性追求的创新框架将环境信息和敏感属性与分类特征的嵌入表示分开，以提高模型在不同和陌生领域中的泛化能力，并解决公平问题。

    

    鉴于领域转移在机器学习中普遍存在的挑战，已经开发出各种领域泛化（DG）技术，以提高处理分布外（OOD）数据时机器学习系统的性能。此外，在实际场景中，数据分布可能会在连续的序列领域中逐渐变化。虽然当前的方法主要集中在改进在这些新领域内的模型效果，但往往忽视了学习过程中的公平问题。为此，我们引入了一种创新的框架，称为具有顺序自编码器的反事实公平性感知领域泛化（CDSAE）。这种方法有效地将环境信息和敏感属性与分类特征的嵌入表示分开。这种并行分离不仅极大地提高了模型在不同和陌生的领域中的泛化能力，还有效地解决了相关挑战。

    Recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges rela
    
[^10]: 表达性变分量子电路在联邦学习中提供固有隐私

    Expressive variational quantum circuits provide inherent privacy in federated learning. (arXiv:2309.13002v1 [quant-ph])

    [http://arxiv.org/abs/2309.13002](http://arxiv.org/abs/2309.13002)

    表达性变分量子电路模型在联邦学习中提供固有隐私保护，同时通过使用过度参数化保证模型可训练性。通过解决高次多元切比雪夫多项式方程的复杂性，实现对梯度反转攻击的困难性。

    

    联邦学习已经成为一种可行的分布式解决方案，可以在不与中央聚合器共享数据的情况下训练机器学习模型。然而，已经显示出标准的基于神经网络的联邦学习模型容易受到与服务器共享的梯度的数据泄露攻击。在这项工作中，我们介绍了使用表达性编码映射和过度参数化ans\"tze构建的变分量子电路模型的联邦学习。我们证明了表达性映射导致对梯度反转攻击具有固有的隐私保护，而过度参数化确保了模型的可训练性。我们的隐私框架集中在通过量子电路梯度生成的高次多元切比雪夫多项式的解决复杂性上。我们提出了令人信服的论点，强调在精确和近似情况下解决这些方程的固有困难性。此外，我们深入探讨了基于机器学习的攻击策略。

    Federated learning has emerged as a viable distributed solution to train machine learning models without the actual need to share data with the central aggregator. However, standard neural network-based federated learning models have been shown to be susceptible to data leakage from the gradients shared with the server. In this work, we introduce federated learning with variational quantum circuit model built using expressive encoding maps coupled with overparameterized ans\"atze. We show that expressive maps lead to inherent privacy against gradient inversion attacks, while overparameterization ensures model trainability. Our privacy framework centers on the complexity of solving the system of high-degree multivariate Chebyshev polynomials generated by the gradients of quantum circuit. We present compelling arguments highlighting the inherent difficulty in solving these equations, both in exact and approximate scenarios. Additionally, we delve into machine learning-based attack strate
    
[^11]: 点云网络：线性层参数数量的数量级改进

    Point Cloud Network: An Order of Magnitude Improvement in Linear Layer Parameter Count. (arXiv:2309.12996v1 [cs.LG])

    [http://arxiv.org/abs/2309.12996](http://arxiv.org/abs/2309.12996)

    本文提出了点云网络（PCN）架构，通过对线性层的改进，在保持与原始架构相当的测试准确性的同时，大幅度减少了参数数量。

    

    本文介绍了点云网络（PCN）架构，这是一种深度学习网络中线性层的新型实现，并提供了实证证据来支持其在线性层中优于多层感知机（MLP）的偏好。我们使用MLP和PCN架构训练了几个模型，包括原始的AlexNet，以直接比较线性层。我们收集的关键结果是模型参数数量和在CIFAR-10和CIFAR-100数据集上的top-1测试准确性。我们的PCN等效于AlexNet的AlexNet-PCN16，在其线性层中参数减少99.5%的情况下，实现了与原始架构相当的效果（测试准确性）。所有训练都是在云RTX 4090 GPU上进行的，利用pytorch进行模型构建和训练。提供了代码供任何人复制本文中的实验。

    This paper introduces the Point Cloud Network (PCN) architecture, a novel implementation of linear layers in deep learning networks, and provides empirical evidence to advocate for its preference over the Multilayer Perceptron (MLP) in linear layers. We train several models, including the original AlexNet, using both MLP and PCN architectures for direct comparison of linear layers (Krizhevsky et al., 2012). The key results collected are model parameter count and top-1 test accuracy over the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009). AlexNet-PCN16, our PCN equivalent to AlexNet, achieves comparable efficacy (test accuracy) to the original architecture with a 99.5% reduction of parameters in its linear layers. All training is done on cloud RTX 4090 GPUs, leveraging pytorch for model construction and training. Code is provided for anyone to reproduce the trials from this paper.
    
[^12]: 深度学习在活性物质中的概率流和熵产生速率研究

    Deep learning probability flows and entropy production rates in active matter. (arXiv:2309.12991v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2309.12991](http://arxiv.org/abs/2309.12991)

    本论文通过深度学习方法，结合生成模型，提出了一个估计活性物质系统熵产生速率和概率流大小的框架，解决了传统计算方法中高维概率密度的问题，并给出了直接计算这些物理量的方法。

    

    主动物质系统，从自驱胶体到运动的细菌，其特点是将自由能转化为微观尺度的有效工作。这些系统通常涉及到超出平衡统计力学范畴的物理学，理解它们的非平衡态质性一直是一个挑战。熵产生速率和稳态概率流的大小提供了量化的方式，通过测量时间反演对称性的破缺和非平衡输运强度来理解它们。然而，它们的高效计算一直受到限制，因为它们依赖于系统的未知和高维概率密度。基于最近的生成模型进展，我们在这里开发了一个深度学习框架来估计这种密度的分数。我们展示了这种分数以及微观运动方程可以直接获取熵产生速率，其概率流大小等信息。

    Active matter systems, from self-propelled colloids to motile bacteria, are characterized by the conversion of free energy into useful work at the microscopic scale. These systems generically involve physics beyond the reach of equilibrium statistical mechanics, and a persistent challenge has been to understand the nature of their nonequilibrium states. The entropy production rate and the magnitude of the steady-state probability current provide quantitative ways to do so by measuring the breakdown of time-reversal symmetry and the strength of nonequilibrium transport of measure. Yet, their efficient computation has remained elusive, as they depend on the system's unknown and high-dimensional probability density. Here, building upon recent advances in generative modeling, we develop a deep learning framework that estimates the score of this density. We show that the score, together with the microscopic equations of motion, gives direct access to the entropy production rate, the probabi
    
[^13]: 基于花瓣拉普拉斯在简单复合体上的高阶图卷积网络

    Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes. (arXiv:2309.12971v1 [cs.LG])

    [http://arxiv.org/abs/2309.12971](http://arxiv.org/abs/2309.12971)

    本文提出了基于花瓣拉普拉斯的高阶图卷积网络，通过利用简单复合体来建模高阶交互，在不同拓扑尺度上识别内在特征，并使用可学习的图滤波器来量化高阶交互强度。

    

    尽管普通图神经网络（GNNs）在许多任务上取得了成功，但其基于配对交互网络的基础本质上限制了其识别复杂系统中潜在高阶交互的能力。为了弥补这种能力差距，我们提出了一种新颖的方法，利用复杂系统的高阶交互建模的丰富数学理论，即简单复合体（SCs）-一种对建模高阶交互具有鲁棒性的工具。目前基于SC的GNNs存在复杂度高和刻板的问题，并且量化高阶交互强度仍然具有挑战性。创新地，我们提出了一个高阶花瓣（FP）模型，将FP拉普拉斯引入到SC中。此外，我们引入了一个以FP拉普拉斯为基础的高阶图卷积网络（HiGCN），能够识别不同拓扑尺度上的内在特征。通过使用可学习的图滤波器，FP拉普拉斯域内的参数组，我们可以识别出具有不同模式的图案，其中滤波器的权重用作数量化高阶交互的工具。

    Despite the recent successes of vanilla Graph Neural Networks (GNNs) on many tasks, their foundation on pairwise interaction networks inherently limits their capacity to discern latent higher-order interactions in complex systems. To bridge this capability gap, we propose a novel approach exploiting the rich mathematical theory of simplicial complexes (SCs) - a robust tool for modeling higher-order interactions. Current SC-based GNNs are burdened by high complexity and rigidity, and quantifying higher-order interaction strengths remains challenging. Innovatively, we present a higher-order Flower-Petals (FP) model, incorporating FP Laplacians into SCs. Further, we introduce a Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians, capable of discerning intrinsic features across varying topological scales. By employing learnable graph filters, a parameter group within each FP Laplacian domain, we can identify diverse patterns where the filters' weights serve as a quan
    
[^14]: 自监督变形器中的分别归一化

    On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v1 [cs.CL])

    [http://arxiv.org/abs/2309.12931](http://arxiv.org/abs/2309.12931)

    在自监督变形器中，通过为标记和[CLS]符号分别使用归一化层，可以更好地捕捉它们各自的特点并提高下游任务的性能。

    

    自监督变形器的训练方法在各个领域展现了显著的性能。以往的基于变形器的模型（如遮蔽自编码器）通常会为[CLS]符号和标记使用单独的归一化层。我们在本文中提出了一种简单的修改，为标记和[CLS]符号分别使用归一化层，以更好地捕捉它们各自的特点并增强下游任务的性能。我们的方法旨在缓解将相同的归一化统计数据应用于两种标记类型可能带来的负面效果，这些统计数据可能无法与它们各自的角色最佳匹配。通过使用单独的归一化层，我们经验证明[CLS]嵌入能够更好地编码全局语境信息，并在其非各向同性空间中分布更均匀。当用这两个单独的归一化层替换常规的归一化层时，我们观察到平均性能提升了2.7%。

    Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average 2.7% performa
    
[^15]: BayesDLL: 贝叶斯深度学习库

    BayesDLL: Bayesian Deep Learning Library. (arXiv:2309.12928v1 [cs.LG])

    [http://arxiv.org/abs/2309.12928](http://arxiv.org/abs/2309.12928)

    BayesDLL是一个用于PyTorch的贝叶斯深度学习库，与其他现有库相比，它可以处理非常大规模的深度网络，无需修改用户代码，并且可以使用预训练模型权重作为先验均值，适用于贝叶斯推断。

    

    我们发布了一个新的用于PyTorch的贝叶斯神经网络库，用于大规模深度网络。我们的库实现了主流的近似贝叶斯推断算法：变分推断、MC-dropout、随机梯度MCMC和拉普拉斯近似。与其他现有的贝叶斯神经网络库相比，我们的库有以下主要区别：1）我们的库可以处理包括视觉变换器（ViTs）在内的非常大规模的深度网络。2）用户几乎不需要修改代码（例如，骨干网络定义代码根本不需要修改）。3）我们的库还允许预训练模型权重作为先验均值，这对于使用仅仅依靠下游数据难以从头开始优化的大规模基础模型（如ViTs）进行贝叶斯推断非常有用。我们的代码公开可用于: \url{https://github.com/SamsungLabs/BayesDLL}（备用存储库也可在此处找到：\url{https://github.com/miny})

    We release a new Bayesian neural network library for PyTorch for large-scale deep networks. Our library implements mainstream approximate Bayesian inference algorithms: variational inference, MC-dropout, stochastic-gradient MCMC, and Laplace approximation. The main differences from other existing Bayesian neural network libraries are as follows: 1) Our library can deal with very large-scale deep networks including Vision Transformers (ViTs). 2) We need virtually zero code modifications for users (e.g., the backbone network definition codes do not neet to be modified at all). 3) Our library also allows the pre-trained model weights to serve as a prior mean, which is very useful for performing Bayesian inference with the large-scale foundation models like ViTs that are hard to optimise from scratch with the downstream data alone. Our code is publicly available at: \url{https://github.com/SamsungLabs/BayesDLL}\footnote{A mirror repository is also available at: \url{https://github.com/miny
    
[^16]: 态度问题：关注积极和主动梯度来提升显著图

    A matter of attitude: Focusing on positive and active gradients to boost saliency maps. (arXiv:2309.12913v1 [cs.AI])

    [http://arxiv.org/abs/2309.12913](http://arxiv.org/abs/2309.12913)

    本文研究了在显著图中考虑梯度符号和影响的作用，揭示了更好地识别卷积神经网络关注的图像像素的方法，并阐明了遮挡或改变这些像素会如何影响结果。

    

    由于其简单性和提供的见解质量，显著图已成为卷积神经网络（CNN）最广泛使用的可解释性技术之一。然而，对于这些见解是否可信仍存在一些疑问，是否真正代表CNN用于预测的信息。本文探讨了如何通过从显著图中挽救梯度的符号来更深入地理解多类别分类问题。通过使用预训练和从头开始训练的CNN，我们揭示了考虑正确类别的符号和影响，以及其他类别的影响，可以更好地识别网络真正关注的图像像素。此外，遮挡或改变这些像素预计会如何影响结果也变得更加清晰。

    Saliency maps have become one of the most widely used interpretability techniques for convolutional neural networks (CNN) due to their simplicity and the quality of the insights they provide. However, there are still some doubts about whether these insights are a trustworthy representation of what CNNs use to come up with their predictions. This paper explores how rescuing the sign of the gradients from the saliency map can lead to a deeper understanding of multi-class classification problems. Using both pretrained and trained from scratch CNNs we unveil that considering the sign and the effect not only of the correct class, but also the influence of the other classes, allows to better identify the pixels of the image that the network is really focusing on. Furthermore, how occluding or altering those pixels is expected to affect the outcome also becomes clearer.
    
[^17]: 使用稀疏学习构建可解释的图神经网络用于药物-蛋白质结合预测

    Building explainable graph neural network by sparse learning for the drug-protein binding prediction. (arXiv:2309.12906v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.12906](http://arxiv.org/abs/2309.12906)

    本研究提出了一种稀疏学习的图神经网络（SLGNN），通过使用基于化学亚结构的图来表示药物分子，并结合广义融合套索和消息传递算法以识别对药物-蛋白质结合预测至关重要的连接亚图。

    

    可解释的图神经网络（GNN）已经被开发并应用于药物-蛋白质结合预测，以识别与目标蛋白质有活跃相互作用的药物中的关键化学结构。然而，当前可解释的GNN模型所识别出的关键结构通常是化学上无效的。此外，需要手动选择一个阈值来从其他结构中确定关键结构。为了克服当前可解释的GNN模型的局限性，我们提出了SLGNN，即使用稀疏学习来构建图神经网络。我们的SLGNN依赖于使用基于化学亚结构的图（其中节点是化学亚结构）来表示药物分子。此外，SLGNN将广义融合套索与消息传递算法相结合，以识别对药物-蛋白质结合预测至关重要的连接亚图。由于使用了基于化学亚结构的图，因此保证了药物中的任何亚图可以由化学亚结构组成。

    Explainable Graph Neural Networks (GNNs) have been developed and applied to drug-protein binding prediction to identify the key chemical structures in a drug that have active interactions with the target proteins. However, the key structures identified by the current explainable GNN models are typically chemically invalid. Furthermore, a threshold needs to be manually selected to pinpoint the key structures from the rest. To overcome the limitations of the current explainable GNN models, we propose our SLGNN, which stands for using Sparse Learning to Graph Neural Networks. Our SLGNN relies on using a chemical-substructure-based graph (where nodes are chemical substructures) to represent a drug molecule. Furthermore, SLGNN incorporates generalized fussed lasso with message-passing algorithms to identify connected subgraphs that are critical for the drug-protein binding prediction. Due to the use of the chemical-substructure-based graph, it is guaranteed that any subgraphs in a drug iden
    
[^18]: FairComp：关于普适计算中机器学习的公平性和鲁棒性研讨会

    FairComp: Workshop on Fairness and Robustness in Machine Learning for Ubiquitous Computing. (arXiv:2309.12877v1 [cs.CY])

    [http://arxiv.org/abs/2309.12877](http://arxiv.org/abs/2309.12877)

    本研讨会旨在讨论普适计算研究中的公平性及其社会、技术和法律影响，通过社会角度、技术角度和法律角度来探究公正的途径，并为该领域的未来研究规划出明确的路径。

    

    我们如何确保普适计算（UbiComp）的研究成果既具有道德，又具有公平性？尽管机器学习中的公平性近年来引起了人们的关注，但在普适计算中的公平性仍未被探索。本次研讨会旨在讨论普适计算研究中的公平性及其社会、技术和法律影响。从社会角度来看，我们将研究公平性与普适计算研究之间的关系，并确定确保普适技术不会造成伤害或侵犯个人权利的途径。从技术角度出发，我们将发起关于数据实践的讨论，以开发适合普适计算研究的偏见消减方法。从法律角度来看，我们将研究新政策如何塑造我们社区的工作和未来研究。我们旨在建立一个关于负责任普适计算主题的充满活力的社区，同时为未来的研究努力在这一领域中规划出明确的路径。

    How can we ensure that Ubiquitous Computing (UbiComp) research outcomes are both ethical and fair? While fairness in machine learning (ML) has gained traction in recent years, fairness in UbiComp remains unexplored. This workshop aims to discuss fairness in UbiComp research and its social, technical, and legal implications. From a social perspective, we will examine the relationship between fairness and UbiComp research and identify pathways to ensure that ubiquitous technologies do not cause harm or infringe on individual rights. From a technical perspective, we will initiate a discussion on data practices to develop bias mitigation approaches tailored to UbiComp research. From a legal perspective, we will examine how new policies shape our community's work and future research. We aim to foster a vibrant community centered around the topic of responsible UbiComp, while also charting a clear path for future research endeavours in this field.
    
[^19]: 角度优化的文本嵌入

    AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])

    [http://arxiv.org/abs/2309.12871](http://arxiv.org/abs/2309.12871)

    本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。

    

    高质量的文本嵌入对于提升语义文本相似度（STS）任务至关重要，而这些任务又是大型语言模型（LLM）应用中的关键组成部分。然而，现有的文本嵌入模型面临的一个普遍挑战是渐变消失问题，主要是由于它们在优化目标中依赖余弦函数，而余弦函数具有饱和区域。为了解决这个问题，本文提出了一种称为AnglE的新型角度优化文本嵌入模型。AnglE的核心思想是在一个复杂空间中引入角度优化。这种新颖的方法有效地缓解了余弦函数饱和区域产生的不利影响，从而可以阻碍梯度并阻碍优化过程。为了建立全面的STS评估，我们在现有的短文本STS数据集和从GitHub Issues中新收集的长文本STS数据集上进行了实验。此外，我们还研究了具有有限标签数据的特定领域STS场景，并探讨了AnglE的工作原理。

    High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
    
[^20]: 关联变换器是一种稀疏表示学习器

    Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v1 [cs.LG])

    [http://arxiv.org/abs/2309.12862](http://arxiv.org/abs/2309.12862)

    关联变换器（AiT）是一种采用低秩显式记忆和关联记忆的稀疏表示学习器，通过联合端到端训练实现模块特化和注意力瓶颈的形成。

    

    在传统的Transformer模型中，出现了一种新兴的基于稀疏交互的注意力机制，这种机制与生物原理更为接近。包括Set Transformer和Perceiver在内的方法采用了与有限能力的潜在空间相结合的交叉注意力机制。基于最近对全局工作空间理论和关联记忆的神经科学研究，我们提出了关联变换器（AiT）。AiT引入了低秩显式记忆，既可以作为先验来指导共享工作空间的瓶颈注意力，又可以作为关联记忆的吸引子。通过联合端到端训练，这些先验自然地发展出模块的特化，每个模块对形成注意力瓶颈的归纳偏好有所贡献。瓶颈可以促进输入之间为将信息写入内存而进行竞争。我们展示了AiT是一种稀疏表示学习器。

    Emerging from the monolithic pairwise attention mechanism in conventional Transformer models, there is a growing interest in leveraging sparse interactions that align more closely with biological principles. Approaches including the Set Transformer and the Perceiver employ cross-attention consolidated with a latent space that forms an attention bottleneck with limited capacity. Building upon recent neuroscience studies of Global Workspace Theory and associative memory, we propose the Associative Transformer (AiT). AiT induces low-rank explicit memory that serves as both priors to guide bottleneck attention in the shared workspace and attractors within associative memory of a Hopfield network. Through joint end-to-end training, these priors naturally develop module specialization, each contributing a distinct inductive bias to form attention bottlenecks. A bottleneck can foster competition among inputs for writing information into the memory. We show that AiT is a sparse representation 
    
[^21]: 机器人从示教中学习处理软性食品对象

    Robotic Handling of Compliant Food Objects by Robust Learning from Demonstration. (arXiv:2309.12856v1 [cs.RO])

    [http://arxiv.org/abs/2309.12856](http://arxiv.org/abs/2309.12856)

    本文介绍了一种机器人从示教中学习处理软性食品对象的稳健学习策略，通过合并RGB-D图像和触觉数据进行抓取。

    

    处理软性和变形的食品原材料的机器人操作目前在海洋空间、农业和食品行业中需求巨大。这些行业中的许多任务是由人工操作员手动完成的，由于任务的繁重和乏味，执行的变化性很大，结果也会有所不同。由于当前的机器人学习策略，引入机器人自动化进行复杂处理任务一直是具有挑战性的。希望能够通过一个更一致的学习策略来解决这个问题，包括熟练的操作员。在本文中，我们解决了机器人在面对不一致示教时的学习问题。为此，我们提出了一种基于示教学习（LfD）的机器人抓取软性食品对象的稳健学习策略。该方法利用RGB-D图像和触觉数据的合并来估计...

    The robotic handling of compliant and deformable food raw materials, characterized by high biological variation, complex geometrical 3D shapes, and mechanical structures and texture, is currently in huge demand in the ocean space, agricultural, and food industries. Many tasks in these industries are performed manually by human operators who, due to the laborious and tedious nature of their tasks, exhibit high variability in execution, with variable outcomes. The introduction of robotic automation for most complex processing tasks has been challenging due to current robot learning policies. A more consistent learning policy involving skilled operators is desired. In this paper, we address the problem of robot learning when presented with inconsistent demonstrations. To this end, we propose a robust learning policy based on Learning from Demonstration (LfD) for robotic grasping of food compliant objects. The approach uses a merging of RGB-D images and tactile data in order to estimate th
    
[^22]: 跨模态翻译与对齐技术在生存分析中的应用

    Cross-Modal Translation and Alignment for Survival Analysis. (arXiv:2309.12855v1 [eess.IV])

    [http://arxiv.org/abs/2309.12855](http://arxiv.org/abs/2309.12855)

    本论文提出了一种跨模态翻译与对齐 (CMTA) 框架，用于将病理图像和基因组特征结合起来进行生存分析，通过探索内在的跨模态相关性和传递互补信息，提高了生存分析预测的准确性和效果。

    

    随着高通量测序技术的快速发展，生存分析的重点已经从研究临床指标转移到将基因组特征与病理图像结合起来。然而，现有方法要么直接将病理特征和基因组特征融合进行生存预测，要么将基因组特征作为指导用于整合病理图像的特征。前者会忽视内在的跨模态相关性，而后者会丢弃与基因表达无关的病理信息。为了解决这些问题，我们提出了一种跨模态翻译与对齐 (CMTA) 框架，用于探索内在的跨模态相关性并传递潜在的互补信息。具体而言，我们构建了两个并行的编码器-解码器结构，用于处理多模态数据，整合模态内信息并生成跨模态表示。利用生成的跨模态表示来增强生存分析预测任务。

    With the rapid advances in high-throughput sequencing technologies, the focus of survival analysis has shifted from examining clinical indicators to incorporating genomic profiles with pathological images. However, existing methods either directly adopt a straightforward fusion of pathological features and genomic profiles for survival prediction, or take genomic profiles as guidance to integrate the features of pathological images. The former would overlook intrinsic cross-modal correlations. The latter would discard pathological information irrelevant to gene expression. To address these issues, we present a Cross-Modal Translation and Alignment (CMTA) framework to explore the intrinsic cross-modal correlations and transfer potential complementary information. Specifically, we construct two parallel encoder-decoder structures for multi-modal data to integrate intra-modal information and generate cross-modal representation. Taking the generated cross-modal representation to enhance an
    
[^23]: DeepOPF-U: 一种统一的深度神经网络用于解决多网络中的交流最优功率流问题

    DeepOPF-U: A Unified Deep Neural Network to Solve AC Optimal Power Flow in Multiple Networks. (arXiv:2309.12849v1 [cs.LG])

    [http://arxiv.org/abs/2309.12849](http://arxiv.org/abs/2309.12849)

    DeepOPF-U 是一种统一的深度神经网络，用于解决不同电力网络中的交流最优功率流问题，具有较强的泛化性和性能优势。

    

    传统的机器学习模型解决最优功率流问题多用于给定的电力网络，并且缺乏对当前具有不同拓扑和日益增长的即插即用分布式能源资源的电力网络的泛化性。本文提出了DeepOPF-U，它使用一个统一的深度神经网络来解决不同电力网络中的交流最优功率流问题，包括一个不断扩展的电力网络集合。具体而言，我们为不同网络中给定负荷和最优功率流解的向量设计了弹性输入层和输出层。所提出的方法使用单个统一的深度神经网络，可以处理不同和不断增长的母线、线路、负荷和分布式能源资源的数量。IEEE 57/118/300母线测试系统的仿真以及一个从73个母线增长到118个母线的网络验证了DeepOPF-U相比已有的基于深度神经网络的解决方法的改进性能。

    The traditional machine learning models to solve optimal power flow (OPF) are mostly trained for a given power network and lack generalizability to today's power networks with varying topologies and growing plug-and-play distributed energy resources (DERs). In this paper, we propose DeepOPF-U, which uses one unified deep neural network (DNN) to solve alternating-current (AC) OPF problems in different power networks, including a set of power networks that is successively expanding. Specifically, we design elastic input and output layers for the vectors of given loads and OPF solutions with varying lengths in different networks. The proposed method, using a single unified DNN, can deal with different and growing numbers of buses, lines, loads, and DERs. Simulations of IEEE 57/118/300-bus test systems and a network growing from 73 to 118 buses verify the improved performance of DeepOPF-U compared to existing DNN-based solution methods.
    
[^24]: 基于强化学习的人群模拟中的奖励函数设计

    Reward Function Design for Crowd Simulation via Reinforcement Learning. (arXiv:2309.12841v1 [cs.LG])

    [http://arxiv.org/abs/2309.12841](http://arxiv.org/abs/2309.12841)

    本论文研究了基于强化学习的人群模拟中的奖励函数设计。通过理论分析和经验评估，我们发现直接最小化能源使用并配合适当的引导势是一种有效的策略，并且不同奖励组成对模拟人群行为具有影响。这些发现对于新的人群模拟技术的开发具有指导意义。

    

    人群模拟对于视频游戏设计非常重要，它可以在虚拟世界中放置具有人类般行为的自主角色。强化学习在模拟虚拟人群方面表现出巨大潜力，但奖励函数的设计对于实现有效和高效的结果至关重要。本研究探讨了基于强化学习的人群模拟中的奖励函数设计。我们提供了理论上关于某些奖励函数的有效性的洞见，并使用能源效率作为指标，在各种场景下进行了经验评估。我们的实验表明，直接最小化能源使用是一个可行的策略，只要它与适当调整的引导势相结合，并且使我们能够研究不同奖励组成对模拟人群行为的影响。我们的发现可以为新的人群模拟技术的发展提供信息。

    Crowd simulation is important for video-games design, since it enables to populate virtual worlds with autonomous avatars that navigate in a human-like manner. Reinforcement learning has shown great potential in simulating virtual crowds, but the design of the reward function is critical to achieving effective and efficient results. In this work, we explore the design of reward functions for reinforcement learning-based crowd simulation. We provide theoretical insights on the validity of certain reward functions according to their analytical properties, and evaluate them empirically using a range of scenarios, using the energy efficiency as the metric. Our experiments show that directly minimizing the energy usage is a viable strategy as long as it is paired with an appropriately scaled guiding potential, and enable us to study the impact of the different reward components on the behavior of the simulated crowd. Our findings can inform the development of new crowd simulation techniques
    
[^25]: AxOCS: 使用配置超采样来扩展基于FPGA的近似运算符

    AxOCS: Scaling FPGA-based Approximate Operators using Configuration Supersampling. (arXiv:2309.12830v1 [cs.AR])

    [http://arxiv.org/abs/2309.12830](http://arxiv.org/abs/2309.12830)

    近似计算作为低成本机器学习实现的解决方案在嵌入式系统中得到广泛研究。其中，设计特定于平台的近似算术运算符成为主要问题，现有方法主要使用基于机器学习的代理函数预测性能和行为影响，缺乏更先进的方法。

    

    AI和基于机器学习的处理在各个应用领域的使用日益增加，加剧了对低成本机器学习实现的需求，特别是对资源受限的嵌入式系统而言。为此，近似计算作为一种探索功耗、性能、面积（PPA）和行为准确性（BEHAV）之间权衡的方法，已经成为在嵌入式机器学习中实现的可能解决方案之一。在机器学习中，由于MAC操作的占主导地位，设计特定于平台的近似算术运算符形成了近似计算中的主要研究问题之一。最近，越来越多的基于AI/ML的设计空间探索技术被用于实现近似运算符。然而，大多数这些方法仅限于使用基于机器学习的代理函数来预测一组相关设计决策的PPA和BEHAV影响。尽管这种方法利用了机器学习方法的回归能力，但没有充分利用更先进的方法。

    The rising usage of AI and ML-based processing across application domains has exacerbated the need for low-cost ML implementation, specifically for resource-constrained embedded systems. To this end, approximate computing, an approach that explores the power, performance, area (PPA), and behavioral accuracy (BEHAV) trade-offs, has emerged as a possible solution for implementing embedded machine learning. Due to the predominance of MAC operations in ML, designing platform-specific approximate arithmetic operators forms one of the major research problems in approximate computing. Recently there has been a rising usage of AI/ML-based design space exploration techniques for implementing approximate operators. However, most of these approaches are limited to using ML-based surrogate functions for predicting the PPA and BEHAV impact of a set of related design decisions. While this approach leverages the regression capabilities of ML methods, it does not exploit the more advanced approaches i
    
[^26]: 合成提升：利用合成数据增强超声心动图中的视觉-语言分割

    Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography. (arXiv:2309.12829v1 [cs.CV])

    [http://arxiv.org/abs/2309.12829](http://arxiv.org/abs/2309.12829)

    本研究探讨了使用合成数据集来增强超声心动图分割的视觉-语言分割模型（VLSM），结果显示合成数据集可以提高分割模型的指标和训练速度。

    

    准确的分割对于基于超声心动图的心血管疾病评估至关重要。然而，超声图像的变异性和固有挑战阻碍了精确的分割。通过利用图像和文本模态的联合表示，视觉-语言分割模型（VLSM）可以融入丰富的上下文信息，可能有助于精确和可解释的分割。然而，超声心动图中缺乏现成的数据阻碍了VLSM的训练。本研究中，我们探讨了使用语义扩散模型（SDM）生成的合成数据集来增强超声心动图分割的VLSM。我们使用从超声心动图图像、分割掩模和元数据中自动提取的多个属性导出的七种不同的语言提示来评估两个流行的VLSM模型（CLIPSeg和CRIS）的结果。我们的结果显示，在预训练VLSM时，转换和收敛速度更快。

    Accurate segmentation is essential for echocardiography-based assessment of cardiovascular diseases (CVDs). However, the variability among sonographers and the inherent challenges of ultrasound images hinder precise segmentation. By leveraging the joint representation of image and text modalities, Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual information, potentially aiding in accurate and explainable segmentation. However, the lack of readily available data in echocardiography hampers the training of VLSMs. In this study, we explore using synthetic datasets from Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography segmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS) using seven different kinds of language prompts derived from several attributes, automatically extracted from echocardiography images, segmentation masks, and their metadata. Our results show improved metrics and faster convergence when pretraining VLSMs
    
[^27]: 连续治疗的双重稳健近端因果学习

    Doubly Robust Proximal Causal Learning for Continuous Treatments. (arXiv:2309.12819v1 [stat.ME])

    [http://arxiv.org/abs/2309.12819](http://arxiv.org/abs/2309.12819)

    本文提出了一种基于核函数的双重稳健近端因果学习方法，用于处理连续治疗，并提出了一种高效求解干扰函数的新方法。

    

    近端因果学习是在存在未测量混淆因素下识别因果效应的有希望的框架。在该框架中，补充稳健（DR）估计器被推导出来，并在估计中展示了其有效性，特别是在模型假设被违反时。然而，当前形式的DR估计器仅限于二进制治疗，而在许多现实世界的应用中，治疗可以是连续的。连续治疗的主要障碍在于在原始DR估计器中存在的delta函数，使得在因果效应估计中不可行，并在干扰函数估计中引入了沉重的计算负担。为了解决这些挑战，我们提出了一种基于核函数的连续治疗的DR估计器，可以很好地处理连续治疗。配备其平滑性，我们展示了其Oracle形式是影响函数的一致近似。此外，我们提出了一种新的方法来高效解决干扰函数。

    Proximal causal learning is a promising framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatment can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We
    
[^28]: 用数据增强改进模仿学习中游戏智能体的泛化能力

    Improving Generalization in Game Agents with Data Augmentation in Imitation Learning. (arXiv:2309.12815v1 [cs.LG])

    [http://arxiv.org/abs/2309.12815](http://arxiv.org/abs/2309.12815)

    本文提出了一种改进模仿学习中游戏智能体泛化能力的方法，通过数据增强技术使训练数据更好地代表真实场景中的状态和行动分布，并在多个3D环境中进行了性能测试，结果表明数据增强可以显著提升模仿学习智能体的泛化能力。

    

    模仿学习是训练游戏智能体和高效游戏生成的有效方法。然而，泛化能力——在相关但未见过的场景中表现出良好性能的能力——对于游戏人工智能来说仍然是一个未解决的挑战。模仿学习智能体的泛型化很困难，因为它需要算法在训练分布之外采取有意义的行动。本文提出了解决这一挑战的方法。在受监督学习中数据增强的成功启发下，我们通过增强训练数据的方法，使数据集中的状态和行动分布更好地代表真实的状态-行动分布。本研究评估了将数据增强应用于观察的方法，以提高模仿学习智能体的泛化能力，并在多个3D环境中对这些增强技术进行了性能基准测试。结果表明，数据增强可以显著提高模仿学习智能体的泛化能力。

    Imitation learning is an effective approach for training game-playing agents and, consequently, for efficient game production. However, generalization - the ability to perform well in related but unseen scenarios - is an essential requirement that remains an unsolved challenge for game AI. Generalization is difficult for imitation learning agents because it requires the algorithm to take meaningful actions outside of the training distribution. In this paper we propose a solution to this challenge. Inspired by the success of data augmentation in supervised learning, we augment the training data so the distribution of states and actions in the dataset better represents the real state-action distribution. This study evaluates methods for combining and applying data augmentations to observations, to improve generalization of imitation learning agents. It also provides a performance benchmark of these augmentations across several 3D environments. These results demonstrate that data augmenta
    
[^29]: 自动测试代码翻译模型的功能性质

    Automatically Testing Functional Properties of Code Translation Models. (arXiv:2309.12813v1 [cs.SE])

    [http://arxiv.org/abs/2309.12813](http://arxiv.org/abs/2309.12813)

    本研究介绍了一种自动、功能性的代码翻译模型测试方法，能够捕捉各种属性从纯语法到纯语义的相关信息，并在实验中证明其有效性。

    

    对于跨编程语言进行代码翻译的大型语言模型，即$transpiling$，正变得日益实用。尽管自动翻译显著提高了开发者的生产力，但一个关键问题是生成的代码是否正确。现有的工作最初使用手工制作的测试套件来测试小规模程序的翻译，后来又将这些测试套件自动化。相反，我们提出了一种用于自动化功能性属性测试的代码翻译模型的方法。我们关于转换后代码的一般用户提供的规范涵盖了一系列属性，从纯语法到纯语义。正如我们的实验所示，这种方法在检测流行的代码翻译模型中的属性违规方面非常有效，因此可以根据给定的属性评估模型质量。我们还进一步探索了用户只需获得

    Large language models are becoming increasingly practical for translating code across programming languages, a process known as $transpiling$. Even though automated transpilation significantly boosts developer productivity, a key concern is whether the generated code is correct. Existing work initially used manually crafted test suites to test the translations of a small corpus of programs; these test suites were later automated. In contrast, we devise the first approach for automated, functional, property-based testing of code translation models. Our general, user-provided specifications about the transpiled code capture a range of properties, from purely syntactic to purely semantic ones. As shown by our experiments, this approach is very effective in detecting property violations in popular code translation models, and therefore, in evaluating model quality with respect to given properties. We also go a step further and explore the usage scenario where a user simply aims to obtain a
    
[^30]: 深度伪造音频作为训练自动语音转文字模型的数据增强技术

    Deepfake audio as a data augmentation technique for training automatic speech to text transcription models. (arXiv:2309.12802v1 [cs.SD])

    [http://arxiv.org/abs/2309.12802](http://arxiv.org/abs/2309.12802)

    本论文提出了一种基于深度伪造音频的数据增强框架，用于训练自动语音转文字模型。通过使用已有的深度伪造和转录模型进行实验，验证了该框架的有效性。

    

    为了训练产生鲁棒结果的转录器模型，需要一个大而多样的标记数据集。找到具备所需特征的这样的数据是一项具有挑战性的任务，特别是对于不如英语流行的语言。此外，生成这样的数据需要大量努力和经费。因此，缓解这个问题的策略是使用数据增强技术。在这项工作中，我们提出了一个基于深度伪造音频的数据增强框架。为了验证产生的框架，我们使用了现有的深度伪造和转录模型进行实验。选择了一个语音克隆器和由印度人（以英语为主）制作的数据集，以确保数据集中只有一个口音。随后，使用增强的数据在各种情景下训练语音到文本模型。

    To train transcriptor models that produce robust results, a large and diverse labeled dataset is required. Finding such data with the necessary characteristics is a challenging task, especially for languages less popular than English. Moreover, producing such data requires significant effort and often money. Therefore, a strategy to mitigate this problem is the use of data augmentation techniques. In this work, we propose a framework that approaches data augmentation based on deepfake audio. To validate the produced framework, experiments were conducted using existing deepfake and transcription models. A voice cloner and a dataset produced by Indians (in English) were selected, ensuring the presence of a single accent in the dataset. Subsequently, the augmented data was used to train speech to text models in various scenarios.
    
[^31]: 基于深度卷积神经网络和无监督方法的离心泵新型故障类别智能检测方法

    An Intelligent Approach to Detecting Novel Fault Classes for Centrifugal Pumps Based on Deep CNNs and Unsupervised Methods. (arXiv:2309.12765v1 [cs.LG])

    [http://arxiv.org/abs/2309.12765](http://arxiv.org/abs/2309.12765)

    本文提出了一种基于深度卷积神经网络和无监督方法的智能检测离心泵新型故障类别的方法，通过部分知识训练神经网络并使用t-SNE方法和聚类技术检测新故障，并通过新数据增强网络，实验证明该方法能够高准确性地检测新故障。

    

    尽管近年来在基于数据驱动的旋转机器故障诊断方面取得了一定的成功，这一领域仍然存在一些挑战。其中一个问题是系统可能在现场遇到的各种故障缺乏信息。本文假设对系统故障具有部分知识，并利用相应数据训练了一个卷积神经网络。然后使用t-SNE方法和聚类技术的组合来检测新的故障。在检测到故障后，利用新数据增强网络。最后，利用一个离心泵的测试设置来验证这个两阶段的方法论，实验结果显示在检测新故障方面具有高准确性。

    Despite the recent success in data-driven fault diagnosis of rotating machines, there are still remaining challenges in this field. Among the issues to be addressed, is the lack of information about variety of faults the system may encounter in the field. In this paper, we assume a partial knowledge of the system faults and use the corresponding data to train a convolutional neural network. A combination of t-SNE method and clustering techniques is then employed to detect novel faults. Upon detection, the network is augmented using the new data. Finally, a test setup is used to validate this two-stage methodology on a centrifugal pump and experimental results show high accuracy in detecting novel faults.
    
[^32]: 对于ConvNets来说，遮盖（masking）能改善对比自监督学习，而显著性告诉你何处。（arXiv:2309.12757v1 [cs.CV]）

    Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where. (arXiv:2309.12757v1 [cs.CV])

    [http://arxiv.org/abs/2309.12757](http://arxiv.org/abs/2309.12757)

    该论文研究了如何将遮盖操作引入卷积神经网络的对比学习框架中，以提高自监督学习的效果。同时，研究还发现遮盖操作可能存在一些副作用，作者提出了解决方案来应对这些问题。

    

    图像数据开始受益于简单而有效的自监督学习方案，该方案建立在遮盖和自重构目标之上，这要归功于令牌化程序和视觉转换器骨干结构的引入。然而，作为图像数据的另一种重要且广泛采用的架构，卷积神经网络，尽管具有驱动自监督学习的对比学习技术，仍然面临将这种直接而通用的遮盖操作显著地利用于其学习过程中的困难。本研究旨在减轻将遮盖操作纳入对比学习框架的负担，作为一种额外的增强方法，以缓解ConvNets中因遮罩操作而产生的额外边缘（遮盖和未遮盖区域之间）以及其他不利影响的问题，这些问题已经在先前的研究中讨论过。

    While image data starts to enjoy the simple-but-effective self-supervised learning scheme built upon masking and self-reconstruction objective thanks to the introduction of tokenization procedure and vision transformer backbone, convolutional neural networks as another important and widely-adopted architecture for image data, though having contrastive-learning techniques to drive the self-supervised learning, still face the difficulty of leveraging such straightforward and general masking operation to benefit their learning process significantly. In this work, we aim to alleviate the burden of including masking operation into the contrastive-learning framework for convolutional neural networks as an extra augmentation method. In addition to the additive but unwanted edges (between masked and unmasked regions) as well as other adverse effects caused by the masking operations for ConvNets, which have been discussed by prior works, we particularly identify the potential problem where for 
    
[^33]: 让UDA中的U变得重要：无监督领域自适应的不变一致性学习

    Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation. (arXiv:2309.12742v1 [cs.LG])

    [http://arxiv.org/abs/2309.12742](http://arxiv.org/abs/2309.12742)

    本文提出了一种名为"不变一致性学习"（ICON）的方法，通过给予源域和目标域相等的地位，学习一个不变的分类器，从而消除了在目标域中的虚假相关性不一致。

    

    领域自适应(DA)在处理领域内部的相关特征（如类别身份）和领域特定特征（如环境）之间的虚假相关性时，总是面临挑战，这种相关性在目标领域中不具有普遍性。不幸的是，即使使用了额外的无监督目标领域，现有的无监督DA(UDA)方法仍然受到影响。这是因为源域监督只将目标域样本视为辅助数据（如通过伪标签），而目标域中有价值的去相关线索的固有分布被忽视了。我们建议通过给予两个域相等的地位，来使UDA中的U变得重要。具体而言，我们学习一个不变的分类器，其预测同时与源域标签和目标域聚类一致，从而消除了目标域中的虚假相关性不一致。我们将我们的方法称为"不变一致性学习"（ICON）。

    Domain Adaptation (DA) is always challenged by the spurious correlation between domain-invariant features (e.g., class identity) and domain-specific features (e.g., environment) that does not generalize to the target domain. Unfortunately, even enriched with additional unsupervised target domains, existing Unsupervised DA (UDA) methods still suffer from it. This is because the source domain supervision only considers the target domain samples as auxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the target domain -- where the valuable de-correlation clues hide -- is disregarded. We propose to make the U in UDA matter by giving equal status to the two domains. Specifically, we learn an invariant classifier whose prediction is simultaneously consistent with the labels in the source domain and clusters in the target domain, hence the spurious correlation inconsistent in the target domain is removed. We dub our approach "Invariant CONsistency learning" (ICON). Exte
    
[^34]: 区块链资源的最优动态费用设计

    Optimal Dynamic Fees for Blockchain Resources. (arXiv:2309.12735v1 [cs.GT])

    [http://arxiv.org/abs/2309.12735](http://arxiv.org/abs/2309.12735)

    我们开发了一个通用且实用的框架来解决多个区块链资源的动态费用机制的最优设计问题，并通过最优策略正确处理了资源需求中的交叉效应。我们还展示了如何利用这些交叉效应来指导资源设计，并演示了如何使用这个框架来完善或指导启发式费用更新规则。

    

    我们开发了一个通用且实用的框架来解决多个区块链资源的动态费用机制的最优设计问题。我们的框架可以计算出最优策略，以在处理持久需求转变和对观察到的区块需求的局部噪声鲁棒性之间进行权衡调整资源价格。在有多个资源的一般情况下，我们的最优策略能够正确处理资源需求中的交叉效应（互补性和替代性）。我们还展示了如何利用这些交叉效应来指导资源设计，即将资源组合成具有低需求端交叉效应的捆绑商品可以产生更简单和更高效的价格更新规则。我们的框架也是实用的，我们通过两个案例研究演示了如何使用它来完善或指导启发式费用更新规则，如EIP-1559或EIP-4844。然后，我们使用以太坊的真实市场数据估计了我们模型的单维度版本。

    We develop a general and practical framework to address the problem of the optimal design of dynamic fee mechanisms for multiple blockchain resources. Our framework allows to compute policies that optimally trade-off between adjusting resource prices to handle persistent demand shifts versus being robust to local noise in the observed block demand. In the general case with more than one resource, our optimal policies correctly handle cross-effects (complementarity and substitutability) in resource demands. We also show how these cross-effects can be used to inform resource design, i.e. combining resources into bundles that have low demand-side cross-effects can yield simpler and more efficient price-update rules. Our framework is also practical, we demonstrate how it can be used to refine or inform the design of heuristic fee update rules such as EIP-1559 or EIP-4844 with two case studies. We then estimate a uni-dimensional version of our model using real market data from the Ethereum 
    
[^35]: H2O+: 一种改进的混合离线和在线强化学习框架，用于动力学差距问题

    H2O+: An Improved Framework for Hybrid Offline-and-Online RL with Dynamics Gaps. (arXiv:2309.12716v1 [cs.LG])

    [http://arxiv.org/abs/2309.12716](http://arxiv.org/abs/2309.12716)

    H2O+是一种改进的混合离线和在线强化学习框架，通过综合考虑真实和模拟环境的动力学差距，同时利用有限的离线数据和不完美的模拟器进行策略学习，并在广泛的仿真和实际机器人实验中展示了卓越的性能和灵活性。

    

    在没有高精度模拟环境或大量离线数据的情况下，使用强化学习（RL）解决实际复杂任务可能相当具有挑战性。在非完美模拟环境中训练的在线RL代理可能会受到严重的模拟与现实问题。虽然离线RL方法可以绕过对模拟器的需求，但往往对离线数据集的大小和质量提出了苛刻的要求。最近出现的混合离线和在线RL提供了一个有吸引力的框架，可以同时使用有限的离线数据和不完美的模拟器进行可转移策略学习。本文提出了一种名为H2O+的新算法，该算法在桥接不同的离线和在线学习方法的同时，也考虑了真实和模拟环境之间的动力学差距。通过广泛的仿真和实际机器人实验，我们证明了H2O+在性能和灵活性上优于先进的跨域在线方法

    Solving real-world complex tasks using reinforcement learning (RL) without high-fidelity simulation environments or large amounts of offline data can be quite challenging. Online RL agents trained in imperfect simulation environments can suffer from severe sim-to-real issues. Offline RL approaches although bypass the need for simulators, often pose demanding requirements on the size and quality of the offline datasets. The recently emerged hybrid offline-and-online RL provides an attractive framework that enables joint use of limited offline data and imperfect simulator for transferable policy learning. In this paper, we develop a new algorithm, called H2O+, which offers great flexibility to bridge various choices of offline and online learning methods, while also accounting for dynamics gaps between the real and simulation environment. Through extensive simulation and real-world robotics experiments, we demonstrate superior performance and flexibility over advanced cross-domain online
    
[^36]: 无监督表示改进了监督学习中的语音情感识别

    Unsupervised Representations Improve Supervised Learning in Speech Emotion Recognition. (arXiv:2309.12714v1 [eess.AS])

    [http://arxiv.org/abs/2309.12714](http://arxiv.org/abs/2309.12714)

    这项研究提出了一种新颖的方法，利用无监督特征提取和监督分类相结合的方式，在小样本音频片段中进行情感识别。通过使用自监督特征提取器和基于CNN的模型，该方法在情感识别上表现出优于传统方法的结果。

    

    语音情感识别（SER）在增强人机交互方面起着重要作用，通过对情感状态进行更深入的理解，为更具共情力和有效沟通做出贡献。本研究提出了一种创新方法，将自监督特征提取与监督分类相结合，用于从小音频片段中识别情感。在预处理步骤中，为消除手工制作音频特征的需要，我们采用了基于Wav2Vec模型的自监督特征提取器，从音频数据中捕捉声学特征。然后，将预处理步骤的输出特征图输入到基于卷积神经网络（CNN）的自定义模型中进行情感分类。在ShEMO数据集上测试，所提出的方法超过了两种基准方法：支持向量机分类器和预训练CNN的迁移学习。

    Speech Emotion Recognition (SER) plays a pivotal role in enhancing human-computer interaction by enabling a deeper understanding of emotional states across a wide range of applications, contributing to more empathetic and effective communication. This study proposes an innovative approach that integrates self-supervised feature extraction with supervised classification for emotion recognition from small audio segments. In the preprocessing step, to eliminate the need of crafting audio features, we employed a self-supervised feature extractor, based on the Wav2Vec model, to capture acoustic features from audio data. Then, the output featuremaps of the preprocessing step are fed to a custom designed Convolutional Neural Network (CNN)-based model to perform emotion classification. Utilizing the ShEMO dataset as our testing ground, the proposed method surpasses two baseline methods, i.e. support vector machine classifier and transfer learning of a pretrained CNN. comparing the propose meth
    
[^37]: 只针对困难音频的大型模型：基于样本依赖的Whisper模型选择用于高效的推断

    Big model only for hard audios: Sample dependent Whisper model selection for efficient inferences. (arXiv:2309.12712v1 [eess.AS])

    [http://arxiv.org/abs/2309.12712](http://arxiv.org/abs/2309.12712)

    提出了一个决策模块来选择最小的足够模型用于音频转录，可以实现大幅度的计算节约。

    

    自动语音识别（ASR）领域的最新进展已经伴随着模型大小的大幅增加，现在可能包含数十亿个参数，即使在适应的硬件上也会导致推断速度缓慢。在这种情况下，存在不同大小、推断成本不同导致性能水平不同的ASR模型。基于一个观察结果，即较小的模型在大部分测试语料库上表现最佳，我们提出训练一个决策模块，可以根据音频样本使用最小的足够模型来获得良好的转录。我们将我们的方法应用于两个不同大小的Whisper模型。通过保持决策过程的计算效率，我们构建了一个决策模块，可以在减小性能损失的同时实现大幅度的计算节约。

    Recent progress in Automatic Speech Recognition (ASR) has been coupled with a substantial increase in the model sizes, which may now contain billions of parameters, leading to slow inferences even with adapted hardware. In this context, several ASR models exist in various sizes, with different inference costs leading to different performance levels. Based on the observation that smaller models perform optimally on large parts of testing corpora, we propose to train a decision module, that would allow, given an audio sample, to use the smallest sufficient model leading to a good transcription. We apply our approach to two Whisper models with different sizes. By keeping the decision process computationally efficient, we build a decision module that allows substantial computational savings with reduced performance drops.
    
[^38]: PointSSC：一个用于语义场景补全的车辆基础设施点云合作基准

    PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion. (arXiv:2309.12708v1 [cs.CV])

    [http://arxiv.org/abs/2309.12708](http://arxiv.org/abs/2309.12708)

    PointSSC是第一个为了语义场景补全而引入的车辆基础设施点云合作基准，具备长距离感知和最小遮挡。通过使用Segment Anything进行自动化注释，我们提出了一种基于激光雷达的模型，结合补全和分割的合作模块，来推动语义点云补全在真实世界导航中的发展。

    

    语义场景补全旨在为复杂的3D场景生成空间占用和语义标签。大多数现有的语义场景补全模型都集中在体素表示上，对于大型室外空间来说存在内存效率低下的问题。点云提供了一种轻量级的替代方案，但现有基准缺乏带有语义标签的室外点云场景。为了解决这个问题，我们引入了第一个用于语义场景补全的车辆基础设施点云合作基准PointSSC。这些场景具有长距离感知和最小的遮挡。我们利用Segment Anything开发了一个自动化注释流程，以高效地分配语义标签。为了评估进展，我们提出了一个基于激光雷达的模型，其中包括一个空间感知变换器用于全局和局部特征提取，以及一个补全和分割合作模块用于联合补全和分割。PointSSC提供了一个具有挑战性的测试平台，推动了语义点云补全在真实世界导航中的进展。

    Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navi
    
[^39]: 多标签噪声转移矩阵估计与标签相关性：理论与算法

    Multi-Label Noise Transition Matrix Estimation with Label Correlations: Theory and Algorithm. (arXiv:2309.12706v1 [cs.LG])

    [http://arxiv.org/abs/2309.12706](http://arxiv.org/abs/2309.12706)

    本论文提出了一种解决多标签噪声转移矩阵估计问题的方法，通过研究可辨识性，并结合标签相关性，提出了一种新的估计器，可以在噪声多标签学习中实现统计一致性算法。

    

    由于收集大规模准确标签的挑战，噪声多标签学习引起了越来越多的关注，使得噪声标签成为更实际的选择。受到噪声多类别学习的启发，引入转移矩阵可以帮助建模多标签噪声，并实现对噪声多标签学习的统计一致性算法的开发。然而，估计多标签噪声转移矩阵仍然是一个具有挑战性的任务，因为在噪声多类别学习中的大多数现有估计器依赖于锚点和准确拟合噪声类后验概率，而在噪声多标签学习中很难满足这些条件。在本文中，我们首先研究了在噪声多标签学习中基于类别的转移矩阵的可辨识性。在可辨识性结果的基础上，我们提出了一种新的估计器，可以利用标签相关性，而无需锚点或精确拟合噪声类后验概率。

    Noisy multi-label learning has garnered increasing attention due to the challenges posed by collecting large-scale accurate labels, making noisy labels a more practical alternative. Motivated by noisy multi-class learning, the introduction of transition matrices can help model multi-label noise and enable the development of statistically consistent algorithms for noisy multi-label learning. However, estimating multi-label noise transition matrices remains a challenging task, as most existing estimators in noisy multi-class learning rely on anchor points and accurate fitting of noisy class posteriors, which is hard to satisfy in noisy multi-label learning. In this paper, we address this problem by first investigating the identifiability of class-dependent transition matrices in noisy multi-label learning. Building upon the identifiability results, we propose a novel estimator that leverages label correlations without the need for anchor points or precise fitting of noisy class posterior
    
[^40]: 使用动态规划发现决策树的可解释性-性能帕累托前沿

    Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming. (arXiv:2309.12701v1 [cs.LG])

    [http://arxiv.org/abs/2309.12701](http://arxiv.org/abs/2309.12701)

    本文提出了一种使用动态规划找到最优决策树的方法，可以得到多个可解释性-性能权衡的最优决策树，使用户可以根据自己的需求选择最适合的树。

    

    众所周知，决策树由于可以被人类检查和解释而具有固有的可解释性。此外，最近硬件的进步重新引起了对最优决策树算法的关注，这些算法比通常的贪婪方法产生更准确的树。然而，这些最优算法返回的是一个优化手动定义的可解释性-性能权衡的单个树，通过指定最大决策节点数量来获得，对于这个权衡的质量没有进一步的洞察。在本文中，我们提出了一种新的马尔可夫决策问题（MDP）形式来找到最优决策树。这种形式的主要优点是，我们可以通过解决一个单一的动态规划问题计算出多个可解释性-性能权衡的最优决策树，让用户事后选择最适合他们需求的树。在实证方面，我们证明我们的方法在准确性和运行时间方面与最先进的算法竞争力相当。

    Decision trees are known to be intrinsically interpretable as they can be inspected and interpreted by humans. Furthermore, recent hardware advances have rekindled an interest for optimal decision tree algorithms, that produce more accurate trees than the usual greedy approaches. However, these optimal algorithms return a single tree optimizing a hand defined interpretability-performance trade-off, obtained by specifying a maximum number of decision nodes, giving no further insights about the quality of this trade-off. In this paper, we propose a new Markov Decision Problem (MDP) formulation for finding optimal decision trees. The main interest of this formulation is that we can compute the optimal decision trees for several interpretability-performance trade-offs by solving a single dynamic program, letting the user choose a posteriori the tree that best suits their needs. Empirically, we show that our method is competitive with state-of-the-art algorithms in terms of accuracy and run
    
[^41]: 语义相似性预测优于其他语义相似性度量方法

    Semantic similarity prediction is better than other semantic similarity measures. (arXiv:2309.12697v1 [cs.CL])

    [http://arxiv.org/abs/2309.12697](http://arxiv.org/abs/2309.12697)

    本文提出了一种使用经过微调的模型直接预测语义相似性的方法，并将其与其他方法进行比较，结果表明所得到的相似性更加符合我们对鲁棒的语义相似性度量的预期。

    

    自然语言文本之间的语义相似性通常通过检查子序列的重叠（例如BLEU）或使用嵌入（例如BERTScore，S-BERT）来衡量。在本文中，我们认为当我们仅对衡量语义相似性感兴趣时，直接使用经过微调的模型来预测相似性比其他方法更好。我们使用从GLUE基准测试中微调的STS-B模型，定义了STSScore方法，并且显示出所得到的相似性与我们对鲁棒的语义相似性度量的预期更加一致。

    Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the STS-B from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches.
    
[^42]: 循环时间修订图网络

    Recurrent Temporal Revision Graph Networks. (arXiv:2309.12694v1 [cs.LG])

    [http://arxiv.org/abs/2309.12694](http://arxiv.org/abs/2309.12694)

    该论文提出了一种循环时间修订图网络的新框架，通过使用循环神经网络和逐节点的隐藏状态，将所有历史邻居的信息整合起来，以获得每个节点的完整邻居信息。该框架在理论表现能力和实际应用中都取得了优越的性能。

    

    与静态图相比，时间图能更准确地建模许多现实场景。然而，邻居聚合是图网络的关键构建块，对于时间图来说，目前是从静态图直接拓展而来的。当在这种聚合过程中涉及所有历史邻居时，计算成本可能非常高昂。实际上，通常只涉及最近邻居的一个子集。然而，这种子抽样会导致邻居信息不完整和有偏。为了解决这个限制，我们提出了一个新颖的框架，用于时间邻居聚合，它使用循环神经网络和逐节点的隐藏状态，以获得每个节点的完整邻居信息。我们证明了该框架具有优越的理论表现能力，并在真实世界应用中展示了其最先进的性能。值得注意的是，它实现了显著的+9.6%的改进。

    Temporal graphs offer more accurate modeling of many real-world scenarios than static graphs. However, neighbor aggregation, a critical building block of graph networks, for temporal graphs, is currently straightforwardly extended from that of static graphs. It can be computationally expensive when involving all historical neighbors during such aggregation. In practice, typically only a subset of the most recent neighbors are involved. However, such subsampling leads to incomplete and biased neighbor information. To address this limitation, we propose a novel framework for temporal neighbor aggregation that uses the recurrent neural network with node-wise hidden states to integrate information from all historical neighbors for each node to acquire the complete neighbor information. We demonstrate the superior theoretical expressiveness of the proposed framework as well as its state-of-the-art performance in real-world applications. Notably, it achieves a significant +9.6% improvement o
    
[^43]: AMPLIFY: 基于注意力机制的Mixup方法，用于提高Transformer模型的性能和标签平滑

    AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])

    [http://arxiv.org/abs/2309.12689](http://arxiv.org/abs/2309.12689)

    AMPLIFY提出了一种基于注意力机制的Mixup方法，用于减少原始样本中的噪音和异常值对于模型的影响，并在文本分类任务中表现出更好的性能。

    

    Mixup是一种有效的数据增强方法，通过对不同原始样本的线性组合生成新的增强样本。然而，如果原始样本中存在噪音或异常特征，Mixup可能将其传播到增强样本中，导致模型对这些异常值过于敏感。为了解决这个问题，本文提出了一种新的Mixup方法称为AMPLIFY。该方法利用Transformer自身的注意力机制减少原始样本中噪音和异常值对预测结果的影响，无需增加可训练参数，计算成本非常低，从而避免了常见Mixup方法（例如语句Mixup）中资源消耗过高的问题。实验结果表明，在更小的计算资源成本下，AMPLIFY在7个基准数据集的文本分类任务中优于其他Mixup方法，为进一步提高模型性能提供了新的思路和方法。

    Mixup is an effective data augmentation method that generates new augmented samples by aggregating linear combinations of different original samples. However, if there are noises or aberrant features in the original samples, Mixup may propagate them to the augmented samples, leading to over-sensitivity of the model to these outliers . To solve this problem, this paper proposes a new Mixup method called AMPLIFY. This method uses the Attention mechanism of Transformer itself to reduce the influence of noises and aberrant values in the original samples on the prediction results, without increasing additional trainable parameters, and the computational cost is very low, thereby avoiding the problem of high resource consumption in common Mixup methods such as Sentence Mixup . The experimental results show that, under a smaller computational resource cost, AMPLIFY outperforms other Mixup methods in text classification tasks on 7 benchmark datasets, providing new ideas and new ways to further
    
[^44]: 关于稀疏的现代 Hopfield 模型

    On Sparse Modern Hopfield Model. (arXiv:2309.12673v1 [cs.LG])

    [http://arxiv.org/abs/2309.12673](http://arxiv.org/abs/2309.12673)

    本文介绍了稀疏的现代 Hopfield 模型，通过引入稀疏能量函数和稀疏记忆检索动力学，实现了对稀疏注意机制的一步近似。相比密集模型，稀疏模型的记忆检索误差上界更紧凑，具有明确的稀疏优势条件。同时，稀疏的现代 Hopfield 模型还保持了其密集对应物的稳健理论性质。

    

    我们介绍了稀疏的现代 Hopfield 模型作为现代 Hopfield 模型的一种扩展。与其密集的对应物一样，稀疏的现代 Hopfield 模型具备一种记忆检索动力学，其一步近似对应于稀疏的注意机制。从理论上讲，我们的关键贡献是通过稀疏熵正则化器的凸共轭导出了封闭形式的稀疏 Hopfield 能量。在此基础上，我们从稀疏能量函数中推导出稀疏记忆检索动力学，并展示了它的一步近似等价于稀疏结构化注意力。重要的是，我们提供了一个依赖于稀疏度的记忆检索误差上界，该上界在证明上要比其密集对应物更紧凑。因此，我们确定并讨论了稀疏优势出现的条件。此外，我们还表明稀疏的现代 Hopfield 模型保持了其密集对应物的稳健理论性质，包括快速的固定点收敛。

    We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. Theoretically, our key contribution is a principled derivation of a closed-form sparse Hopfield energy using the convex conjugate of the sparse entropic regularizer. Building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. Importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog. The conditions for the benefits of sparsity to arise are therefore identified and discussed. In addition, we show that the sparse modern Hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point conver
    
[^45]: 如何微调模型：统一模型偏移和模型偏差策略优化

    How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization. (arXiv:2309.12671v1 [cs.LG])

    [http://arxiv.org/abs/2309.12671](http://arxiv.org/abs/2309.12671)

    本文提出了一个统一模型偏移和模型偏差的优化目标，并通过微调过程实现了自适应的模型更新，以提供性能改进保证和避免模型过拟合。

    

    设计和推导出具有性能改进保证的有效基于模型的强化学习（MBRL）算法具有挑战性，这主要归因于模型学习和策略优化之间的高耦合性。许多先前的方法依靠回报差异来指导模型学习，忽略了模型偏移的影响，这可能导致由于过多的模型更新而性能下降。其他方法使用性能差异边界来明确考虑模型偏移。然而，这些方法依赖于固定的阈值来限制模型偏移，导致对阈值的严重依赖，并且在训练过程中缺乏适应性。在本文中，我们从理论上推导出一个可以统一模型偏移和模型偏差的优化目标，然后制定一个微调过程。这个过程可以自适应地调整模型更新，以获得性能改进保证，同时避免模型过拟合。基于这些，我们开发了一个简单直观的方法

    Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward 
    
[^46]: OneNet: 通过在线集成增强概念漂移下的时间序列预测模型

    OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling. (arXiv:2309.12659v1 [cs.LG])

    [http://arxiv.org/abs/2309.12659](http://arxiv.org/abs/2309.12659)

    OneNet是一个在线的时间序列预测模型，通过在线集成两个模型来适应概念漂移问题，其中一个模型用于建模时间维度上的依赖关系，另一个模型用于跨变量的依赖关系。实验证明，OneNet在适应概念漂移方面比其他方法更快且更有效。

    

    在线更新时间序列预测模型旨在通过基于流数据的高效更新预测模型来解决概念漂移问题。许多算法被设计用于在线时间序列预测，其中一些利用交叉变量依赖性，而其他算法则假设变量之间相互独立。鉴于每个数据假设在在线时间序列建模中都有其优缺点，我们提出了OneNet（在线集成网络）。它动态更新并结合两个模型，一个模型专注于建模时间维度上的依赖关系，另一个模型专注于跨变量的依赖关系。我们的方法将基于强化学习的方法融入传统的在线凸规划框架中，允许线性组合这两个模型，并动态调整权重。OneNet解决了经典在线学习方法在适应概念漂移方面过于缓慢的主要缺点。实证结果表明，OneNet在各种数据集上显著优于其他现有方法。

    Online updating of time series forecasting models aims to address the concept drifting problem by efficiently updating forecasting models based on streaming data. Many algorithms are designed for online time series forecasting, with some exploiting cross-variable dependency while others assume independence among variables. Given every data assumption has its own pros and cons in online time series modeling, we propose \textbf{On}line \textbf{e}nsembling \textbf{Net}work (OneNet). It dynamically updates and combines two models, with one focusing on modeling the dependency across the time dimension and the other on cross-variate dependency. Our method incorporates a reinforcement learning-based approach into the traditional online convex programming framework, allowing for the linear combination of the two models with dynamically adjusted weights. OneNet addresses the main shortcoming of classical online learning methods that tend to be slow in adapting to the concept drift. Empirical re
    
[^47]: 基于正则化Stein差异的神经算子变分推断用于深度高斯过程

    Neural Operator Variational Inference based on Regularized Stein Discrepancy for Deep Gaussian Processes. (arXiv:2309.12658v1 [cs.LG])

    [http://arxiv.org/abs/2309.12658](http://arxiv.org/abs/2309.12658)

    基于正则化Stein差异的神经算子变分推断用于深度高斯过程，通过使用神经生成器获得取样器以及使用蒙特卡罗估计和子采样随机优化技术解决极小极大问题，提高了深度高斯过程模型的表达能力和推断效果。

    

    深度高斯过程（DGP）模型提供了一种强大的非参数贝叶斯推断方法，但精确推断通常是难以求解的，这促使我们使用各种近似方法。然而，现有的方法，如均值场高斯假设，限制了DGP模型的表达能力和效果，而随机逼近可能计算代价高昂。为解决这些挑战，我们引入了基于神经算子的变分推断（NOVI）用于深度高斯过程。NOVI使用神经生成器获得取样器，并在L2空间中最小化生成分布和真实后验之间的正则化Stein差异。我们使用蒙特卡罗估计和子采样随机优化技术解决了极小极大问题。我们证明了通过将Fisher散度与常数相乘来控制方法引入的偏差，从而实现了鲁棒的误差控制，确保了算法的稳定性和精确性。

    Deep Gaussian Process (DGP) models offer a powerful nonparametric approach for Bayesian inference, but exact inference is typically intractable, motivating the use of various approximations. However, existing approaches, such as mean-field Gaussian assumptions, limit the expressiveness and efficacy of DGP models, while stochastic approximation can be computationally expensive. To tackle these challenges, we introduce Neural Operator Variational Inference (NOVI) for Deep Gaussian Processes. NOVI uses a neural generator to obtain a sampler and minimizes the Regularized Stein Discrepancy in L2 space between the generated distribution and true posterior. We solve the minimax problem using Monte Carlo estimation and subsampling stochastic optimization techniques. We demonstrate that the bias introduced by our method can be controlled by multiplying the Fisher divergence with a constant, which leads to robust error control and ensures the stability and precision of the algorithm. Our experim
    
[^48]: FP-PET: 大模型，多种损失和专注实践

    FP-PET: Large Model, Multiple Loss And Focused Practice. (arXiv:2309.12650v1 [cs.CV])

    [http://arxiv.org/abs/2309.12650](http://arxiv.org/abs/2309.12650)

    本研究提出了FP-PET，使用STUNet-large、SwinUNETR和VNet等多种机器学习模型在CT和PET图像分割领域取得了最先进的结果。研究引入了综合评估指标，综合考虑了Dice分数、假阳性体积和假阴性体积，并讨论了相关的计算挑战和解决方案。研究还探索了预处理和后处理技术来进一步优化分割输出，为医学图像分割领域提供了宝贵的见解。

    

    本研究提出了FP-PET，一种综合的医学图像分割方法，重点关注CT和PET图像。利用AutoPet2023挑战赛的数据集，研究采用了多种机器学习模型，包括STUNet-large，SwinUNETR和VNet，实现了最先进的分割性能。文章引入了一个综合评估指标，结合了多个评估指标，如Dice分数，假阳性体积（FPV）和假阴性体积（FNV），以提供全面的模型有效性度量。研究还讨论了与模型训练相关的计算挑战和解决方案，这是在高性能GPU上进行的。进一步优化分割输出，探索了包括高斯加权方案和形态学操作在内的预处理和后处理技术。本研究为高级医学图像分割的挑战和解决方案提供了有价值的见解。

    This study presents FP-PET, a comprehensive approach to medical image segmentation with a focus on CT and PET images. Utilizing a dataset from the AutoPet2023 Challenge, the research employs a variety of machine learning models, including STUNet-large, SwinUNETR, and VNet, to achieve state-of-the-art segmentation performance. The paper introduces an aggregated score that combines multiple evaluation metrics such as Dice score, false positive volume (FPV), and false negative volume (FNV) to provide a holistic measure of model effectiveness. The study also discusses the computational challenges and solutions related to model training, which was conducted on high-performance GPUs. Preprocessing and postprocessing techniques, including gaussian weighting schemes and morphological operations, are explored to further refine the segmentation output. The research offers valuable insights into the challenges and solutions for advanced medical image segmentation.
    
[^49]: 深度学习在CT扫描分类中的结果是否公正可解释？

    Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?. (arXiv:2309.12632v1 [cs.LG])

    [http://arxiv.org/abs/2309.12632](http://arxiv.org/abs/2309.12632)

    深度学习在CT扫描分类中的结果往往只关注准确性，而忽视了公正性和解释性，导致模型不可信和不适用于真实场景。

    

    鉴于深度学习方法在图像和物体分类中的巨大成功，生物医学图像处理领域也面临着深度学习应用于各种自动诊断案例的压力。不幸的是，文献中大多数基于深度学习的分类尝试仅仅关注极高的准确性，而不考虑可解释性或者患者训练和测试数据的分离。例如，大部分使用深度学习的肺结节分类论文会对数据进行随机洗牌，并将其分为训练、验证和测试集，导致一个人的CT扫描图像中的某些图像位于训练集中，而其他图像则位于验证或测试图像集中。这可能导致误导性的准确率报告和学习到的无关特征，最终降低了这些模型在实际应用中的可用性。

    Following the great success of various deep learning methods in image and object classification, the biomedical image processing society is also overwhelmed with their applications to various automatic diagnosis cases. Unfortunately, most of the deep learning-based classification attempts in the literature solely focus on the aim of extreme accuracy scores, without considering interpretability, or patient-wise separation of training and test data. For example, most lung nodule classification papers using deep learning randomly shuffle data and split it into training, validation, and test sets, causing certain images from the CT scan of a person to be in the training set, while other images of the exact same person to be in the validation or testing image sets. This can result in reporting misleading accuracy rates and the learning of irrelevant features, ultimately reducing the real-life usability of these models. When the deep neural networks trained on the traditional, unfair data sh
    
[^50]: 强化学习中的序贯动作引发不变表示

    Sequential Action-Induced Invariant Representation for Reinforcement Learning. (arXiv:2309.12628v1 [cs.LG])

    [http://arxiv.org/abs/2309.12628](http://arxiv.org/abs/2309.12628)

    本文提出了一种称为Sequential Action--Induced Invariant Representation (SAR)的方法，通过将包含任务关键信号的动作序列纳入表示学习，解决了从视觉干扰的高维观测中准确学习与任务相关的状态表示的问题。

    

    如何从视觉干扰的高维观测中准确学习与任务相关的状态表示是视觉强化学习中一个现实而具有挑战性的问题。最近，基于对比、预测和重建的无监督表示学习方法已经显示出提取与任务相关信息的能力。然而，由于在预测、对比和重建方法中缺乏适当的任务信息提取机制以及在稀疏奖励领域中的双模拟相关方法的局限性，这些方法仍然难以有效地扩展到具有干扰的环境中。为了缓解这些问题，在本文中，将包含任务关键信号的动作序列纳入表示学习中。具体而言，我们提出了一种称为Sequential Action--Induced Invariant Representation (SAR)的方法，其中编码器被优化...

    How to accurately learn task-relevant state representations from high-dimensional observations with visual distractions is a realistic and challenging problem in visual reinforcement learning. Recently, unsupervised representation learning methods based on bisimulation metrics, contrast, prediction, and reconstruction have shown the ability for task-relevant information extraction. However, due to the lack of appropriate mechanisms for the extraction of task information in the prediction, contrast, and reconstruction-related approaches and the limitations of bisimulation-related methods in domains with sparse rewards, it is still difficult for these methods to be effectively extended to environments with distractions. To alleviate these problems, in the paper, the action sequences, which contain task-intensive signals, are incorporated into representation learning. Specifically, we propose a Sequential Action--induced invariant Representation (SAR) method, in which the encoder is optim
    
[^51]: 基于数据驱动的多准则排序模型在时间准则下的应用研究

    Data-driven Preference Learning Methods for Multiple Criteria Sorting with Temporal Criteria. (arXiv:2309.12620v1 [cs.LG])

    [http://arxiv.org/abs/2309.12620](http://arxiv.org/abs/2309.12620)

    本研究提出了一种新的基于偏好学习的方法，用于解决存在时间准则的多准则排序问题。首先，提出了一个凸二次规划模型，并引入正则化框架。其次，设计了一种集成学习算法，用于合并多个优化器的输出。为了提高可扩展性和适应可学习的时间折扣因子，引入了一种新颖的单调循环神经网络(mRNN)。这些方法有效地处理了时间序列数据，并保持了多准则排序问题的关键属性。

    

    预测方法的出现促进了数据驱动的决策支持方法在各个领域中的应用。然而，开发能够有效处理时间序列数据的模型仍然是一个长期的挑战。本研究提出了一种新颖的基于偏好学习的方法，用于解决存在时间准则的多准则排序问题。我们首先提出了一个凸二次规划模型，该模型具有固定时间折扣因子，并在正则化框架内运行。此外，我们提出了一种集成学习算法，旨在合并多个可能较弱的优化器的输出，通过并行计算高效执行此过程。为了提高可扩展性并适应可学习的时间折扣因子，我们引入了一种新颖的单调循环神经网络(mRNN)。它旨在捕捉随时间演化的偏好动态，同时保持多准则排序问题固有的关键属性，包括准则的顺序性。

    The advent of predictive methodologies has catalyzed the emergence of data-driven decision support across various domains. However, developing models capable of effectively handling input time series data presents an enduring challenge. This study presents novel preference learning approaches to multiple criteria sorting problems in the presence of temporal criteria. We first formulate a convex quadratic programming model characterized by fixed time discount factors, operating within a regularization framework. Additionally, we propose an ensemble learning algorithm designed to consolidate the outputs of multiple, potentially weaker, optimizers, a process executed efficiently through parallel computation. To enhance scalability and accommodate learnable time discount factors, we introduce a novel monotonic Recurrent Neural Network (mRNN). It is designed to capture the evolving dynamics of preferences over time while upholding critical properties inherent to MCS problems, including crit
    
[^52]: 零后悔的不平等约束下的演化性预测

    Zero-Regret Performative Prediction Under Inequality Constraints. (arXiv:2309.12618v1 [cs.LG])

    [http://arxiv.org/abs/2309.12618](http://arxiv.org/abs/2309.12618)

    本文研究了一种零后悔的不平等约束下的演化性预测框架，该框架在实际学习问题中具有重要应用。通过开发稳健的原始-对偶框架，可以在考虑数据分布演化性的情况下找到最优解。

    

    演化性预测是一个最近提出的框架，其中预测指导决策并因此影响未来的数据分布。这样的演化现象在交通、金融、公共政策和推荐系统等各个领域中普遍存在。到目前为止，关于演化性预测的工作只关注无约束情况，忽视了许多实际学习问题受限制的事实。本文通过研究不平等约束下的演化性预测来填补这一空白。与大多数现有工作只提供演化稳定点不同，我们的目标是找到最优解。预测性梯度的预测是一项具有挑战性的任务，这是由于对数据分布的演化性的无知所致。为了解决这个问题，我们首先开发了一个稳健的原始-对偶框架，它只需要近似梯度达到一定的精度，但提供与随机原始-对偶算法相同数量级的性能。

    Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained scenarios, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradients is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stochastic primal-dual algorith
    
[^53]: 多样性鲁棒性联邦估计的目标平均处理效应

    Multiply Robust Federated Estimation of Targeted Average Treatment Effects. (arXiv:2309.12600v1 [stat.ML])

    [http://arxiv.org/abs/2309.12600](http://arxiv.org/abs/2309.12600)

    本研究提出了一种多样性鲁棒性联邦方法，用于通过多中心数据进行合理的因果推断，解决了数据隐私保护和个体协变量分布异质性的挑战，并展示了在效率和鲁棒性方面相对于现有方法的有限样本优势。

    

    联邦或多中心研究相比单中心研究具有明显优势，包括增加普适性、能够研究少数群体和研究稀有暴露与结果的机会。然而，这些研究存在数据隐私保护和个体协变量分布异质性的挑战。我们提出了一种新颖的联邦方法，通过开发多样性鲁棒性和隐私保护的辅助函数估计，为目标人群提供有效的因果推断。我们通过集成学习建立了转移学习的方法来估计源站点的组合权重以组合信息。我们证明了这些学习到的权重在不同场景下是高效和最优的。我们展示了我们的方法在效率和鲁棒性方面相对于现有方法的有限样本优势。

    Federated or multi-site studies have distinct advantages over single-site studies, including increased generalizability, the ability to study underrepresented populations, and the opportunity to study rare exposures and outcomes. However, these studies are challenging due to the need to preserve the privacy of each individual's data and the heterogeneity in their covariate distributions. We propose a novel federated approach to derive valid causal inferences for a target population using multi-site data. We adjust for covariate shift and covariate mismatch between sites by developing multiply-robust and privacy-preserving nuisance function estimation. Our methodology incorporates transfer learning to estimate ensemble weights to combine information from source sites. We show that these learned weights are efficient and optimal under different scenarios. We showcase the finite sample advantages of our approach in terms of efficiency and robustness compared to existing approaches.
    
[^54]: 通过对抗训练提高机器学习的鲁棒性

    Improving Machine Learning Robustness via Adversarial Training. (arXiv:2309.12593v1 [cs.LG])

    [http://arxiv.org/abs/2309.12593](http://arxiv.org/abs/2309.12593)

    本文通过对抗训练在集中式和分布式环境中研究了机器学习的鲁棒性，取得了较现有研究更好的效果。

    

    随着机器学习在各种实际应用中的使用越来越广泛，确保机器学习算法对任何潜在的最坏情况噪声、对抗攻击和非常不寻常的情况都具有鲁棒性至关重要。研究机器学习的鲁棒性将在设计机器学习算法方面起到重要作用。本文通过对抗训练在集中式和分布式环境中研究了机器学习的鲁棒性，其中机器学习的训练和测试在一个或多个计算机上进行。在集中式环境中，我们分别使用Fast Gradient Sign Method和DeepFool生成的对抗样本进行分类，得到了65.41%和83.0%的测试准确率。与现有研究相比，这些结果分别提高了18.41%和47%。在分布式环境中，我们通过使用对抗训练研究了联邦学习的鲁棒性，其中采用了独立同分布（IID）和非IID的方法。

    As Machine Learning (ML) is increasingly used in solving various tasks in real-world applications, it is crucial to ensure that ML algorithms are robust to any potential worst-case noises, adversarial attacks, and highly unusual situations when they are designed. Studying ML robustness will significantly help in the design of ML algorithms. In this paper, we investigate ML robustness using adversarial training in centralized and decentralized environments, where ML training and testing are conducted in one or multiple computers. In the centralized environment, we achieve a test accuracy of 65.41% and 83.0% when classifying adversarial examples generated by Fast Gradient Sign Method and DeepFool, respectively. Comparing to existing studies, these results demonstrate an improvement of 18.41% for FGSM and 47% for DeepFool. In the decentralized environment, we study Federated learning (FL) robustness by using adversarial training with independent and identically distributed (IID) and non-I
    
[^55]: 采样频率无关的通用声音分离方法

    Sampling-Frequency-Independent Universal Sound Separation. (arXiv:2309.12581v1 [eess.AS])

    [http://arxiv.org/abs/2309.12581](http://arxiv.org/abs/2309.12581)

    本文提出了一种采样频率无关的通用声音分离方法，能够处理未经训练的采样频率，为实现普适的源分离器提供了关键技术。

    

    本文提出了一种能够处理未经训练的采样频率（SF）的通用声音分离（USS）方法。USS旨在分离不同类型的任意源，并且可以作为任何下游任务的预处理器而被普遍使用的关键技术。为了实现通用的源分离器，有两个必要属性：对源类型和录制条件的通用性。前者的属性已经在USS文献中研究过，大大增加了单个神经网络可以处理的源类型数量。然而，后者的属性（例如，SF）尽管其必要性，但却受到了较少的关注。由于SF根据下游任务的不同而有很大差异，通用的源分离器必须能处理各种 SF。在本文中，为了涵盖这两个属性，我们提出了一个对计算效率较高的USS网络SuDoRM-RF的SF无关（SFI）扩展。

    This paper proposes a universal sound separation (USS) method capable of handling untrained sampling frequencies (SFs). The USS aims at separating arbitrary sources of different types and can be the key technique to realize a source separator that can be universally used as a preprocessor for any downstream tasks. To realize a universal source separator, there are two essential properties: universalities with respect to source types and recording conditions. The former property has been studied in the USS literature, which has greatly increased the number of source types that can be handled by a single neural network. However, the latter property (e.g., SF) has received less attention despite its necessity. Since the SF varies widely depending on the downstream tasks, the universal source separator must handle a wide variety of SFs. In this paper, to encompass the two properties, we propose an SF-independent (SFI) extension of a computationally efficient USS network, SuDoRM-RF. The pro
    
[^56]: SPION：通过卷积泛滥填充实现Transformer的逐层稀疏训练

    SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling. (arXiv:2309.12578v1 [cs.LG])

    [http://arxiv.org/abs/2309.12578](http://arxiv.org/abs/2309.12578)

    本文提出了一种新颖的Transformer稀疏化方案，通过集成卷积滤波器和泛滥填充方法，高效地实现了注意力操作的逐层稀疏模式，减少了计算复杂度和内存占用。

    

    稀疏化Transformer已经引起了相当大的兴趣，因为训练Transformer需要很大的计算量。先前稀疏化Transformer的方法要么使用固定的模式，要么使用数据驱动的方法来减少涉及计算多头注意力的操作数量，这是Transformer的主要瓶颈。然而，现有方法存在不可避免的问题，如由于在所有层中应用统一的固定模式而导致的潜在序列关键特征损失，以及由于使用额外参数学习注意力操作中的稀疏模式而导致模型大小的增加。在本文中，我们提出了一种新颖的Transformer稀疏化方案，该方案将卷积滤波器和泛滥填充方法相结合，以高效地捕捉注意力操作的逐层稀疏模式。我们的稀疏化方法在Transformer的计算复杂度和内存占用方面表现出色。

    Sparsifying the Transformer has garnered considerable interest, as training the Transformer is very computationally demanding. Prior efforts to sparsify the Transformer have either used a fixed pattern or data-driven approach to reduce the number of operations involving the computation of multi-head attention, which is the main bottleneck of the Transformer. However, existing methods suffer from inevitable problems, such as the potential loss of essential sequence features due to the uniform fixed pattern applied across all layers, and an increase in the model size resulting from the use of additional parameters to learn sparsity patterns in attention operations. In this paper, we propose a novel sparsification scheme for the Transformer that integrates convolution filters and the flood filling method to efficiently capture the layer-wise sparse pattern in attention operations. Our sparsification approach reduces the computational complexity and memory footprint of the Transformer duri
    
[^57]: 用深度学习在眼动数据上对阿尔茨海默病进行分类

    Classification of Alzheimers Disease with Deep Learning on Eye-tracking Data. (arXiv:2309.12574v1 [cs.CV])

    [http://arxiv.org/abs/2309.12574](http://arxiv.org/abs/2309.12574)

    该论文研究了使用深度学习模型在原始眼动数据上进行阿尔茨海默病分类的方法，称为VTNet。研究表明，VTNet在AD分类任务中的性能优于现有方法，显示了该模型从眼动数据进行预测的潜力。

    

    现有研究表明，使用依赖于任务特定特征的分类器对眼动数据进行阿尔茨海默病 (AD) 分类具有潜力。本文探讨是否可以通过使用以原始眼动数据为训练数据的端到端深度学习分类器来改进现有结果。该分类器 (VTNet) 同时利用 GRU 和 CNN 来利用眼动数据的视觉 (V) 和时间 (T) 表示，并且先前被用于检测用户在处理视觉显示时的困惑。将VTNet应用于AD分类任务的一个主要挑战是可用的眼动数据序列比先前的困惑检测任务中使用的序列要长得多，这将LSTM-based模型的可处理性推向极限。我们讨论了如何应对这个挑战，并展示了VTNet在AD分类方面优于现有方法的性能，为这个模型从眼动数据中进行预测的普适性提供了鼓舞人心的证据。

    Existing research has shown the potential of classifying Alzheimers Disease (AD) from eye-tracking (ET) data with classifiers that rely on task-specific engineered features. In this paper, we investigate whether we can improve on existing results by using a Deep-Learning classifier trained end-to-end on raw ET data. This classifier (VTNet) uses a GRU and a CNN in parallel to leverage both visual (V) and temporal (T) representations of ET data and was previously used to detect user confusion while processing visual displays. A main challenge in applying VTNet to our target AD classification task is that the available ET data sequences are much longer than those used in the previous confusion detection task, pushing the limits of what is manageable by LSTM-based models. We discuss how we address this challenge and show that VTNet outperforms the state-of-the-art approaches in AD classification, providing encouraging evidence on the generality of this model to make predictions from ET dat
    
[^58]: 可解释的3D多模态残差卷积神经网络用于轻度颅脑损伤诊断

    Interpretable 3D Multi-Modal Residual Convolutional Neural Network for Mild Traumatic Brain Injury Diagnosis. (arXiv:2309.12572v1 [eess.IV])

    [http://arxiv.org/abs/2309.12572](http://arxiv.org/abs/2309.12572)

    这项研究提出了一种可解释的3D多模态残差卷积神经网络（MRCNN）用于轻度颅脑损伤（mTBI）诊断，结合遮蔽敏感度图（OSM）进行增强。与传统的CT图像诊断相比，MRCNN模型在特异度和准确率上都有显著提升。

    

    轻度颅脑损伤（mTBI）由于其高发病率和潜在的长期健康影响而成为一个重要的公共卫生挑战。尽管计算机断层扫描（CT）是mTBI的标准诊断工具，但在mTBI患者中往往会出现正常结果，尽管存在症状证据。这一事实凸显了准确诊断的复杂性。在本研究中，我们介绍了一种可解释的3D多模态残差卷积神经网络（MRCNN）用于mTBI诊断模型，结合遮蔽敏感度图（OSM）进行增强。我们的MRCNN模型在mTBI诊断方面展现出良好的性能，经由五折交叉验证流程验证，平均准确率为82.4%，灵敏度为82.6%，特异度为81.6%。值得注意的是，与基于CT的残差卷积神经网络（RCNN）模型相比，MRCNN在特异度上提高了4.4%，准确率提高了9.0%。我们展示了OSM在比较CT图像时提供了更好的数据驱动洞察力。

    Mild Traumatic Brain Injury (mTBI) is a significant public health challenge due to its high prevalence and potential for long-term health effects. Despite Computed Tomography (CT) being the standard diagnostic tool for mTBI, it often yields normal results in mTBI patients despite symptomatic evidence. This fact underscores the complexity of accurate diagnosis. In this study, we introduce an interpretable 3D Multi-Modal Residual Convolutional Neural Network (MRCNN) for mTBI diagnostic model enhanced with Occlusion Sensitivity Maps (OSM). Our MRCNN model exhibits promising performance in mTBI diagnosis, demonstrating an average accuracy of 82.4%, sensitivity of 82.6%, and specificity of 81.6%, as validated by a five-fold cross-validation process. Notably, in comparison to the CT-based Residual Convolutional Neural Network (RCNN) model, the MRCNN shows an improvement of 4.4% in specificity and 9.0% in accuracy. We show that the OSM offers superior data-driven insights into CT images compa
    
[^59]: 通过充分因素和必要因素的概率进行不变学习

    Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])

    [http://arxiv.org/abs/2309.12559](http://arxiv.org/abs/2309.12559)

    本研究通过引入充分因素和必要因素的概率（PNS）来改善在未知测试分布上的泛化问题，以解决现有方法主要关注因果性的不变性属性而忽视充分性和必要性条件的问题。

    

    在野外学习中，对于未知的、与训练分布不同的测试分布，外部分布（OOD）泛化是不可或缺的。最近从因果性引发的方法在实现OOD泛化方面显示出了巨大的潜力。然而，现有方法主要关注因果性的不变性属性，而在很大程度上忽视了充分性和必要性条件的属性。换句话说，一个必要但不充分的原因（特征）对于分布转换是不变的，但可能没有所需的准确度。相反，一个充分但不必要的原因（特征）倾向于很好地适应特定数据，但可能存在适应新领域的风险。为了捕捉充分和必要因素的信息，我们采用了经典概念——充分和必要因素的概率（PNS），它指示了一个因素是必要和充分原因的概率。为了将PNS与OOD泛化联系起来，我们提出了一种方法

    Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
    
[^60]: 通过鲁棒优化方法为神经网络提供可证明的鲁棒和可信的反事实解释

    Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation. (arXiv:2309.12545v1 [cs.LG])

    [http://arxiv.org/abs/2309.12545](http://arxiv.org/abs/2309.12545)

    本文提出了一种名为PROPLACE的方法，通过鲁棒优化技术为神经网络提供可证明的鲁棒和可信的反事实解释，解决了现有方法在保持鲁棒性的同时生成不合理解释的问题。

    

    反事实解释(CEs)作为解释神经网络分类器的主要方法已经引起了越来越多的关注。通常，CEs对于输入-输出对被定义为到输入的最小距离的数据点，其与输出具有不同的标签。为了解决CEs在模型参数更新(比如重新训练)时很容易被无效的问题，研究提出了一种通过模型参数变化的范数球界限来证明CEs的鲁棒性的方法。然而，现有的针对这种鲁棒性的方法不是完全正确的，或者可能生成不合理的CEs，即与训练数据集存在离群值。事实上，目前没有一种方法能够同时优化距离和可信度，并保持鲁棒性保证。在这项工作中，我们提出了一种名为PROPLACE的方法，利用鲁棒优化技术来解决上述问题。

    Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for proximity and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limi
    
[^61]: 无线数据传输需求的自动驾驶车辆行程规划使用强化学习

    Trip Planning for Autonomous Vehicles with Wireless Data Transfer Needs Using Reinforcement Learning. (arXiv:2309.12534v1 [cs.LG])

    [http://arxiv.org/abs/2309.12534](http://arxiv.org/abs/2309.12534)

    本文研究了无线数据传输需求的自动驾驶车辆行程规划问题，在城市区域中考虑了驾驶时间和数据传输需求，并使用强化学习方法进行解决。

    

    随着通信和物联网领域的最新进展，车辆对其环境的认知越来越多，并向着完全自主驾驶进化。车辆通信打开了车辆与基础设施进行交互的可能性，其中车辆可以与支持国家道路系统的组件，如摄像头、交通灯和标识分享信息。因此，车辆不仅仅是交通工具，它们收集、处理和传输大量数据用于使驾驶更安全、更便捷。随着5G蜂窝网络及更高版本的出现，我们的道路上将有更多的数据带宽可用，但由于视线、基础设施和道路上异构的流量等限制，它们可能会是异构的。本文解决了城市区域自动驾驶车辆行程规划的问题，考虑了驾驶时间和数据传输需求。我们提出了一种新颖的方法

    With recent advancements in the field of communications and the Internet of Things, vehicles are becoming more aware of their environment and are evolving towards full autonomy. Vehicular communication opens up the possibility for vehicle-to-infrastructure interaction, where vehicles could share information with components such as cameras, traffic lights, and signage that support a countrys road system. As a result, vehicles are becoming more than just a means of transportation; they are collecting, processing, and transmitting massive amounts of data used to make driving safer and more convenient. With 5G cellular networks and beyond, there is going to be more data bandwidth available on our roads, but it may be heterogeneous because of limitations like line of sight, infrastructure, and heterogeneous traffic on the road. This paper addresses the problem of route planning for autonomous vehicles in urban areas accounting for both driving time and data transfer needs. We propose a nove
    
[^62]: 级联预测模块系统的置信度校准

    Confidence Calibration for Systems with Cascaded Predictive Modules. (arXiv:2309.12510v1 [cs.LG])

    [http://arxiv.org/abs/2309.12510](http://arxiv.org/abs/2309.12510)

    本研究提出了一种针对级联预测模块系统的置信度校准方法，通过利用模块级别的验证数据，解决了现有算法在多模块系统中无法提供可靠预测的问题。

    

    现有的合规预测算法在目标置信度水平上估计预测间隔，以表征回归模型在新测试样本上的性能。然而，考虑到包含多个模块的自主系统，为单个模块构建的预测间隔不足以容纳不同模块之间的不确定性传播，因此无法为系统行为提供可靠的预测。我们解决了这个限制，并提出了基于合规预测的新的解决方案，以提供针对级联模块组成的预测系统（例如，上游特征提取模块和下游回归模块）校准的预测间隔。我们的关键思想是利用模块级别的验证数据，以在没有直接访问端到端验证数据的情况下表征系统级误差分布。我们提供了理论证明和经验证的实验结果，以证明所提出的解决方案的有效性。

    Existing conformal prediction algorithms estimate prediction intervals at target confidence levels to characterize the performance of a regression model on new test samples. However, considering an autonomous system consisting of multiple modules, prediction intervals constructed for individual modules fall short of accommodating uncertainty propagation over different modules and thus cannot provide reliable predictions on system behavior. We address this limitation and present novel solutions based on conformal prediction to provide prediction intervals calibrated for a predictive system consisting of cascaded modules (e.g., an upstream feature extraction module and a downstream regression module). Our key idea is to leverage module-level validation data to characterize the system-level error distribution without direct access to end-to-end validation data. We provide theoretical justification and empirical experimental results to demonstrate the effectiveness of proposed solutions. I
    
[^63]: 一种联合交互式导航的扩散模型

    A Diffusion-Model of Joint Interactive Navigation. (arXiv:2309.12508v1 [cs.LG])

    [http://arxiv.org/abs/2309.12508](http://arxiv.org/abs/2309.12508)

    本文提出了一种基于扩散模型的方法 DJINN，用于生成交通场景。通过联合扩散所有代理的轨迹，并以灵活的状态观察为条件，我们在轨迹预测上取得了最先进的性能。此外，DJINN还能灵活地从多种有价值的条件分布中进行测试时抽样。

    

    模拟自动驾驶系统需要模拟出展现多样和真实行为的交通参与者。在模拟中使用实际世界交通场景确保了真实性，但是安全关键事件的罕见性使得大规模收集驾驶场景具有高昂的成本。在本文中，我们提出了一种基于扩散的生成交通场景的方法DJINN。我们的方法联合扩散所有代理的轨迹，以过去、现在或未来的一系列灵活的状态观察为条件。在流行的轨迹预测数据集上，我们报道了在联合轨迹评估指标上的最先进性能。此外，我们展示了DJINN如何灵活地使得从各种有价值的条件分布中直接进行测试时抽样，包括基于目标的抽样、行为类别抽样和场景编辑。

    Simulation of autonomous vehicle systems requires that simulated traffic participants exhibit diverse and realistic behaviors. The use of prerecorded real-world traffic scenarios in simulation ensures realism but the rarity of safety critical events makes large scale collection of driving scenarios expensive. In this paper, we present DJINN - a diffusion based method of generating traffic scenarios. Our approach jointly diffuses the trajectories of all agents, conditioned on a flexible set of state observations from the past, present, or future. On popular trajectory forecasting datasets, we report state of the art performance on joint trajectory metrics. In addition, we demonstrate how DJINN flexibly enables direct test-time sampling from a variety of valuable conditional distributions including goal-based sampling, behavior-class sampling, and scenario editing.
    
[^64]: 知识图谱嵌入：综述

    Knowledge Graph Embedding: An Overview. (arXiv:2309.12501v1 [cs.AI])

    [http://arxiv.org/abs/2309.12501](http://arxiv.org/abs/2309.12501)

    该论文综述了知识图谱嵌入的研究状态，介绍了两个主要分支：基于距离和基于语义匹配的方法。还讨论了CompoundE和CompoundE3D模型，并揭示了一个潜在的研究趋势。

    

    许多数学模型已被利用来设计嵌入，以表示知识图谱（KG）中的实体和关系，用于链接预测和许多下游任务。这些数学启发的模型不仅在大型KG中进行推理时高度可扩展，而且在建模不同关系模式方面具有很多可解释的优势，这些优势可以通过形式化证明和经验结果来验证。在本文中，我们对KG完成领域的当前研究状态进行了全面的概述。特别是，我们着重介绍了KG嵌入（KGE）设计的两个主要分支：1）基于距离的方法和2）基于语义匹配的方法。我们发现了最近提出的模型之间的联系，并提出了一个潜在的趋势，这可能有助于研究人员发明新颖且更有效的模型。接下来，我们深入探讨了从2D和3D仿射操作中汲取灵感的CompoundE和CompoundE3D。它们涵盖了包括dis在内的广泛技术谓词的范围。

    Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including dis
    
[^65]: 用户级差分隐私在少量用户示例下的应用

    User-Level Differential Privacy With Few Examples Per User. (arXiv:2309.12500v1 [cs.DS])

    [http://arxiv.org/abs/2309.12500](http://arxiv.org/abs/2309.12500)

    本论文研究了用户级差分隐私在少量示例的情况下的应用。对于近似差分隐私，提供了一种转换方法来将项级差分隐私算法转换为用户级差分隐私算法，可以在保持相同效用的前提下减少所需的用户数量。对于纯差分隐私，提出了一种简单的技术来适应指数机制。

    

    以前关于用户级差分隐私的研究主要针对示例丰富的场景，即每个用户都有足够多的示例，可以自行解决问题。本文考虑了示例稀缺的情况，即每个用户只有少量示例，并得到以下结果：1. 对于近似差分隐私，我们提供了一种通用的转换方法，将任何项级差分隐私算法转换为用户级差分隐私算法。粗略地说，后者在达到相同效用时，所需的用户数量相对于示例数量$m$以$O_{\varepsilon,\delta}(\sqrt{m})$的速度减少，其中$m$是每个用户的示例数量。这个算法在恢复大多数已知问题的界限的同时，还给出了新的界限，例如对于PAC学习。2. 对于纯差分隐私，我们提出了一种简单的技术，用于调整指数机制[McSherry，Talwar

    Previous work on user-level differential privacy (DP) [Ghazi et al. NeurIPS 2021, Bun et al. STOC 2023] obtained generic algorithms that work for various learning tasks. However, their focus was on the example-rich regime, where the users have so many examples that each user could themselves solve the problem. In this work we consider the example-scarce regime, where each user has only a few examples, and obtain the following results:  1. For approximate-DP, we give a generic transformation of any item-level DP algorithm to a user-level DP algorithm. Roughly speaking, the latter gives a (multiplicative) savings of $O_{\varepsilon,\delta}(\sqrt{m})$ in terms of the number of users required for achieving the same utility, where $m$ is the number of examples per user. This algorithm, while recovering most known bounds for specific problems, also gives new bounds, e.g., for PAC learning.  2. For pure-DP, we present a simple technique for adapting the exponential mechanism [McSherry, Talwar
    
[^66]: 有关富标签在主动学习中的证据不确定性

    Evidential uncertainties on rich labels for active learning. (arXiv:2309.12494v1 [cs.LG])

    [http://arxiv.org/abs/2309.12494](http://arxiv.org/abs/2309.12494)

    本文提出了两种策略来应对主动学习中的不确定性问题，即采用Klir不确定性采样和证据学派不确定性采样，在考虑到标签中已存在的不确定性的基础上，对模型的不确定性进行分解和处理。

    

    最近关于主动学习的研究，尤其是关于不确定采样的研究，主要集中在将模型不确定性分解为可降低和不可降低的不确定性上。在本文中，我们提出简化计算阶段，并消除对观察结果的依赖，更重要的是考虑标签中已经存在的不确定性，即答题者的不确定性。我们提出了两种策略，Klir不确定性采样和证据学派不确定性采样，这两种方法都使用了信任函数理论来解决探索-利用问题。

    Recent research in active learning, and more precisely in uncertainty sampling, has focused on the decomposition of model uncertainty into reducible and irreducible uncertainties. In this paper, we propose to simplify the computational phase and remove the dependence on observations, but more importantly to take into account the uncertainty already present in the labels, \emph{i.e.} the uncertainty of the oracles. Two strategies are proposed, sampling by Klir uncertainty, which addresses the exploration-exploitation problem, and sampling by evidential epistemic uncertainty, which extends the reducible uncertainty to the evidential framework, both using the theory of belief functions.
    
[^67]: 锐度感知最小化和稳定性边界。

    Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])

    [http://arxiv.org/abs/2309.12488](http://arxiv.org/abs/2309.12488)

    本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。

    

    最近的实验表明，当使用梯度下降(GD)训练神经网络时，损失函数的Hessian矩阵的操作符范数会增长，直到接近$2/\eta$，之后会在该值周围波动。根据对损失函数的局部二次逼近，$2/\eta$被称为“稳定性边界”。我们使用类似的计算方法，为锐度感知最小化(SAM)确定了一个“稳定性边界”，SAM是一种改进泛化性能的GD变种。与GD不同，SAM的稳定性边界取决于梯度的范数。通过三个深度学习任务的实证，我们观察到SAM在这个分析中确定的稳定性边界上运行。

    Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
    
[^68]: 研究和改进人类和机器的推理能力

    Studying and improving reasoning in humans and machines. (arXiv:2309.12485v1 [cs.CL])

    [http://arxiv.org/abs/2309.12485](http://arxiv.org/abs/2309.12485)

    本研究通过对大型语言模型（LLM）和人类的推理能力进行比较研究，发现LLM在推理中存在类似于人类启发式推理的错误，但与人类推理有重要差异，最新的LLM版本几乎消除了模型的限制。此外，人类和机器对相同的提示方案的反应不同。这些结果对我们的认识论有重大影响。

    

    在本研究中，我们使用传统用于研究（有限）理性的认知心理学工具，研究和比较了大型语言模型（LLM）和人类的推理能力。为此，我们向人类参与者和一系列预训练的LLM呈现了新的经典认知实验的变体，并对它们的表现进行了交叉比较。我们的结果显示，大多数模型呈现出类似于常见的错误倾向于启发式人类推理的推理错误。尽管有这种表面上的相似性，人类和LLM之间的深入比较表明了人类样式推理的重要差异，随着最近LLM版本的推出，模型的限制几乎完全消失。此外，我们还展示出，虽然可能制定策略以获得更好的表现，但人类和机器对相同的提示方案的反应并不相同。我们通过讨论这一认识论的影响来总结。

    In the present study, we investigate and compare reasoning in large language models (LLM) and humans using a selection of cognitive psychology tools traditionally dedicated to the study of (bounded) rationality. To do so, we presented to human participants and an array of pretrained LLMs new variants of classical cognitive experiments, and cross-compared their performances. Our results showed that most of the included models presented reasoning errors akin to those frequently ascribed to error-prone, heuristic-based human reasoning. Notwithstanding this superficial similarity, an in-depth comparison between humans and LLMs indicated important differences with human-like reasoning, with models limitations disappearing almost entirely in more recent LLMs releases. Moreover, we show that while it is possible to devise strategies to induce better performance, humans and machines are not equally-responsive to the same prompting schemes. We conclude by discussing the epistemological implicat
    
[^69]: 在移动应用开发中基于缺失值抗干扰元启发式神经网络的稳健能耗预测

    Robust Energy Consumption Prediction with a Missing Value-Resilient Metaheuristic-based Neural Network in Mobile App Development. (arXiv:2309.12484v1 [cs.NE])

    [http://arxiv.org/abs/2309.12484](http://arxiv.org/abs/2309.12484)

    本研究提出了一种基于元启发式方法增强的神经网络框架，旨在实现在移动应用开发中稳健的能耗预测。

    

    能耗是移动应用开发中的一个基本问题，对开发人员和终端用户都具有重要意义。此外，在消费者考虑智能手机购买时，能耗是决策过程中的一个关键因素。从可持续发展的角度来看，鉴于数十亿部智能手机的广泛使用所带来的重大全球影响，探索旨在减少移动设备能耗的方法变得至关重要，这对环境产生了深远影响。尽管安卓平台（主导的移动生态系统）中存在各种节能编程实践，但仍需要专门面向移动应用开发的机器学习-based能耗预测算法的文档化。因此，本研究的主要目标是提出一种基于元启发式方法增强的神经网络框架，以实现稳健的能耗预测。

    Energy consumption is a fundamental concern in mobile application development, bearing substantial significance for both developers and end-users. Moreover, it is a critical determinant in the consumer's decision-making process when considering a smartphone purchase. From the sustainability perspective, it becomes imperative to explore approaches aimed at mitigating the energy consumption of mobile devices, given the significant global consequences arising from the extensive utilisation of billions of smartphones, which imparts a profound environmental impact. Despite the existence of various energy-efficient programming practices within the Android platform, the dominant mobile ecosystem, there remains a need for documented machine learning-based energy prediction algorithms tailored explicitly for mobile app development. Hence, the main objective of this research is to propose a novel neural network-based framework, enhanced by a metaheuristic approach, to achieve robust energy predi
    
[^70]: State2Explanation:基于概念的解释：有利于Agent学习和用户理解

    State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding. (arXiv:2309.12482v1 [cs.LG])

    [http://arxiv.org/abs/2309.12482](http://arxiv.org/abs/2309.12482)

    本论文致力于开发一种基于概念的解释方法，旨在提高非AI专家对AI决策的理解。通过定义顺序决策设置中的“概念”以及探索基于概念的解释对RL agent学习效果和最终用户对agent决策理解的双重好处，我们提出了一个统一的框架。

    

    随着非AI专家使用更复杂的AI系统来完成日常任务，人们越来越努力开发能够为非AI专家理解的AI决策提供解释的方法。为了实现这个目标，利用高级概念并生成基于概念的解释已经成为一种流行的方法。大多数基于概念的解释都是为分类技术而开发的，我们认为目前关于顺序决策的方法还存在一定限制。在这项工作中，我们首先提出了在顺序决策设置中定义“概念”的愿望。受到“Protege效应”的启发，该效应说明解释知识通常会增强个体的自主学习能力，我们探索了基于概念的解释对RL agent的学习效果和最终用户对agent决策理解的双重好处。为此，我们提出了一个统一的框架，St

    With more complex AI systems used by non-AI experts to complete daily tasks, there is an increasing effort to develop methods that produce explanations of AI decision making understandable by non-AI experts. Towards this effort, leveraging higher-level concepts and producing concept-based explanations have become a popular method. Most concept-based explanations have been developed for classification techniques, and we posit that the few existing methods for sequential decision making are limited in scope. In this work, we first contribute a desiderata for defining "concepts" in sequential decision making settings. Additionally, inspired by the Protege Effect which states explaining knowledge often reinforces one's self-learning, we explore the utility of concept-based explanations providing a dual benefit to the RL agent by improving agent learning rate, and to the end-user by improving end-user understanding of agent decision making. To this end, we contribute a unified framework, St
    
[^71]: 架构对多光谱深度神经网络的稳健性和解释性的影响

    Impact of architecture on robustness and interpretability of multispectral deep neural networks. (arXiv:2309.12463v1 [cs.CV])

    [http://arxiv.org/abs/2309.12463](http://arxiv.org/abs/2309.12463)

    这项工作研究了不同融合策略对多光谱深度学习模型性能，依赖性和稳健性的影响。

    

    包含额外光谱波段（例如近红外）的信息可以提高深度学习模型在许多视觉任务中的性能。有许多可能的方法将这些额外信息纳入深度学习模型中，但最佳融合策略尚未确定，并且在不同应用场景中可能有所变化。在其中一个极端，称为"早期融合"，额外波段被堆叠为额外通道，以获得具有超过三个通道的输入图像。在另一个极端，称为"晚期融合"，RGB和非RGB波段通过深度学习模型的单独分支传递，并在最终分类或分割层之前立即合并。在这项工作中，我们表征了一套不同融合方法的多光谱深度学习模型的性能，量化了它们对不同输入波段的相对依赖性，并评估了它们对影响一个或多个输入通道的自然图像失真的稳健性。

    Including information from additional spectral bands (e.g., near-infrared) can improve deep learning model performance for many vision-oriented tasks. There are many possible ways to incorporate this additional information into a deep learning model, but the optimal fusion strategy has not yet been determined and can vary between applications. At one extreme, known as "early fusion," additional bands are stacked as extra channels to obtain an input image with more than three channels. At the other extreme, known as "late fusion," RGB and non-RGB bands are passed through separate branches of a deep learning model and merged immediately before a final classification or segmentation layer. In this work, we characterize the performance of a suite of multispectral deep learning models with different fusion approaches, quantify their relative reliance on different input bands and evaluate their robustness to naturalistic image corruptions affecting one or more input channels.
    
[^72]: 多模态深度学习用于科学成像解释

    Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v1 [cs.LG])

    [http://arxiv.org/abs/2309.12460](http://arxiv.org/abs/2309.12460)

    本研究提出了一种多模态深度学习框架，通过模拟人类与扫描电子显微镜图像的交互，利用文本和视觉数据进行精细数据合成和评估。该模型（GlassLLaVA）能够准确解释、识别关键特征和检测以前未见的SEM图像中的缺陷，同时引入了适用于多种科学成像应用的灵活评估指标。

    

    在科学成像领域，解释视觉数据常常需要人类专业知识和对主题材料的深入理解的复杂组合。本研究提出了一种新的方法，通过多模态深度学习框架来模拟并评估与扫描电子显微镜（SEM）图像的人类交互，特别是玻璃材料图像。我们的方法利用从同行评议的文章中收集的文本和视觉数据，进一步借助 GPT-4 的能力进行精细数据合成和评估。尽管存在诸多挑战，如细微的解释和专业数据集的有限可用性，但我们的模型（GlassLLaVA）在制定准确的解释、识别关键特征和检测以前未见的SEM图像中的缺陷方面表现出色。此外，我们引入了适用于多种科学成像应用的灵活评估指标，使得进行综合评估成为可能。

    In the domain of scientific imaging, interpreting visual data often demands an intricate combination of human expertise and deep comprehension of the subject materials. This study presents a novel methodology to linguistically emulate and subsequently evaluate human-like interactions with Scanning Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a multimodal deep learning framework, our approach distills insights from both textual and visual data harvested from peer-reviewed articles, further augmented by the capabilities of GPT-4 for refined data synthesis and evaluation. Despite inherent challenges--such as nuanced interpretations and the limited availability of specialized datasets--our model (GlassLLaVA) excels in crafting accurate interpretations, identifying key features, and detecting defects in previously unseen SEM images. Moreover, we introduce versatile evaluation metrics, suitable for an array of scientific imaging applications, which allows for
    
[^73]: 多模态学习的理论

    A Theory of Multimodal Learning. (arXiv:2309.12458v1 [cs.LG])

    [http://arxiv.org/abs/2309.12458](http://arxiv.org/abs/2309.12458)

    这篇论文提供了一个理论框架来解释多模态学习中的一个有趣发现，即在单模态任务上，训练在多个模态上的模型可以胜过经过精细调节的单模态模型。

    

    人类对经验世界的感知涉及到识别基础物体的各种外观或“模态”。尽管哲学和认知科学领域长期以来一直考虑这一观点，但是在机器学习领域中，对多模态的研究相对较少。然而，目前关于多模态机器学习的研究仅限于经验实践，缺乏理论基础，只有启发式论证。多模态学习实践中的一个有趣发现是，在单模态任务上，训练在多个模态上的模型可以胜过经过精细调节的单模态模型。本文提供了一个理论框架来解释这一现象，通过研究多模态学习算法的泛化性质。我们证明了多模态学习相比于单模态学习具有更优的泛化界限，高达$O(\sqrt{n})$的因子，其中$n$表示样本大小。

    Human perception of the empirical world involves recognizing the diverse appearances, or 'modalities', of underlying objects. Despite the longstanding consideration of this perspective in philosophy and cognitive science, the study of multimodality remains relatively under-explored within the field of machine learning. Nevertheless, current studies of multimodal machine learning are limited to empirical practices, lacking theoretical foundations beyond heuristic arguments. An intriguing finding from the practice of multimodal learning is that a model trained on multiple modalities can outperform a finely-tuned unimodal model, even on unimodal tasks. This paper provides a theoretical framework that explains this phenomenon, by studying generalization properties of multimodal learning algorithms. We demonstrate that multimodal learning allows for a superior generalization bound compared to unimodal learning, up to a factor of $O(\sqrt{n})$, where $n$ represents the sample size. Such adva
    
[^74]: LongDocFACTScore: 评估长文档生成摘要的实证性。

    LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation. (arXiv:2309.12455v1 [cs.CL])

    [http://arxiv.org/abs/2309.12455](http://arxiv.org/abs/2309.12455)

    LongDocFACTScore是一种评估长文档生成摘要实证性的评估框架，可以解决传统自动评估度量标准无法评估长文档摘要事实一致性的问题。

    

    保持事实一致性是生成性文本摘要中的一个关键问题，然而，传统的用于评估文本摘要的自动度量标准（如ROUGE得分）无法评估事实一致性。最近，人们致力于开发使用预训练语言模型来测量事实一致性的改进度量标准，但这些度量标准有限制性的令牌限制，因此不适用于评估长文档生成摘要。此外，目前有限的研究评估了现有自动评估度量标准在应用于长文档数据集时是否适用。在这项工作中，我们评估了自动度量标准在评估长文档生成摘要的事实一致性方面的功效，并提出了一种新的评估框架LongDocFACTScore。该框架允许度量标准扩展到任意长度的文档。该框架在与人类事实一致性度量的相关性方面优于现有的最先进度量标准。

    Maintaining factual consistency is a critical issue in abstractive text summarisation, however, it cannot be assessed by traditional automatic metrics used for evaluating text summarisation, such as ROUGE scoring. Recent efforts have been devoted to developing improved metrics for measuring factual consistency using pre-trained language models, but these metrics have restrictive token limits, and are therefore not suitable for evaluating long document text summarisation. Moreover, there is limited research evaluating whether existing automatic evaluation metrics are fit for purpose when applied to long document data sets. In this work, we evaluate the efficacy of automatic metrics at assessing factual consistency in long document text summarisation and propose a new evaluation framework LongDocFACTScore. This framework allows metrics to be extended to any length document. This framework outperforms existing state-of-the-art metrics in its ability to correlate with human measures of fac
    
[^75]: 支撑鲁棒推断的凸框架

    A Convex Framework for Confounding Robust Inference. (arXiv:2309.12450v1 [stat.ML])

    [http://arxiv.org/abs/2309.12450](http://arxiv.org/abs/2309.12450)

    本文提出了一个支撑鲁棒推断的凸框架，通过利用凸规划提供策略价值的精确下界。此外，该方法还可以进行多种扩展，并且具有强理论保证。

    

    我们研究了受未观察到的混淆因素影响的离线上下文强化学习中的策略评估问题。传统的敏感性分析方法常被用来在给定的不确定性集合上估计在最坏混淆情况下的策略价值。然而，现有的工作通常为了可行性而采用一些粗糙的松弛不确定性集合的方法，导致对策略价值的估计过于保守。在本文中，我们提出了一种通用估计器，利用凸规划提供了策略价值的一个较为精确的下界。我们的估计器的广泛适用性使得其能够进行多种扩展，例如基于f-分歧的敏感性分析、基于交叉验证和信息准则的模型选择以及利用上界进行鲁棒策略学习等。此外，我们的估计方法可以通过强对偶性重新表述为经验风险最小化问题，从而利用M技术提供了对所提出估计器的强理论保证。

    We study policy evaluation of offline contextual bandits subject to unobserved confounders. Sensitivity analysis methods are commonly used to estimate the policy value under the worst-case confounding over a given uncertainty set. However, existing work often resorts to some coarse relaxation of the uncertainty set for the sake of tractability, leading to overly conservative estimation of the policy value. In this paper, we propose a general estimator that provides a sharp lower bound of the policy value using convex programming. The generality of our estimator enables various extensions such as sensitivity analysis with f-divergence, model selection with cross validation and information criterion, and robust policy learning with the sharp lower bound. Furthermore, our estimation method can be reformulated as an empirical risk minimization problem thanks to the strong duality, which enables us to provide strong theoretical guarantees of the proposed estimator using techniques of the M-
    
[^76]: 基于集成神经网络的剩余寿命预测

    Ensemble Neural Networks for Remaining Useful Life (RUL) Prediction. (arXiv:2309.12445v1 [cs.LG])

    [http://arxiv.org/abs/2309.12445](http://arxiv.org/abs/2309.12445)

    提出了一种使用集成神经网络的方法进行概率性剩余寿命预测，该方法解耦了来自系统和模型参数的不确定性，并且可以准确地建模和解释预测的信心。

    

    维护计划的核心部分是一个监测系统，它提供健康和退化的良好预测，通常被表示为剩余寿命(RUL)。目前大多数的数据驱动RUL预测方法都集中在单点预测上。这些点预测方法并没有考虑到故障的概率性质。到目前为止，少数的概率性方法要么包括来自系统的aleatoric不确定性，要么包括来自模型参数的epistemic不确定性，要么同时包含两者作为总的不确定性。在这里，我们提出了集成神经网络用于概率性RUL预测，考虑了这两种不确定性并将其解耦。这些解耦的不确定性在了解和解释预测的信心方面非常重要。这个方法在NASA的涡喷式发动机CMAPSS数据集上进行了测试。我们的结果展示了如何建模这些不确定性以及如何解开不确定性。

    A core part of maintenance planning is a monitoring system that provides a good prognosis on health and degradation, often expressed as remaining useful life (RUL). Most of the current data-driven approaches for RUL prediction focus on single-point prediction. These point prediction approaches do not include the probabilistic nature of the failure. The few probabilistic approaches to date either include the aleatoric uncertainty (which originates from the system), or the epistemic uncertainty (which originates from the model parameters), or both simultaneously as a total uncertainty. Here, we propose ensemble neural networks for probabilistic RUL predictions which considers both uncertainties and decouples these two uncertainties. These decoupled uncertainties are vital in knowing and interpreting the confidence of the predictions. This method is tested on NASA's turbofan jet engine CMAPSS data-set. Our results show how these uncertainties can be modeled and how to disentangle the cont
    
[^77]: 利用生成建模在数字孪生中进行变更管理

    Change Management using Generative Modeling on Digital Twins. (arXiv:2309.12421v1 [cs.CR])

    [http://arxiv.org/abs/2309.12421](http://arxiv.org/abs/2309.12421)

    本论文展示了如何利用生成建模在数字孪生中进行变更管理，为小型和中型企业提供了安全地管理软件更新和变更的解决方案。

    

    小型和中型企业面临的主要挑战之一是安全地管理软件更新和变更。特别是在快速发展的网络安全威胁下，需对软件系统进行必要的变更/更新/补丁，以保持对新兴威胁的应对能力，同时也经常是监管机构要求的。但是，安全补丁/更新需要在生产系统发布之前进行压力测试。在生产环境中进行压力测试存在风险并带来安全威胁。大型企业通常具有非生产环境，可以在发布到生产环境之前进行变更和测试。然而，小型企业没有这样的设施。在这项工作中，我们展示了如何在云上创建“数字孪生”，特别是针对IT和物联网环境的混合环境。这些数字孪生可以作为非生产环境，在这里可以进行变更，并可以在发布补丁之前进行安全测试。

    A key challenge faced by small and medium-sized business entities is securely managing software updates and changes. Specifically, with rapidly evolving cybersecurity threats, changes/updates/patches to software systems are necessary to stay ahead of emerging threats and are often mandated by regulators or statutory authorities to counter these. However, security patches/updates require stress testing before they can be released in the production system. Stress testing in production environments is risky and poses security threats. Large businesses usually have a non-production environment where such changes can be made and tested before being released into production. Smaller businesses do not have such facilities. In this work, we show how "digital twins", especially for a mix of IT and IoT environments, can be created on the cloud. These digital twins act as a non-production environment where changes can be applied, and the system can be securely tested before patch release. Additio
    
[^78]: 通过针对网络层进行低秩分解加速Resnet架构

    Speeding up Resnet Architecture with Layers Targeted Low Rank Decomposition. (arXiv:2309.12412v1 [cs.CV])

    [http://arxiv.org/abs/2309.12412](http://arxiv.org/abs/2309.12412)

    本研究通过针对网络进行低秩分解的压缩方法，加速了Resnet架构的训练和推断，通过对ResNet50的研究案例，证明了硬件目标压缩的优势。

    

    对神经网络进行压缩可以加速网络的训练和推断。本研究研究了在网络层上应用低秩分解进行压缩的方法。我们的研究表明，为了获得加速效果，压缩方法应该考虑底层硬件，并进行分析选择需要压缩的层次。我们通过对ResNet50进行压缩和在完整的ImageNet-ILSVRC2012上训练的案例研究来展示我们的方法的优势。我们在两个不同的硬件系统Nvidia V100和华为Ascend910上进行了测试。通过针对硬件的压缩，Ascend910上的结果显示相比原始未压缩模型，训练加速了5.36%，在Ascend310上推断速度提高了15.79%，仅有1%的准确率下降。

    Compression of a neural network can help in speeding up both the training and the inference of the network. In this research, we study applying compression using low rank decomposition on network layers. Our research demonstrates that to acquire a speed up, the compression methodology should be aware of the underlying hardware as analysis should be done to choose which layers to compress. The advantage of our approach is demonstrated via a case study of compressing ResNet50 and training on full ImageNet-ILSVRC2012. We tested on two different hardware systems Nvidia V100 and Huawei Ascend910. With hardware targeted compression, results on Ascend910 showed 5.36% training speedup and 15.79% inference speed on Ascend310 with only 1% drop in accuracy compared to the original uncompressed model
    
[^79]: 内存高效的混合精度优化器

    Memory Efficient Mixed-Precision Optimizers. (arXiv:2309.12381v1 [cs.LG])

    [http://arxiv.org/abs/2309.12381](http://arxiv.org/abs/2309.12381)

    本论文提出了一种内存高效的混合精度优化算法，通过消除参数的浮点副本和去除梯度值的方法，实现了高效降低内存使用和加快训练速度，同时保持模型准确性。

    

    传统的优化方法依赖于使用单精度浮点运算，这在内存大小和计算能力方面都是昂贵的。然而，混合精度优化技术利用单精度和半精度浮点运算相结合，以减少内存需求同时保持模型准确性。我们提供了一种算法，在模型训练过程中进一步减少内存使用，通过消除参数的浮点副本，实际上只保留半精度数字。我们还探索了在后向传播期间通过执行优化器步骤来去除梯度值的好处。实践中，我们实现了高达25%的峰值内存降低和15%的更快训练速度，同时保持相同水平的准确性。

    Traditional optimization methods rely on the use of single-precision floating point arithmetic, which can be costly in terms of memory size and computing power. However, mixed precision optimization techniques leverage the use of both single and half-precision floating point arithmetic to reduce memory requirements while maintaining model accuracy. We provide here an algorithm to further reduce memory usage during the training of a model by getting rid of the floating point copy of the parameters, virtually keeping only half-precision numbers. We also explore the benefits of getting rid of the gradient's value by executing the optimizer step during the back-propagation. In practice, we achieve up to 25% lower peak memory use and 15% faster training while maintaining the same level of accuracy.
    
[^80]: 生成和评估合成纵向患者数据的方法：一项系统综述

    Methods for generating and evaluating synthetic longitudinal patient data: a systematic review. (arXiv:2309.12380v1 [stat.ME])

    [http://arxiv.org/abs/2309.12380](http://arxiv.org/abs/2309.12380)

    本文对生成和评估合成纵向患者数据的方法进行了系统综述，以解决医学领域中数据使用和隐私保护的问题。

    

    近年来数据的迅猛增长促进了各种统计和深度学习技术的发展和应用，加快了研究和开发活动。然而，并非所有行业都能从数据的增加中同等受益，部分原因是由于数据使用和隐私规定的法律限制，例如医学领域。为了解决这个问题，提出了各种统计披露和隐私保护方法，包括使用合成数据生成。合成数据是基于一些现有数据生成的，目的是尽可能地复制它们，并充当真实敏感数据的代理。本文对生成和评估合成纵向患者数据的方法进行了系统综述，这是医学领域中一种常见的数据类型。该综述遵循PRISMA指南，并涵盖了自2022年底以来的五个数据库的文献。本文描述了17种方法，从传统方法到深度学习方法。

    The proliferation of data in recent years has led to the advancement and utilization of various statistical and deep learning techniques, thus expediting research and development activities. However, not all industries have benefited equally from the surge in data availability, partly due to legal restrictions on data usage and privacy regulations, such as in medicine. To address this issue, various statistical disclosure and privacy-preserving methods have been proposed, including the use of synthetic data generation. Synthetic data are generated based on some existing data, with the aim of replicating them as closely as possible and acting as a proxy for real sensitive data. This paper presents a systematic review of methods for generating and evaluating synthetic longitudinal patient data, a prevalent data type in medicine. The review adheres to the PRISMA guidelines and covers literature from five databases until the end of 2022. The paper describes 17 methods, ranging from traditi
    
[^81]: 对特级初榨橄榄油的老化进行研究：利用荧光光谱和机器学习技术探究温度的影响

    Shedding Light on the Ageing of Extra Virgin Olive Oil: Probing the Impact of Temperature with Fluorescence Spectroscopy and Machine Learning Techniques. (arXiv:2309.12377v1 [cs.LG])

    [http://arxiv.org/abs/2309.12377](http://arxiv.org/abs/2309.12377)

    本研究使用荧光光谱和机器学习技术探究了温度对特级初榨橄榄油的老化影响，并提出了一种基于机器学习的方法来评估油质量。

    

    本研究系统地调查了在加速储存条件下使用UV吸收和总荧光光谱对特级初榨橄榄油（EVOO）进行氧化研究。通过收集的大量数据，提出了一种基于机器学习应用于高度聚合数据的方法来监测油质量。尽管特级初榨橄榄油以其众多的健康益处和卓越的口味而享誉全球，但它随着时间的推移会发生降解，由于氧化作用，其健康品质和风味都可能受到影响。因此，量化氧化对特级初榨橄榄油的影响并开发易于在现场条件下实施而非专门实验室的评估方法非常重要。这项研究表明，荧光光谱具有监测氧化效果和评估特级初榨橄榄油质量的能力，即使数据高度聚合也可以实现。

    This work systematically investigates the oxidation of extra virgin olive oil (EVOO) under accelerated storage conditions with UV absorption and total fluorescence spectroscopy. With the large amount of data collected, it proposes a method to monitor the oil's quality based on machine learning applied to highly-aggregated data. EVOO is a high-quality vegetable oil that has earned worldwide reputation for its numerous health benefits and excellent taste. Despite its outstanding quality, EVOO degrades over time owing to oxidation, which can affect both its health qualities and flavour. Therefore, it is highly relevant to quantify the effects of oxidation on EVOO and develop methods to assess it that can be easily implemented under field conditions, rather than in specialized laboratories. The following study demonstrates that fluorescence spectroscopy has the capability to monitor the effect of oxidation and assess the quality of EVOO, even when the data are highly aggregated. It shows t
    
[^82]: Fairness Hub技术简报: AUC Gap

    Fairness Hub Technical Briefs: AUC Gap. (arXiv:2309.12371v1 [cs.LG])

    [http://arxiv.org/abs/2309.12371](http://arxiv.org/abs/2309.12371)

    本论文介绍了一种称为AUC Gap的指标，它可以测量AI/ML模型在不同子群体中的性能差异，从而实现非二元的公平评估，为实现共同目标提供了基准和策略分享的基础。

    

    为了测量偏见，我们鼓励团队考虑使用AUC Gap：子群体（例如性别、种族、SES、先前知识）的最高和最低测试AUC的绝对差异。它不依赖于AI/ML算法，并捕捉模型在任意数量的子群体中的性能差异，从而实现了非二元的公平评估，例如针对交叉身份群体。LEVI团队在追求在低收入中学中将数学成就翻倍的共同目标时，使用各种AI/ML模型。确保这些模型在许多不同背景下收集的数据集上训练时不引入或放大偏见，对于实现LEVI目标非常重要。我们在此提供了一种灵活且易于计算的模型偏见度量方法，供所有LEVI团队使用，以创建一个共同的基准和分析基础，用于共享不同团队已经采取的策略。

    To measure bias, we encourage teams to consider using AUC Gap: the absolute difference between the highest and lowest test AUC for subgroups (e.g., gender, race, SES, prior knowledge). It is agnostic to the AI/ML algorithm used and it captures the disparity in model performance for any number of subgroups, which enables non-binary fairness assessments such as for intersectional identity groups. The LEVI teams use a wide range of AI/ML models in pursuit of a common goal of doubling math achievement in low-income middle schools. Ensuring that the models, which are trained on datasets collected in many different contexts, do not introduce or amplify biases is important for achieving the LEVI goal. We offer here a versatile and easy-to-compute measure of model bias for all LEVI teams in order to create a common benchmark and an analytical basis for sharing what strategies have worked for different teams.
    
[^83]: 在复杂医疗决策中重新思考人工智能与人类的合作：以脓毒症诊断为案例研究

    Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis. (arXiv:2309.12368v1 [cs.HC])

    [http://arxiv.org/abs/2309.12368](http://arxiv.org/abs/2309.12368)

    本研究探索了在复杂医疗决策中重新思考人工智能与人类合作的设计要求，以脓毒症诊断为例。研究发现，在人工智能系统中，支持临床专家在决策过程的中间阶段发挥作用（如生成假设或收集数据）是至关重要的，而不仅仅关注最终决策。

    

    如今的医疗决策支持人工智能系统在研究论文中取得了成功，但在实际部署中却面临失败的问题。本研究聚焦于脓毒症的决策过程，这是一种需要临床医生早期高度不确定性诊断的急性致命全身性感染。我们的目标是探索能够支持临床专家做出更好脓毒症早期诊断决策的人工智能系统的设计要求。研究从一个形成性研究开始，调查为什么临床专家在电子病历系统中放弃了一个现有的脓毒症预测模块。我们认为，一个以人为中心的人工智能系统需要在医疗决策过程的中间阶段（如生成假设或收集数据）支持人类专家，而不仅仅关注最终决策。因此，我们基于先进的人工智能算法构建了SepsisLab，并将其扩展到预测未来趋势。

    Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection o
    
[^84]: 在基于GPT的智能辅导系统中研究领域知识库不同程度的影响

    Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors. (arXiv:2309.12367v1 [cs.HC])

    [http://arxiv.org/abs/2309.12367](http://arxiv.org/abs/2309.12367)

    本文研究了在基于GPT的智能辅导系统中将领域知识库与语言模型集成，以提高回答的可靠性。通过设计可扩展的知识库和评估实验，我们展示了该系统的有效性。学生和领域专家对于智能辅导系统的回答进行了验证和排名。

    

    最近大型语言模型（LLM）的进展促进了具有复杂对话能力的聊天机器人的发展。然而，LLM对查询的回答经常不准确，这限制了在教育环境中的应用。本文研究了将知识库（KB）与LLM智能辅导系统集成以增加回答可靠性的效果。为了实现这一目标，我们设计了一个可扩展的知识库，教育监督员可以无缝集成课程，该课程会被智能辅导系统自动处理。然后，我们详细介绍了一个评估实验，学生参与者需要回答有关人工智能课程的问题。 GPT-4智能辅导系统具有不同层次的KB访问权限，并由人类领域专家评估这些回答。最后，学生对智能辅导系统的回答进行了与领域专家的交叉验证，并对它们的各种教学能力进行了排名。

    Recent advancements in large language models (LLMs) have facilitated the development of chatbots with sophisticated conversational capabilities. However, LLMs exhibit frequent inaccurate responses to queries, hindering applications in educational settings. In this paper, we investigate the effectiveness of integrating a knowledge base (KB) with LLM intelligent tutors to increase response reliability. To achieve this, we design a scaleable KB that affords educational supervisors seamless integration of lesson curricula, which is automatically processed by the intelligent tutoring system. We then detail an evaluation, where student participants were presented with questions about the artificial intelligence curriculum to respond to. GPT-4 intelligent tutors with varying hierarchies of KB access and human domain experts then assessed these responses. Lastly, students cross-examined the intelligent tutors' responses to the domain experts' and ranked their various pedagogical abilities. Res
    
[^85]: 通过自然语言处理和抽样实现高效的社会选择

    Efficient Social Choice via NLP and Sampling. (arXiv:2309.12360v1 [cs.CY])

    [http://arxiv.org/abs/2309.12360](http://arxiv.org/abs/2309.12360)

    本文通过结合自然语言处理和抽样技术，提出了一种高效的注意力感知社会选择系统，该系统使用训练有素的NLP模型估计了提案通过的概率，并通过采样多数来决定提案。

    

    注意力感知社会选择解决了一些代理社区面临的基本冲突，即在决策过程中包括所有成员的渴望与社区成员可支配的有限时间和注意力之间的矛盾。本文研究了注意力感知社会选择的两种技术组合，即自然语言处理（NLP）和抽样。基本上，我们提出了一个系统，其中每个改变现状的治理提案首先发送到训练有素的NLP模型，该模型估计了如果所有社区成员直接对其进行投票，该提案通过的概率；然后，基于这种估计，选择一个确定大小的人群样本，并通过采样多数决定提案。我们根据上述方案开发了几种具体算法，并使用各种数据进行评估，包括多个分散自治组织（DAO）的数据。

    Attention-Aware Social Choice tackles the fundamental conflict faced by some agent communities between their desire to include all members in the decision making processes and the limited time and attention that are at the disposal of the community members. Here, we investigate a combination of two techniques for attention-aware social choice, namely Natural Language Processing (NLP) and Sampling. Essentially, we propose a system in which each governance proposal to change the status quo is first sent to a trained NLP model that estimates the probability that the proposal would pass if all community members directly vote on it; then, based on such an estimation, a population sample of a certain size is being selected and the proposal is decided upon by taking the sample majority. We develop several concrete algorithms following the scheme described above and evaluate them using various data, including such from several Decentralized Autonomous Organizations (DAOs).
    
[^86]: 通过测序和多模态推理对抗解释并直接揭示偏见

    Antagonising explanation and revealing bias directly through sequencing and multimodal inference. (arXiv:2309.12345v1 [cs.HC])

    [http://arxiv.org/abs/2309.12345](http://arxiv.org/abs/2309.12345)

    本研究通过测序和多模态推理，对抗解释并直接揭示偏见，探讨了利用生成模型进行未来计算的潜力和应用，特别是在电影和视听艺术领域。研究发现，通过承认记录的存在，将过去与未来紧密联系起来，可以更好地实现生成模型的效果。

    

    深度生成模型通过学习表示来生成数据，例如扩散模型，通过近似计算可能的样本。近似可以被理解为重建，用于训练模型的大型数据集可以看作是用某种数据结构（照片、音频记录、手稿）来表示物理世界的记录集。在重建过程中，例如，图像帧会根据学习的偏见逐步发展成为文本输入描述。我们在这里提出，生成过程是可以被认为是倒退的时间；不是通过对倒退扩散过程的灵感，而是承认文化在记录中特有的标记。生成模型在未来，特别是在电影和视听艺术领域，可以通过将扩散系统视为一种将未来计算与过去紧密联系的过程来获益，只要承认记录的存在。

    Deep generative models produce data according to a learned representation, e.g. diffusion models, through a process of approximation computing possible samples. Approximation can be understood as reconstruction and the large datasets used to train models as sets of records in which we represent the physical world with some data structure (photographs, audio recordings, manuscripts). During the process of reconstruction, e.g., image frames develop each timestep towards a textual input description. While moving forward in time, frame sets are shaped according to learned bias and their production, we argue here, can be considered as going back in time; not by inspiration on the backward diffusion process but acknowledging culture is specifically marked in the records. Futures of generative modelling, namely in film and audiovisual arts, can benefit by dealing with diffusion systems as a process to compute the future by inevitably being tied to the past, if acknowledging the records as to 
    
[^87]: 大型语言模型中的文化对齐：基于霍夫斯泰德文化维度的解释性分析

    Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions. (arXiv:2309.12342v1 [cs.CY])

    [http://arxiv.org/abs/2309.12342](http://arxiv.org/abs/2309.12342)

    本研究提出了一种使用霍夫斯泰德文化维度框架来量化大型语言模型与不同文化之间的对齐程度的文化对齐测试（CAT）。通过在不同文化国家应用该方法，我们发现LLMs在解释性文化维度上存在差异，并能量化其与特定国家的文化对齐情况。

    

    大型语言模型（LLMs）的部署引发了关于其文化对齐和对不同文化规范个体的潜在影响的担忧。现有研究主要关注政治和社会偏见以及公众意见，而未涉及文化价值观。为了解决这个局限性，我们提出了文化对齐测试（CAT），利用霍夫斯泰德的文化维度框架量化文化对齐，通过潜变量分析提供解释性的跨文化比较。我们将该方法应用于评估最先进的LLMs（如ChatGPT和Bard）在不同文化国家（美国、沙特阿拉伯、中国和斯洛伐克）中嵌入的文化价值观，使用不同的提示风格和超参数设置。我们的结果不仅量化了LLMs与特定国家的文化对齐程度，而且揭示了LLMs在解释性文化维度上的差异。尽管所有的LLMs都没有提供令人满意的结果

    The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms. Existing work investigated political and social biases and public opinions rather than their cultural values. To address this limitation, the proposed Cultural Alignment Test (CAT) quantifies cultural alignment using Hofstede's cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to assess the cultural values embedded in state-of-the-art LLMs, such as: ChatGPT and Bard, across diverse cultures of countries: United States (US), Saudi Arabia, China, and Slovakia, using different prompting styles and hyperparameter settings. Our results not only quantify cultural alignment of LLMs with certain countries, but also reveal the difference between LLMs in explanatory cultural dimensions. While all LLMs did not provide satisfactory res
    
[^88]: 深度知识追踪是一个隐含的动态多维项目反应理论模型

    Deep Knowledge Tracing is an implicit dynamic multidimensional item response theory model. (arXiv:2309.12334v1 [cs.CY])

    [http://arxiv.org/abs/2309.12334](http://arxiv.org/abs/2309.12334)

    本文将深度知识追踪视为一种编码器-解码器结构，通过在多个数据集上进行实验发现，一个更简单的解码器可以比DKT预测学生表现更好。

    

    知识追踪是根据学生先前问题的表现来预测其在新问题上的表现，这可以作为优化评估和学习的先前步骤。深度知识追踪（DKT）是一种基于循环神经网络的知识追踪竞争模型，即使一些更简单的模型可能与其性能相匹配，但对于为什么DKT能够如此成功的了解还很少。在本文中，我们将深度知识追踪视为一种编码器-解码器结构。这个观点不仅使我们能够在性能、简单性或表达性方面提出更好的模型，还为未来的研究方向打开了有希望的途径。特别是，我们在几个小型和大型数据集上展示了一个更简单的解码器，可能比DKT使用的解码器参数更少，可以更好地预测学生表现。

    Knowledge tracing consists in predicting the performance of some students on new questions given their performance on previous questions, and can be a prior step to optimizing assessment and learning. Deep knowledge tracing (DKT) is a competitive model for knowledge tracing relying on recurrent neural networks, even if some simpler models may match its performance. However, little is known about why DKT works so well. In this paper, we frame deep knowledge tracing as a encoderdecoder architecture. This viewpoint not only allows us to propose better models in terms of performance, simplicity or expressivity but also opens up promising avenues for future research directions. In particular, we show on several small and large datasets that a simpler decoder, with possibly fewer parameters than the one used by DKT, can predict student performance better.
    
[^89]: 使用UBET自动市场制造者进行链上体育博彩

    Onchain Sports Betting using UBET Automated Market Maker. (arXiv:2309.12333v1 [cs.CY])

    [http://arxiv.org/abs/2309.12333](http://arxiv.org/abs/2309.12333)

    本文介绍了一种使用UBET自动市场制造者进行链上体育博彩的方法，通过此方法可以解决传统中心化平台的缺点，确保透明度、安全性和较低的费用。这种方法利用智能合约和算法来定价体育赔率，提供流动性并实现全球可访问性。

    

    本文强调了体育博彩中的去中心化解决了传统中心化平台的缺点，确保了透明度、安全性和较低的费用。非监管解决方案使投注者拥有资金所有权，绕过了地理限制。去中心化平台增强了安全性、隐私性和民主决策。然而，去中心化体育博彩需要自动市场制造者（AMMs）来提供有效的流动性。现有的AMMs如Uniswap缺乏与公平赔率的一致性，给流动性提供者带来了风险。为了缓解这个问题，本文引入了UBET AMM（UAMM），利用智能合约和算法来公平定价体育赔率。它建立了一个链上博彩框架，详细介绍市场创建、UAMM应用、抵押流动性池以及展示积极结果的实验。UAMM通过确保流动性、去中心化定价和全球可访问性来增强去中心化体育博彩，促进无需信任的

    The paper underscores how decentralization in sports betting addresses the drawbacks of traditional centralized platforms, ensuring transparency, security, and lower fees. Non-custodial solutions empower bettors with ownership of funds, bypassing geographical restrictions. Decentralized platforms enhance security, privacy, and democratic decision-making. However, decentralized sports betting necessitates automated market makers (AMMs) for efficient liquidity provision. Existing AMMs like Uniswap lack alignment with fair odds, creating risks for liquidity providers. To mitigate this, the paper introduces UBET AMM (UAMM), utilizing smart contracts and algorithms to price sports odds fairly. It establishes an on-chain betting framework, detailing market creation, UAMM application, collateral liquidity pools, and experiments that exhibit positive outcomes. UAMM enhances decentralized sports betting by ensuring liquidity, decentralized pricing, and global accessibility, promoting trustless 
    
[^90]: 使用高光谱图像和多块非负矩阵分解进行单一/多物质表征

    Mono/Multi-material Characterization Using Hyperspectral Images and Multi-Block Non-Negative Matrix Factorization. (arXiv:2309.12329v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.12329](http://arxiv.org/abs/2309.12329)

    使用高光谱图像和多块非负矩阵分解方法进行单一/多物质的表征，在塑料分类中具有重要的应用价值。

    

    塑料分类是废物管理中非常重要的一步，特别是由于多层塑料的存在。这些单一物质和多物质塑料广泛应用于增强包装的功能性能，结合了厚度、机械强度和耐热性等有益特性。然而，含有多种聚合物物种的材料在回收为单一物质前需要进行预处理，因此不应该出现在单一物质流中。工业4.0大大改进了塑料包装材料分类的速度和准确性，特别是通过近红外高光谱成像(NIRHSI)提供了自动化、快速和准确的物质表征，无需进行样品制备。然而，用HSI识别多物质需要新的专用化学模式识别方法。非负矩阵分解(NMF)被广泛应用于化学分辨率

    Plastic sorting is a very essential step in waste management, especially due to the presence of multilayer plastics. These monomaterial and multimaterial plastics are widely employed to enhance the functional properties of packaging, combining beneficial properties in thickness, mechanical strength, and heat tolerance. However, materials containing multiple polymer species need to be pretreated before they can be recycled as monomaterials and therefore should not end up in monomaterial streams. Industry 4.0 has significantly improved materials sorting of plastic packaging in speed and accuracy compared to manual sorting, specifically through Near Infrared Hyperspectral Imaging (NIRHSI) that provides an automated, fast, and accurate material characterization, without sample preparation. Identification of multimaterials with HSI however requires novel dedicated approaches for chemical pattern recognition. Non negative Matrix Factorization, NMF, is widely used for the chemical resolution 
    
[^91]: FUTURE-AI：在医疗保健领域的可信和可部署人工智能的国际共识指南

    FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])

    [http://arxiv.org/abs/2309.12325](http://arxiv.org/abs/2309.12325)

    FUTURE-AI是第一个国际共识框架，为医疗保健领域的可信AI工具开发和部署提供指导原则和最佳实践。

    

    尽管在医学和医疗保健领域人工智能（AI）取得了重大进展，但AI技术在现实临床实践中的部署和采用仍受限。近年来，人们对医疗AI的技术、临床、伦理和法律风险提出了关注。为了增加在现实世界中的采用，医疗AI工具必须得到患者、临床医生、健康组织和当局的信任和接受。本文描述了FUTURE-AI指南作为第一个用于指导医疗保健领域可信AI工具开发和部署的国际共识框架。FUTURE-AI联盟成立于2021年，目前包括来自51个国家的118位跨学科专家，代表了所有大洲，包括AI科学家、临床医生、伦理学家和社会科学家。在为期两年的时间里，联盟通过迭代过程定义了可信AI的指导原则和最佳实践，其中包括

    Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
    
[^92]: 航空安全风险分析和飞行技术评估问题

    Aviation Safety Risk Analysis and Flight Technology Assessment Issues. (arXiv:2309.12324v1 [cs.CY])

    [http://arxiv.org/abs/2309.12324](http://arxiv.org/abs/2309.12324)

    该论文研究了中国民航业中飞行安全的重要性，并提出了解决超额事件分析不充分原因的方法。通过数据处理、可靠性评估、神经网络飞行控制、数据分析、机器学习飞行人员评估和实时警报等手段，旨在提升航空安全、人员评估和警报机制。

    

    本文重点强调了中国民航业中飞行安全的重要性，并强调了全面研究的必要性。它主要关注两个方面：分析超额事件和统计评估非超额数据。当前方法的挑战在于对超额事件不充分的原因分析。提出的解决方案包括数据预处理、可靠性评估、利用神经网络量化飞行控制、探索性数据分析、利用机器学习评估飞行人员技能和建立实时自动警报。这些努力旨在增强飞行安全、人员评估和警报机制，为更安全、更高效的民航业做出贡献。

    This text highlights the significance of flight safety in China's civil aviation industry and emphasizes the need for comprehensive research. It focuses on two main areas: analyzing exceedance events and statistically evaluating non-exceedance data. The challenges of current approaches lie in insufficient cause analysis for exceedances. The proposed solutions involve data preprocessing, reliability assessment, quantifying flight control using neural networks, exploratory data analysis, flight personnel skill evaluation with machine learning, and establishing real-time automated warnings. These endeavors aim to enhance flight safety, personnel assessment, and warning mechanisms, contributing to a safer and more efficient civil aviation sector.
    
[^93]: 评估生成模型提出的材料的多样性和实用性

    Evaluating the diversity and utility of materials proposed by generative models. (arXiv:2309.12323v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.12323](http://arxiv.org/abs/2309.12323)

    本研究评估了物理引导的晶体生成模型（PGCGM）的多样性和实用性，并提出了改进生成模型以实现更好逆向设计的建议。

    

    生成性机器学习模型可以利用科学建模生成的数据创建大量新颖的材料结构。在这里，我们评估了一个最先进的生成模型，物理引导的晶体生成模型（PGCGM），在逆向设计过程中的应用。我们表明，默认的PGCGM的输入空间在参数变化方面并不平滑，使得材料优化困难且受限。我们还证明，大多数生成的结构被一个单独的属性预测模型预测为热力学不稳定，部分原因是由于域外数据挑战。我们的研究结果表明，如何改进生成模型以实现更好的逆向设计。

    Generative machine learning models can use data generated by scientific modeling to create large quantities of novel material structures. Here, we assess how one state-of-the-art generative model, the physics-guided crystal generation model (PGCGM), can be used as part of the inverse design process. We show that the default PGCGM's input space is not smooth with respect to parameter variation, making material optimization difficult and limited. We also demonstrate that most generated structures are predicted to be thermodynamically unstable by a separate property-prediction model, partially due to out-of-domain data challenges. Our findings suggest how generative models might be improved to enable better inverse design.
    
[^94]: 神经表征的拓扑和几何结构

    The Topology and Geometry of Neural Representations. (arXiv:2309.11028v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.11028](http://arxiv.org/abs/2309.11028)

    本文探索了从几何结构到拓扑结构的抽象步骤，并提出了一种拓扑表征相似分析方法（tRSA），通过一系列地理拓扑摘要统计量对大脑表征进行表征。

    

    神经科学所关心的一个核心问题是如何表征感知和认知内容的大脑表征。一个理想的表征应该能够区分不同的功能区域，并且对噪声和个体大脑的特异性具有稳健性，而不会与计算差异相对应。以前的研究通过表征几何结构来表征大脑表征，几何结构由表征不相似矩阵（RDM）定义，RDM是一个摘要统计量，摘要了个体神经元（或响应通道）的作用，并表征了刺激的可辨别性。在这里，我们进一步探索了从几何结构到大脑表征拓扑的抽象步骤。我们提出了拓扑表征相似分析（tRSA），它是表征相似分析（RSA）的一种扩展，使用了一系列地理拓扑摘要统计量，将RDM进行泛化以表征拓扑结构并减弱几何结构的作用。

    A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. 
    
[^95]: Des-q: 一种用于回归和二分类的构建和高效重新训练决策树的量子算法

    Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification. (arXiv:2309.09976v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2309.09976](http://arxiv.org/abs/2309.09976)

    Des-q是一种量子算法，用于在回归和二分类任务中构建和重新训练决策树。它显著减少了树重新训练所需的时间复杂度，并且能够处理新样本的加载时间。该算法通过 k 分段线性树分裂来构建决策树，将数据划分为不同的子空间。

    

    决策树由于其简单构造和可解释性而广泛应用于机器学习。然而，随着数据规模的增长，传统的决策树构建和重新训练方法变得越来越慢，与训练样本数量呈多项式规模。在本研究中，我们介绍了一种新颖的量子算法Des-q，用于在回归和二分类任务中构建和重新训练决策树。假设数据流产生较小的新训练样本增量，我们证明了我们的Des-q算法显著减少了树重新训练所需的时间，即使考虑将新样本加载到量子可访问内存所需的时间，其时间复杂度也达到了多对数级别。我们的方法涉及构建一个决策树算法，在每个内部节点执行k分段线性树分裂。这些分裂同时生成多个超平面，将数据划分为不同的子空间。

    Decision trees are widely used in machine learning due to their simplicity in construction and interpretability. However, as data sizes grow, traditional methods for constructing and retraining decision trees become increasingly slow, scaling polynomially with the number of training examples. In this work, we introduce a novel quantum algorithm, named Des-q, for constructing and retraining decision trees in regression and binary classification tasks. Assuming the data stream produces small increments of new training examples, we demonstrate that our Des-q algorithm significantly reduces the time required for tree retraining, achieving a poly-logarithmic time complexity in the number of training examples, even accounting for the time needed to load the new examples into quantum-accessible memory. Our approach involves building a decision tree algorithm to perform k-piecewise linear tree splits at each internal node. These splits simultaneously generate multiple hyperplanes, dividing the
    
[^96]: FedDCSR: 通过解缠表示学习实现联邦跨领域顺序推荐

    FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])

    [http://arxiv.org/abs/2309.08420](http://arxiv.org/abs/2309.08420)

    提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。

    

    近年来，利用来自多个领域的用户序列数据的跨领域顺序推荐(CSR)受到了广泛关注。然而，现有的CSR方法需要在领域之间共享原始用户数据，这违反了《通用数据保护条例》(GDPR)。因此，有必要将联邦学习(FL)和CSR相结合，充分利用不同领域的知识，同时保护数据隐私。然而，不同领域之间的序列特征异质性对FL的整体性能有显著影响。在本文中，我们提出了FedDCSR，这是一种通过解缠表示学习的新型联邦跨领域顺序推荐框架。具体而言，为了解决不同领域之间的序列特征异质性，我们引入了一种称为领域内-领域间序列表示解缠(SRD)的方法，将用户序列特征解缠成领域共享和领域专属特征。

    Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
    
[^97]: 基于学习的局部对比和最大似然估计的实时微弱空间碎片探测器

    A Real-time Faint Space Debris Detector With Learning-based LCM. (arXiv:2309.08244v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2309.08244](http://arxiv.org/abs/2309.08244)

    本文提出了一种基于学习的方法来解决低强度和高角速度下的微弱空间碎片探测问题。该方法使用局部对比和最大似然估计相结合的方式，能够高效地检测信噪比为2.0的空间物体。

    

    随着航空航天技术的发展，空间碎片的不断增加对宇宙飞船的安全构成了巨大威胁。然而，空间碎片反射光的低强度和高角速度阻碍了提取工作。此外，由于地面观测方法的限制，小型空间碎片很难被检测到，因此需要加强航天器的空间态势感知能力。考虑到传统方法在低信噪比目标检测方面存在一些缺陷，如效果低下和时间消耗大，本文提出了一种基于局部对比和最大似然估计的低信噪比条纹提取方法，能够高效地检测信噪比为2.0的空间物体。在提出的算法中，将使用局部对比进行粗略分类，返回连接部件作为初步结果，然后进行最大似然估计来重建连接部件。

    With the development of aerospace technology, the increasing population of space debris has posed a great threat to the safety of spacecraft. However, the low intensity of reflected light and high angular velocity of space debris impede the extraction. Besides, due to the limitations of the ground observation methods, small space debris can hardly be detected, making it necessary to enhance the spacecraft's capacity for space situational awareness (SSA). Considering that traditional methods have some defects in low-SNR target detection, such as low effectiveness and large time consumption, this paper proposes a method for low-SNR streak extraction based on local contrast and maximum likelihood estimation (MLE), which can detect space objects with SNR 2.0 efficiently. In the proposed algorithm, local contrast will be applied for crude classifications, which will return connected components as preliminary results, and then MLE will be performed to reconstruct the connected components of 
    
[^98]: 折叠注意力：面向设备的Transformer流式语音识别的内存和功耗优化

    Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition. (arXiv:2309.07988v1 [cs.LG])

    [http://arxiv.org/abs/2309.07988](http://arxiv.org/abs/2309.07988)

    本论文提出了一种名为折叠注意力的技术，在基于Transformer的流式语音识别模型中，通过减少线性投影层的数量，显著减小了模型大小，提高了内存和功耗效率，实验证明可以将模型大小减小24%、功耗减小23%。

    

    基于Transformer的模型在语音识别中表现出色。现有的用于优化Transformer推断的努力，通常针对长上下文应用，主要集中在简化注意力得分计算上。然而，流式语音识别模型通常每次只处理有限数量的令牌，因此注意力得分计算在这种情况下并不是瓶颈所在。相反，瓶颈在于多头注意力和前馈网络的线性投影层，它们构成了模型大小的相当部分，并对计算、内存和功耗的使用产生重要影响。为了解决这一瓶颈，我们提出了折叠注意力，这是一种针对这些线性层的技术，显著减小了模型大小，并提高了内存和功耗效率。设备上的基于Transformer的流式语音识别模型的实验证明，折叠注意力可以将模型大小（和相应的内存消耗）减小多达24%，并将功耗减小多达23%，而无需进行补充。

    Transformer-based models excel in speech recognition. Existing efforts to optimize Transformer inference, typically for long-context applications, center on simplifying attention score calculations. However, streaming speech recognition models usually process a limited number of tokens each time, making attention score calculation less of a bottleneck. Instead, the bottleneck lies in the linear projection layers of multi-head attention and feedforward networks, constituting a substantial portion of the model size and contributing significantly to computation, memory, and power usage.  To address this bottleneck, we propose folding attention, a technique targeting these linear layers, significantly reducing model size and improving memory and power efficiency. Experiments on on-device Transformer-based streaming speech recognition models show that folding attention reduces model size (and corresponding memory consumption) by up to 24% and power consumption by up to 23%, all without comp
    
[^99]: Virchow: 数百万张全数字病理学基础模型

    Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])

    [http://arxiv.org/abs/2309.07778](http://arxiv.org/abs/2309.07778)

    Virchow是一个数百万参数的深度神经网络基础模型，通过在数百万张全数字病理学切片图像上进行自监督学习训练，有效解决了计算病理学任务中数据不足的问题，并在多个下游任务上超越了最先进的系统。

    

    计算病理学利用人工智能通过分析全数字切片图像实现精准医学和决策支持系统，有潜力彻底改变癌症的诊断和治疗。然而，实现这个目标的一个主要挑战是对于许多特定的计算病理学任务，数据量不足以进行开发。为了应对这个挑战，我们创建了Virchow，一个632百万参数的深度神经网络基础模型，用于计算病理学。通过自监督学习，Virchow在1.5百万个不同组织样本的苏木精和伊红染色全数字切片图像上进行训练，这比之前的研究数据量大得多。在包括瓦片级全癌检测和亚型以及幻灯片级生物标志物预测在内的下游任务上，Virchow在来自与预训练数据相同人群的内部数据集和外部公开数据集上均胜过最先进的系统。

    Computational pathology uses artificial intelligence to enable precision medicine and decision support systems through the analysis of whole slide images. It has the potential to revolutionize the diagnosis and treatment of cancer. However, a major challenge to this objective is that for many specific computational pathology tasks the amount of data is inadequate for development. To address this challenge, we created Virchow, a 632 million parameter deep neural network foundation model for computational pathology. Using self-supervised learning, Virchow is trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue groups, which is orders of magnitude more data than previous works. When evaluated on downstream tasks including tile-level pan-cancer detection and subtyping and slide-level biomarker prediction, Virchow outperforms state-of-the-art systems both on internal datasets drawn from the same population as the pretraining data as well as external pu
    
[^100]: 在在线设置下学习线性算子的无限维回归

    Online Infinite-Dimensional Regression: Learning Linear Operators. (arXiv:2309.06548v1 [stat.ML])

    [http://arxiv.org/abs/2309.06548](http://arxiv.org/abs/2309.06548)

    在这篇论文中，我们研究了在线设置下学习无限维线性算子的问题。我们证明了在一定的条件下，线性算子是可以在线学习的，而在另一些条件下则不可以。我们还证明了在线均一收敛和学习能力之间的分离，并在PAC设置下得到了相同的结果。

    

    我们考虑在线设置下学习两个无限维希尔伯特空间之间的线性算子问题，通过最小二乘损失函数进行学习。我们证明了在$p \in [1, \infty)$范围内，具有均匀有界$p$-Schatten范数的线性算子类是可以在线学习的。另一方面，我们证明了具有均匀有界算子范数的线性算子类\textit{不}是可以在线学习的。此外，我们通过找到一类有界线性算子，证明了在线均一收敛和学习能力之间的分离。最后，我们证明了不可能性结果和均一收敛与学习能力之间的分离在PAC设置下同样成立。

    We consider the problem of learning linear operators under squared loss between two infinite-dimensional Hilbert spaces in the online setting. We show that the class of linear operators with uniformly bounded $p$-Schatten norm is online learnable for any $p \in [1, \infty)$. On the other hand, we prove an impossibility result by showing that the class of uniformly bounded linear operators with respect to the operator norm is \textit{not} online learnable. Moreover, we show a separation between online uniform convergence and online learnability by identifying a class of bounded linear operators that is online learnable but uniform convergence does not hold. Finally, we prove that the impossibility result and the separation between uniform convergence and learnability also hold in the agnostic PAC setting.
    
[^101]: 建模推荐系统生态系统：机制设计、强化学习和生成模型的交叉研究挑战

    Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models. (arXiv:2309.06375v1 [cs.AI])

    [http://arxiv.org/abs/2309.06375](http://arxiv.org/abs/2309.06375)

    建模推荐系统生态系统需要考虑参与者激励、行为以及策略引发的相互作用，通过强化学习进行长期优化，使用社会选择方法进行权衡，并减少信息不对称。

    

    现代推荐系统位于涵盖用户、内容提供商、广告商和其他参与者行为的复杂生态系统的核心。尽管如此，大多数推荐系统研究的重点，以及大多数重要实用推荐系统，仅限于个别用户推荐的局部、短视优化。这给推荐系统可能为用户带来的长期效用带来了重大成本。我们认为，如果要最大化系统对这些参与者的价值并提高整体生态系统的“健康”状况，有必要明确地对系统中所有参与者的激励和行为进行建模，并对其策略引发的相互作用进行建模。为此需要：使用强化学习等技术进行长期优化；使用社会选择方法为不同参与者的效用进行不可避免的权衡；减少信息不对称。

    Modern recommender systems lie at the heart of complex ecosystems that couple the behavior of users, content providers, advertisers, and other actors. Despite this, the focus of the majority of recommender research -- and most practical recommenders of any import -- is on the local, myopic optimization of the recommendations made to individual users. This comes at a significant cost to the long-term utility that recommenders could generate for its users. We argue that explicitly modeling the incentives and behaviors of all actors in the system -- and the interactions among them induced by the recommender's policy -- is strictly necessary if one is to maximize the value the system brings to these actors and improve overall ecosystem "health". Doing so requires: optimization over long horizons using techniques such as reinforcement learning; making inevitable tradeoffs in the utility that can be generated for different actors using the methods of social choice; reducing information asymm
    
[^102]: 计算和通信高效的无线网络联合学习

    Computation and Communication Efficient Federated Learning over Wireless Networks. (arXiv:2309.01816v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01816](http://arxiv.org/abs/2309.01816)

    提出了一种计算和通信高效的无线网络联合学习框架，通过模型剪枝和个性化，在分布式学习中减少计算和通信延迟，并提高非独立同分布数据设备的学习准确度。

    

    联合学习（FL）能够在边缘设备上进行分布式学习，并保护数据隐私。然而，由于设备数据的异质性，学习准确度下降，在计算能力有限和无线资源有限的设备上更新大规模学习模型会增加计算和通信延迟。我们考虑了一种新颖的FL框架，其中包括模型剪枝和个性化，以克服这些挑战。该框架将学习模型分为全局部分和个性化部分，全局部分通过模型剪枝与所有设备共享以学习数据表示，个性化部分针对特定设备进行微调，在FL过程中调整模型大小以减少计算和通信延迟，并提高非独立同分布（non-IID）数据设备的学习准确度。然后，对所提出的FL框架的计算和通信延迟以及收敛分析进行了数学分析。

    Federated learning (FL) enables distributed learning across edge devices while protecting data privacy. However, the learning accuracy decreases due to the heterogeneity of devices' data, and the computation and communication latency increase when updating large-scale learning models on devices with limited computational capability and wireless resources. We consider a novel FL framework with partial model pruning and personalization to overcome these challenges. This framework splits the learning model into a global part with model pruning shared with all devices to learn data representations and a personalized part to be fine-tuned for a specific device, which adapts the model size during FL to reduce both computation and communication latency and increases the learning accuracy for the device with non-independent and identically distributed (non-IID) data. Then, the computation and communication latency and the convergence analysis of the proposed FL framework are mathematically ana
    
[^103]: 大型语言模型中的命名实体上下文偏倚研究

    Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v1 [cs.CL])

    [http://arxiv.org/abs/2309.00723](http://arxiv.org/abs/2309.00723)

    本文研究了使用大型语言模型进行上下文偏倚的方法，通过在第二次打分时提供额外的上下文信息，以提高自动语音识别性能。我们利用提示信息对大型语言模型进行boosting，并采用多任务训练以预测实体类别和下一个标记。此外，我们提出了动态提示方法来提高效率。

    

    本文研究了在大型语言模型(LLMs)中进行上下文偏倚，即在第二次打分时为LLM提供额外的上下文信息，以提高自动语音识别(ASR)性能。我们提出了在打分期间利用提示信息对LLM进行boosting，而无需进行微调，这些提示信息包括偏倚列表和少样本示例，用于在计算假设得分时作为附加信息。除了少样本提示学习外，我们还提出了LLM的多任务训练，以预测实体类别和下一个标记。为了提高上下文偏倚的效率并避免超过LLMs的最大序列长度，我们提出了动态提示，即使用类别标签预测选择最可能的类别，并仅使用这个类别中的实体作为下一个标记预测的上下文。对内部的呼叫、消息和口述数据集以及SLUE-Voxpopuli数据集进行了词错误率(WER)评估。

    This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results
    
[^104]: 基于神经预测的零样本NAS范式的有效性

    Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])

    [http://arxiv.org/abs/2308.16775](http://arxiv.org/abs/2308.16775)

    这项研究提出了一种新的方法，通过深度学习进行零样本架构搜索，通过使用可学习的傅里叶正弦和求和编码来构建计算的前馈图，从而解决了基于预测的神经架构搜索中性能指标泛化的限制。

    

    在基于预测的神经架构搜索（NAS）中，通过图卷积网络得到的性能指标取得了显著的成功。然而，通过one-hot编码将前馈结构表示为组件图的这些指标面临一个限制：无法在不同的搜索空间中评估架构的性能。相反，手工性能指标（零样本NAS）可以在多个搜索空间中泛化，因为它们使用相同的架构和随机初始化。为了解决这个限制，我们提出了一种新的深度学习方法，用于零样本NAS。我们的方法采用傅里叶正弦和求和编码来进行卷积核的编码，从而构建了一个计算的前馈图，其结构类似于正在评估的架构。这些编码是可学习的，并提供了架构拓扑信息的全面视图。然后，伴随的多层感知器（MLP）对架构进行排序。

    In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
    
[^105]: 3D-MuPPET: 3D多鸽姿态估计与跟踪

    3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking. (arXiv:2308.15316v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2308.15316](http://arxiv.org/abs/2308.15316)

    3D-MuPPET是一个用于估计和跟踪多只鸽子三维姿势的框架，通过多视角实时推测2D关键点并将其三角化到3D空间，同时使用动态匹配和2D跟踪器维持对应关系。相比最先进的3D姿势估计器，具有可比的准确性。该框架还能在使用单只鸽子数据训练的情况下应用于多只鸽子数据，简化领域转换。

    

    近年来，对动物姿势跟踪的无标记方法已有所发展，但仍缺乏用于追踪大规模动物群体的三维框架和基准。为了弥补文献中的这一空白，我们提出了3D-MuPPET，一个使用多视角实时估计和跟踪多达10只鸽子的三维姿势的框架。我们训练了一个姿势估计器，用于推测多只鸽子的2D关键点和边界框，然后将关键点三角化到3D空间。对于匹配对应关系，我们首先动态地将2D检测结果与第一帧中的全局身份进行匹配，然后使用2D跟踪器在后续帧中维持对应关系。我们达到了与最先进的3D姿势估计器相当的准确性，即均方根误差（RMSE）和正确关键点百分比（PCK）。我们还展示了一个新颖的用例，即我们使用单只鸽子的数据训练模型并在包含多只鸽子的数据上得到可比较的结果，这可以简化对新场景的领域转换。

    Markerless methods for animal posture tracking have been developing recently, but frameworks and benchmarks for tracking large animal groups in 3D are still lacking. To overcome this gap in the literature, we present 3D-MuPPET, a framework to estimate and track 3D poses of up to 10 pigeons at interactive speed using multiple-views. We train a pose estimator to infer 2D keypoints and bounding boxes of multiple pigeons, then triangulate the keypoints to 3D. For correspondence matching, we first dynamically match 2D detections to global identities in the first frame, then use a 2D tracker to maintain correspondences accross views in subsequent frames. We achieve comparable accuracy to a state of the art 3D pose estimator for Root Mean Square Error (RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel use case where our model trained with data of single pigeons provides comparable results on data containing multiple pigeons. This can simplify the domain shift to new sp
    
[^106]: 重新审视多任务学习中的标量化：一个理论的视角

    Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective. (arXiv:2308.13985v1 [cs.LG])

    [http://arxiv.org/abs/2308.13985](http://arxiv.org/abs/2308.13985)

    本论文重新审视了多任务学习中的标量化方法，并从理论的角度探讨了标量化是否能够充分探索帕累托前沿。结果显示，与最近的研究声称的经验优势相反，标量化本质上无法进行全面探索，特别是对于那些平衡了paren

    

    线性标量化，即通过加权总和来组合所有损失函数，自从多任务学习（MTL）的创立以来一直是文献中的默认选择。近年来，越来越多的人对开发专门的多任务优化器（SMTOs）来处理MTL作为多目标优化问题产生了兴趣。然而，目前还不清楚SMTOs是否比标量化有根本上的优势。实际上，社区中存在对比这两种算法的激烈讨论，主要是从经验角度出发。为了回答上述问题，本文从理论的角度重新审视了标量化。我们专注于线性MTL模型，并研究标量化是否能够充分探索帕累托前沿。我们的研究发现，与那些声称标量化具有经验优势的最近工作相反，标量化本质上无法进行全面探索，特别是对于那些平衡了paren

    Linear scalarization, i.e., combining all loss functions by a weighted sum, has been the default choice in the literature of multi-task learning (MTL) since its inception. In recent years, there is a surge of interest in developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a multi-objective optimization problem. However, it remains open whether there is a fundamental advantage of SMTOs over scalarization. In fact, heated debates exist in the community comparing these two types of algorithms, mostly from an empirical perspective. To approach the above question, in this paper, we revisit scalarization from a theoretical perspective. We focus on linear MTL models and study whether scalarization is capable of fully exploring the Pareto front. Our findings reveal that, in contrast to recent works that claimed empirical advantages of scalarization, scalarization is inherently incapable of full exploration, especially for those Pareto optimal solutions that strike the balanc
    
[^107]: BridgeData V2:一个用于规模化机器人学习的数据集

    BridgeData V2: A Dataset for Robot Learning at Scale. (arXiv:2308.12952v1 [cs.RO])

    [http://arxiv.org/abs/2308.12952](http://arxiv.org/abs/2308.12952)

    BridgeData V2是一个大规模且多样化的机器人操作行为数据集，广泛应用于机器人学习研究。该数据集具备任务和环境变异性，并且兼容多种学习方法，通过实验表明，使用此数据集可以提高模型性能。

    

    我们介绍了BridgeData V2，这是一个大规模且多样化的机器人操作行为数据集，旨在促进规模化机器人学习的研究。BridgeData V2包含了在一个公开可用且成本较低的机器人上收集的60,096个轨迹，覆盖了24个环境。BridgeData V2提供了广泛的任务和环境变异性，使得可以在不同的环境、领域和机构之间进行泛化的技能，使得该数据集成为广大研究人员的有用资源。此外，该数据集与多种开放词汇、多任务学习方法以目标图像或自然语言指令为条件是兼容的。在我们的实验中，我们在我们的数据集上训练了6种最先进的模仿学习和离线强化学习方法，并发现它们在一系列需要不同泛化程度的任务上取得了成功。我们还展示了这些方法的性能随着更多的数据和更高容量的模型而改善，并且通过训练集大小的增加和模型容量的增加获得了更好的表现。

    We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that trai
    
[^108]: 简易注意力：一种用于Transformer的简单自注意机制

    Easy attention: A simple self-attention mechanism for Transformers. (arXiv:2308.12874v1 [cs.LG])

    [http://arxiv.org/abs/2308.12874](http://arxiv.org/abs/2308.12874)

    本论文提出了一种名为简易注意力的注意力机制，用于提高Transformer神经网络在混沌系统时间动态预测中的鲁棒性。该方法不依赖于键、查询和softmax，直接将注意力得分作为可学习参数。实验结果表明，该方法在重构和预测混沌系统的时间动态方面比传统的自注意机制和长短期记忆方法更具鲁棒性和简化性。

    

    为了提高用于混沌系统时间动态预测的Transformer神经网络的鲁棒性，我们提出了一种新颖的注意力机制，称为简易注意力。由于自注意机制仅使用查询和键的内积，因此证明了为了获取捕捉时间序列的长期依赖关系所需的注意力得分，并不需要键、查询和softmax。通过在softmax注意力得分上实施奇异值分解（SVD），我们进一步观察到自注意力在注意力得分的张成空间中压缩了来自查询和键的贡献。因此，我们提出的简易注意力方法直接将注意力得分作为可学习参数。这种方法在重构和预测展现更强鲁棒性和更少复杂性的混沌系统的时间动态时取得了出色的结果，比自注意机制或广泛使用的长短期记忆

    To improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention. Due to the fact that self attention only makes usage of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through implementing singular-value decomposition (SVD) on the softmax attention score, we further observe that the self attention compresses contribution from both queries and keys in the spanned space of the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than the self attention or the widely-used long short-ter
    
[^109]: 用Schrödinger桥改进基于生成模型的展开

    Improving Generative Model-based Unfolding with Schr\"{o}dinger Bridges. (arXiv:2308.12351v1 [hep-ph])

    [http://arxiv.org/abs/2308.12351](http://arxiv.org/abs/2308.12351)

    本研究提出了一种使用Schrödinger桥和扩散模型创建的展开方法SBUnfold，它将判别模型和生成模型的优势结合起来。与最先进方法相比，在合成的Z+jets数据集上获得了卓越的性能。

    

    基于机器学习的展开已经实现了无bin和高维微分截面测量。这个研究领域出现了两种主要方法：一种基于判别模型，一种基于生成模型。判别模型的主要优势在于，它们学习了对起始模拟的小修正，而生成模型在数据稀疏的相空间区域具有更好的扩展性。我们提出使用Schrödinger桥和扩散模型创建SBUnfold，一种将判别模型和生成模型的优势结合起来的展开方法。SBUnfold的关键特点是它的生成模型将一组事件映射到另一组事件，而无需通过已知的概率密度（与标准扩散模型和标准扩散模型不同）。我们展示了SBUnfold在合成的Z+jets数据集上与最先进方法相比的出色性能。

    Machine learning-based unfolding has enabled unbinned and high-dimensional differential cross section measurements. Two main approaches have emerged in this research area: one based on discriminative models and one based on generative models. The main advantage of discriminative models is that they learn a small correction to a starting simulation while generative models scale better to regions of phase space with little data. We propose to use Schroedinger Bridges and diffusion models to create SBUnfold, an unfolding approach that combines the strengths of both discriminative and generative models. The key feature of SBUnfold is that its generative model maps one set of events into another without having to go through a known probability density as is the case for normalizing flows and standard diffusion models. We show that SBUnfold achieves excellent performance compared to state of the art methods on a synthetic Z+jets dataset.
    
[^110]: 预门控MoE：快速且可扩展混合专家推理的算法和系统共同设计

    Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference. (arXiv:2308.12066v1 [cs.LG])

    [http://arxiv.org/abs/2308.12066](http://arxiv.org/abs/2308.12066)

    预门控MoE系统通过算法和系统的共同设计，有效解决了传统MoE架构的计算和存储挑战。

    

    在最近几年中，基于transformers的大型语言模型（LLMs）取得了重大进展，其成功源于模型规模的扩大。尽管算法性能很高，但LLMs的计算和存储需求带来了前所未有的挑战。为了解决LLMs的高计算需求，引入了混合专家（MoE）架构，能够在不成比例地扩大计算需求的情况下扩展模型大小。然而，MoE的高存储需求和稀疏专家的动态激活限制了其在实际问题中的适用性。之前的解决方案将MoE的内存占用高的专家参数转移到CPU内存上，但是从CPU迁移已激活的专家到GPU的延迟导致了高性能开销。我们提出的预门控MoE系统通过算法和系统的共同设计，有效解决了传统MoE架构的计算和存储挑战。

    Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE 
    
[^111]: 基于深度学习的水电站管理流量分解

    Deep learning-based flow disaggregation for hydropower plant management. (arXiv:2308.11631v1 [eess.SP])

    [http://arxiv.org/abs/2308.11631](http://arxiv.org/abs/2308.11631)

    本研究提出了一种基于深度学习的时间序列分解模型，用于将每日流量分解为每小时流量，并在挪威某流量测站的数据上进行了测试，初步结果显示了该模型的一些有希望的方面。

    

    高时空分辨率数据对于水电站管理至关重要。目前，大部分挪威水电站只有每日分辨率的数据，然而，为了实现更精确的管理，通常需要亚日分辨率的数据。为了解决亚日数据的普遍缺失，时间序列分解是一种潜在的工具。在本研究中，我们提出了一个基于深度学习的时间序列分解模型，该模型使用挪威某流量测站的数据进行测试，将每日流量分解为每小时流量。初步结果显示了该模型的一些有希望的方面。

    High temporal resolution data is a vital resource for hydropower plant management. Currently, only daily resolution data are available for most of Norwegian hydropower plant, however, to achieve more accurate management, sub-daily resolution data are often required. To deal with the wide absence of sub-daily data, time series disaggregation is a potential tool. In this study, we proposed a time series disaggregation model based on deep learning, the model is tested using flow data from a Norwegian flow station, to disaggregate the daily flow into hourly flow. Preliminary results show some promising aspects for the proposed model.
    
[^112]: ALI-DPFL: 具有自适应本地迭代的差分隐私联邦学习

    ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations. (arXiv:2308.10457v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10457](http://arxiv.org/abs/2308.10457)

    ALI-DPFL是一种进行差分隐私联邦学习的算法，通过自适应本地迭代来优化性能，并在实验中展示了显著的改进。

    

    联邦学习是一种分布式机器学习技术，通过共享训练参数而不是原始数据，允许多个设备或组织之间进行模型训练。然而，攻击者仍然可以通过对这些训练参数的推理攻击（例如差分攻击）来推断个体信息。因此，差分隐私被广泛应用于联邦学习中以防止此类攻击。我们在资源受限的场景中考虑差分隐私联邦学习，其中既有隐私预算受限，又有通信轮次受限。通过理论分析收敛性，我们可以找到在任意两个顺序全局更新之间的客户机之间的最佳差分隐私本地迭代次数。基于此，我们设计了一种具有自适应本地迭代的差分隐私联邦学习算法（ALI-DPFL）。我们在FashionMNIST和CIFAR10数据集上对我们的算法进行实验，并展示了显著更好的性能。

    Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations by sharing training parameters instead of raw data. However, adversaries can still infer individual information through inference attacks (e.g. differential attacks) on these training parameters. As a result, Differential Privacy (DP) has been widely used in FL to prevent such attacks. We consider differentially private federated learning in a resource-constrained scenario, where both privacy budget and communication round are constrained. By theoretically analyzing the convergence, we can find the optimal number of differentially private local iterations for clients between any two sequential global updates. Based on this, we design an algorithm of differentially private federated learning with adaptive local iterations (ALI-DPFL). We experiment our algorithm on the FashionMNIST and CIFAR10 datasets, and demonstrate significantly better performances th
    
[^113]: 慢性疾病的因果可解释性进展轨迹分析

    Causal Interpretable Progression Trajectory Analysis of Chronic Disease. (arXiv:2308.09735v1 [cs.LG])

    [http://arxiv.org/abs/2308.09735](http://arxiv.org/abs/2308.09735)

    提出了一种名为因果轨迹预测（CTP）的新模型，通过将轨迹预测和因果发现相结合，准确预测慢性疾病的进展轨迹并揭示特征间的因果关系。

    

    慢性疾病是导致死亡的主要原因，强调了准确预测疾病进展轨迹和知情临床决策的需求。机器学习模型通过捕捉患者特征中的非线性模式，在这个领域显示出了潜力。然而，现有的基于机器学习的模型缺乏提供因果可解释性预测和评估治疗效果的能力，限制了其决策辅助的角度。在本研究中，我们提出了一种名为因果轨迹预测（CTP）的新模型来解决这一限制。CTP模型将轨迹预测和因果发现相结合，以实现准确预测疾病进展轨迹和揭示特征间的因果关系。通过将因果图结合到预测过程中，CTP确保祖先特征不受对后代特征的治疗影响，从而增强了模型的可解释性。

    Chronic disease is the leading cause of death, emphasizing the need for accurate prediction of disease progression trajectories and informed clinical decision-making. Machine learning (ML) models have shown promise in this domain by capturing non-linear patterns within patient features. However, existing ML-based models lack the ability to provide causal interpretable predictions and estimate treatment effects, limiting their decision-assisting perspective. In this study, we propose a novel model called causal trajectory prediction (CTP) to tackle the limitation. The CTP model combines trajectory prediction and causal discovery to enable accurate prediction of disease progression trajectories and uncovering causal relationships between features. By incorporating a causal graph into the prediction process, CTP ensures that ancestor features are not influenced by treatment on descendant features, thereby enhancing the interpretability of the model. By estimating the bounds of treatment e
    
[^114]: 利用视觉-语言模型在医学图像分割中探索迁移学习

    Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])

    [http://arxiv.org/abs/2308.07706](http://arxiv.org/abs/2308.07706)

    本论文提出使用视觉-语言模型进行医学图像分割的迁移学习，并评估了其在医学领域的可迁移性。通过捕捉语义信息和引入新的图像描述变化，实现了对多样化医学图像的分割。

    

    医学图像分割在医学领域的各种临床应用中至关重要。尽管最先进的分割模型已被证明有效，但在这个任务中整合文本指导以增强视觉特征仍然是一个进展有限的领域。现有利用文本指导的分割模型主要在开放领域图像上训练，这引发了在医学领域直接应用的难题，需要手动介入或进行微调。为了解决这些挑战，我们提出使用多模态的视觉-语言模型从图像描述和图像中捕捉语义信息，使得能够对多样化的医学图像进行分割。该研究全面评估了现有的视觉-语言模型在多个数据集上的可迁移性，以评估其从开放领域向医学领域的迁移能力。此外，我们对数据集中以前未见图像的图像描述引入了变化，揭示了显著的变异。

    Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
    
[^115]: 基于差分进化算法的Transformer神经网络模型用于负荷预测的超参数选择

    Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v1 [cs.NE])

    [http://arxiv.org/abs/2307.15299](http://arxiv.org/abs/2307.15299)

    本研究使用差分进化算法选择Transformer神经网络模型的优化超参数，以提高负荷预测的准确性。

    

    精确的负荷预测在众多领域都起着重要作用，但准确捕捉动力系统的复杂动态仍然是传统统计模型面临的挑战。因此，时间序列模型（ARIMA）和深度学习模型（ANN，LSTM，GRU等）经常被使用，并且通常能够取得更好的成功率。本文分析了最近开发的Transformer-based神经网络模型在负荷预测中的效果。Transformer模型有望改进负荷预测，因为它们能够通过其Attention机制学习到长期依赖关系。我们运用了几种元启发式算法，如差分进化，以寻找Transformer-based神经网络的最优超参数，以产生精确的预测。差分进化为非可微分、多目标或约束优化问题提供了可扩展、强健和全局的解决方案。我们的工作比较了所提出的基于Transformer的神经网络与其他模型在负荷预测上的性能。

    Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Ne
    
[^116]: MiVOLO: 多输入变换器用于年龄和性别估计

    MiVOLO: Multi-input Transformer for Age and Gender Estimation. (arXiv:2307.04616v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.04616](http://arxiv.org/abs/2307.04616)

    本论文提出了一种简单的方法MiVOLO，使用最新的视觉变换器进行年龄和性别估计。该方法将面部信息和人物图像数据集成到一个统一的模型中，提高了模型的泛化能力，并在图像中人脸不可见的情况下仍能提供令人满意的结果。实验证明了该方法在四个基准测试上达到了最先进的性能，并具有实时处理能力。

    

    在自然环境中进行年龄和性别识别是一项极具挑战性的任务：除了条件的可变性、姿势的复杂性和图像质量的变化之外，还存在人脸部分或完全遮挡的情况。我们提出了MiVOLO（多输入VOLO），这是一种使用最新的视觉变换器进行年龄和性别估计的简单方法。我们的方法将两个任务集成到一个统一的双输入/输出模型中，不仅利用了面部信息，还利用了人物图像数据。这提高了我们模型的泛化能力，使其即使在图像中人脸不可见的情况下也能提供令人满意的结果。为了评估我们提出的模型，我们在四个流行的基准测试上进行了实验，并取得了最先进的性能，同时展示了实时处理能力。此外，我们还引入了一个基于Open Images数据集的新基准测试。这个基准测试的真实注释由认真生成。

    Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded. We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer. Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data. This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image. To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities. Additionally, we introduce a novel benchmark based on images from the Open Images Dataset. The ground truth annotations for this benchmark have been meticulously generated by 
    
[^117]: FITS：模拟具有10k个参数的时间序列

    FITS: Modeling Time Series with $10k$ Parameters. (arXiv:2307.03756v1 [cs.LG])

    [http://arxiv.org/abs/2307.03756](http://arxiv.org/abs/2307.03756)

    FITS是一种轻量而强大的时间序列分析模型，通过在复杂频率域中进行插值操作，丢弃对时间序列数据影响微小的高频分量，实现了与最先进模型相当的性能，并且具有较小的模型参数数量，适用于边缘设备。

    

    本文介绍了FITS，一种轻量而强大的时间序列分析模型。与直接处理原始时间域数据的现有模型不同，FITS基于在复杂频率域中进行插值的原理操作时间序列。通过丢弃对时间序列数据影响微小的高频分量，FITS在时间序列预测和异常检测任务中实现了与最先进模型相当的性能，同时具有近似10k个参数的显著紧凑大小。这种轻量级模型可以轻松地在边缘设备上进行训练和部署，为各种应用创造了机会。

    In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain. By discarding high-frequency components with negligible impact on time series data, FITS achieves performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks, while having a remarkably compact size of only approximately $10k$ parameters. Such a lightweight model can be easily trained and deployed in edge devices, creating opportunities for various applications. The anonymous code repo is available in: \url{https://anonymous.4open.science/r/FITS}
    
[^118]: 弹性决策变压器

    Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02484](http://arxiv.org/abs/2307.02484)

    弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。

    

    本文介绍了弹性决策变压器（EDT），它是现有决策变压器（DT）及其变体的重大进展。尽管DT声称能够生成最佳轨迹，但实证证据表明它在轨迹拼接方面存在困难，轨迹拼接是指从一组次优轨迹中生成最优或接近最优轨迹的过程。提出的EDT通过在测试时间进行动作推断时调整DT中维护的历史长度来实现轨迹拼接，从而使自己与众不同。此外，当前轨迹是最优的时候，EDT通过保持较长的历史，当当前轨迹是次优的时候，EDT通过保持较短的历史来优化轨迹，使其能够与更优的轨迹进行“拼接”。广泛的实验表明，EDT能够填补基于DT和基于Q-Learning方法之间的性能差距。特别是，EDT在多任务情况下胜过基于Q-Learning的方法。

    This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
    
[^119]: 交叉扩散：通过自监督学习改进基于扩散的视觉机器人策略

    Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning. (arXiv:2307.01849v1 [cs.RO])

    [http://arxiv.org/abs/2307.01849](http://arxiv.org/abs/2307.01849)

    本论文提出了一种名为Crossway Diffusion的方法，通过使用自监督学习目标增强了基于扩散的视觉机器人策略学习，实验证明了其在各种机器人任务中的有效性和优势。

    

    序列建模方法在机器人模仿学习中显示出有希望的结果。最近，扩散模型已经被采用于行为克隆中，并从其在建模复杂数据分布方面的特异能力中获益。在这项工作中，我们提出了一种名为Crossway Diffusion的方法，通过使用额外的自监督学习（SSL）目标来增强基于扩散的视觉机器人策略学习。标准的基于扩散的策略从随机噪声中生成动作序列，条件是视觉观测和其他低维状态。我们进一步扩展了这一方法，引入了一个新的解码器，从反向扩散过程的中间表示中重构原始图像像素（和其他状态信息），并使用SSL损失联合训练模型。我们的实验证明了Crossway Diffusion在各种模拟和真实世界机器人任务中的有效性，验证了其相对于标准基于扩散的策略的优势。

    Sequence modeling approaches have shown promising results in robot imitation learning. Recently, diffusion models have been adopted for behavioral cloning, benefiting from their exceptional capabilities in modeling complex data distribution. In this work, we propose Crossway Diffusion, a method to enhance diffusion-based visuomotor policy learning by using an extra self-supervised learning (SSL) objective. The standard diffusion-based policy generates action sequences from random noise conditioned on visual observations and other low-dimensional states. We further extend this by introducing a new decoder that reconstructs raw image pixels (and other state information) from the intermediate representations of the reverse diffusion process, and train the model jointly using the SSL loss. Our experiments demonstrate the effectiveness of Crossway Diffusion in various simulated and real-world robot tasks, confirming its advantages over the standard diffusion-based policy. We demonstrate tha
    
[^120]: 微观尺度多智能体强化学习中的环境对新兴策略的影响

    Environmental effects on emergent strategy in micro-scale multi-agent reinforcement learning. (arXiv:2307.00994v2 [physics.bio-ph] UPDATED)

    [http://arxiv.org/abs/2307.00994](http://arxiv.org/abs/2307.00994)

    本研究使用基于粒子的动力学模拟研究了微观环境中温度对多智能体强化学习系统中策略形成和有效性的影响。实验结果显示，在较高温度下，强化学习智能体能够发展出新的策略，为解决微观尺度控制问题提供了洞见。

    

    多智能体强化学习（MARL）是实现微观粒子（如微型机器人）高效控制的有前景的候选方案。然而，微观粒子的环境存在着独特的挑战，例如在足够小的尺度上的布朗运动。本研究利用基于粒子的Langevin分子动力学模拟作为微观环境的逼真表示，探讨了温度在MARL系统中策略形成和有效性方面的作用。为此，我们在不同温度下对微观环境中的两个不同的多智能体任务进行了实验，这包括检测浓度梯度的来源和杆的旋转。我们发现，在较高温度下，RL智能体能够识别出实现这些任务的新策略，突显了理解该温度范围的重要性，并为弥合模拟与实际环境之间的泛化差距提供了训练策略的洞见。

    Multi-Agent Reinforcement Learning (MARL) is a promising candidate for realizing efficient control of microscopic particles, of which micro-robots are a subset. However, the microscopic particles' environment presents unique challenges, such as Brownian motion at sufficiently small length-scales. In this work, we explore the role of temperature in the emergence and efficacy of strategies in MARL systems using particle-based Langevin molecular dynamics simulations as a realistic representation of micro-scale environments. To this end, we perform experiments on two different multi-agent tasks in microscopic environments at different temperatures, detecting the source of a concentration gradient and rotation of a rod. We find that at higher temperatures, the RL agents identify new strategies for achieving these tasks, highlighting the importance of understanding this regime and providing insight into optimal training strategies for bridging the generalization gap between simulation and re
    
[^121]: 虚假黎明：重新评估谷歌强化学习在芯片宏观布局中的应用

    The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])

    [http://arxiv.org/abs/2306.09633](http://arxiv.org/abs/2306.09633)

    谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。

    

    谷歌2021年在《自然》杂志上发表的有关使用强化学习设计芯片的论文，因为所声称的结果缺乏充分的文件记录和关键步骤的说明，引发争议并受到媒体的批评报道。 而两项独立的评估填补了空白，证明谷歌强化学习落后于人类设计师、落后于一种众所周知的算法（模拟退火），并且还落后于普遍可用的商业软件。交叉检查的数据表明，由于行为、分析和报告中的错误，该《自然》文章的完整性受到了严重的损害。

    Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
    
[^122]: TopP\&R: 具有鲁棒性的支持估计方法，用于评估生成模型中的保真度和多样性

    TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])

    [http://arxiv.org/abs/2306.08013](http://arxiv.org/abs/2306.08013)

    本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。

    

    本文提出了一种鲁棒可靠的生成模型评估指标，通过引入拓扑和统计处理进行严格的支持估计。现有的度量标准，如Inception Score（IS），Fr\'echet Inception Distance（FID）以及Precision and Recall（P\&R）的变体，严重依赖于从样本特征估计的支持。然而，尽管评估的质量完全取决于其可靠性，但其估计的可靠性并没有得到严肃的讨论（并被忽视）。本文提出了拓扑精度和召回率（TopP\&R，发音为“topper”），它提供了一种系统的方法来估计支持，仅保留具有一定置信水平的具有拓扑和统计上重要性的特征。这不仅使TopP\&R对于噪声特征具有强大的鲁棒性，而且还提供了统计一致性。我们的理论和实验结果表明，TopP\&R对于离群值和非独立同分布具有鲁棒性。

    We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&R is robust to outliers and non-independent and identically distributed
    
[^123]: 基于神经网络的城市时空数据合成方法

    Urban Spatiotemporal Data Synthesis via Neural Disaggregation. (arXiv:2306.07292v1 [cs.LG])

    [http://arxiv.org/abs/2306.07292](http://arxiv.org/abs/2306.07292)

    本研究提出了一种基于神经网络的城市时空数据合成方法，旨在通过分解粗糙的低分辨率地理单元的聚合城市数据来合成细粒度，高分辨率的城市数据，以增加高度聚合的城市数据的可用性和实现价值。

    

    开放数据的细节级别常常与其所能提供的实际效益发生冲突。较不细化的数据可以保护个人隐私，但在一定程度上牺牲了开放数据促进透明度和协助研究的承诺。类似于城市环境中，高层次地理单元的聚合城市数据可能会掩盖城市动态的底层特征，低级别地理单元的变化可能更为明显。本研究旨在通过分解粗糙的低分辨率地理单元的聚合城市数据，合成细粒度，高分辨率的城市数据，以增加高度聚合的城市数据的可用性和实现价值。为了解决一些传统分解方法的简单性问题-1) 我们尝试了许多神经网络模型，这些模型能够建模特征之间复杂的非线性关系。神经方法也可以同时利用空间和时间信息。我们展示了这些神经网络方法的优点。

    The level of granularity of open data often conflicts the benefits it can provide. Less granular data can protect individual privacy, but to certain degrees, sabotage the promise of open data to promote transparency and assist research. Similar in the urban setting, aggregated urban data at high-level geographic units can mask out the underline particularities of city dynamics that may vary at lower areal levels. In this work, we aim to synthesize fine-grained, high resolution urban data, by breaking down aggregated urban data at coarse, low resolution geographic units. The goal is to increase the usability and realize the values as much as possible of highly aggregated urban data. To address the issue of simplicity of some traditional disaggregation methods -- 1) we experimented with numerous neural-based models that are capable of modeling intricate non-linear relationships among features. Neural methods can also leverage both spatial and temporal information concurrently. We showed 
    
[^124]: ShiftAddViT：多种乘法原语混合实现高效的视觉变换器

    ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06446](http://arxiv.org/abs/2306.06446)

    ShiftAddViT通过使用位移和加法等多种乘法原语对ViT进行重新参数化，实现了减少乘法操作的高效视觉变换器模型，可以在GPU上实现端到端的推理加速，无需从头训练。

    

    视觉变换器（ViT）展示了令人印象深刻的性能，并成为多个视觉任务的统一骨干。但是，ViTs中的注意力和多层感知器（MLPs）由于密集的乘法而不够高效，导致训练和推理代价高昂。为此，我们提出了一种将预训练的ViT以多种乘法原语（例如位移和加法）重新参数化的方法，以实现全新类型的减少乘法的模型，称为ShiftAddViT，旨在实现GPU上的端到端推理加速，无需从头开始训练。具体而言，我们将查询和键映射为汉明空间中的二进制码之后，采用加法核对查询、键和值之间的MatMul进行重新参数化。剩余的MLPs或线性层则采用位移核进行重新参数化。我们利用TVM在GPU上实施并优化这些定制核，以实现实际硬件部署。我们发现，这种重新参数化方法可以显著提高推理速度，而无需从头开始训练。

    Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
    
[^125]: 推理时间干预：从语言模型中引导出真实的答案

    Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])

    [http://arxiv.org/abs/2306.03341](http://arxiv.org/abs/2306.03341)

    本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。

    

    我们介绍了推理时间干预（ITI）技术，旨在增强大型语言模型（LLMs）的真实性。ITI通过在推理过程中沿着一组方向移动模型激活，跨越有限数量的注意力头。这种干预显着提高了LLaMA模型在TruthfulQA基准上的表现。在指令微调的LLaMA Alpaca上，ITI将其真实性从32.5％提高到65.1％。我们确定了真实性和可用性之间的权衡，并演示了如何通过调整干预强度来平衡它。ITI 取得了最低程度的干扰且计算廉价。此外，该技术在数据效率上表现优异：虽然像RLHF这样的方法需要广泛注释，但是ITI仅使用了几百个例子就能定位真实的方向。我们的研究结果表明，LLMs可能具有某种内部表示方法来表示某事是真实的可能性，即使它们在表面上产生了虚假的结果。

    We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
    
[^126]: 通过潜在量化进行解缠

    Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])

    [http://arxiv.org/abs/2305.18378](http://arxiv.org/abs/2305.18378)

    本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。

    

    在解缠表示学习中，模型需要将数据集的基础变化因素分开并独立地表示出来，而模型并没有提供有关这些因素的真实信息，归纳偏见在实现解缠方面发挥着重要作用。在本文中，我们通过施加严格的交流瓶颈和强大的模型规范化，构建了一种朝着组合编码和解码数据的归纳偏见。具体来说，我们对潜在维度进行可学习的离散编码，并为每个维度应用一个单独的标量码书。潜在量化迫使编码器在许多数据点上使用少量潜在值，从而使解码器能够为每个值分配一致的含义。规范化有助于将模型引向这种简明策略。我们在多个基准数据集上展示了该方法的广泛应用性，并且展示了我们的方法显著提高了一系列标准VAE模型学习的表征的可解释性。

    In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards compositionally encoding and decoding data by enforcing a harsh communication bottleneck. Concretely, we do this by (i) quantizing the latent space into learnable discrete codes with a separate scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the quantization forces the encoder to use a small number of latent values across many datapoints, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this appr
    
[^127]: 基于虚拟粒子随机逼近的可证速限制变种的SVGD算法。

    Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation. (arXiv:2305.17558v1 [stat.ML])

    [http://arxiv.org/abs/2305.17558](http://arxiv.org/abs/2305.17558)

    本论文提出了两种基于虚拟粒子随机逼近的可证速限制变种的SVGD算法，具有可证速的有限粒子收敛率。

    

    Stein变分梯度下降（SVGD）是一种流行的变分推断算法，它模拟相互作用的粒子系统以近似从目标分布中采样，具有各种领域的令人印象深刻的经验性能。在理论上，它的群体（即，无限粒子）极限动力学已经得到了很好的研究，但是SVGD在有限粒子体制下的行为则不太清楚。在这项工作中，我们设计了两种计算效率高的SVGD变体，即VP-SVGD（从概念上讲很优雅）和GB-SVGD（从经验上看很有效），具有可证速的有限粒子收敛率。我们引入了“虚拟粒子”的概念，并在概率测度空间中开发了人口极限SVGD动力学的新型随机逼近方法，它们可以使用有限数量的粒子精确实现。我们的算法可以看作是SVGD的特定随机批处理逼近，比普通方法更具计算效率。

    Stein Variational Gradient Descent (SVGD) is a popular variational inference algorithm which simulates an interacting particle system to approximately sample from a target distribution, with impressive empirical performance across various domains. Theoretically, its population (i.e, infinite-particle) limit dynamics is well studied but the behavior of SVGD in the finite-particle regime is much less understood. In this work, we design two computationally efficient variants of SVGD, namely VP-SVGD (which is conceptually elegant) and GB-SVGD (which is empirically effective), with provably fast finite-particle convergence rates. We introduce the notion of \emph{virtual particles} and develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, which are exactly implementable using a finite number of particles. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinar
    
[^128]: 在协同学习和优化中激励竞争对手诚实行为的研究

    Incentivizing Honesty among Competitors in Collaborative Learning and Optimization. (arXiv:2305.16272v1 [cs.LG])

    [http://arxiv.org/abs/2305.16272](http://arxiv.org/abs/2305.16272)

    这项研究提出了一个模型来描述在协作学习中竞争对手的不诚实行为，提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。

    

    协同学习技术能够让机器学习模型的训练比仅利用单一数据源的模型效果更好。然而，在许多情况下，潜在的参与者是下游任务中的竞争对手，如每个都希望通过提供最佳推荐来吸引客户的公司。这可能会激励不诚实的更新，损害其他参与者的模型，从而可能破坏协作的好处。在这项工作中，我们制定了一个模型来描述这种交互，并在该框架内研究了两个学习任务：单轮均值估计和强凸目标的多轮 SGD。对于一类自然的参与者行为，我们发现理性的客户会被激励强烈地操纵他们的更新，从而防止学习。然后，我们提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。最后，我们通过实验证明了这一点。

    Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the
    
[^129]: 竞争对手之间的战略数据共享

    Strategic Data Sharing between Competitors. (arXiv:2305.16052v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16052](http://arxiv.org/abs/2305.16052)

    这项研究介绍了竞争对手之间战略数据共享的问题，提出了一个分析框架，研究了市场条件对数据共享激励的影响。

    

    近年来，协作学习技术取得了显著进展，可以实现多个组织之间的私密模型训练。然而，企业在考虑与竞争对手共享数据时面临着困境——虽然协作可以改善公司的机器学习模型，但也可能使竞争对手受益，从而降低利润。在这项工作中，我们引入了一个用于分析这种数据共享权衡的通用框架。该框架包括三个组成部分，分别代表企业的生产决策、额外数据对模型质量的影响以及数据共享协商过程。然后，我们研究了基于经济理论中的传统市场模型的框架实例，以确定影响协作激励的关键因素。我们的研究结果表明市场条件对数据共享激励具有深远影响。特别是，我们发现市场竞争的减少，即企业之间的相似性。

    Collaborative learning techniques have significantly advanced in recent years, enabling private model training across multiple organizations. Despite this opportunity, firms face a dilemma when considering data sharing with competitors -- while collaboration can improve a company's machine learning model, it may also benefit competitors and hence reduce profits. In this work, we introduce a general framework for analyzing this data-sharing trade-off. The framework consists of three components, representing the firms' production decisions, the effect of additional data on model quality, and the data-sharing negotiation process, respectively. We then study an instantiation of the framework, based on a conventional market model from economic theory, to identify key factors that affect collaboration incentives. Our findings indicate a profound impact of market conditions on the data-sharing incentives. In particular, we find that reduced competition, in terms of the similarities between th
    
[^130]: 从儿童视角学习高级视觉表示而不引入强归纳偏差

    Learning high-level visual representations from a child's perspective without strong inductive biases. (arXiv:2305.15372v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.15372](http://arxiv.org/abs/2305.15372)

    通过儿童的视觉经验，我们在没有引入强归纳偏差的情况下训练了最先进的神经网络模型，并成功学习了广泛的语义类别和对象定位能力。嵌入模型表现为ImageNet模型的70%水平，尽管训练数据存在差异。

    

    年幼的儿童通过他们的视觉经验发展出复杂的世界内部模型。可以从儿童的视觉经验中学习这样的模型而不引入强归纳偏差吗？为了调查这个问题，我们在一个儿童的真实视觉经验的代理上训练了最先进的神经网络，没有任何明确的监督或领域特定的归纳偏差。具体地，我们使用来自单个儿童的200小时头戴摄像机视频训练了嵌入模型和生成模型，并使用各种参考模型作为衡量标准全面评估了它们在下游任务中的性能。平均而言，最佳的嵌入模型在表现上达到了一种高性能的ImageNet训练模型的70%，尽管训练数据存在相当大的差异。它们还学习了广泛的语义类别和对象定位能力，而不需要明确的监督，但它们比在全部ImageNet上训练的模型更少关注对象。生成模型训练效果较差…

    Young children develop sophisticated internal models of the world based on their visual experience. Can such models be learned from a child's visual experience without strong inductive biases? To investigate this, we train state-of-the-art neural networks on a realistic proxy of a child's visual experience without any explicit supervision or domain-specific inductive biases. Specifically, we train both embedding models and generative models on 200 hours of headcam video from a single child collected over two years and comprehensively evaluate their performance in downstream tasks using various reference models as yardsticks. On average, the best embedding models perform at a respectable 70% of a high-performance ImageNet-trained model, despite substantial differences in training data. They also learn broad semantic categories and object localization capabilities without explicit supervision, but they are less object-centric than models trained on all of ImageNet. Generative models trai
    
[^131]: SlotDiffusion: 基于Diffusion模型的物体中心生成建模

    SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models. (arXiv:2305.11281v1 [cs.CV])

    [http://arxiv.org/abs/2305.11281](http://arxiv.org/abs/2305.11281)

    本文提出了一种名为SlotDiffusion的对象中心潜在扩散模型，它具有强大的建模能力，能够提高物体中心槽到图像解码的质量，超越了先前的槽模型。

    

    物体中心学习旨在用对象实体（也称为槽）表示视觉数据，提供结构化表示，从而实现系统化泛化。本文提出了一种名为SlotDiffusion的对象中心潜在扩散模型 (LDM)，旨在提高槽到图像解码的质量，是一种既可用于图像数据又可用于视频数据的新型生成模型。由于LDM的强大建模能力，SlotDiffusion在无监督物体分割和视觉生成方面超过了先前的槽模型。

    Object-centric learning aims to represent visual data with a set of object entities (a.k.a. slots), providing structured representations that enable systematic generalization. Leveraging advanced architectures like Transformers, recent approaches have made significant progress in unsupervised object discovery. In addition, slot-based representations hold great potential for generative modeling, such as controllable image generation and object manipulation in image editing. However, current slot-based methods often produce blurry images and distorted objects, exhibiting poor generative modeling capabilities. In this paper, we focus on improving slot-to-image decoding, a crucial aspect for high-quality visual generation. We introduce SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for both image and video data. Thanks to the powerful modeling capacity of LDMs, SlotDiffusion surpasses previous slot models in unsupervised object segmentation and visual generation a
    
[^132]: 如何为推荐基础模型索引项目ID

    How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])

    [http://arxiv.org/abs/2305.06569](http://arxiv.org/abs/2305.06569)

    本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。

    

    推荐基础模型将推荐任务转换为自然语言任务，利用大型语言模型（LLM）进行推荐。它通过直接生成建议的项目而不是计算传统推荐模型中每个候选项目的排名得分，简化了推荐管道，避免了多段过滤的问题。为了避免在决定要推荐哪些项目时生成过长的文本，为推荐基础模型创建LLM兼容的项目ID是必要的。本研究系统地研究了推荐基础模型的项目索引问题，以P5为代表的主干模型，并使用各种索引方法复制其结果。我们首先讨论了几种微不足道的项目索引方法（如独立索引、标题索引和随机索引）的问题，并表明它们不适用于推荐基础模型，然后提出了一种新的索引方法，称为上下文感知索引。我们表明，这种索引方法在项目推荐准确性和文本生成质量方面优于其他索引方法。

    Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
    
[^133]: N-元事实的少样本链接预测

    Few-shot Link Prediction on N-ary Facts. (arXiv:2305.06104v1 [cs.AI])

    [http://arxiv.org/abs/2305.06104](http://arxiv.org/abs/2305.06104)

    本文提出了一个新任务——少样本N-元事实链接预测，并提出了一个名为FLEN的模型来实现。FLEN由三个模块组成，可以从有限的标记实例中预测N-元事实中的缺失实体。

    

    N-元事实由主要三元组（头实体、关系、尾实体）和任意数量的辅助属性值对组成，这在现实世界的知识图谱中很常见。对于N-元事实的链接预测是预测其中一个元素的缺失，填补缺失元素有助于丰富知识图谱并促进许多下游应用程序。以往的研究通常需要大量高质量的数据来理解N-元事实中的元素，但这些研究忽视了少样本关系，在现实世界的场景中却很常见。因此，本文引入一个新任务——少样本N-元事实链接预测，旨在使用有限的标记实例来预测N-元事实中的缺失实体。我们也提出了一个针对N-元事实的少样本链接预测模型FLEN，它由三个模块组成：关系学习模块、支持特定调整模块和查询推理模块。

    N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation me
    
[^134]: 自我监督学习和联邦学习的制造业模型泛化最大化

    Maximizing Model Generalization for Manufacturing with Self-Supervised Learning and Federated Learning. (arXiv:2304.14398v1 [cs.LG])

    [http://arxiv.org/abs/2304.14398](http://arxiv.org/abs/2304.14398)

    本研究提出了一种利用自我监督学习和联邦学习提高制造业模型的泛化能力，在处理未标记的和有限的状况监测数据以及领域移位时具有较好的效果，同时通过联邦学习保护了数据隐私和提高了计算效率。

    

    深度学习可以在没有手动设计的统计特征的情况下，从原始的状况监测数据中诊断故障和评估机器健康度。然而，现有的深度学习方法仍然极其困难适用于实际制造应用。机器数据通常是未标记的，只有很少的健康条件（例如，仅有正常操作数据）。此外，由于工艺参数的变化和新的故障类别的出现，模型经常遇到域的移位。传统的监督学习可能难以学习紧凑、有区别力的表示，并且不能推广到这些未见过的目标域，因为它依赖于拥有丰富的类来划分特征空间和决策边界。通过领域自适应进行的迁移学习尝试将这些模型适应到未标记的目标域，但假定了类似的子结构，在新的故障出现时可能不存在。本研究提出了一种专注于最大化源域特征普适性的方法，并应用自我监督学习结合联邦学习来改善实际制造应用中的故障诊断和机器健康评估。所提出的方法解决了未标记和有限状况监测数据以及域移位的问题，同时通过联邦学习实现了高效的计算和隐私保护。

    Deep Learning (DL) can diagnose faults and assess machine health from raw condition monitoring data without manually designed statistical features. However, practical manufacturing applications remain extremely difficult for existing DL methods. Machine data is often unlabeled and from very few health conditions (e.g., only normal operating data). Furthermore, models often encounter shifts in domain as process parameters change and new categories of faults emerge. Traditional supervised learning may struggle to learn compact, discriminative representations that generalize to these unseen target domains since it depends on having plentiful classes to partition the feature space with decision boundaries. Transfer Learning (TL) with domain adaptation attempts to adapt these models to unlabeled target domains but assumes similar underlying structure that may not be present if new faults emerge. This study proposes focusing on maximizing the feature generality on the source domain and apply
    
[^135]: 可区分的图结构模型用于晶格材料反设计

    Differentiable graph-structured models for inverse design of lattice materials. (arXiv:2304.05422v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2304.05422](http://arxiv.org/abs/2304.05422)

    本文提出了一种使用图形表示结构和属性的晶格材料的计算方法，使用可微分传递算法计算机械属性以实现反向设计，进而实现了可扩展的、具有前所未有的结构和功能多样性的晶格材料设计。

    

    处于深空恶劣环境中能够根据需要自适应的物理化学性质的材料将在定义未来的空间探索方面变得至关重要。在自然界中，微妙的微观结构和格子几何形状是设计适应于特定环境材料的令人兴奋的灵感来源。然而，由于这种不规则拓扑覆盖的巨大设计空间，在分析上进行探索是具有挑战性的。因此，迄今为止，大多数合成晶格材料都是基于周期性结构。在本文中，我们提出了一种计算方法，使用图形表示对规则和不规则晶格材料进行建模。我们的方法使用可微分传递算法计算力学性质，因此可以使用自动微分来调整单个晶格元素的几何结构和属性，从而设计具有所需属性的材料。引入对晶格结构和材料属性的隐式可学习几何表示，结合反设计框架，实现了可扩展的、具有前所未有的结构和功能多样性的晶格材料设计方法。

    Materials possessing flexible physico-chemical properties that adapt on-demand to the hostile environmental conditions of deep space will become essential in defining the future of space exploration. A promising venue for inspiration towards the design of environment-specific materials is in the intricate micro-architectures and lattice geometry found throughout nature. However, the immense design space covered by such irregular topologies is challenging to probe analytically. For this reason, most synthetic lattice materials have to date been based on periodic architectures instead. Here, we propose a computational approach using a graph representation for both regular and irregular lattice materials. Our method uses differentiable message passing algorithms to calculate mechanical properties, and therefore allows using automatic differentiation to adjust both the geometric structure and attributes of individual lattice elements to design materials with desired properties. The introdu
    
[^136]: 机器学习中公平性的关键回顾：超越准确性在移动和可穿戴计算中的应用

    Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing. (arXiv:2303.15585v1 [cs.CY])

    [http://arxiv.org/abs/2303.15585](http://arxiv.org/abs/2303.15585)

    本文通过对IMWUT期刊上过去五年发表的论文进行系统回顾，发现UbiComp社区在算法公平方面的进展滞后，存在敏感属性偏差导致的歧视性结果，需要探索报告数据集的信息以解决这些偏差。

    

    移动、可穿戴和普及计算领域正在经历着机器学习的革命性整合。设备现在可以诊断疾病、预测心脏不规则动，发掘人类认知的全部潜力。然而，相关算法在敏感属性（如性别、种族等）方面可能存在偏差，导致歧视性结果。近期，人机交互（HCI）和人工智能伦理学（AI-Ethics）研究社区开始探索报告数据集的信息以揭示并最终对抗这些偏差。本文旨在探讨在这些报告方面UbiComp社区所采纳的程度，并强调潜在不足之处。通过对过去五年（2018-2022）在ACM交互、移动、可穿戴和普适技术（IMWUT）期刊上发表的论文进行系统回顾，我们发现UbiComp社区在算法公平方面的进展滞后。

    The field of mobile, wearable, and ubiquitous computing (UbiComp) is undergoing a revolutionary integration of machine learning. Devices can now diagnose diseases, predict heart irregularities, and unlock the full potential of human cognition. However, the underlying algorithms are not immune to biases with respect to sensitive attributes (e.g., gender, race), leading to discriminatory outcomes. The research communities of HCI and AI-Ethics have recently started to explore ways of reporting information about datasets to surface and, eventually, counter those biases. The goal of this work is to explore the extent to which the UbiComp community has adopted such ways of reporting and highlight potential shortcomings. Through a systematic review of papers published in the Proceedings of the ACM Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal over the past 5 years (2018-2022), we found that progress on algorithmic fairness within the UbiComp community lags behind. 
    
[^137]: 等变增强神经网络的优化动态

    Optimization Dynamics of Equivariant and Augmented Neural Networks. (arXiv:2303.13458v1 [cs.LG])

    [http://arxiv.org/abs/2303.13458](http://arxiv.org/abs/2303.13458)

    本论文研究了在对称数据上优化多层感知机的方法，比较了等变和增强两种策略的优缺点，证明了在自然假设下等变稳定点的集合和等变层的集合具有不变性，但增强模型的稳定点可能是不稳定的。

    

    我们研究了在对称数据上优化多层感知机的方法。我们比较了限制架构等变和使用增强的策略。我们证明，在对损失和非线性性进行自然假设的情况下，等变稳定点的集合对于这两种策略是相同的，并且等变层的集合在增强模型的梯度流下是不变的。最后，我们表明，尽管等变模型的稳定点是稳定的，增强训练的稳定点可能是不稳定的。

    We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models
    
[^138]: 数据集增强：提高模型准确性和鲁棒性

    Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])

    [http://arxiv.org/abs/2303.08983](http://arxiv.org/abs/2303.08983)

    提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性、鲁棒性和校准性。例如，使用ImageDataNet+训练的ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。

    

    我们提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性，对用户没有额外的训练成本。我们提出了一种基于数据增强和知识蒸馏的数据集增强策略。我们的通用策略是基于广泛的CNN和基于transformer的模型的分析，以及对带有各种数据增强的最先进模型进行大规模的蒸馏研究。我们创建了ImageDataNet+的增强版本，以及增强的数据集CIFAR-100+，Flowers-102+和Food-101+。使用ImageDataNet+训练的模型更准确、更有鲁棒性和校准性，并且对下游任务（例如分割和检测）具有很好的迁移能力。例如，ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。在ImageDataNet+上测量的Expected Calibration Error（ECE）也有显著改进。

    We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the Ima
    
[^139]: 合成经验回放：旨在用扩充数据来提高深度强化学习的效果

    Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06614](http://arxiv.org/abs/2303.06614)

    本文提出了合成经验回放方法解决深度强化学习中数据匮乏问题，通过巧妙应用生成建模技术来扩充数据效果显著。

    

    过去十年的一个关键主题是，当大型神经网络和大型数据集相结合时，它们可以产生令人惊异的结果。在深度强化学习中，这种范式通常通过经验回放实现，其中过去的经验数据集用于训练策略或值函数。然而，与监督学习或自监督学习不同，强化学习代理必须收集自己的数据，这通常是有限的。因此，利用深度学习的好处是具有挑战性的，即使是小型神经网络在训练开始时也可能出现过拟合现象。在这项工作中，我们利用了生成建模的巨大进步，并提出了合成经验回放（SynthER），一种基于扩散的方法来灵活地上采样代理收集的经验。我们证明了SynthER是一种有效的方法，可以在离线和在线设置下训练强化学习代理，无论是在感知环境还是在像素环境中。在离线设置中，我们观察到了显着的改进。

    A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
    
[^140]: Neural-BO: 使用深度神经网络的黑盒优化算法

    Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks. (arXiv:2303.01682v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01682](http://arxiv.org/abs/2303.01682)

    Neural-BO是一种使用神经网络模型的黑盒优化算法，避免了高斯过程中的缩放和维数问题，具有高效收敛的特性。

    

    贝叶斯优化（BO）是一种对黑盒函数进行全局优化的有效方法，当函数评估代价高时。之前的大部分工作使用高斯过程来模拟黑盒函数，然而，高斯过程中使用的核函数导致了两个问题：一是基于核函数的方法在数据点数量较大时缩放困难，二是核方法在复杂结构高维数据上通常效果不佳，因为维数灾难。因此，我们提出了一种新颖的黑盒优化算法，其中黑盒函数使用神经网络进行建模。我们的算法不需要贝叶斯神经网络来估计预测不确定性，因此计算上更加有利。我们使用NTK理论的进展分析了我们算法的理论行为，展示了其收敛的高效性。我们在合成和真实世界的优化任务上进行了实验，证明了我们算法的

    Bayesian Optimization (BO) is an effective approach for global optimization of black-box functions when function evaluations are expensive. Most prior works use Gaussian processes to model the black-box function, however, the use of kernels in Gaussian processes leads to two problems: first, the kernel-based methods scale poorly with the number of data points and second, kernel methods are usually not effective on complex structured high dimensional data due to curse of dimensionality. Therefore, we propose a novel black-box optimization algorithm where the black-box function is modeled using a neural network. Our algorithm does not need a Bayesian neural network to estimate predictive uncertainty and is therefore computationally favorable. We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory showing its efficient convergence. We perform experiments with both synthetic and real-world optimization tasks and show that our algorithm is
    
[^141]: 深度不平衡时间序列预测：基于局部差异密度的方法

    Deep Imbalanced Time-series Forecasting via Local Discrepancy Density. (arXiv:2302.13563v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13563](http://arxiv.org/abs/2302.13563)

    该论文提出了一种基于局部差异密度的重新加权框架，用于解决时间序列预测中的深度不平衡问题。这个框架通过降低突发变化引起的损失，增加正常状态引起的损失，使模型能够学习可推广的模式。

    

    时间序列预测模型通常会遇到在某个时间段内的突发变化，这些变化通常是由意外事件或未知事件导致的。尽管在训练集中发生的次数很少，但突发变化引起的损失会显著影响总损失。因此，它们作为嘈杂的训练样本阻止了模型学习可推广的模式，即正常状态。基于我们的发现，我们提出了一个重新加权的框架，降低突发变化引起的损失，增加正常状态引起的损失。对于重新加权的框架，我们首先定义了一个称为局部差异度（LD）的度量，用于衡量在给定时间段内变化的突然程度。由于训练集主要由正常状态组成，我们进一步考虑了基于LD在训练集中出现的时间变化的频率。我们的重新加权框架适用于现有的时间序列预测模型，无论其架构如何。

    Time-series forecasting models often encounter abrupt changes in a given period of time which generally occur due to unexpected or unknown events. Despite their scarce occurrences in the training set, abrupt changes incur loss that significantly contributes to the total loss. Therefore, they act as noisy training samples and prevent the model from learning generalizable patterns, namely the normal states. Based on our findings, we propose a reweighting framework that down-weights the losses incurred by abrupt changes and up-weights those by normal states. For the reweighting framework, we first define a measurement termed Local Discrepancy (LD) which measures the degree of abruptness of a change in a given period of time. Since a training set is mostly composed of normal states, we then consider how frequently the temporal changes appear in the training set based on LD. Our reweighting framework is applicable to existing time-series forecasting models regardless of the architectures. T
    
[^142]: 使用特征合成工具对深度神经网络进行红队演练

    Red Teaming Deep Neural Networks with Feature Synthesis Tools. (arXiv:2302.10894v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10894](http://arxiv.org/abs/2302.10894)

    本文提出了一个用于评估可解释性工具的基准，通过训练模型以对特定触发器产生特定输出的方式，可以解决传统可解释性方法无法分析未知特征行为的问题。

    

    可解释的人工智能工具通常旨在理解模型在超出分布范围（OOD）的情况下的行为。尽管这个研究领域受到了关注，但在这些工具中很少有能够发现模型中的新颖、以前未知的错误的案例。我们认为，这部分原因在于许多可解释性方法的共同特点：它们使用特定的数据集分析和解释模型的行为。虽然这很有用，但这些工具只能分析用户可以事先采样或识别的特征所引发的行为。为了解决这个问题，一个不断增加的研究领域涉及使用不依赖于数据集的特征合成方法来解释模型。本文的主要贡献是提出了一个评估可解释性工具的基准。我们的关键观点是，我们可以训练模型以对特定触发器（例如，插入图像的特定补丁）产生特定输出（即标签），然后评估可解释性工具的有效性。

    Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified novel, previously unknown, bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze and explain the behavior of a model using a particular dataset. While this is useful, such tools can only analyze behaviors induced by features that the user can sample or identify in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods which do not depend on a dataset.  In this paper, our primary contribution is a benchmark to evaluate interpretability tools. Our key insight is that we can train models that respond to specific triggers (e.g., a specific patch inserted into an image) with specific outputs (i.e. a label) and then evaluate inte
    
[^143]: 强健的Fitted-Q评估和迭代在顺序外源未观察到的混淆因素下

    Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved Confounders. (arXiv:2302.00662v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00662](http://arxiv.org/abs/2302.00662)

    提出了一种在顺序外源未观察到的混淆因素下的强健策略评估和优化方法，使用正交化的强健Fitted-Q迭代，并添加了分位数估计的偏差校正。

    

    在医学、经济和电子商务等领域，离线强化学习非常重要，因为在线实验可能成本高昂、危险或不道德，并且真实模型未知。然而，大多数方法假设行为策略的所有协变量都是已观察到的。尽管这个假设"顺序可忽略性"在观察数据中不太可能成立，但大部分考虑进入治疗因素的数据可能是观察到的，这激励了敏感性分析。我们研究了在敏感性模型下顺序外源未观察到的混淆因素下的强健策略评估和策略优化。我们提出并分析了正交化的强健Fitted-Q迭代，该方法使用强健贝尔曼算子的封闭形式解来导出强健Q函数的损失最小化问题，并对分位数估计加入偏差校正。我们的算法兼具Fitted-Q迭代的计算简便性和统计优势。

    Offline reinforcement learning is important in domains such as medicine, economics, and e-commerce where online experimentation is costly, dangerous or unethical, and where the true model is unknown. However, most methods assume all covariates used in the behavior policy's action decisions are observed. Though this assumption, sequential ignorability/unconfoundedness, likely does not hold in observational data, most of the data that accounts for selection into treatment may be observed, motivating sensitivity analysis. We study robust policy evaluation and policy optimization in the presence of sequentially-exogenous unobserved confounders under a sensitivity model. We propose and analyze orthogonalized robust fitted-Q-iteration that uses closed-form solutions of the robust Bellman operator to derive a loss minimization problem for the robust Q function, and adds a bias-correction to quantile estimation. Our algorithm enjoys the computational ease of fitted-Q-iteration and statistical 
    
[^144]: 一招两得：粗粒化分子动力学的扩散模型和力场

    Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics. (arXiv:2302.00600v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00600](http://arxiv.org/abs/2302.00600)

    本文通过训练扩散生成模型来学习粗粒化分子动力学的力场，无需使用任何力场输入，从而实现了对生物过程的精确模拟，并展示了在多个蛋白质模拟中具有优越性能。

    

    粗粒化（CG）分子动力学使得在原子分辨率下无法解决的生物过程可以得以研究。然而，准确学习CG力场仍然是一个挑战。在本文中，我们利用基于评分的生成模型、力场和分子动力学之间的联系，学习了一个CG力场，而在训练过程中不需要任何力场输入。具体地，我们对来自分子动力学模拟的蛋白质结构进行了扩散生成模型的训练，我们展示了它的评分函数近似一个可以直接用于模拟CG分子动力学的力场。尽管相比以前的工作，我们的方法具有大大简化的训练设置，但我们证明了我们的方法在几个小型到中型蛋白质模拟中具有改进的性能，能够重现CG平衡分布，并保持蛋白质折叠等全原子模拟的动力学。

    Coarse-grained (CG) molecular dynamics enables the study of biological processes at temporal and spatial scales that would be intractable at an atomistic resolution. However, accurately learning a CG force field remains a challenge. In this work, we leverage connections between score-based generative models, force fields and molecular dynamics to learn a CG force field without requiring any force inputs during training. Specifically, we train a diffusion generative model on protein structures from molecular dynamics simulations, and we show that its score function approximates a force field that can directly be used to simulate CG molecular dynamics. While having a vastly simplified training setup compared to previous work, we demonstrate that our approach leads to improved performance across several small- to medium-sized protein simulations, reproducing the CG equilibrium distribution, and preserving dynamics of all-atom simulations such as protein folding events.
    
[^145]: 使用Tsallis KL散度的广义Munchausen强化学习

    Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence. (arXiv:2301.11476v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11476](http://arxiv.org/abs/2301.11476)

    这篇论文通过研究广义的Tsallis KL散度，扩展了Munchausen强化学习算法，并提供了一种将KL正则化纳入实际算法的方法。对于Tsallis KL，当$q > 1$时，可以获得新的策略优化选项。

    

    许多强化学习中的策略优化方法都采用Kullback-Leibler（KL）散度到上一个策略，以防止策略变化过快。这个想法最初是在Conservative Policy Iteration的一篇重要论文中提出的，近似算法如TRPO和Munchausen Value Iteration（MVI）给出了有限的方法。我们通过研究一种广义的KL散度 - 称为Tsallis KL散度 - 来继续这一工作，它在定义中使用了$q$-对数。这种方法是一种严格的推广，因为$q = 1$对应于标准的KL散度；$q > 1$提供了一系列新的选项。我们对在Tsallis KL下学习的策略类型进行了表征，并阐述了何时$ q > 1 $可能是有益的。为了获得一个将Tsallis KL正则化纳入实际算法的方法，我们扩展了MVI，它是一种最简单的包含KL正则化的方法之一。我们展示了这种广义MVI（$q$）获得了显著的改进。

    Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leilbler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence -- called the Tsallis KL divergence -- which use the $q$-logarithm in the definition. The approach is a strict generalization, as $q = 1$ corresponds to the standard KL divergence; $q > 1$ provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when $q >1$ could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI($q$) obtains significant improve
    
[^146]: A-NeSI: 一种可扩展的近似方法用于概率神经符号推理。

    A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference. (arXiv:2212.12393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12393](http://arxiv.org/abs/2212.12393)

    本文介绍了一种名为A-NeSI的新颖PNL框架，它使用神经网络实现了近似推理，能够保证概率逻辑语义的同时解决了PNL的可扩展性问题，能够在安全关键应用中保证逻辑约束的满足。

    

    本文研究了将神经网络与符号推理相结合的问题。最近引入的概率神经符号学习（PNL）框架，如DeepProbLog，执行指数时间的精确推理，限制了PNL解决方案的可扩展性。我们介绍了近似神经符号推理（A-NeSI）：一种新的PNL框架，它使用神经网络进行可扩展的近似推理。A-NeSI 1) 在不改变概率逻辑语义的情况下，以多项式时间执行近似推理；2) 使用由背景知识生成的数据进行训练；3) 可以生成有关预测的符号解释；4) 可以在测试时间保证逻辑约束的满足，这在安全关键应用中非常重要。我们的实验表明，A-NeSI是第一个能够解决具有指数组合扩展的三种神经符号任务的端到端方法。最后，我们的实验表明，A-NeSI实现了可解释性和安全性，而没有惩罚。

    We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in p
    
[^147]: 用于未知物体实例分割的均值漂移掩模变换器

    Mean Shift Mask Transformer for Unseen Object Instance Segmentation. (arXiv:2211.11679v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11679](http://arxiv.org/abs/2211.11679)

    本文提出了一种新的均值漂移掩模变换器，用于联合训练和推断特征提取器和聚类器，可用于未知物体实例分割，在COCO数据集上表现出竞争性的性能，并在罕见和未知物体类别上具有显着优势。

    

    物体实例的分割是机器人需要掌握的关键感知技能之一，它有助于机器人抓取和操作未知物体。均值漂移聚类是一种广泛用于图像分割任务的方法。然而，传统的均值漂移聚类算法不可微分，使其难以集成到端到端的神经网络训练框架中。在本文中，我们提出了均值漂移掩模变换器（MSMFormer），这是一种新的变换器体系结构，模拟 von Mises-Fisher（vMF）均值漂移聚类算法，允许联合训练和推断特征提取器和聚类器。其核心组件是超球面注意力机制，可在超球面上更新物体查询。为了说明我们方法的有效性，我们将MSMFormer应用于未知物体实例分割。实验结果表明，MSMFormer在COCO数据集上与现有方法相比取得了竞争性的性能，并且在罕见和未知物体类别上具有显着优势。

    Segmenting unseen objects from images is a critical perception skill that a robot needs to acquire. In robot manipulation, it can facilitate a robot to grasp and manipulate unseen objects. Mean shift clustering is a widely used method for image segmentation tasks. However, the traditional mean shift clustering algorithm is not differentiable, making it difficult to integrate it into an end-to-end neural network training framework. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to unseen object instance segmentation. Our experiments show that MSMFormer achieves competitive performance compared to
    
[^148]: "任务相关的自编码"增强了人类神经科学的机器学习

    "Task-relevant autoencoding" enhances machine learning for human neuroscience. (arXiv:2208.08478v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2208.08478](http://arxiv.org/abs/2208.08478)

    "任务相关的自编码"（TRACE）是一种用于人类神经科学的机器学习方法，可以从人类神经影像数据中提取与受试者行为相关的低维表示，并在分类准确率和发现能力上超过了其他常用模型。

    

    在人类神经科学中，机器学习可以帮助揭示与受试者行为相关的低维神经表示。然而，最先进的模型通常需要大量的训练数据，因此容易在人类神经影像数据上过拟合，这些数据往往具有较少的样本但很多输入维度。在这里，我们利用了一个事实，即我们在人类神经科学中寻求的特征恰好与受试者行为相关。因此，我们开发了一种通过分类器增强的任务相关自编码器（TRACE），并将其与标准自编码器、变分自编码器和主成分分析进行了比较，对两个严重缩减的机器学习数据集提取了行为相关的、可分离的表示。然后，我们在59名观察动物和物体的受试者的fMRI数据上评估了所有模型。TRACE几乎在所有模型中表现出色，显示出高达12%的分类准确率提升和高达56%的发现改善。

    In human neuroscience, machine learning can help reveal lower-dimensional neural representations relevant to subjects' behavior. However, state-of-the-art models typically require large datasets to train, so are prone to overfitting on human neuroimaging data that often possess few samples but many input dimensions. Here, we capitalized on the fact that the features we seek in human neuroscience are precisely those relevant to subjects' behavior. We thus developed a Task-Relevant Autoencoder via Classifier Enhancement (TRACE), and tested its ability to extract behaviorally-relevant, separable representations compared to a standard autoencoder, a variational autoencoder, and principal component analysis for two severely truncated machine learning datasets. We then evaluated all models on fMRI data from 59 subjects who observed animals and objects. TRACE outperformed all models nearly unilaterally, showing up to 12% increased classification accuracy and up to 56% improvement in discoveri
    
[^149]: 自编码器的自监督训练用于视觉异常检测

    Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.11723](http://arxiv.org/abs/2206.11723)

    本文提出了一种自监督学习方法，通过修改重构误差的方式集中在数据流形上，从而解决深度卷积自编码器在视觉异常检测中容易出现的重构异常信号而导致检测效果不佳的问题。

    

    深度卷积自编码器提供了一种有效的工具，可以以无监督的方式学习非线性降维。最近，它们已被用于视觉领域的异常检测任务。通过使用无异常的样本优化重构误差，普遍认为相应的网络应在应用阶段未能准确重构异常区域。这个目标通常通过控制网络的容量来解决，要么通过减少瓶颈层的大小，要么通过对其激活施加稀疏约束。然而，这两种技术都没有明确惩罚异常信号的重构，通常导致检测效果不佳。我们通过自适应自监督学习方法来解决这个问题，这种方法允许在训练过程中使用判别信息，通过修改的重构误差集中在数据流形上。这使得模型能够产生局部一致的重构结果。

    Deep convolutional autoencoders provide an effective tool for learning non-linear dimensionality reduction in an unsupervised way. Recently, they have been used for the task of anomaly detection in the visual domain. By optimising for the reconstruction error using anomaly-free examples, the common belief is that a corresponding network should fail to accurately reconstruct anomalous regions in the application phase. This goal is typically addressed by controlling the capacity of the network by either reducing the size of the bottleneck layer or enforcing sparsity constraints on its activations. However, neither of these techniques does explicitly penalize reconstruction of anomalous signals often resulting in poor detection. We tackle this problem by adapting a self-supervised learning regime, which allows to use discriminative information during training focusing on the data manifold by means of a modified reconstruction error. This regularizes the model to produce locally consistent
    
[^150]: 随机初始化的深度神经网络的量化高斯逼近

    Quantitative Gaussian Approximation of Randomly Initialized Deep Neural Networks. (arXiv:2203.07379v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.07379](http://arxiv.org/abs/2203.07379)

    本论文通过量化高斯逼近的方法，研究了随机初始化的深度神经网络的输出分布与高斯过程的二次Wasserstein距离的上界，揭示了隐藏层和输出层大小对网络高斯行为的影响，并定量地恢复了在宽限制下的分布收敛结果。

    

    对于任意一个使用随机高斯参数初始化的深度全连接神经网络，我们对其输出分布与适当的高斯过程之间的二次Wasserstein距离进行了上界限制。我们的明确不等式表明隐藏层和输出层的大小如何影响网络的高斯行为，并且定量地恢复了宽限制下的分布收敛结果，即如果所有隐藏层的大小变得很大。

    Given any deep fully connected neural network, initialized with random Gaussian parameters, we bound from above the quadratic Wasserstein distance between its output distribution and a suitable Gaussian process. Our explicit inequalities indicate how the hidden and output layers sizes affect the Gaussian behaviour of the network and quantitatively recover the distributional convergence results in the wide limit, i.e., if all the hidden layers sizes become large.
    
[^151]: Pixyz:用于开发深度生成模型的Python库

    Pixyz: a Python library for developing deep generative models. (arXiv:2107.13109v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.13109](http://arxiv.org/abs/2107.13109)

    Pixyz是一个用于开发深度生成模型的Python库，通过封装深度神经网络为概率分布以及基于目标函数的模型设计和学习，可以更简洁直观地实现各种DGMs。此外，该库还引入了记忆化技术加快计算速度，并在实验证明在训练DGMs时比现有的概率编程语言更快。

    

    随着深度生成模型（DGMs）研究的快速进展，需要一个能够以简单且通用的方式实现它们的框架。本研究关注DGMs的两个特点：（1）深度神经网络被概率分布所封装，（2）模型是基于目标函数进行设计和学习的。考虑到这些特点，我们提出了一个名为Pixyz的新的Python库来实现DGMs。该库采用逐步实现方法，具有三个API，可以更简洁直观地实现各种DGMs。此外，该库引入了记忆化技术，以减少DGMs中重复计算的成本，加快计算速度。我们通过实验证明，与现有的概率编程语言相比，该库在训练DGMs时速度更快。

    With the recent rapid progress in the study of deep generative models (DGMs), there is a need for a framework that can implement them in a simple and generic way. In this research, we focus on two features of DGMs: (1) deep neural networks are encapsulated by probability distributions, and (2) models are designed and learned based on an objective function. Taking these features into account, we propose a new Python library to implement DGMs called Pixyz. This library adopts a step-by-step implementation method with three APIs, which allows us to implement various DGMs more concisely and intuitively. In addition, the library introduces memoization to reduce the cost of duplicate computations in DGMs to speed up the computation. We demonstrate experimentally that this library is faster than existing probabilistic programming languages in training DGMs.
    
[^152]: 评估自动临床语言简化的基准：数据集、算法和评估

    Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation. (arXiv:2012.02420v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2012.02420](http://arxiv.org/abs/2012.02420)

    该论文构建了一个名为MedLane的新数据集，支持自动临床语言简化方法的开发和评估。提出了一种叫做DECLARE的新模型，与八种基准模型相比取得了最先进的性能，并提出了三个特定的评估指标。

    

    低健康素养的患者通常很难理解医学术语和专业医学语言的复杂结构。尽管有一些研究提出了自动将专业语言翻译成普通人可以理解的语言，但其中只有少数关注了临床领域中准确性和可读性的两个方面。因此，临床语言的简化仍然是一个具有挑战性的任务，但可惜的是，在之前的工作中尚未完全解决。为了评估这项任务，我们构建了一个名为MedLane的新数据集，以支持自动临床语言简化方法的开发和评估。此外，我们提出了一种名为DECLARE的新模型，遵循人工注释过程，与八种强基准模型相比取得了最先进的性能。为了公平评估性能，我们还提出了三个特定的评估指标。实验结果表明了MedLane数据集的实用性。

    Patients with low health literacy usually have difficulty understanding medical jargon and the complex structure of professional medical language. Although some studies are proposed to automatically translate expert language into layperson-understandable language, only a few of them focus on both accuracy and readability aspects simultaneously in the clinical domain. Thus, simplification of the clinical language is still a challenging task, but unfortunately, it is not yet fully addressed in previous work. To benchmark this task, we construct a new dataset named MedLane to support the development and evaluation of automated clinical language simplification approaches. Besides, we propose a new model called DECLARE that follows the human annotation procedure and achieves state-of-the-art performance compared with eight strong baselines. To fairly evaluate the performance, we also propose three specific evaluation metrics. Experimental results demonstrate the utility of the annotated Med
    
[^153]: 使用卷积神经网络学习宇宙学中的中微子效应

    Learning neutrino effects in Cosmology with Convolutional Neural Networks. (arXiv:1910.04255v2 [astro-ph.CO] UPDATED)

    [http://arxiv.org/abs/1910.04255](http://arxiv.org/abs/1910.04255)

    该论文使用基于深度学习网络的新方法快速生成包含大质量中微子的模拟数据，并且通过计算多个相关统计量验证了其准确性。

    

    测量三种活动中微子质量之和Mv是现代宇宙学中最重要的挑战之一。大质量中微子在宇宙的大尺度结构上留下了特征性的签名。为了最大化可以从星系调查中获取的信息，需要在非线性区域提供准确的理论预测。目前，实现这些预测的一种方法是运行宇宙数值模拟。然而，生成这些模拟需要大量的计算资源，对于每个中微子质量情况，需要数百到数千小时的核心计算时间。在这项工作中，我们提出了一种基于深度学习网络的新方法，快速生成没有中微子的标准ΛCDM模拟中含有大质量中微子的模拟。我们计算了深度学习生成模拟的多个相关统计量，并得出结论，我们的方法是一种准确的替代方法。

    Measuring the sum of the three active neutrino masses, $M_\nu$, is one of the most important challenges in modern cosmology. Massive neutrinos imprint characteristic signatures on several cosmological observables in particular on the large-scale structure of the Universe. In order to maximize the information that can be retrieved from galaxy surveys, accurate theoretical predictions in the non-linear regime are needed. Currently, one way to achieve those predictions is by running cosmological numerical simulations. Unfortunately, producing those simulations requires high computational resources -- several hundred to thousand core-hours for each neutrino mass case. In this work, we propose a new method, based on a deep learning network, to quickly generate simulations with massive neutrinos from standard $\Lambda$CDM simulations without neutrinos. We computed multiple relevant statistical measures of deep-learning generated simulations, and conclude that our approach is an accurate alte
    
[^154]: 迭代随机算子的概率收缩分析

    Probabilistic Contraction Analysis of Iterated Random Operators. (arXiv:1804.01195v6 [math.PR] UPDATED)

    [http://arxiv.org/abs/1804.01195](http://arxiv.org/abs/1804.01195)

    本文提出了一种新的概率收缩分析方法，用于证明由迭代随机算子产生的马尔可夫链在特定条件下的收敛性，为理解各种蒙特卡洛方法的收敛性提供了一个通用框架。

    

    在许多工程学的领域中，Banach收缩映射定理被用来证明特定确定性算法的收敛性。已经开发了这些算法的随机版本，在数据驱动问题中证明了其实用性。在一类随机化算法中，每次迭代中，收缩映射被近似为使用某些随机变量的独立同分布样本的算子。这导致了作用在完备度量空间中的初始点上的迭代随机算子，并且生成了一个马尔可夫链。在本文中，我们发展了一种新的基于随机支配的证明技术，称为概率收缩分析，用于在特定极限规则下证明由这些迭代随机算子产生的马尔可夫链的收敛性。本文中开发的方法为理解各种蒙特卡洛方法的收敛性提供了一个通用框架。

    In many branches of engineering, Banach contraction mapping theorem is employed to establish the convergence of certain deterministic algorithms. Randomized versions of these algorithms have been developed that have proved useful in data-driven problems. In a class of randomized algorithms, in each iteration, the contraction map is approximated with an operator that uses independent and identically distributed samples of certain random variables. This leads to iterated random operators acting on an initial point in a complete metric space, and it generates a Markov chain. In this paper, we develop a new stochastic dominance based proof technique, called probabilistic contraction analysis, for establishing the convergence in probability of Markov chains generated by such iterated random operators in certain limiting regime. The methods developed in this paper provides a general framework for understanding convergence of a wide variety of Monte Carlo methods in which contractive property
    

