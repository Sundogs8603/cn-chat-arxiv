# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [EGC: Image Generation and Classification via a Single Energy-Based Model.](http://arxiv.org/abs/2304.02012) | EGC是一种使用单个神经网络在图像分类和图像生成任务中实现卓越性能的方法，可以较好地生成出高质量图像，并在多项数据集上实现了领先的分类结果。 |
| [^2] | [FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer.](http://arxiv.org/abs/2304.02011) | 本文提出了一种使用加性噪声和神经风格迁移技术来模拟电子显微镜正向算子，以解决深度学习方法需要大量训练数据集的问题。该方法在粒子定位和分类任务上表现良好。 |
| [^3] | [Autoregressive Neural TensorNet: Bridging Neural Networks and Tensor Networks for Quantum Many-Body Simulation.](http://arxiv.org/abs/2304.01996) | 本研究提出了自回归神经张量网络（ANTN）来桥接张量网络和自回归神经网络，可以提高多体量子模拟的表达能力和精度，具有广泛的应用前景。 |
| [^4] | [DWA: Differential Wavelet Amplifier for Image Super-Resolution.](http://arxiv.org/abs/2304.01994) | 本文介绍了一种基于小波的图像超分辨率模块DWA，通过利用两个卷积滤波器的差异改进小波SR模型，在小波域中提高相关特征提取并抑制噪声。在现有的SR模型中集成DWA，如DWSR和MWCNN，可以显示出其有效性。 |
| [^5] | [ERM++: An Improved Baseline for Domain Generalization.](http://arxiv.org/abs/2304.01973) | ERM++是一个用于域通用性的改进基准方法，通过更好地利用训练数据、模型参数选择和权重空间正则化等关键技术，在多个数据集上比标准ERM更有效，同时计算复杂度更低，表现也优于最先进方法。 |
| [^6] | [Model-corrected learned primal-dual models for fast limited-view photoacoustic tomography.](http://arxiv.org/abs/2304.01963) | 本研究提出了一种模型修正的学习主对偶框架，可用于快速有限视角光声层析成像，通过在数据空间中共同学习模型修正与图像空间中的学习更新操作，有效解决了计算开销较大的前向模型的问题，并具有一定的理论指导意义和实际应用价值。 |
| [^7] | [Randomized Adversarial Style Perturbations for Domain Generalization.](http://arxiv.org/abs/2304.01959) | 本文提出了一种随机对抗风格扰动技术，它能够通过对抗性扰动特征风格达到域泛化的效果，同时结合混合原始特征的方法缓解扰动带来的挑战。 |
| [^8] | [TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems.](http://arxiv.org/abs/2304.01951) | TransPimLib提供了处理器内存系统上高效的超越函数计算方法，有助于提高PIM系统的计算能力和支持更广泛的工作负载，特别是机器学习应用中的激活函数。 |
| [^9] | [High-Throughput Vector Similarity Search in Knowledge Graphs.](http://arxiv.org/abs/2304.01926) | 本篇论文讨论了在知识图谱中进行向量相似性搜索的方法，主要关注于混合查询，既有向量相似度搜索，又有与底层数据向量相关的关系属性。 |
| [^10] | [Accelerating and Compressing Deep Neural Networks for Massive MIMO CSI Feedback.](http://arxiv.org/abs/2304.01914) | 本文提出了一种用于 Massive MIMO CSI 反馈的加速和压缩有效神经网络，采用了网络修剪、训练后动态范围量化和权重聚类等优化方法，以缓解神经网络在实际无线系统中的限制。 |
| [^11] | [Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable.](http://arxiv.org/abs/2304.01910) | 神经网络训练的运行变化在实际中更少遇到问题，我们提出了一种简化的统计假设并证明方差主要由于训练过程对初始条件的高敏感性所导致。 |
| [^12] | [Leveraging Deep Learning Approaches for Deepfake Detection: A Review.](http://arxiv.org/abs/2304.01908) | 本文综述了Deepfake检测的研究现状，探讨了使用深度学习方法判断Deepfake的不同技术，并旨在实现一种成本效益高且具有高准确性的模型来解决数据集泛化问题。 |
| [^13] | [Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python.](http://arxiv.org/abs/2304.01906) | 本文介绍了一款名为 Torch-Choice 的 PyTorch 软件包，用于管理数据库、构建多项式Logit和嵌套Logit模型，并支持GPU加速，具有灵活性和高效性。 |
| [^14] | [Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition.](http://arxiv.org/abs/2304.01905) | 本文介绍了一种新的“双关注神经变换器”，可以通过优化唤醒词检测来选择计算路径，从而提高唤醒词的准确性和推理时间效率，并且计算成本可以降低90％而仅增加1％的参数。这种架构可以在语音识别领域中大有裨益。 |
| [^15] | [Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion.](http://arxiv.org/abs/2304.01893) | 通过引导轨迹扩散控制的行人动画生成方法，可以实现对行人轨迹和全身动画的精准控制和模拟，为特定场景的处理提供了新的思路。 |
| [^16] | [Sociocultural knowledge is needed for selection of shots in hate speech detection tasks.](http://arxiv.org/abs/2304.01890) | HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。 |
| [^17] | [Measure theoretic results for approximation by neural networks with limited weights.](http://arxiv.org/abs/2304.01880) | 本文探讨了有限权重神经网络的逼近性质，给出了获得密度的必要且充分的度量论条件，同时针对一种特定激活函数和固定神经元数量的神经网络证明了其在连续函数空间中具有密度。 |
| [^18] | [Incremental Verification of Neural Networks.](http://arxiv.org/abs/2304.01874) | 提出了一种新的、基于设计新的理论、数据结构和算法的神经网络增量与完全验证的通用框架，实现了对MNIST和CIFAR10以及ACAS-XU分类器的更高效的验证。 |
| [^19] | [A Survey on Vertical Federated Learning: From a Layered Perspective.](http://arxiv.org/abs/2304.01829) | 垂直联邦学习（VFL）是一种适用于数据垂直分区情况的有前途的联邦学习方法，该方法丰富了样本描述，以提高模型容量。研究人员在硬件层到垂直联邦系统层各个方面做了贡献，VFL的应用已涵盖多个领域，尤其隐私保护是关键问题。 |
| [^20] | [Learning Stable and Robust Linear Parameter-Varying State-Space Models.](http://arxiv.org/abs/2304.01828) | 该论文提出了稳定和鲁棒的线性参数可变状态空间模型的两种直接参数化方法，训练出的模型具有收缩意义或通过用户定义的值被限制在 Lipschitz 常数内，对进一步的凸分析或控制器设计非常有用。 |
| [^21] | [CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization.](http://arxiv.org/abs/2304.01814) | 本文提出了一种新的上下文误差调制广义扩散模型（CoreDiff），用于低剂量CT（LDCT）的去噪。该模型利用LDCT图像来消除随机高斯噪声并模拟CT退化的物理过程，减少采样步骤，并引入上下文误差调制以增强鲁棒性和泛化能力。 |
| [^22] | [HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation.](http://arxiv.org/abs/2304.01811) | HarsanyiNet 是一种新型的深度神经网络架构，它可以在单次前向传播中计算输入变量的精确 Shapley 值。 |
| [^23] | [Machine Learning Discovery of Optimal Quadrature Rules for Isogeometric Analysis.](http://arxiv.org/abs/2304.01802) | 本论文使用机器学习技术找到了在等几何分析中构造刚度和质量矩阵的最佳求积规则，并与传统的Gauss型求积规则和标准中点规则进行了比较，在许多基准问题中表现出更好的性能。 |
| [^24] | [Personalized Federated Learning with Local Attention.](http://arxiv.org/abs/2304.01783) | 本文提出了一个名为pFedLA的算法，通过将注意力机制并入个性化模型来解决联邦学习中客户端数据异质性的问题，并在实验中表现出了优异的表现，尤其是在缓解特征漂移问题方面。 |
| [^25] | [Imitation Learning from Nonlinear MPC via the Exact Q-Loss and its Gauss-Newton Approximation.](http://arxiv.org/abs/2304.01782) | 本文提出了一种新的 Q-loss 函数，通过精确嵌入最优控制问题的目标和约束，以及 Gauss-Newton 近似损失加速训练，从而在带约束的非线性系统控制中进行模仿学习。 |
| [^26] | [Mixing predictions for online metric algorithms.](http://arxiv.org/abs/2304.01781) | 本文提出了一种在线算法的混合预测方法，针对度量任务系统，我们获得了$O(\ell^2)$的竞争比，可以使算法跟随不同的预测器，对限制切换次数的情况可以获得$(1+\epsilon)$-竞争算法。 |
| [^27] | [A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System.](http://arxiv.org/abs/2304.01774) | 本文介绍了一种以用户为中心的交互式人机交互主题建模系统，该系统除了支持从整个语料库中学习主题外，还支持目标主题建模，并提供了一种新颖的主题词建议功能。该系统可以很好地迭代地完善模型，且在用户研究中获得了良好的评价。 |
| [^28] | [A differentiable programming framework for spin models.](http://arxiv.org/abs/2304.01772) | 本研究提出了一种可微编程框架，用于高效地模拟自旋系统，在伊辛模型、波茨模型和细胞波茨模型上进行了实验验证，实现了显著的加速效果。 |
| [^29] | [Convergence of alternating minimisation algorithms for dictionary learning.](http://arxiv.org/abs/2304.01768) | 本文探讨了字典学习中两种交替极小化算法的收敛性，在良好的初始化下，这两种算法能够以几何收敛速率收敛于生成的字典，且可适用于非均匀分布的数据模型。 |
| [^30] | [Incorporating Unlabelled Data into Bayesian Neural Networks.](http://arxiv.org/abs/2304.01762) | 该论文提出了一种利用未标记数据学习贝叶斯神经网络（BNNs）的对比框架，通过该框架提出了一种同时具备自监督学习的标签效率和贝叶斯方法中的不确定性估计的实用BNN算法。最后，该方法在半监督和低预算主动学习问题中展现出了数据高效学习的优势。 |
| [^31] | [Black Box Few-Shot Adaptation for Vision-Language models.](http://arxiv.org/abs/2304.01752) | 本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的视觉-语言模型的快速少样本适应，适用于有监督和无监督训练，并且可以用于对单模型计算的图像和文本特征进行对齐。 |
| [^32] | [Adaptive learning of effective dynamics: Adaptive real-time, online modeling for complex systems.](http://arxiv.org/abs/2304.01732) | AdaLED是一个新的系统性框架，它将大规模模拟与降阶模型连接起来，以自适应方式提取和预测多尺度系统的有效动力学。它使用自编码器来识别系统动态的降阶表示，并使用概率循环神经网络（RNNs）的集合作为潜在时间步进器。 |
| [^33] | [Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher.](http://arxiv.org/abs/2304.01731) | 本文提出了有选择的联邦蒸馏机制Selective-FD，可以精确地识别来自本地和集合预测的知识，以解决局部数据分布的变异和缺乏好的教师模型而导致的误导和模糊的知识共享问题，并取得了显著提高的模型性能和准确度。 |
| [^34] | [Characterizing the contribution of dependent features in XAI methods.](http://arxiv.org/abs/2304.01717) | 该论文探讨了在XAI方法中考虑预测变量的依赖关系，提出了一种简单快速的方法，并证明其模型无关性。 |
| [^35] | [Optimal Transport for Correctional Learning.](http://arxiv.org/abs/2304.01701) | 本文提出了一种基于最优传输的纠正学习方法，能够有效地提高参数估计准确性，允许估计更复杂的特征并考虑多种教师的干预策略。 |
| [^36] | [Denoising Diffusion Probabilistic Models to Predict the Density of Molecular Clouds.](http://arxiv.org/abs/2304.01670) | 本文引入了一种新的深度学习去噪扩散概率模型，可以从投影质量表面密度图预测巨大分子云的体积或数密度，并在数密度预测精度方面获得了一个量级的提高。 |
| [^37] | [Re-thinking Model Inversion Attacks Against Deep Neural Networks.](http://arxiv.org/abs/2304.01669) | 本文重新审视深度学习中的模型逆推攻击，提出了一种改进的优化目标和一个新型的“模型增强”思路，可以显著提高攻击性能。 |
| [^38] | [On the Stability-Plasticity Dilemma of Class-Incremental Learning.](http://arxiv.org/abs/2304.01663) | 本文探讨类增量学习算法如何有效解决稳定性-可塑性权衡问题，发现大多数算法更倾向于保持稳定性而不是可塑性，并对训练在初始类上的模型的特征提取器几乎没有改变。 |
| [^39] | [De-novo Identification of Small Molecules from Their GC-EI-MS Spectra.](http://arxiv.org/abs/2304.01634) | 该论文提出了一种针对GC-EI-MS光谱的新型\emph{de-novo}方法，能够直接从质谱数据推导小分子的结构，克服了当前可靠的光谱数据库无法覆盖足够密集潜在化学空间的难题。 |
| [^40] | [Equivariant Networks for Porous Crystalline Materials.](http://arxiv.org/abs/2304.01628) | 本研究开发了一种模型，它在架构中合并了晶体的单元格对称性，并显式地建模了多孔结构，可更准确地预测多孔晶体材料的吸附热。 |
| [^41] | [MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan.](http://arxiv.org/abs/2304.01576) | 本文提出了一种名为MESAHA-Net的高效端到端框架，集成了三种类型的输入，通过采用自适应硬注意力机制，逐层2D分割，实现了 CT扫描中精确的肺结节分割。 |
| [^42] | [The expressive power of pooling in Graph Neural Networks.](http://arxiv.org/abs/2304.01575) | 本文研究了池化算子在图神经网络中的表达能力，并提供了一个通用标准来选择或设计池化算子。 |
| [^43] | [Spatiotemporal and Semantic Zero-inflated Urban Anomaly Prediction.](http://arxiv.org/abs/2304.01569) | 提出了STS模型来预测城市异常，解决了由于异常数据稀疏零膨胀导致的问题，并且可以统一预测多种异常，在交通事故和犯罪预测数据集上表现良好。 |
| [^44] | [Arrhythmia Classifier Based on Ultra-Lightweight Binary Neural Network.](http://arxiv.org/abs/2304.01568) | 本研究提出了一种能够通过ECG信号进行5类和17类心律失常分类的超轻量级二值神经网络(BNN)，在存储使用率最低的情况下，分别达到了96.90%和97.50%的准确率。 |
| [^45] | [A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material.](http://arxiv.org/abs/2304.01565) | 该论文为图扩散模型在生成科学中AI内容（AIGC）领域的综述。该模型已成为各领域生成建模的新趋势，涵盖了分子、蛋白质和材料科学等多个领域。 |
| [^46] | [Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression.](http://arxiv.org/abs/2304.01561) | 本文研究了Shallow ReLU$^k$神经网络的逼近容量，证明了其最优逼近速率。应用到非参数回归上，证明了浅层神经网络可以实现学习H\"older函数的最优渐进速率，补充了深层神经网络的最近结果。 |
| [^47] | [Real-time Driver Monitoring Systems on Edge AI Device.](http://arxiv.org/abs/2304.01555) | 本论文介绍了一种运行在边缘AI设备上的实时驾驶员监控系统，该系统经过模型手术，在硬件加速器的帮助下实现了高帧率的处理效果。 |
| [^48] | [Heating and dynamics of the Solar atmosphere.](http://arxiv.org/abs/2304.01553) | 该论文研究太阳大气中不同结构的太阳风源区、形成高度以及加热机制，揭示太阳风与日冕空洞之间的相关性，并提供了预测和警告近地球空间天气的方法。 |
| [^49] | [How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis.](http://arxiv.org/abs/2304.01545) | 本研究探讨了使用3D卷积神经网络（3D-CNN）对时空数据进行风速预测的精度，并发现使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。 |
| [^50] | [Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation.](http://arxiv.org/abs/2304.01541) | 本论文研究了在通信和差分隐私约束下，平均值和频率估计的最优准确性，证明每个客户端只需发送$\Theta\left( n \min\left(\varepsilon, \varepsilon^2\right)\right)$比特的FL问题和$\Theta\left(\log\left( n\min\left(\varepsilon, \varepsilon^2\right) \right)\right)$比特的FA问题即可实现最优误差，从而在联合学习和分析中实现了隐私、精确性和通信的最优权衡。 |
| [^51] | [Online Learning with Adversaries: A Differential Inclusion Analysis.](http://arxiv.org/abs/2304.01525) | 本文提出了第一个能够以几乎确定的方式收敛到 $\mu$ 的异步在线算法，应用了微分包容分析，并提供了两个关键亮点。 |
| [^52] | [Multimodal Neural Processes for Uncertainty Estimation.](http://arxiv.org/abs/2304.01518) | 本论文提出了一种新的神经过程模型，即多模态神经过程，用于对多模态数据进行不确定性估计，该模型具有动态上下文记忆、多模态贝叶斯聚合和校准预测的注意机制，经实验表明在多模态不确定性估计方面性能最先进，对于噪声样本具有良好抵抗能力，并且对于领域之外的检测是可靠的。 |
| [^53] | [Handling Concept Drift in Global Time Series Forecasting.](http://arxiv.org/abs/2304.01512) | 本文提出两种新的概念漂移处理方法，应用于全球时间序列预测，填补了处理分类领域中概念漂移方法的空白。 |
| [^54] | [EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition.](http://arxiv.org/abs/2304.01508) | EPVT是一种基于环境感知的提示视觉Transformer，用于解决皮肤病变识别中深度神经网络可能过度依赖疾病不相关图像特征的问题，通过嵌入一组领域提示和一个共享提示来进行领域一般化，并且引入了领域提示生成器促进知识共享。 |
| [^55] | [RARE: Robust Masked Graph Autoencoder.](http://arxiv.org/abs/2304.01507) | RARE是一种鲁棒性抗干扰的掩码图自编码器，通过在高阶潜在特征空间中进行掩码和重构节点样本来提高推断掩码数据的确定性和自监督机制的可靠性，并在下游任务中优于现有的SGP方法。 |
| [^56] | [OneShotSTL: One-Shot Seasonal-Trend Decomposition For Online Time Series Anomaly Detection And Forecasting.](http://arxiv.org/abs/2304.01506) | OneShotSTL提出了一种高效准确的算法，用于在线时间序列分解，在处理时间上仅需O(1)的更新时间复杂度，并可同时保持较高的精度，解决了现有批处理方法无法支持实时分析的挑战。 |
| [^57] | [SLPerf: a Unified Framework for Benchmarking Split Learning.](http://arxiv.org/abs/2304.01502) | SLPerf是一个统一的研究和开放式研究库，用于共享学习，通过对不同情况下不同共享学习范式的基准比较，提供了改进共享学习范式的见解。 |
| [^58] | [Physics-aware Roughness Optimization for Diffractive Optical Neural Networks.](http://arxiv.org/abs/2304.01500) | 本研究提出了一种物理感知的衍射光学神经网络训练框架，在训练过程中引入粗糙度建模正则化和物理感知的稀疏化方法，以提高DONN的预测精度并提供实际部署的可行解决方案。 |
| [^59] | [Multi model LSTM architecture for Track Association based on Automatic Identification System Data.](http://arxiv.org/abs/2304.01491) | 本文提出了基于LSTM的多模型框架来进行轨迹关联，能够更好的捕捉船只轨迹的不同模式和特征，提高了关联准确性和鲁棒性。 |
| [^60] | [To ChatGPT, or not to ChatGPT: That is the question!.](http://arxiv.org/abs/2304.01487) | 研究评估了聊天GPT检测中的最新技术和其他AI生成文本检测工具的表现，并提出区分人工生成和AI生成文本的重要性。 |
| [^61] | [Blockwise Compression of Transformer-based Models without Retraining.](http://arxiv.org/abs/2304.01483) | 本论文提出了一种名为BCT的分块压缩框架，可以对整个Transformer模型进行更细粒度的压缩，实现了降低部署门槛的目的。 |
| [^62] | [Time-space-frequency feature Fusion for 3-channel motor imagery classification.](http://arxiv.org/abs/2304.01461) | 本文提出了一种新网络架构TSFF-Net，将时间-空间-频率特征融合，解决了单模特征提取网络在时间序列或时间-频率模态下的限制。TSFF-Net包括时间-频率表示、时间-频率特征提取、时间-空间特征提取和特征融合与分类四个主要组件。 |
| [^63] | [Exploring Vision-Language Models for Imbalanced Learning.](http://arxiv.org/abs/2304.01457) | 本文探索了如何通过向视觉-语言模型添加轻量级解码器和利用不平衡算法来改进性能，实验表明改进后的VLM在iNaturalist18、CIFAR-100和Visual Genome数据集上分类准确度显著提高，特别是对于少数类，性能提升很大。 |
| [^64] | [Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.01447) | 本文提出了一个名为OffPA2的新框架，通过离线策略行动预测方法来提高多智能体强化学习中的学习预测效率。 |
| [^65] | [A Deep Multi-Modal Cyber-Attack Detection in Industrial Control Systems.](http://arxiv.org/abs/2304.01440) | 本研究提出了一种利用工业控制系统网络和传感器数据，采用深度多模态网络攻击检测模型提高网络攻击检测效果的方法，实验结果表明，该方法能够超越现有的单模态模型和最近的文献作品，具有检测网络攻击的较高精度、召回率和f-measure。 |
| [^66] | [Optimizing Irrigation Efficiency using Deep Reinforcement Learning in the Field.](http://arxiv.org/abs/2304.01435) | 本文提出了一种利用深度强化学习优化灌溉效率的方法，名为DRLIC，它使用一个神经网络来学习最佳控制策略，并考虑到当前的土壤水分测量和未来的土壤水分损失。通过引入一个灌溉奖励函数，DRLIC可以从以往的经验中学习。实验表明，DRLIC和其简化版本在水分利用效率和作物产量方面均优于当前的灌溉方法。 |
| [^67] | [VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution.](http://arxiv.org/abs/2304.01434) | 本文提出了通过规范化表示的von Neumann熵( VNE ) 来改善深度表示的方法，通过操纵特征值分布来优化表示品质，广泛适用于不同的算法，可以增强其领域通用性、元学习、自监督学习和生成模型等方面。 |
| [^68] | [TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings.](http://arxiv.org/abs/2304.01433) | TPU v4是一款支持嵌入式硬件的可重构光学超级计算机，采用光学电路交换机重新配置互连拓扑，提高规模、可用性、利用率、模块化、部署、安全、功率和性能，它通过SparseCores加速嵌入式模型，性能优越，功耗低。 |
| [^69] | [Reducing Discretization Error in the Frank-Wolfe Method.](http://arxiv.org/abs/2304.01432) | 本论文提出了两个改进方法：一个多步的Frank-Wolfe方法，直接应用优化的高阶离散化方案；以及一种具有较少离散化误差的LMO-平均方案，其收敛速率加速到$O(1/k^{3/2})$，从而更好地解决了Frank-Wolfe方法中的离散化误差问题。 |
| [^70] | [Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots.](http://arxiv.org/abs/2304.01430) | 该论文提出了一种新的无监督多对象发现方法，通过一种上下文分隔的槽结构来将视觉场分割为独立运动区域，并用对抗性标准来保证解码器无法重构整个光流。 |
| [^71] | [Learning from data with structured missingness.](http://arxiv.org/abs/2304.01429) | 带有结构性缺失的数据学习是一个尚未被系统解决的问题，它对规模化的机器学习构成了重要阻碍，并且需要进一步的研究和解决。 |
| [^72] | [Learned Tree Search for Long-Horizon Social Robot Navigation in Shared Airspace.](http://arxiv.org/abs/2304.01428) | 本文提出了一种基于学习的树搜索算法SoRTS，用于在共享空域中实现社交机器人长期导航，并通过FAA认证飞行员的评估证明了其表现与一名熟练的人类飞行员相当，明显优于基准算法。 |
| [^73] | [Conformalized Unconditional Quantile Regression.](http://arxiv.org/abs/2304.01426) | 本文提出了一种新的预测推断过程，将符合预测与条件无限分位数回归相结合，能够适应异方差性并提供透明的覆盖保证，具有竞争力的性能。 |
| [^74] | [Learning with augmented target information: An alternative theory of Feedback Alignment.](http://arxiv.org/abs/2304.01406) | 本文提出了一种新的、架构不可知的反馈对齐工作理论，通过将目标信息嵌入到神经网络中学习有效的表征，而不是像BP一样用同样的参数来近似梯度。并基于这一理论设计了三种实现FA的方法。 |
| [^75] | [Adaptive Defective Area Identification in Material Surface Using Active Transfer Learning-based Level Set Estimation.](http://arxiv.org/abs/2304.01404) | 本文提出了一种自适应映射方法来更高效地识别材料表面的缺陷区域，解决了以往逐点测量导致耗时大的问题，同时引入了主动学习和迁移学习方法以降低测量次数和利用先前生产材料信息。 |
| [^76] | [Learning Personalized Models with Clustered System Identification.](http://arxiv.org/abs/2304.01395) | 该文提出了一种基于聚类的系统识别学习个性化模型的算法，将多个系统划分为群集，同一簇中的系统可以从其他系统的观察中获益。该算法实现了正确估计群集标识并具有高效和个性化的系统识别过程。 |
| [^77] | [Counterfactual Learning on Graphs: A Survey.](http://arxiv.org/abs/2304.01391) | 本文综述了图上反事实学习的研究进展，包括反事实公平性、可解释性、链路预测等不同应用问题，并提出了未来的研究方向。 |
| [^78] | [Faulty Branch Identification in Passive Optical Networks using Machine Learning.](http://arxiv.org/abs/2304.01376) | 该论文介绍了针对PON系统中可能出现的故障分支，使用机器学习进行错误分支识别的方法。同时探讨了在多个长度相似的分支产生反射重叠的情况下如何进行隔离。 |
| [^79] | [Adaptive SpikeDeep-Classifier: Self-organizing and self-supervised machine learning algorithm for online spike sorting.](http://arxiv.org/abs/2304.01355) | Ada-SpikeDeep-Classifier是一种用于实时脑机接口信号处理的自适应自组织算法，它使用了SpikeDeeptector进行信道选择、Ada-BAR进行信号预处理、OCM进行分类，旨在提高脑-计算机接口的解码效果，并在实验中表现出高精度和强健性。 |
| [^80] | [Accelerated parallel MRI using memory efficient and robust monotone operator learning (MOL).](http://arxiv.org/abs/2304.01351) | 本论文使用单调算子学习（MOL）框架，结合单调卷积神经网络（CNN）和共轭梯度算法实现加速并行MRI，在内存效率和性能保证方面具有优势。 |
| [^81] | [Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis.](http://arxiv.org/abs/2304.01347) | 本文提出了一种基于动态功能连接的脑网络分析模型，通过构建动态同步特征和革命性的图卷积方法实现精神分裂症诊断和侧化分析，并在实验证明其表现优于其他最先进模型。 |
| [^82] | [Towards an Hybrid Hodgkin-Huxley Action Potential Generation Model.](http://arxiv.org/abs/2304.01346) | 本文提出了一种使用简单测量和简化模型构建混合型霍奇金-赫胥黎模型的方法，可以在显著减少测量量的情况下准确地表示神经元动作电位。 |
| [^83] | [Charting the Topography of the Neural Network Landscape with Thermal-Like Noise.](http://arxiv.org/abs/2304.01335) | 本文采用统计力学方法研究全连接神经网络的优化问题，发现在低损失区域存在一个低维流形，并由靠近分类决策边界的数据点数量决定维度。 |
| [^84] | [On the Prime Number Divisibility by Deep Learning.](http://arxiv.org/abs/2304.01333) | 本文提出了使用深度学习判断质数整除性的方法，并发现关键在于提供给深度学习模型的特征空间。此外，商业可用的自动化机器学习管道无法解决此问题，需要提供适当的特征工程来解决。研究者还提出了一个封闭式解决方案。 |
| [^85] | [Learning the Delay Using Neural Delay Differential Equations.](http://arxiv.org/abs/2304.01329) | 本文提出了一种基于时滞微分方程的连续时间神经网络方法，使用伴随灵敏度方法直接学习模型参数和时滞，具有学习DDE参数的能力。 |
| [^86] | [Empirical Design in Reinforcement Learning.](http://arxiv.org/abs/2304.01315) | 本文是一个关于如何进行良好实验的资源，旨在解决强化学习中实证设计的挑战，并弥补实证研究中可能导致的弱的统计证据。 |
| [^87] | [Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice.](http://arxiv.org/abs/2304.01311) | 本研究通过访谈19位知识图谱（KG）实践者，发现KG构建者需求架构执行程序，KG分析师需要可自定义查询构建器，KG消费者需要领域特定可视化，并指出在实践中实施KG需要技术和社交方面的解决方案。 |
| [^88] | [Improved Bound for Mixing Time of Parallel Tempering.](http://arxiv.org/abs/2304.01303) | 本研究提出了一种新的并行退火的下界，对除$\log L$之外的所有参数具有多项式依赖性，其改进了现有界限。因此，该算法的混合时间可能更优。 |
| [^89] | [Kernel Affine Hull Machines for Differentially Private Learning.](http://arxiv.org/abs/2304.01300) | 本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。 |
| [^90] | [Dynamic Accommodation Measurement using Purkinje Images and ML Algorithms.](http://arxiv.org/abs/2304.01296) | 本研究开发出一种基于普尔金氏图像和机器学习算法的原型设备，可用于动态注视和调节测量，预测调节可以精确到0.25D，正在使用机器学习生成大量合成数据集。 |
| [^91] | [Unified Emulation-Simulation Training Environment for Autonomous Cyber Agents.](http://arxiv.org/abs/2304.01244) | 本文提出了一种自动生成高保真度的模拟器解决方案，在智能学习的Cyber Gym for Intelligent Learning（CyGIL）中提供高度真实的网络Cyber Operations（CyOp）训练环境，并通过集成模拟器生成和代理训练过程来降低代理训练时间。 |
| [^92] | [CoReFusion: Contrastive Regularized Fusion for Guided Thermal Super-Resolution.](http://arxiv.org/abs/2304.01243) | 本文提出了一种新颖的数据融合框架和正则化技术，用于热成像的引导超分辨率，具有计算效率，轻量级和鲁棒性，实现了在缺失数据的情况下性能不受影响，并且在基准数据集上表现出较高的性能。 |
| [^93] | [Detection of Homophobia & Transphobia in Dravidian Languages: Exploring Deep Learning Methods.](http://arxiv.org/abs/2304.01241) | 本研究旨在探讨不同深度学习模型在德拉维达语社交媒体评论分类中的适用性，以便将其识别为恐同、跨性别歧视和非反LGBT+内容。 |
| [^94] | [Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach.](http://arxiv.org/abs/2304.01240) | 该研究使用机器学习技术，成功地识别出心理健康电子健康记录中的疼痛提及，提高了对疼痛和心理健康之间关系的理解。 |
| [^95] | [Online Distillation with Continual Learning for Cyclic Domain Shifts.](http://arxiv.org/abs/2304.01239) | 本文提出了一种在线知识蒸馏与持续学习相结合的方法，旨在解决领域变化引起的灾难性遗忘问题。实验结果表明这种方法有效地提高了在线知识蒸馏的鲁棒性和准确性，具有潜在的应用价值。 |
| [^96] | [A Guide for Practical Use of ADMG Causal Data Augmentation.](http://arxiv.org/abs/2304.01237) | ADMG因果数据增强方法能够在已知因果关系的情况下提高模型性能，但如果不合适地使用，它也可能对模型产生负面影响。 |
| [^97] | [Astronomical image time series classification using CONVolutional attENTION (ConvEntion).](http://arxiv.org/abs/2304.01236) | 本研究提出了一种基于深度学习的新方法ConvEntion，用于对天体图像时间序列分类，并能够直接使用图像对不同类型的空间对象进行分类。 |
| [^98] | [Fair Evaluation of Graph Markov Neural Networks.](http://arxiv.org/abs/2304.01235) | 本论文通过引入适用于GMNN的新测试方法，对三类不同信息源对GMNN在WikiVitals数据集中的预测准确性的贡献进行严格评估，结果表明标签相关性是帮助GMNN获得优势的关键信息源。 |
| [^99] | [Prediction of solar wind speed by applying convolutional neural network to potential field source surface (PFSS) magnetograms.](http://arxiv.org/abs/2304.01234) | 本研究通过卷积神经网络和潜在场源表面磁图构建了一个模型，能够预测太阳风速度，并且在连续测试数据集上的平均相关系数为0.52。 |
| [^100] | [Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department.](http://arxiv.org/abs/2304.01233) | 本文提出了一种基于多模态知觉语言模型的方法，用于结果预测和患者分诊。实验结果表明，该模型在急诊科临床决策方面具有显著的潜力。 |
| [^101] | [SEENN: Towards Temporal Spiking Early-Exit Neural Networks.](http://arxiv.org/abs/2304.01230) | 本研究提出了一种名为SEENN的方法，通过对时间步数进行细粒度调整，以减少不必要的计算并提高有效性。同时，SEENN达到了多个基准数据集的最先进准确度表现。 |
| [^102] | [Resolution-Invariant Image Classification based on Fourier Neural Operators.](http://arxiv.org/abs/2304.01227) | 本文研究了基于傅里叶神经算子的图像分类算法，提出了一种离散不变的神经网络用于无限维函数空间中的算子近似，并将CNNs转换为FNOs。 |
| [^103] | [Abnormal Event Detection via Hypergraph Contrastive Learning.](http://arxiv.org/abs/2304.01226) | 本论文提出了一种基于超图对比学习的异常事件检测方法，可以完全捕捉异常事件模式，实验表明该方法在两个公共数据集上明显优于现有方法。 |
| [^104] | [Optimizing Data Shapley Interaction Calculation from O(2^n) to O(t n^2) for KNN models.](http://arxiv.org/abs/2304.01224) | 本文提出了一种名为 "STI-KNN" 的算法，可以在短时间内对 KNN 模型进行精确的配对交互 Shapley 值计算，从而有效地评估每个训练数据点的价值，提高训练结果和人工智能应用的有效性。 |
| [^105] | [Multi-Microgrid Collaborative Optimization Scheduling Using an Improved Multi-Agent Soft Actor-Critic Algorithm.](http://arxiv.org/abs/2304.01223) | 本文提出了一种基于多智能体集中式训练分布式执行框架的多微电网协同优化调度模型，并使用改进的MASAC算法对能源管理问题进行处理，成功实现了不同实体之间的功率互补和降低系统运行成本。 |
| [^106] | [NeuroDAVIS: A neural network model for data visualization.](http://arxiv.org/abs/2304.01222) | NeuroDAVIS是一种无监督深度神经网络模型，它可以在不影响数据局部和全局结构的情况下提取重要特征并在更低的维度上进行数据可视化。 |
| [^107] | [DoE2Vec: Deep-learning Based Features for Exploratory Landscape Analysis.](http://arxiv.org/abs/2304.01219) | DoE2Vec 是一种基于深度学习的方法，用于学习任何实验设计（DoE）的信息潜在表达，并可以满足优化景观特征的下游元学习任务，同时避免了经典ELA分析中的特征工程问题。在分类任务中与经典ELA特征互补使用时，可显着提高性能。 |
| [^108] | [POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems.](http://arxiv.org/abs/2304.01218) | POLAR-Express 是一种高效且准确的形式可达性分析工具，用于验证神经网络控制系统的安全性。它使用 Taylor 模型算术和逐层传播技术，可以分析具有连续激活功能的前馈神经网络，并在 ReLU 激活函数上提供了一种更有效的精确传播 TM 的新方法。 |
| [^109] | [A Machine Learning Approach to Forecasting Honey Production with Tree-Based Methods.](http://arxiv.org/abs/2304.01215) | 该论文使用树模型方法预测意大利蜂箱的蜜蜂生产量变化，帮助蜜蜂养殖者评估风险，保护蜜蜂活动。 |
| [^110] | [PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction.](http://arxiv.org/abs/2304.01209) | 提出了“基于提示的开放关系抽取”模型，在无监督设置下不需要超参数调整，实现了全新的无监督关系抽取方法。 |
| [^111] | [Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models.](http://arxiv.org/abs/2304.01046) | 本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。 |
| [^112] | [Optimal Mass Transport over the Euler Equation.](http://arxiv.org/abs/2304.00595) | 本文分析了欧拉方程的控制问题，发现其是一个最优质量输运问题的实例，并提供了该问题的静态和动态版本的Eulerian OMT解决方案。 |
| [^113] | [From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding.](http://arxiv.org/abs/2304.00553) | 本文提出了一个统一的语义空间Poincare行为语义空间，通过将以前数据集的类别与这个语义空间对齐，收集（图像/视频/骨架/MoCap）数据集到一个统一的数据库中，即将“孤立的岛屿”桥接成一个“泛大陆”，这将有助于推进可推广的行为学习。 |
| [^114] | [Geometric constraints improve inference of sparsely observed stochastic dynamics.](http://arxiv.org/abs/2304.00423) | 本文提出一种新的方法，该方法利用数据驱动的控制，既考虑了系统不变密度的几何形状，又能对系统进行有效识别。 |
| [^115] | [Multilevel CNNs for Parametric PDEs.](http://arxiv.org/abs/2304.00388) | 该论文提出了一种用于有效数值解决参数化PDEs的多级CNN方法，有实质性的改进并能以任意精度近似多重网格V循环。 |
| [^116] | [Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning.](http://arxiv.org/abs/2304.00252) | 本文提出了恢复触发状态(RTS)方法，用于保护RL代理免受反向攻击。该方法涉及构建替代网络来近似动态模型，并将触发状态恢复为干净状态来防止攻击者通过触发器激活隐藏在代理中的后门。 |
| [^117] | [oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes.](http://arxiv.org/abs/2303.17612) | oBERTa是一组易于使用的语言模型，通过改进初始化、蒸馏、剪枝等技术，可以在不需要模型压缩方面的专业知识的情况下提高稀疏迁移学习的效率和准确性。 |
| [^118] | [Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts.](http://arxiv.org/abs/2303.17595) | 本研究指出传统的图片分类器学习过程忽视注释过程中的辅助信息，提出了使用注释副产品来训练模型的新方法，该方法可以减少虚假相关性并提高模型精度。 |
| [^119] | [The Graphical Nadaraya-Watson Estimator on Latent Position Models.](http://arxiv.org/abs/2303.17229) | 研究了潜在位置模型上的图形Nadaraya-Watson估计器的性质，对于更复杂的方法有理论指导意义。 |
| [^120] | [Module-based regularization improves Gaussian graphical models when observing noisy data.](http://arxiv.org/abs/2303.16796) | 建议将推断网络的模块化结构整合到正则化强度的交叉验证中，以改善高斯图模型在观察含噪数据时的表现。 |
| [^121] | [List Online Classification.](http://arxiv.org/abs/2303.15383) | 本文研究了多标签列表的在线预测问题，提出了 $b$-ary Littlestone 维度可学习模型，并且在懵懂的情况下探索不同的情况。可以使用改编自 Littlestone 的 SOA 和 Rosenblatt 的感知器等算法进行预测，同时还建立了列表可学习的组合结果。 |
| [^122] | [Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System.](http://arxiv.org/abs/2303.14524) | 本文介绍了一种创新的推荐系统模式-Chat-Rec，通过将LLMs与对话式推荐相结合，解决了传统推荐系统中互动性和可解释性不足的问题。因此，Chat-Rec能够更有效地学习用户偏好，并在推荐过程中建立用户-产品之间的联系，具有更大的透明度和控制。 |
| [^123] | [Data-Driven Control with Inherent Lyapunov Stability.](http://arxiv.org/abs/2303.03157) | 该论文介绍了一种名为CoILS的数据驱动控制方法，可以联合学习非线性动力学模型和稳定控制器，同时学习参数化李亚普诺夫函数，从而使动力学模型本身具有稳定性，可以由学习的控制器实现。 |
| [^124] | [3D Generative Model Latent Disentanglement via Local Eigenprojection.](http://arxiv.org/abs/2302.12798) | 本文提出了一种基于谱几何的全新损失函数，应用于不同的3D头部和身体网格生成模型，通过激励潜在变量遵循特征向量投影并改善潜在空间解耦，实现对生成本地形状属性的控制。 |
| [^125] | [Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests.](http://arxiv.org/abs/2302.12260) | 本文介绍了物理信息神经网络（PINNs）求解微分方程的方法，该方法利用物理学知识，通过损失函数的专用项来实现微分方程的求解。通过使用较少的数据，可以预测解，适用于非线性弱的问题。 |
| [^126] | [Causal Razors.](http://arxiv.org/abs/2302.10331) | 本文比较了许多出现在文献中的因果剃刀，并特别研究了在多项式因果模型中不太受欢迎的因果剃刀——参数最小性。逻辑结果揭示了选择合理得分标准时的困境。 |
| [^127] | [Variational Mixture of HyperGenerators for Learning Distributions Over Functions.](http://arxiv.org/abs/2302.06223) | 本文提出了一种新的深度生成模型VAMoH，结合了INRs对连续函数进行建模的能力和VAEs的推断能力，以及归一化流和超网络混合方法。在不同类型的数据上进行实验证明，VAMoH可以有效地学习连续函数的分布，并可以执行与推断相关的任务。 |
| [^128] | [A Categorical Archive of ChatGPT Failures.](http://arxiv.org/abs/2302.03494) | 本研究对ChatGPT的11个失败类别进行了全面分析，其中包括推理、事实错误、数学、编码和偏见。找出失败原因以帮助研究人员和开发人员改进未来的语言模型和聊天机器人。 |
| [^129] | [Predicting the Silent Majority on Graphs: Knowledge Transferable Graph Neural Network.](http://arxiv.org/abs/2302.00873) | 本文提出了一种新颖的知识可迁移模块和适应性学习机制，通过将有声节点的知识转移给无声节点，实现了针对含有“沉默大多数”的图表的抗差建模。实验证明，该方法在各种真实世界数据集上都具有有效性和优越性。 |
| [^130] | [Analyzing Leakage of Personally Identifiable Information in Language Models.](http://arxiv.org/abs/2302.00539) | 本研究针对语言模型中泄漏个人身份信息的风险进行了严格的定义，并通过黑盒提取、推断和重建攻击进行了实证评估。 |
| [^131] | [DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion.](http://arxiv.org/abs/2301.09474) | DIFFormer是一种能量受限扩散模型，通过逐渐融合其他实例信息的演化状态，导出了一类新的神经编码器，称为DIFFormer（基于扩散的Transformer），能够揭示真实世界中复杂的数据生成过程。 |
| [^132] | [Learning-Rate-Free Learning by D-Adaptation.](http://arxiv.org/abs/2301.07733) | D-Adaptation是一种可以自动设置学习率的方法，针对最小化凸性Lipschitz函数，用于实现最优收敛速率，而无需超参数，也无需额外对数因子改进，能够在各种机器学习问题中自动匹配手动调整的学习率。 |
| [^133] | [Async-HFL: Efficient and Robust Asynchronous Federated Learning in Hierarchical IoT Networks.](http://arxiv.org/abs/2301.06646) | 提出了异步HFL框架，以解决复杂、层次化的IoT网络中的联合学习挑战。该算法采用了异步聚合来避免长时间等待，并在网关和云级别上采用设备选择和设备网关调度来提高收敛速度、鲁棒性和可扩展性。 |
| [^134] | [A Meta Path-based Approach for Rumor Detection on Social Media.](http://arxiv.org/abs/2301.04341) | 本文提出了一种基于元路径的全局局部注意网络（MGLAN）的模型，通过提取谣言传播的结构特征，有效解决谣言检测问题。 |
| [^135] | [Hierarchical Explanations for Video Action Recognition.](http://arxiv.org/abs/2301.00436) | 本文提出了分层原型解释器，能够解释深度神经网络对视频动作的分类，同时能够将类和原型建立成更有层次的关系，可以处理不确定性。 |
| [^136] | [Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models.](http://arxiv.org/abs/2212.12380) | 本论文研究了物理一致神经网络(PCNNs) 在模拟建筑温度动态方面的扩展性和准确性。结果发现，PCNNs既确保了物理一致性，同时又能在复杂的多区域热建筑模型中取得高精度的性能表现，且在可用数据量有限的情况下超越经典灰盒模型，具有可扩展性优势。 |
| [^137] | [Reusable Options through Gradient-based Meta Learning.](http://arxiv.org/abs/2212.11726) | 本文提出了一个基于梯度元学习的方法，以解决学习可重复使用的选项的问题。实验表明，该方法能够学习可转移的组件，加速学习，并表现优于现有的先前方法。 |
| [^138] | [A Probabilistic Framework for Lifelong Test-Time Adaptation.](http://arxiv.org/abs/2212.09713) | 本文提出了一种用于处理测试输入分布随时间持续变化的概率框架PETAL，通过提供可靠的不确定性估计和使用源模型作为正则化项来推断时正则化模型更新，实现了终身TTA。 |
| [^139] | [Bayesian posterior approximation with stochastic ensembles.](http://arxiv.org/abs/2212.08123) | 本文提出一种新方法，即使用随机神经网络集合来近似贝叶斯后验，并通过变分推断进行训练，实验证明该方法比其他流行的贝叶斯推断基线提供了更准确的后验估计。 |
| [^140] | [ALSO: Automotive Lidar Self-supervision by Occupancy estimation.](http://arxiv.org/abs/2212.05867) | 该论文提出了一种简单易操作的自我监督方法，通过重构表面和利用其中的语义信息来提高3D感知模型的准确性。 |
| [^141] | [Compiler Optimization for Quantum Computing Using Reinforcement Learning.](http://arxiv.org/abs/2212.04508) | 这篇论文提出了一种基于强化学习算法的量子编译器优化框架，旨在提高量子编译的效率和品质，克服了传统启发式方法的缺点，并可适应各种约束条件下的编译要求。 |
| [^142] | [Collective Intelligence for 2D Push Manipulation with Mobile Robots.](http://arxiv.org/abs/2211.15136) | 本研究利用基于软体物理模拟器的规划器和基于注意力的神经网络，实现了移动机器人2D协作推动操作中的集体智能，比传统方法具有更好的性能并具备环境自适应能力。 |
| [^143] | [FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee.](http://arxiv.org/abs/2211.15072) | 本研究提出了FaiREE算法，它是一种可满足群体公平性约束的公平分类算法，并且具有有限样本和无分布理论保证。在实验中表现优异。 |
| [^144] | [GAMMT: Generative Ambiguity Modeling Using Multiple Transformers.](http://arxiv.org/abs/2211.09812) | GAMMT是一种生成不确定性模型，使用多个Transformer处理模糊不确定的概率。该模型有望实现高质量和多样性的序列建模。 |
| [^145] | [PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention Networks with Imbalanced PU Labels.](http://arxiv.org/abs/2211.08604) | 本文提出了一种基于图注意力网络和PU损失函数的欺诈检测方法PU GNN，通过改进的GraphSMOTE算法来处理P2E MMORPGs欺诈检测数据集中的标签分布不平衡问题，实验证明该方法在欺诈检测方面具有良好的性能表现。 |
| [^146] | [From Cubes to Networks: Fast Generic Model for Synthetic Networks Generation.](http://arxiv.org/abs/2211.02811) | 本论文提出了一种快速通用的模型，将立方体转化为相应的网络，生成的网络更加接近实际网络。 |
| [^147] | [Coresets for Wasserstein Distributionally Robust Optimization Problems.](http://arxiv.org/abs/2210.04260) | 本文提出了一种构建一般Wasserstein分布鲁棒优化问题核心集的统一框架。 |
| [^148] | [Federated Learning with Server Learning: Enhancing Performance for Non-IID Data.](http://arxiv.org/abs/2210.02614) | 基于辅助学习的联邦学习可以显著提高在非独立同分布数据上的模型精度和收敛时间 |
| [^149] | [Obstacle Identification and Ellipsoidal Decomposition for Fast Motion Planning in Unknown Dynamic Environments.](http://arxiv.org/abs/2209.14233) | 本文提出一种基于椭球的障碍物识别与测速方法，并定义了基于椭球的特征向量，能够适用于带有静态和动态障碍物的环境，其运行速度比现有算法更快且不需要预先知道聚类数量。 |
| [^150] | [Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction.](http://arxiv.org/abs/2209.05135) | 本文提出了一种从视频示例学习手语拼写在机器人中的实现的方法。通过训练模仿运动的策略，利用预训练的深度视觉模型从RGB视频中提取手的三维姿态，并识别出最佳的模仿超参数集，本文成功展示了方法的普适性。 |
| [^151] | [Convergence Rates of Training Deep Neural Networks via Alternating Minimization Methods.](http://arxiv.org/abs/2208.14318) | 本文提出了一个统一的框架用于分析AM类型的网络训练方法的收敛速率。研究基于非单调的$j$-步充分减少条件和Kurdyka-Lojasiewicz（KL）性质，并在KL指数$ \theta $在$ [0,1) $变化时展示了详细的局部收敛速率和局部R-线性收敛。 |
| [^152] | [A methodology for identifying resiliency in renewable electrical distribution system using complex network.](http://arxiv.org/abs/2208.11543) | 本文提出一种使用复杂网络理论来识别可再生电力分布系统弹性的方法，可以识别系统中太阳能电池板的托管能力，从而有助于提高系统的韧性。 |
| [^153] | [Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost.](http://arxiv.org/abs/2208.10842) | 本论文提出了一种名为抽奖池（Lottery Pools）的方法，它可以通过直接平均相邻学习得到的子网络的权重或者通过简单的插值策略对迭代剪枝确定的子网络执行“集成”，从而提高抽奖票（LTs）的性能。 |
| [^154] | [MENLI: Robust Evaluation Metrics from Natural Language Inference.](http://arxiv.org/abs/2208.07316) | 本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。 |
| [^155] | [Memorization-Dilation: Modeling Neural Collapse Under Label Noise.](http://arxiv.org/abs/2206.05530) | 本文提出了一个更现实的无限制特征表示变体，它考虑了网络的有限表达性。结合记忆和 dropout 的新模型 MD-Dropout 有效防止了膨胀神经崩溃，并提高了鲁棒性。 |
| [^156] | [A Closer Look at Rehearsal-Free Continual Learning.](http://arxiv.org/abs/2203.17269) | 本文介绍了一种新的渐进式学习方法，使用知识蒸馏和参数正则化以避免重复训练，并在不会退化已学数据的情况下实现了强大的性能。 |
| [^157] | [Importance Sampling CAMs for Weakly-Supervised Segmentation.](http://arxiv.org/abs/2203.12459) | 本文提出了基于重要性采样和特征相似性损失项的CAM改进方法，显著提高了弱监督分割的轮廓精度性能。 |
| [^158] | [A Framework and Benchmark for Deep Batch Active Learning for Regression.](http://arxiv.org/abs/2203.09410) | 本研究提出了一个深度批量主动学习回归的框架和基准测试，其中包括许多现有的贝叶斯和非贝叶斯方法。提出了一种替换常用最后一层特征的新方法，并结合一种新颖的聚类方法。在15个大型表格回归数据集上进行测试，该方法在基准测试中表现优异，适用于大型数据集且易于使用。 |
| [^159] | [Reachability In Simple Neural Networks.](http://arxiv.org/abs/2203.07941) | 本研究研究了简单神经网络中的可达性问题，并证明了对于仅具有一个隐含层和一个输出维度以及仅具有一个负、零和一个正权重或偏置的神经网络来说，它是NP难度问题。 |
| [^160] | [Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding.](http://arxiv.org/abs/2203.05711) | 这个研究收集并发布了一个视频-语言故事数据集SYMON，用于推进多模态故事理解的发展。 |
| [^161] | [Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication.](http://arxiv.org/abs/2202.00396) | 本文利用大规模多语言 Twitter 数据，分析了希腊、西班牙和联合王国议会成员的推文，并发现消极情绪更易传播。 |
| [^162] | [Central-Smoothing Hypergraph Neural Networks for Predicting Drug-Drug Interactions.](http://arxiv.org/abs/2112.07837) | 本文研究了预测药物相互作用问题，提出了一种新的超图神经网络 CentSmoothie，能够充分考虑标签之间的相关性，取得了最先进的性能表现。 |
| [^163] | [Iterated Block Particle Filter for High-dimensional Parameter Learning: Beating the Curse of Dimensionality.](http://arxiv.org/abs/2110.10745) | 本文提出了一种迭代分块粒子滤波算法，用于学习高维参数。该算法克服了维度灾难，表现出良好的收敛性和似然最大化，成功地在非线性和非高斯空间时间模型上实现了参数学习。 |
| [^164] | [Adaptive joint distribution learning.](http://arxiv.org/abs/2110.04829) | 该论文提出了一种自适应联合分布学习的框架，可以从大量数据点中估计低维、归一化和正的Radon-Nikodym导数模型，并在不同学习问题上取得了良好的结果。 |
| [^165] | [Communication-Efficient Federated Linear and Deep Generalized Canonical Correlation Analysis.](http://arxiv.org/abs/2109.12400) | 本文提出了一种通信高效的联邦学习框架，用于线性和深度广义典型相关分析，通过压缩本地数据统计信息和使用子空间共识算法减少通信成本，同时具有与中心化算法相当的性能。 |
| [^166] | [Universal set of Observables for Forecasting Physical Systems through Causal Embedding.](http://arxiv.org/abs/2105.10759) | 本文实现了一种基于因果嵌入的预测方案，可以唯一地表示基础动力系统的整个左无限轨道或来自这样的轨道的观测，该方案具有通用性、可计算性和误差容错性，可以在长期保证预测一致性。 |
| [^167] | [Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors.](http://arxiv.org/abs/2103.15949) | 本文提出使用字典学习将上下文嵌入作为Transformer因子的线性叠加来打开Transformer“黑匣子”，通过可视化展示其捕获的层次化语义结构，为更好地理解Transformer网络的工作方式带来新的见解。 |
| [^168] | [MARS: Masked Automatic Ranks Selection in Tensor Decompositions.](http://arxiv.org/abs/2006.10859) | 本文介绍了一种名为MARS的新型高效方法，在一般的张量分解中自动选择秩，学习二值掩码来选择最佳的张量结构，在实验中显示出更好的结果。 |

# 详细

[^1]: EGC: 一种通过单一能量模型生成与分类图像的方法

    EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])

    [http://arxiv.org/abs/2304.02012](http://arxiv.org/abs/2304.02012)

    EGC是一种使用单个神经网络在图像分类和图像生成任务中实现卓越性能的方法，可以较好地生成出高质量图像，并在多项数据集上实现了领先的分类结果。

    

    使用相同的网络参数学习图像分类和生成图像是一个具有挑战性的问题。最近的先进方法在一项任务上表现良好，但在另一项任务上却表现不佳。本文引入了一种名为EGC的基于能量的分类器和生成器，它可以使用单个神经网络在两个任务中实现卓越性能。与传统的分类器输出给定图像的标签（即条件分布$p(y|\mathbf{x})$）不同，EGC的前向传递器是一个分类器，它输出一个联合分布$p(\mathbf{x},y)$，在后向传递器中通过边缘化标签$y$实现生成器。在前向传递中，估计给定噪声图像的能量和分类概率，而在后向传递中，通过估计得分函数对其进行去噪。EGC在ImageNet-1k、CelebA-HQ和LSUN Church上实现了与最先进方法相当的生成结果，同时在CIFAR-10、CIFAR-100和ImageNet-1k上实现了最先进的分类结果。

    Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an image generator in its backward pass by marginalizing out the label $y$. This is done by estimating the energy and classification probability given a noisy image in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achi
    
[^2]: FakET: 利用神经风格迁移模拟冷冻电子断层图像

    FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer. (arXiv:2304.02011v1 [cs.LG])

    [http://arxiv.org/abs/2304.02011](http://arxiv.org/abs/2304.02011)

    本文提出了一种使用加性噪声和神经风格迁移技术来模拟电子显微镜正向算子，以解决深度学习方法需要大量训练数据集的问题。该方法在粒子定位和分类任务上表现良好。

    

    粒子定位和分类是计算显微学中最基本的问题之一。近年来，深度学习方法在这些任务中取得了巨大成功。这些监督式学习方法的一个关键缺点是它们需要大量的训练数据集，通常是与模拟透射电子显微镜物理的复杂数值正向模型中的粒子模型结合生成的。这些模型的计算机实现非常耗费计算资源，限制了它们的适用范围。本文提出了一种基于加性噪声和神经风格迁移技术模拟电子显微镜正向算子的简单方法。我们使用目前最先进的已经建立的状态之一对定位和分类任务进行评估，显示出与基准测试相当的性能。与以前的方法不同，我们的方法加速了运算，显著减少了计算成本。

    Particle localization and -classification constitute two of the most fundamental problems in computational microscopy. In recent years, deep learning based approaches have been introduced for these tasks with great success. A key shortcoming of these supervised learning methods is their need for large training data sets, typically generated from particle models in conjunction with complex numerical forward models simulating the physics of transmission electron microscopes. Computer implementations of such forward models are computationally extremely demanding and limit the scope of their applicability. In this paper we propose a simple method for simulating the forward operator of an electron microscope based on additive noise and Neural Style Transfer techniques. We evaluate the method on localization and classification tasks using one of the established state-of-the-art architectures showing performance on par with the benchmark. In contrast to previous approaches, our method acceler
    
[^3]: 自回归神经张量网络: 桥接量子神经网络和张量网络进行多体量子模拟

    Autoregressive Neural TensorNet: Bridging Neural Networks and Tensor Networks for Quantum Many-Body Simulation. (arXiv:2304.01996v1 [quant-ph])

    [http://arxiv.org/abs/2304.01996](http://arxiv.org/abs/2304.01996)

    本研究提出了自回归神经张量网络（ANTN）来桥接张量网络和自回归神经网络，可以提高多体量子模拟的表达能力和精度，具有广泛的应用前景。

    

    多体量子物理的模拟对于理解基础科学及应用于量子材料设计和量子技术具有重要影响。然而，由于希尔伯特空间大小随粒子数呈指数级增长，直接模拟是不可行的。张量网络和神经网络是近似模拟的两种最先进方法，但在表达能力和优化方面各自有其局限性。为了应对这些挑战，我们开发了一种新型架构——自回归神经张量网络（ANTN），它桥接了张量网络和自回归神经网络。我们展示了ANTN用于参数化具有精确采样的归一化波函数，扩展了张量网络和自回归神经网络的表达力，继承了许多自回归神经网络的对称性。我们在二维 $J_1$-$J_2$ 模型上展示了我们的方法，并表明ANTN优于最先进的张量网络方法，并在现有自回归神经网络中具有竞争性能力。

    Quantum many-body physics simulation has important impacts on understanding fundamental science and has applications to quantum materials design and quantum technology. However, due to the exponentially growing size of the Hilbert space with respect to the particle number, a direct simulation is intractable. While representing quantum states with tensor networks and neural networks are the two state-of-the-art methods for approximate simulations, each has its own limitations in terms of expressivity and optimization. To address these challenges, we develop a novel architecture, Autoregressive Neural TensorNet (ANTN), which bridges tensor networks and autoregressive neural networks. We show that Autoregressive Neural TensorNet parameterizes normalized wavefunctions with exact sampling, generalizes the expressivity of tensor networks and autoregressive neural networks, and inherits a variety of symmetries from autoregressive neural networks. We demonstrate our approach on the 2D $J_1$-$J
    
[^4]: DWA：差分小波放大器用于图像超分辨率

    DWA: Differential Wavelet Amplifier for Image Super-Resolution. (arXiv:2304.01994v1 [cs.CV])

    [http://arxiv.org/abs/2304.01994](http://arxiv.org/abs/2304.01994)

    本文介绍了一种基于小波的图像超分辨率模块DWA，通过利用两个卷积滤波器的差异改进小波SR模型，在小波域中提高相关特征提取并抑制噪声。在现有的SR模型中集成DWA，如DWSR和MWCNN，可以显示出其有效性。

    

    本文介绍了一种差分小波放大器(DWA)，这是一种基于小波的图像超分辨率(SR)模块。DWA为最近收到较少关注的混合离散小波变换(DWT)方法注入活力。DWT能够有效地为SR提供图像表示，并将其输入的空间面积减少4倍，从而减小了模型总大小和计算成本，并且成为可持续ML的一种有吸引力的方法。我们提出的DWA模型通过利用两个卷积滤波器之间的差异来改进小波SR模型，在小波域中提高相关特征提取，强调局部对比度并抑制输入信号中的常见噪声。将其集成到现有的SR模型中，如DWSR和MWCNN，可以显示出其有效性，并在SR任务中实现了明显的提高。此外，DWA使DWSR和MWCNN可以直接应用于输入图像空间，因为它省略了DWT表示的通道方式。

    This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing the DWT representation channel-wise since it omits 
    
[^5]: ERM++：用于域通用性的改进基准方法

    ERM++: An Improved Baseline for Domain Generalization. (arXiv:2304.01973v1 [cs.LG])

    [http://arxiv.org/abs/2304.01973](http://arxiv.org/abs/2304.01973)

    ERM++是一个用于域通用性的改进基准方法，通过更好地利用训练数据、模型参数选择和权重空间正则化等关键技术，在多个数据集上比标准ERM更有效，同时计算复杂度更低，表现也优于最先进方法。

    

    多源域通用性（DG）衡量分类器对于它没有接受过训练的新数据分布的泛化能力，并考虑了多个训练域。虽然已经提出了几种多源DG方法，但是它们在训练过程中使用域标签增加了额外的复杂性。最近的研究表明，经过良好调整的经验风险最小化（ERM）训练过程，即在源域上简单地最小化经验风险，可以胜过大多数现有的DG方法。我们确定了几个关键候选技术，以进一步提高ERM的性能，例如更好地利用训练数据、模型参数选择和权重空间正则化。我们将结果称为ERM ++，并展示它相对于标准ERM在五个多源数据集上将DG的性能显着提高了5％以上，并且尽管计算复杂度更低，但击败了最先进的方法。此外，我们还证明了ERM ++在WILDS-FMOW数据集上的有效性。

    Multi-source Domain Generalization (DG) measures a classifier's ability to generalize to new distributions of data it was not trained on, given several training domains. While several multi-source DG methods have been proposed, they incur additional complexity during training by using domain labels. Recent work has shown that a well-tuned Empirical Risk Minimization (ERM) training procedure, that is simply minimizing the empirical risk on the source domains, can outperform most existing DG methods. We identify several key candidate techniques to further improve ERM performance, such as better utilization of training data, model parameter selection, and weight-space regularization. We call the resulting method ERM++, and show it significantly improves the performance of DG on five multi-source datasets by over 5% compared to standard ERM, and beats state-of-the-art despite being less computationally expensive. Additionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset,
    
[^6]: 模型修正的学习主对偶模型用于快速有限视角光声层析成像

    Model-corrected learned primal-dual models for fast limited-view photoacoustic tomography. (arXiv:2304.01963v1 [eess.IV])

    [http://arxiv.org/abs/2304.01963](http://arxiv.org/abs/2304.01963)

    本研究提出了一种模型修正的学习主对偶框架，可用于快速有限视角光声层析成像，通过在数据空间中共同学习模型修正与图像空间中的学习更新操作，有效解决了计算开销较大的前向模型的问题，并具有一定的理论指导意义和实际应用价值。

    

    学习迭代重建在经验鲁棒性和加速层析成像方面表现出伟大的潜力，然而，其在光声层析成像中的应用受到需要反复评估计算开销前向模型的影响。通过使用快速近似模型，可以获得计算可行性，但需要补偿模型误差。本研究通过将模型修正嵌入到学习主对偶框架中，推进了学习图像重建中模型修正的方法论和理论基础。在未经展开的端到端学习迭代重建方法中，模型修正与图像空间中的学习更新操作在数据空间中共同学习。所提出的公式允许扩展到主对偶深平衡模型，提供固定点收敛以及减少训练的内存需求。我们在快速有限视角光声层析成像上提供了理论和实证的性能和有效性的研究。

    Learned iterative reconstructions hold great promise to accelerate tomographic imaging with empirical robustness to model perturbations. Nevertheless, an adoption for photoacoustic tomography is hindered by the need to repeatedly evaluate the computational expensive forward model. Computational feasibility can be obtained by the use of fast approximate models, but a need to compensate model errors arises. In this work we advance the methodological and theoretical basis for model corrections in learned image reconstructions by embedding the model correction in a learned primal-dual framework. Here, the model correction is jointly learned in data space coupled with a learned updating operator in image space within an unrolled end-to-end learned iterative reconstruction approach. The proposed formulation allows an extension to a primal-dual deep equilibrium model providing fixed-point convergence as well as reduced memory requirements for training. We provide theoretical and empirical ins
    
[^7]: 随机对抗风格扰动技术用于域泛化

    Randomized Adversarial Style Perturbations for Domain Generalization. (arXiv:2304.01959v1 [cs.CV])

    [http://arxiv.org/abs/2304.01959](http://arxiv.org/abs/2304.01959)

    本文提出了一种随机对抗风格扰动技术，它能够通过对抗性扰动特征风格达到域泛化的效果，同时结合混合原始特征的方法缓解扰动带来的挑战。

    

    本文提出了一种新颖的域泛化技术，称为随机对抗风格扰动技术（RASP），其动机在于特征统计学捕捉到每个域的特征。该算法在对抗性方向上扰动一个特征的风格，朝着一个随机选择的类别方向并使模型学习避免被在未知目标域中观察到的意外风格所误导。虽然RASP能有效处理域漂移，但它的简单融合到训练过程中可能会降低从源域学习知识的能力，因为它并不限制表征的扰动。这个挑战由规一化特征Mixup（NFM）缓解，它通过训练过程中的特征混合来促进原始特征的学习，同时实现了对扰动表示的鲁棒性。我们通过广泛的实验评估了所提出的算法。

    We propose a novel domain generalization technique, referred to as Randomized Adversarial Style Perturbation (RASP), which is motivated by the observation that the characteristics of each domain are captured by the feature statistics corresponding to style. The proposed algorithm perturbs the style of a feature in an adversarial direction towards a randomly selected class, and makes the model learn against being misled by the unexpected styles observed in unseen target domains. While RASP is effective to handle domain shifts, its naive integration into the training procedure might degrade the capability of learning knowledge from source domains because it has no restriction on the perturbations of representations. This challenge is alleviated by Normalized Feature Mixup (NFM), which facilitates the learning of the original features while achieving robustness to perturbed representations via their mixup during training. We evaluate the proposed algorithm via extensive experiments on var
    
[^8]: TransPimLib：用于处理器内存系统上高效的超越函数的库

    TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems. (arXiv:2304.01951v1 [cs.MS])

    [http://arxiv.org/abs/2304.01951](http://arxiv.org/abs/2304.01951)

    TransPimLib提供了处理器内存系统上高效的超越函数计算方法，有助于提高PIM系统的计算能力和支持更广泛的工作负载，特别是机器学习应用中的激活函数。

    

    处理器内存系统（PIM）承诺减轻现代计算系统中的数据移动瓶颈。然而，现有的真实PIM系统有一个内在的劣势，即它们的硬件比传统的处理器（CPU、GPU）更加受限，因为在内存附近或内部构建处理元件的难度和成本很高。因此，通用的PIM架构支持相当有限的指令集，并且难以执行复杂的操作，例如超越函数和其他难以计算的操作（例如平方根）。这些操作对于一些现代工作负载尤其重要，例如机器学习应用中的激活函数。为了在通用的PIM系统中提供对超越（和其他难以计算）函数的支持，我们介绍了TransPimLib，这是一个库，提供基于CORDIC和LUT的三角函数、双曲函数、指数、对数、平方根等难以计算的函数的方法。

    Processing-in-memory (PIM) promises to alleviate the data movement bottleneck in modern computing systems. However, current real-world PIM systems have the inherent disadvantage that their hardware is more constrained than in conventional processors (CPU, GPU), due to the difficulty and cost of building processing elements near or inside the memory. As a result, general-purpose PIM architectures support fairly limited instruction sets and struggle to execute complex operations such as transcendental functions and other hard-to-calculate operations (e.g., square root). These operations are particularly important for some modern workloads, e.g., activation functions in machine learning applications.  In order to provide support for transcendental (and other hard-to-calculate) functions in general-purpose PIM systems, we present \emph{TransPimLib}, a library that provides CORDIC-based and LUT-based methods for trigonometric functions, hyperbolic functions, exponentiation, logarithm, squar
    
[^9]: 知识图谱中的高吞吐量向量相似度搜索

    High-Throughput Vector Similarity Search in Knowledge Graphs. (arXiv:2304.01926v1 [cs.DB])

    [http://arxiv.org/abs/2304.01926](http://arxiv.org/abs/2304.01926)

    本篇论文讨论了在知识图谱中进行向量相似性搜索的方法，主要关注于混合查询，既有向量相似度搜索，又有与底层数据向量相关的关系属性。

    

    随着越来越多地将数据编码为向量以提供在线推荐和搜索用例，机器学习的使用呈递增趋势。因此，最近的数据管理系统提供了在线向量相似度搜索的查询处理增强方法。在本研究中，我们探讨了在知识图谱（KGs）中进行向量相似性搜索的方法。在查询已有KG查询工作负载相关的KG查询和实体任务的驱动下，我们专注于混合向量相似度搜索（混合查询），其中查询的一部分对应向量相似度搜索，而查询的另一部分对应与底层数据向量相关的关系属性。例如，给定过去为歌曲实体的KG查询，我们希望构建新的查询，以寻找向量表示接近过去KG查询中的实体向量表示的新歌曲实体。但是，在KG中的实体也具有非向量属性，例如与艺术家相关的歌曲。

    There is an increasing adoption of machine learning for encoding data into vectors to serve online recommendation and search use cases. As a result, recent data management systems propose augmenting query processing with online vector similarity search. In this work, we explore vector similarity search in the context of Knowledge Graphs (KGs). Motivated by the tasks of finding related KG queries and entities for past KG query workloads, we focus on hybrid vector similarity search (hybrid queries for short) where part of the query corresponds to vector similarity search and part of the query corresponds to predicates over relational attributes associated with the underlying data vectors. For example, given past KG queries for a song entity, we want to construct new queries for new song entities whose vector representations are close to the vector representation of the entity in the past KG query. But entities in a KG also have non-vector attributes such as a song associated with an arti
    
[^10]: 加速和压缩深度神经网络用于 Massive MIMO CSI 反馈

    Accelerating and Compressing Deep Neural Networks for Massive MIMO CSI Feedback. (arXiv:2304.01914v1 [cs.NI])

    [http://arxiv.org/abs/2304.01914](http://arxiv.org/abs/2304.01914)

    本文提出了一种用于 Massive MIMO CSI 反馈的加速和压缩有效神经网络，采用了网络修剪、训练后动态范围量化和权重聚类等优化方法，以缓解神经网络在实际无线系统中的限制。

    

    最近机器学习和深度神经网络的进步使得它们成为了无线通信的候选者，如信道估计、解码和下行信道状态信息（CSI）压缩。然而，大多数这些神经网络都是庞大和低效的，这成为它们在需要个别网络功能低延迟和低内存占用的实际无线系统部署中的障碍。为了减轻这些限制，我们提出了用于 Massive MIMO CSI 反馈的加速和压缩有效神经网络。具体来说，我们深入研究了采用网络修剪、训练后动态范围量化和权重聚类来优化 Massive MIMO 系统的 CSI 反馈压缩。此外，我们还在商用硬件上部署了所提出的模型压缩技术，并展示了为实现推理增益，需要加速稀疏网络计算的专用库。

    The recent advances in machine learning and deep neural networks have made them attractive candidates for wireless communications functions such as channel estimation, decoding, and downlink channel state information (CSI) compression. However, most of these neural networks are large and inefficient making it a barrier for deployment in practical wireless systems that require low-latency and low memory footprints for individual network functions. To mitigate these limitations, we propose accelerated and compressed efficient neural networks for massive MIMO CSI feedback. Specifically, we have thoroughly investigated the adoption of network pruning, post-training dynamic range quantization, and weight clustering to optimize CSI feedback compression for massive MIMO systems. Furthermore, we have deployed the proposed model compression techniques on commodity hardware and demonstrated that in order to achieve inference gains, specialized libraries that accelerate computations for sparse ne
    
[^11]: 校准混乱：神经网络训练的运行变化在无意中且无害

    Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable. (arXiv:2304.01910v1 [cs.LG])

    [http://arxiv.org/abs/2304.01910](http://arxiv.org/abs/2304.01910)

    神经网络训练的运行变化在实际中更少遇到问题，我们提出了一种简化的统计假设并证明方差主要由于训练过程对初始条件的高敏感性所导致。

    

    典型的神经网络训练在重复测试时会有显著的测试集性能差异，影响模型超参数的比较和训练的可重复性。本文通过以下比较来解释这种变化：（1）尽管在测试集上有显著的方差，但标准的CIFAR-10与ImageNet训练在测试分布上的表现却非常一致，这表明方差不像之前想象的那么严重。（2）我们提出了一种简化的统计假设，以紧密近似测试集准确性分布结构。（3）我们认为，在以下两个意义上，测试集方差是不可避免的。首先，我们展示了方差主要是由于训练过程对初始条件的高敏感性而不是特定的随机源（如数据排序和扩充）所导致的。其次，我们证明了方差是频率极限的，但可以通过训练多个模型来减少。

    Typical neural network trainings have substantial variance in test-set performance between repeated runs, impeding hyperparameter comparison and training reproducibility. We present the following results towards understanding this variation. (1) Despite having significant variance on their test-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have very little variance in their performance on the test-distributions from which those test-sets are sampled, suggesting that variance is less of a practical issue than previously thought. (2) We present a simplifying statistical assumption which closely approximates the structure of the test-set accuracy distribution. (3) We argue that test-set variance is inevitable in the following two senses. First, we show that variance is largely caused by high sensitivity of the training process to initial conditions, rather than by specific sources of randomness like the data order and augmentations. Second, we prove that variance is u
    
[^12]: 利用深度学习方法进行Deepfake检测：一项综述

    Leveraging Deep Learning Approaches for Deepfake Detection: A Review. (arXiv:2304.01908v1 [cs.LG])

    [http://arxiv.org/abs/2304.01908](http://arxiv.org/abs/2304.01908)

    本文综述了Deepfake检测的研究现状，探讨了使用深度学习方法判断Deepfake的不同技术，并旨在实现一种成本效益高且具有高准确性的模型来解决数据集泛化问题。

    

    机器学习和深度学习领域的显著进展使得高度逼真的虚假媒体——Deepfake（深度伪造）的出现成为可能。Deepfake是通过复杂的人工智能生成的虚假媒体，有时很难与真实媒体区分。到目前为止，这种媒体可以上传到各种社交媒体平台，因此宣传变得容易，这需要采取有效的对策。因此，对Deepfake的乐观对策之一将是Deepfake检测。为了对付这一威胁，过去的研究人员基于卷积神经网络等机器学习和深度学习技术创建了检测Deepfake的模型。本文旨在探索不同的方法，以实现具有更高准确性的成本有效模型，并使用不同类型的数据集来解决数据集的泛化问题。

    Conspicuous progression in the field of machine learning and deep learning have led the jump of highly realistic fake media, these media oftentimes referred as deepfakes. Deepfakes are fabricated media which are generated by sophisticated AI that are at times very difficult to set apart from the real media. So far, this media can be uploaded to the various social media platforms, hence advertising it to the world got easy, calling for an efficacious countermeasure. Thus, one of the optimistic counter steps against deepfake would be deepfake detection. To undertake this threat, researchers in the past have created models to detect deepfakes based on ML/DL techniques like Convolutional Neural Networks. This paper aims to explore different methodologies with an intention to achieve a cost-effective model with a higher accuracy with different types of the datasets, which is to address the generalizability of the dataset.
    
[^13]: Torch-Choice: 用Python实现大规模选择建模的PyTorch包

    Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python. (arXiv:2304.01906v1 [cs.LG])

    [http://arxiv.org/abs/2304.01906](http://arxiv.org/abs/2304.01906)

    本文介绍了一款名为 Torch-Choice 的 PyTorch 软件包，用于管理数据库、构建多项式Logit和嵌套Logit模型，并支持GPU加速，具有灵活性和高效性。

    

    $\texttt{torch-choice}$ 是一款开源软件包，使用Python和PyTorch实现灵活、快速的选择建模。它提供了 $\texttt{ChoiceDataset}$ 数据结构，以便灵活而高效地管理数据库。本文演示了如何从各种格式的数据库中构建 $\texttt{ChoiceDataset}$，并展示了 $\texttt{ChoiceDataset}$ 的各种功能。该软件包实现了两种常用的模型: 多项式Logit和嵌套Logit模型，并支持模型估计期间的正则化。该软件包还支持使用GPU进行估计，使其可以扩展到大规模数据集而且在计算上更高效。模型可以使用R风格的公式字符串或Python字典进行初始化。最后，我们比较了 $\texttt{torch-choice}$ 和 R中的 $\texttt{mlogit}$ 在以下几个方面的计算效率: (1) 观测数增加时，(2) 协变量个数增加时， (3) 测试数升高时。

    The $\texttt{torch-choice}$ is an open-source library for flexible, fast choice modeling with Python and PyTorch. $\texttt{torch-choice}$ provides a $\texttt{ChoiceDataset}$ data structure to manage databases flexibly and memory-efficiently. The paper demonstrates constructing a $\texttt{ChoiceDataset}$ from databases of various formats and functionalities of $\texttt{ChoiceDataset}$. The package implements two widely used models, namely the multinomial logit and nested logit models, and supports regularization during model estimation. The package incorporates the option to take advantage of GPUs for estimation, allowing it to scale to massive datasets while being computationally efficient. Models can be initialized using either R-style formula strings or Python dictionaries. We conclude with a comparison of the computational efficiencies of $\texttt{torch-choice}$ and $\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the number of covariates increases, and (3) th
    
[^14]: 双关注神经变换器用于语音识别时的高效唤醒词识别

    Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])

    [http://arxiv.org/abs/2304.01905](http://arxiv.org/abs/2304.01905)

    本文介绍了一种新的“双关注神经变换器”，可以通过优化唤醒词检测来选择计算路径，从而提高唤醒词的准确性和推理时间效率，并且计算成本可以降低90％而仅增加1％的参数。这种架构可以在语音识别领域中大有裨益。

    

    本文提出了一种称为双关注神经网络的架构，旨在提高唤醒词识别的准确率并改善语音识别任务的推理时间。该架构通过利用唤醒词检测来选择哪个注意力网络执行输入音频帧的运行时计算路径。使用这种方法，作者有效提高了唤醒词识别的准确性，并定义了浮点运算（FLOPs）的运行时计算成本。在使用作者的内部数据集时，作者证明了所提出的双关注网络可以将唤醒词音频帧的计算成本降低$90\%$，而参数数量仅增加$1\%$。与基线相比，该架构提高了唤醒词F1得分$16\%$，并将一般的罕见词错误率提高了$3\%$。

    We present dual-attention neural biasing, an architecture designed to boost Wake Words (WW) recognition and improve inference time latency on speech recognition tasks. This architecture enables a dynamic switch for its runtime compute paths by exploiting WW spotting to select which branch of its attention networks to execute for an input audio frame. With this approach, we effectively improve WW spotting accuracy while saving runtime compute cost as defined by floating point operations (FLOPs). Using an in-house de-identified dataset, we demonstrate that the proposed dual-attention network can reduce the compute cost by $90\%$ for WW audio frames, with only $1\%$ increase in the number of parameters. This architecture improves WW F1 score by $16\%$ relative and improves generic rare word error rate by $3\%$ relative compared to the baselines.
    
[^15]: 通过引导轨迹扩散控制的行人动画生成方法

    Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion. (arXiv:2304.01893v1 [cs.CV])

    [http://arxiv.org/abs/2304.01893](http://arxiv.org/abs/2304.01893)

    通过引导轨迹扩散控制的行人动画生成方法，可以实现对行人轨迹和全身动画的精准控制和模拟，为特定场景的处理提供了新的思路。

    

    我们引入了一种生成真实行人轨迹和全身动画的方法，可以控制以满足用户定义的目标。我们利用了最近在引导扩散建模方面的进展，实现了在测试时间对轨迹进行可控制，这通常只与基于规则的系统相关。我们的引导扩散模型允许用户通过目标路径点、速度和指定的社交群体来限制轨迹，同时考虑周围环境情况。此轨迹扩散模型与一种新颖的基于物理的人形控制器相结合，形成了一个闭环、全身行人动画系统，能够将大批人群放置在具有不同地形的模拟环境中。我们还提出利用在RL训练动画控制器期间学习到的值函数来引导扩散，以生成更适合特定场景的轨迹，例如避免碰撞和穿越不平地形。视频结果是令人满意的。

    We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are a
    
[^16]: 社会文化知识在仇恨言论检测任务中对选项的选择是必要的

    Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])

    [http://arxiv.org/abs/2304.01890](http://arxiv.org/abs/2304.01890)

    HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。

    

    我们引入了HATELEXICON，这是一个包含巴西，德国，印度和肯尼亚的蔑称和仇恨言论目标的词汇表，以帮助模型的训练和可解释性。我们展示了我们的词汇表如何用于解释模型预测，表明发展用于分类极端言论的模型，在进行预测时严重依赖目标词。此外，我们提出了一种通过HATELEXICON来辅助低资源环境下训练选项的方法，选项选择在小样本学习中尤为重要。在我们的工作中，我们使用HASOC数据对德语和印地语进行了几个示范学习，并将Multilingual HateCheck（MHC）作为基准。我们展示了根据我们的词汇表选择样本，相对于随机采样的模型，能够更好地在MHC上表现。因此，当仅有少量的训练样本时，使用我们的词汇表来选择包含更多社会文化信息的样本能够更好地提高在仇恨言论检测任务中的性能。

    We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
    
[^17]: 有限权重神经网络的度量论逼近结果

    Measure theoretic results for approximation by neural networks with limited weights. (arXiv:2304.01880v1 [cs.LG])

    [http://arxiv.org/abs/2304.01880](http://arxiv.org/abs/2304.01880)

    本文探讨了有限权重神经网络的逼近性质，给出了获得密度的必要且充分的度量论条件，同时针对一种特定激活函数和固定神经元数量的神经网络证明了其在连续函数空间中具有密度。

    

    本文研究了单隐藏层神经网络在权重在有限方向和开区间阈值上变化的逼近性质。我们获得了一种必要且同时充分的度量论条件，以使此类网络在连续函数空间中具有密度。此外，我们证明了一种构建激活函数和固定神经元数量的神经网络的密度结论。

    In this paper, we study approximation properties of single hidden layer neural networks with weights varying on finitely many directions and thresholds from an open interval. We obtain a necessary and at the same time sufficient measure theoretic condition for density of such networks in the space of continuous functions. Further, we prove a density result for neural networks with a specifically constructed activation function and a fixed number of neurons.
    
[^18]: 神经网络的增量验证

    Incremental Verification of Neural Networks. (arXiv:2304.01874v1 [cs.LG])

    [http://arxiv.org/abs/2304.01874](http://arxiv.org/abs/2304.01874)

    提出了一种新的、基于设计新的理论、数据结构和算法的神经网络增量与完全验证的通用框架，实现了对MNIST和CIFAR10以及ACAS-XU分类器的更高效的验证。

    

    深度神经网络（DNNs）的完全验证可以确定DNNs是否在无限输入集上满足所需的可信属性（例如，鲁棒性，公正性）。尽管多年来已经取得了极大的进展，以改善完全验证器在单个DNNs上的可扩展性，但是当部署的DNN进行更新以提高其推理速度或准确性时，它们在本质上效率低下。这是因为需要从头开始运行昂贵的验证器来验证更新后的DNN。为了提高效率，我们提出了一种新的、基于设计新的理论、数据结构和算法的增量和完全DNN验证的通用框架。我们的贡献在一个名为IVAN的工具中实现，对于验证MNIST和CIFAR10分类器，我们的总体几何平均加速比为2.4倍，对于ACAS-XU分类器，我们的总体几何平均加速比为3.8倍，超过了现有最先进的基线。

    Complete verification of deep neural networks (DNNs) can exactly determine whether the DNN satisfies a desired trustworthy property (e.g., robustness, fairness) on an infinite set of inputs or not. Despite the tremendous progress to improve the scalability of complete verifiers over the years on individual DNNs, they are inherently inefficient when a deployed DNN is updated to improve its inference speed or accuracy. The inefficiency is because the expensive verifier needs to be run from scratch on the updated DNN. To improve efficiency, we propose a new, general framework for incremental and complete DNN verification based on the design of novel theory, data structure, and algorithms. Our contributions implemented in a tool named IVAN yield an overall geometric mean speedup of 2.4x for verifying challenging MNIST and CIFAR10 classifiers and a geometric mean speedup of 3.8x for the ACAS-XU classifiers over the state-of-the-art baselines.
    
[^19]: 垂直联邦学习综述：以分层视角为基础

    A Survey on Vertical Federated Learning: From a Layered Perspective. (arXiv:2304.01829v1 [cs.LG])

    [http://arxiv.org/abs/2304.01829](http://arxiv.org/abs/2304.01829)

    垂直联邦学习（VFL）是一种适用于数据垂直分区情况的有前途的联邦学习方法，该方法丰富了样本描述，以提高模型容量。研究人员在硬件层到垂直联邦系统层各个方面做了贡献，VFL的应用已涵盖多个领域，尤其隐私保护是关键问题。

    

    垂直联邦学习（VFL）是一种有前途的联邦学习范畴，适用于数据垂直分区并分布在各方之间的情况。VFL使用来自不同方的特征来丰富样本描述，以提高模型容量。与水平联邦学习相比，在大多数情况下，VFL应用于公司的商业合作场景中，因此VFL包含巨大的商业价值。在过去的几年中，VFL在学术界和工业界引起了越来越多的关注。本文从分层视角系统地调查了VFL的现有研究工作。从硬件层到垂直联邦系统层，研究人员贡献了VFL的各个方面。此外，VFL的应用已覆盖了广泛的领域，例如金融、医疗等。在每个层面上，我们对现有工作进行分类并探讨了进一步研究和发展VFL的挑战。特别关注隐私保护，这是VFL中由于数据分布在各方之间而具有关键性的问题。

    Vertical federated learning (VFL) is a promising category of federated learning for the scenario where data is vertically partitioned and distributed among parties. VFL enriches the description of samples using features from different parties to improve model capacity. Compared with horizontal federated learning, in most cases, VFL is applied in the commercial cooperation scenario of companies. Therefore, VFL contains tremendous business values. In the past few years, VFL has attracted more and more attention in both academia and industry. In this paper, we systematically investigate the current work of VFL from a layered perspective. From the hardware layer to the vertical federated system layer, researchers contribute to various aspects of VFL. Moreover, the application of VFL has covered a wide range of areas, e.g., finance, healthcare, etc. At each layer, we categorize the existing work and explore the challenges for the convenience of further research and development of VFL. Espec
    
[^20]: 学习稳定和鲁棒的线性参数可变状态空间模型

    Learning Stable and Robust Linear Parameter-Varying State-Space Models. (arXiv:2304.01828v1 [eess.SY])

    [http://arxiv.org/abs/2304.01828](http://arxiv.org/abs/2304.01828)

    该论文提出了稳定和鲁棒的线性参数可变状态空间模型的两种直接参数化方法，训练出的模型具有收缩意义或通过用户定义的值被限制在 Lipschitz 常数内，对进一步的凸分析或控制器设计非常有用。

    

    本文提出了两种直接参数化的稳定和鲁棒线性参数可变状态空间 (LPV-SS) 模型。模型参数化在训练期间保证所有参数值的模型都是稳定的，具有收缩意义，或其 Lipschitz 常数被用户定义的值 $\gamma$ 限制。此外，由于参数化是直接的，因此可以使用无约束优化来训练模型。由于训练出的模型属于 LPV-SS 类，因此它们对进一步的凸分析或控制器设计非常有用。该方法在 LPV 识别问题上展示了出色的效果。

    This paper presents two direct parameterizations of stable and robust linear parameter-varying state-space (LPV-SS) models. The model parametrizations guarantee a priori that for all parameter values during training, the allowed models are stable in the contraction sense or have their Lipschitz constant bounded by a user-defined value $\gamma$. Furthermore, since the parametrizations are direct, the models can be trained using unconstrained optimization. The fact that the trained models are of the LPV-SS class makes them useful for, e.g., further convex analysis or controller design. The effectiveness of the approach is demonstrated on an LPV identification problem.
    
[^21]: CoreDiff: 上下文误差调制广义扩散模型用于低剂量CT去噪和泛化

    CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization. (arXiv:2304.01814v1 [eess.IV])

    [http://arxiv.org/abs/2304.01814](http://arxiv.org/abs/2304.01814)

    本文提出了一种新的上下文误差调制广义扩散模型（CoreDiff），用于低剂量CT（LDCT）的去噪。该模型利用LDCT图像来消除随机高斯噪声并模拟CT退化的物理过程，减少采样步骤，并引入上下文误差调制以增强鲁棒性和泛化能力。

    

    低剂量计算机断层扫描（CT）图像由于光子匮乏和电子噪声而受到噪声和伪影的影响。最近，一些研究尝试使用扩散模型来解决以前基于深度学习的去噪模型遇到的过度平滑和训练不稳定性问题。然而，由于涉及大量采样步骤，扩散模型的推理时间很长。最近，冷扩散模型推广了经典扩散模型，并具有更大的灵活性。受冷扩散的启发，本文提出了一种新颖的针对低剂量CT（LDCT）去噪的上下文误差调制广义扩散模型，称为CoreDiff。首先，CoreDiff利用LDCT图像来消除随机高斯噪声，并采用新型均值保持退化算子来模拟CT退化的物理过程，由于启动采样过程的信息丰富的LDCT图像，大幅减少采样步骤。其次，为缓解扩散模型中过多采样步骤的问题，引入上下文误差调制，使CoreDiff更具有鲁棒性和泛化能力。

    Low-dose computed tomography (CT) images suffer from noise and artifacts due to photon starvation and electronic noise. Recently, some works have attempted to use diffusion models to address the over-smoothness and training instability encountered by previous deep-learning-based denoising models. However, diffusion models suffer from long inference times due to the large number of sampling steps involved. Very recently, cold diffusion model generalizes classical diffusion models and has greater flexibility. Inspired by the cold diffusion, this paper presents a novel COntextual eRror-modulated gEneralized Diffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First, CoreDiff utilizes LDCT images to displace the random Gaussian noise and employs a novel mean-preserving degradation operator to mimic the physical process of CT degradation, significantly reducing sampling steps thanks to the informative LDCT images as the starting point of the sampling process. Second, to allevi
    
[^22]: HarsanyiNet: 在单次前向传播中计算准确的 Shapley 值

    HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation. (arXiv:2304.01811v1 [cs.LG])

    [http://arxiv.org/abs/2304.01811](http://arxiv.org/abs/2304.01811)

    HarsanyiNet 是一种新型的深度神经网络架构，它可以在单次前向传播中计算输入变量的精确 Shapley 值。

    

    Shapley 值被广泛认为是一种可信的属性度量方法。然而，当人们使用 Shapley 值来解释深度神经网络（DNN）的输入变量的属性时，通常需要非常高的计算成本才能在实际应用中近似计算出比较精确的 Shapley 值。因此，我们提出一种新型网络架构 HarsanyiNet，在输入样本的推理过程中同时计算输入变量的精确 Shapley 值，只需要一次前向传播即可。HarsanyiNet 是构建在 Shapley 值可以被重新构建为网络编码的 Harsanyi 交互重新分配的理论基础之上的。

    The Shapley value is widely regarded as a trustworthy attribution metric. However, when people use Shapley values to explain the attribution of input variables of a deep neural network (DNN), it usually requires a very high computational cost to approximate relatively accurate Shapley values in real-world applications. Therefore, we propose a novel network architecture, the HarsanyiNet, which makes inferences on the input sample and simultaneously computes the exact Shapley values of the input variables in a single forward propagation. The HarsanyiNet is designed on the theoretical foundation that the Shapley value can be reformulated as the redistribution of Harsanyi interactions encoded by the network.
    
[^23]: 机器学习发现等几何分析最佳求积规则

    Machine Learning Discovery of Optimal Quadrature Rules for Isogeometric Analysis. (arXiv:2304.01802v1 [math.NA])

    [http://arxiv.org/abs/2304.01802](http://arxiv.org/abs/2304.01802)

    本论文使用机器学习技术找到了在等几何分析中构造刚度和质量矩阵的最佳求积规则，并与传统的Gauss型求积规则和标准中点规则进行了比较，在许多基准问题中表现出更好的性能。

    

    我们提出使用机器学习技术找到等几何分析（IGA）中构造刚度和质量矩阵的最佳求积规则。我们最初考虑跨越统一和非统一节点序列的任意次1D样条空间，然后使用张量积意义下生成的最佳规则进行高维空间的积分。 求积规则搜索被提出为一个优化问题，并通过基于梯度下降的机器学习策略来解决。然而，由于优化空间高度非凸，搜索的成功强烈依赖于求积点数和参数初始化。因此，我们使用一种动态规划策略，该策略使用具有较少节点的样条空间上的最优解来初始化参数。使用该方法，我们发现了在使用高达50个均匀元素和多项式度数高达8的IGA离散化时，样条空间的最佳求积规则。生成的求积规则是稳健高效的，并且在许多基准问题中表现出比Gauss型求积规则和标准中点规则更好的性能。

    We propose the use of machine learning techniques to find optimal quadrature rules for the construction of stiffness and mass matrices in isogeometric analysis (IGA). We initially consider 1D spline spaces of arbitrary degree spanned over uniform and non-uniform knot sequences, and then the generated optimal rules are used for integration over higher-dimensional spaces using tensor product sense. The quadrature rule search is posed as an optimization problem and solved by a machine learning strategy based on gradient-descent. However, since the optimization space is highly non-convex, the success of the search strongly depends on the number of quadrature points and the parameter initialization. Thus, we use a dynamic programming strategy that initializes the parameters from the optimal solution over the spline space with a lower number of knots. With this method, we found optimal quadrature rules for spline spaces when using IGA discretizations with up to 50 uniform elements and polyno
    
[^24]: 个性化联邦学习与本地注意力

    Personalized Federated Learning with Local Attention. (arXiv:2304.01783v1 [cs.LG])

    [http://arxiv.org/abs/2304.01783](http://arxiv.org/abs/2304.01783)

    本文提出了一个名为pFedLA的算法，通过将注意力机制并入个性化模型来解决联邦学习中客户端数据异质性的问题，并在实验中表现出了优异的表现，尤其是在缓解特征漂移问题方面。

    

    联邦学习旨在学习一个单一的全局模型，使得中央服务器可以帮助在本地客户端进行模型训练，而不必访问其本地数据。联邦学习的关键挑战是不同客户端中本地数据的异质性，例如异质标签分布和特征偏移，这可能导致学习模型的显着性能降低。为解决这个问题，我们提出了一种简单而有效的算法，即具有本地注意力的个性化联邦学习（pFedLA），通过将注意机制并入客户端的个性化模型，同时保持注意块特定于客户端。具体而言，pFedLA提出了两个模块，即个性化单注意模块和个性化混合注意模块。此外，我们还介绍了一个新的FL数据集SplitMNIST-C，通过引入训练和测试数据之间的偏移。在SplitMNIST-C和EMNIST上的实验结果表明，pFedLA在准确性和收敛速度方面优于最先进的FL算法，并且在缓解特征漂移问题方面特别有效。

    Federated Learning (FL) aims to learn a single global model that enables the central server to help the model training in local clients without accessing their local data. The key challenge of FL is the heterogeneity of local data in different clients, such as heterogeneous label distribution and feature shift, which could lead to significant performance degradation of the learned models. Although many studies have been proposed to address the heterogeneous label distribution problem, few studies attempt to explore the feature shift issue. To address this issue, we propose a simple yet effective algorithm, namely \textbf{p}ersonalized \textbf{Fed}erated learning with \textbf{L}ocal \textbf{A}ttention (pFedLA), by incorporating the attention mechanism into personalized models of clients while keeping the attention blocks client-specific. Specifically, two modules are proposed in pFedLA, i.e., the personalized single attention module and the personalized hybrid attention module. In addit
    
[^25]: 通过精确 Q-loss 及其 Gauss-Newton 近似从非线性 MPC 中进行模仿学习

    Imitation Learning from Nonlinear MPC via the Exact Q-Loss and its Gauss-Newton Approximation. (arXiv:2304.01782v1 [cs.LG])

    [http://arxiv.org/abs/2304.01782](http://arxiv.org/abs/2304.01782)

    本文提出了一种新的 Q-loss 函数，通过精确嵌入最优控制问题的目标和约束，以及 Gauss-Newton 近似损失加速训练，从而在带约束的非线性系统控制中进行模仿学习。

    

    本文提出了一种新颖的损失函数，通过模仿学习学习非线性模型预测控制策略。我们提出了一种基于 Q 函数的损失，直接嵌入了与最优控制问题相关的性能目标和约束满足性。为了缓解计算负担，我们得出了一个基于 Gauss-Newton 近似的第二个 Q-loss，从而实现更快的训练时间。通过在带约束的非线性系统的控制上对我们的损失进行验证，我们将其与行为克隆相比较，结果显示，基于 Q 函数的损失显著减少了约束违规的数量。

    This work presents a novel loss function for learning nonlinear Model Predictive Control policies via Imitation Learning. Standard approaches to Imitation Learning neglect information about the expert and generally adopt a loss function based on the distance between expert and learned controls. In this work, we present a loss based on the Q-function directly embedding the performance objectives and constraint satisfaction of the associated Optimal Control Problem (OCP). However, training a Neural Network with the Q-loss requires solving the associated OCP for each new sample. To alleviate the computational burden, we derive a second Q-loss based on the Gauss-Newton approximation of the OCP resulting in a faster training time. We validate our losses against Behavioral Cloning, the standard approach to Imitation Learning, on the control of a nonlinear system with constraints. The final results show that the Q-function-based losses significantly reduce the amount of constraint violations 
    
[^26]: 在线指标算法的混合预测

    Mixing predictions for online metric algorithms. (arXiv:2304.01781v1 [cs.LG])

    [http://arxiv.org/abs/2304.01781](http://arxiv.org/abs/2304.01781)

    本文提出了一种在线算法的混合预测方法，针对度量任务系统，我们获得了$O(\ell^2)$的竞争比，可以使算法跟随不同的预测器，对限制切换次数的情况可以获得$(1+\epsilon)$-竞争算法。

    

    在学习-增强的在线算法中，主要技术之一是组合多个算法或预测器。由于每个预测器的性能可能随时间变化，因此希望使用动态组合，根据不同的时间跟随不同的预测器，而不是使用单个最佳预测器作为基准。我们设计了一些组合预测并针对广泛的在线问题类别（即度量任务系统）与这样的动态组合进行竞争。针对最佳（事后）无约束组合的$\ell$个预测器，我们获得了$O(\ell^2)$的竞争比，并证明这是最优的。然而，对于一个约束在不同预测器之间切换次数的基准，我们可以获得$(1+\epsilon)$-竞争算法。此外，我们的算法可以适应类似于赌博机式的访问预测器的方式，每次查询一个预测器。我们其中一条下界的一个意外推论是，出现了新的结构。

    A major technique in learning-augmented online algorithms is combining multiple algorithms or predictors. Since the performance of each predictor may vary over time, it is desirable to use not the single best predictor as a benchmark, but rather a dynamic combination which follows different predictors at different times. We design algorithms that combine predictions and are competitive against such dynamic combinations for a wide class of online problems, namely, metrical task systems. Against the best (in hindsight) unconstrained combination of $\ell$ predictors, we obtain a competitive ratio of $O(\ell^2)$, and show that this is best possible. However, for a benchmark with slightly constrained number of switches between different predictors, we can get a $(1+\epsilon)$-competitive algorithm. Moreover, our algorithms can be adapted to access predictors in a bandit-like fashion, querying only one predictor at a time. An unexpected implication of one of our lower bounds is a new structu
    
[^27]: 以用户为中心的交互式人机交互主题建模系统

    A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System. (arXiv:2304.01774v1 [cs.CL])

    [http://arxiv.org/abs/2304.01774](http://arxiv.org/abs/2304.01774)

    本文介绍了一种以用户为中心的交互式人机交互主题建模系统，该系统除了支持从整个语料库中学习主题外，还支持目标主题建模，并提供了一种新颖的主题词建议功能。该系统可以很好地迭代地完善模型，且在用户研究中获得了良好的评价。

    

    人机交互主题建模系统将用户的知识融入到建模过程中，使其能够迭代地完善模型。最近的研究已经展示了用户反馈的价值，但仍存在一些问题，例如难以跟踪变化、比较不同模型以及缺乏基于实际使用的真实世界的评估。我们开发了一种新颖的交互式人机交互主题建模系统，其具有用户友好的界面，使用户可以比较和记录每个步骤，并具有新颖的主题词建议功能，以帮助用户提供真实可靠的反馈。我们的系统不仅支持传统主题模型的功能，即从整个语料库中学习主题，还支持目标主题建模，即针对语料库特定方面进行主题学习。在本文中，我们概述了该系统，并介绍了一系列用户研究的结果，以评估其价值。

    Human-in-the-loop topic modelling incorporates users' knowledge into the modelling process, enabling them to refine the model iteratively. Recent research has demonstrated the value of user feedback, but there are still issues to consider, such as the difficulty in tracking changes, comparing different models and the lack of evaluation based on real-world examples of use. We developed a novel, interactive human-in-the-loop topic modeling system with a user-friendly interface that enables users compare and record every step they take, and a novel topic words suggestion feature to help users provide feedback that is faithful to the ground truth. Our system also supports not only what traditional topic models can do, i.e., learning the topics from the whole corpus, but also targeted topic modelling, i.e., learning topics for specific aspects of the corpus. In this article, we provide an overview of the system and present the results of a series of user studies designed to assess the value
    
[^28]: 一种用于自旋模型的可微编程框架

    A differentiable programming framework for spin models. (arXiv:2304.01772v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2304.01772](http://arxiv.org/abs/2304.01772)

    本研究提出了一种可微编程框架，用于高效地模拟自旋系统，在伊辛模型、波茨模型和细胞波茨模型上进行了实验验证，实现了显著的加速效果。

    

    自旋系统是建模各种物理系统的有效工具。本文提出了一种使用可微编程建模自旋系统的新型框架。我们的方法使我们能够高效地模拟自旋系统，从而能够在大规模复杂系统中进行建模。具体来说，我们通过将其应用于三种不同的自旋系统——伊辛模型、波茨模型和细胞波茨模型——来证明我们技术的有效性。我们的模拟结果表明，与传统的模拟方法相比，我们的框架能够在不同的硬件架构（包括图形处理器和张量处理器）上高效地执行代码，从而实现显著的加速。

    Spin systems are a powerful tool for modeling a wide range of physical systems. In this paper, we propose a novel framework for modeling spin systems using differentiable programming. Our approach enables us to efficiently simulate spin systems, making it possible to model complex systems at scale. Specifically, we demonstrate the effectiveness of our technique by applying it to three different spin systems: the Ising model, the Potts model, and the Cellular Potts model. Our simulations show that our framework offers significant speedup compared to traditional simulation methods, thanks to its ability to execute code efficiently across different hardware architectures, including Graphical Processing Units and Tensor Processing Units.
    
[^29]: 字典学习中交替极小化算法的收敛性

    Convergence of alternating minimisation algorithms for dictionary learning. (arXiv:2304.01768v1 [math.OC])

    [http://arxiv.org/abs/2304.01768](http://arxiv.org/abs/2304.01768)

    本文探讨了字典学习中两种交替极小化算法的收敛性，在良好的初始化下，这两种算法能够以几何收敛速率收敛于生成的字典，且可适用于非均匀分布的数据模型。

    

    本文导出了针对字典学习两种流行的交替极小化算法 - 最优方向法（MOD）和在线字典学习（ODL）的收敛性足够的条件。我们表明，只要初始值良好，即距离生成的字典不超过$1/\log(K)$或具有一定的结构，确保初始值中的每个元素只指向一个生成元，两种算法将以几何收敛速率收敛于生成的字典。这在具有非均匀分布的数据模型上也能实现，该模型中稀疏系数的支撑集的出现频率可以变化很大，从而更接近真实数据。

    In this paper we derive sufficient conditions for the convergence of two popular alternating minimisation algorithms for dictionary learning - the Method of Optimal Directions (MOD) and Online Dictionary Learning (ODL), which can also be thought of as approximative K-SVD. We show that given a well-behaved initialisation that is either within distance at most $1/\log(K)$ to the generating dictionary or has a special structure ensuring that each element of the initialisation only points to one generating element, both algorithms will converge with geometric convergence rate to the generating dictionary. This is done even for data models with non-uniform distributions on the supports of the sparse coefficients. These allow the appearance frequency of the dictionary elements to vary heavily and thus model real data more closely.
    
[^30]: 将未标记数据纳入贝叶斯神经网络中

    Incorporating Unlabelled Data into Bayesian Neural Networks. (arXiv:2304.01762v1 [cs.LG])

    [http://arxiv.org/abs/2304.01762](http://arxiv.org/abs/2304.01762)

    该论文提出了一种利用未标记数据学习贝叶斯神经网络（BNNs）的对比框架，通过该框架提出了一种同时具备自监督学习的标签效率和贝叶斯方法中的不确定性估计的实用BNN算法。最后，该方法在半监督和低预算主动学习问题中展现出了数据高效学习的优势。

    

    我们提出了一个对贝叶斯神经网络（BNNs）中先验分布进行学习的对比框架，利用未标记数据来优化。基于该框架，我们提出了一种实用的BNN算法，同时具备自监督学习的标签效率和贝叶斯方法中的根据原则的不确定性估计。最后，我们展示了我们的方法在半监督和低预算主动学习问题中的数据高效学习优势。

    We develop a contrastive framework for learning better prior distributions for Bayesian Neural Networks (BNNs) using unlabelled data. With this framework, we propose a practical BNN algorithm that offers the label-efficiency of self-supervised learning and the principled uncertainty estimates of Bayesian methods. Finally, we demonstrate the advantages of our approach for data-efficient learning in semi-supervised and low-budget active learning problems.
    
[^31]: 视觉-语言模型的黑匣子少样本适应

    Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])

    [http://arxiv.org/abs/2304.01752](http://arxiv.org/abs/2304.01752)

    本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的视觉-语言模型的快速少样本适应，适用于有监督和无监督训练，并且可以用于对单模型计算的图像和文本特征进行对齐。

    

    通过对比学习训练的视觉-语言模型在少样本情况下表现出很强的学习能力。软提示学习是少样本领域适用的最受欢迎的方法，旨在通过新领域引发的分布偏移来缩小模态差距。虽然该方法性能高效，但仍需要访问模型权重，并且在具有数十亿个参数的大型模型上可能会导致计算上的不可行性。本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的 V-L 少样本适应，不需要访问模型权重，训练速度快数个数量级，适用于有监督和无监督训练，并且还可以用于对单模型计算的图像和文本特征进行对齐。

    Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
    
[^32]: 自适应学习有效动力学：面向复杂系统的自适应实时在线建模

    Adaptive learning of effective dynamics: Adaptive real-time, online modeling for complex systems. (arXiv:2304.01732v1 [physics.comp-ph])

    [http://arxiv.org/abs/2304.01732](http://arxiv.org/abs/2304.01732)

    AdaLED是一个新的系统性框架，它将大规模模拟与降阶模型连接起来，以自适应方式提取和预测多尺度系统的有效动力学。它使用自编码器来识别系统动态的降阶表示，并使用概率循环神经网络（RNNs）的集合作为潜在时间步进器。

    

    预测性模拟在从天气预报到材料设计的各种应用中都至关重要。这些模拟的准确性取决于其捕获有效的系统动态的能力。大规模并行模拟通过解析所有时空尺度来预测系统动态，往往以成本作为代价阻止了实验。另一方面，降阶模型速度快，但常常受到系统动力学线性化和所采用的启发式封闭的限制。我们提出了一个新的系统性框架，将大规模模拟与降阶模型连接起来，以自适应方式提取和预测多尺度系统的有效动力学（AdaLED）。AdaLED采用自编码器来识别系统动态的降阶表示，并使用概率循环神经网络（RNNs）的集合作为潜在时间步进器。该框架在计算求解器和替代模型之间交替，加速学习动力学同时保留了解的动态特征。

    Predictive simulations are essential for applications ranging from weather forecasting to material design. The veracity of these simulations hinges on their capacity to capture the effective system dynamics. Massively parallel simulations predict the systems dynamics by resolving all spatiotemporal scales, often at a cost that prevents experimentation. On the other hand, reduced order models are fast but often limited by the linearization of the system dynamics and the adopted heuristic closures. We propose a novel systematic framework that bridges large scale simulations and reduced order models to extract and forecast adaptively the effective dynamics (AdaLED) of multiscale systems. AdaLED employs an autoencoder to identify reduced-order representations of the system dynamics and an ensemble of probabilistic recurrent neural networks (RNNs) as the latent time-stepper. The framework alternates between the computational solver and the surrogate, accelerating learned dynamics while leav
    
[^33]: 面向隐私保护联邦蒸馏的有选择知识共享方法

    Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher. (arXiv:2304.01731v1 [cs.LG])

    [http://arxiv.org/abs/2304.01731](http://arxiv.org/abs/2304.01731)

    本文提出了有选择的联邦蒸馏机制Selective-FD，可以精确地识别来自本地和集合预测的知识，以解决局部数据分布的变异和缺乏好的教师模型而导致的误导和模糊的知识共享问题，并取得了显著提高的模型性能和准确度。

    

    联邦学习是一种隐私保护的协作学习方法，但是容易受到白盒攻击，并且难以适应异构客户端。基于知识蒸馏的联邦蒸馏是一种提供增强隐私保证并解决模型异构性的替代范例。本文提出了一种有选择的联邦蒸馏机制Selective-FD来应对局部数据分布的变异和缺乏好的教师模型而导致的误导和模糊的知识共享问题。它包括客户端选择器和服务器选择器，以精确地识别来自本地和集合预测的知识。实证研究表明，Selective-FD可显著提高模型性能和准确度。

    While federated learning is promising for privacy-preserving collaborative learning without revealing local data, it remains vulnerable to white-box attacks and struggles to adapt to heterogeneous clients. Federated distillation (FD), built upon knowledge distillation--an effective technique for transferring knowledge from a teacher model to student models--emerges as an alternative paradigm, which provides enhanced privacy guarantees and addresses model heterogeneity. Nevertheless, challenges arise due to variations in local data distributions and the absence of a well-trained teacher model, which leads to misleading and ambiguous knowledge sharing that significantly degrades model performance. To address these issues, this paper proposes a selective knowledge sharing mechanism for FD, termed Selective-FD. It includes client-side selectors and a server-side selector to accurately and precisely identify knowledge from local and ensemble predictions, respectively. Empirical studies, bac
    
[^34]: 揭示XAI方法中依赖特征的贡献

    Characterizing the contribution of dependent features in XAI methods. (arXiv:2304.01717v1 [stat.ML])

    [http://arxiv.org/abs/2304.01717](http://arxiv.org/abs/2304.01717)

    该论文探讨了在XAI方法中考虑预测变量的依赖关系，提出了一种简单快速的方法，并证明其模型无关性。

    

    可解释的人工智能（XAI）提供了工具，帮助理解机器学习模型的工作原理和实现特定结果的方法。它有助于增加模型的可解释性，使得模型更为可信和透明。在这种情况下，许多XAI方法被提出，其中SHAP和LIME最广为人知。然而，这些方法假设机器学习模型中使用的预测变量相互独立，这在一般情况下并不一定成立。这种假设使得XAI结果的稳健性受到影响，比如信息预测变量的列表。在这里，我们提出了一个简单但有用的代理，修改任何XAI特征排名方法的结果，使其能够考虑预测变量之间的相关性。所提出的方法具有模型无关性，并且可以简单地计算在共线性存在的情况下每个预测变量在模型中的影响。

    Explainable Artificial Intelligence (XAI) provides tools to help understanding how the machine learning models work and reach a specific outcome. It helps to increase the interpretability of models and makes the models more trustworthy and transparent. In this context, many XAI methods were proposed being SHAP and LIME the most popular. However, the proposed methods assume that used predictors in the machine learning models are independent which in general is not necessarily true. Such assumption casts shadows on the robustness of the XAI outcomes such as the list of informative predictors. Here, we propose a simple, yet useful proxy that modifies the outcome of any XAI feature ranking method allowing to account for the dependency among the predictors. The proposed approach has the advantage of being model-agnostic as well as simple to calculate the impact of each predictor in the model in presence of collinearity.
    
[^35]: 用最优传输优化纠正学习

    Optimal Transport for Correctional Learning. (arXiv:2304.01701v1 [cs.LG])

    [http://arxiv.org/abs/2304.01701](http://arxiv.org/abs/2304.01701)

    本文提出了一种基于最优传输的纠正学习方法，能够有效地提高参数估计准确性，允许估计更复杂的特征并考虑多种教师的干预策略。

    

    本文提供了一种基于最优传输的纠正学习通用公式，它是关于如何最优地将一个质量分布转移到另一个的。纠正学习是一种利用师生方法增强参数估计过程准确性的框架。在这个框架中，一个专家代理（称之为教师）修改学习代理（称之为学生）所使用的数据，以改善其估计过程。教师的目标是改变数据，使学生的估计误差最小，同时保持一定的干预预算。与现有的纠正学习公式相比，我们的最优传输方法提供了多种好处。它允许估计更复杂的特征，同时考虑多种教师的干预策略。我们在两个理论实例和一个人机交互应用上评估了我们的方法。

    The contribution of this paper is a generalized formulation of correctional learning using optimal transport, which is about how to optimally transport one mass distribution to another. Correctional learning is a framework developed to enhance the accuracy of parameter estimation processes by means of a teacher-student approach. In this framework, an expert agent, referred to as the teacher, modifies the data used by a learning agent, known as the student, to improve its estimation process. The objective of the teacher is to alter the data such that the student's estimation error is minimized, subject to a fixed intervention budget. Compared to existing formulations of correctional learning, our novel optimal transport approach provides several benefits. It allows for the estimation of more complex characteristics as well as the consideration of multiple intervention policies for the teacher. We evaluate our approach on two theoretical examples, and on a human-robot interaction applica
    
[^36]: 去噪扩散概率模型预测分子云密度

    Denoising Diffusion Probabilistic Models to Predict the Density of Molecular Clouds. (arXiv:2304.01670v1 [astro-ph.GA])

    [http://arxiv.org/abs/2304.01670](http://arxiv.org/abs/2304.01670)

    本文引入了一种新的深度学习去噪扩散概率模型，可以从投影质量表面密度图预测巨大分子云的体积或数密度，并在数密度预测精度方面获得了一个量级的提高。

    

    本文引入了最新的深度学习去噪扩散概率模型（DDPM）作为一种从投影质量表面密度图推断巨大分子云（GMCs）体积或数密度的方法。我们采用了具有不同全局磁场强度和大尺度动态、即不碰撞和碰撞GMCs的磁流体力学模拟。我们在所有模拟中训练扩散模型，使用来自不同视角的质量表面密度图及其对应的质量加权数密度图。我们将扩散模型的表现与更传统的经验二分法和三分法幂律拟合方法以及更传统的神经网络机器学习方法（CASI-2D ）进行了比较。我们得出结论，与其他方法相比，扩散模型在预测数密度的准确度上提高了一个量级。我们将扩散方法应用于一些示例天文柱密度图中。

    We introduce the state-of-the-art deep learning Denoising Diffusion Probabilistic Model (DDPM) as a method to infer the volume or number density of giant molecular clouds (GMCs) from projected mass surface density maps. We adopt magnetohydrodynamic simulations with different global magnetic field strengths and large-scale dynamics, i.e., noncolliding and colliding GMCs. We train a diffusion model on both mass surface density maps and their corresponding mass-weighted number density maps from different viewing angles for all the simulations. We compare the diffusion model performance with a more traditional empirical two-component and three-component power-law fitting method and with a more traditional neural network machine learning approach (CASI-2D). We conclude that the diffusion model achieves an order of magnitude improvement on the accuracy of predicting number density compared to that by other methods. We apply the diffusion method to some example astronomical column density map
    
[^37]: 重新审视针对深度神经网络的模型逆推攻击

    Re-thinking Model Inversion Attacks Against Deep Neural Networks. (arXiv:2304.01669v1 [cs.LG])

    [http://arxiv.org/abs/2304.01669](http://arxiv.org/abs/2304.01669)

    本文重新审视深度学习中的模型逆推攻击，提出了一种改进的优化目标和一个新型的“模型增强”思路，可以显著提高攻击性能。

    

    模型逆推（MI）攻击旨在通过滥用对模型的访问来推断和重构私有培训数据。MI攻击引起了有关泄露敏感信息（例如用于训练人脸识别系统的私人面部图像）的担忧。最近，已经提出了几种算法来改善MI的攻击表现。在这项工作中，我们重新审视MI，研究了所有最先进（SOTA） MI算法所涉及的两个基本问题，并提出了解决这些问题的解决方案，这些解决方案可以显著提高所有SOTA MI的攻击表现。特别是，我们的贡献有两个方面：1）我们分析了SOTA MI算法的优化目标，认为该目标对于实现MI是次优的，并提出了一种改进的优化目标，显著提高了攻击性能。2）我们分析了“MI过度拟合”，展示了它会阻止重构图像从学习培训数据的语义，提出了一种新型的“模型增强”思路。

    Model inversion (MI) attacks aim to infer and reconstruct private training data by abusing access to a model. MI attacks have raised concerns about the leaking of sensitive information (e.g. private face images used in training a face recognition system). Recently, several algorithms for MI have been proposed to improve the attack performance. In this work, we revisit MI, study two fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms, and propose solutions to these issues which lead to a significant boost in attack performance for all SOTA MI. In particular, our contributions are two-fold: 1) We analyze the optimization objective of SOTA MI algorithms, argue that the objective is sub-optimal for achieving MI, and propose an improved optimization objective that boosts attack performance significantly. 2) We analyze "MI overfitting", show that it would prevent reconstructed images from learning semantics of training data, and propose a novel "model augmentation" ide
    
[^38]: 对类增量学习的稳定性-可塑性困境的探讨

    On the Stability-Plasticity Dilemma of Class-Incremental Learning. (arXiv:2304.01663v1 [cs.CV])

    [http://arxiv.org/abs/2304.01663](http://arxiv.org/abs/2304.01663)

    本文探讨类增量学习算法如何有效解决稳定性-可塑性权衡问题，发现大多数算法更倾向于保持稳定性而不是可塑性，并对训练在初始类上的模型的特征提取器几乎没有改变。

    

    类增量学习的主要目标是在稳定性和可塑性之间取得平衡，即模型应该既稳定到足以保留从先前看到的类中学到的知识，又应该具备足够的可塑性，可以学习新的类别。本文旨在探讨最近的类增量学习算法如何有效地解决稳定性-可塑性权衡问题。我们建立了度量特征表示的稳定性和可塑性的分析工具，并利用这些工具来研究各种算法在大规模的类增量基准测试中训练的模型。令人惊讶的是，我们发现大部分类增量学习算法都非常倾向于稳定性而不是可塑性，以至于训练在初始类上的模型的特征提取器几乎没有改变。

    A primary goal of class-incremental learning is to strike a balance between stability and plasticity, where models should be both stable enough to retain knowledge learned from previously seen classes, and plastic enough to learn concepts from new classes. While previous works demonstrate strong performance on class-incremental benchmarks, it is not clear whether their success comes from the models being stable, plastic, or a mixture of both. This paper aims to shed light on how effectively recent class-incremental learning algorithms address the stability-plasticity trade-off. We establish analytical tools that measure the stability and plasticity of feature representations, and employ such tools to investigate models trained with various algorithms on large-scale class-incremental benchmarks. Surprisingly, we find that the majority of class-incremental learning algorithms heavily favor stability over plasticity, to the extent that the feature extractor of a model trained on the initi
    
[^39]: GC-EI-MS光谱中小分子的新型异构体鉴定方法

    De-novo Identification of Small Molecules from Their GC-EI-MS Spectra. (arXiv:2304.01634v1 [physics.data-an])

    [http://arxiv.org/abs/2304.01634](http://arxiv.org/abs/2304.01634)

    该论文提出了一种针对GC-EI-MS光谱的新型\emph{de-novo}方法，能够直接从质谱数据推导小分子的结构，克服了当前可靠的光谱数据库无法覆盖足够密集潜在化学空间的难题。

    

    针对当前可靠的光谱数据库无法覆盖足够密集的潜在化学空间而带来的挑战，基于机器学习的新型\emph{de-novo}方法已受到关注直接从质谱数据推导分子结构。我们提出了一种新颖的方法来解决GC-EI-MS光谱的特定用例，由于缺乏先前已发表方法所依赖的MS / MS实验的额外信息，因此该用例尤其困难。我们分析了我们的方法的优点和缺点，并讨论了未来的方向。

    Identification of experimentally acquired mass spectra of unknown compounds presents a~particular challenge because reliable spectral databases do not cover the potential chemical space with sufficient density. Therefore machine learning based \emph{de-novo} methods, which derive molecular structure directly from its mass spectrum gained attention recently. We present a~novel method in this family, addressing a~specific usecase of GC-EI-MS spectra, which is particularly hard due to lack of additional information from the first stage of MS/MS experiments, on which the previously published methods rely. We analyze strengths and drawbacks or our approach and discuss future directions.
    
[^40]: 多孔晶态材料的等变网络

    Equivariant Networks for Porous Crystalline Materials. (arXiv:2304.01628v1 [cs.LG])

    [http://arxiv.org/abs/2304.01628](http://arxiv.org/abs/2304.01628)

    本研究开发了一种模型，它在架构中合并了晶体的单元格对称性，并显式地建模了多孔结构，可更准确地预测多孔晶体材料的吸附热。

    

    高效地预测多孔晶体材料的性质具有加速开发新材料的高通量筛选过程的巨大潜力，因为使用第一原理模型进行的模拟往往是计算密集型的。为了有效地利用深度学习方法来建模这些材料，我们需要利用晶体中存在的对称性，这些对称性由它们的空间群定义。现有的晶体性质预测方法要么具有过于严格的对称性限制，要么仅包括单元格之间的对称性。此外，这些模型没有明确地建模晶体的多孔结构。在本文中，我们开发了一种模型，它在其架构中合并了晶体的单元格对称性，并显式地建模了多孔结构。我们通过预测不同构型的莫尔定石沸石的CO$_2$吸附热来评估我们的模型。我们的结果证实，我们的方法在准确性和效率方面优于现有的晶体性质预测模型。

    Efficiently predicting properties of porous crystalline materials has great potential to accelerate the high throughput screening process for developing new materials, as simulations carried out using first principles model are often computationally expensive. To effectively make use of Deep Learning methods to model these materials, we need to utilize the symmetries present in the crystals, which are defined by their space group. Existing methods for crystal property prediction either have symmetry constraints that are too restrictive or only incorporate symmetries between unit cells. In addition, these models do not explicitly model the porous structure of the crystal. In this paper, we develop a model which incorporates the symmetries of the unit cell of a crystal in its architecture and explicitly models the porous structure. We evaluate our model by predicting the heat of adsorption of CO$_2$ for different configurations of the mordenite zeolite. Our results confirm that our metho
    
[^41]: 基于多编码器的最大强度投影自适应硬注意力网络的CT扫描肺结节分割 MESAHA-Net（arXiv：2304.01576v1 [eess.IV]）

    MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan. (arXiv:2304.01576v1 [eess.IV])

    [http://arxiv.org/abs/2304.01576](http://arxiv.org/abs/2304.01576)

    本文提出了一种名为MESAHA-Net的高效端到端框架，集成了三种类型的输入，通过采用自适应硬注意力机制，逐层2D分割，实现了 CT扫描中精确的肺结节分割。

    

    准确的肺结节分割对早期肺癌诊断非常重要，因为它可以大大提高患者的生存率。计算机断层扫描（CT）图像被广泛用于肺结节分析的早期诊断。然而，肺结节的异质性，大小多样性以及周围环境的复杂性对开发鲁棒的结节分割方法提出了挑战。在本研究中，我们提出了一个高效的端到端框架，即基于多编码器的自适应硬注意力网络（MESAHA-Net），用于CT扫描中精确的肺结节分割。MESAHA-Net包括三个编码路径，一个注意力块和一个解码器块，有助于集成三种类型的输入：CT切片补丁，前向和后向的最大强度投影（MIP）图像以及包含结节的感兴趣区域（ROI）掩码。通过采用新颖的自适应硬注意力机制，MESAHA-Net逐层执行逐层2D分割。

    Accurate lung nodule segmentation is crucial for early-stage lung cancer diagnosis, as it can substantially enhance patient survival rates. Computed tomography (CT) images are widely employed for early diagnosis in lung nodule analysis. However, the heterogeneity of lung nodules, size diversity, and the complexity of the surrounding environment pose challenges for developing robust nodule segmentation methods. In this study, we propose an efficient end-to-end framework, the multi-encoder-based self-adaptive hard attention network (MESAHA-Net), for precise lung nodule segmentation in CT scans. MESAHA-Net comprises three encoding paths, an attention block, and a decoder block, facilitating the integration of three types of inputs: CT slice patches, forward and backward maximum intensity projection (MIP) images, and region of interest (ROI) masks encompassing the nodule. By employing a novel adaptive hard attention mechanism, MESAHA-Net iteratively performs slice-by-slice 2D segmentation 
    
[^42]: 图神经网络中池化的表达能力

    The expressive power of pooling in Graph Neural Networks. (arXiv:2304.01575v1 [cs.LG])

    [http://arxiv.org/abs/2304.01575](http://arxiv.org/abs/2304.01575)

    本文研究了池化算子在图神经网络中的表达能力，并提供了一个通用标准来选择或设计池化算子。

    

    在图神经网络（GNNs）中，分层池化算子通过创建图结构和其顶点特征的本地摘要来生成输入数据的更粗糙的表示。虽然已经致力于研究GNN中消息传递（MP）层的表达能力，但缺乏关于池化算子如何影响GNN表达能力的研究。此外，尽管最近在有效池化算子的设计方面取得了进展，但没有一个原则性的标准来比较它们。我们的工作旨在通过提供足够的条件使池化算子在其之前的MP层中完全保留表达能力来填补这一空白。这些条件作为选择现有池化算子或设计新的池化算子的通用和理论基础的标准。基于我们的理论发现，我们审查了几个现有的池化算子，并确定了那些不能满足表达性假设的算子。

    In Graph Neural Networks (GNNs), hierarchical pooling operators generate a coarser representation of the input data by creating local summaries of the graph structure and its vertex features. Considerable attention has been devoted to studying the expressive power of message-passing (MP) layers in GNNs, while a study on how pooling operators affect the expressivity of a GNN is still lacking. Additionally, despite the recent advances in the design of effective pooling operators, there is not a principled criterion to compare them. Our work aims to fill this gap by providing sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically-grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we reviewed several existing pooling operators and identified those that fail to satisfy the expressiveness assumptions. Finally,
    
[^43]: 时空语义零膨胀城市异常预测

    Spatiotemporal and Semantic Zero-inflated Urban Anomaly Prediction. (arXiv:2304.01569v1 [cs.LG])

    [http://arxiv.org/abs/2304.01569](http://arxiv.org/abs/2304.01569)

    提出了STS模型来预测城市异常，解决了由于异常数据稀疏零膨胀导致的问题，并且可以统一预测多种异常，在交通事故和犯罪预测数据集上表现良好。

    

    城市异常预测，如交通事故预测和犯罪预测，对智慧城市的安全和维护至关重要。现有的方法通常使用深度学习来捕捉空间和时间维度内的内部依赖关系。然而，仍存在许多关键挑战，例如，由于城市异常发生频率低导致的稀疏零膨胀数据（可能会在真实世界的数据集上表现不佳），以及跨越空间，时间和语义维度的异常模式的内部和互间依赖性。此外，还需要探索统一的方法来预测多种异常。在本文中，我们提出了STS来共同捕捉三个维度内的模式和影响因素之间的内部和互间依赖性。此外，我们使用一个带有定制损失函数的多任务预测模块来解决零膨胀问题。为了验证模型的有效性，我们将其应用于两个城市异常数据集：交通事故预测和犯罪预测。实验结果表明，我们的方法优于一系列基线模型。

    Urban anomaly predictions, such as traffic accident prediction and crime prediction, are of vital importance to smart city security and maintenance. Existing methods typically use deep learning to capture the intra-dependencies in spatial and temporal dimensions. However, numerous key challenges remain unsolved, for instance, sparse zero-inflated data due to urban anomalies occurring with low frequency (which can lead to poor performance on real-world datasets), and both intra- and inter-dependencies of abnormal patterns across spatial, temporal, and semantic dimensions. Moreover, a unified approach to predict multiple kinds of anomaly is left to explore. In this paper, we propose STS to jointly capture the intra- and inter-dependencies between the patterns and the influential factors in three dimensions. Further, we use a multi-task prediction module with a customized loss function to solve the zero-inflated issue. To verify the effectiveness of the model, we apply it to two urban ano
    
[^44]: 基于超轻量级二值神经网络的心律失常分类器

    Arrhythmia Classifier Based on Ultra-Lightweight Binary Neural Network. (arXiv:2304.01568v1 [eess.SP])

    [http://arxiv.org/abs/2304.01568](http://arxiv.org/abs/2304.01568)

    本研究提出了一种能够通过ECG信号进行5类和17类心律失常分类的超轻量级二值神经网络(BNN)，在存储使用率最低的情况下，分别达到了96.90%和97.50%的准确率。

    

    合理有效地通过心电图(ECG)信号监测心律失常对人类健康具有重要意义。随着深度学习的发展，出现了许多基于深度学习的ECG分类算法。但是，大多数现有算法在高精度和复杂模型之间进行权衡，导致存储使用率和功耗很高。这也不可避免地增加了在资源有限的可穿戴人工智能物联网设备上实现的难度。本研究提出了一种通用的超轻量级二值神经网络(BNN)，能够通过ECG信号进行5类和17类心律失常分类。我们的BNN在5类和17类分类中分别达到了96.90% (完全精度97.09%)和97.50% (完全精度98.00%)的准确率，存储使用率最低(3.76 KB和4.45 KB)。与其他二值化作品相比，我们的方法在支持两个多分类方面表现出色。

    Reasonably and effectively monitoring arrhythmias through ECG signals has significant implications for human health. With the development of deep learning, numerous ECG classification algorithms based on deep learning have emerged. However, most existing algorithms trade off high accuracy for complex models, resulting in high storage usage and power consumption. This also inevitably increases the difficulty of implementation on wearable Artificial Intelligence-of-Things (AIoT) devices with limited resources. In this study, we proposed a universally applicable ultra-lightweight binary neural network(BNN) that is capable of 5-class and 17-class arrhythmia classification based on ECG signals. Our BNN achieves 96.90% (full precision 97.09%) and 97.50% (full precision 98.00%) accuracy for 5-class and 17-class classification, respectively, with state-of-the-art storage usage (3.76 KB and 4.45 KB). Compared to other binarization works, our approach excels in supporting two multi-classificatio
    
[^45]: 图扩散模型综述：分子、蛋白质和材料科学中的生成型 AI

    A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material. (arXiv:2304.01565v1 [cs.LG])

    [http://arxiv.org/abs/2304.01565](http://arxiv.org/abs/2304.01565)

    该论文为图扩散模型在生成科学中AI内容（AIGC）领域的综述。该模型已成为各领域生成建模的新趋势，涵盖了分子、蛋白质和材料科学等多个领域。

    

    扩散模型已成为各个领域新的 SOTA 生成建模方法，有多篇综述文献为此进行了总体概述。随着扩散模型文章数量在过去几年中呈指数增长，对特定领域的扩散模型进行综述的需求也在增加。在这项工作中，我们致力于对图扩散模型进行调查。虽然我们的重点是涵盖图中扩散模型的进展，但我们首先简要总结了其他生成建模方法在图中的应用。之后，我们介绍了各种形式的扩散模型机制，这有助于讨论图扩散模型。图扩散模型的应用主要归为科学中的生成型 AI 内容（AIGC）类别，我们主要关注图扩散模型如何用于生成分子和蛋白质，但也覆盖其他案例，包括材料设计。

    Diffusion models have become a new SOTA generative modeling method in various fields, for which there are multiple survey works that provide an overall survey. With the number of articles on diffusion models increasing exponentially in the past few years, there is an increasing need for surveys of diffusion models on specific fields. In this work, we are committed to conducting a survey on the graph diffusion models. Even though our focus is to cover the progress of diffusion models in graphs, we first briefly summarize how other generative modeling methods are used for graphs. After that, we introduce the mechanism of diffusion models in various forms, which facilitates the discussion on the graph diffusion models. The applications of graph diffusion models mainly fall into the category of AI-generated content (AIGC) in science, for which we mainly focus on how graph diffusion models are utilized for generating molecules and proteins but also cover other cases, including materials des
    
[^46]: Shallow ReLU$^k$神经网络的逼近速率及其在非参数回归中的应用

    Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression. (arXiv:2304.01561v1 [stat.ML])

    [http://arxiv.org/abs/2304.01561](http://arxiv.org/abs/2304.01561)

    本文研究了Shallow ReLU$^k$神经网络的逼近容量，证明了其最优逼近速率。应用到非参数回归上，证明了浅层神经网络可以实现学习H\"older函数的最优渐进速率，补充了深层神经网络的最近结果。

    

    本研究探讨与Shallow ReLU$^k$神经网络相关的变异空间的逼近容量。结果表明，在有限变异范数下，容纳了足够平滑的函数。对于较少平滑的函数，根据变异范数建立了逼近速率。运用这些结果，我们可以证明Shallow ReLU$^k$神经网络的最优逼近速率。同时阐明了这些结果如何用于推导深层神经网络和卷积神经网络(CNNs)的逼近界限。为应用研究，我们使用了三种ReLU神经网络模型：浅层神经网络，超参数神经网络和CNN进行非参数回归收敛速率研究。特别地，我们展示了浅层神经网络可以实现学习H\"older函数的最优渐进速率，这补充了深层神经网络的最近结果。

    We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\"older functions, which complements recent results for deep neural networks. It is also proven th
    
[^47]: 边缘AI设备上的实时驾驶员监控系统

    Real-time Driver Monitoring Systems on Edge AI Device. (arXiv:2304.01555v1 [cs.CV])

    [http://arxiv.org/abs/2304.01555](http://arxiv.org/abs/2304.01555)

    本论文介绍了一种运行在边缘AI设备上的实时驾驶员监控系统，该系统经过模型手术，在硬件加速器的帮助下实现了高帧率的处理效果。

    

    随着司机不注意力导致路上事故的增加，自动化驾驶员监控系统(DMS)得到了越来越多的认可。在本报告中，我们介绍了一个运行于硬件加速器的边缘设备上的实时DMS系统。该系统由红外摄像头记录驾驶员的画面和边缘设备处理数据组成。为了在边缘设备上成功移植深度学习模型，充分利用硬件加速器，进行了模型手术。最终的DMS系统在TI-TDA4VM边缘设备上实现了63帧每秒(FPS)的效果。

    As road accident cases are increasing due to the inattention of the driver, automated driver monitoring systems (DMS) have gained an increase in acceptance. In this report, we present a real-time DMS system that runs on a hardware-accelerator-based edge device. The system consists of an InfraRed camera to record the driver footage and an edge device to process the data. To successfully port the deep learning models to run on the edge device taking full advantage of the hardware accelerators, model surgery was performed. The final DMS system achieves 63 frames per second (FPS) on the TI-TDA4VM edge device.
    
[^48]: 太阳大气的加热和动力学

    Heating and dynamics of the Solar atmosphere. (arXiv:2304.01553v1 [astro-ph.SR])

    [http://arxiv.org/abs/2304.01553](http://arxiv.org/abs/2304.01553)

    该论文研究太阳大气中不同结构的太阳风源区、形成高度以及加热机制，揭示太阳风与日冕空洞之间的相关性，并提供了预测和警告近地球空间天气的方法。

    

    太阳大气从5500 K的光球到百万度开尔文的日冕都呈现出异常的温度变化。日冕本身向星际介质扩展为流动的太阳风，影响着近地球空间天气。不同日冕结构的精确源区、其形成高度以及太阳大气的加热在天体物理学中是密不可分且未解决的问题。观察结果表明，冷却、强度缺陷结构的日冕空洞(Coronal Holes, CHs)与太阳风中的结构之间存在相关性。观测结果还表明，通过幂律分布的脉冲事件在日冕中局部加热等。本文使用狭带光度计、光谱学、以及从近紫外线到X射线的太阳大气的盘际辐射，结合现场太阳风测量，来理解（i）不同结构的太阳风源区、（ii）它们在哪里形成、（iii）日冕中的加热是如何发生的，以及（iv）如何为近地空间天气提供预测和预警。

    The solar atmosphere shows anomalous variation in temperature, starting from the 5500 K photosphere to the million-degree Kelvin corona. The corona itself expands into the interstellar medium as the free streaming solar wind, which modulates and impacts the near-Earth space weather. The precise source regions of different structures in the solar wind, their formation height, and the heating of the solar atmosphere are inextricably linked and unsolved problems in astrophysics. Observations suggest correlations between Coronal holes (CHs), which are cool, intensity deficit structures in the solar corona, with structures in the solar wind. Observations also suggest the local plasma heating in the corona through power-law distributed impulsive events. In this thesis, we use narrowband photometric, spectroscopic, and disc-integrated emission of the solar atmosphere ranging from Near Ultraviolet to X-rays along with in-situ solar wind measurements to understand (i). the source regions of the
    
[^49]: 区域风力特征如何影响基于CNN的风速预测：来自时空相关性分析的见解。

    How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis. (arXiv:2304.01545v1 [cs.LG])

    [http://arxiv.org/abs/2304.01545](http://arxiv.org/abs/2304.01545)

    本研究探讨了使用3D卷积神经网络（3D-CNN）对时空数据进行风速预测的精度，并发现使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。

    

    本研究探讨了时空数据维度对利用人工神经网络构建的风速预测模型精度的影响。尽管以前的研究表明，加入空间数据可以提高风速预测模型的精度，但很少有研究探讨了基于神经网络的预测模型中不同空间尺度改进的程度。此外，对于这些模型的最佳时间长度的输入数据的研究也很有限。为了解决这个问题，本研究在使用3D卷积神经网络（3D-CNN）预测风速时，采用具有不同时空维度的数据作为输入，并评估其预测性能。结果表明，使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。此外，多时间数据对预测性能的影响更为显著。

    This study investigates the impact of spatiotemporal data dimensions on the precision of a wind forecasting model developed using an artificial neural network. Although previous studies have shown that incorporating spatial data can enhance the accuracy of wind forecasting models, few investigations have explored the extent of the improvement owing to different spatial scales in neural network-based predictive models. Additionally, there are limited studies on the optimal temporal length of the input data for these models. To address this gap, this study employs data with various spatiotemporal dimensions as inputs when forecasting wind using 3D-Convolutional Neural Networks (3D-CNN) and assesses their predictive performance. The results indicate that using spatial data of the surrounding area for 3D-CNN training can achieve better predictive performance than using only single-point information. Additionally, multi-time data had a more positive effect on the predictive performance than
    
[^50]: 通过压缩实现隐私放大：在分布式均值估计中实现最优隐私-精度-通信权衡

    Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation. (arXiv:2304.01541v1 [stat.ML])

    [http://arxiv.org/abs/2304.01541](http://arxiv.org/abs/2304.01541)

    本论文研究了在通信和差分隐私约束下，平均值和频率估计的最优准确性，证明每个客户端只需发送$\Theta\left( n \min\left(\varepsilon, \varepsilon^2\right)\right)$比特的FL问题和$\Theta\left(\log\left( n\min\left(\varepsilon, \varepsilon^2\right) \right)\right)$比特的FA问题即可实现最优误差，从而在联合学习和分析中实现了隐私、精确性和通信的最优权衡。

    

    隐私和通信约束是联合学习（FL）和分析（FA）中的两个主要瓶颈。我们研究了在联合通信和$(\varepsilon, \delta)$-差分隐私（DP）约束下平均值和频率估计（FL和FA的标准模型）的最优准确性。我们展示了为了在$(\varepsilon, \delta)$-DP下达到最优误差，每个客户端只需要向服务器发送$\Theta\left( n \min\left(\varepsilon, \varepsilon^2\right)\right)$比特的FL问题和$\Theta\left(\log\left( n\min\left(\varepsilon, \varepsilon^2\right) \right)\right)$比特的FA问题。如果没有压缩，每个客户机需要$O(d)$比特和$\log d$比特来解决平均值估计和频率估计问题（其中$d$对应于FL中可训练参数的数量或FA中域的大小），这意味着我们可以获得在$n\min\left(\varepsilon,\varepsilon^2\right)$的区间中获得显著的节省。

    Privacy and communication constraints are two major bottlenecks in federated learning (FL) and analytics (FA). We study the optimal accuracy of mean and frequency estimation (canonical models for FL and FA respectively) under joint communication and $(\varepsilon, \delta)$-differential privacy (DP) constraints. We show that in order to achieve the optimal error under $(\varepsilon, \delta)$-DP, it is sufficient for each client to send $\Theta\left( n \min\left(\varepsilon, \varepsilon^2\right)\right)$ bits for FL and $\Theta\left(\log\left( n\min\left(\varepsilon, \varepsilon^2\right) \right)\right)$ bits for FA to the server, where $n$ is the number of participating clients. Without compression, each client needs $O(d)$ bits and $\log d$ bits for the mean and frequency estimation problems respectively (where $d$ corresponds to the number of trainable parameters in FL or the domain size in FA), which means that we can get significant savings in the regime $ n \min\left(\varepsilon, \va
    
[^51]: 带对手的在线学习：微分包容分析

    Online Learning with Adversaries: A Differential Inclusion Analysis. (arXiv:2304.01525v1 [cs.LG])

    [http://arxiv.org/abs/2304.01525](http://arxiv.org/abs/2304.01525)

    本文提出了第一个能够以几乎确定的方式收敛到 $\mu$ 的异步在线算法，应用了微分包容分析，并提供了两个关键亮点。

    

    我们考虑测量模型 $Y = AX$，其中 $X$ 和 $Y$ 是随机变量，$A$ 是先验已知的高矩阵。在每个时间实例，可以获得 $Y$ 的一个坐标的样本，并且目标是通过这些样本估计 $\mu := \mathbb{E}[X]$。然而，挑战在于：小但未知的 $Y$ 的坐标子集由对手控制，并具有无限的能力：每次查询样本时，他们可以返回任何实数。对于这种对抗性环境，我们提出了第一个能够以几乎确定的方式收敛到 $\mu$ 的异步在线算法。我们使用一种新颖的微分包容基于两个时间尺度的分析来证明这个结果。我们证明的两个关键亮点包括：(a) 使用一种新颖的 Lyapunov 函数来证明 $\mu$ 是我们算法极限动态的唯一全局吸引子，(b) 使用鞅和停时理论来证明我们算法的迭代几乎必定有界。

    We consider the measurement model $Y = AX,$ where $X$ and, hence, $Y$ are random variables and $A$ is an a priori known tall matrix. At each time instance, a sample of one of $Y$'s coordinates is available, and the goal is to estimate $\mu := \mathbb{E}[X]$ via these samples. However, the challenge is that a small but unknown subset of $Y$'s coordinates are controlled by adversaries with infinite power: they can return any real number each time they are queried for a sample. For such an adversarial setting, we propose the first asynchronous online algorithm that converges to $\mu$ almost surely. We prove this result using a novel differential inclusion based two-timescale analysis. Two key highlights of our proof include: (a) the use of a novel Lyapunov function for showing that $\mu$ is the unique global attractor for our algorithm's limiting dynamics, and (b) the use of martingale and stopping time theory to show that our algorithm's iterates are almost surely bounded.
    
[^52]: 多模态神经过程用于不确定性估计

    Multimodal Neural Processes for Uncertainty Estimation. (arXiv:2304.01518v1 [cs.LG])

    [http://arxiv.org/abs/2304.01518](http://arxiv.org/abs/2304.01518)

    本论文提出了一种新的神经过程模型，即多模态神经过程，用于对多模态数据进行不确定性估计，该模型具有动态上下文记忆、多模态贝叶斯聚合和校准预测的注意机制，经实验表明在多模态不确定性估计方面性能最先进，对于噪声样本具有良好抵抗能力，并且对于领域之外的检测是可靠的。

    

    神经过程( Neural Processes, NPs)将参数化深度神经网络的表示能力和非参数高斯过程可靠的不确定性估计结合在了一起。虽然最近NPs的发展已经在回归和分类方面取得了成功，但是如何将NPs适应多模态数据尚未受到仔细的研究。我们首次提出了一种新的NP家族模型，用于多模态不确定性估计，即多模态神经过程。我们通过一种整体的、基于原则的方法，开发了一个由分类误差更新的动态上下文记忆，一个聚合多模态表示的多模态贝叶斯聚合机制，以及一个用于校准预测的新的注意机制。在广泛的经验评估中，我们的方法实现了最先进的多模态不确定性估计性能，展示了它的吸引力，即能够抵抗噪声样本的干扰，并可靠地在领域之外进行检测。

    Neural processes (NPs) have brought the representation power of parametric deep neural networks and the reliable uncertainty estimation of non-parametric Gaussian processes together. Although recent development of NPs has shown success in both regression and classification, how to adapt NPs to multimodal data has not be carefully studied. For the first time, we propose a new model of NP family for multimodal uncertainty estimation, namely Multimodal Neural Processes. In a holistic and principled way, we develop a dynamic context memory updated by the classification error, a multimodal Bayesian aggregation mechanism to aggregate multimodal representations, and a new attention mechanism for calibrated predictions. In extensive empirical evaluation, our method achieves the state-of-the-art multimodal uncertainty estimation performance, showing its appealing ability of being robust against noisy samples and reliable in out-of-domain detection.
    
[^53]: 全球时间序列预测中的概念漂移处理

    Handling Concept Drift in Global Time Series Forecasting. (arXiv:2304.01512v1 [cs.LG])

    [http://arxiv.org/abs/2304.01512](http://arxiv.org/abs/2304.01512)

    本文提出两种新的概念漂移处理方法，应用于全球时间序列预测，填补了处理分类领域中概念漂移方法的空白。

    

    基于机器学习的时间序列预测模型通常需要并假设数据在产生预测时具有一定程度的平稳性。然而，在许多实际情况下，数据分布不是稳态的，而它们随着时间的推移而改变，从而降低了预测模型的准确性，这在机器学习文献中被称为概念漂移。处理预测中的概念漂移对于许多现今使用的机器学习模型至关重要。本文探讨了用于全球预测模型（GFM）中处理概念漂移的方法，并提出了两种新的概念漂移处理方法：误差贡献加权（ECW）和梯度下降加权（GDW），基于连续的自适应权重概念使用两个单独训练的预测模型。

    Machine learning (ML) based time series forecasting models often require and assume certain degrees of stationarity in the data when producing forecasts. However, in many real-world situations, the data distributions are not stationary and they can change over time while reducing the accuracy of the forecasting models, which in the ML literature is known as concept drift. Handling concept drift in forecasting is essential for many ML methods in use nowadays, however, the prior work only proposes methods to handle concept drift in the classification domain. To fill this gap, we explore concept drift handling methods in particular for Global Forecasting Models (GFM) which recently have gained popularity in the forecasting domain. We propose two new concept drift handling methods, namely: Error Contribution Weighting (ECW) and Gradient Descent Weighting (GDW), based on a continuous adaptive weighting concept. These methods use two forecasting models which are separately trained with the m
    
[^54]: EPVT: 基于环境感知的提示视觉Transformer在皮肤病变识别领域一般化中的应用

    EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition. (arXiv:2304.01508v1 [cs.CV])

    [http://arxiv.org/abs/2304.01508](http://arxiv.org/abs/2304.01508)

    EPVT是一种基于环境感知的提示视觉Transformer，用于解决皮肤病变识别中深度神经网络可能过度依赖疾病不相关图像特征的问题，通过嵌入一组领域提示和一个共享提示来进行领域一般化，并且引入了领域提示生成器促进知识共享。

    

    利用深度学习进行皮肤病变识别已取得重大进展，而在现实世界场景中部署这些系统的需求不断增加。然而，最近的研究表明，用于皮肤病变识别的深度神经网络可能过度依赖于与疾病不相关的图像特征（如暗角、浓密毛发），导致在看不见的环境中表现不佳。为了解决这个问题，我们提出了一种新颖的领域一般化方法——EPVT，它将提示嵌入到Vision Transformer中，以协同学习来自不同领域的知识。具体而言，EPVT利用一组领域提示，每个领域提示都扮演领域专家的角色，以捕获领域特定的知识；以及一个共享提示来获得整个数据集的通用知识。为了促进知识共享和不同提示之间的交互，我们引入了一个领域提示生成器，它使得领域提示与共享提示之间可以进行低秩乘性更新。

    Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A
    
[^55]: RARE：鲁棒性抗干扰的掩码图自编码器

    RARE: Robust Masked Graph Autoencoder. (arXiv:2304.01507v1 [cs.LG])

    [http://arxiv.org/abs/2304.01507](http://arxiv.org/abs/2304.01507)

    RARE是一种鲁棒性抗干扰的掩码图自编码器，通过在高阶潜在特征空间中进行掩码和重构节点样本来提高推断掩码数据的确定性和自监督机制的可靠性，并在下游任务中优于现有的SGP方法。

    

    掩码图自编码器（MGAE）由于其简单和有效的特性，在自监督图预训练（SGP）方面已成为一种很有前途的范例。然而，现有的方法在原始数据空间中执行掩码-重构操作，类似于计算机视觉（CV）和自然语言处理（NLP）领域，而忽略了图数据的重要非欧几里得属性。结果，高度不稳定的局部连接结构大大增加了推断掩码数据的不确定性，并降低了利用自监督信号的可靠性，导致下游评估中的表示效果不佳。为了解决这个问题，我们提出了一种新的SGP方法，称为Robust mAsked gRaph autoEncoder（RARE），通过高阶潜在特征空间中更多的掩码和重构节点样本来提高推断掩码数据的确定性和自监督机制的可靠性。通过理论和实证分析，我们发现RARE能够有效地捕捉图数据的内在结构，并在不同的下游任务中优于现有的SGP方法。

    Masked graph autoencoder (MGAE) has emerged as a promising self-supervised graph pre-training (SGP) paradigm due to its simplicity and effectiveness. However, existing efforts perform the mask-then-reconstruct operation in the raw data space as is done in computer vision (CV) and natural language processing (NLP) areas, while neglecting the important non-Euclidean property of graph data. As a result, the highly unstable local connection structures largely increase the uncertainty in inferring masked data and decrease the reliability of the exploited self-supervision signals, leading to inferior representations for downstream evaluations. To address this issue, we propose a novel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve the certainty in inferring masked data and the reliability of the self-supervision mechanism by further masking and reconstructing node samples in the high-order latent feature space. Through both theoretical and empirical analyses, we have dis
    
[^56]: OneShotSTL：一种单次季节趋势分解方法，用于在线时间序列异常检测和预测

    OneShotSTL: One-Shot Seasonal-Trend Decomposition For Online Time Series Anomaly Detection And Forecasting. (arXiv:2304.01506v1 [cs.LG])

    [http://arxiv.org/abs/2304.01506](http://arxiv.org/abs/2304.01506)

    OneShotSTL提出了一种高效准确的算法，用于在线时间序列分解，在处理时间上仅需O(1)的更新时间复杂度，并可同时保持较高的精度，解决了现有批处理方法无法支持实时分析的挑战。

    

    季节趋势分解是时间序列分析中最基本的概念之一，它支持包括时间序列异常检测和预测在内的各种下游任务。然而，现有的分解方法依赖于批处理，时间复杂度为O(W)，其中W是时间窗口内的数据点数。因此，它们不能始终有效地支持需要低处理延迟的实时分析。为了解决这一挑战，我们提出了OneShotSTL，这是一种高效和准确的算法，可以在线上对时间序列进行分解，更新时间复杂度为O(1)。OneShotSTL比批处理方法快10倍以上，精度与最佳对手相当。广泛的实验在下游时间序列异常检测和预测任务的真实基准数据集上表明，OneShotSTL比现有技术快10倍以上，同时仍提供相当甚至更好的精度。

    Seasonal-trend decomposition is one of the most fundamental concepts in time series analysis that supports various downstream tasks, including time series anomaly detection and forecasting. However, existing decomposition methods rely on batch processing with a time complexity of O(W), where W is the number of data points within a time window. Therefore, they cannot always efficiently support real-time analysis that demands low processing delay. To address this challenge, we propose OneShotSTL, an efficient and accurate algorithm that can decompose time series online with an update time complexity of O(1). OneShotSTL is more than $1,000$ times faster than the batch methods, with accuracy comparable to the best counterparts. Extensive experiments on real-world benchmark datasets for downstream time series anomaly detection and forecasting tasks demonstrate that OneShotSTL is from 10 to over 1,000 times faster than the state-of-the-art methods, while still providing comparable or even be
    
[^57]: SLPerf：基准测试共享学习的统一框架

    SLPerf: a Unified Framework for Benchmarking Split Learning. (arXiv:2304.01502v1 [cs.LG])

    [http://arxiv.org/abs/2304.01502](http://arxiv.org/abs/2304.01502)

    SLPerf是一个统一的研究和开放式研究库，用于共享学习，通过对不同情况下不同共享学习范式的基准比较，提供了改进共享学习范式的见解。

    

    随着数据隐私问题的日益严重，分布式数据的中央化训练变得不可行，需要协作学习框架。为了解决这个问题，出现了两种主要的框架：联邦学习（FL）和分割学习（SL）。虽然FL已经建立了各种基准框架和研究库，但SL目前尚缺乏统一的库，尤其是在标签共享、模型聚合和切割层选择方面的多样性。这种标准化缺乏使得比较SL范式变得困难。为了解决这个问题，我们提出了SLPerf，这是一个统一的共享学习研究框架和开放式研究库，并在四个广泛使用的数据集上进行了大量实验，这些数据集涵盖了IID和非IID数据的情况。我们的贡献包括对最近提出的SL范式的全面调查，不同SL范式在不同情况下进行详细的基准比较，以及用于改进SL范式的丰富工程带走信息和研究见解。

    Data privacy concerns has made centralized training of data, which is scattered across silos, infeasible, leading to the need for collaborative learning frameworks. To address that, two prominent frameworks emerged, i.e., federated learning (FL) and split learning (SL). While FL has established various benchmark frameworks and research libraries, SL currently lacks a unified library despite its diversity in terms of label sharing, model aggregation, and cut layer choice. This lack of standardization makes comparing SL paradigms difficult. To address this, we propose SLPerf, a unified research framework and open research library for SL, and conduct extensive experiments on four widely-used datasets under both IID and Non-IID data settings. Our contributions include a comprehensive survey of recently proposed SL paradigms, a detailed benchmark comparison of different SL paradigms in different situations, and rich engineering take-away messages and research insights for improving SL parad
    
[^58]: 物理感知的粗糙度优化用于衍射光学神经网络

    Physics-aware Roughness Optimization for Diffractive Optical Neural Networks. (arXiv:2304.01500v1 [cs.LG])

    [http://arxiv.org/abs/2304.01500](http://arxiv.org/abs/2304.01500)

    本研究提出了一种物理感知的衍射光学神经网络训练框架，在训练过程中引入粗糙度建模正则化和物理感知的稀疏化方法，以提高DONN的预测精度并提供实际部署的可行解决方案。

    

    衍射光学神经网络（DONN）作为一种下一代设备/电路技术已显示出比传统深度神经网络更快的计算速度（光速）和低能耗的优势。然而，由于衍射层内的像素间相互作用，在DONN的数值建模和物理光学器件部署之间存在显著的预测准确性损失。本文提出了一种物理感知的衍射光学神经网络训练框架，以减少数值模拟和实际部署之间的性能差异。具体而言，我们在训练过程中提出了粗糙度建模正则化，并集成了物理感知的稀疏化方法，以引入稀疏性到相位掩模中，以减少衍射层中相邻像素之间的尖锐相位变化。我们进一步开发了$2\pi$周期性优化方法，以考虑物理衍射过程并降低相位掩膜的粗糙度。所提出的方法有效地提高了DONN的预测精度，并为DONN的实际部署提供了可行的解决方案。

    As a representative next-generation device/circuit technology beyond CMOS, diffractive optical neural networks (DONNs) have shown promising advantages over conventional deep neural networks due to extreme fast computation speed (light speed) and low energy consumption. However, there is a mismatch, i.e., significant prediction accuracy loss, between the DONN numerical modelling and physical optical device deployment, because of the interpixel interaction within the diffractive layers. In this work, we propose a physics-aware diffractive optical neural network training framework to reduce the performance difference between numerical modeling and practical deployment. Specifically, we propose the roughness modeling regularization in the training process and integrate the physics-aware sparsification method to introduce sparsity to the phase masks to reduce sharp phase changes between adjacent pixels in diffractive layers. We further develop $2\pi$ periodic optimization to reduce the roug
    
[^59]: 基于自动识别系统数据的轨迹关联的多模型LSTM架构

    Multi model LSTM architecture for Track Association based on Automatic Identification System Data. (arXiv:2304.01491v1 [cs.LG])

    [http://arxiv.org/abs/2304.01491](http://arxiv.org/abs/2304.01491)

    本文提出了基于LSTM的多模型框架来进行轨迹关联，能够更好的捕捉船只轨迹的不同模式和特征，提高了关联准确性和鲁棒性。

    

    几十年来，航迹关联一直是海洋监视中的一个具有挑战性的问题，涉及到对船只观测数据的识别和关联。然而，自动识别系统（AIS）通过提供大量的船只动态和地理空间信息，为研究人员提供了一个新的解决该问题的机会。随着如此大型数据库的可用性，研究人员现在可以开发复杂模型和算法，利用数据的增加，有效地解决轨迹关联挑战。此外，随着深度学习的出现，轨迹关联现在可以被视为是一个数据密集型的问题。在这项研究中，我们提出了一种基于长短期记忆（LSTM）的多模型框架来进行轨迹关联。 LSTM是一种递归神经网络结构，能够以顺序方式处理多元时间数据，从而预测船只的当前轨迹。我们提出的框架集成了多个模型，捕捉了船只轨迹的不同模式和特征，例如船只的方向，速度和位置，从而提高了轨迹关联的准确性和鲁棒性。我们在实际的AIS数据集上评估了我们的方法，并在准确性和效率方面取得了良好的结果。

    For decades, track association has been a challenging problem in marine surveillance, which involves the identification and association of vessel observations over time. However, the Automatic Identification System (AIS) has provided a new opportunity for researchers to tackle this problem by offering a large database of dynamic and geo-spatial information of marine vessels. With the availability of such large databases, researchers can now develop sophisticated models and algorithms that leverage the increased availability of data to address the track association challenge effectively. Furthermore, with the advent of deep learning, track association can now be approached as a data-intensive problem. In this study, we propose a Long Short-Term Memory (LSTM) based multi-model framework for track association. LSTM is a recurrent neural network architecture that is capable of processing multivariate temporal data collected over time in a sequential manner, enabling it to predict current v
    
[^60]: 聊天GPT，还是不聊天GPT：这是一个问题！

    To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])

    [http://arxiv.org/abs/2304.01487](http://arxiv.org/abs/2304.01487)

    研究评估了聊天GPT检测中的最新技术和其他AI生成文本检测工具的表现，并提出区分人工生成和AI生成文本的重要性。

    

    聊天GPT已经成为一种全球感知。随着聊天GPT和其他大型语言模型（LLM）的出现，对于他们的误用的担忧也增加了，例如传播虚假消息，抄袭，操纵公众舆论，欺骗和欺诈。因此，区分人工生成和AI生成的文本变得越来越重要。研究人员提出了各种检测方法，从基本的二元分类器到更复杂的深度学习模型。一些检测技术依赖于统计特征或句法模式，而其他一些则包含语义或上下文信息以提高准确性。本研究的主要目标是对聊天GPT检测中最新技术进行全面和现代化的评估。此外，我们还评估了其他未专门声称检测聊天GPT生成内容的AI生成文本检测工具以评估它们在检测聊天GPT生成内容方面的表现。在我们的评估中，我们使用了一个包含人工编写和聊天GPT生成的文本的大型数据集。

    ChatGPT has become a global sensation. As ChatGPT and other Large Language Models (LLMs) emerge, concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud. Hence, distinguishing AI-generated from human-generated becomes increasingly essential. Researchers have proposed various detection methodologies, ranging from basic binary classifiers to more complex deep-learning models. Some detection techniques rely on statistical characteristics or syntactic patterns, while others incorporate semantic or contextual information to improve accuracy. The primary objective of this study is to provide a comprehensive and contemporary assessment of the most recent techniques in ChatGPT detection. Additionally, we evaluated other AI-generated text detection tools that do not specifically claim to detect ChatGPT-generated content to assess their performance in detecting ChatGPT-generated content. For our evaluation,
    
[^61]: 无需重新训练的Transformer模型分块压缩

    Blockwise Compression of Transformer-based Models without Retraining. (arXiv:2304.01483v1 [cs.CL])

    [http://arxiv.org/abs/2304.01483](http://arxiv.org/abs/2304.01483)

    本论文提出了一种名为BCT的分块压缩框架，可以对整个Transformer模型进行更细粒度的压缩，实现了降低部署门槛的目的。

    

    基于Transformer模型的GPT-3、ChatGPT和GPT-4近年来备受关注，但它们的巨大计算资源和存储开销仍然是不可避免的挑战。为了解决这个问题，我们提出了一种名为BCT的分块压缩框架，可以对整个Transformer模型进行更细粒度的压缩，包括嵌入、矩阵乘法、GELU、Softmax、层规范化以及所有中间结果。我们对一个高效模型使用BCT进行了压缩并在多个GLUE数据集上进行了评估，结果显示在大多数任务中，BCT只会带来少于0.90%的准确率下降。

    Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have recently attracted increasing interest, research enthusiasm, and business demand. However, their massive computation resources and huge memory footprint are inevitable challenges. To tackle this issue, we propose BCT, a framework of blockwise compression for transformers without retraining, to lower deployment thresholds. BCT achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, Softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient model with BCT and evaluate it on several General Language Understanding Evaluation (GLUE) datasets. The results show that BCT can achieve a less than 0.90% accuracy drop in most tasks.
    
[^62]: 时间-空间-频率特征融合的3通道运动想象分类

    Time-space-frequency feature Fusion for 3-channel motor imagery classification. (arXiv:2304.01461v1 [cs.LG])

    [http://arxiv.org/abs/2304.01461](http://arxiv.org/abs/2304.01461)

    本文提出了一种新网络架构TSFF-Net，将时间-空间-频率特征融合，解决了单模特征提取网络在时间序列或时间-频率模态下的限制。TSFF-Net包括时间-频率表示、时间-频率特征提取、时间-空间特征提取和特征融合与分类四个主要组件。

    

    低通道EEG设备对于便携和娱乐应用至关重要，然而EEG低空间分辨率存在挑战，难以解码低通道运动想象。本研究引入TSFF-Net，一种新颖的网络架构，将时间-空间-频率特征融合，在时间序列或时间-频率模态下的单模特征提取网络的限制下发挥作用。TSFF-Net包括四个主要组件：时间-频率表示、时间-频率特征提取、时间-空间特征提取和特征融合与分类。时间-频率表示和特征提取将原始EEG信号转换为时间-频率谱图并提取相关特征。时空网络将时间序列EEG试验作为输入处理，并提取时间-空间特征。特征融合采用MMD损失在再生核希尔伯特空间中约束时间-频率和时间-空间特征的分布。

    Low-channel EEG devices are crucial for portable and entertainment applications. However, the low spatial resolution of EEG presents challenges in decoding low-channel motor imagery. This study introduces TSFF-Net, a novel network architecture that integrates time-space-frequency features, effectively compensating for the limitations of single-mode feature extraction networks based on time-series or time-frequency modalities. TSFF-Net comprises four main components: time-frequency representation, time-frequency feature extraction, time-space feature extraction, and feature fusion and classification. Time-frequency representation and feature extraction transform raw EEG signals into time-frequency spectrograms and extract relevant features. The time-space network processes time-series EEG trials as input and extracts temporal-spatial features. Feature fusion employs MMD loss to constrain the distribution of time-frequency and time-space features in the Reproducing Kernel Hilbert Space, 
    
[^63]: 探索视觉-语言模型在不平衡学习中的应用

    Exploring Vision-Language Models for Imbalanced Learning. (arXiv:2304.01457v1 [cs.AI])

    [http://arxiv.org/abs/2304.01457](http://arxiv.org/abs/2304.01457)

    本文探索了如何通过向视觉-语言模型添加轻量级解码器和利用不平衡算法来改进性能，实验表明改进后的VLM在iNaturalist18、CIFAR-100和Visual Genome数据集上分类准确度显著提高，特别是对于少数类，性能提升很大。

    

    使用对比语言-图像预训练的视觉-语言模型（VLMs）已经显示出有希望的零样本分类表现。然而，在不平衡数据集上，它们的性能相对较差，在训练数据集中类的分布倾斜，导致在预测少数类方面性能不佳。我们提出向VLM添加轻量级解码器，以避免由于大量类别导致的内存不足问题，并捕捉尾部类别的微妙特征。然后，我们探索了利用提示调整、微调以及加入不平衡算法（例如Focal Loss、Balanced SoftMax和Distribution Alignment）来改进VLM。实验表明，在使用解码器和不平衡方法时，VLM的性能可以进一步提高。具体而言，我们改进的VLM在iNaturalist18、CIFAR-100和Visual Genome数据集上的分类准确度平均提高了6.58%、69.82%和10.43%。

    Vision-Language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid OOM (out of memory) problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,
    
[^64]: 多智能体强化学习中的离线策略行动预测

    Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning. (arXiv:2304.01447v1 [cs.MA])

    [http://arxiv.org/abs/2304.01447](http://arxiv.org/abs/2304.01447)

    本文提出了一个名为OffPA2的新框架，通过离线策略行动预测方法来提高多智能体强化学习中的学习预测效率。

    

    学习预测是多智能体强化学习（MARL）中的一种推理范式，其中智能体预测其他智能体的学习步骤，以提高它们之间的合作。然而，现有的基于高阶梯度（HOG）方法在非可微分博弈或状态空间较大的博弈中效率低下。为了解决这些问题，本文提出了OffPA2框架，利用离线策略行动预测方法来提高学习预测的效率。

    Learning anticipation in Multi-Agent Reinforcement Learning (MARL) is a reasoning paradigm where agents anticipate the learning steps of other agents to improve cooperation among themselves. As MARL uses gradient-based optimization, learning anticipation requires using Higher-Order Gradients (HOG), with so-called HOG methods. Existing HOG methods are based on policy parameter anticipation, i.e., agents anticipate the changes in policy parameters of other agents. Currently, however, these existing HOG methods have only been applied to differentiable games or games with small state spaces. In this work, we demonstrate that in the case of non-differentiable games with large state spaces, existing HOG methods do not perform well and are inefficient due to their inherent limitations related to policy parameter anticipation and multiple sampling stages. To overcome these problems, we propose Off-Policy Action Anticipation (OffPA2), a novel framework that approaches learning anticipation thro
    
[^65]: 工业控制系统中的深度多模态网络攻击检测

    A Deep Multi-Modal Cyber-Attack Detection in Industrial Control Systems. (arXiv:2304.01440v1 [cs.CR])

    [http://arxiv.org/abs/2304.01440](http://arxiv.org/abs/2304.01440)

    本研究提出了一种利用工业控制系统网络和传感器数据，采用深度多模态网络攻击检测模型提高网络攻击检测效果的方法，实验结果表明，该方法能够超越现有的单模态模型和最近的文献作品，具有检测网络攻击的较高精度、召回率和f-measure。

    

    近年来，工业控制系统遭受的网络攻击数量不断增加，由于其潜在的灾难性影响，人们对其安全性越来越关注。考虑到工业控制系统的复杂性，检测其中的网络攻击是极具挑战性的，需要运用能够利用多种数据模态的先进方法。本研究利用来自工业控制系统的网络和传感器数据，使用深度多模态网络攻击检测模型进行处理。实验结果表明，所提出的模型可以优于现有的单模态模型和最近的文献作品，其精度达到0.99、召回率为0.98、f-measure为0.98，表明了结合多种数据模态的检测方法在检测网络攻击方面的有效性。

    The growing number of cyber-attacks against Industrial Control Systems (ICS) in recent years has elevated security concerns due to the potential catastrophic impact. Considering the complex nature of ICS, detecting a cyber-attack in them is extremely challenging and requires advanced methods that can harness multiple data modalities. This research utilizes network and sensor modality data from ICS processed with a deep multi-modal cyber-attack detection model for ICS. Results using the Secure Water Treatment (SWaT) system show that the proposed model can outperform existing single modality models and recent works in the literature by achieving 0.99 precision, 0.98 recall, and 0.98 f-measure, which shows the effectiveness of using both modalities in a combined model for detecting cyber-attacks.
    
[^66]: 用深度强化学习优化田间灌溉效率

    Optimizing Irrigation Efficiency using Deep Reinforcement Learning in the Field. (arXiv:2304.01435v1 [cs.LG])

    [http://arxiv.org/abs/2304.01435](http://arxiv.org/abs/2304.01435)

    本文提出了一种利用深度强化学习优化灌溉效率的方法，名为DRLIC，它使用一个神经网络来学习最佳控制策略，并考虑到当前的土壤水分测量和未来的土壤水分损失。通过引入一个灌溉奖励函数，DRLIC可以从以往的经验中学习。实验表明，DRLIC和其简化版本在水分利用效率和作物产量方面均优于当前的灌溉方法。

    

    农业灌溉是淡水消耗的重要贡献者。然而，目前田间使用的灌溉系统效率不高。它们主要依赖于土壤湿度传感器和种植者的经验，但并不考虑未来的土壤水分损失。预测土壤水分损失很具有挑战性，因为它受到许多因素的影响，包括土壤质地、天气条件和植物特性。本文提出了一种名为DRLIC的解决方案来提高灌溉效率。DRLIC是一种复杂的灌溉系统，它使用深度强化学习(DRL)来优化其性能。该系统采用一种神经网络，称为DRL控制代理，可以学习考虑当前土壤水分测量和未来土壤水分损失的最佳控制策略。我们引入了一个灌溉奖励函数，使控制代理能够从以往的经验中学习。然而，由于硬件限制，DRLIC的操作在某些情况下可能不实用。因此，本文提出了DRLIC的简化版本，它使用更小的神经网络和更少的输入。结果表明，DRLIC和简化版本在水分利用效率和作物产量方面均优于当前的灌溉方法。

    Agricultural irrigation is a significant contributor to freshwater consumption. However, the current irrigation systems used in the field are not efficient. They rely mainly on soil moisture sensors and the experience of growers, but do not account for future soil moisture loss. Predicting soil moisture loss is challenging because it is influenced by numerous factors, including soil texture, weather conditions, and plant characteristics. This paper proposes a solution to improve irrigation efficiency, which is called DRLIC. DRLIC is a sophisticated irrigation system that uses deep reinforcement learning (DRL) to optimize its performance. The system employs a neural network, known as the DRL control agent, which learns an optimal control policy that considers both the current soil moisture measurement and the future soil moisture loss. We introduce an irrigation reward function that enables our control agent to learn from previous experiences. However, there may be instances where the o
    
[^67]: VNE: 通过操纵特征值分布来提高深度表示的有效方法

    VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution. (arXiv:2304.01434v1 [cs.CV])

    [http://arxiv.org/abs/2304.01434](http://arxiv.org/abs/2304.01434)

    本文提出了通过规范化表示的von Neumann熵( VNE ) 来改善深度表示的方法，通过操纵特征值分布来优化表示品质，广泛适用于不同的算法，可以增强其领域通用性、元学习、自监督学习和生成模型等方面。

    

    自从深度学习被引入以来，很多表示特性 (如去相关、白化、解缠、秩、等度性和互信息) 已经被研究出来，以提高表示品质。然而，操纵这些特性在实现有效性和普适适用性方面都具有挑战性。为了解决这些限制，我们提出了对表示的von Neumann熵(VNE)进行规范化。首先，我们证明了VNE的数学表述在有效操纵表示自相关矩阵的特征值方面是优越的。然后，我们通过调查领域通用性，元学习，自监督学习和生成模型等方面，证明了它在提高现有先进算法或流行基准算法中的广泛适用性。此外，我们在理论上建立了表示的秩、解缠和等度性的联系。最后，我们提供了讨论。

    Since the introduction of deep learning, a wide scope of representation properties, such as decorrelation, whitening, disentanglement, rank, isotropy, and mutual information, have been studied to improve the quality of representation. However, manipulating such properties can be challenging in terms of implementational effectiveness and general applicability. To address these limitations, we propose to regularize von Neumann entropy~(VNE) of representation. First, we demonstrate that the mathematical formulation of VNE is superior in effectively manipulating the eigenvalues of the representation autocorrelation matrix. Then, we demonstrate that it is widely applicable in improving state-of-the-art algorithms or popular benchmark algorithms by investigating domain-generalization, meta-learning, self-supervised learning, and generative models. In addition, we formally establish theoretical connections with rank, disentanglement, and isotropy of representation. Finally, we provide discuss
    
[^68]: TPU v4：一款支持嵌入式硬件的可重构光学超级计算机用于机器学习

    TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings. (arXiv:2304.01433v1 [cs.AR])

    [http://arxiv.org/abs/2304.01433](http://arxiv.org/abs/2304.01433)

    TPU v4是一款支持嵌入式硬件的可重构光学超级计算机，采用光学电路交换机重新配置互连拓扑，提高规模、可用性、利用率、模块化、部署、安全、功率和性能，它通过SparseCores加速嵌入式模型，性能优越，功耗低。

    

    针对机器学习模型的创新，生产工作负载发生了根本性和迅速的变化。TPU v4是谷歌的第五代面向特定领域架构（DSA），是其第三个用于处理此类机器学习模型的超级计算机。光学电路交换机（OCS）动态重新配置其互连拓扑，以提高规模、可用性、利用率、模块化、部署、安全、功率和性能。部署自2020年以来，TPU v4超级计算机的表现优于TPU v3，同时性能/Watt提高了2.7倍。

    In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are <5% of system cost and <3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x-7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus ~10x faster overall, which along with OCS flexibility helps large language models. For sim
    
[^69]: 降低Frank-Wolfe方法中的离散化误差

    Reducing Discretization Error in the Frank-Wolfe Method. (arXiv:2304.01432v1 [math.OC])

    [http://arxiv.org/abs/2304.01432](http://arxiv.org/abs/2304.01432)

    本论文提出了两个改进方法：一个多步的Frank-Wolfe方法，直接应用优化的高阶离散化方案；以及一种具有较少离散化误差的LMO-平均方案，其收敛速率加速到$O(1/k^{3/2})$，从而更好地解决了Frank-Wolfe方法中的离散化误差问题。

    

    Frank-Wolfe算法是结构受限机器学习应用中常用的方法，因其快速迭代复杂度而受欢迎。然而，该方法的一个主要限制是收敛速度缓慢，由于步长方向的不规则震荡而难以加速，即使在接近解的渐近情况下也是如此。我们认为这是离散化的产物；也就是说，Frank-Wolfe的流（即渐近小步长情况下的轨迹）不会出现不规则震荡，因此减少离散化误差将与产生更稳定的方法和更好的收敛特性相辅相成。我们提出了两个改进：一个多步Frank-Wolfe方法，直接应用优化的高阶离散化方案；和一个具有降低离散化误差的LMO-平均方案，其在一般凸集上的局部收敛速率从$O(1/k)$加速到$O(1/k^{3/2})$ 。

    The Frank-Wolfe algorithm is a popular method in structurally constrained machine learning applications, due to its fast per-iteration complexity. However, one major limitation of the method is a slow rate of convergence that is difficult to accelerate due to erratic, zig-zagging step directions, even asymptotically close to the solution. We view this as an artifact of discretization; that is to say, the Frank-Wolfe \emph{flow}, which is its trajectory at asymptotically small step sizes, does not zig-zag, and reducing discretization error will go hand-in-hand in producing a more stabilized method, with better convergence properties. We propose two improvements: a multistep Frank-Wolfe method that directly applies optimized higher-order discretization schemes; and an LMO-averaging scheme with reduced discretization error, and whose local convergence rate over general convex sets accelerates from a rate of $O(1/k)$ to up to $O(1/k^{3/2})$.
    
[^70]: 分离的关注力：基于上下文分离槽的无监督多对象发现

    Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots. (arXiv:2304.01430v1 [cs.CV])

    [http://arxiv.org/abs/2304.01430](http://arxiv.org/abs/2304.01430)

    该论文提出了一种新的无监督多对象发现方法，通过一种上下文分隔的槽结构来将视觉场分割为独立运动区域，并用对抗性标准来保证解码器无法重构整个光流。

    

    我们提出了一种将视觉场分割为独立运动区域的方法，不需要任何基础真值或监督。它由基于槽关注的对抗条件编码器-解码器架构组成，修改为使用图像作为上下文来解码光流，而不是尝试重构图像本身。在结果的多模式表示中，一种模式（流）将馈送给编码器以产生单独的潜在代码（槽），而另一种模式（图像）将决定解码器从槽生成第一个模式（流）。由于惯常的自编码基于最小化重构误差，并不能防止整个流被编码到一个槽中，因此我们将损失修改为基于上下文信息分离的对抗性标准。

    We introduce a method to segment the visual field into independently moving regions, trained with no ground truth or supervision. It consists of an adversarial conditional encoder-decoder architecture based on Slot Attention, modified to use the image as context to decode optical flow without attempting to reconstruct the image itself. In the resulting multi-modal representation, one modality (flow) feeds the encoder to produce separate latent codes (slots), whereas the other modality (image) conditions the decoder to generate the first (flow) from the slots. This design frees the representation from having to encode complex nuisance variability in the image due to, for instance, illumination and reflectance properties of the scene. Since customary autoencoding based on minimizing the reconstruction error does not preclude the entire flow from being encoded into a single slot, we modify the loss to an adversarial criterion based on Contextual Information Separation. The resulting min-m
    
[^71]: 带有结构性缺失的数据学习

    Learning from data with structured missingness. (arXiv:2304.01429v1 [stat.ML])

    [http://arxiv.org/abs/2304.01429](http://arxiv.org/abs/2304.01429)

    带有结构性缺失的数据学习是一个尚未被系统解决的问题，它对规模化的机器学习构成了重要阻碍，并且需要进一步的研究和解决。

    

    缺失数据是许多机器学习任务中不可避免的复杂问题。当数据是“随机缺失”时，存在一系列的工具和技术来解决此问题。然而，随着机器学习研究变得更加雄心勃勃，并且试图从越来越大量的异构数据中学习，越来越普遍的问题是出现了缺失值的关联或结构，无论是明确还是隐含。这种“结构性缺失”提出了一系列尚未系统解决的挑战，并对规模化的机器学习构成了重要阻碍。在这里，我们概述了当前的文献，并提出了一组关于如何在带有结构性缺失的数据中学习的挑战。

    Missing data are an unavoidable complication in many machine learning tasks. When data are `missing at random' there exist a range of tools and techniques to deal with the issue. However, as machine learning studies become more ambitious, and seek to learn from ever-larger volumes of heterogeneous data, an increasingly encountered problem arises in which missing values exhibit an association or structure, either explicitly or implicitly. Such `structured missingness' raises a range of challenges that have not yet been systematically addressed, and presents a fundamental hindrance to machine learning at scale. Here, we outline the current literature and propose a set of grand challenges in learning from data with structured missingness.
    
[^72]: 基于学习的树搜索算法用于在共享空域中实现社交机器人长期导航

    Learned Tree Search for Long-Horizon Social Robot Navigation in Shared Airspace. (arXiv:2304.01428v1 [cs.RO])

    [http://arxiv.org/abs/2304.01428](http://arxiv.org/abs/2304.01428)

    本文提出了一种基于学习的树搜索算法SoRTS，用于在共享空域中实现社交机器人长期导航，并通过FAA认证飞行员的评估证明了其表现与一名熟练的人类飞行员相当，明显优于基准算法。

    

    近年来，对无人机在拥挤动态的共享空间中进行自主操作的需求迅速增长，因此需要开发可信的代理程序以实现无缝安全导航。本文提出了Social Robot Tree Search (SoRTS)，一种用于在社交领域中移动机器人安全导航的算法。SoRTS旨在通过Monte Carlo Tree Search规划器增强现有的社交感知轨迹预测策略，以实现更好的移动机器人下游导航效果。为了评估我们方法的性能，我们选择了一般航空领域的社交导航应用。为了促进这一评估，我们还引入了高保真度的X-Plane ROS（机载操作系统）飞行模拟器，以实现在完全自主操作上的更多研究。通过对26名FAA认证飞行员的行业评估，我们证明了SoRTS的表现与一名熟练的人类飞行员相当，明显优于我们的基准算法。我们进一步补充了这些结果。

    The fast-growing demand for fully autonomous aerial operations in shared spaces necessitates developing trustworthy agents that can safely and seamlessly navigate in crowded, dynamic spaces. In this work, we propose Social Robot Tree Search (SoRTS), an algorithm for the safe navigation of mobile robots in social domains. SoRTS aims to augment existing socially-aware trajectory prediction policies with a Monte Carlo Tree Search planner for improved downstream navigation of mobile robots. To evaluate the performance of our method, we choose the use case of social navigation for general aviation. To aid this evaluation, within this work, we also introduce X-PlaneROS, a high-fidelity aerial simulator, to enable more research in full-scale aerial autonomy. By conducting a user study based on the assessments of 26 FAA certified pilots, we show that SoRTS performs comparably to a competent human pilot, significantly outperforming our baseline algorithm. We further complement these results wit
    
[^73]: 条件无限分位数回归的一种合规预测方法

    Conformalized Unconditional Quantile Regression. (arXiv:2304.01426v1 [cs.LG])

    [http://arxiv.org/abs/2304.01426](http://arxiv.org/abs/2304.01426)

    本文提出了一种新的预测推断过程，将符合预测与条件无限分位数回归相结合，能够适应异方差性并提供透明的覆盖保证，具有竞争力的性能。

    

    我们开发了一种将符合预测（CP）与条件无限分位数回归（QR）相结合的预测推断过程，后者是经济计量学中常用的工具，涉及在输入协变量上回归分位函数的重新居中影响函数（RIF）。与更为广为人知的条件QR不同，条件无限QR明确捕捉协变量分布变化对结果边际分布的分位数的影响。利用这个特性，我们的过程使用训练数据拟合RIFs的机器学习模型，然后对于任何测试协变量，在围绕新实例的“假设”协变量分布上应用CP过程。实验表明，我们的方法能够适应异方差性，提供与手头测试实例相关的透明覆盖保证，并在各种预测任务中表现有竞争力。

    We develop a predictive inference procedure that combines conformal prediction (CP) with unconditional quantile regression (QR) -- a commonly used tool in econometrics that involves regressing the recentered influence function (RIF) of the quantile functional over input covariates. Unlike the more widely-known conditional QR, unconditional QR explicitly captures the impact of changes in covariate distribution on the quantiles of the marginal distribution of outcomes. Leveraging this property, our procedure issues adaptive predictive intervals with localized frequentist coverage guarantees. It operates by fitting a machine learning model for the RIFs using training data, and then applying the CP procedure for any test covariate with respect to a ``hypothetical'' covariate distribution localized around the new instance. Experiments show that our procedure is adaptive to heteroscedasticity, provides transparent coverage guarantees that are relevant to the test instance at hand, and perfor
    
[^74]: 学习增强目标信息：反馈对齐的替代理论

    Learning with augmented target information: An alternative theory of Feedback Alignment. (arXiv:2304.01406v1 [q-bio.NC])

    [http://arxiv.org/abs/2304.01406](http://arxiv.org/abs/2304.01406)

    本文提出了一种新的、架构不可知的反馈对齐工作理论，通过将目标信息嵌入到神经网络中学习有效的表征，而不是像BP一样用同样的参数来近似梯度。并基于这一理论设计了三种实现FA的方法。

    

    错误反向传播（BP）长期以来一直占据着几乎所有现代神经网络的训练主导地位，但它存在着许多生物合理性问题，如对称权重要求和同步更新。反馈对齐（FA）被提出作为BP的替代方案以解决这些困境，并已被证明在各种任务和网络体系结构上都具有有效性。尽管它简单而有效，但还缺乏一个令人满意的解释，解释FA如何在不同的体系结构中发挥作用。在这里，我们提出了一种新的、架构不可知的FA工作理论，通过信息理论的视角，将目标信息嵌入到被训练的神经网络中，从而学习有效的表征，而不是用相同的参数来近似BP计算出的梯度。我们通过对理想情况下的FA动态进行分析来展示这一点，然后通过一系列实验来验证。基于这一理论的意义，我们设计了三种实现FA的方法，并测试了它们的有效性和可扩展性。

    While error backpropagation (BP) has dominated the training of nearly all modern neural networks for a long time, it suffers from several biological plausibility issues such as the symmetric weight requirement and synchronous updates. Feedback Alignment (FA) was proposed as an alternative to BP to address those dilemmas and has been demonstrated to be effective on various tasks and network architectures. Despite its simplicity and effectiveness, a satisfying explanation of how FA works across different architectures is still lacking. Here we propose a novel, architecture-agnostic theory of how FA works through the lens of information theory: Instead of approximating gradients calculated by BP with the same parameter, FA learns effective representations by embedding target information into neural networks to be trained. We show this through the analysis of FA dynamics in idealized settings and then via a series of experiments. Based on the implications of this theory, we designed three 
    
[^75]: 基于主动迁移学习的水平集估计中物体表面缺陷区域的自适应识别

    Adaptive Defective Area Identification in Material Surface Using Active Transfer Learning-based Level Set Estimation. (arXiv:2304.01404v1 [cs.LG])

    [http://arxiv.org/abs/2304.01404](http://arxiv.org/abs/2304.01404)

    本文提出了一种自适应映射方法来更高效地识别材料表面的缺陷区域，解决了以往逐点测量导致耗时大的问题，同时引入了主动学习和迁移学习方法以降低测量次数和利用先前生产材料信息。

    

    在材料表征中，识别物体表面上的缺陷区域是基础性的任务。传统的方法是在表面预设的网格点上逐点测量相关物理量，并确定未达到期望水平的区域。为了更高效地识别缺陷区域，我们提出了自适应映射方法，该方法优先使用测量资源来检测缺陷区域的边界。我们将该问题视为水平集估计问题的主动学习领域。基于主动学习的水平集估计的目标是以尽可能少的测量次数来确定定义在表面上的物理函数的水平集。此外，为了处理重复生产具有相似规格的材料的情况，我们引入了一种迁移学习方法，以便有效利用先前生产的材料的信息。

    In material characterization, identifying defective areas on a material surface is fundamental. The conventional approach involves measuring the relevant physical properties point-by-point at the predetermined mesh grid points on the surface and determining the area at which the property does not reach the desired level. To identify defective areas more efficiently, we propose adaptive mapping methods in which measurement resources are used preferentially to detect the boundaries of defective areas. We interpret this problem as an active-learning (AL) of the level set estimation (LSE) problem. The goal of AL-based LSE is to determine the level set of the physical property function defined on the surface with as small number of measurements as possible. Furthermore, to handle the situations in which materials with similar specifications are repeatedly produced, we introduce a transfer learning approach so that the information of previously produced materials can be effectively utilized.
    
[^76]: 基于聚类的系统识别学习个性化模型

    Learning Personalized Models with Clustered System Identification. (arXiv:2304.01395v1 [math.OC])

    [http://arxiv.org/abs/2304.01395](http://arxiv.org/abs/2304.01395)

    该文提出了一种基于聚类的系统识别学习个性化模型的算法，将多个系统划分为群集，同一簇中的系统可以从其他系统的观察中获益。该算法实现了正确估计群集标识并具有高效和个性化的系统识别过程。

    

    我们解决从不同系统动力学观察多个轨迹中学习线性系统模型的问题。这个框架包括协作场景，其中寻求估计其动力学的多个系统被划分为根据其系统相似性的群集。因此，同一簇中的系统可以从其他系统的观察中获益。考虑到这个框架，我们提出了一个算法，每个系统交替估计其群集标识并执行动态估计。然后聚合以更新每个群集的模型。我们证明，在温和的假设下，我们的算法正确地估计了群集标识，并实现了近似样本复杂度，其与群集中系统数成反比，从而促进了更高效和个性化的系统识别过程。

    We address the problem of learning linear system models from observing multiple trajectories from different system dynamics. This framework encompasses a collaborative scenario where several systems seeking to estimate their dynamics are partitioned into clusters according to their system similarity. Thus, the systems within the same cluster can benefit from the observations made by the others. Considering this framework, we present an algorithm where each system alternately estimates its cluster identity and performs an estimation of its dynamics. This is then aggregated to update the model of each cluster. We show that under mild assumptions, our algorithm correctly estimates the cluster identities and achieves an approximate sample complexity that scales inversely with the number of systems in the cluster, thus facilitating a more efficient and personalized system identification process.
    
[^77]: 图上反事实学习：综述

    Counterfactual Learning on Graphs: A Survey. (arXiv:2304.01391v1 [cs.LG])

    [http://arxiv.org/abs/2304.01391](http://arxiv.org/abs/2304.01391)

    本文综述了图上反事实学习的研究进展，包括反事实公平性、可解释性、链路预测等不同应用问题，并提出了未来的研究方向。

    

    图结构数据在现实世界中非常普遍，如社交网络、分子图和交易网络。图神经网络（GNNs）在图表示学习方面取得了巨大的成功，促进了各种下游任务。然而，GNN具有一些缺点，如缺乏可解释性、容易继承训练数据的偏见，不能建模因果关系。最近，图上反事实学习已经显示出在缓解这些缺点方面具有很有前途的结果。为了促进这个有前途的方向的发展，本综述将分类和全面地评估反事实图学习论文，为每个类别提供背景和激励性例子、一般框架和代表性方法的讨论。最后，我们总结了图上反事实学习的挑战和机遇，并指出未来的研究方向。

    Graph-structured data are pervasive in the real-world such as social networks, molecular graphs and transaction networks. Graph neural networks (GNNs) have achieved great success in representation learning on graphs, facilitating various downstream tasks. However, GNNs have several drawbacks such as lacking interpretability, can easily inherit the bias of the training data and cannot model the casual relations. Recently, counterfactual learning on graphs has shown promising results in alleviating these drawbacks. Various graph counterfactual learning approaches have been proposed for counterfactual fairness, explainability, link prediction and other applications on graphs. To facilitate the development of this promising direction, in this survey, we categorize and comprehensively review papers on graph counterfactual learning. We divide existing methods into four categories based on research problems studied. For each category, we provide background and motivating examples, a general f
    
[^78]: 《使用机器学习识别被动式光网络中的错误分支》

    Faulty Branch Identification in Passive Optical Networks using Machine Learning. (arXiv:2304.01376v1 [cs.LG])

    [http://arxiv.org/abs/2304.01376](http://arxiv.org/abs/2304.01376)

    该论文介绍了针对PON系统中可能出现的故障分支，使用机器学习进行错误分支识别的方法。同时探讨了在多个长度相似的分支产生反射重叠的情况下如何进行隔离。

    

    被动式光网络已成为一种有前途的宽带接入网络解决方案。为确保可靠传输并满足服务水平协议，PON系统必须不断监测，以快速识别和定位网络故障。针对PON系统中可能出现的故障分支，可以通过分析记录的光时域反射（OTDR）跟踪信息来进行识别。然而，当两个或更多长度相似的分支产生的反射重叠时，故障分支的隔离变得非常具有挑战性。最近，基于机器学习的方法在管理PON系统中的光学故障方面展示了巨大的潜力。

    Passive optical networks (PONs) have become a promising broadband access network solution. To ensure a reliable transmission, and to meet service level agreements, PON systems have to be monitored constantly in order to quickly identify and localize networks faults. Typically, a service disruption in a PON system is mainly due to fiber cuts and optical network unit (ONU) transmitter/receiver failures. When the ONUs are located at different distances from the optical line terminal (OLT), the faulty ONU or branch can be identified by analyzing the recorded optical time domain reflectometry (OTDR) traces. However, faulty branch isolation becomes very challenging when the reflections originating from two or more branches with similar length overlap, which makes it very hard to discriminate the faulty branches given the global backscattered signal. Recently, machine learning (ML) based approaches have shown great potential for managing optical faults in PON systems. Such techniques perform 
    
[^79]: 自适应SpikeDeep-分类器:用于实时脑机接口信号处理的自组织自监督机器学习算法

    Adaptive SpikeDeep-Classifier: Self-organizing and self-supervised machine learning algorithm for online spike sorting. (arXiv:2304.01355v1 [q-bio.NC])

    [http://arxiv.org/abs/2304.01355](http://arxiv.org/abs/2304.01355)

    Ada-SpikeDeep-Classifier是一种用于实时脑机接口信号处理的自适应自组织算法，它使用了SpikeDeeptector进行信道选择、Ada-BAR进行信号预处理、OCM进行分类，旨在提高脑-计算机接口的解码效果，并在实验中表现出高精度和强健性。

    

    本研究旨在提高脑-计算机接口的解码效果，通过针对密集微电极阵列数据的处理，提出了一种自适应自组织算法——自适应SpikeDeep-分类器(Ada-SpikeDeepClassifier)。该算法使用SpikeDeeptector进行信道选择，采用自适应背景活动拒绝器(Ada-BAR)进行信号预处理，并采用自监督在线聚类模块(OCM)进行分类。Ada-SpikeDeep-Classifier在模拟和实际数据记录中均表现出高精度和强健性，优于现有的算法。该算法可望扩展到神经科学和机器学习社区的其他神经数据分析任务中。

    Objective. Research on brain-computer interfaces (BCIs) is advancing towards rehabilitating severely disabled patients in the real world. Two key factors for successful decoding of user intentions are the size of implanted microelectrode arrays and a good online spike sorting algorithm. A small but dense microelectrode array with 3072 channels was recently developed for decoding user intentions. The process of spike sorting determines the spike activity (SA) of different sources (neurons) from recorded neural data. Unfortunately, current spike sorting algorithms are unable to handle the massively increasing amount of data from dense microelectrode arrays, making spike sorting a fragile component of the online BCI decoding framework. Approach. We proposed an adaptive and self-organized algorithm for online spike sorting, named Adaptive SpikeDeep-Classifier (Ada-SpikeDeepClassifier), which uses SpikeDeeptector for channel selection, an adaptive background activity rejector (Ada-BAR) for 
    
[^80]: 利用内存高效和鲁棒单调算子学习的加速并行MRI方法

    Accelerated parallel MRI using memory efficient and robust monotone operator learning (MOL). (arXiv:2304.01351v1 [cs.LG])

    [http://arxiv.org/abs/2304.01351](http://arxiv.org/abs/2304.01351)

    本论文使用单调算子学习（MOL）框架，结合单调卷积神经网络（CNN）和共轭梯度算法实现加速并行MRI，在内存效率和性能保证方面具有优势。

    

    基于模型的深度学习方法，将成像物理与学习的规范化先验相结合，已成为加速并行MRI的强大工具。本文的重点是确定单调算子学习（MOL）框架在并行MRI设置中的效用。MOL算法通过使用单调卷积神经网络（CNN）进行梯度下降步骤以及使用共轭梯度算法鼓励数据一致性来实现交替优化。这种方法的好处包括具有压缩感知算法类似的独特性、收敛性和稳定性保证，同时比展开方法具有更高的内存效率。我们通过比较在静态和动态环境下的不同展开算法，验证了所提出的方案的有效性。

    Model-based deep learning methods that combine imaging physics with learned regularization priors have been emerging as powerful tools for parallel MRI acceleration. The main focus of this paper is to determine the utility of the monotone operator learning (MOL) framework in the parallel MRI setting. The MOL algorithm alternates between a gradient descent step using a monotone convolutional neural network (CNN) and a conjugate gradient algorithm to encourage data consistency. The benefits of this approach include similar guarantees as compressive sensing algorithms including uniqueness, convergence, and stability, while being significantly more memory efficient than unrolled methods. We validate the proposed scheme by comparing it with different unrolled algorithms in the context of accelerated parallel MRI for static and dynamic settings.
    
[^81]: 时间动态同步功能脑网络在精神分裂症诊断和侧化分析中的应用

    Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis. (arXiv:2304.01347v1 [q-bio.NC])

    [http://arxiv.org/abs/2304.01347](http://arxiv.org/abs/2304.01347)

    本文提出了一种基于动态功能连接的脑网络分析模型，通过构建动态同步特征和革命性的图卷积方法实现精神分裂症诊断和侧化分析，并在实验证明其表现优于其他最先进模型。

    

    有证据表明，动态功能连接可以捕捉静息态功能磁共振成像数据中的脑活动时变异常，并在揭示精神分裂症（SZ）患者异常脑活动机制方面具有天然优势。因此，本文采用了一种先进的动态脑网络分析模型——时态脑类别图卷积网络（temporal-BCGCN）。首先设计了独特的动态脑网络分析模块DSF-BrainNet，用于构建动态同步特征。随后，提出了一种革命性的图卷积方法TemporalConv，基于特征的同步时间属性。最后，提出了一种基于静息态功能磁共振成像数据的深度学习模块化异常半球侧化检测工具，称为CategoryPool。该研究在COBRE和UCLA数据集上进行验证，分别达到83.62％和89.71％的平均准确率，优于基线模型和其他最先进模型，在精神分裂症诊断和侧化分析方面表现出色。

    Available evidence suggests that dynamic functional connectivity (dFC) can capture time-varying abnormalities in brain activity in rs-fMRI data and has a natural advantage in uncovering mechanisms of abnormal brain activity in schizophrenia(SZ) patients. Hence, an advanced dynamic brain network analysis model called the temporal brain category graph convolutional network (temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysis module, DSF-BrainNet, was designed to construct dynamic synchronization features. Subsequently, a revolutionary graph convolution method, TemporalConv, was proposed, based on the synchronous temporal properties of feature. Finally, the first modular abnormal hemispherical lateralization test tool in deep learning based on rs-fMRI data, named CategoryPool, was proposed. This study was validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% average accuracy, respectively, outperforming the baseline model and other State-of-the-Art
    
[^82]: 一种混合型霍奇金-赫胥黎动作电位生成模型的研究

    Towards an Hybrid Hodgkin-Huxley Action Potential Generation Model. (arXiv:2304.01346v1 [q-bio.NC])

    [http://arxiv.org/abs/2304.01346](http://arxiv.org/abs/2304.01346)

    本文提出了一种使用简单测量和简化模型构建混合型霍奇金-赫胥黎模型的方法，可以在显著减少测量量的情况下准确地表示神经元动作电位。

    

    通过数学模型对于神经元电活动引起生理机制的理解得到了提高。其中，一些包括膜电位经验函数的方程被定义。最常见的霍奇金-赫胥黎模型是这种范例的一个例子，因为它将离子通道的电导定义为通道中的每种门打开和关闭速率。这些函数需要从实验室测量中获得，而这些实验通常非常昂贵，又因为涉及单个细胞膜通道电压的时间空间独立测量，因此产生的数据有限。在这项工作中，我们探讨了仅使用两个简单的测量（时间的膜电压函数和触发该电压的注入电流）以及将霍奇金-赫胥黎模型与简单等效电路组合而成的简化模型来寻找霍奇金-赫胥黎模型参数函数的可能性。我们的结果表明，这种混合模型能够以非常高的准确性表示动作电位，并且所需的测量量显著减少，这使得它成为研究神经元电活动的有前途的途径。

    Mathematical models for the generation of the action potential can improve the understanding of physiological mechanisms that are consequence of the electrical activity in neurons. In such models, some equations involving empirically obtained functions of the membrane potential are usually defined. The best known of these models, the Hodgkin-Huxley model, is an example of this paradigm since it defines the conductances of ion channels in terms of the opening and closing rates of each type of gate present in the channels. These functions need to be derived from laboratory measurements that are often very expensive and produce little data because they involve a time-space-independent measurement of the voltage in a single channel of the cell membrane. In this work, we investigate the possibility of finding the Hodgkin-Huxley model's parametric functions using only two simple measurements (the membrane voltage as a function of time and the injected current that triggered that voltage) and
    
[^83]: 用热噪声描绘神经网络景观的地形

    Charting the Topography of the Neural Network Landscape with Thermal-Like Noise. (arXiv:2304.01335v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2304.01335](http://arxiv.org/abs/2304.01335)

    本文采用统计力学方法研究全连接神经网络的优化问题，发现在低损失区域存在一个低维流形，并由靠近分类决策边界的数据点数量决定维度。

    

    神经网络训练是一个复杂的高维、非凸且嘈杂的优化问题，其理论理解在应用角度和基础研究方面均具有重要意义。本文采用标准的统计力学方法，即通过Langevin动态相空间探测方法研究过参数全连接网络在随机数据上执行分类任务的梯度下降过程。通过分析涨落统计数据，类比于体系在恒定温度下的热动力学，我们推断出了一个清晰的地形描述——低损失区域是一个低维流形，其维度可以轻易地从波动性中获得。此外，该维度受到靠近分类决策边界的数据点数量的控制。重要的是，我们发现一个四阶相互作用是关键的，而标准的 Langevin 方法不能准确描述这种相互作用。

    The training of neural networks is a complex, high-dimensional, non-convex and noisy optimization problem whose theoretical understanding is interesting both from an applicative perspective and for fundamental reasons. A core challenge is to understand the geometry and topography of the landscape that guides the optimization. In this work, we employ standard Statistical Mechanics methods, namely, phase-space exploration using Langevin dynamics, to study this landscape for an over-parameterized fully connected network performing a classification task on random data. Analyzing the fluctuation statistics, in analogy to thermal dynamics at a constant temperature, we infer a clear geometric description of the low-loss region. We find that it is a low-dimensional manifold whose dimension can be readily obtained from the fluctuations. Furthermore, this dimension is controlled by the number of data points that reside near the classification decision boundary. Importantly, we find that a quadra
    
[^84]: 关于使用深度学习判断质数整除性

    On the Prime Number Divisibility by Deep Learning. (arXiv:2304.01333v1 [cs.LG])

    [http://arxiv.org/abs/2304.01333](http://arxiv.org/abs/2304.01333)

    本文提出了使用深度学习判断质数整除性的方法，并发现关键在于提供给深度学习模型的特征空间。此外，商业可用的自动化机器学习管道无法解决此问题，需要提供适当的特征工程来解决。研究者还提出了一个封闭式解决方案。

    

    对于确定一个整数是否能够被2、3或其他质数整除这样的任务，对于人类来说可能很简单，但在没有预先指定算法的情况下，这对于计算机来说可能并不容易。本文测试了多个深度学习体系结构和特征工程方法，并评估了在确定大有限整数（高达$2^{32}$）是否能够被小质数整除的情况下，各种框架和网络结构（CNN、RNN、Transformer等）的能力。结果表明，预测质数整除性的能力极大地取决于提供给深度学习模型的特征空间，而不是网络框架或网络结构的复杂性（CNN、RNN、Transformer等）。我们还评估了来自亚马逊、谷歌和微软的商业可用的自动化机器学习（AutoML）管道，并证明除非提供适当的特征工程，否则它们无法解决此问题。我们进一步提出了一个封闭式解决方案来解决这个问题。

    Certain tasks such as determining whether a given integer can be divided by 2, 3, or other prime numbers may be trivial for human beings, but can be less straightforward for computers in the absence of pre-specified algorithms. In this paper, we tested multiple deep learning architectures and feature engineering approaches, and evaluated the scenario of determining divisibility of large finite integers (up to $2^{32}$) by small prime numbers. It turns out that, regardless of the network frameworks or the complexity of the network structures (CNN, RNN, Transformer, etc.), the ability to predict the prime number divisibility critically depends on the feature space fed into the deep learning models. We also evaluated commercially available Automated Machine Learning (AutoML) pipelines from Amazon, Google and Microsoft, and demonstrated that they failed to address this issue unless appropriately engineered features were provided. We further proposed a closed form solution to the problem us
    
[^85]: 使用神经时滞微分方程学习延迟

    Learning the Delay Using Neural Delay Differential Equations. (arXiv:2304.01329v1 [math.OC])

    [http://arxiv.org/abs/2304.01329](http://arxiv.org/abs/2304.01329)

    本文提出了一种基于时滞微分方程的连续时间神经网络方法，使用伴随灵敏度方法直接学习模型参数和时滞，具有学习DDE参数的能力。

    

    机器学习和动力系统的交叉点最近引起了人们的广泛兴趣。神经常微分方程（NODEs）代表了这些领域之间丰富的交叠。本文提出了一种基于时滞微分方程（DDEs）的连续时间神经网络方法。我们的模型使用伴随灵敏度方法从数据中直接学习模型参数和时滞。我们的方法受到NODEs的启发，并扩展了早期的神经DDE模型，后者假设时滞的值是已知的。我们对我们提出的方法进行了灵敏度分析，并展示了它从基准系统中学习DDE参数的能力。我们在讨论中得出结论，提出未来可能的方向和应用。

    The intersection of machine learning and dynamical systems has generated considerable interest recently. Neural Ordinary Differential Equations (NODEs) represent a rich overlap between these fields. In this paper, we develop a continuous time neural network approach based on Delay Differential Equations (DDEs). Our model uses the adjoint sensitivity method to learn the model parameters and delay directly from data. Our approach is inspired by that of NODEs and extends earlier neural DDE models, which have assumed that the value of the delay is known a priori. We perform a sensitivity analysis on our proposed approach and demonstrate its ability to learn DDE parameters from benchmark systems. We conclude our discussion with potential future directions and applications.
    
[^86]: 强化学习中的实证设计

    Empirical Design in Reinforcement Learning. (arXiv:2304.01315v1 [cs.LG])

    [http://arxiv.org/abs/2304.01315](http://arxiv.org/abs/2304.01315)

    本文是一个关于如何进行良好实验的资源，旨在解决强化学习中实证设计的挑战，并弥补实证研究中可能导致的弱的统计证据。

    

    强化学习中的实证设计不是小任务。进行良好实验需要讲究细节，并且在某些时候需要大量计算资源。最近的研究表明，常用算法对超参数设置和实现细节敏感，并且常见的实证做法会导致弱的统计证据。本文不仅呼吁行动，而且是如何在强化学习中进行良好实验的全面资源。

    Empirical design in reinforcement learning is no small task. Running good experiments requires attention to detail and at times significant computational resources. While compute resources available per dollar have continued to grow rapidly, so have the scale of typical experiments in reinforcement learning. It is now common to benchmark agents with millions of parameters against dozens of tasks, each using the equivalent of 30 days of experience. The scale of these experiments often conflict with the need for proper statistical evidence, especially when comparing algorithms. Recent studies have highlighted how popular algorithms are sensitive to hyper-parameter settings and implementation details, and that common empirical practice leads to weak statistical evidence (Machado et al., 2018; Henderson et al., 2018). Here we take this one step further.  This manuscript represents both a call to action, and a comprehensive resource for how to do good experiments in reinforcement learning. 
    
[^87]: 实践中知识图谱用户、挑战和可视化需求的特征化研究

    Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice. (arXiv:2304.01311v1 [cs.HC])

    [http://arxiv.org/abs/2304.01311](http://arxiv.org/abs/2304.01311)

    本研究通过访谈19位知识图谱（KG）实践者，发现KG构建者需求架构执行程序，KG分析师需要可自定义查询构建器，KG消费者需要领域特定可视化，并指出在实践中实施KG需要技术和社交方面的解决方案。

    

    本研究通过对19位来自企业和学术环境下、涉及各种用例的知识图谱（KG）实践者的访谈，提出了KG实践者在创建、探索和分析KG时遇到的重要挑战，这些挑战可以通过可视化设计来缓解。我们的研究发现，KG实践者可以分为三类：KG构建者、分析师和消费者，每个人都有自己的专业知识和需求。我们发现，KG构建者可以从架构执行程序中获益，而KG分析师需要提供中间查询结果的可自定义查询构建器。对于KG消费者，我们确定节点链接图的效力不足，并需要定制的领域特定可视化来促进KG的采用和理解。最后，我们发现，在实践中有效地实施KG需要不仅技术上的，还有社交上的解决方案，而这些解决方案目前并未被当前的工具、技术和最佳实践所考虑。

    This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners - KG Builders, Analysts, and Consumers - each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, tec
    
[^88]: 并行退火混合时间的改进界限

    Improved Bound for Mixing Time of Parallel Tempering. (arXiv:2304.01303v1 [cs.LG])

    [http://arxiv.org/abs/2304.01303](http://arxiv.org/abs/2304.01303)

    本研究提出了一种新的并行退火的下界，对除$\log L$之外的所有参数具有多项式依赖性，其改进了现有界限。因此，该算法的混合时间可能更优。

    

    在采样算法领域中，当直接采样不可行时，MCMC（马尔科夫链蒙特卡罗）方法被广泛使用。然而，目标分布的多模态通常导致收敛速度缓慢和混合不佳。一种常见的解决方案是并行退火。尽管其在实践中非常有效，但其性能的理论保证有限。在本文中，我们提出了一个新的关于并行退火谱间隙的下界，其对除$\log L$之外的所有参数具有多项式依赖性，其中$(L + 1)$是级数的数量。这改进了现有界限，其与模态数呈指数关系。此外，我们用谱间隙的假设上界来补充我们的结果，其对$\log L$具有指数依赖性，这表明在某种意义上，我们的界限是紧密的。

    In the field of sampling algorithms, MCMC (Markov Chain Monte Carlo) methods are widely used when direct sampling is not possible. However, multimodality of target distributions often leads to slow convergence and mixing. One common solution is parallel tempering. Though highly effective in practice, theoretical guarantees on its performance are limited. In this paper, we present a new lower bound for parallel tempering on the spectral gap that has a polynomial dependence on all parameters except $\log L$, where $(L + 1)$ is the number of levels. This improves the best existing bound which depends exponentially on the number of modes. Moreover, we complement our result with a hypothetical upper bound on spectral gap that has an exponential dependence on $\log L$, which shows that, in some sense, our bound is tight.
    
[^89]: 基于核凸包机的差分隐私学习研究

    Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])

    [http://arxiv.org/abs/2304.01300](http://arxiv.org/abs/2304.01300)

    本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。

    

    本文探讨了通过学习再生核希尔伯特空间中的点的凸包来表示数据的方法，旨在将数据空间划分为几何体，从而隐藏有关单个数据点的隐私信息，同时保留原始学习问题的结构。为此，我们引入了核凸包机（KAHM），它提供了一种有效的方法来计算从结果有界几何体中的距离度量。KAHM是广泛和深入的自编码器的关键构建块，它们使数据表示学习用于分类应用。为了确保隐私保护学习，我们提出了一种新颖的生成虚假数据的方法，该方法涉及将差分隐私数据样本通过转换过程进行平滑处理。生成的虚假数据不仅保证差分隐私，而且确保KAHM建模误差不大于原始数据误差。

    This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
    
[^90]: 利用普尔金氏图像和机器学习算法进行动态调节测量

    Dynamic Accommodation Measurement using Purkinje Images and ML Algorithms. (arXiv:2304.01296v1 [physics.med-ph])

    [http://arxiv.org/abs/2304.01296](http://arxiv.org/abs/2304.01296)

    本研究开发出一种基于普尔金氏图像和机器学习算法的原型设备，可用于动态注视和调节测量，预测调节可以精确到0.25D，正在使用机器学习生成大量合成数据集。

    

    我们开发了一种基于4个普尔金氏反射（PR）的原型设备，用于适用于AR和眼科应用的动态注视和调节测量。 PR1和2以及PR3和4分别用于准确测量凝视和调节。我们的眼睛模型在ZEMAX中开发，并与实验结果相匹配。 我们的模型能够以超过0.25D的精度从4度到1度预测调节。我们进行了重复性测试，并从受试者身上获得了准确的凝视和调节估计。我们正在使用物理精确的模型和机器学习生成大量合成数据集。

    We developed a prototype device for dynamic gaze and accommodation measurements based on 4 Purkinje reflections (PR) suitable for use in AR and ophthalmology applications. PR1&2 and PR3&4 are used for accurate gaze and accommodation measurements, respectively. Our eye model was developed in ZEMAX and matches the experiments well. Our model predicts the accommodation from 4 diopters to 1 diopter with better than 0.25D accuracy. We performed repeatability tests and obtained accurate gaze and accommodation estimations from subjects. We are generating a large synthetic data set using physically accurate models and machine learning.
    
[^91]: 自主网络攻击代理的统一仿真模拟训练环境

    Unified Emulation-Simulation Training Environment for Autonomous Cyber Agents. (arXiv:2304.01244v1 [cs.LG])

    [http://arxiv.org/abs/2304.01244](http://arxiv.org/abs/2304.01244)

    本文提出了一种自动生成高保真度的模拟器解决方案，在智能学习的Cyber Gym for Intelligent Learning（CyGIL）中提供高度真实的网络Cyber Operations（CyOp）训练环境，并通过集成模拟器生成和代理训练过程来降低代理训练时间。

    

    通过强化学习和深度强化学习（RL / DRL），可以开发自主网络攻击代理，并在代表性环境中对代理进行训练。训练环境必须高度真实地模拟代理所要探索的网络Cyber Operations（CyOp）。本文介绍了一种系统解决方案，在智能学习的Cyber Gym for Intelligent Learning（CyGIL）中自动生成高保真度的模拟器。通过表征学习和连续学习，CyGIL提供统一的CyOp培训环境，其中仿真的CyGIL-S由自动生成的CyGIL-E生成。将模拟器生成与代理训练过程集成，以进一步减少所需的代理训练时间。在CyGIL-S中训练的代理可以直接被传输到CyGIL-E，完全可转移至仿真的“真实”网络。实验结果展示了这些解决方案的实际应用。

    Autonomous cyber agents may be developed by applying reinforcement and deep reinforcement learning (RL/DRL), where agents are trained in a representative environment. The training environment must simulate with high-fidelity the network Cyber Operations (CyOp) that the agent aims to explore. Given the complexity of net-work CyOps, a good simulator is difficult to achieve. This work presents a systematic solution to automatically generate a high-fidelity simulator in the Cyber Gym for Intelligent Learning (CyGIL). Through representation learning and continuous learning, CyGIL provides a unified CyOp training environment where an emulated CyGIL-E automatically generates a simulated CyGIL-S. The simulator generation is integrated with the agent training process to further reduce the required agent training time. The agent trained in CyGIL-S is transferrable directly to CyGIL-E showing full transferability to the emulated "real" network. Experimental results are presented to demonstrate th
    
[^92]: CoReFusion: 基于对比正则化融合实现热红外引导超分辨率

    CoReFusion: Contrastive Regularized Fusion for Guided Thermal Super-Resolution. (arXiv:2304.01243v1 [eess.IV])

    [http://arxiv.org/abs/2304.01243](http://arxiv.org/abs/2304.01243)

    本文提出了一种新颖的数据融合框架和正则化技术，用于热成像的引导超分辨率，具有计算效率，轻量级和鲁棒性，实现了在缺失数据的情况下性能不受影响，并且在基准数据集上表现出较高的性能。

    

    热成像由于在低光情况下表现出色而比一般可见光成像具有更多优势。 超分辨率技术可以通过使用低成本、低分辨率的热成像传感器测量来复制准确的高分辨率热成像图片，而且在关键地区，无法捕捉到可见光再生能力的应用程序可能会失败。然而，由于成像光谱范围不一致，使用可见光图像进行热成像引导超分辨率很困难。我们提出了一种新颖的数据融合框架和正则化技术，用于热成像的引导超分辨率。所提出的体系结构在丢失一个模态（高分辨率RGB图像或更低分辨率的热成像）时仍具备计算效率和轻量级的特点，并且在数据缺失的情况下具备鲁棒性。该方法在基准数据集上提供了最先进的性能，并在视觉质量和客观指标方面优于现有方法。

    Thermal imaging has numerous advantages over regular visible-range imaging since it performs well in low-light circumstances. Super-Resolution approaches can broaden their usefulness by replicating accurate high-resolution thermal pictures using measurements from low-cost, low-resolution thermal sensors. Because of the spectral range mismatch between the images, Guided Super-Resolution of thermal images utilizing visible range images is difficult. However, In case of failure to capture Visible Range Images can prevent the operations of applications in critical areas. We present a novel data fusion framework and regularization technique for Guided Super Resolution of Thermal images. The proposed architecture is computationally in-expensive and lightweight with the ability to maintain performance despite missing one of the modalities, i.e., high-resolution RGB image or the lower-resolution thermal image, and is designed to be robust in the presence of missing data. The proposed method pr
    
[^93]: 发现德拉维达语中的恐同和跨性别歧视：探索深度学习方法

    Detection of Homophobia & Transphobia in Dravidian Languages: Exploring Deep Learning Methods. (arXiv:2304.01241v1 [cs.CL])

    [http://arxiv.org/abs/2304.01241](http://arxiv.org/abs/2304.01241)

    本研究旨在探讨不同深度学习模型在德拉维达语社交媒体评论分类中的适用性，以便将其识别为恐同、跨性别歧视和非反LGBT+内容。

    

    在线社交媒体平台上辱骂性内容的增加正在影响在线用户的社交生活。使用冒犯和仇恨言论使得社交媒体变得有毒。恐同和跨性别歧视是针对LGBT+社群的冒犯性评论。及时检测和处理这些评论，向涉及此类行为的用户发出警告或及时标记是非常必要的。然而，在德拉维达语这种被认为是低资源语言的语言中，自动检测此类内容是一个具有挑战性的任务。因此，本文试图探讨不同深度学习模型在马来语和泰米尔语社交媒体评论分类中的适用性，以便将其识别为恐同、跨性别歧视和非反LGBT+内容。而其中应用的深度学习模型有卷积神经网络（CNN）、在GloVe嵌入下使用长短期记忆（LSTM）和基于变压器的学习模型（多语言BERT和IndicBERT）。

    The increase in abusive content on online social media platforms is impacting the social life of online users. Use of offensive and hate speech has been making so-cial media toxic. Homophobia and transphobia constitute offensive comments against LGBT+ community. It becomes imperative to detect and handle these comments, to timely flag or issue a warning to users indulging in such behaviour. However, automated detection of such content is a challenging task, more so in Dravidian languages which are identified as low resource languages. Motivated by this, the paper attempts to explore applicability of different deep learning mod-els for classification of the social media comments in Malayalam and Tamil lan-guages as homophobic, transphobic and non-anti-LGBT+content. The popularly used deep learning models- Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) using GloVe embedding and transformer-based learning models (Multilingual BERT and IndicBERT) are applied to the class
    
[^94]: 用自然语言处理的方法识别心理健康记录中的疼痛提及

    Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v1 [cs.CL])

    [http://arxiv.org/abs/2304.01240](http://arxiv.org/abs/2304.01240)

    该研究使用机器学习技术，成功地识别出心理健康电子健康记录中的疼痛提及，提高了对疼痛和心理健康之间关系的理解。

    

    疼痛是访问医疗资源的常见原因，并且是一个研究领域，特别是在与心理健康的重叠方面。心理健康电子健康记录是研究此重叠的良好数据来源。然而，疼痛的大量信息保存在这些记录的自由文本中，由于其歧义性，疼痛的提及呈现出独特的自然语言处理问题。本项目使用匿名的心理健康电子健康记录数据库中的数据。利用这些数据训练基于机器学习的分类算法，将句子分类为讨论患者疼痛或不讨论。这将有助于从大型数据库中提取相关疼痛信息，并将这些输出用于进一步研究疼痛和心理健康。共手动三重注释了1,985份文件，以创建黄金标准训练数据，并用于训练三种常用的分类算法。最佳模型的F1分数为0.787。结果证明了使用自然语言处理识别心理健康电子健康记录中的疼痛提及的可行性，这可以改善对疼痛和心理健康之间关系的理解。

    Pain is a common reason for accessing healthcare resources and is a growing area of research, especially in its overlap with mental health. Mental health electronic health records are a good data source to study this overlap. However, much information on pain is held in the free text of these records, where mentions of pain present a unique natural language processing problem due to its ambiguous nature. This project uses data from an anonymised mental health electronic health records database. The data are used to train a machine learning based classification algorithm to classify sentences as discussing patient pain or not. This will facilitate the extraction of relevant pain information from large databases, and the use of such outputs for further studies on pain and mental health. 1,985 documents were manually triple-annotated for creation of gold standard training data, which was used to train three commonly used classification algorithms. The best performing model achieved an F1-
    
[^95]: 基于循环领域变化的在线知识蒸馏与持续学习

    Online Distillation with Continual Learning for Cyclic Domain Shifts. (arXiv:2304.01239v1 [cs.CV])

    [http://arxiv.org/abs/2304.01239](http://arxiv.org/abs/2304.01239)

    本文提出了一种在线知识蒸馏与持续学习相结合的方法，旨在解决领域变化引起的灾难性遗忘问题。实验结果表明这种方法有效地提高了在线知识蒸馏的鲁棒性和准确性，具有潜在的应用价值。

    

    近年来，使用缓慢但准确的教师模型进行在线深度神经网络调整的在线知识蒸馏技术已经成为一种强大的技术。然而，在线知识蒸馏的一个重要挑战是当领域发生变化时出现的灾难性遗忘，这是当学生模型使用新域的数据进行更新时遗忘之前学习的知识所导致。在本文中，我们提出了一种解决这个问题的方法，通过利用持续学习方法的优势来降低领域变化的影响。具体而言，我们将几种最先进的持续学习方法集成到在线知识蒸馏的上下文中，并展示了它们在减少灾难性遗忘方面的有效性。此外，我们在循环领域变化的情况下对我们提出的解决方案进行了详细分析。我们的实验结果表明，在提高在线知识蒸馏的鲁棒性和准确性方面，我们的方法非常有效，并且具有潜在的应用价值。

    In recent years, online distillation has emerged as a powerful technique for adapting real-time deep neural networks on the fly using a slow, but accurate teacher model. However, a major challenge in online distillation is catastrophic forgetting when the domain shifts, which occurs when the student model is updated with data from the new domain and forgets previously learned knowledge. In this paper, we propose a solution to this issue by leveraging the power of continual learning methods to reduce the impact of domain shifts. Specifically, we integrate several state-of-the-art continual learning methods in the context of online distillation and demonstrate their effectiveness in reducing catastrophic forgetting. Furthermore, we provide a detailed analysis of our proposed solution in the case of cyclic domain shifts. Our experimental results demonstrate the efficacy of our approach in improving the robustness and accuracy of online distillation, with potential applications in domains 
    
[^96]: ADMG因果数据增强的实际应用指南

    A Guide for Practical Use of ADMG Causal Data Augmentation. (arXiv:2304.01237v1 [cs.LG])

    [http://arxiv.org/abs/2304.01237](http://arxiv.org/abs/2304.01237)

    ADMG因果数据增强方法能够在已知因果关系的情况下提高模型性能，但如果不合适地使用，它也可能对模型产生负面影响。

    

    在小数据环境下应用机器学习时，数据增强是必不可少的。它可以生成新样本，遵循观测到的数据分布，同时增加它们的多样性和变异性，帮助研究人员和从业人员改善他们的模型的稳健性，并将其部署到实际世界中。然而，在表格数据中使用它仍有待改进，因为很少考虑到底层数据机制的先验知识，从而限制了生成数据的准确性和多样性。因果数据增强策略被认为是解决这些问题的一种方法，它依靠因果图中编码的条件独立性。在这个框架下，本文实验性地分析了ADMG因果增强方法并考虑了不同的设置，以支持研究人员和从业人员了解在哪些条件下先验知识有助于生成新的数据点，并因此增强他们的模型的稳健性。结果表明，基于ADMG的因果数据增强确实可以在已知因果关系的情况下提高模型性能，但如果不合适地使用，它也可能对模型产生负面影响。

    Data augmentation is essential when applying Machine Learning in small-data regimes. It generates new samples following the observed data distribution while increasing their diversity and variability to help researchers and practitioners improve their models' robustness and, thus, deploy them in the real world. Nevertheless, its usage in tabular data still needs to be improved, as prior knowledge about the underlying data mechanism is seldom considered, limiting the fidelity and diversity of the generated data. Causal data augmentation strategies have been pointed out as a solution to handle these challenges by relying on conditional independence encoded in a causal graph. In this context, this paper experimentally analyzed the ADMG causal augmentation method considering different settings to support researchers and practitioners in understanding under which conditions prior knowledge helps generate new data points and, consequently, enhances the robustness of their models. The results
    
[^97]: 使用卷积注意力（ConvEntion）对天体图像时间序列进行分类

    Astronomical image time series classification using CONVolutional attENTION (ConvEntion). (arXiv:2304.01236v1 [astro-ph.IM])

    [http://arxiv.org/abs/2304.01236](http://arxiv.org/abs/2304.01236)

    本研究提出了一种基于深度学习的新方法ConvEntion，用于对天体图像时间序列分类，并能够直接使用图像对不同类型的空间对象进行分类。

    

    近年来，天体图像时间序列的处理越来越受到关注。事实上，许多跟踪瞬态天体的调查正在进行或建设中，例如 Vera Rubin Observatory Legacy Survey for Space and Time (LSST)，该瞬变事件观测计划将产生大量这些时间序列数据。相关的科学主题非常广泛，从我们银河系中的物体研究到观测测量宇宙膨胀的最远的超新星。由于有如此大量的数据可用，需要稳健自动检测和分类天体物体的工具的需求正在稳步增长。本研究基于天体图像包含比光曲线更多的信息的假设，提出了一种基于深度学习的新方法，直接使用图像对不同类型的空间对象进行分类。我们将这种方法命名为ConvEntion，即卷积注意力。

    Aims. The treatment of astronomical image time series has won increasing attention in recent years. Indeed, numerous surveys following up on transient objects are in progress or under construction, such as the Vera Rubin Observatory Legacy Survey for Space and Time (LSST), which is poised to produce huge amounts of these time series. The associated scientific topics are extensive, ranging from the study of objects in our galaxy to the observation of the most distant supernovae for measuring the expansion of the universe. With such a large amount of data available, the need for robust automatic tools to detect and classify celestial objects is growing steadily. Methods. This study is based on the assumption that astronomical images contain more information than light curves. In this paper, we propose a novel approach based on deep learning for classifying different types of space objects directly using images. We named our approach ConvEntion, which stands for CONVolutional attENTION. I
    
[^98]: 图马尔可夫神经网络的公平评估

    Fair Evaluation of Graph Markov Neural Networks. (arXiv:2304.01235v1 [cs.LG])

    [http://arxiv.org/abs/2304.01235](http://arxiv.org/abs/2304.01235)

    本论文通过引入适用于GMNN的新测试方法，对三类不同信息源对GMNN在WikiVitals数据集中的预测准确性的贡献进行严格评估，结果表明标签相关性是帮助GMNN获得优势的关键信息源。

    

    最近提出采用图马尔可夫神经网络（GMNN）改进常规图神经网络（GNN），将标签依赖性纳入半监督节点分类任务中。GMNN从理论上以严谨的方式解决问题，并使用三类信息来预测标签。与常规的GNN一样，他们使用节点特征和图结构，但他们还利用相邻节点标签的信息，提高预测的准确性。本文介绍了一个名为WikiVitals的新数据集，其中包含48k个互相引用的维基百科文章，被分类为32个类别，由2.3M边连接。我们的目标是对GMNN对这个数据集的贡献的三种不同信息源进行严格评估：文章内容、文章互相之间的连接以及标签之间的相关性。为此，我们采用了一种最近提出的适用于GNN的样本外测试方法，并将其适用于GMNN。我们的实验结果显示，GMNN在此数据集上始终优于GNN，并且标签相关性是帮助GMNN实现这些增益的关键信息源。

    Graph Markov Neural Networks (GMNN) have recently been proposed to improve regular graph neural networks (GNN) by including label dependencies into the semi-supervised node classification task. GMNNs do this in a theoretically principled way and use three kinds of information to predict labels. Just like ordinary GNNs, they use the node features and the graph structure but they moreover leverage information from the labels of neighboring nodes to improve the accuracy of their predictions. In this paper, we introduce a new dataset named WikiVitals which contains a graph of 48k mutually referred Wikipedia articles classified into 32 categories and connected by 2.3M edges. Our aim is to rigorously evaluate the contributions of three distinct sources of information to the prediction accuracy of GMNN for this dataset: the content of the articles, their connections with each other and the correlations among their labels. For this purpose we adapt a method which was recently proposed for perf
    
[^99]: 应用卷积神经网络对潜在场源面磁图预测太阳风速度

    Prediction of solar wind speed by applying convolutional neural network to potential field source surface (PFSS) magnetograms. (arXiv:2304.01234v1 [astro-ph.SR])

    [http://arxiv.org/abs/2304.01234](http://arxiv.org/abs/2304.01234)

    本研究通过卷积神经网络和潜在场源表面磁图构建了一个模型，能够预测太阳风速度，并且在连续测试数据集上的平均相关系数为0.52。

    

    准确的太阳风速模型对于太空天气预测、灾难性事件警告和其他与太阳风-磁层相互作用有关的问题非常重要。本研究基于卷积神经网络（CNN）和潜在场源表面（PFSS）磁图构建一个模型，考虑太阳风源表面$R_{\rm SS}=2.5R_\odot$，旨在预测太阳-地球系统的Lagrange 1（L1）点的太阳风速。我们的模型输入由$R_{\rm SS}$处的四张PFSS磁图组成，这些磁图距目标时刻分别为7、6、5和4天。为提高模型效率，我们使用了归一化磁图。我们使用全球振荡网络组（GONG）光球磁图和潜在场外推模型生成PFSS磁图。该模型对连续测试数据集的预测平均相关系数（CC）为0.52，均方根误差为...

    An accurate solar wind speed model is important for space weather predictions, catastrophic event warnings, and other issues concerning solar wind - magnetosphere interaction. In this work, we construct a model based on convolutional neural network (CNN) and Potential Field Source Surface (PFSS) magnetograms, considering a solar wind source surface of $R_{\rm SS}=2.5R_\odot$, aiming to predict the solar wind speed at the Lagrange 1 (L1) point of the Sun-Earth system. The input of our model consists of four Potential Field Source Surface (PFSS) magnetograms at $R_{\rm SS}$, which are 7, 6, 5, and 4 days before the target epoch. Reduced magnetograms are used to promote the model's efficiency. We use the Global Oscillation Network Group (GONG) photospheric magnetograms and the potential field extrapolation model to generate PFSS magnetograms at the source surface. The model provides predictions of the continuous test dataset with an averaged correlation coefficient (CC) of 0.52 and a root
    
[^100]: 多模态知觉语言模型在急诊室结果预测中的应用

    Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department. (arXiv:2304.01233v1 [cs.CL])

    [http://arxiv.org/abs/2304.01233](http://arxiv.org/abs/2304.01233)

    本文提出了一种基于多模态知觉语言模型的方法，用于结果预测和患者分诊。实验结果表明，该模型在急诊科临床决策方面具有显著的潜力。

    

    语言建模已经在生成具有良好准确性和高语义连贯性的令人信服的文本方面取得了重大进展。一个有趣的研究方向是使用上下文信息来增强这些强大的模型以用于特定的应用程序。在这项工作中，我们探索了面向医疗保健应用的多模态语言建模。我们对基于三角洲区域记录的首席投诉信息和生命体征的文本信息进行了结果预测和患者分诊。我们改编了Perceiver——一种通用变换器模型，它已经在多个应用程序中展现出可行的结果。由于生命体征模态以表格形式呈现，我们修改了Perceiver的位置编码以确保置换不变性。我们使用MIMIC-IV ED数据集上的120K次访问来评估多模态语言模型对诊断代码预测的任务。在实验分析中，我们展示了多模态改善了预测表现，相比于单模态模型，并且Perceiver在任务中胜过了其他现有的多模态语言模型。该模型在改进急诊科临床决策方面具有显著的潜力。

    Language modeling have shown impressive progress in generating compelling text with good accuracy and high semantic coherence. An interesting research direction is to augment these powerful models for specific applications using contextual information. In this work, we explore multi-modal language modeling for healthcare applications. We are interested in outcome prediction and patient triage in hospital emergency department based on text information in chief complaints and vital signs recorded at triage. We adapt Perceiver - a modality-agnostic transformer-based model that has shown promising results in several applications. Since vital-sign modality is represented in tabular format, we modified Perceiver position encoding to ensure permutation invariance. We evaluated the multi-modal language model for the task of diagnosis code prediction using MIMIC-IV ED dataset on 120K visits. In the experimental analysis, we show that mutli-modality improves the prediction performance compared w
    
[^101]: SEENN: 实现时间编码早期退出神经网络的研究

    SEENN: Towards Temporal Spiking Early-Exit Neural Networks. (arXiv:2304.01230v1 [cs.NE])

    [http://arxiv.org/abs/2304.01230](http://arxiv.org/abs/2304.01230)

    本研究提出了一种名为SEENN的方法，通过对时间步数进行细粒度调整，以减少不必要的计算并提高有效性。同时，SEENN达到了多个基准数据集的最先进准确度表现。

    

    脉冲神经网络（SNNs）因其生物学特性成为传统人工神经网络（ANNs）的替代品，最近变得越来越流行。SNNs既费用效益又易于部署，因为它们可以用二进制脉冲以空间和时间方式处理输入。然而，我们观察到SNNs中的信息容量受到时间步骤数量的影响，导致准确性和效率的权衡。在本研究中，我们研究了SNNs中时间步骤数量的细粒度调整。具体地，我们将时间步数视为一个变量，针对不同的输入样本来减少冗余时间步骤。我们称这种方法为早期退出脉冲神经网络（SEENN）。为了确定适当的时间步数，我们提出了SEENN-I，它使用置信度阈值来过滤不确定的预测，以及SEENN-II，它通过强化学习确定时间步骤的数量。此外，我们证明SEENN比传统SNNs更有效，并在几个基准数据集上实现了最先进的准确性。

    Spiking Neural Networks (SNNs) have recently become more popular as a biologically plausible substitute for traditional Artificial Neural Networks (ANNs). SNNs are cost-efficient and deployment-friendly because they process input in both spatial and temporal manners using binary spikes. However, we observe that the information capacity in SNNs is affected by the number of timesteps, leading to an accuracy-efficiency tradeoff. In this work, we study a fine-grained adjustment of the number of timesteps in SNNs. Specifically, we treat the number of timesteps as a variable conditioned on different input samples to reduce redundant timesteps for certain data. We call our method Spiking Early-Exit Neural Networks (SEENNs). To determine the appropriate number of timesteps, we propose SEENN-I which uses a confidence score thresholding to filter out the uncertain predictions, and SEENN-II which determines the number of timesteps by reinforcement learning. Moreover, we demonstrate that SEENN is 
    
[^102]: 基于傅里叶神经算子的分辨率不变分类算法研究

    Resolution-Invariant Image Classification based on Fourier Neural Operators. (arXiv:2304.01227v1 [cs.CV])

    [http://arxiv.org/abs/2304.01227](http://arxiv.org/abs/2304.01227)

    本文研究了基于傅里叶神经算子的图像分类算法，提出了一种离散不变的神经网络用于无限维函数空间中的算子近似，并将CNNs转换为FNOs。

    

    本文研究了在图像分类中使用傅里叶神经算子（FNOs）相对于标准卷积神经网络（CNNs）的优势。神经算子是一种离散不变的神经网络的推广，用于近似无限维函数空间中的算子。FNOs是一种具有特定参数化的神经算子，已经成功地应用于参数化PDEs的上下文中。我们将FNO架构导出为Lebesgue空间上连续和Fréchet可微神经算子的示例。我们进一步展示了如何将CNN转换为FNO，反之亦然，并提出了等价差值适应的架构。

    In this paper we investigate the use of Fourier Neural Operators (FNOs) for image classification in comparison to standard Convolutional Neural Networks (CNNs). Neural operators are a discretization-invariant generalization of neural networks to approximate operators between infinite dimensional function spaces. FNOs - which are neural operators with a specific parametrization have been applied successfully in the context of parametric PDEs. We derive the FNO architecture as an example for continuous and Fr\'echet-differentiable neural operators on Lebesgue spaces. We further show how CNNs can be converted into FNOs and vice versa and propose an interpolation-equivariant adaptation of the architecture.
    
[^103]: 基于超图对比学习的异常事件检测

    Abnormal Event Detection via Hypergraph Contrastive Learning. (arXiv:2304.01226v1 [cs.LG])

    [http://arxiv.org/abs/2304.01226](http://arxiv.org/abs/2304.01226)

    本论文提出了一种基于超图对比学习的异常事件检测方法，可以完全捕捉异常事件模式，实验表明该方法在两个公共数据集上明显优于现有方法。

    

    异常事件检测在许多实际应用中扮演着重要角色，它指的是挖掘涉及实体之间不寻常的相互作用。 以往的研究大多将此任务简化为检测异常的成对交互作用。 然而，现实世界中的事件可能包含多种类型的属性实体和它们之间的复杂相互作用，这形成了属性异构信息网络。随着社交网络的蓬勃发展，属性异构信息网络中的异常事件检测已成为一项重要但很少被探索的任务。

    Abnormal event detection, which refers to mining unusual interactions among involved entities, plays an important role in many real applications. Previous works mostly over-simplify this task as detecting abnormal pair-wise interactions. However, real-world events may contain multi-typed attributed entities and complex interactions among them, which forms an Attributed Heterogeneous Information Network (AHIN). With the boom of social networks, abnormal event detection in AHIN has become an important, but seldom explored task. In this paper, we firstly study the unsupervised abnormal event detection problem in AHIN. The events are considered as star-schema instances of AHIN and are further modeled by hypergraphs. A novel hypergraph contrastive learning method, named AEHCL, is proposed to fully capture abnormal event patterns. AEHCL designs the intra-event and inter-event contrastive modules to exploit self-supervised AHIN information. The intra-event contrastive module captures the pair
    
[^104]: 优化 KNN 模型的 Shapley Interaction 计算从 O(2^n) 到 O(t n^2)。

    Optimizing Data Shapley Interaction Calculation from O(2^n) to O(t n^2) for KNN models. (arXiv:2304.01224v1 [cs.LG])

    [http://arxiv.org/abs/2304.01224](http://arxiv.org/abs/2304.01224)

    本文提出了一种名为 "STI-KNN" 的算法，可以在短时间内对 KNN 模型进行精确的配对交互 Shapley 值计算，从而有效地评估每个训练数据点的价值，提高训练结果和人工智能应用的有效性。

    

    随着数据可用性和使用率的快速增长，量化每个训练数据点的附加价值已成为人工智能领域中关键的过程。Shapley 值已被公认为是一种有效的数据估值方法，使得训练集汇总、获取和异常值删除变得更加高效。本文介绍了一种创新算法 "STI-KNN"，它可以在O(t n^2)时间内计算准确的 KNN 模型的精确配对交互 Shapley 值，这是比基线方法的 O(2^n) 时间复杂度显著提高了。使用 STI-KNN，我们可以高效准确地评估单个数据点的价值，从而改善训练结果，最终增强人工智能应用的有效性。

    With the rapid growth of data availability and usage, quantifying the added value of each training data point has become a crucial process in the field of artificial intelligence. The Shapley values have been recognized as an effective method for data valuation, enabling efficient training set summarization, acquisition, and outlier removal. In this paper, we introduce "STI-KNN", an innovative algorithm that calculates the exact pair-interaction Shapley values for KNN models in O(t n^2) time, which is a significant improvement over the O(2^n)$ time complexity of baseline methods. By using STI-KNN, we can efficiently and accurately evaluate the value of individual data points, leading to improved training outcomes and ultimately enhancing the effectiveness of artificial intelligence applications.
    
[^105]: 基于改进的多智能体软actor-critic算法的多微电网协同优化调度

    Multi-Microgrid Collaborative Optimization Scheduling Using an Improved Multi-Agent Soft Actor-Critic Algorithm. (arXiv:2304.01223v1 [eess.SY])

    [http://arxiv.org/abs/2304.01223](http://arxiv.org/abs/2304.01223)

    本文提出了一种基于多智能体集中式训练分布式执行框架的多微电网协同优化调度模型，并使用改进的MASAC算法对能源管理问题进行处理，成功实现了不同实体之间的功率互补和降低系统运行成本。

    

    多微电网系统实现了多种可再生能源的互补共存，促进了电力交易。本文针对由不同运营实体拥有的多个可再生能源微电网组成的多微电网系统的能源管理问题，提出了一种基于多智能体集中式训练分布式执行框架的多微电网协同优化调度模型。为了增强处理各种不确定性的通用能力，我们还提出了一种改进的多智能体软actor-critic（MASAC）算法，在多智能体之间实现能量交易，并使用自动化机器学习（AutoML）来优化MASAC超参数，以进一步提高深度强化学习（DRL）的泛化能力。测试结果表明，所提出的方法成功实现了不同实体之间的功率互补，降低了多微电网系统的运行成本。

    The implementation of a multi-microgrid (MMG) system with multiple renewable energy sources enables the facilitation of electricity trading. To tackle the energy management problem of a MMG system, which consists of multiple renewable energy microgrids belonging to different operating entities, this paper proposes a MMG collaborative optimization scheduling model based on a multi-agent centralized training distributed execution framework. To enhance the generalization ability of dealing with various uncertainties, we also propose an improved multi-agent soft actor-critic (MASAC) algorithm, which facilitates en-ergy transactions between multi-agents in MMG, and employs automated machine learning (AutoML) to optimize the MASAC hyperparameters to further improve the generalization of deep reinforcement learning (DRL). The test results demonstrate that the proposed method successfully achieves power complementarity between different entities, and reduces the MMG system operating cost. Addi
    
[^106]: NeuroDAVIS: 用于数据可视化的神经网络模型

    NeuroDAVIS: A neural network model for data visualization. (arXiv:2304.01222v1 [cs.HC])

    [http://arxiv.org/abs/2304.01222](http://arxiv.org/abs/2304.01222)

    NeuroDAVIS是一种无监督深度神经网络模型，它可以在不影响数据局部和全局结构的情况下提取重要特征并在更低的维度上进行数据可视化。

    

    高维数据的降维和可视化一直是一个具有挑战性的问题。现代高通量技术产生了多种视图的新的高维数据集，这些数据集包含了新的数据类型。对这些数据集的可视化需要适当的方法，可以在不影响数据中的局部和全局结构的情况下发现数据中的隐藏模式。然而，能够实现这个任务的方法非常少。在这项工作中，我们介绍了一种新的无监督深度神经网络模型NeuroDAVIS，用于数据可视化。NeuroDAVIS能够从数据中提取重要特征，而不需要假设任何数据分布，并在更低的维度上进行有效的可视化。理论上证明了高维数据的邻近关系在低维空间中得到了保留。NeuroDAVIS的性能已在各种合成和实际数据集上进行了评估。

    The task of dimensionality reduction and visualization of high-dimensional datasets remains a challenging problem since long. Modern high-throughput technologies produce newer high-dimensional datasets having multiple views with relatively new data types. Visualization of these datasets require proper methodology that can uncover hidden patterns in the data without affecting the local and global structures within the data. To this end, however, very few such methodology exist, which can realise this task. In this work, we have introduced a novel unsupervised deep neural network model, called NeuroDAVIS, for data visualization. NeuroDAVIS is capable of extracting important features from the data, without assuming any data distribution, and visualize effectively in lower dimension. It has been shown theoritically that neighbourhood relationship of the data in high dimension remains preserved in lower dimension. The performance of NeuroDAVIS has been evaluated on a wide variety of synthet
    
[^107]: DoE2Vec：基于深度学习的特征用于探索性景观分析

    DoE2Vec: Deep-learning Based Features for Exploratory Landscape Analysis. (arXiv:2304.01219v1 [math.OC])

    [http://arxiv.org/abs/2304.01219](http://arxiv.org/abs/2304.01219)

    DoE2Vec 是一种基于深度学习的方法，用于学习任何实验设计（DoE）的信息潜在表达，并可以满足优化景观特征的下游元学习任务，同时避免了经典ELA分析中的特征工程问题。在分类任务中与经典ELA特征互补使用时，可显着提高性能。

    

    我们提出DoE2Vec，这是一种基于变分自编码器（VAE）的方法，用于学习优化景观特征，以用于下游元学习任务，例如自动选择优化算法。通过使用随机函数生成器生成的大型训练数据集，DoE2Vec可以自学习任何实验设计（DoE）的信息潜在表达。与经典的探索性景观分析（ELA）方法不同，我们的方法不需要任何特征工程，并且易于应用于高维搜索空间。为了验证，我们检查了潜在重建的质量，并使用不同的实验分析了潜在表达式。这些潜在表达式不仅在识别类似（易于评估）的替代函数上显示出有前途的潜力，而且在分类任务中与经典的ELA特征互补使用时也可以显着提升性能。

    We propose DoE2Vec, a variational autoencoder (VAE)-based methodology to learn optimization landscape characteristics for downstream meta-learning tasks, e.g., automated selection of optimization algorithms. Principally, using large training data sets generated with a random function generator, DoE2Vec self-learns an informative latent representation for any design of experiments (DoE). Unlike the classical exploratory landscape analysis (ELA) method, our approach does not require any feature engineering and is easily applicable for high dimensional search spaces. For validation, we inspect the quality of latent reconstructions and analyze the latent representations using different experiments. The latent representations not only show promising potentials in identifying similar (cheap-to-evaluate) surrogate functions, but also can significantly boost performances when being used complementary to the classical ELA features in classification tasks.
    
[^108]: POLAR-Express: 神经网络控制系统的高效准确形式可达性分析

    POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems. (arXiv:2304.01218v1 [eess.SY])

    [http://arxiv.org/abs/2304.01218](http://arxiv.org/abs/2304.01218)

    POLAR-Express 是一种高效且准确的形式可达性分析工具，用于验证神经网络控制系统的安全性。它使用 Taylor 模型算术和逐层传播技术，可以分析具有连续激活功能的前馈神经网络，并在 ReLU 激活函数上提供了一种更有效的精确传播 TM 的新方法。

    

    在挑战性的控制问题上，扮演控制器角色的神经网络 (NN) 展示出了令人印象深刻的实验性能。但神经网络控制系统 (NNCS) 在实际应用中的潜在采用也引起了日益增长的对这些 NNCS 安全性的担忧，特别是在安全关键应用中的使用。本文提出了 POLAR-Express，一种高效且准确的形式可达性分析工具，用于验证 NNCS 的安全性。POLAR-Express 使用 Taylor 模型算术，逐层横跨神经网络来传播 Taylor 模型 (TM) 以计算神经网络函数的近似值。它可以用于分析任何具有连续激活功能的前馈神经网络。我们还提出了一种在 ReLU 激活函数上更有效地精确传播 TM 的新方法。此外，POLAR-Express 为逐层传播提供了并行计算支持。

    Neural networks (NNs) playing the role of controllers have demonstrated impressive empirical performances on challenging control problems. However, the potential adoption of NN controllers in real-life applications also gives rise to a growing concern over the safety of these neural-network controlled systems (NNCSs), especially when used in safety-critical applications. In this work, we present POLAR-Express, an efficient and precise formal reachability analysis tool for verifying the safety of NNCSs. POLAR-Express uses Taylor model arithmetic to propagate Taylor models (TMs) across a neural network layer-by-layer to compute an overapproximation of the neural-network function. It can be applied to analyze any feed-forward neural network with continuous activation functions. We also present a novel approach to propagate TMs more efficiently and precisely across ReLU activation functions. In addition, POLAR-Express provides parallel computation support for the layer-by-layer propagation
    
[^109]: 基于树模型的机器学习方法预测蜜蜂生产量

    A Machine Learning Approach to Forecasting Honey Production with Tree-Based Methods. (arXiv:2304.01215v1 [cs.LG])

    [http://arxiv.org/abs/2304.01215](http://arxiv.org/abs/2304.01215)

    该论文使用树模型方法预测意大利蜂箱的蜜蜂生产量变化，帮助蜜蜂养殖者评估风险，保护蜜蜂活动。

    

    蜜蜂养殖业在过去几年中经历了可观的生产变化，这是由于逐渐恶化的气候条件造成的。这些现象可能会对蜜蜂活动不利。我们使用树模型方法区分蜜蜂生产量的影响因素，并预测意大利蜂箱的蜜蜂生产量变化，意大利是欧洲最大的蜂蜜生产国之一。该数据库包含了从2019年到2022年收集的数百个蜂箱的数据，采用了先进的精度蜜蜂养殖技术。我们训练和解释机器学习模型，使其具有指导性而不仅仅是预测性。与标准线性技术相比，树模型方法具有更高的预测性能，可以更好地保护蜜蜂活动并评估风险管理中蜜蜂养殖者的潜在损失。

    The beekeeping sector has undergone considerable production variations over the past years due to adverse weather conditions, occurring more frequently as climate change progresses. These phenomena can be high-impact and cause the environment to be unfavorable to the bees' activity. We disentangle the honey production drivers with tree-based methods and predict honey production variations for hives in Italy, one of the largest honey producers in Europe. The database covers hundreds of beehive data from 2019-2022 gathered with advanced precision beekeeping techniques. We train and interpret the machine learning models making them prescriptive other than just predictive. Superior predictive performances of tree-based methods compared to standard linear techniques allow for better protection of bees' activity and assess potential losses for beekeepers for risk management.
    
[^110]: PromptORE -- 一种全新的无监督关系抽取方法

    PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction. (arXiv:2304.01209v1 [cs.CL])

    [http://arxiv.org/abs/2304.01209](http://arxiv.org/abs/2304.01209)

    提出了“基于提示的开放关系抽取”模型，在无监督设置下不需要超参数调整，实现了全新的无监督关系抽取方法。

    

    无监督关系抽取旨在识别文本中实体之间的关系，而在训练期间没有标记的数据可用。这对于没有注释数据集的特定领域关系抽取和先验未知关系类型的开放领域关系抽取特别相关。虽然最近的方法取得了有希望的结果，但它们严重依赖于超参数，调整这些超参数通常需要标记数据。为了减轻对超参数的依赖，我们提出了PromptORE，即“基于提示的开放关系抽取”模型。我们将新的提示调整范例适应于无监督设置，并用它来嵌入表达关系的句子。然后我们对这些嵌入进行聚类，发现候选关系，并尝试不同的策略来自动估计适当的聚类数量。据我们所知，PromptORE是第一个不需要超参数调整的无监督关系抽取模型。

    Unsupervised Relation Extraction (RE) aims to identify relations between entities in text, without having access to labeled data during training. This setting is particularly relevant for domain specific RE where no annotated dataset is available and for open-domain RE where the types of relations are a priori unknown. Although recent approaches achieve promising results, they heavily depend on hyperparameters whose tuning would most often require labeled data. To mitigate the reliance on hyperparameters, we propose PromptORE, a ''Prompt-based Open Relation Extraction'' model. We adapt the novel prompt-tuning paradigm to work in an unsupervised setting, and use it to embed sentences expressing a relation. We then cluster these embeddings to discover candidate relations, and we experiment different strategies to automatically estimate an adequate number of clusters. To the best of our knowledge, PromptORE is the first unsupervised RE model that does not need hyperparameter tuning. Resul
    
[^111]: “Polytuplet Loss: 训练阅读理解和逻辑推理模型的反向方法”

    Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2304.01046](http://arxiv.org/abs/2304.01046)

    本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。

    

    在整个学校教育过程中，学生们将受到阅读理解和逻辑推理的考验。学生们已经开发了各种策略来完成此类考试，其中有些被认为是通常表现优于其他策略的。这样一种策略涉及强调相对准确性而非绝对准确性，理论上可以在不完全掌握解题所需信息的情况下得出正确答案。本文研究了应用这种策略来训练迁移学习模型以解决阅读理解和逻辑推理问题的有效性。这些模型在具有挑战性的阅读理解和逻辑推理基准数据集ReClor上进行了评估。尽管以前的研究集中于逻辑推理技能，但我们专注于一种通用的训练方法和模型架构。我们提出了Polytuplet Loss函数，是三元组损失函数的扩展，以确保优先学习答案选择的相对正确性而非学习绝对正确性。

    Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
    
[^112]: Euler Equation上的最优质量输运

    Optimal Mass Transport over the Euler Equation. (arXiv:2304.00595v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2304.00595](http://arxiv.org/abs/2304.00595)

    本文分析了欧拉方程的控制问题，发现其是一个最优质量输运问题的实例，并提供了该问题的静态和动态版本的Eulerian OMT解决方案。

    

    我们考虑有限时间内的最佳操纵问题，其中涉及到由欧拉方程控制的联合状态概率分布的优化传递。该问题及其解决方案相当于通过反馈控制刚体的旋转，这在实践中非常重要，例如在具有随机初始和终端状态的航天器的角稳定中。我们阐明了这个问题是具有双线性先验漂移的最优质量输运问题的一个实例。我们推导了Eulerian OMT的静态和动态版本，并提供了最优控制器综合的分析和数值结果。

    We consider the finite horizon optimal steering of the joint state probability distribution subject to the angular velocity dynamics governed by the Euler equation. The problem and its solution amounts to controlling the spin of a rigid body via feedback, and is of practical importance, for example, in angular stabilization of a spacecraft with stochastic initial and terminal states. We clarify how this problem is an instance of the optimal mass transport (OMT) problem with bilinear prior drift. We deduce both static and dynamic versions of the Eulerian OMT, and provide analytical and numerical results for the synthesis of the optimal controller.
    
[^113]: 从孤立的岛屿到泛大陆：统一语义空间用于人类行为理解

    From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding. (arXiv:2304.00553v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.00553](http://arxiv.org/abs/2304.00553)

    本文提出了一个统一的语义空间Poincare行为语义空间，通过将以前数据集的类别与这个语义空间对齐，收集（图像/视频/骨架/MoCap）数据集到一个统一的数据库中，即将“孤立的岛屿”桥接成一个“泛大陆”，这将有助于推进可推广的行为学习。

    

    行为理解是一项重要的研究领域并且备受关注。它可以被理解为从行为的物理空间到语义空间的映射。通常，研究人员会根据独特的选择构建行为数据集，以定义各种类别并将基准线推向极限。因此，数据集之间存在语义差距和不同的类别粒度，就像“孤立的岛屿”一样互不兼容，例如数据集A中的家务和数据集B中的洗盘子。我们认为需要一个更具原则性的语义空间来集中社区的力量，并使我们能够一起使用所有数据集以追求可推广的行为学习。为此，我们设计了一个Poincare行为语义空间，给定动词分类层次结构并涵盖大量行为。通过将以前数据集的类别与我们的语义空间对齐，我们将（图像/视频/骨架/MoCap）数据集收集到一个统一的数据库中，使用统一的标签系统，即将“孤立的岛屿”桥接成一个“泛大陆”。因此，我们对这个统一的数据库进行了广泛的实验，结果证明了我们提出的语义空间和统一数据库的有效性。

    Action understanding matters and attracts attention. It can be formed as the mapping from the action physical space to the semantic space. Typically, researchers built action datasets according to idiosyncratic choices to define classes and push the envelope of benchmarks respectively. Thus, datasets are incompatible with each other like "Isolated Islands" due to semantic gaps and various class granularities, e.g., do housework in dataset A and wash plate in dataset B. We argue that a more principled semantic space is an urgent need to concentrate the community efforts and enable us to use all datasets together to pursue generalizable action learning. To this end, we design a Poincare action semantic space given verb taxonomy hierarchy and covering massive actions. By aligning the classes of previous datasets to our semantic space, we gather (image/video/skeleton/MoCap) datasets into a unified database in a unified label system, i.e., bridging "isolated islands" into a "Pangea". Accord
    
[^114]: 几何约束提高了对稀疏观测的随机动力学的推断

    Geometric constraints improve inference of sparsely observed stochastic dynamics. (arXiv:2304.00423v1 [stat.ME])

    [http://arxiv.org/abs/2304.00423](http://arxiv.org/abs/2304.00423)

    本文提出一种新的方法，该方法利用数据驱动的控制，既考虑了系统不变密度的几何形状，又能对系统进行有效识别。

    

    许多自由度的系统在多个尺度上演化的动力学通常以随机微分方程的形式进行建模。通常这些方程的结构形式是未知的，系统动力学的唯一表现形式是在离散时间点上的观测。尽管它们被广泛使用，但准确地从稀疏时域观测中推断这些系统仍然具有挑战性。传统的推断方法要么集中于观测的时间结构，忽略系统不变密度的几何形状，要么使用系统不变密度的几何逼近，这些逼近仅适用于保守的驱动力。为了解决这些局限性，我们在此提出了一种新的方法，它将这两个视角调和在一起。我们提出了一种路径增强方案，它使用数据驱动的控制来考虑不变系统密度的几何形状。对增强路径的非参数推断，实现了对系统的有效识别。

    The dynamics of systems of many degrees of freedom evolving on multiple scales are often modeled in terms of stochastic differential equations. Usually the structural form of these equations is unknown and the only manifestation of the system's dynamics are observations at discrete points in time. Despite their widespread use, accurately inferring these systems from sparse-in-time observations remains challenging. Conventional inference methods either focus on the temporal structure of observations, neglecting the geometry of the system's invariant density, or use geometric approximations of the invariant density, which are limited to conservative driving forces. To address these limitations, here, we introduce a novel approach that reconciles these two perspectives. We propose a path augmentation scheme that employs data-driven control to account for the geometry of the invariant system's density. Non-parametric inference on the augmented paths, enables efficient identification of the
    
[^115]: 参数化PDE的多级CNN

    Multilevel CNNs for Parametric PDEs. (arXiv:2304.00388v1 [cs.LG])

    [http://arxiv.org/abs/2304.00388](http://arxiv.org/abs/2304.00388)

    该论文提出了一种用于有效数值解决参数化PDEs的多级CNN方法，有实质性的改进并能以任意精度近似多重网格V循环。

    

    我们将部分微分方程（PDEs）的多级求解器的概念与基于神经网络的深度学习相结合，提出一种新的解决高维参数PDEs的有效数值方法。理论分析表明，该架构能够以任意精度近似多重网格V循环，其权重数量仅与最细网格的分辨率对数有关，这种方法有实质性的改进。

    We combine concepts from multilevel solvers for partial differential equations (PDEs) with neural network based deep learning and propose a new methodology for the efficient numerical solution of high-dimensional parametric PDEs. An in-depth theoretical analysis shows that the proposed architecture is able to approximate multigrid V-cycles to arbitrary precision with the number of weights only depending logarithmically on the resolution of the finest mesh. As a consequence, approximation bounds for the solution of parametric PDEs by neural networks that are independent on the (stochastic) parameter dimension can be derived. The performance of the proposed method is illustrated on high-dimensional parametric linear elliptic PDEs that are common benchmark problems in uncertainty quantification. We find substantial improvements over state-of-the-art deep learning-based solvers. As particularly challenging examples, random conductivity with high-dimensional non-affine Gaussian fields in 10
    
[^116]: RL中的反向攻击保护：恢复触发状态方法

    Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. (arXiv:2304.00252v1 [cs.LG])

    [http://arxiv.org/abs/2304.00252](http://arxiv.org/abs/2304.00252)

    本文提出了恢复触发状态(RTS)方法，用于保护RL代理免受反向攻击。该方法涉及构建替代网络来近似动态模型，并将触发状态恢复为干净状态来防止攻击者通过触发器激活隐藏在代理中的后门。

    

    反向攻击可以使恶意用户操纵环境或破坏训练数据，并将一个隐藏的后门插入到训练代理程序中。这种攻击危及RL系统的可靠性，在各个关键领域可能会造成灾难性的影响。与此相比，对于RL中的反向攻击有效的防御措施的研究相对较少。本文提出了一种新颖的方法——恢复触发状态(RTS)，能够有效地保护受害代理免受反向攻击。 RTS需要构建一个替代网络来近似动态模型。开发人员可以通过将触发状态恢复为干净状态来防止攻击者通过触发器激活代理中隐藏的后门。在训练替代网络来预测状态时，我们将代理动作信息并入，减少代理在预测状态上采取的动作和实际状态上采取的动作之间的差异。

    A backdoor attack allows a malicious user to manipulate the environment or corrupt the training data, thus inserting a backdoor into the trained agent. Such attacks compromise the RL system's reliability, leading to potentially catastrophic results in various key fields. In contrast, relatively limited research has investigated effective defenses against backdoor attacks in RL. This paper proposes the Recovery Triggered States (RTS) method, a novel approach that effectively protects the victim agents from backdoor attacks. RTS involves building a surrogate network to approximate the dynamics model. Developers can then recover the environment from the triggered state to a clean state, thereby preventing attackers from activating backdoors hidden in the agent by presenting the trigger. When training the surrogate to predict states, we incorporate agent action information to reduce the discrepancy between the actions taken by the agent on predicted states and the actions taken on real sta
    
[^117]: oBERTa: 通过改进初始化、蒸馏和剪枝来提高稀疏迁移学习

    oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])

    [http://arxiv.org/abs/2303.17612](http://arxiv.org/abs/2303.17612)

    oBERTa是一组易于使用的语言模型，通过改进初始化、蒸馏、剪枝等技术，可以在不需要模型压缩方面的专业知识的情况下提高稀疏迁移学习的效率和准确性。

    

    本文介绍了oBERTa语言模型的范围，它是一组易于使用的语言模型，允许自然语言处理（NLP）从业者在不需要模型压缩方面的专业知识的情况下获得3.8到24.3倍的更快速的模型。oBERTa扩展了现有的剪枝、知识蒸馏和量化工作，并利用冻结的嵌入来改进知识蒸馏，并改进模型初始化，以在广泛的传递任务上提供更高的准确性。在生成oBERTa时，我们探索了高度优化的RoBERTa与BERT在预训练和微调期间剪枝方面的不同之处，并发现它在微调期间不太适合压缩。我们探索了oBERTa在七个具有代表性的NLP任务上的使用，并发现改进的压缩技术使得经过剪枝的oBERTa模型能够匹配BERTBASE的性能，并超过SQUAD V1.1问答数据的Prune OFA Large的性能。

    In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
    
[^118]: 被忽视的免费午餐——使用注释副产品学习图像分类器

    Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts. (arXiv:2303.17595v1 [cs.CV])

    [http://arxiv.org/abs/2303.17595](http://arxiv.org/abs/2303.17595)

    本研究指出传统的图片分类器学习过程忽视注释过程中的辅助信息，提出了使用注释副产品来训练模型的新方法，该方法可以减少虚假相关性并提高模型精度。

    

    图像分类器的监督学习将人类知识通过图像和相应标签（X，Y）的对应关系转化为参数模型。本文认为这种简单且广泛使用的人类知识表示忽视了注释过程中丰富的辅助信息，例如在图像选择后留下的鼠标轨迹和点击的时间序列等。我们的洞见是，这些注释副产品Z提供了近似的人类关注信息，弱化了模型对前景线索的关注，减少了虚假相关性并防止了捷径学习。为了验证这一点，我们创建了ImageNet-AB和COCO-AB。它们是通过复制相应的原始注释任务来获得的ImageNet和COCO训练集，增加了样本级别的注释副产品。我们称使用注释副产品来训练模型的新方法为学习注释副产品（LUAB）。我们展示了一个简单的多任务损失，用于同时回归Z和Y已经可以提高模型精度。

    Supervised learning of image classifiers distills human knowledge into a parametric model through pairs of images and corresponding labels (X,Y). We argue that this simple and widely used representation of human knowledge neglects rich auxiliary information from the annotation procedure, such as the time-series of mouse traces and clicks left after image selection. Our insight is that such annotation byproducts Z provide approximate human attention that weakly guides the model to focus on the foreground cues, reducing spurious correlations and discouraging shortcut learning. To verify this, we create ImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with sample-wise annotation byproducts, collected by replicating the respective original annotation tasks. We refer to the new paradigm of training models with annotation byproducts as learning using annotation byproducts (LUAB). We show that a simple multitask loss for regressing Z together with Y already improves 
    
[^119]: 潜在位置模型上的图形Nadaraya-Watson估计器

    The Graphical Nadaraya-Watson Estimator on Latent Position Models. (arXiv:2303.17229v1 [stat.ML])

    [http://arxiv.org/abs/2303.17229](http://arxiv.org/abs/2303.17229)

    研究了潜在位置模型上的图形Nadaraya-Watson估计器的性质，对于更复杂的方法有理论指导意义。

    

    鉴于有标记节点的图形，我们对估计器的质量感兴趣，该估计器针对未标记节点预测其标记邻居的观测值的平均值。我们在这个背景下严格研究了浓度属性、方差界和风险界。虽然估计器本身非常简单，数据生成过程对于实际应用过于理想，但我们相信我们的小步骤将有助于更复杂方法（如图形神经网络）的理论理解。

    Given a graph with a subset of labeled nodes, we are interested in the quality of the averaging estimator which for an unlabeled node predicts the average of the observations of its labeled neighbours. We rigorously study concentration properties, variance bounds and risk bounds in this context. While the estimator itself is very simple and the data generating process is too idealistic for practical applications, we believe that our small steps will contribute towards the theoretical understanding of more sophisticated methods such as Graph Neural Networks.
    
[^120]: 基于模块化正则化可改善观察含噪数据的高斯图模型

    Module-based regularization improves Gaussian graphical models when observing noisy data. (arXiv:2303.16796v1 [physics.data-an])

    [http://arxiv.org/abs/2303.16796](http://arxiv.org/abs/2303.16796)

    建议将推断网络的模块化结构整合到正则化强度的交叉验证中，以改善高斯图模型在观察含噪数据时的表现。

    

    研究人员经常使用高斯图模型表示多变量相关数据中的关系，这些模型需要正则化来稀疏模型。我们建议在正则化强度的交叉验证中，将推断网络的模块化结构整合起来以平衡欠拟合和过拟合。使用合成和真实数据，我们发现与使用高斯对数似然进行交叉验证的标准方法（图形套索法）相比，这种方法可以更好地恢复和推断含噪声数据中的模块化结构。

    Researchers often represent relations in multi-variate correlational data using Gaussian graphical models, which require regularization to sparsify the models. Acknowledging that they often study the modular structure of the inferred network, we suggest integrating it in the cross-validation of the regularization strength to balance under- and overfitting. Using synthetic and real data, we show that this approach allows us to better recover and infer modular structure in noisy data compared with the graphical lasso, a standard approach using the Gaussian log-likelihood when cross-validating the regularization strength.
    
[^121]: 基于列表的在线分类

    List Online Classification. (arXiv:2303.15383v1 [cs.LG])

    [http://arxiv.org/abs/2303.15383](http://arxiv.org/abs/2303.15383)

    本文研究了多标签列表的在线预测问题，提出了 $b$-ary Littlestone 维度可学习模型，并且在懵懂的情况下探索不同的情况。可以使用改编自 Littlestone 的 SOA 和 Rosenblatt 的感知器等算法进行预测，同时还建立了列表可学习的组合结果。

    

    我们研究多分类在线预测，其中学习者可以使用多个标签的列表进行预测（与传统设置中仅使用一种标签不同）。我们使用 $b$-ary Littlestone 维度表征了该模型中的可学习性。该维度是经典 Littlestone 维度的变体，其中二进制错误树被替换为 $(k+1)$-ary 错误树，其中 k 是列表中标签的数量。在懵懂的场景中，我们根据比较类中是否包含单标签或多标签函数以及它与算法使用的列表大小之间的权衡来探索不同的情况。我们发现在某些情况下可以实现负悔，同时提供了什么情况下实现负悔的完整特性化。作为我们工作的一部分，我们改编了经典算法，如 Littlestone 的 SOA 和 Rosenblatt 的感知器，以使用标签列表进行预测。我们还为可以进行列表学习的组合结果建立了基础。

    We study multiclass online prediction where the learner can predict using a list of multiple labels (as opposed to just one label in the traditional setting). We characterize learnability in this model using the $b$-ary Littlestone dimension. This dimension is a variation of the classical Littlestone dimension with the difference that binary mistake trees are replaced with $(k+1)$-ary mistake trees, where $k$ is the number of labels in the list. In the agnostic setting, we explore different scenarios depending on whether the comparator class consists of single-labeled or multi-labeled functions and its tradeoff with the size of the lists the algorithm uses. We find that it is possible to achieve negative regret in some cases and provide a complete characterization of when this is possible. As part of our work, we adapt classical algorithms such as Littlestone's SOA and Rosenblatt's Perceptron to predict using lists of labels. We also establish combinatorial results for list-learnable c
    
[^122]: Chat-REC：面向互动和可解释性的LLM增强推荐系统

    Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. (arXiv:2303.14524v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2303.14524](http://arxiv.org/abs/2303.14524)

    本文介绍了一种创新的推荐系统模式-Chat-Rec，通过将LLMs与对话式推荐相结合，解决了传统推荐系统中互动性和可解释性不足的问题。因此，Chat-Rec能够更有效地学习用户偏好，并在推荐过程中建立用户-产品之间的联系，具有更大的透明度和控制。

    

    大型语言模型(LLMs)在解决各种应用任务方面具有巨大的潜力。然而，传统的推荐系统仍面临很大的挑战，如互动性和可解释性差，这实际上也阻碍了它们在真实世界系统中的广泛部署。为了解决这些限制，本文提出了一个创新的模式，称为Chat-REC（ChatGPT增强推荐系统），通过将用户配置文件和历史交互转换为提示，创新地增强LLMs用于构建对话式推荐系统。通过在上下文中学习，Chat-Rec被证明在学习用户偏好和建立用户与产品之间的联系方面非常有效，这也使得推荐过程更具互动性和可解释性。此外，在Chat-Rec框架内，用户的偏好可以转移到不同的产品进行跨领域推荐，并且基于提示的注入允许更大的透明度和对推荐过程的控制。

    Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of informa
    
[^123]: 具有内在李亚普诺夫稳定性的数据驱动控制

    Data-Driven Control with Inherent Lyapunov Stability. (arXiv:2303.03157v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2303.03157](http://arxiv.org/abs/2303.03157)

    该论文介绍了一种名为CoILS的数据驱动控制方法，可以联合学习非线性动力学模型和稳定控制器，同时学习参数化李亚普诺夫函数，从而使动力学模型本身具有稳定性，可以由学习的控制器实现。

    

    最近，学习型控制方面取得了重大进展，它借助深度函数逼近器，例如神经网络，来模拟受控动态系统随时间的演化。然而，学习动态模型和稳定控制器的问题仍然存在，因为对于已知的非线性系统，合成稳定反馈律是一项困难的任务，更何况对于必须适应数据的复杂参数表示。为此，我们提出了具有内在李亚普诺夫稳定性（CoILS）的控制方法，该方法可以从数据中联合学习非线性动力学模型和稳定控制器的参数表示。为此，我们的方法同时学习参数化李亚普诺夫函数，从而使动力学模型本身具有稳定性，可以由学习的控制器实现。除了我们新构建的具有学习动态稳定性保证的稳定性外，我们还展示了在某些假设下，学习的控制器可以使真实动态稳定。

    Recent advances in learning-based control leverage deep function approximators, such as neural networks, to model the evolution of controlled dynamical systems over time. However, the problem of learning a dynamics model and a stabilizing controller persists, since the synthesis of a stabilizing feedback law for known nonlinear systems is a difficult task, let alone for complex parametric representations that must be fit to data. To this end, we propose Control with Inherent Lyapunov Stability (CoILS), a method for jointly learning parametric representations of a nonlinear dynamics model and a stabilizing controller from data. To do this, our approach simultaneously learns a parametric Lyapunov function which intrinsically constrains the dynamics model to be stabilizable by the learned controller. In addition to the stabilizability of the learned dynamics guaranteed by our novel construction, we show that the learned controller stabilizes the true dynamics under certain assumptions on 
    
[^124]: 基于本地特征向量投影的3D生成模型潜在空间解耦

    3D Generative Model Latent Disentanglement via Local Eigenprojection. (arXiv:2302.12798v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.12798](http://arxiv.org/abs/2302.12798)

    本文提出了一种基于谱几何的全新损失函数，应用于不同的3D头部和身体网格生成模型，通过激励潜在变量遵循特征向量投影并改善潜在空间解耦，实现对生成本地形状属性的控制。

    

    设计逼真的数字人物是非常复杂的。大多数数据驱动的生成模型用于简化底层几何形状的创建并不提供对本地形状属性生成的控制。在本文中，我们通过引入一种基于谱几何的全新损失函数，应用于不同的基于神经网络的3D头部和身体网格生成模型，克服了这一限制。通过鼓舞网格变分自编码器或生成对抗网络的潜在变量遵循特征向量投影，我们改善了潜在空间解耦并正确地分离了属性的生成。实验结果表明，我们的本地特征向量投影解耦模型不仅相对于现有最先进技术有改进，而且在保持良好的生成能力的同时，其训练时间也与模型的基本实现相当。

    Designing realistic digital humans is extremely complex. Most data-driven generative models used to simplify the creation of their underlying geometric shape do not offer control over the generation of local shape attributes. In this paper, we overcome this limitation by introducing a novel loss function grounded in spectral geometry and applicable to different neural-network-based generative models of 3D head and body meshes. Encouraging the latent variables of mesh variational autoencoders (VAEs) or generative adversarial networks (GANs) to follow the local eigenprojections of identity attributes, we improve latent disentanglement and properly decouple the attribute creation. Experimental results show that our local eigenprojection disentangled (LED) models not only offer improved disentanglement with respect to the state-of-the-art, but also maintain good generation capabilities with training times comparable to the vanilla implementations of the models.
    
[^125]: 用物理学知识的深度学习求解微分方程：基于基准测试的实践教程

    Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests. (arXiv:2302.12260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12260](http://arxiv.org/abs/2302.12260)

    本文介绍了物理信息神经网络（PINNs）求解微分方程的方法，该方法利用物理学知识，通过损失函数的专用项来实现微分方程的求解。通过使用较少的数据，可以预测解，适用于非线性弱的问题。

    

    本文重访了使用深度学习和神经网络求解微分方程的原始方法，并将方程知识纳入考虑。在训练过程中，通过添加一个专用项到损失函数来实现这一点。我们测试了所谓的物理信息神经网络（PINNs）解决各种学术常微分方程的能力，以突显与标准积分方法相比这种方法的优点和缺点。重点是在训练过程中使用最少量的数据。回顾了通过惩罚项强制执行物理定律来解决微分方程的PINNs的原理。一个简单的方程模型的教程向我们演示了如何将此方法应用于普通微分方程。基准测试显示，当问题的非线性弱时，非常少量的训练数据就足以预测解。

    We revisit the original approach of using deep learning and neural networks to solve differential equations by incorporating the knowledge of the equation. This is done by adding a dedicated term to the loss function during the optimization procedure in the training process. The so-called physics-informed neural networks (PINNs) are tested on a variety of academic ordinary differential equations in order to highlight the benefits and drawbacks of this approach with respect to standard integration methods. We focus on the possibility to use the least possible amount of data into the training process. The principles of PINNs for solving differential equations by enforcing physical laws via penalizing terms are reviewed. A tutorial on a simple equation model illustrates how to put into practice the method for ordinary differential equations. Benchmark tests show that a very small amount of training data is sufficient to predict the solution when the non linearity of the problem is weak. H
    
[^126]: 因果剃刀

    Causal Razors. (arXiv:2302.10331v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10331](http://arxiv.org/abs/2302.10331)

    本文比较了许多出现在文献中的因果剃刀，并特别研究了在多项式因果模型中不太受欢迎的因果剃刀——参数最小性。逻辑结果揭示了选择合理得分标准时的困境。

    

    在进行因果推断时，必须对真实因果机制如何与底层联合概率分布相对应做出假设。本文将这些假设称为因果剃刀。我们回顾了许多出现在文献中的因果剃刀，对它们进行了全面的逻辑比较。特别地，我们对在多项式因果模型中不太受欢迎的因果剃刀——参数最小性进行了深入的研究，并研究了它与其他广泛研究的因果剃刀之间的逻辑关系。我们的逻辑结果在为基于分数的因果搜索算法选择合理得分标准时提出了困境。

    When performing causal discovery, assumptions have to be made on how the true causal mechanism corresponds to the underlying joint probability distribution. These assumptions are labeled as causal razors in this work. We review numerous causal razors that appeared in the literature, and offer a comprehensive logical comparison of them. In particular, we scrutinize an unpopular causal razor, namely parameter minimality, in multinomial causal models and its logical relations with other well-studied causal razors. Our logical result poses a dilemma in selecting a reasonable scoring criterion for score-based casual search algorithms.
    
[^127]: 变分混合超生成器用于学习函数分布的方法

    Variational Mixture of HyperGenerators for Learning Distributions Over Functions. (arXiv:2302.06223v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06223](http://arxiv.org/abs/2302.06223)

    本文提出了一种新的深度生成模型VAMoH，结合了INRs对连续函数进行建模的能力和VAEs的推断能力，以及归一化流和超网络混合方法。在不同类型的数据上进行实验证明，VAMoH可以有效地学习连续函数的分布，并可以执行与推断相关的任务。

    

    最近的一些方法基于隐式神经表示（INRs）提出了函数空间上的生成模型。然而，处理推断任务（如缺失数据插值）时，它们在计算上代价高，或者根本不能处理这些问题。在本文中，我们提出了一种新的深度生成模型，称为VAMoH。VAMoH结合了使用INRs对连续函数进行建模的能力和变分自编码器（VAEs）的推断能力。此外，VAMoH依赖于一个归一化流来定义先验，以及一个超网络混合来参数化数据对数似然。这使得VAMoH具有高度表达能力和可解释性。通过在各种数据类型（如图像、体素和气候数据）上进行实验证明，VAMoH可以有效地学习连续函数的丰富分布。此外，它可以执行与推断相关的任务，如条件超分辨率生成和修复，效果优于或不亚于其他方法。

    Recent approaches build on implicit neural representations (INRs) to propose generative models over function spaces. However, they are computationally costly when dealing with inference tasks, such as missing data imputation, or directly cannot tackle them. In this work, we propose a novel deep generative model, named VAMoH. VAMoH combines the capabilities of modeling continuous functions using INRs and the inference capabilities of Variational Autoencoders (VAEs). In addition, VAMoH relies on a normalizing flow to define the prior, and a mixture of hypernetworks to parametrize the data log-likelihood. This gives VAMoH a high expressive capability and interpretability. Through experiments on a diverse range of data types, such as images, voxels, and climate data, we show that VAMoH can effectively learn rich distributions over continuous functions. Furthermore, it can perform inference-related tasks, such as conditional super-resolution generation and in-painting, as well or better tha
    
[^128]: ChatGPT失败分类存档

    A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v8 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03494](http://arxiv.org/abs/2302.03494)

    本研究对ChatGPT的11个失败类别进行了全面分析，其中包括推理、事实错误、数学、编码和偏见。找出失败原因以帮助研究人员和开发人员改进未来的语言模型和聊天机器人。

    

    大型语言模型已经在不同领域证明了其价值。由OpenAI开发的ChatGPT使用大量数据进行训练，通过理解上下文并生成适当的响应来模拟人类对话。它因能够有效地回答广泛的人类问题而受到重视，其流利和全面的答案在安全性和实用性方面超越了先前的公共聊天机器人。然而，缺乏ChatGPT失效的全面分析，这是本研究的重点。本研究提出并讨论了11个失败类别，包括推理、事实错误、数学、编码和偏见。还突出了ChatGPT的风险、限制和社会影响。本研究的目标是帮助研究人员和开发人员增强未来的语言模型和聊天机器人。

    Large language models have been demonstrated to be valuable in different fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.
    
[^129]: 针对含有“沉默大多数”的图表进行抗差建模的知识可迁移图神经网络

    Predicting the Silent Majority on Graphs: Knowledge Transferable Graph Neural Network. (arXiv:2302.00873v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00873](http://arxiv.org/abs/2302.00873)

    本文提出了一种新颖的知识可迁移模块和适应性学习机制，通过将有声节点的知识转移给无声节点，实现了针对含有“沉默大多数”的图表的抗差建模。实验证明，该方法在各种真实世界数据集上都具有有效性和优越性。

    

    在现实世界中，存在由有声节点（“有声少数”）和无声节点（“沉默大多数”）组成的图表，即 VS-Graph，其中有声节点往往具有丰富的特征和标签，而无声节点仅具有不完整的特征和稀缺的标签。 预测沉默大多数仍然是一个至关重要且具有挑战性的问题。 为了解决这个问题，提出了一种称为知识可迁移图神经网络（KT-GNN）的方法，该方法通过将有声节点的知识转移给无声节点来实现消息传递和表征学习过程中的分布漂移建模。 实验结果表明，KT-GNN 在各种真实世界数据集上都具有有效性和优越性。

    Graphs consisting of vocal nodes ("the vocal minority") and silent nodes ("the silent majority"), namely VS-Graph, are ubiquitous in the real world. The vocal nodes tend to have abundant features and labels. In contrast, silent nodes only have incomplete features and rare labels, e.g., the description and political tendency of politicians (vocal) are abundant while not for ordinary people (silent) on the twitter's social network. Predicting the silent majority remains a crucial yet challenging problem. However, most existing message-passing based GNNs assume that all nodes belong to the same domain, without considering the missing features and distribution-shift between domains, leading to poor ability to deal with VS-Graph. To combat the above challenges, we propose Knowledge Transferable Graph Neural Network (KT-GNN), which models distribution shifts during message passing and representation learning by transferring knowledge from vocal nodes to silent nodes. Specifically, we design 
    
[^130]: 分析语言模型中个人识别信息泄露的情况

    Analyzing Leakage of Personally Identifiable Information in Language Models. (arXiv:2302.00539v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00539](http://arxiv.org/abs/2302.00539)

    本研究针对语言模型中泄漏个人身份信息的风险进行了严格的定义，并通过黑盒提取、推断和重建攻击进行了实证评估。

    

    语言模型已经被证明会通过句子级成员推断和重构攻击泄漏训练数据的信息。然而，我们对于语言模型泄露个人身份信息的风险了解不足。目前已经假设数据集整理技术（如数据清洗）足以防止个人身份信息泄露，但这一假设是错误的。实际上，数据清洗技术可以减少Pll泄露的风险，但并不能完全绝对地防止泄露。本文中，我们引入了三种类型的个人身份信息泄漏的严格基于博弈的定义，通过API访问语言模型进行黑盒提取、推断和重建攻击，并对其进行实证评估。

    Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the 
    
[^131]: DIFFormer：通过受能量限制的扩散引出的可扩展（图形）Transformer

    DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion. (arXiv:2301.09474v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09474](http://arxiv.org/abs/2301.09474)

    DIFFormer是一种能量受限扩散模型，通过逐渐融合其他实例信息的演化状态，导出了一类新的神经编码器，称为DIFFormer（基于扩散的Transformer），能够揭示真实世界中复杂的数据生成过程。

    

    真实世界的数据生成常常涉及实例之间的复杂相互依赖，违反了标准学习范式的IID数据假设，从而对揭示几何结构以学习所需要的实例表示形成了挑战。为此，我们引入了一种能量受限扩散模型，将一批数据集中的实例编码为逐渐融合了其他实例信息的演化状态。扩散过程受限于基于合理能量函数的下降标准，该函数表征了潜在结构上实例表示的全局一致性。我们提供了严谨的理论，该理论暗示了任意实例对之间的最优扩散强度的闭合形式估计，这导致了一类新的神经编码器的产生：DIFFormer（基于扩散的Transformer），其中包含两个版本：一个简单版本具有线性复杂度，面临着禁忌的实例。

    Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t.~a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instanc
    
[^132]: 通过D适应实现学习率自由学习

    Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07733](http://arxiv.org/abs/2301.07733)

    D-Adaptation是一种可以自动设置学习率的方法，针对最小化凸性Lipschitz函数，用于实现最优收敛速率，而无需超参数，也无需额外对数因子改进，能够在各种机器学习问题中自动匹配手动调整的学习率。

    

    D适应是一种自动设置学习率的方法，可以渐近地实现最优收敛速率，用于最小化凸性Lipschitz函数，无需回溯或线性搜索，并且每步无需进行额外的函数值或梯度评估。我们的方法是这一类问题的第一个无超参数且收敛速率无需额外对数因子改进的方法。我们针对SGD和Adam变体展示了广泛的实验，其中该方法自动匹配手动调整的学习率，在十多个不同的机器学习问题中应用，包括大规模的视觉和语言问题。开源实现在 \url{https://github.com/facebookresearch/dadaptation}.

    D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \url{https://github.com/facebookresearch/dadaptation}.
    
[^133]: 异步HFL：在分层IoT网络中进行高效、鲁棒的异步联邦学习 (arXiv:2301.06646v3 [cs.LG] UPDATED)

    Async-HFL: Efficient and Robust Asynchronous Federated Learning in Hierarchical IoT Networks. (arXiv:2301.06646v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.06646](http://arxiv.org/abs/2301.06646)

    提出了异步HFL框架，以解决复杂、层次化的IoT网络中的联合学习挑战。该算法采用了异步聚合来避免长时间等待，并在网关和云级别上采用设备选择和设备网关调度来提高收敛速度、鲁棒性和可扩展性。

    

    近年来，联邦学习 (FL) 作为一种分布式设备学习范式受到了越来越多的关注。然而，在具有层次结构的真实物联网 (IoT) 网络中部署FL仍然面临着多重挑战。虽然现有的研究已经提出了各种方法来解决数据异质性、系统异质性、意外延迟者和可扩展性等问题，但没有一种方法提供一个系统性的解决方案来应对分层和不可靠的IoT网络中的所有挑战。在本文中，我们提出了一个异步和分层框架 (Async-HFL)，用于在常见的三层IoT网络结构中执行FL。针对大量不同的延迟，Async-HFL在网关和云级别均采用异步聚合，从而避免长时间等待。为了充分发挥在系统异质性和延迟者下异步HFL的收敛速度潜力，我们分别设计了网关级别的设备选择和云级别的设备网关调度。结果表明，Async-HFL在收敛速度、鲁棒性和可扩展性方面优于现有的FL算法。

    Federated Learning (FL) has gained increasing interest in recent years as a distributed on-device learning paradigm. However, multiple challenges remain to be addressed for deploying FL in real-world Internet-of-Things (IoT) networks with hierarchies. Although existing works have proposed various approaches to account data heterogeneity, system heterogeneity, unexpected stragglers and scalibility, none of them provides a systematic solution to address all of the challenges in a hierarchical and unreliable IoT network. In this paper, we propose an asynchronous and hierarchical framework (Async-HFL) for performing FL in a common three-tier IoT network architecture. In response to the largely varied delays, Async-HFL employs asynchronous aggregations at both the gateway and the cloud levels thus avoids long waiting time. To fully unleash the potential of Async-HFL in converging speed under system heterogeneities and stragglers, we design device selection at the gateway level and device-ga
    
[^134]: 基于元路径的社交媒体谣言检测方法研究

    A Meta Path-based Approach for Rumor Detection on Social Media. (arXiv:2301.04341v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2301.04341](http://arxiv.org/abs/2301.04341)

    本文提出了一种基于元路径的全局局部注意网络（MGLAN）的模型，通过提取谣言传播的结构特征，有效解决谣言检测问题。

    

    社交媒体在人们的日常生活中起着重要作用，使得人们更倾向于通过社交网络而不是传统来源获取新闻。这种公众行为的转变为一些人传播假新闻在社交媒体上打开了大门，随之带来了负面的经济、政治和社会后果，也引起了公众的不信任。本文提出了一种基于元路径的全局局部注意网络（MGLAN）的模型，通过从异质谣言传播中抽取结构特征来解决谣言检测问题。实验结果表明，MGLAN超越其他模型，能够捕捉节点的重要性和网络的动态变化。

    The prominent role of social media in people's daily lives has made them more inclined to receive news through social networks than traditional sources. This shift in public behavior has opened doors for some to diffuse fake news on social media; and subsequently cause negative economic, political, and social consequences as well as distrust among the public.  There are many proposed methods to solve the rumor detection problem, most of which do not take full advantage of the heterogeneous nature of news propagation networks. With this intention, we considered a previously proposed architecture as our baseline and performed the idea of structural feature extraction from the heterogeneous rumor propagation over its architecture using the concept of meta path-based embeddings. We named our model Meta Path-based Global Local Attention Network (MGLAN). Extensive experimental analysis on three state-of-the-art datasets has demonstrated that MGLAN outperforms other models by capturing node-l
    
[^135]: 视频动作识别的分层解释

    Hierarchical Explanations for Video Action Recognition. (arXiv:2301.00436v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.00436](http://arxiv.org/abs/2301.00436)

    本文提出了分层原型解释器，能够解释深度神经网络对视频动作的分类，同时能够将类和原型建立成更有层次的关系，可以处理不确定性。

    

    解释深度神经网络的主要方法之一是分解视觉输入并找到负责分类的典型部分。然而，现有方法通常忽略这些原型之间的分层关系，因此无法在更高层次（例如，水上运动）和更低层次（例如，游泳）上解释语义概念。在本文中，我们受到人类认知系统的启发，利用分层信息处理不确定性：当我们观察到水和人类活动，但没有明确的动作时，可以将其识别为水上运动的父类。只有观察到一个人在游泳后，我们才能明确将其细分为游泳动作。为此，我们提出了分层原型解释器（HIPE）来建立原型和类之间的分层关系。 HIPE通过在类层次结构的多个级别上分解输入视频帧，实现了视频动作分类的推理过程，我们的方法也适用于视频动作识别之外的其他识别任务。

    To interpret deep neural networks, one main approach is to dissect the visual input and find the prototypical parts responsible for the classification. However, existing methods often ignore the hierarchical relationship between these prototypes, and thus can not explain semantic concepts at both higher level (e.g., water sports) and lower level (e.g., swimming). In this paper inspired by human cognition system, we leverage hierarchal information to deal with uncertainty: When we observe water and human activity, but no definitive action it can be recognized as the water sports parent class. Only after observing a person swimming can we definitively refine it to the swimming action. To this end, we propose HIerarchical Prototype Explainer (HIPE) to build hierarchical relations between prototypes and classes. HIPE enables a reasoning process for video action classification by dissecting the input video frames on multiple levels of the class hierarchy, our method is also applicable to ot
    
[^136]: 面向可扩展物理一致的神经网络：在数据驱动多区域热建筑模型中的应用研究

    Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models. (arXiv:2212.12380v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12380](http://arxiv.org/abs/2212.12380)

    本论文研究了物理一致神经网络(PCNNs) 在模拟建筑温度动态方面的扩展性和准确性。结果发现，PCNNs既确保了物理一致性，同时又能在复杂的多区域热建筑模型中取得高精度的性能表现，且在可用数据量有限的情况下超越经典灰盒模型，具有可扩展性优势。

    

    随着越来越多的数据被收集，数据驱动建模方法近年来越来越受欢迎。虽然经典灰盒模型在物理上是可靠的，但通常很难识别和扩展，并且受其有限的表现力影响可能会影响其准确性。另一方面，常常依赖神经网络 (NNs) 的经典黑盒方法通常能够从数据中推导出统计模式，即使在扩展方面也能取得令人印象深刻的性能。然而，它们对潜在的物理定律完全无视，如果基于它们做决策用于实际物理系统，可能会导致潜在的灾难性后果。最近开发了物理一致神经网络 (PCNNs) 来解决这些问题，确保物理一致性，同时利用 NNs 实现最先进的准确性。在这项工作中，我们将 PCNN 扩展到建筑温度动态建模，并提出与经典灰盒和黑盒方法的彻底比较。特别是，我们研究多区域建筑模型，其中每个区域的热行为由能量平衡方程式统治，其参数必须通过测量数据进行识别。所得结果表明，即使涉及许多相互作用的组件构成的复杂和动态系统，PCNNs 也可以在确保物理一致性的同时实现高精度。此外，我们证明 PCNN 在经典灰盒模型上提供了可扩展性优势，在有限的可用训练数据下表现出色。

    With more and more data being collected, data-driven modeling methods have been gaining in popularity in recent years. While physically sound, classical gray-box models are often cumbersome to identify and scale, and their accuracy might be hindered by their limited expressiveness. On the other hand, classical black-box methods, typically relying on Neural Networks (NNs) nowadays, often achieve impressive performance, even at scale, by deriving statistical patterns from data. However, they remain completely oblivious to the underlying physical laws, which may lead to potentially catastrophic failures if decisions for real-world physical systems are based on them. Physically Consistent Neural Networks (PCNNs) were recently developed to address these aforementioned issues, ensuring physical consistency while still leveraging NNs to attain state-of-the-art accuracy.  In this work, we scale PCNNs to model building temperature dynamics and propose a thorough comparison with classical gray-b
    
[^137]: 基于梯度元学习的重复使用的选项方法

    Reusable Options through Gradient-based Meta Learning. (arXiv:2212.11726v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11726](http://arxiv.org/abs/2212.11726)

    本文提出了一个基于梯度元学习的方法，以解决学习可重复使用的选项的问题。实验表明，该方法能够学习可转移的组件，加速学习，并表现优于现有的先前方法。

    

    强化学习中的分层方法有潜力减少智能体在学习新任务时需要执行的决策数量。然而，在寻找有用的可重复使用的时间抽象方面，仍存在挑战。最近，提出了几种深度学习方法，以端到端的方式学习这些时间抽象，形成了选项。本文指出了这些方法的几个不足之处并讨论了它们的潜在负面影响。随后，我们制定了可重复使用选项的愿望和使用这些愿望来将学习选项问题框架化为一个基于梯度的元学习问题。这使我们能够制定一种目标，明确激励能够使高层决策者能够在少数步骤中适应不同任务的选项。实验表明，我们的方法能够学习可转移的组件，加速学习，并表现优于现有的先前方法。

    Hierarchical methods in reinforcement learning have the potential to reduce the amount of decisions that the agent needs to perform when learning new tasks. However, finding reusable useful temporal abstractions that facilitate fast learning remains a challenging problem. Recently, several deep learning approaches were proposed to learn such temporal abstractions in the form of options in an end-to-end manner. In this work, we point out several shortcomings of these methods and discuss their potential negative consequences. Subsequently, we formulate the desiderata for reusable options and use these to frame the problem of learning options as a gradient-based meta-learning problem. This allows us to formulate an objective that explicitly incentivizes options which allow a higher-level decision maker to adjust in few steps to different tasks. Experimentally, we show that our method is able to learn transferable components which accelerate learning and performs better than existing prior
    
[^138]: 一种用于终身测试时间自适应的概率框架

    A Probabilistic Framework for Lifelong Test-Time Adaptation. (arXiv:2212.09713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.09713](http://arxiv.org/abs/2212.09713)

    本文提出了一种用于处理测试输入分布随时间持续变化的概率框架PETAL，通过提供可靠的不确定性估计和使用源模型作为正则化项来推断时正则化模型更新，实现了终身TTA。

    

    测试时间适应（TTA）是在推理时针对来自不同目标域的测试输入更新预先训练的源模型的问题。目前大多数TTA方法都假设目标域是静态的，即所有测试输入都来自单个目标域。然而，在许多实际应用场景中，测试输入分布可能随着时间的推移而发生终身/持续变化。此外，现有的TTA方法也缺乏提供可靠的不确定性估计的能力，而在源域和目标域之间发生分布变化时，这一点至关重要。为了解决这些问题，我们提出了PETAL（具有自我训练先验知识的概率终身测试时间自适应），它使用概率方法解决了终身TTA问题，自然地得到了（1）学生-教师框架，其中教师模型是学生模型的指数移动平均值，以及（2）使用源模型作为正则化项来推断时正则化模型更新。

    Test-time adaptation (TTA) is the problem of updating a pre-trained source model at inference time given test input(s) from a different target domain. Most existing TTA approaches assume the setting in which the target domain is stationary, i.e., all the test inputs come from a single target domain. However, in many practical settings, the test input distribution might exhibit a lifelong/continual shift over time. Moreover, existing TTA approaches also lack the ability to provide reliable uncertainty estimates, which is crucial when distribution shifts occur between the source and target domain. To address these issues, we present PETAL (Probabilistic lifElong Test-time Adaptation with seLf-training prior), which solves lifelong TTA using a probabilistic approach, and naturally results in (1) a student-teacher framework, where the teacher model is an exponential moving average of the student model, and (2) regularizing the model updates at inference time using the source model as a reg
    
[^139]: 带有随机集合的贝叶斯后验近似

    Bayesian posterior approximation with stochastic ensembles. (arXiv:2212.08123v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08123](http://arxiv.org/abs/2212.08123)

    本文提出一种新方法，即使用随机神经网络集合来近似贝叶斯后验，并通过变分推断进行训练，实验证明该方法比其他流行的贝叶斯推断基线提供了更准确的后验估计。

    

    我们引入一种基于随机神经网络集的方法来近似贝叶斯后验。它将随机方法（如dropout）与深度集成相结合，并将随机集合公式化为分布族，并使用变分推断训练以近似贝叶斯后验。我们在一个玩具问题和CIFAR图像分类上实现了基于Monte Carlo Dropout，DropConnect和新颖的非参数版本的随机集合，并直接与哈密顿马尔可夫蒙特卡罗模拟比较质量来测试后验。结果表明，随机集合提供了比其他流行的贝叶斯推断基线更准确的后验估计。

    We introduce ensembles of stochastic neural networks to approximate the Bayesian posterior, combining stochastic methods such as dropout with deep ensembles. The stochastic ensembles are formulated as families of distributions and trained to approximate the Bayesian posterior with variational inference. We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and a novel non-parametric version of dropout and evaluate them on a toy problem and CIFAR image classification. For both tasks, we test the quality of the posteriors directly against Hamiltonian Monte Carlo simulations. Our results show that stochastic ensembles provide more accurate posterior estimates than other popular baselines for Bayesian inference.
    
[^140]: 自我监督的汽车激光雷达通过占据估计进行自我监督

    ALSO: Automotive Lidar Self-supervision by Occupancy estimation. (arXiv:2212.05867v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.05867](http://arxiv.org/abs/2212.05867)

    该论文提出了一种简单易操作的自我监督方法，通过重构表面和利用其中的语义信息来提高3D感知模型的准确性。

    

    我们提出了一种新的自我监督方法，用于预训练在点云上运行的深度感知模型的骨干。其核心思想是通过预文本任务训练模型，该任务是重构3D点所采样的表面，并将潜在向量用作感知头的输入。直觉是，如果网络能够在仅有稀疏输入点的情况下重构场景表面，则它可能还捕获了一些语义信息的片段，可以用于提高实际感知任务的准确性。

    We propose a new self-supervised method for pre-training the backbone of deep perception models operating on point clouds. The core idea is to train the model on a pretext task which is the reconstruction of the surface on which the 3D points are sampled, and to use the underlying latent vectors as input to the perception head. The intuition is that if the network is able to reconstruct the scene surface, given only sparse input points, then it probably also captures some fragments of semantic information, that can be used to boost an actual perception task. This principle has a very simple formulation, which makes it both easy to implement and widely applicable to a large range of 3D sensors and deep networks performing semantic segmentation or object detection. In fact, it supports a single-stream pipeline, as opposed to most contrastive learning approaches, allowing training on limited resources. We conducted extensive experiments on various autonomous driving datasets, involving ve
    
[^141]: 使用强化学习的编译器优化量子计算

    Compiler Optimization for Quantum Computing Using Reinforcement Learning. (arXiv:2212.04508v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2212.04508](http://arxiv.org/abs/2212.04508)

    这篇论文提出了一种基于强化学习算法的量子编译器优化框架，旨在提高量子编译的效率和品质，克服了传统启发式方法的缺点，并可适应各种约束条件下的编译要求。

    

    任何一个量子计算应用，一旦被编码成量子电路，就必须在在量子计算机上执行之前进行编译。与经典编译相似，量子编译是一个有序的过程，其中有许多编译步骤和许多可供优化的操作。尽管有相似之处，但量子计算机编译器的开发仍然处于起步阶段，缺乏对最佳操作序列、兼容性、适应性和灵活性的共识。在这项工作中，我们借鉴了几十年经典编译器优化的经验，提出了一种强化学习框架，用于开发经过优化的量子电路编译流程。通过不同的约束条件和统一的接口，该框架支持将不同编译器和优化工具的技术组合在一个单一的编译流程中。实验评估表明，所提出的框架 - 借助IBM的Qiskit和Google的Cirq和TensorFlow Quantum的一系列编译操作，旨在提高量子编译的效率和质量。通过强化学习算法优化其编译流程，克服了传统启发式的缺点，并可适应各种约束条件下的编译要求。

    Any quantum computing application, once encoded as a quantum circuit, must be compiled before being executable on a quantum computer. Similar to classical compilation, quantum compilation is a sequential process with many compilation steps and numerous possible optimization passes. Despite the similarities, the development of compilers for quantum computing is still in its infancy -lacking mutual consolidation on the best sequence of passes, compatibility, adaptability, and flexibility. In this work, we take advantage of decades of classical compiler optimization and propose a reinforcement learning framework for developing optimized quantum circuit compilation flows. Through distinct constraints and a unifying interface, the framework supports the combination of techniques from different compilers and optimization tools in a single compilation flow. Experimental evaluations show that the proposed framework -set up with a selection of compilation passes from IBM's Qiskit and Quanti
    
[^142]: 移动机器人的2D推动操作中的集体智能

    Collective Intelligence for 2D Push Manipulation with Mobile Robots. (arXiv:2211.15136v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.15136](http://arxiv.org/abs/2211.15136)

    本研究利用基于软体物理模拟器的规划器和基于注意力的神经网络，实现了移动机器人2D协作推动操作中的集体智能，比传统方法具有更好的性能并具备环境自适应能力。

    

    自然系统通常表现出能够自我组织和适应变化的集体智能，但大多数人工系统缺乏这种等效性。本文探讨使用移动机器人进行2D协作推动操作的集体智能系统的可能性。我们展示了将从软体物理模拟派生的规划器提炼为基于注意力的神经网络后，我们的多机器人推动操作系统相对于基线系统具有更好的性能，并可适应外部扰动和环境变化完成任务。

    While natural systems often present collective intelligence that allows them to self-organize and adapt to changes, the equivalent is missing in most artificial systems. We explore the possibility of such a system in the context of cooperative 2D push manipulations using mobile robots. Although conventional works demonstrate potential solutions for the problem in restricted settings, they have computational and learning difficulties. More importantly, these systems do not possess the ability to adapt when facing environmental changes. In this work, we show that by distilling a planner derived from a differentiable soft-body physics simulator into an attention-based neural network, our multi-robot push manipulation system achieves better performance than baselines. In addition, our system also generalizes to configurations not seen during training and is able to adapt toward task completions when external turbulence and environmental changes are applied. Supplementary videos can be foun
    
[^143]: FaiREE：具有有限样本和无分布保证的公平分类算法

    FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee. (arXiv:2211.15072v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.15072](http://arxiv.org/abs/2211.15072)

    本研究提出了FaiREE算法，它是一种可满足群体公平性约束的公平分类算法，并且具有有限样本和无分布理论保证。在实验中表现优异。

    

    算法公平性在机器学习研究中发挥着越来越重要的作用。已经提出了几种群体公平性概念和算法。然而，现有公平分类方法的公平保证主要依赖于特定的数据分布假设，通常需要大样本量，并且在样本量较小的情况下可能会违反公平性，而这在实践中经常发生。本文提出了FaiREE算法，它是一种公平分类算法，可以在有限样本和无分布理论保证下满足群体公平性约束。FaiREE可以适应各种群体公平性概念（例如，机会平等，平衡几率，人口统计学平衡等）并实现最佳准确性。这些理论保证进一步得到了对合成和实际数据的实验支持。FaiREE表现出比最先进的算法更好的性能。

    Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depends on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm that can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfy various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.
    
[^144]: GAMMT: 使用多个Transformer的生成不确定性建模

    GAMMT: Generative Ambiguity Modeling Using Multiple Transformers. (arXiv:2211.09812v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09812](http://arxiv.org/abs/2211.09812)

    GAMMT是一种生成不确定性模型，使用多个Transformer处理模糊不确定的概率。该模型有望实现高质量和多样性的序列建模。

    

    我们提出了一种新的模型，称为GAMMT（使用多个Transformer的生成不确定性模型），用于基于概率集合的序列数据。与传统模型不同，我们的方法认为序列的数据生成过程不是确定性的，而是模糊的，并受到一组概率的影响。为了捕捉这种不确定性，GAMMT采用了多个并行的Transformer，通过选择机制相互关联，允许近似处理模糊不确定的概率。我们的方法的生成特性还使得输入符号和序列可以有多个表征形式。虽然我们的模型尚未经过实验验证，但我们相信我们的模型在建模具有不确定的数据生成过程的序列的高质量和多样性方面具有巨大潜力。

    We introduce a novel model called GAMMT (Generative Ambiguity Models using Multiple Transformers) for sequential data that is based on sets of probabilities. Unlike conventional models, our approach acknowledges that the data generation process of a sequence is not deterministic, but rather ambiguous and influenced by a set of probabilities. To capture this ambiguity, GAMMT employs multiple parallel transformers that are linked by a selection mechanism, allowing for the approximation of ambiguous probabilities. The generative nature of our approach also enables multiple representations of input tokens and sequences. While our models have not yet undergone experimental validation, we believe that our model has great potential to achieve high quality and diversity in modeling sequences with uncertain data generation processes.
    
[^145]: 基于图注意力网络与不平衡PU标签的P2E MMORPGs欺诈检测方法

    PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention Networks with Imbalanced PU Labels. (arXiv:2211.08604v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08604](http://arxiv.org/abs/2211.08604)

    本文提出了一种基于图注意力网络和PU损失函数的欺诈检测方法PU GNN，通过改进的GraphSMOTE算法来处理P2E MMORPGs欺诈检测数据集中的标签分布不平衡问题，实验证明该方法在欺诈检测方面具有良好的性能表现。

    

    最近，在大型多人在线角色扮演游戏中，游戏点卡能够直接转换为比特币、以太坊或Klaytn等加密货币，因此play-to-earn（P2E）系统的出现使得游戏物品与现实世界的价值交换比以往更加频繁。本文提出了一种新的欺诈检测方法PU GNN，该方法采用图注意力网络和PU损失函数捕捉玩家的游戏行为、P2E代币交易模式，同时采用改进的GraphSMOTE算法处理欺诈检测数据集中的标签分布不平衡问题。该方法在三个实际的P2E MMORPGs数据集上进行实验证明，取得了最新的欺诈检测性能。

    The recent advent of play-to-earn (P2E) systems in massively multiplayer online role-playing games (MMORPGs) has made in-game goods interchangeable with real-world values more than ever before. The goods in the P2E MMORPGs can be directly exchanged with cryptocurrencies such as Bitcoin, Ethereum, or Klaytn via blockchain networks. Unlike traditional in-game goods, once they had been written to the blockchains, P2E goods cannot be restored by the game operation teams even with chargeback fraud such as payment fraud, cancellation, or refund. To tackle the problem, we propose a novel chargeback fraud prediction method, PU GNN, which leverages graph attention networks with PU loss to capture both the players' in-game behavior with P2E token transaction patterns. With the adoption of modified GraphSMOTE, the proposed model handles the imbalanced distribution of labels in chargeback fraud datasets. The conducted experiments on three real-world P2E MMORPG datasets demonstrate that PU GNN achi
    
[^146]: 从立方体到网络：用于合成网络生成的快速通用模型

    From Cubes to Networks: Fast Generic Model for Synthetic Networks Generation. (arXiv:2211.02811v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2211.02811](http://arxiv.org/abs/2211.02811)

    本论文提出了一种快速通用的模型，将立方体转化为相应的网络，生成的网络更加接近实际网络。

    

    复杂网络分析和立方体（即多维数据集）的探索是两个当前不同策略的独立研究领域。为了通过独特的网络域方法获得关于立方体动态的更多洞察，并获得丰富的合成网络，我们需要一种将立方体转化为相应网络的转化方法。为此，我们提出了FGM，一种快速通用模型，将样本重塑为节点，并在最近邻概念的指导下引导网络动态。通过与以前模型的比较，我们显示出FGM可以高效地生成更接近实际网络的典型模式的网络，例如更真实的度分布、幂律平均最近邻度依赖性，以及我们认为对于网络至关重要的影响衰减现象。此外，我们通过各种立方体评估FGM生成的网络。结果表明，

    Analytical explorations on complex networks and cubes (i.e., multi-dimensional datasets) are currently two separate research fields with different strategies. To gain more insights into cube dynamics via unique network-domain methodologies and to obtain abundant synthetic networks, we need a transformation approach from cubes into associated networks. To this end, we propose FGM, a fast generic model converting cubes into interrelated networks, whereby samples are remodeled into nodes and network dynamics are guided under the concept of nearest-neighbor searching. Through comparison with previous models, we show that FGM can cost-efficiently generate networks exhibiting typical patterns more closely aligned to factual networks, such as more authentic degree distribution, power-law average nearest-neighbor degree dependency, and the influence decay phenomenon we consider vital for networks. Furthermore, we evaluate the networks that FGM generates through various cubes. Results show that
    
[^147]: Wasserstein分布鲁棒优化问题的核心集

    Coresets for Wasserstein Distributionally Robust Optimization Problems. (arXiv:2210.04260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04260](http://arxiv.org/abs/2210.04260)

    本文提出了一种构建一般Wasserstein分布鲁棒优化问题核心集的统一框架。

    

    Wasserstein分布鲁棒优化（WDRO）是一种通过含糊数据增强机器学习鲁棒性的流行模型。然而，WDRO的复杂度在实践中可能是禁止性的，因为解决其“极小极大”表达式需要大量计算。最近，已经开发了一些针对特定机器学习任务（例如逻辑回归）的快速WDRO训练算法。然而，据我们所知，对于一般的大规模WDRO的设计高效算法的研究仍然非常有限。核心集是一种重要的工具，用于压缩大型数据集，因此已广泛应用于减少许多优化问题的计算复杂性。本文介绍了一种构建一般WDRO问题的$\epsilon$-coreset的统一框架。尽管由于不确定性而获取WDRO的传统核心集具有挑战性。

    Wasserstein distributionally robust optimization (\textsf{WDRO}) is a popular model to enhance the robustness of machine learning with ambiguous data. However, the complexity of \textsf{WDRO} can be prohibitive in practice since solving its ``minimax'' formulation requires a great amount of computation. Recently, several fast \textsf{WDRO} training algorithms for some specific machine learning tasks (e.g., logistic regression) have been developed. However, the research on designing efficient algorithms for general large-scale \textsf{WDRO}s is still quite limited, to the best of our knowledge. \textit{Coreset} is an important tool for compressing large dataset, and thus it has been widely applied to reduce the computational complexities for many optimization problems. In this paper, we introduce a unified framework to construct the $\epsilon$-coreset for the general \textsf{WDRO} problems. Though it is challenging to obtain a conventional coreset for \textsf{WDRO} due to the uncertaint
    
[^148]: 带有服务器学习的联邦学习：提高非独立同分布数据的性能

    Federated Learning with Server Learning: Enhancing Performance for Non-IID Data. (arXiv:2210.02614v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02614](http://arxiv.org/abs/2210.02614)

    基于辅助学习的联邦学习可以显著提高在非独立同分布数据上的模型精度和收敛时间

    

    联邦学习（FL）已成为使用客户端存储的本地数据进行分布式学习的一种手段，其中协调服务器。最近的研究表明，当训练客户端数据不独立同分布时，FL可能会遭受性能下降和收敛缓慢的问题。在这里，我们考虑一种新的补充方法来减轻这种性能下降，即允许服务器从小数据集上执行辅助学习。我们的分析和实验表明，即使服务器数据集很小且其分布与所有客户端聚合数据不同，这种新方法也可以在模型精度和收敛时间方面实现显着的改进。

    Federated Learning (FL) has emerged as a means of distributed learning using local data stored at clients with a coordinating server. Recent studies showed that FL can suffer from poor performance and slower convergence when training data at clients are not independent and identically distributed. Here we consider a new complementary approach to mitigating this performance degradation by allowing the server to perform auxiliary learning from a small dataset. Our analysis and experiments show that this new approach can achieve significant improvements in both model accuracy and convergence time even when the server dataset is small and its distribution differs from that of the aggregated data from all clients.
    
[^149]: 未知动态环境下快速运动规划的障碍物识别与椭球分解

    Obstacle Identification and Ellipsoidal Decomposition for Fast Motion Planning in Unknown Dynamic Environments. (arXiv:2209.14233v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.14233](http://arxiv.org/abs/2209.14233)

    本文提出一种基于椭球的障碍物识别与测速方法，并定义了基于椭球的特征向量，能够适用于带有静态和动态障碍物的环境，其运行速度比现有算法更快且不需要预先知道聚类数量。

    

    在无人系统中，避免与未知动态障碍物的碰撞是最重要的挑战之一。本文提出了一种通过椭球来识别障碍物，并估计其线性和角速度的方法。我们的方法基于一个任何物体都可以近似表示为椭球的理念。为了实现这一点，我们提出了一种基于变分贝叶斯估计高斯混合模型、Kyachiyan算法和优化算法的方法。与现有的基于优化的方法不同，我们提出的方法不需要知道聚类数目，并且可以实时操作。此外，我们定义了基于椭球的特征向量，以匹配给定的两个时间接近的点帧的障碍物。我们的方法可以应用于任何带有静态和动态障碍物的环境，包括具有旋转障碍物的环境。我们将我们的算法与其他聚类方法进行了比较，并展示了其优越性。

    Collision avoidance in the presence of dynamic obstacles in unknown environments is one of the most critical challenges for unmanned systems. In this paper, we present a method that identifies obstacles in terms of ellipsoids to estimate linear and angular obstacle velocities. Our proposed method is based on the idea of any object can be approximately expressed by ellipsoids. To achieve this, we propose a method based on variational Bayesian estimation of Gaussian mixture model, the Kyachiyan algorithm, and a refinement algorithm. Our proposed method does not require knowledge of the number of clusters and can operate in real-time, unlike existing optimization-based methods. In addition, we define an ellipsoid-based feature vector to match obstacles given two timely close point frames. Our method can be applied to any environment with static and dynamic obstacles, including the ones with rotating obstacles. We compare our algorithm with other clustering methods and show that when coupl
    
[^150]: 语言符号的表现：基于示范的人机交互中，体感手语手指拼写的翻译机器人获取

    Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.05135](http://arxiv.org/abs/2209.05135)

    本文提出了一种从视频示例学习手语拼写在机器人中的实现的方法。通过训练模仿运动的策略，利用预训练的深度视觉模型从RGB视频中提取手的三维姿态，并识别出最佳的模仿超参数集，本文成功展示了方法的普适性。

    

    学习机器人中细致的动作是一个具挑战性的问题，特别是在机器人手的上下文中。本文提出一种方法，通过视频示例学习无额外信息下的熟练运动模仿，以获得手语拼写在机器人中的实现。我们首先建立了一个机器人手的URDF模型，并使每个关节只有一个致动器。然后我们利用预训练的深度视觉模型从RGB视频中提取手的三维姿态。接着，我们利用最先进的强化学习算法(即近端策略优化和软演员-评论家算法)来训练一种能够复制示范运动的策略。我们基于参考运动识别出最佳的模仿超参数集。最后，我们通过对六个对应于拼写字母的不同任务进行测试，证明了我们方法的普适性。

    Learning fine-grained movements is a challenging topic in robotics, particularly in the context of robotic hands. One specific instance of this challenge is the acquisition of fingerspelling sign language in robots. In this paper, we propose an approach for learning dexterous motor imitation from video examples without additional information. To achieve this, we first build a URDF model of a robotic hand with a single actuator for each joint. We then leverage pre-trained deep vision models to extract the 3D pose of the hand from RGB videos. Next, using state-of-the-art reinforcement learning algorithms for motion imitation (namely, proximal policy optimization and soft actor-critic), we train a policy to reproduce the movement extracted from the demonstrations. We identify the optimal set of hyperparameters for imitation based on a reference motion. Finally, we demonstrate the generalizability of our approach by testing it on six different tasks, corresponding to fingerspelled letters.
    
[^151]: 通过交替最小化方法训练深度神经网络的收敛速率

    Convergence Rates of Training Deep Neural Networks via Alternating Minimization Methods. (arXiv:2208.14318v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14318](http://arxiv.org/abs/2208.14318)

    本文提出了一个统一的框架用于分析AM类型的网络训练方法的收敛速率。研究基于非单调的$j$-步充分减少条件和Kurdyka-Lojasiewicz（KL）性质，并在KL指数$ \theta $在$ [0,1) $变化时展示了详细的局部收敛速率和局部R-线性收敛。

    

    由于其非凸性和非可分性，训练深度神经网络（DNN）是机器学习中的一个重要且具有挑战性的优化问题。交替最小化（AM）方法在DNN的复合结构方面进行了拆分，并在深度学习和优化社区引起了极大的关注。在本文中，我们提出了一个统一的框架来分析AM类型的网络训练方法的收敛速率。我们的分析基于非单调的$j$-步充分减少条件和Kurdyka-Lojasiewicz（KL）性质，这放宽了设计下降算法的要求。我们在KL指数$ \theta $在$ [0,1) $变化时展示了详细的局部收敛速率。此外，在更强的$j$-步充分减少条件下讨论了局部R-线性收敛。

    Training deep neural networks (DNNs) is an important and challenging optimization problem in machine learning due to its non-convexity and non-separable structure. The alternating minimization (AM) approaches split the composition structure of DNNs and have drawn great interest in the deep learning and optimization communities. In this paper, we propose a unified framework for analyzing the convergence rate of AM-type network training methods. Our analysis is based on the non-monotone $j$-step sufficient decrease conditions and the Kurdyka-Lojasiewicz (KL) property, which relaxes the requirement of designing descent algorithms. We show the detailed local convergence rate if the KL exponent $\theta$ varies in $[0,1)$. Moreover, the local R-linear convergence is discussed under a stronger $j$-step sufficient decrease condition.
    
[^152]: 一种利用复杂网络识别可再生电力分布系统弹性的方法

    A methodology for identifying resiliency in renewable electrical distribution system using complex network. (arXiv:2208.11543v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2208.11543](http://arxiv.org/abs/2208.11543)

    本文提出一种使用复杂网络理论来识别可再生电力分布系统弹性的方法，可以识别系统中太阳能电池板的托管能力，从而有助于提高系统的韧性。

    

    近年来，电力配电系统广泛采用分布式能源资源（DER）以满足能源需求，普遍认为这可以提高系统的弹性。然而，由于各种因素（如间歇性可用性、天气条件的动态变化、非线性等）可能对电网运营产生不利影响。本文提出了一种使用复杂网络理论来识别带有太阳能光伏发电的配电系统弹性的方法。我们根据不同条件获得了不同条件下的复杂相关网络，并计算了各种网络参数，以识别网络的弹性。所提出的方法可以识别系统中太阳能电池板的托管能力，并在不同不良条件下保持系统的弹性，从而有助于提高系统的韧性。

    Recently, Electrical Distribution Systems are extensively penetrated with the Distributed Energy Resources (DERs) to cater the energy demands with general perception that it enhances the system resiliency. However, it may be adverse for the grid operation due to various factors like its intermittent availability, dynamics in weather condition, introduction of nonlinearity, complexity etc. This needs a detailed understanding of system resiliency that our method proposes here. We introduce a methodology using complex network theory to identify the resiliency of distribution system when incorporated with Solar PV generation under various undesirable configurations. Complex correlated networks for different conditions were obtained and various network parameters were computed for identifying the resiliency of those networks. The proposed methodology identifies the hosting capacity of solar panels in the system while maintaining the resiliency under different unwanted conditions hence helps
    
[^153]: 抽奖池：通过插值票据而不增加训练或推理成本来获胜

    Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost. (arXiv:2208.10842v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10842](http://arxiv.org/abs/2208.10842)

    本论文提出了一种名为抽奖池（Lottery Pools）的方法，它可以通过直接平均相邻学习得到的子网络的权重或者通过简单的插值策略对迭代剪枝确定的子网络执行“集成”，从而提高抽奖票（LTs）的性能。

    

    抽奖票（LTs）可以发现精确且稀疏的子网络，这些子网络可以被单独训练以匹配密集网络的性能。而集成（Ensemble）是机器学习中最古老的经过时间验证的技巧之一，通过组合多个独立模型的输出来提高性能。然而，在LTs的背景下，集成的好处会被稀疏子网络的预测结果所削弱。本文首先观察到直接平均相邻学习得到的次级子网络的权重可以显著提高LTs的性能。受到这一观察的鼓舞，我们进一步提出了一种通过简单的插值策略对迭代剪枝确定的子网络执行“集成”的替代方法。我们将这种方法称为抽奖池。与没有性能增益的朴素集成不同，扩展抽奖池可以提高每个单独的子网络的性能。

    Lottery tickets (LTs) is able to discover accurate and sparse subnetworks that could be trained in isolation to match the performance of dense networks. Ensemble, in parallel, is one of the oldest time-proven tricks in machine learning to improve performance by combining the output of multiple independent models. However, the benefits of ensemble in the context of LTs will be diluted since ensemble does not directly lead to stronger sparse subnetworks, but leverages their predictions for a better decision. In this work, we first observe that directly averaging the weights of the adjacent learned subnetworks significantly boosts the performance of LTs. Encouraged by this observation, we further propose an alternative way to perform an 'ensemble' over the subnetworks identified by iterative magnitude pruning via a simple interpolating strategy. We call our method Lottery Pools. In contrast to the naive ensemble which brings no performance gains to each single subnetwork, Lottery Pools yi
    
[^154]: MENLI: 自然语言推理的鲁棒性评估指标

    MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.07316](http://arxiv.org/abs/2208.07316)

    本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。

    

    最近被提出的基于BERT的文本生成评估指标在标准基准测试中表现良好，但易受到对信息正确性的攻击。我们认为这部分原因是此类模型是基于语义相似性建模的。相反，我们提出一种基于自然语言推理（NLI）的鲁棒性评估指标，这种指标更适合建模。我们设计了一种基于偏好的对抗性攻击框架，并表明我们的NLI基础指标比最近的BERT基础指标更具鲁棒性。在标准基准测试中，我们的NLI基础指标优于现有的摘要评估指标，但低于SOTA MT指标。然而，在现有指标与我们的NLI指标相结合时，我们既获得了更高的对抗鲁棒性（15％-30％），又获得了标准基准测试中更高的质量指标（+5％至30％）。

    Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
    
[^155]: 记忆-膨胀：建模标签噪声下神经崩溃

    Memorization-Dilation: Modeling Neural Collapse Under Label Noise. (arXiv:2206.05530v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05530](http://arxiv.org/abs/2206.05530)

    本文提出了一个更现实的无限制特征表示变体，它考虑了网络的有限表达性。结合记忆和 dropout 的新模型 MD-Dropout 有效防止了膨胀神经崩溃，并提高了鲁棒性。

    

    神经崩溃是指在各种典型分类问题中经验观察到的多种紧急现象。在训练深度神经网络的终止阶段，同一类别的所有示例的特征嵌入 tend to collapse 到单个表示，并且不同类别的特征 tend to separate。本文提出了一个更现实的无限制特征表示变体，它考虑了网络的有限表达性。实证证据表明记忆噪声数据点会导致神经崩溃的恶化（膨胀）。使用记忆膨胀（M-D）现象模型，我们展示了标签噪声如何导致神经崩溃的机制。具体而言，我们提出了一种结合记忆和 dropout 的新模型 MD-Dropout，作为一种正则化器来防止膨胀神经崩溃。我们的实验表明，MD-Dropout 提高了标签噪声的鲁棒性，并在多个数据集上优于竞争方法。

    The notion of neural collapse refers to several emergent phenomena that have been empirically observed across various canonical classification problems. During the terminal phase of training a deep neural network, the feature embedding of all examples of the same class tend to collapse to a single representation, and the features of different classes tend to separate as much as possible. Neural collapse is often studied through a simplified model, called the unconstrained feature representation, in which the model is assumed to have "infinite expressivity" and can map each data point to any arbitrary representation. In this work, we propose a more realistic variant of the unconstrained feature representation that takes the limited expressivity of the network into account. Empirical evidence suggests that the memorization of noisy data points leads to a degradation (dilation) of the neural collapse. Using a model of the memorization-dilation (M-D) phenomenon, we show one mechanism by wh
    
[^156]: 《深入研究无需重复训练的渐进式学习》

    A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.17269](http://arxiv.org/abs/2203.17269)

    本文介绍了一种新的渐进式学习方法，使用知识蒸馏和参数正则化以避免重复训练，并在不会退化已学数据的情况下实现了强大的性能。

    

    渐进式学习是机器学习模型在不断变化的训练数据中学习新概念的一种环境，同时避免以前学习的类别出现“灾难性遗忘”现象。当前的单任务扩展性渐进式学习方法需要大量重复训练以避免知识退化，但重复训练会占用大量内存，并可能违反数据隐私。相反，我们探索了将知识蒸馏和参数正则化以新的方式结合起来，以在不进行重复训练的情况下实现强大的渐进式学习性能。

    Continual learning is a setting where machine learning models learn novel concepts from continuously shifting training data, while simultaneously avoiding degradation of knowledge on previously seen classes which may disappear from the training data for extended periods of time (a phenomenon known as the catastrophic forgetting problem). Current approaches for continual learning of a single expanding task (aka class-incremental continual learning) require extensive rehearsal of previously seen data to avoid this degradation of knowledge. Unfortunately, rehearsal comes at a cost to memory, and it may also violate data-privacy. Instead, we explore combining knowledge distillation and parameter regularization in new ways to achieve strong continual learning performance without rehearsal. Specifically, we take a deep dive into common continual learning techniques: prediction distillation, feature distillation, L2 parameter regularization, and EWC parameter regularization. We first disprove
    
[^157]: 弱监督分割的重要性采样CAM方法

    Importance Sampling CAMs for Weakly-Supervised Segmentation. (arXiv:2203.12459v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.12459](http://arxiv.org/abs/2203.12459)

    本文提出了基于重要性采样和特征相似性损失项的CAM改进方法，显著提高了弱监督分割的轮廓精度性能。

    

    通过类激活图（CAM）可以利用分类神经网络对图像中的物体进行定位和分割。然而，在没有像素级标注的情况下，分类网络往往只关注区分性较强的区域，并且会产生模糊的CAM，没有明确的预测边缘。本文提出了两种改进CAM方法的贡献，以解决这两个问题。首先，我们基于CAM产生的类别概率质量函数，引入重要性采样，产生随机的图像级别的预测，从而使CAM覆盖更广泛的物体区域。其次，我们提出了一种特征相似性损失项，旨在将预测边缘与图像中的边缘匹配。第三，我们在PASCAL VOC 2012基准数据集上进行实验，证明这些改进显著提高了轮廓精度性能，同时与当前状态相当。

    Classification networks can be used to localize and segment objects in images by means of class activation maps (CAMs). However, without pixel-level annotations, classification networks are known to (1) mainly focus on discriminative regions, and (2) to produce diffuse CAMs without well-defined prediction contours. In this work, we approach both problems with two contributions for improving CAM learning. First, we incorporate importance sampling based on the class-wise probability mass function induced by the CAMs to produce stochastic image-level class predictions. This results in CAMs which activate over a larger extent of objects. Second, we formulate a feature similarity loss term which aims to match the prediction contours with edges in the image. As a third contribution, we conduct experiments on the PASCAL VOC 2012 benchmark dataset to demonstrate that these modifications significantly increase the performance in terms of contour accuracy, while being comparable to current state
    
[^158]: 深度批量主动学习回归的框架和基准

    A Framework and Benchmark for Deep Batch Active Learning for Regression. (arXiv:2203.09410v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.09410](http://arxiv.org/abs/2203.09410)

    本研究提出了一个深度批量主动学习回归的框架和基准测试，其中包括许多现有的贝叶斯和非贝叶斯方法。提出了一种替换常用最后一层特征的新方法，并结合一种新颖的聚类方法。在15个大型表格回归数据集上进行测试，该方法在基准测试中表现优异，适用于大型数据集且易于使用。

    

    标注监督学习数据的获取成本较高。为了提高神经网络回归的样本效率，我们研究了自适应选择无标签数据批次进行标注的主动学习方法。我们提出了一个框架，用于构建这样的方法，基于(网络相关的)基础核、核变换和选择方法。我们的框架包括许多现有的基于高斯过程逼近神经网络的贝叶斯方法以及非贝叶斯方法。此外，我们建议用描绘有限宽度神经正切核替换常用的最后一层特征，并将它们与一种新颖的聚类方法相结合。为了评估不同的方法，我们介绍了一个由15个大型表格回归数据集组成的开放源代码的基准测试。我们提出的方法在基准测试中优于现有技术水平，适用于大型数据集，并且可以直接使用，无需调整网络架构或训练。

    The acquisition of labels for supervised learning can be expensive. In order to improve the sample-efficiency of neural network regression, we study active learning methods that adaptively select batches of unlabeled data for labeling. We present a framework for constructing such methods out of (network-dependent) base kernels, kernel transformations and selection methods. Our framework encompasses many existing Bayesian methods based on Gaussian Process approximations of neural networks as well as non-Bayesian methods. Additionally, we propose to replace the commonly used last-layer features with sketched finite-width Neural Tangent Kernels, and to combine them with a novel clustering method. To evaluate different methods, we introduce an open-source benchmark consisting of 15 large tabular regression data sets. Our proposed method outperforms the state-of-the-art on our benchmark, scales to large data sets, and works out-of-the-box without adjusting the network architecture or traini
    
[^159]: 简单神经网络中的可达性问题研究

    Reachability In Simple Neural Networks. (arXiv:2203.07941v3 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2203.07941](http://arxiv.org/abs/2203.07941)

    本研究研究了简单神经网络中的可达性问题，并证明了对于仅具有一个隐含层和一个输出维度以及仅具有一个负、零和一个正权重或偏置的神经网络来说，它是NP难度问题。

    

    我们研究了（深度）神经网络的可达性问题的复杂性：在给定一些有效输入的情况下，它是否计算出有效输出？最近有人声称，对于一般的神经网络和由线性不等式的合取组成的输入/输出维度的规范，该问题是NP完全问题。 我们总结了证明并修复了原始上界和下界证明中的一些缺陷。受到通用结果的启发，我们展示了NP难度已经适用于简单规范和神经网络的受限类。允许一个隐藏层和一个输出维数以及仅具有一个负、零和一个正权重或偏置的神经网络就足以确保NP难度。此外，我们对神经网络验证研究的这个方向进行了全面的讨论和展望。

    We investigate the complexity of the reachability problem for (deep) neural networks: does it compute valid output given some valid input? It was recently claimed that the problem is NP-complete for general neural networks and specifications over the input/output dimension given by conjunctions of linear inequalities. We recapitulate the proof and repair some flaws in the original upper and lower bound proofs. Motivated by the general result, we show that NP-hardness already holds for restricted classes of simple specifications and neural networks. Allowing for a single hidden layer and an output dimension of one as well as neural networks with just one negative, zero and one positive weight or bias is sufficient to ensure NP-hardness. Additionally, we give a thorough discussion and outlook of possible extensions for this direction of research on neural network verification.
    
[^160]: 电影叙述摘要：一个用于故事理解的视频语言数据集

    Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.05711](http://arxiv.org/abs/2203.05711)

    这个研究收集并发布了一个视频-语言故事数据集SYMON，用于推进多模态故事理解的发展。

    

    尽管AI有了最近的进展，但故事理解仍然是一个未被充分研究的问题。我们收集、预处理并公开发布了一个视频语言故事数据集SYMON，其中包含5,193个流行电影和电视剧的视频摘要。SYMON捕捉了由人类创作者制作的面向人类观众的自然故事叙述视频。作为一个原型和自然故事数据集，SYMON具有高覆盖的多模态故事事件、丰富的心理状态描述和视觉和文本模态之间的大语义差距。我们建立了视频文本检索和电影摘要视频的零样本对齐的基准，展示了在故事理解中领域内数据的重要性。通过SYMON，我们希望为多模态故事理解的进展打下基础。

    Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
    
[^161]: 消极情绪传播更快：基于大规模多语言 Twitter 分析的情感在政治沟通中的作用

    Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication. (arXiv:2202.00396v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.00396](http://arxiv.org/abs/2202.00396)

    本文利用大规模多语言 Twitter 数据，分析了希腊、西班牙和联合王国议会成员的推文，并发现消极情绪更易传播。

    

    社交媒体已经在现代社会中对政策制定产生了极大影响，尤其是在西方世界中，Twitter 等平台让用户能够关注政治家，使公民更多地参与政治讨论。同样，政治家也利用 Twitter 表达自己的观点，在当前话题上与他人辩论，并推动自己的政治议程，旨在影响选民行为。本文试图分析三个欧洲国家政治家的推文，并探索它们的传播度。先前的研究表明，传达消极情绪的推文往往会被更频繁地转发。通过利用先进的预训练语言模型，我们对来自希腊、西班牙和联合王国（包括分权政府）议会成员的数十万条推文进行了情感分析。我们通过系统地探究和分析差异，取得了良好的效果。

    Social media has become extremely influential when it comes to policy making in modern societies, especially in the western world, where platforms such as Twitter allow users to follow politicians, thus making citizens more involved in political discussion. In the same vein, politicians use Twitter to express their opinions, debate among others on current topics and promote their political agendas aiming to influence voter behaviour. In this paper, we attempt to analyse tweets of politicians from three European countries and explore the virality of their tweets. Previous studies have shown that tweets conveying negative sentiment are likely to be retweeted more frequently. By utilising state-of-the-art pre-trained language models, we performed sentiment analysis on hundreds of thousands of tweets collected from members of parliament in Greece, Spain and the United Kingdom, including devolved administrations. We achieved this by systematically exploring and analysing the differences bet
    
[^162]: 中心平滑超图神经网络用于预测药物相互作用

    Central-Smoothing Hypergraph Neural Networks for Predicting Drug-Drug Interactions. (arXiv:2112.07837v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.07837](http://arxiv.org/abs/2112.07837)

    本文研究了预测药物相互作用问题，提出了一种新的超图神经网络 CentSmoothie，能够充分考虑标签之间的相关性，取得了最先进的性能表现。

    

    预测药物相互作用（DDI）是使用药品信息和许多对已知标签的相互作用药品来预测药品对的副作用（不良反应）的问题。这个问题可以被表述为预测 DDI 图中每对节点的标签（即副作用），其中节点是药品，边缘是具有已知标签的相互作用药品。针对这个问题，现有方法是图神经网络（GNN），它利用图中的邻域信息来学习节点表示。然而，对于 DDI，由于副作用的性质，存在许多带有复杂关系的标签。通常的 GNN 经常将标签固定为独热向量，这不能反映标签之间的关系，并且在罕见标签的困难情况下可能无法获得最佳性能。在本文中，我们将 DDI 表示为一个超图，其中每个超边是一个三元组：两个药品节点和一个标签节点。我们提出了 CentSmoothie，一种超图神经网络，通过在超图上的平滑池化来融合标签之间的相关性。CentSmoothie 在两个大型 DDI 预测基准数据集上取得了最先进的性能。

    Predicting drug-drug interactions (DDI) is the problem of predicting side effects (unwanted outcomes) of a pair of drugs using drug information and known side effects of many pairs. This problem can be formulated as predicting labels (i.e. side effects) for each pair of nodes in a DDI graph, of which nodes are drugs and edges are interacting drugs with known labels. State-of-the-art methods for this problem are graph neural networks (GNNs), which leverage neighborhood information in the graph to learn node representations. For DDI, however, there are many labels with complicated relationships due to the nature of side effects. Usual GNNs often fix labels as one-hot vectors that do not reflect label relationships and potentially do not obtain the highest performance in the difficult cases of infrequent labels. In this paper, we formulate DDI as a hypergraph where each hyperedge is a triple: two nodes for drugs and one node for a label. We then present CentSmoothie, a hypergraph neural n
    
[^163]: 高维参数学习的迭代分块粒子滤波算法：摆脱维度灾难。

    Iterated Block Particle Filter for High-dimensional Parameter Learning: Beating the Curse of Dimensionality. (arXiv:2110.10745v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.10745](http://arxiv.org/abs/2110.10745)

    本文提出了一种迭代分块粒子滤波算法，用于学习高维参数。该算法克服了维度灾难，表现出良好的收敛性和似然最大化，成功地在非线性和非高斯空间时间模型上实现了参数学习。

    

    高维、部分观测和非线性随机过程的参数学习是一种方法论上的挑战。空间时间疾病传播系统提供了这种产生开放推断问题的过程的示例。我们提出了迭代分块粒子滤波算法，用于学习具有一般状态空间、测量、转移密度和图结构的图形状态空间模型的高维参数。对击败维度灾难 (COD)、算法收敛和似然最大化获得了理论性能保证。对麻疹传播的高度非线性和非高斯空间时间模型进行的实验表明，迭代集合卡尔曼滤波算法 (Li et al. (2020)) 是无效的，而迭代滤波算法 (Ionides et al. (2015)) 受到维度灾难的困扰，而我们的IBPF算法在各种具有不同度量的实验中始终击败了COD。

    Parameter learning for high-dimensional, partially observed, and nonlinear stochastic processes is a methodological challenge. Spatiotemporal disease transmission systems provide examples of such processes giving rise to open inference problems. We propose the iterated block particle filter (IBPF) algorithm for learning high-dimensional parameters over graphical state space models with general state spaces, measures, transition densities and graph structure. Theoretical performance guarantees are obtained on beating the curse of dimensionality (COD), algorithm convergence, and likelihood maximization. Experiments on a highly nonlinear and non-Gaussian spatiotemporal model for measles transmission reveal that the iterated ensemble Kalman filter algorithm (Li et al. (2020)) is ineffective and the iterated filtering algorithm (Ionides et al. (2015)) suffers from the COD, while our IBPF algorithm beats COD consistently across various experiments with different metrics.
    
[^164]: 自适应联合分布学习

    Adaptive joint distribution learning. (arXiv:2110.04829v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.04829](http://arxiv.org/abs/2110.04829)

    该论文提出了一种自适应联合分布学习的框架，可以从大量数据点中估计低维、归一化和正的Radon-Nikodym导数模型，并在不同学习问题上取得了良好的结果。

    

    我们开发了一个新的框架，用于将联合概率分布嵌入张量积再生核希尔伯特空间（RKHS）中。我们的框架可以容纳一个低维、归一化和正的Radon-Nikodym导数模型，该模型可以从多达数百万个数据点的样本大小中进行估计，减轻了RKHS建模的固有限制。我们的方法自然产生了定义良好的归一化和正的条件分布。嵌入计算速度快且适用于从预测到分类的各种学习问题。我们的理论结果得到了有益的数值结果的支持。

    We develop a new framework for embedding joint probability distributions in tensor product reproducing kernel Hilbert spaces (RKHS). Our framework accommodates a low-dimensional, normalized and positive model of a Radon-Nikodym derivative, which we estimate from sample sizes of up to several million data points, alleviating the inherent limitations of RKHS modeling. Well-defined normalized and positive conditional distributions are natural by-products to our approach. The embedding is fast to compute and accommodates learning problems ranging from prediction to classification. Our theoretical findings are supplemented by favorable numerical results.
    
[^165]: 通信高效的联邦线性和深度广义典型相关分析

    Communication-Efficient Federated Linear and Deep Generalized Canonical Correlation Analysis. (arXiv:2109.12400v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.12400](http://arxiv.org/abs/2109.12400)

    本文提出了一种通信高效的联邦学习框架，用于线性和深度广义典型相关分析，通过压缩本地数据统计信息和使用子空间共识算法减少通信成本，同时具有与中心化算法相当的性能。

    

    经典和深度广义典型相关分析（GCCA）算法分别使用线性变换和神经网络从多个“视图”（例如音频和图像）中寻找低维的共同表示。当这些视图由不同的计算代理（例如组织和边缘设备）进行获取和存储，并且由于隐私或通信成本等考虑不希望数据共享时，联邦学习的GCCA是合理的。在联邦学习中，这些视图在代理处保持本地，并且只允许与中央服务器进行导出的有限信息交换。然而，将现有的GCCA算法应用到这些联邦学习环境中可能会产生过高的通信开销。本研究提出了一种通信高效的联邦学习框架，用于最大方差（MAX-VAR）形式下的线性和深度GCCA。通过在传输前积极压缩（通过量化编码）本地数据统计信息和使用一种新颖的子空间共识算法来解决开销问题，该方法在极大地减少通信成本的同时实现了与现有中心化GCCA算法相当的竞争性能。

    Classic and deep generalized canonical correlation analysis (GCCA) algorithms seek low-dimensional common representations of data entities from multiple ``views'' (e.g., audio and image) using linear transformations and neural networks, respectively. When the views are acquired and stored at different computing agents (e.g., organizations and edge devices) and data sharing is undesired due to privacy or communication cost considerations, federated learning-based GCCA is well-motivated. In federated learning, the views are kept locally at the agents and only derived, limited information exchange with a central server is allowed. However, applying existing GCCA algorithms onto such federated learning settings may incur prohibitively high communication overhead. This work puts forth a communication-efficient federated learning framework for both linear and deep GCCA under the maximum variance (MAX-VAR) formulation. The overhead issue is addressed by aggressively compressing (via quantizat
    
[^166]: 基于因果嵌入的物理系统预测的通用可观测集

    Universal set of Observables for Forecasting Physical Systems through Causal Embedding. (arXiv:2105.10759v3 [math.DS] UPDATED)

    [http://arxiv.org/abs/2105.10759](http://arxiv.org/abs/2105.10759)

    本文实现了一种基于因果嵌入的预测方案，可以唯一地表示基础动力系统的整个左无限轨道或来自这样的轨道的观测，该方案具有通用性、可计算性和误差容错性，可以在长期保证预测一致性。

    

    我们展示了何时以及如何可以通过不同空间中的一对元素唯一地表示基础动力系统的整个左无限轨道或来自这样的左无限轨道的观测，我们称之为“因果嵌入”现象。这些成对的集合是从驱动的动力系统中导出的，并用于学习一个函数，该函数与驱动系统一起确定一个拓扑共轭于基础系统的系统，使得预测基础系统的动态成为可能，因为这种共轭是可计算且通用的，即它不依赖于基础系统，即使在学习函数时存在误差，也能保证吸引子包含因果嵌入物体的图像。通过实现这些，我们引领了一种新的预测方案，其优于现有的储备计算方案，后者通常导致长期不一致性很差，因为无法保证存在可学习的系统。

    We demonstrate when and how an entire left-infinite orbit of an underlying dynamical system or observations from such left-infinite orbits can be uniquely represented by a pair of elements in a different space, a phenomenon which we call \textit{causal embedding}. The collection of such pairs is derived from a driven dynamical system and is used to learn a function which together with the driven system would: (i). determine a system that is topologically conjugate to the underlying system (ii). enable forecasting the underlying system's dynamics since the conjugacy is computable and universal, i.e., it does not depend on the underlying system (iii). guarantee an attractor containing the image of the causally embedded object even if there is an error made in learning the function. By accomplishing these we herald a new forecasting scheme that beats the existing reservoir computing schemes that often lead to poor long-term consistency as there is no guarantee of the existence of a learna
    
[^167]: 通过字典学习实现Transformer可视化:将上下文嵌入作为Transformer因子的线性叠加

    Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. (arXiv:2103.15949v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2103.15949](http://arxiv.org/abs/2103.15949)

    本文提出使用字典学习将上下文嵌入作为Transformer因子的线性叠加来打开Transformer“黑匣子”，通过可视化展示其捕获的层次化语义结构，为更好地理解Transformer网络的工作方式带来新的见解。

    

    自Transformer网络问世以来，它们在自然语言处理表示学习中掀起了一场革命。尽管已经做出了大量努力来解释Transformer网络中的表示，但广泛认为我们的理解还不够。其中一个重要原因是缺乏足够的可视化工具来进行详细的分析。本文提出使用字典学习将其作为Transformer因子的线性叠加来打开这些“黑匣子”。通过可视化，我们展示了被Transformer因子捕获的层次化语义结构，例如词级多义消歧、句子级模式形成和长距离依赖。虽然一些模式符合传统的语言知识，但其余的模式相对出乎意料，可能提供新的见解。我们希望这种可视化工具能带来更多的知识和对Transformer网络工作方式的更好理解。代码可在https://github.com/z中找到。

    Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these "black boxes" as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at https://github.com/z
    
[^168]: MARS:张量分解中的自动排名选择问题

    MARS: Masked Automatic Ranks Selection in Tensor Decompositions. (arXiv:2006.10859v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.10859](http://arxiv.org/abs/2006.10859)

    本文介绍了一种名为MARS的新型高效方法，在一般的张量分解中自动选择秩，学习二值掩码来选择最佳的张量结构，在实验中显示出更好的结果。

    

    张量分解方法已被证明在各种应用中具有很好的效果，包括神经网络的压缩和加速。同时，确定最优分解秩的问题仍然很严峻，因为它是控制压缩-准确性平衡的关键参数。本文介绍了一种名为MARS的新型高效方法，在一般的张量分解中自动选择秩。在训练过程中，该方法学习二值掩码，这些掩码可以选择最佳的张量结构。学习是通过特定的贝叶斯模型中的松弛最大后验(MAP)估计来完成的，并可以自然地嵌入到标准的神经网络训练过程中。各种实验表明，与先前的工作相比，MARS在各种任务中都取得了更好的结果。

    Tensor decomposition methods have proven effective in various applications, including compression and acceleration of neural networks. At the same time, the problem of determining optimal decomposition ranks, which present the crucial parameter controlling the compression-accuracy trade-off, is still acute. In this paper, we introduce MARS -- a new efficient method for the automatic selection of ranks in general tensor decompositions. During training, the procedure learns binary masks over decomposition cores that "select" the optimal tensor structure. The learning is performed via relaxed maximum a posteriori (MAP) estimation in a specific Bayesian model and can be naturally embedded into the standard neural network training routine. Diverse experiments demonstrate that MARS achieves better results compared to previous works in various tasks.
    

