# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation.](http://arxiv.org/abs/2311.01447) | CADSim是一个能够自动重建车辆几何形状的方法，通过野外传感器数据和CAD模型的组合，克服了稀疏性和噪声问题。 |
| [^2] | [Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop Simulation.](http://arxiv.org/abs/2311.01446) | 论文提出了Adv3D框架，通过闭环传感器仿真评估自主性能，并通过优化车辆形状使得场景更具挑战，导致自主失败和SDV操纵不舒服。 |
| [^3] | [Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models.](http://arxiv.org/abs/2311.01442) | 本文探讨了深度学习模型在时间序列预测中的训练模式，发现了深度双下降现象，并提出了通过增加训练轮次消除过拟合的方法，取得了在大多数基准测试上的最先进结果。 |
| [^4] | [Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models.](http://arxiv.org/abs/2311.01441) | 本文提出了一个概念简单且轻量级的框架，通过结合知识蒸馏和数据增强的方法来提高视觉模型的鲁棒性，并从预训练的基础模型中获得鲁棒教师模型的知识。借助离散对抗蒸馏方法，我们生成更有信息量的对抗样本，取得了在对抗性样本上鲁棒性显著提升的结果。此外，我们提供了理论框架来支持在知识蒸馏和数据增强设置中使用鲁棒教师模型，并展示了在不同学生模型上的显著性能提升。我们的方法在计算负载方面的开销较小，并可以与其他数据增强方法轻松结合。 |
| [^5] | [Contrastive Moments: Unsupervised Halfspace Learning in Polynomial Time.](http://arxiv.org/abs/2311.01435) | 这篇论文提出了一个多项式时间算法，在未知的情况下，通过使用对比矩来学习具有边缘的高维半空间，而不需要标签，并在这个分布假设下建立了隐藏半空间的唯一性和高效性。 |
| [^6] | [Tailoring Mixup to Data using Kernel Warping functions.](http://arxiv.org/abs/2311.01434) | 本研究提出了一种利用核扭曲函数对Mixup数据进行个性化处理的方法，通过动态改变插值系数的概率分布来实现更频繁和更强烈的混合相似数据点。实验证明这种方法不仅提高了模型性能，还提高了模型的校准性。 |
| [^7] | [Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods.](http://arxiv.org/abs/2311.01428) | 本研究提出了一种使用RF、SVM和CNN算法结合MRI图像分水岭分割进行特征提取的方法来鉴别四个阿尔茨海默病痴呆阶段的分类，其中SVM结合分水岭特征的准确率达到了96.25%。 |
| [^8] | [Exploring Deep Learning Techniques for Glaucoma Detection: A Comprehensive Review.](http://arxiv.org/abs/2311.01425) | 本文综述了使用深度学习方法在青光眼分割、分类和检测方面的尖端技术。通过分析近期研究，发现深度学习算法在自动化青光眼检测中具有显著的潜力，并为进一步研究提供了潜在领域。 |
| [^9] | [Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data.](http://arxiv.org/abs/2311.01420) | 这篇论文提出了一个学习问题，即如何将预先训练的源模型适应到目标领域，使用仅覆盖部分标签空间的目标数据进行分类。论文通过构建基准数据集，并进行大量实验证明，揭示了这个问题的挑战。为了解决这一问题，论文提出了两个关键方向：分离领域梯度和分类梯度，并保持类别关系。 |
| [^10] | [Castor: Causal Temporal Regime Structure Learning.](http://arxiv.org/abs/2311.01412) | “Castor”是一个用于学习多元时间序列数据中因果关系的框架，能够综合学习各个区域的因果图。它通过最大化得分函数来推断区域的数量，并学习每个区域中的线性或非线性因果关系。 |
| [^11] | [The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing.](http://arxiv.org/abs/2311.01410) | 本论文提出了一种概率形式的扩散图像编辑方法，说明了SDE在图像编辑中的优势，并提供了SDE和ODE在各种任务中的对应关系。此外，还提出了一种基于SDE的点在图像上拖动的方法，并建立了一个具有开放集的挑战性基准。 |
| [^12] | [A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference.](http://arxiv.org/abs/2311.01409) | 这篇论文提出了一种基于核心集的、温和变分后验的高斯过程推理方法，通过利用稀疏的、可解释的数据表示来降低参数大小，并且具有数值稳定性和较低的时间和空间复杂度。 |
| [^13] | [Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability.](http://arxiv.org/abs/2311.01406) | 本研究提出了一种使用图卷积网络和强化学习模型的创新方法，分析以太坊网络中的信息传播模式，并优化网络效率和可扩展性。最终目标是学习在不同网络状态下采取的最佳行动，提高网络效率和可扩展性。 |
| [^14] | [Learning to See Physical Properties with Active Sensing Motor Policies.](http://arxiv.org/abs/2311.01405) | 该论文提出了一种通过自我监督标记图像来学习机器人对地形的物理性质进行预测的方法，通过主动感知运动策略，机器人能够增加物理参数估计的准确性。 |
| [^15] | [Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs.](http://arxiv.org/abs/2311.01404) | 本文以线性控制神经ODE的流动作为归一化流构造最优传输映射的近似。通过离散最优耦合问题和数值方案，实现了对最优传输映射的近似。最终结果有助于构建深度神经网络中的可逆传输映射。 |
| [^16] | [Learning Realistic Traffic Agents in Closed-loop.](http://arxiv.org/abs/2311.01394) | 本文提出了一种在闭环中学习真实交通代理的方法，利用模仿学习和强化学习相结合的方式，通过匹配专家演示的方式，实现了更加真实和合规的驾驶行为。 |
| [^17] | [Time-series Generation by Contrastive Imitation.](http://arxiv.org/abs/2311.01388) | 本研究探讨了一种生成框架，旨在将自回归模型的显式转换分布和基于对抗训练的隐式转换相结合，通过对比估计训练全局能量模型，并优化本地转换策略来解决时间序列生成中的挑战。 |
| [^18] | [Vision-Language Foundation Models as Effective Robot Imitators.](http://arxiv.org/abs/2311.01378) | 该论文介绍了一种利用视觉语言基础模型进行机器人操作的方法，通过在语言条件的操作数据集上进行微调，实现了在低性能平台上的有效模仿控制。 |
| [^19] | [Monotone Generative Modeling via a Gromov-Monge Embedding.](http://arxiv.org/abs/2311.01375) | 该论文提出了一种使用Gromov-Monge嵌入的深度生成模型，通过识别数据背后的底层结构，并将其映射到低维潜空间中，解决了生成对抗网络（GAN）中对初始条件敏感性和模式崩溃的问题。 |
| [^20] | [Respiratory Anomaly Detection using Reflected Infrared Light-wave Signals.](http://arxiv.org/abs/2311.01367) | 利用反射红外光波信号，开发了一种利用低成本光源和传感器进行非接触呼吸异常检测的方法，实现了96.6%的平均准确率，并能够检测到错误数据。 |
| [^21] | [On the Lipschitz constant of random neural networks.](http://arxiv.org/abs/2311.01356) | 本文研究了随机ReLU神经网络的Lipschitz常数，对于浅层神经网络，我们得到了Lipschitz常数的精确刻画，对于足够宽度的深层神经网络，我们给出了上下界，并匹配一个依赖于深度的对数因子。 |
| [^22] | [Deep learning based Image Compression for Microscopy Images: An Empirical Study.](http://arxiv.org/abs/2311.01352) | 本研究实证分析了经典和基于深度学习的图像压缩方法在深度学习图像处理模型中的影响，并以基于深度学习的无标签预测模型为例展示了图像压缩的有效性，以减小数据尺寸并保留必要信息，从而减轻数据管理基础设施的负担并方便数据共享或云计算。 |
| [^23] | [Unreading Race: Purging Protected Features from Chest X-ray Embeddings.](http://arxiv.org/abs/2311.01349) | 论文的目的是利用正交化方法消除胸部X射线嵌入中的保护特征影响，并证明其有效性。研究结果表明保护特征对病理预测有显著影响，而应用正交化方法可以消除这些影响。 |
| [^24] | [Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers.](http://arxiv.org/abs/2311.01344) | 本文研究了如何通过简单的功率分析方法，在32位微控制器上提取深度神经网络模型的架构信息。 |
| [^25] | [Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching.](http://arxiv.org/abs/2311.01331) | 本论文提出了一种通过最小化原始Wasserstein距离来匹配专家和学习者状态占用的方法，以解决离线学习从观察中模仿任务的问题。 |
| [^26] | [A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories.](http://arxiv.org/abs/2311.01329) | 本文提出了一种使用离线观测和示例的简单解决方案，通过使用基于轨迹的加权行为克隆和专家状态鉴别器来稳定地学习，以解决离线模仿中由于不完整轨迹而导致的不稳定问题。 |
| [^27] | [High-dimensional Linear Bandits with Knapsacks.](http://arxiv.org/abs/2311.01327) | 本文研究了具有背包约束的高维线性赌臂问题，利用稀疏结构实现改进遗憾。通过开发在线硬阈值算法和原始-对偶框架结合的方法，实现了对特征维度的对数改进的次线性遗憾。 |
| [^28] | [Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly.](http://arxiv.org/abs/2311.01323) | 本文建立了一个传递性攻击基准（TA-Bench），评估了30多种方法在25个热门替代/受害模型上的性能，以系统地、公正地、实用地比较这些方法。 |
| [^29] | [Scattering Vision Transformer: Spectral Mixing Matters.](http://arxiv.org/abs/2311.01310) | 本文提出了一种名为散射视觉变换（SVT）的新方法，通过光谱混合来解决视觉变换中的注意力复杂性和信息捕捉问题。 |
| [^30] | [AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models.](http://arxiv.org/abs/2311.01305) | AWEQ是一种后训练量化和激活权重均衡方法，能够在大型语言模型中实现超低位量化和8-bit权重和激活量化，并通过改进的均衡方法减小量化偏差误差，提高模型的鲁棒性。 |
| [^31] | [TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models.](http://arxiv.org/abs/2311.01301) | TRIALSCOPE是一个统一的框架，利用生物医学语言模型将临床文本进行结构化，采用概率建模进行去噪和插补，并应用因果推断技术来应对混杂因素，以从实际世界数据中提取实证证据和推理临床假设。 |
| [^32] | [DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning.](http://arxiv.org/abs/2311.01295) | 本文研究了差分隐私学习中数据增强技术的问题，并提出了两种针对差分隐私学习约束的新型数据增强技术，通过对自我增强数据进行mixup和合成数据的使用，实现了优于其他方法的分类性能。 |
| [^33] | [FlashDecoding++: Faster Large Language Model Inference on GPUs.](http://arxiv.org/abs/2311.01282) | FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。 |
| [^34] | [Long-Range Neural Atom Learning for Molecular Graphs.](http://arxiv.org/abs/2311.01276) | 这项研究提出了一种针对分子图的长程神经原子学习方法，通过将原子投射为神经原子并在其之间交换信息，实现了远距离节点之间的通信，缩小了任意节点对的相互作用范围。 |
| [^35] | [An energy-based comparative analysis of common approaches to text classification in the Legal domain.](http://arxiv.org/abs/2311.01256) | 本研究通过在法律领域的文本分类任务上进行比较分析，综合考虑性能和能源消耗等指标，探讨了大型语言模型与传统方法的优劣，并强调了在性能相近的情况下应重视生产成本、能源消耗和碳足迹等方面的考量。 |
| [^36] | [Sanitized Clustering against Confounding Bias.](http://arxiv.org/abs/2311.01252) | 本文提出了一个名为“Sanitized Clustering Against confounding Bias (SCAB)”的新的聚类框架，用于在复杂数据的语义潜空间中去除混淆因素。 |
| [^37] | [Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching.](http://arxiv.org/abs/2311.01248) | 本研究利用视觉触觉传感器和模仿学习相结合，通过配对优化触觉力量曲线和简化传感器应用，对接触丰富的操作任务进行了研究。 |
| [^38] | [Multi-Operational Mathematical Derivations in Latent Space.](http://arxiv.org/abs/2311.01230) | 本文研究在潜在空间中逼近多个数学运算进行表达式推导的可能性，并通过构建大规模数据集和使用最先进的神经编码器实例化，探索了不同编码机制在潜在空间中逼近方程推理的能力。 |
| [^39] | [Diffusion Models for Reinforcement Learning: A Survey.](http://arxiv.org/abs/2311.01223) | 强化学习中的扩散模型已经成为一种突出的生成模型，通过在样本质量和训练稳定性方面的优势改进了强化学习解决方案。该综述提供了这一新兴领域发展的概述，并探讨了扩散模型在强化学习中的分类法和应用。 |
| [^40] | [Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent.](http://arxiv.org/abs/2311.01205) | 我们设计了Injectivity Bit Flip Attack来针对图神经网络，成功地降低了其对图结构的识别能力和表达能力，从而增加了其对位反转攻击的易受攻击性。 |
| [^41] | [Federated Learning on Edge Sensing Devices: A Review.](http://arxiv.org/abs/2311.01201) | 本文综述了在边缘感知设备上的联邦学习的研究现状。边缘感知设备的快速增长使得监测环境特征并获取环境信息成为可能，但云端或服务器上的分析面临隐私、硬件和连接性等挑战。联邦学习作为一种解决方案逐渐受到关注。 |
| [^42] | [A Study of Continual Learning Under Language Shift.](http://arxiv.org/abs/2311.01200) | 本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。 |
| [^43] | [Gaussian Processes on Cellular Complexes.](http://arxiv.org/abs/2311.01198) | 本论文在细胞复合物上应用高斯过程，提出了两个新的核函数来捕捉高阶细胞之间的交互作用。 |
| [^44] | [Combating Bilateral Edge Noise for Robust Link Prediction.](http://arxiv.org/abs/2311.01196) | 该论文提出了一种对抗双边边缘噪声的方法，通过引入信息论引导的原则，提取可靠的监督信号和避免表示崩溃，从而实现稳健的图链接预测。 |
| [^45] | [Batch Bayesian Optimization for Replicable Experimental Design.](http://arxiv.org/abs/2311.01195) | 这项研究提出了适用于可复制实验设计的批量贝叶斯优化框架，通过自适应选择复制次数来平衡评估更多唯一条件与增加每个条件的复制次数之间的权衡，并考虑到了从业人员的风险规避倾向。 |
| [^46] | [VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification.](http://arxiv.org/abs/2311.01191) | VIGraph是一个基于自我监督学习的模型，通过利用自编码器生成少数类节点来解决图数据中的类别不平衡问题，并通过引入孪生对比策略提高生成节点的质量。 |
| [^47] | [A Review of Digital Twins and their Application in Cybersecurity based on Artificial Intelligence.](http://arxiv.org/abs/2311.01154) | 数字孪生技术与人工智能工具之间的强互动可以改善数字平台的网络安全，但由于缺乏信息和安全标准，网络犯罪分子能够利用数字孪生技术对整个集合造成威胁。 |
| [^48] | [Add and Thin: Diffusion for Temporal Point Processes.](http://arxiv.org/abs/2311.01139) | 本研究提出了一种基于概率去噪扩散模型的时间点过程模型，相比于现有的方法，该模型在预测方面取得了较好的性能，对具有离散和连续成分的数据具有处理能力。 |
| [^49] | [AeroPath: An airway segmentation benchmark dataset with challenging pathology.](http://arxiv.org/abs/2311.01138) | 在本研究中，我们介绍了一个新的具有挑战性病理的气道分割基准数据集（AeroPath），以提高肺部疾病早期诊断和治疗的效果。我们还提出了一种多尺度融合设计，用于自动气道分割。模型在该数据集上进行了训练。 |
| [^50] | [Generating QM1B with PySCF$_{\text{IPU}}$.](http://arxiv.org/abs/2311.01135) | 本文介绍了使用IPU硬件加速器的数据生成器PySCF_IPU来生成具有十亿个训练样本的QM1B数据集，此数据集包含9-11个重原子。通过对这个数据集的实验，我们证明了一个简单的基准神经网络的优秀性能。 |
| [^51] | [A deep learning experiment for semantic segmentation of overlapping characters in palimpsests.](http://arxiv.org/abs/2311.01130) | 本文提出了一种基于深度学习的语义分割方法，用于识别和分割叠写字符中的个体字母。该方法在普里西亚诺的《语法文献》这个案例研究中进行了验证，并讨论了结合多光谱成像的局限性和前景。 |
| [^52] | [AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning.](http://arxiv.org/abs/2311.01118) | 该论文介绍了一种新型反应预测器系统RMechRP，通过对比学习和机理路径，可以准确且可解释地预测自由基反应，具有广泛应用潜力。 |
| [^53] | [H-NeXt: The next step towards roto-translation invariant networks.](http://arxiv.org/abs/2311.01111) | H-NeXt是一个参数高效的旋转平移不变网络，通过等变和不变的组件实现鲁棒性和性能优于现有技术水平。 |
| [^54] | [In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer.](http://arxiv.org/abs/2311.01106) | 本文辩护了采用Softmax参数化的学习推迟方法，解释了先前关于其估计器的误校准和无界估计的原因，并提出了一种新颖的统计一致性非对称Softmax估计器。 |
| [^55] | [Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning.](http://arxiv.org/abs/2311.01075) | 本文提出了一种名为对比模块与时间注意力(CMTA)的方法，该方法通过对比学习使模块之间的差异，并以更细粒度的方式在任务级别之下使用共享模块和时间注意力来解决多任务领域中现有方法的限制。 |
| [^56] | [Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images.](http://arxiv.org/abs/2311.01064) | 本论文提出了一种新颖的零样本物种分类框架，通过使用多模式基础模型，利用视觉-语言模型生成相机陷阱图像的视觉描述，并通过匹配外部知识库中的描述来确定零样本条件下的物种。 |
| [^57] | [Deep Learning for real-time neural decoding of grasp.](http://arxiv.org/abs/2311.01061) | 本文提出了一种基于深度学习的神经信号解码方法，利用LSTM网络将含有神经数据的时间序列进行分类，以实现物体抓取类型的分类。该方法在不依赖先前的神经科学知识的情况下，通过提取数据相关性来提高解码准确性，取得了显著的分类准确度提升。 |
| [^58] | [Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment.](http://arxiv.org/abs/2311.01059) | 本研究提出了一种名为ROAM的方法，通过利用先前学习到的行为来实时调节机器人在部署过程中应对未曾见过的情况。在测试中，ROAM可以在单个阶段内实现快速适应，并且在模拟环境和真实场景中取得了成功，具有较高的效率和适应性。 |
| [^59] | [Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis.](http://arxiv.org/abs/2311.01052) | 这项研究引入了韧性多选学习（rMCL）方法，通过使用基于Voronoi tessellations的数学框架和学习评分方案，在回归设置中实现了对于每个训练输入可能采样多个目标的条件分布估计。该方法在合成数据和声源定位问题上得到了实证验证和进一步评估，展示了其实际的有用性和解释的相关性。 |
| [^60] | [Application and Energy-Aware Data Aggregation using Vector Synchronization in Distributed Battery-less IoT Networks.](http://arxiv.org/abs/2311.01050) | 该论文提出了一种使用向量同步在分布式无电池物联网网络中进行应用和节能数据聚合的机制，旨在为无电池物联网设备提供可持续的应用支持，并解决了由于动态能量收集环境导致的任务执行不稳定性的挑战。 |
| [^61] | [Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective.](http://arxiv.org/abs/2311.01047) | 本论文提出了一种基于通信理论的方法，通过神经竞争来增强神经网络层输出的信噪比，从而提高深度网络的稳健性。 |
| [^62] | [Time-Independent Information-Theoretic Generalization Bounds for SGLD.](http://arxiv.org/abs/2311.01046) | 该论文提出了针对SGLD的无时间信息论广义界，尽管迭代次数和步长可能不固定，但这些界在样本大小增加时会衰减为零。同时，还建立了在训练和测试损失相同时的信息论广义界，并解决了现有工作中步长依赖的问题，从而得到了改进的过度风险界。 |
| [^63] | [Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks.](http://arxiv.org/abs/2311.01038) | 本文提出了一种基于数据激活视角的图神经网络预训练方法，通过精心选择较少的数据提高预训练模型的性能。这一方法能够在图数据中选择最具代表性和指导性的数据点进行训练。 |
| [^64] | [Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction.](http://arxiv.org/abs/2311.01033) | 本论文提出了一种基于扩散的非自回归连续时间点过程模型，用于长期事件预测。通过整体预测未来事件序列、开发双向映射和设计降噪网络等手段，得到了更优的预测质量。 |
| [^65] | [Distance-Based Propagation for Efficient Knowledge Graph Reasoning.](http://arxiv.org/abs/2311.01024) | 提出了一种基于距离的传播策略TAGNet，用于知识图谱补全任务中的高效推理。与其他方法相比，TAGNet能够在保持性能的前提下减少传播消息的数量，并且复杂度与层数无关。 |
| [^66] | [Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion.](http://arxiv.org/abs/2311.01017) | 本论文提出了一种通过离散扩散学习无监督的自动驾驶世界模型的新方法，通过使用VQVAE对传感器观察进行标记化并通过离散扩散预测未来，我们的模型在点云观察中实现了显著改进，将1秒预测的SOTA Chamfer距离降低了65%以上。 |
| [^67] | [Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game.](http://arxiv.org/abs/2311.01011) | 这项研究提供了一个解释性的提示注入攻击的数据集，并利用该数据集创建了两种类型的抵抗基准。研究结果表明，许多模型容易受到这些攻击策略的攻击。 |
| [^68] | [Exploring Unified Perspective For Fast Shapley Value Estimation.](http://arxiv.org/abs/2311.01010) | 这篇论文探索了统一视角下快速计算Shapley值的方法，提出了一种简单高效的摊销估计器SimSHAP，通过消除冗余技术显著加速了准确Shapley值的计算。 |
| [^69] | [Effective Human-AI Teams via Learned Natural Language Rules and Onboarding.](http://arxiv.org/abs/2311.01007) | 本论文提出了一种通过学习自然语言规则和引导的方法，以提高人工智能团队的效果。通过找到数据的局部区域和使用语言模型进行描述，我们教导人类如何与AI合作。通过目标检测和问答任务的用户研究，我们证明了我们的方法可以使人工智能团队更加准确。 |
| [^70] | [Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy.](http://arxiv.org/abs/2311.01002) | 该论文提出了一种通过最大化重新标记准确性来进行鲁棒数据修剪的算法，该算法能够找到一个子集，使得所有训练示例的邻域置信度之和最大化。这个方法在现代深度学习中具有重要的应用价值。 |
| [^71] | [Scalable Probabilistic Forecasting in Retail with Gradient Boosted Trees: A Practitioner's Approach.](http://arxiv.org/abs/2311.00993) | 本论文从实践者的角度出发，提出了在零售领域中使用梯度提升树进行可扩展概率预测的方法，通过研究两层次的层次结构和自上而下的预测方法，在处理大规模数据集和间断性数据方面具有较好的性能。 |
| [^72] | [Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks.](http://arxiv.org/abs/2311.00983) | 本文提出了一种基于决策导向学习的方法来解决库存配送问题，通过直接集成库存预测和路径优化，可能确保一个强大的供应链策略。 |
| [^73] | [Autonomous Learning of Generative Models with Chemical Reaction Network Ensembles.](http://arxiv.org/abs/2311.00975) | 本论文通过化学反应网络实现了生成模型的自主学习，能够优化任何详细平衡的化学反应网络，并使用隐藏单元学习复杂分布。这个方法可以看作是积分反馈控制的一种形式，同时我们还能够推导出与学习过程相关的热力学成本和权衡。 |
| [^74] | [Federated Linear Bandits with Finite Adversarial Actions.](http://arxiv.org/abs/2311.00973) | 文章研究了具有有限对抗动作的联邦线性赌博机模型，并提出了FedSupLinUCB算法，实现了$\tilde{O}(\sqrt{d T})$的总遗憾，并且通信成本可以被控制在$O(d M^2 \log(d)\log(T))$和$O(\sqrt{d^3 M^3} \log(d))$内。 |
| [^75] | [An Integrated Framework Integrating Monte Carlo Tree Search and Supervised Learning for Train Timetabling Problem.](http://arxiv.org/abs/2311.00971) | 提出了一个综合应用蒙特卡洛树搜索和监督学习的框架，用于解决列车运行图问题(TTP)。实验证明该框架在提高TTP的解决效率方面是有效的。 |
| [^76] | [Invariant-Feature Subspace Recovery: A New Class of Provable Domain Generalization Algorithms.](http://arxiv.org/abs/2311.00966) | ISR是一种新型的可证明的领域泛化算法，它可以通过类条件分布的一阶矩来识别不变特征所张成的子空间。 |
| [^77] | [On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications.](http://arxiv.org/abs/2311.00964) | 本文研究了在金融科技应用中寻找高质量的双目标 Pareto 最优欺诈预防规则集的问题。通过采用 Pareto 最优性概念和启发式框架 PORS，我们成功提出了一组非支配的规则子集，并通过实证评估证明了其有效性。 |
| [^78] | [Dynamic Fair Federated Learning Based on Reinforcement Learning.](http://arxiv.org/abs/2311.00959) | 本研究提出了一种基于强化学习的动态公平联邦学习算法(DQFFL)，旨在解决联邦学习中的公平性问题。该算法通过动态调整参数，减轻设备聚合过程中的差异，提升对所有群体的公平处理。 |
| [^79] | [E3 TTS: Easy End-to-End Diffusion-based Text to Speech.](http://arxiv.org/abs/2311.00945) | E3 TTS是一种简单高效的端到端基于扩散的文本到语音模型，不依赖于中间表示，通过扩散过程建模波形的时间结构，能够轻松适应零样本任务。 |
| [^80] | [Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization.](http://arxiv.org/abs/2311.00944) | 本论文提出了一种新算法，FESS-GDA，利用平滑技术进行联邦极小极大优化。通过解决不同类型的联邦极小极大问题，我们证明了FESS-GDA的收敛性，并展示了其在实际联邦学习任务中的实际效果。 |
| [^81] | [Gaussian Mixture Solvers for Diffusion Models.](http://arxiv.org/abs/2311.00941) | 这篇论文提出了一种名为高斯混合解算器(GMS)的新型SDE-based解算器用于扩散模型，通过估计前三阶矩并优化高斯混合参数来解决现有SDE-based解算器的效率-有效性困境问题。 |
| [^82] | [Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance.](http://arxiv.org/abs/2311.00938) | 本文介绍了一种更新的损失函数，以更好地对齐传统训练方法与扩散模型所期望的条件采样行为之间的差异。实验证明该方法能够以更少的采样时间步长生成更高质量的样本，并对于引导规模的选择更具鲁棒性。 |
| [^83] | [SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data.](http://arxiv.org/abs/2311.00936) | 利用遥感和公众科学数据进行鸟类物种分布建模，填补了传统方法中的知识空白，为改善生物多样性监测和复杂生态系统建模提供了机会。 |
| [^84] | [Learning Defect Prediction from Unrealistic Data.](http://arxiv.org/abs/2311.00931) | 该论文研究了从不真实的数据集中学习缺陷预测的问题，并提出了一种基于学习表示的方法来识别与真实数据集最相似的子集。 |
| [^85] | [Scalable Counterfactual Distribution Estimation in Multivariate Causal Models.](http://arxiv.org/abs/2311.00927) | 该论文提出了一种可扩展的方法，用于在多变量因果模型中估计多个感兴趣量的反事实联合分布。通过利用原始高维空间中的一维潜在子空间和单一变量因果模型，该方法可以同时处理多变量结果的相关结构并产生准确的反事实分布估计。 |
| [^86] | [A Review and Roadmap of Deep Causal Model from Different Causal Structures and Representations.](http://arxiv.org/abs/2311.00923) | 本研究综述了深度因果模型在不同因果结构和表示方面的应用，并提出了明确数据、半明确数据和不明确数据三种因果数据范例的定义，以应对将原始的因果概念和理论拓展到复杂非统计数据的挑战。 |
| [^87] | [MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training.](http://arxiv.org/abs/2311.00919) | 通过成员不变子空间训练的MIST算法有效防御成员推理攻击，能够识别容易受到攻击的实例并避免过度拟合。 |
| [^88] | [Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition.](http://arxiv.org/abs/2311.00906) | 本文提出了一种重新加权的主动学习策略，通过为每个标记分配动态平滑的权重，解决了命名实体识别中的数据不平衡问题，取得了显著的性能提升。 |
| [^89] | [Data-Driven Model Selections of Second-Order Particle Dynamics via Integrating Gaussian Processes with Low-Dimensional Interacting Structures.](http://arxiv.org/abs/2311.00902) | 本文提出了一种将高斯过程与低维交互结构相结合的基于数据驱动的二阶粒子动力学模型选择方法，该方法能够处理相互作用代理的聚合和集体行为，具有较高的可扩展性和不确定性量化能力。 |
| [^90] | [COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning.](http://arxiv.org/abs/2311.00886) | 这项研究提出了一种名为COSTAR的新方法，通过整合自监督学习，改进了时间反事实结果的估计。该方法在处理时间相关混淆因素时结合了时间和特征关注以及分量对比损失，相比现有模型在估计准确性和对分布之外数据的泛化能力方面表现出更优越的性能。 |
| [^91] | [SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization.](http://arxiv.org/abs/2311.00880) | SCPO是一种安全强化学习算法，通过引入安全批判器来确保遵守安全约束并平衡回报的最大化。 |
| [^92] | [Learning Collective Behaviors from Observation.](http://arxiv.org/abs/2311.00875) | 本文介绍了一系列学习方法，用于从观察中识别动态系统的结构，并理解复杂系统中相互作用代理的 emergent behaviors。这些方法不仅具备理论收敛保证，还能高效处理高维观测数据，解决观测/随机噪声、复杂的交互规则、丢失的交互特征和现实世界观测数据等问题。通过采用变分逆问题方法设计合适的损失函数，我们的学习方法具备降维能力。 |
| [^93] | [Low-latency Real-time Voice Conversion on CPU.](http://arxiv.org/abs/2311.00873) | 这项研究实现了在CPU上进行低延迟实时语音转换，通过适应之前的音频神经网络架构，并使用对抗生成网络和知识蒸馏，模型LLVC在16kHz的比特率下具有不到20ms的延迟，并在消费级CPU上以2.8倍的速度运行。同时，LLVC还是所有开源语音转换模型中资源使用最低延迟最低的。 |
| [^94] | [Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models.](http://arxiv.org/abs/2311.00871) | Transformer模型通过预训练数据混合实现了狭窄的模型选择能力，能够在上下文中识别和学习不同的任务，但对于任务或函数的处理相对有限。 |
| [^95] | [Generalizing Nonlinear ICA Beyond Structural Sparsity.](http://arxiv.org/abs/2311.00866) | 本论文研究了超越结构稀疏性的非线性ICA的泛化问题，提出了一组新的在不完备性、部分稀疏性、源依赖性和灵活的分组结构下的可辨识性结果。 |
| [^96] | [Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2311.00865) | 本文介绍了一种选择性多智能体强化学习方法，即选择性多智能体优先体验中继，代理之间共享有限数量的训练经验。与其他算法相比，该方法实现了去中心化训练，并取得了比基准算法和最先进算法更好的性能。 |
| [^97] | [Training Dynamics of Contextual N-Grams in Language Models.](http://arxiv.org/abs/2311.00863) | 这篇论文研究了训练过程中上下文N-Gram的动态，发现了上下文神经元存在于更广泛的上下文N-Gram电路中，这被称为二阶电路。在训练早期，这两个电路具有相互独立的功能，只有在它们都形成之后才能组合成一个二阶电路。 |
| [^98] | [Role of Structural and Conformational Diversity for Machine Learning Potentials.](http://arxiv.org/abs/2311.00862) | 研究发现在机器学习势函数中，需要谨慎平衡结构和构象多样性以实现最佳的泛化效果，并强调了定义适用领域对于模型部署的重要性。 |
| [^99] | [Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning.](http://arxiv.org/abs/2311.00860) | 本文提出了一种用于物理约束操作学习的新型自动微分算法，通过零坐标移动（ZCS）的技巧，将所需导数的复杂度从“多根多叶”简化为“一根多叶”，从而显著提高了性能。 |
| [^100] | [Optimal Cost Constrained Adversarial Attacks For Multiple Agent Systems.](http://arxiv.org/abs/2311.00859) | 本研究提出了一种基于最优成本约束的分布式攻击代理的对抗攻击方法，可以在多智能体系统中显著降低受攻击代理获得的奖励。 |
| [^101] | [SmoothHess: ReLU Network Feature Interactions via Stein's Lemma.](http://arxiv.org/abs/2311.00858) | SmoothHess是一种通过Stein引理对ReLU网络的特征交互进行建模的方法，它能估计网络和高斯卷积的Hessian矩阵，具有高效的抽样算法和可控的平滑程度。在基准数据集和真实世界的医学肺功能数据集上验证了SmoothHess捕捉交互作用的卓越能力。 |
| [^102] | [A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan.](http://arxiv.org/abs/2311.00855) | 本论文提出了一种多智能体强化学习（MARL）框架，用于评估美国终结HIV流行计划。该框架能够进行特定地区的决策分析，并考虑到地区之间的流行病学相互作用。 |
| [^103] | [Electronic excited states from physically-constrained machine learning.](http://arxiv.org/abs/2311.00844) | 本文介绍了一种综合建模方法，通过训练一个对称自适应的机器学习模型来再现量子力学计算中的电子激发，从而实现了针对更大更复杂分子的预测，并通过最小原子中心基础对应的参数化实现了极大的计算效益。 |
| [^104] | [Sharp Noisy Binary Search with Monotonic Probabilities.](http://arxiv.org/abs/2311.00840) | 这篇论文提出了一个带有单调概率的尖锐噪音二分查找模型，并给出了一个算法，该算法通过解决高概率行为和尖锐常数两个挑战，可以在给定样本数的情况下以高概率成功地找到概率交叉的地方。 |
| [^105] | [A quantum-classical performance separation in nonconvex optimization.](http://arxiv.org/abs/2311.00811) | 本文揭示了一类非凸优化实例的量子-经典性能差异，用量子哈密顿下降算法可以高效解决，而经典算法则需要超多项式时间。 |
| [^106] | [Mahalanobis-Aware Training for Out-of-Distribution Detection.](http://arxiv.org/abs/2311.00808) | 本文提出了一种用于改善基于密度的超出分布敏感性的新的损失函数和训练方法，在CIFAR-10数据集上取得了显著的效果，尤其在远离分布的任务上将相对马哈拉诺比斯距离方法的误检率降低超过50%。 |
| [^107] | [VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization.](http://arxiv.org/abs/2311.00807) | VQA-GEN是首个用于领域通用化的多模态基准测试数据集，揭示了现有VQA方法对于联合多模态分布变化的脆弱性，验证了全面的多模态变化对于VQA的稳健性通用化至关重要。 |
| [^108] | [Neural Field Dynamics Model for Granular Object Piles Manipulation.](http://arxiv.org/abs/2311.00802) | 本论文提出了一种用于颗粒材料操作的学习动力学模型。该模型采用了基于密度场的表示方法和全卷积神经网络，并具有可微分的动作渲染模块。在模拟和实际实验中的评估结果表明，该模型在准确性和计算效率上都显著超过了现有的方法，并且在不同环境和任务中具有零样本泛化能力。 |
| [^109] | [GIST: Generated Inputs Sets Transferability in Deep Learning.](http://arxiv.org/abs/2311.00801) | 这篇论文介绍了一种在深度学习模型之间高效迁移测试集的新方法，通过选择具有用户感兴趣的属性的良好测试集，以达到改善可验证性和测试性的目的。 |
| [^110] | [Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling.](http://arxiv.org/abs/2311.00797) | 该论文通过机器学习辅助的数据驱动建模方法，研究了自适应易感-感染-易感流行病学网络的临界转折点集体动力学。他们识别出了一个有效的随机微分方程以描述网络的演化行为，并观察到了罕见的大幅度集体振荡现象。 |
| [^111] | [Accelerating Electronic Stopping Power Predictions by 10 Million Times with a Combination of Time-Dependent Density Functional Theory and Machine Learning.](http://arxiv.org/abs/2311.00787) | 使用时间相关密度泛函理论和机器学习的方法将电子阻止能力预测加快了1000万倍，并提供了关于原子细节如何影响电子阻止的宝贵数据。 |
| [^112] | [Harnessing machine learning for accurate treatment of overlapping opacity species in GCMs.](http://arxiv.org/abs/2311.00775) | 本论文基于机器学习提出了一种快速且准确的方法来处理GCM中重叠的透明度物种，通过有效地结合各个相关-k透明度表（k-tables），该方法在热木星HD~209458 b的模拟中表现出了精确性和高效性。 |
| [^113] | [Conformalized Deep Splines for Optimal and Efficient Prediction Sets.](http://arxiv.org/abs/2311.00774) | 该论文提出了一种新的一致化回归方法，通过神经网络参数化样条估计条件密度的样条预测区间，证明了其普适逼近和最优性，实验结果表明在基准数据集上表现出色。 |
| [^114] | [Language Model Training Paradigms for Clinical Feature Embeddings.](http://arxiv.org/abs/2311.00768) | 本研究使用自监督训练范式的语言模型，通过表示学习为临床时间序列推导出高质量的通用临床特征嵌入。通过无监督的降维技术可视化学习到的嵌入，并在MIMIC-III基准测试中验证了它们的有效性。 |
| [^115] | [Learning to Design and Use Tools for Robotic Manipulation.](http://arxiv.org/abs/2311.00754) | 本论文提出了学习一种设计策略来制作适用于不同任务的专用工具，并通过这些工具进行机器人操纵。这可以解锁机器人的额外能力。 |
| [^116] | [Are These the Same Apple? Comparing Images Based on Object Intrinsics.](http://arxiv.org/abs/2311.00750) | 本研究提出了一种基于物体内在特性的图像相似度度量方法，通过对通用对象类别进行扩展，并收集了大规模的CUTE数据集来评估该方法。 |
| [^117] | [Sorting with Predictions.](http://arxiv.org/abs/2311.00749) | 本论文通过学习增强的算法视角探索了基于预测的排序问题，并设计了新的简单算法，实现了理论上最优的比较复杂性，对应用预测排序方法具有潜在的优势。 |
| [^118] | [Can Large Language Models Design Accurate Label Functions?.](http://arxiv.org/abs/2311.00739) | 本研究引入了DataSculpt，它是一个利用预训练语言模型自动生成标签函数的交互式框架。通过多种技术和方法的结合，DataSculpt在各种任务和真实数据集上展现了优点和局限性。 |
| [^119] | [Real-Time Magnetic Tracking and Diagnosis of COVID-19 via Machine Learning.](http://arxiv.org/abs/2311.00737) | 本研究将磁性呼吸感应技术与机器学习相结合，创建了一个实时诊断平台，用于追踪和诊断COVID-19和其他呼吸系统疾病。通过对COVID-19患者和健康人士的呼吸数据进行训练和验证，我们评估了多个机器学习算法的诊断能力，选出最优模型，平衡了诊断精度和模型可解释性。 |
| [^120] | [PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network.](http://arxiv.org/abs/2311.00735) | 本研究开发了一种示踪剂转化可逆神经网络（TC-INN），通过深度学习将FDG图像映射到DOPA图像，以解决DOPA在PET成像中广泛应用的挑战。 |
| [^121] | [tmn at #SMM4H 2023: Comparing Text Preprocessing Techniques for Detecting Tweets Self-reporting a COVID-19 Diagnosis.](http://arxiv.org/abs/2311.00732) | 本文研究了用于检测自我报告COVID-19诊断推文的不同文本预处理技术，通过使用四个基于transformer的模型进行实验，并通过微调语言模型集成获得了比平均值高出4.1%的84.5%的F1得分。 |
| [^122] | [Enhancing Clustering Representations with Positive Proximity and Cluster Dispersion Learning.](http://arxiv.org/abs/2311.00731) | 本论文提出了一种新的深度聚类方法，名为PIPCDR，通过结合正向实例相似性损失和簇离散正则化器，以提升聚类性能。 |
| [^123] | [Investigating Relative Performance of Transfer and Meta Learning.](http://arxiv.org/abs/2311.00727) | 本文研究了迁移学习和元学习作为解决有限数据学习问题的两种方法的相对性能，以建立一个在不同的机器学习场景中选择最适合的方法的稳健标准。 |
| [^124] | [Fraud Analytics Using Machine-learning & Engineering on Big Data (FAME) for Telecom.](http://arxiv.org/abs/2311.00724) | 本文提出了一种工业化解决方案，利用自适应数据挖掘技术和大数据技术来准确、高效、低成本地检测电信行业的欺诈。已成功应用于检测国际收入分成欺诈，并发现了新的欺诈模式。 |
| [^125] | [Empathy Detection Using Machine Learning on Text, Audiovisual, Audio or Physiological Signals.](http://arxiv.org/abs/2311.00721) | 本论文对共情检测领域的机器学习研究进行了综述和分析，包括文本、视听、音频和生理信号四种输入模态的处理和网络设计，以及评估协议和数据集的描述。 |
| [^126] | [Deep Neural Networks for Automatic Speaker Recognition Do Not Learn Supra-Segmental Temporal Features.](http://arxiv.org/abs/2311.00489) | 这项研究表明深度神经网络在自动说话人识别中无法充分模拟超分段时间特征，这为未来更好地利用完整语音信号进行研究提供了基础。 |
| [^127] | [JADE: A Linguistic-based Safety Evaluation Platform for LLM.](http://arxiv.org/abs/2311.00286) | JADE是一种基于语言分析的LLM安全评估平台，能够破坏广泛使用的中文和英文LLM，并生成高度威胁的不安全问题。 |
| [^128] | [AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data.](http://arxiv.org/abs/2310.20280) | 这篇论文提出了AutoMixer，一个基于时间序列基础模型的自动混合模型，通过通道压缩预训练和微调工作流技术，有效解耦了BizITOps数据中有用和嘈杂的跨通道交互，提高了多变量时间序列预测的性能。 |
| [^129] | [On Learning Gaussian Multi-index Models with Gradient Flow.](http://arxiv.org/abs/2310.19793) | 本研究探讨了在高维高斯数据的多索引回归问题中，通过梯度流学习低秩线性投影和低维连接函数，建立了全局收敛性和定量描述的算法。 |
| [^130] | [Gradient-free online learning of subgrid-scale dynamics with neural emulators.](http://arxiv.org/abs/2310.19385) | 本文提出了一种利用神经仿真器在线训练亚网格参数化的算法，通过后验损失函数适应非可微分数值求解器，并通过时间积分步骤允许梯度传播。实验证明，将神经仿真器和参数化组件分别用相应的损失量进行训练是必要的，以最小化某些近似偏差的传播。 |
| [^131] | [Understanding and Improving Ensemble Adversarial Defense.](http://arxiv.org/abs/2310.18477) | 通过新的错误理论，作者提出了一个名为iGAT的方法用于改进集成对抗防御，该方法通过选择性地分配对抗样本和正则化来提高防御效果。该方法在各种情况下进行了测试并取得成功。 |
| [^132] | [Meaning Representations from Trajectories in Autoregressive Models.](http://arxiv.org/abs/2310.18348) | 本文提出了一种从自回归语言模型中提取意义表征的方法，通过考虑输入文本的所有可能轨迹的分布。这种方法可以模拟非对称关系，且在语义相似性任务上优于其他方法。 |
| [^133] | [Deep Transformed Gaussian Processes.](http://arxiv.org/abs/2310.18230) | 本文提出了一种名为深度转换高斯过程（DTGPs）的转换高斯过程（TGPs）的推广，该模型采用串联层级的随机过程，并实现了相对于TGPs和DGPs的灵活性增强。通过使用变分推理，可以近似所需的计算，从而得到了简单直接的推理算法扩展。 |
| [^134] | [Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow.](http://arxiv.org/abs/2310.17403) | 本文对光流中的对抗性贴片攻击进行了研究，发现目前的检测和去除防御策略不仅降低了光流质量，同时也损害了抵御贴片攻击的鲁棒性。 |
| [^135] | [De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks.](http://arxiv.org/abs/2310.17341) | 本研究结合递归神经网络（RNN）和临时卷积神经网络（TCN），使用新的反应Smiles-like表示实现了全新的化学反应生成，并通过迁移学习发现微调协议对模型生成范围有重要影响。 |
| [^136] | [MimicTouch: Learning Human's Control Strategy with Multi-Modal Tactile Feedback.](http://arxiv.org/abs/2310.16917) | MimicTouch是一种新的框架，能够模仿人类的触觉引导控制策略，通过收集来自人类示范者的多模态触觉数据集，来学习并执行复杂任务。 |
| [^137] | [Getting aligned on representational alignment.](http://arxiv.org/abs/2310.13018) | 该论文研究了生物和人工信息处理系统的表示一致性，探讨了不同系统之间的表示是否一致以及如何调整表示以更好地匹配其他系统。为了改善领域之间的交流，提出了一个统一的框架作为共同语言。 |
| [^138] | [Fast Model Debias with Machine Unlearning.](http://arxiv.org/abs/2310.12560) | 这篇论文提出了一种快速模型去偏置的框架（FMD），可以有效识别、评估和消除深度神经网络中的偏见，解决了现有方法在成本和解释性方面的不足。 |
| [^139] | [Transparent Anomaly Detection via Concept-based Explanations.](http://arxiv.org/abs/2310.10702) | 本论文提出了一种透明的基于概念解释的异常检测方法（ACE），能够提供人类可解释的解释和异常预测。该方法在推进异常检测的透明度的同时，实现了有效的人机交互，并且在性能上要么更高，要么与黑盒不可解释模型相当。 |
| [^140] | [Unraveling Fundamental Properties of Power System Resilience Curves using Unsupervised Machine Learning.](http://arxiv.org/abs/2310.10030) | 本研究使用无监督机器学习对电力系统的恢复曲线进行了研究，发现了两种主要的恢复曲线原型：三角形曲线和梯形曲线，分别表征了快速恢复和逐步恢复的恢复过程。 |
| [^141] | [Towards End-to-end 4-Bit Inference on Generative Large Language Models.](http://arxiv.org/abs/2310.09259) | 本论文介绍了一种使用名为QUIK的混合量化策略，在保持良好精度的同时实现大型生成模型的实际速度提升，通过将权重和激活值转换为4位，并提供高效率的逐层运行时GPU内核，实现了高达3.1倍的实际端到端吞吐量提升。 |
| [^142] | [Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds.](http://arxiv.org/abs/2310.06157) | 本研究提出了一种基于模型的参数化方法，利用测地线和流动来描述可微流形上的距离和长度最小化曲线。这为在不同iable流形上进行统计和降阶建模提供了机会。 |
| [^143] | [Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization.](http://arxiv.org/abs/2310.04015) | 本文提出了一种名为“类似样本聚类”的技术，通过替换个体的敏感特征为聚类的平均值来增强隐私。通过对使用匿名聚类中心训练模型的精确分析，我们揭示了不同模型组成部分对泛化误差的影响，并证明在某些高维情况下，使用匿名聚类中心进行训练可以取得更好的效果。 |
| [^144] | [Dual Prompt Tuning for Domain-Aware Federated Learning.](http://arxiv.org/abs/2310.03103) | 本文提出了一种面向领域感知的联邦学习方法，通过双提示调优实现领域适应。实验结果表明，该方法在联邦学习中具有显著的效果。 |
| [^145] | [Differentiable Chemical Physics by Geometric Deep Learning for Gradient-based Property Optimization of Mixtures.](http://arxiv.org/abs/2310.03047) | 本文开发了一个不同iable的化学物理框架DiffMix，利用几何深度学习将分子物种、组成和环境条件映射到混合物物理定律中的物理系数上。通过创建可学习的物理系数，我们扩展了混合物热力学和输运定律，并展示了DiffMix相比纯数据驱动变量改进的预测准确性和模型鲁棒性。 |
| [^146] | [1D-CapsNet-LSTM: A Deep Learning-Based Model for Multi-Step Stock Index Forecasting.](http://arxiv.org/abs/2310.02090) | 本研究提出了一个基于一维CapsNet和LSTM网络的混合模型，用于多步股指预测。该模型利用CapsNet从序列数据中生成高级胶囊，同时利用LSTM网络捕获时间依赖关系。模型在真实股票市场指数上表现优于基线模型。 |
| [^147] | [Score-based Data Assimilation for a Two-Layer Quasi-Geostrophic Model.](http://arxiv.org/abs/2310.01853) | 本论文评估了基于评分的数据同化方法在高维度的地球物理动力系统中的可扩展性，并通过在双层拟地转动模型上的实验证明了该方法的良好性能。 |
| [^148] | [Bayesian Design Principles for Frequentist Sequential Learning.](http://arxiv.org/abs/2310.00806) | 该论文提出了一种贝叶斯设计原则用于优化频率式序贯学习问题的通用理论，通过生成“算法信念”并使用贝叶斯后验进行决策，实现了无先验的贝叶斯类型算法在对抗性环境中的泛化和优化应用。 |
| [^149] | [MiliPoint: A Point Cloud Dataset for mmWave Radar.](http://arxiv.org/abs/2309.13425) | MiliPoint是一个大规模、开放的点云数据集，用于探索如何利用毫米波雷达进行人体活动识别。与现有数据集相比，它具有更大的规模、更多样化的人体活动表示，并涵盖了人体活动识别的所有关键任务。 |
| [^150] | [Data Summarization beyond Monotonicity: Non-monotone Two-Stage Submodular Maximization.](http://arxiv.org/abs/2309.05183) | 这篇论文研究了两阶段子模最大化问题，目标是使用子模训练函数来减少底层集合，并引入了非单调子模函数的第一个恒定因子逼近算法。 |
| [^151] | [SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks.](http://arxiv.org/abs/2309.04037) | 提出了一种基于深度学习的科学数据错误有界损失压缩器SRN-SZ，用于改善难以压缩的数据集的压缩情况。 |
| [^152] | [Representing Edge Flows on Graphs via Sparse Cell Complexes.](http://arxiv.org/abs/2309.01632) | 本文针对图边缘的流量数据，提出了一种通过稀疏细胞复合体来表示边流的方法。我们将图结构转化为一个单纯复合体，利用Hodge-Laplacian的特征向量和关联矩阵进行Hodge分解，得到梯度、旋量和谐波流的表示。同时，我们引入了细胞推断优化问题，通过添加细胞来增强观测到的图，使表示稀疏可解释。实验证明这个问题是NP难的，我们提出了一个高效的近似算法。 |
| [^153] | [Deception Game: Closing the Safety-Learning Loop in Interactive Robot Autonomy.](http://arxiv.org/abs/2309.01267) | 本文提出了一种闭环安全学习的范式，用于合成机器人的安全控制策略。该方法考虑机器人学习能力和不确定性，能够快速应对未来场景，从而在确保安全的同时保持性能表现。 |
| [^154] | [Learned Visual Features to Textual Explanations.](http://arxiv.org/abs/2309.00733) | 本研究提出了一种名为TExplain的方法，将大型语言模型与预训练图像分类器的特征空间连接起来，通过生成解释性句子来理解分类器学习到的特征。该方法首次利用这些频繁单词揭示出分类器的决策过程，实现了检测虚假特征的能力。 |
| [^155] | [ILCAS: Imitation Learning-Based Configuration-Adaptive Streaming for Live Video Analytics with Cross-Camera Collaboration.](http://arxiv.org/abs/2308.10068) | ILCAS是首个基于模仿学习的适应性配置流式传输系统，用于实时视频分析，并在传输过程中实现相机之间的协作。与基于深度强化学习的解决方案不同，ILCAS通过模仿学习训练代理，提高了性能表现和应用目标的多样性。 |
| [^156] | [Dyadic Reinforcement Learning.](http://arxiv.org/abs/2308.07843) | 该论文介绍了一个称为二元强化学习的在线算法，用于根据上下文因素和目标人与其照顾伴侣的过去反馈，个性化地提供干预措施。该算法是贝叶斯和层次的，并通过模拟展示了良好的实证效果。 |
| [^157] | [Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models.](http://arxiv.org/abs/2308.06534) | 本研究评估了在医学影像领域使用自监督预训练方法的可行性，比较了共同对比学习和掩码自编码器方法在CT扫描卷积模型中的性能。 |
| [^158] | [Atlas-Based Interpretable Age Prediction.](http://arxiv.org/abs/2307.07439) | 本研究提出了一种基于图谱的可解释年龄预测方法，利用全身图像研究了各个身体部位的年龄相关变化。通过使用解释性方法和配准技术，确定了最能预测年龄的身体区域，并创下了整个身体年龄预测的最新水平。研究结果表明，脊柱、本原性背部肌肉和心脏区域是最重要的关注领域。 |
| [^159] | [Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\"odinger Equation.](http://arxiv.org/abs/2307.07050) | 这篇论文提出了一种新的方法，即基于Wasserstein量子蒙特卡洛的方法，用于解决量子多体Schr\"odinger方程。该方法重新制定了能量泛函的最小化问题，并将其转化为Born分布空间中的粒子排列（反）对称波函数的问题，同时利用深度学习方法来表示丰富的波函数族。 |
| [^160] | [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models.](http://arxiv.org/abs/2307.05973) | VoxPoser提出了一种新方法，通过组合3D价值映射和语言模型，实现了机器人在多种操作任务下根据自由形式的指令和对象合成机器人轨迹的能力。 |
| [^161] | [Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning.](http://arxiv.org/abs/2307.03486) | 通过对比学习方法，我们在强化学习中提出了新的成就蒸馏方法，可以加强代理对下一个解锁成就的预测能力，并优于先前的模型驱动和层次化方法。 |
| [^162] | [Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation.](http://arxiv.org/abs/2307.02598) | 这篇论文解决了表示学习中的潜变量识别和"支持外"图像生成问题，展示了加法解码器能够对潜变量进行识别，并提供了理论依据支持这种方法的有效性。 |
| [^163] | [EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models.](http://arxiv.org/abs/2307.02028) | 该论文介绍了EHRSHOT，一个用于少样本评估基础模型的电子健康记录基准。该论文利用EHRSHOT数据集和预训练模型CLMBR-T-base，为医疗保健ML的发展提供了解决方案。 |
| [^164] | [Fitted Value Iteration Methods for Bicausal Optimal Transport.](http://arxiv.org/abs/2306.12658) | 本文提出了一种适用于双因果最优传输问题的拟合值迭代方法，能够在保证精度的同时具有良好的可扩展性，数值实验结果也证明了该方法的优越性。 |
| [^165] | [Fedstellar: A Platform for Decentralized Federated Learning.](http://arxiv.org/abs/2306.09750) | Fedstellar是一个联邦学习平台，支持物理或虚拟设备的去中心化、半去中心化和中心化的方式训练模型，旨在解决现有平台在处理异构联盟网络拓扑等问题时的挑战。 |
| [^166] | [(Amplified) Banded Matrix Factorization: A unified approach to private training.](http://arxiv.org/abs/2306.08153) | 本文提出了利用带状矩阵构建的矩阵分解机制，该机制能够在所有隐私预算中将先前最先进的算法纳入分散和联合训练设置中。对于跨设备联合学习，这意味着可以使用一种放松的设备参与模式，与实际的FL基础设施相容。在集中式设置中，带状矩阵具有与 ubiquitous DP-SGD algorithm 相同的隐私放大结果。 |
| [^167] | [Parting with Misconceptions about Learning-based Vehicle Motion Planning.](http://arxiv.org/abs/2306.07962) | 该论文提出了nuPlan，一个大规模真实世界数据集和评估方案，针对精确的短期规划和长期目标预测。证实了现有系统难以同时满足两个要求。最终提出一个非常简单高效的规划器。 |
| [^168] | [Boosting Adversarial Transferability by Achieving Flat Local Maxima.](http://arxiv.org/abs/2306.05225) | 本文提出了一种近似优化方法来实现平坦局部极小值，该方法可以提高对抗性转移能力，并在实验中证实了该方法的有效性。 |
| [^169] | [Fine-grained Expressivity of Graph Neural Networks.](http://arxiv.org/abs/2306.03698) | 本研究通过考虑将1-WL和MPNN连续扩展为graphon，并通过准确的拓扑表征揭示了MPNN在图上的表达能力，为图和graphon相似度提供了理论框架。 |
| [^170] | [Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning.](http://arxiv.org/abs/2305.19913) | 这项研究提出了一种名为ReNO的框架，解决了神经操作符在离散实现时出现的完整性损失和误差问题，为神经操作符的学习提供了新的视角。 |
| [^171] | [BiSLS/SPS: Auto-tune Step Sizes for Stable Bi-level Optimization.](http://arxiv.org/abs/2305.18666) | 本文研究了稳定双层优化中自动调整步长的方法，并提出了两个变体的改进版本来替代现有的算法。这些方法使用了最近提出的自适应步长方法进行计算，以缓解计算超梯度时可能引起的近似误差问题。 |
| [^172] | [Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics.](http://arxiv.org/abs/2305.18333) | 研究了物品流行度、质量和位置偏差对用户福利的影响，提出了通过探索减轻流行度偏见负面影响的算法。 |
| [^173] | [Diable: Efficient Dialogue State Tracking as Operations on Tables.](http://arxiv.org/abs/2305.17020) | Diable是一个高效的对话状态跟踪系统，它通过在表格上进行操作来更新对话状态，相比现有方法时间效率提高了2.4倍，同时保持了竞争性的目标准确性。 |
| [^174] | [The Behavior and Convergence of Local Bayesian Optimization.](http://arxiv.org/abs/2305.15572) | 本文研究了贝叶斯本地优化策略的行为和收敛性，并在高维问题上提供了强大的实证性能。统计数据表明，单个高斯过程样本路径的本地解比全局方法恢复的预期值更好。Müller等人提出的贝叶斯本地优化算法的收敛速率在有噪音和无噪音的情况下都有推导。 |
| [^175] | [Generalized Bayesian Inference for Scientific Simulators via Amortized Cost Estimation.](http://arxiv.org/abs/2305.15208) | 提出了摊销成本估计方法，能够解决广义贝叶斯推理方法中多个模拟的计算问题，从而为深度神经网络提供了一种处理高维度、复杂复现，且贝叶斯后验未必是最佳方案的科学模拟器的优化方法。 |
| [^176] | [Generalizing Importance Weighting to A Universal Solver for Distribution Shift Problems.](http://arxiv.org/abs/2305.14690) | 本文推广了重要性加权方法，将其变成了适用于所有情况的通用求解器，有效地解决了包括部分重叠情况在内的分布偏移问题。 |
| [^177] | [Textually Pretrained Speech Language Models.](http://arxiv.org/abs/2305.13009) | 本论文提出了一种使用预训练的文本语言模型训练语音语言模型的方法，通过对模型设计选择和数据集规模的经验性分析，构建了参数数量和训练数据最多的语音语言模型，并引入了两个Spoken版本的文本基准，以进一步改善模型评估和推动未来研究。 |
| [^178] | [Online Continual Learning Without the Storage Constraint.](http://arxiv.org/abs/2305.09253) | 本文提出了一种无存储限制的在线持续学习算法，使用kNN分类器和通用预训练特征提取器，在小算力的情况下紧凑地存储和利用整个输入数据流，并实现了更好的性能。 |
| [^179] | [Description and Discussion on DCASE 2023 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring.](http://arxiv.org/abs/2305.07828) | DCASE 2023 挑战任务2旨在解决机器状态监测中，部署新型机器的无监督异常声音检测，仅使用极少量的正常数据进行训练，且无需超参数调整。 |
| [^180] | [Calibrated Explanations: with Uncertainty Information and Counterfactuals.](http://arxiv.org/abs/2305.02305) | 该论文提出了一种新的特征重要性解释方法，Calibrated Explanations (CE)，它可以提供准确、稳定的解释，并且可以为概率估计和特征重要性权重提供不确定性量化信息，是一种快速、可靠且强健的解释方法。 |
| [^181] | [Sequence Modeling with Multiresolution Convolutional Memory.](http://arxiv.org/abs/2305.01638) | 本论文提出了一种新的用于序列建模的构建块，称为MultiresLayer，通过多分辨率卷积捕获输入序列中的多尺度趋势，既具有卷积网络的计算优势，又具有小波分解的有理论基础的动机。 |
| [^182] | [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.](http://arxiv.org/abs/2305.00586) | 本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。 |
| [^183] | [When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability.](http://arxiv.org/abs/2304.14274) | 同源性原则不一定是影响图神经网络优越性的唯一原因；本文提出Contextual Stochastic Block Model for Homophily (CSBM-H)以深入研究同源性对节点可区分性的影响。 |
| [^184] | [Improving Adversarial Transferability by Intermediate-level Perturbation Decay.](http://arxiv.org/abs/2304.13410) | 本文提出了一种名为中间层扰动衰减（ILPD）的新型中间层方法，可以通过单个优化阶段制作可转移的对抗性样本，并在过程中鼓励中间层扰动处于有效的敌对方向，并同时具有很大的幅度。 |
| [^185] | [A Deep Learning algorithm to accelerate Algebraic Multigrid methods in Finite Element solvers of 3D elliptic PDEs.](http://arxiv.org/abs/2304.10832) | 该论文介绍了一种新的深度学习算法，通过解释线性系统的稀疏矩阵为黑白图像，利用池化操作将其转换为小的多通道图像，从而调整代数多重网格方法中的强门槛参数。该算法最小化了AMG方法在有限元求解器中的计算成本，并在解决三维椭圆偏微分方程时比现有最先进的AMG求解器更快。 |
| [^186] | [CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society.](http://arxiv.org/abs/2303.17760) | 本文介绍了一个名为角色扮演的新型交互式代理框架，用于实现语言模型之间的自主合作，并展示了其在生成对话数据方面的有效性。 |
| [^187] | [Conformal Prediction for Time Series with Modern Hopfield Networks.](http://arxiv.org/abs/2303.12783) | 该论文提出了一种名为 HopCPT 的新一致性时间序列预测方法，不仅能够处理时间结构，而且能够利用其优势，已在多种真实世界的时间序列数据集上证明了优于现有方法的性能。 |
| [^188] | [SIESTA: Efficient Online Continual Learning with Sleep.](http://arxiv.org/abs/2303.10725) | SIESTA是一种在线持续学习方法，通过使用无需回忆、无需反向传播和数据驱动的网络更新规则，在更少的时间和能源消耗下高效地更新深度神经网络(DNN)。该方法基于训练休眠/觉醒框架，可以应用于设备端学习。 |
| [^189] | [Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning.](http://arxiv.org/abs/2303.10180) | 本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来实现全麻药物剂量控制，添加了保守Q-Learning方法和策略约束项以确保智能体做出更安全的决策。 |
| [^190] | [The Re-Label Method For Data-Centric Machine Learning.](http://arxiv.org/abs/2302.04391) | 本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。 |
| [^191] | [Sample-efficient Multi-objective Molecular Optimization with GFlowNets.](http://arxiv.org/abs/2302.04040) | 本研究提出了一种使用GFlowNets的高效多目标分子优化算法，该算法通过使用超网络来优化收益函数，从而在考虑多样性的同时实现了从近似帕累托前沿中采样出多样化的候选分子图。同时还使用了一种类似于事后认识的离线策略来加快优化速度。 |
| [^192] | [Extremal Domain Translation with Neural Optimal Transport.](http://arxiv.org/abs/2301.12874) | 该论文提出了一种称为“极值传输(ET)”的方法，可用于进行给定相似性函数下的一对域之间的最佳可能的非配对翻译，并且提出了一种可扩展的基于神经最优输运(OT)的算法来逼近ET映射。 |
| [^193] | [Causal Falsification of Digital Twins.](http://arxiv.org/abs/2301.07210) | 这篇论文提出了一种数字孪生的因果伪证方法，以可靠并实用的方式在最小限度的假设下提供孪生的信息和评估结果。 |
| [^194] | [Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients.](http://arxiv.org/abs/2212.14319) | 该论文提出了一种名为EPGP的高斯过程先验，用于线性偏微分方程系统，并且构造了反映标准谱方法的GP核函数。该方法可以推断线性PDE系统的可能解，并具有算法性强、普适性广、适用于大数据集的稀疏版本。 |
| [^195] | [Inversion of Bayesian Networks.](http://arxiv.org/abs/2212.10649) | 本文研究了识别网络如何模拟真实后验分布的必要和充分条件，通过导出全局条件和局部条件，发现完美性为其具备期望性质起到了重要作用。 |
| [^196] | [Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting.](http://arxiv.org/abs/2211.15856) | 本文研究了利用气候模型集合进行地表季节性预测的应用，超越了传统的平均方法，利用集合预测中的信息提高了预测准确性，关注了极端事件的预测，同时考虑了空间变化的预测集合。 |
| [^197] | [Hierarchical Proxy Modeling for Improved HPO in Time Series Forecasting.](http://arxiv.org/abs/2211.15092) | 该论文提出了一种使用分层代理建模的技术H-Pro，通过利用时间序列数据的层次结构，通过测试代理驱动超参数优化，以解决测试验证期间不匹配的问题，并验证了该技术在时间序列数据集上的有效性。 |
| [^198] | [A Finite-Particle Convergence Rate for Stein Variational Gradient Descent.](http://arxiv.org/abs/2211.09721) | 本文提供了Stein变分梯度下降算法的有限粒子收敛速度，证明了当目标分布为次高斯且具有Lipschitz积分核时，使用适当的步长序列和粒子数量，可以以1/√(log log n)的速度将核Stein差异逼近零。 |
| [^199] | [Bridging Machine Learning and Sciences: Opportunities and Challenges.](http://arxiv.org/abs/2210.13441) | 本文探讨了机器学习在科学领域的应用，尤其是在离群样本检测方面取得了重要进展，同时提出了数据普适性、实验协议和模型鲁棒性等方面的挑战。 |
| [^200] | [Computational Choreography using Human Motion Synthesis.](http://arxiv.org/abs/2210.04366) | 本文介绍了一种利用深度学习模型分析舞蹈动作和生成新动作序列的方法，同时也结合了前人的努力来开发出一套系统。 |
| [^201] | [Targeted Separation and Convergence with Kernel Discrepancies.](http://arxiv.org/abs/2209.12835) | 通过核差异度量，我们推导出了新的充分必要条件，实现了将目标分离出来，以及控制对目标的弱收敛性。此外，我们在$\mathbb{R}^d$上使用了这些结果来扩展了核Stein差异分离和收敛控制的已知条件，并开发了能够精确度量目标的弱收敛性的核差异度量。 |
| [^202] | [Lossy Image Compression with Conditional Diffusion Models.](http://arxiv.org/abs/2209.06950) | 本文提出了一种利用条件扩散模型进行有损图像压缩的优化框架。通过引入额外的内容潜变量以及合成纹理变量，该方法在图像质量评估指标上表现出更强的性能。 |
| [^203] | [SensorSCAN: Self-Supervised Learning and Deep Clustering for Fault Diagnosis in Chemical Processes.](http://arxiv.org/abs/2208.08879) | "SensorSCAN"是一种自我监督学习和深度聚类的方法，用于在化工过程中进行故障诊断。使用这种方法，在无需专家注释的情况下，可以有效检测大多数过程故障，并且通过在少量标记数据上微调，几乎达到了最优模型的性能水平。 |
| [^204] | [Private Graph Extraction via Feature Explanations.](http://arxiv.org/abs/2206.14724) | 本论文研究了隐私与可解释性在图机器学习中的相互作用，提出了几种图重构攻击方法，并发现后续特征解释对攻击的成功率有显著影响。对于图神经网络的不同解释方法，基于梯度的解释揭示了最多的图结构信息，但效用并不总是最高。 |
| [^205] | [Hypothesis Testing for Unknown Dynamical Systems and System Anomaly Detection via Autoencoders.](http://arxiv.org/abs/2201.12358) | 本文研究了未知动态系统的假设检验问题，并将异常检测定义为通过备选假设来进行假设检验。通过假设检验和图模型的分析，提出了一种新的神经网络设计，DyAD，该设计可以用于改进系统异常检测的性能。 |
| [^206] | [Combining optimal path search with task-dependent learning in a neural network.](http://arxiv.org/abs/2201.11104) | 这篇论文提出了一种在神经网络中结合最优路径搜索和任务相关学习的方法，通过将成本值转化为神经网络的权重来实现在线权重适应。实验结果表明，该方法与经典算法Bellman-Ford具有相同的解，并且网络学习机制可以进一步增强算法的性能。 |
| [^207] | [Long Story Short: Omitted Variable Bias in Causal Machine Learning.](http://arxiv.org/abs/2112.13398) | 在因果机器学习中，我们通过推导出遗漏变量偏差的尖锐上界，为广泛的线性泛函因果参数提供了一种简单而通用的方法。这种方法可以应用于许多传统的因果推断研究目标，并且仅取决于潜变量在结果和参数的Riesz表示器中所导致的额外变异。 |
| [^208] | [Bounding Wasserstein distance with couplings.](http://arxiv.org/abs/2112.03152) | 该论文提出了基于马尔可夫链耦合的估计器，用于评估渐近有偏采样方法的质量，并给出了渐近有偏采样方法的极限分布与原始目标分布之间的Wasserstein距离的经验上界。 |
| [^209] | [Releasing Graph Neural Networks with Differential Privacy Guarantees.](http://arxiv.org/abs/2109.08907) | 这篇论文提出了PrivGNN，一种隐私保护的框架，用于在集中式环境中发布具有差分隐私保证的图神经网络。该方法结合了知识蒸馏框架和两个噪声机制，通过在公共数据上训练和私有数据的知识，实现了严格的隐私保证。实验结果表明，该方法在性能上表现出色。 |
| [^210] | [Exclusive Group Lasso for Structured Variable Selection.](http://arxiv.org/abs/2108.10284) | 本文提出了一种基于原子范数的独占群组套索方法，用于解决结构变量选择问题。该方法通过适当设计的复合范数促进独占群组稀疏模式，并使用高效灵活的优化算法进行支持恢复。通过逐步将结构原子包含到估计的支持中构建解，并在一定假设下证明了解决方案的有效性。 |
| [^211] | [Exploration noise for learning linear-quadratic mean field games.](http://arxiv.org/abs/2107.00839) | 本文证明了常见噪声可以作为学习均场博弈解的探索噪声，并且通过实例表明这种噪声能够恢复解的存在性和唯一性，并且强制学习算法收敛，无需额外的结构。 |
| [^212] | [Entropy-based Discovery of Summary Causal Graphs in Time Series.](http://arxiv.org/abs/2105.10381) | 该研究提出了一种基于熵的方法，在时间序列中学习摘要因果图，并通过PC-like和FCI-like算法展示了其有效性和高效性。 |
| [^213] | [Deep learning-based Edge-aware pre and post-processing methods for JPEG compressed images.](http://arxiv.org/abs/2104.04926) | 本论文提出了一种基于深度学习的适用于JPEG压缩图像的边缘感知预处理和后处理方法，通过引入边缘感知损失函数和超分辨率CNN，在低比特率下实现了显著的PSNR提升。 |
| [^214] | [Model-free Policy Learning with Reward Gradients.](http://arxiv.org/abs/2103.05147) | 本研究开发了一种新颖的方法，无需学习模型即可集成奖赏梯度，提高了策略学习的样本效率。 |
| [^215] | [Computable Phenotypes of Patient Acuity in the Intensive Care Unit.](http://arxiv.org/abs/2005.05163) | 本研究开发了一种利用电子健康记录自动获取病情变量的计算表型，并描述了重症监护室病人的病情转换。通过连续的病情状态和聚类方法，提供了对ICU病人临床进展的展示。 |
| [^216] | [LocoGAN -- Locally Convolutional GAN.](http://arxiv.org/abs/2002.07897) | LocoGAN是一个完全卷积的生成对抗网络模型，使用了局部学习和位置通道技术，能够生成任意维度的图像，包括周期性或无限长的图像。 |

# 详细

[^1]: CADSim: 鲁棒且可扩展的野外可控传感器模拟的三维重建

    CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation. (arXiv:2311.01447v1 [cs.CV])

    [http://arxiv.org/abs/2311.01447](http://arxiv.org/abs/2311.01447)

    CADSim是一个能够自动重建车辆几何形状的方法，通过野外传感器数据和CAD模型的组合，克服了稀疏性和噪声问题。

    

    真实的模拟是实现自动驾驶车辆安全和可扩展开发的关键。核心组件是模拟传感器，以便整个自动驾驶系统能在模拟中进行测试。传感器模拟涉及对交通参与者（如车辆）进行建模，具有高质量的外观和可调整几何形状，并实时进行渲染。自动驾驶行业通常需要艺术家来构建这些元素。然而，这种方法成本高、速度慢，并且可能无法反映真实情况。相反，通过从野外收集的传感器数据自动重建元素，可以更好地生成多样化且覆盖真实世界的大量元素。然而，当前的重建方法在野外传感器数据上存在困难，主要原因是其稀疏性和噪声。为了解决这些问题，我们提出了CADSim，它通过一小组不同可微渲染的CAD模型，结合部件感知的目标类别先验知识，自动重建车辆的几何形状。

    Realistic simulation is key to enabling safe and scalable development of % self-driving vehicles. A core component is simulating the sensors so that the entire autonomy system can be tested in simulation. Sensor simulation involves modeling traffic participants, such as vehicles, with high quality appearance and articulated geometry, and rendering them in real time. The self-driving industry has typically employed artists to build these assets. However, this is expensive, slow, and may not reflect reality. Instead, reconstructing assets automatically from sensor data collected in the wild would provide a better path to generating a diverse and large set with good real-world coverage. Nevertheless, current reconstruction approaches struggle on in-the-wild sensor data, due to its sparsity and noise. To tackle these issues, we present CADSim, which combines part-aware object-class priors via a small set of CAD models with differentiable rendering to automatically reconstruct vehicle geome
    
[^2]: Adv3D：通过闭环仿真生成安全关键的3D对象

    Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop Simulation. (arXiv:2311.01446v1 [cs.RO])

    [http://arxiv.org/abs/2311.01446](http://arxiv.org/abs/2311.01446)

    论文提出了Adv3D框架，通过闭环传感器仿真评估自主性能，并通过优化车辆形状使得场景更具挑战，导致自主失败和SDV操纵不舒服。

    

    自动驾驶车辆（SDVs）必须在各种场景下进行严格测试以确保安全部署。该行业通常依靠闭环仿真来评估SDV在一系列合成和真实场景中的交互情况，并验证其是否正常运行。然而，他们主要只测试系统的运动规划模块，并且只考虑行为变化。在闭环中评估完整的自主系统，并理解基于场景外观的传感器数据（如演员的形状）的变化如何影响系统性能是关键。在本文中，我们提出了一个名为Adv3D的框架，该框架采用真实世界的场景，并进行闭环传感器仿真来评估自主性能，并找到使场景更具挑战性、导致自主失败和不舒服的SDV操纵的车辆形状。与之前仅在车顶或路边添加假对抗形状以损害感知的作品不同，我们优化了一个低-

    Self-driving vehicles (SDVs) must be rigorously tested on a wide range of scenarios to ensure safe deployment. The industry typically relies on closed-loop simulation to evaluate how the SDV interacts on a corpus of synthetic and real scenarios and verify it performs properly. However, they primarily only test the system's motion planning module, and only consider behavior variations. It is key to evaluate the full autonomy system in closed-loop, and to understand how variations in sensor data based on scene appearance, such as the shape of actors, affect system performance. In this paper, we propose a framework, Adv3D, that takes real world scenarios and performs closed-loop sensor simulation to evaluate autonomy performance, and finds vehicle shapes that make the scenario more challenging, resulting in autonomy failures and uncomfortable SDV maneuvers. Unlike prior works that add contrived adversarial shapes to vehicle roof-tops or roadside to harm perception only, we optimize a low-
    
[^3]: 深度双下降用于时间序列预测：避免模型欠拟合

    Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models. (arXiv:2311.01442v1 [cs.LG])

    [http://arxiv.org/abs/2311.01442](http://arxiv.org/abs/2311.01442)

    本文探讨了深度学习模型在时间序列预测中的训练模式，发现了深度双下降现象，并提出了通过增加训练轮次消除过拟合的方法，取得了在大多数基准测试上的最先进结果。

    

    深度学习模型，特别是Transformer，在包括时间序列预测在内的各个领域取得了令人印象深刻的结果。尽管现有的时间序列文献主要关注模型架构修改和数据增强技术，但本文探讨了深度学习模型在时间序列预测中的训练模式，即如何训练模型而不考虑其架构。我们进行了广泛的实验，研究了在公共时间序列数据集上训练的几个Transformer模型中的深度双下降现象。我们展示了epoch-wise的深度双下降以及通过增加训练轮次可以消除过拟合的方法。利用这些发现，我们在72个基准测试中，在长序列时间序列预测方面实现了近70%的最先进结果。这意味着许多文献中的模型可能具有未被发掘的潜力。此外，我们引入了一个分类训练模式修改的分类法，包括数据增强等方法。

    Deep learning models, particularly Transformers, have achieved impressive results in various domains, including time series forecasting. While existing time series literature primarily focuses on model architecture modifications and data augmentation techniques, this paper explores the training schema of deep learning models for time series; how models are trained regardless of their architecture. We perform extensive experiments to investigate the occurrence of deep double descent in several Transformer models trained on public time series data sets. We demonstrate epoch-wise deep double descent and that overfitting can be reverted using more epochs. Leveraging these findings, we achieve state-of-the-art results for long sequence time series forecasting in nearly 70% of the 72 benchmarks tested. This suggests that many models in the literature may possess untapped potential. Additionally, we introduce a taxonomy for classifying training schema modifications, covering data augmentation
    
[^4]: 从视觉-语言基础模型中提取对抗性鲁棒性的框架

    Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])

    [http://arxiv.org/abs/2311.01441](http://arxiv.org/abs/2311.01441)

    本文提出了一个概念简单且轻量级的框架，通过结合知识蒸馏和数据增强的方法来提高视觉模型的鲁棒性，并从预训练的基础模型中获得鲁棒教师模型的知识。借助离散对抗蒸馏方法，我们生成更有信息量的对抗样本，取得了在对抗性样本上鲁棒性显著提升的结果。此外，我们提供了理论框架来支持在知识蒸馏和数据增强设置中使用鲁棒教师模型，并展示了在不同学生模型上的显著性能提升。我们的方法在计算负载方面的开销较小，并可以与其他数据增强方法轻松结合。

    

    我们提出了一个概念简单且轻量级的框架，通过知识蒸馏和数据增强的结合来提高视觉模型的鲁棒性。我们通过从预训练的基础模型中进行蒸馏，展示了在对抗性样本上获得的鲁棒性增益，以此反驳了更大的模型不一定会成为更好的教师的猜想。我们还提出了离散对抗蒸馏（Discrete Adversarial Distillation，DAD）方法，利用鲁棒的教师模型生成对抗样本，并通过VQGAN将其离散化，从而创造出比标准数据增强技术更有信息量的样本。我们提供了一个理论框架，用于在知识蒸馏和数据增强的设置中使用鲁棒的教师模型，并在不同的学生模型中展示了在对抗性样本上的鲁棒性和干净准确性的显著提升。值得注意的是，与类似技术相比，我们的方法增加了少量的计算负载，并且可以轻松与其他数据增强方法相结合。

    We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmen
    
[^5]: 对比矩：多项式时间无监督半空间学习

    Contrastive Moments: Unsupervised Halfspace Learning in Polynomial Time. (arXiv:2311.01435v1 [cs.LG])

    [http://arxiv.org/abs/2311.01435](http://arxiv.org/abs/2311.01435)

    这篇论文提出了一个多项式时间算法，在未知的情况下，通过使用对比矩来学习具有边缘的高维半空间，而不需要标签，并在这个分布假设下建立了隐藏半空间的唯一性和高效性。

    

    当环境分布是未知关于d维空间d倍对称、对数凹的分布的其中一组分布，半空间是通过删除至少一个分量分布中的ε比例的数据引入的情况下，我们提供了一个多项式时间算法来学习具有边缘的高维半空间，目标是在所需的TV距离内。值得注意的是，我们的算法不需要标签，并且在这个分布假设下建立了隐藏半空间的唯一性（和高效性）。算法的样本和时间复杂性在维度和1/ε上都是多项式。该算法只使用经验分布的适当重新加权的前两个矩，我们将其称为对比矩；其分析使用了关于广义Dirichlet多项式的经典事实，并且关键依赖于对对数凹截断的矩比的一个新单调性质。

    We give a polynomial-time algorithm for learning high-dimensional halfspaces with margins in $d$-dimensional space to within desired TV distance when the ambient distribution is an unknown affine transformation of the $d$-fold product of an (unknown) symmetric one-dimensional logconcave distribution, and the halfspace is introduced by deleting at least an $\epsilon$ fraction of the data in one of the component distributions. Notably, our algorithm does not need labels and establishes the unique (and efficient) identifiability of the hidden halfspace under this distributional assumption. The sample and time complexity of the algorithm are polynomial in the dimension and $1/\epsilon$. The algorithm uses only the first two moments of suitable re-weightings of the empirical distribution, which we call contrastive moments; its analysis uses classical facts about generalized Dirichlet polynomials and relies crucially on a new monotonicity property of the moment ratio of truncations of logcon
    
[^6]: 通过核扭曲函数定制Mixup数据

    Tailoring Mixup to Data using Kernel Warping functions. (arXiv:2311.01434v1 [cs.LG])

    [http://arxiv.org/abs/2311.01434](http://arxiv.org/abs/2311.01434)

    本研究提出了一种利用核扭曲函数对Mixup数据进行个性化处理的方法，通过动态改变插值系数的概率分布来实现更频繁和更强烈的混合相似数据点。实验证明这种方法不仅提高了模型性能，还提高了模型的校准性。

    

    数据增强是学习高效深度学习模型的重要基础。在所有提出的增强技术中，线性插值训练数据点（也称为Mixup）已被证明在许多应用中非常有效。然而，大多数研究都集中在选择合适的点进行混合，或者应用复杂的非线性插值，而我们则对更相似的点进行更频繁和更强烈的混合感兴趣。为此，我们提出了通过扭曲函数动态改变插值系数的概率分布的方法，取决于要组合的数据点之间的相似性。我们定义了一个高效而灵活的框架来实现这一点，以避免多样性的损失。我们进行了广泛的分类和回归任务实验，结果显示我们提出的方法既提高了模型的性能，又提高了模型的校准性。代码可在https://github.com/ENSTA-U2IS/torch-uncertainty上找到。

    Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty
    
[^7]: 使用机器学习方法鉴别阿尔茨海默病痴呆水平

    Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods. (arXiv:2311.01428v1 [cs.LG])

    [http://arxiv.org/abs/2311.01428](http://arxiv.org/abs/2311.01428)

    本研究提出了一种使用RF、SVM和CNN算法结合MRI图像分水岭分割进行特征提取的方法来鉴别四个阿尔茨海默病痴呆阶段的分类，其中SVM结合分水岭特征的准确率达到了96.25%。

    

    痴呆是一种常见的神经退行性疾病，是阿尔茨海默病的主要表现。随着病情从轻度到严重的进展，显著影响患者独立完成日常任务的能力，迫切需要及时准确的阿尔茨海默病分类。机器学习或深度学习模型已成为达到此目的的有效工具。在本研究中，我们提出了一种通过RF、SVM和CNN算法结合MRI图像的分水岭分割进行特征提取的方法来对四个痴呆阶段进行分类。我们的结果表明，SVM结合分水岭特征的准确率达到了96.25%，超过了其他分类方法。我们使用ADNI数据集评估了我们方法的有效性，并观察到分水岭分割的引入提升了模型的性能。

    Dementia, a prevalent neurodegenerative condition, is a major manifestation of Alzheimer's disease (AD). As the condition progresses from mild to severe, it significantly impairs the individual's ability to perform daily tasks independently, necessitating the need for timely and accurate AD classification. Machine learning or deep learning models have emerged as effective tools for this purpose. In this study, we suggested an approach for classifying the four stages of dementia using RF, SVM, and CNN algorithms, augmented with watershed segmentation for feature extraction from MRI images. Our results reveal that SVM with watershed features achieves an impressive accuracy of 96.25%, surpassing other classification methods. The ADNI dataset is utilized to evaluate the effectiveness of our method, and we observed that the inclusion of watershed segmentation contributes to the enhanced performance of the models.
    
[^8]: 探索深度学习技术用于青光眼检测：一项综合评估

    Exploring Deep Learning Techniques for Glaucoma Detection: A Comprehensive Review. (arXiv:2311.01425v1 [eess.IV])

    [http://arxiv.org/abs/2311.01425](http://arxiv.org/abs/2311.01425)

    本文综述了使用深度学习方法在青光眼分割、分类和检测方面的尖端技术。通过分析近期研究，发现深度学习算法在自动化青光眼检测中具有显著的潜力，并为进一步研究提供了潜在领域。

    

    青光眼是全球视力丧失的主要原因之一，需要准确高效的检测方法。传统的手动检测方法在成本、时间和主观性方面存在局限性。近期深度学习方法的发展展示了从视网膜底层图像中检测相关特征以自动化青光眼检测的潜力。本文提供了使用于青光眼分割、分类和检测的尖端深度学习方法的综合概述。通过分析近期研究，评估了这些技术的有效性和局限性，突出了关键发现，并确定了进一步研究的潜在领域。深度学习算法的使用可能显著提高青光眼检测的功效、有用性和准确性。这项研究的发现对于自动化青光眼检测的持续推进具有贡献，并对提高患者的治疗和管理有着重要意义。

    Glaucoma is one of the primary causes of vision loss around the world, necessitating accurate and efficient detection methods. Traditional manual detection approaches have limitations in terms of cost, time, and subjectivity. Recent developments in deep learning approaches demonstrate potential in automating glaucoma detection by detecting relevant features from retinal fundus images. This article provides a comprehensive overview of cutting-edge deep learning methods used for the segmentation, classification, and detection of glaucoma. By analyzing recent studies, the effectiveness and limitations of these techniques are evaluated, key findings are highlighted, and potential areas for further research are identified. The use of deep learning algorithms may significantly improve the efficacy, usefulness, and accuracy of glaucoma detection. The findings from this research contribute to the ongoing advancements in automated glaucoma detection and have implications for improving patient o
    
[^9]: 全面迁移：向具有部分目标数据的非中断微调迈进

    Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data. (arXiv:2311.01420v1 [cs.LG])

    [http://arxiv.org/abs/2311.01420](http://arxiv.org/abs/2311.01420)

    这篇论文提出了一个学习问题，即如何将预先训练的源模型适应到目标领域，使用仅覆盖部分标签空间的目标数据进行分类。论文通过构建基准数据集，并进行大量实验证明，揭示了这个问题的挑战。为了解决这一问题，论文提出了两个关键方向：分离领域梯度和分类梯度，并保持类别关系。

    

    我们提出了一个学习问题，涉及将预先训练的源模型适应到目标领域，以对源数据中出现的所有类进行分类，使用仅覆盖部分标签空间的目标数据。这个问题是实际的，因为目标终端用户在适应之前收集所有类别的数据是不现实的。然而，在现有文献中，对此问题的关注有限。为了揭示这个问题，我们构建了基准数据集，并进行了大量实验证明其固有的挑战。我们发现一个困境 - 一方面，适应到新的目标领域对于获得更好的性能至关重要；另一方面，我们观察到，在目标适应数据中缺失的类别保持分类准确性是非常具有挑战性的，更不用说对其进行改进了。为了解决这个问题，我们确定了两个关键方向：1）将领域梯度与分类梯度分离，并2）保持类别关系。我们提出了几个有效的方法

    We propose a learning problem involving adapting a pre-trained source model to the target domain for classifying all classes that appeared in the source data, using target data that covers only a partial label space. This problem is practical, as it is unrealistic for the target end-users to collect data for all classes prior to adaptation. However, it has received limited attention in the literature. To shed light on this issue, we construct benchmark datasets and conduct extensive experiments to uncover the inherent challenges. We found a dilemma -- on the one hand, adapting to the new target domain is important to claim better performance; on the other hand, we observe that preserving the classification accuracy of classes missing in the target adaptation data is highly challenging, let alone improving them. To tackle this, we identify two key directions: 1) disentangling domain gradients from classification gradients, and 2) preserving class relationships. We present several effect
    
[^10]: Castor: 因果时序区域结构学习

    Castor: Causal Temporal Regime Structure Learning. (arXiv:2311.01412v1 [cs.LG])

    [http://arxiv.org/abs/2311.01412](http://arxiv.org/abs/2311.01412)

    “Castor”是一个用于学习多元时间序列数据中因果关系的框架，能够综合学习各个区域的因果图。它通过最大化得分函数来推断区域的数量，并学习每个区域中的线性或非线性因果关系。

    

    揭示多元时间序列数据之间的因果关系是一个重要且具有挑战性的目标，涉及到从气候科学到医疗保健等各个学科的广泛范围。这些数据包含线性或非线性关系，并且通常遵循多个先验未知的区域。现有的因果发现方法可以从具有已知区域的异构数据中推断出摘要因果图，但在全面学习区域和相应的因果图方面存在不足。在本文中，我们介绍了CASTOR，这是一个新颖的框架，旨在学习由不同因果图统治的各种异构时间序列数据中的因果关系。通过EM算法通过最大化一个得分函数，CASTOR推断出区域的数量并学习每个区域中的线性或非线性因果关系。我们展示了CASTOR的稳健收敛性质，特别突出了其有效性。

    The task of uncovering causal relationships among multivariate time series data stands as an essential and challenging objective that cuts across a broad array of disciplines ranging from climate science to healthcare. Such data entails linear or non-linear relationships, and usually follow multiple a priori unknown regimes. Existing causal discovery methods can infer summary causal graphs from heterogeneous data with known regimes, but they fall short in comprehensively learning both regimes and the corresponding causal graph. In this paper, we introduce CASTOR, a novel framework designed to learn causal relationships in heterogeneous time series data composed of various regimes, each governed by a distinct causal graph. Through the maximization of a score function via the EM algorithm, CASTOR infers the number of regimes and learns linear or non-linear causal relationships in each regime. We demonstrate the robust convergence properties of CASTOR, specifically highlighting its profic
    
[^11]: 随机性的福音：SDE在一般扩散图像编辑中超过ODE

    The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing. (arXiv:2311.01410v1 [cs.CV])

    [http://arxiv.org/abs/2311.01410](http://arxiv.org/abs/2311.01410)

    本论文提出了一种概率形式的扩散图像编辑方法，说明了SDE在图像编辑中的优势，并提供了SDE和ODE在各种任务中的对应关系。此外，还提出了一种基于SDE的点在图像上拖动的方法，并建立了一个具有开放集的挑战性基准。

    

    我们提出了一种统一的概率形式的扩散图像编辑方法，在这种方法中，一个隐变量以特定任务的方式进行编辑，并且通常与原始随机微分方程（SDE）或常微分方程（ODE）引发的边缘分布有所不同。相反，它为编辑定义了相应的SDE或ODE。在这种方法中，我们证明了两个SDE的边缘分布之间的Kullback-Leibler散度逐渐减小，而对于ODE来说，随着时间趋近于零，散度保持不变，这显示了SDE在图像编辑中的优势。受此启发，我们提供了各种任务中常用的ODE基线的SDE对应物，包括修复和图像到图像的转换，在这些任务中，SDE显示了一致且显著的改进。此外，我们还提出了SDE-Drag - 一种简单而有效的基于SDE公式的点在图像上拖动的方法。我们还建立了一个具有开放集的挑战性基准（称为DragBench） 。

    We present a unified probabilistic formulation for diffusion-based image editing, where a latent variable is edited in a task-specific manner and generally deviates from the corresponding marginal distribution induced by the original stochastic or ordinary differential equation (SDE or ODE). Instead, it defines a corresponding SDE or ODE for editing. In the formulation, we prove that the Kullback-Leibler divergence between the marginal distributions of the two SDEs gradually decreases while that for the ODEs remains as the time approaches zero, which shows the promise of SDE in image editing. Inspired by it, we provide the SDE counterparts for widely used ODE baselines in various tasks including inpainting and image-to-image translation, where SDE shows a consistent and substantial improvement. Moreover, we propose SDE-Drag -- a simple yet effective method built upon the SDE formulation for point-based content dragging. We build a challenging benchmark (termed DragBench) with open-set 
    
[^12]: 基于核心集的、温和变分后验的精确和可扩展随机高斯过程推理方法

    A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference. (arXiv:2311.01409v1 [cs.LG])

    [http://arxiv.org/abs/2311.01409](http://arxiv.org/abs/2311.01409)

    这篇论文提出了一种基于核心集的、温和变分后验的高斯过程推理方法，通过利用稀疏的、可解释的数据表示来降低参数大小，并且具有数值稳定性和较低的时间和空间复杂度。

    

    我们提出了一种新颖的随机变分高斯过程($\mathcal{GP}$)推理方法，该方法基于可学习的权重伪输入输出点的后验（核心集）。与自由形式的变分族不同，提出的基于核心集的、温和变分的$\mathcal{GP}$（CVTGP）是基于$\mathcal{GP}$先验和数据似然函数来定义的，因此适应了建模的归纳偏差。我们通过对提出的后验进行潜在的$\mathcal{GP}$核心集变量的边缘化，推导出CVTGP的对数边际似然下界，并且证明其适用于随机优化。CVTGP通过利用基于核心集的温和后验来减小可学习参数的大小到$\mathcal{O}(M)$，具有数值稳定性，并且通过提供稀疏且可解释的数据表示来保持$\mathcal{O}(M^3)$时间复杂度和$\mathcal{O}(M^2)$空间复杂度。在模拟和真实回归问题上的实验结果显示了CVTGP的性能优势。

    We present a novel stochastic variational Gaussian process ($\mathcal{GP}$) inference method, based on a posterior over a learnable set of weighted pseudo input-output points (coresets). Instead of a free-form variational family, the proposed coreset-based, variational tempered family for $\mathcal{GP}$s (CVTGP) is defined in terms of the $\mathcal{GP}$ prior and the data-likelihood; hence, accommodating the modeling inductive biases. We derive CVTGP's lower bound for the log-marginal likelihood via marginalization of the proposed posterior over latent $\mathcal{GP}$ coreset variables, and show it is amenable to stochastic optimization. CVTGP reduces the learnable parameter size to $\mathcal{O}(M)$, enjoys numerical stability, and maintains $\mathcal{O}(M^3)$ time- and $\mathcal{O}(M^2)$ space-complexity, by leveraging a coreset-based tempered posterior that, in turn, provides sparse and explainable representations of the data. Results on simulated and real-world regression problems wi
    
[^13]: 使用综合图注意力网络和强化学习分析以太坊网络中的信息传播，以优化网络效率和可扩展性

    Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability. (arXiv:2311.01406v1 [cs.LG])

    [http://arxiv.org/abs/2311.01406](http://arxiv.org/abs/2311.01406)

    本研究提出了一种使用图卷积网络和强化学习模型的创新方法，分析以太坊网络中的信息传播模式，并优化网络效率和可扩展性。最终目标是学习在不同网络状态下采取的最佳行动，提高网络效率和可扩展性。

    

    区块链技术在分布式网络中革新了信息传播的方式。以太坊在推动智能合约和分布式应用方面起到了关键作用。了解以太坊中的信息传播动态对于确保网络效率、安全性和可扩展性非常重要。在本研究中，我们提出了一种创新的方法，利用图卷积网络（GCNs）分析以太坊网络中的信息传播模式。我们的研究的第一阶段涉及从以太坊区块链中收集数据，包括区块、交易和节点度。我们使用邻接矩阵构建了一个交易图表示，以捕捉节点嵌入；而我们的主要贡献是开发了一个综合图注意力网络（GAT）和强化学习（RL）模型来优化网络效率和可扩展性。它学习在不同网络状态下采取的最佳行动，最终导致网络效率的提升。

    Blockchain technology has revolutionized the way information is propagated in decentralized networks. Ethereum plays a pivotal role in facilitating smart contracts and decentralized applications. Understanding information propagation dynamics in Ethereum is crucial for ensuring network efficiency, security, and scalability. In this study, we propose an innovative approach that utilizes Graph Convolutional Networks (GCNs) to analyze the information propagation patterns in the Ethereum network. The first phase of our research involves data collection from the Ethereum blockchain, consisting of blocks, transactions, and node degrees. We construct a transaction graph representation using adjacency matrices to capture the node embeddings; while our major contribution is to develop a combined Graph Attention Network (GAT) and Reinforcement Learning (RL) model to optimize the network efficiency and scalability. It learns the best actions to take in various network states, ultimately leading t
    
[^14]: 学习通过主动感知运动策略来观察物理性质

    Learning to See Physical Properties with Active Sensing Motor Policies. (arXiv:2311.01405v1 [cs.RO])

    [http://arxiv.org/abs/2311.01405](http://arxiv.org/abs/2311.01405)

    该论文提出了一种通过自我监督标记图像来学习机器人对地形的物理性质进行预测的方法，通过主动感知运动策略，机器人能够增加物理参数估计的准确性。

    

    从彩色图像中推断出地形的物理性质可以帮助制定高效的机器人运动计划。然而，与图像分类不同，人类难以直观地将图像块标记为物理性质。在没有标记数据的情况下，构建一个以观测到的地形为输入并预测物理性质的视觉系统仍然具有挑战性。我们提出了一种通过机器人在实际环境中遍历期间捕获的图像进行自我监督标记的方法，并使用在模拟中训练的物理性质估计器来训练它。为了确保准确标记，我们引入了主动感知运动策略（ASMP），该策略被训练为探索增加物理参数估计准确度的运动行为。例如，四足机器人学会刷脚与地面接触以准确估计摩擦系数。我们展示了使用少量实际环境遍历数据训练的视觉系统能够准确预测。

    Knowledge of terrain's physical properties inferred from color images can aid in making efficient robotic locomotion plans. However, unlike image classification, it is unintuitive for humans to label image patches with physical properties. Without labeled data, building a vision system that takes as input the observed terrain and predicts physical properties remains challenging. We present a method that overcomes this challenge by self-supervised labeling of images captured by robots during real-world traversal with physical property estimators trained in simulation. To ensure accurate labeling, we introduce Active Sensing Motor Policies (ASMP), which are trained to explore locomotion behaviors that increase the accuracy of estimating physical parameters. For instance, the quadruped robot learns to swipe its foot against the ground to estimate the friction coefficient accurately. We show that the visual system trained with a small amount of real-world traversal data accurately predicts
    
[^15]: 使用线性控制神经ODE将归一化流作为最优传输映射的近似

    Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs. (arXiv:2311.01404v1 [math.OC])

    [http://arxiv.org/abs/2311.01404](http://arxiv.org/abs/2311.01404)

    本文以线性控制神经ODE的流动作为归一化流构造最优传输映射的近似。通过离散最优耦合问题和数值方案，实现了对最优传输映射的近似。最终结果有助于构建深度神经网络中的可逆传输映射。

    

    "归一化流"一词与通过深度神经网络构建概率测度之间的可逆传输映射相关。本文考虑将$W_2$-最优传输映射$T$恢复为线性控制神经ODE的流动的问题。我们首先展示了在合适的假设下，对于绝对连续测度$\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$和受控向量场，最优传输映射包含在系统产生的流动的$C^0_c$闭包中。假设原始测度$\mu,\nu$的离散近似$\mu_N,\nu_N$可用，我们使用离散最优耦合$\gamma_N$来定义最优控制问题。通过$\Gamma$-收敛论证，我们证明其解对应于近似最优传输映射$T$的流动。最后，利用Pontryagin最大原理，我们提出了一种迭代数值方案来解决问题。

    The term "Normalizing Flows" is related to the task of constructing invertible transport maps between probability measures by means of deep neural networks. In this paper, we consider the problem of recovering the $W_2$-optimal transport map $T$ between absolutely continuous measures $\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$ as the flow of a linear-control neural ODE. We first show that, under suitable assumptions on $\mu,\nu$ and on the controlled vector fields, the optimal transport map is contained in the $C^0_c$-closure of the flows generated by the system. Assuming that discrete approximations $\mu_N,\nu_N$ of the original measures $\mu,\nu$ are available, we use a discrete optimal coupling $\gamma_N$ to define an optimal control problem. With a $\Gamma$-convergence argument, we prove that its solutions correspond to flows that approximate the optimal transport map $T$. Finally, taking advantage of the Pontryagin Maximum Principle, we propose an iterative numerical scheme for the reso
    
[^16]: 在闭环中学习真实交通代理

    Learning Realistic Traffic Agents in Closed-loop. (arXiv:2311.01394v1 [cs.RO])

    [http://arxiv.org/abs/2311.01394](http://arxiv.org/abs/2311.01394)

    本文提出了一种在闭环中学习真实交通代理的方法，利用模仿学习和强化学习相结合的方式，通过匹配专家演示的方式，实现了更加真实和合规的驾驶行为。

    

    在真实的交通模拟中学习交通代理对于在实际部署前以安全和可扩展的方式开发自动驾驶软件至关重要。通常，模仿学习(Imitation Learning, IL)用于直接从离线收集的真实观测中学习类似人类的交通代理，但在没有明确交通规则的情况下，仅通过IL训练的代理经常表现出碰撞和偏离道路等不真实的违规行为。这个问题在分布不同和长尾场景下被放大。另一方面，强化学习(Reinforcement Learning, RL)可以训练交通代理避免违规行为，但仅使用RL会导致不像人类驾驶行为的结果。我们提出了强化交通规则(Reinforcing Traffic Rules, RTR)，这是一个综合的闭环学习目标，以在交通合规约束下匹配专家演示，自然地产生了一种联合IL+RL方法，兼具二者的优点。我们的方法在真实场景和闭环模拟中学习，

    Realistic traffic simulation is crucial for developing self-driving software in a safe and scalable manner prior to real-world deployment. Typically, imitation learning (IL) is used to learn human-like traffic agents directly from real-world observations collected offline, but without explicit specification of traffic rules, agents trained from IL alone frequently display unrealistic infractions like collisions and driving off the road. This problem is exacerbated in out-of-distribution and long-tail scenarios. On the other hand, reinforcement learning (RL) can train traffic agents to avoid infractions, but using RL alone results in unhuman-like driving behaviors. We propose Reinforcing Traffic Rules (RTR), a holistic closed-loop learning objective to match expert demonstrations under a traffic compliance constraint, which naturally gives rise to a joint IL + RL approach, obtaining the best of both worlds. Our method learns in closed-loop simulations of both nominal scenarios from real
    
[^17]: 对比模仿的时间序列生成

    Time-series Generation by Contrastive Imitation. (arXiv:2311.01388v1 [stat.ML])

    [http://arxiv.org/abs/2311.01388](http://arxiv.org/abs/2311.01388)

    本研究探讨了一种生成框架，旨在将自回归模型的显式转换分布和基于对抗训练的隐式转换相结合，通过对比估计训练全局能量模型，并优化本地转换策略来解决时间序列生成中的挑战。

    

    考虑学习时间序列数据的生成模型。序列设置提出了一个独特的挑战：生成器不仅应该捕捉（逐步）转换的条件动力学，而且其开环回滚应该保持（多步）轨迹的联合分布。一方面，MLE训练的自回归模型允许学习和计算显式的转换分布，但在回滚过程中会受到复合误差的影响。另一方面，基于GAN训练的对抗模型减轻了这种暴露偏差，但转换是隐式的且难以评估。在这项工作中，我们研究了一个旨在结合两者优势的生成框架：受到匹配矩法的目标激发，我们优化一个本地（但前瞻性的）转换策略，其中强化信号由全局（但可逐步分解）能量模型通过对比估计训练提供。在训练中，两个组件被学习生成对抗样本。

    Consider learning a generative model for time-series data. The sequential setting poses a unique challenge: Not only should the generator capture the conditional dynamics of (stepwise) transitions, but its open-loop rollouts should also preserve the joint distribution of (multi-step) trajectories. On one hand, autoregressive models trained by MLE allow learning and computing explicit transition distributions, but suffer from compounding error during rollouts. On the other hand, adversarial models based on GAN training alleviate such exposure bias, but transitions are implicit and hard to assess. In this work, we study a generative framework that seeks to combine the strengths of both: Motivated by a moment-matching objective to mitigate compounding error, we optimize a local (but forward-looking) transition policy, where the reinforcement signal is provided by a global (but stepwise-decomposable) energy model trained by contrastive estimation. At training, the two components are learne
    
[^18]: Vision-Language Foundation Models作为有效的机器人模仿者

    Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v1 [cs.RO])

    [http://arxiv.org/abs/2311.01378](http://arxiv.org/abs/2311.01378)

    该论文介绍了一种利用视觉语言基础模型进行机器人操作的方法，通过在语言条件的操作数据集上进行微调，实现了在低性能平台上的有效模仿控制。

    

    最近在视觉语言基础模型方面的进展显示出它们理解多模态数据和解决复杂的视觉语言任务（包括机器人操作）的能力。我们寻求一种简单的方式来利用现有的视觉语言模型（VLMs）在机器人数据上进行简单微调。为此，我们提出了一个简单而新颖的视觉语言操作框架，名为RoboFlamingo，它建立在开源的VLMs，OpenFlamingo之上。与以前的工作不同，RoboFlamingo利用预训练的VLMs进行单步视觉语言理解，使用显式策略头模拟顺序历史信息，并只在语言条件的操作数据集上进行微调。这种分解为RoboFlamingo提供了在低性能平台上进行开环控制和部署的灵活性。通过在测试基准上大幅超过现有技术水平，我们展示了RoboFlamingo可以成为一种有效的机器人模仿者。

    Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective an
    
[^19]: 通过Gromov-Monge嵌入实现单调生成建模

    Monotone Generative Modeling via a Gromov-Monge Embedding. (arXiv:2311.01375v1 [cs.LG])

    [http://arxiv.org/abs/2311.01375](http://arxiv.org/abs/2311.01375)

    该论文提出了一种使用Gromov-Monge嵌入的深度生成模型，通过识别数据背后的底层结构，并将其映射到低维潜空间中，解决了生成对抗网络（GAN）中对初始条件敏感性和模式崩溃的问题。

    

    生成对抗网络（GAN）是创建新内容的强大工具，但面临着对初始条件的敏感性和模式崩溃等挑战。为了解决这些问题，我们提出了一种利用Gromov-Monge嵌入（GME）的深度生成模型。它帮助识别数据背后的底层测度的低维结构，然后将其映射到保持几何性质的低维潜空间中的一个测度，并将其最优地传输到参考测度。通过GME的保持底层几何性质和生成映射的$c$-周期性单调性来保证它们。后一特性是确保更好的参数初始化鲁棒性和模式崩溃的第一步。数值实验证明了我们的方法在生成高质量图像、避免模式崩溃和对不同起始条件具有鲁棒性方面的有效性。

    Generative Adversarial Networks (GANs) are powerful tools for creating new content, but they face challenges such as sensitivity to starting conditions and mode collapse. To address these issues, we propose a deep generative model that utilizes the Gromov-Monge embedding (GME). It helps identify the low-dimensional structure of the underlying measure of the data and then maps it, while preserving its geometry, into a measure in a low-dimensional latent space, which is then optimally transported to the reference measure. We guarantee the preservation of the underlying geometry by the GME and $c$-cyclical monotonicity of the generative map, where $c$ is an intrinsic embedding cost employed by the GME. The latter property is a first step in guaranteeing better robustness to initialization of parameters and mode collapse. Numerical experiments demonstrate the effectiveness of our approach in generating high-quality images, avoiding mode collapse, and exhibiting robustness to different star
    
[^20]: 利用反射红外光波信号进行呼吸异常检测

    Respiratory Anomaly Detection using Reflected Infrared Light-wave Signals. (arXiv:2311.01367v1 [eess.SP])

    [http://arxiv.org/abs/2311.01367](http://arxiv.org/abs/2311.01367)

    利用反射红外光波信号，开发了一种利用低成本光源和传感器进行非接触呼吸异常检测的方法，实现了96.6%的平均准确率，并能够检测到错误数据。

    

    在这项研究中，我们提出了一种利用机械机器人胸部反射的非相干光波信号进行非接触呼吸异常检测的方法。与现有的雷达和摄像头感应系统相比，这项技术只使用了低成本的普遍光源（如红外发光二极管）和传感器（如光电检测器）。这个光波感应（LWS）系统通过测量机器人胸部反射的光强变化来识别不同的呼吸异常，在0.5米至1.5米范围内。该异常检测模型使用机器学习能够以96.6%的平均准确率分类7种不同类型的呼吸数据。该模型还可以检测到系统收集的不包含呼吸信息的错误数据。开发的系统可以作为智能、非接触和隐蔽的呼吸监测方法，在家庭或医疗机构中使用。

    In this study, we present a non-contact respiratory anomaly detection method using incoherent light-wave signals reflected from the chest of a mechanical robot that can breathe like human beings. In comparison to existing radar and camera-based sensing systems for vitals monitoring, this technology uses only a low-cost ubiquitous light source (e.g., infrared light emitting diode) and sensor (e.g., photodetector). This light-wave sensing (LWS) system recognizes different breathing anomalies from the variations of light intensity reflected from the chest of the robot within a 0.5m-1.5m range. The anomaly detection model demonstrates up to 96.6% average accuracy in classifying 7 different types of breathing data using machine learning. The model can also detect faulty data collected by the system that does not contain breathing information. The developed system can be utilized at home or healthcare facilities as a smart, non-contact and discreet respiration monitoring method.
    
[^21]: 关于随机神经网络的Lipschitz常数

    On the Lipschitz constant of random neural networks. (arXiv:2311.01356v1 [stat.ML])

    [http://arxiv.org/abs/2311.01356](http://arxiv.org/abs/2311.01356)

    本文研究了随机ReLU神经网络的Lipschitz常数，对于浅层神经网络，我们得到了Lipschitz常数的精确刻画，对于足够宽度的深层神经网络，我们给出了上下界，并匹配一个依赖于深度的对数因子。

    

    实证研究广泛证明神经网络对输入的微小对抗性扰动非常敏感。这些所谓的对抗性示例的最坏情况鲁棒性可以通过神经网络的Lipschitz常数来量化。然而，关于这个量的理论结果在文献中仅有少数。在本文中，我们开始研究随机ReLU神经网络的Lipschitz常数，即选择随机权重并采用ReLU激活函数的神经网络。对于浅层神经网络，我们将Lipschitz常数刻画到一个绝对数值常数。此外，我们将我们的分析扩展到足够宽度的深层神经网络，我们证明了Lipschitz常数的上下界。这些界匹配到一个依赖于深度的对数因子上。

    Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. However, only few theoretical results regarding this quantity exist in the literature. In this paper, we initiate the study of the Lipschitz constant of random ReLU neural networks, i.e., neural networks whose weights are chosen at random and which employ the ReLU activation function. For shallow neural networks, we characterize the Lipschitz constant up to an absolute numerical constant. Moreover, we extend our analysis to deep neural networks of sufficiently large width where we prove upper and lower bounds for the Lipschitz constant. These bounds match up to a logarithmic factor that depends on the depth.
    
[^22]: 基于深度学习的显微图像压缩：一项实证研究

    Deep learning based Image Compression for Microscopy Images: An Empirical Study. (arXiv:2311.01352v1 [eess.IV])

    [http://arxiv.org/abs/2311.01352](http://arxiv.org/abs/2311.01352)

    本研究实证分析了经典和基于深度学习的图像压缩方法在深度学习图像处理模型中的影响，并以基于深度学习的无标签预测模型为例展示了图像压缩的有效性，以减小数据尺寸并保留必要信息，从而减轻数据管理基础设施的负担并方便数据共享或云计算。

    

    随着现代显微镜和生物成像技术的快速发展，产生了大量的显微图像数据，这些数据通过网络进行存储、分析甚至共享。数据的规模对当前的数据基础设施提出了巨大挑战。减小数据尺寸的一种常见方法是图像压缩。本研究分析了经典的和基于深度学习的图像压缩方法，并研究了它们对基于深度学习的图像处理模型的影响。基于深度学习的无标签预测模型（即从亮场图像预测荧光图像）被用作比较和分析的示例应用。有效的图像压缩方法可以在不丢失必要信息的情况下显著减小数据尺寸，从而减轻数据管理基础设施的负担，并实现数据共享或云计算下的快速传输。为了以这样的方式压缩图像，多种经典的丢失压缩方法和基于深度学习的无损压缩方法得到了评估。

    With the fast development of modern microscopes and bioimaging techniques, an unprecedentedly large amount of imaging data are being generated, stored, analyzed, and even shared through networks. The size of the data poses great challenges for current data infrastructure. One common way to reduce the data size is by image compression. This present study analyzes classic and deep learning based image compression methods, and their impact on deep learning based image processing models. Deep learning based label-free prediction models (i.e., predicting fluorescent images from bright field images) are used as an example application for comparison and analysis. Effective image compression methods could help reduce the data size significantly without losing necessary information, and therefore reduce the burden on data management infrastructure and permit fast transmission through the network for data sharing or cloud computing. To compress images in such a wanted way, multiple classical los
    
[^23]: 取消保护特征：从胸部X射线嵌入中消除保护特征

    Unreading Race: Purging Protected Features from Chest X-ray Embeddings. (arXiv:2311.01349v1 [cs.LG])

    [http://arxiv.org/abs/2311.01349](http://arxiv.org/abs/2311.01349)

    论文的目的是利用正交化方法消除胸部X射线嵌入中的保护特征影响，并证明其有效性。研究结果表明保护特征对病理预测有显著影响，而应用正交化方法可以消除这些影响。

    

    目的：分析并消除深度学习模型中胸部X射线嵌入的保护特征影响。方法：使用正交化方法消除胸部X射线嵌入中的保护特征（如年龄、性别、种族）的影响，确保特征独立的结果。为了验证该方法的有效性，我们使用三个预训练模型（有监督对比、自监督对比和基线分类器模型）对MIMIC和CheXpert数据集进行了回顾性研究。我们的统计分析涉及通过估计保护特征影响和评估使用两种类型嵌入的能力来预测种族、年龄或性别的原始与正交嵌入的比较。结果：我们的实验揭示了保护特征对病理预测的显着影响。应用正交化方法可以消除这些特征影响。除了消除对病理分类的影响之外，

    Purpose: To analyze and remove protected feature effects in chest radiograph embeddings of deep learning models.  Materials and Methods: An orthogonalization is utilized to remove the influence of protected features (e.g., age, sex, race) in chest radiograph embeddings, ensuring feature-independent results. To validate the efficacy of the approach, we retrospectively study the MIMIC and CheXpert datasets using three pre-trained models, namely a supervised contrastive, a self-supervised contrastive, and a baseline classifier model. Our statistical analysis involves comparing the original versus the orthogonalized embeddings by estimating protected feature influences and evaluating the ability to predict race, age, or sex using the two types of embeddings.  Results: Our experiments reveal a significant influence of protected features on predictions of pathologies. Applying orthogonalization removes these feature effects. Apart from removing any influence on pathology classification, whil
    
[^24]: 用简单的功率分析在32位微控制器上阅读神经网络架构

    Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers. (arXiv:2311.01344v1 [cs.CR])

    [http://arxiv.org/abs/2311.01344](http://arxiv.org/abs/2311.01344)

    本文研究了如何通过简单的功率分析方法，在32位微控制器上提取深度神经网络模型的架构信息。

    

    模型提取是AI系统安全的一个不断增长的关注点。对于深度神经网络模型来说，架构是对手试图恢复的最重要的信息。作为一系列重复计算块，部署在边缘设备上的神经网络模型将产生独特的侧信道泄露。当目标平台在物理上可访问时，可以利用这些信道泄露来提取关键信息。通过结合对深度学习实践的理论知识和对广泛使用的实现库（ARM CMSIS-NN）的分析，我们的目的是回答这个关键问题：通过简单地检查一个EM侧信道跟踪，我们可以提取多远的架构信息？我们首次提出了一种用于传统MLP和CNN模型的提取方法，这些模型在高端32位微控制器（Cortex-M7）上运行，仅依赖于简单的模式识别分析。尽管存在几个具有挑战性的情况，但我们声称，与参数提取相反，我们可以通过这种方法从简单的功率分析中提取出架构信息。

    Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction
    
[^25]: 通过原始Wasserstein状态占用匹配实现的离线观察模仿

    Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching. (arXiv:2311.01331v1 [cs.LG])

    [http://arxiv.org/abs/2311.01331](http://arxiv.org/abs/2311.01331)

    本论文提出了一种通过最小化原始Wasserstein距离来匹配专家和学习者状态占用的方法，以解决离线学习从观察中模仿任务的问题。

    

    在现实世界的情境中，与环境的任意交互往往是昂贵的，并且专家示范的行为并不总是可用的。为了减少这两者的需求，离线学习从观察（LfO）得到了广泛研究，其中代理通过只有专家状态和与任务无关的非专家状态-动作对来学习解决任务。最先进的分布校正估计（DICE）方法最小化了学习者和专家策略之间的状态占用差异。然而，它们仅限于$f$-divergences（KL和$\chi^2$）或带有Rubinstein对偶的Wasserstein距离，后者限制了对性能关键的基础距离度量的使用。为了解决这个问题，我们提出了原始Wasserstein DICE（PW-DICE），它通过悲观正则化器最小化专家和学习者状态占用之间的原始Wasserstein距离，并利用了对比学习的dis

    In real-world scenarios, arbitrary interactions with the environment can often be costly, and actions of expert demonstrations are not always available. To reduce the need for both, Offline Learning from Observations (LfO) is extensively studied, where the agent learns to solve a task with only expert states and \textit{task-agnostic} non-expert state-action pairs. The state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the state occupancy divergence between the learner and expert policies. However, they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein distance with Rubinstein duality, the latter of which constrains the underlying distance metric crucial to the performance of Wasserstein-based solutions. To address this problem, we propose Primal Wasserstein DICE (PW-DICE), which minimizes the primal Wasserstein distance between the expert and learner state occupancies with a pessimistic regularizer and leverages a contrastively learned dis
    
[^26]: 离线观测和示例中用于模仿的简单解决方案，可能包含不完整的轨迹

    A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories. (arXiv:2311.01329v1 [cs.LG])

    [http://arxiv.org/abs/2311.01329](http://arxiv.org/abs/2311.01329)

    本文提出了一种使用离线观测和示例的简单解决方案，通过使用基于轨迹的加权行为克隆和专家状态鉴别器来稳定地学习，以解决离线模仿中由于不完整轨迹而导致的不稳定问题。

    

    从观测中进行离线模仿旨在解决仅具有任务特定专家状态和任务不可知非专家状态-操作对的马尔可夫决策过程（MDP）。离线模仿在现实世界中的场景中非常有用，其中任意的交互都是昂贵的且专家的操作不可用。目前最先进的“分布矫正估计”（DICE）方法通过最小化专家和学习者策略之间的状态占领差异，并使用加权行为克隆检索到一个策略；然而，当从不完整轨迹学习时，由于双域的非鲁棒优化，它们的结果是不稳定的。为了解决这个问题，在本文中，我们提出了基于观测的轨迹感知模仿学习（TAILO）。TAILO使用未来轨迹上的折现和作为加权行为克隆的权重。这些权重是由一个旨在识别专家状态的鉴别器的输出进行缩放的。尽管简单，TAILO在存在轨迹的情况下表现良好。

    Offline imitation from observations aims to solve MDPs where only task-specific expert states and task-agnostic non-expert state-action pairs are available. Offline imitation is useful in real-world scenarios where arbitrary interactions are costly and expert actions are unavailable. The state-of-the-art "DIstribution Correction Estimation" (DICE) methods minimize divergence of state occupancy between expert and learner policies and retrieve a policy with weighted behavior cloning; however, their results are unstable when learning from incomplete trajectories, due to a non-robust optimization in the dual domain. To address the issue, in this paper, we propose Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a discounted sum along the future trajectory as the weight for weighted behavior cloning. The terms for the sum are scaled by the output of a discriminator, which aims to identify expert states. Despite simplicity, TAILO works well if there exist trajectorie
    
[^27]: 具有背包约束的高维线性赌臂问题研究

    High-dimensional Linear Bandits with Knapsacks. (arXiv:2311.01327v1 [cs.LG])

    [http://arxiv.org/abs/2311.01327](http://arxiv.org/abs/2311.01327)

    本文研究了具有背包约束的高维线性赌臂问题，利用稀疏结构实现改进遗憾。通过开发在线硬阈值算法和原始-对偶框架结合的方法，实现了对特征维度的对数改进的次线性遗憾。

    

    我们研究了在特征维度较大的高维设置下的具有背包约束的上下文赌臂问题。每个手臂拉动的奖励等于稀疏高维权重向量与当前到达的特征的乘积，加上额外的随机噪声。在本文中，我们研究如何利用这种稀疏结构来实现CBwK问题的改进遗憾。为此，我们首先开发了一种在线的硬阈值算法的变体，以在线方式进行稀疏估计。我们进一步将我们的在线估计器与原始-对偶框架结合起来，在每个背包约束上分配一个对偶变量，并利用在线学习算法来更新对偶变量，从而控制背包容量的消耗。我们证明了这种集成方法使我们能够实现对特征维度的对数改进的次线性遗憾，从而改进了多项式相关性。

    We study the contextual bandits with knapsack (CBwK) problem under the high-dimensional setting where the dimension of the feature is large. The reward of pulling each arm equals the multiplication of a sparse high-dimensional weight vector and the feature of the current arrival, with additional random noise. In this paper, we investigate how to exploit this sparsity structure to achieve improved regret for the CBwK problem. To this end, we first develop an online variant of the hard thresholding algorithm that performs the sparse estimation in an online manner. We further combine our online estimator with a primal-dual framework, where we assign a dual variable to each knapsack constraint and utilize an online learning algorithm to update the dual variable, thereby controlling the consumption of the knapsack capacity. We show that this integrated approach allows us to achieve a sublinear regret that depends logarithmically on the feature dimension, thus improving the polynomial depend
    
[^28]: 将传递性攻击系统地、实用地和公正地进行评估

    Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly. (arXiv:2311.01323v1 [cs.LG])

    [http://arxiv.org/abs/2311.01323](http://arxiv.org/abs/2311.01323)

    本文建立了一个传递性攻击基准（TA-Bench），评估了30多种方法在25个热门替代/受害模型上的性能，以系统地、公正地、实用地比较这些方法。

    

    深度神经网络（DNN）的对抗性脆弱性引起了极大关注，因为在现实世界应用这些模型存在安全风险。基于对抗样本的可传递性，已经开发了越来越多的传递性方法来欺骗无法访问其架构和参数的黑盒DNN模型。尽管付出了巨大努力，但仍然缺乏一个标准化的基准，以便系统、公正和实用地比较这些方法。我们的调查显示，某些方法的评估需要更合理、更全面地验证其有效性，避免不公平比较和对可能的替代/受害模型的不充分考虑。因此，我们建立了一个传递性攻击基准（TA-Bench），其中实施了30多种方法。在本文中，我们全面评估和比较了它们在25个热门替代/受害模型上的性能。

    The adversarial vulnerability of deep neural networks (DNNs) has drawn great attention due to the security risk of applying these models in real-world applications. Based on transferability of adversarial examples, an increasing number of transfer-based methods have been developed to fool black-box DNN models whose architecture and parameters are inaccessible. Although tremendous effort has been exerted, there still lacks a standardized benchmark that could be taken advantage of to compare these methods systematically, fairly, and practically. Our investigation shows that the evaluation of some methods needs to be more reasonable and more thorough to verify their effectiveness, to avoid, for example, unfair comparison and insufficient consideration of possible substitute/victim models. Therefore, we establish a transfer-based attack benchmark (TA-Bench) which implements 30+ methods. In this paper, we evaluate and compare them comprehensively on 25 popular substitute/victim models on Im
    
[^29]: 散射视觉变换：光谱混合的重要性

    Scattering Vision Transformer: Spectral Mixing Matters. (arXiv:2311.01310v1 [cs.CV])

    [http://arxiv.org/abs/2311.01310](http://arxiv.org/abs/2311.01310)

    本文提出了一种名为散射视觉变换（SVT）的新方法，通过光谱混合来解决视觉变换中的注意力复杂性和信息捕捉问题。

    

    视觉变换器在各种计算机视觉任务中，包括图像分类、实例分割和目标检测中获得了显著的关注，并取得了最先进的性能。然而，解决注意力复杂性和有效捕捉图像中细粒度信息仍然存在挑战。现有的解决方案通常采用降采样操作（如池化）来减少计算成本。然而，这种操作是不可逆的，可能导致信息丢失。在本文中，我们提出了一种称为散射视觉变换（SVT）的新方法来解决这些挑战。SVT结合了一个光谱散射网络，实现了对复杂图像细节的捕捉。SVT通过分离低频和高频分量，克服了与降采样操作相关的不可逆问题。此外，SVT引入了一个独特的光谱门控网络，利用Einstein乘法来处理令牌和通道。

    Vision transformers have gained significant attention and achieved state-of-the-art performance in various computer vision tasks, including image classification, instance segmentation, and object detection. However, challenges remain in addressing attention complexity and effectively capturing fine-grained information within images. Existing solutions often resort to down-sampling operations, such as pooling, to reduce computational cost. Unfortunately, such operations are non-invertible and can result in information loss. In this paper, we present a novel approach called Scattering Vision Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally scattering network that enables the capture of intricate image details. SVT overcomes the invertibility issue associated with down-sampling operations by separating low-frequency and high-frequency components. Furthermore, SVT introduces a unique spectral gating network utilizing Einstein multiplication for token and channel 
    
[^30]: AWEQ：用于大型语言模型的后训练量化和激活权重均衡方法

    AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])

    [http://arxiv.org/abs/2311.01305](http://arxiv.org/abs/2311.01305)

    AWEQ是一种后训练量化和激活权重均衡方法，能够在大型语言模型中实现超低位量化和8-bit权重和激活量化，并通过改进的均衡方法减小量化偏差误差，提高模型的鲁棒性。

    

    大型语言模型(LLMs)在各种任务中表现出色，但其计算和存储成本也相对较高。量化这些模型是缓解这个问题的有效方法。然而，现有方法很难在模型准确性和硬件效率之间取得平衡。因此，我们引入了AWEQ，一种后训练方法，不需要额外的训练开销。AWEQ在超低位量化和8-bit权重和激活(W8A8)量化方面表现出色。观察到权重量化比激活量化更容易。AWEQ通过通道均衡将激活量化的难度转移到权重上，实现了两者量化困难的平衡，从而最大化了性能。我们进一步改进了均衡方法，减小了量化偏差误差，确保模型的鲁棒性。在像LLaMA这样的流行模型上进行了大量实验。

    Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
    
[^31]: TRIALSCOPE：一个统一的因果框架，用于利用生物医学语言模型扩展实际世界证据生成

    TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models. (arXiv:2311.01301v1 [cs.LG])

    [http://arxiv.org/abs/2311.01301](http://arxiv.org/abs/2311.01301)

    TRIALSCOPE是一个统一的框架，利用生物医学语言模型将临床文本进行结构化，采用概率建模进行去噪和插补，并应用因果推断技术来应对混杂因素，以从实际世界数据中提取实证证据和推理临床假设。

    

    实际世界数据的快速数字化为优化医疗服务和加速生物医学发现提供了前所未有的机会。然而，在实践中，这些数据往往以非结构化形式存在，如电子医疗记录中的临床笔记，并且通常受到混杂因素的困扰。本文介绍了TRIALSCOPE，一个用于从人群级观察数据中提取实际世界证据的统一框架。TRIALSCOPE利用生物医学语言模型来扩展规模化的临床文本，采用先进的概率建模进行去噪和插补，并结合最先进的因果推断技术来应对常见的混杂因素。利用临床试验规范作为通用表示形式，TRIALSCOPE提供了一个一键式解决方案，可使用观察数据生成和推理临床假设。在一个包含超过一百万个癌症患者的大规模实际世界数据集上进行了广泛的实验和分析。

    The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million canc
    
[^32]: DP-Mix：基于Mixup的差分隐私学习数据增强

    DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning. (arXiv:2311.01295v1 [cs.LG])

    [http://arxiv.org/abs/2311.01295](http://arxiv.org/abs/2311.01295)

    本文研究了差分隐私学习中数据增强技术的问题，并提出了两种针对差分隐私学习约束的新型数据增强技术，通过对自我增强数据进行mixup和合成数据的使用，实现了优于其他方法的分类性能。

    

    数据增强技术，如简单的图像变换和组合，在改善计算机视觉模型的泛化能力方面非常有效，特别是在训练数据有限的情况下。然而，这些技术与差分隐私学习方法根本不兼容，原因是差分隐私学习方法内置假设每个训练图像对学习模型的贡献是有界的。本文研究了为何对多样本数据增强技术（如mixup）的朴素应用无法取得良好性能，并提出了两种针对差分隐私学习约束的新型数据增强技术。我们的第一种技术，DP-Mix_Self，在自我增强数据上执行mixup，实现了在各种数据集和设置下的SoTA分类性能。我们的第二种技术DP-Mix_Diff进一步通过将从预训练扩散模型中获得的合成数据纳入增强过程，提高了性能。

    Data augmentation techniques, such as simple image transformations and combinations, are highly effective at improving the generalization of computer vision models, especially when training data is limited. However, such techniques are fundamentally incompatible with differentially private learning approaches, due to the latter's built-in assumption that each training image's contribution to the learned model is bounded. In this paper, we investigate why naive applications of multi-sample data augmentation techniques, such as mixup, fail to achieve good performance and propose two novel data augmentation techniques specifically designed for the constraints of differentially private learning. Our first technique, DP-Mix_Self, achieves SoTA classification performance across a range of datasets and settings by performing mixup on self-augmented data. Our second technique, DP-Mix_Diff, further improves performance by incorporating synthetic data from a pre-trained diffusion model into the 
    
[^33]: FlashDecoding++: 在GPU上加速大规模语言模型推理的更快算法

    FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])

    [http://arxiv.org/abs/2311.01282](http://arxiv.org/abs/2311.01282)

    FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。

    

    随着大规模语言模型在各个领域的重要性日益增加，加速语言模型推理仍然存在一些挑战未解决：(1) 同步部分softmax更新。softmax操作需要同步更新每个部分softmax结果，导致LLM中注意力计算的开销增加约20%。(2) 未充分利用扁平GEMM计算。在LLM推理中执行GEMM的矩阵形状是扁平的，导致在先前的设计中填充零后计算未充分利用，性能损失超过50%。(3) 静态数据流导致的性能损失。LLM中的内核性能取决于不同的输入数据特征、硬件配置等。单一和静态的数据流可能导致LLM推理中不同形状的GEMM的性能损失达到50.25%。我们提出了FlashDecoding++，一种快速支持主流LLM和硬件后端的LLM推理引擎。为了解决上述挑战，FlashDecoding++实现了以下目标：

    As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and >50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
    
[^34]: 针对分子图的长距离神经原子学习

    Long-Range Neural Atom Learning for Molecular Graphs. (arXiv:2311.01276v1 [cs.LG])

    [http://arxiv.org/abs/2311.01276](http://arxiv.org/abs/2311.01276)

    这项研究提出了一种针对分子图的长程神经原子学习方法，通过将原子投射为神经原子并在其之间交换信息，实现了远距离节点之间的通信，缩小了任意节点对的相互作用范围。

    

    图神经网络（GNN）已广泛应用于药物发现中的分子图。然而，当前的GNN主要擅长利用短程相互作用（SRI），但难以捕捉长程相互作用（LRI），这两者对于确定分子性质都至关重要。为解决这个问题，我们提出了一种将所有原子隐式投射为少数神经原子的方法，这些神经原子抽象出分子内原子组的集体信息。具体而言，我们明确地在神经原子之间交换信息，并将其作为一种增强将其重新投射到原子的表示上。通过这种机制，神经原子在远距离节点之间建立通信通道，有效地将任意节点对的相互作用范围减少到单次跳跃。为了从物理角度对我们的方法进行审查，我们揭示了它与传统的LRI计算方法Ewald求和的联系。我们进行了大量实验验证

    Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs are mainly good at leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties. To tackle this issue, we propose a method that implicitly projects all original atoms into a few Neural Atoms, which abstracts the collective information of atomic groups within a molecule. Specifically, we explicitly exchange the information among neural atoms and project them back to the atoms' representations as an enhancement. With this mechanism, neural atoms establish the communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. To provide an inspection of our method from a physical perspective, we reveal its connection with the traditional LRI calculation method, Ewald Summation. We conduct extensive experiment
    
[^35]: 基于能源的法律领域文本分类常见方法的比较分析

    An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])

    [http://arxiv.org/abs/2311.01256](http://arxiv.org/abs/2311.01256)

    本研究通过在法律领域的文本分类任务上进行比较分析，综合考虑性能和能源消耗等指标，探讨了大型语言模型与传统方法的优劣，并强调了在性能相近的情况下应重视生产成本、能源消耗和碳足迹等方面的考量。

    

    大部分机器学习研究评估最佳解决方案的性能。然而，在追求最佳性能的竞争中，经常忽视许多重要因素，而事实上，这些因素应该被仔细考虑。实际上，有时不同方法之间的性能差距可以忽略不计，而生产成本、能源消耗和碳足迹等因素必须考虑在内。大型语言模型（LLMs）被广泛应用于学术界和工业界的NLP问题。在这项工作中，我们在LexGLUE基准上对LLM和传统方法（例如SVM）进行了详细的定量比较，同时考虑性能（标准指标）和其他指标，如时间、耗能和成本，总之就是碳足迹。在我们的分析中，我们分别考虑了原型设计阶段（通过训练-验证-测试迭代进行模型选择）和生产阶段。

    Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they fol
    
[^36]: 防止混杂偏差的清洗聚类方法

    Sanitized Clustering against Confounding Bias. (arXiv:2311.01252v1 [cs.LG])

    [http://arxiv.org/abs/2311.01252](http://arxiv.org/abs/2311.01252)

    本文提出了一个名为“Sanitized Clustering Against confounding Bias (SCAB)”的新的聚类框架，用于在复杂数据的语义潜空间中去除混淆因素。

    

    现实世界中的数据集不可避免地包含由数据收集过程中不同来源或条件引起的偏差。因此，这种不一致性本身作为混淆因素干扰了聚类分析。现有方法通过在聚类之前将数据投影到与混淆因素扩展的正交补空间上来消除偏差。在这种方法中，聚类因素和混淆因素在原始特征空间中被粗略地考虑，理想情况下假设数据与混淆因素之间的相关性是线性的，以方便求解。然而，这些方法的适用范围有限，因为在实际应用中的数据通常与混淆因素呈复杂的非线性相关。本文提出了一个名为“Sanitized Clustering Against confounding Bias (SCAB)”的新的聚类框架，通过非线性依赖度在复杂数据的语义潜空间中去除混淆因素。

    Real-world datasets inevitably contain biases that arise from different sources or conditions during data collection. Consequently, such inconsistency itself acts as a confounding factor that disturbs the cluster analysis. Existing methods eliminate the biases by projecting data onto the orthogonal complement of the subspace expanded by the confounding factor before clustering. Therein, the interested clustering factor and the confounding factor are coarsely considered in the raw feature space, where the correlation between the data and the confounding factor is ideally assumed to be linear for convenient solutions. These approaches are thus limited in scope as the data in real applications is usually complex and non-linearly correlated with the confounding factor. This paper presents a new clustering framework named Sanitized Clustering Against confounding Bias (SCAB), which removes the confounding factor in the semantic latent space of complex data through a non-linear dependence mea
    
[^37]: 将其推向展示极限：多模态视觉触觉模仿学习与力匹配

    Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])

    [http://arxiv.org/abs/2311.01248](http://arxiv.org/abs/2311.01248)

    本研究利用视觉触觉传感器和模仿学习相结合，通过配对优化触觉力量曲线和简化传感器应用，对接触丰富的操作任务进行了研究。

    

    光学触觉传感器已经成为机器人操作过程中获取密集接触信息的有效手段。最近引入的“透视你的皮肤”（STS）型传感器具有视觉和触觉模式，通过利用半透明表面和可控照明实现。本文研究了视觉触觉传感与模仿学习在富有接触的操作任务中的好处。首先，我们使用触觉力测量和一种新的算法，在运动示范中产生更好匹配人体示范者的力曲线。其次，我们添加了视觉/触觉STS模式切换作为控制策略输出，简化传感器的应用。最后，我们研究了多种观察配置，比较和对比了视觉/触觉数据（包括模式切换和不切换）与手腕挂载的眼在手摄像机的视觉数据的价值。我们在一个广泛的实验系列上进行实验。

    Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
    
[^38]: 潜在空间中的多个数学运算推导

    Multi-Operational Mathematical Derivations in Latent Space. (arXiv:2311.01230v1 [cs.LG])

    [http://arxiv.org/abs/2311.01230](http://arxiv.org/abs/2311.01230)

    本文研究在潜在空间中逼近多个数学运算进行表达式推导的可能性，并通过构建大规模数据集和使用最先进的神经编码器实例化，探索了不同编码机制在潜在空间中逼近方程推理的能力。

    

    本文研究在潜在空间中逼近多个数学运算进行表达式推导的可能性。为此，我们引入了不同的多操作表示范式，将数学运算建模为显式的几何变换。通过利用符号引擎，我们构建了一个包含61K个前提和6个运算符的大规模数据集，分析了每个范式在与最先进的神经编码器实例化时的性质。具体而言，我们研究了不同的编码机制在潜在空间中如何逼近方程推理，并探讨了学习不同运算符和在单个运算中专门化之间的权衡，以及支持多步推导和超越分布广义化的能力。我们的实证分析表明，多操作范式对于解开不同运算符是至关重要的，同时可以区分结论。

    This paper investigates the possibility of approximating multiple mathematical operations in latent space for expression derivation. To this end, we introduce different multi-operational representation paradigms, modelling mathematical operations as explicit geometric transformations. By leveraging a symbolic engine, we construct a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises and 6 operators, analysing the properties of each paradigm when instantiated with state-of-the-art neural encoders. Specifically, we investigate how different encoding mechanisms can approximate equational reasoning in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation. Our empirical analysis reveals that the multi-operational paradigm is crucial for disentangling different operators, while discriminating the conclusion
    
[^39]: 强化学习的扩散模型: 一份综述

    Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])

    [http://arxiv.org/abs/2311.01223](http://arxiv.org/abs/2311.01223)

    强化学习中的扩散模型已经成为一种突出的生成模型，通过在样本质量和训练稳定性方面的优势改进了强化学习解决方案。该综述提供了这一新兴领域发展的概述，并探讨了扩散模型在强化学习中的分类法和应用。

    

    扩散模型作为一种突出的生成模型类别已经出现，超越了以往方法在样本质量和训练稳定性方面的优势。最近的研究表明，扩散模型在改进强化学习（RL）解决方案方面具有优势，包括作为轨迹规划器、表达能力丰富的策略类别、数据合成器等。本综述旨在提供该新兴领域发展的概述，并希望能启发新的研究方向。首先，我们审查了当前RL算法遇到的一些挑战。然后，我们根据扩散模型在RL中所扮演的角色，提出了现有方法的分类法，并探讨了如何解决现有挑战。我们进一步概述了扩散模型在各种与RL相关任务中的成功应用，并讨论了当前方法的局限性。最后，我们总结了这项综述，并提出了对未来研究方向的见解，重点是提高模型性能和应用扩散模型的方法。

    Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
    
[^40]: 使用位反转攻击图神经网络：Weisfeiler和Lehman变得冷漠了

    Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent. (arXiv:2311.01205v1 [cs.LG])

    [http://arxiv.org/abs/2311.01205](http://arxiv.org/abs/2311.01205)

    我们设计了Injectivity Bit Flip Attack来针对图神经网络，成功地降低了其对图结构的识别能力和表达能力，从而增加了其对位反转攻击的易受攻击性。

    

    先前对图神经网络的攻击主要集中在图毒化和规避上，忽略了网络的权重和偏置。传统的基于权重的故障注入攻击，如用于卷积神经网络的位反转攻击，没有考虑到图神经网络的独特属性。我们提出了注入率位反转攻击（Injectivity Bit Flip Attack），这是第一个专门针对图神经网络设计的位反转攻击。我们的攻击针对量化消息传递神经网络中的可学习邻域聚合函数，降低了其区分图结构的能力，失去了Weisfeiler-Lehman测试的表达能力。我们的发现表明，利用特定于某些图神经网络架构的数学属性可能会显著增加其对位反转攻击的易受攻击性。注入率位反转攻击可以将各种图属性预测数据集上训练的最大表达性同构网络降级为随机输出。

    Prior attacks on graph neural networks have mostly focused on graph poisoning and evasion, neglecting the network's weights and biases. Traditional weight-based fault injection attacks, such as bit flip attacks used for convolutional neural networks, do not consider the unique properties of graph neural networks. We propose the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and losing the expressivity of the Weisfeiler-Lehman test. Our findings suggest that exploiting mathematical properties specific to certain graph neural network architectures can significantly increase their vulnerability to bit flip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive Graph Isomorphism Networks trained on various graph property prediction datasets to rando
    
[^41]: 在边缘感知设备上的联邦学习：一项综述

    Federated Learning on Edge Sensing Devices: A Review. (arXiv:2311.01201v1 [cs.LG])

    [http://arxiv.org/abs/2311.01201](http://arxiv.org/abs/2311.01201)

    本文综述了在边缘感知设备上的联邦学习的研究现状。边缘感知设备的快速增长使得监测环境特征并获取环境信息成为可能，但云端或服务器上的分析面临隐私、硬件和连接性等挑战。联邦学习作为一种解决方案逐渐受到关注。

    

    快速增长的边缘感知设备（如物联网、移动设备和可穿戴设备）及其集成传感器的测量能力使得监控环境特征、与之交互并获取环境信息成为可能。尽管这些设备体积小、数据存储和处理能力较低，但它们产生了大量的数据。传感器数据的收集和处理应用领域包括医疗保健、环境（包括空气质量和污染水平）、汽车、工业、航空航天和农业等。从边缘设备收集的这些海量感知数据使用各种机器学习（ML）和深度学习（DL）方法进行分析。然而，在云端或服务器上进行分析会带来与隐私、硬件和连接性限制相关的挑战。联邦学习（FL）正逐渐成为解决这些问题的解决方案。

    The ability to monitor ambient characteristics, interact with them, and derive information about the surroundings has been made possible by the rapid proliferation of edge sensing devices like IoT, mobile, and wearable devices and their measuring capabilities with integrated sensors. Even though these devices are small and have less capacity for data storage and processing, they produce vast amounts of data. Some example application areas where sensor data is collected and processed include healthcare, environmental (including air quality and pollution levels), automotive, industrial, aerospace, and agricultural applications. These enormous volumes of sensing data collected from the edge devices are analyzed using a variety of Machine Learning (ML) and Deep Learning (DL) approaches. However, analyzing them on the cloud or a server presents challenges related to privacy, hardware, and connectivity limitations. Federated Learning (FL) is emerging as a solution to these problems while pre
    
[^42]: 持续学习在语言转换中的研究

    A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])

    [http://arxiv.org/abs/2311.01200](http://arxiv.org/abs/2311.01200)

    本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。

    

    最近语言模型预训练的数据和模型规模的增加导致了巨大的训练成本。在随时间推移而出现新数据的情况下，更新模型而不是完全重新训练可以带来显著的收益。在本文中，我们研究了在新语言出现时更新语言模型时的好处和弊端，即在语言转换中持续学习的情况。从单语英语语言模型出发，我们逐步添加了来自挪威语和冰岛语的数据，以研究前向和后向转移效果如何取决于预训练顺序和语言特征，对于不同的模型大小和学习率调度器。我们的结果表明，尽管前向转移主要是正向的，不受语言顺序的影响，但后向转移则可能是正向的或负向的，具体取决于新语言的顺序和特征。为了解释这些模式，我们探索了几种语言相似度度量方法。

    The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
    
[^43]: 高斯过程在细胞复合物上的应用

    Gaussian Processes on Cellular Complexes. (arXiv:2311.01198v1 [cs.LG])

    [http://arxiv.org/abs/2311.01198](http://arxiv.org/abs/2311.01198)

    本论文在细胞复合物上应用高斯过程，提出了两个新的核函数来捕捉高阶细胞之间的交互作用。

    

    近年来，人们对在图上开发机器学习模型来考虑拓扑归纳偏置产生了相当大的兴趣。特别是，最近关注的是在这些结构上的高斯过程，因为它们能够同时考虑不确定性。然而，图仅限于对两个顶点之间的关系进行建模。在本文中，我们超越了这种对称配置，并考虑了包括顶点、边和它们的一种广义化称为细胞的交互关系。具体地说，我们提出了高斯过程在细胞复合物上的应用，这是对图的一种推广，可以捕捉这些高阶细胞之间的交互作用。我们的一个关键贡献是推导出两个新型核函数，一个是对图Mat\'ern核进行推广，另一个是额外地混合了不同细胞类型的信息。

    In recent years, there has been considerable interest in developing machine learning models on graphs in order to account for topological inductive biases. In particular, recent attention was given to Gaussian processes on such structures since they can additionally account for uncertainty. However, graphs are limited to modelling relations between two vertices. In this paper, we go beyond this dyadic setting and consider polyadic relations that include interactions between vertices, edges and one of their generalisations, known as cells. Specifically, we propose Gaussian processes on cellular complexes, a generalisation of graphs that captures interactions between these higher-order cells. One of our key contributions is the derivation of two novel kernels, one that generalises the graph Mat\'ern kernel and one that additionally mixes information of different cell types.
    
[^44]: 对抗双边边缘噪声以实现稳健的链接预测

    Combating Bilateral Edge Noise for Robust Link Prediction. (arXiv:2311.01196v1 [cs.LG])

    [http://arxiv.org/abs/2311.01196](http://arxiv.org/abs/2311.01196)

    该论文提出了一种对抗双边边缘噪声的方法，通过引入信息论引导的原则，提取可靠的监督信号和避免表示崩溃，从而实现稳健的图链接预测。

    

    尽管随着图神经网络（GNNs）的发展，图上的链接预测已经取得了巨大的成功，但其在边缘噪声下的鲁棒性潜力仍然较少被研究。为了填补这个空白，我们首先进行了一项经验研究，揭示了边缘噪声双向干扰了输入拓扑和目标标签，导致性能严重下降和表示崩溃。为了解决这个困境，我们提出了一种信息论引导的原则，稳健的图信息瓶颈（RGIB），用于提取可靠的监督信号并避免表示崩溃。与基本的信息瓶颈不同，RGIB进一步解耦和平衡了图拓扑、目标标签和表示之间的相互依赖关系，为抗双边噪声的稳健表示构建了新的学习目标。我们探索了两种实例化方法，即RGIB-SSL和RGIB-REP，以利用不同方法的优势，即自监督学习和数据重建。

    Although link prediction on graphs has achieved great success with the development of graph neural networks (GNNs), the potential robustness under the edge noise is still less investigated. To close this gap, we first conduct an empirical study to disclose that the edge noise bilaterally perturbs both input topology and target label, yielding severe performance degradation and representation collapse. To address this dilemma, we propose an information-theory-guided principle, Robust Graph Information Bottleneck (RGIB), to extract reliable supervision signals and avoid representation collapse. Different from the basic information bottleneck, RGIB further decouples and balances the mutual dependence among graph topology, target labels, and representation, building new learning objectives for robust representation against the bilateral noise. Two instantiations, RGIB-SSL and RGIB-REP, are explored to leverage the merits of different methodologies, i.e., self-supervised learning and data r
    
[^45]: Batch Bayesian Optimization for Replicable Experimental Design

    Batch Bayesian Optimization for Replicable Experimental Design. (arXiv:2311.01195v1 [cs.LG])

    [http://arxiv.org/abs/2311.01195](http://arxiv.org/abs/2311.01195)

    这项研究提出了适用于可复制实验设计的批量贝叶斯优化框架，通过自适应选择复制次数来平衡评估更多唯一条件与增加每个条件的复制次数之间的权衡，并考虑到了从业人员的风险规避倾向。

    

    许多现实世界的实验设计问题在并行评估多个实验条件的同时，由于存在大量和异方差的观测噪声，还会对每个条件进行多次复制。在给定固定总预算的情况下，这自然地引发了在评估更多唯一条件的同时减少每个条件的复制次数与评估较少的唯一条件并增加每个条件的复制次数之间的权衡。此外，在这些问题中，从业人员可能具有风险规避倾向，因此更加偏好既具有良好平均性能又具有较小变异性的输入。为了解决这两个挑战，我们提出了适用于可复制实验设计的批量 Thompson Sampling (BTS-RED) 框架，其中包括三个算法。我们的 BTS-RED-Known 和 BTS-RED-Unknown 算法分别用于已知和未知噪声方差，通过自适应选择复制次数而不是确定性地为具有较大噪声方差的输入进行更多次复制。因此，尽管输入的噪声方差较大，我们的方法仍能实现复制更多次的效果。

    Many real-world experimental design problems (a) evaluate multiple experimental conditions in parallel and (b) replicate each condition multiple times due to large and heteroscedastic observation noise. Given a fixed total budget, this naturally induces a trade-off between evaluating more unique conditions while replicating each of them fewer times vs. evaluating fewer unique conditions and replicating each more times. Moreover, in these problems, practitioners may be risk-averse and hence prefer an input with both good average performance and small variability. To tackle both challenges, we propose the Batch Thompson Sampling for Replicable Experimental Design (BTS-RED) framework, which encompasses three algorithms. Our BTS-RED-Known and BTS-RED-Unknown algorithms, for, respectively, known and unknown noise variance, choose the number of replications adaptively rather than deterministically such that an input with a larger noise variance is replicated more times. As a result, despite 
    
[^46]: VIGraph：自我监督学习用于类别不平衡节点分类

    VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification. (arXiv:2311.01191v1 [cs.LG])

    [http://arxiv.org/abs/2311.01191](http://arxiv.org/abs/2311.01191)

    VIGraph是一个基于自我监督学习的模型，通过利用自编码器生成少数类节点来解决图数据中的类别不平衡问题，并通过引入孪生对比策略提高生成节点的质量。

    

    图数据中的类别不平衡为节点分类带来了重大挑战。现有方法，如基于SMOTE的方法，在不平衡场景构建过程中仍存在局限性。自我监督学习（SSL）通过从数据中合成少数类节点提供了一个有前景的解决方案，然而其潜力尚未被探索。本文分析了基于SMOTE的方法的限制，并引入了VIGraph，这是一种基于自我监督变分图自编码器（VGAE）的新型SSL模型，利用变分推断（VI）生成少数类节点。具体而言，VIGraph在构建不平衡图时严格遵循不平衡的概念，并利用生成型VGAE生成少数类节点。此外，VIGraph在解码阶段引入了一种新颖的孪生对比策略，以提高生成节点的整体质量。VIGraph能够生成高质量的节点，无需重新集成。

    Class imbalance in graph data poses significant challenges for node classification. Existing methods, represented by SMOTE-based approaches, partially alleviate this issue but still exhibit limitations during imbalanced scenario construction. Self-supervised learning (SSL) offers a promising solution by synthesizing minority nodes from the data itself, yet its potential remains unexplored. In this paper, we analyze the limitations of SMOTE-based approaches and introduce VIGraph, a novel SSL model based on the self-supervised Variational Graph Auto-Encoder (VGAE) that leverages Variational Inference (VI) to generate minority nodes. Specifically, VIGraph strictly adheres to the concept of imbalance when constructing imbalanced graphs and utilizes the generative VGAE to generate minority nodes. Moreover, VIGraph introduces a novel Siamese contrastive strategy at the decoding phase to improve the overall quality of generated nodes. VIGraph can generate high-quality nodes without reintegrat
    
[^47]: 数字孪生与基于人工智能的网络安全应用综述

    A Review of Digital Twins and their Application in Cybersecurity based on Artificial Intelligence. (arXiv:2311.01154v1 [cs.CR])

    [http://arxiv.org/abs/2311.01154](http://arxiv.org/abs/2311.01154)

    数字孪生技术与人工智能工具之间的强互动可以改善数字平台的网络安全，但由于缺乏信息和安全标准，网络犯罪分子能够利用数字孪生技术对整个集合造成威胁。

    

    数字孪生技术的潜力尚未完全发挥，由于其多样性和未开发的潜力。数字孪生使得系统的分析、设计、优化和演化能够通过数字方式或与协同的网络-物理方法相结合进行，以提高传统工程方法的速度、准确性和效率。工业4.0、未来工厂和数字孪生技术继续从该技术中受益，并在现有系统中提供了增强的效率。由于缺乏与网络数字化过渡相关的信息和安全标准，网络犯罪分子能够利用这种情况。访问产品或服务的数字孪生等同于威胁整个集合。数字孪生与人工智能工具之间存在着强有力的互动，这导致这些技术之间存在着紧密的互动，因此可以用来改善这些数字平台的网络安全。

    The potential of digital twin technology is yet to be fully realized due to its diversity and untapped potential. Digital twins enable systems' analysis, design, optimization, and evolution to be performed digitally or in conjunction with a cyber-physical approach to improve speed, accuracy, and efficiency over traditional engineering methods. Industry 4.0, factories of the future, and digital twins continue to benefit from the technology and provide enhanced efficiency within existing systems. Due to the lack of information and security standards associated with the transition to cyber digitization, cybercriminals have been able to take advantage of the situation. Access to a digital twin of a product or service is equivalent to threatening the entire collection. There is a robust interaction between digital twins and artificial intelligence tools, which leads to strong interaction between these technologies, so it can be used to improve the cybersecurity of these digital platforms ba
    
[^48]: 添加和稀疏：一种用于时间点过程的扩散方法

    Add and Thin: Diffusion for Temporal Point Processes. (arXiv:2311.01139v1 [cs.LG])

    [http://arxiv.org/abs/2311.01139](http://arxiv.org/abs/2311.01139)

    本研究提出了一种基于概率去噪扩散模型的时间点过程模型，相比于现有的方法，该模型在预测方面取得了较好的性能，对具有离散和连续成分的数据具有处理能力。

    

    在时间点过程（TPP）框架内，自回归神经网络已成为建模连续时间事件数据的标准。尽管这些模型可以以一步预测的方式精确地捕捉事件序列，但由于其顺序性质引起的误差积累，它们在长期预测应用中具有固有的局限性。为了克服这些限制，我们推导出ADD-THIN，一种面向整个事件序列工作的基于概率去噪扩散模型，它自然地处理具有离散和连续成分的数据。在合成和真实数据集的实验中，我们的模型在密度估计方面与最先进的TPP模型相匹配，并在预测方面表现出色。

    Autoregressive neural networks within the temporal point process (TPP) framework have become the standard for modeling continuous-time event data. Even though these models can expressively capture event sequences in a one-step-ahead fashion, they are inherently limited for long-term forecasting applications due to the accumulation of errors caused by their sequential nature. To overcome these limitations, we derive ADD-THIN, a principled probabilistic denoising diffusion model for TPPs that operates on entire event sequences. Unlike existing diffusion approaches, ADD-THIN naturally handles data with discrete and continuous components. In experiments on synthetic and real-world datasets, our model matches the state-of-the-art TPP models in density estimation and strongly outperforms them in forecasting.
    
[^49]: AeroPath: 一种具有挑战性病理的气道分割基准数据集

    AeroPath: An airway segmentation benchmark dataset with challenging pathology. (arXiv:2311.01138v1 [cs.CV])

    [http://arxiv.org/abs/2311.01138](http://arxiv.org/abs/2311.01138)

    在本研究中，我们介绍了一个新的具有挑战性病理的气道分割基准数据集（AeroPath），以提高肺部疾病早期诊断和治疗的效果。我们还提出了一种多尺度融合设计，用于自动气道分割。模型在该数据集上进行了训练。

    

    为了改善患有肺部疾病（如肺癌）的患者的预后，早期诊断和治疗至关重要。CT图像的分析对于诊断至关重要，而对于干预计划和支气管镜检查过程中的实时引导，需要对气道树进行高质量的分割。最近，多领域气道树建模（ATM'22）挑战发布了一个大型数据集，既可以训练基于深度学习的模型，又显著提高了气道分割任务的最新技术水平。然而，ATM'22数据集中只包括少数患有影响气道树解剖结构的严重病理的患者。在这项研究中，我们介绍了一个新的公共基准数据集（AeroPath），包括27个CT图像，来自患有从肺气肿到大型肿瘤等病理的患者，并附带有对应的气管和支气管注释。其次，我们提出了一种用于自动气道分割的多尺度融合设计。模型是在该数据集上训练的。

    To improve the prognosis of patients suffering from pulmonary diseases, such as lung cancer, early diagnosis and treatment are crucial. The analysis of CT images is invaluable for diagnosis, whereas high quality segmentation of the airway tree are required for intervention planning and live guidance during bronchoscopy. Recently, the Multi-domain Airway Tree Modeling (ATM'22) challenge released a large dataset, both enabling training of deep-learning based models and bringing substantial improvement of the state-of-the-art for the airway segmentation task. However, the ATM'22 dataset includes few patients with severe pathologies affecting the airway tree anatomy. In this study, we introduce a new public benchmark dataset (AeroPath), consisting of 27 CT images from patients with pathologies ranging from emphysema to large tumors, with corresponding trachea and bronchi annotations. Second, we present a multiscale fusion design for automatic airway segmentation. Models were trained on the
    
[^50]: 用PySCF_IPU生成QM1B数据集

    Generating QM1B with PySCF$_{\text{IPU}}$. (arXiv:2311.01135v1 [cs.LG])

    [http://arxiv.org/abs/2311.01135](http://arxiv.org/abs/2311.01135)

    本文介绍了使用IPU硬件加速器的数据生成器PySCF_IPU来生成具有十亿个训练样本的QM1B数据集，此数据集包含9-11个重原子。通过对这个数据集的实验，我们证明了一个简单的基准神经网络的优秀性能。

    

    计算机视觉和自然语言处理中的基础模型的出现在下游任务上取得了巨大的进展。这种进展得益于拥有数十亿个训练样本的数据集。相比之下，在量子化学领域还未实现类似的好处，因为深度学习的潜力受限于仅有10万到2000万个训练样本的相对较小数据集。这些数据集的规模有限，是因为标签是使用密度泛函理论（DFT）的准确（但计算量大）预测计算得出的。值得注意的是，之前的DFT数据集是使用CPU超级计算机创建的，没有利用硬件加速。在本文中，我们向利用硬件加速器迈出了第一步，引入了使用智能处理单元（IPU）的数据生成器PySCF_IPU。这使我们能够创建包含有9-11个重原子的十亿个训练样本的数据集QM1B。我们证明了一个简单的基准神经网络可以在这个数据集上表现优秀。

    The emergence of foundation models in Computer Vision and Natural Language Processing have resulted in immense progress on downstream tasks. This progress was enabled by datasets with billions of training examples. Similar benefits are yet to be unlocked for quantum chemistry, where the potential of deep learning is constrained by comparatively small datasets with 100k to 20M training examples. These datasets are limited in size because the labels are computed using the accurate (but computationally demanding) predictions of Density Functional Theory (DFT). Notably, prior DFT datasets were created using CPU supercomputers without leveraging hardware acceleration. In this paper, we take a first step towards utilising hardware accelerators by introducing the data generator PySCF$_{\text{IPU}}$ using Intelligence Processing Units (IPUs). This allowed us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms. We demonstrate that a simple baseline neural n
    
[^51]: 用于叠写字符语义分割的深度学习实验

    A deep learning experiment for semantic segmentation of overlapping characters in palimpsests. (arXiv:2311.01130v1 [cs.CV])

    [http://arxiv.org/abs/2311.01130](http://arxiv.org/abs/2311.01130)

    本文提出了一种基于深度学习的语义分割方法，用于识别和分割叠写字符中的个体字母。该方法在普里西亚诺的《语法文献》这个案例研究中进行了验证，并讨论了结合多光谱成像的局限性和前景。

    

    Palimpsests是指历史手稿中被第二个写作部分覆盖的已经擦除的写作。通过使用成像技术，如多光谱成像，可以识别出肉眼难以察觉的特征，包括褪色和擦除的墨迹。当处理重叠墨迹时，可以利用人工智能技术来解开重叠字母的复杂节点。在这项工作中，我们提出了基于深度学习的语义分割作为一种识别和分割重叠字符中的个体字母的方法。该实验被构想为一个概念验证，以普里西亚诺的《语法文献》作为案例研究。此外，我们还讨论了结合多光谱成像的方法的限制和前景。

    Palimpsests refer to historical manuscripts where erased writings have been partially covered by the superimposition of a second writing. By employing imaging techniques, e.g., multispectral imaging, it becomes possible to identify features that are imperceptible to the naked eye, including faded and erased inks. When dealing with overlapping inks, Artificial Intelligence techniques can be utilized to disentangle complex nodes of overlapping letters. In this work, we propose deep learning-based semantic segmentation as a method for identifying and segmenting individual letters in overlapping characters. The experiment was conceived as a proof of concept, focusing on the palimpsests of the Ars Grammatica by Prisciano as a case study. Furthermore, caveats and prospects of our approach combined with multispectral imaging are also discussed.
    
[^52]: 可解释化化学的人工智能：通过对比学习预测自由基机理路径

    AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning. (arXiv:2311.01118v1 [cs.LG])

    [http://arxiv.org/abs/2311.01118](http://arxiv.org/abs/2311.01118)

    该论文介绍了一种新型反应预测器系统RMechRP，通过对比学习和机理路径，可以准确且可解释地预测自由基反应，具有广泛应用潜力。

    

    基于深度学习的反应预测器经历了显著的架构演变。然而，它们对美国专利局反应的依赖导致了不可解释的预测和对其他化学领域（如自由基和大气化学）的有限泛化能力。为了应对这些挑战，我们引入了一种新的反应预测器系统RMechRP，它结合了最可解释的化学反应表示之一的机理路径和对比学习。RMechRP专为自由基反应而设计，提供了不同层次的化学反应解释。我们使用RMechDB，一个公共的自由基反应数据库，开发和训练了多个深度学习模型，建立了预测自由基反应的首个基准。我们的结果表明，RMechRP在提供准确和可解释的自由基反应预测方面的有效性，并且在各种应用中具有潜力。

    Deep learning-based reaction predictors have undergone significant architectural evolution. However, their reliance on reactions from the US Patent Office results in a lack of interpretable predictions and limited generalization capability to other chemistry domains, such as radical and atmospheric chemistry. To address these challenges, we introduce a new reaction predictor system, RMechRP, that leverages contrastive learning in conjunction with mechanistic pathways, the most interpretable representation of chemical reactions. Specifically designed for radical reactions, RMechRP provides different levels of interpretation of chemical reactions. We develop and train multiple deep-learning models using RMechDB, a public database of radical reactions, to establish the first benchmark for predicting radical reactions. Our results demonstrate the effectiveness of RMechRP in providing accurate and interpretable predictions of radical reactions, and its potential for various applications in 
    
[^53]: H-NeXt：迈向旋转平移不变网络的下一步

    H-NeXt: The next step towards roto-translation invariant networks. (arXiv:2311.01111v1 [cs.CV])

    [http://arxiv.org/abs/2311.01111](http://arxiv.org/abs/2311.01111)

    H-NeXt是一个参数高效的旋转平移不变网络，通过等变和不变的组件实现鲁棒性和性能优于现有技术水平。

    

    等变网络的广泛流行凸显了参数高效模型和有效利用训练数据的重要性。在对未知变形的鲁棒性变得越来越重要的时候，我们提出了H-NeXt框架，填补了等变性和不变性之间的差距。H-NeXt是一个参数高效的旋转平移不变网络，训练集中没有任何增强图像。我们的网络包括三个组件：一个等变的主干网络用于学习旋转平移不相关的特征，一个不变的汇聚层用于丢弃旋转平移信息，和一个分类层。H-NeXt在MNIST和CIFAR-10的无增强训练集和增强测试集上优于现有技术水平。

    The widespread popularity of equivariant networks underscores the significance of parameter efficient models and effective use of training data. At a time when robustness to unseen deformations is becoming increasingly important, we present H-NeXt, which bridges the gap between equivariance and invariance. H-NeXt is a parameter-efficient roto-translation invariant network that is trained without a single augmented image in the training set. Our network comprises three components: an equivariant backbone for learning roto-translation independent features, an invariant pooling layer for discarding roto-translation information, and a classification layer. H-NeXt outperforms the state of the art in classification on unaugmented training sets and augmented test sets of MNIST and CIFAR-10.
    
[^54]: 对于校准和一致学习推迟，采用Softmax参数化的辩护

    In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer. (arXiv:2311.01106v1 [cs.LG])

    [http://arxiv.org/abs/2311.01106](http://arxiv.org/abs/2311.01106)

    本文辩护了采用Softmax参数化的学习推迟方法，解释了先前关于其估计器的误校准和无界估计的原因，并提出了一种新颖的统计一致性非对称Softmax估计器。

    

    当下游专家更准确时，使得机器学习分类器能推迟其决策将确保提高安全性和性能。这个目标可以通过学习推迟框架来实现，该框架旨在共同学习如何分类和如何推迟给专家。最近的研究已经理论上证明，用Softmax参数化的学习推迟的流行估计器提供了无界估计，这使得它们无法校准。然而，目前尚不清楚这是否是由于广泛使用的Softmax参数化，以及我们是否可以找到一个既具有统计一致性又具有有效概率估计器的基于Softmax的估计器。在这项工作中，我们首先展示了先前文献中误校准和无界估计器的原因是由于所使用的代替损失的对称性质，而不是由于Softmax。然后，我们提出了一种新颖的统计一致性非对称Softmax估计器。

    Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and performance. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. However, it remains unknown whether this is due to the widely used softmax parameterization and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax. We then propose a novel statistically consistent asymmetric softma
    
[^55]: 多任务强化学习中具有时间注意力的对比模块

    Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning. (arXiv:2311.01075v1 [cs.LG])

    [http://arxiv.org/abs/2311.01075](http://arxiv.org/abs/2311.01075)

    本文提出了一种名为对比模块与时间注意力(CMTA)的方法，该方法通过对比学习使模块之间的差异，并以更细粒度的方式在任务级别之下使用共享模块和时间注意力来解决多任务领域中现有方法的限制。

    

    在多任务强化学习领域，模块化原则被广泛采用作为一种有前景的方法，通过将功能专门化到不同的模块中并适当地组合它们，以防止由于任务间冲突而导致的性能下降问题。然而，大多数现有的多任务RL方法只在任务级别上组合共享模块，忽视了任务内可能存在的冲突。此外，这些方法没有考虑到，如果没有约束，一些模块可能会学习相似的功能，从而限制了模块化方法的表达能力和推广能力。在本文中，我们提出了具有时间注意力的对比模块（CMTA）方法来解决这些限制。

    In the field of multi-task reinforcement learning, the modular principle, which involves specializing functionalities into different modules and combining them appropriately, has been widely adopted as a promising approach to prevent the negative transfer problem that performance degradation due to conflicts between tasks. However, most of the existing multi-task RL methods only combine shared modules at the task level, ignoring that there may be conflicts within the task. In addition, these methods do not take into account that without constraints, some modules may learn similar functions, resulting in restricting the model's expressiveness and generalization capability of modular methods. In this paper, we propose the Contrastive Modules with Temporal Attention(CMTA) method to address these limitations. CMTA constrains the modules to be different from each other by contrastive learning and combining shared modules at a finer granularity than the task level with temporal attention, al
    
[^56]: 零样本条件下基于多模式基础模型的相机陷阱图像动物物种识别

    Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images. (arXiv:2311.01064v1 [cs.CV])

    [http://arxiv.org/abs/2311.01064](http://arxiv.org/abs/2311.01064)

    本论文提出了一种新颖的零样本物种分类框架，通过使用多模式基础模型，利用视觉-语言模型生成相机陷阱图像的视觉描述，并通过匹配外部知识库中的描述来确定零样本条件下的物种。

    

    由于环境恶化和人类活动增加，野生动物保护工作至关重要。运动触发的相机陷阱是全球追踪和监测野生动物数量的有效工具。监督学习技术已成功应用于分析这类图像，但是训练这些技术需要来自专家的注释。减少对昂贵标记数据的依赖，因此在开发具有明显较少人力劳动的大规模野生动物追踪解决方案方面具有巨大潜力。在这项工作中，我们提出了WildMatch，一个利用多模式基础模型的新型零样本物种分类框架。特别地，我们通过指导调整视觉-语言模型，使用类似专家的术语生成相机陷阱图像的详细视觉描述。然后，我们将生成的描述与外部知识库中的描述进行匹配，以确定零样本条件下的物种。

    Due to deteriorating environmental conditions and increasing human activity, conservation efforts directed towards wildlife is crucial. Motion-activated camera traps constitute an efficient tool for tracking and monitoring wildlife populations across the globe. Supervised learning techniques have been successfully deployed to analyze such imagery, however training such techniques requires annotations from experts. Reducing the reliance on costly labelled data therefore has immense potential in developing large-scale wildlife tracking solutions with markedly less human labor. In this work we propose WildMatch, a novel zero-shot species classification framework that leverages multimodal foundation models. In particular, we instruction tune vision-language models to generate detailed visual descriptions of camera trap images using similar terminology to experts. Then, we match the generated caption to an external knowledge base of descriptions in order to determine the species in a zero-s
    
[^57]: 实时神经解码的深度学习方法

    Deep Learning for real-time neural decoding of grasp. (arXiv:2311.01061v1 [cs.LG])

    [http://arxiv.org/abs/2311.01061](http://arxiv.org/abs/2311.01061)

    本文提出了一种基于深度学习的神经信号解码方法，利用LSTM网络将含有神经数据的时间序列进行分类，以实现物体抓取类型的分类。该方法在不依赖先前的神经科学知识的情况下，通过提取数据相关性来提高解码准确性，取得了显著的分类准确度提升。

    

    神经解码是将从大脑获取的信号与肢体运动或机器人控制等物理世界变量相互关联的一种方法，应用于脑机接口。本文基于一组来自猴子运动皮层的神经记录数据集，提出了一种基于深度学习的神经信号解码方法，用于抓取类型的分类。具体而言，我们提出了一种利用LSTM网络将含有神经数据（即尖峰电位）的时间序列进行分类的方法，以表示被抓取的物体类别。所提出的方法的主要目标是在不依赖任何先前的神经科学知识的情况下，仅依靠深度学习模型从数据中提取相关性来提高解码准确性。本文介绍了在所考虑的数据集上取得的结果，并将其与先前在相同数据集上的工作进行了比较，显示了分类准确度的显著提升。

    Neural decoding involves correlating signals acquired from the brain to variables in the physical world like limb movement or robot control in Brain Machine Interfaces. In this context, this work starts from a specific pre-existing dataset of neural recordings from monkey motor cortex and presents a Deep Learning-based approach to the decoding of neural signals for grasp type classification. Specifically, we propose here an approach that exploits LSTM networks to classify time series containing neural data (i.e., spike trains) into classes representing the object being grasped. The main goal of the presented approach is to improve over state-of-the-art decoding accuracy without relying on any prior neuroscience knowledge, and leveraging only the capability of deep learning models to extract correlations from data. The paper presents the results achieved for the considered dataset and compares them with previous works on the same dataset, showing a significant improvement in classificat
    
[^58]: 在部署时进行实时调节：用于单机器人部署的行为调控

    Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment. (arXiv:2311.01059v1 [cs.RO])

    [http://arxiv.org/abs/2311.01059](http://arxiv.org/abs/2311.01059)

    本研究提出了一种名为ROAM的方法，通过利用先前学习到的行为来实时调节机器人在部署过程中应对未曾见过的情况。在测试中，ROAM可以在单个阶段内实现快速适应，并且在模拟环境和真实场景中取得了成功，具有较高的效率和适应性。

    

    为了在现实世界中取得成功，机器人必须应对训练过程中未曾见过的情况。本研究探讨了在部署过程中针对这些新场景的实时调节问题，通过利用先前学习到的多样化行为库。我们的方法，RObust Autonomous Modulation（ROAM），引入了基于预训练行为的感知价值的机制，以在特定情况下选择和调整预训练行为。关键是，这种调节过程在测试时的单个阶段内完成，无需任何人类监督。我们对选择机制进行了理论分析，并证明了ROAM使得机器人能够在模拟环境和真实的四足动物Go1上快速适应动态变化，甚至在脚上套着滚轮滑鞋的情况下成功前进。与现有方法相比，我们的方法在面对各种分布情况的部署时能够以超过2倍的效率进行调节，通过有效选择来实现适应。

    To succeed in the real world, robots must cope with situations that differ from those seen during training. We study the problem of adapting on-the-fly to such novel scenarios during deployment, by drawing upon a diverse repertoire of previously learned behaviors. Our approach, RObust Autonomous Modulation (ROAM), introduces a mechanism based on the perceived value of pre-trained behaviors to select and adapt pre-trained behaviors to the situation at hand. Crucially, this adaptation process all happens within a single episode at test time, without any human supervision. We provide theoretical analysis of our selection mechanism and demonstrate that ROAM enables a robot to adapt rapidly to changes in dynamics both in simulation and on a real Go1 quadruped, even successfully moving forward with roller skates on its feet. Our approach adapts over 2x as efficiently compared to existing methods when facing a variety of out-of-distribution situations during deployment by effectively choosing
    
[^59]: 韧性多选学习：用于音频场景分析的学习评分方案的引入

    Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis. (arXiv:2311.01052v1 [stat.ML])

    [http://arxiv.org/abs/2311.01052](http://arxiv.org/abs/2311.01052)

    这项研究引入了韧性多选学习（rMCL）方法，通过使用基于Voronoi tessellations的数学框架和学习评分方案，在回归设置中实现了对于每个训练输入可能采样多个目标的条件分布估计。该方法在合成数据和声源定位问题上得到了实证验证和进一步评估，展示了其实际的有用性和解释的相关性。

    

    我们引入了韧性多选学习（rMCL），这是一种对于每个训练输入可能采样多个目标的回归设置下条件分布估计的MCL方法的扩展。多选学习是一个简单的框架，用于处理多模态密度估计，使用了一组假设的胜者全拿（WTA）损失。在回归设置中，现有的MCL变体主要集中在合并假设上，从而最终牺牲了预测的多样性。相反，我们的方法依赖于一个基于Voronoi tessellations的输出空间的数学框架支持的新颖的学习评分方案，我们可以从中得出概率解释。在对合成数据进行实证验证后，我们进一步评估了rMCL在声源定位问题上的优点，展示了其实际的有用性和解释的相关性。

    We introduce Resilient Multiple Choice Learning (rMCL), an extension of the MCL approach for conditional distribution estimation in regression settings where multiple targets may be sampled for each training input. Multiple Choice Learning is a simple framework to tackle multimodal density estimation, using the Winner-Takes-All (WTA) loss for a set of hypotheses. In regression settings, the existing MCL variants focus on merging the hypotheses, thereby eventually sacrificing the diversity of the predictions. In contrast, our method relies on a novel learned scoring scheme underpinned by a mathematical framework based on Voronoi tessellations of the output space, from which we can derive a probabilistic interpretation. After empirically validating rMCL with experiments on synthetic data, we further assess its merits on the sound source localization problem, demonstrating its practical usefulness and the relevance of its interpretation.
    
[^60]: 使用向量同步在分布式无电池物联网网络中应用和节能数据聚合

    Application and Energy-Aware Data Aggregation using Vector Synchronization in Distributed Battery-less IoT Networks. (arXiv:2311.01050v1 [cs.NI])

    [http://arxiv.org/abs/2311.01050](http://arxiv.org/abs/2311.01050)

    该论文提出了一种使用向量同步在分布式无电池物联网网络中进行应用和节能数据聚合的机制，旨在为无电池物联网设备提供可持续的应用支持，并解决了由于动态能量收集环境导致的任务执行不稳定性的挑战。

    

    无电池物联网设备是下一代无线网络可持续绿色倡议的重要组成部分，这些无电池设备利用环境中的环境能量进行工作。能量收集环境是动态的，导致间歇性任务执行。收集到的能量存储在小型电容器中，保证应用任务的执行是具有挑战性的。主要目标是提供一种机制来对传感器数据进行聚合，并在分布式无电池物联网网络中提供可持续的应用支持。我们对由许多无电池物联网传感器硬件模块和异构物联网应用组成的分布式物联网网络系统进行了建模，这些应用在设备-边缘-云的连续体中获得支持。这些应用程序需要来自分布式的无电池硬件模块的传感器数据，并具有对模块执行器的联合控制。

    The battery-less Internet of Things (IoT) devices are a key element in the sustainable green initiative for the next-generation wireless networks. These battery-free devices use the ambient energy, harvested from the environment. The energy harvesting environment is dynamic and causes intermittent task execution. The harvested energy is stored in small capacitors and it is challenging to assure the application task execution. The main goal is to provide a mechanism to aggregate the sensor data and provide a sustainable application support in the distributed battery-less IoT network. We model the distributed IoT network system consisting of many battery-free IoT sensor hardware modules and heterogeneous IoT applications that are being supported in the device-edge-cloud continuum. The applications require sensor data from a distributed set of battery-less hardware modules and there is provision of joint control over the module actuators. We propose an application-aware task and energy ma
    
[^61]: 通过倾斜指数层改善稳健性：基于通信理论的视角

    Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective. (arXiv:2311.01047v1 [cs.LG])

    [http://arxiv.org/abs/2311.01047](http://arxiv.org/abs/2311.01047)

    本论文提出了一种基于通信理论的方法，通过神经竞争来增强神经网络层输出的信噪比，从而提高深度网络的稳健性。

    

    提升深度网络稳健性的最新技术大多依赖于合适的数据增强的经验风险最小化。本文提出了一种基于通信理论的互补方法，旨在通过学习和推理中的神经竞争来增强神经网络层输出的信噪比。除了最小化标准的端到端代价外，神经元通过最大化倾斜指数（TEXP）层的目标函数来竞争以稀疏地表示层输入。TEXP学习可以被解释为在数据噪声的高斯模型下通过最大似然估计来匹配滤波器。在TEXP层中，通过使用倾斜的softmax替代批量归一化来进行推理，可以解释为计算每个神经元代表的竞争信号假设的后验概率。通过简化模型提供洞察，我们通过在标准图像上的实验表明，

    State-of-the-art techniques for enhancing robustness of deep networks mostly rely on empirical risk minimization with suitable data augmentation. In this paper, we propose a complementary approach motivated by communication theory, aimed at enhancing the signal-to-noise ratio at the output of a neural network layer via neural competition during learning and inference. In addition to minimization of a standard end-to-end cost, neurons compete to sparsely represent layer inputs by maximization of a tilted exponential (TEXP) objective function for the layer. TEXP learning can be interpreted as maximum likelihood estimation of matched filters under a Gaussian model for data noise. Inference in a TEXP layer is accomplished by replacing batch norm by a tilted softmax, which can be interpreted as computation of posterior probabilities for the competing signaling hypotheses represented by each neuron. After providing insights via simplified models, we show, by experimentation on standard image
    
[^62]: SGLD的无时间信息论广义界的翻译

    Time-Independent Information-Theoretic Generalization Bounds for SGLD. (arXiv:2311.01046v1 [cs.LG])

    [http://arxiv.org/abs/2311.01046](http://arxiv.org/abs/2311.01046)

    该论文提出了针对SGLD的无时间信息论广义界，尽管迭代次数和步长可能不固定，但这些界在样本大小增加时会衰减为零。同时，还建立了在训练和测试损失相同时的信息论广义界，并解决了现有工作中步长依赖的问题，从而得到了改进的过度风险界。

    

    在光滑性和耗散性的假设下，我们提供了随机梯度 Langevin 动力学 (SGLD) 的新颖信息论广义界。我们的界不依赖于时间，在样本大小增加时会衰减至零，不论迭代次数和步长是否固定。与以前的研究不同，我们通过关注 Kullback--Leibler 散度的时间演化来推导广义误差界，该散度与数据集的稳定性有关并且是输出参数与输入数据集之间互信息的上界。此外，我们通过证明 SGLD 的损失函数是次指数的来建立第一个当训练和测试损失相同时的信息论广义界。这个界也是无时间关联的，并且消除了现有工作中步长依赖的问题，从而得到了改进的过度风险界。

    We provide novel information-theoretic generalization bounds for stochastic gradient Langevin dynamics (SGLD) under the assumptions of smoothness and dissipativity, which are widely used in sampling and non-convex optimization studies. Our bounds are time-independent and decay to zero as the sample size increases, regardless of the number of iterations and whether the step size is fixed. Unlike previous studies, we derive the generalization error bounds by focusing on the time evolution of the Kullback--Leibler divergence, which is related to the stability of datasets and is the upper bound of the mutual information between output parameters and an input dataset. Additionally, we establish the first information-theoretic generalization bound when the training and test loss are the same by showing that a loss function of SGLD is sub-exponential. This bound is also time-independent and removes the problematic step size dependence in existing work, leading to an improved excess risk bound
    
[^63]: 更少更好：基于数据激活视角的图神经网络预训练

    Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks. (arXiv:2311.01038v1 [cs.LG])

    [http://arxiv.org/abs/2311.01038](http://arxiv.org/abs/2311.01038)

    本文提出了一种基于数据激活视角的图神经网络预训练方法，通过精心选择较少的数据提高预训练模型的性能。这一方法能够在图数据中选择最具代表性和指导性的数据点进行训练。

    

    图神经网络（GNN）的预训练旨在利用无标注数据学习可迁移的知识以用于下游任务，并且最近已成为一个活跃的研究领域。然而，在这篇论文中，我们发现图预训练中存在着大数据的诅咒现象：更多的训练数据并不一定导致更好的下游性能。受这个观察的启发，我们提出了一个更少更好的图预训练框架：选择少量但精心选择的数据输入到GNN模型中以增强预训练。所提出的预训练流程被称为数据激活图预训练（APT）框架，由图选择器和预训练模型组成。图选择器根据图的固有属性和预测不确定性选择最具代表性和指导性的数据点。所提出的预测不确定性作为来自预训练模型的反馈信息。

    Pre-training on graph neural networks (GNNs) aims to learn transferable knowledge for downstream tasks with unlabeled data, and it has recently become an active research area. The success of graph pre-training models is often attributed to the massive amount of input data. In this paper, however, we identify the curse of big data phenomenon in graph pre-training: more training data do not necessarily lead to better downstream performance. Motivated by this observation, we propose a better-with-less framework for graph pre-training: fewer, but carefully chosen data are fed into a GNN model to enhance pre-training. The proposed pre-training pipeline is called the data-active graph pre-training (APT) framework, and is composed of a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on the inherent properties of graphs as well as predictive uncertainty. The proposed predictive uncertainty, as feedback from the pre-t
    
[^64]: 非自回归基于扩散的连续时间长期事件预测的时间点过程

    Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction. (arXiv:2311.01033v1 [cs.LG])

    [http://arxiv.org/abs/2311.01033](http://arxiv.org/abs/2311.01033)

    本论文提出了一种基于扩散的非自回归连续时间点过程模型，用于长期事件预测。通过整体预测未来事件序列、开发双向映射和设计降噪网络等手段，得到了更优的预测质量。

    

    连续时间长期事件预测在许多应用场景中起着重要作用。大多数现有工作依赖于自回归框架来预测事件序列，但这种方法容易累积错误，从而影响预测的质量。受到降噪扩散概率模型成功的启发，我们提出了一种基于扩散的非自回归连续时间点过程模型，用于长期事件预测。我们的模型整体预测未来的事件序列，而不是以自回归的方式逐个生成事件。为了在事件序列上进行扩散处理，我们开发了目标事件序列和欧几里德向量空间之间的双向映射。此外，我们设计了一个新颖的降噪网络，以捕捉顺序和上下文特征，提高样本质量。通过大量实验证明，我们提出的模型在长期事件上优于现有方法。

    Continuous-time long-term event prediction plays an important role in many application scenarios. Most existing works rely on autoregressive frameworks to predict event sequences, which suffer from error accumulation, thus compromising prediction quality. Inspired by the success of denoising diffusion probabilistic models, we propose a diffusion-based non-autoregressive temporal point process model for long-term event prediction in continuous time. Instead of generating events one at a time in an autoregressive way, our model predicts the future event sequence entirely as a whole. In order to perform diffusion processes on event sequences, we develop a bidirectional map between target event sequences and the Euclidean vector space. Furthermore, we design a novel denoising network to capture both sequential and contextual features for better sample quality. Extensive experiments are conducted to prove the superiority of our proposed model over state-of-the-art methods on long-term event
    
[^65]: 基于距离的传播策略用于知识图谱推理的效率提升

    Distance-Based Propagation for Efficient Knowledge Graph Reasoning. (arXiv:2311.01024v1 [cs.LG])

    [http://arxiv.org/abs/2311.01024](http://arxiv.org/abs/2311.01024)

    提出了一种基于距离的传播策略TAGNet，用于知识图谱补全任务中的高效推理。与其他方法相比，TAGNet能够在保持性能的前提下减少传播消息的数量，并且复杂度与层数无关。

    

    知识图谱补全旨在预测知识图谱中未见的边，从而发现新的事实。一类新的方法提出了通过聚合路径信息来解决这个问题。这些方法在知识图谱补全的任务中展示了巨大的能力。然而，它们饱受效率问题的困扰。虽然最近有一些尝试通过可学习的路径修剪来解决这个问题，但它们往往在性能上做出了牺牲以换取效率。在这项工作中，我们确定了这些方法的两个固有限制，这些限制影响了效率和表示质量。为了解决这些限制，我们引入了一种新方法，TAGNet，它能够有效地传播信息。这是通过仅在每对源-目标对中聚合固定窗口中的路径来实现的。我们证明了TAGNet的复杂度与层数无关。大量实验证明TAGNet可以减少传播消息的数量。

    Knowledge graph completion (KGC) aims to predict unseen edges in knowledge graphs (KGs), resulting in the discovery of new facts. A new class of methods have been proposed to tackle this problem by aggregating path information. These methods have shown tremendous ability in the task of KGC. However they are plagued by efficiency issues. Though there are a few recent attempts to address this through learnable path pruning, they often sacrifice the performance to gain efficiency. In this work, we identify two intrinsic limitations of these methods that affect the efficiency and representation quality. To address the limitations, we introduce a new method, TAGNet, which is able to efficiently propagate information. This is achieved by only aggregating paths in a fixed window for each source-target pair. We demonstrate that the complexity of TAGNet is independent of the number of layers. Extensive experiments demonstrate that TAGNet can cut down on the number of propagated messages by as m
    
[^66]: 通过离散扩散学习无监督的自动驾驶世界模型

    Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])

    [http://arxiv.org/abs/2311.01017](http://arxiv.org/abs/2311.01017)

    本论文提出了一种通过离散扩散学习无监督的自动驾驶世界模型的新方法，通过使用VQVAE对传感器观察进行标记化并通过离散扩散预测未来，我们的模型在点云观察中实现了显著改进，将1秒预测的SOTA Chamfer距离降低了65%以上。

    

    学习世界模型可以以无监督的方式教会智能体世界的运作方式。尽管它可以看作是序列建模的特殊情况，但在自动驾驶等机器人应用中，与使用生成预训练转换器（GPT）扩展语言模型相比，扩展世界模型的进展相对较慢。我们指出了两个主要瓶颈：处理复杂和无结构的观察空间以及具有可扩展性的生成模型。因此，我们提出了一种新颖的世界建模方法，首先使用VQVAE对传感器观察进行标记化，然后通过离散扩散预测未来。为了有效地并行解码和去噪标记，我们将遮蔽生成图像转换器转换为离散扩散框架，并进行了一些简单的改进，取得了显着的改进效果。当应用于点云观察的世界模型学习时，我们的模型将1秒预测的SOTA Chamfer距离降低了65%以上。

    Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
    
[^67]: Tensor Trust：来自在线游戏的可解释提示注入攻击

    Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game. (arXiv:2311.01011v1 [cs.LG])

    [http://arxiv.org/abs/2311.01011](http://arxiv.org/abs/2311.01011)

    这项研究提供了一个解释性的提示注入攻击的数据集，并利用该数据集创建了两种类型的抵抗基准。研究结果表明，许多模型容易受到这些攻击策略的攻击。

    

    随着大型语言模型（LLMs）在现实应用中的增加，它们仍然容易受到提示注入攻击的威胁：恶意第三方提示会扭曲系统设计者的意图。为了帮助研究人员研究此问题，我们提供了一个包含超过126,000个提示注入攻击和46,000个基于提示的“防御”（由一个名为Tensor Trust的在线游戏的玩家创建）的数据集。据我们所知，这是目前针对LLMs的最大的人工生成的对抗性示例数据集。我们的数据集中的攻击具有很多易于解释的结构，并揭示了LLMs的弱点。我们还利用该数据集创建了两种类型的提示注入抵抗基准，分别称为提示提取和提示劫持。我们的基准结果显示，许多模型容易受到Tensor Trust数据集中的攻击策略的攻击。此外，我们还表明，一些

    While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based "defenses" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some 
    
[^68]: 探索快速Shapley值估计的统一视角

    Exploring Unified Perspective For Fast Shapley Value Estimation. (arXiv:2311.01010v1 [cs.LG])

    [http://arxiv.org/abs/2311.01010](http://arxiv.org/abs/2311.01010)

    这篇论文探索了统一视角下快速计算Shapley值的方法，提出了一种简单高效的摊销估计器SimSHAP，通过消除冗余技术显著加速了准确Shapley值的计算。

    

    Shapley值已经成为一种被广泛接受和可靠的工具，它以理论公理为基础，用于解决深度神经网络等黑盒模型所带来的挑战。然而，在特征数目上，计算Shapley值会遇到指数级复杂性。已经探索了各种方法，包括ApproSemivalue、KernelSHAP和FastSHAP，以加速计算。我们分析了现有工作的一致性，并得出结论，即随机估计器可以统一为特征子集重要性抽样的线性变换。基于此，我们研究了设计简单摊销估计器的可能性，并提出了一种直接和高效的方法，即SimSHAP，通过消除冗余技术。在表格和图像数据集上进行的大量实验验证了我们的SimSHAP的有效性，它显着加速了准确Shapley值的计算。

    Shapley values have emerged as a widely accepted and trustworthy tool, grounded in theoretical axioms, for addressing challenges posed by black-box models like deep neural networks. However, computing Shapley values encounters exponential complexity in the number of features. Various approaches, including ApproSemivalue, KernelSHAP, and FastSHAP, have been explored to expedite the computation. We analyze the consistency of existing works and conclude that stochastic estimators can be unified as the linear transformation of importance sampling of feature subsets. Based on this, we investigate the possibility of designing simple amortized estimators and propose a straightforward and efficient one, SimSHAP, by eliminating redundant techniques. Extensive experiments conducted on tabular and image datasets validate the effectiveness of our SimSHAP, which significantly accelerates the computation of accurate Shapley values.
    
[^69]: 通过学习自然语言规则和引导来提高人工智能团队的效果

    Effective Human-AI Teams via Learned Natural Language Rules and Onboarding. (arXiv:2311.01007v1 [cs.LG])

    [http://arxiv.org/abs/2311.01007](http://arxiv.org/abs/2311.01007)

    本论文提出了一种通过学习自然语言规则和引导的方法，以提高人工智能团队的效果。通过找到数据的局部区域和使用语言模型进行描述，我们教导人类如何与AI合作。通过目标检测和问答任务的用户研究，我们证明了我们的方法可以使人工智能团队更加准确。

    

    人们越来越依赖于AI代理来帮助他们完成各种任务。人类必须知道何时依赖于代理，与代理合作或忽略其建议。在这项工作中，我们提出了一种通过数据区域和自然语言描述的学习规则的方法，以说明人类应该如何与AI合作。我们的新颖区域发现算法在嵌入空间中找到数据的局部区域作为邻域，纠正了人类的先验知识。然后，每个区域都通过迭代和对比过程进行描述，其中一个大型语言模型描述该区域。然后我们通过引导阶段将这些规则教给人类。通过在目标检测和问答任务上的用户研究，我们证明了我们的方法可以使人工智能团队更加准确。我们还分别评估了我们的区域发现和描述算法。

    People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately.
    
[^70]: 通过最大化重新标记准确性来进行鲁棒数据修剪

    Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy. (arXiv:2311.01002v1 [cs.LG])

    [http://arxiv.org/abs/2311.01002](http://arxiv.org/abs/2311.01002)

    该论文提出了一种通过最大化重新标记准确性来进行鲁棒数据修剪的算法，该算法能够找到一个子集，使得所有训练示例的邻域置信度之和最大化。这个方法在现代深度学习中具有重要的应用价值。

    

    数据修剪旨在将大型训练集缩减为一个小而信息丰富的子集，对于减少现代深度学习的巨大计算成本至关重要。尽管大规模数据集中不可避免地含有注释噪声，并且已经开发了许多鲁棒学习方法，但对于噪声鲁棒学习场景下的数据修剪几乎未受到关注。通过自校正错误标签的最新重新标记方法在训练过程中，很难确定哪个子集能够在整个训练集中引发最准确的重新标记。在本文中，我们形式化了重新标记的数据修剪问题。首先我们展示了一个训练示例被正确重新标记的可能性与其邻域中的预测置信度成比例。因此，我们提出了一种全新的数据修剪算法，名为Prune4Rel，它能找到一个子集，使得所有训练示例的邻域置信度之和最大化。

    Data pruning, which aims to downsize a large training set into a small informative subset, is crucial for reducing the enormous computational costs of modern deep learning. Though large-scale data collections invariably contain annotation noise and numerous robust learning methods have been developed, data pruning for the noise-robust learning scenario has received little attention. With state-of-the-art Re-labeling methods that self-correct erroneous labels while training, it is challenging to identify which subset induces the most accurate re-labeling of erroneous labels in the entire training set. In this paper, we formalize the problem of data pruning with re-labeling. We first show that the likelihood of a training example being correctly re-labeled is proportional to the prediction confidence of its neighborhood in the subset. Therefore, we propose a novel data pruning algorithm, Prune4Rel, that finds a subset maximizing the total neighborhood confidence of all training examples,
    
[^71]: 在零售领域中使用梯度提升树的可扩展概率预测：从实践者角度出发

    Scalable Probabilistic Forecasting in Retail with Gradient Boosted Trees: A Practitioner's Approach. (arXiv:2311.00993v1 [cs.LG])

    [http://arxiv.org/abs/2311.00993](http://arxiv.org/abs/2311.00993)

    本论文从实践者的角度出发，提出了在零售领域中使用梯度提升树进行可扩展概率预测的方法，通过研究两层次的层次结构和自上而下的预测方法，在处理大规模数据集和间断性数据方面具有较好的性能。

    

    最近的M5竞赛在零售预测领域推动了技术的发展。然而，我们注意到竞赛挑战与我们在一个大型电子商务公司面临的挑战之间存在重要的差异。我们场景中的数据集比较大（数十万个时间序列），而电子商务可以提供比实体零售商更多的产品选择，导致数据更加间断。为了在可行的计算工作量下扩展到更大的数据集大小，首先，我们研究了一个两层次的层次结构，并提出了一种自上而下的预测方法，在聚合级别上预测较少数量的系列和间断性，然后进行分解以获得决策级别的预测。概率预测是根据分布假设生成的。其次，直接对子样本进行下层级的训练也可以是一种扩展的替代方式。使用主要数据集评估了子集建模的性能。除了专有的数据集外，还使用了其他不同领域的数据集。

    The recent M5 competition has advanced the state-of-the-art in retail forecasting. However, we notice important differences between the competition challenge and the challenges we face in a large e-commerce company. The datasets in our scenario are larger (hundreds of thousands of time series), and e-commerce can afford to have a larger assortment than brick-and-mortar retailers, leading to more intermittent data. To scale to larger dataset sizes with feasible computational effort, firstly, we investigate a two-layer hierarchy and propose a top-down approach to forecasting at an aggregated level with less amount of series and intermittency, and then disaggregating to obtain the decision-level forecasts. Probabilistic forecasts are generated under distributional assumptions. Secondly, direct training at the lower level with subsamples can also be an alternative way of scaling. Performance of modelling with subsets is evaluated with the main dataset. Apart from a proprietary dataset, the
    
[^72]: 优化库存配送：一种基于神经网络的决策导向学习方法

    Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks. (arXiv:2311.00983v1 [cs.LG])

    [http://arxiv.org/abs/2311.00983](http://arxiv.org/abs/2311.00983)

    本文提出了一种基于决策导向学习的方法来解决库存配送问题，通过直接集成库存预测和路径优化，可能确保一个强大的供应链策略。

    

    库存配送问题（IRP）是供应链管理中的一个关键挑战，它涉及在考虑库存需求规划的不确定性的情况下优化有效的路径选择。为了解决IRP问题，通常采用两阶段的方法，首先使用机器学习技术预测需求，然后使用优化算法来最小化配送成本。我们的实验表明，机器学习模型在实现完美准确度方面存在不足，因为库存水平受动态业务环境的影响，进而影响到下一阶段的优化问题，导致次优决策。在本文中，我们提出了一种基于决策导向学习的方法来解决现实世界的IRP问题。这种方法在一个端到端的系统中直接集成了库存预测和路径优化，可能确保一个强大的供应链策略。

    Inventory Routing Problem (IRP) is a crucial challenge in supply chain management as it involves optimizing efficient route selection while considering the uncertainty of inventory demand planning. To solve IRPs, usually a two-stage approach is employed, where demand is predicted using machine learning techniques first, and then an optimization algorithm is used to minimize routing costs. Our experiment shows machine learning models fall short of achieving perfect accuracy because inventory levels are influenced by the dynamic business environment, which, in turn, affects the optimization problem in the next stage, resulting in sub-optimal decisions. In this paper, we formulate and propose a decision-focused learning-based approach to solving real-world IRPs. This approach directly integrates inventory prediction and routing optimization within an end-to-end system potentially ensuring a robust supply chain strategy.
    
[^73]: 用化学反应网络集合实现生成模型的自主学习

    Autonomous Learning of Generative Models with Chemical Reaction Network Ensembles. (arXiv:2311.00975v1 [q-bio.MN])

    [http://arxiv.org/abs/2311.00975](http://arxiv.org/abs/2311.00975)

    本论文通过化学反应网络实现了生成模型的自主学习，能够优化任何详细平衡的化学反应网络，并使用隐藏单元学习复杂分布。这个方法可以看作是积分反馈控制的一种形式，同时我们还能够推导出与学习过程相关的热力学成本和权衡。

    

    一颗微米大小的相互作用分子囊是否能够自主学习复杂且波动的环境的内部模型？我们从控制理论、机器学习理论、化学反应网络理论和统计物理学中汲取了经验，开发出一种通用架构，使得广泛类别的化学系统能够自主学习复杂分布。我们的构造采用了机器学习中优化的核心方法：相对熵损失函数上的梯度下降的化学实现。我们展示了如何应用这种方法来优化任何一个详细平衡的化学反应网络，并且这个构造能够使用隐藏单元来学习复杂分布。这个结果又被重新解释为积分反馈控制的形式。最后，由于我们使用了显式的物理学习模型，我们能够推导出与这个过程相关的热力学成本和权衡。

    Can a micron sized sack of interacting molecules autonomously learn an internal model of a complex and fluctuating environment? We draw insights from control theory, machine learning theory, chemical reaction network theory, and statistical physics to develop a general architecture whereby a broad class of chemical systems can autonomously learn complex distributions. Our construction takes the form of a chemical implementation of machine learning's optimization workhorse: gradient descent on the relative entropy cost function. We show how this method can be applied to optimize any detailed balanced chemical reaction network and that the construction is capable of using hidden units to learn complex distributions. This result is then recast as a form of integral feedback control. Finally, due to our use of an explicit physical model of learning, we are able to derive thermodynamic costs and trade-offs associated to this process.
    
[^74]: 具有有限对抗动作的联邦线性赌博机

    Federated Linear Bandits with Finite Adversarial Actions. (arXiv:2311.00973v1 [cs.LG])

    [http://arxiv.org/abs/2311.00973](http://arxiv.org/abs/2311.00973)

    文章研究了具有有限对抗动作的联邦线性赌博机模型，并提出了FedSupLinUCB算法，实现了$\tilde{O}(\sqrt{d T})$的总遗憾，并且通信成本可以被控制在$O(d M^2 \log(d)\log(T))$和$O(\sqrt{d^3 M^3} \log(d))$内。

    

    我们研究了一种联邦线性赌博机模型，其中$M$个客户端与中央服务器通信，解决具有不同对抗动作集的有限对抗动作集的线性上下文赌博机问题。为了解决对抗性有限动作集的独特挑战，我们提出了FedSupLinUCB算法，它在线性上下文赌博机中扩展了SupLinUCB和OFUL算法的原则。我们证明FedSupLinUCB的总遗憾为$\tilde{O}(\sqrt{d T})$，其中$T$是所有客户端的总臂拉次数，$d$是线性模型的环境维度。这与极小值下界相匹配，因此是最优的（多项式对数项）。我们研究了异步和同步两种情况，并展示了通信成本可以分别控制为$O(d M^2 \log(d) \log(T))$和$O(\sqrt{d^3 M^3} \log(d))$。FedSupLinUCB设计进一步扩展为两种情景：（1）方差自适应，总遗憾为$\tilde{O}(\sqrt{

    We study a federated linear bandits model, where $M$ clients communicate with a central server to solve a linear contextual bandits problem with finite adversarial action sets that may be different across clients. To address the unique challenges of adversarial finite action sets, we propose the FedSupLinUCB algorithm, which extends the principles of SupLinUCB and OFUL algorithms in linear contextual bandits. We prove that FedSupLinUCB achieves a total regret of $\tilde{O}(\sqrt{d T})$, where $T$ is the total number of arm pulls from all clients, and $d$ is the ambient dimension of the linear model. This matches the minimax lower bound and thus is order-optimal (up to polylog terms). We study both asynchronous and synchronous cases and show that the communication cost can be controlled as $O(d M^2 \log(d)\log(T))$ and $O(\sqrt{d^3 M^3} \log(d))$, respectively. The FedSupLinUCB design is further extended to two scenarios: (1) variance-adaptive, where a total regret of $\tilde{O} (\sqrt{
    
[^75]: 一个集成蒙特卡洛树搜索和监督学习的列车运行图问题综合框架

    An Integrated Framework Integrating Monte Carlo Tree Search and Supervised Learning for Train Timetabling Problem. (arXiv:2311.00971v1 [cs.LG])

    [http://arxiv.org/abs/2311.00971](http://arxiv.org/abs/2311.00971)

    提出了一个综合应用蒙特卡洛树搜索和监督学习的框架，用于解决列车运行图问题(TTP)。实验证明该框架在提高TTP的解决效率方面是有效的。

    

    单线铁路列车运行图问题(TTP)是一个重要且复杂的问题。本文提出了一个综合应用启发式方法、无监督学习方法和监督学习方法的蒙特卡洛树搜索(MCTS)计算框架，用于解决离散动作空间下的TTP。本文首先描述了TTP的数学模型和仿真系统动力学，并从MCTS的角度分析了解决方案的特点，提出了一些启发式方法以改进MCTS。本文将这些方法视为所提框架中的规划器。其次，本文利用深度卷积神经网络来近似节点的值，并进一步应用于MCTS搜索过程中，称为学习器。实验证明，所提出的启发式MCTS方法有助于解决TTP；将规划器和学习器集成到算法框架中可以提高解决TTP的数据效率；该算法框架还可以应用于其他类似的优化问题。

    The single-track railway train timetabling problem (TTP) is an important and complex problem. This article proposes an integrated Monte Carlo Tree Search (MCTS) computing framework that combines heuristic methods, unsupervised learning methods, and supervised learning methods for solving TTP in discrete action spaces. This article first describes the mathematical model and simulation system dynamics of TTP, analyzes the characteristics of the solution from the perspective of MCTS, and proposes some heuristic methods to improve MCTS. This article considers these methods as planners in the proposed framework. Secondly, this article utilizes deep convolutional neural networks to approximate the value of nodes and further applies them to the MCTS search process, referred to as learners. The experiment shows that the proposed heuristic MCTS method is beneficial for solving TTP; The algorithm framework that integrates planners and learners can improve the data efficiency of solving TTP; The 
    
[^76]: 不变特征子空间恢复：一类新的可证明的领域泛化算法

    Invariant-Feature Subspace Recovery: A New Class of Provable Domain Generalization Algorithms. (arXiv:2311.00966v1 [cs.LG])

    [http://arxiv.org/abs/2311.00966](http://arxiv.org/abs/2311.00966)

    ISR是一种新型的可证明的领域泛化算法，它可以通过类条件分布的一阶矩来识别不变特征所张成的子空间。

    

    领域泛化要求在一组训练环境中训练的模型能够在未知的测试环境中良好地泛化。最近，一系列算法，如不变风险最小化（IRM），已被提出用于领域泛化。然而，Rosenfeld等人（2021）表明，在一个简单的线性数据模型中，即使忽略了非凸性问题，IRM及其扩展也无法对具有少于$d_s+1$个训练环境的未知环境进行泛化，其中$d_s$是虚假特征子空间的维度。在这项工作中，我们提出了不变特征子空间恢复（ISR）：一类新的算法，用于在分类和回归问题设置中实现可证明的领域泛化。首先，在Rosenfeld等人（2021）的二分类设置中，我们展示了我们的第一个算法ISR-Mean，它可以通过类条件分布的一阶矩来识别不变特征所张成的子空间，并实现可证明的泛化。

    Domain generalization asks for models trained over a set of training environments to generalize well in unseen test environments. Recently, a series of algorithms such as Invariant Risk Minimization (IRM) have been proposed for domain generalization. However, Rosenfeld et al. (2021) shows that in a simple linear data model, even if non-convexity issues are ignored, IRM and its extensions cannot generalize to unseen environments with less than $d_s+1$ training environments, where $d_s$ is the dimension of the spurious-feature subspace. In this work, we propose Invariant-feature Subspace Recovery (ISR): a new class of algorithms to achieve provable domain generalization across the settings of classification and regression problems. First, in the binary classification setup of Rosenfeld et al. (2021), we show that our first algorithm, ISR-Mean, can identify the subspace spanned by invariant features from the first-order moments of the class-conditional distributions, and achieve provable 
    
[^77]: 在金融科技应用中寻找双目标 Pareto 最优欺诈预防规则集

    On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications. (arXiv:2311.00964v1 [cs.LG])

    [http://arxiv.org/abs/2311.00964](http://arxiv.org/abs/2311.00964)

    本文研究了在金融科技应用中寻找高质量的双目标 Pareto 最优欺诈预防规则集的问题。通过采用 Pareto 最优性概念和启发式框架 PORS，我们成功提出了一组非支配的规则子集，并通过实证评估证明了其有效性。

    

    规则在金融科技机构中被广泛用于进行欺诈预防决策，因为规则具有直观的 if-then 结构，易于理解。在实践中，大型金融科技机构通常采用两阶段欺诈预防决策规则集挖掘框架。本文关注于从初始规则集中找到高质量的规则子集，以双目标空间（如精确率和召回率）为基础。为此，我们采用 Pareto 最优性概念，旨在找到一组非支配的规则子集，构成一个 Pareto 前沿。我们提出了一个基于启发式的框架 PORS，并确定了 PORS 的核心问题是前沿解决方案选择（SSF）问题。我们对 SSF 问题进行了系统分类，并在公开和专有数据集上进行了全面的实证评估。我们还引入了一种名为 SpectralRules 的新颖变体的顺序覆盖算法，以鼓励规则的多样性。

    Rules are widely used in Fintech institutions to make fraud prevention decisions, since rules are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions. This paper is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall) from an initial pool of rules. To this end, we adopt the concept of Pareto optimality and aim to find a set of non-dominated rule subsets, which constitutes a Pareto front. We propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. We also introduce a novel variant of sequential covering algorithm called SpectralRules to encourage the diver
    
[^78]: 基于强化学习的动态公平联邦学习

    Dynamic Fair Federated Learning Based on Reinforcement Learning. (arXiv:2311.00959v1 [cs.LG])

    [http://arxiv.org/abs/2311.00959](http://arxiv.org/abs/2311.00959)

    本研究提出了一种基于强化学习的动态公平联邦学习算法(DQFFL)，旨在解决联邦学习中的公平性问题。该算法通过动态调整参数，减轻设备聚合过程中的差异，提升对所有群体的公平处理。

    

    联邦学习使得一组设备能够进行协作训练和优化全局模型，而无需共享本地数据样本。然而，在联邦学习中数据的异质性可能导致全局模型在不同设备上的不公平表示。为了解决联邦学习中的公平问题，我们提出了一种基于强化学习的动态q公平联邦学习算法，称为DQFFL。DQFFL旨在减轻设备聚合中的差异，并提升对参与联邦学习的所有群体的公平处理。为了量化公平性，DQFFL利用全局联邦模型在每个设备上的性能，并将α-公平性纳入聚合过程中客户端权重的分布，从而实现公平性的保持。考虑到衡量公平性的参数的敏感性，我们提出利用强化学习来动态调整参数。

    Federated learning enables a collaborative training and optimization of global models among a group of devices without sharing local data samples. However, the heterogeneity of data in federated learning can lead to unfair representation of the global model across different devices. To address the fairness issue in federated learning, we propose a dynamic q fairness federated learning algorithm with reinforcement learning, called DQFFL. DQFFL aims to mitigate the discrepancies in device aggregation and enhance the fairness of treatment for all groups involved in federated learning. To quantify fairness, DQFFL leverages the performance of the global federated model on each device and incorporates {\alpha}-fairness to transform the preservation of fairness during federated aggregation into the distribution of client weights in the aggregation process. Considering the sensitivity of parameters in measuring fairness, we propose to utilize reinforcement learning for dynamic parameters durin
    
[^79]: E3 TTS: 简单高效的端到端基于扩散的文本到语音模型

    E3 TTS: Easy End-to-End Diffusion-based Text to Speech. (arXiv:2311.00945v1 [cs.SD])

    [http://arxiv.org/abs/2311.00945](http://arxiv.org/abs/2311.00945)

    E3 TTS是一种简单高效的端到端基于扩散的文本到语音模型，不依赖于中间表示，通过扩散过程建模波形的时间结构，能够轻松适应零样本任务。

    

    我们提出了一种简单高效的端到端基于扩散的文本到语音模型，称为E3 TTS。E3 TTS直接接受纯文本作为输入，并通过迭代细化过程生成音频波形。与许多先前的工作不同，E3 TTS不依赖于任何中间表示，如频谱特征或对齐信息。相反，E3 TTS通过扩散过程建模波形的时间结构。不依赖于额外的条件信息，E3 TTS可以支持给定音频中的灵活潜在结构。这使得E3 TTS能够轻松适应零样本任务，例如在没有额外训练的情况下进行编辑。实验证明，E3 TTS能够生成高保真度音频，接近最先进的神经TTS系统的性能。音频样本可在https://e3tts.github.io上获得。

    We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.
    
[^80]: 基于随机平滑梯度上升下降法的联邦极小极大优化研究

    Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization. (arXiv:2311.00944v1 [stat.ML])

    [http://arxiv.org/abs/2311.00944](http://arxiv.org/abs/2311.00944)

    本论文提出了一种新算法，FESS-GDA，利用平滑技术进行联邦极小极大优化。通过解决不同类型的联邦极小极大问题，我们证明了FESS-GDA的收敛性，并展示了其在实际联邦学习任务中的实际效果。

    

    近年来，由于其在各种机器学习任务中的广泛应用，联邦极小极大优化引起了越来越多的关注。虽然在集中非凸极小极大优化中，平滑交替梯度上升下降（Smoothed-AGDA）已经证明了其成功之处，但平滑技术在联邦设置中的作用和是否有所帮助尚未被探究。在本文中，我们提出了一种新算法，称为联邦随机平滑梯度上升下降（FESS-GDA），该算法利用平滑技术进行联邦极小极大优化。我们证明了FESS-GDA可以统一解决几类联邦极小极大问题，并为这些设置提供了新的或更好的收敛结果分析。我们展示了FESS-GDA在实际联邦学习任务中，如生成对抗网络（GANs）的训练和公平分类中的实际效率。

    In recent years, federated minimax optimization has attracted growing interest due to its extensive applications in various machine learning tasks. While Smoothed Alternative Gradient Descent Ascent (Smoothed-AGDA) has proved its success in centralized nonconvex minimax optimization, how and whether smoothing technique could be helpful in federated setting remains unexplored. In this paper, we propose a new algorithm termed Federated Stochastic Smoothed Gradient Descent Ascent (FESS-GDA), which utilizes the smoothing technique for federated minimax optimization. We prove that FESS-GDA can be uniformly used to solve several classes of federated minimax problems and prove new or better analytical convergence results for these settings. We showcase the practical efficiency of FESS-GDA in practical federated learning tasks of training generative adversarial networks (GANs) and fair classification.
    
[^81]: 高斯混合解算器用于扩散模型

    Gaussian Mixture Solvers for Diffusion Models. (arXiv:2311.00941v1 [cs.LG])

    [http://arxiv.org/abs/2311.00941](http://arxiv.org/abs/2311.00941)

    这篇论文提出了一种名为高斯混合解算器(GMS)的新型SDE-based解算器用于扩散模型，通过估计前三阶矩并优化高斯混合参数来解决现有SDE-based解算器的效率-有效性困境问题。

    

    最近，扩散模型在生成任务中取得了巨大的成功。从扩散模型中采样等价于解决反扩散随机微分方程（SDEs）或相应的概率流常微分方程（ODEs）。与之相比，基于SDE的解算器可以生成更高质量的样本，并适用于基于笔划合成的图像转换任务。然而，在推断过程中，现有的基于SDE的解算器受到效率-有效性困境的严重限制。我们的调查表明，这是因为反向转换核中的高斯假设在有限数量的离散化步骤中经常被违反（即使在简单混合数据的情况下）。为了克服这个限制，我们引入了一种新的基于SDE的解算器类，称为高斯混合解算器（GMS）用于扩散模型。我们的解算器估计了前三阶矩，并优化了高斯混合的参数

    Recently, diffusion models have achieved great success in generative tasks. Sampling from diffusion models is equivalent to solving the reverse diffusion stochastic differential equations (SDEs) or the corresponding probability flow ordinary differential equations (ODEs). In comparison, SDE-based solvers can generate samples of higher quality and are suited for image translation tasks like stroke-based synthesis. During inference, however, existing SDE-based solvers are severely constrained by the efficiency-effectiveness dilemma. Our investigation suggests that this is because the Gaussian assumption in the reverse transition kernel is frequently violated (even in the case of simple mixture data) given a limited number of discretization steps. To overcome this limitation, we introduce a novel class of SDE-based solvers called \emph{Gaussian Mixture Solvers (GMS)} for diffusion models. Our solver estimates the first three-order moments and optimizes the parameters of a Gaussian mixture
    
[^82]: 缩小差距：解决分类模型训练中的差异问题以实现无分类器引导

    Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance. (arXiv:2311.00938v1 [cs.LG])

    [http://arxiv.org/abs/2311.00938](http://arxiv.org/abs/2311.00938)

    本文介绍了一种更新的损失函数，以更好地对齐传统训练方法与扩散模型所期望的条件采样行为之间的差异。实验证明该方法能够以更少的采样时间步长生成更高质量的样本，并对于引导规模的选择更具鲁棒性。

    

    扩散模型已经成为生成模型中的一个重要进展，为生成的实例质量设定了新的标准。本文旨在强调传统训练方法与这些模型所期望的条件采样行为之间存在的差异。虽然流行的无分类器引导技术效果不错，但也存在一些缺陷。在引导规模参数$w$取较高值时，我们经常得到分布之外的样本和模式崩溃，而在$w$取较低值时，可能无法获得所期望的特异性。为了解决这些挑战，我们引入了一种更新的损失函数，更好地将训练目标与采样行为对齐。在CIFAR-10上的FID分数的实验证明了我们的方法能够以较少的采样时间步长生成更高质量的样本，并且对于引导规模$w$的选择更具鲁棒性。我们还尝试了在提出的损失上对稳定性扩散进行微调，以提供e。

    Diffusion models have emerged as a pivotal advancement in generative models, setting new standards to the quality of the generated instances. In the current paper we aim to underscore a discrepancy between conventional training methods and the desired conditional sampling behavior of these models. While the prevalent classifier-free guidance technique works well, it's not without flaws. At higher values for the guidance scale parameter $w$, we often get out of distribution samples and mode collapse, whereas at lower values for $w$ we may not get the desired specificity. To address these challenges, we introduce an updated loss function that better aligns training objectives with sampling behaviors. Experimental validation with FID scores on CIFAR-10 elucidates our method's ability to produce higher quality samples with fewer sampling timesteps, and be more robust to the choice of guidance scale $w$. We also experiment with fine-tuning Stable Diffusion on the proposed loss, to provide e
    
[^83]: SatBird: 利用遥感和公众科学数据进行鸟类物种分布建模

    SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data. (arXiv:2311.00936v1 [cs.LG])

    [http://arxiv.org/abs/2311.00936](http://arxiv.org/abs/2311.00936)

    利用遥感和公众科学数据进行鸟类物种分布建模，填补了传统方法中的知识空白，为改善生物多样性监测和复杂生态系统建模提供了机会。

    

    生物多样性正以前所未有的速度下降，影响到食物、水和人类健康和福祉的生态系统服务。了解物种及其栖息地的分布对于保护政策规划非常重要。然而，在物种分布模型 (SDMs) 中，传统的生态学方法通常仅关注少量物种或狭窄的地理区域，并且对物种分布仍存在重大知识空白。其中一个主要原因是传统监测所需的工作量和专业知识限制了传统数据的可用性。遥感数据的广泛可用性以及公众科学工具在低成本收集物种观察数据方面的日益普及为改善生物多样性监测和复杂生态系统建模提供了机会。我们介绍了一项新的任务，即通过预测鸟类物种到其栖息地的映射来实现鸟类物种分布的绘制。

    Biodiversity is declining at an unprecedented rate, impacting ecosystem services necessary to ensure food, water, and human health and well-being. Understanding the distribution of species and their habitats is crucial for conservation policy planning. However, traditional methods in ecology for species distribution models (SDMs) generally focus either on narrow sets of species or narrow geographical areas and there remain significant knowledge gaps about the distribution of species. A major reason for this is the limited availability of data traditionally used, due to the prohibitive amount of effort and expertise required for traditional field monitoring. The wide availability of remote sensing data and the growing adoption of citizen science tools to collect species observations data at low cost offer an opportunity for improving biodiversity monitoring and enabling the modelling of complex ecosystems. We introduce a novel task for mapping bird species to their habitats by predictin
    
[^84]: 从不真实数据中学习缺陷预测

    Learning Defect Prediction from Unrealistic Data. (arXiv:2311.00931v1 [cs.LG])

    [http://arxiv.org/abs/2311.00931](http://arxiv.org/abs/2311.00931)

    该论文研究了从不真实的数据集中学习缺陷预测的问题，并提出了一种基于学习表示的方法来识别与真实数据集最相似的子集。

    

    预训练的代码模型，如CodeBERT和CodeT5，成为代码理解和生成任务的流行选择。这些模型往往庞大且需要相应数量的训练数据，但在下游任务中很少提供。相反，使用远比真实数据集更大但不真实的数据集（如人为注入缺陷的函数）来训练模型已经变得流行。然而，使用此类数据训练的模型往往只在类似数据上表现良好，在真实世界程序上表现不佳。本文假设这种差异是由于存在干扰样本，这些样本使模型偏离了真实世界任务分布。为了验证这个假设，我们提出了一种基于学习表示的方法，用于识别这些大而不真实的数据集中与真实数据集中的示例最相似的子集。我们的方法提取了真实世界和人工程序的高维嵌入表示。

    Pretrained models of code, such as CodeBERT and CodeT5, have become popular choices for code understanding and generation tasks. Such models tend to be large and require commensurate volumes of training data, which are rarely available for downstream tasks. Instead, it has become popular to train models with far larger but less realistic datasets, such as functions with artificially injected bugs. Models trained on such data, however, tend to only perform well on similar data, while underperforming on real world programs. In this paper, we conjecture that this discrepancy stems from the presence of distracting samples that steer the model away from the real-world task distribution. To investigate this conjecture, we propose an approach for identifying the subsets of these large yet unrealistic datasets that are most similar to examples in real-world datasets based on their learned representations. Our approach extracts high-dimensional embeddings of both real-world and artificial progr
    
[^85]: 可扩展的多变量因果模型中的反事实分布估计

    Scalable Counterfactual Distribution Estimation in Multivariate Causal Models. (arXiv:2311.00927v1 [stat.ML])

    [http://arxiv.org/abs/2311.00927](http://arxiv.org/abs/2311.00927)

    该论文提出了一种可扩展的方法，用于在多变量因果模型中估计多个感兴趣量的反事实联合分布。通过利用原始高维空间中的一维潜在子空间和单一变量因果模型，该方法可以同时处理多变量结果的相关结构并产生准确的反事实分布估计。

    

    我们考虑了在经典的差异差异设计的基础上扩展的多变量因果模型中估计多个感兴趣量（例如结果）的反事实联合分布的问题。现有的方法要么忽略多变量结果各维度间的相关结构，通过在每个维度上考虑单一变量因果模型而产生错误的反事实分布；要么在直接处理这种多变量因果模型时，在中等大小的数据集上表现不佳。我们提出了一种方法，可以同时减轻这两个问题，方法是利用原始高维空间中鲁棒的一维潜在子空间，并利用单一变量因果模型在该空间上的高效估计。由于一维子空间的构建使用了来自所有维度的信息，我们的方法可以捕捉相关结构并产生反事实分布的良好估计。

    We consider the problem of estimating the counterfactual joint distribution of multiple quantities of interests (e.g., outcomes) in a multivariate causal model extended from the classical difference-in-difference design. Existing methods for this task either ignore the correlation structures among dimensions of the multivariate outcome by considering univariate causal models on each dimension separately and hence produce incorrect counterfactual distributions, or poorly scale even for moderate-size datasets when directly dealing with such multivariate causal model. We propose a method that alleviates both issues simultaneously by leveraging a robust latent one-dimensional subspace of the original high-dimension space and exploiting the efficient estimation from the univariate causal model on such space. Since the construction of the one-dimensional subspace uses information from all the dimensions, our method can capture the correlation structures and produce good estimates of the coun
    
[^86]: 深度因果模型的不同因果结构和表示的综述和路线图

    A Review and Roadmap of Deep Causal Model from Different Causal Structures and Representations. (arXiv:2311.00923v1 [cs.LG])

    [http://arxiv.org/abs/2311.00923](http://arxiv.org/abs/2311.00923)

    本研究综述了深度因果模型在不同因果结构和表示方面的应用，并提出了明确数据、半明确数据和不明确数据三种因果数据范例的定义，以应对将原始的因果概念和理论拓展到复杂非统计数据的挑战。

    

    将因果模型与深度学习相结合，引入越来越复杂的数据集，例如图像内部的因果关联或文本组件之间的因果关联，成为一个重要的研究领域。然而，将原始的因果概念和理论拓展到这种复杂的非统计数据中遇到了严重的挑战。为此，我们的研究提出了从因果结构和表示的角度对因果数据进行重新定义，包括明确数据、半明确数据和不明确数据。明确数据主要涉及传统因果场景中使用的统计数据，而半明确数据涉及与深度学习相关的一系列数据格式，包括时间序列、图像、文本等。不明确数据是我们推断的一个新兴研究领域，其来源是数据形式的演变。为了全面介绍这三种数据范例，我们详细阐述了它们的形式定义和差异。

    The fusion of causal models with deep learning introducing increasingly intricate data sets, such as the causal associations within images or between textual components, has surfaced as a focal research area. Nonetheless, the broadening of original causal concepts and theories to such complex, non-statistical data has been met with serious challenges. In response, our study proposes redefinitions of causal data into three distinct categories from the standpoint of causal structure and representation: definite data, semi-definite data, and indefinite data. Definite data chiefly pertains to statistical data used in conventional causal scenarios, while semi-definite data refers to a spectrum of data formats germane to deep learning, including time-series, images, text, and others. Indefinite data is an emergent research sphere inferred from the progression of data forms by us. To comprehensively present these three data paradigms, we elaborate on their formal definitions, differences mani
    
[^87]: MIST: 通过成员不变子空间训练对抗成员推理攻击

    MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training. (arXiv:2311.00919v1 [cs.CR])

    [http://arxiv.org/abs/2311.00919](http://arxiv.org/abs/2311.00919)

    通过成员不变子空间训练的MIST算法有效防御成员推理攻击，能够识别容易受到攻击的实例并避免过度拟合。

    

    在成员推理（MI）攻击中，对手试图确定一个实例是否被用来训练一个机器学习（ML）模型。MI攻击是在使用私有数据训练ML模型时的一个主要隐私问题。文献中的大多数MI攻击利用了ML模型被训练得很好以适应训练数据的特点，因此在训练实例上具有非常低的损失。因此，大多数对抗MI攻击的防御方法试图使模型在训练数据上的拟合程度降低。然而，这样做通常会导致较低的准确率。我们观察到训练实例对MI攻击具有不同程度的脆弱性。大多数实例即使不包含在训练中也会有低的损失。对于这些实例，模型可以很好地适应它们而不用担心MI攻击。有效的防御只需要（可能是隐式地）识别出容易受到MI攻击的实例，并避免过度拟合。一个主要的挑战是如何在高效的训练过程中实现这样的效果。

    In Member Inference (MI) attacks, the adversary try to determine whether an instance is used to train a machine learning (ML) model. MI attacks are a major privacy concern when using private data to train ML models. Most MI attacks in the literature take advantage of the fact that ML models are trained to fit the training data well, and thus have very low loss on training instances. Most defenses against MI attacks therefore try to make the model fit the training data less well. Doing so, however, generally results in lower accuracy. We observe that training instances have different degrees of vulnerability to MI attacks. Most instances will have low loss even when not included in training. For these instances, the model can fit them well without concerns of MI attacks. An effective defense only needs to (possibly implicitly) identify instances that are vulnerable to MI attacks and avoids overfitting them. A major challenge is how to achieve such an effect in an efficient training proc
    
[^88]: 重新加权标记：一种简单且有效的适用于命名实体识别的主动学习策略

    Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition. (arXiv:2311.00906v1 [cs.CL])

    [http://arxiv.org/abs/2311.00906](http://arxiv.org/abs/2311.00906)

    本文提出了一种重新加权的主动学习策略，通过为每个标记分配动态平滑的权重，解决了命名实体识别中的数据不平衡问题，取得了显著的性能提升。

    

    主动学习是一种被广泛应用于文本和图像分类任务中，通过有限的注释资源提升机器学习模型性能的技术。然而，在命名实体识别领域，主动学习相对较少被关注。命名实体识别中的数据不平衡问题妨碍了主动学习的效果，因为序列标记模型缺乏足够的学习信号。为了解决这些挑战，本文提出了一种基于重新加权的主动学习策略，为每个标记分配了动态平滑的权重。这种适应性策略与各种标记级采集函数兼容，并有助于开发健壮的主动学习器。在多个语料库上的实验结果表明，将我们的重新加权策略与现有的采集函数结合起来能够显著提高性能，验证了其实际有效性。

    Active learning, a widely adopted technique for enhancing machine learning models in text and image classification tasks with limited annotation resources, has received relatively little attention in the domain of Named Entity Recognition (NER). The challenge of data imbalance in NER has hindered the effectiveness of active learning, as sequence labellers lack sufficient learning signals. To address these challenges, this paper presents a novel reweighting-based active learning strategy that assigns dynamic smoothed weights to individual tokens. This adaptable strategy is compatible with various token-level acquisition functions and contributes to the development of robust active learners. Experimental results on multiple corpora demonstrate the substantial performance improvement achieved by incorporating our re-weighting strategy into existing acquisition functions, validating its practical efficacy.
    
[^89]: 通过将高斯过程与低维交互结构相结合，基于数据驱动的选择二阶粒子动力学模型

    Data-Driven Model Selections of Second-Order Particle Dynamics via Integrating Gaussian Processes with Low-Dimensional Interacting Structures. (arXiv:2311.00902v1 [stat.ML])

    [http://arxiv.org/abs/2311.00902](http://arxiv.org/abs/2311.00902)

    本文提出了一种将高斯过程与低维交互结构相结合的基于数据驱动的二阶粒子动力学模型选择方法，该方法能够处理相互作用代理的聚合和集体行为，具有较高的可扩展性和不确定性量化能力。

    

    本文重点研究了一种基于数据驱动的通用二阶粒子模型的发现，该模型包含了许多用于建模相似大小和体型的相互作用代理的聚合和集体行为的最新模型。该模型采用了由两个相互作用核参数化的高维常微分方程系统的形式，这些核评估了位置和速度的对齐。我们提出了一种基于高斯过程的方法来解决这个问题，其中未知的模型参数通过使用两个独立的高斯过程先验进行边缘化，这些先验约束在动力学和观测数据上。这导致一个非参数模型，用于考虑不确定性量化的相互作用动力学系统。我们还开发了加速技术来提高可扩展性。此外，我们进行了理论分析来解释方法论并研究核可以满足的条件。

    In this paper, we focus on the data-driven discovery of a general second-order particle-based model that contains many state-of-the-art models for modeling the aggregation and collective behavior of interacting agents of similar size and body type. This model takes the form of a high-dimensional system of ordinary differential equations parameterized by two interaction kernels that appraise the alignment of positions and velocities. We propose a Gaussian Process-based approach to this problem, where the unknown model parameters are marginalized by using two independent Gaussian Process (GP) priors on latent interaction kernels constrained to dynamics and observational data. This results in a nonparametric model for interacting dynamical systems that accounts for uncertainty quantification. We also develop acceleration techniques to improve scalability. Moreover, we perform a theoretical analysis to interpret the methodology and investigate the conditions under which the kernels can be 
    
[^90]: COSTAR: 使用自监督学习改进的时间反事实估计

    COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning. (arXiv:2311.00886v1 [cs.LG])

    [http://arxiv.org/abs/2311.00886](http://arxiv.org/abs/2311.00886)

    这项研究提出了一种名为COSTAR的新方法，通过整合自监督学习，改进了时间反事实结果的估计。该方法在处理时间相关混淆因素时结合了时间和特征关注以及分量对比损失，相比现有模型在估计准确性和对分布之外数据的泛化能力方面表现出更优越的性能。

    

    在许多领域，如医疗保健和电子商务，在观察到的历史数据中估计时间反事实结果对决策至关重要，特别是随机对照试验(RCTs)成本高或者不可行的情况下。对于现实世界的数据集，由于复杂的动态，长范围依赖性以及过去的治疗和协变量对未来结果的影响，建模时间相关的混淆因素是具有挑战性的。在本文中，我们引入了COunterfactual Self-supervised TrAnsformeR（COSTAR），一种整合了自监督学习来改进历史表示的新方法。所提出的框架将时间和特征关注与针对时间处理结果观察的分量对比损失相结合，与现有模型相比，在估计准确性和泛化到分布之外数据方面具有优越性能，通过对合成数据集和现实世界数据集的实证结果验证该优越性能。

    Estimation of temporal counterfactual outcomes from observed history is crucial for decision-making in many domains such as healthcare and e-commerce, particularly when randomized controlled trials (RCTs) suffer from high cost or impracticality. For real-world datasets, modeling time-dependent confounders is challenging due to complex dynamics, long-range dependencies and both past treatments and covariates affecting the future outcomes. In this paper, we introduce COunterfactual Self-supervised TrAnsformeR (COSTAR), a novel approach that integrates self-supervised learning for improved historical representations. The proposed framework combines temporal and feature-wise attention with a component-wise contrastive loss tailored for temporal treatment outcome observations, yielding superior performance in estimation accuracy and generalization to out-of-distribution data compared to existing models, as validated by empirical results on both synthetic and real-world datasets.
    
[^91]: SCPO: 安全批判策略优化下的安全强化学习

    SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization. (arXiv:2311.00880v1 [cs.LG])

    [http://arxiv.org/abs/2311.00880](http://arxiv.org/abs/2311.00880)

    SCPO是一种安全强化学习算法，通过引入安全批判器来确保遵守安全约束并平衡回报的最大化。

    

    将安全性纳入强化学习在现实场景中的实际应用中是必不可少的。为了应对这一挑战，我们利用了约束马尔可夫决策过程（CMDPs），引入了表示安全违规的独立成本函数。在CMDPs的设置中，先前的算法采用了拉格朗日松弛技术将约束优化问题转化为无约束对偶问题。然而，这些算法可能会不准确地预测不安全行为，导致在学习拉格朗日乘子时产生不稳定性。本研究介绍了一种新的安全强化学习算法——安全批判策略优化（SCPO）。在本研究中，我们定义了安全批判器，一种通过违反安全约束而获得的奖励被抵消的机制。此外，我们的理论分析表明，所提出的算法可以自动平衡在遵守安全约束和最大化回报之间的权衡。

    Incorporating safety is an essential prerequisite for broadening the practical applications of reinforcement learning in real-world scenarios. To tackle this challenge, Constrained Markov Decision Processes (CMDPs) are leveraged, which introduce a distinct cost function representing safety violations. In CMDPs' settings, Lagrangian relaxation technique has been employed in previous algorithms to convert constrained optimization problems into unconstrained dual problems. However, these algorithms may inaccurately predict unsafe behavior, resulting in instability while learning the Lagrange multiplier. This study introduces a novel safe reinforcement learning algorithm, Safety Critic Policy Optimization (SCPO). In this study, we define the safety critic, a mechanism that nullifies rewards obtained through violating safety constraints. Furthermore, our theoretical analysis indicates that the proposed algorithm can automatically balance the trade-off between adhering to safety constraints 
    
[^92]: 从观察中学习集体行为

    Learning Collective Behaviors from Observation. (arXiv:2311.00875v1 [cs.LG])

    [http://arxiv.org/abs/2311.00875](http://arxiv.org/abs/2311.00875)

    本文介绍了一系列学习方法，用于从观察中识别动态系统的结构，并理解复杂系统中相互作用代理的 emergent behaviors。这些方法不仅具备理论收敛保证，还能高效处理高维观测数据，解决观测/随机噪声、复杂的交互规则、丢失的交互特征和现实世界观测数据等问题。通过采用变分逆问题方法设计合适的损失函数，我们的学习方法具备降维能力。

    

    我们提出了一系列学习方法的综述，用于识别动态系统的结构，旨在理解相互作用代理系统中的 emergent behaviors。这些方法不仅提供了收敛的理论保证，还展示了处理高维观测数据的计算效率。它们可以处理来自一阶和二阶动态系统的观测数据，考虑观测/随机噪声、复杂的交互规则、丢失的交互特征以及与现实世界中相互作用代理系统的观测数据。发展这一系列学习方法的核心在于使用变分逆问题方法设计适当的损失函数，这种方法天然地提供了我们学习方法的降维能力。

    We present a review of a series of learning methods used to identify the structure of dynamical systems, aiming to understand emergent behaviors in complex systems of interacting agents. These methods not only offer theoretical guarantees of convergence but also demonstrate computational efficiency in handling high-dimensional observational data. They can manage observation data from both first- and second-order dynamical systems, accounting for observation/stochastic noise, complex interaction rules, missing interaction features, and real-world observations of interacting agent systems. The essence of developing such a series of learning methods lies in designing appropriate loss functions using the variational inverse problem approach, which inherently provides dimension reduction capabilities to our learning methods.
    
[^93]: 在CPU上的低延迟实时语音转换

    Low-latency Real-time Voice Conversion on CPU. (arXiv:2311.00873v1 [cs.SD])

    [http://arxiv.org/abs/2311.00873](http://arxiv.org/abs/2311.00873)

    这项研究实现了在CPU上进行低延迟实时语音转换，通过适应之前的音频神经网络架构，并使用对抗生成网络和知识蒸馏，模型LLVC在16kHz的比特率下具有不到20ms的延迟，并在消费级CPU上以2.8倍的速度运行。同时，LLVC还是所有开源语音转换模型中资源使用最低延迟最低的。

    

    我们将之前的音频处理和生成神经网络的架构适应到实时任意到一的语音转换任务中。我们的模型LLVC（低延迟低资源语音转换）在16kHz的比特率下具有不到20ms的延迟，在消费级CPU上运行速度几乎是实时的2.8倍。LLVC使用了对抗生成网络的架构和知识蒸馏来实现这种性能。据我们所知，LLVC是所有开源语音转换模型中资源使用最低延迟最低的。我们在https://github.com/KoeAI/LLVC提供开源样例、代码和预训练模型权重。

    We adapt the architectures of previous audio manipulation and generation neural networks to the task of real-time any-to-one voice conversion. Our resulting model, LLVC ($\textbf{L}$ow-latency $\textbf{L}$ow-resource $\textbf{V}$oice $\textbf{C}$onversion), has a latency of under 20ms at a bitrate of 16kHz and runs nearly 2.8x faster than real-time on a consumer CPU. LLVC uses both a generative adversarial architecture as well as knowledge distillation in order to attain this performance. To our knowledge LLVC achieves both the lowest resource usage as well as the lowest latency of any open-source voice conversion model. We provide open-source samples, code, and pretrained model weights at https://github.com/KoeAI/LLVC.
    
[^94]: 预训练数据混合使得Transformer模型具备狭窄的模型选择能力

    Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models. (arXiv:2311.00871v1 [cs.LG])

    [http://arxiv.org/abs/2311.00871](http://arxiv.org/abs/2311.00871)

    Transformer模型通过预训练数据混合实现了狭窄的模型选择能力，能够在上下文中识别和学习不同的任务，但对于任务或函数的处理相对有限。

    

    Transformer模型，特别是大型语言模型（LLM），具有在上下文中学习（ICL）的显著能力-在未经过任何明确模型训练的情况下，根据未见过的输入-输出例子执行新的任务。本研究探讨了Transformer模型如何有效地在其预训练数据混合中建立桥梁，以在上下文中识别和学习既包括预训练分布内又包括其外的新任务。在之前的研究基础上，我们在一个受控的环境中进行了实验，我们研究了基于$(x, f(x))$对序列而不是自然语言进行训练的Transformer模型。我们的实证结果表明，Transformer模型在无监督模型选择能力方面表现接近最优，在能够首先在上下文中识别不同的任务族群并在其中进行学习时（任务族群在预训练数据中有很好的表示）。然而，当面对任务或函数时，情况会稍有不同。

    Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of $(x, f(x))$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functi
    
[^95]: 超越结构稀疏性的非线性ICA的泛化

    Generalizing Nonlinear ICA Beyond Structural Sparsity. (arXiv:2311.00866v1 [cs.LG])

    [http://arxiv.org/abs/2311.00866](http://arxiv.org/abs/2311.00866)

    本论文研究了超越结构稀疏性的非线性ICA的泛化问题，提出了一组新的在不完备性、部分稀疏性、源依赖性和灵活的分组结构下的可辨识性结果。

    

    非线性独立成分分析（ICA）旨在从可观测的非线性混合中揭示真正的潜在源。尽管其重要性，非线性ICA的可辨识性在没有附加假设的情况下是不可能的。最近的进展提出了源到观测变量的连接结构的条件，称为结构稀疏性，以实现无监督的可辨识性。然而，在实践中，稀疏约束可能不适用于所有源。此外，源的混合过程的双射性和所有源之间的独立性的假设，这些假设来自ICA的设定，在许多现实场景中也可能被违背。为了解决这些限制并泛化非线性ICA，我们提出了一组新的可辨识性结果，涵盖不完备性、部分稀疏性、源依赖性和灵活的分组结构的一般设置。具体而言，在存在更多源时，我们证明了可辨识性。

    Nonlinear independent component analysis (ICA) aims to uncover the true latent sources from their observable nonlinear mixtures. Despite its significance, the identifiability of nonlinear ICA is known to be impossible without additional assumptions. Recent advances have proposed conditions on the connective structure from sources to observed variables, known as Structural Sparsity, to achieve identifiability in an unsupervised manner. However, the sparsity constraint may not hold universally for all sources in practice. Furthermore, the assumptions of bijectivity of the mixing process and independence among all sources, which arise from the setting of ICA, may also be violated in many real-world scenarios. To address these limitations and generalize nonlinear ICA, we propose a set of new identifiability results in the general settings of undercompleteness, partial sparsity and source dependence, and flexible grouping structures. Specifically, we prove identifiability when there are mor
    
[^96]: 选择性分享体验提升多智能体强化学习

    Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning. (arXiv:2311.00865v1 [cs.LG])

    [http://arxiv.org/abs/2311.00865](http://arxiv.org/abs/2311.00865)

    本文介绍了一种选择性多智能体强化学习方法，即选择性多智能体优先体验中继，代理之间共享有限数量的训练经验。与其他算法相比，该方法实现了去中心化训练，并取得了比基准算法和最先进算法更好的性能。

    

    我们提出了一种新颖的多智能体强化学习方法，即选择性多智能体优先体验中继，其中代理通过分享训练过程中观察到的有限的转换与其他代理共享。其背后的直觉是，来自其他代理的少量相关经验可以帮助每个代理学习。与许多其他多智能体强化学习算法不同，该方法允许基本去中心化的训练，只需要代理之间的有限通信渠道。我们展示了我们的方法优于基准无共享去中心化训练和最先进的多智能体强化学习算法。此外，仅分享少量高度相关的经验优于代理之间的所有经验共享，而且选择性体验共享的性能提升在各种超参数和DQN变体中均具有鲁棒性。我们的算法的参考实现可在https://github.com/mgerstgrasser/super获得。

    We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized Experience Relay, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants. A reference implementation of our algorithm is available at https://github.com/mgerstgrasser/super.
    
[^97]: 训练语言模型中上下文N-Gram的动态

    Training Dynamics of Contextual N-Grams in Language Models. (arXiv:2311.00863v1 [cs.LG])

    [http://arxiv.org/abs/2311.00863](http://arxiv.org/abs/2311.00863)

    这篇论文研究了训练过程中上下文N-Gram的动态，发现了上下文神经元存在于更广泛的上下文N-Gram电路中，这被称为二阶电路。在训练早期，这两个电路具有相互独立的功能，只有在它们都形成之后才能组合成一个二阶电路。

    

    先前的研究已经表明，语言模型中存在上下文神经元，包括一个在德语文本上激活的神经元。我们展示了这个神经元存在于更广泛的上下文N-Gram电路中：我们发现晚期的神经元能够识别和继续德语文本中常见的N-Gram，但只有在德语神经元激活时才会被激活。我们研究了训练过程中这个电路的形成，并发现这是一个我们称之为二阶电路的示例。特别地，早期的训练中，组成N-Gram电路和最终形成德语神经元的德语检测电路具有独立的功能-德语检测电路部分通过建模德语单字统计数据进行形成，而N-Gram则通过提升适当的完成来形成。只有在这两个电路已经形成之后，它们才能组合成为一个二阶电路。与先前的研究假设相反，我们发现上下文N-Gram电路逐渐形成。

    Prior work has shown the existence of contextual neurons in language models, including a neuron that activates on German text. We show that this neuron exists within a broader contextual n-gram circuit: we find late layer neurons which recognize and continue n-grams common in German text, but which only activate if the German neuron is active. We investigate the formation of this circuit throughout training and find that it is an example of what we call a second-order circuit. In particular, both the constituent n-gram circuits and the German detection circuit which culminates in the German neuron form with independent functions early in training - the German detection circuit partially through modeling German unigram statistics, and the n-grams by boosting appropriate completions. Only after both circuits have already formed do they fit together into a second-order circuit. Contrary to the hypotheses presented in prior work, we find that the contextual n-gram circuit forms gradually r
    
[^98]: 机器学习势函数中结构和构象多样性的作用

    Role of Structural and Conformational Diversity for Machine Learning Potentials. (arXiv:2311.00862v1 [physics.chem-ph])

    [http://arxiv.org/abs/2311.00862](http://arxiv.org/abs/2311.00862)

    研究发现在机器学习势函数中，需要谨慎平衡结构和构象多样性以实现最佳的泛化效果，并强调了定义适用领域对于模型部署的重要性。

    

    在机器学习势函数（MLIPs）领域中，理解数据偏差（特别是构象和结构多样性）与模型泛化之间的复杂关系对于提高量子力学（QM）数据生成的质量至关重要。我们通过两个不同的实验来研究这些动态：一个是固定预算的实验，数据集大小保持恒定；另一个是固定分子集的实验，重点在于固定结构多样性而变化构象多样性。我们的结果揭示了泛化度量中微妙的模式。值得注意的是，为了实现最佳的结构和构象泛化，需要在结构和构象多样性之间达到谨慎的平衡，但现有的QM数据集并不满足这种权衡。此外，我们的结果突出了MLIP模型在超出其训练分布泛化的局限性，强调了在模型部署过程中定义适用领域的重要性。

    In the field of Machine Learning Interatomic Potentials (MLIPs), understanding the intricate relationship between data biases, specifically conformational and structural diversity, and model generalization is critical in improving the quality of Quantum Mechanics (QM) data generation efforts. We investigate these dynamics through two distinct experiments: a fixed budget one, where the dataset size remains constant, and a fixed molecular set one, which focuses on fixed structural diversity while varying conformational diversity. Our results reveal nuanced patterns in generalization metrics. Notably, for optimal structural and conformational generalization, a careful balance between structural and conformational diversity is required, but existing QM datasets do not meet that trade-off. Additionally, our results highlight the limitation of the MLIP models at generalizing beyond their training distribution, emphasizing the importance of defining applicability domain during model deploymen
    
[^99]: 零坐标移动：针对物理约束操作学习的优化自动微分方法

    Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning. (arXiv:2311.00860v1 [cs.LG])

    [http://arxiv.org/abs/2311.00860](http://arxiv.org/abs/2311.00860)

    本文提出了一种用于物理约束操作学习的新型自动微分算法，通过零坐标移动（ZCS）的技巧，将所需导数的复杂度从“多根多叶”简化为“一根多叶”，从而显著提高了性能。

    

    自动微分（AD）是物理约束机器学习中的关键步骤，用于计算网络输出相对于坐标的高阶导数。本文提出了一种新颖且轻量级的算法，用于进行针对物理约束操作学习的自动微分，称为零坐标移动（ZCS）的技巧。ZCS引入了一个标量值的叶变量，用于每个空间或时间维度，通过将所需导数从“多根多叶”简化为“一根多叶”，从而实现了性能的巨大提升。ZCS很容易在当前的深度学习库中实现；我们使用DeepXDE软件包进行了自己的实现。我们进行了全面的基准分析和多个案例研究，训练物理约束的DeepONets来解决无数据的偏微分方程（PDE）。结果表明，ZCS一直通过降低GPU内存消耗提供了改进效果。

    Automatic differentiation (AD) is a critical step in physics-informed machine learning, required for computing the high-order derivatives of network output w.r.t. coordinates. In this paper, we present a novel and lightweight algorithm to conduct such AD for physics-informed operator learning, as we call the trick of Zero Coordinate Shift (ZCS). Instead of making all sampled coordinates leaf variables, ZCS introduces only one scalar-valued leaf variable for each spatial or temporal dimension, leading to a game-changing performance leap by simplifying the wanted derivatives from "many-roots-many-leaves" to "one-root-many-leaves". ZCS is easy to implement with current deep learning libraries; our own implementation is by extending the DeepXDE package. We carry out a comprehensive benchmark analysis and several case studies, training physics-informed DeepONets to solve partial differential equations (PDEs) without data. The results show that ZCS has persistently brought down GPU memory co
    
[^100]: 多智能体系统中基于最优成本约束的对抗攻击

    Optimal Cost Constrained Adversarial Attacks For Multiple Agent Systems. (arXiv:2311.00859v1 [cs.LG])

    [http://arxiv.org/abs/2311.00859](http://arxiv.org/abs/2311.00859)

    本研究提出了一种基于最优成本约束的分布式攻击代理的对抗攻击方法，可以在多智能体系统中显著降低受攻击代理获得的奖励。

    

    寻找最优的对抗攻击策略是强化学习和马尔可夫决策过程中的重要课题。先前的研究通常假设一个全知的协调者（攻击者），攻击不同的接收者（受害者）代理会产生统一的成本。然而，在现实中，攻击通常需要由分布式攻击代理执行，而不是使用一个无限制的中心攻击者。我们在多智能体系统中提出了一个问题，即如何使用分布式攻击代理进行最优的对抗代理间攻击，其中对每个不同的攻击者-受害者对都施加不同的成本约束。我们提出了一种最优方法，通过在步骤内进行静态约束攻击资源分配优化，并在步骤间进行动态规划，以实现多智能体系统中的最优对抗攻击。我们的数值结果表明，所提出的攻击能够显著降低受攻击代理所获得的奖励。

    Finding optimal adversarial attack strategies is an important topic in reinforcement learning and the Markov decision process. Previous studies usually assume one all-knowing coordinator (attacker) for whom attacking different recipient (victim) agents incurs uniform costs. However, in reality, instead of using one limitless central attacker, the attacks often need to be performed by distributed attack agents. We formulate the problem of performing optimal adversarial agent-to-agent attacks using distributed attack agents, in which we impose distinct cost constraints on each different attacker-victim pair. We propose an optimal method integrating within-step static constrained attack-resource allocation optimization and between-step dynamic programming to achieve the optimal adversarial attack in a multi-agent system. Our numerical results show that the proposed attacks can significantly reduce the rewards received by the attacked agents.
    
[^101]: SmoothHess: 通过Stein引理对ReLU网络的特征交互进行建模

    SmoothHess: ReLU Network Feature Interactions via Stein's Lemma. (arXiv:2311.00858v1 [cs.LG])

    [http://arxiv.org/abs/2311.00858](http://arxiv.org/abs/2311.00858)

    SmoothHess是一种通过Stein引理对ReLU网络的特征交互进行建模的方法，它能估计网络和高斯卷积的Hessian矩阵，具有高效的抽样算法和可控的平滑程度。在基准数据集和真实世界的医学肺功能数据集上验证了SmoothHess捕捉交互作用的卓越能力。

    

    最近的一些解释性方法通过查看神经网络的Hessian矩阵来建模特征的交互作用。然而，对于ReLU网络来说，这是一个挑战，因为它们是分段线性的，在几乎所有地方都具有零的Hessian矩阵。我们提出了SmoothHess方法，通过Stein引理估计二阶交互作用。具体而言，我们通过高效的抽样算法，估计了网络和高斯卷积的Hessian矩阵，只需要网络的梯度调用。SmoothHess是后处理方法，不需要对ReLU网络架构进行修改，平滑程度可以明确控制。我们提供了对于估计过程样本复杂度的非渐进界。我们验证了SmoothHess在基准数据集和真实世界的医学肺功能数据集上捕捉交互作用的卓越能力。

    Several recent methods for interpretability model feature interactions by looking at the Hessian of a neural network. This poses a challenge for ReLU networks, which are piecewise-linear and thus have a zero Hessian almost everywhere. We propose SmoothHess, a method of estimating second-order interactions through Stein's Lemma. In particular, we estimate the Hessian of the network convolved with a Gaussian through an efficient sampling algorithm, requiring only network gradient calls. SmoothHess is applied post-hoc, requires no modifications to the ReLU network architecture, and the extent of smoothing can be controlled explicitly. We provide a non-asymptotic bound on the sample complexity of our estimation procedure. We validate the superior ability of SmoothHess to capture interactions on benchmark datasets and a real-world medical spirometry dataset.
    
[^102]: 一种用于评估美国终结HIV流行计划的多智能体强化学习框架

    A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan. (arXiv:2311.00855v1 [cs.AI])

    [http://arxiv.org/abs/2311.00855](http://arxiv.org/abs/2311.00855)

    本论文提出了一种多智能体强化学习（MARL）框架，用于评估美国终结HIV流行计划。该框架能够进行特定地区的决策分析，并考虑到地区之间的流行病学相互作用。

    

    人类免疫缺陷病毒（HIV）是美国的主要公共卫生问题，每年有约1.2万人感染HIV，其中有3.5万人是新感染者。美国的HIV负担和护理接触存在着地理差异。2019年的终结HIV流行计划旨在到2030年将新感染人数减少90%，通过提高诊断、治疗和预防干预措施的覆盖率，并优先考虑HIV高流行地区。确定最佳干预措施的规模扩大将有助于资源分配的决策。现有的HIV决策模型要么只评估特定城市，要么评估整个国家人口，忽视地方的相互作用或差异。在本文中，我们提出了一种多智能体强化学习（MARL）模型，它能够进行特定地区的决策分析，同时考虑跨地区的流行病互动。在实验分析中，

    Human immunodeficiency virus (HIV) is a major public health concern in the United States, with about 1.2 million people living with HIV and 35,000 newly infected each year. There are considerable geographical disparities in HIV burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE) initiative aims to reduce new infections by 90% by 2030, by improving coverage of diagnoses, treatment, and prevention interventions and prioritizing jurisdictions with high HIV prevalence. Identifying optimal scale-up of intervention combinations will help inform resource allocation. Existing HIV decision analytic models either evaluate specific cities or the overall national population, thus overlooking jurisdictional interactions or differences. In this paper, we propose a multi-agent reinforcement learning (MARL) model, that enables jurisdiction-specific decision analyses but in an environment with cross-jurisdictional epidemiological interactions. In experimental analyses, conduct
    
[^103]: 通过物理约束的机器学习来研究电子激发态

    Electronic excited states from physically-constrained machine learning. (arXiv:2311.00844v1 [physics.chem-ph])

    [http://arxiv.org/abs/2311.00844](http://arxiv.org/abs/2311.00844)

    本文介绍了一种综合建模方法，通过训练一个对称自适应的机器学习模型来再现量子力学计算中的电子激发，从而实现了针对更大更复杂分子的预测，并通过最小原子中心基础对应的参数化实现了极大的计算效益。

    

    数据驱动技术越来越多地用于替代物质的电子结构计算。在这种情况下，一个相关的问题是，机器学习(ML)是否应当直接应用于预测所需的性质，还是明确地与物理基础操作结合起来。我们提出了一个综合建模方法的示例，其中对一个有效哈密顿量的对称自适应ML模型进行训练，以再现量子力学计算中的电子激发。所得模型可以预测比其训练集更大更复杂的分子，并通过间接针对良好收敛计算的输出而使用与最小原子中心基础对应的参数化，实现了极大的计算效益。这些结果强调了将数据驱动技术与物理近似相结合的优点，提高了ML模型的可转移性和可解释性，而不需要明确建立物理模型。

    Data-driven techniques are increasingly used to replace electronic-structure calculations of matter. In this context, a relevant question is whether machine learning (ML) should be applied directly to predict the desired properties or be combined explicitly with physically-grounded operations. We present an example of an integrated modeling approach, in which a symmetry-adapted ML model of an effective Hamiltonian is trained to reproduce electronic excitations from a quantum-mechanical calculation. The resulting model can make predictions for molecules that are much larger and more complex than those that it is trained on, and allows for dramatic computational savings by indirectly targeting the outputs of well-converged calculations while using a parameterization corresponding to a minimal atom-centered basis. These results emphasize the merits of intertwining data-driven techniques with physical approximations, improving the transferability and interpretability of ML models without a
    
[^104]: 带有单调概率的尖锐噪音二分查找

    Sharp Noisy Binary Search with Monotonic Probabilities. (arXiv:2311.00840v1 [cs.DS])

    [http://arxiv.org/abs/2311.00840](http://arxiv.org/abs/2311.00840)

    这篇论文提出了一个带有单调概率的尖锐噪音二分查找模型，并给出了一个算法，该算法通过解决高概率行为和尖锐常数两个挑战，可以在给定样本数的情况下以高概率成功地找到概率交叉的地方。

    

    我们重新审视了Karp和Kleinberg的噪音二分查找模型，在该模型中，我们有n个未知概率p_i的硬币可以翻转。硬币按照增加的p_i进行排序，并且我们想找到概率在目标值τ（在ε范围内）附近交叉的地方。这将Burnashev和Zigangirov的固定噪音模型推广到硬币接近目标时可能无法区分的情况。Karp和Kleinberg表明，对于这个任务，需要和足够的样本数量是Θ(1/ε^2 log n)。我们通过解决两个理论上的挑战：高概率行为和尖锐常数，提出了一个实用的算法。我们给出了一个算法，它可以在\[ \frac{1}{C_{\tau, \varepsilon}} \cdot \left(\lg n + O(\log^{2/3} n \log^{1/3} \frac{1}{\delta} + \log \frac{1}{\delta})\right) \]个样本时以概率1-δ成功，其中C_{\tau, \varepsilon}是这样的最优常数。

    We revisit the noisy binary search model of Karp and Kleinberg, in which we have $n$ coins with unknown probabilities $p_i$ that we can flip. The coins are sorted by increasing $p_i$, and we would like to find where the probability crosses (to within $\varepsilon$) of a target value $\tau$. This generalized the fixed-noise model of Burnashev and Zigangirov , in which $p_i = \frac{1}{2} \pm \varepsilon$, to a setting where coins near the target may be indistinguishable from it. Karp and Kleinberg showed that $\Theta(\frac{1}{\varepsilon^2} \log n)$ samples are necessary and sufficient for this task.  We produce a practical algorithm by solving two theoretical challenges: high-probability behavior and sharp constants. We give an algorithm that succeeds with probability $1-\delta$ from  \[  \frac{1}{C_{\tau, \varepsilon}} \cdot \left(\lg n + O(\log^{2/3} n \log^{1/3} \frac{1}{\delta} + \log \frac{1}{\delta})\right)  \]  samples, where $C_{\tau, \varepsilon}$ is the optimal such constant a
    
[^105]: 在非凸优化中观察到的量子-经典性能差异

    A quantum-classical performance separation in nonconvex optimization. (arXiv:2311.00811v1 [quant-ph])

    [http://arxiv.org/abs/2311.00811](http://arxiv.org/abs/2311.00811)

    本文揭示了一类非凸优化实例的量子-经典性能差异，用量子哈密顿下降算法可以高效解决，而经典算法则需要超多项式时间。

    

    本文通过确定一类非凸连续优化实例，每个d维实例具有2^d个局部最小值，来展示了量子-经典性能差异。具体来说，我们证明了最近提出的量子哈密顿下降（QHD）算法[Leng等人，arXiv:2303.01471]能够使用Τ(ilda{O}(d^3))的量子查询函数值和Τ(ilda{O}(d^4))个附加的1比特和2比特基本量子门来解决该类实例的任意d维问题。另一方面，一项全面的实证研究表明，代表性的最先进的经典优化算法/求解器（包括Gurobi）需要超多项式时间来解决这类优化实例。

    In this paper, we identify a family of nonconvex continuous optimization instances, each $d$-dimensional instance with $2^d$ local minima, to demonstrate a quantum-classical performance separation. Specifically, we prove that the recently proposed Quantum Hamiltonian Descent (QHD) algorithm [Leng et al., arXiv:2303.01471] is able to solve any $d$-dimensional instance from this family using $\widetilde{\mathcal{O}}(d^3)$ quantum queries to the function value and $\widetilde{\mathcal{O}}(d^4)$ additional 1-qubit and 2-qubit elementary quantum gates. On the other side, a comprehensive empirical study suggests that representative state-of-the-art classical optimization algorithms/solvers (including Gurobi) would require a super-polynomial time to solve such optimization instances.
    
[^106]: Mahalanobis感知训练用于异常样本检测

    Mahalanobis-Aware Training for Out-of-Distribution Detection. (arXiv:2311.00808v1 [cs.LG])

    [http://arxiv.org/abs/2311.00808](http://arxiv.org/abs/2311.00808)

    本文提出了一种用于改善基于密度的超出分布敏感性的新的损失函数和训练方法，在CIFAR-10数据集上取得了显著的效果，尤其在远离分布的任务上将相对马哈拉诺比斯距离方法的误检率降低超过50%。

    

    尽管深度学习模型在受控环境中取得了广泛成功，但在开放世界环境中使用仍存在一些障碍。确保安全部署的一个关键任务是检测可能需要人工干预的异常或超出分布的样本。在本文中，我们提出了一种新的损失函数和训练网络的方法，以提高基于密度的超出分布敏感性。我们在CIFAR-10数据集上证明了我们方法的有效性，尤其在远离分布的任务上，相对马哈拉诺比斯距离方法的误检率降低超过50%。

    While deep learning models have seen widespread success in controlled environments, there are still barriers to their adoption in open-world settings. One critical task for safe deployment is the detection of anomalous or out-of-distribution samples that may require human intervention. In this work, we present a novel loss function and recipe for training networks with improved density-based out-of-distribution sensitivity. We demonstrate the effectiveness of our method on CIFAR-10, notably reducing the false-positive rate of the relative Mahalanobis distance method on far-OOD tasks by over 50%.
    
[^107]: VQA-GEN:一个用于领域通用化的视觉问答基准测试

    VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization. (arXiv:2311.00807v1 [cs.CV])

    [http://arxiv.org/abs/2311.00807](http://arxiv.org/abs/2311.00807)

    VQA-GEN是首个用于领域通用化的多模态基准测试数据集，揭示了现有VQA方法对于联合多模态分布变化的脆弱性，验证了全面的多模态变化对于VQA的稳健性通用化至关重要。

    

    视觉问答（VQA）模型旨在展示视觉-文本推理能力。然而，由于缺乏综合性的基准测试数据集，它们的实际应用受到了限制。现有的VQA通用性数据集单向关注文本变化，而VQA作为一个多模态任务，包含了视觉和文本领域的变化。我们提出了VQA-GEN，这是第一个通过引入变化的流程生成的多模态基准测试数据集。实验证明，VQA-GEN数据集揭示了现有方法对于联合多模态分布变化的脆弱性，验证了全面的多模态变化对于VQA的稳健性通用化至关重要。在VQA-GEN上训练的模型显示出跨领域和领域内性能的提高，证实了VQA-GEN的价值。此外，我们分析了我们的流程中每个变化技术对于模型通用化的重要性。

    Visual question answering (VQA) models are designed to demonstrate visual-textual reasoning capabilities. However, their real-world applicability is hindered by a lack of comprehensive benchmark datasets. Existing domain generalization datasets for VQA exhibit a unilateral focus on textual shifts while VQA being a multi-modal task contains shifts across both visual and textual domains. We propose VQA-GEN, the first ever multi-modal benchmark dataset for distribution shift generated through a shift induced pipeline. Experiments demonstrate VQA-GEN dataset exposes the vulnerability of existing methods to joint multi-modal distribution shifts. validating that comprehensive multi-modal shifts are critical for robust VQA generalization. Models trained on VQA-GEN exhibit improved cross-domain and in-domain performance, confirming the value of VQA-GEN. Further, we analyze the importance of each shift technique of our pipeline contributing to the generalization of the model.
    
[^108]: 用于颗粒物体堆积操作的神经场动力学模型

    Neural Field Dynamics Model for Granular Object Piles Manipulation. (arXiv:2311.00802v1 [cs.RO])

    [http://arxiv.org/abs/2311.00802](http://arxiv.org/abs/2311.00802)

    本论文提出了一种用于颗粒材料操作的学习动力学模型。该模型采用了基于密度场的表示方法和全卷积神经网络，并具有可微分的动作渲染模块。在模拟和实际实验中的评估结果表明，该模型在准确性和计算效率上都显著超过了现有的方法，并且在不同环境和任务中具有零样本泛化能力。

    

    我们提出了一种基于学习的颗粒材料操作动力学模型。受流体动力学中常用的欧拉方法的启发，我们的方法采用了一个完全卷积神经网络，它可以在基于密度场的物体堆积和推动器表示上操作，使其能够利用物体之间相互作用的空间局部性以及通过卷积操作进行的平移等变性。此外，我们的可微行为渲染模块使模型完全可微分，并可以直接与基于梯度的轨迹优化算法集成。我们通过在模拟和实际世界实验证明了我们的模型在宽范围的堆积操作任务中明显超过了现有的基于潜变量或基于粒子的方法，在准确性和计算效率上都表现出色，并且在不同环境和任务之间具有零样本泛化能力。

    We present a learning-based dynamics model for granular material manipulation. Inspired by the Eulerian approach commonly used in fluid dynamics, our method adopts a fully convolutional neural network that operates on a density field-based representation of object piles and pushers, allowing it to exploit the spatial locality of inter-object interactions as well as the translation equivariance through convolution operations. Furthermore, our differentiable action rendering module makes the model fully differentiable and can be directly integrated with a gradient-based trajectory optimization algorithm. We evaluate our model with a wide array of piles manipulation tasks both in simulation and real-world experiments and demonstrate that it significantly exceeds existing latent or particle-based methods in both accuracy and computation efficiency, and exhibits zero-shot generalization capabilities across various environments and tasks.
    
[^109]: GIST: 在深度学习中生成输入集合的可迁移性

    GIST: Generated Inputs Sets Transferability in Deep Learning. (arXiv:2311.00801v1 [cs.LG])

    [http://arxiv.org/abs/2311.00801](http://arxiv.org/abs/2311.00801)

    这篇论文介绍了一种在深度学习模型之间高效迁移测试集的新方法，通过选择具有用户感兴趣的属性的良好测试集，以达到改善可验证性和测试性的目的。

    

    随着对神经网络可验证性和可测试性的需求不断增加，越来越多的生成测试集的方法被开发出来。然而，这些技术中的每一种都倾向于强调特定的测试方面，并且可能非常耗时。缓解这个问题的一个简单解决方案是根据希望迁移的期望属性，在一些经过基准测试的模型和新测试模型之间转移测试集。本文介绍了GIST（生成输入集合的可迁移性），一种用于在深度学习模型之间高效迁移测试集的新方法。给定用户希望迁移的一个感兴趣的属性（例如，覆盖准则），GIST能够从基准提供的可用测试集中，从该属性的角度选择良好的测试集。我们通过两种模态和不同的测试集生成过程，在故障类型覆盖属性上对GIST进行经验评估，以证明该方法的可行性。

    As the demand for verifiability and testability of neural networks continues to rise, an increasing number of methods for generating test sets are being developed. However, each of these techniques tends to emphasize specific testing aspects and can be quite time-consuming. A straightforward solution to mitigate this issue is to transfer test sets between some benchmarked models and a new model under test, based on a desirable property one wishes to transfer. This paper introduces GIST (Generated Inputs Sets Transferability), a novel approach for the efficient transfer of test sets among Deep Learning models. Given a property of interest that a user wishes to transfer (e.g., coverage criterion), GIST enables the selection of good test sets from the point of view of this property among available ones from a benchmark. We empirically evaluate GIST on fault types coverage property with two modalities and different test set generation procedures to demonstrate the approach's feasibility. E
    
[^110]: 演化流行病学网络的临界转折点：机器学习辅助的数据驱动有效建模

    Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v1 [cs.LG])

    [http://arxiv.org/abs/2311.00797](http://arxiv.org/abs/2311.00797)

    该论文通过机器学习辅助的数据驱动建模方法，研究了自适应易感-感染-易感流行病学网络的临界转折点集体动力学。他们识别出了一个有效的随机微分方程以描述网络的演化行为，并观察到了罕见的大幅度集体振荡现象。

    

    我们通过数据驱动的、机器学习辅助的方式研究自适应易感-感染-易感(SIS)流行病学网络的临界转折点集体动力学。我们通过受数值随机积分器启发的深度学习ResNet架构，识别出一个参数相关的基于物理意义的粗粒度均场变量的有效随机微分方程(eSDE)。我们基于eSDE的确定偏移项构建了一个近似的有效分岔图，并将其与均场SIS模型的分岔图进行对比。我们观察到演化网络的有效SIS动力学中的次临界Hopf分岔，它引起了临界转折行为；这表现为大幅度的集体振荡，它们从(噪声的)固定状态的邻域中自发地、罕见地出现。我们通过重复的暴力模拟和使用已建立的数学工具研究了这些罕见事件的统计特性。

    We study the tipping point collective dynamics of an adaptive susceptible-infected-susceptible (SIS) epidemiological network in a data-driven, machine learning-assisted manner. We identify a parameter-dependent effective stochastic differential equation (eSDE) in terms of physically meaningful coarse mean-field variables through a deep-learning ResNet architecture inspired by numerical stochastic integrators. We construct an approximate effective bifurcation diagram based on the identified drift term of the eSDE and contrast it with the mean-field SIS model bifurcation diagram. We observe a subcritical Hopf bifurcation in the evolving network's effective SIS dynamics, that causes the tipping point behavior; this takes the form of large amplitude collective oscillations that spontaneously -- yet rarely -arise from the neighborhood of a (noisy) stationary state. We study the statistics of these rare events both through repeated brute force simulations and by using established mathemati
    
[^111]: 使用时间相关密度泛函理论和机器学习将电子阻止能力预测加快了1000万倍

    Accelerating Electronic Stopping Power Predictions by 10 Million Times with a Combination of Time-Dependent Density Functional Theory and Machine Learning. (arXiv:2311.00787v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2311.00787](http://arxiv.org/abs/2311.00787)

    使用时间相关密度泛函理论和机器学习的方法将电子阻止能力预测加快了1000万倍，并提供了关于原子细节如何影响电子阻止的宝贵数据。

    

    知道粒子辐射在材料中释放能量的速率，也就是阻止能力，对于设计核反应堆、医疗治疗、半导体和量子材料以及许多其他技术都是关键。虽然关于阻止能力的核贡献，即原子之间的弹性散射，在文献中已经得到了很好的理解，但是获取电子贡献数据的途径几十年来一直代价高昂且依赖许多简化假设，包括材料是各向同性的。我们建立了一种方法，将时间相关密度泛函理论（TDDFT）和机器学习相结合，将评估新材料的时间缩短到仅需几个小时，并提供有关原子细节如何影响电子阻止的宝贵数据。我们的方法使用TDDFT来从第一原理计算电子阻止对阻止能力的贡献，并通过机器学习插值到其他方向，速度提高了1000万倍。

    Knowing the rate at which particle radiation releases energy in a material, the stopping power, is key to designing nuclear reactors, medical treatments, semiconductor and quantum materials, and many other technologies. While the nuclear contribution to stopping power, i.e., elastic scattering between atoms, is well understood in the literature, the route for gathering data on the electronic contribution has for decades remained costly and reliant on many simplifying assumptions, including that materials are isotropic. We establish a method that combines time-dependent density functional theory (TDDFT) and machine learning to reduce the time to assess new materials to mere hours on a supercomputer and provides valuable data on how atomic details influence electronic stopping. Our approach uses TDDFT to compute the electronic stopping contributions to stopping power from first principles in several directions and then machine learning to interpolate to other directions at rates 10 milli
    
[^112]: 利用机器学习来准确处理GCM中重叠透明度物种的论文

    Harnessing machine learning for accurate treatment of overlapping opacity species in GCMs. (arXiv:2311.00775v1 [astro-ph.EP])

    [http://arxiv.org/abs/2311.00775](http://arxiv.org/abs/2311.00775)

    本论文基于机器学习提出了一种快速且准确的方法来处理GCM中重叠的透明度物种，通过有效地结合各个相关-k透明度表（k-tables），该方法在热木星HD~209458 b的模拟中表现出了精确性和高效性。

    

    为了理解外行星和棕矮星的高精度观测结果，我们需要详细和复杂的涵盖了流体力学、化学和辐射的通用环流模型（GCMs）。本研究具体考察了GCMs中化学和辐射之间的耦合关系，并比较了在相关-k假设中混合不同化学物种透明度的不同方法，在不能假设平衡化学反应的情况下。我们提出了一种基于DeepSets（DS）的快速机器学习方法，有效地结合了各个相关-k透明度表（k-tables）。我们将DS方法与其他已发表的方法如自适应等价消光（AEE）和带有重新分组和排序的随机重叠（RORR）进行了评估。我们将这些混合方法整合到了我们的GCM (expeRT/MITgcm)中，并对热木星HD~209458 b进行了准确性和性能评估。我们的研究结果表明，DS方法在GCM使用时既准确又高效，而RORR方法则不然。

    To understand high precision observations of exoplanets and brown dwarfs, we need detailed and complex general circulation models (GCMs) that incorporate hydrodynamics, chemistry, and radiation. In this study, we specifically examine the coupling between chemistry and radiation in GCMs and compare different methods for mixing opacities of different chemical species in the correlated-k assumption, when equilibrium chemistry cannot be assumed. We propose a fast machine learning method based on DeepSets (DS), which effectively combines individual correlated-k opacities (k-tables). We evaluate the DS method alongside other published methods like adaptive equivalent extinction (AEE) and random overlap with rebinning and resorting (RORR). We integrate these mixing methods into our GCM (expeRT/MITgcm) and assess their accuracy and performance for the example of the hot Jupiter HD~209458 b. Our findings indicate that the DS method is both accurate and efficient for GCM usage, whereas RORR is t
    
[^113]: 深度样条在最优和高效预测集中的一致化应用

    Conformalized Deep Splines for Optimal and Efficient Prediction Sets. (arXiv:2311.00774v1 [cs.LG])

    [http://arxiv.org/abs/2311.00774](http://arxiv.org/abs/2311.00774)

    该论文提出了一种新的一致化回归方法，通过神经网络参数化样条估计条件密度的样条预测区间，证明了其普适逼近和最优性，实验结果表明在基准数据集上表现出色。

    

    在高风险的机器学习应用中，不确定性估计是至关重要的。一种有效的估计不确定性的方法是一致化预测，它可以提供带有统计覆盖保证的预测推断。我们提出了一种新的一致化回归方法，通过神经网络参数化样条估计条件密度的样条预测区间（SPICE）。我们证明了SPICE的普适逼近和最优性结果，并通过我们的实验进行了实证验证。SPICE兼容两种不同的高效计算的一致化评分，一种是对于边际覆盖率的理论最优（SPICE-ND），另一种是对于条件覆盖率渐近最优（SPICE-HPD）。基准数据集的结果表明，SPICE-ND模型实现了最小的平均预测集大小，某些数据集的平均大小减少了近50%，与其他基准相比。SPICE-HPD模型实现了最佳的条件覆盖率。

    Uncertainty estimation is critical in high-stakes machine learning applications. One effective way to estimate uncertainty is conformal prediction, which can provide predictive inference with statistical coverage guarantees. We present a new conformal regression method, Spline Prediction Intervals via Conformal Estimation (SPICE), that estimates the conditional density using neural-network-parameterized splines. We prove universal approximation and optimality results for SPICE, which are empirically validated by our experiments. SPICE is compatible with two different efficient-to-compute conformal scores, one oracle-optimal for marginal coverage (SPICE-ND) and the other asymptotically optimal for conditional coverage (SPICE-HPD). Results on benchmark datasets demonstrate SPICE-ND models achieve the smallest average prediction set sizes, including average size reductions of nearly 50% for some datasets compared to the next best baseline. SPICE-HPD models achieve the best conditional cov
    
[^114]: 用于临床特征嵌入的语言模型训练范式

    Language Model Training Paradigms for Clinical Feature Embeddings. (arXiv:2311.00768v1 [cs.LG])

    [http://arxiv.org/abs/2311.00768](http://arxiv.org/abs/2311.00768)

    本研究使用自监督训练范式的语言模型，通过表示学习为临床时间序列推导出高质量的通用临床特征嵌入。通过无监督的降维技术可视化学习到的嵌入，并在MIMIC-III基准测试中验证了它们的有效性。

    

    在数据稀缺的研究领域，表示学习起着重要的作用。本研究旨在通过对临床时间序列进行表示学习，推导出临床特征（如心率和血压）的通用嵌入。我们使用语言模型的自监督训练范式，学习高质量的临床特征嵌入，实现比现有的时间步和患者级别表示学习更细粒度的表征。我们通过无监督的降维技术可视化学习到的嵌入，并观察到与先前的临床知识高度一致。我们还在MIMIC-III基准测试上评估模型性能，并展示了使用临床特征嵌入的有效性。我们将我们的代码发布在网上以供复制。

    In research areas with scarce data, representation learning plays a significant role. This work aims to enhance representation learning for clinical time series by deriving universal embeddings for clinical features, such as heart rate and blood pressure. We use self-supervised training paradigms for language models to learn high-quality clinical feature embeddings, achieving a finer granularity than existing time-step and patient-level representation learning. We visualize the learnt embeddings via unsupervised dimension reduction techniques and observe a high degree of consistency with prior clinical knowledge. We also evaluate the model performance on the MIMIC-III benchmark and demonstrate the effectiveness of using clinical feature embeddings. We publish our code online for replication.
    
[^115]: 学习设计和使用机器人操纵工具

    Learning to Design and Use Tools for Robotic Manipulation. (arXiv:2311.00754v1 [cs.RO])

    [http://arxiv.org/abs/2311.00754](http://arxiv.org/abs/2311.00754)

    本论文提出了学习一种设计策略来制作适用于不同任务的专用工具，并通过这些工具进行机器人操纵。这可以解锁机器人的额外能力。

    

    当面临自身形态限制时，人类和某些动物种类具有使用环境中的物体来完成原本不可能的任务的能力。机器人可能通过使用工具解锁一系列额外的能力。最近的通过深度学习来联合优化形态和控制的技术在设计移动机器人方面非常有效。但是，尽管输出一个单一的形态对于移动来说是有意义的，但是操纵涉及到根据手头的任务目标采用各种策略。一个操纵机器人必须能够快速制作出适用于不同目标的专用工具。因此，我们提出学习一个设计策略，而不是一个单一的设计。设计策略以任务信息为条件，并输出一个有助于解决任务的工具设计。然后，一个以设计条件为基础的控制策略可以使用这些工具进行操纵。在这项工作中，我们朝着这个目标迈出了一步，引入了一种强化学习方法。

    When limited by their own morphologies, humans and some species of animals have the remarkable ability to use objects from the environment toward accomplishing otherwise impossible tasks. Robots might similarly unlock a range of additional capabilities through tool use. Recent techniques for jointly optimizing morphology and control via deep learning are effective at designing locomotion agents. But while outputting a single morphology makes sense for locomotion, manipulation involves a variety of strategies depending on the task goals at hand. A manipulation agent must be capable of rapidly prototyping specialized tools for different goals. Therefore, we propose learning a designer policy, rather than a single design. A designer policy is conditioned on task information and outputs a tool design that helps solve the task. A design-conditioned controller policy can then perform manipulation using these tools. In this work, we take a step towards this goal by introducing a reinforcement
    
[^116]: 这些是同一个苹果吗？基于物体内在特性比较图像

    Are These the Same Apple? Comparing Images Based on Object Intrinsics. (arXiv:2311.00750v1 [cs.CV])

    [http://arxiv.org/abs/2311.00750](http://arxiv.org/abs/2311.00750)

    本研究提出了一种基于物体内在特性的图像相似度度量方法，通过对通用对象类别进行扩展，并收集了大规模的CUTE数据集来评估该方法。

    

    人类视觉系统可以轻松识别在不同的外在因素下（如光照、物体姿势和背景）的对象，然而当前的计算机视觉系统在这些变异方面经常遇到困难。理解和改进人工视觉系统的重要一步是纯基于定义对象身份的内在物体特性来测量图像相似性。这个问题在计算机视觉文献中已被研究为重新识别，尽管主要局限于特定的对象类别如人和汽车。我们提出将其扩展到通用对象类别，探索基于物体内在特性的图像相似度度量。为了评估这种测量方法，我们收集了Common paired objects Under differenT Extrinsics (CUTE)数据集，包括180个对象的18000张图像，涵盖了不同的外在因素，如光照、姿势和成像条件。然而，现有的方法如LPIPS和CLIP分数并不能很好地测量物体内在特性。

    The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current computer vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image similarity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image similarity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of $18,000$ images of $180$ objects under different extrinsic factors such as lighting, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics wel
    
[^117]: 基于预测的排序

    Sorting with Predictions. (arXiv:2311.00749v1 [cs.DS])

    [http://arxiv.org/abs/2311.00749](http://arxiv.org/abs/2311.00749)

    本论文通过学习增强的算法视角探索了基于预测的排序问题，并设计了新的简单算法，实现了理论上最优的比较复杂性，对应用预测排序方法具有潜在的优势。

    

    我们通过学习增强的算法视角探索了排序的基本问题，其中算法可以利用可能存在错误的预测来提高效率。我们考虑了两种不同的情景：在第一种情景中，每个项目都被提供了一个其在排序列表中位置的预测。在第二种情景中，我们假设存在一种“快速且粗糙”的比较方式，除此之外还存在慢而准确的比较方式。对于这两种情景，我们设计了新的简单算法，仅利用$O(\sum_i \log \eta_i)$个准确比较，其中$\eta_i$是第$i$个元素的适当定义的预测误差。特别地，随着预测的质量恶化，比较的数量从$O(n)$平滑地下降到$O(n\log n)$。我们证明了比较复杂性在所考察的误差度量方面在理论上是最优的。与现有的自适应和非自适应排序算法进行的实验评估表明了应用预测的潜力。

    We explore the fundamental problem of sorting through the lens of learning-augmented algorithms, where algorithms can leverage possibly erroneous predictions to improve their efficiency. We consider two different settings: In the first setting, each item is provided a prediction of its position in the sorted list. In the second setting, we assume there is a "quick-and-dirty" way of comparing items, in addition to slow-and-exact comparisons. For both settings, we design new and simple algorithms using only $O(\sum_i \log \eta_i)$ exact comparisons, where $\eta_i$ is a suitably defined prediction error for the $i$th element. In particular, as the quality of predictions deteriorates, the number of comparisons degrades smoothly from $O(n)$ to $O(n\log n)$. We prove that the comparison complexity is theoretically optimal with respect to the examined error measures. An experimental evaluation against existing adaptive and non-adaptive sorting algorithms demonstrates the potential of applying
    
[^118]: 大型语言模型能设计准确的标签函数吗？

    Can Large Language Models Design Accurate Label Functions?. (arXiv:2311.00739v1 [cs.CL])

    [http://arxiv.org/abs/2311.00739](http://arxiv.org/abs/2311.00739)

    本研究引入了DataSculpt，它是一个利用预训练语言模型自动生成标签函数的交互式框架。通过多种技术和方法的结合，DataSculpt在各种任务和真实数据集上展现了优点和局限性。

    

    编程式弱监督方法通过使用封装启发式数据源的标签函数（LFs）加速标记大规模数据集。然而，创建精确的LFs需要领域专业知识和大量努力。最近，预训练语言模型（PLMs）的进展在各种任务中展示了巨大的潜力。然而，PLMs自主制定准确的LFs的能力仍然是一个未被充分探索的领域。在这项研究中，我们通过引入DataSculpt来填补这一空白，这是一个利用PLMs自动生成LFs的交互式框架。在DataSculpt中，我们结合了各种提示技术、实例选择策略和LF过滤方法来探索广阔的设计领域。最终，我们对DataSculpt在12个涵盖多个任务的真实数据集上的性能进行了全面评估。这个评估揭示了DataSculpt的优点和局限性。

    Programmatic weak supervision methodologies facilitate the expedited labeling of extensive datasets through the use of label functions (LFs) that encapsulate heuristic data sources. Nonetheless, the creation of precise LFs necessitates domain expertise and substantial endeavors. Recent advances in pre-trained language models (PLMs) have exhibited substantial potential across diverse tasks. However, the capacity of PLMs to autonomously formulate accurate LFs remains an underexplored domain. In this research, we address this gap by introducing DataSculpt, an interactive framework that harnesses PLMs for the automated generation of LFs. Within DataSculpt, we incorporate an array of prompting techniques, instance selection strategies, and LF filtration methods to explore the expansive design landscape. Ultimately, we conduct a thorough assessment of DataSculpt's performance on 12 real-world datasets, encompassing a range of tasks. This evaluation unveils both the strengths and limitations 
    
[^119]: 实时磁性追踪和机器学习诊断COVID-19

    Real-Time Magnetic Tracking and Diagnosis of COVID-19 via Machine Learning. (arXiv:2311.00737v1 [cs.LG])

    [http://arxiv.org/abs/2311.00737](http://arxiv.org/abs/2311.00737)

    本研究将磁性呼吸感应技术与机器学习相结合，创建了一个实时诊断平台，用于追踪和诊断COVID-19和其他呼吸系统疾病。通过对COVID-19患者和健康人士的呼吸数据进行训练和验证，我们评估了多个机器学习算法的诊断能力，选出最优模型，平衡了诊断精度和模型可解释性。

    

    COVID-19大流行突显了可靠、非侵入性诊断工具在强有力的公共卫生干预中的重要性。本研究将磁性呼吸感应技术（MRST）与机器学习（ML）相结合，创建了一种诊断平台，用于实时追踪和诊断COVID-19和其他呼吸系统疾病。MRST通过三种特定的呼吸测试方案（正常呼吸、屏气和深呼吸）精确捕捉呼吸模式。我们在越南使用这个平台采集了COVID-19患者和健康人士的呼吸数据，然后用这些数据对ML模型进行训练和验证。我们评估了多种ML算法，包括支持向量机和深度学习模型，评估它们诊断COVID-19的能力。我们的多模型验证方法确保了全面比较，并具备选择最优模型的适应性，在诊断精度和模型可解释性之间取得平衡。

    The COVID-19 pandemic underscored the importance of reliable, noninvasive diagnostic tools for robust public health interventions. In this work, we fused magnetic respiratory sensing technology (MRST) with machine learning (ML) to create a diagnostic platform for real-time tracking and diagnosis of COVID-19 and other respiratory diseases. The MRST precisely captures breathing patterns through three specific breath testing protocols: normal breath, holding breath, and deep breath. We collected breath data from both COVID-19 patients and healthy subjects in Vietnam using this platform, which then served to train and validate ML models. Our evaluation encompassed multiple ML algorithms, including support vector machines and deep learning models, assessing their ability to diagnose COVID-19. Our multi-model validation methodology ensures a thorough comparison and grants the adaptability to select the most optimal model, striking a balance between diagnostic precision with model interpretab
    
[^120]: PET通过可变增强可逆网络在脑PET之间进行示踪剂转化

    PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network. (arXiv:2311.00735v1 [cs.LG])

    [http://arxiv.org/abs/2311.00735](http://arxiv.org/abs/2311.00735)

    本研究开发了一种示踪剂转化可逆神经网络（TC-INN），通过深度学习将FDG图像映射到DOPA图像，以解决DOPA在PET成像中广泛应用的挑战。

    

    正电子发射断层成像（PET）作为一种具有高生化敏感性的成像技术，已被广泛应用于脑疾病诊断和脑科学研究。由于不同示踪剂在同一区域呈现出不同效果，示踪剂的选择对PET成像变得越来越重要。目前，在神经精神治疗中广泛应用PET成像，发现6-18F-氟-3,4-二羟基-L-苯丙氨酸（DOPA）在这一领域比18F标记的氟代脱氧葡萄糖（FDG）更有效。然而，由于制备复杂性以及其他限制，DOPA远不如FDG被广泛使用。为解决这个问题，提出了一种示踪剂转化可逆神经网络（TC-INN）用于图像投影，通过深度学习将FDG图像映射到DOPA图像。通过从FDG到DOPA生成PET图像，获得更多的诊断信息。

    Positron emission tomography (PET), as an imaging technique with high biochemical sensitivity, has been widely used in diagnosis of encephalopathy and brain science research used in brain disease diagnosis and brain science research. Since different tracers present different effects on the same focal area, the choice of tracers is getting more significant for PET imaging. Nowadays, with the wide application of PET imaging in neuropsychiatric treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine (DOPA) has been found to be more effective than 18F-labeled fluorine-2-deoxyglucose (FDG) in this field. However, due to the complexity of its preparation and other limitations, DOPA is far less widely used than FDG. To address this issue, a tracer conversion invertible neural network (TC-INN) for image projection is developed to map FDG images to DOPA images through deep learning. More diagnostic information is obtained by generating PET images from FDG to DOPA. Specifically, the proposed TC-I
    
[^121]: tmn在#SMM4H 2023上的论文:比较用于检测自我报告COVID-19诊断推文的文本预处理技术

    tmn at #SMM4H 2023: Comparing Text Preprocessing Techniques for Detecting Tweets Self-reporting a COVID-19 Diagnosis. (arXiv:2311.00732v1 [cs.CL])

    [http://arxiv.org/abs/2311.00732](http://arxiv.org/abs/2311.00732)

    本文研究了用于检测自我报告COVID-19诊断推文的不同文本预处理技术，通过使用四个基于transformer的模型进行实验，并通过微调语言模型集成获得了比平均值高出4.1%的84.5%的F1得分。

    

    本文描述了一个用于SMM4H 2023任务1的系统。该任务的目标是自动区分那些自我报告COVID-19诊断的推文（例如，阳性检测、临床诊断或住院）和那些没有的推文。我们使用四个基于transformer的模型研究了不同的推文预处理技术。经过微调的语言模型集成获得了84.5%的F1得分，比平均值高出4.1%。

    The paper describes a system developed for Task 1 at SMM4H 2023. The goal of the task is to automatically distinguish tweets that self-report a COVID-19 diagnosis (for example, a positive test, clinical diagnosis, or hospitalization) from those that do not. We investigate the use of different techniques for preprocessing tweets using four transformer-based models. The ensemble of fine-tuned language models obtained an F1-score of 84.5%, which is 4.1% higher than the average value.
    
[^122]: 用正向相似性和簇离散学习提升聚类表示

    Enhancing Clustering Representations with Positive Proximity and Cluster Dispersion Learning. (arXiv:2311.00731v1 [cs.LG])

    [http://arxiv.org/abs/2311.00731](http://arxiv.org/abs/2311.00731)

    本论文提出了一种新的深度聚类方法，名为PIPCDR，通过结合正向实例相似性损失和簇离散正则化器，以提升聚类性能。

    

    当代深度聚类方法通常依赖对比或非对比技术来获取用于聚类任务的有效表示。对比方法利用负对来实现同质表示，但可能引入类冲突问题，可能导致聚类性能下降。相反，非对比技术可以防止类冲突，但可能产生非均匀表示，导致聚类崩溃。在本工作中，我们提出了一种新颖的端到端深度聚类方法，名为PIPCDR，旨在兼顾两种方法的优势并减轻它们的局限性。PIPCDR包括一个正向实例相似性损失和一个簇离散正则化器。正向实例相似性损失确保实例的增强视图与其采样邻居之间的对齐，通过在嵌入空间中选择真正的正对来增强簇内紧密度。同时，簇离散正则化器能够鼓励簇之间的分散性，进一步提高聚类性能。

    Contemporary deep clustering approaches often rely on either contrastive or non-contrastive techniques to acquire effective representations for clustering tasks. Contrastive methods leverage negative pairs to achieve homogenous representations but can introduce class collision issues, potentially compromising clustering performance. On the contrary, non-contrastive techniques prevent class collisions but may produce non-uniform representations that lead to clustering collapse. In this work, we propose a novel end-to-end deep clustering approach named PIPCDR, designed to harness the strengths of both approaches while mitigating their limitations. PIPCDR incorporates a positive instance proximity loss and a cluster dispersion regularizer. The positive instance proximity loss ensures alignment between augmented views of instances and their sampled neighbors, enhancing within-cluster compactness by selecting genuinely positive pairs within the embedding space. Meanwhile, the cluster disper
    
[^123]: 研究迁移学习和元学习的相对性能

    Investigating Relative Performance of Transfer and Meta Learning. (arXiv:2311.00727v1 [cs.LG])

    [http://arxiv.org/abs/2311.00727](http://arxiv.org/abs/2311.00727)

    本文研究了迁移学习和元学习作为解决有限数据学习问题的两种方法的相对性能，以建立一个在不同的机器学习场景中选择最适合的方法的稳健标准。

    

    过去十年来，机器学习领域取得了显著的进展。尽管图像识别系统在准确度方面取得了令人印象深刻的成就，但它们仍然依赖于大量的训练数据集。此外，一个重要挑战是在分布外性能方面表现不佳，这要求当神经网络遇到与其训练数据不一致的条件时重新训练。这个限制显著影响了自动驾驶技术的进展。这些紧迫问题引发了对使神经网络能够有效地从有限数据中学习的方法的广泛关注。本文介绍了一项旨在比较两种不同方法——迁移学习和元学习的研究成果，作为解决这个问题的潜在方案。总体目标是建立一个在不同的机器学习场景中选择最适合的方法的稳健标准。

    Over the past decade, the field of machine learning has experienced remarkable advancements. While image recognition systems have achieved impressive levels of accuracy, they continue to rely on extensive training datasets. Additionally, a significant challenge has emerged in the form of poor out-of-distribution performance, which necessitates retraining neural networks when they encounter conditions that deviate from their training data. This limitation has notably contributed to the slow progress in self-driving car technology. These pressing issues have sparked considerable interest in methods that enable neural networks to learn effectively from limited data. This paper presents the outcomes of an extensive investigation designed to compare two distinct approaches, transfer learning and meta learning, as potential solutions to this problem. The overarching objective was to establish a robust criterion for selecting the most suitable method in diverse machine learning scenarios. Bui
    
[^124]: 利用机器学习和大数据分析技术进行电信行业的欺诈分析

    Fraud Analytics Using Machine-learning & Engineering on Big Data (FAME) for Telecom. (arXiv:2311.00724v1 [cs.LG])

    [http://arxiv.org/abs/2311.00724](http://arxiv.org/abs/2311.00724)

    本文提出了一种工业化解决方案，利用自适应数据挖掘技术和大数据技术来准确、高效、低成本地检测电信行业的欺诈。已成功应用于检测国际收入分成欺诈，并发现了新的欺诈模式。

    

    电信行业由于欺诈行为每年全球损失463亿美元。过去使用数据挖掘和机器学习技术（除了规则导向方法）来进行欺诈检测，但效率很低，因为欺诈模式变化非常快。本文提出了一种工业化解决方案，采用自适应数据挖掘技术和大数据技术来准确、高效、低成本地检测欺诈并发现新的欺诈模式。该解决方案已成功应用于<5%的误报率检测国际收入分成欺诈。本研究使用了来自知名批发运营商和海外电信中转运营商的超过1TB的通话详单记录进行实证研究。

    Telecom industries lose globally 46.3 Billion USD due to fraud. Data mining and machine learning techniques (apart from rules oriented approach) have been used in past, but efficiency has been low as fraud pattern changes very rapidly. This paper presents an industrialized solution approach with self adaptive data mining technique and application of big data technologies to detect fraud and discover novel fraud patterns in accurate, efficient and cost effective manner. Solution has been successfully demonstrated to detect International Revenue Share Fraud with <5% false positive. More than 1 Terra Bytes of Call Detail Record from a reputed wholesale carrier and overseas telecom transit carrier has been used to conduct this study.
    
[^125]: 使用机器学习在文本、视听、音频或生理信号上进行共情检测

    Empathy Detection Using Machine Learning on Text, Audiovisual, Audio or Physiological Signals. (arXiv:2311.00721v1 [cs.HC])

    [http://arxiv.org/abs/2311.00721](http://arxiv.org/abs/2311.00721)

    本论文对共情检测领域的机器学习研究进行了综述和分析，包括文本、视听、音频和生理信号四种输入模态的处理和网络设计，以及评估协议和数据集的描述。

    

    共情是一个社交技能，表明一个个体理解他人的能力。近年来，共情引起了包括情感计算、认知科学和心理学在内的各个学科的关注。共情是一个依赖于上下文的术语，因此检测或识别共情在社会、医疗和教育等领域具有潜在的应用。尽管共情检测领域涉及范围广泛且有重叠，但从整体文献角度来看，利用机器学习的共情检测研究仍然相对较少。为此，我们系统收集和筛选了来自10个知名数据库的801篇论文，并分析了选定的54篇论文。我们根据共情检测系统的输入模态，即文本、视听、音频和生理信号，对论文进行分组。我们分别研究了特定模态的预处理和网络架构设计协议、常见数据集的描述和可用性详情，以及评估协议。

    Empathy is a social skill that indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science and Psychology. Empathy is a context-dependent term; thus, detecting or recognising empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection studies leveraging Machine Learning remains underexplored from a holistic literature perspective. To this end, we systematically collect and screen 801 papers from 10 well-known databases and analyse the selected 54 papers. We group the papers based on input modalities of empathy detection systems, i.e., text, audiovisual, audio and physiological signals. We examine modality-specific pre-processing and network architecture design protocols, popular dataset descriptions and availability details, and evaluation protocols. We fur
    
[^126]: 深度神经网络在自动说话人识别中不能学习超分段时间特征

    Deep Neural Networks for Automatic Speaker Recognition Do Not Learn Supra-Segmental Temporal Features. (arXiv:2311.00489v1 [cs.SD])

    [http://arxiv.org/abs/2311.00489](http://arxiv.org/abs/2311.00489)

    这项研究表明深度神经网络在自动说话人识别中无法充分模拟超分段时间特征，这为未来更好地利用完整语音信号进行研究提供了基础。

    

    深度神经网络在自动说话人识别和相关任务中取得了令人印象深刻的结果，但我们对于这些结果的具体原因了解甚少。以前的研究将成功的一部分归因于它们模拟超分段时间信息（SST）的能力，即除了谱特征外还学习语音的韵律和韵律特征。在本文中，我们（i）提出并应用一种新的测试方法，来量化最先进的神经网络在说话人识别方面的性能能够通过建模SST来解释到多大程度；并且（ii）提出几种强制网络更加关注SST的方法，并评估它们的优点。我们发现，即使被强制要求，一系列基于CNN和RNN的神经网络结构在说话人识别中也不能充分地模拟SST。这些结果为未来更好地利用完整语音信号进行研究提供了非常重要的基础。

    While deep neural networks have shown impressive results in automatic speaker recognition and related tasks, it is dissatisfactory how little is understood about what exactly is responsible for these results. Part of the success has been attributed in prior work to their capability to model supra-segmental temporal information (SST), i.e., learn rhythmic-prosodic characteristics of speech in addition to spectral features. In this paper, we (i) present and apply a novel test to quantify to what extent the performance of state-of-the-art neural networks for speaker recognition can be explained by modeling SST; and (ii) present several means to force respective nets to focus more on SST and evaluate their merits. We find that a variety of CNN- and RNN-based neural network architectures for speaker recognition do not model SST to any sufficient degree, even when forced. The results provide a highly relevant basis for impactful future research into better exploitation of the full speech sig
    
[^127]: JADE：基于语言的LLM安全评估平台

    JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])

    [http://arxiv.org/abs/2311.00286](http://arxiv.org/abs/2311.00286)

    JADE是一种基于语言分析的LLM安全评估平台，能够破坏广泛使用的中文和英文LLM，并生成高度威胁的不安全问题。

    

    本文介绍了JADE，一种针对语言分析的模糊测试平台，通过增强种子问题的语言复杂性，同时并始终能够破坏广泛使用的三类LLM：八个开源中文LLM，六个商业中文LLM和四个商业英文LLM。JADE为这三类LLM生成了三个安全基准，其中包含高度威胁的不安全问题：这些问题可以同时触发多个LLM的有害生成，平均不安全生成比例为70%（请参见下表），同时这些问题仍然是自然、流畅且保留了核心的不安全语义。我们在以下链接中发布了对商业英文LLM和开源英文LLM生成的基准演示：https://github.com/whitzard-ai/jade-db。对于对JADE生成的更多问题感兴趣的读者，请与我们联系。

    In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
    
[^128]: 改进的多变量时间序列预测的自动混合模型在BizITOps数据上的应用

    AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data. (arXiv:2310.20280v1 [cs.LG])

    [http://arxiv.org/abs/2310.20280](http://arxiv.org/abs/2310.20280)

    这篇论文提出了AutoMixer，一个基于时间序列基础模型的自动混合模型，通过通道压缩预训练和微调工作流技术，有效解耦了BizITOps数据中有用和嘈杂的跨通道交互，提高了多变量时间序列预测的性能。

    

    业务过程的效率依赖于业务关键绩效指标（Biz-KPIs），而IT故障可能对其产生负面影响。BizITOps数据将Biz-KPIs和IT事件通道融合成多变量时间序列数据。提前预测Biz-KPIs可以通过主动的纠正措施提高效率和收益。然而，BizITOps数据通常展示出Biz-KPIs和IT事件之间有用和嘈杂的跨通道交互，需要有效解耦。当使用现有的多变量预测模型时，这导致预测性能不佳。为了解决这个问题，我们引入了一个称为AutoMixer的时间序列基础模型（FM）方法，该方法基于新颖的通道压缩预训练和微调工作流技术。AutoMixer利用自动编码器进行通道压缩的预训练，并将其与先进的TSMixer模型集成，用于多变量时间序列预测。这种融合极大地增强了TSM

    The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSM
    
[^129]: 关于使用梯度流学习高斯多索引模型的研究

    On Learning Gaussian Multi-index Models with Gradient Flow. (arXiv:2310.19793v1 [stat.ML])

    [http://arxiv.org/abs/2310.19793](http://arxiv.org/abs/2310.19793)

    本研究探讨了在高维高斯数据的多索引回归问题中，通过梯度流学习低秩线性投影和低维连接函数，建立了全局收敛性和定量描述的算法。

    

    我们研究了高维高斯数据的多索引回归问题中的梯度流。多索引函数由未知的低秩线性投影和任意未知的低维连接函数组成。因此，它们构成了神经网络中特征学习的自然模板。我们考虑了一个两时间尺度的算法，其中低维连接函数通过非参数模型比参数化低秩投影的低维空间更快地学习。通过适当地利用框架的相关矩阵上的矩阵半群结构，我们建立了由Grassmannian人口梯度流动力学引起的全局收敛性，并对其相关的“鞍点到鞍点”动力学提供了定量描述。值得注意的是，每个鞍的时间尺度可以明确地用目标连接函数的适当Hermite分解来表征。与这些位置相反的是。

    We study gradient flow on the multi-index regression problem for high-dimensional Gaussian data. Multi-index functions consist of a composition of an unknown low-rank linear projection and an arbitrary unknown, low-dimensional link function. As such, they constitute a natural template for feature learning in neural networks.  We consider a two-timescale algorithm, whereby the low-dimensional link function is learnt with a non-parametric model infinitely faster than the subspace parametrizing the low-rank projection. By appropriately exploiting the matrix semigroup structure arising over the subspace correlation matrices, we establish global convergence of the resulting Grassmannian population gradient flow dynamics, and provide a quantitative description of its associated `saddle-to-saddle' dynamics. Notably, the timescales associated with each saddle can be explicitly characterized in terms of an appropriate Hermite decomposition of the target link function. In contrast with these pos
    
[^130]: 基于神经仿真器的无梯度在线学习亚网格尺度动力学

    Gradient-free online learning of subgrid-scale dynamics with neural emulators. (arXiv:2310.19385v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2310.19385](http://arxiv.org/abs/2310.19385)

    本文提出了一种利用神经仿真器在线训练亚网格参数化的算法，通过后验损失函数适应非可微分数值求解器，并通过时间积分步骤允许梯度传播。实验证明，将神经仿真器和参数化组件分别用相应的损失量进行训练是必要的，以最小化某些近似偏差的传播。

    

    本文提出了一种通用算法，用于在线训练基于机器学习的亚网格参数化，并通过后验损失函数适应非可微分数值求解器。所提出的方法利用神经仿真器训练简化状态空间求解器的近似值，然后通过时间积分步骤允许梯度传播。该算法能够在不计算原始求解器梯度的情况下恢复大部分在线策略的好处。实验证明，将神经仿真器和参数化组件分别用相应的损失量进行训练是必要的，以最小化某些近似偏差的传播。

    In this paper, we propose a generic algorithm to train machine learning-based subgrid parametrizations online, i.e., with $\textit{a posteriori}$ loss functions for non-differentiable numerical solvers. The proposed approach leverage neural emulators to train an approximation of the reduced state-space solver, which is then used to allows gradient propagation through temporal integration steps. The algorithm is able to recover most of the benefit of online strategies without having to compute the gradient of the original solver. It is demonstrated that training the neural emulator and parametrization components separately with respective loss quantities is necessary in order to minimize the propagation of some approximation bias.
    
[^131]: 理解和改进集成对抗防御

    Understanding and Improving Ensemble Adversarial Defense. (arXiv:2310.18477v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.18477](http://arxiv.org/abs/2310.18477)

    通过新的错误理论，作者提出了一个名为iGAT的方法用于改进集成对抗防御，该方法通过选择性地分配对抗样本和正则化来提高防御效果。该方法在各种情况下进行了测试并取得成功。

    

    集成策略已经在对抗防御中变得流行，它训练多个基本分类器以协同方式防御对抗攻击。尽管在经验上取得了成功，但对于为什么对抗训练的分类器集成比单个分类器更强大的理论解释仍然不清楚。为了填补这个空白，我们开发了一个新的错误理论，专门用于理解集成对抗防御，在对抗防御场景中展示了可以证明的在具有挑战性的样本集上的0-1损失降低。在这个理论的指导下，我们提出了一种改进集成对抗防御的有效方法，命名为交互全局对抗训练（iGAT）。该提案包括（1）一种概率分配规则，将全局上对集成具有挑战性的对抗样本选择性地分配给不同的基本分类器，以及（2）一个正则化项，以弥补基本分类器的严重弱点。在各种情况下进行了测试。

    The strategy of ensemble has become popular in adversarial defense, which trains multiple base classifiers to defend against adversarial attacks in a cooperative manner. Despite the empirical success, theoretical explanations on why an ensemble of adversarially trained classifiers is more robust than single ones remain unclear. To fill in this gap, we develop a new error theory dedicated to understanding ensemble adversarial defense, demonstrating a provable 0-1 loss reduction on challenging sample sets in an adversarial defense scenario. Guided by this theory, we propose an effective approach to improve ensemble adversarial defense, named interactive global adversarial training (iGAT). The proposal includes (1) a probabilistic distributing rule that selectively allocates to different base classifiers adversarial examples that are globally challenging to the ensemble, and (2) a regularization term to rescue the severest weaknesses of the base classifiers. Being tested over various exis
    
[^132]: 意义表征来自自回归模型中的轨迹

    Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.18348](http://arxiv.org/abs/2310.18348)

    本文提出了一种从自回归语言模型中提取意义表征的方法，通过考虑输入文本的所有可能轨迹的分布。这种方法可以模拟非对称关系，且在语义相似性任务上优于其他方法。

    

    我们提出通过考虑扩展输入文本的所有可能轨迹的分布来从自回归语言模型中提取意义表征。这种策略是无提示的，不需要微调，并适用于任何预训练的自回归模型。此外，与基于向量的表征不同，基于分布的表征还可以通过使用似然函数之间的代数运算来建模非对称关系（例如，逻辑蕴涵的方向，上位词/下位词关系）。这些想法基于语义的分布视角，并与自动机理论中的标准构造相连接，但据我们所知，它们尚未应用于现代语言模型。我们通过实验证明，从大型模型获得的表征与人类注释很好地一致，在语义相似性任务上优于其他零样本和无提示方法，并可用于解决更复杂的蕴涵和包含任务。

    We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover, unlike vector-based representations, distribution-based representations can also model asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym relations) by using algebraic operations between likelihood functions. These ideas are grounded in distributional perspectives on semantics and are connected to standard constructions in automata theory, but to our knowledge they have not been applied to modern language models. We empirically show that the representations obtained from large models align well with human annotations, outperform other zero-shot and prompt-free methods on semantic similarity tasks, and can be used to solve more complex entailment and containment tasks 
    
[^133]: 深度转换高斯过程

    Deep Transformed Gaussian Processes. (arXiv:2310.18230v1 [cs.LG])

    [http://arxiv.org/abs/2310.18230](http://arxiv.org/abs/2310.18230)

    本文提出了一种名为深度转换高斯过程（DTGPs）的转换高斯过程（TGPs）的推广，该模型采用串联层级的随机过程，并实现了相对于TGPs和DGPs的灵活性增强。通过使用变分推理，可以近似所需的计算，从而得到了简单直接的推理算法扩展。

    

    转换高斯过程（TGPs）是通过使用可逆转换从先验过程（通常是高斯过程）中转换样本来指定的随机过程，从而增加了基本过程的灵活性。此外，与通过高斯过程的层级串联构造的深度高斯过程（DGPs）相比，TGPs实现了竞争性结果。在这项工作中，我们提出了一种名为深度转换高斯过程（DTGPs）的TGP推广，它遵循串联随机过程层的趋势。更准确地说，我们得到了一个多层模型，其中每一层都是一个TGP。这种推广意味着相对于TGPs和DGPs都提高了灵活性。在这样的模型中进行精确推理是困难的。但是，我们展示了可以使用变分推理来近似所需的计算，从而得到了流行的DSVI推理算法的直接扩展。

    Transformed Gaussian Processes (TGPs) are stochastic processes specified by transforming samples from the joint distribution from a prior process (typically a GP) using an invertible transformation; increasing the flexibility of the base process.  Furthermore, they achieve competitive results compared with Deep Gaussian Processes (DGPs), which are another generalization constructed by a hierarchical concatenation of GPs. In this work, we propose a generalization of TGPs named Deep Transformed Gaussian Processes (DTGPs), which follows the trend of concatenating layers of stochastic processes. More precisely, we obtain a multi-layer model in which each layer is a TGP. This generalization implies an increment of flexibility with respect to both TGPs and DGPs. Exact inference in such a model is intractable. However, we show that one can use variational inference to approximate the required computations yielding a straightforward extension of the popular DSVI inference algorithm Salimbeni e
    
[^134]: 检测防御: 光流中对抗性贴片攻击的空洞承诺

    Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow. (arXiv:2310.17403v1 [cs.CV])

    [http://arxiv.org/abs/2310.17403](http://arxiv.org/abs/2310.17403)

    本文对光流中的对抗性贴片攻击进行了研究，发现目前的检测和去除防御策略不仅降低了光流质量，同时也损害了抵御贴片攻击的鲁棒性。

    

    当放置在任意场景位置时，对抗性贴片破坏了光流预测的可靠性。因此，它们对现实世界中的运动检测及其下游应用构成了真实威胁。潜在的解决方案是检测和去除对抗性贴片的防御策略，但其对底层运动预测的影响尚未被研究。在本文中，我们对当前可用的检测和去除防御策略ILP和LGS对一系列先进光流方法进行了彻底的研究，并阐明了它们对最终光流预测的质量和鲁棒性的副作用。特别地，我们实施了防御感知攻击，以调查当前的防御是否能够抵御考虑防御机制的攻击。我们的实验得出了两个令人惊讶的结果：检测和去除防御策略不仅降低了良好场景下的光流质量，同时也损害了针对贴片攻击的鲁棒性。

    Adversarial patches undermine the reliability of optical flow predictions when placed in arbitrary scene locations. Therefore, they pose a realistic threat to real-world motion detection and its downstream applications. Potential remedies are defense strategies that detect and remove adversarial patches, but their influence on the underlying motion prediction has not been investigated. In this paper, we thoroughly examine the currently available detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art optical flow methods, and illuminate their side effects on the quality and robustness of the final flow predictions. In particular, we implement defense-aware attacks to investigate whether current defenses are able to withstand attacks that take the defense mechanism into account. Our experiments yield two surprising results: Detect-and-remove defenses do not only lower the optical flow quality on benign scenes, in doing so, they also harm the robustness under patc
    
[^135]: 通过暂时卷积神经网络实现的全新化学反应生成

    De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks. (arXiv:2310.17341v1 [cs.LG])

    [http://arxiv.org/abs/2310.17341](http://arxiv.org/abs/2310.17341)

    本研究结合递归神经网络（RNN）和临时卷积神经网络（TCN），使用新的反应Smiles-like表示实现了全新的化学反应生成，并通过迁移学习发现微调协议对模型生成范围有重要影响。

    

    我们在使用新颖的反应Smiles-like表示（CGRSmiles）时，将递归神经网络（RNN）和临时卷积神经网络（TCN）相结合，以实现全新的反应生成，并直接融合了原子映射。递归神经网络以其自回归特性而闻名，并经常在语言建模中使用，直接应用于SMILES生成。相对较新的TCN具有类似的性质，具有广泛的感受野，并遵守自然语言处理（NLP）所需的因果性。通过TCN和RNN表达的两种潜在表示的组合相比仅使用RNN时具有更好的性能。此外，研究还表明，通过迁移学习将不同的微调协议应用于感兴趣的数据集时，对模型的生成范围有深远影响。

    We present here a combination of two networks, Recurrent Neural Networks (RNN) and Temporarily Convolutional Neural Networks (TCN) in de novo reaction generation using the novel Reaction Smiles-like representation of reactions (CGRSmiles) with atom mapping directly incorporated. Recurrent Neural Networks are known for their autoregressive properties and are frequently used in language modelling with direct application to SMILES generation. The relatively novel TCNs possess similar properties with wide receptive field while obeying the causality required for natural language processing (NLP). The combination of both latent representations expressed through TCN and RNN results in an overall better performance compared to RNN alone. Additionally, it is shown that different fine-tuning protocols have a profound impact on generative scope of the model when applied on a dataset of interest via transfer learning.
    
[^136]: MimicTouch: 使用多模态触觉反馈学习人类的控制策略

    MimicTouch: Learning Human's Control Strategy with Multi-Modal Tactile Feedback. (arXiv:2310.16917v1 [cs.RO])

    [http://arxiv.org/abs/2310.16917](http://arxiv.org/abs/2310.16917)

    MimicTouch是一种新的框架，能够模仿人类的触觉引导控制策略，通过收集来自人类示范者的多模态触觉数据集，来学习并执行复杂任务。

    

    在机器人技术和人工智能领域，触觉处理的整合变得越来越重要，特别是在学习执行像对准和插入这样复杂任务时。然而，现有研究主要依赖机器人遥操作数据和强化学习，忽视了人类受触觉反馈引导下的控制策略所提供的丰富见解。为了利用人类感觉，现有的从人类学习的方法主要利用视觉反馈，常常忽视了人类本能地利用触觉反馈完成复杂操作的宝贵经验。为了填补这一空白，我们引入了一种新框架"MimicTouch"，模仿人类的触觉引导控制策略。在这个框架中，我们首先从人类示范者那里收集多模态触觉数据集，包括人类触觉引导的控制策略来完成任务。接下来的步骤涉及指令的传递，其中机器人通过模仿人类的触觉引导策略来执行任务。

    In robotics and artificial intelligence, the integration of tactile processing is becoming increasingly pivotal, especially in learning to execute intricate tasks like alignment and insertion. However, existing works focusing on tactile methods for insertion tasks predominantly rely on robot teleoperation data and reinforcement learning, which do not utilize the rich insights provided by human's control strategy guided by tactile feedback. For utilizing human sensations, methodologies related to learning from humans predominantly leverage visual feedback, often overlooking the invaluable tactile feedback that humans inherently employ to finish complex manipulations. Addressing this gap, we introduce "MimicTouch", a novel framework that mimics human's tactile-guided control strategy. In this framework, we initially collect multi-modal tactile datasets from human demonstrators, incorporating human tactile-guided control strategies for task completion. The subsequent step involves instruc
    
[^137]: 对表示一致性达成共识

    Getting aligned on representational alignment. (arXiv:2310.13018v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.13018](http://arxiv.org/abs/2310.13018)

    该论文研究了生物和人工信息处理系统的表示一致性，探讨了不同系统之间的表示是否一致以及如何调整表示以更好地匹配其他系统。为了改善领域之间的交流，提出了一个统一的框架作为共同语言。

    

    生物和人工信息处理系统构建可以用来进行分类、推理、规划、导航和决策的世界表示。这些多样化系统所构建的表示在多大程度上是一致的？即使表示不同，是否仍然能够导致相同的行为？系统如何修改它们的表示以更好地匹配另一个系统的表示？这些关于表示一致性研究的问题是当代认知科学、神经科学和机器学习中一些最活跃的研究领域的核心。不幸的是，对于对表示一致性感兴趣的研究社区之间的知识转移有限，其中大部分在一个领域的进展最终会在另一个领域独立地重新发现，而更广泛的领域间交流将是有利的。为了改善领域之间的交流，我们提出了一个统一的框架，可以作为一种共同的语言。

    Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language b
    
[^138]: 快速模型去偏置与机器取消学习

    Fast Model Debias with Machine Unlearning. (arXiv:2310.12560v1 [cs.LG])

    [http://arxiv.org/abs/2310.12560](http://arxiv.org/abs/2310.12560)

    这篇论文提出了一种快速模型去偏置的框架（FMD），可以有效识别、评估和消除深度神经网络中的偏见，解决了现有方法在成本和解释性方面的不足。

    

    最近的研究发现，深度神经网络在许多现实场景中可能表现出偏差的行为。例如，在一个大规模的人脸识别数据集CelebA上训练的深度网络倾向于预测女性的金色头发和男性的黑色头发。这些偏差不仅危害了模型的稳健性，而且会持续和放大社会偏见，这对于医疗、招聘等自动决策过程尤其令人担忧，因为它们可能加剧不同群体之间的不公平经济和社会不平等。现有的去偏置方法在偏见标记或模型重新训练方面成本高昂，同时也在阐明模型内部偏见的起源方面存在不足。为此，我们提出了一个快速模型去偏置框架(FMD)，它提供了一种有效的方法来识别、评估和消除训练模型中固有的偏见。FMD通过显式的反事实机制来识别偏置属性。

    Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual 
    
[^139]: 透明的基于概念解释的异常检测

    Transparent Anomaly Detection via Concept-based Explanations. (arXiv:2310.10702v1 [cs.LG])

    [http://arxiv.org/abs/2310.10702](http://arxiv.org/abs/2310.10702)

    本论文提出了一种透明的基于概念解释的异常检测方法（ACE），能够提供人类可解释的解释和异常预测。该方法在推进异常检测的透明度的同时，实现了有效的人机交互，并且在性能上要么更高，要么与黑盒不可解释模型相当。

    

    深度学习技术的进步提升了异常检测的性能。然而，现实世界和安全关键应用需要超出准确性的透明度和推理能力。异常检测的任务集中在找出给定样本是否遵循学习到的分布。现有方法缺乏对其结果进行清晰解释的能力。因此，为了克服这一挑战，我们提出了透明的异常检测概念解释（ACE）方法。ACE能够以概念的形式提供人类可解释的解释和异常预测。据我所知，这是第一篇提出设计可解释异常检测的论文。除了促进异常检测的透明度，它还可以实现有效的人机交互。我们提出的模型结果要么更高，要么与黑盒不可解释模型相当。我们验证了ACE在三个现实数据集上的性能。

    Advancements in deep learning techniques have given a boost to the performance of anomaly detection. However, real-world and safety-critical applications demand a level of transparency and reasoning beyond accuracy. The task of anomaly detection (AD) focuses on finding whether a given sample follows the learned distribution. Existing methods lack the ability to reason with clear explanations for their outcomes. Hence to overcome this challenge, we propose Transparent {A}nomaly Detection {C}oncept {E}xplanations (ACE). ACE is able to provide human interpretable explanations in the form of concepts along with anomaly prediction. To the best of our knowledge, this is the first paper that proposes interpretable by-design anomaly detection. In addition to promoting transparency in AD, it allows for effective human-model interaction. Our proposed model shows either higher or comparable results to black-box uninterpretable models. We validate the performance of ACE across three realistic data
    
[^140]: 使用无监督机器学习揭示电力系统恢复曲线的基本特性

    Unraveling Fundamental Properties of Power System Resilience Curves using Unsupervised Machine Learning. (arXiv:2310.10030v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10030](http://arxiv.org/abs/2310.10030)

    本研究使用无监督机器学习对电力系统的恢复曲线进行了研究，发现了两种主要的恢复曲线原型：三角形曲线和梯形曲线，分别表征了快速恢复和逐步恢复的恢复过程。

    

    基础设施恢复的标准模型——恢复三角，一直是表征和量化基础设施恢复能力的主要方式。然而，这个理论模型只提供了一个适用于所有基础设施系统的框架。现有研究大部分是基于模拟系统性能构建的分析模型来研究基础设施恢复曲线的特征。有限的实证研究限制了我们全面理解和预测基础设施系统的恢复特性的能力。为了填补这一空白，本研究考察了与三次重大极端天气事件中断电有关的200多个恢复曲线。通过无监督机器学习，我们研究了不同曲线原型以及每个恢复曲线原型的基本特性。结果显示，电力系统恢复曲线存在两种主要原型：三角形曲线和梯形曲线。三角形曲线表征了恢复过程的快速恢复，而梯形曲线表示了																恢复过程的逐步恢复。

    The standard model of infrastructure resilience, the resilience triangle, has been the primary way of characterizing and quantifying infrastructure resilience. However, the theoretical model merely provides a one-size-fits-all framework for all infrastructure systems. Most of the existing studies examine the characteristics of infrastructure resilience curves based on analytical models constructed upon simulated system performance. Limited empirical studies hindered our ability to fully understand and predict resilience characteristics in infrastructure systems. To address this gap, this study examined over 200 resilience curves related to power outages in three major extreme weather events. Using unsupervised machine learning, we examined different curve archetypes, as well as the fundamental properties of each resilience curve archetype. The results show two primary archetypes for power system resilience curves, triangular, and trapezoidal curves. Triangular curves characterize resil
    
[^141]: 迈向端到端的基于生成大型语言模型的4位推理

    Towards End-to-end 4-Bit Inference on Generative Large Language Models. (arXiv:2310.09259v1 [cs.LG])

    [http://arxiv.org/abs/2310.09259](http://arxiv.org/abs/2310.09259)

    本论文介绍了一种使用名为QUIK的混合量化策略，在保持良好精度的同时实现大型生成模型的实际速度提升，通过将权重和激活值转换为4位，并提供高效率的逐层运行时GPU内核，实现了高达3.1倍的实际端到端吞吐量提升。

    

    我们展示了对于像LLaMA和OPT这样的大型生成模型，大多数推理计算可以通过将权重和激活值都转换为4位来完成，这种方式可以在保持良好精度的同时实现实际速度提升。我们通过一种名为QUIK的混合量化策略实现了这一目标，该策略将大部分权重和激活值压缩为4位，同时保留一些离群权重和激活值的较高精度。关键是，我们的方案考虑到了计算效率：我们提供了高效率的逐层运行时GPU内核，相对于FP16执行可以实现高达3.1倍的实际端到端吞吐量提升。我们在https://github.com/IST-DASLab/QUIK上提供了代码和模型。

    We show that the majority of the inference computations for large generative models such as LLaMA and OPT can be performed with both weights and activations being cast to 4 bits, in a way that leads to practical speedups while at the same time maintaining good accuracy. We achieve this via a hybrid quantization strategy called QUIK, which compresses most of the weights and activations to 4-bit, while keeping some outlier weights and activations in higher-precision. Crucially, our scheme is designed with computational efficiency in mind: we provide GPU kernels with highly-efficient layer-wise runtimes, which lead to practical end-to-end throughput improvements of up to 3.1x relative to FP16 execution. Code and models are provided at https://github.com/IST-DASLab/QUIK.
    
[^142]: 基于流形的Eikonal方程：可微流形上的测地距离和流动

    Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds. (arXiv:2310.06157v1 [cs.CG])

    [http://arxiv.org/abs/2310.06157](http://arxiv.org/abs/2310.06157)

    本研究提出了一种基于模型的参数化方法，利用测地线和流动来描述可微流形上的距离和长度最小化曲线。这为在不同iable流形上进行统计和降阶建模提供了机会。

    

    通过机器学习模型发现的流形提供了底层数据的紧凑表示。这些流形上的测地线定义了局部长度最小化曲线，并提供了距离的概念，这对于降阶建模、统计推断和插值至关重要。在这项工作中，我们提出了一种基于模型的参数化方法来表示流形上的距离场和测地流动，利用扩展的Eikonal方程的解决方案。我们展示了流形的几何特性如何影响距离场，并利用测地流动直接获得全局长度最小化曲线。这项工作为在可微流形上进行统计和降阶建模提供了机会。

    Manifolds discovered by machine learning models provide a compact representation of the underlying data. Geodesics on these manifolds define locally length-minimising curves and provide a notion of distance, which are key for reduced-order modelling, statistical inference, and interpolation. In this work, we propose a model-based parameterisation for distance fields and geodesic flows on manifolds, exploiting solutions of a manifold-augmented Eikonal equation. We demonstrate how the geometry of the manifold impacts the distance field, and exploit the geodesic flow to obtain globally length-minimising curves directly. This work opens opportunities for statistics and reduced-order modelling on differentiable manifolds.
    
[^143]: 通过类似样本聚类学习：对模型泛化的精确分析

    Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization. (arXiv:2310.04015v1 [cs.LG])

    [http://arxiv.org/abs/2310.04015](http://arxiv.org/abs/2310.04015)

    本文提出了一种名为“类似样本聚类”的技术，通过替换个体的敏感特征为聚类的平均值来增强隐私。通过对使用匿名聚类中心训练模型的精确分析，我们揭示了不同模型组成部分对泛化误差的影响，并证明在某些高维情况下，使用匿名聚类中心进行训练可以取得更好的效果。

    

    尽管个性化推荐系统变得越来越流行，但确保用户数据的保护仍然是这些学习系统开发中的一个重要关注点。增强隐私的常见方法是使用匿名数据而不是个体数据来训练模型。在本文中，我们探索了一种名为“类似样本聚类”的自然技术，它涉及将个体的敏感特征替换为聚类的平均值。我们对使用匿名聚类中心训练模型如何影响其泛化能力进行了精确的分析。我们关注一个渐近情况，即训练集的大小与特征维度成比例增长。我们的分析基于凸高斯极小化极大定理（Convex Gaussian Minimax Theorem，CGMT），使我们能够在理论上理解不同模型组成部分对泛化误差的作用。此外，我们证明在某些高维情况下，通过匿名聚类中心进行训练能够取得更好的效果。

    While personalized recommendations systems have become increasingly popular, ensuring user data protection remains a paramount concern in the development of these learning systems. A common approach to enhancing privacy involves training models using anonymous data rather than individual data. In this paper, we explore a natural technique called \emph{look-alike clustering}, which involves replacing sensitive features of individuals with the cluster's average values. We provide a precise analysis of how training models using anonymous cluster centers affects their generalization capabilities. We focus on an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT) and allows us to theoretically understand the role of different model components on the generalization error. In addition, we demonstrate that in certain high-dimensional regimes, training over anonymous cluster cente
    
[^144]: 面向领域感知的联邦学习的双提示调优

    Dual Prompt Tuning for Domain-Aware Federated Learning. (arXiv:2310.03103v1 [cs.LG])

    [http://arxiv.org/abs/2310.03103](http://arxiv.org/abs/2310.03103)

    本文提出了一种面向领域感知的联邦学习方法，通过双提示调优实现领域适应。实验结果表明，该方法在联邦学习中具有显著的效果。

    

    联邦学习是一种分布式机器学习范 paradigm，它允许多个客户端使用本地数据共同训练一个共享模型。然而，由于客户之间普遍存在领域变化，传统的联邦学习算法往往难以很好地泛化。在这项工作中，我们考虑了一个具有挑战性但现实的联邦学习场景，其中每个客户端的训练数据来自不同的领域。我们通过利用提示学习技术来解决领域变化的挑战，并提出了一种名为联邦双提示调优（Fed-DPT）的新方法。具体而言，Fed-DPT采用了一个预训练的视觉语言模型，然后应用了视觉和文本提示调优来促进分布式数据上的领域适应。大量的Fed-DPT实验结果表明，它在领域感知的联邦学习中具有显著的效果。

    Federated learning is a distributed machine learning paradigm that allows multiple clients to collaboratively train a shared model with their local data. Nonetheless, conventional federated learning algorithms often struggle to generalize well due to the ubiquitous domain shift across clients. In this work, we consider a challenging yet realistic federated learning scenario where the training data of each client originates from different domains. We address the challenges of domain shift by leveraging the technique of prompt learning, and propose a novel method called Federated Dual Prompt Tuning (Fed-DPT). Specifically, Fed-DPT employs a pre-trained vision-language model and then applies both visual and textual prompt tuning to facilitate domain adaptation over decentralized data. Extensive experiments of Fed-DPT demonstrate its significant effectiveness in domain-aware federated learning. With a pre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT attains 68.4% av
    
[^145]: 通过几何深度学习实现可微分的化学物理模型，用于基于梯度的混合物性能优化

    Differentiable Chemical Physics by Geometric Deep Learning for Gradient-based Property Optimization of Mixtures. (arXiv:2310.03047v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.03047](http://arxiv.org/abs/2310.03047)

    本文开发了一个不同iable的化学物理框架DiffMix，利用几何深度学习将分子物种、组成和环境条件映射到混合物物理定律中的物理系数上。通过创建可学习的物理系数，我们扩展了混合物热力学和输运定律，并展示了DiffMix相比纯数据驱动变量改进的预测准确性和模型鲁棒性。

    

    化学混合物满足多目标性能度量和约束条件，可用于化学过程和电化学设备。在这项工作中，我们开发了一种不同iable的化学物理框架DiffMix，利用几何深度学习（GDL）将分子物种，组成和环境条件映射到混合物物理定律中的物理系数上。我们通过创建可学习的物理系数来扩展混合物热力学和输运定律，其中我们使用图神经网络作为分子编码器，并强制实现逐分量的排列不变性。我们从二元混合物的热力学开始评估我们的模型，并进一步在多组分电解质混合物上对其输运性质进行基准测试，以测试模型的泛化能力。我们展示了DiffMix的预测准确性和模型鲁棒性优于纯数据驱动变量的改进。

    Chemical mixtures, satisfying multi-objective performance metrics and constraints, enable their use in chemical processes and electrochemical devices. In this work, we develop a differentiable chemical-physics framework for modeling chemical mixtures, DiffMix, where geometric deep learning (GDL) is leveraged to map from molecular species, compositions and environment conditions, to physical coefficients in the mixture physics laws. In particular, we extend mixture thermodynamic and transport laws by creating learnable physical coefficients, where we use graph neural networks as the molecule encoder and enforce component-wise permutation-invariance. We start our model evaluations with thermodynamics of binary mixtures, and further benchmarked multicomponent electrolyte mixtures on their transport properties, in order to test the model generalizability. We show improved prediction accuracy and model robustness of DiffMix than its purely data-driven variants. Furthermore, we demonstrate t
    
[^146]: 1D-CapsNet-LSTM: 基于深度学习的多步股指预测模型

    1D-CapsNet-LSTM: A Deep Learning-Based Model for Multi-Step Stock Index Forecasting. (arXiv:2310.02090v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.02090](http://arxiv.org/abs/2310.02090)

    本研究提出了一个基于一维CapsNet和LSTM网络的混合模型，用于多步股指预测。该模型利用CapsNet从序列数据中生成高级胶囊，同时利用LSTM网络捕获时间依赖关系。模型在真实股票市场指数上表现优于基线模型。

    

    多步股指预测在金融领域对于明智决策至关重要。目前关于此任务的预测方法常常由于数据的随机性和不稳定性产生不令人满意的结果，突显了对高级预测模型的需求。鉴于胶囊网络（CapsNet）在各种预测和分类任务中优于CNN，本研究探讨了将一维CapsNet与LSTM网络结合起来进行多步股指预测的潜力。为此，引入了一个混合的一维CapsNet-LSTM模型，该模型利用一维CapsNet从序列数据中生成高级胶囊，并利用LSTM网络捕获时间依赖关系。为了保持不同预测时间范围上的随机依赖关系，采用了多输入多输出（MIMO）策略。模型在包括标普500、道琼斯工业平均指数、纳斯达克综合指数和纽交所在内的真实股票市场指数上进行了评估，并与基线模型进行了比较。

    Multi-step stock index forecasting is vital in finance for informed decision-making. Current forecasting methods on this task frequently produce unsatisfactory results due to the inherent data randomness and instability, thereby underscoring the demand for advanced forecasting models. Given the superiority of capsule network (CapsNet) over CNN in various forecasting and classification tasks, this study investigates the potential of integrating a 1D CapsNet with an LSTM network for multi-step stock index forecasting. To this end, a hybrid 1D-CapsNet-LSTM model is introduced, which utilizes a 1D CapsNet to generate high-level capsules from sequential data and a LSTM network to capture temporal dependencies. To maintain stochastic dependencies over different forecasting horizons, a multi-input multi-output (MIMO) strategy is employed. The model's performance is evaluated on real-world stock market indices, including S&P 500, DJIA, IXIC, and NYSE, and compared to baseline models, including
    
[^147]: 基于评分的数据同化在双层拟地转动模型中的应用

    Score-based Data Assimilation for a Two-Layer Quasi-Geostrophic Model. (arXiv:2310.01853v1 [stat.ML])

    [http://arxiv.org/abs/2310.01853](http://arxiv.org/abs/2310.01853)

    本论文评估了基于评分的数据同化方法在高维度的地球物理动力系统中的可扩展性，并通过在双层拟地转动模型上的实验证明了该方法的良好性能。

    

    数据同化解决了在给定嘈杂或不完整观测情况下，确定动力系统可行状态轨迹的问题。在地球科学中，由于地球物理动力系统的高维度性，往往超过了数百万维度，因此存在挑战。本文评估了基于评分的数据同化（SDA）这一新颖的数据同化方法在此类系统中的可扩展性。我们提出了针对评分网络架构的修改，旨在显著减少内存消耗和执行时间。我们在一个双层拟地转动模型中展示了有希望的结果。

    Data assimilation addresses the problem of identifying plausible state trajectories of dynamical systems given noisy or incomplete observations. In geosciences, it presents challenges due to the high-dimensionality of geophysical dynamical systems, often exceeding millions of dimensions. This work assesses the scalability of score-based data assimilation (SDA), a novel data assimilation method, in the context of such systems. We propose modifications to the score network architecture aimed at significantly reducing memory consumption and execution time. We demonstrate promising results for a two-layer quasi-geostrophic model.
    
[^148]: 贝叶斯设计原则用于频率式序贯学习问题

    Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00806](http://arxiv.org/abs/2310.00806)

    该论文提出了一种贝叶斯设计原则用于优化频率式序贯学习问题的通用理论，通过生成“算法信念”并使用贝叶斯后验进行决策，实现了无先验的贝叶斯类型算法在对抗性环境中的泛化和优化应用。

    

    我们提出了一种通用理论，用于优化频率误差求和的序贯学习问题，可以通过统一的贝叶斯原则得到高效的强化学习和赌博机算法。我们提出了一种新颖的优化方法，在每一轮生成“算法信念”，并使用贝叶斯后验进行决策。我们提出的优化目标是创建“算法信念”，我们将其称为“算法信息比”，有效地表征了任何算法的频率误差的内在复杂度度量。据我们所知，这是第一种将贝叶斯式算法无先验地并且在对抗性环境中以通用和最优方式应用的系统方法。此外，这些算法简单且通常容易实现。作为一个重要应用，我们提出了一种新的多臂赌博机算法，在随机、对抗和非...

    We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to generate "algorithmic beliefs" at each round, and use Bayesian posteriors to make decisions. The optimization objective to create "algorithmic beliefs," which we term "Algorithmic Information Ratio," represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. To the best of our knowledge, this is the first systematical approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the "best-of-all-worlds" empirical performance in the stochastic, adversarial, and non
    
[^149]: MiliPoint: 一种毫米波雷达的点云数据集

    MiliPoint: A Point Cloud Dataset for mmWave Radar. (arXiv:2309.13425v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13425](http://arxiv.org/abs/2309.13425)

    MiliPoint是一个大规模、开放的点云数据集，用于探索如何利用毫米波雷达进行人体活动识别。与现有数据集相比，它具有更大的规模、更多样化的人体活动表示，并涵盖了人体活动识别的所有关键任务。

    

    毫米波雷达作为传统基于摄像机的系统的一种有吸引力且经济实惠的替代方案，已成为人体活动感知的一种理想选择。毫米波雷达是一种非侵入式技术，提供更好的用户隐私保护。然而，作为一种基于射频的技术，毫米波雷达依赖于捕获物体反射信号，因此相对于摄像机来说更容易受到干扰。这对于深度学习社区提出了一个有趣的问题：我们能否开发更有效的基于点集的深度学习方法来处理这种可靠的传感器？为了回答这个问题，我们的工作——MiliPoint，提供了一个大规模的开放数据集供社区探索如何利用毫米波雷达进行人体活动识别。此外，MiliPoint在大小上超过现有的数据集，代表了更多样化的人类活动，并涵盖了人体活动识别的所有三个关键任务。

    Millimetre-wave (mmWave) radar has emerged as an attractive and cost-effective alternative for human activity sensing compared to traditional camera-based systems. mmWave radars are also non-intrusive, providing better protection for user privacy. However, as a Radio Frequency (RF) based technology, mmWave radars rely on capturing reflected signals from objects, making them more prone to noise compared to cameras. This raises an intriguing question for the deep learning community: Can we develop more effective point set-based deep learning methods for such attractive sensors?  To answer this question, our work, termed MiliPoint, delves into this idea by providing a large-scale, open dataset for the community to explore how mmWave radars can be utilised for human activity recognition. Moreover, MiliPoint stands out as it is larger in size than existing datasets, has more diverse human actions represented, and encompasses all three key tasks in human activity recognition. We have also es
    
[^150]: 超越单调性的数据汇总：非单调的两阶段子模最大化

    Data Summarization beyond Monotonicity: Non-monotone Two-Stage Submodular Maximization. (arXiv:2309.05183v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2309.05183](http://arxiv.org/abs/2309.05183)

    这篇论文研究了两阶段子模最大化问题，目标是使用子模训练函数来减少底层集合，并引入了非单调子模函数的第一个恒定因子逼近算法。

    

    两阶段子模最大化问题的目标是使用提供的子模训练函数来减少底层集合，以确保在减小后的底层集合上优化新的目标函数的结果与在原始底层集合上获得的结果相当。这个问题在数据汇总等各个领域都有应用。现有的研究通常假设目标函数是单调的，而我们的工作将这个研究扩展到了非单调子模函数的情况。我们引入了这种更一般情况的第一个恒定因子逼近算法。

    The objective of a two-stage submodular maximization problem is to reduce the ground set using provided training functions that are submodular, with the aim of ensuring that optimizing new objective functions over the reduced ground set yields results comparable to those obtained over the original ground set. This problem has applications in various domains including data summarization. Existing studies often assume the monotonicity of the objective function, whereas our work pioneers the extension of this research to accommodate non-monotone submodular functions. We have introduced the first constant-factor approximation algorithms for this more general case.
    
[^151]: SRN-SZ: 基于深度学习的科学数据错误有界损失压缩与超分辨率神经网络

    SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks. (arXiv:2309.04037v1 [cs.LG])

    [http://arxiv.org/abs/2309.04037](http://arxiv.org/abs/2309.04037)

    提出了一种基于深度学习的科学数据错误有界损失压缩器SRN-SZ，用于改善难以压缩的数据集的压缩情况。

    

    现代超级计算系统的计算能力和规模的快速增长给超级计算系统的管理带来了巨大挑战。为了保持科学数据的可用性，提出并发展了错误有界损失压缩作为科学数据尺寸缩减的重要技术，以限制数据失真。在各种科学模拟生成的多样数据集中，某些数据集无法通过现有的传统技术的错误有界损失压缩器有效压缩。人工智能的最近成功启发了多位研究人员将神经网络集成到错误有界损失压缩器中。然而，这些工作仍然面临着有限的压缩比和/或极低的效率。为了解决这些问题并改善难以压缩的数据集的压缩情况，本文提出了SRN-SZ，它是一种基于深度学习的科学数据错误有界损失压缩器。

    The fast growth of computational power and scales of modern super-computing systems have raised great challenges for the management of exascale scientific data. To maintain the usability of scientific data, error-bound lossy compression is proposed and developed as an essential technique for the size reduction of scientific data with constrained data distortion. Among the diverse datasets generated by various scientific simulations, certain datasets cannot be effectively compressed by existing error-bounded lossy compressors with traditional techniques. The recent success of Artificial Intelligence has inspired several researchers to integrate neural networks into error-bounded lossy compressors. However, those works still suffer from limited compression ratios and/or extremely low efficiencies. To address those issues and improve the compression on the hard-to-compress datasets, in this paper, we propose SRN-SZ, which is a deep learning-based scientific error-bounded lossy compressor 
    
[^152]: 通过稀疏细胞复合体在图上表示边流

    Representing Edge Flows on Graphs via Sparse Cell Complexes. (arXiv:2309.01632v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2309.01632](http://arxiv.org/abs/2309.01632)

    本文针对图边缘的流量数据，提出了一种通过稀疏细胞复合体来表示边流的方法。我们将图结构转化为一个单纯复合体，利用Hodge-Laplacian的特征向量和关联矩阵进行Hodge分解，得到梯度、旋量和谐波流的表示。同时，我们引入了细胞推断优化问题，通过添加细胞来增强观测到的图，使表示稀疏可解释。实验证明这个问题是NP难的，我们提出了一个高效的近似算法。

    

    在许多机器学习和信号处理任务中，获取稀疏可解释的可观测数据表示是至关重要的。对于表示沿图边缘的流动的数据，一种直观可解释的获取表示的方法是将图结构提升到一个单纯复合体：相关Hodge-Laplacian的特征向量，以及相应单纯复合体的关联矩阵，可引导出Hodge分解，用于以梯度，旋量和谐波流的形式表示观测到的数据。在本文中，我们将这种方法推广到细胞复合体，并引入细胞推断优化问题，即通过添加一组细胞来增强观测到的图，使得关联Hodge Laplacian的特征向量能够提供对图上观测到的边缘流的稀疏可解释表示。我们证明了这个问题是NP难的，并引入了一个高效的近似算法。

    Obtaining sparse, interpretable representations of observable data is crucial in many machine learning and signal processing tasks. For data representing flows along the edges of a graph, an intuitively interpretable way to obtain such representations is to lift the graph structure to a simplicial complex: The eigenvectors of the associated Hodge-Laplacian, respectively the incidence matrices of the corresponding simplicial complex then induce a Hodge decomposition, which can be used to represent the observed data in terms of gradient, curl, and harmonic flows. In this paper, we generalize this approach to cellular complexes and introduce the cell inference optimization problem, i.e., the problem of augmenting the observed graph by a set of cells, such that the eigenvectors of the associated Hodge Laplacian provide a sparse, interpretable representation of the observed edge flows on the graph. We show that this problem is NP-hard and introduce an efficient approximation algorithm for i
    
[^153]: 伪装游戏：在交互式机器人自主性中闭环安全学习

    Deception Game: Closing the Safety-Learning Loop in Interactive Robot Autonomy. (arXiv:2309.01267v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.01267](http://arxiv.org/abs/2309.01267)

    本文提出了一种闭环安全学习的范式，用于合成机器人的安全控制策略。该方法考虑机器人学习能力和不确定性，能够快速应对未来场景，从而在确保安全的同时保持性能表现。

    

    自主车辆等机器人系统的广泛部署面临的一个重要挑战是确保与人类安全互动而不牺牲性能。现有的安全方法往往忽视了机器人在运行时学习和适应的能力，导致过度保守的行为。本文提出了一种新的闭环范式，用于合成安全控制策略，明确考虑机器人不断变化的不确定性和其快速应对未来场景的能力，并联合考虑物理动力学和机器人学习算法。我们利用对抗性强化学习，以可行的方式进行高维学习动力学的安全分析，并通过贝叶斯信念传播和大型预训练神经轨迹预测器展示了我们框架的能力。

    An outstanding challenge for the widespread deployment of robotic systems like autonomous vehicles is ensuring safe interaction with humans without sacrificing performance. Existing safety methods often neglect the robot's ability to learn and adapt at runtime, leading to overly conservative behavior. This paper proposes a new closed-loop paradigm for synthesizing safe control policies that explicitly account for the robot's evolving uncertainty and its ability to quickly respond to future scenarios as they arise, by jointly considering the physical dynamics and the robot's learning algorithm. We leverage adversarial reinforcement learning for tractable safety analysis under high-dimensional learning dynamics and demonstrate our framework's ability to work with both Bayesian belief propagation and implicit learning through large pre-trained neural trajectory predictors.
    
[^154]: 学习的视觉特征到文本解释

    Learned Visual Features to Textual Explanations. (arXiv:2309.00733v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.00733](http://arxiv.org/abs/2309.00733)

    本研究提出了一种名为TExplain的方法，将大型语言模型与预训练图像分类器的特征空间连接起来，通过生成解释性句子来理解分类器学习到的特征。该方法首次利用这些频繁单词揭示出分类器的决策过程，实现了检测虚假特征的能力。

    

    在机器学习领域，解释视觉模型学习到的特征一直是一个长期存在的挑战。为了解决这个问题，我们提出了一种新颖的方法，利用大型语言模型（LLMs）的能力来解释预训练图像分类器学习到的特征。我们的方法名为TExplain，通过训练一个神经网络在图像分类器的特征空间和LLMs之间建立连接来解决这个任务。然后，在推断过程中，我们的方法生成大量的句子来解释分类器对给定图像学习到的特征。这些句子然后用于提取最频繁的单词，从而全面理解分类器中学习到的特征和模式。我们的方法首次利用与视觉表示对应的这些频繁单词来揭示独立训练的分类器的决策过程，从而实现检测虚假特征的能力。

    Interpreting the learned features of vision models has posed a longstanding challenge in the field of machine learning. To address this issue, we propose a novel method that leverages the capabilities of large language models (LLMs) to interpret the learned features of pre-trained image classifiers. Our method, called TExplain, tackles this task by training a neural network to establish a connection between the feature space of image classifiers and LLMs. Then, during inference, our approach generates a vast number of sentences to explain the features learned by the classifier for a given image. These sentences are then used to extract the most frequent words, providing a comprehensive understanding of the learned features and patterns within the classifier. Our method, for the first time, utilizes these frequent words corresponding to a visual representation to provide insights into the decision-making process of the independently trained classifier, enabling the detection of spurious
    
[^155]: ILCAS: 基于模仿学习的适应性配置流式传输在带有跨相机协作的实时视频分析中的应用

    ILCAS: Imitation Learning-Based Configuration-Adaptive Streaming for Live Video Analytics with Cross-Camera Collaboration. (arXiv:2308.10068v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2308.10068](http://arxiv.org/abs/2308.10068)

    ILCAS是首个基于模仿学习的适应性配置流式传输系统，用于实时视频分析，并在传输过程中实现相机之间的协作。与基于深度强化学习的解决方案不同，ILCAS通过模仿学习训练代理，提高了性能表现和应用目标的多样性。

    

    高精度和资源密集型的深度神经网络（DNN）已经被广泛应用于实时视频分析（VA），其中相机视频通过网络流式传输到资源丰富的边缘/云服务器进行DNN推理。常见的视频编码配置（例如分辨率和帧率）已被确定为在带宽消耗和推理准确度之间取得平衡的重要因素，因此它们的调整方案一直是优化的焦点。然而，以前基于性能分析的解决方案存在高昂的性能分析成本，而现有的基于深度强化学习（DRL）的解决方案可能由于使用固定奖励函数训练代理而性能不佳，这无法在各种场景下制定应用目标。在本文中，我们提出了基于模仿学习的ILCAS适应性配置流式传输系统。与基于DRL的解决方案不同，ILCAS通过从跨相机协作中收集的演示数据来训练代理。

    The high-accuracy and resource-intensive deep neural networks (DNNs) have been widely adopted by live video analytics (VA), where camera videos are streamed over the network to resource-rich edge/cloud servers for DNN inference. Common video encoding configurations (e.g., resolution and frame rate) have been identified with significant impacts on striking the balance between bandwidth consumption and inference accuracy and therefore their adaption scheme has been a focus of optimization. However, previous profiling-based solutions suffer from high profiling cost, while existing deep reinforcement learning (DRL) based solutions may achieve poor performance due to the usage of fixed reward function for training the agent, which fails to craft the application goals in various scenarios. In this paper, we propose ILCAS, the first imitation learning (IL) based configuration-adaptive VA streaming system. Unlike DRL-based solutions, ILCAS trains the agent with demonstrations collected from th
    
[^156]: Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG]) 该论文标题已翻译：二元强化学习。

    Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG])

    [http://arxiv.org/abs/2308.07843](http://arxiv.org/abs/2308.07843)

    该论文介绍了一个称为二元强化学习的在线算法，用于根据上下文因素和目标人与其照顾伴侣的过去反馈，个性化地提供干预措施。该算法是贝叶斯和层次的，并通过模拟展示了良好的实证效果。

    

    移动医疗旨在通过在个人日常生活中提供干预来提高健康结果。照顾伴侣和社会支持网络的参与经常在帮助个人管理繁重的医疗条件方面起着关键作用。这为移动医疗提供了机会，设计针对二元关系——目标人和其照顾伴侣之间关系——以提高社会支持的干预措施。在本文中，我们开发了二元强化学习（Dyadic RL），这是一种基于环境因素和目标人及其照顾伴侣的过去反馈个性化干预措施的在线强化学习算法。在这里，多组干预措施影响着二元关系在多个时间间隔内。开发的二元强化学习是贝叶斯和层次的。我们正式介绍了问题设定，开发了二元强化学习并确定了遗憾边界。通过模拟，我们展示了二元强化学习的实证效果。

    Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation st
    
[^157]: 解决医学影像深度学习中的小型注释数据集问题：对比共同对比学习和掩码自编码器方法在CT扫描卷积模型中的自监督预训练的评估

    Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])

    [http://arxiv.org/abs/2308.06534](http://arxiv.org/abs/2308.06534)

    本研究评估了在医学影像领域使用自监督预训练方法的可行性，比较了共同对比学习和掩码自编码器方法在CT扫描卷积模型中的性能。

    

    医学影像中的深度学习有潜力减少诊断错误的风险、减轻放射科医生的工作负担并加速确诊。训练这样的深度学习模型需要大型且准确的数据集，并且需要为所有训练样本提供注释。然而，在医学影像领域，由于注释的高复杂性、受限的获取方式或疾病的罕见性，特定任务的注释数据集通常很小。为了应对这一挑战，深度学习模型可以使用自监督学习领域的方法，在没有注释的大型图像数据集上进行预训练。在预训练之后，小型的已注释数据集就足以对模型进行特定任务的微调，即所谓的“下游任务”。医学影像中最流行的自监督预训练方法基于共同对比学习。然而，最近的自然图像处理研究表明掩码自编码器方法具有很大的潜力。本研究比较了二者在CT扫描卷积模型中的性能。

    Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
    
[^158]: 基于图谱的可解释年龄预测

    Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])

    [http://arxiv.org/abs/2307.07439](http://arxiv.org/abs/2307.07439)

    本研究提出了一种基于图谱的可解释年龄预测方法，利用全身图像研究了各个身体部位的年龄相关变化。通过使用解释性方法和配准技术，确定了最能预测年龄的身体区域，并创下了整个身体年龄预测的最新水平。研究结果表明，脊柱、本原性背部肌肉和心脏区域是最重要的关注领域。

    

    年龄预测是医学评估和研究的重要部分，可以通过突出实际年龄和生物年龄之间的差异来帮助检测疾病和异常衰老。为了全面了解各个身体部位的年龄相关变化，我们使用了全身图像进行研究。我们利用Grad-CAM解释性方法确定最能预测一个人年龄的身体区域。通过使用配准技术生成整个人群的解释性图，我们将分析扩展到个体之外。此外，我们以一个平均绝对误差为2.76年的模型，创下了整个身体年龄预测的最新水平。我们的研究结果揭示了三个主要的关注领域：脊柱、本原性背部肌肉和心脏区域，其中心脏区域具有最重要的作用。

    Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
    
[^159]: Wasserstein量子蒙特卡洛：解决量子多体Schr\"odinger方程的一种新方法

    Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\"odinger Equation. (arXiv:2307.07050v1 [physics.comp-ph])

    [http://arxiv.org/abs/2307.07050](http://arxiv.org/abs/2307.07050)

    这篇论文提出了一种新的方法，即基于Wasserstein量子蒙特卡洛的方法，用于解决量子多体Schr\"odinger方程。该方法重新制定了能量泛函的最小化问题，并将其转化为Born分布空间中的粒子排列（反）对称波函数的问题，同时利用深度学习方法来表示丰富的波函数族。

    

    解决量子多体Schr\"odinger方程是量子物理、量子化学和材料科学领域中一个基本而具有挑战性的问题。针对这个问题，一种常见的计算方法是量子变分蒙特卡洛（QVMC），其中通过在一个参数化波函数族中最小化系统的能量来获得基态解。深度学习方法在一定程度上解决了传统QVMC的局限性，通过使用神经网络表示丰富的波函数族。然而，在QVMC中优化目标仍然难以最小化，需要使用自然梯度等二阶优化方法。本文首先重新制定了能量泛函的最小化问题，将其转化为Born分布空间中的粒子排列（反）对称波函数的问题，而不是波函数的空间。然后我们将QVMC解释为在这个空间中的Fisher-Rao梯度流。

    Solving the quantum many-body Schr\"odinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this
    
[^160]: VoxPoser: 用于带有语言模型的机器人操作的可组合的3D价值映射

    VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])

    [http://arxiv.org/abs/2307.05973](http://arxiv.org/abs/2307.05973)

    VoxPoser提出了一种新方法，通过组合3D价值映射和语言模型，实现了机器人在多种操作任务下根据自由形式的指令和对象合成机器人轨迹的能力。

    

    研究表明，大型语言模型（LLMs）具有丰富的可行动知识，可以以推理和规划的形式提取出用于机器人操作的信息。尽管取得了进展，大多数模型仍然依赖于预定义的运动原语来执行与环境的物理交互，这仍然是一个重大瓶颈。在这项工作中，我们的目标是在给定开集指令和开集对象的情况下，为各种操作任务合成机器人轨迹，即一系列密集的6-DoF末端执行器路径点。我们首先观察到LLMs在给定自由形式的语言指令时擅长推断可行性和约束。更重要的是，通过利用它们的代码编写能力，它们可以与视觉-语言模型（VLM）交互，以组合3D价值映射将知识接地到Agent的观测空间中。然后在基于模型的规划框架中使用组合的价值映射来零试合成闭环轨迹。

    Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop ro
    
[^161]: 通过对比学习在强化学习中发现层次化成就

    Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])

    [http://arxiv.org/abs/2307.03486](http://arxiv.org/abs/2307.03486)

    通过对比学习方法，我们在强化学习中提出了新的成就蒸馏方法，可以加强代理对下一个解锁成就的预测能力，并优于先前的模型驱动和层次化方法。

    

    在生成环境中发现具有层次结构的成就是一个重大挑战。这需要智能体具备广泛的能力，包括泛化和长期推理。许多先前的方法基于模型驱动或层次化方法，认为显式的长期规划模块对于学习层次化成就是有益的。然而，这些方法需要大量的环境交互或大型模型，限制了它们的实用性。在这项工作中，我们发现近期实施实践中的近端策略优化（PPO）算法优于先前的方法。此外，我们发现PPO智能体可以在一定程度上预测下一个要解锁的成就，尽管预测的置信度较低。基于这一观察，我们提出了一种新颖的对比学习方法，称为成就蒸馏，可以加强PPO智能体对下一个解锁成就的预测能力。

    Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the 
    
[^162]: 添加解码器用于潜变量识别和笛卡尔积推算

    Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation. (arXiv:2307.02598v1 [cs.LG])

    [http://arxiv.org/abs/2307.02598](http://arxiv.org/abs/2307.02598)

    这篇论文解决了表示学习中的潜变量识别和"支持外"图像生成问题，展示了加法解码器能够对潜变量进行识别，并提供了理论依据支持这种方法的有效性。

    

    我们解决了表示学习中的潜变量识别和“支持外”图像生成问题。我们展示了在一类我们称为“加法”的解码器中，这两者是可能的，这些解码器类似于用于面向对象表示学习（OCRL）的解码器，并且非常适用于可以分解为多个特定对象图像的图像。我们提供了在使用加法解码器完全解决重构问题时，对潜变量块进行了置换和块状逆变换的识别的条件。这个保证仅基于关于潜因子分布的非常弱的假设，潜因子可能存在统计依赖并且具有几乎任意形状的支持。我们的结果提供了非线性独立成分分析（ICA）可能性的新设置，并且增加了我们对OCRL方法的理论理解。我们还从理论上证明了加法解码器可以

    We tackle the problems of latent variables identification and "out-of-support" image generation in representation learning. We show that both are possible for a class of decoders that we call additive, which are reminiscent of decoders used for object-centric representation learning (OCRL) and well suited for images that can be decomposed as a sum of object-specific images. We provide conditions under which exactly solving the reconstruction problem using an additive decoder is guaranteed to identify the blocks of latent variables up to permutation and block-wise invertible transformations. This guarantee relies only on very weak assumptions about the distribution of the latent factors, which might present statistical dependencies and have an almost arbitrarily shaped support. Our result provides a new setting where nonlinear independent component analysis (ICA) is possible and adds to our theoretical understanding of OCRL methods. We also show theoretically that additive decoders can 
    
[^163]: EHRSHOT:一种用于少样本评估基础模型的电子健康记录基准

    EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02028](http://arxiv.org/abs/2307.02028)

    该论文介绍了EHRSHOT，一个用于少样本评估基础模型的电子健康记录基准。该论文利用EHRSHOT数据集和预训练模型CLMBR-T-base，为医疗保健ML的发展提供了解决方案。

    

    尽管一般的机器学习(ML)社区已经受益于公开的数据集、任务和模型，但是ML在医疗保健领域的进展受到了共享资产的缺乏的阻碍。基础模型的成功为医疗保健ML带来了新的挑战，需要访问共享的预训练模型来验证性能优势。我们通过三个贡献来帮助解决这些挑战。首先，我们发布了一个新的数据集EHRSHOT，其中包含6,739名来自斯坦福医学的患者的去识别结构化的电子健康记录(EHR)数据。与MIMIC-III/IV和其他流行的EHR数据集不同，EHRSHOT是纵向的，不仅局限于ICU/ED患者。其次，我们发布了CLMBR-T-base的权重，这是一个在结构化EHR数据中预训练的141M参数临床基础模型，该数据包括2.57M名患者。我们是最早完全发布这样一个用于编码EHR数据的模型之一；相比之下，大多数先前发布的临床数据模型（如GatorTron、ClinicalBER）并没有完全发布。

    While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains deidentified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBER
    
[^164]: 拟合值迭代方法求解适应结构双因果最优传输问题

    Fitted Value Iteration Methods for Bicausal Optimal Transport. (arXiv:2306.12658v1 [stat.ML])

    [http://arxiv.org/abs/2306.12658](http://arxiv.org/abs/2306.12658)

    本文提出了一种适用于双因果最优传输问题的拟合值迭代方法，能够在保证精度的同时具有良好的可扩展性，数值实验结果也证明了该方法的优越性。

    

    本文提出一种拟合值迭代方法(FVI)用于计算具有适应结构的双因果最优传输(OT)。基于动态规划的形式化表述，FVI采用函数类用于近似双因果OT中的值函数。在可集中条件和近似完备性假设下，我们使用（局部）Rademacher复杂度证明了样本复杂度。此外，我们证明了深度多层神经网络具有适当结构，满足样本复杂度证明所需的关键假设条件。数值实验表明，FVI在时间跨度增加时优于线性规划和适应性Sinkhorn方法，在保持可接受精度的同时具有很好的可扩展性。

    We develop a fitted value iteration (FVI) method to compute bicausal optimal transport (OT) where couplings have an adapted structure. Based on the dynamic programming formulation, FVI adopts a function class to approximate the value functions in bicausal OT. Under the concentrability condition and approximate completeness assumption, we prove the sample complexity using (local) Rademacher complexity. Furthermore, we demonstrate that multilayer neural networks with appropriate structures satisfy the crucial assumptions required in sample complexity proofs. Numerical experiments reveal that FVI outperforms linear programming and adapted Sinkhorn methods in scalability as the time horizon increases, while still maintaining acceptable accuracy.
    
[^165]: Fedstellar：一个去中心化联邦学习平台

    Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])

    [http://arxiv.org/abs/2306.09750](http://arxiv.org/abs/2306.09750)

    Fedstellar是一个联邦学习平台，支持物理或虚拟设备的去中心化、半去中心化和中心化的方式训练模型，旨在解决现有平台在处理异构联盟网络拓扑等问题时的挑战。

    

    2016年，谷歌提出了联邦学习（FL）作为一种新的范式，可以在保护数据隐私的同时跨联盟参与者训练机器学习（ML）模型。虽然中心化联邦学习（CFL）是最常用的方法，但它存在通信瓶颈、单点故障和对中央服务器的依赖等局限。去中心化联邦学习（DFL）通过实现去中心化模型聚合和最小化对中央实体的依赖，来解决这些问题。然而，目前训练DFL模型的平台在处理异构联盟网络拓扑等关键问题方面存在困难。为了克服这些挑战，本文提出了Fedstellar，这是一个新型的平台，旨在在物理或虚拟设备的不同联盟中以去中心化、半去中心化和中心化的方式训练FL模型。

    In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
    
[^166]: （扩大）带状矩阵分解：一种统一的隐私训练方法。

    (Amplified) Banded Matrix Factorization: A unified approach to private training. (arXiv:2306.08153v1 [cs.LG])

    [http://arxiv.org/abs/2306.08153](http://arxiv.org/abs/2306.08153)

    本文提出了利用带状矩阵构建的矩阵分解机制，该机制能够在所有隐私预算中将先前最先进的算法纳入分散和联合训练设置中。对于跨设备联合学习，这意味着可以使用一种放松的设备参与模式，与实际的FL基础设施相容。在集中式设置中，带状矩阵具有与 ubiquitous DP-SGD algorithm 相同的隐私放大结果。

    

    差分隐私（DP）下的矩阵分解（MF）机制在许多场景下显著改进了隐私-效用-计算折衷的最新技术。但是在分散和联合设置中，仍存在MF不易适用的实例，或者其他算法提供更好的折衷（通常随着 ε 变小）。在这项工作中，我们展示了如何使用带状矩阵构建MF机制，在所有隐私预算中将先前最先进的算法纳入分散和联合训练设置中。关键技术是带状矩阵的构造。对于跨设备联合学习（FL），这使得多个设备可以使用一种放松的设备参与模式，与实际的FL基础设施相容（如产品部署所示）。在集中式设置中，我们证明带状矩阵具有与 ubiquitous DP-SGD algorithm 相同的隐私放大结果。

    Matrix factorization (MF) mechanisms for differential privacy (DP) have substantially improved the state-of-the-art in privacy-utility-computation tradeoffs for ML applications in a variety of scenarios, but in both the centralized and federated settings there remain instances where either MF cannot be easily applied, or other algorithms provide better tradeoffs (typically, as $\epsilon$ becomes small).  In this work, we show how MF can subsume prior state-of-the-art algorithms in both federated and centralized training settings, across all privacy budgets. The key technique throughout is the construction of MF mechanisms with banded matrices. For cross-device federated learning (FL), this enables multiple-participations with a relaxed device participation schema compatible with practical FL infrastructure (as demonstrated by a production deployment). In the centralized setting, we prove that banded matrices enjoy the same privacy amplification results as for the ubiquitous DP-SGD algo
    
[^167]: 与学习导向的车辆运动规划的误解告别

    Parting with Misconceptions about Learning-based Vehicle Motion Planning. (arXiv:2306.07962v1 [cs.RO])

    [http://arxiv.org/abs/2306.07962](http://arxiv.org/abs/2306.07962)

    该论文提出了nuPlan，一个大规模真实世界数据集和评估方案，针对精确的短期规划和长期目标预测。证实了现有系统难以同时满足两个要求。最终提出一个非常简单高效的规划器。

    

    nuPlan的发布标志着车辆运动规划研究的一个新时代，提供了第一个需要精确的短期规划和长期目标预测的大规模真实世界数据集和评估方案。现有系统难以同时满足两个要求。实际上，我们发现这些任务存在根本上的不对齐问题，应该分别进行解决。我们进一步评估了领域内闭环规划的现状，揭示了学习为基础的方法在复杂的真实场景中的局限性，以及选择通过车道图搜索算法的简单基于规则的先验项（例如中心线选择）的价值。更令人惊讶的是，在开环子任务中，我们观察到当仅使用这个中心线作为场景上下文时（即忽略所有有关地图和其他代理的信息）可以获得最佳结果。结合这些见解，我们提出了一个非常简单高效的规划器，它的表现优于大量竞争对手。

    The release of nuPlan marks a new era in vehicle motion planning research, offering the first large-scale real-world dataset and evaluation schemes requiring both precise short-term planning and long-horizon ego-forecasting. Existing systems struggle to simultaneously meet both requirements. Indeed, we find that these tasks are fundamentally misaligned and should be addressed independently. We further assess the current state of closed-loop planning in the field, revealing the limitations of learning-based methods in complex real-world scenarios and the value of simple rule-based priors such as centerline selection through lane graph search algorithms. More surprisingly, for the open-loop sub-task, we observe that the best results are achieved when using only this centerline as scene context (\ie, ignoring all information regarding the map and other agents). Combining these insights, we propose an extremely simple and efficient planner which outperforms an extensive set of competitors,
    
[^168]: 实现平坦局部极小值以提高对抗性转移的效果

    Boosting Adversarial Transferability by Achieving Flat Local Maxima. (arXiv:2306.05225v1 [cs.CV])

    [http://arxiv.org/abs/2306.05225](http://arxiv.org/abs/2306.05225)

    本文提出了一种近似优化方法来实现平坦局部极小值，该方法可以提高对抗性转移能力，并在实验中证实了该方法的有效性。

    

    基于转移的攻击采用在替代模型上生成的对抗性示例来攻击各种模型，在物理世界中适用并吸引了越来越多的关注。在这项工作中，受到平坦局部极小值与良好的泛化之间的相关性启发，我们假设并经验性地验证了，在平坦局部区域的对抗性示例倾向于具有良好的转移能力，通过在原始损失函数中引入惩罚梯度范数。由于直接优化梯度正则化范数在生成对抗性示例时计算代价昂贵且难以处理，因此我们提出了一种近似优化方法，以简化目标函数的梯度更新。我们随机采样一个示例，并采用一阶梯度来逼近二阶黑塞矩阵，从而使计算更加容易。

    Transfer-based attack adopts the adversarial examples generated on the surrogate model to attack various models, making it applicable in the physical world and attracting increasing interest. Recently, various adversarial attacks have emerged to boost adversarial transferability from different perspectives. In this work, inspired by the fact that flat local minima are correlated with good generalization, we assume and empirically validate that adversarial examples at a flat local region tend to have good transferability by introducing a penalized gradient norm to the original loss function. Since directly optimizing the gradient regularization norm is computationally expensive and intractable for generating adversarial examples, we propose an approximation optimization method to simplify the gradient update of the objective function. Specifically, we randomly sample an example and adopt the first-order gradient to approximate the second-order Hessian matrix, which makes computing more 
    
[^169]: 图神经网络的细粒度表达能力

    Fine-grained Expressivity of Graph Neural Networks. (arXiv:2306.03698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03698](http://arxiv.org/abs/2306.03698)

    本研究通过考虑将1-WL和MPNN连续扩展为graphon，并通过准确的拓扑表征揭示了MPNN在图上的表达能力，为图和graphon相似度提供了理论框架。

    

    最近的许多研究分析了消息传递图神经网络（MPNN）的表达能力，主要使用了如图同构问题的1维Weisfeiler-Leman测试（1-WL）等组合技术。然而，图同构目标在本质上是二进制的，不能揭示两个给定图之间的相似程度。本工作通过考虑将1-WL和MPNN连续扩展为graphon来解决这个问题。具体而言，我们展示了连续变量的1-WL对于MPNN在graphon上的表达能力进行了准确的拓扑表征，揭示了这些网络可以区分哪些图以及在区分它们时的难度级别。我们确定了MPNN分离点的最细拓扑，并证明了一个普遍逼近定理。因此，我们提供了一个将各种拓扑变体的经典特征结合起来的图和graphon相似度的理论框架。

    Numerous recent works have analyzed the expressive power of message-passing graph neural networks (MPNNs), primarily utilizing combinatorial techniques such as the $1$-dimensional Weisfeiler-Leman test ($1$-WL) for the graph isomorphism problem. However, the graph isomorphism objective is inherently binary, not giving insights into the degree of similarity between two given graphs. This work resolves this issue by considering continuous extensions of both $1$-WL and MPNNs to graphons. Concretely, we show that the continuous variant of $1$-WL delivers an accurate topological characterization of the expressive power of MPNNs on graphons, revealing which graphs these networks can distinguish and the level of difficulty in separating them. We identify the finest topology where MPNNs separate points and prove a universal approximation theorem. Consequently, we provide a theoretical framework for graph and graphon similarity combining various topological variants of classical characterizatio
    
[^170]: Representation Equivalent Neural Operators: 一种无别名的操作符学习框架

    Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning. (arXiv:2305.19913v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19913](http://arxiv.org/abs/2305.19913)

    这项研究提出了一种名为ReNO的框架，解决了神经操作符在离散实现时出现的完整性损失和误差问题，为神经操作符的学习提供了新的视角。

    

    最近，操作符学习，或者学习无限维函数空间之间的映射，引起了相当大的关注，特别是与从数据中学习偏微分方程相关的领域。在纸上概念上很清晰的神经操作符在转换成计算机实现时需要离散化。这一步骤可能会损害它们的完整性，导致它们与底层操作符偏离。这项研究提供了一个对神经操作符的新见解，采用了一种名为表示等效神经操作符（ReNO）的框架来解决这些问题。其核心是操作符别名的概念，用于衡量神经操作符与其离散表示之间的不一致性。我们探索了这一问题在常用的操作符学习技术中的应用。我们的研究结果详细说明了当处理不同的离散化和网格以及关键的连续结构时，别名会引入误差。更一般的说，这一框架不仅能够揭示离散化引入的问题，还能够为神经操作符的学习提供新的视角。

    Recently, operator learning, or learning mappings between infinite-dimensional function spaces, has garnered significant attention, notably in relation to learning partial differential equations from data. Conceptually clear when outlined on paper, neural operators necessitate discretization in the transition to computer implementations. This step can compromise their integrity, often causing them to deviate from the underlying operators. This research offers a fresh take on neural operators with a framework Representation equivalent Neural Operators (ReNO) designed to address these issues. At its core is the concept of operator aliasing, which measures inconsistency between neural operators and their discrete representations. We explore this for widely-used operator learning techniques. Our findings detail how aliasing introduces errors when handling different discretizations and grids and loss of crucial continuous structures. More generally, this framework not only sheds light on ex
    
[^171]: BiSLS/SPS：稳定双层优化的自动调整步长

    BiSLS/SPS: Auto-tune Step Sizes for Stable Bi-level Optimization. (arXiv:2305.18666v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18666](http://arxiv.org/abs/2305.18666)

    本文研究了稳定双层优化中自动调整步长的方法，并提出了两个变体的改进版本来替代现有的算法。这些方法使用了最近提出的自适应步长方法进行计算，以缓解计算超梯度时可能引起的近似误差问题。

    

    双层优化（BO）在深度学习中的流行引发了对基于梯度的BO算法的兴趣。然而，现有算法涉及两个耦合的学习率，当计算超梯度时可能受到近似误差的影响，因此需要仔细微调以确保快速收敛。为了缓解这个问题，我们研究了最近提出的自适应步长方法——随机线搜索（SLS）和随机Polyak步长（SPS），用于计算上层和下层的学习率。首先，我们重新审视了在没有通常在先前研究中假设的额外插值条件的情况下在单层优化中使用SLS和SPS的方法。对于这样的设置，我们研究了SLS和SPS的新变体，改进了文献中的现有建议，并且更容易实现。重要的是，这两个变体可以看作是一类具有包络型结构方法的特殊实例。

    The popularity of bi-level optimization (BO) in deep learning has spurred a growing interest in studying gradient-based BO algorithms. However, existing algorithms involve two coupled learning rates that can be affected by approximation errors when computing hypergradients, making careful fine-tuning necessary to ensure fast convergence. To alleviate this issue, we investigate the use of recently proposed adaptive step-size methods, namely stochastic line search (SLS) and stochastic Polyak step size (SPS), for computing both the upper and lower-level learning rates. First, we revisit the use of SLS and SPS in single-level optimization without the additional interpolation condition that is typically assumed in prior works. For such settings, we investigate new variants of SLS and SPS that improve upon existing suggestions in the literature and are simpler to implement. Importantly, these two variants can be seen as special instances of general family of methods with an envelope-type ste
    
[^172]: 具有流行度偏见的排名：自增强动态下的用户福利

    Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics. (arXiv:2305.18333v1 [cs.IR])

    [http://arxiv.org/abs/2305.18333](http://arxiv.org/abs/2305.18333)

    研究了物品流行度、质量和位置偏差对用户福利的影响，提出了通过探索减轻流行度偏见负面影响的算法。

    

    虽然已经确认流行度偏见在推荐（和其他基于排名的）系统中发挥作用，但其对用户福利的影响的详细分析仍然缺乏。我们提出了一种通用机制，通过它，物品的流行度、质量和位置偏差可以影响用户选择，并且可以负面影响各种推荐策略的集体用户效用。我们将问题表述为非平稳上下文脱靶机，强调不是为了消除流行度偏见而是为了减轻其负面影响而进行探索的重要性。首先，普通的有流行度偏差的推荐系统会通过混淆物品质量和流行度而引发线性遗憾。更一般地，我们展示了即使在线性设置下，由于流行度偏见的混淆效应，物品质量的可识别性也可能无法实现。然而，在足够变异的假设下，我们开发了一种高效的类UCB算法，并证明了有效的遗憾保证。我们通过实验验证了我们提出的算法的有效性，并证实了流行度偏见的负面影响。

    While popularity bias is recognized to play a role in recommmender (and other ranking-based) systems, detailed analyses of its impact on user welfare have largely been lacking. We propose a general mechanism by which item popularity, item quality, and position bias can impact user choice, and how it can negatively impact the collective user utility of various recommender policies. Formulating the problem as a non-stationary contextual bandit, we highlight the importance of exploration, not to eliminate popularity bias, but to mitigate its negative effects. First, naive popularity-biased recommenders are shown to induce linear regret by conflating item quality and popularity. More generally, we show that, even in linear settings, identifiability of item quality may not be possible due to the confounding effects of popularity bias. However, under sufficient variability assumptions, we develop an efficient UCB-style algorithm and prove efficient regret guarantees. We complement our analys
    
[^173]: Diable: 在表格上进行的高效对话状态跟踪

    Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17020](http://arxiv.org/abs/2305.17020)

    Diable是一个高效的对话状态跟踪系统，它通过在表格上进行操作来更新对话状态，相比现有方法时间效率提高了2.4倍，同时保持了竞争性的目标准确性。

    

    目前的对话状态跟踪系统将完整的对话历史作为输入，将当前状态表示为包含所有槽的列表，并在每个对话回合中从头开始生成整个状态。这种方法效率低下，特别是当槽的数量很多且对话很长时。我们提出了Diable，一种新的任务形式化方法，简化了高效对话状态跟踪系统的设计和实现，并且可以轻松地嵌入大型语言模型。我们将对话状态表示为表格，并将对话状态跟踪形式化为表格操作任务。在每个回合中，系统通过基于对话上下文生成表格操作来更新先前的状态。在MultiWoz数据集上进行了大量实验，结果显示，Diable (i) 优于强大的高效对话状态跟踪基准，(ii) 时间效率比当前最先进的方法提高了2.4倍，同时保持竞争性的联合目标准确性，并且(iii) 对无噪声的输入具有鲁棒性。

    Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to no
    
[^174]: 本地贝叶斯优化的行为和收敛性

    The Behavior and Convergence of Local Bayesian Optimization. (arXiv:2305.15572v1 [cs.LG])

    [http://arxiv.org/abs/2305.15572](http://arxiv.org/abs/2305.15572)

    本文研究了贝叶斯本地优化策略的行为和收敛性，并在高维问题上提供了强大的实证性能。统计数据表明，单个高斯过程样本路径的本地解比全局方法恢复的预期值更好。Müller等人提出的贝叶斯本地优化算法的收敛速率在有噪音和无噪音的情况下都有推导。

    

    贝叶斯优化中一项最新的发展是使用本地优化策略，与传统的全局策略相比，可以在高维问题上提供强大的实证性能。文献中的“传统智慧”是，专注于本地优化规避了维度诅咒。然而，对于贝叶斯本地优化例程的预期行为或收敛性了解甚少。我们首先研究了本地方法的行为，并发现高斯过程样本路径单个本地解的统计数据与从全局方法恢复的预期值相比非常好。然后，我们展示了最近由Müller等人提出的基于贝叶斯本地优化算法的第一次严格分析，并在有噪音和无噪音的情况下推导出收敛速率。

    A recent development in Bayesian optimization is the use of local optimization strategies, which can deliver strong empirical performance on high-dimensional problems compared to traditional global strategies. The "folk wisdom" in the literature is that the focus on local optimization sidesteps the curse of dimensionality; however, little is known concretely about the expected behavior or convergence of Bayesian local optimization routines. We first study the behavior of the local approach, and find that the statistics of individual local solutions of Gaussian process sample paths are surprisingly good compared to what we would expect to recover from global methods. We then present the first rigorous analysis of such a Bayesian local optimization algorithm recently proposed by M\"uller et al. (2021), and derive convergence rates in both the noisy and noiseless settings.
    
[^175]: 广义贝叶斯推理方法：通过摊销成本评估为科学模拟器提供贝叶斯推理

    Generalized Bayesian Inference for Scientific Simulators via Amortized Cost Estimation. (arXiv:2305.15208v1 [stat.ML])

    [http://arxiv.org/abs/2305.15208](http://arxiv.org/abs/2305.15208)

    提出了摊销成本估计方法，能够解决广义贝叶斯推理方法中多个模拟的计算问题，从而为深度神经网络提供了一种处理高维度、复杂复现，且贝叶斯后验未必是最佳方案的科学模拟器的优化方法。

    

    基于模拟的推理方法(SBI)通过内含的可能性，为模拟器提供摊销式的贝叶斯推理。但是当我们主要关注的是预测模拟的质量，或者当模型不能完全重现观测数据(即存在缺陷)，以贝叶斯后验为目标就可能过于严格。广义贝叶斯推理(GBI)旨在加强对(有缺陷的)模拟器模型的推理，用评估参数相对于数据的好坏的成本函数替换似然函数。然而，GBI方法通常需要运行多个模拟，以在推理期间估计每个参数值的成本函数，使得即使在中等复杂的模拟程序中也难以计算。在这里，我们提出了基于摊销成本评估(ACE)的广义贝叶斯推理方法，以解决这个挑战：我们训练神经网络来近似成本函数，将成本函数定义为由潜在参数生成的模拟之间的期望距离。

    Simulation-based inference (SBI) enables amortized Bayesian inference for simulators with implicit likelihoods. But when we are primarily interested in the quality of predictive simulations, or when the model cannot exactly reproduce the observed data (i.e., is misspecified), targeting the Bayesian posterior may be overly restrictive. Generalized Bayesian Inference (GBI) aims to robustify inference for (misspecified) simulator models, replacing the likelihood-function with a cost function that evaluates the goodness of parameters relative to data. However, GBI methods generally require running multiple simulations to estimate the cost function at each parameter value during inference, making the approach computationally infeasible for even moderately complex simulators. Here, we propose amortized cost estimation (ACE) for GBI to address this challenge: We train a neural network to approximate the cost function, which we define as the expected distance between simulations produced by a 
    
[^176]: 将重要性加权方法推广为用于分布偏移问题的通用求解器

    Generalizing Importance Weighting to A Universal Solver for Distribution Shift Problems. (arXiv:2305.14690v1 [cs.LG])

    [http://arxiv.org/abs/2305.14690](http://arxiv.org/abs/2305.14690)

    本文推广了重要性加权方法，将其变成了适用于所有情况的通用求解器，有效地解决了包括部分重叠情况在内的分布偏移问题。

    

    分布偏移(DS) 可以有两个层面：分布本身发生变化，支持（即概率密度非零的集合）也发生变化。当考虑训练和测试分布之间的支持变化时，有四种情况：（i）它们完全匹配；（ii）训练支持范围更广（因此覆盖测试支持）；（iii）测试支持范围更广；（iv）它们部分重叠。现有方法对情况（i）和（ii）很有效，而情况（iii）和（iv）现在更常见，但仍未得到充分探索。本文将重要性加权（IW）方法（用于情况（i）和（ii））推广为适用于所有情况的通用求解器。具体来说，我们首先研究了IW在情况（iii）和（iv）中可能失败的原因；基于这些发现，我们提出了泛化重要性加权（GIW），它可以处理情况（iii）和（iv），并且在情况（i）和（ii）中将减少为IW方法。在GIW中，将测试支持划分为训练内部（IT）部分和训练外部（OOT）部分，并为它们分配不同的权重。我们证明了在温和假设下，GIW对所有四种情况都收敛到最优解。我们在合成数据集和基准数据集上进行了广泛的实验证明，GIW确实在所有四种情况下优于现有方法。

    Distribution shift (DS) may have two levels: the distribution itself changes, and the support (i.e., the set where the probability density is non-zero) also changes. When considering the support change between the training and test distributions, there can be four cases: (i) they exactly match; (ii) the training support is wider (and thus covers the test support); (iii) the test support is wider; (iv) they partially overlap. Existing methods are good at cases (i) and (ii), while cases (iii) and (iv) are more common nowadays but still under-explored. In this paper, we generalize importance weighting (IW), a golden solver for cases (i) and (ii), to a universal solver for all cases. Specifically, we first investigate why IW may fail in cases (iii) and (iv); based on the findings, we propose generalized IW (GIW) that could handle cases (iii) and (iv) and would reduce to IW in cases (i) and (ii). In GIW, the test support is split into an in-training (IT) part and an out-of-training (OOT) pa
    
[^177]: 文本预训练的语音语言模型

    Textually Pretrained Speech Language Models. (arXiv:2305.13009v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13009](http://arxiv.org/abs/2305.13009)

    本论文提出了一种使用预训练的文本语言模型训练语音语言模型的方法，通过对模型设计选择和数据集规模的经验性分析，构建了参数数量和训练数据最多的语音语言模型，并引入了两个Spoken版本的文本基准，以进一步改善模型评估和推动未来研究。

    

    语音语言模型（SpeechLMs）仅处理和生成音频数据，没有文字监督。在这项工作中，我们提出了TWIST，一种使用预训练的文本语言模型进行SpeechLMs训练的方法。我们通过自动和人工评估表明，TWIST在各个方面都优于冷启动的SpeechLM。我们经验性地分析了不同的模型设计选择（如语音分词器、预训练的文本模型和数据集大小）的影响。我们发现模型和数据集规模在构建性能更好的SpeechLMs方面都起着重要作用。基于我们的观察，我们介绍了迄今为止参数数量和训练数据最多的SpeechLM（据我们所知）。此外，我们还引入了两个Spoken版本的StoryCloze文本基准，以进一步改善模型评估并推动该领域的未来研究。我们公开提供语音样本、代码和模型：https://pages.cs.huji.ac.il/

    Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/
    
[^178]: 无存储限制的在线持续学习

    Online Continual Learning Without the Storage Constraint. (arXiv:2305.09253v1 [cs.CV])

    [http://arxiv.org/abs/2305.09253](http://arxiv.org/abs/2305.09253)

    本文提出了一种无存储限制的在线持续学习算法，使用kNN分类器和通用预训练特征提取器，在小算力的情况下紧凑地存储和利用整个输入数据流，并实现了更好的性能。

    

    在线持续学习（OCL）的研究主要集中在通过固定和有限的存储分配来减轻灾难性遗忘。然而，数据存储的可负担性提高了一系列不符合这些假设的应用。在这些情况下，主要关注点在于管理计算支出而不是存储。本文针对这种情况，通过放宽存储限制并强调固定的，有限的经济预算，研究在线持续学习问题。我们提供了一个简单的算法，可以使用kNN分类器和通用预训练特征提取器在微小的计算预算下紧凑地存储和利用整个输入数据流。我们的算法提供了一个常态化学习有吸引力的一致性属性：它永远不会忘记过去的数据。我们在两个大规模的OCL数据集上设立了一个新的状态：连续本地化（CL）和可持续的对象识别（SOR）。

    Online continual learning (OCL) research has primarily focused on mitigating catastrophic forgetting with fixed and limited storage allocation throughout the agent's lifetime. However, the growing affordability of data storage highlights a broad range of applications that do not adhere to these assumptions. In these cases, the primary concern lies in managing computational expenditures rather than storage. In this paper, we target such settings, investigating the online continual learning problem by relaxing storage constraints and emphasizing fixed, limited economical budget. We provide a simple algorithm that can compactly store and utilize the entirety of the incoming data stream under tiny computational budgets using a kNN classifier and universal pre-trained feature extractors. Our algorithm provides a consistency property attractive to continual learning: It will never forget past seen data. We set a new state of the art on two large-scale OCL datasets: Continual LOCalization (CL
    
[^179]: DCASE 2023 挑战任务2：面向机器状态监测的首次无监督异常声音检测的描述与讨论(arXiv:2305.07828v1 [cs.SD])

    Description and Discussion on DCASE 2023 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring. (arXiv:2305.07828v1 [cs.SD])

    [http://arxiv.org/abs/2305.07828](http://arxiv.org/abs/2305.07828)

    DCASE 2023 挑战任务2旨在解决机器状态监测中，部署新型机器的无监督异常声音检测，仅使用极少量的正常数据进行训练，且无需超参数调整。

    

    本文介绍了Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 挑战任务 2:“面向机器状态监测的首次无监督异常声音检测”。该任务的主要目标是仅使用少数正常样本就能快速部署针对新型机器的 ASD 系统，无需超参数调整。在过去的 ASD 任务中，发展的方法为每种机器类型调整超参数，因为发展和评估数据集具有相同的机器类型。然而，在实践中收集正常和异常数据集可能是不可行的。在 2023 Task 2 中，我们专注于解决首次问题，即在完全新型的机器类型的少数机器上训练模型的挑战。具体来说，（i）每种机器类型仅有一个部分，（ii）发展和评估数据集中的机器类型完全不同。我们将添加子任务的挑战结果和分析。

    We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge Task 2: "First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring". The main goal is to enable rapid deployment of ASD systems for new kinds of machines using only a few normal samples, without the need for hyperparameter tuning. In the past ASD tasks, developed methods tuned hyperparameters for each machine type, as the development and evaluation datasets had the same machine types. However, collecting normal and anomalous data as the development dataset can be infeasible in practice. In 2023 Task 2, we focus on solving first-shot problem, which is the challenge of training a model on a few machines of a completely novel machine type. Specifically, (i) each machine type has only one section, and (ii) machine types in the development and evaluation datasets are completely different. We will add challenge results and analysis of the sub
    
[^180]: 校准化解释：基于不确定性信息和反事实的解释模型

    Calibrated Explanations: with Uncertainty Information and Counterfactuals. (arXiv:2305.02305v1 [cs.AI])

    [http://arxiv.org/abs/2305.02305](http://arxiv.org/abs/2305.02305)

    该论文提出了一种新的特征重要性解释方法，Calibrated Explanations (CE)，它可以提供准确、稳定的解释，并且可以为概率估计和特征重要性权重提供不确定性量化信息，是一种快速、可靠且强健的解释方法。

    

    人工智能已经成为各种领域决策支持系统中不可或缺的一部分，但人工智能决策系统中预测模型缺乏透明度可能导致滥用或不使用。可解释人工智能旨在创建可以向人类用户解释其推理过程的人工智能系统。可解释人工智能中的局部解释可以提供关于特征重要性的个别预测原因的信息，但存在不稳定性等缺点。为了解决这些问题，我们提出了一种新的特征重要性解释方法，校准化解释(Calibrated Explanations，CE)，它基于 Venn-Abers，同时在生成特征重要性解释的同时校准底层模型。CE不仅提供快速、可靠、稳定和强健的解释，还提供概率估计和特征重要性权重的不确定性量化。此外，该方法是模型无关的，具有易于理解的条件规则，也可以生成反事实推理。

    Artificial Intelligence (AI) has become an integral part of decision support systems (DSSs) in various domains, but the lack of transparency in the predictive models used in AI-based DSSs can lead to misuse or disuse. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance, but they suffer from drawbacks such as instability. To address these issues, we propose a new feature importance explanation method, Calibrated Explanations (CE), which is based on Venn-Abers and calibrates the underlying model while generating feature importance explanations. CE provides fast, reliable, stable, and robust explanations, along with uncertainty quantification of the probability estimates and feature importance weights. Furthermore, the method is model agnostic with easily understood conditional rules and can also genera
    
[^181]: 多分辨率卷积记忆的序列建模

    Sequence Modeling with Multiresolution Convolutional Memory. (arXiv:2305.01638v1 [cs.LG])

    [http://arxiv.org/abs/2305.01638](http://arxiv.org/abs/2305.01638)

    本论文提出了一种新的用于序列建模的构建块，称为MultiresLayer，通过多分辨率卷积捕获输入序列中的多尺度趋势，既具有卷积网络的计算优势，又具有小波分解的有理论基础的动机。

    

    有效地捕捉对于某个任务（如分类和生成建模）显著的顺序数据源中的长程模式是一个基本挑战。我们从基于小波的多分辨率分析中获得灵感，定义了一个新的用于序列建模的构建块，称为MultiresLayer。我们模型的关键组成部分是多分辨率卷积，以捕获输入序列中的多尺度趋势。我们的MultiresConv可以通过在扩张的因果卷积树上使用共享过滤器来实现。因此，它既具有卷积网络的计算优势，又具有小波分解的有理论基础的动机。

    Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence. Our MultiresConv can be implemented with shared filters across a dilated causal convolution tree. Thus it garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions. 
    
[^182]: GPT-2是如何计算大于符号的？解释预训练语言模型中的数学能力

    How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])

    [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586)

    本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。

    

    预训练语言模型在未被明确训练的任务上表现出惊人的能力，但它们如何实现这些功能却不为人所知。本文通过机械式可解释性技术探究预训练语言模型通常具有的基本数学能力。具体来说，我们以GPT-2 Small为例，研究其能否通过输入"战争持续时间是从1732年到17年"，预测出有效的两位数字的截止年份 (大于32年)。我们首先确定了一个电路，即GPT-2 Small计算图的一个小子集，用于计算这个任务的输出，然后我们解释了每个电路组件的作用，显示出GPT-2 Small的最终多层感知器提高了结束年份大于开始年份的概率。最后，我们证明了我们的电路适用于其他任务，在其他大于场景中发挥作用。

    Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
    
[^183]: 图神经网络何时对节点分类有帮助：研究同源性原则对节点可区分性的影响

    When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability. (arXiv:2304.14274v1 [cs.SI])

    [http://arxiv.org/abs/2304.14274](http://arxiv.org/abs/2304.14274)

    同源性原则不一定是影响图神经网络优越性的唯一原因；本文提出Contextual Stochastic Block Model for Homophily (CSBM-H)以深入研究同源性对节点可区分性的影响。

    

    同源性原则指相同类别的节点更有可能连接在一起，一直被认为是图神经网络（GNN）在节点分类（NC）任务上性能优越的主要原因。最近，人们提出理论结果认为，即使同源性原则被打破，只要来自同一类别的节点分享相似的邻居模式，GNN的优势仍然存在，这对同源性的有效性提出了质疑。然而，这个论点仅考虑了同类节点的可区分性，忽略了跨类别的可区分性，这是研究同源性效应的不足之处。在本文中，我们首先通过例子证明了上述不足，并认为可区分性的理想情况是同类节点的可区分性小于跨类别节点的可区分性。为了形式化这个想法，更好地理解同源性，我们提出了Contextual Stochastic Block Model for Homophily (CSBM-H)，并进行了全面的实验分析。

    Homophily principle, i.e. nodes with the same labels are more likely to be connected, was believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks (NNs) on Node Classification (NC) tasks. Recently, people have developed theoretical results arguing that, even though the homophily principle is broken, the advantage of GNNs can still hold as long as nodes from the same class share similar neighborhood patterns, which questions the validity of homophily. However, this argument only considers intra-class Node Distinguishability (ND) and ignores inter-class ND, which is insufficient to study the effect of homophily. In this paper, we first demonstrate the aforementioned insufficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and have a better understanding of homophily, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and def
    
[^184]: 使用中间层扰动衰减来提高敌对迁移能力

    Improving Adversarial Transferability by Intermediate-level Perturbation Decay. (arXiv:2304.13410v1 [cs.LG])

    [http://arxiv.org/abs/2304.13410](http://arxiv.org/abs/2304.13410)

    本文提出了一种名为中间层扰动衰减（ILPD）的新型中间层方法，可以通过单个优化阶段制作可转移的对抗性样本，并在过程中鼓励中间层扰动处于有效的敌对方向，并同时具有很大的幅度。

    

    中间层攻击是指通过遵循对抗方向，尝试扰动特征表示的攻击方法，在制作可转移的对抗样本中展现出良好的性能。现有的这类攻击方法通常由两个分离的阶段构成，首先需要确定一个方向向导，然后扩大中间层扰动对该方向向导的标量投影。然而，得到的扰动在特征空间中难免会偏离向导。本文发现，这种偏离可能导致次优的攻击效果。为了解决这个问题，我们开发了一种名为中间层扰动衰减（ILPD）的新型中间层方法，可以通过单个优化阶段制作可转移的对抗性样本。具体来说，该方法鼓励中间层扰动处于有效的敌对方向，并同时具有很大的幅度。

    Intermediate-level attacks that attempt to perturb feature representations following an adversarial direction drastically have shown favorable performance in crafting transferable adversarial examples. Existing methods in this category are normally formulated with two separate stages, where a directional guide is required to be determined at first and the scalar projection of the intermediate-level perturbation onto the directional guide is enlarged thereafter. The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack. To address this issue, we develop a novel intermediate-level method that crafts adversarial examples within a single stage of optimization. In particular, the proposed method, named intermediate-level perturbation decay (ILPD), encourages the intermediate-level perturbation to be in an effective adversarial direction and to possess a great magnitude simultaneous
    
[^185]: 一种加速有限元求解器中代数多重网格方法的深度学习算法

    A Deep Learning algorithm to accelerate Algebraic Multigrid methods in Finite Element solvers of 3D elliptic PDEs. (arXiv:2304.10832v1 [math.NA])

    [http://arxiv.org/abs/2304.10832](http://arxiv.org/abs/2304.10832)

    该论文介绍了一种新的深度学习算法，通过解释线性系统的稀疏矩阵为黑白图像，利用池化操作将其转换为小的多通道图像，从而调整代数多重网格方法中的强门槛参数。该算法最小化了AMG方法在有限元求解器中的计算成本，并在解决三维椭圆偏微分方程时比现有最先进的AMG求解器更快。

    

    代数多重网格方法是解线性方程组的最有效的求解器之一，广泛用于离散化的偏微分方程问题。然而，该方法存在高度依赖于需要调优的参数，尤其是强门槛参数，这是构建多重网格所需的。我们引入了一种新的深度学习算法，通过把线性系统的稀疏矩阵解释为黑白图像，利用池化操作将它转换为小的多通道图像，从而调整强门槛参数，最小化了AMG方法在有限元求解器中的计算成本。我们展示了该算法对于解决三维椭圆偏微分方程的速度显著快于现有最先进的AMG求解器。

    Algebraic multigrid (AMG) methods are among the most efficient solvers for linear systems of equations and they are widely used for the solution of problems stemming from the discretization of Partial Differential Equations (PDEs). The most severe limitation of AMG methods is the dependence on parameters that require to be fine-tuned. In particular, the strong threshold parameter is the most relevant since it stands at the basis of the construction of successively coarser grids needed by the AMG methods. We introduce a novel Deep Learning algorithm that minimizes the computational cost of the AMG method when used as a finite element solver. We show that our algorithm requires minimal changes to any existing code. The proposed Artificial Neural Network (ANN) tunes the value of the strong threshold parameter by interpreting the sparse matrix of the linear system as a black-and-white image and exploiting a pooling operator to transform it into a small multi-channel image. We experimentall
    
[^186]: CAMEL: 用于“心智”探索大规模语言模型社群的交互式代理

    CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society. (arXiv:2303.17760v1 [cs.AI])

    [http://arxiv.org/abs/2303.17760](http://arxiv.org/abs/2303.17760)

    本文介绍了一个名为角色扮演的新型交互式代理框架，用于实现语言模型之间的自主合作，并展示了其在生成对话数据方面的有效性。

    

    对话式语言模型的快速发展已取得了在复杂任务解决方面的显著进展。然而，它们的成功在很大程度上依赖于人类的指导，以引导对话，这可能是具有挑战性和耗时的。本文探讨了构建可扩展技术以促进交互式代理之间的自主合作并深入了解它们的“认知”过程的潜力。为了解决实现自主合作的挑战，我们提出了一个名为角色扮演的新型交互式代理框架。我们的方法涉及使用启动提示来引导聊天代理完成任务，同时保持与人类意图的一致性。我们展示了如何使用角色扮演来生成对话数据，以研究聊天代理的行为和能力，为研究对话式语言模型提供了有价值的资源。我们的贡献是介绍了一种新型的交互式代理框架，名为角色扮演，用于实现语言模型之间的自主合作，并展示了其在生成对话数据方面的有效性。

    The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framewor
    
[^187]: 基于现代 Hopfield 网络的时间序列一致性预测方法

    Conformal Prediction for Time Series with Modern Hopfield Networks. (arXiv:2303.12783v1 [cs.LG])

    [http://arxiv.org/abs/2303.12783](http://arxiv.org/abs/2303.12783)

    该论文提出了一种名为 HopCPT 的新一致性时间序列预测方法，不仅能够处理时间结构，而且能够利用其优势，已在多种真实世界的时间序列数据集上证明了优于现有方法的性能。

    

    为了量化不确定性，一致性预测方法受到越来越多的关注，并已成功应用于各个领域。然而，它们难以应用于时间序列，因为时间序列的自相关结构违反了一致性预测所需的基本假设。我们提出了 HopCPT，一种新的基于 Hopfield 网络的时间序列一致性预测方法，不仅能够应对时间结构，而且能够利用它们。我们证明了我们的方法在存在时间依赖性的时间序列中在理论上是有很好的理论基础的。在实验中，我们证明了我们的新方法在四个不同领域的多个真实世界时间序列数据集上优于现有的最先进的一致性预测方法。

    To quantify uncertainty, conformal prediction methods are gaining continuously more interest and have already been successfully applied to various domains. However, they are difficult to apply to time series as the autocorrelative structure of time series violates basic assumptions required by conformal prediction. We propose HopCPT, a novel conformal prediction approach for time series that not only copes with temporal structures but leverages them. We show that our approach is theoretically well justified for time series where temporal dependencies are present. In experiments, we demonstrate that our new approach outperforms state-of-the-art conformal prediction methods on multiple real-world time series datasets from four different domains.
    
[^188]: SIESTA: 高效的在线持续学习与休眠 (arXiv:2303.10725v2 [cs.CV] UPDATED)

    SIESTA: Efficient Online Continual Learning with Sleep. (arXiv:2303.10725v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10725](http://arxiv.org/abs/2303.10725)

    SIESTA是一种在线持续学习方法，通过使用无需回忆、无需反向传播和数据驱动的网络更新规则，在更少的时间和能源消耗下高效地更新深度神经网络(DNN)。该方法基于训练休眠/觉醒框架，可以应用于设备端学习。

    

    在监督式持续学习中，深度神经网络(DNN)通过不断增长的数据流进行更新。与数据离线情况不同，我们不能对数据流进行任何分布假设。理想情况下，为了提高计算效率，只需要对数据集进行一次遍历。然而，现有的方法无法满足真实世界应用的条件，同时也无法提高计算效率。在本文中，我们提出了一种基于训练休眠/觉醒框架的新型在线持续学习方法SIESTA，该方法符合设备端学习的需求。SIESTA的主要目标是改进计算效率，以便可以在更少的时间和能源消耗下高效地更新DNN。SIESTA的主要创新点有：在觉醒阶段使用无需回忆、无需反向传播和数据驱动的网络更新规则进行快速在线更新，以及快速收敛的Wake/Sleep训练框架。

    In supervised continual learning, a deep neural network (DNN) is updated with an ever-growing data stream. Unlike the offline setting where data is shuffled, we cannot make any distributional assumptions about the data stream. Ideally, only one pass through the dataset is needed for computational efficiency. However, existing methods are inadequate and make many assumptions that cannot be made for real-world applications, while simultaneously failing to improve computational efficiency. In this paper, we propose a novel online continual learning method, SIESTA based on wake/sleep framework for training, which is well aligned to the needs of on-device learning. The major goal of SIESTA is to advance compute efficient continual learning so that DNNs can be updated efficiently using far less time and energy. The principal innovations of SIESTA are: 1) rapid online updates using a rehearsal-free, backpropagation-free, and data-driven network update rule during its wake phase, and 2) expedi
    
[^189]: 使用深度离线强化学习实现安全的丙泊酚全麻剂量控制

    Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning. (arXiv:2303.10180v1 [cs.LG])

    [http://arxiv.org/abs/2303.10180](http://arxiv.org/abs/2303.10180)

    本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来实现全麻药物剂量控制，添加了保守Q-Learning方法和策略约束项以确保智能体做出更安全的决策。

    

    自动化麻醉有望实现更精确和个性化的麻醉管理，使麻醉师免于重复性任务，专注于患者手术护理的最关键方面。当前的研究通常集中于创建模拟环境，以便智能体进行学习。这些方法已经展示出良好的实验结果，但离临床应用还有很大的差距。本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来解决学习麻醉策略的问题。首先引入了保守Q-Learning方法以缓解脱线情况下Q函数过度估计的问题。在智能体的培训中添加一项策略约束项，以保持智能体和麻醉师的策略分布一致，以确保智能体在全麻情景下做出更安全的决策。通过实验证明了PCQL的有效性。

    Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated b
    
[^190]: 数据中心机器学习的重新标签法

    The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04391](http://arxiv.org/abs/2302.04391)

    本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。

    

    在深度学习应用中，手动标记的数据在一定程度上存在噪声。为了解决这个问题，并在开发数据集上获得90分以上的成绩，本文提出了一种简单的方法来找出噪声数据，并通过采用模型预测作为人类标记的参考来重新标记噪声数据。本文阐述了我们在广泛的深度学习任务中的想法，包括分类、序列标记、物体检测、序列生成、点击率预测。实验结果和人类评估结果验证了我们的想法。

    In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
    
[^191]: 用GFlowNets实现高效的多目标分子优化

    Sample-efficient Multi-objective Molecular Optimization with GFlowNets. (arXiv:2302.04040v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04040](http://arxiv.org/abs/2302.04040)

    本研究提出了一种使用GFlowNets的高效多目标分子优化算法，该算法通过使用超网络来优化收益函数，从而在考虑多样性的同时实现了从近似帕累托前沿中采样出多样化的候选分子图。同时还使用了一种类似于事后认识的离线策略来加快优化速度。

    

    许多关键的科学问题涉及设计具有所需属性的新型分子，这可以被形式化为在离散化的化学空间上的黑盒优化问题。在实践中，多个冲突的目标和昂贵的评估（例如湿实验）使得候选人的多样性至关重要。计算方法已经取得了初步的成功，但仍然在同时考虑目标和搜索空间的多样性方面存在困难。为了填补这个空白，我们提出了一种多目标贝叶斯优化（MOBO）算法，利用基于超网络的GFlowNets（HN-GFN）作为收益函数优化器，目的是从近似帕累托前沿中采样出多样化的候选分子图。使用单一的条件化超网络，HN-GFN学习探索各个目标之间的各种折中。我们进一步提出了一种类似于事后认识的离线策略，以便在不同偏好之间共享高性能分子，以加快速度。

    Many crucial scientific problems involve designing novel molecules with desired properties, which can be formulated as a black-box optimization problem over the discrete chemical space. In practice, multiple conflicting objectives and costly evaluations (e.g., wet-lab experiments) make the diversity of candidates paramount. Computational methods have achieved initial success but still struggle with considering diversity in both objective and search space. To fill this gap, we propose a multi-objective Bayesian optimization (MOBO) algorithm leveraging the hypernetwork-based GFlowNets (HN-GFN) as an acquisition function optimizer, with the purpose of sampling a diverse batch of candidate molecular graphs from an approximate Pareto front. Using a single preference-conditioned hypernetwork, HN-GFN learns to explore various trade-offs between objectives. We further propose a hindsight-like off-policy strategy to share high-performing molecules among different preferences in order to speed u
    
[^192]: 神经最优输运下的极值域翻译

    Extremal Domain Translation with Neural Optimal Transport. (arXiv:2301.12874v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12874](http://arxiv.org/abs/2301.12874)

    该论文提出了一种称为“极值传输(ET)”的方法，可用于进行给定相似性函数下的一对域之间的最佳可能的非配对翻译，并且提出了一种可扩展的基于神经最优输运(OT)的算法来逼近ET映射。

    

    我们提出了一种称为“极值传输(ET)”的方法，这是对于给定相似性函数的一对域之间的最佳可能的非配对翻译的数学形式化。受到神经最优输运(OT)近期发展的启发，我们提出了一种可扩展的算法，以OT的部分映射极限来逼近ET映射。我们在玩具实例和非配对图像到图像的翻译任务上测试了我们的算法。

    We propose the extremal transport (ET) which is a mathematical formalization of the theoretically best possible unpaired translation between a pair of domains w.r.t. the given similarity function. Inspired by the recent advances in neural optimal transport (OT), we propose a scalable algorithm to approximate ET maps as a limit of partial OT maps. We test our algorithm on toy examples and on the unpaired image-to-image translation task.
    
[^193]: 数字孪生的因果伪证

    Causal Falsification of Digital Twins. (arXiv:2301.07210v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2301.07210](http://arxiv.org/abs/2301.07210)

    这篇论文提出了一种数字孪生的因果伪证方法，以可靠并实用的方式在最小限度的假设下提供孪生的信息和评估结果。

    

    数字孪生在很多应用中具有巨大的潜力，但是在安全关键场景下广泛部署它们的精度评估需要严格的程序。通过在因果推理框架内制定这个任务，我们表明，使用现实数据尝试证明孪生的正确性是不可靠的，除非在数据生成过程中进行可能有风险的假设。为了避免这些假设，我们提出了一种评估策略，旨在找到孪生不正确的情况，并提出了用于实现此目标的通用统计过程，可用于各种应用和孪生模型。我们的方法在最小假设下提供了可靠和可操作的孪生信息和评估结果。通过包含脉冲生理学引擎中脓毒症建模的大型案例研究，我们证明了我们方法的有效性。

    Digital twins hold substantial promise in many applications, but rigorous procedures for assessing their accuracy are essential for their widespread deployment in safety-critical settings. By formulating this task within the framework of causal inference, we show that attempts to certify the correctness of a twin using real-world observational data are unsound unless potentially tenuous assumptions are made about the data-generating process. To avoid these assumptions, we propose an assessment strategy that instead aims to find cases where the twin is not correct, and present a general-purpose statistical procedure for doing so that may be used across a wide variety of applications and twin models. Our approach yields reliable and actionable information about the twin under minimal assumptions about the twin and the real-world process of interest. We demonstrate the effectiveness of our methodology via a large-scale case study involving sepsis modelling within the Pulse Physiology Engi
    
[^194]: 系统的线性偏微分方程的高斯过程先验与常系数（翻译自arXiv:2212.14319v3 [stat.ML] 更新）

    Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients. (arXiv:2212.14319v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.14319](http://arxiv.org/abs/2212.14319)

    该论文提出了一种名为EPGP的高斯过程先验，用于线性偏微分方程系统，并且构造了反映标准谱方法的GP核函数。该方法可以推断线性PDE系统的可能解，并具有算法性强、普适性广、适用于大数据集的稀疏版本。

    

    偏微分方程（PDE）是建模物理系统的重要工具，将它们纳入机器学习模型是将物理知识纳入的重要方式。对于任何具有常系数的线性PDE系统，我们提出了一族称为EPGP的高斯过程（GP）先验，使得所有实现都是该系统的精确解。我们应用Ehrenpreis-Palamodov基本原理，它作为一种非线性傅里叶变换，构建了GP核函数，反映了标准的谱方法用于GP。我们的方法可以从任何数据（如有噪声的测量数据或点定义的初始和边界条件）推断线性PDE系统的可能解。构造EPGP先验的算法性强，普适性广，并且有一个稀疏版本（S-EPGP），可以学习相关的谱频率，并在大数据集上运行效果更好。我们在三类PDE系统上演示了我们的方法，包括热方程和波方程。

    Partial differential equations (PDEs) are important tools to model physical systems and including them into machine learning models is an important way of incorporating physical knowledge. Given any system of linear PDEs with constant coefficients, we propose a family of Gaussian process (GP) priors, which we call EPGP, such that all realizations are exact solutions of this system. We apply the Ehrenpreis-Palamodov fundamental principle, which works as a non-linear Fourier transform, to construct GP kernels mirroring standard spectral methods for GPs. Our approach can infer probable solutions of linear PDE systems from any data such as noisy measurements, or pointwise defined initial and boundary conditions. Constructing EPGP-priors is algorithmic, generally applicable, and comes with a sparse version (S-EPGP) that learns the relevant spectral frequencies and works better for big data sets. We demonstrate our approach on three families of systems of PDEs, the heat equation, wave equati
    
[^195]: 贝叶斯网络的逆推

    Inversion of Bayesian Networks. (arXiv:2212.10649v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10649](http://arxiv.org/abs/2212.10649)

    本文研究了识别网络如何模拟真实后验分布的必要和充分条件，通过导出全局条件和局部条件，发现完美性为其具备期望性质起到了重要作用。

    

    变分自编码器和Helmholtz机使用一个识别网络（编码器）来近似生成模型（解码器）的后验分布。本文研究了识别网络具备模拟真实后验分布的必要和充分条件。这些结果基于概率图模型／贝叶斯网络的一般背景，其中网络代表了一组条件独立性语句。我们导出了全局条件（通过d-分离）和局部条件，使得识别网络具备期望的性质。局部条件中，完美性（每个节点只与其父节点相连）发挥了重要作用。

    Variational autoencoders and Helmholtz machines use a recognition network (encoder) to approximate the posterior distribution of a generative model (decoder). In this paper we study the necessary and sufficient properties of a recognition network so that it can model the true posterior distribution exactly. These results are derived in the general context of probabilistic graphical modelling / Bayesian networks, for which the network represents a set of conditional independence statements. We derive both global conditions, in terms of d-separation, and local conditions for the recognition network to have the desired qualities. It turns out that for the local conditions the property perfectness (for every node, all parents are joined) plays an important role.
    
[^196]: 超越集合平均值：利用气候模型集合进行地表季节性预测

    Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting. (arXiv:2211.15856v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15856](http://arxiv.org/abs/2211.15856)

    本文研究了利用气候模型集合进行地表季节性预测的应用，超越了传统的平均方法，利用集合预测中的信息提高了预测准确性，关注了极端事件的预测，同时考虑了空间变化的预测集合。

    

    在操作性预测中，对于关键气候变量（如温度和降水）进行高质量的地表季节性时间尺度预测一直存在着差距。最近的研究表明，使用机器学习（ML）模型来推进地表季节性预测（SSF）取得了有希望的结果，但仍存在一些未解的问题。首先，过去的方法中使用了物理基于模型集合的平均作为这些模型的输入特征，然而集合预测中包含了可以帮助预测的信息，不仅仅是集合均值。其次，过去的方法关注平均性能，然而对于计划和减灾目的来说，极端事件的预测更加重要。第三，气候预测对应于一个空间变化的预测集合，而不同的方法以不同的方式考虑了响应的空间可变性。模型堆叠可以缓解不同方法之间的权衡。本文描述了一种利用模型集合进行地表季节性预测的应用。

    Producing high-quality forecasts of key climate variables such as temperature and precipitation on subseasonal time scales has long been a gap in operational forecasting. Recent studies have shown promising results using machine learning (ML) models to advance subseasonal forecasting (SSF), but several open questions remain. First, several past approaches use the average of an ensemble of physics-based forecasts as an input feature of these models. However, ensemble forecasts contain information that can aid prediction beyond only the ensemble mean. Second, past methods have focused on average performance, whereas forecasts of extreme events are far more important for planning and mitigation purposes. Third, climate forecasts correspond to a spatially-varying collection of forecasts, and different methods account for spatial variability in the response differently. Trade-offs between different approaches may be mitigated with model stacking. This paper describes the application of a va
    
[^197]: 提升时间序列预测中超参数优化的分层代理建模

    Hierarchical Proxy Modeling for Improved HPO in Time Series Forecasting. (arXiv:2211.15092v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15092](http://arxiv.org/abs/2211.15092)

    该论文提出了一种使用分层代理建模的技术H-Pro，通过利用时间序列数据的层次结构，通过测试代理驱动超参数优化，以解决测试验证期间不匹配的问题，并验证了该技术在时间序列数据集上的有效性。

    

    在时间序列预测中，选择正确的超参数集是至关重要的。传统的时间交叉验证框架在超参数优化中经常导致测试性能差，因为验证和测试期间可能存在不匹配的情况。为了解决这个测试-验证不匹配问题，我们提出了一种新颖的技术，即H-Pro，通过利用与时间序列数据集经常相关的数据层次结构，通过测试代理来驱动HPO。由于高层次的聚合时间序列通常显示出较少的不规则性和更好的可预测性，相比之下最低层次的时间序列可能是稀疏和间歇性的，我们通过利用高层次预测器生成的测试期间的代理预测结果来优化最低层次的基本预测器的超参数。H-Pro可以应用于任何现成的机器学习模型进行HPO。我们通过对五个公开可用的分层时间序列数据集进行了大量的经验评估，验证了我们的技术的有效性。

    Selecting the right set of hyperparameters is crucial in time series forecasting. The classical temporal cross-validation framework for hyperparameter optimization (HPO) often leads to poor test performance because of a possible mismatch between validation and test periods. To address this test-validation mismatch, we propose a novel technique, H-Pro to drive HPO via test proxies by exploiting data hierarchies often associated with time series datasets. Since higher-level aggregated time series often show less irregularity and better predictability as compared to the lowest-level time series which can be sparse and intermittent, we optimize the hyperparameters of the lowest-level base-forecaster by leveraging the proxy forecasts for the test period generated from the forecasters at higher levels. H-Pro can be applied on any off-the-shelf machine learning model to perform HPO. We validate the efficacy of our technique with extensive empirical evaluation on five publicly available hierar
    
[^198]: 《Stein变分梯度下降的有限粒子收敛速度》

    A Finite-Particle Convergence Rate for Stein Variational Gradient Descent. (arXiv:2211.09721v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09721](http://arxiv.org/abs/2211.09721)

    本文提供了Stein变分梯度下降算法的有限粒子收敛速度，证明了当目标分布为次高斯且具有Lipschitz积分核时，使用适当的步长序列和粒子数量，可以以1/√(log log n)的速度将核Stein差异逼近零。

    

    我们首次提供了Stein变分梯度下降（SVGD）的有限粒子收敛速度，这是一种用一组粒子逼近概率分布的流行算法。具体来说，只要目标分布是次高斯的，并且具有Lipschitz积分核，使用n个粒子和适当的步长序列进行SVGD，核Stein差异将以1/√(log log n)的速度趋于零。我们怀疑n的依赖性可以改进，希望我们的明确的非渐近证明策略能为未来的改进提供模板。

    We provide the first finite-particle convergence rate for Stein variational gradient descent (SVGD), a popular algorithm for approximating a probability distribution with a collection of particles. Specifically, whenever the target distribution is sub-Gaussian with a Lipschitz score, SVGD with n particles and an appropriate step size sequence drives the kernel Stein discrepancy to zero at an order 1/sqrt(log log n) rate. We suspect that the dependence on n can be improved, and we hope that our explicit, non-asymptotic proof strategy will serve as a template for future refinements.
    
[^199]: 搭建机器学习与科学的桥梁：机遇与挑战

    Bridging Machine Learning and Sciences: Opportunities and Challenges. (arXiv:2210.13441v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.13441](http://arxiv.org/abs/2210.13441)

    本文探讨了机器学习在科学领域的应用，尤其是在离群样本检测方面取得了重要进展，同时提出了数据普适性、实验协议和模型鲁棒性等方面的挑战。

    

    机器学习在科学领域的应用近年来取得了令人振奋的进展。作为一种广泛适用的技术，异常检测在机器学习社区中一直受到关注。特别是基于深度神经网络的离群样本检测在高维数据方面取得了巨大的进步。最近，这些技术展示了在科学学科中的潜力。我们对它们在数据普适性、实验协议、模型鲁棒性等方面进行了批判性探讨。我们讨论了同时展示了可转移实践和领域特定挑战的示例，为在近期建立一个新的跨学科研究范式提供了一个起点。

    The application of machine learning in sciences has seen exciting advances in recent years. As a widely applicable technique, anomaly detection has been long studied in the machine learning community. Especially, deep neural nets-based out-of-distribution detection has made great progress for high-dimensional data. Recently, these techniques have been showing their potential in scientific disciplines. We take a critical look at their applicative prospects including data universality, experimental protocols, model robustness, etc. We discuss examples that display transferable practices and domain-specific challenges simultaneously, providing a starting point for establishing a novel interdisciplinary research paradigm in the near future.
    
[^200]: 利用人体动作合成进行计算编舞

    Computational Choreography using Human Motion Synthesis. (arXiv:2210.04366v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.04366](http://arxiv.org/abs/2210.04366)

    本文介绍了一种利用深度学习模型分析舞蹈动作和生成新动作序列的方法，同时也结合了前人的努力来开发出一套系统。

    

    深度学习模型是否应该被训练来分析人体表演艺术？为了回答这个问题，我们探索了深度神经网络在合成艺术人体动作方面的应用。人体运动合成中的问题任务包括预测野外环境中人体运动，以及生成基于这些预测的新动作序列。我们将讨论一个非传统的应用潜力，即将学习模型应用于预测舞蹈动作。最近有一些显著的努力，以计算的方式分析舞蹈动作，例如Everybody Dance Now（EDN）学习模型和Cal Poly硕士论文Take The Lead（TTL）。我们有效地将这两个作品与我们自己的深度神经网络结合起来，生成了一种新的舞蹈动作预测系统、图像到图像的转换和视频生成。

    Should deep learning models be trained to analyze human performance art? To help answer this question, we explore an application of deep neural networks to synthesize artistic human motion. Problem tasks in human motion synthesis can include predicting the motions of humans in-the-wild, as well as generating new sequences of motions based on said predictions. We will discuss the potential of a less traditional application, where learning models are applied to predicting dance movements. There have been notable, recent efforts to analyze dance movements in a computational light, such as the Everybody Dance Now (EDN) learning model and a Cal Poly master's thesis, Take The Lead (TTL). We have effectively combined these two works along with our own deep neural network to produce a new system for dance motion prediction, image-to-image translation, and video generation.
    
[^201]: 通过核差异实现有针对性的分离与收敛

    Targeted Separation and Convergence with Kernel Discrepancies. (arXiv:2209.12835v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.12835](http://arxiv.org/abs/2209.12835)

    通过核差异度量，我们推导出了新的充分必要条件，实现了将目标分离出来，以及控制对目标的弱收敛性。此外，我们在$\mathbb{R}^d$上使用了这些结果来扩展了核Stein差异分离和收敛控制的已知条件，并开发了能够精确度量目标的弱收敛性的核差异度量。

    

    最大均值差异（MMDs）如核Stein差异（KSD）已经成为广泛应用的中心，包括假设检验、采样器选择、分布近似和变分推断。在每个设置中，这些基于核的差异度量需要实现（i）将目标P与其他概率测度分离，甚至（ii）控制对P的弱收敛。在本文中，我们推导了确保（i）和（ii）的新的充分必要条件。对于可分的度量空间上的MMDs，我们描述了分离Bochner可嵌入测度的核，并引入简单的条件来分离所有具有无界核的测度和用有界核来控制收敛。我们利用这些结果在$\mathbb{R}^d$上大大扩展了KSD分离和收敛控制的已知条件，并开发了首个能够精确度量对P的弱收敛的KSDs。在这个过程中，我们强调了我们的结果的影响。

    Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this article we derive new sufficient and necessary conditions to ensure (i) and (ii). For MMDs on separable metric spaces, we characterize those kernels that separate Bochner embeddable measures and introduce simple conditions for separating all measures with unbounded kernels and for controlling convergence with bounded kernels. We use these results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD separation and convergence control and to develop the first KSDs known to exactly metrize weak convergence to P. Along the way, we highlight the implications of our res
    
[^202]: 基于条件扩散模型的有损图像压缩

    Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v5 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.06950](http://arxiv.org/abs/2209.06950)

    本文提出了一种利用条件扩散模型进行有损图像压缩的优化框架。通过引入额外的内容潜变量以及合成纹理变量，该方法在图像质量评估指标上表现出更强的性能。

    

    本文提出了一种利用扩散生成模型的端到端优化的有损图像压缩框架。该方法基于变换编码范式，将图像映射到潜在空间进行信息熵编码，然后再映射回数据空间进行重构。与基于变分自编码器(VAE)的神经压缩方法不同，我们的解码器是一个条件扩散模型。因此，我们的方法引入了一个额外的“内容”潜变量，反向扩散过程会对其进行条件化，并利用该变量存储图像信息。决定扩散过程的剩余“纹理”变量会在解码时合成。通过实验，我们展示了模型的性能可以根据感知度量进行调整。我们广泛的实验涉及了多个数据集和图像质量评估指标，结果表明我们的方法相较于基于生成对抗网络的方法能够得到更好的FID分数。

    This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional "content" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining "texture" variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based mode
    
[^203]: SensorSCAN: 自我监督学习和深度聚类在化工过程中的故障诊断中的应用

    SensorSCAN: Self-Supervised Learning and Deep Clustering for Fault Diagnosis in Chemical Processes. (arXiv:2208.08879v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.08879](http://arxiv.org/abs/2208.08879)

    "SensorSCAN"是一种自我监督学习和深度聚类的方法，用于在化工过程中进行故障诊断。使用这种方法，在无需专家注释的情况下，可以有效检测大多数过程故障，并且通过在少量标记数据上微调，几乎达到了最优模型的性能水平。

    

    现代工业设施在生产过程中产生大量的原始传感器数据。这些数据用于监控和控制过程，并可分析以检测和预测过程异常。通常，数据必须由专家进行注释，以便在预测建模中使用。然而，在工业环境中手动注释大量数据可能很困难。在本文中，我们提出了一种名为SensorSCAN的新方法，用于无监督故障检测和诊断，专为工业化学过程监测而设计。我们在两个公开可用的Tennessee Eastman工艺数据集上展示了我们模型的性能，包括各种故障。结果表明，我们的方法在没有专家注释的情况下明显优于现有方法（在固定FPR下，增加了0.2-0.3的TPR），有效地检测出大多数过程故障。此外，我们展示了在少量标记数据上微调的模型几乎达到了SOT模型的性能水平。

    Modern industrial facilities generate large volumes of raw sensor data during the production process. This data is used to monitor and control the processes and can be analyzed to detect and predict process abnormalities. Typically, the data has to be annotated by experts in order to be used in predictive modeling. However, manual annotation of large amounts of data can be difficult in industrial settings.  In this paper, we propose SensorSCAN, a novel method for unsupervised fault detection and diagnosis, designed for industrial chemical process monitoring. We demonstrate our model's performance on two publicly available datasets of the Tennessee Eastman Process with various faults. The results show that our method significantly outperforms existing approaches (+0.2-0.3 TPR for a fixed FPR) and effectively detects most of the process faults without expert annotation. Moreover, we show that the model fine-tuned on a small fraction of labeled data nearly reaches the performance of a SOT
    
[^204]: 通过特征解释实现私有图提取

    Private Graph Extraction via Feature Explanations. (arXiv:2206.14724v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14724](http://arxiv.org/abs/2206.14724)

    本论文研究了隐私与可解释性在图机器学习中的相互作用，提出了几种图重构攻击方法，并发现后续特征解释对攻击的成功率有显著影响。对于图神经网络的不同解释方法，基于梯度的解释揭示了最多的图结构信息，但效用并不总是最高。

    

    隐私和可解释性是实现可信机器学习的两个重要因素。我们通过图重构攻击研究了这两个方面在图机器学习中的相互作用。攻击者的目标是在访问模型解释的情况下重建训练数据的图结构。根据攻击者可用的不同辅助信息，我们提出了几种图重建攻击方法。我们发现，对于后续特征解释的附加知识显著提高了这些攻击的成功率。此外，我们详细研究了针对图神经网络的三种不同解释方法（基于梯度、扰动和替代模型的方法）在攻击性能方面的差异。尽管基于梯度的解释在揭示图结构方面最为出色，但我们发现这些解释在效用方面并不总是得分较高。其他两种方法的效果较好。

    Privacy and interpretability are two important ingredients for achieving trustworthy machine learning. We study the interplay of these two aspects in graph machine learning through graph reconstruction attacks. The goal of the adversary here is to reconstruct the graph structure of the training data given access to model explanations. Based on the different kinds of auxiliary information available to the adversary, we propose several graph reconstruction attacks. We show that additional knowledge of post-hoc feature explanations substantially increases the success rate of these attacks. Further, we investigate in detail the differences between attack performance with respect to three different classes of explanation methods for graph neural networks: gradient-based, perturbation-based, and surrogate model-based methods. While gradient-based explanations reveal the most in terms of the graph structure, we find that these explanations do not always score high in utility. For the other tw
    
[^205]: 未知动态系统的假设检验及自编码器进行系统异常检测

    Hypothesis Testing for Unknown Dynamical Systems and System Anomaly Detection via Autoencoders. (arXiv:2201.12358v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12358](http://arxiv.org/abs/2201.12358)

    本文研究了未知动态系统的假设检验问题，并将异常检测定义为通过备选假设来进行假设检验。通过假设检验和图模型的分析，提出了一种新的神经网络设计，DyAD，该设计可以用于改进系统异常检测的性能。

    

    本文研究了未知动态系统的假设检验问题。具体地，我们观测到具有未知参数的动态系统的连续输入和输出数据，旨在确定收集到的数据是否来自零分布。这样的问题可以有许多应用。我们将异常检测定义为通过备择假设来进行假设检验的一种方法。因此，假设检验算法可以检测机器人、天气、能源系统和股票市场等实际系统中的故障。虽然最近的研究使用深度学习模型在这些任务中取得了最先进的性能，但我们展示了使用假设检验和图模型进行仔细分析，不仅可以证明自编码器模型的有效性，还可以导致一种改进性能的新型神经网络设计，称为DyAD（动态系统异常检测）。然后，我们展示了DyAD取得了最先进的性能。

    We study the hypothesis testing problem for unknown dynamical systems. More specifically, we observe sequential input and output data from a dynamical system with unknown parameters, and we aim to determine whether the collected data is from a null distribution. Such a problem can have many applications. Here we formulate anomaly detection as hypothesis testing where the anomaly is defined through the alternative hypothesis. Consequently, hypothesis testing algorithms can detect faults in real-world systems such as robots, weather, energy systems, and stock markets. Although recent works achieved state-of-the-art performances in these tasks with deep learning models, we show that a careful analysis using hypothesis testing and graphical models can not only justify the effectiveness of autoencoder models, but also lead to a novel neural network design, termed DyAD (DYnamical system Anomaly Detection), with improved performances. We then show that DyAD achieves state-of-the-art performan
    
[^206]: 在神经网络中结合最优路径搜索和任务相关学习

    Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.11104](http://arxiv.org/abs/2201.11104)

    这篇论文提出了一种在神经网络中结合最优路径搜索和任务相关学习的方法，通过将成本值转化为神经网络的权重来实现在线权重适应。实验结果表明，该方法与经典算法Bellman-Ford具有相同的解，并且网络学习机制可以进一步增强算法的性能。

    

    在连接图中找到最优路径需要确定沿着图的边缘行进的最小总成本。这个问题可以通过几种经典算法来解决，通常所有边缘的成本都是预先定义好的。因此，在想要根据某个任务的要求以自适应的方式改变成本时，通常无法使用传统规划方法。在这里，我们展示了可以通过将成本值转化为突触权重来定义路径搜索问题的神经网络表示，这允许使用网络学习机制进行在线权重适应。当从一个初始活跃度值为1开始时，在这个网络中的活动传播将导致与Bellman-Ford算法找到的解相同的解。神经网络具有与Bellman-Ford相同的算法复杂度，并且此外，我们可以证明网络学习机制（如赫布学习）可以调整网络中的权重来增强算法的性能。

    Finding optimal paths in connected graphs requires determining the smallest total cost for traveling along the graph's edges. This problem can be solved by several classical algorithms where, usually, costs are predefined for all edges. Conventional planning methods can, thus, normally not be used when wanting to change costs in an adaptive way following the requirements of some task. Here we show that one can define a neural network representation of path finding problems by transforming cost values into synaptic weights, which allows for online weight adaptation using network learning mechanisms. When starting with an initial activity value of one, activity propagation in this network will lead to solutions, which are identical to those found by the Bellman-Ford algorithm. The neural network has the same algorithmic complexity as Bellman-Ford and, in addition, we can show that network learning mechanisms (such as Hebbian learning) can adapt the weights in the network augmenting the r
    
[^207]: 《长话短说：因果机器学习中的遗漏变量偏差》

    Long Story Short: Omitted Variable Bias in Causal Machine Learning. (arXiv:2112.13398v4 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2112.13398](http://arxiv.org/abs/2112.13398)

    在因果机器学习中，我们通过推导出遗漏变量偏差的尖锐上界，为广泛的线性泛函因果参数提供了一种简单而通用的方法。这种方法可以应用于许多传统的因果推断研究目标，并且仅取决于潜变量在结果和参数的Riesz表示器中所导致的额外变异。

    

    我们推导了一类广泛的因果参数的遗漏变量偏差的一般但简单的尖锐上界，这些参数可以被认定为结果的条件期望函数的线性泛函。这样的泛函包括许多因果推断研究中的传统调查目标，例如（加权）潜在结果的平均值、平均处理效应（包括子组效应，如对待处理对象的影响）、（加权）平均导数和来自协变量分布变化的策略效应 - 全部适用于一般的非参数因果模型。我们的构造依赖于目标泛函的Riesz-Fréchet表示。具体来说，我们展示了偏差上界仅取决于潜变量在结果和感兴趣参数的Riesz表示器中所创建的附加变化。此外，在许多重要情况下（例如平均处理效应和平均导数）

    We derive general, yet simple, sharp bounds on the size of the omitted variable bias for a broad class of causal parameters that can be identified as linear functionals of the conditional expectation function of the outcome. Such functionals encompass many of the traditional targets of investigation in causal inference studies, such as, for example, (weighted) average of potential outcomes, average treatment effects (including subgroup effects, such as the effect on the treated), (weighted) average derivatives, and policy effects from shifts in covariate distribution -- all for general, nonparametric causal models. Our construction relies on the Riesz-Frechet representation of the target functional. Specifically, we show how the bound on the bias depends only on the additional variation that the latent variables create both in the outcome and in the Riesz representer for the parameter of interest. Moreover, in many important cases (e.g, average treatment effects and avearage derivative
    
[^208]: 使用耦合方法界定Wasserstein距离

    Bounding Wasserstein distance with couplings. (arXiv:2112.03152v3 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2112.03152](http://arxiv.org/abs/2112.03152)

    该论文提出了基于马尔可夫链耦合的估计器，用于评估渐近有偏采样方法的质量，并给出了渐近有偏采样方法的极限分布与原始目标分布之间的Wasserstein距离的经验上界。

    

    马尔可夫链蒙特卡罗（MCMC）在迭代次数趋于无穷时提供了对难以计算的后验期望的渐近一致估计。然而，在大型数据应用中，MCMC每次迭代的计算成本很高。这促使人们对以提高每次迭代的计算速度为目标的MCMC近似方法产生了兴趣，但这些方法不会产生渐近一致估计。在本文中，我们提出了基于马尔可夫链耦合的估计器，用于评估这种渐近有偏采样方法的质量。这些估计器给出了渐近有偏采样方法的极限分布与原始目标分布之间的Wasserstein距离的经验上界。我们为我们的上界建立了理论保证，并表明我们的估计器在高维情况下仍然有效。我们将我们的质量度量应用于随机梯度MCMC、变分贝叶斯和拉普拉斯近似等方法。

    Markov chain Monte Carlo (MCMC) provides asymptotically consistent estimates of intractable posterior expectations as the number of iterations tends to infinity. However, in large data applications, MCMC can be computationally expensive per iteration. This has catalyzed interest in approximating MCMC in a manner that improves computational speed per iteration but does not produce asymptotically consistent estimates. In this article, we propose estimators based on couplings of Markov chains to assess the quality of such asymptotically biased sampling methods. The estimators give empirical upper bounds of the Wasserstein distance between the limiting distribution of the asymptotically biased sampling method and the original target distribution of interest. We establish theoretical guarantees for our upper bounds and show that our estimators can remain effective in high dimensions. We apply our quality measures to stochastic gradient MCMC, variational Bayes, and Laplace approximations for
    
[^209]: 具有差分隐私保证的发布图神经网络

    Releasing Graph Neural Networks with Differential Privacy Guarantees. (arXiv:2109.08907v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.08907](http://arxiv.org/abs/2109.08907)

    这篇论文提出了PrivGNN，一种隐私保护的框架，用于在集中式环境中发布具有差分隐私保证的图神经网络。该方法结合了知识蒸馏框架和两个噪声机制，通过在公共数据上训练和私有数据的知识，实现了严格的隐私保证。实验结果表明，该方法在性能上表现出色。

    

    随着图神经网络（GNN）在医疗保健和医学等敏感应用中的日益流行，人们对训练的GNN的隐私方面提出了担忧。值得注意的是，即使只授予对训练模型的黑盒访问权限，GNN也容易受到隐私攻击，例如成员推断攻击。我们提出了PrivGNN，这是一个在集中式环境中发布GNN模型的隐私保护框架。假设可以访问公共未标记的图，PrivGNN提供了一个框架，以隐私保护的方式发布明确在公共数据上训练并获取来自私有数据的知识的GNN模型。PrivGNN将知识蒸馏框架与两个噪声机制（随机子采样和嘈杂标记）相结合，以确保严格的隐私保证。我们在Renyi差分隐私框架中对我们的方法进行了理论分析。此外，我们展示了我们的方法与多个基准方法相比的实验性能。

    With the increasing popularity of graph neural networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only black-box access to the trained model is granted. We propose PrivGNN, a privacy-preserving framework for releasing GNN models in a centralized setting. Assuming an access to a public unlabeled graph, PrivGNN provides a framework to release GNN models trained explicitly on public data along with knowledge obtained from the private data in a privacy preserving manner. PrivGNN combines the knowledge-distillation framework with the two noise mechanisms, random subsampling, and noisy labeling, to ensure rigorous privacy guarantees. We theoretically analyze our approach in the Renyi differential privacy framework. Besides, we show the solid experimental performance of our method compared to severa
    
[^210]: 结构变量选择的独占群组套索

    Exclusive Group Lasso for Structured Variable Selection. (arXiv:2108.10284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.10284](http://arxiv.org/abs/2108.10284)

    本文提出了一种基于原子范数的独占群组套索方法，用于解决结构变量选择问题。该方法通过适当设计的复合范数促进独占群组稀疏模式，并使用高效灵活的优化算法进行支持恢复。通过逐步将结构原子包含到估计的支持中构建解，并在一定假设下证明了解决方案的有效性。

    

    本文考虑了一种结构变量选择问题，在这个问题中，协变量被预定义的群组划分，并且根据每个群组中的稀疏模式激活，每个群组仅有少数非零条目。利用原子范数的概念，可以设计出合适的复合范数以促进这种独占群组稀疏模式。由此产生的范数适用于高效和灵活的支持恢复正则化优化算法，如近端算法。此外，提出了一种主动集算法，通过逐步将结构原子包含到估计的支持中来构建解。还表明，这种算法可以针对比纯粹的独占群组稀疏性更严格的结构进行定制。渐近一致性分析（参数数量和群组数量随观察大小增长）在传统假设下证明了所提出解决方案在有符号支持恢复方面的有效性。最后，提出了一种改进的优化算法。

    A structured variable selection problem is considered in which the covariates, divided into predefined groups, activate according to sparse patterns with few nonzero entries per group. Capitalizing on the concept of atomic norm, a composite norm can be properly designed to promote such exclusive group sparsity patterns. The resulting norm lends itself to efficient and flexible regularized optimization algorithms for support recovery, like the proximal algorithm. Moreover, an active set algorithm is proposed that builds the solution by successively including structure atoms into the estimated support. It is also shown that such an algorithm can be tailored to match more rigid structures than plain exclusive group sparsity. Asymptotic consistency analysis (with both the number of parameters as well as the number of groups growing with the observation size) establishes the effectiveness of the proposed solution in terms of signed support recovery under conventional assumptions. Finally, a
    
[^211]: 学习线性二次均场博弈的探索噪声

    Exploration noise for learning linear-quadratic mean field games. (arXiv:2107.00839v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2107.00839](http://arxiv.org/abs/2107.00839)

    本文证明了常见噪声可以作为学习均场博弈解的探索噪声，并且通过实例表明这种噪声能够恢复解的存在性和唯一性，并且强制学习算法收敛，无需额外的结构。

    

    本文的目标是证明常见噪声可以作为学习均场博弈解的探索噪声。通过一个玩具线性二次模型的示例，我们证明了一种适当形式的常见噪声已被证明可以恢复存在性和唯一性。我们进一步证明了相同形式的常见噪声可以强制学习算法“虚构博弈”收敛，而无需进一步的潜在或单调结构。我们提供了一些数值实例来支持我们的理论分析。

    The goal of this paper is to demonstrate that common noise may serve as an exploration noise for learning the solution of a mean field game. This concept is here exemplified through a toy linear-quadratic model, for which a suitable form of common noise has already been proven to restore existence and uniqueness. We here go one step further and prove that the same form of common noise may force the convergence of the learning algorithm called `fictitious play', and this without any further potential or monotone structure. Several numerical examples are provided in order to support our theoretical analysis.
    
[^212]: 基于熵的时间序列摘要因果图的发现

    Entropy-based Discovery of Summary Causal Graphs in Time Series. (arXiv:2105.10381v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2105.10381](http://arxiv.org/abs/2105.10381)

    该研究提出了一种基于熵的方法，在时间序列中学习摘要因果图，并通过PC-like和FCI-like算法展示了其有效性和高效性。

    

    本研究解决了在可能具有不同采样频率的时间序列上学习摘要因果图的问题。为此，我们首先提出了一种用于时间序列的新因果时态互信息度量。然后，我们展示了该度量与熵减原理的关系，可以看作是概率提升原理的特殊情况。最后，我们将这两个要素结合在类似于PC和FCI的算法中，构建了摘要因果图。这些算法在多个数据集上进行了评估，显示出它们的有效性和高效性。

    This study addresses the problem of learning a summary causal graph on time series with potentially different sampling rates. To do so, we first propose a new causal temporal mutual information measure for time series. We then show how this measure relates to an entropy reduction principle that can be seen as a special case of the probability raising principle. We finally combine these two ingredients in PC-like and FCI-like algorithms to construct the summary causal graph. There algorithm are evaluated on several datasets, which shows both their efficacy and efficiency.
    
[^213]: 基于深度学习的适用于JPEG压缩图像的边缘感知预处理和后处理方法

    Deep learning-based Edge-aware pre and post-processing methods for JPEG compressed images. (arXiv:2104.04926v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2104.04926](http://arxiv.org/abs/2104.04926)

    本论文提出了一种基于深度学习的适用于JPEG压缩图像的边缘感知预处理和后处理方法，通过引入边缘感知损失函数和超分辨率CNN，在低比特率下实现了显著的PSNR提升。

    

    我们提出了一种基于学习的压缩方案，在预处理和后处理的深度卷积神经网络(CNN)之间包含了一个标准编解码器。具体而言，我们通过引入边缘感知损失函数来改进先前方法，以防止模糊，并引入了一个用于后处理的超分辨率CNN以及相应的改进了的预处理网络，以在低比特率下提供更好的速率失真性能。该算法在多种数据集上进行了评估，包括低分辨率至高分辨率的数据集，如Set 5、Set 7、Classic 5、Set 14、Live 1、Kodak、General 100、CLIC 2019。与JPEG、JPEG2000、BPG和最近的CNN方法相比，所提出的算法在PSNR方面取得了显著的提升，低比特率下的改进分别达到了20.75％、8.47％、3.22％、3.23％，高比特率下的改进分别达到了24.59％、14.46％、10.14％、8.57％。

    We propose a learning-based compression scheme that envelopes a standard codec between pre and post-processing deep CNNs. Specifically, we demonstrate improvements over prior approaches utilizing a compression-decompression network by introducing: (a) an edge-aware loss function to prevent blurring that is commonly occurred in prior works & (b) a super-resolution convolutional neural network (CNN) for post-processing along with a corresponding pre-processing network for improved rate-distortion performance in the low rate regime. The algorithm is assessed on a variety of datasets varying from low to high resolution namely Set 5, Set 7, Classic 5, Set 14, Live 1, Kodak, General 100, CLIC 2019. When compared to JPEG, JPEG2000, BPG, and recent CNN approach, the proposed algorithm contributes significant improvement in PSNR with an approximate gain of 20.75%, 8.47%, 3.22%, 3.23% and 24.59%, 14.46%, 10.14%, 8.57% at low and high bit-rates respectively. Similarly, this improvement in MS-SSIM
    
[^214]: 无模型的基于奖赏梯度的策略学习

    Model-free Policy Learning with Reward Gradients. (arXiv:2103.05147v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.05147](http://arxiv.org/abs/2103.05147)

    本研究开发了一种新颖的方法，无需学习模型即可集成奖赏梯度，提高了策略学习的样本效率。

    

    尽管策略梯度方法越来越受欢迎，但在样本稀缺的应用中，如机器人学，它们仍未被广泛利用。通过充分利用可用信息，可以提高样本效率。作为增强学习中的重要组成部分，奖赏函数通常被精心设计以引导智能体。因此，奖赏函数通常是已知的，可以访问标量奖赏信号和奖赏梯度。为了从奖赏梯度中获益，之前的工作需要了解环境动态，这是很难获得的。在这项工作中，我们开发了一种新颖的方法——奖赏策略梯度估计器，它可以集成奖赏梯度而无需学习模型。绕过模型动态使我们的估计器能够在偏差-方差权衡方面取得更好的效果，这导致更高的样本效率，如经验分析所示。我们的方法还提升了不同环境下的近端策略优化的性能。

    Despite the increasing popularity of policy gradient methods, they are yet to be widely utilized in sample-scarce applications, such as robotics. The sample efficiency could be improved by making best usage of available information. As a key component in reinforcement learning, the reward function is usually devised carefully to guide the agent. Hence, the reward function is usually known, allowing access to not only scalar reward signals but also reward gradients. To benefit from reward gradients, previous works require the knowledge of environment dynamics, which are hard to obtain. In this work, we develop the \textit{Reward Policy Gradient} estimator, a novel approach that integrates reward gradients without learning a model. Bypassing the model dynamics allows our estimator to achieve a better bias-variance trade-off, which results in a higher sample efficiency, as shown in the empirical analysis. Our method also boosts the performance of Proximal Policy Optimization on different 
    
[^215]: 在重症监护室中的病人病情可计算的表型

    Computable Phenotypes of Patient Acuity in the Intensive Care Unit. (arXiv:2005.05163v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2005.05163](http://arxiv.org/abs/2005.05163)

    本研究开发了一种利用电子健康记录自动获取病情变量的计算表型，并描述了重症监护室病人的病情转换。通过连续的病情状态和聚类方法，提供了对ICU病人临床进展的展示。

    

    连续监测和病人病情评估是重症监护室实践的关键，但都受到医护人员时间限制的限制。此外，预测临床进展仍然不精确。本研究的目标是（1）利用电子健康记录中的自动变量获取开发一个病情表型和（2）描述重症监护室病人的病情转换，以展示临床进展。我们收集了51,372名入住佛罗里达大学健康医院（UFH）盖恩斯维尔（GNV）和杰克逊维尔（JAX）重症监护室的成年病人的两个单中心、纵向的电子健康记录数据集。我们开发了算法，以每四小时为间隔计算每次重症监护室入院的病情状况，并利用连续的病情状况和k-means聚类方法识别病情表型。UFH GNV数据集中有38,749名病人的51,073次入院，JAX数据集中有12,623名病人的22,219次入院。

    Continuous monitoring and patient acuity assessments are key aspects of Intensive Care Unit (ICU) practice, but both are limited by time constraints imposed on healthcare providers. Moreover, anticipating clinical trajectories remains imprecise. The objectives of this study are to (1) develop an electronic phenotype of acuity using automated variable retrieval within the electronic health records and (2) describe transitions between acuity states that illustrate the clinical trajectories of ICU patients. We gathered two single-center, longitudinal electronic health record datasets for 51,372 adult ICU patients admitted to the University of Florida Health (UFH) Gainesville (GNV) and Jacksonville (JAX). We developed algorithms to quantify acuity status at four-hour intervals for each ICU admission and identify acuity phenotypes using continuous acuity status and k-means clustering approach. 51,073 admissions for 38,749 patients in the UFH GNV dataset and 22,219 admissions for 12,623 pati
    
[^216]: LocoGAN -- 本地卷积生成对抗网络

    LocoGAN -- Locally Convolutional GAN. (arXiv:2002.07897v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2002.07897](http://arxiv.org/abs/2002.07897)

    LocoGAN是一个完全卷积的生成对抗网络模型，使用了局部学习和位置通道技术，能够生成任意维度的图像，包括周期性或无限长的图像。

    

    本文中，我们构建了一个完全卷积的生成对抗网络模型：LocoGAN，其中潜空间由可能具有不同分辨率的噪声图像表示。学习过程是局部的，即我们处理的不是整个噪声图像，而是固定尺寸的子图像。因此，LocoGAN能够生成任意尺寸的图像，比如LSUN卧室数据集。我们的方法的另一个优点是我们使用了位置通道，这允许生成完全周期性的（例如圆柱形全景图像）或几乎周期性的“无限长”图像（例如壁纸）。

    In the paper we construct a fully convolutional GAN model: LocoGAN, which latent space is given by noise-like images of possibly different resolutions. The learning is local, i.e. we process not the whole noise-like image, but the sub-images of a fixed size. As a consequence LocoGAN can produce images of arbitrary dimensions e.g. LSUN bedroom data set. Another advantage of our approach comes from the fact that we use the position channels, which allows the generation of fully periodic (e.g. cylindrical panoramic images) or almost periodic ,,infinitely long" images (e.g. wall-papers).
    

