# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations.](http://arxiv.org/abs/2309.15848) | SHACIRA提出了一种简单但有效的通用框架，用于对神经表示中的特征网格进行高水平压缩，通过量化潜在权重和应用熵正则化来实现压缩。这种方法在多样化数据集上取得了定量和定性上的好结果。 |
| [^2] | [Exploiting the Signal-Leak Bias in Diffusion Models.](http://arxiv.org/abs/2309.15842) | 本文展示了如何利用现有扩散模型中的信号泄漏偏差，以实现对生成图像的更好控制，并生成更多样化的亮度以及更满足特定风格和颜色要求的图像。 |
| [^3] | [How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions.](http://arxiv.org/abs/2309.15840) | 本文提出了一个简单但高精确度的谎言检测器，通过在怀疑有谎言的情况下问一组无关的后续问题，并将LLM的是/否答案输入到一个逻辑回归分类器中，该检测器能够推广到不同的LLM架构和实际场景中的谎言情况。 |
| [^4] | [Automated Detection of Persistent Inflammatory Biomarkers in Post-COVID-19 Patients Using Machine Learning Techniques.](http://arxiv.org/abs/2309.15838) | 这项研究利用机器学习技术自动检测COVID-19后患者中的持续性炎症生物标志物，在伊拉克医院收集的医疗数据上进行研究，并构建了预测模型。 |
| [^5] | [Multi-unit soft sensing permits few-shot learning.](http://arxiv.org/abs/2309.15828) | 多单元软测量是利用可转移性学习算法改进软测量的一种方法，能够通过解决多个任务来增强软测量的性能，并且特别适用于具有多个实现的进程。 |
| [^6] | [Identifying the Risks of LM Agents with an LM-Emulated Sandbox.](http://arxiv.org/abs/2309.15817) | 通过使用LM模拟工具执行和开发基于LM的自动安全评估器，该论文提出了一种解决测试LM代理的高成本和寻找高风险问题的方法。 |
| [^7] | [Fair Canonical Correlation Analysis.](http://arxiv.org/abs/2309.15809) | 本文研究了典范相关分析中的公平性和偏见问题，并提出了一种通过最小化相关性差异误差来减轻不公平现象的方法。该方法在保持CCA准确性的同时，减少了相关性差异误差。 |
| [^8] | [ANNCRIPS: Artificial Neural Networks for Cancer Research In Prediction & Survival.](http://arxiv.org/abs/2309.15803) | 本研究通过开发和验证一种智能数学模型，利用人工神经网络提高前列腺癌的早期检测，降低假阳性率，改善患者预后。预计该模型经过进一步改进和验证后，可以成为一个可靠的、市场化的前列腺癌检测解决方案。 |
| [^9] | [Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction.](http://arxiv.org/abs/2309.15798) | Node-Aligned Graph-to-Graph (NAG2G)是一个基于transformer的无模板模型，利用2D分子图和3D构象信息，能够更好地利用分子的拓扑信息和对齐原子，提高单步反合成预测的竞争力。 |
| [^10] | [Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition.](http://arxiv.org/abs/2309.15796) | 本文提出了一种弱监督自动语音识别方法，使用全时分类准则训练模型，可以有效学习语音-文本对齐，并适应训练转录中的错误，避免性能下降。 |
| [^11] | [Targeting Relative Risk Heterogeneity with Causal Forests.](http://arxiv.org/abs/2309.15793) | 本研究提出了一种通过修改因果森林方法，以相对风险为目标，从而捕捉到治疗效应异质性的潜在来源。 |
| [^12] | [Large Language Model Routing with Benchmark Datasets.](http://arxiv.org/abs/2309.15789) | 本论文解决了从一系列模型中为新任务选择最佳大型语言模型的挑战，通过提出了一个基于基准数据集的学习模型来选择模型，并在各种任务中提高了性能。 |
| [^13] | [Partial Transport for Point-Cloud Registration.](http://arxiv.org/abs/2309.15787) | 本论文提出了一种基于最优部分输运问题的点云配准方法，该方法可以应对非刚性动作和部分可见性等实际场景中的复杂性。该方法将点云视为经验度量，并提供了一种数学严格的方法来量化源点和目标点之间的`对应关系'。 |
| [^14] | [Learning the Efficient Frontier.](http://arxiv.org/abs/2309.15775) | 本文引入了NeuralEF，一个快速的神经逼近框架，能够鲁棒地预测高效前沿问题的解，同时处理异构线性约束和可变数量的优化输入。 |
| [^15] | [Importance-Weighted Offline Learning Done Right.](http://arxiv.org/abs/2309.15771) | 本文研究了随机上下文赌博问题中的离线策略优化问题，并提出了一种替代方法，通过使用“隐式探索”估计器来计算策略价值的权重重要估计。与之前的结果相比，在几乎所有情况下都具有更好的性能保证，同时消除了之前所做的非常苛刻的“均匀覆盖”假设。 |
| [^16] | [Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator.](http://arxiv.org/abs/2309.15769) | 本文研究了普通最小二乘插值器在高维环境中的代数和统计属性，并为最小l2范数OLS插值器提供了基本结果。这些结果对理解OLS插值器的泛化能力具有重要意义。 |
| [^17] | [Rapid Network Adaptation: Learning to Adapt Neural Networks Using Test-Time Feedback.](http://arxiv.org/abs/2309.15762) | 本文提出了一种快速网络适应的方法，通过利用测试时的反馈信号来实时调整神经网络，相比基准方法更加灵活且快速数个数量级，通过广泛的实验评估，在各种数据集、任务和分布变化下取得了令人满意的结果。 |
| [^18] | [Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data.](http://arxiv.org/abs/2309.15757) | 本文提出了一种基于潜在图的半监督学习方法，通过利用图的表示来捕捉数据之间的关系，并实现了全局和局部知识的有效融合。在生物医学数据集上的评估中，我们的方法表现出了最先进的结果。 |
| [^19] | [Provably Efficient Exploration in Constrained Reinforcement Learning:Posterior Sampling Is All You Need.](http://arxiv.org/abs/2309.15737) | 该论文提出了一种基于后验抽样的算法，用于约束强化学习中的无限期无折扣马尔可夫决策过程。该算法在遗憾界限上接近最优，并在实证上表现出优势。 |
| [^20] | [Deep Learning-based Analysis of Basins of Attraction.](http://arxiv.org/abs/2309.15732) | 本研究展示了基于深度学习的卷积神经网络方法在表征各种动力系统的吸引盆的复杂性和不可预测性方面的有效性，相比传统方法，该方法具有更低的计算成本且表现更好。 |
| [^21] | [Temporal graph models fail to capture global temporal dynamics.](http://arxiv.org/abs/2309.15730) | 时间图模型无法捕捉全局时间动态，我们提出了一种"最近流行节点"的基线方法，在时间图基准的中等和大规模数据集上胜过其他方法。我们提出了两个基于Wasserstein距离的度量来量化全局动态。我们展示了标准的负采样评估方法在具有强烈时间动态的数据集上可能不适用，我们还展示了简单的负采样方法可能导致模型退化。我们提出了改进的负采样方案，并证明了它们的有效性。我们还将其与无负采样的非对比训练模型进行了比较。 |
| [^22] | [Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation.](http://arxiv.org/abs/2309.15726) | 我们开发了一种无监督神经网络架构，通过去噪扩散目标训练模型来实现同时生成和分割图像。这种架构通过在输入中划分区域并并行去噪以及合并结果，实现了准确的无监督图像分割和高质量的合成图像生成。 |
| [^23] | [Model Share AI: An Integrated Toolkit for Collaborative Machine Learning Model Development, Provenance Tracking, and Deployment in Python.](http://arxiv.org/abs/2309.15719) | Model Share AI是一个用于协作式机器学习模型开发、来源追踪和部署的集成工具包，提供了协作项目空间、标准化的模型评估流程和自动化的模型部署功能。 |
| [^24] | [Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription.](http://arxiv.org/abs/2309.15717) | Timbre-Trap是一个低资源框架，将音乐转录和音频重构统一起来，通过利用音高和音色的强分离性，同时估计音高显著度和重构频谱系数，取得优越性能。 |
| [^25] | [Maximum Weight Entropy.](http://arxiv.org/abs/2309.15704) | 本文提出了在深度学习中使用最大熵原理的最大权重熵方法，通过最大化权重多样性来解决标准方法在超出分布情况下预测多样性缺乏的问题。 |
| [^26] | [HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models.](http://arxiv.org/abs/2309.15701) | 本文引入了第一个开源基准测试，利用大型语言模型进行自动语音识别错误修正，实现了与人类水平相当的性能，具有重要的实际应用价值。 |
| [^27] | [Deep Model Fusion: A Survey.](http://arxiv.org/abs/2309.15698) | 深度模型融合是一种新兴技术，将多个深度学习模型的参数或预测合并到单个模型中，以实现更好的性能。在大规模深度学习模型上面临许多挑战，如高计算成本、高维参数空间、不同异构模型之间的干扰等。本论文为了更好地了解模型融合方法并推动其发展，提出了一项综合调查，总结了最近的进展。 |
| [^28] | [A Unified View of Differentially Private Deep Generative Modeling.](http://arxiv.org/abs/2309.15696) | 本文提出了差分隐私深度生成建模的统一视角，系统化了方法，为满足差分隐私需求的方法提供了一个联合设计空间。 |
| [^29] | [Breaking NoC Anonymity using Flow Correlation Attack.](http://arxiv.org/abs/2309.15687) | 本文研究了NoC架构中现有匿名路由协议的安全性，并展示了现有的匿名路由对基于机器学习的流相关攻击易受攻击。我们提出了一种轻量级的匿名路由，使用流量混淆技术，可以抵御基于机器学习的流相关攻击。 |
| [^30] | [Joint Sampling and Optimisation for Inverse Rendering.](http://arxiv.org/abs/2309.15676) | 该论文提出了逆渲染的联合采样与优化方法，通过交替采样和优化以减小方差，使用有限差分估计器更新和重复使用过去的样本，在与Adam相结合的情况下实现了稳定的优化过程，加快了困难优化问题的收敛速度。 |
| [^31] | [Speech collage: code-switched audio generation by collaging monolingual corpora.](http://arxiv.org/abs/2309.15674) | 本文提出了一种通过拼接单语语料生成混合语音的方法，可以解决混合语数据稀缺的问题。实证结果表明，生成的混合语音数据可以显著提高语音识别的准确率，并减少模型对单语的偏好。 |
| [^32] | [MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection.](http://arxiv.org/abs/2309.15670) | 这个研究构建了一个基于孟加拉语的注释语料库，用于多标签情感检测。通过使用基于上下文的方法以及BERT模型，填补了这一学科领域的空白。 |
| [^33] | [On Computational Entanglement and Its Interpretation in Adversarial Machine Learning.](http://arxiv.org/abs/2309.15669) | 本研究探索了对抗机器学习模型的复杂性和可解释性，通过将其与爱因斯坦的特殊相对论中的纠缠概念联系起来，发现远程特征样本可以表现出纠缠现象，挑战了对抗可传递性现象的传统描述方法。 |
| [^34] | [Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency.](http://arxiv.org/abs/2309.15659) | FeDEQ是一个联邦学习框架，采用深度平衡学习和共识优化，通过紧凑的共享数据表示在边缘节点之间共享模型，解决了深度联邦学习在边缘环境中的通信瓶颈和数据异构性问题。 |
| [^35] | [Generative Speech Recognition Error Correction with Large Language Models.](http://arxiv.org/abs/2309.15649) | 本研究探讨了大型语言模型（LLMs）作为ASR后处理器的能力，通过重新评分和错误校正来提高系统性能。通过使用指令提示和任务激活提示方法，结合上下文学习和微调技术，我们展示了LLMs的泛化能力和有效性。 |
| [^36] | [SANGEA: Scalable and Attributed Network Generation.](http://arxiv.org/abs/2309.15648) | 本文提出了SANGEA，一种可扩展的合成图生成框架，通过将大图分解为社区，每个社区训练一个合成图生成器，然后将社区图链接在一起生成一个类似原始图的大型合成图。 |
| [^37] | [Cold & Warm Net: Addressing Cold-Start Users in Recommender Systems.](http://arxiv.org/abs/2309.15646) | 本文提出了一种名为冷启动和热启动网络的方法(Cold & Warm Net)，用于解决推荐系统中的冷启动用户问题。该方法利用专家模型分别建模冷启动和热启动用户，并引入门控网络和动态知识蒸馏来提高用户表示的学习效果。通过选择与用户行为高度相关的特征，并建立偏差网络来显式建模用户行为偏差。实验证实了该方法的有效性。 |
| [^38] | [Why do Angular Margin Losses work well for Semi-Supervised Anomalous Sound Detection?.](http://arxiv.org/abs/2309.15643) | 角边距损失与辅助任务结合在半监督异常声音检测中表现出色，通过最小化角边距损失同时达到最小化紧凑性损失和防止学习平凡解的效果。 |
| [^39] | [Efficient tensor network simulation of IBM's largest quantum processors.](http://arxiv.org/abs/2309.15642) | 本文展示了如何使用量子启发的二维张量网络高效模拟IBM最大的量子处理器，通过简单的张量更新实现前所未有的准确度和极低的计算资源消耗，并为最新的IBM量子机器设立了基准。 |
| [^40] | [Hedging Properties of Algorithmic Investment Strategies using Long Short-Term Memory and Time Series models for Equity Indices.](http://arxiv.org/abs/2309.15640) | 本文提出了一种使用长短期记忆和时间序列模型构建算法投资策略的对冲方法，并通过利用不同类型的投资策略来对冲风险资产组合。实证结果显示，该方法在金融市场的动荡时期具有多样化的潜力。 |
| [^41] | [Enhancing Sharpness-Aware Optimization Through Variance Suppression.](http://arxiv.org/abs/2309.15639) | 本文通过方差抑制的方法（VaSSO）增强了锐度感知最小化（SAM）的优化算法，提高了深度神经网络的泛化能力，特别适用于模型无关任务和对高水平标签噪声具有鲁棒性的情况。 |
| [^42] | [FRS-Nets: Fourier Parameterized Rotation and Scale Equivariant Networks for Retinal Vessel Segmentation.](http://arxiv.org/abs/2309.15638) | 本研究提出了一种名为FRS-Nets的新型卷积神经网络方法，利用傅里叶参数化实现了对旋转和尺度的等变性，从而提升了视网膜血管分割的准确性。通过在U-Net和Iter-Net中替换传统卷积滤波器，实现了更好的分割效果。 |
| [^43] | [Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution.](http://arxiv.org/abs/2309.15609) | 本文提出了一种端到端的解决方案，用于创建全自动的会议记录和多语言翻译，解决了会议管理文档中现有工作流程的替代和改善问题。 |
| [^44] | [NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit sensitivity maps.](http://arxiv.org/abs/2309.15608) | 本文提出了一种无需显式灵敏度图的展开心脏MRI重建方法，使用深度卷积神经网络和算法展开，通过学习图像之间的接收线圈关系来实现加速心脏MRI重建，在实验中取得了较好的性能。 |
| [^45] | [Entropic Matching for Expectation Propagation of Markov Jump Processes.](http://arxiv.org/abs/2309.15604) | 本文提出了一个基于熵匹配框架的新的可处理的推断方案，可以嵌入到期望传播算法中，对于描述离散状态空间过程的Markov跳跃过程的统计推断问题具有重要意义。我们展示了我们方法的有效性，并通过提供一类近似分布的闭式结果以及应用于化学反应网络的一般类别来加以论证。此外，我们通过一个近似的期望最大化程序导出了潜在参数的点估计的闭式表达式，并在各种化学反应网络示例中评估了我们的方法的性能。我们还讨论了该方法的局限性和未来的潜力。 |
| [^46] | [Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport Regularization.](http://arxiv.org/abs/2309.15603) | 本论文提出了一种在多任务强化学习中使用最优输运正则化来提取知识的方法，通过近似计算任务状态分布之间的最优输运距离，并将其作为奖励来规范信息分享的量。实验证明这种方法能够显著加速代理的学习过程并超越传统方法。 |
| [^47] | [OceanBench: The Sea Surface Height Edition.](http://arxiv.org/abs/2309.15599) | 通过机器学习技术利用海洋卫星数据的信息，可以帮助我们更好地理解海洋表面高度变化，从而对人类活动和气候调节产生重要影响。 |
| [^48] | [Exciton-Polariton Condensates: A Fourier Neural Operator Approach.](http://arxiv.org/abs/2309.15593) | 本研究首次将傅里叶神经算子方法应用于激子极化子库伦凝聚系统，通过机器学习实现对Gross-Pitaevskii方程的求解，可以以接近1000倍的速度准确预测最终状态的解，为激子极化子库伦凝聚系统的大规模应用提供了新的解决方案。 |
| [^49] | [Jointly Training Large Autoregressive Multimodal Models.](http://arxiv.org/abs/2309.15564) | 本研究提出了共同训练大型自回归多模态模型的方法，通过模块化的方式融合语言和图像生成模型，同时引入了数据高效的指令调优策略，使得该模型在生成高质量多模态输出方面表现出卓越的性能。 |
| [^50] | [Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank.](http://arxiv.org/abs/2309.15560) | 研究揭示在无偏学习排名中，当点击数据不能完全拟合时，无法恢复真实相关性，导致排名性能显著降低，提出了可识别性图模型作为解决方案。 |
| [^51] | [Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attribution.](http://arxiv.org/abs/2309.15559) | 本文提出了一种Shapley增加的自我归因神经网络(SASANet)来实现忠实的神经网络内部解释。通过引入Shapley值，SASANet能够确保自我归因值与输出的Shapley值相等，从而提供了一个具有理论保证的解释框架。实验证明，SASANet在性能上超过了现有的自我归因模型，并与黑盒模型相媲美。此外，SASANet在交互式解释和效率方面也表现出更高的精确度和高效性。 |
| [^52] | [Startup success prediction and VC portfolio simulation using CrunchBase data.](http://arxiv.org/abs/2309.15552) | 本研究提出了一个使用CrunchBase数据来预测创业成功和模拟VC投资组合的新颖深度学习模型，并通过全面回溯算法对模型在历史数据上的表现进行了评估。 |
| [^53] | [Identifying confounders in deep-learning-based model predictions using DeepRepViz.](http://arxiv.org/abs/2309.15551) | 这项研究提出了DeepRepViz框架，用于帮助研究人员在深度学习模型预测中识别混淆因素，并通过度量和可视化工具来解决这个问题。实验证明使用DeepRepViz与DL模型结合能够带来明显的益处。 |
| [^54] | [From LAION-5B to LAION-EO: Filtering Billions of Images Using Anchor Datasets for Satellite Image Extraction.](http://arxiv.org/abs/2309.15535) | 本文提出了一种基于锚定数据集和进一步过滤的提取方法，用于从大型图像库中提取卫星图像。这导致了发布了一个高分辨率的文本和卫星图像对应的数据集LAION-EO。 |
| [^55] | [Uncertainty Quantification via Neural Posterior Principal Components.](http://arxiv.org/abs/2309.15533) | 本论文提出了一种使用神经网络在单次前向传递中预测任意输入图像后验分布的主成分的方法，以实现不确定性量化。 |
| [^56] | [Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models.](http://arxiv.org/abs/2309.15531) | 该论文提出了一种重新思考频道维度的方法，以隔离大型语言模型低位权重量化中的异常值。通过将权重按输入通道内进行量化分组，可以解决激活异常值的问题，并成功地使得低于4位的量化成为可能。 |
| [^57] | [Missing-modality Enabled Multi-modal Fusion Architecture for Medical Data.](http://arxiv.org/abs/2309.15529) | 这项研究提出了一种对医疗数据具有鲁棒性且能进一步提高疾病诊断性能的多模态融合架构。通过融合X射线照片、放射学报告和结构化数据，使用Transformer模块进行融合，同时引入了多变量损失函数提高模型在缺失模态情况下的鲁棒性。 |
| [^58] | [Robust Internal Representations for Domain Generalization.](http://arxiv.org/abs/2309.15522) | 本文综合调查了作者利用嵌入空间进行转移学习研究的成果，主要讨论了持续学习和有限标记数据可用性所带来的挑战，为未来领域的进展铺平道路。 |
| [^59] | [MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis.](http://arxiv.org/abs/2309.15521) | 本论文研究在稀缺数据分析中完全应用MLOps的情况，并提出了一种新的整体方法来增强生物医学图像分析。 |
| [^60] | [SAF-Net: Self-Attention Fusion Network for Myocardial Infarction Detection using Multi-View Echocardiography.](http://arxiv.org/abs/2309.15520) | 本文提出了一种自注意力融合网络（SAF-Net）来检测多视角超声心动图中的心肌梗死。该模型通过提取特征、学习特征的依赖关系以及使用密集层进行分类，实现了对MI的有效检测。 |
| [^61] | [GNN4EEG: A Benchmark and Toolkit for Electroencephalography Classification with Graph Neural Network.](http://arxiv.org/abs/2309.15515) | GNN4EEG是一个用于脑电图分类的图神经网络的基准和工具包，通过建模脑电图通道选定的特征并利用拓扑信息，可以提供更好的分类结果。 |
| [^62] | [Finite Scalar Quantization: VQ-VAE Made Simple.](http://arxiv.org/abs/2309.15505) | 该论文提出了有限标量量化 (FSQ) 方法，用来简化 VQ-VAE 方法中的向量量化 (VQ)。通过投影和量化 VAE 表示，我们得到与 VQ 相同大小的码本。在这种离散表示上，我们可以训练相同的模型，并在图像生成、多模态生成和计算机视觉任务中取得竞争性能。 |
| [^63] | [Bayesian Personalized Federated Learning with Shared and Personalized Uncertainty Representations.](http://arxiv.org/abs/2309.15499) | 本文提出了一种贝叶斯个性化联邦学习（BPFL）的框架，用于处理联邦学习系统中的客户端不确定性和异质性。这个框架通过分解和共同学习统计异质性客户端数据上的共享和个性化不确定表示。 |
| [^64] | [Fast Locality Sensitive Hashing with Theoretical Guarantee.](http://arxiv.org/abs/2309.15479) | 本文提出了一种名为FastLSH的简单而高效的局部敏感哈希方案，通过结合随机采样和随机投影，将算法的时间复杂度大幅降低，并且具有可证明的局部敏感哈希属性。 |
| [^65] | [The Robust Semantic Segmentation UNCV2023 Challenge Results.](http://arxiv.org/abs/2309.15478) | 这个论文总结了MUAD不确定性量化挑战的获胜解决方案，并展示了19个提交作品的结果。挑战主要集中在城市环境的语义分割，特别关注自然对抗场景。这个论文提供了表现最好的解决方案，并全面概述了所有参与者部署的多种解决方案。 |
| [^66] | [Enabling Resource-efficient AIoT System with Cross-level Optimization: A survey.](http://arxiv.org/abs/2309.15467) | 该论文调查了资源高效的AIoT系统的跨级别优化，提出了一种算法-系统共同设计的方法，通过优化DL模型和系统调度，改善了运行时资源可用性，推动AIoT性能的进一步提升。 |
| [^67] | [Graph Neural Prompting with Large Language Models.](http://arxiv.org/abs/2309.15427) | 本文提出了一种名为图神经提示（GNP）的方法，可以帮助大型语言模型从知识图中学习有益的知识，以弥补它们在准确捕捉和返回基于知识的信息方面的固有限制。 |
| [^68] | [NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions.](http://arxiv.org/abs/2309.15426) | NeuRBF是一种使用自适应径向基函数进行信号表示的神经场方法，具有更高的空间适应性和能够更好地拟合目标信号的能力。通过与多频正弦函数组合，可以进一步提高径向基函数的通道容量和表示细节的能力。 |
| [^69] | [Deep Learning in Deterministic Computational Mechanics.](http://arxiv.org/abs/2309.15421) | 深度学习在确定性计算力学领域的五个主要类别中发挥重要作用，包括模拟替代、模拟增强、神经网络离散化、生成方法和深度强化学习。 |
| [^70] | [The Triad of Failure Modes and a Possible Way Out.](http://arxiv.org/abs/2309.15420) | 本文提出了一个新颖的目标函数，用于解决聚类自监督学习中的表示崩溃、聚类崩溃和对聚类分配的置换不变性的问题。该目标函数具有简单性和理论基础，适用于标准的主干结构训练，无需使用非对称元素。 |
| [^71] | [Automatic Feature Fairness in Recommendation via Adversaries.](http://arxiv.org/abs/2309.15418) | 通过对手训练实现推荐系统中的特征公平性，提高整体准确性和泛化能力 |
| [^72] | [Revolutionizing Terrain-Precipitation Understanding through AI-driven Knowledge Discovery.](http://arxiv.org/abs/2309.15400) | 通过AI驱动的知识发现技术，首次揭示了地形特征与降水模式之间的复杂关系，并在1995年左右揭示了一个显著的地形与降水关系转变现象。 |
| [^73] | [Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs.](http://arxiv.org/abs/2309.15395) | 本文提出了一种无模型的算法，名为PRI，用于在线CMDPs中的最佳策略识别问题。该算法基于CMDPs的有限随机性属性，能够以低遗憾并以高概率识别出最优策略。 |
| [^74] | [Neural Stochastic Differential Equations for Robust and Explainable Analysis of Electromagnetic Unintended Radiated Emissions.](http://arxiv.org/abs/2309.15386) | 本研究对ResNet-like模型在非意图辐射发射（URE）分类中的鲁棒性和可解释性进行了全面评估，并通过提出神经随机微分方程（SDEs）的应用，解决了模型对噪声的脆弱性以及解释不准确的问题。 |
| [^75] | [ADGym: Design Choices for Deep Anomaly Detection.](http://arxiv.org/abs/2309.15376) | ADGym是一款针对深度异常检测的设计选择的综合评估和自动选择平台。 |
| [^76] | [PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling.](http://arxiv.org/abs/2309.15375) | 通过基于注意力的深度状态空间建模，我们提出了一种不受个体限制的方法，将PPG信号转换为ECG，用于连续性心房颤动检测。 |
| [^77] | [Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences.](http://arxiv.org/abs/2309.15366) | 通过测度传递方法进行密度估计在生物科学中具有广阔的应用前景，尤其是在处理稀疏数据的情况下，使用稀疏传递映射可以揭示数据中隐藏的信息。 |
| [^78] | [C3Net: interatomic potential neural network for prediction of physicochemical properties in heterogenous systems.](http://arxiv.org/abs/2309.15334) | C3Net是一种面向异质系统中物理化学性质预测的神经网络，能嵌入原子类型在分子环境中并遵循基本物理定律的原子间势。该模型在预测物理化学性质上具有良好的泛化能力，并优于基于量子力学和神经网络的最新方法。 |
| [^79] | [Exploring Learned Representations of Neural Networks with Principal Component Analysis.](http://arxiv.org/abs/2309.15328) | 这项研究使用主成分分析探索了深度神经网络的特征表示，并发现在某些层中只需要20%的特征空间方差就能实现高准确度分类。该研究还提供了三个可解释的替代模型，并发现仿射线性模型表现最佳。 |
| [^80] | [Neural Operators for Accelerating Scientific Simulations and Design.](http://arxiv.org/abs/2309.15325) | 本论文介绍了一种称为神经运算符的人工智能框架，用于学习连续域函数之间的映射，可以加速科学模拟和设计中的计算需求，实现零射超分辨率以及替代现有的模拟器。 |
| [^81] | [On the Power of SVD in the Stochastic Block Model.](http://arxiv.org/abs/2309.15322) | 本文研究了奇异值分解算法在随机块模型中的能力，发现在对称设置下，该算法能够正确恢复所有的聚类。 |
| [^82] | [DeepROCK: Error-controlled interaction detection in deep neural networks.](http://arxiv.org/abs/2309.15319) | DeepROCK是一种在深度神经网络中进行误差可控的交互检测的方法，通过使用knockoffs和一种新颖的DNN架构，可以同时控制虚发现率(FDR)和最大化统计功效。 |
| [^83] | [MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees.](http://arxiv.org/abs/2309.15312) | MAPTree是一种通过贝叶斯方法对决策树进行归纳的算法，通过AND/OR搜索实现最大后验树的恢复。在实验中，MAPTree在多个数据集上表现出更好的性能，并且能够以更小的树来实现可比较的性能。在合成数据和实际场景中，MAPTree还展示出更强的抗噪声能力和更好的泛化能力。 |
| [^84] | [Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience.](http://arxiv.org/abs/2309.15302) | 提出一种名为STERLING的自我监督地形表示学习方法，通过无约束的机器人经验学习有关地形的相关表示，以实现地形感知导航。 |
| [^85] | [Beyond Log-Concavity: Theory and Algorithm for Sum-Log-Concave Optimization.](http://arxiv.org/abs/2309.15298) | 本文拓展了凸优化理论，提出了求解和函数对数凹函数最小化的算法，并应用于棋盘回归方法，扩展了逻辑回归到非线性可分问题。 |
| [^86] | [Multiple Physics-Informed Neural Network for Biomedical Tube Flows.](http://arxiv.org/abs/2309.15294) | 本研究探索了多情况PINN方法在计算生物医学管道流动中的应用，通过预训练和参数化不同几何情况，可以实时获取未见几何形状的结果，进而优化了传统CFD方法的使用。 |
| [^87] | [Maximum Diffusion Reinforcement Learning.](http://arxiv.org/abs/2309.15293) | 最大扩散强化学习是一种克服强化学习中数据相关性问题的方法，通过解耦代理的经验实现持续学习，并在各种测试中表现出色。 |
| [^88] | [Scaling Representation Learning from Ubiquitous ECG with State-Space Models.](http://arxiv.org/abs/2309.15292) | 本论文介绍了一种名为WildECG的预训练状态空间模型，用于从无处不在的心电图中进行表示学习。通过在大规模的实际野外数据集上进行训练，该模型能够解决传统监督学习方法在处理大量数据和不同上下文下的挑战。 |
| [^89] | [SEPT: Towards Efficient Scene Representation Learning for Motion Prediction.](http://arxiv.org/abs/2309.15289) | SEPT是一个利用自监督学习进行场景表示学习的建模框架，通过预训练的编码器捕捉轨迹的运动学特征、道路网络的空间结构以及道路和代理之间的交互作用，实现了在运动预测任务上的最先进性能。 |
| [^90] | [Composable Coresets for Determinant Maximization: Greedy is Almost Optimal.](http://arxiv.org/abs/2309.15286) | 这项工作展示了在行列式最大化问题中，贪心算法提供了几乎最优的可组合核心集合，具有近似因子$O(k)^{3k}$。 |
| [^91] | [A Physics Enhanced Residual Learning (PERL) Framework for Traffic State Prediction.](http://arxiv.org/abs/2309.15284) | 这篇论文提出了一种名为物理增强残差学习（PERL）的框架，用于交通状态预测。PERL模型集成了物理模型和数据驱动模型的优势，通过将物理模型结果和预测残差作为修正相结合，具有可解释性，且比数据驱动方法要求更少的数据。 |
| [^92] | [Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models.](http://arxiv.org/abs/2309.15278) | 本文研究了如何对先前观察到但当前被遮挡的对象进行推理和规划，提出了利用转换器关系动力学编码轨迹历史的方法，并在多个挑战性任务中表现出色。 |
| [^93] | [Efficient Low-rank Backpropagation for Vision Transformer Adaptation.](http://arxiv.org/abs/2309.15275) | 本论文提出了一种名为LBP-WHT的新方法，用于解决视觉变换器（ViT）在反向传播中对计算资源的需求过高的问题。LBP-WHT方法通过将梯度投影到低秩空间并进行反向传播，显著减少了适应ViT所需的计算量。实验结果表明，LBP-WHT在多个数据集上对不同模型的适应性能都表现出色。 |
| [^94] | [STARC: A General Framework For Quantifying Differences Between Reward Functions.](http://arxiv.org/abs/2309.15257) | 这篇论文提出了一个通用框架（STARC），用于评估奖励函数之间的差异，填补了奖励学习理论基础的空白。 |
| [^95] | [Method and Validation for Optimal Lineup Creation for Daily Fantasy Football Using Machine Learning and Linear Programming.](http://arxiv.org/abs/2309.15253) | 本文提出了一种方法来预测NFL球员的表现，并使用线性规划找到最佳阵容来最大化奇幻分数。实验结果表明，这种方法可以有效提高阵容的性能。 |
| [^96] | [V2X-Lead: LiDAR-based End-to-End Autonomous Driving with Vehicle-to-Everything Communication Integration.](http://arxiv.org/abs/2309.15252) | V2X-Lead是一种基于激光雷达的自动驾驶方法，通过集成车联网通信来解决城市复杂场景下的自动驾驶挑战，通过融合激光雷达和V2X通信数据，采用深度强化学习算法训练驾驶代理以实现更安全和高效的穿越交叉口，具有良好的泛化性能。 |
| [^97] | [SeMAnD: Self-Supervised Anomaly Detection in Multimodal Geospatial Datasets.](http://arxiv.org/abs/2309.15245) | 提出了一种名为SeMAnD的自监督异常检测技术，可用于检测多模态地理空间数据中的几何异常。该技术通过数据增强和自监督训练目标实现，能够有效地表示和识别不同模态数据中的局部变化和缺陷。 |
| [^98] | [Homotopy Relaxation Training Algorithms for Infinite-Width Two-Layer ReLU Neural Networks.](http://arxiv.org/abs/2309.15244) | 本文提出了一种名为同伦松弛训练算法（HRTA）的新的训练方法，它通过构建无缝连接线性激活函数和ReLU激活函数的同伦激活函数，并松弛同伦参数以增强训练精细化过程，加速了训练过程，在神经切线核（NTK）的背景下，实现了显著改进的收敛速度，并展示了对其他激活函数和深度神经网络的潜力。 |
| [^99] | [Learning Using Generated Privileged Information by Text-to-Image Diffusion Models.](http://arxiv.org/abs/2309.15238) | 本研究提出了一种利用生成的特权信息进行学习的框架，通过文本到图像扩散模型生成合成数据作为特权信息，进一步提升了学生模型在文本分类任务中的性能。 |
| [^100] | [Cross-Validation for Training and Testing Co-occurrence Network Inference Algorithms.](http://arxiv.org/abs/2309.15225) | 该论文介绍了一种用于训练和测试共现网络推理算法的交叉验证方法，通过研究微生物群落和其相互作用，提供对各种疾病的见解。 |
| [^101] | [Collaborative Watermarking for Adversarial Speech Synthesis.](http://arxiv.org/abs/2309.15224) | 本文提出了一种对抗性语音合成的协同水印技术，通过与现有对策模型合作进行训练，实现了对生成语音的有效检测和水印识别。 |
| [^102] | [Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition.](http://arxiv.org/abs/2309.15223) | 这篇论文介绍了一种基于低秩适应技术的神经语言建模系统，用于语音识别的输出重评分。通过使用低秩分解方法和优化插入矩阵，该系统能够以更高效的方式将BERT模型适应到新领域，大大减少了训练时间。 |
| [^103] | [Auto-grading C programming assignments with CodeBERT and Random Forest Regressor.](http://arxiv.org/abs/2309.15216) | 本研究提供了使用机器学习和深度学习方法对C编程作业进行自动评分的分析，并引入了基于代码的转换器词嵌入模型CodeBERT。测试结果表明该策略的有效性，并讨论了统计方法和深度学习技术之间的对比。 |
| [^104] | [Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling.](http://arxiv.org/abs/2309.15214) | 一种用于千米尺度大气降尺度的生成残差扩散建模方法被提出，并展示了在天气和气候的物理灾害预测方面具有潜力。 |
| [^105] | [Balancing Computational Efficiency and Forecast Error in Machine Learning-based Time-Series Forecasting: Insights from Live Experiments on Meteorological Nowcasting.](http://arxiv.org/abs/2309.15207) | 本研究通过实时实验，以气象预报为例，量化了机器学习的计算成本与预测误差之间的关系。研究结果有助于在时间序列预测中平衡计算效率和预测准确性。 |
| [^106] | [ICML 2023 Topological Deep Learning Challenge : Design and Results.](http://arxiv.org/abs/2309.15188) | 本文介绍了ICML 2023拓扑深度学习挑战，该挑战要求参与者在两个月内提供开源实现的拓扑神经网络，吸引了28个合格的提交。 |
| [^107] | [Monitoring Machine Learning Models: Online Detection of Relevant Deviations.](http://arxiv.org/abs/2309.15187) | 本论文提出了一种用于监测机器学习模型的在线检测方案，通过考虑模型质量的时间依赖性，可以减少不必要的警报并优化对相关变化的检测。 |
| [^108] | [Conservative World Models.](http://arxiv.org/abs/2309.15178) | 在零样本强化学习中，研究人员探索了在小样本数据集上训练时，前向-后向算法性能下降的问题，并使用保守性算法来缓解此问题。实验证明，保守的前向-后向算法在总体上表现更好，甚至超过了特定任务的基准算法。 |
| [^109] | [Revealing the Power of Spatial-Temporal Masked Autoencoders in Multivariate Time Series Forecasting.](http://arxiv.org/abs/2309.15169) | 提出了一个利用空间-时间掩蔽自动编码器（STMAE）来提高多元时间序列（MTS）预测性能的框架，通过新颖的双掩蔽策略处理部分可见的MTS数据，包括空间掩蔽和时间掩蔽。 |
| [^110] | [A Review on AI Algorithms for Energy Management in E-Mobility Services.](http://arxiv.org/abs/2309.15140) | 本文综述了AI算法在电动汽车能量管理中的应用，并探讨了其在解决各种挑战和实现能量管理的有效性方面的作用。 |
| [^111] | [PINF: Continuous Normalizing Flows for Physics-Constrained Deep Learning.](http://arxiv.org/abs/2309.15139) | 本文提出了PINF，这是物理约束深度学习的连续标准化流的一种扩展。该方法通过特征值法则和扩散来解决高维时变和稳态福克-普朗克方程。 |
| [^112] | [Deep Generative Methods for Producing Forecast Trajectories in Power Systems.](http://arxiv.org/abs/2309.15137) | 该论文研究了深度生成方法在发电系统预测轨迹中的应用，通过adapt autoregressive networks和normalizing flows捕捉多变量时间序列的时空相关性，相比当前的copula-based统计方法表现出更好的有效性。实验结果基于法国TSO RTE风力预测数据。 |
| [^113] | [Contrastive Continual Multi-view Clustering with Filtered Structural Fusion.](http://arxiv.org/abs/2309.15135) | 提出了一种名为对比度连续多视角聚类与过滤结构融合（CCMVC-FSF）的新方法，用于解决多视角聚类在实时数据收集中的困难。该方法旨在防止先前知识遗忘和利用数据相关性指导新视图的聚类过程。 |
| [^114] | [From Asset Flow to Status, Action and Intention Discovery: Early Malice Detection in Cryptocurrency.](http://arxiv.org/abs/2309.15133) | 本文提出了一种用于比特币的早期恶意检测的意图监控系统，通过定义资产转移路径和提取状态和行动，实现了对不同恶意类型的检测和发现。 |
| [^115] | [Genetic InfoMax: Exploring Mutual Information Maximization in High-Dimensional Imaging Genetics Studies.](http://arxiv.org/abs/2309.15132) | 本研究通过互信息的角度探索了基因组关联研究中图像遗传学的表示学习问题，并引入了一种跨模态学习框架Genetic InfoMax（GIM），该方法在人脑三维MRI数据上表现出显著改进的性能。 |
| [^116] | [Understanding the Structure of QM7b and QM9 Quantum Mechanical Datasets Using Unsupervised Learning.](http://arxiv.org/abs/2309.15130) | 本文使用无监督学习方法研究了QM7b和QM9量子力学数据集的内部结构和特征，发现这两个数据集的内在维度要比描述维度小，QM7b数据由与原子组成相关的明确定义的簇组成，而QM9数据包括一个由异常值组成的外部区域和一个集中了聚类和内部对象的内部核心区域。这些研究结果对于从属性预测原子组成具有重要意义。 |
| [^117] | [Evaluating Cognitive Maps and Planning in Large Language Models with CogEval.](http://arxiv.org/abs/2309.15129) | 这项研究提出了CogEval协议，用于系统评估大型语言模型的认知能力，并使用该协议对八个LLMs的认知地图和规划能力进行了评估。 |
| [^118] | [DPA-WNO: A gray box model for a class of stochastic mechanics problem.](http://arxiv.org/abs/2309.15128) | DPA-WNO是一种将可解释性的数据驱动模型与小波神经操作符相结合的新方法，用于纠正/识别缺失的物理，并解决了纯数据驱动模型的缺点。 |
| [^119] | [Grad DFT: a software library for machine learning enhanced density functional theory.](http://arxiv.org/abs/2309.15127) | Grad DFT是一种机器学习增强的密度泛函理论（DFT）软件库，通过使用权重和神经网络处理交换关联能量泛函，对DFT的能力进行了扩展。 |
| [^120] | [From Peptides to Nanostructures: A Euclidean Transformer for Fast and Stable Machine Learned Force Fields.](http://arxiv.org/abs/2309.15126) | 这项研究提出了一种称为SO3krates的欧几里得变换器架构，它通过组合稀疏等变表示和自注意机制，在机器学习力场中实现了精度、稳定性和速度的独特组合，从而使我们能够在前所未有的时间和系统尺度上对物质的量子属性进行深入分析。 |
| [^121] | [Uncovering Neural Scaling Laws in Molecular Representation Learning.](http://arxiv.org/abs/2309.15123) | 从数据中心的角度研究了分子表示学习的神经缩放行为，发现了数据量和性能之间的一致幂律关系，并提出了潜在的提高学习效率的方法。 |
| [^122] | [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models.](http://arxiv.org/abs/2309.14717) | 本文提出了QA-LoRA算法，它通过使用量化意识以及组内运算符来实现大语言模型的低秩适应。QA-LoRA能够将模型权重量化以减少时间和内存的使用，同时在不损失准确性的情况下将模型集成为一个量化模型。 |
| [^123] | [Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms.](http://arxiv.org/abs/2309.14506) | 本论文提出了一种用于黎曼流形上随机优化的零阶黎曼平均随机逼近算法（Zo-RASA），通过使用黎曼移动平均随机梯度估计器和新颖的黎曼-李雅普诺夫分析技术，实现了生成ε-近似的一阶稳定解的最优样本复杂度，同时通过使用回缩和向量传输替代指数映射和平行传输降低了算法的每次迭代复杂度。 |
| [^124] | [Era Splitting.](http://arxiv.org/abs/2309.14496) | 本研究提出了两种新的分裂准则，使得决策树模型能够利用时代信息进行优化，从而将超分布泛化研究中的思想应用于决策树模型。 |
| [^125] | [Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery.](http://arxiv.org/abs/2309.14425) | 本文研究开发了一个通用服务机器人系统，该系统可以根据不同任务和环境的变化进行自适应，并通过自恢复机制解决信息不足、计划生成错误和执行失败等问题，实现了任务的成功完成。 |
| [^126] | [Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion.](http://arxiv.org/abs/2309.14398) | 本文提出了一个多模态分类器，在动机性访谈中准确区分了变化话语、持续话语和跟随/中立话语三种类别。该分类器利用文本、声调、面部表情和身体表现等多模态特征，并对AnnoMI数据集进行了注释和训练。研究还找到了决策过程中最重要的模态，提供了宝贵的洞察。 |
| [^127] | [Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures.](http://arxiv.org/abs/2309.14298) | 本研究提出了一种改进的随机线性Bandit算法，利用鞍点边界的马丁格尔混合构建了适用于随机Bandit的置信序列，并证明该算法能够以竞争性的最坏情况下遗憾保证实现更好的性能。 |
| [^128] | [Enhancing data efficiency in reinforcement learning: a novel imagination mechanism based on mesh information propagation.](http://arxiv.org/abs/2309.14243) | 这项研究提出了一种基于网格信息传播的想象机制，在强化学习算法中显著提高了数据效率。通过使信息在不同状态间广播，而不仅仅是在同一状态集中传输，这种机制促进了模型对状态间相互依赖性的理解，并提高了对有限样本信息的学习效率。 |
| [^129] | [Explainable Machine Learning for ICU Readmission Prediction.](http://arxiv.org/abs/2309.13781) | 本研究提出了一个标准化且可解释的机器学习流程，用于在多中心数据库中预测加护病房患者的再入院情况。 |
| [^130] | [The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance.](http://arxiv.org/abs/2309.13775) | 提出了一种新的变量重要性框架，该框架在数据分布上是稳定的，并可以与现有的模型类和全局变量重要性指标结合使用。 |
| [^131] | [Impact of architecture on robustness and interpretability of multispectral deep neural networks.](http://arxiv.org/abs/2309.12463) | 这项工作研究了不同融合策略对多光谱深度学习模型性能，依赖性和稳健性的影响。 |
| [^132] | [Error Reduction from Stacked Regressions.](http://arxiv.org/abs/2309.09880) | 本文提出了一种新的堆叠回归方法，通过最小化总体风险并受非负性约束，成功降低了误差。实验证明，堆叠估计器相比其中最佳的单个估计器具有更小的总体风险。 |
| [^133] | [A Geometric Perspective on Autoencoders.](http://arxiv.org/abs/2309.08247) | 本文从几何角度研究了自编码器框架，并提出了解决多解和畸变表示问题的几何方法。 |
| [^134] | [Simultaneous inference for generalized linear models with unmeasured confounders.](http://arxiv.org/abs/2309.07261) | 本文研究了存在混淆效应时的广义线性模型的大规模假设检验问题，并提出了一种利用正交结构和线性投影的统计估计和推断框架，解决了由于未测混淆因素引起的偏差问题。 |
| [^135] | [Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity.](http://arxiv.org/abs/2309.06626) | 通过小型运行时修改引入半结构激活稀疏性，我们设计了一种稀疏训练过程，在保持精度下降最小的情况下，实现了深度神经网络的高效处理和推断加速。 |
| [^136] | [ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation.](http://arxiv.org/abs/2309.06268) | 本研究引入了一种自监督DNN方法，用于估计前列腺肿瘤的VERDICT模型参数并减少计算成本。 |
| [^137] | [Cognitive Architectures for Language Agents.](http://arxiv.org/abs/2309.02427) | 本文提出了一种称为CoALA的认知架构，用于组织语言代理的现有研究并规划未来的发展方向。CoALA描述了一个具有模块化记忆组件、结构化行动空间和通用决策过程的语言代理。通过这一框架，有望发展出更强大的语言代理。 |
| [^138] | [Distributionally Robust Machine Learning with Multi-source Data.](http://arxiv.org/abs/2309.02211) | 本文提出了一种基于多源数据的分布鲁棒机器学习方法，通过引入组分布鲁棒预测模型来提高具有分布偏移的目标人群的预测准确性。 |
| [^139] | [Linear Oscillation: The Aesthetics of Confusion for Vision Transformer.](http://arxiv.org/abs/2308.13670) | 研究提出了一种新的激活函数——线性振动（LoC）激活函数，它通过将线性轨迹和振荡偏差无缝融合，捕捉到了“困惑的重要性”的本质。实证研究表明，在需要区分微妙模式的情境中，使用LoC激活函数可以提高网络的鲁棒性。 |
| [^140] | [Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism.](http://arxiv.org/abs/2308.13150) | 本文介绍了一种使用ResNet模型和轻量级注意机制框架的图像分类方法，通过优化特征表示、增强分类能力和改善特征可辨别性，在乳腺癌分类任务上显示出卓越性能和潜在应用前景。 |
| [^141] | [Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection.](http://arxiv.org/abs/2308.12885) | 对于负责任的AI数据收集，需要对数据的质量进行彻底的审查，避免不公平、偏见或不准确的结果。 |
| [^142] | [FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data.](http://arxiv.org/abs/2308.12388) | FOSA是一种全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法，通过融合FIML估计和自注意力机制，实现了在准确性、计算效率和适应不同数据结构方面的显著优势。 |
| [^143] | [Quantum-Noise-driven Generative Diffusion Models.](http://arxiv.org/abs/2308.12013) | 该论文提出了三种量子噪声驱动的生成扩散模型，利用了量子特性以克服传统模型的主要计算困难，并建议将量子噪声视为可利用的特性而非问题。 |
| [^144] | [An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning.](http://arxiv.org/abs/2308.11677) | 本文分析了无范例类增量学习过程中的初始训练策略。研究发现，初始学习策略的选择会显著影响增量学习模型的性能，但目前还没有进行深入研究。 |
| [^145] | [Enhancing Agent Communication and Learning through Action and Language.](http://arxiv.org/abs/2308.10842) | 通过行动和语言相结合的方式，我们引入了一种新型智能体，其能够同时作为教师和学习者，通过行动演示和语言指令增强了沟通效率，并探索了结合行动和语言沟通模式对学习结果的积极影响。 |
| [^146] | [Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task.](http://arxiv.org/abs/2308.08873) | FE-PINN是一种学习底层物理特征的框架，在主训练之前以低计算成本解决问题的模式。与传统PINN相比，FE-PINN通过执行一系列子任务来解决损失函数不平衡的问题，并具有快速训练和更高的求解速度。 |
| [^147] | [Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness.](http://arxiv.org/abs/2308.03666) | 该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。 |
| [^148] | [Robust Distortion-free Watermarks for Language Models.](http://arxiv.org/abs/2307.15593) | 该论文提出了一种在语言模型中添加鲁棒无畸变水印的方法，通过映射随机数序列到语言模型的样本，可以实现在不改变文本分布的前提下对水印文本进行检测，并且在多种改写攻击下依然保持较高的鲁棒性，实验证明在40-50%的随机扰动下仍可可靠地检测到水印文本。 |
| [^149] | [Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks.](http://arxiv.org/abs/2307.15285) | 本论文解决了Zonoid的最优逼近和浅层神经网络的均匀逼近两个问题。对于Zonoid的逼近，我们填补了在$d=2,3$时的对数差距，实现了在所有维度上的解决方案。对于神经网络的逼近，我们的技术在$k \geq 1$时显著提高了目前的逼近率，并能够均匀逼近目标函数及其导数。 |
| [^150] | [FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout.](http://arxiv.org/abs/2307.02623) | FLuID提出了一种使用不变性丢失的方法来减轻联邦学习中性能较低设备导致的训练时间问题，并开发了一个自适应训练框架。通过动态平衡训练负载，FLuID能有效地减轻阻塞设备的工作负载，同时不影响模型质量。 |
| [^151] | [MoVie: Visual Model-Based Policy Adaptation for View Generalization.](http://arxiv.org/abs/2307.00972) | 本论文提出了一种名为MoVie的方法，通过基于视觉模型的策略自适应实现了视图泛化。该方法在不需要显式奖励信号和训练过程修改的情况下，在多个实际场景中表现出卓越的性能，相对改进达到了33%至152%。 |
| [^152] | [SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating Reproducible Scenes.](http://arxiv.org/abs/2306.15620) | SCENEREPLICA是一个基于YCB对象的可重复性基准测试，用于评估现实世界中的机器人操纵能力。此基准测试易于重复并允许研究人员比较不同的技术和算法，有助于加快机器人操纵方法的发展。 |
| [^153] | [Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data.](http://arxiv.org/abs/2306.13840) | 本论文提出使用多样性系数作为LLM预训练数据质量的指标，研究表明公开可用的LLM数据集的多样性系数很高。 |
| [^154] | [Correcting Underrepresentation and Intersectional Bias for Fair Classification.](http://arxiv.org/abs/2306.11112) | 本文提出一种可以有效纠正数据偏差和交叉偏差的学习方法，并构造了一个重新加权方案，可以精确评估任何假设在真实分布上的损失。 |
| [^155] | [CHORUS: Foundation Models for Unified Data Discovery and Exploration.](http://arxiv.org/abs/2306.09610) | 研究者探索将大型语言模型应用于数据发现和探索任务中，证明这些模型在表格类检测、列类型注释和联接列预测中具有优越性能，并有望将不同的数据管理任务统一在基础模型下。 |
| [^156] | [Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance in the Presence of Outliers.](http://arxiv.org/abs/2305.18974) | 该论文研究了包括异常点的高维健壮线性回归问题，提供了使用不同损失函数的精确渐近特性，对泛化误差进行了简单校准并计算了收敛速率，但由于范数校准不匹配，对估计误差的一致性需要一个较强的收敛假设。 |
| [^157] | [Explainable Brain Age Prediction using coVariance Neural Networks.](http://arxiv.org/abs/2305.18370) | 本文提出了使用协方差神经网络进行可解释的脑龄预测的框架，可以通过皮质厚度特征捕捉加速老化，并反映出增加的神经疾病或认知障碍的风险。 |
| [^158] | [Optimized Custom Dataset for Efficient Detection of Underwater Trash.](http://arxiv.org/abs/2305.16460) | 本文提出了一种自定义数据集和有效检测方法，旨在通过增加垃圾实例的多样性，在深入水下环境中提高其检测精度。 |
| [^159] | [On progressive sharpening, flat minima and generalisation.](http://arxiv.org/abs/2305.14683) | 本文提出了一种用损失黑塞矩阵和输入-输出雅克比矩阵联系起来的假设，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界，给出了关于进化磨锋和平坦极小的泛化性质的新解释。 |
| [^160] | [Structural Pruning for Diffusion Models.](http://arxiv.org/abs/2305.10924) | 本文提出了一种名为Diff-Pruning的高效压缩方法，通过一个Taylor展开过程来识别重要权重，从而从预先存在的模型中学习轻量级扩散模型，性能稳定，并在训练效率上显著提高。 |
| [^161] | [Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM.](http://arxiv.org/abs/2304.12534) | 本研究提出了一种新算法RWSADMM，以解决在动态联邦学习中存在的数据不一致和通信成本高的问题，提高了可扩展性。 |
| [^162] | [Ellipsoid fitting with the Cayley transform.](http://arxiv.org/abs/2304.10630) | 介绍了一种使用Cayley变换在任意维度上将椭球拟合到嘈杂数据中的新算法CTEF，可以拟合任意的椭球，并且能提取其他方法无法识别的数据中的非线性特征，可用于降维、数据可视化和聚类，相比其他方法更优。 |
| [^163] | [Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics.](http://arxiv.org/abs/2304.09123) | 本文提供了有限时间界限，用于被动随机梯度 Langevin 动力学算法，该算法可用于逆强化学习。该算法充当随机采样器，恢复用外部过程优化而来的成本函数。 |
| [^164] | [Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data.](http://arxiv.org/abs/2304.05874) | 本文提出了一种自适应门控图卷积网络(AGGCN)，该网络结合卷积节点特征增强和功能连接度量自适应学习图结构，实现了高精度的阿尔茨海默病诊断，并提供了重要的脑区信息。 |
| [^165] | [TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins.](http://arxiv.org/abs/2303.15954) | TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。 |
| [^166] | [Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems.](http://arxiv.org/abs/2303.12981) | 本文研究了强化学习中的策略优化问题，并证明了优化函数超水平集在网络类策略和表格式下始终是连通的，并应用此结果导出了鲁棒性强化学习的极小极大定理。 |
| [^167] | [A Deep Learning System for Domain-specific speech Recognition.](http://arxiv.org/abs/2303.10510) | 本文提出了一个使用半监督学习注释领域特定数据，基于预训练的声学模型进行微调的ASR系统，并在领域特定上取得了优于商业ASR系统的性能。 |
| [^168] | [Replay Buffer With Local Forgetting for Adaptive Deep Model-Based Reinforcement Learning.](http://arxiv.org/abs/2303.08690) | 提出了一种带有局部遗忘的新型重放缓冲区，可以在状态空间的相关部分快速遗忘过时的经验。实验证明该方法提高了自适应深度模型强化学习代理对环境变化的适应能力，加速了学习速度并改善了策略。 |
| [^169] | [Uncertainty-Aware Off-Policy Learning.](http://arxiv.org/abs/2303.06389) | 本文提出了一种不确定性感知的倒数概率分数估计器（UIPS），用于改进离线学习，通过明确模拟估计的记录策略中的不确定性，相对于广泛的最先进基线具有优越的样本效率。 |
| [^170] | [Taming Contrast Maximization for Learning Sequential, Low-latency, Event-based Optical Flow.](http://arxiv.org/abs/2303.05214) | 本文提出了一种用于顺序估计基于事件的光流的自监督学习流程，该流程通过新型的对比度最大化公式训练持续运行的神经模型，以适应非线性和变化统计的输入事件。实验结果表明，该方法在多个数据集上取得了新的技术水平。 |
| [^171] | [Towards Unbounded Machine Unlearning.](http://arxiv.org/abs/2302.09880) | 本文是第一篇研究不同应用（偏见消除、混淆解决、隐私保护）遗忘问题的论文，提出了适用于不同应用的遗忘定义和指标，并提出了SCRUB，一种在不同应用的遗忘质量度量上始终是顶级表现者的算法。 |
| [^172] | [An Asymptotically Optimal Algorithm for the Convex Hull Membership Problem.](http://arxiv.org/abs/2302.02033) | 本研究提出了一个名为Thompson-CHM的渐近最优算法，用于解决凸包成员问题，且将算法扩展到了一维和多维环境中。该算法基于模块化设计，包括停止规则和采样规则，并通过数值实验验证了理论结果的准确性。 |
| [^173] | [Distillation Policy Optimization.](http://arxiv.org/abs/2302.00533) | 本文展示了一种演员-评论家的学习框架，该框架通过蒸馏优势在利用过去经验的同时遵循稳定的在线策略，实现了快速学习并可以适用于广泛的算法类别。 |
| [^174] | [Self-Motivated Multi-Agent Exploration.](http://arxiv.org/abs/2301.02083) | 这项研究提出了自我激励的多智能体探索方法，在合作多智能体强化学习中找到自我探索和团队合作的平衡，以实现团队任务的成功。 |
| [^175] | [Metalearning generalizable dynamics from trajectories.](http://arxiv.org/abs/2301.00957) | 本研究提出了可解释的元神经常微分方程（iMODE）方法，通过学习轨迹中的动力学，可以快速准确地建模和预测多个不同物理参数的动态系统，并能反向推断系统的物理参数。 |
| [^176] | [UnICLAM:Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering.](http://arxiv.org/abs/2212.10729) | UnICLAM是一种统一和可解释的医学视觉问答模型，通过对比表示学习和对抗性屏蔽，实现了图像和文本之间的对齐和语义表示。 |
| [^177] | [Face Generation and Editing with StyleGAN: A Survey.](http://arxiv.org/abs/2212.09102) | 本文为大家综述了基于深度学习技术的人脸生成和编辑的最新进展，特别介绍了基于GAN架构的最新方法StyleGAN。该方法可以生成高质量的人脸图像，并提供了丰富的可控语义编辑和保持照片质量的接口。 |
| [^178] | [A survey of deep learning optimizers -- first and second order methods.](http://arxiv.org/abs/2211.15596) | 该论文综述了在深度学习研究中成功使用的14种标准优化方法，并从优化文献的角度对数值优化中的困难进行了理论评估。 |
| [^179] | [GPT-Neo for commonsense reasoning -- a theoretical and practical lens.](http://arxiv.org/abs/2211.15593) | 本文评估了GPT-Neo模型在常识推理任务上的性能，并与其他较大模型进行了比较。在适当的超参数设置下，该模型在多个任务上取得了具有竞争力的准确性。 |
| [^180] | [Vertical Federated Learning: Concepts, Advances and Challenges.](http://arxiv.org/abs/2211.12814) | 垂直联合学习（VFL）是一种联合学习设置，多个具有关于同一组用户不同特征的参与方共同训练机器学习模型，而不公开原始数据或模型参数。本文提供了对VFL概念、算法以及各个方面的进展和挑战的综合回顾，深入分析了隐私保护协议的分类、隐私攻击和防御策略，并提出了考虑多个约束条件的统一框架VFLow。此外，还回顾了工业应用中的最新进展和VFL面临的未来挑战和方向。 |
| [^181] | [Variants of SGD for Lipschitz Continuous Loss Functions in Low-Precision Environments.](http://arxiv.org/abs/2211.04655) | 本文研究了在低精度环境下神经网络训练的SGD变种，并测试了不同变种的效果。结果表明，相比于传统的SGD方法，在低精度算术环境下使用自适应步长的SGD变种可以获得更好的测试集准确性。 |
| [^182] | [Verifying And Interpreting Neural Networks using Finite Automata.](http://arxiv.org/abs/2211.01022) | 这项研究提出了一种使用有限自动机来验证和解释神经网络的方法。通过构建特殊的弱Büchi自动机，能够精确地捕捉神经网络的输入输出行为，并用于解决DNN的常见验证和解释任务，如对抗鲁棒性或最小充分原因。 |
| [^183] | [Deep network series for large-scale high-dynamic range imaging.](http://arxiv.org/abs/2210.16060) | 我们提出了一种针对大规模高动态范围成像的深度网络系列方法，通过残差DNN逐步增加动态范围，通过DNN迭代地估计，仅用很少的项就能得到良好的结果。 |
| [^184] | [Label Noise-Robust Learning using a Confidence-Based Sieving Strategy.](http://arxiv.org/abs/2210.05330) | 本文提出了一种使用基于置信度的筛选策略实现标签噪声鲁棒学习的方法。通过利用模型的置信度分数，可以有效区分干净样本和有噪声的样本，提供了理论保证并在实验证明了其相对于最近的研究具有卓越性能。 |
| [^185] | [Multi-Object Navigation with dynamically learned neural implicit representations.](http://arxiv.org/abs/2210.05129) | 本研究提出了一种结构化神经网络的方法，利用动态学习的神经隐性表示对多目标导航进行建模和映射，其中包括语义定位和占用探索隐性表示。 |
| [^186] | [GeONet: a neural operator for learning the Wasserstein geodesic.](http://arxiv.org/abs/2209.14440) | GeONet是一个不受网格影响的深度神经算子网络，学习了从初始和终端分布到连接两个端点分布的Wasserstein测地的非线性映射。通过学习鞍点优化条件，GeONet可以快速进行实时预测，并在仿真示例和测试数据上取得了与标准OT求解器相当的准确性。 |
| [^187] | [Overcoming Referential Ambiguity in Language-Guided Goal-Conditioned Reinforcement Learning.](http://arxiv.org/abs/2209.12758) | 本文研究了语言指导的目标-条件强化学习中的指代歧义问题，并提出了教学法和实用主义的概念来解决这些问题。实验证明，这些概念可以提高学习者的训练效率。 |
| [^188] | [A Robust and Constrained Multi-Agent Reinforcement Learning Electric Vehicle Rebalancing Method in AMoD Systems.](http://arxiv.org/abs/2209.08230) | 该论文提出了一个稳健和约束的多智能体强化学习框架，用于解决电动汽车AMoD系统中的再平衡问题，并考虑了模型不确定性和决策约束。该方法能够设计出稳健的电动汽车再平衡策略，提高系统性能。 |
| [^189] | [Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe.](http://arxiv.org/abs/2209.05324) | 本综述文章探讨了鸟瞰视角感知领域的挑战和方法，主要关注了从透视视图到鸟瞰视图的信息转换、地面真值注释获取、特征融合以及整体流程的构建。 |
| [^190] | [Studying Drowsiness Detection Performance while Driving through Scalable Machine Learning Models using Electroencephalography.](http://arxiv.org/abs/2209.04048) | 本研究通过使用脑电图和机器学习的智能框架，在驾驶场景中检测驾驶员嗜睡状态。结果表明，随机森林（RF）是性能最佳的模型，相比支持向量机（SVM）有更高的f1分数。 |
| [^191] | [Recursively Feasible Probabilistic Safe Online Learning with Control Barrier Functions.](http://arxiv.org/abs/2208.10733) | 本文提出了一种递归可行的概率安全在线学习方法，利用控制屏障函数和高斯过程回归，使得系统在在线训练和执行过程中保持安全。 |
| [^192] | [Susceptibility of Continual Learning Against Adversarial Attacks.](http://arxiv.org/abs/2207.05225) | 本文研究了连续学习任务对抗攻击的易感性，发现学习任务容易受到对抗性攻击导致的目标类别错误分类。这对数据完整性和隐私构成了重大威胁。 |
| [^193] | [FedSS: Federated Learning with Smart Selection of clients.](http://arxiv.org/abs/2207.04569) | 本论文提出了一种名为FedSS的智能选择客户端的联邦学习方法，通过智能客户端选择和调度技术，平衡了快速收敛和异质性之间的关系。 |
| [^194] | [Private independence testing across two parties.](http://arxiv.org/abs/2207.03652) | 我们提出了一种私密独立性检测算法，通过私密估计数据集之间的距离相关性进行统计独立性测试。我们建立了差分隐私测试的加法和乘法误差界限，相信该算法在涉及敏感数据的分布式假设检验中会有应用。 |
| [^195] | [Group-invariant tensor train networks for supervised learning.](http://arxiv.org/abs/2206.15051) | 本论文介绍了一种用于监督学习的群不变张量网络方法，通过构建群的不变张量基并结合群不变张量分解网络，可以获得与最先进的深度学习方法相通的预测准确性。 |
| [^196] | [Pragmatically Learning from Pedagogical Demonstrations in Multi-Goal Environments.](http://arxiv.org/abs/2206.04546) | 该论文介绍了一种在多目标环境中从教学示范中实用地学习的方法。通过结合实用主义机制和教学机制，利用贝叶斯模型进行目标推断，可以加快学习速度并减少目标的歧义。 |
| [^197] | [DIRA: A Framework for Dynamic Domain Incremental Regularised Adaptation.](http://arxiv.org/abs/2205.00147) | DIRA是一个用于DNN分类器的动态领域自适应的框架，使用正则化技术来解决灾难性遗忘问题，并通过少量样本实现重新训练和适应性。 |
| [^198] | [Pedagogical Demonstrations and Pragmatic Learning in Artificial Tutor-Learner Interactions.](http://arxiv.org/abs/2203.00111) | 本文研究了人工智能导师-学习者互动中的教学演示和实用学习机制，并在一个多目标环境中实现。通过使用导师的教学和学习者的实用推理，相比传统的从演示中学习方法，我们实现了显著的改进。 |
| [^199] | [Efficient Direct-Connect Topologies for Collective Communications.](http://arxiv.org/abs/2202.03356) | 本论文提出了一种算法框架，可以构建高效的直连网络拓扑结构，以优化节点延迟和带宽权衡，适用于集体通信负载。 |
| [^200] | [Discovering and Exploiting Sparse Rewards in a Learned Behavior Space.](http://arxiv.org/abs/2111.01919) | 这项研究介绍了一种名为STAX的算法，能够在学习行为空间时实时探索，并且能够有效优化任何发现的奖励。 |
| [^201] | [Unsupervised Movement Detection in Indoor Positioning Systems of Production Halls.](http://arxiv.org/abs/2109.10757) | 本文提出了一个无监督的统计过程来解决生产车间室内定位系统中数据分析中的挑战，通过结合可视分析和运动检测，不仅区分停留与移动，还考虑了不期望的唤醒，从而提供了详细的解释方案，并在实际案例研究中验证了该方法的可行性。 |
| [^202] | [Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions.](http://arxiv.org/abs/2105.13913) | 本论文介绍了一种简单的Frank-Wolfe算法变体，利用广义自协调函数的特性，在不需要使用二阶信息或估计局部平滑度参数的情况下，以$\mathcal{O}(1/t)$的收敛速度达到了优化目标。 |
| [^203] | [MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch Normalization.](http://arxiv.org/abs/2010.09278) | 本文提出了一种名为MimicNorm的归一化方法，通过简化批归一化（BN）的正则化方法并保持其核心影响，即数据去相关性和自适应学习率，来提高网络训练的收敛性和效率。MimicNorm仅包含两个轻量级操作，可与BN相媲美。 |
| [^204] | [Novel and flexible parameter estimation methods for data-consistent inversion in mechanistic modeling.](http://arxiv.org/abs/2009.08267) | 本论文提出了一种新颖灵活的参数估计方法，用于机械建模中的数据一致反演。该方法解决了贝叶斯分析中无信息先验引入的偏差问题，并在随机逆问题框架下推断参数。使用拒绝采样、马尔科夫链蒙特卡洛和生成对抗网络等新方法解决了数据一致反演的限制，并通过约束优化和先验逆问题分析进一步优化了结果。 |
| [^205] | [Improving the convergence of SGD through adaptive batch sizes.](http://arxiv.org/abs/1910.08222) | 通过自适应批大小，本研究提出了一种改善SGD收敛性的方法，既减少了高方差梯度估计的问题，又保持了较高精度的梯度估计。 |
| [^206] | [Optimal Sparse Decision Trees.](http://arxiv.org/abs/1904.12847) | 这篇论文介绍了第一个针对二进制变量的最优决策树的实用算法，通过分析界限和现代系统技术的结合来解决决策树优化的困难，实验证明了其在可扩展性、速度和最优性证明方面的优势。 |

# 详细

[^1]: SHACIRA: 可扩展的哈希网格压缩技术用于隐式神经表示

    SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations. (arXiv:2309.15848v1 [cs.CV])

    [http://arxiv.org/abs/2309.15848](http://arxiv.org/abs/2309.15848)

    SHACIRA提出了一种简单但有效的通用框架，用于对神经表示中的特征网格进行高水平压缩，通过量化潜在权重和应用熵正则化来实现压缩。这种方法在多样化数据集上取得了定量和定性上的好结果。

    

    隐式神经表示（INR）或神经场已成为编码多媒体信号（如图像和辐射场）并保持高质量的流行框架。最近，由Instant-NGP提出的可学习特征网格在训练和采样INR方面提供了显著的加速，它通过用一个多分辨率查找表的特征向量和一个更小的神经网络来替代一个大型神经网络。然而，这些特征网格的内存消耗很大，这可能成为存储和流媒体应用的瓶颈。本研究提出了SHACIRA，一种简单但有效的通用框架，用于对这些特征网格进行压缩，而无需额外的后处理修剪/量化阶段。我们使用量化的潜在权重对特征网格重新参数化，并在潜在空间应用熵正则化，以实现在不同领域间的高水平压缩。对多样化数据集的定量和定性结果验证了我们的方法。

    Implicit Neural Representations (INR) or neural fields have emerged as a popular framework to encode multimedia signals such as images and radiance fields while retaining high-quality. Recently, learnable feature grids proposed by Instant-NGP have allowed significant speed-up in the training as well as the sampling of INRs by replacing a large neural network with a multi-resolution look-up table of feature vectors and a much smaller neural network. However, these feature grids come at the expense of large memory consumption which can be a bottleneck for storage and streaming applications. In this work, we propose SHACIRA, a simple yet effective task-agnostic framework for compressing such feature grids with no additional post-hoc pruning/quantization stages. We reparameterize feature grids with quantized latent weights and apply entropy regularization in the latent space to achieve high levels of compression across various domains. Quantitative and qualitative results on diverse datase
    
[^2]: 利用扩散模型中的信号泄漏偏差

    Exploiting the Signal-Leak Bias in Diffusion Models. (arXiv:2309.15842v1 [cs.CV])

    [http://arxiv.org/abs/2309.15842](http://arxiv.org/abs/2309.15842)

    本文展示了如何利用现有扩散模型中的信号泄漏偏差，以实现对生成图像的更好控制，并生成更多样化的亮度以及更满足特定风格和颜色要求的图像。

    

    大多数扩散模型的推理过程中存在偏差。这种偏差是由信号泄漏引起的，其分布与噪声分布不一致，导致训练和推理过程之间存在差异。我们证明了在模型针对特定风格进行调优时，这种信号泄漏偏差特别显著，导致风格匹配不够优化。最近的研究试图在训练过程中避免信号泄漏。我们相反地展示了如何利用现有的扩散模型中的信号泄漏偏差，以实现对生成图像的更好控制。这使我们能够生成亮度更多样化的图像以及更能匹配所需风格或颜色的图像。通过对空间频率和像素域中的信号泄漏进行建模，并在初始潜变量中引入信号泄漏，我们生成更符合预期结果的图像，而无需额外的训练。

    There is a bias in the inference pipeline of most diffusion models. This bias arises from a signal leak whose distribution deviates from the noise distribution, creating a discrepancy between training and inference processes. We demonstrate that this signal-leak bias is particularly significant when models are tuned to a specific style, causing sub-optimal style matching. Recent research tries to avoid the signal leakage during training. We instead show how we can exploit this signal-leak bias in existing diffusion models to allow more control over the generated images. This enables us to generate images with more varied brightness, and images that better match a desired style or color. By modeling the distribution of the signal leak in the spatial frequency and pixel domains, and including a signal leak in the initial latent, we generate images that better match expected results without any additional training.
    
[^3]: 如何捕捉AI谎言：通过问无关问题在黑盒LLMs中进行谎言检测

    How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. (arXiv:2309.15840v1 [cs.CL])

    [http://arxiv.org/abs/2309.15840](http://arxiv.org/abs/2309.15840)

    本文提出了一个简单但高精确度的谎言检测器，通过在怀疑有谎言的情况下问一组无关的后续问题，并将LLM的是/否答案输入到一个逻辑回归分类器中，该检测器能够推广到不同的LLM架构和实际场景中的谎言情况。

    

    大型语言模型（LLMs）会“说谎”，也就是在明知道真相的情况下输出虚假陈述。当指示输出错误信息时，LLMs可能会“说谎”。在这里，我们开发了一个简单的谎言检测器，既不需要访问LLM的激活（黑盒），也不需要事实问题的真相知识。这个检测器通过在怀疑有谎言的情况下问一组预定义的无关后续问题，并将LLM的是/否答案输入到逻辑回归分类器中来工作。尽管简单，这个谎言检测器非常准确并且令人惊讶地通用。当在单一情境的示例上进行训练 - 促使GPT-3.5在事实问题上撒谎 - 该检测器可以推广到以下情况：（1）其他LLM架构，（2）细调为说谎的LLMs，（3）谄媚的谎言，和（4）出现在实际场景中的谎言，比如销售。这些结果表明，LLMs具有特殊的与谎言相关的行为模式。

    Large language models (LLMs) can "lie", which we define as outputting false statements despite "knowing" the truth in a demonstrable sense. LLMs might "lie", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural pa
    
[^4]: 使用机器学习技术自动检测COVID-19后患者中的持续性炎症生物标志物

    Automated Detection of Persistent Inflammatory Biomarkers in Post-COVID-19 Patients Using Machine Learning Techniques. (arXiv:2309.15838v1 [cs.LG])

    [http://arxiv.org/abs/2309.15838](http://arxiv.org/abs/2309.15838)

    这项研究利用机器学习技术自动检测COVID-19后患者中的持续性炎症生物标志物，在伊拉克医院收集的医疗数据上进行研究，并构建了预测模型。

    

    COVID-19大流行对个体产生了持久影响，许多人在疾病后急性期经历持续的症状，包括炎症。检测和监测这些炎症生物标志物对及时干预和改善患者结果至关重要。本研究利用机器学习技术自动识别290名COVID-19后患者中的持久性炎症生物标志物，根据伊拉克医院收集的医疗数据。数据包括各种临床参数，如C-反应蛋白和白细胞介素-6水平、患者人口统计学、合并症和治疗历史。进行了严格的数据预处理和特征选择过程，以优化机器学习分析的数据集。部署了各种机器学习算法，包括逻辑回归、随机森林、支持向量机和梯度提升，来构建预测模型。这些模型表现出了

    The COVID-19 pandemic has left a lasting impact on individuals, with many experiencing persistent symptoms, including inflammation, in the post-acute phase of the disease. Detecting and monitoring these inflammatory biomarkers is critical for timely intervention and improved patient outcomes. This study employs machine learning techniques to automate the identification of persistent inflammatory biomarkers in 290 post-COVID-19 patients, based on medical data collected from hospitals in Iraq. The data encompassed a wide array of clinical parameters, such as C-reactive protein and interleukin-6 levels, patient demographics, comorbidities, and treatment histories. Rigorous data preprocessing and feature selection processes were implemented to optimize the dataset for machine learning analysis. Various machine learning algorithms, including logistic regression, random forests, support vector machines, and gradient boosting, were deployed to construct predictive models. These models exhibit
    
[^5]: 多单元软测量允许少样本学习

    Multi-unit soft sensing permits few-shot learning. (arXiv:2309.15828v1 [stat.ML])

    [http://arxiv.org/abs/2309.15828](http://arxiv.org/abs/2309.15828)

    多单元软测量是利用可转移性学习算法改进软测量的一种方法，能够通过解决多个任务来增强软测量的性能，并且特别适用于具有多个实现的进程。

    

    近期的研究探索了利用具有可转移性的学习算法来改进软测量的各种方法。总体来说，当一个软测量通过解决多个任务来学习时，其性能可以得到加强。可转移性的有用性取决于所设计的学习任务的相关性。在软测量要应用于有多个实现的进程（例如，有多个可用数据的系统或设备）时，尤其相关。然后，每个实现都提供一个软测量学习任务，并且合理地期望这些不同任务之间具有强相关性。在这种设置中应用可转移性导致了我们所称的多单元软测量，其中软测量通过从所有实现的数据中学习来建模一个进程。本文探讨了多单元软测量的学习能力，它被构建为一个分层模型，并使用...

    Recent literature has explored various ways to improve soft sensors using learning algorithms with transferability. Broadly put, the performance of a soft sensor may be strengthened when it is learned by solving multiple tasks. The usefulness of transferability depends on how strongly related the devised learning tasks are. A particularly relevant case for transferability, is when a soft sensor is to be developed for a process of which there are many realizations, e.g. system or device with many implementations from which data is available. Then, each realization presents a soft sensor learning task, and it is reasonable to expect that the different tasks are strongly related. Applying transferability in this setting leads to what we call multi-unit soft sensing, where a soft sensor models a process by learning from data from all of its realizations.  This paper explores the learning abilities of a multi-unit soft sensor, which is formulated as a hierarchical model and implemented usin
    
[^6]: 使用LM模拟沙盒识别LM代理的风险

    Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])

    [http://arxiv.org/abs/2309.15817](http://arxiv.org/abs/2309.15817)

    通过使用LM模拟工具执行和开发基于LM的自动安全评估器，该论文提出了一种解决测试LM代理的高成本和寻找高风险问题的方法。

    

    最近的语言模型（LM）代理和工具使用的技术进步，例如ChatGPT插件，使得代理具备了丰富的功能，但也放大了潜在的风险，如泄露私人数据或引发财务损失。识别这些风险是一项耗时的工作，需要实施工具，手动设置每个测试场景的环境，并找到风险案例。随着工具和代理变得越来越复杂，测试这些代理的高成本将使寻找高风险、长尾风险变得越来越困难。为了解决这些挑战，我们引入了ToolEmu：一个使用LM来模拟工具执行的框架，可以在不需要手动实例化的情况下对LM代理进行各种工具和场景的测试。除了模拟器，我们还开发了一个基于LM的自动安全评估器，用于检查代理的失败并量化相关风险。我们通过人工评估测试了工具模拟器和评估器，并发现了6个...

    Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 6
    
[^7]: 公平的典范相关分析

    Fair Canonical Correlation Analysis. (arXiv:2309.15809v1 [cs.LG])

    [http://arxiv.org/abs/2309.15809](http://arxiv.org/abs/2309.15809)

    本文研究了典范相关分析中的公平性和偏见问题，并提出了一种通过最小化相关性差异误差来减轻不公平现象的方法。该方法在保持CCA准确性的同时，减少了相关性差异误差。

    

    本文研究了典范相关分析（CCA）中的公平性和偏见问题，CCA是一种广泛应用的统计技术，用于研究两组变量之间的关系。我们提出了一个框架，通过最小化与受保护属性相关的相关性差异误差，来减轻不公平现象。我们的方法使得CCA能够从所有数据点中学习到全局投影矩阵，同时确保这些矩阵产生与组特定投影矩阵相当的相关性水平。对合成和真实数据集的实验评估表明，我们的方法在减少相关性差异误差的同时不会影响CCA的准确性。

    This paper investigates fairness and bias in Canonical Correlation Analysis (CCA), a widely used statistical technique for examining the relationship between two sets of variables. We present a framework that alleviates unfairness by minimizing the correlation disparity error associated with protected attributes. Our approach enables CCA to learn global projection matrices from all data points while ensuring that these matrices yield comparable correlation levels to group-specific projection matrices. Experimental evaluation on both synthetic and real-world datasets demonstrates the efficacy of our method in reducing correlation disparity error without compromising CCA accuracy.
    
[^8]: ANNCRIPS:癌症研究中的人工神经网络在预测和生存中的应用

    ANNCRIPS: Artificial Neural Networks for Cancer Research In Prediction & Survival. (arXiv:2309.15803v1 [cs.LG])

    [http://arxiv.org/abs/2309.15803](http://arxiv.org/abs/2309.15803)

    本研究通过开发和验证一种智能数学模型，利用人工神经网络提高前列腺癌的早期检测，降低假阳性率，改善患者预后。预计该模型经过进一步改进和验证后，可以成为一个可靠的、市场化的前列腺癌检测解决方案。

    

    前列腺癌是50岁及以上男性中常见的恶性肿瘤。目前的诊断方法主要依靠血液测试、前列腺特异性抗原(PSA)水平和直肠指检(DRE)。然而，这些方法存在显著的假阳性率。该研究关注开发和验证一种利用人工神经网络(ANNs)的智能数学模型，以提高前列腺癌的早期检测。本研究的主要目标是展示一种设计用于帮助早期发现前列腺癌、促进医疗专业人员及时干预的新型数学模型。该模型的实施展示了降低假阳性发生率、改善患者预后的潜力。此外，我们预见在进一步改进、广泛测试和验证的情况下，该模型可以成为一个可靠的、市场化的前列腺癌检测解决方案。

    Prostate cancer is a prevalent malignancy among men aged 50 and older. Current diagnostic methods primarily rely on blood tests, PSA:Prostate-Specific Antigen levels, and Digital Rectal Examinations (DRE). However, these methods suffer from a significant rate of false positive results. This study focuses on the development and validation of an intelligent mathematical model utilizing Artificial Neural Networks (ANNs) to enhance the early detection of prostate cancer. The primary objective of this research paper is to present a novel mathematical model designed to aid in the early detection of prostate cancer, facilitating prompt intervention by healthcare professionals. The model's implementation demonstrates promising potential in reducing the incidence of false positives, thereby improving patient outcomes. Furthermore, we envision that, with further refinement, extensive testing, and validation, this model can evolve into a robust, marketable solution for prostate cancer detection. 
    
[^9]: Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction. (arXiv:2309.15798v1 [cs.LG])

    Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction. (arXiv:2309.15798v1 [cs.LG])

    [http://arxiv.org/abs/2309.15798](http://arxiv.org/abs/2309.15798)

    Node-Aligned Graph-to-Graph (NAG2G)是一个基于transformer的无模板模型，利用2D分子图和3D构象信息，能够更好地利用分子的拓扑信息和对齐原子，提高单步反合成预测的竞争力。

    

    单步反合成是有机化学和药物设计中至关重要的任务，需要识别出合成特定化合物所需的反应物。随着计算机辅助合成规划的出现，越来越多的人开始关注使用机器学习技术来简化这个过程。现有的无模板机器学习模型通常使用transformer结构，并将分子表示为ID序列。然而，这些方法往往在充分利用分子的广泛拓扑信息和在生成物和反应物之间对齐原子方面面临挑战，导致结果不如半模板模型竞争力强。我们提出的方法，Node-Aligned Graph-to-Graph (NAG2G)，也是一个基于transformer的无模板模型，但是利用了2D分子图和3D构象信息。此外，我们的方法通过利用n

    Single-step retrosynthesis is a crucial task in organic chemistry and drug design, requiring the identification of required reactants to synthesize a specific compound. with the advent of computer-aided synthesis planning, there is growing interest in using machine-learning techniques to facilitate the process. Existing template-free machine learning-based models typically utilize transformer structures and represent molecules as ID sequences. However, these methods often face challenges in fully leveraging the extensive topological information of the molecule and aligning atoms between the production and reactants, leading to results that are not as competitive as those of semi-template models. Our proposed method, Node-Aligned Graph-to-Graph (NAG2G), also serves as a transformer-based template-free model but utilizes 2D molecular graphs and 3D conformation information. Furthermore, our approach simplifies the incorporation of production-reactant atom mapping alignment by leveraging n
    
[^10]: 学习来自有缺陷的数据：弱监督式自动语音识别

    Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition. (arXiv:2309.15796v1 [eess.AS])

    [http://arxiv.org/abs/2309.15796](http://arxiv.org/abs/2309.15796)

    本文提出了一种弱监督自动语音识别方法，使用全时分类准则训练模型，可以有效学习语音-文本对齐，并适应训练转录中的错误，避免性能下降。

    

    训练自动语音识别（ASR）系统需要大量经过精心筛选的配对数据。然而，人工标注者通常执行“非逐字”转录，这可能导致训练模型不佳。在本文中，我们提出了全时分类（OTC），一种新颖的训练准则，明确地融入了由此类弱监督引起的标签不确定性。这使得模型能够有效地学习语音-文本对齐，并适应训练转录中存在的错误。OTC通过利用加权有限状态转换器扩展了传统的CTC目标函数用于不完美转录。通过在LibriSpeech和LibriVox数据集上进行的实验，我们证明使用OTC训练ASR模型可以避免性能下降，即使转录中包含高达70％的错误，而CTC模型则完全失效。我们的实现可在https://github.com/k2-fsa/icefall获得。

    Training automatic speech recognition (ASR) systems requires large amounts of well-curated paired data. However, human annotators usually perform "non-verbatim" transcription, which can result in poorly trained models. In this paper, we propose Omni-temporal Classification (OTC), a novel training criterion that explicitly incorporates label uncertainties originating from such weak supervision. This allows the model to effectively learn speech-text alignments while accommodating errors present in the training transcripts. OTC extends the conventional CTC objective for imperfect transcripts by leveraging weighted finite state transducers. Through experiments conducted on the LibriSpeech and LibriVox datasets, we demonstrate that training ASR models with OTC avoids performance degradation even with transcripts containing up to 70% errors, a scenario where CTC models fail completely. Our implementation is available at https://github.com/k2-fsa/icefall.
    
[^11]: 用因果森林针对相对风险异质性进行目标化

    Targeting Relative Risk Heterogeneity with Causal Forests. (arXiv:2309.15793v1 [stat.ME])

    [http://arxiv.org/abs/2309.15793](http://arxiv.org/abs/2309.15793)

    本研究提出了一种通过修改因果森林方法，以相对风险为目标，从而捕捉到治疗效应异质性的潜在来源。

    

    在临床试验分析中，治疗效应异质性（TEH）即种群中不同亚群的治疗效应的变异性是非常重要的。因果森林（Wager和Athey，2018）是解决这个问题的一种非常流行的方法，但像许多其他发现TEH的方法一样，它用于分离亚群的标准侧重于绝对风险的差异。这可能会削弱统计功效，掩盖了相对风险中的细微差别，而相对风险通常是临床关注的更合适的数量。在这项工作中，我们提出并实现了一种修改因果森林以针对相对风险的方法，使用基于广义线性模型（GLM）比较的新颖节点分割过程。我们在模拟和真实数据上展示了结果，表明相对风险的因果森林可以捕捉到其他未观察到的异质性源。

    Treatment effect heterogeneity (TEH), or variability in treatment effect for different subgroups within a population, is of significant interest in clinical trial analysis. Causal forests (Wager and Athey, 2018) is a highly popular method for this problem, but like many other methods for detecting TEH, its criterion for separating subgroups focuses on differences in absolute risk. This can dilute statistical power by masking nuance in the relative risk, which is often a more appropriate quantity of clinical interest. In this work, we propose and implement a methodology for modifying causal forests to target relative risk using a novel node-splitting procedure based on generalized linear model (GLM) comparison. We present results on simulated and real-world data that suggest relative risk causal forests can capture otherwise unobserved sources of heterogeneity.
    
[^12]: 大型语言模型选择与基准数据集

    Large Language Model Routing with Benchmark Datasets. (arXiv:2309.15789v1 [cs.CL])

    [http://arxiv.org/abs/2309.15789](http://arxiv.org/abs/2309.15789)

    本论文解决了从一系列模型中为新任务选择最佳大型语言模型的挑战，通过提出了一个基于基准数据集的学习模型来选择模型，并在各种任务中提高了性能。

    

    开源的大型语言模型（LLM）和基准数据集数量迅速增长，用于比较它们。虽然一些模型在这些基准测试中占优势，但通常没有单一模型在所有任务和用例中都能达到最佳准确性。在这项工作中，我们解决了从一系列模型中为新任务选择最佳LLM的挑战。我们提出了一个新的问题表述，在这个问题中，基准数据集被重新用于学习一个"路由器"模型来选择LLM，并且我们表明这个问题可以转化为一系列二元分类任务的集合。我们展示了从各种基准数据集学习模型路由器的效用和限制，我们在所有任务中始终比使用任何单一模型都提高了性能。

    There is a rapidly growing number of open-source Large Language Models (LLMs) and benchmark datasets to compare them. While some models dominate these benchmarks, no single model typically achieves the best accuracy in all tasks and use cases. In this work, we address the challenge of selecting the best LLM out of a collection of models for new tasks. We propose a new formulation for the problem, in which benchmark datasets are repurposed to learn a "router" model for this LLM selection, and we show that this problem can be reduced to a collection of binary classification tasks. We demonstrate the utility and limitations of learning model routers from various benchmark datasets, where we consistently improve performance upon using any single model for all tasks.
    
[^13]: 点云配准的部分转运

    Partial Transport for Point-Cloud Registration. (arXiv:2309.15787v1 [cs.CV])

    [http://arxiv.org/abs/2309.15787](http://arxiv.org/abs/2309.15787)

    本论文提出了一种基于最优部分输运问题的点云配准方法，该方法可以应对非刚性动作和部分可见性等实际场景中的复杂性。该方法将点云视为经验度量，并提供了一种数学严格的方法来量化源点和目标点之间的`对应关系'。

    

    点云配准在机器人、计算机图形学和医学图像等各个领域中起着关键作用。该过程涉及确定不同点集之间的空间关系，通常在3D空间内进行。在现实世界的情景中，由于非刚性动作和部分可见性（例如遮挡或传感器噪声）的复杂性，使得非刚性配准成为一个具有挑战性的问题。经典的非刚性配准方法通常计算复杂，并且具有不稳定的性能，更重要的是，其理论保证有限。最优输运问题及其非平衡变种（例如最优部分输运问题）已成为点云配准的强大工具，在该领域建立了一个强有力的基准。这些方法将点云视为经验度量，并提供了一种数学严格的方法来量化（变换的）源点和目标点之间的`对应关系'。在这篇论文中，我们提出了一种基于最优部分输运问题的点云配准方法。

    Point cloud registration plays a crucial role in various fields, including robotics, computer graphics, and medical imaging. This process involves determining spatial relationships between different sets of points, typically within a 3D space. In real-world scenarios, complexities arise from non-rigid movements and partial visibility, such as occlusions or sensor noise, making non-rigid registration a challenging problem. Classic non-rigid registration methods are often computationally demanding, suffer from unstable performance, and, importantly, have limited theoretical guarantees. The optimal transport problem and its unbalanced variations (e.g., the optimal partial transport problem) have emerged as powerful tools for point-cloud registration, establishing a strong benchmark in this field. These methods view point clouds as empirical measures and provide a mathematically rigorous way to quantify the `correspondence' between (the transformed) source and target points. In this paper,
    
[^14]: 学习高效前沿

    Learning the Efficient Frontier. (arXiv:2309.15775v1 [cs.LG])

    [http://arxiv.org/abs/2309.15775](http://arxiv.org/abs/2309.15775)

    本文引入了NeuralEF，一个快速的神经逼近框架，能够鲁棒地预测高效前沿问题的解，同时处理异构线性约束和可变数量的优化输入。

    

    高效前沿（EF）是一个基本的资源配置问题，在给定风险水平下寻找最优投资组合以最大化收益。传统上，通过解一个凸优化问题来找到最优解。本文引入了NeuralEF：一个快速的神经逼近框架，可以鲁棒地预测相对异构线性约束和可变数量的优化输入的EF凸优化问题的结果。通过将优化问题重新定义为序列到序列问题，我们展示了NeuralEF是加速大规模模拟并处理不连续行为的可行解决方案。

    The efficient frontier (EF) is a fundamental resource allocation problem where one has to find an optimal portfolio maximizing a reward at a given level of risk. This optimal solution is traditionally found by solving a convex optimization problem. In this paper, we introduce NeuralEF: a fast neural approximation framework that robustly forecasts the result of the EF convex optimization problem with respect to heterogeneous linear constraints and variable number of optimization inputs. By reformulating an optimization problem as a sequence to sequence problem, we show that NeuralEF is a viable solution to accelerate large-scale simulation while handling discontinuous behavior.
    
[^15]: 权重重要的离线学习正确地完成

    Importance-Weighted Offline Learning Done Right. (arXiv:2309.15771v1 [cs.LG])

    [http://arxiv.org/abs/2309.15771](http://arxiv.org/abs/2309.15771)

    本文研究了随机上下文赌博问题中的离线策略优化问题，并提出了一种替代方法，通过使用“隐式探索”估计器来计算策略价值的权重重要估计。与之前的结果相比，在几乎所有情况下都具有更好的性能保证，同时消除了之前所做的非常苛刻的“均匀覆盖”假设。

    

    我们研究了随机上下文赌博问题中的离线策略优化问题，目标是基于由次优行为策略收集的决策数据集学习一个近似最优的策略。我们不对奖励函数做任何结构性假设，而是假设可以访问给定的策略类，并且旨在与该类中的最佳比较器策略竞争。在这种情况下，标准方法是计算每个策略价值的权重重要估计，并选择一个最小化估计值的策略，减去估计值中的“悲观”调整以减少其随机波动。在本文中，我们展示了一种基于 \citet{Neu2015} 的“隐式探索”估计器的简单替代方法，其性能保证在几乎所有可能的情况下都优于之前的结果。尤其值得注意的是，我们消除了之前所有工作中非常苛刻的“均匀覆盖”假设。

    We study the problem of offline policy optimization in stochastic contextual bandit problems, where the goal is to learn a near-optimal policy based on a dataset of decision data collected by a suboptimal behavior policy. Rather than making any structural assumptions on the reward function, we assume access to a given policy class and aim to compete with the best comparator policy within this class. In this setting, a standard approach is to compute importance-weighted estimators of the value of each policy, and select a policy that minimizes the estimated value up to a "pessimistic" adjustment subtracted from the estimates to reduce their random fluctuations. In this paper, we show that a simple alternative approach based on the "implicit exploration" estimator of \citet{Neu2015} yields performance guarantees that are superior in nearly all possible terms to all previous results. Most notably, we remove an extremely restrictive "uniform coverage" assumption made in all previous works.
    
[^16]: 普通最小二乘插值器的代数和统计属性

    Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator. (arXiv:2309.15769v1 [math.ST])

    [http://arxiv.org/abs/2309.15769](http://arxiv.org/abs/2309.15769)

    本文研究了普通最小二乘插值器在高维环境中的代数和统计属性，并为最小l2范数OLS插值器提供了基本结果。这些结果对理解OLS插值器的泛化能力具有重要意义。

    

    深度学习研究揭示了对超参数化统计模型的良性过拟合现象，近年来引起了重大的理论兴趣。鉴于其简单性和实用性，普通最小二乘（OLS）插值器已成为获得对这种现象基础洞察力的关键所在。尽管OLS在经典环境中的性质已经得到了很好的建立，但在高维环境中的行为还没有像岭回归或套索回归那样被探索得那么透彻，尽管近年来已取得了显著进展。我们通过为最小l2范数OLS插值器提供基本的代数和统计结果来贡献于这一日益增长的文献。特别地，我们提供了（i）留-k-out残差公式的高维代数等价物，（ii） Cochran公式，以及（iii）Frisch-Waugh-Lovell定理。这些结果有助于理解OLS插值器的泛化能力并具有实质性的影响。

    Deep learning research has uncovered the phenomenon of benign overfitting for over-parameterized statistical models, which has drawn significant theoretical interest in recent years. Given its simplicity and practicality, the ordinary least squares (OLS) interpolator has become essential to gain foundational insights into this phenomenon. While properties of OLS are well established in classical settings, its behavior in high-dimensional settings is less explored (unlike for ridge or lasso regression) though significant progress has been made of late. We contribute to this growing literature by providing fundamental algebraic and statistical results for the minimum $\ell_2$-norm OLS interpolator. In particular, we provide high-dimensional algebraic equivalents of (i) the leave-$k$-out residual formula, (ii) Cochran's formula, and (iii) the Frisch-Waugh-Lovell theorem. These results aid in understanding the OLS interpolator's ability to generalize and have substantive implications for c
    
[^17]: 快速网络适应：利用测试反馈学习适应神经网络

    Rapid Network Adaptation: Learning to Adapt Neural Networks Using Test-Time Feedback. (arXiv:2309.15762v1 [cs.CV])

    [http://arxiv.org/abs/2309.15762](http://arxiv.org/abs/2309.15762)

    本文提出了一种快速网络适应的方法，通过利用测试时的反馈信号来实时调整神经网络，相比基准方法更加灵活且快速数个数量级，通过广泛的实验评估，在各种数据集、任务和分布变化下取得了令人满意的结果。

    

    我们提出了一种在测试时适应分布变化的神经网络方法。与试训练时的鲁棒性机制不同，试图预测和对抗变化，我们创建了一个闭环系统，并利用测试时的反馈信号实时调整网络。我们展示了使用基于学习的函数，实现了网络的一种摊销优化器，从而实现了一种名为快速网络适应(RNA)的适应方法，它比基准方法更加灵活且快速数个数量级。通过使用各种适应信号和目标任务进行广泛的实验，我们研究了该方法的效率和灵活性。我们使用了各种数据集(Taskonomy、Replica、ScanNet、Hypersim、COCO、ImageNet)，任务(深度、光流、语义分割、分类)和分布变化(Cross-datasets、2D和3D Common Corruptions)进行了评估，并取得了令人满意的结果。

    We propose a method for adapting neural networks to distribution shifts at test-time. In contrast to training-time robustness mechanisms that attempt to anticipate and counter the shift, we create a closed-loop system and make use of a test-time feedback signal to adapt a network on the fly. We show that this loop can be effectively implemented using a learning-based function, which realizes an amortized optimizer for the network. This leads to an adaptation method, named Rapid Network Adaptation (RNA), that is notably more flexible and orders of magnitude faster than the baselines. Through a broad set of experiments using various adaptation signals and target tasks, we study the efficiency and flexibility of this method. We perform the evaluations using various datasets (Taskonomy, Replica, ScanNet, Hypersim, COCO, ImageNet), tasks (depth, optical flow, semantic segmentation, classification), and distribution shifts (Cross-datasets, 2D and 3D Common Corruptions) with promising results
    
[^18]: 基于潜在图的生物医学表格数据半监督学习

    Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v1 [cs.LG])

    [http://arxiv.org/abs/2309.15757](http://arxiv.org/abs/2309.15757)

    本文提出了一种基于潜在图的半监督学习方法，通过利用图的表示来捕捉数据之间的关系，并实现了全局和局部知识的有效融合。在生物医学数据集上的评估中，我们的方法表现出了最先进的结果。

    

    在半监督学习领域中，现有方法未充分利用（有）标记数据之间的实例间关系的潜力。本文通过提供一种推断捕捉内在数据关系的潜在图的方法来解决这个限制。通过利用基于图的表示，我们的方法促进了信息在整个图中的无缝传播，能够有效地融合全局和局部知识。通过在生物医学表格数据集上的评估，我们比较了我们的方法与其他当代方法的能力。我们的工作证明了发现实例间关系作为构建强化半监督学习技术的鲁棒潜在图的实际手段的重要性。我们的方法在三个生物医学数据集上取得了最先进的结果。

    In the domain of semi-supervised learning, the current approaches insufficiently exploit the potential of considering inter-instance relationships among (un)labeled data. In this work, we address this limitation by providing an approach for inferring latent graphs that capture the intrinsic data relationships. By leveraging graph-based representations, our approach facilitates the seamless propagation of information throughout the graph, enabling the effective incorporation of global and local knowledge. Through evaluations on biomedical tabular datasets, we compare the capabilities of our approach to other contemporary methods. Our work demonstrates the significance of inter-instance relationship discovery as practical means for constructing robust latent graphs to enhance semi-supervised learning techniques. Our method achieves state-of-the-art results on three biomedical datasets.
    
[^19]: 在约束强化学习中可以证明的高效探索：后验抽样就足够了

    Provably Efficient Exploration in Constrained Reinforcement Learning:Posterior Sampling Is All You Need. (arXiv:2309.15737v1 [cs.LG])

    [http://arxiv.org/abs/2309.15737](http://arxiv.org/abs/2309.15737)

    该论文提出了一种基于后验抽样的算法，用于约束强化学习中的无限期无折扣马尔可夫决策过程。该算法在遗憾界限上接近最优，并在实证上表现出优势。

    

    我们提出了一种基于后验抽样的新算法，用于无限期无折扣约束马尔可夫决策过程（CMDP）中的学习。该算法在实证上相对于现有算法具有优势，同时实现了近似最优的遗憾界限。我们的主要理论结果是对于任意通信CMDP，每个成本部分的贝叶斯遗憾界限为\tilde{O} (HS \sqrt{AT})，其中CMDP具有S个状态、A个行动和命中时间H的边界。这个遗憾界限与时间跨度T的下界匹配，并且是无限期无折扣设置中通信CMDP的已知最佳遗憾界限。实验结果表明，尽管我们的后验抽样算法非常简单，但在约束强化学习中胜过现有算法。

    We present a new algorithm based on posterior sampling for learning in constrained Markov decision processes (CMDP) in the infinite-horizon undiscounted setting. The algorithm achieves near-optimal regret bounds while being advantageous empirically compared to the existing algorithms. Our main theoretical result is a Bayesian regret bound for each cost component of \tilde{O} (HS \sqrt{AT}) for any communicating CMDP with S states, A actions, and bound on the hitting time H. This regret bound matches the lower bound in order of time horizon T and is the best-known regret bound for communicating CMDPs in the infinite-horizon undiscounted setting. Empirical results show that, despite its simplicity, our posterior sampling algorithm outperforms the existing algorithms for constrained reinforcement learning.
    
[^20]: 基于深度学习的吸引盆分析

    Deep Learning-based Analysis of Basins of Attraction. (arXiv:2309.15732v1 [cs.LG])

    [http://arxiv.org/abs/2309.15732](http://arxiv.org/abs/2309.15732)

    本研究展示了基于深度学习的卷积神经网络方法在表征各种动力系统的吸引盆的复杂性和不可预测性方面的有效性，相比传统方法，该方法具有更低的计算成本且表现更好。

    

    本研究展示了卷积神经网络（CNN）在表征不同动力系统吸引盆的复杂性和不可预测性方面的有效性。这种新颖的方法在探索动力系统的不同参数方面是最优的，因为传统方法在表征多个吸引盆时计算成本很高。此外，我们的研究还比较了不同CNN体系结构在这个任务中的表现，表明我们提出的特征提取方法相比传统方法具有优势，即使使用过时的体系结构也是如此。

    This study showcases the effectiveness of convolutional neural networks (CNNs) in characterizing the complexity and unpredictability of basins of attraction for diverse dynamical systems. This novel method is optimal for exploring different parameters of dynamical systems since the conventional methods are computationally expensive for characterizing multiple basins of attraction. Additionally, our research includes a comparison of different CNN architectures for this task showing the superiority of our proposed characterization method over the conventional methods, even with obsolete architectures.
    
[^21]: 时间图模型无法捕捉全局时间动态

    Temporal graph models fail to capture global temporal dynamics. (arXiv:2309.15730v1 [cs.IR])

    [http://arxiv.org/abs/2309.15730](http://arxiv.org/abs/2309.15730)

    时间图模型无法捕捉全局时间动态，我们提出了一种"最近流行节点"的基线方法，在时间图基准的中等和大规模数据集上胜过其他方法。我们提出了两个基于Wasserstein距离的度量来量化全局动态。我们展示了标准的负采样评估方法在具有强烈时间动态的数据集上可能不适用，我们还展示了简单的负采样方法可能导致模型退化。我们提出了改进的负采样方案，并证明了它们的有效性。我们还将其与无负采样的非对比训练模型进行了比较。

    

    在动态链接属性预测的背景下，我们分析了最近发布的时间图基准，并提出了一种"最近流行节点"的基线方法，在时间图基准的中等和大规模数据集上胜过其他方法。我们提出了基于Wasserstein距离的两个度量，可以量化数据集的短期和长期全局动态的强度。通过分析我们出乎意料的强大基线，我们展示了标准的负采样评估方法在具有强烈时间动态的数据集上可能不适用。我们还展示了简单的负采样方法在训练过程中可能导致模型退化，导致无法对时间图网络进行排序的预测完全饱和。我们提出了改进的负采样方案用于训练和评估，并证明了它们的有效性。我们还将其与无负采样的非对比训练模型进行了比较。我们的结果表明...

    A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of "recently popular nodes" outperforming other methods on all medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our resul
    
[^22]: 因式分解扩散架构用于无监督图像生成和分割

    Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation. (arXiv:2309.15726v1 [cs.CV])

    [http://arxiv.org/abs/2309.15726](http://arxiv.org/abs/2309.15726)

    我们开发了一种无监督神经网络架构，通过去噪扩散目标训练模型来实现同时生成和分割图像。这种架构通过在输入中划分区域并并行去噪以及合并结果，实现了准确的无监督图像分割和高质量的合成图像生成。

    

    我们开发了一种神经网络架构，以非监督方式作为去噪扩散模型进行训练，同时学习生成和分割图像。学习完全是由去噪扩散目标驱动的，在训练期间没有任何注释或关于区域的先验知识。神经网络架构中的计算瓶颈鼓励去噪网络将输入划分为区域，同时对它们进行去噪，并将结果合并。我们训练的模型通过简单检查其内部预测的分区，生成合成图像和这些图像的语义分割。我们直接将我们的无监督模型应用于通过添加噪声然后去噪来分割真实图像的下游任务，无需任何微调。实验证明，我们的模型在多个数据集上实现了准确的无监督图像分割和高质量的合成图像生成。

    We develop a neural network architecture which, trained in an unsupervised manner as a denoising diffusion model, simultaneously learns to both generate and segment images. Learning is driven entirely by the denoising diffusion objective, without any annotation or prior knowledge about regions during training. A computational bottleneck, built into the neural architecture, encourages the denoising network to partition an input into regions, denoise them in parallel, and combine the results. Our trained model generates both synthetic images and, by simple examination of its internal predicted partitions, a semantic segmentation of those images. Without any finetuning, we directly apply our unsupervised model to the downstream task of segmenting real images via noising and subsequently denoising them. Experiments demonstrate that our model achieves accurate unsupervised image segmentation and high-quality synthetic image generation across multiple datasets.
    
[^23]: Model Share AI: 一个集成的Python工具包，用于协作式机器学习模型开发、来源追踪和部署

    Model Share AI: An Integrated Toolkit for Collaborative Machine Learning Model Development, Provenance Tracking, and Deployment in Python. (arXiv:2309.15719v1 [cs.SE])

    [http://arxiv.org/abs/2309.15719](http://arxiv.org/abs/2309.15719)

    Model Share AI是一个用于协作式机器学习模型开发、来源追踪和部署的集成工具包，提供了协作项目空间、标准化的模型评估流程和自动化的模型部署功能。

    

    机器学习（ML）有潜力彻底改变许多研究领域和行业，但许多ML项目从概念验证阶段就无法进一步发展。为了解决这个问题，我们引入了Model Share AI（AIMS），这是一个易于使用的MLOps平台，旨在简化协作模型开发、模型来源追踪和模型部署，以及一系列其他功能，以最大化ML研究的实际影响。AIMS具有协作项目空间和标准化的模型评估流程，根据模型在未见过的评估数据上的表现对模型提交进行排名，实现了协作模型开发和众包。自动捕获模型性能和各种模型元数据，以促进来源追踪并允许用户学习和借鉴之前的提交。此外，AIMS允许用户将在Scikit-Learn、TensorFlow Keras、PyTorch和ONNX中构建的ML模型部署到实时的REST API和au

    Machine learning (ML) has the potential to revolutionize a wide range of research areas and industries, but many ML projects never progress past the proof-of-concept stage. To address this issue, we introduce Model Share AI (AIMS), an easy-to-use MLOps platform designed to streamline collaborative model development, model provenance tracking, and model deployment, as well as a host of other functions aiming to maximize the real-world impact of ML research. AIMS features collaborative project spaces and a standardized model evaluation process that ranks model submissions based on their performance on unseen evaluation data, enabling collaborative model development and crowd-sourcing. Model performance and various model metadata are automatically captured to facilitate provenance tracking and allow users to learn from and build on previous submissions. Additionally, AIMS allows users to deploy ML models built in Scikit-Learn, TensorFlow Keras, PyTorch, and ONNX into live REST APIs and au
    
[^24]: Timbre-Trap:一种低资源框架用于与乐器无关的音乐转录

    Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription. (arXiv:2309.15717v1 [eess.AS])

    [http://arxiv.org/abs/2309.15717](http://arxiv.org/abs/2309.15717)

    Timbre-Trap是一个低资源框架，将音乐转录和音频重构统一起来，通过利用音高和音色的强分离性，同时估计音高显著度和重构频谱系数，取得优越性能。

    

    近年来，音乐转录的研究主要集中在架构设计和乐器特定数据采集上。由于多样化数据集的不足，进展通常仅限于钢琴转录等单乐器任务。一些研究探索了多乐器转录作为一种增强模型在低资源任务上性能的手段，但这些方法面临着同样的数据可用性问题。我们提出了一种新颖的框架Timbre-Trap，它通过利用音高和音色之间的强分离性将音乐转录和音频重构统一起来。我们训练一个单独的U-Net来同时估计音高显著度和重构复杂的频谱系数，通过一个简单的切换机制在解码阶段选择两者之一的输出。这样，模型学会了产生对应于没有音色的音频的系数，可以被解释为音高显著度。我们证明了该框架能够取得优越的性能。

    In recent years, research on music transcription has focused mainly on architecture design and instrument-specific data acquisition. With the lack of availability of diverse datasets, progress is often limited to solo-instrument tasks such as piano transcription. Several works have explored multi-instrument transcription as a means to bolster the performance of models on low-resource tasks, but these methods face the same data availability issues. We propose Timbre-Trap, a novel framework which unifies music transcription and audio reconstruction by exploiting the strong separability between pitch and timbre. We train a single U-Net to simultaneously estimate pitch salience and reconstruct complex spectral coefficients, selecting between either output during the decoding stage via a simple switch mechanism. In this way, the model learns to produce coefficients corresponding to timbre-less audio, which can be interpreted as pitch salience. We demonstrate that the framework leads to perf
    
[^25]: 最大权重熵

    Maximum Weight Entropy. (arXiv:2309.15704v1 [cs.LG])

    [http://arxiv.org/abs/2309.15704](http://arxiv.org/abs/2309.15704)

    本文提出了在深度学习中使用最大熵原理的最大权重熵方法，通过最大化权重多样性来解决标准方法在超出分布情况下预测多样性缺乏的问题。

    

    本文研究了在深度学习中使用贝叶斯和集成方法进行不确定性量化和超出分布检测的问题。当在超出分布的情况下使用标准方法时，我们观察到预测多样性的缺乏。针对这个问题，本文提出了一个实用的解决方案，认为标准方法在权重空间中采样的“过度约束”导致了权重多样性的缺乏。本文建议采用最大熵原理来解决这个问题，通过最大化权重多样性，描述最大熵权重分布来表示认知不确定性，从而产生与训练观察一致的神经网络。

    This paper deals with uncertainty quantification and out-of-distribution detection in deep learning using Bayesian and ensemble methods. It proposes a practical solution to the lack of prediction diversity observed recently for standard approaches when used out-of-distribution (Ovadia et al., 2019; Liu et al., 2021). Considering that this issue is mainly related to a lack of weight diversity, we claim that standard methods sample in "over-restricted" regions of the weight space due to the use of "over-regularization" processes, such as weight decay and zero-mean centered Gaussian priors. We propose to solve the problem by adopting the maximum entropy principle for the weight distribution, with the underlying idea to maximize the weight diversity. Under this paradigm, the epistemic uncertainty is described by the weight distribution of maximal entropy that produces neural networks "consistent" with the training observations. Considering stochastic neural networks, a practical optimizati
    
[^26]: HyPoradise：基于大语言模型的生成式语音识别的开放基准线

    HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models. (arXiv:2309.15701v1 [cs.CL])

    [http://arxiv.org/abs/2309.15701](http://arxiv.org/abs/2309.15701)

    本文引入了第一个开源基准测试，利用大型语言模型进行自动语音识别错误修正，实现了与人类水平相当的性能，具有重要的实际应用价值。

    

    深度神经网络的进展使得自动语音识别系统在几个公开的干净语音数据集上达到了人类水平。然而，即使是最先进的自动语音识别系统在面对逆境时也会出现性能下降，因为良好训练的声学模型对于语音领域的变异性很敏感，如背景噪声。受到这一观察的启发，我们引入了第一个开源基准测试，利用外部的大型语言模型（LLMs）来进行自动语音识别错误修正，其中N最佳解码假设为真实转录预测提供了有信息量的元素。这种方法与传统的语言模型重评分策略不同，后者只能选择一个候选假设作为最终预测。

    Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the o
    
[^27]: 深度模型融合：一项综述

    Deep Model Fusion: A Survey. (arXiv:2309.15698v1 [cs.LG])

    [http://arxiv.org/abs/2309.15698](http://arxiv.org/abs/2309.15698)

    深度模型融合是一种新兴技术，将多个深度学习模型的参数或预测合并到单个模型中，以实现更好的性能。在大规模深度学习模型上面临许多挑战，如高计算成本、高维参数空间、不同异构模型之间的干扰等。本论文为了更好地了解模型融合方法并推动其发展，提出了一项综合调查，总结了最近的进展。

    

    深度模型融合是一种新兴技术，将多个深度学习模型的参数或预测合并到单个模型中。它结合了不同模型的能力，弥补了单个模型的偏差和误差，从而实现更好的性能。然而，在大规模深度学习模型（如LLMs和基础模型）上进行深度模型融合面临着许多挑战，包括高计算成本、高维参数空间、不同异构模型之间的干扰等。尽管模型融合由于其解决复杂实际任务的潜力而受到广泛关注，但对该技术的完整和详细调查研究仍然不足。因此，为了更好地了解模型融合方法并推动其发展，我们提出了一项综合调查，总结了最近的进展。具体而言，我们将现有的深度模型融合方法分为四个类别：（1）"模式连接"，即将多个模型通过连接方式融合。

    Deep model fusion/merging is an emerging technique that merges the parameters or predictions of multiple deep learning models into a single one. It combines the abilities of different models to make up for the biases and errors of a single model to achieve better performance. However, deep model fusion on large-scale deep learning models (e.g., LLMs and foundation models) faces several challenges, including high computational cost, high-dimensional parameter space, interference between different heterogeneous models, etc. Although model fusion has attracted widespread attention due to its potential to solve complex real-world tasks, there is still a lack of complete and detailed survey research on this technique. Accordingly, in order to understand the model fusion method better and promote its development, we present a comprehensive survey to summarize the recent progress. Specifically, we categorize existing deep model fusion methods as four-fold: (1) "Mode connectivity", which conne
    
[^28]: 差分隐私深度生成建模的统一视角

    A Unified View of Differentially Private Deep Generative Modeling. (arXiv:2309.15696v1 [cs.LG])

    [http://arxiv.org/abs/2309.15696](http://arxiv.org/abs/2309.15696)

    本文提出了差分隐私深度生成建模的统一视角，系统化了方法，为满足差分隐私需求的方法提供了一个联合设计空间。

    

    富饶且广泛的数据源的可用性极大地推动了各个领域中机器学习应用的发展。然而，涉及隐私问题的数据带来了严格的限制，经常禁止数据访问和数据共享。在遵守隐私考虑的前提下克服这些障碍对于涉及隐私敏感数据的实际应用场景中的技术进步至关重要。差分隐私（DP）数据发布提供了一个有力的解决方案，其中只公开发布数据的一种经过净化处理的形式，实现了保护隐私的下游分析和可重复研究。近年来，已提出了各种方法，通过在深度神经网络上进行私有培训，实现隐私保护的高维数据生成。在本文中，我们提出了一种新颖的统一视角，系统化这些方法。我们的视角为系统地衍生满足差分隐私需求的方法提供了一个联合设计空间。

    The availability of rich and vast data sources has greatly advanced machine learning applications in various domains. However, data with privacy concerns comes with stringent regulations that frequently prohibited data access and data sharing. Overcoming these obstacles in compliance with privacy considerations is key for technological progress in many real-world application scenarios that involve privacy sensitive data. Differentially private (DP) data publishing provides a compelling solution, where only a sanitized form of the data is publicly released, enabling privacy-preserving downstream analysis and reproducible research in sensitive domains. In recent years, various approaches have been proposed for achieving privacy-preserving high-dimensional data generation by private training on top of deep neural networks. In this paper, we present a novel unified view that systematizes these approaches. Our view provides a joint design space for systematically deriving methods that cater
    
[^29]: 打破NoC匿名性使用流相关攻击

    Breaking NoC Anonymity using Flow Correlation Attack. (arXiv:2309.15687v1 [cs.CR])

    [http://arxiv.org/abs/2309.15687](http://arxiv.org/abs/2309.15687)

    本文研究了NoC架构中现有匿名路由协议的安全性，并展示了现有的匿名路由对基于机器学习的流相关攻击易受攻击。我们提出了一种轻量级的匿名路由，使用流量混淆技术，可以抵御基于机器学习的流相关攻击。

    

    网络片上互连（NoC）广泛用作当今多核片上系统（SoC）设计中的内部通信结构。片上通信的安全性至关重要，因为利用共享的NoC中的任何漏洞对攻击者来说都是一个富矿。NoC安全依赖于对各种攻击的有效防范措施。我们研究了NoC架构中现有匿名路由协议的安全性。具体而言，本文作出了两个重要贡献。我们展示了现有的匿名路由对基于机器学习（ML）的流相关攻击是易受攻击的。我们提出了一种轻量级的匿名路由，使用流量混淆技术，可以抵御基于ML的流相关攻击。使用实际和合成流量进行的实验研究表明，我们提出的攻击能够成功地对抗NoC架构中最先进的匿名路由，对于多种流量模式的分类准确率高达99％，同时。

    Network-on-Chip (NoC) is widely used as the internal communication fabric in today's multicore System-on-Chip (SoC) designs. Security of the on-chip communication is crucial because exploiting any vulnerability in shared NoC would be a goldmine for an attacker. NoC security relies on effective countermeasures against diverse attacks. We investigate the security strength of existing anonymous routing protocols in NoC architectures. Specifically, this paper makes two important contributions. We show that the existing anonymous routing is vulnerable to machine learning (ML) based flow correlation attacks on NoCs. We propose a lightweight anonymous routing that use traffic obfuscation techniques which can defend against ML-based flow correlation attacks. Experimental studies using both real and synthetic traffic reveal that our proposed attack is successful against state-of-the-art anonymous routing in NoC architectures with a high accuracy (up to 99%) for diverse traffic patterns, while o
    
[^30]: 逆渲染的联合采样与优化

    Joint Sampling and Optimisation for Inverse Rendering. (arXiv:2309.15676v1 [cs.GR])

    [http://arxiv.org/abs/2309.15676](http://arxiv.org/abs/2309.15676)

    该论文提出了逆渲染的联合采样与优化方法，通过交替采样和优化以减小方差，使用有限差分估计器更新和重复使用过去的样本，在与Adam相结合的情况下实现了稳定的优化过程，加快了困难优化问题的收敛速度。

    

    在处理逆渲染等困难逆问题时，使用Monte Carlo估计梯度来优化参数可能会因方差而导致收敛速度变慢。每次迭代中平均多个梯度样本可以简单地减小这种方差。然而，对于需要进行数千次优化迭代的问题，这种方法的计算成本会迅速上升。我们推导了一个理论框架来交替进行采样和优化。我们使用低方差有限差分估计器来更新和重复使用过去的样本，描述了每次迭代之间估计梯度的变化。通过结合比例和有限差分样本，我们在整个优化过程中不断减小了我们的新颖梯度元估计器的方差。我们研究了我们的估计器如何与Adam相互关联，并推导出一个稳定的组合。我们实现了逆路径跟踪的方法，并展示了我们的估计器如何加速困难优化问题的收敛速度。

    When dealing with difficult inverse problems such as inverse rendering, using Monte Carlo estimated gradients to optimise parameters can slow down convergence due to variance. Averaging many gradient samples in each iteration reduces this variance trivially. However, for problems that require thousands of optimisation iterations, the computational cost of this approach rises quickly.  We derive a theoretical framework for interleaving sampling and optimisation. We update and reuse past samples with low-variance finite-difference estimators that describe the change in the estimated gradients between each iteration. By combining proportional and finite-difference samples, we continuously reduce the variance of our novel gradient meta-estimators throughout the optimisation process. We investigate how our estimator interlinks with Adam and derive a stable combination.  We implement our method for inverse path tracing and demonstrate how our estimator speeds up convergence on difficult opti
    
[^31]: 语音拼贴：通过拼接单语语料生成混合语音

    Speech collage: code-switched audio generation by collaging monolingual corpora. (arXiv:2309.15674v1 [cs.SD])

    [http://arxiv.org/abs/2309.15674](http://arxiv.org/abs/2309.15674)

    本文提出了一种通过拼接单语语料生成混合语音的方法，可以解决混合语数据稀缺的问题。实证结果表明，生成的混合语音数据可以显著提高语音识别的准确率，并减少模型对单语的偏好。

    

    设计有效的用于混合语言的自动语音识别（ASR）系统往往取决于可获得的混合语资源的有效性。为了解决数据稀缺的问题，本文引入了语音拼贴方法，通过拼接音频片段从单语语料中合成混合语数据。我们进一步通过重叠添加的方法提高了音频生成的平滑度。我们研究了生成数据对两种情况下的语音识别的影响：使用领域内混合语文本和使用合成的混合语文本的零样本方法。实证结果显示，在领域内和零样本情况下，混合错误率和词错误率分别相对减少了34.4%和16.2%。最后，我们证明了混合语言增强了模型的混合倾向并减少了单语倾向。

    Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.
    
[^32]: MONOVAB: 用于孟加拉语多标签情感检测的注释语料库

    MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection. (arXiv:2309.15670v1 [cs.LG])

    [http://arxiv.org/abs/2309.15670](http://arxiv.org/abs/2309.15670)

    这个研究构建了一个基于孟加拉语的注释语料库，用于多标签情感检测。通过使用基于上下文的方法以及BERT模型，填补了这一学科领域的空白。

    

    近年来，情感分析(SA)和情感识别(ER)在孟加拉语中越来越流行，孟加拉语是世界上第七大使用人数最多的语言。然而，孟加拉语的结构复杂，这使得准确提取情绪变得困难。在这个研究领域中，已经采用了一些不同的方法，例如提取积极和消极情感以及多类情绪。然而，在这种语言中提取多种情绪几乎是未开发的领域，它涉及基于一段文本识别出多种情感。因此，本研究展示了一种基于从Facebook上抓取的数据构建注释语料库的详细方法，以填补这个学科领域的空白，克服挑战。为了使这种注释更有成果，采用了基于上下文的方法。转换器中的双向编码器表示(BERT)。

    In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT),
    
[^33]: 关于计算纠缠及其在对抗机器学习中的解释

    On Computational Entanglement and Its Interpretation in Adversarial Machine Learning. (arXiv:2309.15669v1 [cs.LG])

    [http://arxiv.org/abs/2309.15669](http://arxiv.org/abs/2309.15669)

    本研究探索了对抗机器学习模型的复杂性和可解释性，通过将其与爱因斯坦的特殊相对论中的纠缠概念联系起来，发现远程特征样本可以表现出纠缠现象，挑战了对抗可传递性现象的传统描述方法。

    

    由于对抗性样本在机器学习中具有欺骗模型的能力，潜在地导致严重后果，因此已成为研究的焦点。在本研究中，我们对对抗机器学习模型进行了全面探索，揭示了它们固有的复杂性和可解释性。我们的调查揭示了机器学习模型复杂性与爱因斯坦的特殊相对论之间的有趣联系，通过纠缠的概念。更具体地说，我们对计算纠缠进行了定义，并证明了远程特征样本可以表现出强相关性，类似于量子领域中的纠缠。这一发现挑战了对当代机器学习模型中观察到的对抗可传递性现象的传统描述方法。

    Adversarial examples in machine learning has emerged as a focal point of research due to their remarkable ability to deceive models with seemingly inconspicuous input perturbations, potentially resulting in severe consequences. In this study, we embark on a comprehensive exploration of adversarial machine learning models, shedding light on their intrinsic complexity and interpretability. Our investigation reveals intriguing links between machine learning model complexity and Einstein's theory of special relativity, through the concept of entanglement. More specific, we define entanglement computationally and demonstrate that distant feature samples can exhibit strong correlations, akin to entanglement in quantum realm. This revelation challenges conventional perspectives in describing the phenomenon of adversarial transferability observed in contemporary machine learning models. By drawing parallels with the relativistic effects of time dilation and length contraction during computatio
    
[^34]: 联邦深度平衡学习：边缘通信效率的紧凑共享表示

    Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency. (arXiv:2309.15659v1 [cs.LG])

    [http://arxiv.org/abs/2309.15659](http://arxiv.org/abs/2309.15659)

    FeDEQ是一个联邦学习框架，采用深度平衡学习和共识优化，通过紧凑的共享数据表示在边缘节点之间共享模型，解决了深度联邦学习在边缘环境中的通信瓶颈和数据异构性问题。

    

    联邦学习是一种卓越的分布式学习范式，促进了边缘网络节点之间的协作，以在不集中数据的情况下共同训练全局模型。通过将计算转移到网络边缘，联邦学习提供了鲁棒和响应迅速的边缘人工智能解决方案，并增强了隐私保护。然而，在边缘环境中部署深度联邦学习模型通常受到通信瓶颈、数据异构性和内存限制的阻碍。为了共同解决这些挑战，我们引入了FeDEQ，这是一个开创性的联邦学习框架，它有效地采用深度平衡学习和共识优化，在边缘节点之间利用紧凑的共享数据表示，允许派生出针对每个节点特定的个性化模型。我们深入探讨了一个独特的模型结构，由一个平衡层和传统神经网络层组成。在这里，平衡层充当全局特征表示，边缘节点可以根据自己的需求进行个性化调整。

    Federated Learning (FL) is a prominent distributed learning paradigm facilitating collaboration among nodes within an edge network to co-train a global model without centralizing data. By shifting computation to the network edge, FL offers robust and responsive edge-AI solutions and enhance privacy-preservation. However, deploying deep FL models within edge environments is often hindered by communication bottlenecks, data heterogeneity, and memory limitations. To address these challenges jointly, we introduce FeDEQ, a pioneering FL framework that effectively employs deep equilibrium learning and consensus optimization to exploit a compact shared data representation across edge nodes, allowing the derivation of personalized models specific to each node. We delve into a unique model structure composed of an equilibrium layer followed by traditional neural network layers. Here, the equilibrium layer functions as a global feature representation that edge nodes can adapt to personalize thei
    
[^35]: 用大型语言模型进行生成式语音识别错误校正

    Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])

    [http://arxiv.org/abs/2309.15649](http://arxiv.org/abs/2309.15649)

    本研究探讨了大型语言模型（LLMs）作为ASR后处理器的能力，通过重新评分和错误校正来提高系统性能。通过使用指令提示和任务激活提示方法，结合上下文学习和微调技术，我们展示了LLMs的泛化能力和有效性。

    

    我们研究了大型语言模型（LLM）作为ASR后处理器的能力，用于重新评分和错误校正。我们的重点是使用指令提示让LLMs执行这些任务而无需微调，我们评估了不同的提示方案，包括零-shot和少-shot的上下文学习，以及一种新颖的任务激活提示（TAP）方法，结合指令和演示。通过在两个领域之外的任务（ATIS和WSJ）上使用预先训练的第一次扫描系统和重新评分输出，我们证明仅通过冻结LLMs的上下文学习进行重新评分可以达到与领域调优的LMs重新评分相竞争的结果。通过将提示技术与微调相结合，我们实现了低于N-best Oracle水平的错误率，展示了LLMs的泛化能力。

    We explore the ability of large language models (LLMs) to act as ASR post-processors that perform rescoring and error correction. Our focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task-activating prompting (TAP) method that combines instruction and demonstration. Using a pre-trained first-pass system and rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs. By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.
    
[^36]: SANGEA：可扩展和带属性的网络生成

    SANGEA: Scalable and Attributed Network Generation. (arXiv:2309.15648v1 [cs.LG])

    [http://arxiv.org/abs/2309.15648](http://arxiv.org/abs/2309.15648)

    本文提出了SANGEA，一种可扩展的合成图生成框架，通过将大图分解为社区，每个社区训练一个合成图生成器，然后将社区图链接在一起生成一个类似原始图的大型合成图。

    

    最近，由于生成建模的最新突破，合成图生成器（SGGs）的话题受到了很多关注。然而，许多最先进的SGGs在图的大小方面不具有良好的可扩展性。事实上，在生成过程中，通常需要考虑固定节点数的所有可能边，这会以$\mathcal{O}(N^2)$的规模增长，其中$N$是图中的节点数。因此，许多最先进的SGGs不适用于大型图。本文介绍了SANGEA，这是一个可扩展的合成图生成框架，它将任何SGG的适用范围扩展到大型图。通过首先将大图分成社区，SANGEA对每个社区训练一个SGG，然后将社区图链接在一起，创建一个合成的大图。我们的实验证明，SANGEA生成的图在拓扑和节点特征分布方面与原始图非常相似。此外，这些生成的图具有良好的可扩展性和属性。

    The topic of synthetic graph generators (SGGs) has recently received much attention due to the wave of the latest breakthroughs in generative modelling. However, many state-of-the-art SGGs do not scale well with the graph size. Indeed, in the generation process, all the possible edges for a fixed number of nodes must often be considered, which scales in $\mathcal{O}(N^2)$, with $N$ being the number of nodes in the graph. For this reason, many state-of-the-art SGGs are not applicable to large graphs. In this paper, we present SANGEA, a sizeable synthetic graph generation framework which extends the applicability of any SGG to large graphs. By first splitting the large graph into communities, SANGEA trains one SGG per community, then links the community graphs back together to create a synthetic large graph. Our experiments show that the graphs generated by SANGEA have high similarity to the original graph, in terms of both topology and node feature distribution. Additionally, these gene
    
[^37]: 冷启动和热启动网络：解决推荐系统中的冷启动用户问题

    Cold & Warm Net: Addressing Cold-Start Users in Recommender Systems. (arXiv:2309.15646v1 [cs.IR])

    [http://arxiv.org/abs/2309.15646](http://arxiv.org/abs/2309.15646)

    本文提出了一种名为冷启动和热启动网络的方法(Cold & Warm Net)，用于解决推荐系统中的冷启动用户问题。该方法利用专家模型分别建模冷启动和热启动用户，并引入门控网络和动态知识蒸馏来提高用户表示的学习效果。通过选择与用户行为高度相关的特征，并建立偏差网络来显式建模用户行为偏差。实验证实了该方法的有效性。

    

    冷启动推荐是推荐系统面临的重大挑战之一。本文主要关注用户冷启动问题。最近，使用侧信息或元学习的方法被用来建模冷启动用户。然而，将这些方法应用于工业级推荐系统仍然存在困难。目前对于匹配阶段中的用户冷启动问题的研究还不多。本文提出了基于专家模型的冷启动和热启动用户建模方法：Cold & Warm Net。通过引入门控网络来结合两个专家的结果。此外，还引入了动态知识蒸馏作为一个教师选择器，帮助专家更好地学习用户表示。通过全面的互信息选择与用户行为高度相关的特征，用于显式建模用户行为偏差的偏差网络。最后，在公共数据集上对Cold & Warm Net进行评估。

    Cold-start recommendation is one of the major challenges faced by recommender systems (RS). Herein, we focus on the user cold-start problem. Recently, methods utilizing side information or meta-learning have been used to model cold-start users. However, it is difficult to deploy these methods to industrial RS. There has not been much research that pays attention to the user cold-start problem in the matching stage. In this paper, we propose Cold & Warm Net based on expert models who are responsible for modeling cold-start and warm-up users respectively. A gate network is applied to incorporate the results from two experts. Furthermore, dynamic knowledge distillation acting as a teacher selector is introduced to assist experts in better learning user representation. With comprehensive mutual information, features highly relevant to user behavior are selected for the bias net which explicitly models user behavior bias. Finally, we evaluate our Cold & Warm Net on public datasets in compar
    
[^38]: 为什么角边距损失在半监督异常声音检测中表现出色？

    Why do Angular Margin Losses work well for Semi-Supervised Anomalous Sound Detection?. (arXiv:2309.15643v1 [eess.AS])

    [http://arxiv.org/abs/2309.15643](http://arxiv.org/abs/2309.15643)

    角边距损失与辅助任务结合在半监督异常声音检测中表现出色，通过最小化角边距损失同时达到最小化紧凑性损失和防止学习平凡解的效果。

    

    最先进的异常声音检测系统通常利用角边距损失来通过一个辅助任务学习合适的声学数据表示，该任务通常是一个监督或自监督分类任务。其基本思想是为了解决这个辅助任务，需要在学习的表示中捕捉到关于正常数据的特定信息，并且这些信息也足以区分正常和异常样本。特别是在噪声条件下，基于角边距损失的判别模型往往明显优于基于生成模型或单类模型的系统。本研究的目标是调查为什么在辅助任务中使用角边距损失对于检测异常声音效果良好。通过理论和实验证明，最小化角边距损失也最小化了紧凑性损失，同时固有地防止学习平凡的解。此外，m

    State-of-the-art anomalous sound detection systems often utilize angular margin losses to learn suitable representations of acoustic data using an auxiliary task, which usually is a supervised or self-supervised classification task. The underlying idea is that, in order to solve this auxiliary task, specific information about normal data needs to be captured in the learned representations and that this information is also sufficient to differentiate between normal and anomalous samples. Especially in noisy conditions, discriminative models based on angular margin losses tend to significantly outperform systems based on generative or one-class models. The goal of this work is to investigate why using angular margin losses with auxiliary tasks works well for detecting anomalous sounds. To this end, it is shown, both theoretically and experimentally, that minimizing angular margin losses also minimizes compactness loss while inherently preventing learning trivial solutions. Furthermore, m
    
[^39]: IBM最大量子处理器的张量网络模拟的高效方法

    Efficient tensor network simulation of IBM's largest quantum processors. (arXiv:2309.15642v1 [quant-ph])

    [http://arxiv.org/abs/2309.15642](http://arxiv.org/abs/2309.15642)

    本文展示了如何使用量子启发的二维张量网络高效模拟IBM最大的量子处理器，通过简单的张量更新实现前所未有的准确度和极低的计算资源消耗，并为最新的IBM量子机器设立了基准。

    

    我们展示了如何使用量子启发的二维张量网络来高效准确地模拟IBM最大的量子处理器，即Eagle（127个量子比特），Osprey（433个量子比特）和Condor（1121个量子比特）。我们使用基于图的投影纠缠对态（gPEPS）模拟了一个复杂的量子多体系统的动力学，具体来说，这是IBM最近在Nature 618年第500-505页（2023年）上考虑的踢击易辛实验-我们在PRB 99, 195105（2019年）中提出了这个模型。我们的结果表明，对于该模型，简单的张量更新已经足以以极低的计算资源实现非常大的前所未有的精度。除了模拟127个量子比特的原始实验外，我们还将结果扩展到433个和1121个量子比特，从而为最新的IBM量子机器设定了一个基准。我们还报道了无限多个量子比特的准确模拟。我们的结果表明，gPEPS是高效模拟量子计算机的自然工具。

    We show how quantum-inspired 2d tensor networks can be used to efficiently and accurately simulate the largest quantum processors from IBM, namely Eagle (127 qubits), Osprey (433 qubits) and Condor (1121 qubits). We simulate the dynamics of a complex quantum many-body system -- specifically, the kicked Ising experiment considered recently by IBM in Nature 618, p. 500-505 (2023) -using graph-based Projected Entangled Pair States (gPEPS), which was proposed by some of us in PRB 99, 195105 (2019). Our results show that simple tensor updates are already sufficient to achieve very large unprecedented accuracy with remarkably low computational resources for this model. Apart from simulating the original experiment for 127 qubits, we also extend our results to 433 and 1121 qubits, thus setting a benchmark for the newest IBM quantum machines. We also report accurate simulations for infinitely-many qubits. Our results show that gPEPS are a natural tool to efficiently simulate quantum computer
    
[^40]: 用长短期记忆和时间序列模型进行算法投资策略对冲的对冲特性

    Hedging Properties of Algorithmic Investment Strategies using Long Short-Term Memory and Time Series models for Equity Indices. (arXiv:2309.15640v1 [q-fin.PM])

    [http://arxiv.org/abs/2309.15640](http://arxiv.org/abs/2309.15640)

    本文提出了一种使用长短期记忆和时间序列模型构建算法投资策略的对冲方法，并通过利用不同类型的投资策略来对冲风险资产组合。实证结果显示，该方法在金融市场的动荡时期具有多样化的潜力。

    

    本文提出了一种在金融市场受金融动荡影响时对冲风险资产组合的新方法。我们引入了一种全新的多元算法投资策略（AIS）的分散化方法，该方法不是在单个资产的级别上进行，而是在基于这些资产的价格的级别上进行。我们采用四种不同的理论模型（LSTM - 长短期记忆、ARIMA-GARCH - 自回归移动平均 - 广义自回归条件异方差、动量和反向交易）来生成价格预测，然后利用这些预测产生单个和复合的AIS的投资信号。通过这种方式，我们能够验证由各种资产（能源商品、贵金属、加密货币或软商品）组成的不同类型的投资策略在对冲用于股票指数（S&P 500指数）的组合AIS中的多样化潜力。

    This paper proposes a novel approach to hedging portfolios of risky assets when financial markets are affected by financial turmoils. We introduce a completely novel approach to diversification activity not on the level of single assets but on the level of ensemble algorithmic investment strategies (AIS) built based on the prices of these assets. We employ four types of diverse theoretical models (LSTM - Long Short-Term Memory, ARIMA-GARCH Autoregressive Integrated Moving Average - Generalized Autoregressive Conditional Heteroskedasticity, momentum, and contrarian) to generate price forecasts, which are then used to produce investment signals in single and complex AIS. In such a way, we are able to verify the diversification potential of different types of investment strategies consisting of various assets (energy commodities, precious metals, cryptocurrencies, or soft commodities) in hedging ensemble AIS built for equity indices (S&P 500 index). Empirical data used in this study cov
    
[^41]: 通过方差抑制增强锐度感知优化

    Enhancing Sharpness-Aware Optimization Through Variance Suppression. (arXiv:2309.15639v1 [cs.LG])

    [http://arxiv.org/abs/2309.15639](http://arxiv.org/abs/2309.15639)

    本文通过方差抑制的方法（VaSSO）增强了锐度感知最小化（SAM）的优化算法，提高了深度神经网络的泛化能力，特别适用于模型无关任务和对高水平标签噪声具有鲁棒性的情况。

    

    锐度感知最小化（SAM）在增强深度神经网络的泛化能力方面有着良好的记录，即使没有大规模的数据增强。SAM借助损失函数的几何特性，通过最小化在邻域内参数对敌对扰动引起的最大损失，寻找“平坦最小值”所在的“平坦山谷”，提高泛化能力。尽管考虑了损失函数的锐度是至关重要的，但这种“过于友好的敌对者”可能会限制泛化的最高水平。本文的新方法通过方差抑制（VaSSO）来稳定敌对者，避免这种友好性。 VaSSO的稳定性可证明，并在模型无关任务中（包括图像分类和机器翻译）相对于SAM有着数值上的改进。此外，实验证实VaSSO赋予SAM对高水平标签噪声的鲁棒性。

    Sharpness-aware minimization (SAM) has well documented merits in enhancing generalization of deep neural networks, even without sizable data augmentation. Embracing the geometry of the loss function, where neighborhoods of 'flat minima' heighten generalization ability, SAM seeks 'flat valleys' by minimizing the maximum loss caused by an adversary perturbing parameters within the neighborhood. Although critical to account for sharpness of the loss function, such an 'over-friendly adversary' can curtail the outmost level of generalization. The novel approach of this contribution fosters stabilization of adversaries through variance suppression (VaSSO) to avoid such friendliness. VaSSO's provable stability safeguards its numerical improvement over SAM in model-agnostic tasks, including image classification and machine translation. In addition, experiments confirm that VaSSO endows SAM with robustness against high levels of label noise.
    
[^42]: FRS-Nets: Fourier参数化的旋转和尺度等变网络用于视网膜血管分割

    FRS-Nets: Fourier Parameterized Rotation and Scale Equivariant Networks for Retinal Vessel Segmentation. (arXiv:2309.15638v1 [eess.IV])

    [http://arxiv.org/abs/2309.15638](http://arxiv.org/abs/2309.15638)

    本研究提出了一种名为FRS-Nets的新型卷积神经网络方法，利用傅里叶参数化实现了对旋转和尺度的等变性，从而提升了视网膜血管分割的准确性。通过在U-Net和Iter-Net中替换传统卷积滤波器，实现了更好的分割效果。

    

    具有平移等变性的卷积神经网络（CNNs）在视网膜血管分割中取得了巨大的成功。然而，CNNs没有对血管形态的其他对称性进行建模，例如旋转和尺度对称性。为了在CNNs中嵌入更多等变性并满足视网膜血管分割的准确性要求，我们构建了一种新颖的卷积算子（FRS-Conv），它是傅里叶参数化的，并且对旋转和缩放等变。具体地，我们首先采用一种新的参数化方案，使卷积滤波器能够以高精度任意进行变换。其次，我们导出了旋转和尺度等变卷积映射的公式。最后，我们根据提出的公式构建了FRS-Conv，并将U-Net和Iter-Net中的传统卷积滤波器替换为FRS-Conv（FRS-Nets）。我们忠实地复现了所有对比方法，并在三个公共数据集上进行了全面实验。

    With translation equivariance, convolution neural networks (CNNs) have achieved great success in retinal vessel segmentation. However, some other symmetries of the vascular morphology are not characterized by CNNs, such as rotation and scale symmetries. To embed more equivariance into CNNs and achieve the accuracy requirement for retinal vessel segmentation, we construct a novel convolution operator (FRS-Conv), which is Fourier parameterized and equivariant to rotation and scaling. Specifically, we first adopt a new parameterization scheme, which enables convolutional filters to arbitrarily perform transformations with high accuracy. Secondly, we derive the formulations for the rotation and scale equivariant convolution mapping. Finally, we construct FRS-Conv following the proposed formulations and replace the traditional convolution filters in U-Net and Iter-Net with FRS-Conv (FRS-Nets). We faithfully reproduce all compared methods and conduct comprehensive experiments on three public
    
[^43]: 发展国际多语种会议的自动逐字转录：一种端到端的解决方案

    Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution. (arXiv:2309.15609v1 [cs.CL])

    [http://arxiv.org/abs/2309.15609](http://arxiv.org/abs/2309.15609)

    本文提出了一种端到端的解决方案，用于创建全自动的会议记录和多语言翻译，解决了会议管理文档中现有工作流程的替代和改善问题。

    

    本文提出了一种完整的端到端解决方案，用于创建全自动的会议记录和对它们进行多种语言的机器翻译。该工具是在世界知识产权组织（WIPO）开发的、使用其内部开发的语音转文本（S2T）和机器翻译（MT）组件的系统。除了描述数据收集和优化过程，生成高度定制和稳健的系统外，本文还描述了技术组件的架构和演变，并突出了用户方面的商业影响和收益。同时，我们还指出了系统在演进和采用过程中的特殊挑战，并介绍了这种新方法如何创造了一种新产品，并取代了会议管理文档中现有的工作流程。

    This paper presents an end-to-end solution for the creation of fully automated conference meeting transcripts and their machine translations into various languages. This tool has been developed at the World Intellectual Property Organization (WIPO) using in-house developed speech-to-text (S2T) and machine translation (MT) components. Beyond describing data collection and fine-tuning, resulting in a highly customized and robust system, this paper describes the architecture and evolution of the technical components as well as highlights the business impact and benefits from the user side. We also point out particular challenges in the evolution and adoption of the system and how the new approach created a new product and replaced existing established workflows in conference management documentation.
    
[^44]: NoSENSE：学习无需显式灵敏度图的展开心脏MRI重建方法

    NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit sensitivity maps. (arXiv:2309.15608v1 [eess.IV])

    [http://arxiv.org/abs/2309.15608](http://arxiv.org/abs/2309.15608)

    本文提出了一种无需显式灵敏度图的展开心脏MRI重建方法，使用深度卷积神经网络和算法展开，通过学习图像之间的接收线圈关系来实现加速心脏MRI重建，在实验中取得了较好的性能。

    

    本文提出了一种新颖的学习图像重建方法，适用于基于多个接收线圈的加速心脏MRI。该方法基于深度卷积神经网络（CNN）和算法展开。与许多现有的学习MR图像重建技术不同，需要将灵敏度映射（CSM）估计作为一个独立的网络组件，我们提出的方法避免了显式的CSM估计。相反，它隐含地捕捉并学习利用图像之间的接收线圈关系。我们的方法由一系列新颖的学习图像块和k空间块组成，共享潜在信息，并通过特征调节和接收线圈数据一致性实现对采集参数的适应性。在MICCAI STACOM CMRxRecon挑战赛的影片追踪和映射追踪验证排行榜中，我们的方法分别在PSNR值上达到了34.89和35.56，SSIM值分别为0.920和0.942，在撰写本文时位列不同小组第4位。

    We present a novel learned image reconstruction method for accelerated cardiac MRI with multiple receiver coils based on deep convolutional neural networks (CNNs) and algorithm unrolling. In contrast to many existing learned MR image reconstruction techniques that necessitate coil-sensitivity map (CSM) estimation as a distinct network component, our proposed approach avoids explicit CSM estimation. Instead, it implicitly captures and learns to exploit the inter-coil relationships of the images. Our method consists of a series of novel learned image and k-space blocks with shared latent information and adaptation to the acquisition parameters by feature-wise modulation (FiLM), as well as coil-wise data-consistency (DC) blocks.  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920 and 0.942 in the cine track and mapping track validation leaderboard of the MICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different teams at the time of writing.  Cod
    
[^45]: Entropic Matching用于Markov跳跃过程的期望传播的熵匹配

    Entropic Matching for Expectation Propagation of Markov Jump Processes. (arXiv:2309.15604v1 [cs.LG])

    [http://arxiv.org/abs/2309.15604](http://arxiv.org/abs/2309.15604)

    本文提出了一个基于熵匹配框架的新的可处理的推断方案，可以嵌入到期望传播算法中，对于描述离散状态空间过程的Markov跳跃过程的统计推断问题具有重要意义。我们展示了我们方法的有效性，并通过提供一类近似分布的闭式结果以及应用于化学反应网络的一般类别来加以论证。此外，我们通过一个近似的期望最大化程序导出了潜在参数的点估计的闭式表达式，并在各种化学反应网络示例中评估了我们的方法的性能。我们还讨论了该方法的局限性和未来的潜力。

    

    本文解决了潜在连续时间随机过程的统计推断问题，该问题通常难以处理，特别是对于由Markov跳跃过程描述的离散状态空间过程。为了克服这个问题，我们提出了一种新的可处理的推断方案，基于熵匹配框架，可以嵌入到众所周知的期望传播算法中。我们通过为一类简单的近似分布提供闭式结果，并将其应用于化学反应网络的一般类别，该类别是系统生物学建模的重要工具，来证明我们方法的有效性。此外，我们使用近似的期望最大化程序导出了潜在参数的点估计的闭式表达式。我们评估了我们方法在各种化学反应网络示例中的性能，包括随机的Lotka-Voltera示例，并讨论了它的局限性和未来的潜力。

    This paper addresses the problem of statistical inference for latent continuous-time stochastic processes, which is often intractable, particularly for discrete state space processes described by Markov jump processes. To overcome this issue, we propose a new tractable inference scheme based on an entropic matching framework that can be embedded into the well-known expectation propagation algorithm. We demonstrate the effectiveness of our method by providing closed-form results for a simple family of approximate distributions and apply it to the general class of chemical reaction networks, which are a crucial tool for modeling in systems biology. Moreover, we derive closed form expressions for point estimation of the underlying parameters using an approximate expectation maximization procedure. We evaluate the performance of our method on various chemical reaction network instantiations, including a stochastic Lotka-Voltera example, and discuss its limitations and potential for future 
    
[^46]: 在多任务强化学习中，通过最优输运正则化来提取知识

    Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport Regularization. (arXiv:2309.15603v1 [cs.LG])

    [http://arxiv.org/abs/2309.15603](http://arxiv.org/abs/2309.15603)

    本论文提出了一种在多任务强化学习中使用最优输运正则化来提取知识的方法，通过近似计算任务状态分布之间的最优输运距离，并将其作为奖励来规范信息分享的量。实验证明这种方法能够显著加速代理的学习过程并超越传统方法。

    

    在多任务强化学习中，通过从其他不同但相关的任务中转移知识，可以提高训练代理的数据效率。传统方法依赖于Kullback-Leibler正则化来稳定从一个任务到其他任务的知识转移。本文中，我们探索了用一种新颖的基于最优输运的正则化来替代Kullback-Leibler散度的方向。通过使用Sinkhorn映射，我们可以近似计算任务的状态分布之间的最优输运距离。然后将该距离作为分摊奖励，用来规范信息共享的量。我们在几个基于栅格的导航多目标环境上进行了实验，验证了这种方法的有效性。结果表明，我们添加的基于最优输运的奖励能够加速代理学习过程并超过了传统方法。

    In multi-task reinforcement learning, it is possible to improve the data efficiency of training agents by transferring knowledge from other different but related tasks. Because the experiences from different tasks are usually biased toward the specific task goals. Traditional methods rely on Kullback-Leibler regularization to stabilize the transfer of knowledge from one task to the others. In this work, we explore the direction of replacing the Kullback-Leibler divergence with a novel Optimal transport-based regularization. By using the Sinkhorn mapping, we can approximate the Optimal transport distance between the state distribution of tasks. The distance is then used as an amortized reward to regularize the amount of sharing information. We experiment our frameworks on several grid-based navigation multi-goal to validate the effectiveness of the approach. The results show that our added Optimal transport-based rewards are able to speed up the learning process of agents and outperform
    
[^47]: OceanBench：海洋表面高度版

    OceanBench: The Sea Surface Height Edition. (arXiv:2309.15599v1 [cs.LG])

    [http://arxiv.org/abs/2309.15599](http://arxiv.org/abs/2309.15599)

    通过机器学习技术利用海洋卫星数据的信息，可以帮助我们更好地理解海洋表面高度变化，从而对人类活动和气候调节产生重要影响。

    

    海洋对人类活动产生了深远影响，并在气候调节中发挥着重要作用。随着卫星遥感数据的出现，我们对海洋的理解在过去几十年里得到了改善，使我们能够捕捉到全球必要的数量，例如海洋表面高度 (SSH)。然而，海洋卫星数据由于稀疏和不规则采样、信号复杂性和噪声等问题，对信息提取构成了挑战。机器学习 (ML) 技术在处理大规模、复杂信号方面展示了其能力。因此，我们认为 ML 模型有机会利用海洋卫星数据中包含的信息。然而，数据表示和相关评估指标可能是确定应用 ML 成功与否的关键因素。从原始观测数据到 ML 可用状态的处理步骤，以及从模型输出到可解释数量的处理步骤，都需要领域专业知识，这可能是对 ML 入门的重要障碍。

    The ocean profoundly influences human activities and plays a critical role in climate regulation. Our understanding has improved over the last decades with the advent of satellite remote sensing data, allowing us to capture essential quantities over the globe, e.g., sea surface height (SSH). However, ocean satellite data presents challenges for information extraction due to their sparsity and irregular sampling, signal complexity, and noise. Machine learning (ML) techniques have demonstrated their capabilities in dealing with large-scale, complex signals. Therefore we see an opportunity for ML models to harness the information contained in ocean satellite data. However, data representation and relevant evaluation metrics can be the defining factors when determining the success of applied ML. The processing steps from the raw observation data to a ML-ready state and from model outputs to interpretable quantities require domain expertise, which can be a significant barrier to entry for M
    
[^48]: 激子极化子库伦凝聚：一种傅里叶神经算子方法

    Exciton-Polariton Condensates: A Fourier Neural Operator Approach. (arXiv:2309.15593v1 [cond-mat.quant-gas])

    [http://arxiv.org/abs/2309.15593](http://arxiv.org/abs/2309.15593)

    本研究首次将傅里叶神经算子方法应用于激子极化子库伦凝聚系统，通过机器学习实现对Gross-Pitaevskii方程的求解，可以以接近1000倍的速度准确预测最终状态的解，为激子极化子库伦凝聚系统的大规模应用提供了新的解决方案。

    

    过去十年中，半导体制造技术的进展催生了对由激子极化子库伦凝聚驱动的全光学器件的广泛研究。包括晶体管在内的这类器件的初步验证已经在环境条件下取得了鼓舞人心的结果。然而，一个重要的挑战仍然存在于大规模应用领域：缺乏一个健壮的求解器，可以用于模拟需要较长时间达到稳定的复杂非线性系统。为了解决这个需求，我们提出了一种基于机器学习的傅里叶神经算子方法，用于求解与额外激子速率方程耦合的Gross-Pitaevskii方程。这项工作标志着神经算子首次直接应用于激子极化子库伦凝聚系统。我们的研究结果表明，与基于CUDA的GPU求解器相比，所提出的方法可以以接近1000倍的速度准确预测最终状态的解。此外，这为今后的深入研究铺平了道路。

    Advancements in semiconductor fabrication over the past decade have catalyzed extensive research into all-optical devices driven by exciton-polariton condensates. Preliminary validations of such devices, including transistors, have shown encouraging results even under ambient conditions. A significant challenge still remains for large scale application however: the lack of a robust solver that can be used to simulate complex nonlinear systems which require an extended period of time to stabilize. Addressing this need, we propose the application of a machine-learning-based Fourier Neural Operator approach to find the solution to the Gross-Pitaevskii equations coupled with extra exciton rate equations. This work marks the first direct application of Neural Operators to an exciton-polariton condensate system. Our findings show that the proposed method can predict final-state solutions to a high degree of accuracy almost 1000 times faster than CUDA-based GPU solvers. Moreover, this paves t
    
[^49]: 共同训练大型自回归多模态模型

    Jointly Training Large Autoregressive Multimodal Models. (arXiv:2309.15564v1 [cs.LG])

    [http://arxiv.org/abs/2309.15564](http://arxiv.org/abs/2309.15564)

    本研究提出了共同训练大型自回归多模态模型的方法，通过模块化的方式融合语言和图像生成模型，同时引入了数据高效的指令调优策略，使得该模型在生成高质量多模态输出方面表现出卓越的性能。

    

    最近几年，语言和文本到图像模型的大规模预训练取得了重大突破，彻底改变了机器学习领域。然而，将这两种模态集成到一个能够生成无缝多模态输出的单一强大模型仍然是一个重大挑战。为了解决这一问题，我们提出了联合自回归混合（JAM）框架，一种系统融合现有文本和图像生成模型的模块化方法。我们还引入了一种专门的、数据高效的指令调优策略，针对混合模态生成任务进行了优化。我们最终的调优模型在生成高质量多模态输出方面表现出无与伦比的性能，并代表了第一个明确为此目的而设计的模型。

    In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.
    
[^50]: 识别性很重要：揭示无偏学习排名中隐藏的可恢复条件

    Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v1 [cs.IR])

    [http://arxiv.org/abs/2309.15560](http://arxiv.org/abs/2309.15560)

    研究揭示在无偏学习排名中，当点击数据不能完全拟合时，无法恢复真实相关性，导致排名性能显著降低，提出了可识别性图模型作为解决方案。

    

    无偏学习排名(Unbiased Learning to Rank, ULTR)在从有偏点击日志训练无偏排名模型的现代系统中被广泛应用。关键在于明确地建模用户行为的生成过程，并基于检验假设对点击数据进行拟合。先前的研究经验性地发现只要点击完全拟合，大多数情况下可以恢复出真实潜在相关性。然而，我们证明并非总是能够实现这一点，从而导致排名性能显著降低。在本工作中，我们旨在回答真实相关性是否能够从点击数据恢复出来的问题，这是ULTR领域的一个基本问题。我们首先将一个排名模型定义为可识别的，如果它可以恢复出真实相关性，最多只有一个缩放变换，这对于成对排名目标来说已足够。然后，我们探讨了一个等价的可识别条件，可以新颖地表达为一个图连通性测试问题：当且仅当一个图（即可识别性图）连通时，该排名模型是可识别的。

    The application of Unbiased Learning to Rank (ULTR) is widespread in modern systems for training unbiased ranking models from biased click logs. The key is to explicitly model a generation process for user behavior and fit click data based on examination hypothesis. Previous research found empirically that the true latent relevance can be recovered in most cases as long as the clicks are perfectly fitted. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. In this work, we aim to answer if or when the true relevance can be recovered from click data, which is a foundation issue for ULTR field. We first define a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, which is enough for pairwise ranking objective. Then we explore an equivalent condition for identifiability that can be novely expressed as a graph connectivity test problem: if and only if a graph (namely identifi
    
[^51]: 以Shapley增加的自我归因实现忠实的神经网络内部解释

    Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attribution. (arXiv:2309.15559v1 [cs.LG])

    [http://arxiv.org/abs/2309.15559](http://arxiv.org/abs/2309.15559)

    本文提出了一种Shapley增加的自我归因神经网络(SASANet)来实现忠实的神经网络内部解释。通过引入Shapley值，SASANet能够确保自我归因值与输出的Shapley值相等，从而提供了一个具有理论保证的解释框架。实验证明，SASANet在性能上超过了现有的自我归因模型，并与黑盒模型相媲美。此外，SASANet在交互式解释和效率方面也表现出更高的精确度和高效性。

    

    自我解释的神经网络在研究中引起了极大的关注。该领域现有的工作通常存在以下问题：(1)缺乏确保真正可解释性的坚实理论基础，或者(2)牺牲模型的表达能力。为此，我们提出了一个通用的增加自我归因(ASA)框架。观察到增加自我归因中缺乏Shapley值，我们提出了Shapley增加的自我归因神经网络(SASANet)，其具有保证自我归因值等于输出的Shapley值的理论保证。具体而言，SASANet使用基于边际贡献的顺序模式和内部蒸馏的训练策略，为任意数量的特征建模有意义的输出，从而得到非近似的有意义的价值函数。我们的实验结果表明，SASANet在性能上超过了现有的自我归因模型，并与黑盒模型相媲美。此外，SASANet在交互式解释和效率方面显示更精确和高效于事后方法。

    Self-interpreting neural networks have garnered significant interest in research. Existing works in this domain often (1) lack a solid theoretical foundation ensuring genuine interpretability or (2) compromise model expressiveness. In response, we formulate a generic Additive Self-Attribution (ASA) framework. Observing the absence of Shapley value in Additive Self-Attribution, we propose Shapley Additive Self-Attributing Neural Network (SASANet), with theoretical guarantees for the self-attribution value equal to the output's Shapley values. Specifically, SASANet uses a marginal contribution-based sequential schema and internal distillation-based training strategies to model meaningful outputs for any number of features, resulting in un-approximated meaningful value function. Our experimental results indicate SASANet surpasses existing self-attributing models in performance and rivals black-box models. Moreover, SASANet is shown more precise and efficient than post-hoc methods in inter
    
[^52]: 使用CrunchBase数据预测创业公司成功和风险投资组合模拟

    Startup success prediction and VC portfolio simulation using CrunchBase data. (arXiv:2309.15552v1 [cs.LG])

    [http://arxiv.org/abs/2309.15552](http://arxiv.org/abs/2309.15552)

    本研究提出了一个使用CrunchBase数据来预测创业成功和模拟VC投资组合的新颖深度学习模型，并通过全面回溯算法对模型在历史数据上的表现进行了评估。

    

    预测创业公司的成功对于创业生态系统的不稳定性而言是一项巨大的挑战。借助CrunchBase等广泛数据库的出现，结合可用的开放数据，可以应用机器学习和人工智能进行更准确的预测分析。本文聚焦于创业公司在B轮和C轮投资阶段，旨在预测关键的成功里程碑，如实现首次公开募股（IPO），达到独角兽地位，或成功实施并购。我们提出了一种新颖的深度学习模型来预测创业公司的成功，整合了各种因素，如资金指标、创始人特征和行业类别。我们研究的一个独特特点是使用了一种全面的回溯算法来模拟风险投资的投资过程。这种模拟允许对我们模型的性能进行针对历史数据的强大评估。

    Predicting startup success presents a formidable challenge due to the inherently volatile landscape of the entrepreneurial ecosystem. The advent of extensive databases like Crunchbase jointly with available open data enables the application of machine learning and artificial intelligence for more accurate predictive analytics. This paper focuses on startups at their Series B and Series C investment stages, aiming to predict key success milestones such as achieving an Initial Public Offering (IPO), attaining unicorn status, or executing a successful Merger and Acquisition (M\&A). We introduce novel deep learning model for predicting startup success, integrating a variety of factors such as funding metrics, founder features, industry category. A distinctive feature of our research is the use of a comprehensive backtesting algorithm designed to simulate the venture capital investment process. This simulation allows for a robust evaluation of our model's performance against historical data
    
[^53]: 使用DeepRepViz来识别基于深度学习模型预测中的混淆因素

    Identifying confounders in deep-learning-based model predictions using DeepRepViz. (arXiv:2309.15551v1 [cs.LG])

    [http://arxiv.org/abs/2309.15551](http://arxiv.org/abs/2309.15551)

    这项研究提出了DeepRepViz框架，用于帮助研究人员在深度学习模型预测中识别混淆因素，并通过度量和可视化工具来解决这个问题。实验证明使用DeepRepViz与DL模型结合能够带来明显的益处。

    

    越来越多地使用深度学习模型分析神经影像数据，揭示大脑、大脑病理和心理特征的见解。然而，诸如参与者年龄、性别或影像伪影等外部的“混淆因素”变量可能会偏导致模型预测，从而阻碍模型学习相关的脑-表型关系。在本研究中，我们提出了一种名为“DeepRepViz”的解决方案，使研究人员能够系统地检测DL模型预测中的混淆因素。该框架包括(1)度量可能混淆因素的影响程度的指标和(2)允许研究人员定性检查DL模型学习内容的可视化工具。通过在模拟和神经影像数据集上进行实验证明了使用DeepRepViz与DL模型结合的益处。例如，神经影像数据集的实验揭示了性别是DL模型预测中的一个显著混淆因素。

    Deep Learning (DL) models are increasingly used to analyze neuroimaging data and uncover insights about the brain, brain pathologies, and psychological traits. However, extraneous `confounders' variables such as the age of the participants, sex, or imaging artifacts can bias model predictions, preventing the models from learning relevant brain-phenotype relationships. In this study, we provide a solution called the `DeepRepViz' framework that enables researchers to systematically detect confounders in their DL model predictions. The framework consists of (1) a metric that quantifies the effect of potential confounders and (2) a visualization tool that allows researchers to qualitatively inspect what the DL model is learning. By performing experiments on simulated and neuroimaging datasets, we demonstrate the benefits of using DeepRepViz in combination with DL models. For example, experiments on the neuroimaging datasets reveal that sex is a significant confounder in a DL model predicti
    
[^54]: 从LAION-5B到LAION-EO：使用锚定数据集过滤数十亿张图片进行卫星图像提取

    From LAION-5B to LAION-EO: Filtering Billions of Images Using Anchor Datasets for Satellite Image Extraction. (arXiv:2309.15535v1 [cs.CV])

    [http://arxiv.org/abs/2309.15535](http://arxiv.org/abs/2309.15535)

    本文提出了一种基于锚定数据集和进一步过滤的提取方法，用于从大型图像库中提取卫星图像。这导致了发布了一个高分辨率的文本和卫星图像对应的数据集LAION-EO。

    

    大型数据集，如LAION-5B，包含在线共享的各种图像。然而，提取大型图像库的领域特定子集是具有挑战性的。本文提出了一种基于锚定数据集的提取方法，结合进一步的过滤，用于卫星图像领域。这导致了LAION-EO的发布，该数据集是从网络中获取的高（逐像素）分辨率的文本和卫星图像对。本文概述了采集过程以及数据集的一些特点。

    Large datasets, such as LAION-5B, contain a diverse distribution of images shared online. However, extraction of domain-specific subsets of large image corpora is challenging. The extraction approach based on an anchor dataset, combined with further filtering, is proposed here and demonstrated for the domain of satellite imagery. This results in the release of LAION-EO, a dataset sourced from the web containing pairs of text and satellite images in high (pixel-wise) resolution. The paper outlines the acquisition procedure as well as some of the features of the dataset.
    
[^55]: 通过神经后验主成分进行不确定性量化

    Uncertainty Quantification via Neural Posterior Principal Components. (arXiv:2309.15533v1 [cs.CV])

    [http://arxiv.org/abs/2309.15533](http://arxiv.org/abs/2309.15533)

    本论文提出了一种使用神经网络在单次前向传递中预测任意输入图像后验分布的主成分的方法，以实现不确定性量化。

    

    不确定性量化对于在自动驾驶和生物成像等安全关键领域中部署图像恢复模型至关重要。迄今为止，关于不确定性可视化的方法主要集中在每像素估计上。然而，每像素方差的热图通常在实际中用途有限，因为它无法捕捉像素之间的强相关性。更自然的不确定性度量对应于后验分布的主成分（PCs）上的方差。理论上，可以通过对输入图像的条件生成模型生成的样本应用PCA来计算PCs。然而，这需要在测试时生成大量的样本，而在目前的最先进（扩散）模型下非常缓慢。在该工作中，我们提出了一种方法来在神经网络的单次前向传递中预测后验分布的PCs，适用于任意输入图像。

    Uncertainty quantification is crucial for the deployment of image restoration models in safety-critical domains, like autonomous driving and biological imaging. To date, methods for uncertainty visualization have mainly focused on per-pixel estimates. However, a heatmap of per-pixel variances is typically of little practical use, as it does not capture the strong correlations between pixels. A more natural measure of uncertainty corresponds to the variances along the principal components (PCs) of the posterior distribution. Theoretically, the PCs can be computed by applying PCA on samples generated from a conditional generative model for the input image. However, this requires generating a very large number of samples at test time, which is painfully slow with the current state-of-the-art (diffusion) models. In this work, we present a method for predicting the PCs of the posterior distribution for any input image, in a single forward pass of a neural network. Our method can either wrap
    
[^56]: 重新思考频道维度以隔离大型语言模型的低位权重量化中的异常值

    Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models. (arXiv:2309.15531v1 [cs.LG])

    [http://arxiv.org/abs/2309.15531](http://arxiv.org/abs/2309.15531)

    该论文提出了一种重新思考频道维度的方法，以隔离大型语言模型低位权重量化中的异常值。通过将权重按输入通道内进行量化分组，可以解决激活异常值的问题，并成功地使得低于4位的量化成为可能。

    

    大型语言模型（LLMs）在各种任务中近期展示了显著成功。然而，在有效地为LLMs提供服务方面一直是个挑战，这主要是由于其大内存瓶颈，特别是在小批量推理设置（如移动设备）中。仅对权重进行量化可能是一种有希望的方法，但是由于存在大幅度激活异常值，低于4位的量化仍然是一个挑战。为了减轻不可取的异常效果，我们首先提出了每个输入通道（IC）内进行量化分组的per-IC量化方法，这是一种简单但有效的方法，而不是传统的每个输出通道（OC）内进行量化分组。我们的方法的动机是观察到激活异常值影响权重矩阵的输入维度，因此在IC方向上对权重进行类似分组可以将异常值隔离到一个分组内。我们还发现激活的异常值并不决定量化的难度，其固有的权重敏感性也存在。通过per-IC量化作为方法，我们的方法成功地解决了大型语言模型低位权重量化中的异常值问题。

    Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers to be within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as
    
[^57]: 医疗数据的缺失模态启用多模态融合架构

    Missing-modality Enabled Multi-modal Fusion Architecture for Medical Data. (arXiv:2309.15529v1 [eess.IV])

    [http://arxiv.org/abs/2309.15529](http://arxiv.org/abs/2309.15529)

    这项研究提出了一种对医疗数据具有鲁棒性且能进一步提高疾病诊断性能的多模态融合架构。通过融合X射线照片、放射学报告和结构化数据，使用Transformer模块进行融合，同时引入了多变量损失函数提高模型在缺失模态情况下的鲁棒性。

    

    融合多模态数据可以提高深度学习模型的性能。然而，由于患者具体情况，医疗数据中的缺失模态是常见的，这对应用中的多模态模型的性能有害。因此，对于缺失模态对模型进行适应是至关重要的。本研究旨在开发一种对缺失模态具有鲁棒性且能进一步改善疾病诊断性能的医疗数据的高效多模态融合架构。本研究融合了X射线胸部放射照片作为图像模态，放射学报告作为文本模态，以及结构化值数据作为表格数据模态。每个模态对都通过基于Transformer的双模态融合模块进行融合，然后将这三个双模态融合模块组合成三模态融合框架。此外，还引入了多变量损失函数到训练过程中，以提高模型在推理过程中对缺失模态的鲁棒性。

    Fusing multi-modal data can improve the performance of deep learning models. However, missing modalities are common for medical data due to patients' specificity, which is detrimental to the performance of multi-modal models in applications. Therefore, it is critical to adapt the models to missing modalities. This study aimed to develop an efficient multi-modal fusion architecture for medical data that was robust to missing modalities and further improved the performance on disease diagnosis.X-ray chest radiographs for the image modality, radiology reports for the text modality, and structured value data for the tabular data modality were fused in this study. Each modality pair was fused with a Transformer-based bi-modal fusion module, and the three bi-modal fusion modules were then combined into a tri-modal fusion framework. Additionally, multivariate loss functions were introduced into the training process to improve model's robustness to missing modalities in the inference process. 
    
[^58]: 鲁棒的内部表示在领域泛化中的应用

    Robust Internal Representations for Domain Generalization. (arXiv:2309.15522v1 [cs.LG])

    [http://arxiv.org/abs/2309.15522](http://arxiv.org/abs/2309.15522)

    本文综合调查了作者利用嵌入空间进行转移学习研究的成果，主要讨论了持续学习和有限标记数据可用性所带来的挑战，为未来领域的进展铺平道路。

    

    本文是我在转移学习中利用嵌入空间进行研究的综合调查。本文主要讨论了持续学习和有限标记数据可用性所带来的固有挑战。通过总结我过去和正在进行中的贡献，本文旨在呈现我研究的整体理解，为未来领域的探索和进展铺平道路。我的研究涉及了转移学习的各种设置，包括少样本学习、零样本学习、持续学习、领域自适应和分布式学习。我希望这个调查提供给那些希望专注于类似研究方向的研究者一个前瞻性的视角。

    This paper which is part of the New Faculty Highlights Invited Speaker Program of AAAI'23, serves as a comprehensive survey of my research in transfer learning by utilizing embedding spaces. The work reviewed in this paper specifically revolves around the inherent challenges associated with continual learning and limited availability of labeled data. By providing an overview of my past and ongoing contributions, this paper aims to present a holistic understanding of my research, paving the way for future explorations and advancements in the field. My research delves into the various settings of transfer learning, including, few-shot learning, zero-shot learning, continual learning, domain adaptation, and distributed learning. I hope this survey provides a forward-looking perspective for researchers who would like to focus on similar research directions.
    
[^59]: 稀缺图像数据的MLOps：显微镜图像分析的一个案例研究

    MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis. (arXiv:2309.15521v1 [cs.LG])

    [http://arxiv.org/abs/2309.15521](http://arxiv.org/abs/2309.15521)

    本论文研究在稀缺数据分析中完全应用MLOps的情况，并提出了一种新的整体方法来增强生物医学图像分析。

    

    如今，机器学习（ML）正在经历前所未有的流行。ML模型的操作化由一组被称为机器学习操作（MLOps）的概念和方法所指导。然而，研究人员和专业人员往往更多地关注自动化方面，忽视MLOps的持续部署和监控方面。结果，由于概念漂移，特别是在处理稀缺数据时，从生产到开发过程中的反馈缺乏连续学习，导致模型会随时间不断恶化。本文探讨了在稀缺数据分析环境中完全应用MLOps的情况。该论文提出了一种新的整体方法来增强生物医学图像分析。我们的方法包括：指纹化过程，使得根据手头的图像分析任务选择最佳模型、数据集和模型开发策略；一种自动化的模型开发过程。

    Nowadays, Machine Learning (ML) is experiencing tremendous popularity that has never been seen before. The operationalization of ML models is governed by a set of concepts and methods referred to as Machine Learning Operations (MLOps). Nevertheless, researchers, as well as professionals, often focus more on the automation aspect and neglect the continuous deployment and monitoring aspects of MLOps. As a result, there is a lack of continuous learning through the flow of feedback from production to development, causing unexpected model deterioration over time due to concept drifts, particularly when dealing with scarce data. This work explores the complete application of MLOps in the context of scarce data analysis. The paper proposes a new holistic approach to enhance biomedical image analysis. Our method includes: a fingerprinting process that enables selecting the best models, datasets, and model development strategy relative to the image analysis task at hand; an automated model deve
    
[^60]: SAF-Net: 使用多视角超声心动图进行心肌梗死检测的自注意力融合网络

    SAF-Net: Self-Attention Fusion Network for Myocardial Infarction Detection using Multi-View Echocardiography. (arXiv:2309.15520v1 [cs.LG])

    [http://arxiv.org/abs/2309.15520](http://arxiv.org/abs/2309.15520)

    本文提出了一种自注意力融合网络（SAF-Net）来检测多视角超声心动图中的心肌梗死。该模型通过提取特征、学习特征的依赖关系以及使用密集层进行分类，实现了对MI的有效检测。

    

    心肌梗死（MI）是冠状动脉疾病（CAD）的严重情况，其检测对于防止心肌组织的进一步损伤至关重要。在本研究中，我们提出了一种新颖的视图融合模型，名为自注意力融合网络（SAF-Net），用于从多视角超声心动图记录中检测MI。所提出的框架利用顶部二腔心室（A2C）和顶部四腔心室（A4C）视图超声心动图记录进行分类。从两个视图的每条记录中提取三个参考帧，并使用预训练的深度网络提取高度代表性的特征。SAF-Net模型利用自注意力机制学习提取特征向量中的依赖关系。所提出的模型由三个主要部分组成，具有紧凑的体系结构：特征嵌入以减少维度，自注意力进行视图池化，密集层进行分类。实验评估使用HMC数据集进行。

    Myocardial infarction (MI) is a severe case of coronary artery disease (CAD) and ultimately, its detection is substantial to prevent progressive damage to the myocardium. In this study, we propose a novel view-fusion model named self-attention fusion network (SAF-Net) to detect MI from multi-view echocardiography recordings. The proposed framework utilizes apical 2-chamber (A2C) and apical 4-chamber (A4C) view echocardiography recordings for classification. Three reference frames are extracted from each recording of both views and deployed pre-trained deep networks to extract highly representative features. The SAF-Net model utilizes a self-attention mechanism to learn dependencies in extracted feature vectors. The proposed model is computationally efficient thanks to its compact architecture having three main parts: a feature embedding to reduce dimensionality, self-attention for view-pooling, and dense layers for the classification. Experimental evaluation is performed using the HMC-
    
[^61]: GNN4EEG：一种用于图神经网络对脑电图分类的基准和工具包

    GNN4EEG: A Benchmark and Toolkit for Electroencephalography Classification with Graph Neural Network. (arXiv:2309.15515v1 [cs.LG])

    [http://arxiv.org/abs/2309.15515](http://arxiv.org/abs/2309.15515)

    GNN4EEG是一个用于脑电图分类的图神经网络的基准和工具包，通过建模脑电图通道选定的特征并利用拓扑信息，可以提供更好的分类结果。

    

    脑电图（EEG）分类是神经科学、神经工程和几个商业应用中关键的任务。然而，传统的EEG分类模型常常忽视或不充分利用大脑的拓扑信息。鉴于此，近年来，利用图神经网络（GNN）来建模由每个EEG通道选择的特征，并利用拓扑信息的潜力已引起了广泛的兴趣。为进一步促进在这个方向上的研究，我们介绍了GNN4EEG，这是一个用于基于GNN对EEG信号建模的通用且易于使用的工具包。GNN4EEG包括三个组成部分：（i）一个大型基准，根据来自123名参与者的EEG数据构建了四个EEG分类任务。（ii）易于使用的各种最先进的基于GNN的EEG分类模型的实现，例如DGCNN、RGNN等。（iii）全面的实验设置的实现。

    Electroencephalography(EEG) classification is a crucial task in neuroscience, neural engineering, and several commercial applications. Traditional EEG classification models, however, have often overlooked or inadequately leveraged the brain's topological information. Recognizing this shortfall, there has been a burgeoning interest in recent years in harnessing the potential of Graph Neural Networks (GNN) to exploit the topological information by modeling features selected from each EEG channel in a graph structure. To further facilitate research in this direction, we introduce GNN4EEG, a versatile and user-friendly toolkit for GNN-based modeling of EEG signals. GNN4EEG comprises three components: (i)A large benchmark constructed with four EEG classification tasks based on EEG data collected from 123 participants. (ii)Easy-to-use implementations on various state-of-the-art GNN-based EEG classification models, e.g., DGCNN, RGNN, etc. (iii)Implementations of comprehensive experimental set
    
[^62]: 有限标量量化: 简化 VQ-VAE 方法

    Finite Scalar Quantization: VQ-VAE Made Simple. (arXiv:2309.15505v1 [cs.CV])

    [http://arxiv.org/abs/2309.15505](http://arxiv.org/abs/2309.15505)

    该论文提出了有限标量量化 (FSQ) 方法，用来简化 VQ-VAE 方法中的向量量化 (VQ)。通过投影和量化 VAE 表示，我们得到与 VQ 相同大小的码本。在这种离散表示上，我们可以训练相同的模型，并在图像生成、多模态生成和计算机视觉任务中取得竞争性能。

    

    我们提出用有限标量量化 (FSQ) 替代 VQ-VAE 潜在表示中的向量量化 (VQ)。在 FSQ 中，我们将 VAE 表示投影到几个维度 (通常少于10个)，每个维度被量化为一组固定的值，从而形成一个（隐式的）码本，由这些值的乘积组成。通过合适地选择维度和每个维度可以取的值的数量，我们获得与 VQ 中相同的码本大小。在这样的离散表示上，我们可以训练已经在 VQ-VAE 表示上训练过的相同模型，例如用于图像生成、多模态生成和密集预测计算机视觉任务的自回归和掩码变换器模型。具体而言，我们在图像生成中使用 FSQ 和 MaskGIT，在深度估计、着色和全景分割中使用 FSQ 和 UViM。尽管 FSQ 的设计要简单得多，我们在所有这些任务中获得了有竞争力的性能。

    We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. W
    
[^63]: 具有共享和个性化不确定表示的贝叶斯个性化联邦学习

    Bayesian Personalized Federated Learning with Shared and Personalized Uncertainty Representations. (arXiv:2309.15499v1 [cs.LG])

    [http://arxiv.org/abs/2309.15499](http://arxiv.org/abs/2309.15499)

    本文提出了一种贝叶斯个性化联邦学习（BPFL）的框架，用于处理联邦学习系统中的客户端不确定性和异质性。这个框架通过分解和共同学习统计异质性客户端数据上的共享和个性化不确定表示。

    

    贝叶斯个性化联邦学习（BPFL）解决了现有个性化联邦学习（PFL）面临的挑战。BPFL旨在通过处理客户端数据的统计异质性来量化客户端内部和跨客户端之间的不确定性和异质性。在PFL中，最近一些初步工作提出将隐藏的神经表示分解为共享和局部组件，并展示了有趣的结果。然而，大多数工作没有解决联邦学习系统中的客户端不确定性和异质性，同时适当地解耦神经表示是具有挑战性且常常是临时性的。在本文中，我们首次尝试引入一个通用的BPFL框架，通过分解和共同学习统计异质性客户端数据上的共享和个性化不确定表示。

    Bayesian personalized federated learning (BPFL) addresses challenges in existing personalized FL (PFL). BPFL aims to quantify the uncertainty and heterogeneity within and across clients towards uncertainty representations by addressing the statistical heterogeneity of client data. In PFL, some recent preliminary work proposes to decompose hidden neural representations into shared and local components and demonstrates interesting results. However, most of them do not address client uncertainty and heterogeneity in FL systems, while appropriately decoupling neural representations is challenging and often ad hoc. In this paper, we make the first attempt to introduce a general BPFL framework to decompose and jointly learn shared and personalized uncertainty representations on statistically heterogeneous client data over time. A Bayesian federated neural network BPFed instantiates BPFL by jointly learning cross-client shared uncertainty and client-specific personalized uncertainty over stat
    
[^64]: 具有理论保证的快速局部敏感哈希

    Fast Locality Sensitive Hashing with Theoretical Guarantee. (arXiv:2309.15479v1 [cs.LG])

    [http://arxiv.org/abs/2309.15479](http://arxiv.org/abs/2309.15479)

    本文提出了一种名为FastLSH的简单而高效的局部敏感哈希方案，通过结合随机采样和随机投影，将算法的时间复杂度大幅降低，并且具有可证明的局部敏感哈希属性。

    

    局部敏感哈希（LSH）是一种在许多机器学习任务中广泛使用的有效的随机化技术。哈希的成本与数据维度成正比，因此当维度很高且涉及大量哈希函数时，哈希往往成为性能瓶颈。然而，出人意料的是，很少有工作在改进LSH计算的效率方面做出努力。在本文中，我们设计了一种简单而高效的LSH方案，名为FastLSH，使用l2范数。通过结合随机采样和随机投影，FastLSH将时间复杂度从O（n）降低到O（m）（m < n），其中n是数据维度，m是抽样维度的数量。此外，FastLSH具有可证明的LSH属性，使其与非LSH快速草图有所区别。我们对一组真实和合成数据集进行了全面的实验，用于最近邻搜索任务。实验结果表明，FastLSH在答案质量方面与现有技术处于同一水平。

    Locality-sensitive hashing (LSH) is an effective randomized technique widely used in many machine learning tasks. The cost of hashing is proportional to data dimensions, and thus often the performance bottleneck when dimensionality is high and the number of hash functions involved is large. Surprisingly, however, little work has been done to improve the efficiency of LSH computation. In this paper, we design a simple yet efficient LSH scheme, named FastLSH, under l2 norm. By combining random sampling and random projection, FastLSH reduces the time complexity from O(n) to O(m) (m<n), where n is the data dimensionality and m is the number of sampled dimensions. Moreover, FastLSH has provable LSH property, which distinguishes it from the non-LSH fast sketches. We conduct comprehensive experiments over a collection of real and synthetic datasets for the nearest neighbor search task. Experimental results demonstrate that FastLSH is on par with the state-of-the-arts in terms of answer qualit
    
[^65]: Robust Semantic Segmentation UNCV2023挑战结果

    The Robust Semantic Segmentation UNCV2023 Challenge Results. (arXiv:2309.15478v1 [cs.CV])

    [http://arxiv.org/abs/2309.15478](http://arxiv.org/abs/2309.15478)

    这个论文总结了MUAD不确定性量化挑战的获胜解决方案，并展示了19个提交作品的结果。挑战主要集中在城市环境的语义分割，特别关注自然对抗场景。这个论文提供了表现最好的解决方案，并全面概述了所有参与者部署的多种解决方案。

    

    本文概述了在ICCV 2023举办的MUAD不确定性量化挑战中采用的获胜解决方案。该挑战围绕城市环境中的语义分割展开，特别关注自然对抗场景。报告呈现了19个提交作品的结果，其中许多技术借鉴了过去几年在计算机视觉和机器学习领域的重要会议和期刊上介绍的前沿不确定性量化方法。本文介绍了这个挑战，阐明了其目的和目标，主要集中在改善在不同自然对抗条件下城市场景中的语义分割的鲁棒性。报告还深入探讨了表现最好的解决方案。此外，本文旨在提供对所有参与者部署的多种解决方案的全面概述。

    This paper outlines the winning solutions employed in addressing the MUAD uncertainty quantification challenge held at ICCV 2023. The challenge was centered around semantic segmentation in urban environments, with a particular focus on natural adversarial scenarios. The report presents the results of 19 submitted entries, with numerous techniques drawing inspiration from cutting-edge uncertainty quantification methodologies presented at prominent conferences in the fields of computer vision and machine learning and journals over the past few years. Within this document, the challenge is introduced, shedding light on its purpose and objectives, which primarily revolved around enhancing the robustness of semantic segmentation in urban scenes under varying natural adversarial conditions. The report then delves into the top-performing solutions. Moreover, the document aims to provide a comprehensive overview of the diverse solutions deployed by all participants. By doing so, it seeks to of
    
[^66]: 通过跨级别优化实现资源高效的AIoT系统: 一项调查

    Enabling Resource-efficient AIoT System with Cross-level Optimization: A survey. (arXiv:2309.15467v1 [cs.LG])

    [http://arxiv.org/abs/2309.15467](http://arxiv.org/abs/2309.15467)

    该论文调查了资源高效的AIoT系统的跨级别优化，提出了一种算法-系统共同设计的方法，通过优化DL模型和系统调度，改善了运行时资源可用性，推动AIoT性能的进一步提升。

    

    随着智能基础设施的广泛使用和深度学习（DL）的令人瞩目的成功，人工智能物联网（AIoT，AI+IoT）这一新兴领域得到了推动。DL模型资源密集，因此现有研究努力在资源有限的基础设施上实现AIoT实时推理和低成本训练。为此，通过联合优化资源友好的DL模型和模型自适应系统调度的算法-系统共同设计改进了运行时资源可用性，从而推动了由独立级别设定的性能界限。

    The emerging field of artificial intelligence of things (AIoT, AI+IoT) is driven by the widespread use of intelligent infrastructures and the impressive success of deep learning (DL). With the deployment of DL on various intelligent infrastructures featuring rich sensors and weak DL computing capabilities, a diverse range of AIoT applications has become possible. However, DL models are notoriously resource-intensive. Existing research strives to realize near-/realtime inference of AIoT live data and low-cost training using AIoT datasets on resource-scare infrastructures. Accordingly, the accuracy and responsiveness of DL models are bounded by resource availability. To this end, the algorithm-system co-design that jointly optimizes the resource-friendly DL models and model-adaptive system scheduling improves the runtime resource availability and thus pushes the performance boundary set by the standalone level. Unlike previous surveys on resource-friendly DL models or hand-crafted DL com
    
[^67]: 使用大型语言模型的图神经提示

    Graph Neural Prompting with Large Language Models. (arXiv:2309.15427v1 [cs.CL])

    [http://arxiv.org/abs/2309.15427](http://arxiv.org/abs/2309.15427)

    本文提出了一种名为图神经提示（GNP）的方法，可以帮助大型语言模型从知识图中学习有益的知识，以弥补它们在准确捕捉和返回基于知识的信息方面的固有限制。

    

    大型语言模型（LLMs）在各种语言建模任务中表现出了卓越的泛化能力和出色的性能，但它们在准确捕捉和返回基于知识的信息方面仍存在固有限制。现有的研究已经探索了利用知识图来通过联合训练和定制模型架构增强语言建模，但是将此应用于LLMs存在参数数量庞大和计算成本高的问题。此外，如何利用预训练的LLMs并避免从头开始训练自定义模型仍然是一个开放的问题。在这项工作中，我们提出了图神经提示（GNP），一种新颖的即插即用方法，可以帮助预训练的LLMs从知识图中学习有益的知识。GNP包括各种设计，包括标准的图神经网络编码器、跨模态汇聚模块、域投影器和自监督链接预测目标。在多个实验中展示了GNP的有效性。

    Large Language Models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. In addition, how to leverage the pre-trained LLMs and avoid training a customized model from scratch remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple
    
[^68]: NeuRBF: 一种具有自适应径向基函数的神经场表示方法

    NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions. (arXiv:2309.15426v1 [cs.CV])

    [http://arxiv.org/abs/2309.15426](http://arxiv.org/abs/2309.15426)

    NeuRBF是一种使用自适应径向基函数进行信号表示的神经场方法，具有更高的空间适应性和能够更好地拟合目标信号的能力。通过与多频正弦函数组合，可以进一步提高径向基函数的通道容量和表示细节的能力。

    

    我们提出了一种新颖的神经场类型，它使用通用的径向基函数来表示信号。目前最先进的神经场通常使用基于网格的表示方法来存储局部神经特征，并使用N维线性核来在连续查询点上进行特征插值。它们的神经特征的空间位置固定在网格节点上，并且无法很好地适应目标信号。相反，我们的方法基于具有灵活的核位置和形状的通用径向基函数，具有更高的空间适应性，可以更好地拟合目标信号。为了进一步提高径向基函数的通道容量，我们提出了将其与多频正弦函数组合的方法。这种技术将径向基函数扩展为不同频带的多个傅里叶径向基函数，而不需要额外的参数，便于表示细节。此外，通过将自适应径向基函数与基于网格的方法结合，我们的混合组合继承了两者的优点。

    We present a novel type of neural fields that uses general radial bases for signal representation. State-of-the-art neural fields typically rely on grid-based representations for storing local neural features and N-dimensional linear kernels for interpolating features at continuous query points. The spatial positions of their neural features are fixed on grid nodes and cannot well adapt to target signals. Our method instead builds upon general radial bases with flexible kernel position and shape, which have higher spatial adaptivity and can more closely fit target signals. To further improve the channel-wise capacity of radial basis functions, we propose to compose them with multi-frequency sinusoid functions. This technique extends a radial basis to multiple Fourier radial bases of different frequency bands without requiring extra parameters, facilitating the representation of details. Moreover, by marrying adaptive radial bases with grid-based ones, our hybrid combination inherits bo
    
[^69]: 确定性计算力学中的深度学习

    Deep Learning in Deterministic Computational Mechanics. (arXiv:2309.15421v1 [cs.LG])

    [http://arxiv.org/abs/2309.15421](http://arxiv.org/abs/2309.15421)

    深度学习在确定性计算力学领域的五个主要类别中发挥重要作用，包括模拟替代、模拟增强、神经网络离散化、生成方法和深度强化学习。

    

    深度学习在计算力学领域中正在快速发展，产生了大量且多样化的文献。为了帮助研究人员找到这个领域中的关键概念和有前景的方法，我们提供了关于确定性计算力学中深度学习的概述。我们确认并探索了五个主要类别：模拟替代、模拟增强、神经网络离散化、生成方法和深度强化学习。该综述关注的是深度学习方法而非计算力学的应用，从而让研究人员更有效地探索这个领域。因此，这篇综述不一定是针对具有深度学习广泛知识的研究人员，而是主要面向即将进入这个领域或者试图获得计算力学中深度学习概述的研究人员。因此，讨论的概念是...

    The rapid growth of deep learning research, including within the field of computational mechanics, has resulted in an extensive and diverse body of literature. To help researchers identify key concepts and promising methodologies within this field, we provide an overview of deep learning in deterministic computational mechanics. Five main categories are identified and explored: simulation substitution, simulation enhancement, discretizations as neural networks, generative approaches, and deep reinforcement learning. This review focuses on deep learning methods rather than applications for computational mechanics, thereby enabling researchers to explore this field more effectively. As such, the review is not necessarily aimed at researchers with extensive knowledge of deep learning -- instead, the primary audience is researchers at the verge of entering this field or those who attempt to gain an overview of deep learning in computational mechanics. The discussed concepts are, therefore,
    
[^70]: 失败模式三元组及可能的解决方法

    The Triad of Failure Modes and a Possible Way Out. (arXiv:2309.15420v1 [cs.LG])

    [http://arxiv.org/abs/2309.15420](http://arxiv.org/abs/2309.15420)

    本文提出了一个新颖的目标函数，用于解决聚类自监督学习中的表示崩溃、聚类崩溃和对聚类分配的置换不变性的问题。该目标函数具有简单性和理论基础，适用于标准的主干结构训练，无需使用非对称元素。

    

    我们提出了一个新颖的聚类自监督学习（SSL）的目标函数，旨在解决表示崩溃、聚类崩溃和对聚类分配的置换不变性等三种失败模式。这个目标函数由三个关键组成部分构成：（i）惩罚表示崩溃的生成项，（ii）促进对数据增强的不变性的项，从而解决标签置换的问题，以及（ii）惩罚聚类崩溃的均匀性项。此外，我们提出的目标函数具有两个显著优势。首先，它可以从贝叶斯的角度解释为数据对数似然的下界。其次，它可以训练一个标准的主干结构，无需使用非对称元素，如停止梯度、动量编码器或专门的聚类层。由于其简单性和理论基础，我们提出的目标函数非常适合

    We present a novel objective function for cluster-based self-supervised learning (SSL) that is designed to circumvent the triad of failure modes, namely representation collapse, cluster collapse, and the problem of invariance to permutations of cluster assignments. This objective consists of three key components: (i) A generative term that penalizes representation collapse, (ii) a term that promotes invariance to data augmentations, thereby addressing the issue of label permutations and (ii) a uniformity term that penalizes cluster collapse. Additionally, our proposed objective possesses two notable advantages. Firstly, it can be interpreted from a Bayesian perspective as a lower bound on the data log-likelihood. Secondly, it enables the training of a standard backbone architecture without the need for asymmetric elements like stop gradients, momentum encoders, or specialized clustering layers. Due to its simplicity and theoretical foundation, our proposed objective is well-suited for 
    
[^71]: 通过对手对推荐系统中的特征公平性的自动处理

    Automatic Feature Fairness in Recommendation via Adversaries. (arXiv:2309.15418v1 [cs.IR])

    [http://arxiv.org/abs/2309.15418](http://arxiv.org/abs/2309.15418)

    通过对手训练实现推荐系统中的特征公平性，提高整体准确性和泛化能力

    

    公平性是推荐系统中广泛讨论的一个主题，但其实践实现在定义敏感特征的同时保持推荐准确性方面面临挑战。我们提出将特征公平性作为实现各个由不同特征组合定义的多样群体之间的公平待遇的基础。通过平衡特征的泛化能力，可以提高整体准确性。我们通过对手训练引入了无偏特征学习，使用对手扰动增强特征表示。对手改进了模型对少数特征的泛化能力。我们根据特征偏差的两种形式自动适应对手：特征值的频率和组合多样性。这使我们能够动态调整扰动强度和对手训练权重。更强的扰动适用于组合变化少的特征值，以改善泛化能力，而对于低频特征，较高的权重可以解决...

    Fairness is a widely discussed topic in recommender systems, but its practical implementation faces challenges in defining sensitive features while maintaining recommendation accuracy. We propose feature fairness as the foundation to achieve equitable treatment across diverse groups defined by various feature combinations. This improves overall accuracy through balanced feature generalizability. We introduce unbiased feature learning through adversarial training, using adversarial perturbation to enhance feature representation. The adversaries improve model generalization for under-represented features. We adapt adversaries automatically based on two forms of feature biases: frequency and combination variety of feature values. This allows us to dynamically adjust perturbation strengths and adversarial training weights. Stronger perturbations are applied to feature values with fewer combination varieties to improve generalization, while higher weights for low-frequency features address 
    
[^72]: 通过AI驱动的知识发现，革新对地形降水的理解

    Revolutionizing Terrain-Precipitation Understanding through AI-driven Knowledge Discovery. (arXiv:2309.15400v1 [physics.ao-ph])

    [http://arxiv.org/abs/2309.15400](http://arxiv.org/abs/2309.15400)

    通过AI驱动的知识发现技术，首次揭示了地形特征与降水模式之间的复杂关系，并在1995年左右揭示了一个显著的地形与降水关系转变现象。

    

    在当今的气候科学中，推进我们对复杂地形地区气候过程的理解是一个重要的挑战，特别是在全球气候变化的背景下。尤其值得注意的是，在这些地区观测数据的缺乏对于理解其中微妙的气候动力学产生了显著限制。首次利用尖端的AI驱动的知识发现技术，我们发现了一些明确的方程来阐明地形特征和降水模式之间复杂关系，揭示了之前隐藏的控制这些关系的复杂性。这些迄今未披露的方程在应用于降水数据时与传统经验模型相比具有显著的准确性。在此基础上，我们揭示了一个被称为“1995年转折点”的现象，表明了大约在1995年左右地形与降水关系的显著转变。

    Advancing our understanding of climate processes in regions characterized by intricate terrain complexity is a paramount challenge in contemporary climate science, particularly in the context of global climate change. Notably, the scarcity of observational data in these regions has imposed substantial limitations on understanding the nuanced climate dynamics therein. For the first time, utilizing cutting-edge AI-driven knowledge discovery techniques, we have uncovered explicit equations that elucidate the intricate relationship between terrain features and precipitation patterns, illuminating the previously concealed complexities governing these relationships. These equations, thus far undisclosed, exhibit remarkable accuracy compared to conventional empirical models when applied to precipitation data. Building on this foundation, we reveal a phenomenon known as the '1995 turning point,' indicating a significant shift in the terrain-precipitation relationship in approximately 1995, rel
    
[^73]: 在在线CMDPs中，无模型、遗憾最优的最佳策略识别

    Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs. (arXiv:2309.15395v1 [cs.LG])

    [http://arxiv.org/abs/2309.15395](http://arxiv.org/abs/2309.15395)

    本文提出了一种无模型的算法，名为PRI，用于在线CMDPs中的最佳策略识别问题。该算法基于CMDPs的有限随机性属性，能够以低遗憾并以高概率识别出最优策略。

    

    本文考虑了在线约束马尔科夫决策过程（CMDPs）中的最佳策略识别（BPI）问题。我们对具有低遗憾并且以高概率识别最优策略的无模型算法感兴趣。现有的在线CMDPs的无模型算法在次线性遗憾和违约时没有提供任何对最优策略的收敛保证，并且只在从以前使用的策略中随机均匀抽样时提供平均性能保证。本文提出了一种新的算法，名为PRUNING-REFINEMENT-IDENTIFICATION（PRI），基于我们发现的CMDPs的一个基本结构性质，称为有限随机性。该属性表明对于具有N约束的CMDP，存在一个最优策略，其中至多有N个随机决策。所提出的算法首先识别出在哪个步骤和哪个状态需要进行随机决策，然后对这些决策的分布进行微调。

    This paper considers the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs). We are interested in algorithms that are model-free, have low regret, and identify an optimal policy with a high probability. Existing model-free algorithms for online CMDPs with sublinear regret and constraint violation do not provide any convergence guarantee to an optimal policy and provide only average performance guarantees when a policy is uniformly sampled at random from all previously used policies. In this paper, we develop a new algorithm, named Pruning-Refinement-Identification (PRI), based on a fundamental structural property of CMDPs we discover, called limited stochasticity. The property says for a CMDP with $N$ constraints, there exists an optimal policy with at most $N$ stochastic decisions.  The proposed algorithm first identifies at which step and in which state a stochastic decision has to be taken and then fine-tunes the distributions of these s
    
[^74]: 神经随机微分方程用于鲁棒性和可解释性电磁非意图辐射发射分析

    Neural Stochastic Differential Equations for Robust and Explainable Analysis of Electromagnetic Unintended Radiated Emissions. (arXiv:2309.15386v1 [cs.LG])

    [http://arxiv.org/abs/2309.15386](http://arxiv.org/abs/2309.15386)

    本研究对ResNet-like模型在非意图辐射发射（URE）分类中的鲁棒性和可解释性进行了全面评估，并通过提出神经随机微分方程（SDEs）的应用，解决了模型对噪声的脆弱性以及解释不准确的问题。

    

    我们在非意图辐射发射（URE）分类的背景下，对ResNet-like模型的鲁棒性和可解释性进行了全面评估，并提出了一种利用神经随机微分方程（SDEs）来解决已发现限制的新方法。我们通过实验证明了ResNet-like模型对高斯噪声扰动的脆弱性，当高斯噪声仅为0.5标准差时，模型性能急剧下降，其F1-score降至接近无意义的0.008。我们还强调了一个令人担忧的不一致性，即ResNet-like模型提供的解释不反映输入数据中固有的周期性，在稳定设备中进行URE检测时这是一个关键属性。针对这些发现，我们提出了一种新的应用神经SDEs构建URE分类模型的方法，这些模型不仅对噪声具有鲁棒性，还能提供更有意义和直观的解释。

    We present a comprehensive evaluation of the robustness and explainability of ResNet-like models in the context of Unintended Radiated Emission (URE) classification and suggest a new approach leveraging Neural Stochastic Differential Equations (SDEs) to address identified limitations. We provide an empirical demonstration of the fragility of ResNet-like models to Gaussian noise perturbations, where the model performance deteriorates sharply and its F1-score drops to near insignificance at 0.008 with a Gaussian noise of only 0.5 standard deviation. We also highlight a concerning discrepancy where the explanations provided by ResNet-like models do not reflect the inherent periodicity in the input data, a crucial attribute in URE detection from stable devices. In response to these findings, we propose a novel application of Neural SDEs to build models for URE classification that are not only robust to noise but also provide more meaningful and intuitive explanations. Neural SDE models mai
    
[^75]: ADGym：深度异常检测的设计选择

    ADGym: Design Choices for Deep Anomaly Detection. (arXiv:2309.15376v1 [cs.LG])

    [http://arxiv.org/abs/2309.15376](http://arxiv.org/abs/2309.15376)

    ADGym是一款针对深度异常检测的设计选择的综合评估和自动选择平台。

    

    深度学习（DL）技术最近被应用于异常检测（AD），在金融、医疗服务和云计算等领域取得了成功的成果。然而，目前的研究往往将深度AD算法作为一个整体进行评估，未能理解个别设计选择（如损失函数和网络架构）的贡献。因此，原始步骤（如预处理）的重要性可能被新颖的损失函数和架构所掩盖。在本文中，我们通过提出两个问题来解决这些疏漏：（i）深度AD方法的哪些组成部分（即设计选择）在检测异常方面是至关重要的？（ii）我们如何通过自动选择最佳设计选择来构建针对特定数据集的定制AD算法，而不是依赖通用的、预先存在的解决方案？为此，我们介绍了ADGym，这是第一个用于全面评估和自动选择AD的平台。

    Deep learning (DL) techniques have recently been applied to anomaly detection (AD), yielding successful outcomes in areas such as finance, medical services, and cloud computing. However, much of the current research evaluates a deep AD algorithm holistically, failing to understand the contributions of individual design choices like loss functions and network architectures. Consequently, the importance of prerequisite steps, such as preprocessing, might be overshadowed by the spotlight on novel loss functions and architectures. In this paper, we address these oversights by posing two questions: (i) Which components (i.e., design choices) of deep AD methods are pivotal in detecting anomalies? (ii) How can we construct tailored AD algorithms for specific datasets by selecting the best design choices automatically, rather than relying on generic, pre-existing solutions? To this end, we introduce ADGym, the first platform designed for comprehensive evaluation and automatic selection of AD d
    
[^76]: 通过基于注意力的深度状态空间建模，将PPG信号转换为ECG，用于连续性心房颤动检测

    PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling. (arXiv:2309.15375v1 [cs.LG])

    [http://arxiv.org/abs/2309.15375](http://arxiv.org/abs/2309.15375)

    通过基于注意力的深度状态空间建模，我们提出了一种不受个体限制的方法，将PPG信号转换为ECG，用于连续性心房颤动检测。

    

    电信号图（ECG或EKG）是一种测量心脏电活动的医学测试。ECG常用于诊断和监测各种心脏疾病，包括心律失常、心肌梗塞和心力衰竭。然而，传统的ECG需要临床测量，限制了其在医疗机构的应用。相比之下，单导联ECG已经在佩戴式设备上应用广泛。另一种ECG的替代方法是光浊度脉搏检测（PPG），它采用非侵入性、低成本的光学方法来测量心脏生理学，使其成为捕捉日常生活中重要心脏信号的合适选择。虽然ECG和PPG之间具有强烈的相关性，但后者并没有提供明显的临床诊断价值。在这里，我们提出了一种不受个体限制的基于注意力的深度状态空间模型，用于将PPG信号转换为ECG，从而实现连续性心房颤动检测。

    An electrocardiogram (ECG or EKG) is a medical test that measures the heart's electrical activity. ECGs are often used to diagnose and monitor a wide range of heart conditions, including arrhythmias, heart attacks, and heart failure. On the one hand, the conventional ECG requires clinical measurement, which restricts its deployment to medical facilities. On the other hand, single-lead ECG has become popular on wearable devices using administered procedures. An alternative to ECG is Photoplethysmography (PPG), which uses non-invasive, low-cost optical methods to measure cardiac physiology, making it a suitable option for capturing vital heart signs in daily life. As a result, it has become increasingly popular in health monitoring and is used in various clinical and commercial wearable devices. While ECG and PPG correlate strongly, the latter does not offer significant clinical diagnostic value. Here, we propose a subject-independent attention-based deep state-space model to translate P
    
[^77]: 通过测度传递进行密度估计：生物科学中的应用展望

    Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences. (arXiv:2309.15366v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.15366](http://arxiv.org/abs/2309.15366)

    通过测度传递方法进行密度估计在生物科学中具有广阔的应用前景，尤其是在处理稀疏数据的情况下，使用稀疏传递映射可以揭示数据中隐藏的信息。

    

    测度传递方法的一个优势是其允许对根据广泛概率测度分布的数据进行统一的处理和分析。在这个框架下，我们通过计算研究的结果来评估测度传递技术的潜力，特别是三角传递映射的使用，作为支持生物科学研究的工作流的一部分。稀疏数据场景在辐射生物学等领域很常见。我们发现，在数据稀缺时，稀疏传递映射是有优势的。具体而言，通过计算一系列（稀疏的）自适应传递映射的统计信息，这些映射是在随机选择的一系列可用数据样本子集上进行训练的，可以揭示数据中隐藏的信息。因此，在本文考虑的辐射生物学应用中，此方法为生成假设提供了一个工具。

    One among several advantages of measure transport methods is that they allow for a unified framework for processing and analysis of data distributed according to a wide class of probability measures. Within this context, we present results from computational studies aimed at assessing the potential of measure transport techniques, specifically, the use of triangular transport maps, as part of a workflow intended to support research in the biological sciences. Scarce data scenarios, which are common in domains such as radiation biology, are of particular interest. We find that when data is scarce, sparse transport maps are advantageous. In particular, statistics gathered from computing series of (sparse) adaptive transport maps, trained on a series of randomly chosen subsets of the set of available data samples, leads to uncovering information hidden in the data. As a result, in the radiation biology application considered here, this approach provides a tool for generating hypotheses ab
    
[^78]: C3Net: 面向异质系统中物理化学性质预测的原子间势神经网络

    C3Net: interatomic potential neural network for prediction of physicochemical properties in heterogenous systems. (arXiv:2309.15334v1 [cs.LG])

    [http://arxiv.org/abs/2309.15334](http://arxiv.org/abs/2309.15334)

    C3Net是一种面向异质系统中物理化学性质预测的神经网络，能嵌入原子类型在分子环境中并遵循基本物理定律的原子间势。该模型在预测物理化学性质上具有良好的泛化能力，并优于基于量子力学和神经网络的最新方法。

    

    理解溶质与其环境的相互作用在化学和生物学中具有基本重要性。本研究提出了一种深度神经网络架构，用于原子类型在分子环境中的嵌入和遵循基本物理定律的原子间势。该架构被应用于预测异质系统中的物理化学性质，包括溶解在不同溶剂中、1-辛醇-水分配和PAMPA。我们展示了我们的架构在物理化学性质上具有良好的泛化能力，并在溶解自由能预测任务中优于基于量子力学和神经网络的最新方法。从模型中获得的溶质中每个原子的原子间势允许进行符合化学和物理推理的原子分辨率的物理化学性质定量分析。该软件可在https://github.com/SehanLee/C3Net上获取。

    Understanding the interactions of a solute with its environment is of fundamental importance in chemistry and biology. In this work, we propose a deep neural network architecture for atom type embeddings in its molecular context and interatomic potential that follows fundamental physical laws. The architecture is applied to predict physicochemical properties in heterogeneous systems including solvation in diverse solvents, 1-octanol-water partitioning, and PAMPA with a single set of network weights. We show that our architecture is generalized well to the physicochemical properties and outperforms state-of-the-art approaches based on quantum mechanics and neural networks in the task of solvation free energy prediction. The interatomic potentials at each atom in a solute obtained from the model allow quantitative analysis of the physicochemical properties at atomic resolution consistent with chemical and physical reasoning. The software is available at https://github.com/SehanLee/C3Net.
    
[^79]: 使用主成分分析探索神经网络的学习表示

    Exploring Learned Representations of Neural Networks with Principal Component Analysis. (arXiv:2309.15328v1 [cs.LG])

    [http://arxiv.org/abs/2309.15328](http://arxiv.org/abs/2309.15328)

    这项研究使用主成分分析探索了深度神经网络的特征表示，并发现在某些层中只需要20%的特征空间方差就能实现高准确度分类。该研究还提供了三个可解释的替代模型，并发现仿射线性模型表现最佳。

    

    在可解释的AI领域中，理解深度神经网络(DNNs)的特征表示仍然是一个开放的问题。我们使用主成分分析(PCA)来研究在CIFAR-10上训练的ResNet-18学习的逐层表示在k最近邻分类器(k-NN)、最近类中心分类器(NCC)和支持向量机的性能。我们发现，在某些层中，只有20%的中间特征空间方差就足以实现高准确度的分类，并且在所有层中，前100个主成分完全决定了k-NN和NCC分类器的性能。我们将我们的发现与神经网络收缩联系起来，并提供了中间神经网络收缩相关现象的部分证据。我们的初步工作提供了三个不同但可解释的特征表示替代模型，其中最佳性能是一个仿射线性模型。我们还证明，利用几个替代模型可以提供更好的性能。

    Understanding feature representation for deep neural networks (DNNs) remains an open question within the general field of explainable AI. We use principal component analysis (PCA) to study the performance of a k-nearest neighbors classifier (k-NN), nearest class-centers classifier (NCC), and support vector machines on the learned layer-wise representations of a ResNet-18 trained on CIFAR-10. We show that in certain layers, as little as 20% of the intermediate feature-space variance is necessary for high-accuracy classification and that across all layers, the first ~100 PCs completely determine the performance of the k-NN and NCC classifiers. We relate our findings to neural collapse and provide partial evidence for the related phenomenon of intermediate neural collapse. Our preliminary work provides three distinct yet interpretable surrogate models for feature representation with an affine linear model the best performing. We also show that leveraging several surrogate models affords u
    
[^80]: 神经运算符加速科学模拟和设计

    Neural Operators for Accelerating Scientific Simulations and Design. (arXiv:2309.15325v1 [cs.LG])

    [http://arxiv.org/abs/2309.15325](http://arxiv.org/abs/2309.15325)

    本论文介绍了一种称为神经运算符的人工智能框架，用于学习连续域函数之间的映射，可以加速科学模拟和设计中的计算需求，实现零射超分辨率以及替代现有的模拟器。

    

    目前，科学发现和工程设计受限于物理实验的时间和成本，这些实验通常是通过试验和直觉选择的，并需要深入的领域专业知识。数值模拟是物理实验的替代方法，但对于复杂的现实领域来说，由于现有数值方法的计算需求，通常是不可行的。人工智能（AI）通过开发快速的数据驱动代理模型，提供了一个潜在的范式转变。特别是，一个称为神经运算符的AI框架提供了一个基于连续域函数之间映射学习的原则性框架，例如时空过程和偏微分方程（PDE）。它们可以在训练过程中未见过的新位置进行外推和预测解决方案，即进行零射超分辨率。神经运算符可以增强甚至替代许多应用中的现有模拟器，例如计算力学流体学。

    Scientific discovery and engineering design are currently limited by the time and cost of physical experiments, selected mostly through trial-and-error and intuition that require deep domain expertise. Numerical simulations present an alternative to physical experiments, but are usually infeasible for complex real-world domains due to the computational requirements of existing numerical methods. Artificial intelligence (AI) presents a potential paradigm shift through the development of fast data-driven surrogate models. In particular, an AI framework, known as neural operators, presents a principled framework for learning mappings between functions defined on continuous domains, e.g., spatiotemporal processes and partial differential equations (PDE). They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution. Neural operators can augment or even replace existing simulators in many applications, such as computational flui
    
[^81]: 关于奇异值分解在随机块模型中的能力

    On the Power of SVD in the Stochastic Block Model. (arXiv:2309.15322v1 [cs.LG])

    [http://arxiv.org/abs/2309.15322](http://arxiv.org/abs/2309.15322)

    本文研究了奇异值分解算法在随机块模型中的能力，发现在对称设置下，该算法能够正确恢复所有的聚类。

    

    在运行聚类算法之前，一种改善聚类结果的常见启发式方法是应用降维技术。已经观察到，基于谱的降维工具，如PCA或SVD，在许多应用中提高了聚类算法的性能。这种现象表明，谱方法不仅可以作为降维工具，还可以在某种程度上对聚类过程作出贡献。理解聚类问题中谱步骤的行为是一个有趣的问题。作为这个方向的一个初始步骤，本文研究了奇异值分解算法在随机块模型 (SBM) 中的能力。我们证明，在对称设置中，普通的奇异值分解算法可以正确恢复所有的聚类。这个结果回答了Van Vu在对称设置中提出的一个未解问题。

    A popular heuristic method for improving clustering results is to apply dimensionality reduction before running clustering algorithms. It has been observed that spectral-based dimensionality reduction tools, such as PCA or SVD, improve the performance of clustering algorithms in many applications. This phenomenon indicates that spectral method not only serves as a dimensionality reduction tool, but also contributes to the clustering procedure in some sense. It is an interesting question to understand the behavior of spectral steps in clustering problems.  As an initial step in this direction, this paper studies the power of vanilla-SVD algorithm in the stochastic block model (SBM). We show that, in the symmetric setting, vanilla-SVD algorithm recovers all clusters correctly. This result answers an open question posed by Van Vu (Combinatorics Probability and Computing, 2018) in the symmetric setting.
    
[^82]: DeepROCK: 深度神经网络中误差可控的交互检测方法

    DeepROCK: Error-controlled interaction detection in deep neural networks. (arXiv:2309.15319v1 [cs.LG])

    [http://arxiv.org/abs/2309.15319](http://arxiv.org/abs/2309.15319)

    DeepROCK是一种在深度神经网络中进行误差可控的交互检测的方法，通过使用knockoffs和一种新颖的DNN架构，可以同时控制虚发现率(FDR)和最大化统计功效。

    

    深度神经网络(DNNs)的复杂性使其强大，但也使其在解释上具有挑战性，从而限制了其在容忍错误的领域中的应用。现有方法通过识别影响预测结果的特征交互来推断DNNs的内部机制。然而，这些方法通常缺乏一种有系统的策略来优先考虑交互作用并控制置信水平，使其难以在科学发现和假设验证中实际应用。本文介绍了一种方法，称为DeepROCK，通过使用knockoffs（一种设计成在给定特征集的条件下与响应变量相互独立的虚拟变量）来解决这个限制。结合一种新颖的DNN架构，其中包括一层成对耦合层，DeepROCK可以同时控制虚发现率(FDR)和最大化统计功效。此外，我们还发现了一项挑战。

    The complexity of deep neural networks (DNNs) makes them powerful but also makes them challenging to interpret, hindering their applicability in error-intolerant domains. Existing methods attempt to reason about the internal mechanism of DNNs by identifying feature interactions that influence prediction outcomes. However, such methods typically lack a systematic strategy to prioritize interactions while controlling confidence levels, making them difficult to apply in practice for scientific discovery and hypothesis validation. In this paper, we introduce a method, called DeepROCK, to address this limitation by using knockoffs, which are dummy variables that are designed to mimic the dependence structure of a given set of features while being conditionally independent of the response. Together with a novel DNN architecture involving a pairwise-coupling layer, DeepROCK jointly controls the false discovery rate (FDR) and maximizes statistical power. In addition, we identify a challenge in
    
[^83]: MAPTree: 用贝叶斯决策树击败“最优”决策树

    MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees. (arXiv:2309.15312v1 [cs.LG])

    [http://arxiv.org/abs/2309.15312](http://arxiv.org/abs/2309.15312)

    MAPTree是一种通过贝叶斯方法对决策树进行归纳的算法，通过AND/OR搜索实现最大后验树的恢复。在实验中，MAPTree在多个数据集上表现出更好的性能，并且能够以更小的树来实现可比较的性能。在合成数据和实际场景中，MAPTree还展示出更强的抗噪声能力和更好的泛化能力。

    

    决策树仍然是当今最流行的机器学习模型之一，主要是因为其开箱即用的性能和可解释性。在这项工作中，我们通过对树上的后验分布进行最大后验推理，提出了一种贝叶斯决策树归纳的方法。我们首先展示了决策树的最大后验推理与AND/OR搜索之间的关联。利用这一关联，我们提出了一种称为MAPTree的AND/OR搜索算法，能够恢复出最大后验树。最后，我们通过在合成数据和实际世界场景中展示最大后验树的经验性能。在16个实际数据集上，MAPTree要么优于基准线，要么在性能相当的情况下具有更小的树。在一个合成数据集上，MAPTree表现出比现有方法更强的抗噪声能力和更好的泛化能力。最后，MAPTree比其他方法更快地恢复出最大后验树。

    Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster tha
    
[^84]: 自我监督的无约束机器人经验中的地形表示学习

    Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience. (arXiv:2309.15302v1 [cs.RO])

    [http://arxiv.org/abs/2309.15302](http://arxiv.org/abs/2309.15302)

    提出一种名为STERLING的自我监督地形表示学习方法，通过无约束的机器人经验学习有关地形的相关表示，以实现地形感知导航。

    

    地形认知，即辨别和区分不同类型的地形，是机器人在自主越野导航中必须具备的关键能力。目前提供机器人这种认知能力的方法要么依赖昂贵的标记数据收集，要么依赖无法泛化的工程特征和成本函数，或者依赖可能无法获得的专家人类示范。为了使机器人不受这些限制地具备地形认知能力，我们引入了自我监督地形表示学习（STERLING），这是一种新颖的学习地形表示的方法，仅依赖于易于收集的、无约束（例如，非专家的）和未标记的机器人经验，对数据采集没有额外的限制。STERLING采用了一种新颖的多模态自我监督目标，通过非对比度表示学习来学习地形相关的表示以用于地形感知导航。通过实物机器人实验

    Terrain awareness, i.e., the ability to identify and distinguish different types of terrain, is a critical ability that robots must have to succeed at autonomous off-road navigation. Current approaches that provide robots with this awareness either rely on labeled data which is expensive to collect, engineered features and cost functions that may not generalize, or expert human demonstrations which may not be available. Towards endowing robots with terrain awareness without these limitations, we introduce Self-supervised TErrain Representation LearnING (STERLING), a novel approach for learning terrain representations that relies solely on easy-to-collect, unconstrained (e.g., non-expert), and unlabelled robot experience, with no additional constraints on data collection. STERLING employs a novel multi-modal self-supervision objective through non-contrastive representation learning to learn relevant terrain representations for terrain-aware navigation. Through physical robot experiments
    
[^85]: 超越对数凹性：求解和优化和函数之和取负对数之最小化的理论和算法

    Beyond Log-Concavity: Theory and Algorithm for Sum-Log-Concave Optimization. (arXiv:2309.15298v1 [math.OC])

    [http://arxiv.org/abs/2309.15298](http://arxiv.org/abs/2309.15298)

    本文拓展了凸优化理论，提出了求解和函数对数凹函数最小化的算法，并应用于棋盘回归方法，扩展了逻辑回归到非线性可分问题。

    

    本文将经典的凸优化理论拓展到对求解和函数之和取负对数的函数的最小化问题上，我们称之为和函数对数凹函数。我们证明了这种函数通常不是凸函数，但仍然满足广义凸性不等式。这些不等式揭示了我们称之为交叉梯度的某个向量的重要性，该向量通常与常规梯度不同。因此，我们提出了交叉梯度下降（XGD）算法，它沿着交叉梯度的相反方向移动，并推导出收敛性分析。作为我们的和函数对数凹框架的应用，我们引入了所谓的棋盘回归方法，它依赖于和函数对数凹函数。这种分类器将（多类）逻辑回归扩展到非线性可分问题，因为它可以通过使用任意数量的超平面来分割特征空间，创建一个棋盘格。

    This paper extends the classic theory of convex optimization to the minimization of functions that are equal to the negated logarithm of what we term as a sum-log-concave function, i.e., a sum of log-concave functions. In particular, we show that such functions are in general not convex but still satisfy generalized convexity inequalities. These inequalities unveil the key importance of a certain vector that we call the cross-gradient and that is, in general, distinct from the usual gradient. Thus, we propose the Cross Gradient Descent (XGD) algorithm moving in the opposite direction of the cross-gradient and derive a convergence analysis. As an application of our sum-log-concave framework, we introduce the so-called checkered regression method relying on a sum-log-concave function. This classifier extends (multiclass) logistic regression to non-linearly separable problems since it is capable of tessellating the feature space by using any given number of hyperplanes, creating a checker
    
[^86]: 用于生物医学管道流动的多物理信息神经网络

    Multiple Physics-Informed Neural Network for Biomedical Tube Flows. (arXiv:2309.15294v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2309.15294](http://arxiv.org/abs/2309.15294)

    本研究探索了多情况PINN方法在计算生物医学管道流动中的应用，通过预训练和参数化不同几何情况，可以实时获取未见几何形状的结果，进而优化了传统CFD方法的使用。

    

    管道几何的流体力学计算在生物医学血管和气道流体力学评估中非常重要。最近，基于物理信息的神经网络（PINN）已经成为传统计算流体动力学（CFD）方法的一个不错替代方案。然而，普通的PINN对于每种特定流动场景需要更长的训练时间，因此无法证明其主流使用的合理性。在这里，我们探索了多情况PINN方法在计算生物医学管道流动中的应用，其中各种不同几何情况经过参数化和预训练在PINN上，从而可以实时获取未见几何形状的结果。我们的目标是通过在一系列理想化的2D狭窄管道流中进行实验，来确定网络架构、管道特定和正则化策略，以优化这一方法。

    Fluid dynamics computations for tube-like geometries are important for biomedical evaluation of vascular and airway fluid dynamics. Physics-Informed Neural Networks (PINNs) have recently emerged as a good alternative to traditional computational fluid dynamics (CFD) methods. The vanilla PINN, however, requires much longer training time than the traditional CFD methods for each specific flow scenario and thus does not justify its mainstream use. Here, we explore the use of the multi-case PINN approach for calculating biomedical tube flows, where varied geometry cases are parameterized and pre-trained on the PINN, such that results for unseen geometries can be obtained in real time. Our objective is to identify network architecture, tube-specific, and regularization strategies that can optimize this, via experiments on a series of idealized 2D stenotic tube flows.
    
[^87]: 最大扩散强化学习

    Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])

    [http://arxiv.org/abs/2309.15293](http://arxiv.org/abs/2309.15293)

    最大扩散强化学习是一种克服强化学习中数据相关性问题的方法，通过解耦代理的经验实现持续学习，并在各种测试中表现出色。

    

    所有机器学习都建立在数据独立且同分布的假设上。然而，在强化学习中，当数据是依次从代理经验中收集而来时，这一假设通常不成立。因此，我们提出了一种名为最大扩散强化学习的方法，利用统计力学中的遍历过程来克服这些限制。我们的方法通过解耦代理的经验，可证明地使代理在单次部署中能够持续学习，而不受初始化方式的影响。此外，我们证明了我们的方法推广了众所周知的最大熵技术，并且通过在流行的基准测试中稳定超过了最先进的性能水平。我们的研究成果极大地促进了物理学、学习和控制的交叉领域，为强化学习代理（如行走机器人和自动驾驶汽车）的透明可靠决策提供了一条道路。

    The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
    
[^88]: 从无处不在的心电图中扩展表示学习与状态空间模型

    Scaling Representation Learning from Ubiquitous ECG with State-Space Models. (arXiv:2309.15292v1 [cs.LG])

    [http://arxiv.org/abs/2309.15292](http://arxiv.org/abs/2309.15292)

    本论文介绍了一种名为WildECG的预训练状态空间模型，用于从无处不在的心电图中进行表示学习。通过在大规模的实际野外数据集上进行训练，该模型能够解决传统监督学习方法在处理大量数据和不同上下文下的挑战。

    

    从野外可穿戴设备的无处不在的传感中提取信息对于增强人类福祉具有巨大的潜力，从诊断临床病症和测量压力到构建自适应健康促进支架。但是，不同上下文中的大量数据对传统的监督学习方法提出了挑战。表示学习从生物信号中是一个新兴的领域，得益于计算建模的最新进展和公开共享数据库的丰富。心电图（ECG）是这一领域中研究的主要模态，其在健康监测、压力和情感估计方面具有应用。然而，大多数研究都受到小规模受控数据收集和过参数化架构选择的限制。我们介绍了一种名为\textbf{WildECG}的预训练状态空间模型，用于从ECG信号中进行表示学习。我们以无处不在收集的275,000个10秒ECG记录为训练数据，在无监督方式下训练该模型，并进行评估。

    Ubiquitous sensing from wearable devices in the wild holds promise for enhancing human well-being, from diagnosing clinical conditions and measuring stress to building adaptive health promoting scaffolds. But the large volumes of data therein across heterogeneous contexts pose challenges for conventional supervised learning approaches. Representation Learning from biological signals is an emerging realm catalyzed by the recent advances in computational modeling and the abundance of publicly shared databases. The electrocardiogram (ECG) is the primary researched modality in this context, with applications in health monitoring, stress and affect estimation. Yet, most studies are limited by small-scale controlled data collection and over-parameterized architecture choices. We introduce \textbf{WildECG}, a pre-trained state-space model for representation learning from ECG signals. We train this model in a self-supervised manner with 275,000 10s ECG recordings collected in the wild and eval
    
[^89]: SEPT: 为运动预测的高效场景表示学习

    SEPT: Towards Efficient Scene Representation Learning for Motion Prediction. (arXiv:2309.15289v1 [cs.CV])

    [http://arxiv.org/abs/2309.15289](http://arxiv.org/abs/2309.15289)

    SEPT是一个利用自监督学习进行场景表示学习的建模框架，通过预训练的编码器捕捉轨迹的运动学特征、道路网络的空间结构以及道路和代理之间的交互作用，实现了在运动预测任务上的最先进性能。

    

    运动预测对于自动驾驶汽车在复杂交通环境中安全运行至关重要。提取交通元素之间的有效时空关系是准确预测的关键。本文受到预训练大型语言模型成功应用的启发，提出了SEPT，这是一个利用自监督学习来开发复杂交通场景中强大的时空理解能力的建模框架。具体而言，我们的方法涉及到在场景输入上进行三个掩码重构建模任务，包括代理路径和道路网络，预训练场景编码器以捕捉轨迹的运动学特征，道路网络的空间结构以及道路和代理之间的交互作用。预训练的编码器然后在下游预测任务上进行微调。大量实验证明，SEPT在Argoverse 1和Argoverse上无需精心设计的架构或手动特征工程，达到了最先进的性能水平。

    Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse
    
[^90]: 可组合的核心集合用于行列式最大化：贪心算法几乎是最优的

    Composable Coresets for Determinant Maximization: Greedy is Almost Optimal. (arXiv:2309.15286v1 [cs.DS])

    [http://arxiv.org/abs/2309.15286](http://arxiv.org/abs/2309.15286)

    这项工作展示了在行列式最大化问题中，贪心算法提供了几乎最优的可组合核心集合，具有近似因子$O(k)^{3k}$。

    

    给定一组在$\mathbb{R}^d$中的$n$个向量，\emph{行列式最大化}问题的目标是选择具有最大体积的$k$个向量。行列式最大化是确定性点过程（DPP）的MAP推断任务，最近在建模多样性方面引起了相当大的关注。由于大多数问题应用使用大量数据，因此在相关的\textit{可组合的核心集合}设置中对该问题进行了研究。特别地，[Indyk-Mahabadi-OveisGharan-Rezaei--SODA'20, ICML'19]表明可以获得近似度为$\tilde O(k)^k$的问题的可组合核心集合，且局部搜索算法实现了近乎最优的近似保证$O(k)^{2k}$。在这项工作中，我们展示了广泛使用的贪心算法也可以提供具有几乎最优近似因子$O(k)^{3k}$的可组合核心集合，这改进了先前已知保证$C^{k^2}$的情况，并支持先前实验的结果。

    Given a set of $n$ vectors in $\mathbb{R}^d$, the goal of the \emph{determinant maximization} problem is to pick $k$ vectors with the maximum volume. Determinant maximization is the MAP-inference task for determinantal point processes (DPP) and has recently received considerable attention for modeling diversity. As most applications for the problem use large amounts of data, this problem has been studied in the relevant \textit{composable coreset} setting. In particular, [Indyk-Mahabadi-OveisGharan-Rezaei--SODA'20, ICML'19] showed that one can get composable coresets with optimal approximation factor of $\tilde O(k)^k$ for the problem, and that a local search algorithm achieves an almost optimal approximation guarantee of $O(k)^{2k}$. In this work, we show that the widely-used Greedy algorithm also provides composable coresets with an almost optimal approximation factor of $O(k)^{3k}$, which improves over the previously known guarantee of $C^{k^2}$, and supports the prior experimental 
    
[^91]: 一种用于交通状态预测的物理增强残差学习（PERL）框架

    A Physics Enhanced Residual Learning (PERL) Framework for Traffic State Prediction. (arXiv:2309.15284v1 [cs.LG])

    [http://arxiv.org/abs/2309.15284](http://arxiv.org/abs/2309.15284)

    这篇论文提出了一种名为物理增强残差学习（PERL）的框架，用于交通状态预测。PERL模型集成了物理模型和数据驱动模型的优势，通过将物理模型结果和预测残差作为修正相结合，具有可解释性，且比数据驱动方法要求更少的数据。

    

    在车辆轨迹预测中，物理模型和数据驱动模型是两种主要方法。然而，每种方法都存在自己的挑战：物理模型在可预测性方面不足，而数据驱动模型则缺乏可解释性。针对这些已确定的缺点，本文提出了一种新颖的框架，即物理增强残差学习（PERL）模型。PERL将物理模型和数据驱动方法的优势融合在一起，用于交通状态预测。PERL包括一个物理模型和一个残差学习模型。它的预测结果是物理模型结果和预测残差的和作为对其的修正。PERL保留了物理模型天然的可解释性，并且相比数据驱动方法具有较小的数据需求。我们使用实际车辆轨迹数据集进行了实验。我们提出了一个PERL模型，其中以智能驾驶模型（IDM）作为其物理车跟模型，长短期记忆（LSTM）模型用于学习残差。

    In vehicle trajectory prediction, physics models and data-driven models are two predominant methodologies. However, each approach presents its own set of challenges: physics models fall short in predictability, while data-driven models lack interpretability. Addressing these identified shortcomings, this paper proposes a novel framework, the Physics-Enhanced Residual Learning (PERL) model. PERL integrates the strengths of physics-based and data-driven methods for traffic state prediction. PERL contains a physics model and a residual learning model. Its prediction is the sum of the physics model result and a predicted residual as a correction to it. It preserves the interpretability inherent to physics-based models and has reduced data requirements compared to data-driven methods. Experiments were conducted using a real-world vehicle trajectory dataset. We proposed a PERL model, with the Intelligent Driver Model (IDM) as its physics car-following model and Long Short-Term Memory (LSTM) 
    
[^92]: 眼不见心不念：利用视频跟踪启用的记忆模型对未被观察到的对象进行推理和规划

    Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models. (arXiv:2309.15278v1 [cs.RO])

    [http://arxiv.org/abs/2309.15278](http://arxiv.org/abs/2309.15278)

    本文研究了如何对先前观察到但当前被遮挡的对象进行推理和规划，提出了利用转换器关系动力学编码轨迹历史的方法，并在多个挑战性任务中表现出色。

    

    机器人需要具有对先前观察到但当前被遮挡的对象的记忆，以在现实环境中可靠地工作。我们研究了将面向对象的记忆编码到多对象操纵推理和规划框架中的问题。我们提出了DOOM和LOOM，它们利用转换器关系动力学来编码给定部分视点云和对象发现与跟踪引擎的轨迹历史。我们的方法可以执行多个具有挑战性的任务，包括处理被遮挡的对象，新出现的对象，以及物体重新出现。通过广泛的仿真和真实世界实验，我们发现我们的方法在不同数量的对象和不同数量的干扰动作方面表现良好。此外，我们展示了我们的方法优于隐式记忆基线。

    Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline.
    
[^93]: Vision Transformer适应的高效低秩反向传播

    Efficient Low-rank Backpropagation for Vision Transformer Adaptation. (arXiv:2309.15275v1 [cs.CV])

    [http://arxiv.org/abs/2309.15275](http://arxiv.org/abs/2309.15275)

    本论文提出了一种名为LBP-WHT的新方法，用于解决视觉变换器（ViT）在反向传播中对计算资源的需求过高的问题。LBP-WHT方法通过将梯度投影到低秩空间并进行反向传播，显著减少了适应ViT所需的计算量。实验结果表明，LBP-WHT在多个数据集上对不同模型的适应性能都表现出色。

    

    视觉变换器（ViT）的规模不断增大，使得为特定需求对这些大模型进行高效微调成为各种应用中的重大挑战。这个问题源于ViT的线性层中需要的计算量大的矩阵乘法在反向传播过程中。在本文中，我们通过提出一种新的基于Walsh-Hadamard变换的低秩反向传播（LBP-WHT）方法来解决这个问题。直观地说，LBP-WHT将梯度投影到低秩空间，并进行反向传播。这种方法大大减少了适应ViT所需的计算量，因为低秩空间中的矩阵乘法较少占用资源。我们在多个数据集上使用不同模型（ViT、混合卷积-ViT模型）进行了广泛实验证明了我们方法的有效性。例如，在对CIFAR100上的EfficientFormer-L1模型进行适应时，我们的LBP-WHT相比标准方法提高了10.4%的准确率。

    The increasing scale of vision transformers (ViT) has made the efficient fine-tuning of these large models for specific needs a significant challenge in various applications. This issue originates from the computationally demanding matrix multiplications required during the backpropagation process through linear layers in ViT. In this paper, we tackle this problem by proposing a new Low-rank BackPropagation via Walsh-Hadamard Transformation (LBP-WHT) method. Intuitively, LBP-WHT projects the gradient into a low-rank space and carries out backpropagation. This approach substantially reduces the computation needed for adapting ViT, as matrix multiplication in the low-rank space is far less resource-intensive. We conduct extensive experiments with different models (ViT, hybrid convolution-ViT model) on multiple datasets to demonstrate the effectiveness of our method. For instance, when adapting an EfficientFormer-L1 model on CIFAR100, our LBP-WHT achieves 10.4% higher accuracy than the st
    
[^94]: STARC:评估奖励函数之间差异的通用框架

    STARC: A General Framework For Quantifying Differences Between Reward Functions. (arXiv:2309.15257v1 [cs.LG])

    [http://arxiv.org/abs/2309.15257](http://arxiv.org/abs/2309.15257)

    这篇论文提出了一个通用框架（STARC），用于评估奖励函数之间的差异，填补了奖励学习理论基础的空白。

    

    为了使用强化学习解决任务，需要将任务的目标形式化为奖励函数。然而，对于许多现实世界的任务来说，手动指定一个永不激励不良行为的奖励函数非常困难。因此，使用奖励学习算法来从数据中学习奖励函数变得越来越流行。然而，奖励学习的理论基础尚未完善。特别地，通常不知道给定的奖励学习算法在高概率下是否会学习到一个安全优化的奖励函数。这意味着奖励学习算法通常必须经过经验评估，这是昂贵的，并且很难预测其失效模式。其中一个阻碍获得更好理论保证的障碍是缺乏较好的方法来量化奖励函数之间的差异。在本文中，我们提供了一种解决方案。

    In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to predict in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution t
    
[^95]: 使用机器学习和线性规划进行每日奇幻足球最佳阵容的方法和验证

    Method and Validation for Optimal Lineup Creation for Daily Fantasy Football Using Machine Learning and Linear Programming. (arXiv:2309.15253v1 [cs.LG])

    [http://arxiv.org/abs/2309.15253](http://arxiv.org/abs/2309.15253)

    本文提出了一种方法来预测NFL球员的表现，并使用线性规划找到最佳阵容来最大化奇幻分数。实验结果表明，这种方法可以有效提高阵容的性能。

    

    每日奇幻体育是每周或每日的在线比赛，其中个人运动员的真实比赛表现被转换为奇幻分数（FPTS）。用户根据设定的球员工资上限选择阵容以最大化他们的FPTS。本文针对两个重点进行研究：（1）在不确定性下预测NFL球员表现的方法的开发，（2）在设定的工资限制下确定最佳阵容以最大化FPTS。创建了一个监督学习神经网络，根据过去的球员表现（本工作使用2018NFL常规赛季）预测FPTS，并将这些预测的FPTS用于混合整数线性规划中寻找最佳阵容。将生成的阵容的性能与随机生成的阵容进行了比较。平均而言，最佳阵容的性能优于随机阵容。然后将生成的阵容与DraftKings用户的真实阵容进行了比较。生成的阵容通常接近31%的真实阵容。

    Daily fantasy sports (DFS) are weekly or daily online contests where real-game performances of individual players are converted to fantasy points (FPTS). Users select players for their lineup to maximize their FPTS within a set player salary cap. This paper focuses on (1) the development of a method to forecast NFL player performance under uncertainty and (2) determining an optimal lineup to maximize FPTS under a set salary limit. A supervised learning neural network was created and used to project FPTS based on past player performance (2018 NFL regular season for this work) prior to the upcoming week. These projected FPTS were used in a mixed integer linear program to find the optimal lineup. The performance of resultant lineups was compared to randomly-created lineups. On average, the optimal lineups outperformed the random lineups. The generated lineups were then compared to real-world lineups from users on DraftKings. The generated lineups generally fell in approximately the 31st p
    
[^96]: V2X-Lead:基于激光雷达的端到端自动驾驶与车联网通信集成

    V2X-Lead: LiDAR-based End-to-End Autonomous Driving with Vehicle-to-Everything Communication Integration. (arXiv:2309.15252v1 [cs.RO])

    [http://arxiv.org/abs/2309.15252](http://arxiv.org/abs/2309.15252)

    V2X-Lead是一种基于激光雷达的自动驾驶方法，通过集成车联网通信来解决城市复杂场景下的自动驾驶挑战，通过融合激光雷达和V2X通信数据，采用深度强化学习算法训练驾驶代理以实现更安全和高效的穿越交叉口，具有良好的泛化性能。

    

    该论文提出了一种基于激光雷达的端到端自动驾驶方法，其中集成了车联网通信(V2X-Lead)，以解决在混合自动驾驶交通环境下导航非规范城市场景的挑战。所提出的方法旨在通过融合车载激光雷达传感器和V2X通信数据来处理不完美的部分观测。采用无模型和离策略深度强化学习(DRL)算法来训练驾驶代理，该算法结合了精心设计的奖励函数和多任务学习技术，以提高对多样化驾驶任务和场景的泛化能力。实验结果表明，所提出的方法在混合自动驾驶交通中穿越非信号交叉口的安全性和效率方面的有效性，并且在之前未见过的场景，如环形交叉口中的泛化性能。

    This paper presents a LiDAR-based end-to-end autonomous driving method with Vehicle-to-Everything (V2X) communication integration, termed V2X-Lead, to address the challenges of navigating unregulated urban scenarios under mixed-autonomy traffic conditions. The proposed method aims to handle imperfect partial observations by fusing the onboard LiDAR sensor and V2X communication data. A model-free and off-policy deep reinforcement learning (DRL) algorithm is employed to train the driving agent, which incorporates a carefully designed reward function and multi-task learning technique to enhance generalization across diverse driving tasks and scenarios. Experimental results demonstrate the effectiveness of the proposed approach in improving safety and efficiency in the task of traversing unsignalized intersections in mixed-autonomy traffic, and its generalizability to previously unseen scenarios, such as roundabouts. The integration of V2X communication offers a significant data source for
    
[^97]: SeMAnD:自监督多模式地理空间数据异常检测

    SeMAnD: Self-Supervised Anomaly Detection in Multimodal Geospatial Datasets. (arXiv:2309.15245v1 [cs.AI])

    [http://arxiv.org/abs/2309.15245](http://arxiv.org/abs/2309.15245)

    提出了一种名为SeMAnD的自监督异常检测技术，可用于检测多模态地理空间数据中的几何异常。该技术通过数据增强和自监督训练目标实现，能够有效地表示和识别不同模态数据中的局部变化和缺陷。

    

    我们提出了一种自监督异常检测技术，称为SeMAnD，用于检测多模态地理空间数据中的几何异常。地理空间数据包括获取和衍生的异构数据模态，我们将其转换为语义上有意义的、类似图像的张量，以解决多模态数据的表示、对齐和融合的挑战。SeMAnD由两部分组成：（i）一种简单的数据增强策略，称为RandPolyAugment，能够生成多样化的矢量几何增强，以及（ii）具有三个组件的自监督训练目标，激励学习对一种模态中的局部变化具有区别性的多模态数据表示，这种变化在其他模态中没有得到证实。在地理空间异常检测中，检测局部缺陷至关重要，即使是小的异常（如移位、错误连接、畸形或缺失的多边形矢量几何，如道路、建筑物、地表覆盖等）也是有害的。

    We propose a Self-supervised Anomaly Detection technique, called SeMAnD, to detect geometric anomalies in Multimodal geospatial datasets. Geospatial data comprises of acquired and derived heterogeneous data modalities that we transform to semantically meaningful, image-like tensors to address the challenges of representation, alignment, and fusion of multimodal data. SeMAnD is comprised of (i) a simple data augmentation strategy, called RandPolyAugment, capable of generating diverse augmentations of vector geometries, and (ii) a self-supervised training objective with three components that incentivize learning representations of multimodal data that are discriminative to local changes in one modality which are not corroborated by the other modalities. Detecting local defects is crucial for geospatial anomaly detection where even small anomalies (e.g., shifted, incorrectly connected, malformed, or missing polygonal vector geometries like roads, buildings, landcover, etc.) are detrimenta
    
[^98]: 无穷宽度两层ReLU神经网络的同伦松弛训练算法

    Homotopy Relaxation Training Algorithms for Infinite-Width Two-Layer ReLU Neural Networks. (arXiv:2309.15244v1 [cs.LG])

    [http://arxiv.org/abs/2309.15244](http://arxiv.org/abs/2309.15244)

    本文提出了一种名为同伦松弛训练算法（HRTA）的新的训练方法，它通过构建无缝连接线性激活函数和ReLU激活函数的同伦激活函数，并松弛同伦参数以增强训练精细化过程，加速了训练过程，在神经切线核（NTK）的背景下，实现了显著改进的收敛速度，并展示了对其他激活函数和深度神经网络的潜力。

    

    本文提出了一种新的训练方法，称为同伦松弛训练算法（HRTA），旨在加速训练过程，与传统方法相比。我们的算法结合了两个关键机制：一个是构建无缝连接线性激活函数和ReLU激活函数的同伦激活函数；另一个技术是松弛同伦参数以增强训练精细化过程。我们在神经切线核（NTK）的背景下对这种新方法进行了深入分析，揭示了显著改进的收敛速度。我们的实验结果，尤其是在考虑更大宽度的网络时，验证了理论结论。这种提议的HRTA展示了对其他激活函数和深度神经网络的潜力。

    In this paper, we present a novel training approach called the Homotopy Relaxation Training Algorithm (HRTA), aimed at accelerating the training process in contrast to traditional methods. Our algorithm incorporates two key mechanisms: one involves building a homotopy activation function that seamlessly connects the linear activation function with the ReLU activation function; the other technique entails relaxing the homotopy parameter to enhance the training refinement process. We have conducted an in-depth analysis of this novel method within the context of the neural tangent kernel (NTK), revealing significantly improved convergence rates. Our experimental results, especially when considering networks with larger widths, validate the theoretical conclusions. This proposed HRTA exhibits the potential for other activation functions and deep neural networks.
    
[^99]: 使用生成的特权信息学习通过文本到图像扩散模型

    Learning Using Generated Privileged Information by Text-to-Image Diffusion Models. (arXiv:2309.15238v1 [cs.CL])

    [http://arxiv.org/abs/2309.15238](http://arxiv.org/abs/2309.15238)

    本研究提出了一种利用生成的特权信息进行学习的框架，通过文本到图像扩散模型生成合成数据作为特权信息，进一步提升了学生模型在文本分类任务中的性能。

    

    使用生成的特权信息进行学习是一种特殊类型的知识蒸馏，其中教师模型在训练过程中从额外的数据表示中获益，这被称为特权信息，并改善了不看到额外表示的学生模型。然而，在实践中很少可获得特权信息。为此，我们提出了一种文本分类框架，利用文本到图像扩散模型生成人工特权信息。生成的图像和原始文本样本进一步用于基于最先进的基于转换器的架构来训练多模态教师模型。最后，多模态教师的知识被蒸馏到基于文本的（单模态）学生模型中。因此，通过使用生成模型产生合成数据作为特权信息，我们引导学生模型的训练。我们的框架称为利用生成的特权信息进行学习（LUGPI），可以显著提高性能。

    Learning Using Privileged Information is a particular type of knowledge distillation where the teacher model benefits from an additional data representation during training, called privileged information, improving the student model, which does not see the extra representation. However, privileged information is rarely available in practice. To this end, we propose a text classification framework that harnesses text-to-image diffusion models to generate artificial privileged information. The generated images and the original text samples are further used to train multimodal teacher models based on state-of-the-art transformer-based architectures. Finally, the knowledge from multimodal teachers is distilled into a text-based (unimodal) student. Hence, by employing a generative model to produce synthetic data as privileged information, we guide the training of the student model. Our framework, called Learning Using Generated Privileged Information (LUGPI), yields noticeable performance g
    
[^100]: 用于训练和测试共现网络推理算法的交叉验证

    Cross-Validation for Training and Testing Co-occurrence Network Inference Algorithms. (arXiv:2309.15225v1 [cs.LG])

    [http://arxiv.org/abs/2309.15225](http://arxiv.org/abs/2309.15225)

    该论文介绍了一种用于训练和测试共现网络推理算法的交叉验证方法，通过研究微生物群落和其相互作用，提供对各种疾病的见解。

    

    微生物存在于几乎所有的环境中，包括土壤、水、空气和其他生物体内，如动物和植物。虽然一些微生物会引起疾病，但大多数微生物在生物过程中起到帮助分解、发酵和养分循环的作用。已经进行了大量研究，以研究各种环境中的微生物群落及其相互作用和关系如何为各种疾病提供见解。共现网络推理算法帮助我们理解微生物的复杂关联，特别是细菌。现有的网络推理算法采用相关性、正则化线性回归和条件依赖等技术，这些技术具有不同的超参数，确定网络的稀疏程度。以往评估推断网络质量的方法包括使用外部数据和在子样本中网络的一致性，但这些方法都有一些局限性。

    Microorganisms are found in almost every environment, including the soil, water, air, and inside other organisms, like animals and plants. While some microorganisms cause diseases, most of them help in biological processes such as decomposition, fermentation and nutrient cycling. A lot of research has gone into studying microbial communities in various environments and how their interactions and relationships can provide insights into various diseases. Co-occurrence network inference algorithms help us understand the complex associations of micro-organisms, especially bacteria. Existing network inference algorithms employ techniques such as correlation, regularized linear regression, and conditional dependence, which have different hyper-parameters that determine the sparsity of the network. Previous methods for evaluating the quality of the inferred network include using external data, and network consistency across sub-samples, both which have several drawbacks that limit their appli
    
[^101]: 对抗性语音合成的协同水印技术

    Collaborative Watermarking for Adversarial Speech Synthesis. (arXiv:2309.15224v1 [eess.AS])

    [http://arxiv.org/abs/2309.15224](http://arxiv.org/abs/2309.15224)

    本文提出了一种对抗性语音合成的协同水印技术，通过与现有对策模型合作进行训练，实现了对生成语音的有效检测和水印识别。

    

    神经语音合成的进展使得技术不仅接近人类的自然度，而且能够以少量数据进行即时语音克隆，并且借助预训练模型具有高度可访问性。当然，生成内容的潜在泛滥引起了对合成语音检测和水印技术的需求。最近，合成语音检测的研究工作主要集中在自动说话人验证和欺骗对策挑战（ASVspoof）上，该挑战专注于被动对策。本文从另一角度出发，针对生成语音的检测，提出了一种协同训练方案，以在不干扰人类听众的情况下，能够通过协同机器检测到生成语音的水印。我们提出了一种与ASVspoof 2021基线对策模型合作的HiFi-GAN神经声码器的合作训练方案，并展示了其有效性。

    Advances in neural speech synthesis have brought us technology that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. Recently, considerable research effort in synthetic speech detection has been related to the Automatic Speaker Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on passive countermeasures. This paper takes a complementary view to generated speech detection: a synthesis system should make an active effort to watermark the generated speech in a way that aids detection by another machine, but remains transparent to a human listener. We propose a collaborative training scheme for synthetic speech watermarking and show that a HiFi-GAN neural vocoder collaborating with the ASVspoof 2021 baseline countermeasure models consis
    
[^102]: 大规模语言模型重评分的低秩适应技术在参数高效的语音识别中的应用

    Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])

    [http://arxiv.org/abs/2309.15223](http://arxiv.org/abs/2309.15223)

    这篇论文介绍了一种基于低秩适应技术的神经语言建模系统，用于语音识别的输出重评分。通过使用低秩分解方法和优化插入矩阵，该系统能够以更高效的方式将BERT模型适应到新领域，大大减少了训练时间。

    

    我们提出了一种基于低秩适应（LoRA）的神经语言建模系统，用于语音识别输出重评分。尽管预训练的语言模型（如BERT）在第二次重评分中表现出优越的性能，但将预训练阶段扩展和将预训练模型适应到特定领域的高计算成本限制了它们在重评分中的实际应用。我们提出了一种基于低秩分解的方法，仅使用预训练参数的一小部分（0.08%）来训练重评分的BERT模型并将其适应到新领域。这些插入的矩阵通过相关性正则化损失和判别性训练目标进行优化。所提出的低秩适应Rescore-BERT（LoRB）体系结构在LibriSpeech和内部数据集上评估，训练时间减少了5.4至3.6倍。

    We propose a neural language modeling system based on low-rank adaptation (LoRA) for speech recognition output rescoring. Although pretrained language models (LMs) like BERT have shown superior performance in second-pass rescoring, the high computational cost of scaling up the pretraining stage and adapting the pretrained models to specific domains limit their practical use in rescoring. Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters. These inserted matrices are optimized through a discriminative training objective along with a correlation-based regularization loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets with decreased training times by factors between 5.4 and 3.6.
    
[^103]: 使用CodeBERT和Random Forest Regressor进行自动评分的C编程作业

    Auto-grading C programming assignments with CodeBERT and Random Forest Regressor. (arXiv:2309.15216v1 [cs.LG])

    [http://arxiv.org/abs/2309.15216](http://arxiv.org/abs/2309.15216)

    本研究提供了使用机器学习和深度学习方法对C编程作业进行自动评分的分析，并引入了基于代码的转换器词嵌入模型CodeBERT。测试结果表明该策略的有效性，并讨论了统计方法和深度学习技术之间的对比。

    

    手动评分编程作业因复杂性和主观性而具有挑战性。然而，使用深度学习进行自动评分简化了任务。它客观地评估代码质量，检测错误，并准确地分配分数，减轻了教师的负担，同时确保了高效和公平的评估。本研究使用回归、卷积神经网络（CNN）和长短期记忆（LSTM）等机器学习和深度学习方法对C编程作业进行自动评分的分析。使用一种基于代码的转换器词嵌入模型CodeBERT，将文本代码输入转换为向量，然后将向量输入到几个模型中。测试结果证明了建议策略的有效性，均方根误差（RMSE）为1.89。本研究还讨论了统计方法和深度学习技术之间的对比。

    Grading coding assignments manually is challenging due to complexity and subjectivity. However, auto-grading with deep learning simplifies the task. It objectively assesses code quality, detects errors, and assigns marks accurately, reducing the burden on instructors while ensuring efficient and fair assessment. This study provides an analysis of auto-grading of the C programming assignments using machine learning and deep learning approaches like regression, convolutional neural networks (CNN) and long short-term memory (LSTM). Using a code-based transformer word embedding model called CodeBERT, the textual code inputs were transformed into vectors, and the vectors were then fed into several models. The testing findings demonstrated the efficacy of the suggested strategy with a root mean squared error (RMSE) of 1.89. The contrast between statistical methods and deep learning techniques is discussed in the study.
    
[^104]: 用于千米尺度大气降尺度的生成残差扩散建模

    Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling. (arXiv:2309.15214v1 [cs.LG])

    [http://arxiv.org/abs/2309.15214](http://arxiv.org/abs/2309.15214)

    一种用于千米尺度大气降尺度的生成残差扩散建模方法被提出，并展示了在天气和气候的物理灾害预测方面具有潜力。

    

    当前从天气和气候中进行物理灾害预测的最先进方法需要进行昂贵的千米尺度数值模拟，并驱动较粗分辨率的全球输入。本文提出了一种千米尺度降尺度扩散模型作为一种具有成本效益的替代方法。该模型是从台湾的区域高分辨率天气模型训练得到的，并在ERA5再分析数据的基础下进行了条件训练。为了解决降尺度的不确定性，大分辨率比率（25km至2km），不同尺度上涉及的不同物理过程以及在输入数据中不存在的预测通道，我们采用了一个两步的方法（ResDiff），其中一个（UNet）回归在第一步预测平均值，而扩散模型在第二步预测残差。\textit{ResDiff}在块均方根误差和CRPS得分上表现出了令人鼓舞的技能。ResDiff预测的光谱和分布忠实地恢复了调节有害风和雨的重要幂律关系。统一的天气现象案例研究

    The state of the art for physical hazard prediction from weather and climate requires expensive km-scale numerical simulations driven by coarser resolution global inputs. Here, a km-scale downscaling diffusion model is presented as a cost effective alternative. The model is trained from a regional high-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis data. To address the downscaling uncertainties, large resolution ratios (25km to 2km), different physics involved at different scales and predict channels that are not in the input data, we employ a two-step approach (\textit{ResDiff}) where a (UNet) regression predicts the mean in the first step and a diffusion model predicts the residual in the second step. \textit{ResDiff} exhibits encouraging skill in bulk RMSE and CRPS scores. The predicted spectra and distributions from ResDiff faithfully recover important power law relationships regulating damaging wind and rain extremes. Case studies of coherent weather phen
    
[^105]: 在基于机器学习的时间序列预测中平衡计算效率和预测误差：从气象预报的实时实验中获得的洞见

    Balancing Computational Efficiency and Forecast Error in Machine Learning-based Time-Series Forecasting: Insights from Live Experiments on Meteorological Nowcasting. (arXiv:2309.15207v1 [cs.LG])

    [http://arxiv.org/abs/2309.15207](http://arxiv.org/abs/2309.15207)

    本研究通过实时实验，以气象预报为例，量化了机器学习的计算成本与预测误差之间的关系。研究结果有助于在时间序列预测中平衡计算效率和预测准确性。

    

    机器学习在时间序列预测中的应用仍然是一个重要的研究领域。尽管许多机器学习技术已经成功应用，但计算效率和预测误差之间的关系仍然是一个未被充分探索的领域。本文通过一系列实时实验，以气象预报作为示例应用，量化计算成本与预测误差之间的关系。我们采用了多种流行的回归技术（XGBoost，FC-MLP，Transformer和LSTM）对多地点的温度、风速和云量等三个变量进行多时段、短期预测。在为期5天的实时实验中，我们以每小时推断144个模型的速度训练和推断了4000个数据源。这些模型使用了两种计算成本最小化方法进行参数化，分别是一种新颖的自适应数据减少技术（Variance Horizon）和一种基于性能的概念漂移检测机制。

    Machine learning for time-series forecasting remains a key area of research. Despite successful application of many machine learning techniques, relating computational efficiency to forecast error remains an under-explored domain. This paper addresses this topic through a series of real-time experiments to quantify the relationship between computational cost and forecast error using meteorological nowcasting as an example use-case. We employ a variety of popular regression techniques (XGBoost, FC-MLP, Transformer, and LSTM) for multi-horizon, short-term forecasting of three variables (temperature, wind speed, and cloud cover) for multiple locations. During a 5-day live experiment, 4000 data sources were streamed for training and inferencing 144 models per hour. These models were parameterized to explore forecast error for two computational cost minimization methods: a novel auto-adaptive data reduction technique (Variance Horizon) and a performance-based concept drift-detection mechani
    
[^106]: ICML 2023拓扑深度学习挑战：设计与结果

    ICML 2023 Topological Deep Learning Challenge : Design and Results. (arXiv:2309.15188v1 [cs.LG])

    [http://arxiv.org/abs/2309.15188](http://arxiv.org/abs/2309.15188)

    本文介绍了ICML 2023拓扑深度学习挑战，该挑战要求参与者在两个月内提供开源实现的拓扑神经网络，吸引了28个合格的提交。

    

    本文介绍了ICML 2023拓扑与几何机器学习研讨会中举办的拓扑深度学习计算挑战。该比赛要求参与者通过贡献于python包TopoNetX（数据处理）和TopoModelX（深度学习）的开源实现来提供文献中的拓扑神经网络。该挑战在两个月的时间内吸引了28个合格的提交。本文描述了挑战的设计并总结了其主要发现。

    This paper presents the computational challenge on topological deep learning that was hosted within the ICML 2023 Workshop on Topology and Geometry in Machine Learning. The competition asked participants to provide open-source implementations of topological neural networks from the literature by contributing to the python packages TopoNetX (data processing) and TopoModelX (deep learning). The challenge attracted twenty-eight qualifying submissions in its two-month duration. This paper describes the design of the challenge and summarizes its main findings.
    
[^107]: 监测机器学习模型：在线检测相关偏差

    Monitoring Machine Learning Models: Online Detection of Relevant Deviations. (arXiv:2309.15187v1 [cs.LG])

    [http://arxiv.org/abs/2309.15187](http://arxiv.org/abs/2309.15187)

    本论文提出了一种用于监测机器学习模型的在线检测方案，通过考虑模型质量的时间依赖性，可以减少不必要的警报并优化对相关变化的检测。

    

    机器学习模型是各个领域中重要的工具，但其性能可能会随时间的推移而降低，原因是数据分布的变化或其他因素。一方面，检测和解决这种降级对于保持模型的可靠性至关重要。另一方面，给定足够的数据，可以检测到任意小的质量变化。由于模型重新训练或替换等干预措施可能代价高昂，我们认为仅当变化超过给定阈值时才应该进行这些干预措施。我们提出了一种顺序监测方案来检测这些相关变化。所提出的方法通过考虑所测量模型质量的时间依赖性来减少不必要的警报并克服多重测试问题。文中提供了一致性和指定渐近水平的条件。使用模拟和真实数据进行的实证验证证明了我们的方法在检测模型质量相关变化方面的优越性，相比基准方法

    Machine learning models are essential tools in various domains, but their performance can degrade over time due to changes in data distribution or other factors. On one hand, detecting and addressing such degradations is crucial for maintaining the models' reliability. On the other hand, given enough data, any arbitrary small change of quality can be detected. As interventions, such as model re-training or replacement, can be expensive, we argue that they should only be carried out when changes exceed a given threshold. We propose a sequential monitoring scheme to detect these relevant changes. The proposed method reduces unnecessary alerts and overcomes the multiple testing problem by accounting for temporal dependence of the measured model quality. Conditions for consistency and specified asymptotic levels are provided. Empirical validation using simulated and real data demonstrates the superiority of our approach in detecting relevant changes in model quality compared to benchmark m
    
[^108]: 保守的世界模型

    Conservative World Models. (arXiv:2309.15178v1 [cs.LG])

    [http://arxiv.org/abs/2309.15178](http://arxiv.org/abs/2309.15178)

    在零样本强化学习中，研究人员探索了在小样本数据集上训练时，前向-后向算法性能下降的问题，并使用保守性算法来缓解此问题。实验证明，保守的前向-后向算法在总体上表现更好，甚至超过了特定任务的基准算法。

    

    零样本强化学习承诺在离线预训练阶段后，提供能够在任何环境中执行任何任务的代理。前向-后向（FB）表示在这个理想的实现上取得了显著进展，可以在这种设置中达到特定任务代理的85%的性能。然而，这样的性能取决于对于大规模且多样化的数据集的访问，而大多数真实问题无法期望这样的数据集。在这里，我们探讨了在训练集缺乏多样性的情况下FB性能如何降低，并通过保守性来减轻这种情况，这是一个成熟的离线RL算法的特点。我们在各种数据集、领域和任务上评估了我们的方法家族，在总体上达到了150%的普通FB性能。有些令人惊讶的是，保守的FB算法在没有访问奖励标签且需要维护所有任务策略的情况下，也优于特定任务基线。

    Zero-shot reinforcement learning (RL) promises to provide agents that can perform any task in an environment after an offline pre-training phase. Forward-backward (FB) representations represent remarkable progress towards this ideal, achieving 85% of the performance of task-specific agents in this setting. However, such performance is contingent on access to large and diverse datasets for pre-training, which cannot be expected for most real problems. Here, we explore how FB performance degrades when trained on small datasets that lack diversity, and mitigate it with conservatism, a well-established feature of performant offline RL algorithms. We evaluate our family of methods across various datasets, domains and tasks, reaching 150% of vanilla FB performance in aggregate. Somewhat surprisingly, conservative FB algorithms also outperform the task-specific baseline, despite lacking access to reward labels and being required to maintain policies for all tasks. Conservative FB algorithms p
    
[^109]: 揭示空间-时间掩蔽自动编码器在多元时间序列预测中的力量

    Revealing the Power of Spatial-Temporal Masked Autoencoders in Multivariate Time Series Forecasting. (arXiv:2309.15169v1 [cs.LG])

    [http://arxiv.org/abs/2309.15169](http://arxiv.org/abs/2309.15169)

    提出了一个利用空间-时间掩蔽自动编码器（STMAE）来提高多元时间序列（MTS）预测性能的框架，通过新颖的双掩蔽策略处理部分可见的MTS数据，包括空间掩蔽和时间掩蔽。

    

    多元时间序列（MTS）预测涉及基于历史观测来预测未来时间序列数据。现有研究主要强调开发能够明确捕捉时间序列变量之间的空间依赖性和时间相关性的复杂空间-时间模型。然而，最近的进展受到了数据稀缺性和模型鲁棒性的挑战。为了解决这些问题，我们提出了空间-时间掩蔽自动编码器（STMAE），这是一个MTS预测框架，利用掩蔽自动编码器来提高空间-时间基线模型的性能。STMAE包括两个学习阶段。在预训练阶段，采用编码器-解码器架构。编码器通过一种新颖的双掩蔽策略处理部分可见的MTS数据，包括基于偏置随机游走的空间掩蔽和基于补丁的时间掩蔽。随后，解码器旨在重构两个掩蔽对应物。

    Multivariate time series (MTS) forecasting involves predicting future time series data based on historical observations. Existing research primarily emphasizes the development of complex spatial-temporal models that capture spatial dependencies and temporal correlations among time series variables explicitly. However, recent advances have been impeded by challenges relating to data scarcity and model robustness. To address these issues, we propose Spatial-Temporal Masked Autoencoders (STMAE), an MTS forecasting framework that leverages masked autoencoders to enhance the performance of spatial-temporal baseline models. STMAE consists of two learning stages. In the pretraining stage, an encoder-decoder architecture is employed. The encoder processes the partially visible MTS data produced by a novel dual-masking strategy, including biased random walk-based spatial masking and patch-based temporal masking. Subsequently, the decoders aim to reconstruct the masked counterparts from both spa
    
[^110]: AI算法在电动汽车能量管理中的应用综述

    A Review on AI Algorithms for Energy Management in E-Mobility Services. (arXiv:2309.15140v1 [cs.LG])

    [http://arxiv.org/abs/2309.15140](http://arxiv.org/abs/2309.15140)

    本文综述了AI算法在电动汽车能量管理中的应用，并探讨了其在解决各种挑战和实现能量管理的有效性方面的作用。

    

    电动出行作为解决交通部门紧迫的环境和可持续性问题的关键解决方案已经崛起。化石燃料的消耗、不断上升的温室气体排放以及对抗气候变化的迫切需求凸显了向电动汽车发展的重要性。本文旨在探索人工智能在电动汽车能量管理中面临的各种挑战，并提出解决方案。这些挑战涉及关键因素如续航焦虑、充电速率优化以及电动汽车能量存储的寿命。通过分析现有文献，我们深入探讨了人工智能在解决这些问题和实现电动汽车能量管理有效性方面的作用。我们的目标是提供当前研究领域的最新发展概述，并提出未来研究的有效途径。

    E-mobility, or electric mobility, has emerged as a pivotal solution to address pressing environmental and sustainability concerns in the transportation sector. The depletion of fossil fuels, escalating greenhouse gas emissions, and the imperative to combat climate change underscore the significance of transitioning to electric vehicles (EVs). This paper seeks to explore the potential of artificial intelligence (AI) in addressing various challenges related to effective energy management in e-mobility systems (EMS). These challenges encompass critical factors such as range anxiety, charge rate optimization, and the longevity of energy storage in EVs. By analyzing existing literature, we delve into the role that AI can play in tackling these challenges and enabling efficient energy management in EMS. Our objectives are twofold: to provide an overview of the current state-of-the-art in this research domain and propose effective avenues for future investigations. Through this analysis, we a
    
[^111]: PINF：物理约束深度学习的连续标准化流

    PINF: Continuous Normalizing Flows for Physics-Constrained Deep Learning. (arXiv:2309.15139v1 [cs.LG])

    [http://arxiv.org/abs/2309.15139](http://arxiv.org/abs/2309.15139)

    本文提出了PINF，这是物理约束深度学习的连续标准化流的一种扩展。该方法通过特征值法则和扩散来解决高维时变和稳态福克-普朗克方程。

    

    概率密度的标准化约束对于解决福克-普朗克方程构成了重大挑战。标准化流是一种可逆生成模型，利用变量变换公式确保概率密度守恒，并能够学习复杂数据分布。本文介绍了连续标准化流的新颖扩展——物理约束标准化流（PINF），通过特征值法则结合扩散，实现了无网格和无因果性的高效解决高维时变和稳态福克-普朗克方程。

    The normalization constraint on probability density poses a significant challenge for solving the Fokker-Planck equation. Normalizing Flow, an invertible generative model leverages the change of variables formula to ensure probability density conservation and enable the learning of complex data distributions. In this paper, we introduce Physics-Informed Normalizing Flows (PINF), a novel extension of continuous normalizing flows, incorporating diffusion through the method of characteristics. Our method, which is mesh-free and causality-free, can efficiently solve high dimensional time-dependent and steady-state Fokker-Planck equations.
    
[^112]: 深度生成方法用于发电系统预测轨迹的生成

    Deep Generative Methods for Producing Forecast Trajectories in Power Systems. (arXiv:2309.15137v1 [cs.LG])

    [http://arxiv.org/abs/2309.15137](http://arxiv.org/abs/2309.15137)

    该论文研究了深度生成方法在发电系统预测轨迹中的应用，通过adapt autoregressive networks和normalizing flows捕捉多变量时间序列的时空相关性，相比当前的copula-based统计方法表现出更好的有效性。实验结果基于法国TSO RTE风力预测数据。

    

    随着可再生能源在电力混合中的扩张，电网的变化性将增加，因此需要加强系统以保证其安全性。因此，输电系统运营商必须进行分析，模拟未来发电系统的运行情况。然后，这些模拟结果被用作决策过程的输入。在这种情况下，我们研究使用深度学习模型来生成能源产量和负荷预测轨迹。为了捕捉这些多变量时间序列中的时空相关性，我们改进了自回归网络和归一化流模型，并证明它们对比当前基于copula的统计方法的有效性。我们对法国输电系统运营商RTE的风力预测数据进行了大量实验，并使用特定的时间序列生成评估指标比较了不同的模型。

    With the expansion of renewables in the electricity mix, power grid variability will increase, hence a need to robustify the system to guarantee its security. Therefore, Transport System Operators (TSOs) must conduct analyses to simulate the future functioning of power systems. Then, these simulations are used as inputs in decision-making processes. In this context, we investigate using deep learning models to generate energy production and load forecast trajectories. To capture the spatiotemporal correlations in these multivariate time series, we adapt autoregressive networks and normalizing flows, demonstrating their effectiveness against the current copula-based statistical approach. We conduct extensive experiments on the French TSO RTE wind forecast data and compare the different models with \textit{ad hoc} evaluation metrics for time series generation.
    
[^113]: 对比度连续多视角聚类与过滤结构融合

    Contrastive Continual Multi-view Clustering with Filtered Structural Fusion. (arXiv:2309.15135v1 [cs.LG])

    [http://arxiv.org/abs/2309.15135](http://arxiv.org/abs/2309.15135)

    提出了一种名为对比度连续多视角聚类与过滤结构融合（CCMVC-FSF）的新方法，用于解决多视角聚类在实时数据收集中的困难。该方法旨在防止先前知识遗忘和利用数据相关性指导新视图的聚类过程。

    

    多视角聚类适用于先前收集视图并提取一致和互补信息的应用，但忽略了数据视图按顺序收集的实时数据的情况。为了解决这个问题，我们提出了一种名为对比度连续多视角聚类与过滤结构融合（CCMVC-FSF）的新方法，以应对先前知识遗忘和新视图聚类的问题。

    Multi-view clustering thrives in applications where views are collected in advance by extracting consistent and complementary information among views. However, it overlooks scenarios where data views are collected sequentially, i.e., real-time data. Due to privacy issues or memory burden, previous views are not available with time in these situations. Some methods are proposed to handle it but are trapped in a stability-plasticity dilemma. In specific, these methods undergo a catastrophic forgetting of prior knowledge when a new view is attained. Such a catastrophic forgetting problem (CFP) would cause the consistent and complementary information hard to get and affect the clustering performance. To tackle this, we propose a novel method termed Contrastive Continual Multi-view Clustering with Filtered Structural Fusion (CCMVC-FSF). Precisely, considering that data correlations play a vital role in clustering and prior knowledge ought to guide the clustering process of a new view, we de
    
[^114]: 从资产流到状态、行动和意图的发现：加密货币中的早期恶意检测

    From Asset Flow to Status, Action and Intention Discovery: Early Malice Detection in Cryptocurrency. (arXiv:2309.15133v1 [cs.LG])

    [http://arxiv.org/abs/2309.15133](http://arxiv.org/abs/2309.15133)

    本文提出了一种用于比特币的早期恶意检测的意图监控系统，通过定义资产转移路径和提取状态和行动，实现了对不同恶意类型的检测和发现。

    

    由于加密货币交易主体的伪匿名性质，加密货币往往比传统金融资产更容易受到非法活动的影响。理想的检测模型应该满足早期检测、良好可解释性和适用于各种非法活动的三个关键属性。然而，现有的解决方案无法满足所有这些要求，因为它们大多依赖于深度学习而缺乏可解释性，并且只适用于特定非法类型的回顾性分析。为了解决所有这些挑战，我们提出了一种用于比特币 (BTC) 的早期恶意检测的意图监控系统，其中某个地址的链上记录数据比其他加密货币平台更稀缺。我们首先使用基于决策树的特征选择和补充（DT-SC）来定义资产转移路径，为不同的恶意类型构建不同的特征集。然后，通过状态/行动提案模块（S/A-PM）获取可能的状态和行动，进一步发现恶意意图。

    Cryptocurrency has been subject to illicit activities probably more often than traditional financial assets due to the pseudo-anonymous nature of its transacting entities. An ideal detection model is expected to achieve all three critical properties of (I) early detection, (II) good interpretability, and (III) versatility for various illicit activities. However, existing solutions cannot meet all these requirements, as most of them heavily rely on deep learning without interpretability and are only available for retrospective analysis of a specific illicit type. To tackle all these challenges, we propose Intention-Monitor for early malice detection in Bitcoin (BTC), where the on-chain record data for a certain address are much scarcer than other cryptocurrency platforms. We first define asset transfer paths with the Decision-Tree based feature Selection and Complement (DT-SC) to build different feature sets for different malice types. Then, the Status/Action Proposal Module (S/A-PM) an
    
[^115]: 高维成像遗传学研究中的互信息最大化探索——Genetic InfoMax

    Genetic InfoMax: Exploring Mutual Information Maximization in High-Dimensional Imaging Genetics Studies. (arXiv:2309.15132v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.15132](http://arxiv.org/abs/2309.15132)

    本研究通过互信息的角度探索了基因组关联研究中图像遗传学的表示学习问题，并引入了一种跨模态学习框架Genetic InfoMax（GIM），该方法在人脑三维MRI数据上表现出显著改进的性能。

    

    基因组关联研究主要用于识别基因变异与特定特征之间的关系。将其应用于高维医学成像数据时，关键步骤是提取低维但有信息量的数据表示作为特征。由于与典型的视觉表示学习相比，GWAS所面临的独特挑战，使得图像遗传学的表示学习尚未得到全面探索。在本研究中，我们从互信息的角度解决了这个问题，指出了现有方法的主要局限，并引入了一种基因信息最大化（Genetic InfoMax，GIM）的跨模态学习框架，其中包括正则化的互信息估计器和一种新颖的基因信息启示的转换器，以应对GWAS的特定挑战。我们在人脑三维MRI数据上评估了GIM，并建立了标准化评估协议，与现有方法进行比较。结果表明，GIM具有较好的效果，并在GWAS上表现出显著改进的性能。

    Genome-wide association studies (GWAS) are used to identify relationships between genetic variations and specific traits. When applied to high-dimensional medical imaging data, a key step is to extract lower-dimensional, yet informative representations of the data as traits. Representation learning for imaging genetics is largely under-explored due to the unique challenges posed by GWAS in comparison to typical visual representation learning. In this study, we tackle this problem from the mutual information (MI) perspective by identifying key limitations of existing methods. We introduce a trans-modal learning framework Genetic InfoMax (GIM), including a regularized MI estimator and a novel genetics-informed transformer to address the specific challenges of GWAS. We evaluate GIM on human brain 3D MRI data and establish standardized evaluation protocols to compare it to existing approaches. Our results demonstrate the effectiveness of GIM and a significantly improved performance on GWAS
    
[^116]: 使用无监督学习理解QM7b和QM9量子力学数据集的结构

    Understanding the Structure of QM7b and QM9 Quantum Mechanical Datasets Using Unsupervised Learning. (arXiv:2309.15130v1 [physics.chem-ph])

    [http://arxiv.org/abs/2309.15130](http://arxiv.org/abs/2309.15130)

    本文使用无监督学习方法研究了QM7b和QM9量子力学数据集的内部结构和特征，发现这两个数据集的内在维度要比描述维度小，QM7b数据由与原子组成相关的明确定义的簇组成，而QM9数据包括一个由异常值组成的外部区域和一个集中了聚类和内部对象的内部核心区域。这些研究结果对于从属性预测原子组成具有重要意义。

    

    本文探索了两个量子力学数据集（QM7b、QM9）的内部结构，这些数据集由数千个有机分子组成，并以电子性质描述。了解这类数据的结构和特征在从属性预测原子组成方面是重要的。本研究使用内在维度分析、聚类和异常检测方法。研究结果表明，对于这两个数据集，内在维度要比描述维度小几倍。QM7b数据由与原子组成相关的明确定义的簇组成。QM9数据包括一个主要由异常值组成的外部区域，以及一个集中了聚类和内部对象的内部核心区域。分子中的原子数量与其异常/内部特性之间存在显著关系。尽管结构存在差异，感兴趣的变量的可预测性在两个数据集中是存在的。

    This paper explores the internal structure of two quantum mechanics datasets (QM7b, QM9), composed of several thousands of organic molecules and described in terms of electronic properties. Understanding the structure and characteristics of this kind of data is important when predicting the atomic composition from the properties in inverse molecular designs. Intrinsic dimension analysis, clustering, and outlier detection methods were used in the study. They revealed that for both datasets the intrinsic dimensionality is several times smaller than the descriptive dimensions. The QM7b data is composed of well defined clusters related to atomic composition. The QM9 data consists of an outer region predominantly composed of outliers, and an inner core region that concentrates clustered, inliner objects. A significant relationship exists between the number of atoms in the molecule and its outlier/inner nature. Despite the structural differences, the predictability of variables of interest f
    
[^117]: 用CogEval评估大型语言模型中的认知地图和规划能力

    Evaluating Cognitive Maps and Planning in Large Language Models with CogEval. (arXiv:2309.15129v1 [cs.AI])

    [http://arxiv.org/abs/2309.15129](http://arxiv.org/abs/2309.15129)

    这项研究提出了CogEval协议，用于系统评估大型语言模型的认知能力，并使用该协议对八个LLMs的认知地图和规划能力进行了评估。

    

    最近，大量的研究声称大型语言模型（LLMs）具有新兴的认知能力。然而，大多数研究依赖于案例，忽视了训练集的污染，或者缺乏涉及多个任务、控制条件、多次迭代和统计鲁棒性测试的系统评估。在这里，我们做出了两个重大贡献。首先，我们提出了CogEval，这是一个受认知科学启发的协议，用于对大型语言模型的认知能力进行系统评估。CogEval协议可以用于评估各种能力。其次，我们使用CogEval协议对八个LLMs（OpenAI GPT-4、GPT-3.5-turbo-175B、davinci-003-175B、Google Bard、Cohere-xlarge-52.4B、Anthropic Claude-1-52B、LLaMA-13B和Alpaca-7B）的认知地图和规划能力进行了系统评估。我们的任务提示基于人类实验，既具有评估规划的已建立构造效度，又不存在于LLM的训练集中。我们发现，尽管LLMs展示了一些

    Recently an influx of studies claim emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in Large Language Models. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show a
    
[^118]: DPA-WNO：一类随机力学问题的灰箱模型

    DPA-WNO: A gray box model for a class of stochastic mechanics problem. (arXiv:2309.15128v1 [cs.LG])

    [http://arxiv.org/abs/2309.15128](http://arxiv.org/abs/2309.15128)

    DPA-WNO是一种将可解释性的数据驱动模型与小波神经操作符相结合的新方法，用于纠正/识别缺失的物理，并解决了纯数据驱动模型的缺点。

    

    在科学和工程中，众所周知的物理定律常常基于某些假设和近似。因此，基于这些方程进行的分析和设计也是近似的。数据驱动模型的出现在一定程度上解决了这个挑战；然而，纯数据驱动模型往往存在以下问题：(a)缺乏可解释性，(b)需要大量数据，(c)无法超越训练范围的泛化能力。最近，操作符学习被提出作为一个潜在的替代方案来解决上述挑战；然而，这些挑战仍然存在。我们在这里认为，可能的解决方案之一存在于数据物理融合中，其中数据驱动模型用于纠正/识别缺失的物理。为此，我们提出了一种新颖的可微分物理增强小波神经操作符(DPA-WNO)。该提出的DPA-WNO将可微分物理求解器与小波神经操作符(WNO)融合在一起，其中WNO的作用是模拟

    The well-known governing physics in science and engineering is often based on certain assumptions and approximations. Therefore, analyses and designs carried out based on these equations are also approximate. The emergence of data-driven models has, to a certain degree, addressed this challenge; however, the purely data-driven models often (a) lack interpretability, (b) are data-hungry, and (c) do not generalize beyond the training window. Operator learning has recently been proposed as a potential alternative to address the aforementioned challenges; however, the challenges are still persistent. We here argue that one of the possible solutions resides in data-physics fusion, where the data-driven model is used to correct/identify the missing physics. To that end, we propose a novel Differentiable Physics Augmented Wavelet Neural Operator (DPA-WNO). The proposed DPA-WNO blends a differentiable physics solver with the Wavelet Neural Operator (WNO), where the role of WNO is to model the 
    
[^119]: Grad DFT：一种用于机器学习增强密度泛函理论的软件库

    Grad DFT: a software library for machine learning enhanced density functional theory. (arXiv:2309.15127v1 [physics.chem-ph])

    [http://arxiv.org/abs/2309.15127](http://arxiv.org/abs/2309.15127)

    Grad DFT是一种机器学习增强的密度泛函理论（DFT）软件库，通过使用权重和神经网络处理交换关联能量泛函，对DFT的能力进行了扩展。

    

    密度泛函理论（DFT）作为计算量子化学和材料科学中的基石方法，因其出色的多功能性和可扩展性而闻名。然而，在处理强关联系统时，DFT存在精度限制。为了解决这些缺点，最近的研究开始探索如何利用机器学习扩展DFT的能力，这是一个充满许多开放问题和技术挑战的努力。在这项工作中，我们介绍了Grad DFT：一个完全可微的基于JAX的DFT库，能够快速原型设计和实验机器学习增强的交换关联能量泛函。Grad DFT采用了一种先驱性的交换关联泛函参数化方法，该方法使用能量密度的加权和来确定权重，权重通过神经网络确定。此外，Grad DFT包含了一套全面的辅助函数，其中最重要的特点是可即时编译和完全可微的。

    Density functional theory (DFT) stands as a cornerstone method in computational quantum chemistry and materials science due to its remarkable versatility and scalability. Yet, it suffers from limitations in accuracy, particularly when dealing with strongly correlated systems. To address these shortcomings, recent work has begun to explore how machine learning can expand the capabilities of DFT; an endeavor with many open questions and technical challenges. In this work, we present Grad DFT: a fully differentiable JAX-based DFT library, enabling quick prototyping and experimentation with machine learning-enhanced exchange-correlation energy functionals. Grad DFT employs a pioneering parametrization of exchange-correlation functionals constructed using a weighted sum of energy densities, where the weights are determined using neural networks. Moreover, Grad DFT encompasses a comprehensive suite of auxiliary functions, notably featuring a just-in-time compilable and fully differentiable s
    
[^120]: 从肽到纳米结构：一种用于快速稳定的机器学习力场的欧几里得变换器

    From Peptides to Nanostructures: A Euclidean Transformer for Fast and Stable Machine Learned Force Fields. (arXiv:2309.15126v1 [physics.chem-ph])

    [http://arxiv.org/abs/2309.15126](http://arxiv.org/abs/2309.15126)

    这项研究提出了一种称为SO3krates的欧几里得变换器架构，它通过组合稀疏等变表示和自注意机制，在机器学习力场中实现了精度、稳定性和速度的独特组合，从而使我们能够在前所未有的时间和系统尺度上对物质的量子属性进行深入分析。

    

    近年来，基于从头计算的机器学习力场（MLFFs）的发展取得了巨大进展。尽管在测试误差上取得了较低的效果，但MLFFs在分子动力学（MD）模拟中的适用性越来越受到人们的关注，因为其稳定性受到质疑。我们的研究结果表明MLFFs中的等变表示与MD模拟稳定性之间可能存在潜在联系，但计算成本可能限制了它们带来的实际优势。为了解决这个问题，我们提出了一种叫做SO3krates的变换器架构，它结合了稀疏等变表示（欧几里得变量）和自注意机制，可以分离不变和等变信息，消除了昂贵的张量积操作。SO3krates实现了精度、稳定性和速度的独特组合，使得我们能够在前所未有的时间和系统尺度上对物质的量子属性进行深入分析。

    Recent years have seen vast progress in the development of machine learned force fields (MLFFs) based on ab-initio reference calculations. Despite achieving low test errors, the suitability of MLFFs in molecular dynamics (MD) simulations is being increasingly scrutinized due to concerns about instability. Our findings suggest a potential connection between MD simulation stability and the presence of equivariant representations in MLFFs, but their computational cost can limit practical advantages they would otherwise bring.  To address this, we propose a transformer architecture called SO3krates that combines sparse equivariant representations (Euclidean variables) with a self-attention mechanism that can separate invariant and equivariant information, eliminating the need for expensive tensor products. SO3krates achieves a unique combination of accuracy, stability, and speed that enables insightful analysis of quantum properties of matter on unprecedented time and system size scales. T
    
[^121]: 揭示分子表示学习中的神经缩放定律

    Uncovering Neural Scaling Laws in Molecular Representation Learning. (arXiv:2309.15123v1 [physics.chem-ph])

    [http://arxiv.org/abs/2309.15123](http://arxiv.org/abs/2309.15123)

    从数据中心的角度研究了分子表示学习的神经缩放行为，发现了数据量和性能之间的一致幂律关系，并提出了潜在的提高学习效率的方法。

    

    分子表示学习（MRL）已经成为药物和材料发现的强大工具，在虚拟筛选和反向设计等各种任务中发挥着重要作用。虽然对于分子表示的数据数量和质量对MRL的影响的模型中心技术的推进引起了大量关注，但在这个领域内，对于这两个因素的影响还不完全清楚。在本文中，我们从数据中心的角度深入研究了MRL的神经缩放行为，考察了四个关键维度：（1）数据模态，（2）数据集划分，（3）预训练的作用，和（4）模型容量。我们的实证研究证实了数据量和MRL性能之间的一致的幂律关系。此外，通过详细分析，我们确定了提高学习效率的潜在途径。为了挑战这些缩放定律，我们将七种常见的数据修剪策略应用于分子数据并进行了性能评估。我们的研究结果强调了改善学习效率的潜在途径。

    Molecular Representation Learning (MRL) has emerged as a powerful tool for drug and materials discovery in a variety of tasks such as virtual screening and inverse design. While there has been a surge of interest in advancing model-centric techniques, the influence of both data quantity and quality on molecular representations is not yet clearly understood within this field. In this paper, we delve into the neural scaling behaviors of MRL from a data-centric viewpoint, examining four key dimensions: (1) data modalities, (2) dataset splitting, (3) the role of pre-training, and (4) model capacity. Our empirical studies confirm a consistent power-law relationship between data volume and MRL performance across these dimensions. Additionally, through detailed analysis, we identify potential avenues for improving learning efficiency. To challenge these scaling laws, we adapt seven popular data pruning strategies to molecular data and benchmark their performance. Our findings underline the im
    
[^122]: QA-LoRA: 基于量化意识的大语言模型低秩适应

    QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v1 [cs.LG])

    [http://arxiv.org/abs/2309.14717](http://arxiv.org/abs/2309.14717)

    本文提出了QA-LoRA算法，它通过使用量化意识以及组内运算符来实现大语言模型的低秩适应。QA-LoRA能够将模型权重量化以减少时间和内存的使用，同时在不损失准确性的情况下将模型集成为一个量化模型。

    

    近年来，大型语言模型（LLMs）得到了快速发展。尽管在许多语言理解任务中具有强大的能力，但沉重的计算负担在很大程度上限制了LLMs的应用，特别是当需要将它们部署到边缘设备时。本文提出了一种基于量化意识的低秩适应（QA-LoRA）算法。动机在于量化和适应的自由度不平衡，解决方案是使用组内运算符，增加量化的自由度，同时减少适应的自由度。QA-LoRA可以用几行代码轻松实现，并使原始的LoRA具备了两个能力：（i）在微调过程中，LLM的权重被量化（例如转换为INT4），以减少时间和内存的使用；（ii）经过微调后，LLM和辅助权重自然地集成到一个量化模型中，而不会损失准确性。我们将QA-LoRA应用到LLaMA和LLaMA2模型家族中。

    Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model famil
    
[^123]: 零阶黎曼平均随机逼近算法

    Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms. (arXiv:2309.14506v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2309.14506](http://arxiv.org/abs/2309.14506)

    本论文提出了一种用于黎曼流形上随机优化的零阶黎曼平均随机逼近算法（Zo-RASA），通过使用黎曼移动平均随机梯度估计器和新颖的黎曼-李雅普诺夫分析技术，实现了生成ε-近似的一阶稳定解的最优样本复杂度，同时通过使用回缩和向量传输替代指数映射和平行传输降低了算法的每次迭代复杂度。

    

    我们提出了一种用于黎曼流形上随机优化的零阶黎曼平均随机逼近（Zo-RASA）算法。我们证明Zo-RASA仅使用每次迭代中的一个样本或常数阶的批处理就能实现生成ε-近似的一阶稳定解的最优样本复杂度。我们的方法采用了黎曼移动平均随机梯度估计器，并利用了一种新颖的黎曼-李雅普诺夫分析技术进行收敛分析。通过使用回缩和向量传输代替指数映射和平行传输，我们改进了算法的可行性，从而降低了每次迭代的复杂度。此外，我们还引入了一个新颖的几何条件，满足有有界第二基本形式的流形，从而为用向量传输逼近平行传输提供新的误差界。

    We present Zeroth-order Riemannian Averaging Stochastic Approximation (\texttt{Zo-RASA}) algorithms for stochastic optimization on Riemannian manifolds. We show that \texttt{Zo-RASA} achieves optimal sample complexities for generating $\epsilon$-approximation first-order stationary solutions using only one-sample or constant-order batches in each iteration. Our approach employs Riemannian moving-average stochastic gradient estimators, and a novel Riemannian-Lyapunov analysis technique for convergence analysis. We improve the algorithm's practicality by using retractions and vector transport, instead of exponential mappings and parallel transports, thereby reducing per-iteration complexity. Additionally, we introduce a novel geometric condition, satisfied by manifolds with bounded second fundamental form, which enables new error bounds for approximating parallel transport with vector transport.
    
[^124]: Era Splitting.（arXiv:2309.14496v1 [cs.LG]）

    Era Splitting. (arXiv:2309.14496v1 [cs.LG])

    [http://arxiv.org/abs/2309.14496](http://arxiv.org/abs/2309.14496)

    本研究提出了两种新的分裂准则，使得决策树模型能够利用时代信息进行优化，从而将超分布泛化研究中的思想应用于决策树模型。

    

    现实生活中的机器学习问题在时间和空间上会呈现出数据的分布变化。这种行为超出了传统的经验风险最小化范式的范围，该范式假设数据在时间和地点上是独立同分布的。新兴的超分布泛化领域通过将环境或时代信息融入算法中，来应对这个现实。迄今为止，大部分研究都集中在线性模型和/或神经网络上。在本研究中，我们针对决策树模型，包括随机森林和梯度提升决策树，开发了两种新的分裂准则，使得树模型能够利用与每个数据点相关的时代信息，来找到在数据的所有不相交时代中都是最优的切分点，从而将超分布泛化研究中的思想应用于决策树模型。

    Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
    
[^125]: 自恢复提示：基于基础模型和自恢复的通用服务机器人系统

    Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery. (arXiv:2309.14425v1 [cs.RO])

    [http://arxiv.org/abs/2309.14425](http://arxiv.org/abs/2309.14425)

    本文研究开发了一个通用服务机器人系统，该系统可以根据不同任务和环境的变化进行自适应，并通过自恢复机制解决信息不足、计划生成错误和执行失败等问题，实现了任务的成功完成。

    

    通用服务机器人（GPSR）能够在各种环境中执行多种任务，需要一个具有高通用性和适应性的系统来应对不同的任务和环境。本文首先基于多个基础模型开发了一个顶层GPSR系统，用于全球竞赛（RoboCup@Home 2023）。该系统既可以适应多种变化，又可以通过提示每个模型来实现自适应。然后，通过分析所开发系统的性能，我们发现在更加现实的GPSR应用设置中存在三种失败类型：信息不足、错误的计划生成和计划执行失败。我们提出了自恢复提示管道，该管道探索必要的信息，并修改其提示来从失败中恢复。我们通过实验证实，具有自恢复机制的系统可以通过解决各种失败案例来完成任务。供补充的视频可在https://sites.google.com/view/srgpsr上找到。

    A general-purpose service robot (GPSR), which can execute diverse tasks in various environments, requires a system with high generalizability and adaptability to tasks and environments. In this paper, we first developed a top-level GPSR system for worldwide competition (RoboCup@Home 2023) based on multiple foundation models. This system is both generalizable to variations and adaptive by prompting each model. Then, by analyzing the performance of the developed system, we found three types of failure in more realistic GPSR application settings: insufficient information, incorrect plan generation, and plan execution failure. We then propose the self-recovery prompting pipeline, which explores the necessary information and modifies its prompts to recover from failure. We experimentally confirm that the system with the self-recovery mechanism can accomplish tasks by resolving various failure cases. Supplementary videos are available at https://sites.google.com/view/srgpsr .
    
[^126]: 看见和听到没被说的话：可解释融合的多模态动机性访谈客户行为分类器

    Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion. (arXiv:2309.14398v1 [cs.LG])

    [http://arxiv.org/abs/2309.14398](http://arxiv.org/abs/2309.14398)

    本文提出了一个多模态分类器，在动机性访谈中准确区分了变化话语、持续话语和跟随/中立话语三种类别。该分类器利用文本、声调、面部表情和身体表现等多模态特征，并对AnnoMI数据集进行了注释和训练。研究还找到了决策过程中最重要的模态，提供了宝贵的洞察。

    

    动机性访谈（MI）是一种强调合作并鼓励行为改变的治疗方法。为了评估MI对话的质量，可以利用MISC代码将客户话语分类为变化话语、持续话语或跟随/中立话语。MI对话中变化话语的比例与治疗结果呈正相关，因此准确分类客户话语至关重要。本文提出了一个分类器，利用文本、声调、面部表情和身体表现等多模态特征准确区分三个MISC类别（变化话语、持续话语和跟随/中立话语）。为了训练我们的模型，我们对公开可用的AnnoMI数据集进行注释，收集了文本、音频、面部表情和身体表现等多模态信息。此外，我们还确定了决策过程中最重要的模态，提供了宝贵的洞察。

    Motivational Interviewing (MI) is an approach to therapy that emphasizes collaboration and encourages behavioral change. To evaluate the quality of an MI conversation, client utterances can be classified using the MISC code as either change talk, sustain talk, or follow/neutral talk. The proportion of change talk in a MI conversation is positively correlated with therapy outcomes, making accurate classification of client utterances essential. In this paper, we present a classifier that accurately distinguishes between the three MISC classes (change talk, sustain talk, and follow/neutral talk) leveraging multimodal features such as text, prosody, facial expressivity, and body expressivity. To train our model, we perform annotations on the publicly available AnnoMI dataset to collect multimodal information, including text, audio, facial expressivity, and body expressivity. Furthermore, we identify the most important modalities in the decision-making process, providing valuable insights i
    
[^127]: 使用鞍点边界的马丁格尔混合改进随机线性Bandit算法

    Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures. (arXiv:2309.14298v1 [stat.ML])

    [http://arxiv.org/abs/2309.14298](http://arxiv.org/abs/2309.14298)

    本研究提出了一种改进的随机线性Bandit算法，利用鞍点边界的马丁格尔混合构建了适用于随机Bandit的置信序列，并证明该算法能够以竞争性的最坏情况下遗憾保证实现更好的性能。

    

    我们提出了一种对随机线性Bandit问题具有最坏情况下遗憾保证的改进算法。广泛使用的"面对不确定性时的乐观原则"可以将随机Bandit问题转化为对未知奖励函数构建置信序列的问题。结果算法的性能取决于置信序列的大小，置信集较小可提供更好的经验性能和更强的遗憾保证。本研究中，我们使用了一种对自适应马丁格尔混合的尾部边界来构建适用于随机Bandit的置信序列。这些置信序列允许通过凸规划进行高效的动作选择。我们证明了基于我们的置信序列的线性Bandit算法能够保证达到具有竞争力的最坏情况下遗憾。我们实证和理论上证明了我们的置信序列比竞争对手更紧致。最后，我们证明了我们的紧致置信序列可以提供和置信集比较容易配置的更好的性能。

    We present improved algorithms with worst-case regret guarantees for the stochastic linear bandit problem. The widely used "optimism in the face of uncertainty" principle reduces a stochastic bandit problem to the construction of a confidence sequence for the unknown reward function. The performance of the resulting bandit algorithm depends on the size of the confidence sequence, with smaller confidence sets yielding better empirical performance and stronger regret guarantees. In this work, we use a novel tail bound for adaptive martingale mixtures to construct confidence sequences which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming. We prove that a linear bandit algorithm based on our confidence sequences is guaranteed to achieve competitive worst-case regret. We show that our confidence sequences are tighter than competitors, both empirically and theoretically. Finally, we demonstrate that our tighter confi
    
[^128]: 增强强化学习中的数据效率：基于网格信息传播的新型想象机制

    Enhancing data efficiency in reinforcement learning: a novel imagination mechanism based on mesh information propagation. (arXiv:2309.14243v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14243](http://arxiv.org/abs/2309.14243)

    这项研究提出了一种基于网格信息传播的想象机制，在强化学习算法中显著提高了数据效率。通过使信息在不同状态间广播，而不仅仅是在同一状态集中传输，这种机制促进了模型对状态间相互依赖性的理解，并提高了对有限样本信息的学习效率。

    

    强化学习算法在处理高维状态空间和大规模问题时面临数据效率有限的挑战。大多数强化学习方法在更新智能体的评论家时往往仅依赖于同一集中的状态转换信息，这可能导致数据效率低和训练时间消耗亚优。受到人类类比推理能力的启发，我们引入了一种名为“想象机制（IM）”的新型网格信息传播机制，旨在显著提高强化学习算法的数据效率。具体而言，IM使得由单个样本生成的信息能够有效地广播到不同的集中状态，而不仅仅是在同一集中传输。这种能力增强了模型对状态间相互依赖性的理解，并促进了对有限样本信息更高效的学习。为了提高多功能性，我们将IM扩展为一种可以作为

    Reinforcement learning(RL) algorithms face the challenge of limited data efficiency, particularly when dealing with high-dimensional state spaces and large-scale problems. Most of RL methods often rely solely on state transition information within the same episode when updating the agent's Critic, which can lead to low data efficiency and sub-optimal training time consumption. Inspired by human-like analogical reasoning abilities, we introduce a novel mesh information propagation mechanism, termed the 'Imagination Mechanism (IM)', designed to significantly enhance the data efficiency of RL algorithms. Specifically, IM enables information generated by a single sample to be effectively broadcasted to different states across episodes, instead of simply transmitting in the same episode. This capability enhances the model's comprehension of state interdependencies and facilitates more efficient learning of limited sample information. To promote versatility, we extend the IM to function as a
    
[^129]: ICU 重新入院预测的可解释机器学习

    Explainable Machine Learning for ICU Readmission Prediction. (arXiv:2309.13781v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13781](http://arxiv.org/abs/2309.13781)

    本研究提出了一个标准化且可解释的机器学习流程，用于在多中心数据库中预测加护病房患者的再入院情况。

    

    加护病房（ICU）是一个复杂的医院环境，医生的决策对患者的生命构成高风险。必须遵循一条全面的护理路径来减少并发症。在这种环境中，不确定性、竞争性和非计划性的因素增加了统一实施护理路径的困难。再入院是该路径的困难之一，即患者在短时间内再次入住ICU，导致高死亡率和高资源利用率。一些研究尝试通过患者的医疗信息来预测再入院情况。尽管它们在预测再入院时有一定的成功，但这些研究并未对再入院预测进行适当的评估、描述和理解。本研究提出了一个标准化且可解释的机器学习流程，用于在多中心数据库（即包含166,355名患者的eICU队列，200,859名...）

    The intensive care unit (ICU) comprises a complex hospital environment, where decisions made by clinicians have a high level of risk for the patients' lives. A comprehensive care pathway must then be followed to reduce p complications. Uncertain, competing and unplanned aspects within this environment increase the difficulty in uniformly implementing the care pathway. Readmission contributes to this pathway's difficulty, occurring when patients are admitted again to the ICU in a short timeframe, resulting in high mortality rates and high resource utilisation. Several works have tried to predict readmission through patients' medical information. Although they have some level of success while predicting readmission, those works do not properly assess, characterise and understand readmission prediction. This work proposes a standardised and explainable machine learning pipeline to model patient readmission on a multicentric database (i.e., the eICU cohort with 166,355 patients, 200,859 ad
    
[^130]: 论文标题：The Rashomon Importance Distribution: 摆脱不稳定的基于单一模型的变量重要性

    The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance. (arXiv:2309.13775v1 [cs.LG])

    [http://arxiv.org/abs/2309.13775](http://arxiv.org/abs/2309.13775)

    提出了一种新的变量重要性框架，该框架在数据分布上是稳定的，并可以与现有的模型类和全局变量重要性指标结合使用。

    

    量化变量重要性对于回答遗传学、公共政策和医学等领域的重大问题至关重要。当前的方法通常计算给定数据集上训练的给定模型的变量重要性。然而，对于给定数据集，可能有许多模型同样能解释目标结果;如果不考虑所有可能的解释，不同的研究者可能会得出许多冲突但同样有效的结论。此外，即使考虑了给定数据集的所有可能解释，这些洞察力可能不具有普适性，因为并非所有好的解释在合理的数据扰动下都是稳定的。我们提出了一种新的变量重要性框架，该框架量化了在所有好的模型集合中的变量重要性，并且在数据分布上是稳定的。我们的框架非常灵活，可以与大多数现有的模型类和全局变量重要性指标结合使用。

    Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We d
    
[^131]: 架构对多光谱深度神经网络的稳健性和解释性的影响

    Impact of architecture on robustness and interpretability of multispectral deep neural networks. (arXiv:2309.12463v1 [cs.CV])

    [http://arxiv.org/abs/2309.12463](http://arxiv.org/abs/2309.12463)

    这项工作研究了不同融合策略对多光谱深度学习模型性能，依赖性和稳健性的影响。

    

    包含额外光谱波段（例如近红外）的信息可以提高深度学习模型在许多视觉任务中的性能。有许多可能的方法将这些额外信息纳入深度学习模型中，但最佳融合策略尚未确定，并且在不同应用场景中可能有所变化。在其中一个极端，称为"早期融合"，额外波段被堆叠为额外通道，以获得具有超过三个通道的输入图像。在另一个极端，称为"晚期融合"，RGB和非RGB波段通过深度学习模型的单独分支传递，并在最终分类或分割层之前立即合并。在这项工作中，我们表征了一套不同融合方法的多光谱深度学习模型的性能，量化了它们对不同输入波段的相对依赖性，并评估了它们对影响一个或多个输入通道的自然图像失真的稳健性。

    Including information from additional spectral bands (e.g., near-infrared) can improve deep learning model performance for many vision-oriented tasks. There are many possible ways to incorporate this additional information into a deep learning model, but the optimal fusion strategy has not yet been determined and can vary between applications. At one extreme, known as "early fusion," additional bands are stacked as extra channels to obtain an input image with more than three channels. At the other extreme, known as "late fusion," RGB and non-RGB bands are passed through separate branches of a deep learning model and merged immediately before a final classification or segmentation layer. In this work, we characterize the performance of a suite of multispectral deep learning models with different fusion approaches, quantify their relative reliance on different input bands and evaluate their robustness to naturalistic image corruptions affecting one or more input channels.
    
[^132]: 由堆叠回归减少误差

    Error Reduction from Stacked Regressions. (arXiv:2309.09880v1 [stat.ML])

    [http://arxiv.org/abs/2309.09880](http://arxiv.org/abs/2309.09880)

    本文提出了一种新的堆叠回归方法，通过最小化总体风险并受非负性约束，成功降低了误差。实验证明，堆叠估计器相比其中最佳的单个估计器具有更小的总体风险。

    

    堆叠回归是一种集成技术，它通过形成不同回归估计器的线性组合来提高预测准确性。传统方法使用交叉验证数据来生成由构成估计器预测，并使用带非负性约束的最小二乘法学习组合权重。在本文中，我们类似地通过最小化一种估计的总体风险来学习这些权重，并受到非负性约束。当构成的估计器是通过至少三个维度分隔的嵌套子空间的线性最小二乘投影时，我们证明由于收缩效应，所得到的堆叠估计器的总体风险严格小于其中最佳的单个估计器。这里的“最佳”是指最小化选择准则如AIC或BIC的模型。换句话说，在这种情况下，最佳的单个估计器是不可接受的。因为优化问题可以重构为同信息回归，所以...

    Stacking regressions is an ensemble technique that forms linear combinations of different regression estimators to enhance predictive accuracy. The conventional approach uses cross-validation data to generate predictions from the constituent estimators, and least-squares with nonnegativity constraints to learn the combination weights. In this paper, we learn these weights analogously by minimizing an estimate of the population risk subject to a nonnegativity constraint. When the constituent estimators are linear least-squares projections onto nested subspaces separated by at least three dimensions, we show that thanks to a shrinkage effect, the resulting stacked estimator has strictly smaller population risk than best single estimator among them. Here ``best'' refers to a model that minimizes a selection criterion such as AIC or BIC. In other words, in this setting, the best single estimator is inadmissible. Because the optimization problem can be reformulated as isotonic regression, t
    
[^133]: 对自编码器的几何角度的研究

    A Geometric Perspective on Autoencoders. (arXiv:2309.08247v1 [cs.LG])

    [http://arxiv.org/abs/2309.08247](http://arxiv.org/abs/2309.08247)

    本文从几何角度研究了自编码器框架，并提出了解决多解和畸变表示问题的几何方法。

    

    本文提出了自编码器框架的几何方面，尽管其重要性，但被相对较少地认识到。给定一组几乎位于某个较低维度流形上的高维数据点，自编码器同时学习流形和其坐标图。这种几何角度自然引发了一些问题，比如“有限的数据点对应于单一的流形吗？”或者“只有一个坐标图可以表示流形吗？”对这些问题的回答是否定的，这意味着给定一个数据集，有多个解的自编码器。因此，它们有时会产生具有严重畸变的潜在空间表示的错误流形。在本文中，我们介绍了解决这些问题的最近的几何方法。

    This paper presents the geometric aspect of the autoencoder framework, which, despite its importance, has been relatively less recognized. Given a set of high-dimensional data points that approximately lie on some lower-dimensional manifold, an autoencoder learns the \textit{manifold} and its \textit{coordinate chart}, simultaneously. This geometric perspective naturally raises inquiries like "Does a finite set of data points correspond to a single manifold?" or "Is there only one coordinate chart that can represent the manifold?". The responses to these questions are negative, implying that there are multiple solution autoencoders given a dataset. Consequently, they sometimes produce incorrect manifolds with severely distorted latent space representations. In this paper, we introduce recent geometric approaches that address these issues.
    
[^134]: 具有未测混淆因素的广义线性模型的同时推断

    Simultaneous inference for generalized linear models with unmeasured confounders. (arXiv:2309.07261v1 [stat.ME])

    [http://arxiv.org/abs/2309.07261](http://arxiv.org/abs/2309.07261)

    本文研究了存在混淆效应时的广义线性模型的大规模假设检验问题，并提出了一种利用正交结构和线性投影的统计估计和推断框架，解决了由于未测混淆因素引起的偏差问题。

    

    在基因组研究中，常常进行成千上万个同时假设检验，以确定差异表达的基因。然而，由于存在未测混淆因素，许多标准统计方法可能存在严重的偏差。本文研究了存在混淆效应时的多元广义线性模型的大规模假设检验问题。在任意混淆机制下，我们提出了一个统一的统计估计和推断方法，利用正交结构并将线性投影整合到三个关键阶段中。首先，利用多元响应变量分离边际和不相关的混淆效应，恢复混淆系数的列空间。随后，利用$\ell_1$正则化进行稀疏性估计，并强加正交性限制于混淆系数，联合估计潜在因子和主要效应。最后，我们结合投影和加权偏差校正步骤。

    Tens of thousands of simultaneous hypothesis tests are routinely performed in genomic studies to identify differentially expressed genes. However, due to unmeasured confounders, many standard statistical approaches may be substantially biased. This paper investigates the large-scale hypothesis testing problem for multivariate generalized linear models in the presence of confounding effects. Under arbitrary confounding mechanisms, we propose a unified statistical estimation and inference framework that harnesses orthogonal structures and integrates linear projections into three key stages. It first leverages multivariate responses to separate marginal and uncorrelated confounding effects, recovering the confounding coefficients' column space. Subsequently, latent factors and primary effects are jointly estimated, utilizing $\ell_1$-regularization for sparsity while imposing orthogonality onto confounding coefficients. Finally, we incorporate projected and weighted bias-correction steps 
    
[^135]: 通过半结构激活稀疏加速深度神经网络

    Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity. (arXiv:2309.06626v1 [cs.CV])

    [http://arxiv.org/abs/2309.06626](http://arxiv.org/abs/2309.06626)

    通过小型运行时修改引入半结构激活稀疏性，我们设计了一种稀疏训练过程，在保持精度下降最小的情况下，实现了深度神经网络的高效处理和推断加速。

    

    在嵌入式设备上高效处理深度神经网络（DNNs）的需求是限制其部署的重要挑战之一。利用网络特征图中的稀疏性是减少推断延迟的一种方式。已知非结构化稀疏性与结构化稀疏性相比对精度下降的影响较小，但前者需要进行广泛的推断引擎更改以获得延迟优势。为了解决这个问题，我们提出了一种通过小型运行时修改引入半结构激活稀疏性的解决方案。为了在推断时获得高加速度水平，我们设计了一种稀疏训练过程，同时在计算广义矩阵乘法（GEMM）时考虑激活的最终位置。我们对各种图像分类和目标检测任务的模型对所提出的解决方案进行了广泛评估。值得注意的是，我们的方法在保持精度下降最小的情况下提供了1.25倍的速度提升。

    The demand for efficient processing of deep neural networks (DNNs) on embedded devices is a significant challenge limiting their deployment. Exploiting sparsity in the network's feature maps is one of the ways to reduce its inference latency. It is known that unstructured sparsity results in lower accuracy degradation with respect to structured sparsity but the former needs extensive inference engine changes to get latency benefits. To tackle this challenge, we propose a solution to induce semi-structured activation sparsity exploitable through minor runtime modifications. To attain high speedup levels at inference time, we design a sparse training procedure with awareness of the final position of the activations while computing the General Matrix Multiplication (GEMM). We extensively evaluate the proposed solution across various models for image classification and object detection tasks. Remarkably, our approach yields a speed improvement of $1.25 \times$ with a minimal accuracy drop 
    
[^136]: ssVERDICT：用于增强前列腺肿瘤表征的自监督VERDICT-MRI

    ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation. (arXiv:2309.06268v1 [eess.IV])

    [http://arxiv.org/abs/2309.06268](http://arxiv.org/abs/2309.06268)

    本研究引入了一种自监督DNN方法，用于估计前列腺肿瘤的VERDICT模型参数并减少计算成本。

    

    MRI在前列腺癌（PCa）的诊断中越来越被使用，其中扩散MRI（dMRI）起着重要作用。当与计算模型结合时，dMRI可以估计细胞大小等微观结构信息。传统上，这些模型使用非线性最小二乘（NLLS）曲线拟合方法进行拟合，与高计算成本相关。监督式深度神经网络（DNN）是一种高效的替代方法，但其性能受到合成训练数据的底层分布的显著影响。自监督学习是一种有吸引力的替代方法，网络在此方法中学习输入数据的特征，而不使用单独的训练数据集。到目前为止，这种方法仅应用于对微不足道的dMRI模型的拟合。在这里，我们引入了一种自监督DNN，用于估计前列腺VERDICT（Tumours中的血管、细胞外和受限扩散）模型的参数。我们展示了，通过这种方法，用于前列腺肿瘤的VERDICT模型的参数可以有效估计并且具有较低的计算成本。

    MRI is increasingly being used in the diagnosis of prostate cancer (PCa), with diffusion MRI (dMRI) playing an integral role. When combined with computational models, dMRI can estimate microstructural information such as cell size. Conventionally, such models are fit with a nonlinear least squares (NLLS) curve fitting approach, associated with a high computational cost. Supervised deep neural networks (DNNs) are an efficient alternative, however their performance is significantly affected by the underlying distribution of the synthetic training data. Self-supervised learning is an attractive alternative, where instead of using a separate training dataset, the network learns the features of the input data itself. This approach has only been applied to fitting of trivial dMRI models thus far. Here, we introduce a self-supervised DNN to estimate the parameters of the VERDICT (Vascular, Extracellular and Restricted DIffusion for Cytometry in Tumours) model for prostate. We demonstrate, for
    
[^137]: 语言代理的认知架构

    Cognitive Architectures for Language Agents. (arXiv:2309.02427v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.02427](http://arxiv.org/abs/2309.02427)

    本文提出了一种称为CoALA的认知架构，用于组织语言代理的现有研究并规划未来的发展方向。CoALA描述了一个具有模块化记忆组件、结构化行动空间和通用决策过程的语言代理。通过这一框架，有望发展出更强大的语言代理。

    

    最近的研究在大规模语言模型（LLMs）中增加了外部资源（例如互联网）或内部控制流（例如提示链），用于需要基于语境或推理的任务，从而产生了一类新的语言代理。尽管这些代理取得了实证成功，但我们缺乏一个系统的框架来组织现有代理并规划未来的发展。在本文中，我们借鉴了认知科学和符号人工智能的丰富历史，提出了语言代理的认知架构（CoALA）。CoALA描述了一个具有模块化记忆组件、用于与内部记忆和外部环境交互的结构化行动空间以及选择行动的通用决策过程的语言代理。我们使用CoALA对最近的大量研究进行了回顾和组织，并展望了更强大代理的可行方向。总的来说，CoALA将当今的语言代理置于上下文中。

    Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within th
    
[^138]: 基于多源数据的分布鲁棒机器学习

    Distributionally Robust Machine Learning with Multi-source Data. (arXiv:2309.02211v1 [stat.ML])

    [http://arxiv.org/abs/2309.02211](http://arxiv.org/abs/2309.02211)

    本文提出了一种基于多源数据的分布鲁棒机器学习方法，通过引入组分布鲁棒预测模型来提高具有分布偏移的目标人群的预测准确性。

    

    当目标分布与源数据集不同时，传统的机器学习方法可能导致较差的预测性能。本文利用多个数据源，并引入了一种基于组分布鲁棒预测模型来优化关于目标分布类的可解释方差的对抗性奖励。与传统的经验风险最小化相比，所提出的鲁棒预测模型改善了具有分布偏移的目标人群的预测准确性。我们证明了组分布鲁棒预测模型是源数据集条件结果模型的加权平均。我们利用这一关键鉴别结果来提高任意机器学习算法的鲁棒性，包括随机森林和神经网络等。我们设计了一种新的偏差校正估计器来估计通用机器学习算法的最优聚合权重，并展示了其在c方面的改进。

    Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the c
    
[^139]: 线性振动：视觉转换器中的困惑美学

    Linear Oscillation: The Aesthetics of Confusion for Vision Transformer. (arXiv:2308.13670v1 [cs.LG])

    [http://arxiv.org/abs/2308.13670](http://arxiv.org/abs/2308.13670)

    研究提出了一种新的激活函数——线性振动（LoC）激活函数，它通过将线性轨迹和振荡偏差无缝融合，捕捉到了“困惑的重要性”的本质。实证研究表明，在需要区分微妙模式的情境中，使用LoC激活函数可以提高网络的鲁棒性。

    

    激活函数是深度学习的关键，深刻影响神经网络的表示能力和训练动力学。它们不仅塑造了表示的性质，还优化了收敛速度并增强了泛化能力。鉴于这一关键作用，我们提出了线性振动（LoC）激活函数，定义为$f(x) = x \times \sin(\alpha x + \beta)$。与传统的激活函数不同，LoC将线性轨迹与振荡偏差无缝融合。命名为“线性振动”是对其独特属性的致敬，即通过和谐的振动融入线性激活，捕捉“困惑的重要性”的本质。网络激活中的“控制性困惑”概念被认为能够促进更稳健的学习，特别是在需要区分微妙模式的情境中。我们的实证研究表明，当使用LoC激活函数时，网络可以更好地捕捉到隐藏的模式，提高了学习的鲁棒性。

    Activation functions are the linchpins of deep learning, profoundly influencing both the representational capacity and training dynamics of neural networks. They shape not only the nature of representations but also optimize convergence rates and enhance generalization potential. Appreciating this critical role, we present the Linear Oscillation (LoC) activation function, defined as $f(x) = x \times \sin(\alpha x + \beta)$. Distinct from conventional activation functions which primarily introduce non-linearity, LoC seamlessly blends linear trajectories with oscillatory deviations. The nomenclature ``Linear Oscillation'' is a nod to its unique attribute of infusing linear activations with harmonious oscillations, capturing the essence of the 'Importance of Confusion'. This concept of ``controlled confusion'' within network activations is posited to foster more robust learning, particularly in contexts that necessitate discerning subtle patterns. Our empirical studies reveal that, when i
    
[^140]: 使用带轻量级注意机制的迁移ResNet增强乳腺癌分类

    Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism. (arXiv:2308.13150v1 [eess.IV])

    [http://arxiv.org/abs/2308.13150](http://arxiv.org/abs/2308.13150)

    本文介绍了一种使用ResNet模型和轻量级注意机制框架的图像分类方法，通过优化特征表示、增强分类能力和改善特征可辨别性，在乳腺癌分类任务上显示出卓越性能和潜在应用前景。

    

    深度学习模型通过学习原始像素数据中的复杂特征层次结构，彻底改变了图像分类。本文介绍了一种基于ResNet模型的图像分类方法，并引入了轻量级注意机制框架来提高性能。该框架优化特征表示，增强分类能力，改善特征可辨别性。我们在Breakhis数据集上验证了算法的有效性，在许多方面显示出卓越的性能。我们的方法不仅在传统模型方面表现优越，在当代视觉变换器等最新方法上也显示出优势。在诸如精度、准确度、召回率、F1分数和G-means等指标上取得了显著改进，同时在收敛时间方面表现良好。这些结果增强了算法的性能，巩固了其在实际图像分类任务中的应用前景。

    Deep learning models have revolutionized image classification by learning complex feature hierarchies in raw pixel data. This paper introduces an image classification method based on the ResNet model, and introduces a lightweight attention mechanism framework to improve performance. The framework optimizes feature representation, enhances classification capabilities, and improves feature discriminativeness. We verified the effectiveness of the algorithm on the Breakhis dataset, showing its superior performance in many aspects. Not only in terms of conventional models, our method also shows advantages on state-of-the-art methods such as contemporary visual transformers. Significant improvements have been achieved in metrics such as precision, accuracy, recall, F1-score, and G-means, while also performing well in terms of convergence time. These results strengthen the performance of the algorithm and solidify its application prospects in practical image classification tasks. Keywords: Re
    
[^141]: 收集，测量，重复：负责任的AI数据收集的可靠性因素

    Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection. (arXiv:2308.12885v1 [cs.LG])

    [http://arxiv.org/abs/2308.12885](http://arxiv.org/abs/2308.12885)

    对于负责任的AI数据收集，需要对数据的质量进行彻底的审查，避免不公平、偏见或不准确的结果。

    

    机器学习方法迅速进入我们的日常活动和高风险领域，要求对其公平性和可靠性进行透明和审查。为了评估机器学习模型的健壮性，研究通常会集中在其部署所使用的大规模数据集上，例如创建和维护文件以了解其来源、开发过程和伦理考虑。然而，AI的数据收集通常仍然是一次性的实践，而且经常为特定目的或应用程序收集的数据集会被重复用于其他问题。此外，数据集的注释可能随时间不具有代表性，包含模糊或错误的注释，或者无法跨问题或领域进行泛化。最近的研究表明，这些做法可能导致不公平、偏见或不准确的结果。我们认为，AI的数据收集应该以负责任的方式进行，对数据的质量进行彻底的审查。

    The rapid entry of machine learning approaches in our daily activities and high-stakes domains demands transparency and scrutiny of their fairness and reliability. To help gauge machine learning models' robustness, research typically focuses on the massive datasets used for their deployment, e.g., creating and maintaining documentation for understanding their origin, process of development, and ethical considerations. However, data collection for AI is still typically a one-off practice, and oftentimes datasets collected for a certain purpose or application are reused for a different problem. Additionally, dataset annotations may not be representative over time, contain ambiguous or erroneous annotations, or be unable to generalize across issues or domains. Recent research has shown these practices might lead to unfair, biased, or inaccurate outcomes. We argue that data collection for AI should be performed in a responsible manner where the quality of the data is thoroughly scrutinized
    
[^142]: FOSA: 全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法

    FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data. (arXiv:2308.12388v1 [cs.LG])

    [http://arxiv.org/abs/2308.12388](http://arxiv.org/abs/2308.12388)

    FOSA是一种全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法，通过融合FIML估计和自注意力机制，实现了在准确性、计算效率和适应不同数据结构方面的显著优势。

    

    数据补全中，有效地处理缺失值尤为重要，特别是在复杂的数据集中。本论文深入研究了FIML优化自注意力（FOSA）框架，这是一种融合了全信息最大似然（FIML）估计和自注意力神经网络能力的创新方法。我们的方法首先通过FIML对缺失值进行初始估计，然后通过利用自注意力机制来进一步提炼这些估计值。我们在模拟数据集和真实数据集上的全面实验证明了FOSA相对于传统的FIML技术在准确性、计算效率和适应不同数据结构方面的显著优势。有趣的是，即使在结构方程模型（SEM）可能错误规定导致子优的FIML估计的情况下，FOSA自注意力组件的稳健架构能够灵活地纠正和优化补全结果。

    In data imputation, effectively addressing missing values is pivotal, especially in intricate datasets. This paper delves into the FIML Optimized Self-attention (FOSA) framework, an innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of self-attention neural networks. Our methodology commences with an initial estimation of missing values via FIML, subsequently refining these estimates by leveraging the self-attention mechanism. Our comprehensive experiments on both simulated and real-world datasets underscore FOSA's pronounced advantages over traditional FIML techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Intriguingly, even in scenarios where the Structural Equation Model (SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust architecture of FOSA's self-attention component adeptly rectifies and optimizes the imputati
    
[^143]: 量子噪声驱动的生成扩散模型

    Quantum-Noise-driven Generative Diffusion Models. (arXiv:2308.12013v1 [quant-ph])

    [http://arxiv.org/abs/2308.12013](http://arxiv.org/abs/2308.12013)

    该论文提出了三种量子噪声驱动的生成扩散模型，利用了量子特性以克服传统模型的主要计算困难，并建议将量子噪声视为可利用的特性而非问题。

    

    通过机器学习技术实现的生成模型是从有限的训练样本中推断出复杂和未知数据分布并产生新的合成数据的强大工具。扩散模型是一种新兴的框架，最近在创建合成文本和高质量图像方面已经超越了生成对抗性网络的性能。在这里，我们提出并讨论了扩散模型的量子推generalization，即三种可能在实际量子系统上进行实验的量子噪声驱动的生成扩散模型。我们的想法是利用独特的量子特性，特别是目前可用的有噪声量子处理器不可避免地受到的相干性、纠缠性和噪声之间的非平凡相互作用，以克服传统扩散模型在推断过程中的主要计算负担。因此，我们建议将量子噪声不作为需要检测和解决的问题，而是作为一种可利用的特性，使得扩散模型能够更好地工作。

    Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a ver
    
[^144]: 无范例类增量学习的初始训练策略分析

    An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning. (arXiv:2308.11677v1 [cs.LG])

    [http://arxiv.org/abs/2308.11677](http://arxiv.org/abs/2308.11677)

    本文分析了无范例类增量学习过程中的初始训练策略。研究发现，初始学习策略的选择会显著影响增量学习模型的性能，但目前还没有进行深入研究。

    

    类增量学习旨在从数据流中构建分类模型。在类增量学习过程的每一步中，新的类别必须被整合到模型中。由于灾难性遗忘，当无法存储过去类别的样本时，类增量学习变得尤为具有挑战性，这正是我们在此研究的对象。迄今为止，大多数方法仅基于类增量学习过程的目标数据集。然而，最近在大量数据上通过自监督方式预训练模型的使用已经逐渐增多。类增量学习过程的初始模型可能仅使用目标数据集的第一批数据，或者还可以使用在辅助数据集上获得的预训练权重。这两种初始学习策略的选择可以极大地影响增量学习模型的性能，但目前还没有进行深入研究。性能还受到类增量学习算法的选择、神经网络结构、目标任务的性质、类别分布的影响。

    Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of clas
    
[^145]: 通过行动和语言提升智能体的沟通和学习能力

    Enhancing Agent Communication and Learning through Action and Language. (arXiv:2308.10842v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.10842](http://arxiv.org/abs/2308.10842)

    通过行动和语言相结合的方式，我们引入了一种新型智能体，其能够同时作为教师和学习者，通过行动演示和语言指令增强了沟通效率，并探索了结合行动和语言沟通模式对学习结果的积极影响。

    

    我们引入了一种新型的GC智能体类别，能够同时充当教师和学习者的角色。借助基于行动的演示和基于语言的指令，这些智能体增强了沟通效率。我们研究了在人类沟通和目标实现中的重要元素——教育学和实用主义的融入，提升了智能体的教学和学习能力。此外，我们探讨了结合行动和语言沟通模式对学习结果的影响，强调了多模式方法的优势。

    We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach.
    
[^146]: 特征强化物理信息神经网络（FE-PINN）：在目标任务之前学习底层物理特征的框架

    Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task. (arXiv:2308.08873v1 [cs.LG])

    [http://arxiv.org/abs/2308.08873](http://arxiv.org/abs/2308.08873)

    FE-PINN是一种学习底层物理特征的框架，在主训练之前以低计算成本解决问题的模式。与传统PINN相比，FE-PINN通过执行一系列子任务来解决损失函数不平衡的问题，并具有快速训练和更高的求解速度。

    

    本文介绍了一种名为特征强化物理信息神经网络（FE-PINN）的新型无数据框架。该框架能够在主训练循环之前以较低的计算成本学习任何问题的底层模式。由于存在偏微分残差和边界条件均方误差两个项，普通PINN的损失函数不平衡。FE-PINN通过只需一分钟的训练，而不是耗时数小时的超参数调优来解决这个挑战。FE-PINN通过执行一系列子任务来完成这个过程。第一个子任务学习有关底层物理的有用特征。然后，模型在目标任务上进行训练以完善计算。FE-PINN应用于三个基准问题：圆柱体上的流动、二维热传导以及计算入口速度的逆问题。FE-PINN可以分别加速15倍、2倍和5倍地解决每个案例。另外

    In this work, a new data-free framework called Feature Enforcing Physics Informed Neural Network (FE-PINN) is introduced. This framework is capable of learning the underlying pattern of any problem with low computational cost before the main training loop. The loss function of vanilla PINN due to the existence of two terms of partial differential residuals and boundary condition mean squared error is imbalanced. FE-PINN solves this challenge with just one minute of training instead of time-consuming hyperparameter tuning for loss function that can take hours. The FE-PINN accomplishes this process by performing a sequence of sub-tasks. The first sub-task learns useful features about the underlying physics. Then, the model trains on the target task to refine the calculations. FE-PINN is applied to three benchmarks, flow over a cylinder, 2D heat conduction, and an inverse problem of calculating inlet velocity. FE-PINN can solve each case with, 15x, 2x, and 5x speed up accordingly. Another
    
[^147]: 架起可信度与开放世界学习的桥梁：一种探索性神经方法，用于增强可解释性、泛化性和鲁棒性

    Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness. (arXiv:2308.03666v1 [stat.ML])

    [http://arxiv.org/abs/2308.03666](http://arxiv.org/abs/2308.03666)

    该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。

    

    随着研究人员努力缩小机器智能与人类之间的差距，通过发展人工智能技术，我们必须认识到可信度在开放世界中的关键重要性，在日常生活的各个方面对每个人都已经无处不在。然而，目前的人工智能系统存在几个挑战，可能会导致信任危机：1）对预测结果的解释不足；2）学习模型的泛化性不足；3）对不确定环境的适应能力差。因此，我们探索了一种神经程序，用于架起可信度与开放世界学习之间的桥梁，从单模态扩展到多模态场景，以供读者使用。1）为了增强设计级可解释性，我们首先定制了具有特定物理含义的可信网络；2）然后，通过灵活的学习正则化器设计环境福祉任务接口，以改善可信网络的泛化性能。

    As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy
    
[^148]: 语言模型的鲁棒无畸变水印方法

    Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])

    [http://arxiv.org/abs/2307.15593](http://arxiv.org/abs/2307.15593)

    该论文提出了一种在语言模型中添加鲁棒无畸变水印的方法，通过映射随机数序列到语言模型的样本，可以实现在不改变文本分布的前提下对水印文本进行检测，并且在多种改写攻击下依然保持较高的鲁棒性，实验证明在40-50%的随机扰动下仍可可靠地检测到水印文本。

    

    我们提出了一种在自回归语言模型中添加水印的方法，并且这些水印对扰动具有鲁棒性，而不会改变文本的分布，同时保证生成预算在一定范围内。我们用随机水印密钥计算的随机数序列映射到语言模型的样本来生成带水印的文本。要检测水印文本，只要知道密钥的任何一方都可以将文本与随机数序列对齐。我们使用两种采样方案来实例化水印方法：反变换采样和指数最小采样。我们将这些水印应用于三个语言模型——OPT-1.3B、LLaMA-7B和Alpaca-7B，以实验证明它们的统计功效和对各种改写攻击的鲁棒性。值得注意的是，对于OPT-1.3B和LLaMA-7B模型，即使在随机扰动了40-50%的词元后，我们仍然可以可靠地检测到带水印的文本（$p \leq 0.01$），只需要35个词元。

    We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random
    
[^149]: Zonoid的最优逼近和浅层神经网络的均匀逼近

    Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks. (arXiv:2307.15285v1 [stat.ML])

    [http://arxiv.org/abs/2307.15285](http://arxiv.org/abs/2307.15285)

    本论文解决了Zonoid的最优逼近和浅层神经网络的均匀逼近两个问题。对于Zonoid的逼近，我们填补了在$d=2,3$时的对数差距，实现了在所有维度上的解决方案。对于神经网络的逼近，我们的技术在$k \geq 1$时显著提高了目前的逼近率，并能够均匀逼近目标函数及其导数。

    

    我们研究了以下两个相关问题。第一个问题是确定一个任意的在$\mathbb{R}^{d+1}$空间中的Zonoid可以通过$n$个线段的Hausdorff距离来逼近的误差。第二个问题是确定浅层ReLU$^k$神经网络在其变分空间中的均匀范数的最优逼近率。第一个问题已经在$d \neq 2, 3$时得到解决，但当$d = 2, 3$时，最优上界和最优下界之间仍存在一个对数差距。我们填补了这个差距，完成了所有维度上的解决方案。对于第二个问题，我们的技术在$k \geq 1$时显著提高了现有的逼近率，并实现了目标函数及其导数的均匀逼近。

    We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
    
[^150]: FLuID: 使用不变性丢失减轻联邦学习中的阻塞问题

    FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout. (arXiv:2307.02623v1 [cs.LG])

    [http://arxiv.org/abs/2307.02623](http://arxiv.org/abs/2307.02623)

    FLuID提出了一种使用不变性丢失的方法来减轻联邦学习中性能较低设备导致的训练时间问题，并开发了一个自适应训练框架。通过动态平衡训练负载，FLuID能有效地减轻阻塞设备的工作负载，同时不影响模型质量。

    

    联邦学习（FL）允许机器学习模型在个体移动设备上进行本地训练，并通过共享服务器同步模型更新。这种方法保护用户隐私，但也由于不同设备的性能差异而产生了一个异构的训练环境。因此，在FL中，性能较低的阻塞设备经常决定整体训练时间。在这项工作中，我们旨在通过系统动态平衡训练负载来减轻由于阻塞器产生的性能瓶颈。我们引入了不变性丢失，一种基于权重更新阈值提取子模型的方法，从而最小化对准确性的潜在影响。在此丢失技术的基础上，我们开发了一种自适应训练框架FLuID。FLuID提供了一种轻量级的子模型提取方法来调节计算强度，从而减少阻塞设备的负载而不影响模型质量。

    Federated Learning (FL) allows machine learning models to train locally on individual mobile devices, synchronizing model updates via a shared server. This approach safeguards user privacy; however, it also generates a heterogeneous training environment due to the varying performance capabilities across devices. As a result, straggler devices with lower performance often dictate the overall training time in FL. In this work, we aim to alleviate this performance bottleneck due to stragglers by dynamically balancing the training load across the system. We introduce Invariant Dropout, a method that extracts a sub-model based on the weight update threshold, thereby minimizing potential impacts on accuracy. Building on this dropout technique, we develop an adaptive training framework, Federated Learning using Invariant Dropout (FLuID). FLuID offers a lightweight sub-model extraction to regulate computational intensity, thereby reducing the load on straggler devices without affecting model q
    
[^151]: MoVie: 基于视觉模型的策略自适应用于视图泛化

    MoVie: Visual Model-Based Policy Adaptation for View Generalization. (arXiv:2307.00972v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00972](http://arxiv.org/abs/2307.00972)

    本论文提出了一种名为MoVie的方法，通过基于视觉模型的策略自适应实现了视图泛化。该方法在不需要显式奖励信号和训练过程修改的情况下，在多个实际场景中表现出卓越的性能，相对改进达到了33%至152%。

    

    在有限的视图上训练的视觉强化学习（RL）智能体在将其学到的能力推广到未见过的视图时面临着巨大的挑战。这个固有的困难被称为$\textit{view generalization}$问题。在这项工作中，我们将这个基本问题系统地分为四个不同的，高度具有挑战性的情景，这些情景与实际情况很相似。随后，我们提出了一个简单而有效的方法，在测试时使基于视觉的$\textbf{Mo}$del-based策略能够成功适应$\textbf{Vie}$w generalization ($\textbf{MoVie}$)，而无需显式的奖励信号和训练过程中的任何修改。我们的方法在来自DMControl、xArm和Adroit的总共$\textbf{18}$个任务中的所有四个情景中展示了显著进展，相对改进分别为$\mathbf{33}$%，$\mathbf{86}$%和$\mathbf{152}$%。优越的结果凸显出其巨大的潜力。

    Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\textbf{Mo}$del-based policies for $\textbf{Vie}$w generalization ($\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\mathbf{33}$%, $\mathbf{86}$%, and $\mathbf{152}$% respectively. The superior results highlight the immens
    
[^152]: SCENEREPLICA：通过创建可重复的场景来评估现实世界中的机器人操纵能力的基准测试

    SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating Reproducible Scenes. (arXiv:2306.15620v1 [cs.RO])

    [http://arxiv.org/abs/2306.15620](http://arxiv.org/abs/2306.15620)

    SCENEREPLICA是一个基于YCB对象的可重复性基准测试，用于评估现实世界中的机器人操纵能力。此基准测试易于重复并允许研究人员比较不同的技术和算法，有助于加快机器人操纵方法的发展。

    

    我们提出了一个新的可重复性基准测试，用于评估现实世界中的机器人操纵能力，特别关注抓取和放置任务。我们的基准测试使用了YCB对象，这是机器人学界常用的数据集，确保我们的结果可以与其他研究进行比较。此外，此基准测试还被设计为在现实世界中易于重复，使其可供研究人员和实践者使用。我们还提供了对基准测试中基于模型和无模型的6D机器人抓取的实验结果和分析，其中评估了代表性算法在物体感知、抓取规划和运动规划方面的性能。我们相信我们的基准测试将成为推动机器人操纵领域发展的宝贵工具。通过提供一个标准化的评估框架，研究人员可以更容易地比较不同的技术和算法，从而加快发展机器人操纵方法的进展。

    We present a new reproducible benchmark for evaluating robot manipulation in the real world, specifically focusing on pick-and-place. Our benchmark uses the YCB objects, a commonly used dataset in the robotics community, to ensure that our results are comparable to other studies. Additionally, the benchmark is designed to be easily reproducible in the real world, making it accessible to researchers and practitioners. We also provide our experimental results and analyzes for model-based and model-free 6D robotic grasping on the benchmark, where representative algorithms are evaluated for object perception, grasping planning, and motion planning. We believe that our benchmark will be a valuable tool for advancing the field of robot manipulation. By providing a standardized evaluation framework, researchers can more easily compare different techniques and algorithms, leading to faster progress in developing robot manipulation methods.
    
[^153]: 超越规模：多样性系数作为数据质量指标证明了LLMs是在形式多样的数据上预先训练的

    Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])

    [http://arxiv.org/abs/2306.13840](http://arxiv.org/abs/2306.13840)

    本论文提出使用多样性系数作为LLM预训练数据质量的指标，研究表明公开可用的LLM数据集的多样性系数很高。

    

    当前，预先训练强大的大语言模型(LLMs)的趋势主要集中在模型和数据集规模的扩大。然而，预先训练数据的质量对于训练强大的LLMs来说是一个重要因素，但它是一个模糊的概念，尚未完全表征。因此，我们使用最近提出的Task2Vec多样性系数来基于数据质量的形式方面，超越规模本身。具体而言，我们测量公开可用的预先训练数据集的多样性系数，以证明它们的形式多样性高于理论的下限和上限。此外，为了建立对多样性系数的信心，我们进行可解释性实验，并发现该系数与多样性的直观属性相吻合，例如，随着潜在概念数量的增加，它增加。我们得出结论，多样性系数是可靠的，表明公开可用的LLM数据集的多样性系数很高，并推测它可以作为预训练LLMs模型的数据质量指标。

    Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
    
[^154]: 纠正公平分类中的低估偏差和交叉偏差

    Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v1 [cs.LG])

    [http://arxiv.org/abs/2306.11112](http://arxiv.org/abs/2306.11112)

    本文提出一种可以有效纠正数据偏差和交叉偏差的学习方法，并构造了一个重新加权方案，可以精确评估任何假设在真实分布上的损失。

    

    我们考虑学习被低估偏差损坏的数据的问题，其中正例在固定数量的敏感组中以不同的未知速率从数据中过滤掉。我们表明，在有少量无偏数据的情况下，我们可以有效地估计每个组的减少参数，即使在交叉组成员资格使得学习每个交叉率变得计算上不可行的情况下。利用这个分组丢失率的估计，我们构造了一个重新加权方案，可以使我们近似评估任何假设在真实分布上的损失，即使我们只能在一个有偏样本上观察到经验误差。最后，我们提出了一个封装了这个学习和重新加权过程的算法，并提供了强PAC风格的保证，即有很高的概率我们对假设在真实分布上的风险的估计将与真实风险任意接近。

    We consider the problem of learning from data corrupted by underrepresentation bias, where positive examples are filtered from the data at different, unknown rates for a fixed number of sensitive groups. We show that with a small amount of unbiased data, we can efficiently estimate the group-wise drop-out parameters, even in settings where intersectional group membership makes learning each intersectional rate computationally infeasible. Using this estimate for the group-wise drop-out rate, we construct a re-weighting scheme that allows us to approximate the loss of any hypothesis on the true distribution, even if we only observe the empirical error on a biased sample. Finally, we present an algorithm encapsulating this learning and re-weighting process, and we provide strong PAC-style guarantees that, with high probability, our estimate of the risk of the hypothesis over the true distribution will be arbitrarily close to the true risk.
    
[^155]: CHORUS: 统一数据发现和探索的基础模型。

    CHORUS: Foundation Models for Unified Data Discovery and Exploration. (arXiv:2306.09610v1 [cs.DB])

    [http://arxiv.org/abs/2306.09610](http://arxiv.org/abs/2306.09610)

    研究者探索将大型语言模型应用于数据发现和探索任务中，证明这些模型在表格类检测、列类型注释和联接列预测中具有优越性能，并有望将不同的数据管理任务统一在基础模型下。

    

    我们探索了将基础模型应用于数据发现和探索任务中。基础模型是一种大型语言模型 (LLMs)，在各种与其训练无关的不同任务上表现出了良好的性能。我们表明这些模型在数据发现和数据探索领域非常适用。在谨慎使用的情况下，它们具有优越的能力，可以优化表格类检测、列类型注释和联接列预测这三种代表性任务。在这三个任务上，我们展示了基于基础模型的方法优于任务特定的模型和最先进的技术。此外，我们的方法通常超过人类专家的任务表现。这表明了将不同的数据管理任务统一在基础模型下的未来方向。

    We explore the application of foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMs) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. This suggests a future direction in which disparate data management tasks can be unified under foundation models.
    
[^156]: 异常点存在时健壮经验风险最小化性能的渐进特性研究

    Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance in the Presence of Outliers. (arXiv:2305.18974v1 [stat.ML])

    [http://arxiv.org/abs/2305.18974](http://arxiv.org/abs/2305.18974)

    该论文研究了包括异常点的高维健壮线性回归问题，提供了使用不同损失函数的精确渐近特性，对泛化误差进行了简单校准并计算了收敛速率，但由于范数校准不匹配，对估计误差的一致性需要一个较强的收敛假设。

    

    我们研究了高维健壮线性回归问题，当维度$d$和数据点数量$n$以固定比率$\alpha=n/d$发散，并研究了包括异常点在内的数据模型。我们对使用$\ell_2$ -正则化$\ell_2$，$\ell_1$，和 Huber 损失的经验风险最小化（ERM）性能提供了精确的渐近特性，这是解决这类问题的标准方法。我们关注性能的两个指标：具有异常点的相似数据集的泛化误差和原始无污染函数的估计误差。我们将结果与信息论贝叶斯最优估计界进行了比较。对于泛化误差，我们发现如果进行简单的校准并计算收敛速率，则最优正则化ERM在大样本复杂度限制下是渐近一致的。然而，对于估计误差，由于范数校准不匹配，我们表明估计器的一致性需要一个较强的收敛假设，这对问题的解决还需要进一步的研究。

    We study robust linear regression in high-dimension, when both the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha=n/d$, and study a data model that includes outliers. We provide exact asymptotics for the performances of the empirical risk minimisation (ERM) using $\ell_2$-regularised $\ell_2$, $\ell_1$, and Huber loss, which are the standard approach to such problems. We focus on two metrics for the performance: the generalisation error to similar datasets with outliers, and the estimation error of the original, unpolluted function. Our results are compared with the information theoretic Bayes-optimal estimation bound. For the generalization error, we find that optimally-regularised ERM is asymptotically consistent in the large sample complexity limit if one perform a simple calibration, and compute the rates of convergence. For the estimation error however, we show that due to a norm calibration mismatch, the consistency of the estimator requires an
    
[^157]: 使用协方差神经网络进行可解释的脑龄预测

    Explainable Brain Age Prediction using coVariance Neural Networks. (arXiv:2305.18370v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.18370](http://arxiv.org/abs/2305.18370)

    本文提出了使用协方差神经网络进行可解释的脑龄预测的框架，可以通过皮质厚度特征捕捉加速老化，并反映出增加的神经疾病或认知障碍的风险。

    

    在计算神经科学中，越来越多的机器学习算法被用于利用脑成像数据为个体提供“脑龄”估计。由于脑龄与实际年龄存在差异（称为“脑龄差”），因此可以捕捉由于不良健康状况导致的加速老化，并因此反映出增加的神经疾病或认知障碍的风险。然而，大多数现有的脑龄预测算法缺乏透明度和方法论依据，限制了其在临床决策支持方面的广泛应用。在本文中，我们利用协方差神经网络 (VNN)来提出一种解剖可解释的框架，利用皮质厚度特征进行脑龄预测。具体而言，我们的脑龄预测框架不仅扩展到阿尔茨海默病 (AD) 中脑龄差的粗略指标，而且我们还提出了两个重要观察。

    In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of "brain age" for an individual. Importantly, the discordance between brain age and chronological age (referred to as "brain age gap") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer's disease (AD) and we make two important obser
    
[^158]: 优化的自定义数据集用于高效检测水下垃圾

    Optimized Custom Dataset for Efficient Detection of Underwater Trash. (arXiv:2305.16460v1 [cs.CV])

    [http://arxiv.org/abs/2305.16460](http://arxiv.org/abs/2305.16460)

    本文提出了一种自定义数据集和有效检测方法，旨在通过增加垃圾实例的多样性，在深入水下环境中提高其检测精度。

    

    准确评估和清除潜在的水下废物对于保护海洋生物和环境至关重要。本文针对水下垃圾检测所存在的挑战，如光折射、吸收、悬浮颗粒和色彩扭曲等因素，提出了一种自定义数据集和有效检测方法。该数据集涵盖了多种水下环境，并包括对废弃物实例的精确定位标注。最终，使用最先进的深度学习结构，目的是通过增加垃圾实例的多样性，在深入水下环境中提高其检测精度。

    Accurately quantifying and removing submerged underwater waste plays a crucial role in safeguarding marine life and preserving the environment. While detecting floating and surface debris is relatively straightforward, quantifying submerged waste presents significant challenges due to factors like light refraction, absorption, suspended particles, and color distortion. This paper addresses these challenges by proposing the development of a custom dataset and an efficient detection approach for submerged marine debris. The dataset encompasses diverse underwater environments and incorporates annotations for precise labeling of debris instances. Ultimately, the primary objective of this custom dataset is to enhance the diversity of litter instances and improve their detection accuracy in deep submerged environments by leveraging state-of-the-art deep learning architectures.
    
[^159]: 论进化磨锋、平坦极小和泛化

    On progressive sharpening, flat minima and generalisation. (arXiv:2305.14683v1 [cs.LG])

    [http://arxiv.org/abs/2305.14683](http://arxiv.org/abs/2305.14683)

    本文提出了一种用损失黑塞矩阵和输入-输出雅克比矩阵联系起来的假设，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界，给出了关于进化磨锋和平坦极小的泛化性质的新解释。

    

    我们提出了一种新的方法来理解深度学习中损失曲率与泛化之间的关系。具体来说，我们利用现有的深度网络损失黑塞矩阵频谱经验分析，提出了一个将损失黑塞矩阵和深度神经网络的输入-输出雅克比矩阵联系起来的假设。然后，我们证明了一系列理论结果，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界。我们利用我们的假设和理论结果，给出了关于最近观察到的进化磨锋现象以及平坦极小的泛化性质的新描述。实验证据验证了我们的主张。

    We present a new approach to understanding the relationship between loss curvature and generalisation in deep learning. Specifically, we use existing empirical analyses of the spectrum of deep network loss Hessians to ground an ansatz tying together the loss Hessian and the input-output Jacobian of a deep neural network. We then prove a series of theoretical results which quantify the degree to which the input-output Jacobian of a model approximates its Lipschitz norm over a data distribution, and deduce a novel generalisation bound in terms of the empirical Jacobian. We use our ansatz, together with our theoretical results, to give a new account of the recently observed progressive sharpening phenomenon, as well as the generalisation properties of flat minima. Experimental evidence is provided to validate our claims.
    
[^160]: 扩散模型的结构剪枝

    Structural Pruning for Diffusion Models. (arXiv:2305.10924v1 [cs.LG])

    [http://arxiv.org/abs/2305.10924](http://arxiv.org/abs/2305.10924)

    本文提出了一种名为Diff-Pruning的高效压缩方法，通过一个Taylor展开过程来识别重要权重，从而从预先存在的模型中学习轻量级扩散模型，性能稳定，并在训练效率上显著提高。

    

    生成建模最近取得了显著的进展，主要是因为扩散概率模型（DPM）的转型意义。然而，这些模型的令人印象深刻的能力通常涉及到显著的计算开销，在训练和推理期间都是如此。为了应对这一挑战，我们提出了Diff-Pruning，一种专为从预先存在的模型中学习轻量级扩散模型而设计的高效压缩方法，无需进行大量的重新训练。Diff-Pruning的本质是通过剪枝时间步长的Taylor展开，在过滤掉无贡献扩散步骤和整合有信息的梯度来识别重要权重的过程。我们在四个不同数据集上进行的实证评估突出了我们所提出方法的两个主要优点：1）效率：它可以以原始训练投入的仅10％到20％的代价实现约50％的FLOPs减少; 2）一致性: 剪枝后的扩散模型产生的效果与原始模型相当，不会影响生成建模的质量。

    Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across four diverse datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusio
    
[^161]: 通过随机行走随机交替方向乘法算法推动个性化联邦学习

    Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM. (arXiv:2304.12534v1 [cs.LG])

    [http://arxiv.org/abs/2304.12534](http://arxiv.org/abs/2304.12534)

    本研究提出了一种新算法RWSADMM，以解决在动态联邦学习中存在的数据不一致和通信成本高的问题，提高了可扩展性。

    

    在本研究中，我们探讨了在现实世界中实现联邦学习（FL）时存在的障碍，其中不能维护中央服务器与所有客户端之间的一致连接，并且数据分布是异构的。为了解决这些挑战，我们专注于动态联邦学习，其中服务器在相邻客户端组之间移动以学习本地模型。具体来说，我们提出了一种新算法，即随机行走随机交替方向乘法算法（RWSADMM），只要有足够数量的连接客户端用于模型训练，就能适应动态和即席网络条件。在RWSADMM中，服务器随机向一组客户端行走。它基于硬不等式约束形成相邻客户端之间的局部近似，而不是采用一致更新来解决数据异构性。我们提出的方法是收敛的，可以降低通信成本，通过减少训练时间提高可扩展性。

    In this research, we investigate the barriers associated with implementing Federated Learning (FL) in real-world scenarios, where a consistent connection between the central server and all clients cannot be maintained, and data distribution is heterogeneous. To address these challenges, we focus on mobilizing the federated setting, where the server moves between groups of adjacent clients to learn local models. Specifically, we propose a new algorithm, Random Walk Stochastic Alternating Direction Method of Multipliers (RWSADMM), capable of adapting to dynamic and ad-hoc network conditions as long as a sufficient number of connected clients are available for model training. In RWSADMM, the server walks randomly toward a group of clients. It formulates local proximity among adjacent clients based on hard inequality constraints instead of consensus updates to address data heterogeneity. Our proposed method is convergent, reduces communication costs, and enhances scalability by reducing th
    
[^162]: 用Cayley变换拟合椭球

    Ellipsoid fitting with the Cayley transform. (arXiv:2304.10630v1 [stat.ML])

    [http://arxiv.org/abs/2304.10630](http://arxiv.org/abs/2304.10630)

    介绍了一种使用Cayley变换在任意维度上将椭球拟合到嘈杂数据中的新算法CTEF，可以拟合任意的椭球，并且能提取其他方法无法识别的数据中的非线性特征，可用于降维、数据可视化和聚类，相比其他方法更优。

    

    我们引入了一种算法，Cayley变换椭球拟合(CTEF)，它使用Cayley变换在任意维度上将椭球拟合到嘈杂的数据中。与许多椭球拟合方法不同，CTEF是椭球特定的——意味着它总是返回椭圆解——并且可以拟合任意的椭球。当数据不均匀地分布在椭球表面上时，它也优于其他拟合方法。受机器学习中可解释和可重复方法的呼吁启发，我们将CTEF应用于降维、数据可视化和聚类。由于CTEF捕捉全局曲率，因此它能够提取其他方法无法识别的数据中的非线性特征。这在人类细胞周期数据的降维和在经典玩具例子的聚类的背景下得到了说明。在后一种情况下，CTEF优于10种流行的聚类算法。

    We introduce an algorithm, Cayley transform ellipsoid fitting (CTEF), that uses the Cayley transform to fit ellipsoids to noisy data in any dimension. Unlike many ellipsoid fitting methods, CTEF is ellipsoid specific -- meaning it always returns elliptic solutions -- and can fit arbitrary ellipsoids. It also outperforms other fitting methods when data are not uniformly distributed over the surface of an ellipsoid. Inspired by calls for interpretable and reproducible methods in machine learning, we apply CTEF to dimension reduction, data visualization, and clustering. Since CTEF captures global curvature, it is able to extract nonlinear features in data that other methods fail to identify. This is illustrated in the context of dimension reduction on human cell cycle data, and in the context of clustering on classical toy examples. In the latter case, CTEF outperforms 10 popular clustering algorithms.
    
[^163]: 使用被动 Langevin 动力学的自适应逆强化学习的有限样本界限

    Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics. (arXiv:2304.09123v1 [cs.LG])

    [http://arxiv.org/abs/2304.09123](http://arxiv.org/abs/2304.09123)

    本文提供了有限时间界限，用于被动随机梯度 Langevin 动力学算法，该算法可用于逆强化学习。该算法充当随机采样器，恢复用外部过程优化而来的成本函数。

    

    随机梯度 Langevin 动力学 (SGLD) 是从概率分布采样的有用方法。本文提供了一个被动随机梯度 Langevin 动力学算法 (PSGLD) 的有限样本分析，旨在实现逆强化学习。此处的“被动”是指 PSGLD 算法(逆学习过程)可用的噪声渐变是由外部随机梯度算法(正向学习器)在随机选择的点上评估的。PSGLD 算法因此充当一个随机采样器，可恢复正在被此外部过程优化的成本函数。以前的工作使用随机逼近技术分析了这个被动算法的渐近性能；在本文中，我们分析了它的有限时间性能。具体而言，我们提供了在被动算法和其稳定测度之间的 2-Wasserstein 距离上的有限时间界限，从中可以获得重建的成本函数。

    Stochastic gradient Langevin dynamics (SGLD) are a useful methodology for sampling from probability distributions. This paper provides a finite sample analysis of a passive stochastic gradient Langevin dynamics algorithm (PSGLD) designed to achieve inverse reinforcement learning. By "passive", we mean that the noisy gradients available to the PSGLD algorithm (inverse learning process) are evaluated at randomly chosen points by an external stochastic gradient algorithm (forward learner). The PSGLD algorithm thus acts as a randomized sampler which recovers the cost function being optimized by this external process. Previous work has analyzed the asymptotic performance of this passive algorithm using stochastic approximation techniques; in this work we analyze the non-asymptotic performance. Specifically, we provide finite-time bounds on the 2-Wasserstein distance between the passive algorithm and its stationary measure, from which the reconstructed cost function is obtained.
    
[^164]: 自适应门控图卷积网络用于基于EEG数据的阿尔茨海默病可解释诊断

    Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data. (arXiv:2304.05874v1 [q-bio.NC])

    [http://arxiv.org/abs/2304.05874](http://arxiv.org/abs/2304.05874)

    本文提出了一种自适应门控图卷积网络(AGGCN)，该网络结合卷积节点特征增强和功能连接度量自适应学习图结构，实现了高精度的阿尔茨海默病诊断，并提供了重要的脑区信息。

    

    近来，图神经网络(GNN)模型越来越多地被用于分类脑电图(EEG)数据，然而，基于GNN的神经系统疾病，如阿尔茨海默病(AD)的诊断仍然是相对未开发的领域。因此，本文提出了一种新颖的自适应门控图卷积网络(AGGCN)，该网络可以提供可解释的预测结果。AGGCN通过将基于卷积的节点特征增强与基于功能连接性的著名相关度量相结合来自适应学习图结构。此外，门控图卷积可以动态地加权考虑各种空间尺度的贡献。实验结果表明，该模型在闭眼和睁眼状态下均能取得较高的精度，表明学习到的表征结果的稳定性。最后，我们证明了所提出的AGGCN模型可以提供有关AD最受影响的脑区的重要见解。

    Graph neural network (GNN) models are increasingly being used for the classification of electroencephalography (EEG) data. However, GNN-based diagnosis of neurological disorders, such as Alzheimer's disease (AD), remains a relatively unexplored area of research. Previous studies have relied on functional connectivity methods to infer brain graph structures and used simple GNN architectures for the diagnosis of AD. In this work, we propose a novel adaptive gated graph convolutional network (AGGCN) that can provide explainable predictions. AGGCN adaptively learns graph structures by combining convolution-based node feature enhancement with a well-known correlation-based measure of functional connectivity. Furthermore, the gated graph convolution can dynamically weigh the contribution of various spatial scales. The proposed model achieves high accuracy in both eyes-closed and eyes-open conditions, indicating the stability of learned representations. Finally, we demonstrate that the propos
    
[^165]: TraffNet：学习道路网络数字孪生交通生成因果关系

    TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])

    [http://arxiv.org/abs/2303.15954](http://arxiv.org/abs/2303.15954)

    TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。

    

    道路网络数字孪生（RNDT）在开发下一代智能交通系统中发挥着关键作用，可以实现更精确的交通规划和控制。为了支持实时决策，RNDT需要一个模型，从在线传感器数据中动态学习交通模式并生成高保真模拟结果。尽管基于图神经网络的当前交通预测技术已经实现了最先进的性能，但是这些技术仅通过挖掘历史交通数据中的相关性来预测未来交通，而忽略了交通生成的原因，例如交通需求和路径选择。因此，它们的性能对于实时决策是不可靠的。为了填补这一差距，我们引入了一个新的深度学习框架称为 TraffNet，该框架从车辆轨迹数据中学习交通量的因果性。首先，我们使用异构图来表示道路网络，使模型能够并入预测所需的其他数据，然后我们提出了一种新颖的递归神经网络结构，从而能够预测交通量的因果联系。

    Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
    
[^166]: (深度)强化学习中的连接超水平集及其在极小极大定理中的应用

    Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems. (arXiv:2303.12981v1 [cs.LG])

    [http://arxiv.org/abs/2303.12981](http://arxiv.org/abs/2303.12981)

    本文研究了强化学习中的策略优化问题，并证明了优化函数超水平集在网络类策略和表格式下始终是连通的，并应用此结果导出了鲁棒性强化学习的极小极大定理。

    

    本文的目的是改善强化学习中策略优化问题的优化函数图景理解。具体而言，我们证明了策略参数的优化目标函数的超水平集，在网络类策略和表格式下始终是连通的。同时，我们证明了奖励作为超水平集的函数满足更强的“等连通”性质。此外，我们还应用这些超水平集的连通性结果，导出了鲁棒性强化学习的极小极大定理。我们发现，任何一个在一侧为凸的，另一侧为等连通的极小极大优化问题都有纳什均衡。这些结论是新颖而且之前未知的。

    The aim of this paper is to improve the understanding of the optimization landscape for policy optimization problems in reinforcement learning. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. In addition, we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger "equiconnectedness" property. To our best knowledge, these are novel and previously unknown discoveries.  We present an application of the connectedness of these superlevel sets to the derivation of minimax theorems for robust reinforcement learning. We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality (i.e. has a Nash equilibrium). We find that this exact structure is exhibited by an interesting robust reinforceme
    
[^167]: 面向领域特定语音识别的深度学习系统

    A Deep Learning System for Domain-specific speech Recognition. (arXiv:2303.10510v1 [cs.CL])

    [http://arxiv.org/abs/2303.10510](http://arxiv.org/abs/2303.10510)

    本文提出了一个使用半监督学习注释领域特定数据，基于预训练的声学模型进行微调的ASR系统，并在领域特定上取得了优于商业ASR系统的性能。

    

    随着人机语音接口越来越便捷，许多最先进的自动语音识别（ASR）系统被提出。然而，商业ASR系统通常在领域特定语音，特别是在低资源情况下的表现较差。作者使用预训练的DeepSpeech2和Wav2Vec2声学模型，开发了受益特定的ASR系统。使用半监督学习注释领域特定数据，只需少量人工干预即可。最佳性能来自一种经过微调的Wav2Vec2-Large-LV60声学模型，带有外部KenLM，在受益特定语音上超越了Google和AWS ASR系统。还研究了将容易出错的ASR转录作为口语理解（SLU）的一部分的可行性。受益特定自然语言理解（NLU）任务的结果表明，领域特定微调的ASR系统可以超越商业ASR系统并提高NLU任务的准确性。

    As human-machine voice interfaces provide easy access to increasingly intelligent machines, many state-of-the-art automatic speech recognition (ASR) systems are proposed. However, commercial ASR systems usually have poor performance on domain-specific speech especially under low-resource settings. The author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to develop benefit-specific ASR systems. The domain-specific data are collected using proposed semi-supervised learning annotation with little human intervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60 acoustic model with an external KenLM, which surpasses the Google and AWS ASR systems on benefit-specific speech. The viability of using error prone ASR transcriptions as part of spoken language understanding (SLU) is also investigated. Results of a benefit-specific natural language understanding (NLU) task show that the domain-specific fine-tuned ASR system can outperform the commercial ASR sys
    
[^168]: 带有局部遗忘的重放缓冲区用于自适应深度模型强化学习

    Replay Buffer With Local Forgetting for Adaptive Deep Model-Based Reinforcement Learning. (arXiv:2303.08690v1 [cs.LG])

    [http://arxiv.org/abs/2303.08690](http://arxiv.org/abs/2303.08690)

    提出了一种带有局部遗忘的新型重放缓冲区，可以在状态空间的相关部分快速遗忘过时的经验。实验证明该方法提高了自适应深度模型强化学习代理对环境变化的适应能力，加速了学习速度并改善了策略。

    

    神经科学中用于确定所研究的对象（无论是啮齿动物还是人类）是否表现出模型为基础的学习的关键行为特征之一是对环境中局部变化的有效适应。然而，在强化学习中，最近的研究表明，现代深度模型强化学习（MBRL）方法较难适应这种变化。本文提出了一种新的重放缓冲区，带有局部遗忘，可以快速地在状态空间中的相关部分遗忘过时的经验而在其他地方保留旧数据。通过在一系列具有挑战性的导航任务上进行实验，我们证明了该方法改善了对环境变化的适应能力，加快了学习速度并改善了策略。

    One of the key behavioral characteristics used in neuroscience to determine whether the subject of study -- be it a rodent or a human -- exhibits model-based learning is effective adaptation to local changes in the environment. In reinforcement learning, however, recent work has shown that modern deep model-based reinforcement-learning (MBRL) methods adapt poorly to such changes. An explanation for this mismatch is that MBRL methods are typically designed with sample-efficiency on a single task in mind and the requirements for effective adaptation are substantially higher, both in terms of the learned world model and the planning routine. One particularly challenging requirement is that the learned world model has to be sufficiently accurate throughout relevant parts of the state-space. This is challenging for deep-learning-based world models due to catastrophic forgetting. And while a replay buffer can mitigate the effects of catastrophic forgetting, the traditional first-in-first-out
    
[^169]: 不确定性感知的离线学习

    Uncertainty-Aware Off-Policy Learning. (arXiv:2303.06389v1 [cs.LG])

    [http://arxiv.org/abs/2303.06389](http://arxiv.org/abs/2303.06389)

    本文提出了一种不确定性感知的倒数概率分数估计器（UIPS），用于改进离线学习，通过明确模拟估计的记录策略中的不确定性，相对于广泛的最先进基线具有优越的样本效率。

    This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, which explicitly models the uncertainty in the estimated logging policy and demonstrates advantageous sample efficiency against an extensive list of state-of-the-art baselines on synthetic and three real-world recommendation datasets.

    离线学习是指仅通过记录的反馈数据进行策略优化的过程，在各种实际应用中显示出重要性，例如搜索引擎、推荐系统等。虽然生成记录数据的真实记录策略通常是未知的，但以前的工作仅在离线学习中采用其估计值，忽略了由于这种估计器导致的高偏差和高方差，特别是在具有小且估计不准确的记录概率的样本上。在这项工作中，我们明确地模拟了估计的记录策略中的不确定性，并提出了一种不确定性感知的倒数概率分数估计器（UIPS）来改进离线学习。在合成和三个真实的推荐数据集上的实验结果表明，所提出的UIPS估计器相对于广泛的最先进基线具有优越的样本效率。

    Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines, recommender systems, and etc. While the ground-truth logging policy, which generates the logged data, is usually unknown, previous work simply takes its estimated value in off-policy learning, ignoring both high bias and high variance resulted from such an estimator, especially on samples with small and inaccurately estimated logging probabilities. In this work, we explicitly model the uncertainty in the estimated logging policy and propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning. Experiment results on synthetic and three real-world recommendation datasets demonstrate the advantageous sample efficiency of the proposed UIPS estimator against an extensive list of state-of-the-art baselines.
    
[^170]: 驯服对比度最大化以学习顺序、低延迟、基于事件的光流

    Taming Contrast Maximization for Learning Sequential, Low-latency, Event-based Optical Flow. (arXiv:2303.05214v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05214](http://arxiv.org/abs/2303.05214)

    本文提出了一种用于顺序估计基于事件的光流的自监督学习流程，该流程通过新型的对比度最大化公式训练持续运行的神经模型，以适应非线性和变化统计的输入事件。实验结果表明，该方法在多个数据集上取得了新的技术水平。

    

    事件相机最近受到了广泛关注，因为它们为复杂的计算机视觉问题提供了低延迟和低功率的解决方案。为了实现这些解决方案，需要开发能够利用事件数据的独特性质的算法。然而，目前的最新技术仍然受到基于帧的文献的影响，并且往往无法兑现这些承诺。在这项工作中，我们考虑到了这一点，并提出了一种新颖的自监督学习流程，用于顺序估计基于事件的光流，使得模型能够扩展到高推理频率。核心是一个持续运行的带状态神经模型，使用对比度最大化的新型公式进行训练，使其对输入事件中的非线性和变化统计保持稳健。多个数据集上的结果证实了我们方法的有效性，从而在精度方面创造了新的技术水平。

    Event cameras have recently gained significant traction since they open up new avenues for low-latency and low-power solutions to complex computer vision problems. To unlock these solutions, it is necessary to develop algorithms that can leverage the unique nature of event data. However, the current state-of-the-art is still highly influenced by the frame-based literature, and usually fails to deliver on these promises. In this work, we take this into consideration and propose a novel self-supervised learning pipeline for the sequential estimation of event-based optical flow that allows for the scaling of the models to high inference frequencies. At its core, we have a continuously-running stateful neural model that is trained using a novel formulation of contrast maximization that makes it robust to nonlinearities and varying statistics in the input events. Results across multiple datasets confirm the effectiveness of our method, which establishes a new state of the art in terms of ac
    
[^171]: 迈向无界机器遗忘

    Towards Unbounded Machine Unlearning. (arXiv:2302.09880v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09880](http://arxiv.org/abs/2302.09880)

    本文是第一篇研究不同应用（偏见消除、混淆解决、隐私保护）遗忘问题的论文，提出了适用于不同应用的遗忘定义和指标，并提出了SCRUB，一种在不同应用的遗忘质量度量上始终是顶级表现者的算法。

    

    深度机器遗忘是指从训练好的神经网络中“移除”其训练集的一个子集的问题。这个问题非常及时，并且有许多应用，包括解除偏见（RB）、消除混淆（RC）（由训练模型中的错误标签数据引起），以及允许用户行使“被遗忘权”以保护用户隐私（UP）的关键任务。本文是我们所知的第一篇研究不同应用（RB、RC、UP）的遗忘问题的论文，我们认为每个应用都有自己的忘记需求、忘记定义和与忘记质量相关的指标。对于UP，我们提出了一种用于遗忘的新颖适应性强的成员推断攻击。我们还提出了SCRUB，一种新颖的遗忘算法，在RB、RC和UP的不同应用相关度量指标上始终是忘记质量的顶级表现者。同时，SCRUB还在衡量模式的度量指标上始终是顶级表现者。

    Deep machine unlearning is the problem of `removing' from a trained neural network a subset of its training set. This problem is very timely and has many applications, including the key tasks of removing biases (RB), resolving confusion (RC) (caused by mislabelled data in trained models), as well as allowing users to exercise their `right to be forgotten' to protect User Privacy (UP). This paper is the first, to our knowledge, to study unlearning for different applications (RB, RC, UP), with the view that each has its own desiderata, definitions for `forgetting' and associated metrics for forget quality. For UP, we propose a novel adaptation of a strong Membership Inference Attack for unlearning. We also propose SCRUB, a novel unlearning algorithm, which is the only method that is consistently a top performer for forget quality across the different application-dependent metrics for RB, RC, and UP. At the same time, SCRUB is also consistently a top performer on metrics that measure mode
    
[^172]: 一个渐近最优的凸包成员问题算法

    An Asymptotically Optimal Algorithm for the Convex Hull Membership Problem. (arXiv:2302.02033v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02033](http://arxiv.org/abs/2302.02033)

    本研究提出了一个名为Thompson-CHM的渐近最优算法，用于解决凸包成员问题，且将算法扩展到了一维和多维环境中。该算法基于模块化设计，包括停止规则和采样规则，并通过数值实验验证了理论结果的准确性。

    

    本研究将凸包成员问题的纯探索设置与凸包均值的有限分布集合中有效准确地确定给定点是否在凸包中相关。我们在一维环境中完全刻画了凸包成员问题的样本复杂性。我们提出了第一个渐近最优算法，名为Thompson-CHM，其模块化设计包括停止规则和采样规则。此外，我们将算法扩展到了一些在多臂赌博机文献中广义的重要问题。此外，我们还讨论了Thompson-CHM在高维情况下的扩展。最后，我们进行了数值实验，以展示算法的经验行为与我们在实际时间范围内的理论结果相匹配。

    This work studies the pure-exploration setting for the convex hull membership (CHM) problem where one aims to efficiently and accurately determine if a given point lies in the convex hull of means of a finite set of distributions. We give a complete characterization of the sample complexity of the CHM problem in the one-dimensional setting. We present the first asymptotically optimal algorithm called Thompson-CHM, whose modular design consists of a stopping rule and a sampling rule. In addition, we extend the algorithm to settings that generalize several important problems in the multi-armed bandit literature. Furthermore, we discuss the extension of Thompson-CHM to higher dimensions. Finally, we provide numerical experiments to demonstrate the empirical behavior of the algorithm matches our theoretical results for realistic time horizons.
    
[^173]: 蒸馏策略优化

    Distillation Policy Optimization. (arXiv:2302.00533v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00533](http://arxiv.org/abs/2302.00533)

    本文展示了一种演员-评论家的学习框架，该框架通过蒸馏优势在利用过去经验的同时遵循稳定的在线策略，实现了快速学习并可以适用于广泛的算法类别。

    

    本文提出了一个演员-评论家学习框架，它借鉴了分布式学习的视角和两种策略改进数据的交叉融合，实现了快速学习并可应用于广泛的算法类别。在该框架中，首先提出了方差减少机制，例如统一优势估计器 (UAE) 和一个学习的基线，不仅是连接到动作值函数的桥梁，还能提炼优势。

    On-policy algorithms are supposed to be stable, however, sample-intensive yet. Off-policy algorithms utilizing past experiences are deemed to be sample-efficient, nevertheless, unstable in general. Can we design an algorithm that can employ the off-policy data, while exploit the stable learning by sailing along the course of the on-policy walkway? In this paper, we present an actor-critic learning framework that borrows the distributional perspective of interest to evaluate, and cross-breeds two sources of the data for policy improvement, which enables fast learning and can be applied to a wide class of algorithms. In its backbone, the variance reduction mechanisms, such as unified advantage estimator (UAE), that extends generalized advantage estimator (GAE) to be applicable on any state-dependent baseline, and a learned baseline, that is competent to stabilize the policy gradient, are firstly put forward to not merely be a bridge to the action-value function but also distill the advan
    
[^174]: 自我激励的多智能体探索

    Self-Motivated Multi-Agent Exploration. (arXiv:2301.02083v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02083](http://arxiv.org/abs/2301.02083)

    这项研究提出了自我激励的多智能体探索方法，在合作多智能体强化学习中找到自我探索和团队合作的平衡，以实现团队任务的成功。

    

    在合作多智能体强化学习中，智能体在自我探索和团队合作之间实现平衡至关重要。然而，智能体在没有协调的情况下几乎无法完成团队任务，并且容易陷入只进行简单合作的局部最优解中，无法进行足够的个体探索。最近的研究主要集中在智能体的协调探索，导致状态空间的探索指数级增加。为了解决这个问题，我们提出了自我激励的多智能体探索（SMMAE），旨在通过自适应地找到自我探索和团队合作之间的权衡来取得团队任务的成功。在SMMAE中，我们针对每个智能体训练一个独立的探索策略，以最大化它们自己所访问的状态空间。每个智能体根据联合团队策略的稳定性学习可调节的探索概率。在StarCraft II微管理基准测试中进行了实验，验证了SMMAE的有效性。

    In cooperative multi-agent reinforcement learning (CMARL), it is critical for agents to achieve a balance between self-exploration and team collaboration. However, agents can hardly accomplish the team task without coordination and they would be trapped in a local optimum where easy cooperation is accessed without enough individual exploration. Recent works mainly concentrate on agents' coordinated exploration, which brings about the exponentially grown exploration of the state space. To address this issue, we propose Self-Motivated Multi-Agent Exploration (SMMAE), which aims to achieve success in team tasks by adaptively finding a trade-off between self-exploration and team cooperation. In SMMAE, we train an independent exploration policy for each agent to maximize their own visited state space. Each agent learns an adjustable exploration probability based on the stability of the joint team policy. The experiments on highly cooperative tasks in StarCraft II micromanagement benchmark (
    
[^175]: 从轨迹中元学习可泛化的动力学

    Metalearning generalizable dynamics from trajectories. (arXiv:2301.00957v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00957](http://arxiv.org/abs/2301.00957)

    本研究提出了可解释的元神经常微分方程（iMODE）方法，通过学习轨迹中的动力学，可以快速准确地建模和预测多个不同物理参数的动态系统，并能反向推断系统的物理参数。

    

    我们提出了可解释的元神经常微分方程（iMODE）方法，用于快速学习可泛化（即不特定于参数）的多个动态系统的动力学，这些系统在其物理参数上有所变化。iMODE方法通过采用双层优化框架来学习元知识，即动力系统实例的力场的功能变化，而不知道物理参数：外层捕捉研究的动力系统实例之间的共同力场形式，内层则适应各个系统实例。先验的物理知识可以方便地嵌入到神经网络架构中作为归纳偏差，例如保守力场和欧几里得对称性。通过学习到的元知识，iMODE可以在几秒钟内对未知系统进行建模，并反向揭示关于系统物理参数的知识，或作为“测量”未知系统物理参数的神经量规。

    We present the interpretable meta neural ordinary differential equation (iMODE) method to rapidly learn generalizable (i.e., not parameter-specific) dynamics from trajectories of multiple dynamical systems that vary in their physical parameters. The iMODE method learns meta-knowledge, the functional variations of the force field of dynamical system instances without knowing the physical parameters, by adopting a bi-level optimization framework: an outer level capturing the common force field form among studied dynamical system instances and an inner level adapting to individual system instances. A priori physical knowledge can be conveniently embedded in the neural network architecture as inductive bias, such as conservative force field and Euclidean symmetry. With the learned meta-knowledge, iMODE can model an unseen system within seconds, and inversely reveal knowledge on the physical parameters of a system, or as a Neural Gauge to "measure" the physical parameters of an unseen syste
    
[^176]: UnICLAM：对抗性屏蔽的对比表示学习用于统一和可解释的医学视觉问答

    UnICLAM:Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering. (arXiv:2212.10729v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.10729](http://arxiv.org/abs/2212.10729)

    UnICLAM是一种统一和可解释的医学视觉问答模型，通过对比表示学习和对抗性屏蔽，实现了图像和文本之间的对齐和语义表示。

    

    医学视觉问答（Medical-VQA）旨在回答有关放射学图像的临床问题，为医生提供决策选项。然而，当前的Medical-VQA模型通过将视觉和纹理编码器分别放置在双独立空间中来学习跨模态表示，这导致间接的语义对齐。在本文中，我们提出了UnICLAM，一种通过对比表示学习和对抗性屏蔽实现统一和可解释的医学视觉问答模型。具体来说，为了学习对齐的图像-文本表示，我们首先建立了一个统一的双流预训练结构，采用逐渐软参数共享策略。技术上，所提出的策略学习了一个约束，使得视觉和纹理编码器在同一空间中接近，随着层数的增加，逐渐放松。此外，为了把握统一的语义表示，我们将对抗性屏蔽数据增强拓展到对比表示中。

    Medical Visual Question Answering (Medical-VQA) aims to to answer clinical questions regarding radiology images, assisting doctors with decision-making options. Nevertheless, current Medical-VQA models learn cross-modal representations through residing vision and texture encoders in dual separate spaces, which lead to indirect semantic alignment. In this paper, we propose UnICLAM, a Unified and Interpretable Medical-VQA model through Contrastive Representation Learning with Adversarial Masking. Specifically, to learn an aligned image-text representation, we first establish a unified dual-stream pre-training structure with the gradually soft-parameter sharing strategy. Technically, the proposed strategy learns a constraint for the vision and texture encoders to be close in a same space, which is gradually loosened as the higher number of layers. Moreover, for grasping the unified semantic representation, we extend the adversarial masking data augmentation to the contrastive representati
    
[^177]: 基于StyleGAN的人脸生成和编辑：综述

    Face Generation and Editing with StyleGAN: A Survey. (arXiv:2212.09102v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.09102](http://arxiv.org/abs/2212.09102)

    本文为大家综述了基于深度学习技术的人脸生成和编辑的最新进展，特别介绍了基于GAN架构的最新方法StyleGAN。该方法可以生成高质量的人脸图像，并提供了丰富的可控语义编辑和保持照片质量的接口。

    

    本文目的在于提供深度学习技术在人脸生成和编辑中的最新进展综述。我们将介绍流行的最新架构并讨论关键思想，如反演，潜在表示，损失函数，训练程序，编辑方法和跨域样式转移。我们特别关注以GAN架构为基础的方法，其中最终集成在StyleGAN中，它允许生成高质量的人脸图像并提供丰富的可控语义编辑和保持照片质量的接口。我们旨在为对深度学习领域有基本了解且寻求易于理解的介绍和概述的读者提供一个入门的机会。

    Our goal with this survey is to provide an overview of the state of the art deep learning technologies for face generation and editing. We will cover popular latest architectures and discuss key ideas that make them work, such as inversion, latent representation, loss functions, training procedures, editing methods, and cross domain style transfer. We particularly focus on GAN-based architectures that have culminated in the StyleGAN approaches, which allow generation of high-quality face images and offer rich interfaces for controllable semantics editing and preserving photo quality. We aim to provide an entry point into the field for readers that have basic knowledge about the field of deep learning and are looking for an accessible introduction and overview.
    
[^178]: 深度学习优化器综述 - 一阶和二阶方法

    A survey of deep learning optimizers -- first and second order methods. (arXiv:2211.15596v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15596](http://arxiv.org/abs/2211.15596)

    该论文综述了在深度学习研究中成功使用的14种标准优化方法，并从优化文献的角度对数值优化中的困难进行了理论评估。

    

    深度学习优化涉及在权重空间中最小化高维损失函数，由于其固有的困难，如鞍点、局部最小值、Hessian矩阵的病态和有限的计算资源，通常被认为是困难的。在本文中，我们综合评述了深度学习研究中成功使用的14种标准优化方法，并从优化文献的角度对数值优化的困难进行了理论评估。

    Deep Learning optimization involves minimizing a high-dimensional loss function in the weight space which is often perceived as difficult due to its inherent difficulties such as saddle points, local minima, ill-conditioning of the Hessian and limited compute resources. In this paper, we provide a comprehensive review of $14$ standard optimization methods successfully used in deep learning research and a theoretical assessment of the difficulties in numerical optimization from the optimization literature.
    
[^179]: GPT-Neo用于常识推理--理论与实践视角

    GPT-Neo for commonsense reasoning -- a theoretical and practical lens. (arXiv:2211.15593v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15593](http://arxiv.org/abs/2211.15593)

    本文评估了GPT-Neo模型在常识推理任务上的性能，并与其他较大模型进行了比较。在适当的超参数设置下，该模型在多个任务上取得了具有竞争力的准确性。

    

    最近的研究展示了在大型语言模型（LLM）上进行预训练，然后在下游任务中进行有监督微调可以取得显著的进展。在本文中，我们对GPT-Neo模型在6个常识推理基准任务上的性能进行了评估。我们旨在对使用GPT-Neo模型的较小模型与GPT-3、Llama-2、MPT和Falcon等几个较大模型基准进行性能比较。在使用适当的超参数集进行微调后，我们的模型在多个任务上取得了有竞争力的准确性。我们还使用注意力头可视化来调查和证实我们的结果，以更好地理解模型的性能。最后，我们使用各种方法进行了多种鲁棒性测试，以评估模型在多种设置下的性能。

    Recent work has demonstrated substantial gains in pre-training large-language models (LLMs) followed by supervised fine-tuning on the downstream task. In this paper, we evaluate the performance of the GPT-neo model using $6$ commonsense reasoning benchmark tasks. We aim to examine the performance of smaller models using the GPT-neo models against several larger model baselines such as GPT-$3$, Llama-$2$, MPT and Falcon. Upon fine-tuning with the appropriate set of hyperparameters, our model achieves competitive accuracy on several tasks. We also investigate and substantiate our results using attention-head visualization to better understand the model performance. Finally, we conduct various robustness tests using various methods to gauge the model performance under numerous settings.
    
[^180]: 垂直联合学习：概念、进展和挑战

    Vertical Federated Learning: Concepts, Advances and Challenges. (arXiv:2211.12814v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12814](http://arxiv.org/abs/2211.12814)

    垂直联合学习（VFL）是一种联合学习设置，多个具有关于同一组用户不同特征的参与方共同训练机器学习模型，而不公开原始数据或模型参数。本文提供了对VFL概念、算法以及各个方面的进展和挑战的综合回顾，深入分析了隐私保护协议的分类、隐私攻击和防御策略，并提出了考虑多个约束条件的统一框架VFLow。此外，还回顾了工业应用中的最新进展和VFL面临的未来挑战和方向。

    

    垂直联合学习（VFL）是一种联合学习设置，多个具有关于同一组用户不同特征的参与方共同训练机器学习模型，而不公开原始数据或模型参数。受VFL研究和实际应用的快速增长的推动，我们全面回顾了VFL的概念和算法，以及各个方面的当前进展和挑战，包括有效性、效率和隐私。我们提供了对VFL设置和隐私保护协议的详尽分类，并对每个协议的隐私攻击和防御策略进行了全面分析。最后，我们提出了一个统一的框架，称为VFLow，它考虑了VFL问题在通信、计算、隐私以及有效性和公平性约束下的情况。最后，我们回顾了工业应用中最新的进展，突出了VFL面临的开放性挑战和未来方向。

    Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.
    
[^181]: 低精度环境下利普希茨连续损失函数的SGD变种

    Variants of SGD for Lipschitz Continuous Loss Functions in Low-Precision Environments. (arXiv:2211.04655v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.04655](http://arxiv.org/abs/2211.04655)

    本文研究了在低精度环境下神经网络训练的SGD变种，并测试了不同变种的效果。结果表明，相比于传统的SGD方法，在低精度算术环境下使用自适应步长的SGD变种可以获得更好的测试集准确性。

    

    本文研究了在低位浮点和定点环境下神经网络训练时使用自适应步长的SGD变种的收敛性。考虑到一般随机利普希茨连续的损失函数，假设只能计算损失函数随机梯度的近似值以及计算SGD步骤本身时的误差，给出了一个渐近收敛到Clarke稳定点的结果和到近似稳定点的非渐近收敛。在各种低精度算术环境下经验地测试了不同的SGD变种，在两个图像识别任务中与SGD相比观察到了改进的测试集准确性。

    Motivated by neural network training in low-bit floating and fixed-point environments, this work studies the convergence of variants of SGD using adaptive step sizes with computational error. Considering a general stochastic Lipschitz continuous loss function, an asymptotic convergence result to a Clarke stationary point, and the non-asymptotic convergence to an approximate stationary point are presented assuming that only an approximation of the loss function's stochastic gradient can be computed, as well as error in computing the SGD step itself. Different variants of SGD are tested empirically in a variety of low-precision arithmetic environments, where improved test set accuracy is observed compared to SGD for two image recognition tasks.
    
[^182]: 使用有限自动机验证和解释神经网络

    Verifying And Interpreting Neural Networks using Finite Automata. (arXiv:2211.01022v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2211.01022](http://arxiv.org/abs/2211.01022)

    这项研究提出了一种使用有限自动机来验证和解释神经网络的方法。通过构建特殊的弱Büchi自动机，能够精确地捕捉神经网络的输入输出行为，并用于解决DNN的常见验证和解释任务，如对抗鲁棒性或最小充分原因。

    

    鉴于深度神经网络（DNN）在包括安全关键应用在内的各个领域的普遍使用和其黑盒特性，验证属性和解释DNN的行为是一项重要任务。我们提出了一种基于自动机理论的方法来解决DNN分析中出现的问题。我们展示了DNN的输入输出行为可以被一个（特殊的）弱Büchi自动机精确地捕获，并且展示了如何利用这些来解决DNN的常见验证和解释任务，如对抗鲁棒性或最小充分原因。

    Verifying properties and interpreting the behaviour of deep neural networks (DNN) is an important task given their ubiquitous use in applications, including safety-critical ones, and their black-box nature. We propose an automata-theoric approach to tackling problems arising in DNN analysis. We show that the input-output behaviour of a DNN can be captured precisely by a (special) weak B\"uchi automaton and we show how these can be used to address common verification and interpretation tasks of DNN like adversarial robustness or minimum sufficient reasons.
    
[^183]: 针对大规模高动态范围成像的深度网络系列

    Deep network series for large-scale high-dynamic range imaging. (arXiv:2210.16060v3 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2210.16060](http://arxiv.org/abs/2210.16060)

    我们提出了一种针对大规模高动态范围成像的深度网络系列方法，通过残差DNN逐步增加动态范围，通过DNN迭代地估计，仅用很少的项就能得到良好的结果。

    

    我们提出了一种针对大规模高动态范围计算成像的新方法。通过端到端训练的深度神经网络（DNN）几乎可以瞬间解决线性反演成像问题。虽然展开的架构对于测量设置的变化具有鲁棒性，但在DNN架构中嵌入大规模测量运算符是不实际的。替代的即插即用（PnP）方法对可扩展性和高动态范围的挑战证明是有效的，但依赖于高度迭代的算法。我们提出了一种残差DNN系列方法，也可以解释为学习版的匹配追踪，其中重建的图像是逐步增加动态范围的残余图像的和，通过DNN迭代地估计，以前一次迭代的反投影数据残差作为输入。我们在射电天文成像模拟中证明，仅有几个项的系列就可以提供较好的结果。

    We propose a new approach for large-scale high-dynamic range computational imaging. Deep Neural Networks (DNNs) trained end-to-end can solve linear inverse imaging problems almost instantaneously. While unfolded architectures provide robustness to measurement setting variations, embedding large-scale measurement operators in DNN architectures is impractical. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, have proven effective to address scalability and high-dynamic range challenges, but rely on highly iterative algorithms. We propose a residual DNN series approach, also interpretable as a learned version of matching pursuit, where the reconstructed image is a sum of residual images progressively increasing the dynamic range, and estimated iteratively by DNNs taking the back-projected data residual of the previous iteration as input. We demonstrate on radio-astronomical imaging simulations that a series of only few terms provid
    
[^184]: 使用基于置信度的筛选策略实现标签噪声鲁棒学习

    Label Noise-Robust Learning using a Confidence-Based Sieving Strategy. (arXiv:2210.05330v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05330](http://arxiv.org/abs/2210.05330)

    本文提出了一种使用基于置信度的筛选策略实现标签噪声鲁棒学习的方法。通过利用模型的置信度分数，可以有效区分干净样本和有噪声的样本，提供了理论保证并在实验证明了其相对于最近的研究具有卓越性能。

    

    在存在标签噪声的学习任务中，改善模型对过拟合的鲁棒性是一个关键挑战，因为模型最终会记住包括噪声标签在内的标签。识别具有噪声标签的样本并防止模型学习它们是解决这个挑战的一种有效方法。在训练过程中，模型的每个类别的置信度分数，表示为类别概率，可以作为评估输入标签是否真实标签或者是损坏标签的可靠标准。在这项工作中，我们利用这一观察结果，提出了一种新颖的判别度量称为置信度误差和一种称为CONFES的筛选策略，能够有效区分干净样本和有噪声的样本。我们提供了关于我们提出的度量的误差概率的理论保证。然后，通过实验证明了我们提出的方法在各种设置下（例如合成数据和真实世界数据）相对于最近的研究具有卓越性能。

    In learning tasks with label noise, improving model robustness against overfitting is a pivotal challenge because the model eventually memorizes labels, including the noisy ones. Identifying the samples with noisy labels and preventing the model from learning them is a promising approach to address this challenge. When training with noisy labels, the per-class confidence scores of the model, represented by the class probabilities, can be reliable criteria for assessing whether the input label is the true label or the corrupted one. In this work, we exploit this observation and propose a novel discriminator metric called confidence error and a sieving strategy called CONFES to differentiate between the clean and noisy samples effectively. We provide theoretical guarantees on the probability of error for our proposed metric. Then, we experimentally illustrate the superior performance of our proposed approach compared to recent studies on various settings, such as synthetic and real-world
    
[^185]: 动态学习的神经隐性表示的多目标导航

    Multi-Object Navigation with dynamically learned neural implicit representations. (arXiv:2210.05129v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.05129](http://arxiv.org/abs/2210.05129)

    本研究提出了一种结构化神经网络的方法，利用动态学习的神经隐性表示对多目标导航进行建模和映射，其中包括语义定位和占用探索隐性表示。

    

    理解和映射新环境是任何自主导航代理的核心能力。传统的机器人学通常使用SLAM变体以独立的方式估计地图，这些变体维护了拓扑或度量表示，而端到端学习的导航则在神经网络中保持了某种形式的记忆。网络通常具有归纳偏差，可以从矢量表示到鸟瞰度量张量或拓扑结构。在这项工作中，我们提出使用两个神经隐性表示来构造神经网络，在每个episode期间动态学习并映射场景内容：(i) 语义定位器预测先前看到的查询对象的位置；(ii) 占用和探索隐性表示封装了已探索区域和障碍物的信息，并通过新颖的全局读取机制进行查询，该机制直接从函数空间映射到可用的嵌入空间。

    Understanding and mapping a new environment are core abilities of any autonomously navigating agent. While classical robotics usually estimates maps in a stand-alone manner with SLAM variants, which maintain a topological or metric representation, end-to-end learning of navigation keeps some form of memory in a neural network. Networks are typically imbued with inductive biases, which can range from vectorial representations to birds-eye metric tensors or topological structures. In this work, we propose to structure neural networks with two neural implicit representations, which are learned dynamically during each episode and map the content of the scene: (i) the Semantic Finder predicts the position of a previously seen queried object; (ii) the Occupancy and Exploration Implicit Representation encapsulates information about explored area and obstacles, and is queried with a novel global read mechanism which directly maps from function space to a usable embedding space. Both representa
    
[^186]: GeONet：一种学习Wasserstein测地的神经算子

    GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14440](http://arxiv.org/abs/2209.14440)

    GeONet是一个不受网格影响的深度神经算子网络，学习了从初始和终端分布到连接两个端点分布的Wasserstein测地的非线性映射。通过学习鞍点优化条件，GeONet可以快速进行实时预测，并在仿真示例和测试数据上取得了与标准OT求解器相当的准确性。

    

    最优传输(OT)提供了一种将复杂数据分布进行几何上有意义比较的通用框架。传统的计算概率测度的Wasserstein距离和测地的方法需要网格依赖的域离散化，同时受到维度灾难的影响。我们提出了GeONet，一种不受网格影响的深度神经算子网络，它学习了将输入的初始和终端分布映射到连接两个端点分布的Wasserstein测地的非线性映射。在脱机训练阶段，GeONet通过耦合的PDE系统表征的原始和对偶空间中的动态最优条件学习了OT问题的鞍点优化条件。后续的推理阶段是瞬时完成的，并可以在在线学习设置中用于实时预测。我们证明了GeONet在仿真示例和...

    Optimal transport (OT) offers a versatile framework to compare complex data distributions in a geometrically meaningful way. Traditional methods for computing the Wasserstein distance and geodesic between probability measures require mesh-dependent domain discretization and suffer from the curse-of-dimensionality. We present GeONet, a mesh-invariant deep neural operator network that learns the non-linear mapping from the input pair of initial and terminal distributions to the Wasserstein geodesic connecting the two endpoint distributions. In the offline training stage, GeONet learns the saddle point optimality conditions for the dynamic formulation of the OT problem in the primal and dual spaces that are characterized by a coupled PDE system. The subsequent inference stage is instantaneous and can be deployed for real-time predictions in the online learning setting. We demonstrate that GeONet achieves comparable testing accuracy to the standard OT solvers on simulation examples and the
    
[^187]: 克服语言指导的目标-条件强化学习中指代歧义

    Overcoming Referential Ambiguity in Language-Guided Goal-Conditioned Reinforcement Learning. (arXiv:2209.12758v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12758](http://arxiv.org/abs/2209.12758)

    本文研究了语言指导的目标-条件强化学习中的指代歧义问题，并提出了教学法和实用主义的概念来解决这些问题。实验证明，这些概念可以提高学习者的训练效率。

    

    当教师用自然语言指导一个代理执行新任务时，解释的歧义很容易成为阻碍。当教师通过参考物体的特征向学习者提供指令时，学习者可能会误解教师的意图，特别是当指令模糊地涉及物体的特征时，这种现象称为指代歧义。我们研究了两个源自认知科学的概念如何帮助解决这些指代歧义：教学法（选择合适的指令）和实用主义（通过归纳推理了解其他代理的偏好）。我们将这些思想应用于一个带有两个人工代理的模拟机器人任务（堆积木块）。我们展示了这些概念如何提高学习者的训练样本效率。

    Teaching an agent to perform new tasks using natural language can easily be hindered by ambiguities in interpretation. When a teacher provides an instruction to a learner about an object by referring to its features, the learner can misunderstand the teacher's intentions, for instance if the instruction ambiguously refer to features of the object, a phenomenon called referential ambiguity. We study how two concepts derived from cognitive sciences can help resolve those referential ambiguities: pedagogy (selecting the right instructions) and pragmatism (learning the preferences of the other agents using inductive reasoning). We apply those ideas to a teacher/learner setup with two artificial agents on a simulated robotic task (block-stacking). We show that these concepts improve sample efficiency for training the learner.
    
[^188]: AMoD系统中的稳健和约束多智能体强化学习电动汽车再平衡方法

    A Robust and Constrained Multi-Agent Reinforcement Learning Electric Vehicle Rebalancing Method in AMoD Systems. (arXiv:2209.08230v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2209.08230](http://arxiv.org/abs/2209.08230)

    该论文提出了一个稳健和约束的多智能体强化学习框架，用于解决电动汽车AMoD系统中的再平衡问题，并考虑了模型不确定性和决策约束。该方法能够设计出稳健的电动汽车再平衡策略，提高系统性能。

    

    电动汽车在自主移动出行（AMoD）系统中发挥着关键作用，但其独特的充电模式增加了AMoD系统中的模型不确定性（例如状态转移概率）。由于训练环境和测试/真实环境之间通常存在不匹配情况，将模型不确定性纳入系统设计是实际应用中至关重要的。然而，现有文献尚未明确考虑模型不确定性对电动汽车AMoD系统再平衡的影响，而决策需要满足的模型不确定性和约束的共存使问题变得更加具有挑战性。在这项工作中，我们设计了一个具有状态转移核不确定性的稳健和约束的多智能体强化学习（MARL）框架用于电动汽车AMoD系统。然后，我们提出了一个使用稳健自然策略梯度（RNPG）训练稳健的电动汽车再平衡策略的稳健和约束的MARL算法（ROCOMA）。

    Electric vehicles (EVs) play critical roles in autonomous mobility-on-demand (AMoD) systems, but their unique charging patterns increase the model uncertainties in AMoD systems (e.g. state transition probability). Since there usually exists a mismatch between the training and test/true environments, incorporating model uncertainty into system design is of critical importance in real-world applications. However, model uncertainties have not been considered explicitly in EV AMoD system rebalancing by existing literature yet, and the coexistence of model uncertainties and constraints that the decision should satisfy makes the problem even more challenging. In this work, we design a robust and constrained multi-agent reinforcement learning (MARL) framework with state transition kernel uncertainty for EV AMoD systems. We then propose a robust and constrained MARL algorithm (ROCOMA) with robust natural policy gradients (RNPG) that trains a robust EV rebalancing policy to balance the supply-d
    
[^189]: 探索鸟瞰视角感知中的挑战：一项综述、评估和方法

    Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe. (arXiv:2209.05324v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.05324](http://arxiv.org/abs/2209.05324)

    本综述文章探讨了鸟瞰视角感知领域的挑战和方法，主要关注了从透视视图到鸟瞰视图的信息转换、地面真值注释获取、特征融合以及整体流程的构建。

    

    在感知任务中，学习鸟瞰视角（BEV）的强大表示正变得越来越流行，并引起了业界和学术界的广泛关注。传统方法中，大多数自动驾驶算法在前方或透视视图中进行检测、分割、跟踪等操作。随着传感器配置越来越复杂，从不同传感器中集成多源信息并以统一的视图表示特征变得非常重要。BEV感知具有几个优势，即以BEV表示周围场景直观且易于融合；以BEV表示物体对于后续的规划和/或控制模块是最理想的。BEV感知的核心问题在于：（a）如何通过从透视视图到BEV的视角转换重建丢失的3D信息；（b）如何在BEV网格中获取地面真值注释；（c）如何构建包含来自不同来源和视图的特征的流程；以及（d）...

    Learning powerful representations in bird's-eye-view (BEV) for perception tasks is trending and drawing extensive attention both from industry and academia. Conventional approaches for most autonomous driving algorithms perform detection, segmentation, tracking, etc., in a front or perspective view. As sensor configurations get more complex, integrating multi-source information from different sensors and representing features in a unified view come of vital importance. BEV perception inherits several advantages, as representing surrounding scenes in BEV is intuitive and fusion-friendly; and representing objects in BEV is most desirable for subsequent modules as in planning and/or control. The core problems for BEV perception lie in (a) how to reconstruct the lost 3D information via view transformation from perspective view to BEV; (b) how to acquire ground truth annotations in BEV grid; (c) how to formulate the pipeline to incorporate features from different sources and views; and (d) 
    
[^190]: 使用可扩展的机器学习模型和脑电图研究驾驶中的嗜睡检测性能

    Studying Drowsiness Detection Performance while Driving through Scalable Machine Learning Models using Electroencephalography. (arXiv:2209.04048v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2209.04048](http://arxiv.org/abs/2209.04048)

    本研究通过使用脑电图和机器学习的智能框架，在驾驶场景中检测驾驶员嗜睡状态。结果表明，随机森林（RF）是性能最佳的模型，相比支持向量机（SVM）有更高的f1分数。

    

    背景 / 引言：驾驶员嗜睡是一个重要问题，也是交通事故的主要原因之一。认知神经科学和计算机科学的进展使得可以利用脑-计算机接口（BCIs）和机器学习（ML）检测驾驶员的嗜睡状态。然而，文献中缺乏对使用不同机器学习算法进行综合评估的嗜睡检测性能，并且有必要研究适用于被试群体的可扩展机器学习模型的性能。- 方法：为了解决这些限制，本研究提出了一种智能框架，利用脑-计算机接口和基于脑电图的特征，用于检测驾驶场景中的嗜睡状态。使用SEED-VIG数据集评估了最佳性能模型在个体和群体上的表现。- 结果：结果显示，随机森林（RF）在个体驾驶员上的性能优于其他在文献中使用的模型，如支持向量机（SVM），具有78％的f1分数。

    - Background / Introduction: Driver drowsiness is a significant concern and one of the leading causes of traffic accidents. Advances in cognitive neuroscience and computer science have enabled the detection of drivers' drowsiness using Brain-Computer Interfaces (BCIs) and Machine Learning (ML). However, the literature lacks a comprehensive evaluation of drowsiness detection performance using a heterogeneous set of ML algorithms, and it is necessary to study the performance of scalable ML models suitable for groups of subjects. - Methods: To address these limitations, this work presents an intelligent framework employing BCIs and features based on electroencephalography for detecting drowsiness in driving scenarios. The SEED-VIG dataset is used to evaluate the best-performing models for individual subjects and groups. - Results: Results show that Random Forest (RF) outperformed other models used in the literature, such as Support Vector Machine (SVM), with a 78% f1-score for individual 
    
[^191]: 递归可行的带有控制屏障函数的概率安全在线学习

    Recursively Feasible Probabilistic Safe Online Learning with Control Barrier Functions. (arXiv:2208.10733v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2208.10733](http://arxiv.org/abs/2208.10733)

    本文提出了一种递归可行的概率安全在线学习方法，利用控制屏障函数和高斯过程回归，使得系统在在线训练和执行过程中保持安全。

    

    最近，基于学习的控制方案在各种应用中展示出了执行复杂任务的高效性。然而，为了在实际系统中部署这些方案，保证系统在在线训练和执行过程中保持安全至关重要。在当前最流行的解决这一挑战的方法中，控制屏障函数（CBFs）作为数学工具，为具有已知动力学的系统提供正式的保持安全的控制合成过程。在本文中，我们首先引入了一个模型不确定性感知的CBF安全关键控制器的重构，使用高斯过程（GP）回归来建立近似数学模型与真实系统之间的关联。与以前的方法相比，我们研究了得到的鲁棒安全关键控制器的可行性。该可行性分析结果导致了关于系统可用信息应满足的一系列丰富条件，以确保。

    Learning-based control schemes have recently shown great efficacy performing complex tasks for a wide variety of applications. However, in order to deploy them in real systems, it is of vital importance to guarantee that the system will remain safe during online training and execution. Among the currently most popular methods to tackle this challenge, Control Barrier Functions (CBFs) serve as mathematical tools that provide a formal safety-preserving control synthesis procedure for systems with known dynamics. In this paper, we first introduce a model-uncertainty-aware reformulation of CBF-based safety-critical controllers using Gaussian Process (GP) regression to bridge the gap between an approximate mathematical model and the real system. Compared to previous approaches, we study the feasibility of the resulting robust safety-critical controller. This feasibility analysis results in a set of richness conditions that the available information about the system should satisfy to guarant
    
[^192]: 连续学习对抗对抗性攻击的易感性

    Susceptibility of Continual Learning Against Adversarial Attacks. (arXiv:2207.05225v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.05225](http://arxiv.org/abs/2207.05225)

    本文研究了连续学习任务对抗攻击的易感性，发现学习任务容易受到对抗性攻击导致的目标类别错误分类。这对数据完整性和隐私构成了重大威胁。

    

    近期的连续学习方法主要集中在减少灾难性遗忘。然而，有两个关键领域相对未被探索：1）评估所提出方法的鲁棒性和2）确保学习任务的安全性。本文研究了连续学习任务，包括当前和先前获取的任务，对抗攻击的易感性。具体而言，我们观察到任何属于任何任务的类别都可以很容易地成为任何其他任务所需目标类别的目标，并且被错误分类。这种学习任务对抗攻击的易感性引发了有关数据完整性和隐私的深刻关切。为了评估连续学习方法的鲁棒性，我们考虑了三种场景下的连续学习方法，即任务递增学习、领域递增学习和类递增学习。在这方面，我们探索了三种基于正则化的方法的鲁棒性。

    Recent continual learning approaches have primarily focused on mitigating catastrophic forgetting. Nevertheless, two critical areas have remained relatively unexplored: 1) evaluating the robustness of proposed methods and 2) ensuring the security of learned tasks. This paper investigates the susceptibility of continually learned tasks, including current and previously acquired tasks, to adversarial attacks. Specifically, we have observed that any class belonging to any task can be easily targeted and misclassified as the desired target class of any other task. Such susceptibility or vulnerability of learned tasks to adversarial attacks raises profound concerns regarding data integrity and privacy. To assess the robustness of continual learning approaches, we consider continual learning approaches in all three scenarios, i.e., task-incremental learning, domain-incremental learning, and class-incremental learning. In this regard, we explore the robustness of three regularization-based me
    
[^193]: FedSS: 智能选择客户端的联邦学习

    FedSS: Federated Learning with Smart Selection of clients. (arXiv:2207.04569v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2207.04569](http://arxiv.org/abs/2207.04569)

    本论文提出了一种名为FedSS的智能选择客户端的联邦学习方法，通过智能客户端选择和调度技术，平衡了快速收敛和异质性之间的关系。

    

    联邦学习提供了在保护用户隐私的同时以分布式方式学习异构用户数据的能力。然而，其当前的客户端选择技术存在偏见，因为它歧视慢速客户端。首先，它选择满足某些网络和系统特定条件的客户端，因此没有选择慢速客户端。即使这些客户端被包括在训练过程中，它们要么难以训练，要么因为太慢而被放弃。我们提出的想法是通过智能客户端选择和调度技术找到快速收敛和异质性之间的平衡点。

    Federated learning provides the ability to learn over heterogeneous user data in a distributed manner while preserving user privacy. However, its current client selection technique is a source of bias as it discriminates against slow clients. For starters, it selects clients that satisfy certain network and system-specific criteria, thus not selecting slow clients. Even when such clients are included in the training process, they either struggle with the training or are dropped altogether for being too slow. Our proposed idea looks to find a sweet spot between fast convergence and heterogeneity by looking at smart client selection and scheduling techniques.
    
[^194]: 两方私密独立性检测

    Private independence testing across two parties. (arXiv:2207.03652v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2207.03652](http://arxiv.org/abs/2207.03652)

    我们提出了一种私密独立性检测算法，通过私密估计数据集之间的距离相关性进行统计独立性测试。我们建立了差分隐私测试的加法和乘法误差界限，相信该算法在涉及敏感数据的分布式假设检验中会有应用。

    

    我们介绍了一种名为 $\pi$-test 的保护隐私算法，用于在多个方之间测试统计独立性。我们的算法依赖于对数据集之间的距离相关性进行私密估计，这是一种在 Sz\'ekely 等人 [2007] 中引入的独立性的定量度量。我们对我们的差分隐私测试的效用建立了加法和乘法误差界限，我们相信它将在涉及敏感数据的各种分布式假设检验场景中找到应用。

    We introduce $\pi$-test, a privacy-preserving algorithm for testing statistical independence between data distributed across multiple parties. Our algorithm relies on privately estimating the distance correlation between datasets, a quantitative measure of independence introduced in Sz\'ekely et al. [2007]. We establish both additive and multiplicative error bounds on the utility of our differentially private test, which we believe will find applications in a variety of distributed hypothesis testing settings involving sensitive data.
    
[^195]: 用于监督学习的群不变张量网络

    Group-invariant tensor train networks for supervised learning. (arXiv:2206.15051v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.15051](http://arxiv.org/abs/2206.15051)

    本论文介绍了一种用于监督学习的群不变张量网络方法，通过构建群的不变张量基并结合群不变张量分解网络，可以获得与最先进的深度学习方法相通的预测准确性。

    

    近年来，不变性在机器学习模型中被证明是一种强大的归纳偏置。张量网络是一种预测性或生成性模型的类别之一。我们引入了一种新的数值算法来构建群的正常矩阵表示下不变的张量基。这种方法比以前的方法快上几个数量级。然后将群不变张量组合成群不变张量分解网络，可以用作监督机器学习模型。我们将此模型应用于蛋白质结合分类问题，考虑了问题特定的不变性，并获得了与最先进的深度学习方法相符的预测准确性。

    Invariance has recently proven to be a powerful inductive bias in machine learning models. One such class of predictive or generative models are tensor networks. We introduce a new numerical algorithm to construct a basis of tensors that are invariant under the action of normal matrix representations of an arbitrary discrete group. This method can be up to several orders of magnitude faster than previous approaches. The group-invariant tensors are then combined into a group-invariant tensor train network, which can be used as a supervised machine learning model. We applied this model to a protein binding classification problem, taking into account problem-specific invariances, and obtained prediction accuracy in line with state-of-the-art deep learning approaches.
    
[^196]: 在多目标环境下从教学示范中实用地学习

    Pragmatically Learning from Pedagogical Demonstrations in Multi-Goal Environments. (arXiv:2206.04546v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04546](http://arxiv.org/abs/2206.04546)

    该论文介绍了一种在多目标环境中从教学示范中实用地学习的方法。通过结合实用主义机制和教学机制，利用贝叶斯模型进行目标推断，可以加快学习速度并减少目标的歧义。

    

    从示范中学习的方法通常利用接近最佳示范来加快训练速度。相比之下，人类教师在示范任务时会偏离最佳示范，并通过给出最能消除目标歧义的示范来教学。类似地，人类学习者擅长实用地推断教师的意图，促进两个代理之间的交流。这些机制在少示范情景中至关重要，因为在这种情况下推断目标更困难。在本文中，我们通过利用示范的目标推断的贝叶斯模型（BGI）来实现教学和实用主义机制。我们展示了该模型在具有两个基于目标的强化学习的人工代理的多目标教师-学习者设置中的优势。我们表明，将BGI代理（教导者和实用学习者）相结合可以加快学习速度并减少目标的歧义。

    Learning from demonstration methods usually leverage close to optimal demonstrations to accelerate training. By contrast, when demonstrating a task, human teachers deviate from optimal demonstrations and pedagogically modify their behavior by giving demonstrations that best disambiguate the goal they want to demonstrate. Analogously, human learners excel at pragmatically inferring the intent of the teacher, facilitating communication between the two agents. These mechanisms are critical in the few demonstrations regime, where inferring the goal is more difficult. In this paper, we implement pedagogy and pragmatism mechanisms by leveraging a Bayesian model of Goal Inference from demonstrations (BGI). We highlight the benefits of this model in multi-goal teacher-learner setups with two artificial agents that learn with goal-conditioned Reinforcement Learning. We show that combining BGI-agents (a pedagogical teacher and a pragmatic learner) results in faster learning and reduced goal ambi
    
[^197]: DIRA: 一种用于动态领域增量正则化自适应的框架

    DIRA: A Framework for Dynamic Domain Incremental Regularised Adaptation. (arXiv:2205.00147v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.00147](http://arxiv.org/abs/2205.00147)

    DIRA是一个用于DNN分类器的动态领域自适应的框架，使用正则化技术来解决灾难性遗忘问题，并通过少量样本实现重新训练和适应性。

    

    自主系统（AS）经常使用深度神经网络（DNN）分类器，使它们能够在复杂、高维、非线性和动态变化的环境中运行。由于这些环境的复杂性，当DNN分类器面对开发过程中未识别的领域时，可能会在操作过程中输出错误分类。随着AS的数量增加，将系统从运行中移除进行重新训练变得不切实际。为增加AS的可靠性并克服这一限制，DNN分类器需要在操作过程中适应不同的操作领域，并能够使用少量样本（例如100个样本）进行重新训练。然而，已知在少量样本上重新训练DNN会导致灾难性遗忘。在本文中，我们介绍了一种名为动态增量正则化自适应（DIRA）的框架，用于使用正则化技术来实现DNN分类器的操作领域适应，从而克服灾难性遗忘并实现适应性。

    Autonomous systems (AS) often use Deep Neural Network (DNN) classifiers to allow them to operate in complex, high dimensional, non-linear, and dynamically changing environments. Due to the complexity of these environments, DNN classifiers may output misclassifications during operation when they face domains not identified during development. Removing a system from operation for retraining becomes impractical as the number of such AS increase. To increase AS reliability and overcome this limitation, DNN classifiers need to have the ability to adapt during operation when faced with different operational domains using a few samples (e.g. 100 samples). However, retraining DNNs on a few samples is known to cause catastrophic forgetting. In this paper, we introduce Dynamic Incremental Regularised Adaptation (DIRA), a framework for operational domain adaption of DNN classifiers using regularisation techniques to overcome catastrophic forgetting and achieve adaptation when retraining using few
    
[^198]: 人工导师-学习者互动中的教学演示和实用学习

    Pedagogical Demonstrations and Pragmatic Learning in Artificial Tutor-Learner Interactions. (arXiv:2203.00111v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00111](http://arxiv.org/abs/2203.00111)

    本文研究了人工智能导师-学习者互动中的教学演示和实用学习机制，并在一个多目标环境中实现。通过使用导师的教学和学习者的实用推理，相比传统的从演示中学习方法，我们实现了显著的改进。

    

    在展示任务时，人类导师通过“展示”任务而不仅仅是“完成”任务（夸大演示的相关部分）或者给出能最清晰表达目标的演示来教学。类似地，人类学习者会推断导师的交际意图：他们解释导师试图教他们什么，并推断出学习所需的相关信息。如果没有这样的机制，传统的从演示中学习（Learning from Demonstration，LfD）算法将认为这样的演示是次优的。本文研究在一个多目标环境中，导师和学习者都是人工智能代理人的学习者-导师设置中实现这样的机制。通过导师的教学和学习者的实用推理，我们展示了相比标准的从演示中学习方法的大幅改进。

    When demonstrating a task, human tutors pedagogically modify their behavior by either "showing" the task rather than just "doing" it (exaggerating on relevant parts of the demonstration) or by giving demonstrations that best disambiguate the communicated goal. Analogously, human learners pragmatically infer the communicative intent of the tutor: they interpret what the tutor is trying to teach them and deduce relevant information for learning. Without such mechanisms, traditional Learning from Demonstration (LfD) algorithms will consider such demonstrations as sub-optimal. In this paper, we investigate the implementation of such mechanisms in a tutor-learner setup where both participants are artificial agents in an environment with multiple goals. Using pedagogy from the tutor and pragmatism from the learner, we show substantial improvements over standard learning from demonstrations.
    
[^199]: 高效直连拓扑结构用于集体通信

    Efficient Direct-Connect Topologies for Collective Communications. (arXiv:2202.03356v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2202.03356](http://arxiv.org/abs/2202.03356)

    本论文提出了一种算法框架，可以构建高效的直连网络拓扑结构，以优化节点延迟和带宽权衡，适用于集体通信负载。

    

    本文研究了如何构建适用于集体通信的高效网络拓扑结构。我们提出了一种算法框架，用于构建针对节点延迟与带宽权衡优化的直连拓扑结构。这个算法框架从小的基础拓扑结构和相关的通信进度开始，并使用一组可以迭代应用的技术来派生更大的拓扑结构。这些衍生的拓扑结构的时间表可以与扩展一起合成，也可以使用优化公式计算。我们的方法允许我们为给定的集群大小和度数合成许多不同的拓扑结构和时间表，然后为给定的工作负载确定适当的拓扑和时间表。我们在使用补丁面板配置所需拓扑结构的12节点光学实验平台上评估了我们的方法，并增加了基于分析模型的评估，用于更大的部署。

    We consider the problem of distilling efficient network topologies for collective communications. We provide an algorithmic framework for constructing direct-connect topologies optimized for the node latency vs bandwidth trade-off given a collective communication workload. Our algorithmic framework allows us to start from small base topologies and associated communication schedules and use a set of techniques that can be iteratively applied to derive much larger topologies. The schedules for these derived topologies are either synthesized along with the expansions or computed using an optimization formulation. Our approach allows us to synthesize many different topologies and schedules for a given cluster size and degree, and then identify the appropriate topology and schedule for a given workload. We evaluate our approach on a 12-node optical testbed that uses patch panels for configuring the desired topology and augment it with an analytical-model-based evaluation for larger deployme
    
[^200]: 在学习行为空间中发现和利用稀疏奖励

    Discovering and Exploiting Sparse Rewards in a Learned Behavior Space. (arXiv:2111.01919v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.01919](http://arxiv.org/abs/2111.01919)

    这项研究介绍了一种名为STAX的算法，能够在学习行为空间时实时探索，并且能够有效优化任何发现的奖励。

    

    在稀疏奖励的环境中学习最优策略是困难的，因为学习代理几乎没有关于行为质量的反馈。在这种情况下，一个好的策略是专注于探索，希望能发现一个奖励信号以进行改进。一个能够处理这种情况的学习算法必须能够（1）探索可能的代理行为和（2）利用可能发现的任何奖励。已经提出了高效的探索算法，需要定义一个行为空间，将代理与其在可以探索的空间中表现出的行为相关联。需要定义这个空间是这些算法的一个限制。在这项工作中，我们介绍了STAX，一种设计用于实时学习行为空间并在有效地优化任何发现的奖励的算法。它通过将行为空间的探索和学习与奖励的利用分开，以替代的方式实现。

    Learning optimal policies in sparse rewards settings is difficult as the learning agent has little to no feedback on the quality of its actions. In these situations, a good strategy is to focus on exploration, hopefully leading to the discovery of a reward signal to improve on. A learning algorithm capable of dealing with this kind of settings has to be able to (1) explore possible agent behaviors and (2) exploit any possible discovered reward. Efficient exploration algorithms have been proposed that require to define a behavior space, that associates to an agent its resulting behavior in a space that is known to be worth exploring. The need to define this space is a limitation of these algorithms. In this work, we introduce STAX, an algorithm designed to learn a behavior space on-the-fly and to explore it while efficiently optimizing any reward discovered. It does so by separating the exploration and learning of the behavior space from the exploitation of the reward through an alterna
    
[^201]: 无监督的生产车间室内定位系统中的运动检测

    Unsupervised Movement Detection in Indoor Positioning Systems of Production Halls. (arXiv:2109.10757v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.10757](http://arxiv.org/abs/2109.10757)

    本文提出了一个无监督的统计过程来解决生产车间室内定位系统中数据分析中的挑战，通过结合可视分析和运动检测，不仅区分停留与移动，还考虑了不期望的唤醒，从而提供了详细的解释方案，并在实际案例研究中验证了该方法的可行性。

    

    本文考虑生产车间中的室内定位系统，其中装有传感器的物体发送其当前位置。鉴于数据容量较大，对产生的原始数据进行分析具有挑战性，原因是受到噪声干扰的影响。问题包括精度问题和传感器由于物流过程的动态性（例如经过的叉车的振动）而产生的不期望的唤醒。我们提出了一个量身定制的统计过程来解决这些问题，并将可视分析与运动检测相结合。与普通的停留点算法不同，我们不仅区分停留与移动，还考虑了不期望的唤醒。这导致了一个更详细的解释方案，适用于在线（例如订单监控）和离线应用（例如问题区域的检测）。该方法不需要除原始室内定位系统输出之外的其他信息，并能进行临时分析。我们在一个包含真实场景的广泛案例研究中验证了我们的发现。

    Consider indoor positioning systems (IPS) in production halls where objects equipped with sensors send their current position. Beside its large volume, the analyzation of the resulting raw data is challenging due to the susceptibility towards noise. Reasons are accuracy issues and undesired awakenings of sensors that occur due to the dynamics of logistic processes (e.g.~vibrations of passing forklifts). We propose a tailor-made statistical procedure for these challenges and combine visual analytics with movement detection. Contrary to common stay-point algorithms, we do not only distinguish between stops and moves, but also consider undesired awakenings. This leads to a more detailed interpretation scheme offering usages for online (e.g.~monitoring of orders) and offline applications (e.g.~detection of problematic areas). The approach does not require other information than the raw IPS output and enables an ad-hoc analysis. We underline our findings in an extensive case study with real
    
[^202]: 只需简单步骤：Frank-Wolfe算法和广义自协调函数

    Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions. (arXiv:2105.13913v6 [math.OC] UPDATED)

    [http://arxiv.org/abs/2105.13913](http://arxiv.org/abs/2105.13913)

    本论文介绍了一种简单的Frank-Wolfe算法变体，利用广义自协调函数的特性，在不需要使用二阶信息或估计局部平滑度参数的情况下，以$\mathcal{O}(1/t)$的收敛速度达到了优化目标。

    

    广义自协调是许多重要学习问题的目标函数中存在的一个关键特性。我们建立了一个简单的Frank-Wolfe变体的收敛速度，该变体使用了开环步长策略$\gamma_t=2/(t+2)$，对于这类函数在原始间隙和Frank-Wolfe间隙方面获得了$\mathcal{O}(1/t)$的收敛速度，其中$t$是迭代次数。这避免了使用二阶信息或需要估计先前工作的局部平滑度参数。我们还展示了不同常见情况下的改进收敛速度，例如，当所考虑的可行域是均匀凸的或者是多面体的时候。

    Generalized self-concordance is a key property present in the objective function of many important learning problems. We establish the convergence rate of a simple Frank-Wolfe variant that uses the open-loop step size strategy $\gamma_t = 2/(t+2)$, obtaining a $\mathcal{O}(1/t)$ convergence rate for this class of functions in terms of primal gap and Frank-Wolfe gap, where $t$ is the iteration count. This avoids the use of second-order information or the need to estimate local smoothness parameters of previous work. We also show improved convergence rates for various common cases, e.g., when the feasible region under consideration is uniformly convex or polyhedral.
    
[^203]: MimicNorm: 权重均值和最后一层批归一化层模仿批归一化的动态

    MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch Normalization. (arXiv:2010.09278v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.09278](http://arxiv.org/abs/2010.09278)

    本文提出了一种名为MimicNorm的归一化方法，通过简化批归一化（BN）的正则化方法并保持其核心影响，即数据去相关性和自适应学习率，来提高网络训练的收敛性和效率。MimicNorm仅包含两个轻量级操作，可与BN相媲美。

    

    大量实验证实了批归一化（BN）层在收敛和泛化效果上的成功。然而，BN需要额外的内存和浮点计算。此外，在微小批次上，BN会变得不准确，因为它依赖于批次统计信息。在本文中，我们通过简化BN的正则化方法来解决这些问题，同时保持BN层的两个基本影响，即数据去相关性和自适应学习率。我们提出了一种新的归一化方法，称为MimicNorm，来改善网络训练中的收敛性和效率。 MimicNorm仅包含两个轻量级操作，包括修改的权重均值操作（从权重参数张量中减去均值值）和损失函数（最后的BN层）之前的一个BN层。我们利用神经切线核（NTK）理论证明了我们的权重均值操作可以白化激活，使网络转化为类似BN层的混沌状态，从而导致了收敛性的提升。

    Substantial experiments have validated the success of Batch Normalization (BN) Layer in benefiting convergence and generalization. However, BN requires extra memory and float-point calculation. Moreover, BN would be inaccurate on micro-batch, as it depends on batch statistics. In this paper, we address these problems by simplifying BN regularization while keeping two fundamental impacts of BN layers, i.e., data decorrelation and adaptive learning rate. We propose a novel normalization method, named MimicNorm, to improve the convergence and efficiency in network training. MimicNorm consists of only two light operations, including modified weight mean operations (subtract mean values from weight parameter tensor) and one BN layer before loss function (last BN layer). We leverage the neural tangent kernel (NTK) theory to prove that our weight mean operation whitens activations and transits network into the chaotic regime like BN layer, and consequently, leads to an enhanced convergence. T
    
[^204]: 机械建模中数据一致反演的新颖灵活参数估计方法

    Novel and flexible parameter estimation methods for data-consistent inversion in mechanistic modeling. (arXiv:2009.08267v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2009.08267](http://arxiv.org/abs/2009.08267)

    本论文提出了一种新颖灵活的参数估计方法，用于机械建模中的数据一致反演。该方法解决了贝叶斯分析中无信息先验引入的偏差问题，并在随机逆问题框架下推断参数。使用拒绝采样、马尔科夫链蒙特卡洛和生成对抗网络等新方法解决了数据一致反演的限制，并通过约束优化和先验逆问题分析进一步优化了结果。

    

    物理系统的预测通常依赖于从模拟集合中获得的知识，例如生物科学中的细胞集合。为了定性和定量分析，这些集合使用参数化的机械模型（MM）进行模拟。基于贝叶斯推断和模型族方法是目前物理系统参数估计的两类主流方法。然而，在贝叶斯分析中，对MM参数使用无信息先验会引入不可取的偏差。在这里，我们提出了如何在随机逆问题（SIP）框架中推断参数，该框架也被称为数据一致反演，其中先验只关注由于MM不可逆造成的不确定性。为了演示，我们引入了基于拒绝采样、马尔科夫链蒙特卡洛和生成对抗网络（GANs）的新方法来解决SIP。此外，为了克服SIP的局限性，我们基于约束优化和预

    Predictions for physical systems often rely upon knowledge acquired from ensembles of entities, e.g., ensembles of cells in biological sciences. For qualitative and quantitative analysis, these ensembles are simulated with parametric families of mechanistic models (MM). Two classes of methodologies, based on Bayesian inference and Population of Models, currently prevail in parameter estimation for physical systems. However, in Bayesian analysis, uninformative priors for MM parameters introduce undesirable bias. Here, we propose how to infer parameters within the framework of stochastic inverse problems (SIP), also termed data-consistent inversion, wherein the prior targets only uncertainties that arise due to MM non-invertibility. To demonstrate, we introduce new methods to solve SIP based on rejection sampling, Markov chain Monte Carlo, and generative adversarial networks (GANs). In addition, to overcome limitations of SIP, we reformulate SIP based on constrained optimization and pres
    
[^205]: 通过自适应批大小改善SGD的收敛性

    Improving the convergence of SGD through adaptive batch sizes. (arXiv:1910.08222v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.08222](http://arxiv.org/abs/1910.08222)

    通过自适应批大小，本研究提出了一种改善SGD收敛性的方法，既减少了高方差梯度估计的问题，又保持了较高精度的梯度估计。

    

    小批量随机梯度下降（SGD）及其变种使用少量训练样本来近似目标函数的梯度，也就是批大小。小批量大小在每个模型更新时需要较少的计算量，但可能导致高方差的梯度估计，这对优化来说是一些挑战。相反，大批量需要更多计算量，但可能产生更高精度的梯度估计。本文提出了一种将批大小调整到模型训练损失的方法。对于各种函数类，我们证明了我们的方法对于模型更新来说需要与梯度下降相同数量的次数，同时对于梯度计算来说需要与SGD相同数量的次数。该方法需要在每个模型更新时计算整个数据集上的损失，但通过近似训练损失可以大大减少所需的计算量。我们提供了实验证明我们的方法需要更少的模型更新而不增加总计算量。

    Mini-batch stochastic gradient descent (SGD) and variants thereof approximate the objective function's gradient with a small number of training examples, aka the batch size. Small batch sizes require little computation for each model update but can yield high-variance gradient estimates, which poses some challenges for optimization. Conversely, large batches require more computation but can yield higher precision gradient estimates. This work presents a method to adapt the batch size to the model's training loss. For various function classes, we show that our method requires the same order of model updates as gradient descent while requiring the same order of gradient computations as SGD. This method requires evaluating the model's loss on the entire dataset every model update. However, the required computation is greatly reduced by approximating the training loss. We provide experiments that illustrate our methods require fewer model updates without increasing the total amount of comp
    
[^206]: 最优稀疏决策树

    Optimal Sparse Decision Trees. (arXiv:1904.12847v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1904.12847](http://arxiv.org/abs/1904.12847)

    这篇论文介绍了第一个针对二进制变量的最优决策树的实用算法，通过分析界限和现代系统技术的结合来解决决策树优化的困难，实验证明了其在可扩展性、速度和最优性证明方面的优势。

    

    决策树算法自从1980年代初以来就一直是可解释（透明）机器学习中最受欢迎的算法之一。然而，自从它们问世以来，困扰决策树算法的问题就是它们的非最优性，或者说缺乏接近最优的保证：决策树算法往往是贪婪的或者目光短浅的，有时会产生明显非最优的模型。决策树优化的困难既是一个理论上的障碍，也是一个实际上的障碍，即使是仔细的数学规划方法也无法高效地解决这些问题。本文提出了第一个针对二进制变量的最优决策树的实用算法。该算法通过分析界限减小搜索空间，并利用现代系统技术，包括数据结构和自定义位向量库。我们的实验证明了在可扩展性、速度和最优性证明方面的优势。代码可在https://github.com/xi获得。

    Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980's. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. Our experiments highlight advantages in scalability, speed, and proof of optimality. The code is available at https://github.com/xi
    

