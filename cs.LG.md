# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods.](http://arxiv.org/abs/2310.20380) | 本文提出了一种Dropout技术来限制策略优化方法中替代目标方差的增长，并将其应用于PPO算法中。实验结果表明，D-PPO算法相较于PPO算法在Atari 2600游戏上表现更好。 |
| [^2] | [Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm.](http://arxiv.org/abs/2310.20369) | 本文研究了分布式随机梯度上升算法（D-SGDA）在各种情况下的稳定性和泛化性能，并证明分布式结构不会破坏D-SGDA的稳定性和泛化性能，在某些情况下其泛化性能可以媲美原始的SGDA。 |
| [^3] | [A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs.](http://arxiv.org/abs/2310.20367) | 本文提出一种基于机器学习的框架，通过对住宅电力负荷轮廓进行聚类，增强需求响应计划。通过使用伦敦家庭的数据进行实证分析和多个评估指标，我们应用了四种聚类算法，并将问题重新定义为概率分类问题，利用可解释的人工智能（xAI）来增强解释性。 |
| [^4] | [Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?.](http://arxiv.org/abs/2310.20366) | 本文研究了网络级交通预测中是否需要更多循环检测器数据的问题，提出了一个不确定性感知的交通预测框架，结合交通流理论和图神经网络，使用证据学习来量化不确定性。 |
| [^5] | [CAFE: Conflict-Aware Feature-wise Explanations.](http://arxiv.org/abs/2310.20363) | CAFE是一种冲突感知的特征解释方法，通过解决现有方法的局限性，提供了对特征冲突的增强鲁棒性和更好的解释能力。 |
| [^6] | [Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory.](http://arxiv.org/abs/2310.20360) | 本书提供了对深度学习算法的数学介绍，包括不同的神经网络架构和优化算法，并涵盖了深度学习算法的理论方面。此外，还介绍了深度学习逼近偏微分方程的方法。希望对学生和科学家们有所帮助。 |
| [^7] | [Muscle volume quantification: guiding transformers with anatomical priors.](http://arxiv.org/abs/2310.20355) | 本研究提出了一种在三维磁共振图像中自动分割下肢18个肌肉的方法，以辅助形态测量分析。该方法基于混合架构，结合了卷积和视觉变压器模块，解决了肌肉组织无法分辨和轮廓难以检测的问题。 |
| [^8] | [Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand.](http://arxiv.org/abs/2310.20350) | 本文提出了一种结合形状完成和抓取预测的方法，实现了快速灵活的多指抓取。通过使用基于深度图像的形状完成模块和基于预测的抓取预测器，实现了在具有有限或无先验知识的情况下，对物体进行抓取的任务。 |
| [^9] | [Class Incremental Learning with Pre-trained Vision-Language Models.](http://arxiv.org/abs/2310.20348) | 本文提出了一种利用预训练视觉-语言模型进行类增量学习的方法，通过在CLIP模型中增加适配器层，并使用参数保留的方法，取得了最佳结果。 |
| [^10] | [GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object Detectors on LiDAR-Data.](http://arxiv.org/abs/2310.20319) | GACE是一种基于几何感知的方法，通过聚合几何线索和空间关系来改进黑盒3D物体检测器的置信度估计，从而在各种检测器中实现了持续的性能提升。 |
| [^11] | [Causal Interpretation of Self-Attention in Pre-Trained Transformers.](http://arxiv.org/abs/2310.20307) | 本研究提出了一种对自注意力进行因果解释的方法，并利用已有的预训练Transformer进行零样本因果发现。通过计算最深注意层中相应表示之间的偏相关，我们可以学习输入序列上的因果结构。该方法在两个任务中为Transformer的结果提供了因果解释。 |
| [^12] | [Verification of Neural Networks Local Differential Classification Privacy.](http://arxiv.org/abs/2310.20299) | 该论文提出了局部差分分类隐私（LDCP）的概念，扩展了局部鲁棒性的差分隐私设置，旨在验证神经网络对隐私的保护能力。作者提出了Sphynx算法，通过计算网络的抽象来验证LDCP，解决了传统算法中需要训练大量网络和逐个验证的问题。 |
| [^13] | [Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents.](http://arxiv.org/abs/2310.20287) | 这项研究提出一种基于重置的深度集合学习方法，以提高深度强化学习的样本效率和解决复位方法的局限性。实验结果表明，该方法在安全强化学习领域取得了良好的性能。 |
| [^14] | [Accelerating Generalized Linear Models by Trading off Computation for Uncertainty.](http://arxiv.org/abs/2310.20285) | 本论文提出了一种迭代方法，通过增加不确定性来降低计算量，并显著提高广义线性模型的训练速度。 |
| [^15] | [AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data.](http://arxiv.org/abs/2310.20280) | 这篇论文提出了AutoMixer，一个基于时间序列基础模型的自动混合模型，通过通道压缩预训练和微调工作流技术，有效解耦了BizITOps数据中有用和嘈杂的跨通道交互，提高了多变量时间序列预测的性能。 |
| [^16] | [Extracting Entities of Interest from Comparative Product Reviews.](http://arxiv.org/abs/2310.20274) | 本文提出了一种基于深度学习的方法，用于从比较性产品评论中提取产品比较信息，并通过实验证明其在这个任务中优于现有的语义角色标注框架。 |
| [^17] | [Advancing Bayesian Optimization via Learning Correlated Latent Space.](http://arxiv.org/abs/2310.20258) | 本文提出了一种通过学习相关的潜在空间来推进贝叶斯优化的方法。该方法引入了Lipschitz正则化、损失加权和信任区域重新协调，以减小在潜在空间和目标函数之间的差距，并在多个优化任务中展示了其有效性。 |
| [^18] | [Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior.](http://arxiv.org/abs/2310.20249) | 本文提出了一种基于姿势先验的跨领域动作重定向方法，通过从另一个具有不同骨架的角色的现有动作数据集中转移动作，为仅具有姿势数据的角色生成合理的动作。实验证明该方法在小样本或噪声数据集情况下表现鲁棒性。 |
| [^19] | [Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering.](http://arxiv.org/abs/2310.20224) | 本论文提出了一种新的张量狄利克雷过程多项式混合模型，利用图形结构和空间语义图对基于轨迹记录的乘客聚类进行了改进，能在一步中自动确定聚类数量，并保留了多维出行信息的分层结构。 |
| [^20] | [STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction.](http://arxiv.org/abs/2310.20223) | STDA-Meta是一种用于少样本交通预测的元学习框架，能够解决传统方法在数据稀缺的情况下性能较差的问题。 |
| [^21] | [A Systematic Review for Transformer-based Long-term Series Forecasting.](http://arxiv.org/abs/2310.20218) | 基于Transformer的长期系列预测的系统综述，介绍了Transformer架构及其改进、公开可用的数据集和评估指标、有效训练Transformer的最佳实践和技术，并提出了潜在研究方向。 |
| [^22] | [Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization.](http://arxiv.org/abs/2310.20215) | 本研究提出了一种面向LEO卫星网络的深度强化学习切换协议(DHO)，通过预测能力跳过测量报告阶段，简化切换过程并消除访问延迟，同时在网络条件下表现优越，展示了实际应用价值。 |
| [^23] | [Calibration by Distribution Matching: Trainable Kernel Calibration Metrics.](http://arxiv.org/abs/2310.20211) | 该论文提出了基于核的校准度量方法，统一和推广了分类和回归中常见的校准形式。这些度量可以产生可微的样本估计，易于纳入经验风险最小化，并通过定制校准度量来优化决策任务。 |
| [^24] | [General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History.](http://arxiv.org/abs/2310.20204) | 基于电子健康记录，我们提出了一种称为REMed的检索增强医学预测模型，通过无限评估临床事件并自动选择相关事件进行预测，消除了人工特征选择和观察窗口的限制，并在实验中表现出优异的效果。 |
| [^25] | [Importance Estimation with Random Gradient for Neural Network Pruning.](http://arxiv.org/abs/2310.20203) | 本文提出了一种使用随机梯度进行神经网络剪枝的重要性估计方法，该方法通过启发式推导出Taylor一阶近似方法。此外，通过在传播过程中使用随机梯度和归一化处理，可以避免对有标签样本的需求，并使得所有样本对重要性得分的贡献相似。在实验中，该方法相比于先前方法在不同数据集和架构上表现更好。 |
| [^26] | [FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems.](http://arxiv.org/abs/2310.20193) | FedRec+是一个集成框架，旨在增强隐私性和解决联邦推荐系统中的异质性问题。它利用特征相似性来生成伪项目的虚拟评分，减少噪声，并通过使用Wasserstein距离来估计异质性问题。 |
| [^27] | [Visible to Thermal image Translation for improving visual task in low light conditions.](http://arxiv.org/abs/2310.20190) | 本文提出了一种新的方法来改善低光条件下的视觉任务，利用可见光图像到热红外图像的翻译。通过使用生成网络和检测网络，实现了可见光图像到热红外图像的转换，并展示了该方法的可行性和效果。这对于安防和监控应用具有重要意义。 |
| [^28] | [Self-supervised Pre-training for Precipitation Post-processor.](http://arxiv.org/abs/2310.20187) | 该论文提出了一种基于深度学习的降水后处理方法，使用自监督预训练和转移学习来提高数值天气预报模型的准确性。实验结果表明该方法在区域降水校正方面表现优于其他方法。 |
| [^29] | [Learning to Discover Skills through Guidance.](http://arxiv.org/abs/2310.20178) | 提出了一种名为DISCO-DANCE的无监督技能发现算法，通过引导学习提高探索效果，并在具有挑战性的环境中优于其他方法。 |
| [^30] | [Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer.](http://arxiv.org/abs/2310.20172) | 提出了一种叫做CBS-GPT的生成预训练Transformer模型，用于紧凑二进制系统波形生成，在预测准确性上达到了较高的准确率，并且具有显著的解释性能。 |
| [^31] | [Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds.](http://arxiv.org/abs/2310.20168) | 通过利用变分自编码器（VAEs）的紧凑潜在表示，我们能够产生新颖且直观的可视化效果，展示了浅云模拟中液滴尺寸的组织和演化特征。对比不同气溶胶浓度的模拟，我们发现液滴谱的演化在不同的气溶胶水平下很相似，但速度不同，这为气溶胶与云的相互作用提供了新的认识。 |
| [^32] | [Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs.](http://arxiv.org/abs/2310.20145) | 本文介绍了一种新颖的robust Bayesian Optimization算法，AIRBO，它能够在任意输入不确定性下有效识别出表现一致良好的鲁棒最优解。 |
| [^33] | [EELBERT: Tiny Models through Dynamic Embeddings.](http://arxiv.org/abs/2310.20144) | EELBERT是一种通过动态嵌入实现微型模型的方法，具有最小的准确性回归和显著的模型尺寸缩小。最小的模型UNO-EELBERT在GLUE得分上与完全训练的BERT-tiny相差4%，并且体积只有其15倍之一（1.2MB）。 |
| [^34] | [Contrastive Difference Predictive Coding.](http://arxiv.org/abs/2310.20141) | 本文介绍了一种时间差异版本的对比预测编码，通过将不同时间序列数据的片段组合在一起，来减少学习预测未来事件所需的数据量。实验证明，与先前的方法相比，我们的方法在成功率上提高了2倍，并且对于随机环境有更好的适应能力。 |
| [^35] | [Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds.](http://arxiv.org/abs/2310.20102) | 通过构建"相邻假设"矩阵和引入样本条件的假设稳定性，本文提出了新的信息论泛化保证，改进了先前信息论界限，并解决了随机凸优化问题中信息论界限的局限性。 |
| [^36] | [Robust Learning for Smoothed Online Convex Optimization with Feedback Delay.](http://arxiv.org/abs/2310.20098) | 这项研究提出了一种鲁棒学习算法RCL，用于解决具有多步切换成本和反馈延迟的平滑在线凸优化问题。该算法将不可信的机器学习预测与可信的专家算法相结合，通过约束投影来提高机器学习预测的鲁棒性。研究结果证明了RCL能够在任何给定的专家算法下实现$(1+\lambda)$-竞争性，并通过鲁棒化感知的方式训练机器学习模型以提高平均性能。实验证明了RCL在电动交通的电池管理中的应用表现出了鲁棒性和平均性能的提升。 |
| [^37] | [Beyond U: Making Diffusion Models Faster & Lighter.](http://arxiv.org/abs/2310.20092) | 本文介绍了一种利用连续动力系统设计扩散模型的新型去噪网络，该网络具有更高的参数效率、快速收敛和更好的噪声稳健性。实验证明，与标准U-Net相比，我们的模型在参数和计算成本方面显著减少，并且在推理速度和质量解方面都取得了优于基准模型的结果。 |
| [^38] | [Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows.](http://arxiv.org/abs/2310.20090) | 本文研究了消除变分推断与Wasserstein梯度流之间的差距，证明了布雷斯-瓦瑟斯坦梯度流可以重新表示为欧氏梯度流，提出了路径导数梯度估计器和蒸馏过程来拓展该方法，同时可以适用于f-散度和非高斯变分族。 |
| [^39] | [Efficient Subgraph GNNs by Learning Effective Selection Policies.](http://arxiv.org/abs/2310.20082) | 本文通过学习有效的选择策略提高了子图图神经网络的效率，并且实验证明该方法优于现有的基准方法。 |
| [^40] | [Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance Dynamics in Tokamak Fusion Reactors.](http://arxiv.org/abs/2310.20079) | 本研究将神经常微分方程（Neural ODE）框架应用于预测聚变反应堆等离子体的耦合等离子体电流和内部感应动力学，以解决物理模型和数据驱动模型的瓶颈问题。 |
| [^41] | [Partial Tensorized Transformers for Natural Language Processing.](http://arxiv.org/abs/2310.20077) | 论文研究了在自然语言处理中应用部分张量化的变压器架构，通过对BERT和ViT等神经网络的嵌入层压缩和部分张量化，显著提高了模型的准确性，并打破了张量分解领域的新局面。 |
| [^42] | [Meek Separators and Their Applications in Targeted Causal Discovery.](http://arxiv.org/abs/2310.20075) | 本论文研究了Meek分离器及其在目标因果发现中的应用。通过引入Meek分离器，我们可以设计出高效的算法来寻找小规模的分离器，从而实现对目标因果发现问题的优化。 |
| [^43] | [Automatic Evaluation of Generative Models with Instruction Tuning.](http://arxiv.org/abs/2310.20072) | 本论文提出了一种基于指导调整的学习度量方法，通过对预训练语言模型进行微调来实现对生成模型的自动评估。实验结果表明，这种方法在各种评估任务上取得了良好的性能，并且多任务联合训练可以进一步提高性能。 |
| [^44] | [FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space.](http://arxiv.org/abs/2310.20071) | 本文提出了FOCAL框架，可以通过自监督训练从多模时间序列感知信号中提取全面的特征。它通过使每个模态都编码到一个因子化的潜空间中，同时突出共享特征和专属特征，从而有效处理感知模态之间的共享信息和专属信息。 |
| [^45] | [LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart.](http://arxiv.org/abs/2310.20065) | 本文提出了一种新的深度学习模型，通过两阶段形变和新型损失函数的应用，实现了自动生成心脏薄壁结构的计算机模型。该模型在准确性和网格质量方面表现出色，可以直接用于物理模拟，减少后处理的需求。 |
| [^46] | [A Scalable Training Strategy for Blind Multi-Distribution Noise Removal.](http://arxiv.org/abs/2310.20064) | 提出了一种使用自适应采样/主动学习策略来训练去噪网络的方法，解决了通用去噪网络在不同噪声分布下表现差的问题。 |
| [^47] | [Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation.](http://arxiv.org/abs/2310.20062) | 这篇论文介绍了一种去中心化、可扩展且保护隐私的合成数据生成系统，使真实数据的贡献者能够参与差分隐私合成数据生成，从而提供更好的隐私和统计保证，并在机器学习流程中更好地利用合成数据。 |
| [^48] | [AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces.](http://arxiv.org/abs/2310.20060) | AdaSub是一种使用低维子空间中的二阶信息进行随机优化的算法，通过选择搜索的子空间维度来管理计算开销和算法效率。初步数值结果显示，AdaSub在时间和迭代次数方面优于其他随机优化器。 |
| [^49] | [Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo.](http://arxiv.org/abs/2310.20053) | 本研究使用Hamiltonian Monte Carlo方法估计最优PAC-Bayes界限，研究了通过限制后验分布为因子化高斯分布在优化PAC-Bayes界限方面所失去的紧致度，并提出了三种方法来获得高概率界限。实验证明在MNIST数据集上存在显著的紧致度差距，高达5-6％。 |
| [^50] | [The Expressibility of Polynomial based Attention Scheme.](http://arxiv.org/abs/2310.20051) | 本文研究了基于多项式的注意力机制的表达能力，揭示了其与传统softmax的差异。 |
| [^51] | [SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics.](http://arxiv.org/abs/2310.20049) | 提出了一个名为SURF的基准测试，用于评估和比较基于图的学习流体模拟器的泛化能力。SURF包括各种数据集和具体的性能和泛化度量指标。通过深入研究两种最先进的模型，我们证明了SURF的适用性。 |
| [^52] | [Scaling Riemannian Diffusion Models.](http://arxiv.org/abs/2310.20030) | 本文提出了一种改进的黎曼扩散模型，通过重新审视近似方法和利用对称空间的性质，实现了高精度计算和在高维任务上的扩展能力。 |
| [^53] | [GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models.](http://arxiv.org/abs/2310.20025) | GOPlan是一个使用学习模型进行计划的目标条件下的离线强化学习方法，通过预训练先验策略和使用重新分析方法生成虚构轨迹，用以提高性能和处理有限数据预算和未见目标泛化的能力。 |
| [^54] | [Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach.](http://arxiv.org/abs/2310.20024) | 本论文提出了一种基于数据驱动的容错方法来预测自组织机器人网络的拓扑可恢复性。通过将问题转化为二分类问题，并使用贝叶斯高斯混合模型，该方法通过前故障和后故障的两个不同预测路径预测典型问题的解决方案。结果表明，与当前文献中最佳策略相比，该方法在解决拓扑可恢复性预测问题上取得了成功。 |
| [^55] | [Multiscale Feature Attribution for Outliers.](http://arxiv.org/abs/2310.20012) | 本论文提出了一种逆多尺度遮挡的特征归因方法，用于识别异常点，并且在Dark Energy Survey Instrument中的星系光谱异常点上取得了更易解释的结果。 |
| [^56] | [Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning.](http://arxiv.org/abs/2310.20007) | 本文在多种情境下证明了汤普森采样在强化学习中的贝叶斯遗憾界，并通过对信息比的精确分析提出了一个基于时间不均匀强化学习问题的上界估计。同时，本文找到了各种设置中具体的界限，并讨论了这些结果是第一个其类别或改进了最先进方法的情况。 |
| [^57] | [PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices.](http://arxiv.org/abs/2310.19991) | PolyThrottle是一种在边缘设备上进行节能神经网络推理的解决方案，通过优化设备上的硬件元素配置，可以达到高达36%的能量节省，并且能够快速收敛到近乎最优的设置。 |
| [^58] | [Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?.](http://arxiv.org/abs/2310.19990) | 本研究对学习的局部搜索启发式的限制进行了全面调查，结果表明，基于禁忌搜索的简单学习启发式超越了最先进的学习启发式方法。 |
| [^59] | [Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models.](http://arxiv.org/abs/2310.19986) | 本研究针对机器学习模型在图像分类中对弱势群体的表现不一致问题，提出了利用网络搜索和生成模型的方法。通过在具有代表性的子类上进行实验，证明了该方法在增强模型鲁棒性和减轻偏见方面的有效性。 |
| [^60] | [Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations.](http://arxiv.org/abs/2310.19978) | 本论文提出了一种通过改进Frank-Wolfe算法来训练差分隐私回归模型的方法，并在稀疏输入数据上有效。通过这种方法，可以显著减少算法的训练时间，并在实验中取得了显著的性能提升。 |
| [^61] | [Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy.](http://arxiv.org/abs/2310.19973) | 本文通过$f$-差分隐私方法改进了洗牌模型和DP-GD中随机初始化的隐私边界，折衷函数的闭式表达式优于$(\epsilon,\delta)$-DP的结果，并且随机初始化可以增强DP-GD的隐私性。 |
| [^62] | [Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records.](http://arxiv.org/abs/2310.19967) | 通过多模态机器学习从血液测试、半结构化和非结构化病历中学习，早期检测炎症性关节炎可以改善转诊，为及时治疗和预防病情恶化提供决策支持。 |
| [^63] | [ExPT: Synthetic Pretraining for Few-Shot Experimental Design.](http://arxiv.org/abs/2310.19961) | ExPT是一种用于少样本实验设计的合成预训练方法，通过将条件生成任务应用于少量带标签的样本和期望输出，生成最优的输入设计。这种方法在不依赖主动数据收集或大规模标记数据集的情况下，在现实场景中具有实用性。 |
| [^64] | [Topological Learning for Motion Data via Mixed Coordinates.](http://arxiv.org/abs/2310.19960) | 本文通过混合坐标和拓扑诱导的聚类在一种多输出高斯过程模型中引入了拓扑信息，以提取结构信息，并能在时间和运动序列分析中应用。 |
| [^65] | [PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning.](http://arxiv.org/abs/2310.19958) | 本文针对剪枝联邦学习中的隐私问题进行了调查，推导出了泄露信息的上限，并进行了理论验证和实验验证。 |
| [^66] | [Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges.](http://arxiv.org/abs/2310.19957) | 本文介绍了深度学习在时空大数据中的应用，讨论了机遇和挑战，并指出了一些未来研究方向。 |
| [^67] | [Conditional Unscented Autoencoders for Trajectory Prediction.](http://arxiv.org/abs/2310.19944) | 本文提出了使用条件非线性自动编码器(CVAE)进行轨迹预测的方法，通过利用变分自动编码器(VAE)中的非线性采样过程和其他改进，超越了现有技术，为自动驾驶领域的轨迹预测提供了更好的性能。 |
| [^68] | [The Acquisition of Physical Knowledge in Generative Neural Networks.](http://arxiv.org/abs/2310.19943) | 本文研究了生成式神经网络在物质知识获取中的应用，并发现其学习轨迹与儿童的发展轨迹不一致。 |
| [^69] | [Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications.](http://arxiv.org/abs/2310.19942) | 本研究提出了一种名为Split-NER的系统，通过将命名实体识别问题分成提取实体提及跨度和跨度分类两个子任务，然后利用问答模型解决这两个子任务，实现了高效和准确的命名实体识别。 |
| [^70] | [Lyapunov-Based Dropout Deep Neural Network (Lb-DDNN) Controller.](http://arxiv.org/abs/2310.19938) | 本文提出了一种基于Dropout的DNN自适应控制器，通过引入基于Lyapunov的实时权重调整定律和Dropout技术，实现了在线无监督学习，并取得了显著的跟踪误差改进和精度提升。 |
| [^71] | [Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?.](http://arxiv.org/abs/2310.19936) | 本文提出了一种针对当前最先进目标检测器的半监督方法，使用师生架构在少标注学习设置中避免了依赖敏感的后处理步骤，并在标注稀缺的情况下表现优异。 |
| [^72] | [Sim2Real for Environmental Neural Processes.](http://arxiv.org/abs/2310.19932) | "Sim2Real"提出了一种解决深度学习模型与真实环境观测数据稀疏性之间差距的方法，通过在再分析数据上进行预训练，然后在观测数据上进行微调，实现了对环境神经过程的模拟到真实转换。 |
| [^73] | [Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms.](http://arxiv.org/abs/2310.19927) | 研究者通过对基于模型的重新参数化策略梯度方法进行理论研究，发现在长期强化学习问题中可能会遇到优化困难，导致收敛速度较慢。他们提出了一种谱归一化方法来缓解梯度方差爆炸问题。 |
| [^74] | [Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.](http://arxiv.org/abs/2310.19923) | Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。 |
| [^75] | [Solving a Class of Cut-Generating Linear Programs via Machine Learning.](http://arxiv.org/abs/2310.19920) | 本文通过机器学习提出了一种新的框架，可以近似确定一类通过线性规划生成割平面问题的最优解，解决了在分支界限算法中运行CGLP的计算困难的问题。 |
| [^76] | [Meta-Learning Strategies through Value Maximization in Neural Networks.](http://arxiv.org/abs/2310.19919) | 本文理论上研究了在神经网络中的元学习最优策略，并提出了一个学习努力的框架，可以高效地优化控制信号，从而提升学习性能。 |
| [^77] | [Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records.](http://arxiv.org/abs/2310.19917) | 本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。 |
| [^78] | [GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models.](http://arxiv.org/abs/2310.19915) | 本文提出了一种名为GPCR-BERT的模型，使用蛋白质语言模型解释了G蛋白偶联受体(GPCR)的顺序设计。通过利用预训练的蛋白质模型和微调预测任务，我们揭示了结合口袋中残基之间的关系和一些保守的基序。 |
| [^79] | [Bayesian Simulation-based Inference for Cosmological Initial Conditions.](http://arxiv.org/abs/2310.19910) | 本论文提出了一种基于仿真推断的贝叶斯场重建算法，结合自回归建模，可以从观测数据中恢复宇宙初始条件。 |
| [^80] | [Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks.](http://arxiv.org/abs/2310.19909) | 本研究通过对多种预训练模型进行大规模比较，帮助从业者更好地选择骨干网络，从而提升计算机视觉系统的性能和研究进展的方向。 |
| [^81] | [Interpretable Prototype-based Graph Information Bottleneck.](http://arxiv.org/abs/2310.19906) | 这项工作提出了一种新颖的可解释的GNN框架，通过在信息瓶颈框架中将原型学习与输入图的关键子图相结合，为模型的解释能力和性能提供了改进。 |
| [^82] | [MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder.](http://arxiv.org/abs/2310.19898) | 提出了一种医学图像分割Transformer（MIST），利用Convolutional Attention Mixing（CAM）解码器解决了Transformer在多模态尺寸中捕捉像素局部上下文的局限性。 |
| [^83] | [Exploring Geometry of Blind Spots in Vision Models.](http://arxiv.org/abs/2310.19889) | 本研究探索了在视觉模型中存在的不敏感现象，并提出了一种通过研究网络的等置信度级别集合的几何形状和范围的技术。通过提出的级别集遍历算法，可以找到与给定源图像相似但属于相同等置信度级别集的输入图像。 |
| [^84] | [BTRec: BERT-Based Trajectory Recommendation for Personalized Tours.](http://arxiv.org/abs/2310.19886) | BTRec是一种基于BERT的轨迹推荐算法，它利用用户的人口统计信息和过去的POI访问信息，通过修改后的BERT语言模型生成个性化的POI行程预测，从而为旅游者提供有针对性的推荐行程。 |
| [^85] | [Learning quantum states and unitaries of bounded gate complexity.](http://arxiv.org/abs/2310.19882) | 本文证明了学习具有有界门复杂度的量子态和酉算符的样本复杂度与相应的门复杂度线性相关，并且在计算复杂度上存在指数关系，这一结果限制了量子机器学习模型的表达能力。 |
| [^86] | [Metric Flows with Neural Networks.](http://arxiv.org/abs/2310.19870) | 本论文开发了一种基于神经网络梯度下降的度量流理论，实现了在黎曼度量空间中的流动。其应用于数值Calabi-Yau度量，并探讨了特征学习的重要性。 |
| [^87] | [Posterior Sampling for Competitive RL: Function Approximation and Partial Observation.](http://arxiv.org/abs/2310.19861) | 本文研究了使用后验采样算法在竞争性强化学习中的应用。通过引入自对弈和对抗性广义艾略特系数，提出了用于探索-利用平衡的模型方法，并且成功处理了状态的部分可观测性。同时，提出了学习具有潜在部分可观测性的对抗性博弈模型的后验采样方法，并给出了低遗憾界限。 |
| [^88] | [Exact Recovery and Bregman Hard Clustering of Node-Attributed Stochastic Block Model.](http://arxiv.org/abs/2310.19854) | 该论文介绍了一种精确恢复节点属性随机块模型的聚类算法，该算法利用网络信息和节点属性信息交换，实现更可靠的网络信息需要更少可靠的属性信息的精确恢复。 |
| [^89] | [Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model.](http://arxiv.org/abs/2310.19849) | 本文提出了一种基于表示学习的方法来预测蛋白质相互作用中氨基酸突变的效应。该方法利用黎曼扩散模型学习侧链构象的生成过程，并可以给出蛋白质相互作用界面上突变的结构上下文表示。通过利用学到的表示，实现了在预测蛋白质相互作用的突变效应方面的最先进性能。 |
| [^90] | [Efficient Exploration in Continuous-time Model-based Reinforcement Learning.](http://arxiv.org/abs/2310.19848) | 连续时间模型驱动的强化学习算法使用非线性常微分方程（ODEs）表示连续时间动态，并通过使用乐观原则进行探索来降低遗憾，同时提出了一种自适应、数据相关的测量选择策略，具有更少的样本和次线性遗憾。 |
| [^91] | [Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction.](http://arxiv.org/abs/2310.19845) | 本文提出了一种修改的遗传算法，用于同时降低维度和优化超参数，在不平衡数据集上进行垃圾邮件预测。实验结果表明，该模型在几何平均和准确率上表现良好。 |
| [^92] | [Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach.](http://arxiv.org/abs/2310.19843) | 本研究提出了一个使用遗传算法和极限增强模型进行电话营销过程建模的方法，其中包括特征选择和成本敏感的分析方法。研究通过利用电话营销数据和社会经济指标对客户意愿进行建模，并构建出可解释的预测模型。 |
| [^93] | [Musical Form Generation.](http://arxiv.org/abs/2310.19842) | 本论文介绍了一种生成结构化、任意长音乐作品的方法，通过使用条件生成模型创建音乐片段，并利用大型语言模型来建议音乐的形式。 |
| [^94] | [An interpretable clustering approach to safety climate analysis: examining driver group distinction in safety climate perceptions.](http://arxiv.org/abs/2310.19841) | 该研究引入了可解释的聚类方法来分析安全氛围，通过聚类司机群体的安全氛围感知，可以更好地了解组织的员工特点，并制定更有效的干预措施。 |
| [^95] | [CrossEAI: Using Explainable AI to generate better bounding boxes for Chest X-ray images.](http://arxiv.org/abs/2310.19835) | 这篇论文利用可解释的人工智能方法生成胸部X射线图像的边界框，提供了对医学成像诊断的解释，并试图改进现有方法中边界框生成的问题。 |
| [^96] | [Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning.](http://arxiv.org/abs/2310.19831) | 本文提出了一种新的模型基于的贝叶斯方法，通过可解释的策略学习来理解决策，该方法具有透明度、适应部分可观测性和完全离线运行的特点。通过对阿尔茨海默病诊断问题的实验验证，展示了该方法作为审计和分析工具的潜力。 |
| [^97] | [FuXi-Extreme: Improving extreme rainfall and wind forecasts with diffusion model.](http://arxiv.org/abs/2310.19822) | FuXi-Extreme模型通过使用去噪扩散概率模型（DDPM）来恢复精细尺度的细节，提高了极端降雨和风暴预报的性能。 |
| [^98] | [A Risk-Averse Framework for Non-Stationary Stochastic Multi-Armed Bandits.](http://arxiv.org/abs/2310.19821) | 我们提出了一种适用于非平稳环境的自适应风险感知策略框架，通过结合各种风险度量和重启贝叶斯在线变点检测算法，解决了高波动性领域中简单奖励最大化方法的不可靠性问题。 |
| [^99] | [NetDistiller: Empowering Tiny Deep Learning via In-Situ Distillation.](http://arxiv.org/abs/2310.19820) | NetDistiller提出了一个框架，通过将微型神经网络作为权重共享教师模型的子网络，通过联合训练和不确定性-aware的蒸馏来提高微型神经网络的准确性。 |
| [^100] | [Machine Learning and Knowledge: Why Robustness Matters.](http://arxiv.org/abs/2310.19819) | 机器学习算法的鲁棒性对于信任和知识的形成至关重要，只有在正确特征下跨情景良好运行的算法才能提供可靠的知识。 |
| [^101] | [Benchmarking GPUs on SVBRDF Extractor Model.](http://arxiv.org/abs/2310.19816) | 本文针对操作更大输入图像（256x256）的神经网络模型，尝试区分不同GPU的性能。为了用户选择合适的GPU以实现特定任务的最佳性能提供了参考。 |
| [^102] | [Training binary neural networks without floating point precision.](http://arxiv.org/abs/2310.19815) | 本研究提出了两种解决方案，通过拓扑变化和策略训练，实现了高效训练的二进制神经网络，具有低延迟和低能耗的特点。 |
| [^103] | [Learning Gradient Fields for Scalable and Generalizable Irregular Packing.](http://arxiv.org/abs/2310.19814) | 本论文提出了一种基于机器学习的方法来解决不规则装箱问题。通过学习梯度场，该方法能够处理对象有效性约束和碰撞避免等挑战，并生成最优的装箱解决方案。 |
| [^104] | [Enhancing Genetic Improvement Mutations Using Large Language Models.](http://arxiv.org/abs/2310.19813) | 本文研究了利用大型语言模型作为遗传改进的变异操作符来提高搜索过程，发现使用LLM编辑的补丁通过单元测试的数量高达75％，但相比较标准编辑，LLMs找到的补丁较少多样化。尽管LLM增强的GI找到了许多改进的补丁，但最好的改进补丁是通过标准GI找到的。 |
| [^105] | [Brain decoding: toward real-time reconstruction of visual perception.](http://arxiv.org/abs/2310.19812) | 本研究提出了一种基于脑磁图（MEG）的脑解码方法，通过训练一个具有预训练嵌入、MEG模块和图像生成器的模型，在实时应用中实现了对视觉知觉的高时间分辨率解码，并在图像检索上取得了7倍的改进。 |
| [^106] | [A Historical Context for Data Streams.](http://arxiv.org/abs/2310.19811) | 本文回顾了数据流研究的历史背景，并将机器学习对数据流的常见假设放置在其历史背景中。 |
| [^107] | [Advantages of Machine Learning in Bus Transport Analysis.](http://arxiv.org/abs/2310.19810) | 本研究利用监督机器学习算法分析了影响德黑兰快速巴士系统准点性的因素，并构建了准确的模型来预测公交线路是否符合准时性标准。通过深入研究算法的决策过程，发现了影响公交线路有效性的关键特征。 |
| [^108] | [MgNO: Efficient Parameterization of Linear Operators via Multigrid.](http://arxiv.org/abs/2310.19809) | 本文提出了一个简洁的神经算子架构，通过多重网格结构有效参数化线性算子，实现了算子学习的数学严密性和实用性。 |
| [^109] | [SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning.](http://arxiv.org/abs/2310.19805) | 这篇论文提出了一种称为SERA的奖励增强框架，用于改善离线到在线强化学习中的探索能力。它通过设计内在奖励来鼓励agent进行探索，并实现更好的在线微调效果。 |
| [^110] | [A Kernel Perspective on Behavioural Metrics for Markov Decision Processes.](http://arxiv.org/abs/2310.19804) | 本文从核的角度论述了马尔科夫决策过程行为度量的新视角，并提出了一种新的度量与MICo距离等价。此外，核的视角还使我们能够提供新的理论结果，包括界定价值函数差异和嵌入到低失真误差的欧氏空间中。这些结果对于使用行为度量构建强化学习表示至关重要。同时，我们通过实证结果证明了这些方法的实际有效性。 |
| [^111] | [Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models.](http://arxiv.org/abs/2310.19802) | 本文将生成式机器学习问题视为参数概率模型的时间演化过程，通过研究模型参数与生成样本之间的热力学交换，发现模型通过耗散热量来学习，参数子系统充当热库存储学到的信息。这为超参数模型的泛化能力提供了有价值的热力学洞察。 |
| [^112] | [SyMPox: An Automated Monkeypox Detection System Based on Symptoms Using XGBoost.](http://arxiv.org/abs/2310.19801) | SyMPox是一个基于症状的自动猴痘检测系统，利用XGBoost算法分析症状模式并提供准确的猴痘诊断，为用户提供了一个用户友好的平台。 |
| [^113] | [From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces.](http://arxiv.org/abs/2310.19786) | 通过新的约化方法，我们改进了经典的Swap遗憾最小化算法，并提供了一个无外部遗憾算法的替代方法。对于学习专家建议问题，我们的算法可以在较少的轮数和更低的复杂度下达到相同的Swap遗憾限制。 |
| [^114] | [Combining Language Models For Specialized Domains: A Colorful Approach.](http://arxiv.org/abs/2310.19708) | 该论文提出了一种将领域特定语言模型与通用语言模型结合的新颖方法，通过对词语进行标记或“上色”来有效处理领域术语，显著降低了领域专用任务的错误率。 |
| [^115] | [Non-parametric regression for robot learning on manifolds.](http://arxiv.org/abs/2310.19561) | 本文提出了一种在机器人学习中针对流形值数据的非参数回归方法，通过在流形上操作概率分布参数来直接估计函数，以改善预测准确性和简化算法。 |
| [^116] | [Hodge-Compositional Edge Gaussian Processes.](http://arxiv.org/abs/2310.19450) | 本论文提出了一种新的方法用于对边缘集合上的函数进行建模，该方法基于Hodge分解开发了适用于不同应用场景的无散度和无旋度的高斯过程，并通过组合它们来表示任意边缘函数。实验结果表明这种方法在流动数据推断中具有潜在的实际应用价值。 |
| [^117] | [BERT Lost Patience Won't Be Robust to Adversarial Slowdown.](http://arxiv.org/abs/2310.19152) | 本文评估了多出口语言模型对抗恶意减速的稳健性, 发现复杂的机制更易受到恶意减速的攻击。此外，对抗训练无效，但对话模型的输入清洗是有效的。 |
| [^118] | [Behavior Alignment via Reward Function Optimization.](http://arxiv.org/abs/2310.19007) | 本论文介绍了一种通过优化奖励函数来实现行为一致性的新框架。该框架通过将设计师的启发和领域知识与环境的主要奖励相结合，自动确定了最有效的奖励结构，以避免引起不希望的行为。 |
| [^119] | [Kernel-based Joint Multiple Graph Learning and Clustering of Graph Signals.](http://arxiv.org/abs/2310.19005) | 在图信号处理中，我们提出了一种基于核的联合多图学习和图信号聚类算法，可以有效地将节点侧信息整合到信号的分区和图学习中。 |
| [^120] | [LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery.](http://arxiv.org/abs/2310.18356) | LoRAShear是一种高效的大型语言模型结构剪枝和知识恢复方法，通过逐步剪枝和动态微调，有效减少LLMs的占用空间并且保持性能。 |
| [^121] | [Data-Free Distillation Improves Efficiency and Privacy in Federated Thorax Disease Analysis.](http://arxiv.org/abs/2310.18346) | 我们提出了一种名为FedKDF的无数据蒸馏联邦学习方法，可以在保护隐私的前提下提高胸部疾病分析的效率和准确性。通过使用轻量级生成器聚合来自不同客户端的知识，FedKDF能够生成一个统一的预测器，并在其基础上进行进一步优化。 |
| [^122] | [AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models.](http://arxiv.org/abs/2310.18331) | AllTogether是一个标准化的提示模板，通过增强任务背景表示，提高了大型语言模型（LLMs）在基于HTML的Web导航中的性能。研究结果显示，像GPT-4这样的模型在这类任务中优于较小的模型，并且HTML代码片段的长度和历史轨迹对性能有显著影响。同时，在实时环境反馈方面，优于之前的逐步指导。这项工作为未来LLM驱动的Web代理研究提供了宝贵的见解。 |
| [^123] | [Causal Q-Aggregation for CATE Model Selection.](http://arxiv.org/abs/2310.16945) | 该论文提出了一种基于Q集成的CATE模型选择方法，其通过使用双重鲁棒损失实现了统计上的最佳预测模型选择遗憾率 |
| [^124] | [On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms.](http://arxiv.org/abs/2310.15848) | 这篇论文讨论了负责任的机器学习数据集的重要性，并提出了一个通过负责任评价标准来评估数据集的框架。 |
| [^125] | [Learning Interpretable Rules for Scalable Data Representation and Classification.](http://arxiv.org/abs/2310.14336) | 这项研究提出了一种名为RRL的新型分类器，通过自动学习可解释的非模糊规则，实现了数据表示和分类的良好可扩展性和解释性。 |
| [^126] | [Quality-Diversity through AI Feedback.](http://arxiv.org/abs/2310.13032) | 基于AI反馈的质量-多样性（QDAIF）算法利用语言模型来生成和评估创造性写作，比传统算法更广泛地覆盖高质量样本的搜索空间。 |
| [^127] | [Canonical normalizing flows for manifold learning.](http://arxiv.org/abs/2310.12743) | 该论文介绍了一种规范化正态流方法，用于流形学习。通过可学习的可逆变换将数据嵌入到高维空间中，从而实现了在流形上计算概率密度并优化网络参数的目标。然而，当前的方法在学习到的流形表示中存在着与流形关联且退化的内在基函数的问题。 |
| [^128] | [Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance.](http://arxiv.org/abs/2310.10385) | 该论文研究了零样本神经机器翻译性能变化的原因，发现目标语言的翻译质量、词汇重叠和语言特性是影响性能变化的关键因素。 |
| [^129] | [Federated Multi-Objective Learning.](http://arxiv.org/abs/2310.09866) | 本研究提出了一种新的联邦多目标学习（FMOL）框架，在满足多代理多任务学习应用的分布式性质和数据隐私需求的同时，支持不同客户端上的不同目标函数集合。通过引入联邦学习的范式，将多目标优化（MOO）推广到联邦学习领域。 |
| [^130] | [MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization.](http://arxiv.org/abs/2310.09833) | MIR2提出了一种针对鲁棒多智能体强化学习的方法，通过在常规情况下训练策略并最小化互信息作为鲁棒正则化，实现了在不准备每种可能的最坏情况的情况下提升鲁棒性的目标。 |
| [^131] | [Evolutionary Dynamic Optimization and Machine Learning.](http://arxiv.org/abs/2310.08748) | 进化计算和机器学习的结合为优化复杂的机器学习任务提供了有价值的机会，并通过利用进化计算算法生成的数据来提供对搜索空间和种群动态的洞察。 |
| [^132] | [Teaching Language Models to Hallucinate Less with Synthetic Tasks.](http://arxiv.org/abs/2310.06827) | 通过设计合成任务，我们的研究表明减少合成任务上的幻觉可以帮助减少现实世界的抽象概括任务上的幻觉。 |
| [^133] | [RetSeg: Retention-based Colorectal Polyps Segmentation Network.](http://arxiv.org/abs/2310.05446) | 本研究探索将保留机制整合到结直肠息肉分割中，解决了视觉变换器在资源受限设备上实时疾病检测中的内存和并行性挑战。 |
| [^134] | [BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph.](http://arxiv.org/abs/2310.03320) | BioBridge是一种通过知识图谱桥接单模态生物医学基础模型的参数高效学习框架。实验证明，BioBridge在跨模态检索任务中胜过最佳基线KG嵌入方法，具有泛化能力。 |
| [^135] | [Towards Robust Cardiac Segmentation using Graph Convolutional Networks.](http://arxiv.org/abs/2310.01210) | 这项研究使用了图卷积网络来实现心脏分割，通过预测轮廓点而不是标记每个像素，消除了心脏分割中的解剖学错误。同时还对图卷积网络进行了消融研究，并评估了临床测量指标的性能。 |
| [^136] | [DataDAM: Efficient Dataset Distillation with Attention Matching.](http://arxiv.org/abs/2310.00093) | 本研究提出了一种基于注意力匹配的高效数据集精炼(DataDAM)方法。通过匹配空间注意力来学习合成图像，从而实现了最新技术水平的性能，同时减少了训练成本。 |
| [^137] | [Reusability report: Prostate cancer stratification with diverse biologically-informed neural architectures.](http://arxiv.org/abs/2309.16645) | 该研究通过验证和重新实现作者提出的生物信息化神经网络P-NET的方法，量化了使用Reactome生物通路进行网络稀疏化的贡献，并探索了其他神经架构和方法。研究结果表明，不同结构的深度神经网络对个体患者进行了错误的预测。 |
| [^138] | [COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs.](http://arxiv.org/abs/2309.14356) | COCO-Counterfactuals是一个自动构建图像-文本对的反事实例的框架，通过使用文本到图像扩散模型来自动生成多模态反事实例。通过人工评估，我们验证了COCO-Counterfactuals的质量，并展示了其对于改善域外泛化能力的实用性。 |
| [^139] | [Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors.](http://arxiv.org/abs/2309.13135) | 该研究提出了一种使用深度学习和药动学先验预测治疗反应的方法。研究者通过一个新颖的编码器提供药物的药动学信息，从而实现对时间序列的精确预测。实验结果显示，在逼真模拟和真实世界数据上，该方法比基准模型的预测准确性提高了约11%和8%。这种方法在临床实践中具有多种有益应用，如发出早期警告和定量特定患者的治疗效果。 |
| [^140] | [Sharpness-Aware Minimization and the Edge of Stability.](http://arxiv.org/abs/2309.12488) | 本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。 |
| [^141] | [Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms.](http://arxiv.org/abs/2309.05961) | 本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。 |
| [^142] | [Emergent learning in physical systems as feedback-based aging in a glassy landscape.](http://arxiv.org/abs/2309.04382) | 通过训练线性物理网络学习线性变换，我们发现其学习行为与无序和玻璃状系统中的老化和记忆形成过程相似。学习动态类似于老化过程，通过重复施加反馈边界力来放松并编码输入-输出关系的记忆。 |
| [^143] | [Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity.](http://arxiv.org/abs/2309.04272) | 这项研究提出了改进样本复杂性的零和线性二次博弈，并发现了自然策略梯度方法的隐式正则化属性。在无模型参数知识的情况下，他们还提出了第一个多项式样本复杂性算法来达到Nash均衡。 |
| [^144] | [Reconstruction of Unstable Heavy Particles Using Deep Symmetry-Preserving Attention Networks.](http://arxiv.org/abs/2309.01886) | 使用对称保持注意力网络 (Spa-Net) 对不稳定的重粒子进行重建，并扩展其能力以处理多种输入物体类型和全局事件特征。在顶夸克对的半轻子衰变和与希格斯玻色子共同产生的顶夸克对背景下，我们发现了显著的性能改进。 |
| [^145] | [SGMM: Stochastic Approximation to Generalized Method of Moments.](http://arxiv.org/abs/2308.13564) | 我们提出了一种新的随机广义矩方法（SGMM），用于估计和推断矩限制模型。该方法具有快速和可扩展的实时处理能力，并且能够处理大规模和在线数据集。 |
| [^146] | [Beyond Document Page Classification: Design, Datasets, and Challenges.](http://arxiv.org/abs/2308.12896) | 本文强调了将文档分类基准测试更接近于现实世界应用的需求，通过提出多页文档分类数据集和不同分类任务，以及高效的多页文档表示，来解决现有基准测试不适用于实际完整文档评估的问题。 |
| [^147] | [Addressing Fairness and Explainability in Image Classification Using Optimal Transport.](http://arxiv.org/abs/2308.11090) | 本论文提出了使用最优输运理论解决图像分类中公平性和可解释性问题的综合方法，通过发现和解释模型中偏见区域的原因和影响来提供细粒度的解释。 |
| [^148] | [SE(3) Equivariant Augmented Coupling Flows.](http://arxiv.org/abs/2308.10364) | 本文提出了一种SE(3)等变增强耦合流，可以快速采样和密度评估，通过在坐标分割中保持等变性。 |
| [^149] | [Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics.](http://arxiv.org/abs/2308.05483) | 这项研究针对机器人抓取任务中的奖励稀疏性、行为稀疏性和行为空间错位等挑战，探讨了优质多样性方法解决该问题的能力。通过在多个实验领域中进行实验，结果表明... |
| [^150] | [A digital twin framework for civil engineering structures.](http://arxiv.org/abs/2308.01445) | 本研究提出了一个用于土木工程结构的预测数字孪生方法，它采用概率图模型编码资产孪生耦合动态系统，通过深度学习模型同化感测数据来提供实时的结构健康诊断，不断更新数字孪生状态并用于优化维护和管理规划。 |
| [^151] | [How is ChatGPT's behavior changing over time?.](http://arxiv.org/abs/2307.09009) | 本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。 |
| [^152] | [Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients.](http://arxiv.org/abs/2307.08507) | 本文设计了一种借鉴了多个文献的算法，通过使用镜像下降和共轭梯度的技术，能够高效准确地计算Wasserstein距离，并且在高维问题上比其他算法具有快速收敛的优势。 |
| [^153] | [Surgical Phase and Instrument Recognition: How to identify appropriate Dataset Splits.](http://arxiv.org/abs/2306.16879) | 本研究开发了一个基于Web的应用程序，用于手术工作流和器械识别的机器学习模型。通过可视化框架，能够评估手术工作流识别的数据集划分，特别是识别次优的划分。 |
| [^154] | [Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis.](http://arxiv.org/abs/2306.16803) | 本文提出了一种新的基于模型的信用分配算法，通过量化反事实查询来测量动作对未来奖励的影响。与现有方法不同的是，我们通过测量对奖励或奖励对象表示的贡献，获得了具有更低方差的梯度估计。 |
| [^155] | [Separable Physics-Informed Neural Networks.](http://arxiv.org/abs/2306.15969) | 这项研究提出了一种可分离的物理信息神经网络（SPINN），通过逐个处理轴来显著减少了多维 PDE 中的网络传播数量，并使用正向模式自动微分降低了计算成本，使得可以在单个普通 GPU 上使用大量的配点。 |
| [^156] | [Simplifying and Empowering Transformers for Large-Graph Representations.](http://arxiv.org/abs/2306.10759) | 本文通过实验证明，在大型图上使用一层注意力即可获得令人惊讶的竞争性能，挑战了在语言和视觉任务中复杂模型的应用。这促使我们重新思考在大型图上设计Transformer的理念，以提高可扩展性。 |
| [^157] | [PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning.](http://arxiv.org/abs/2306.10711) | PLASTIC算法通过改善模型的输入和标签可塑性，提高样本高效强化学习的效果。 |
| [^158] | [Score-based Data Assimilation.](http://arxiv.org/abs/2306.10574) | 本文介绍了基于评分的数据同化方法，通过对状态轨迹模型的训练，实现了无需依赖传统推断方法和满足高维系统与长时间跨度下进行推断。 |
| [^159] | [Stabilized Neural Differential Equations for Learning Constrained Dynamics.](http://arxiv.org/abs/2306.09739) | 本文提出了一种稳定神经微分方程（SNDEs）的方法，可以强制使用任意流形约束。该方法通过添加稳定项使约束流形成为渐进稳定的，并且在实验中表现优于现有方法。 |
| [^160] | [QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules.](http://arxiv.org/abs/2306.09549) | 该论文提出了一种新的量子哈密顿数据集QH9，用于为各种分子提供精确的哈密顿矩阵。通过设计基准任务，展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。 |
| [^161] | [Identification of Nonlinear Latent Hierarchical Models.](http://arxiv.org/abs/2306.07916) | 本文提出了一种方法，可以在观测变量由因果相关的潜变量生成的非线性潜变量层次因果模型中实现因果结构和潜变量的可识别性。 |
| [^162] | [On the Robustness of Removal-Based Feature Attributions.](http://arxiv.org/abs/2306.07462) | 本文研究了Removal-Based特征归因的鲁棒性，提供了全面的理论和实验分析，并证明了所提方法的实际有效性。 |
| [^163] | [TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models.](http://arxiv.org/abs/2306.06815) | 本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。 |
| [^164] | [Variational Imbalanced Regression.](http://arxiv.org/abs/2306.06599) | 本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。 |
| [^165] | [Differentially Private Image Classification by Learning Priors from Random Processes.](http://arxiv.org/abs/2306.06076) | 本文提出了一种名为DP-RandP的方法，从随机过程中学习先验知识，并将其传递给私有数据，以改进差分隐私的图像分类，实现了新的最先进的结果，提高了CIFAR-10的精度。 |
| [^166] | [Explaining Predictive Uncertainty with Information Theoretic Shapley Values.](http://arxiv.org/abs/2306.05724) | 本文提出了一种新的方法，通过Shapley值解释不确定性预测，可以量化每个特征对个别模型输出条件熵的贡献，适用于协变量转移检测、主动学习、特征选择和活动特征价值评估等方面。 |
| [^167] | [Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation.](http://arxiv.org/abs/2306.05584) | 本文提出了一种基于SE（3）等变结构和非监督训练策略的方法，可以实现刚性分割和运动估计，不需要类别信息且具有极高的模型效率。 |
| [^168] | [Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning.](http://arxiv.org/abs/2306.04746) | 该论文提出了一种新算法，使用大型语言模型（LLMs）输出进行下游统计分析，以实现有效的下游统计推断，并降低标签获取的研究成本80％，同时保证CSS研究的统计属性。 |
| [^169] | [Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models.](http://arxiv.org/abs/2306.04675) | 系统地研究了图像生成模型的评估，发现常见的评价指标如FID等不能很好地体现扩散模型的感知真实性，建议使用SwAV特征提取器结合FID进行评估。 |
| [^170] | [Get More for Less in Decentralized Learning Systems.](http://arxiv.org/abs/2306.04377) | 本文提出了一种名为JWINS的分布式学习系统，它仅通过稀疏化的方式共享部分模型参数，使用小波变换来补偿由稀疏化引起的信息损失，并通过随机通信截断来减少通信用量。实验证明，JWINS可以在发送更少的字节的情况下实现与完全共享分布式学习相似的准确性。 |
| [^171] | [Multimodal Fusion Interactions: A Study of Human and Automatic Quantification.](http://arxiv.org/abs/2306.04125) | 本文比较研究了两种人类注释者可以用于注释多模态交互的分类，并提出了一种基于信息分解的分类学。 |
| [^172] | [Variational Gaussian Process Diffusion Processes.](http://arxiv.org/abs/2306.02066) | 本文提出一种高斯变分过程参数化方法来更好地学习具有非线性扩散过程的潜在过程，此方法采用具有连续指数族描述的算法实现凸优化，可以代替缓慢的具有固定点迭代的算法。 |
| [^173] | [A Bayesian Perspective On Training Data Attribution.](http://arxiv.org/abs/2305.19765) | 本文介绍了一种TDA任务的贝叶斯视角，从中发现个别训练样本的影响常被噪声掩盖，TDA只能用于解释对模型预测影响稳定、独立于其他噪声因素的训练数据。 |
| [^174] | [Faith and Fate: Limits of Transformers on Compositionality.](http://arxiv.org/abs/2305.18654) | 研究了Transformer模型在三个代表性组合型任务中的表现，发现其通过线性子图匹配解决多步组合推理问题。 |
| [^175] | [Fine-Tuning Language Models with Just Forward Passes.](http://arxiv.org/abs/2305.17333) | 本论文提出了一种内存高效的零阶优化器，可以使用与推理相同的存储空间微调语言模型，其可以在大规模模型下更快地优化，具有更好的实验结果。 |
| [^176] | [DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models.](http://arxiv.org/abs/2305.16381) | 本论文提出了DPOK，一种使用在线强化学习（RL）微调文本到图像扩散模型的方法。该方法在COCO数据集上实现了最先进的性能。 |
| [^177] | [Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models.](http://arxiv.org/abs/2305.15618) | 该论文提出了一种统计降尺度的概率框架，能够在不使用成对数据的情况下，通过反偏置和概率扩散模型来恢复存在偏见样本的真实物理统计信息。 |
| [^178] | [Neural network reconstruction of cosmology using the Pantheon compilation.](http://arxiv.org/abs/2305.15499) | 本研究使用神经网络重构宇宙学和Pantheon编译中的Hubble图。通过扩展ReFANN算法以处理非高斯数据点和具有协方差矩阵的数据集，并与高斯过程的结果进行对比，还进行了零测试来验证宇宙学的协调模型的有效性。 |
| [^179] | [Replicable Reinforcement Learning.](http://arxiv.org/abs/2305.15284) | 本篇论文提供了可复制的强化学习算法，是控制问题的第一个正式的可复制性结果 |
| [^180] | [Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees.](http://arxiv.org/abs/2305.15249) | 提出了一种具有函数逼近和理论保证的决策感知演员-评论家算法，通过设计联合目标来解决演员和评论家之间的不匹配，并且无论策略和评论家参数化的选择如何，该算法都保证单调策略改进。 |
| [^181] | [How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench.](http://arxiv.org/abs/2305.14947) | 本研究通过对BIG-bench实验记录的研究，发现大型语言模型（LLM）的能力具有可预测性，并提出了寻找信息丰富的子集以最大程度恢复整个集合性能的问题。 |
| [^182] | [Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4.](http://arxiv.org/abs/2305.14928) | 本研究提出利用泛化、不确定性和最新的大型语言模型，寻求解决假新闻问题。实验证明GPT-4在多语言环境下表现优于之前的方法。研究还探讨了泛化和不确定性处理技术，并在其他语言模型、温度、提示、版本控制、可解释性和网络检索方面取得了实际见解和未来研究方向。此外，研究还发布了新颖的英法配对假新闻数据集LIAR-New，为信息真实性评估提供了可行性标签。 |
| [^183] | [Enabling Large Language Models to Generate Text with Citations.](http://arxiv.org/abs/2305.14627) | 本文提出ALCE，是首个自动LLMs引文评估基准，实现大型语言模型生成带引文的文本，提高其事实正确性和可验证性；提示LLMs特定的关键词或利用外部知识源可以显著提高其引文准确性。 |
| [^184] | [Uncertainty Quantification over Graph with Conformalized Graph Neural Networks.](http://arxiv.org/abs/2305.14535) | 本文提出了一种基于符合性的图神经网络模型（CF-GNN），通过将符合性预测（CP）扩展到基于图的模型中，对GNN不确定性进行了有效估计。CF-GNN生成的预测集/区间可根据预定义的覆盖概率保证包含真实标签，并且提供了一种减少预测集大小/区间长度的拓扑意识输出校正方法。 |
| [^185] | [Counterfactual Augmentation for Multimodal Learning Under Presentation Bias.](http://arxiv.org/abs/2305.14083) | 本文提出了一种用于纠正表示偏见的新颖方法，即反事实增强。实证评估表明，反事实增强相比于未修正的模型和现有的偏见校正方法，可以获得更好的下游性能。模型分析进一步指出，在理想情况下，生成的反事实与真实反事实密切相关。 |
| [^186] | [Flexible Grammar-Based Constrained Decoding for Language Models.](http://arxiv.org/abs/2305.13971) | 本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。 |
| [^187] | [Perception Test: A Diagnostic Benchmark for Multimodal Video Models.](http://arxiv.org/abs/2305.13786) | 该论文提出了一个名为“感知测试”的多模态视频基准测试，可以评估预训练模型的感知和推理能力，测试涵盖了记忆、抽象、物理、语义等技能和描述性、解释性、预测性、反事实性等推理类型。 |
| [^188] | [A Fractional Graph Laplacian Approach to Oversmoothing.](http://arxiv.org/abs/2305.13084) | 本文提出了一种针对过度平滑问题的分数图拉普拉斯方法，通过采用神经图ODE框架来描述非局部动力学，允许在远处节点之间传播信息，同时保持低概率的远距离跳跃，进而减轻过度平滑问题，并在合成和实际图上进行了广泛实验验证。 |
| [^189] | [A Scalable Neural Network for DSIC Affine Maximizer Auction Design.](http://arxiv.org/abs/2305.12162) | 该论文提出了一种可扩展的神经网络AMenuNet来构造AMAs参数和生成候选分配，解决了现有方法在占优策略激励兼容性和可扩展性方面的限制，其在协商一致的价值和社会残余价值方面优于强基线模型。 |
| [^190] | [ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings.](http://arxiv.org/abs/2305.11554) | 本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。 |
| [^191] | [Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks.](http://arxiv.org/abs/2305.06986) | 本文研究了三层神经网络的特征学习能力，相比之下，它具有比两层网络更丰富的可证的特征学习能力，并提出了一个通用定理，限制了目标结构的样本复杂度和宽度，以实现低测试误差。 |
| [^192] | [Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation.](http://arxiv.org/abs/2305.06563) | 本文提出了一种基于流形正则化Tucker分解的时空交通数据填充方法，该方法利用稀疏正则化项改善了Tucker核的稀疏性，并引入流形正则化和时间约束项来优化张量的填充性能。 |
| [^193] | [Causal Discovery from Subsampled Time Series with Proxy Variables.](http://arxiv.org/abs/2305.05276) | 本研究提出了一种使用代理变量方法的无模型算法，可以从子采样时间序列中无需参数约束地识别整个因果结构。 |
| [^194] | [When Do Neural Nets Outperform Boosted Trees on Tabular Data?.](http://arxiv.org/abs/2305.02997) | 这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。 |
| [^195] | [Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.](http://arxiv.org/abs/2305.01210) | 本论文提出了一个严格的代码综合基准评估框架EvalPlus，用于评估利用大型语言模型生成的代码的功能正确性。 |
| [^196] | [Learning to Reason and Memorize with Self-Notes.](http://arxiv.org/abs/2305.00833) | 该论文提出了一种学习使用自注记进行推理和记忆的方法，通过允许模型明确思考、记录自己的想法，并整合先前的推理步骤，从而提高了多步推理的能力。 |
| [^197] | [Learning Trajectories are Generalization Indicators.](http://arxiv.org/abs/2304.12579) | 本文研究了DNN的学习轨迹与其在优化后的泛化能力的联系，提出了一种基于学习轨迹复杂性和训练集偏置和多样性比率的新的泛化上界，并通过实验验证了其有效性。 |
| [^198] | [Electrical Impedance Tomography with Deep Calder\'on Method.](http://arxiv.org/abs/2304.09074) | Calderón方法是一种快速的EIT成像算法，但图像模糊且低估电导率值。该论文基于U-net模型对Calderón方法的图像进行后处理，提高了图像分辨率和电导率估计的准确性。 |
| [^199] | [Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model.](http://arxiv.org/abs/2304.06174) | 本文开发了一种物体感知 SE(3) 等变扩散模型，可以在几秒钟内精确地生成过渡态结构，与基于量子化学的优化相比，计算时间大大缩短，其生成的过渡态结构与真实结构的平均误差为 0.13 A 根均方差，可以实现反应速率估计所需的精度。 |
| [^200] | [Classification of Superstatistical Features in High Dimensions.](http://arxiv.org/abs/2304.02912) | 本文利用经验风险最小化的方法，对高维超统计特征下的数据进行分类，并分析了正则化和分布尺度参数对分类的影响。 |
| [^201] | [Applications of No-Collision Transportation Maps in Manifold Learning.](http://arxiv.org/abs/2304.00199) | 本文研究了在流形学习中应用无碰撞运输图的方法，其可以比OT图更便宜地计算距离，并提供单个概率测度的平移和伸缩的等距性。 |
| [^202] | [BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection.](http://arxiv.org/abs/2303.18138) | BERT4ETH是一个用于以太坊欺诈检测的预训练Transformer编码器，它通过三种实用有效策略来解决针对高度重复、偏斜分布和异构以太坊交易这些挑战，并且比现有技术更为优秀。 |
| [^203] | [Information Maximizing Curriculum: A Curriculum-Based Approach for Imitating Diverse Skills.](http://arxiv.org/abs/2303.15349) | 本文提出了一种信息最大化课程的模仿学习方法，通过为每个数据点分配权重，让模型专注于能够表示的数据，从而解决模平均问题。为了实现多样性行为，该方法采用了专家混合策略，并引入了最大熵约束。 |
| [^204] | [Time Series as Images: Vision Transformer for Irregularly Sampled Time Series.](http://arxiv.org/abs/2303.12799) | 本文提出了一种新颖的方法，将不规则采样的时间序列转换为线图像，并适应强大的视觉transformer进行时间序列分类。该方法简化了算法设计，具有通用性，并展示了在多个医疗和人体活动数据集上明显优于最先进的专业算法的表现。 |
| [^205] | [FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving Federated Learning System.](http://arxiv.org/abs/2303.10837) | FedML-HE是一种基于高效同态加密的实用联邦学习系统，它通过选择性加密敏感参数来显著减少计算和通信开销，并提供可定制的隐私保护。 |
| [^206] | [Object-Centric Slot Diffusion.](http://arxiv.org/abs/2303.10834) | 基于对象中心的槽扩散是一种将扩散模型整合到对象中心学习的新颖方法，它使用Latent Slot Diffusion (LSD)模型替换了传统的槽解码器，同时也是第一个能够无监督进行组合条件扩散的模型。 |
| [^207] | [Your representations are in the network: composable and parallel adaptation for large scale models.](http://arxiv.org/abs/2303.04105) | InCA是一种轻量级的迁移学习方法，可以在预训练模型的任何激活层进行交互式关注。与其他形式的适应方法相比，InCA的性能接近全面微调，但计算成本仅为基线的51%。 |
| [^208] | [GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces.](http://arxiv.org/abs/2303.01621) | 本文提出了一种新颖的GAN框架-GlucoSynth，采用差分隐私机制来生成合成血糖轨迹，保证数据隐私安全的同时能生成高质量的合成数据. |
| [^209] | [The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness in ReLU Networks.](http://arxiv.org/abs/2303.01456) | 这项研究研究了ReLU网络中梯度流的隐式偏差对泛化和对抗鲁棒性的影响，发现梯度流倾向于泛化能力强但对抗性高的解决方案，并且这种偏差还导致非鲁棒性解决方案的出现。 |
| [^210] | [Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels.](http://arxiv.org/abs/2302.10586) | 本文介绍了一种名为双伪训练（DPT）的训练策略，该策略结合了强大的半监督学习器和扩散模型来进一步推进半监督生成和分类任务。实验结果表明，DPT在各种情况下都能实现半监督生成和分类任务的SOTA性能，特别是在每个类别只有一个或两个标签的情况下，超过了其他一些模型。 |
| [^211] | [Machine Learning for Cutting Planes in Integer Programming: A Survey.](http://arxiv.org/abs/2302.09166) | 该论文调查了机器学习技术在混合整数线性规划中选择切割平面的最新研究，提出了使用数据进行优化切割选择的有前景的方法。 |
| [^212] | [Self-Supervised Temporal Graph learning with Temporal and Structural Intensity Alignment.](http://arxiv.org/abs/2302.07491) | 该论文提出了一种自监督的时间图学习方法，通过提取时间和结构信息来学习更具信息量的节点表示。 |
| [^213] | [Data pruning and neural scaling laws: fundamental limitations of score-based algorithms.](http://arxiv.org/abs/2302.06960) | 评分数据修剪算法在高压缩区域失败，通过随机化的校准协议可以提高现有修剪算法在该区域的性能。 |
| [^214] | [Message Passing Meets Graph Neural Networks: A New Paradigm for Massive MIMO Systems.](http://arxiv.org/abs/2302.06896) | 消息传递与图神经网络的结合提出了一个新的范式，用于大规模MIMO系统的设计，解决了现有算法在面对6G系统部署大量天线时的计算复杂度问题。 |
| [^215] | [Is Distance Matrix Enough for Geometric Deep Learning?.](http://arxiv.org/abs/2302.05743) | 本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。 |
| [^216] | [CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code.](http://arxiv.org/abs/2302.05527) | CodeBERTScore是一种用于代码生成的评估指标，通过对生成的代码以及其所给定的自然语言上下文进行编码，能够更可靠地评估代码生成模型的输出。 |
| [^217] | [A Multimodal Sensing Ring for Quantification of Scratch Intensity.](http://arxiv.org/abs/2302.03813) | 本文介绍了一种用于量化搔抓强度的多模态感应手环，通过加速度计和接触麦克风等设备的使用，以及机器学习算法的运用，实现了对搔抓强度的估计，并展示了其在临床相关鉴别上的成果。 |
| [^218] | [Self-Consistent Velocity Matching of Probability Flows.](http://arxiv.org/abs/2301.13737) | 我们提出了一种无网格的框架，用于求解守恒性偏微分方程，包括时间相关的Fokker-Planck方程和Wasserstein梯度流。通过自洽的速度匹配方法和迭代形式，我们的方法绕过了计算障碍，并在高维情况下表现出优越性能。 |
| [^219] | [Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions.](http://arxiv.org/abs/2301.11781) | 该论文研究了机器学习模型中的偶然性和认知性歧视，将其分类为数据分布中固有的歧视和模型开发过程中的决策导致的歧视。通过量化偶然性歧视的性能限制和刻画认知性歧视，揭示了公平干预的基本限制。研究还应用这种方法评估了现有的公平干预措施，并探究了在存在缺失值的数据中的公平风险。 |
| [^220] | [Joint Training of Deep Ensembles Fails Due to Learner Collusion.](http://arxiv.org/abs/2301.11323) | 深度神经网络的深度集成联合训练通常会导致基础学习者勾结，因此直接优化整个集成的损失很少被应用。 |
| [^221] | [Learning List-Level Domain-Invariant Representations for Ranking.](http://arxiv.org/abs/2212.10764) | 本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。 |
| [^222] | [A Vision-free Baseline for Multimodal Grammar Induction.](http://arxiv.org/abs/2212.10564) | 本论文研究了在多模式设置下，只使用文本进行训练的大型语言模型（LLMs）是否能够提供强大的辅助来进行语法归纳。结果显示，基于LLM的纯文本方法在多种多模式数据集上优于先前的方法，并且在性能、参数数量和训练速度方面取得了最先进的结果。 |
| [^223] | [Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases.](http://arxiv.org/abs/2212.02648) | 这个论文提出了一种使用排序数据来测量和减少模型对虚假线索的偏见的简单有效方法。通过排名图像的虚假性，可以识别出少数子群体，并通过准确率差来评估模型的偏见。此外，通过在虚假性较低的图像上微调模型，可以在几乎不损失准确率的情况下消除模型的偏见，实现对样本的公正处理。 |
| [^224] | [Classification by sparse additive models.](http://arxiv.org/abs/2212.01792) | 这篇论文研究了非参数的稀疏加性模型用于分类，通过对分量系数施加稀疏组Lasso和稀疏组Slope型惩罚来设计分类器，实验证明了分类器在未知稀疏性和平滑性上的自适应性能。 |
| [^225] | [Taming Reachability Analysis of DNN-Controlled Systems via Abstraction-Based Training.](http://arxiv.org/abs/2211.11127) | 本文提出了一种基于抽象的方法，用于绕过在DNN控制系统中对DNN进行过度近似的可达性分析问题。通过在传统的DNN中插入一个抽象层，将实数抽象化为一个区间进行训练，进而实现对DNN控制系统的黑盒可达性分析。 |
| [^226] | [Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel.](http://arxiv.org/abs/2210.09809) | 本研究通过理论方法分析了图神经网络中的卷积、非线性和深度对网络性能的影响，同时对基于图拉普拉斯和邻接矩阵的归一化方法进行了比较，并揭示了线性GNN与非线性ReLU-GNN性能相当的现象缺乏严格的理论解释。 |
| [^227] | [Measures of Information Reflect Memorization Patterns.](http://arxiv.org/abs/2210.09404) | 本研究通过信息论度量神经网络中神经元激活的多样性，发现其与模型的泛化能力和记忆之间存在关联。实验证明，即使在未标记的分布内部计算神经激活时，信息的组织也可以指向两种形式的记忆。 |
| [^228] | [Sparse PCA With Multiple Components.](http://arxiv.org/abs/2209.14790) | 本研究提出了一种新的方法来解决稀疏主成分分析问题，通过将正交性条件重新表述为秩约束，并同时对稀疏性和秩约束进行优化。我们设计了紧凑的半正定松弛来提供高质量的上界，当每个主成分的个体稀疏性被指定时，我们通过额外的二阶锥不等式加强上界。 |
| [^229] | [EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics.](http://arxiv.org/abs/2209.09593) | EffEval是一种对机器翻译评价指标效率进行全面评估的方法，其中TinyBERT在质量和效率之间提供了最佳平衡，CPU加速比GPU更显著，WMD近似没有提高效率但降低了质量，适配器提高了训练效率并在某些情况下提高了指标的质量。 |
| [^230] | [Double logistic regression approach to biased positive-unlabeled data.](http://arxiv.org/abs/2209.07787) | 该论文提出了一种基于双逻辑回归的方法，用于处理存在偏差的正向无标记数据。通过避免假设倾向得分函数为常数，作者共同估计后验概率和倾向得分函数，并提出了两种估计方法。实验结果显示，所提出的方法与现有的基于期望最大化方案的方法相比是可比较或更优的。 |
| [^231] | [Recovery Guarantees for Distributed-OMP.](http://arxiv.org/abs/2209.07230) | 该论文研究了基于正交匹配追踪的分布式方案在高维稀疏线性回归中的应用。通过适当的假设，分布式OMP方案能够以较低的信噪比下实现线性通信复杂度，并能与更复杂的方法相竞争。 |
| [^232] | [On the Optimization Landscape of Dynamic Output Feedback: A Case Study for Linear Quadratic Regulator.](http://arxiv.org/abs/2209.05042) | 本文研究了线性二次调节器中动态输出反馈策略的优化景观，推导了最优变换并证明了当其可观测时静止点的唯一性，从而为使用策略梯度方法解决动态控制器提供了最优性证明。 |
| [^233] | [Progressive Domain Adaptation with Contrastive Learning for Object Detection in the Satellite Imagery.](http://arxiv.org/abs/2209.02564) | 本文提出了一种在高分辨率卫星图像中改进对象检测的小物体检测管线，通过对特征提取过程的改进和对图像难度评分的应用，以及利用对比学习和渐进域自适应生成域不变的特征，有效提高了对象识别效果。 |
| [^234] | [Stronger Privacy Amplification by Shuffling for R\'enyi and Approximate Differential Privacy.](http://arxiv.org/abs/2208.04591) | 本研究在R\'enyi和近似差分隐私中通过洗牌提出了更强隐私放大的方法，并对理论和数值进行了改进和优化。 |
| [^235] | [Learning to Order for Inventory Systems with Lost Sales and Uncertain Supplies.](http://arxiv.org/abs/2207.04550) | 本论文提出了一种计算有效的在线学习算法，用于解决在计划时间内无法处理的库存控制问题。该算法实现了 $O(L+\sqrt{T})$ 的后悔。 |
| [^236] | [Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization.](http://arxiv.org/abs/2207.02016) | 该论文提出了一种新的正则化器USR，通过构建转换函数参数空间上的不确定性集合来提高连续控制任务中的强化学习性能。通过对值函数进行对抗生成未知不确定性集合，进一步增强了USR的灵活性。在真实世界强化学习基准测试中得到了改进的结果。 |
| [^237] | [Universum-inspired Supervised Contrastive Learning.](http://arxiv.org/abs/2204.10695) | Mixup is a popular data augmentation method that synthesizes extra samples. This paper explores the potential of Mixup to generate in-domain samples as universum negatives in supervised contrastive learning, providing high-quality hard negatives and reducing the need for large batch sizes. The proposed UniCon incorporates Mixup strategy to generate Mixup-induced universum as negatives. |
| [^238] | [DAMNETS: A Deep Autoregressive Model for Generating Markovian Network Time Series.](http://arxiv.org/abs/2203.15009) | DAMNETS是一种用于生成马尔可夫网络时间序列的深度自回归模型，通过在真实数据和合成数据集上的表现超过竞争方法，达到了设计灵活且可扩展的生成模型的目标。 |
| [^239] | [Label Hierarchy Transition: Delving into Class Hierarchies to Enhance Deep Classifiers.](http://arxiv.org/abs/2112.02353) | 本文提出了Label Hierarchy Transition (LHT)框架，基于深度学习，用于改进层次分类。LHT框架主要包括转换网络和混淆损失两个部分，通过显式学习标签层次转换矩阵和鼓励分类网络处理混淆情况，有效地利用类层次结构的相关性。 |
| [^240] | [Pharmacoprint -- a combination of pharmacophore fingerprint and artificial intelligence as a tool for computer-aided drug design.](http://arxiv.org/abs/2110.01339) | 提出了一种将药效固指纹和人工智能结合的新方法 Pharmacoprint，通过编码分子的药效固特征，提供了高分辨率的药物设计工具，优于其他常用的分子指纹方法。 |
| [^241] | [A general class of surrogate functions for stable and efficient reinforcement learning.](http://arxiv.org/abs/2108.05828) | 本研究提出了一个基于函数镜像上升的普适框架(FMA-PG)，构建了一系列替代函数，这些函数可以实现策略改进，并且不受策略参数化选择的影响。通过实验证实，该方法具有良好的性能和理论保证。 |
| [^242] | [Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent.](http://arxiv.org/abs/2106.08502) | 本文研究了用于计算高斯分布相对于最优输运度量的重心的算法，在Bures-Wasserstein流形上证明了新的测地凸性结果，提供了更强的迭代控制，实现了无维收敛率，同时还提供了用于这些问题的Riemannian GD 的第一个收敛保证。 |
| [^243] | [Helmholtzian Eigenmap: Topological feature discovery & edge flow learning from point cloud data.](http://arxiv.org/abs/2103.07626) | 本论文提出了一种从点云数据中估计流形Helmholtzian的方法，通过加权1-Laplacian构建了图Helmholtzian作为连续算子的一致估计器，并利用Helmholtz-Hodge定理对流和向量场进行分析。通过该方法，可以对流进行平滑、预测和特征提取。 |
| [^244] | [Synthetic Interventions.](http://arxiv.org/abs/2006.07691) | 提出了一个称为合成干预的因果框架，能够在观察到每个单元最多两个干预措施的情况下推断每个单元对每个干预措施的预期潜在结果，具有有限样本一致性和渐近正态性。 |
| [^245] | [Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning.](http://arxiv.org/abs/2005.13625) | 本研究重访了多智能体深度强化学习中的参数共享方法。我们通过引入智能体指示信号实现了在不同策略网络共享参数的同时学习不同策略或任务的能力，并且证明了这些方法在异构观测和行动空间学习中可以收敛到最优策略。 |
| [^246] | [Open-set learning with augmented category by exploiting unlabeled data (Open-LACU).](http://arxiv.org/abs/2002.01368) | Open-LACU是一种新的开放式学习策略，它可以将分类器推广到观察到的和未观察到的新颖类别之间，并通过定义不同的背景和未知类别来提高训练成本效益性，确保在存在未观察到的新颖类别时进行安全分类。 |
| [^247] | [Weighted bandits or: How bandits learn distorted values that are not expected.](http://arxiv.org/abs/1611.10283) | 本论文研究了带有扭曲概率的随机多臂赌博机问题，并提出了以UCB算法为基础、考虑了奖励扭曲并具有次线性后悔的算法。 |

# 详细

[^1]: 强化学习中的Dropout策略：限制策略优化方法中替代目标方差的增长

    Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods. (arXiv:2310.20380v1 [cs.LG])

    [http://arxiv.org/abs/2310.20380](http://arxiv.org/abs/2310.20380)

    本文提出了一种Dropout技术来限制策略优化方法中替代目标方差的增长，并将其应用于PPO算法中。实验结果表明，D-PPO算法相较于PPO算法在Atari 2600游戏上表现更好。

    

    基于策略的强化学习算法在各个领域中被广泛应用。其中，主流的策略优化算法如PPO和TRPO引入了重要性采样到强化学习中，这允许重用历史数据。然而，这也导致了替代目标方差的增加，间接影响了算法的稳定性和收敛性。本文首先推导出了替代目标方差的上界，它可以随替代目标的增加而呈二次增长。接下来，我们提出了一种Dropout技术，以避免重要性采样引起的替代目标方差过度增加。然后，我们引入了一个通用的强化学习框架，适用于主流的策略优化方法，并将Dropout技术应用于PPO算法，得到了D-PPO变体。最后，我们在Atari 2600游戏上对D-PPO和PPO算法进行了比较实验。

    Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as PPO and TRPO introduce importance sampling into reinforcement learning, which allows the reuse of historical data. However, this also results in high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the variance of the surrogate objective, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 
    
[^2]: 分布式随机梯度上升算法的稳定性和泛化性能研究

    Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm. (arXiv:2310.20369v1 [cs.LG])

    [http://arxiv.org/abs/2310.20369](http://arxiv.org/abs/2310.20369)

    本文研究了分布式随机梯度上升算法（D-SGDA）在各种情况下的稳定性和泛化性能，并证明分布式结构不会破坏D-SGDA的稳定性和泛化性能，在某些情况下其泛化性能可以媲美原始的SGDA。

    

    可用数据的规模不断增长，对于解决分布式机器学习任务中的极小极大问题，分布式方式引起了越来越多的关注。以前的理论研究主要集中在分布式极小极大算法的收敛速度和通信复杂度，对其泛化性能关注较少。本文通过算法稳定性的方法，研究了分布式随机梯度上升算法（D-SGDA）在凸凹和非凸非凹情况下的原始-对偶泛化界。我们的理论对分布式稳定性进行了精细化探究，并表明分布式结构并不会破坏D-SGDA的稳定性和泛化性能，在某些情况下其泛化性能可以媲美原始的SGDA。我们的结果分析了不同拓扑结构对D-SGDA算法的泛化界的影响，超越了一些微不足道的因素。

    The growing size of available data has attracted increasing interest in solving minimax problems in a decentralized manner for various machine learning tasks. Previous theoretical research has primarily focused on the convergence rate and communication complexity of decentralized minimax algorithms, with little attention given to their generalization. In this paper, we investigate the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm using the approach of algorithmic stability under both convex-concave and nonconvex-nonconcave settings. Our theory refines the algorithmic stability in a decentralized manner and demonstrates that the decentralized structure does not destroy the stability and generalization of D-SGDA, implying that it can generalize as well as the vanilla SGDA in certain situations. Our results analyze the impact of different topologies on the generalization bound of the D-SGDA algorithm beyond trivial factors such
    
[^3]: 基于机器学习的框架用于对住宅电力负荷轮廓进行聚类以增强需求响应计划

    A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs. (arXiv:2310.20367v1 [cs.LG])

    [http://arxiv.org/abs/2310.20367](http://arxiv.org/abs/2310.20367)

    本文提出一种基于机器学习的框架，通过对住宅电力负荷轮廓进行聚类，增强需求响应计划。通过使用伦敦家庭的数据进行实证分析和多个评估指标，我们应用了四种聚类算法，并将问题重新定义为概率分类问题，利用可解释的人工智能（xAI）来增强解释性。

    

    智能电表数据衍生出的负荷曲线经常被用于分析日常能源消耗模式，特别是在需求响应（DR）等应用领域。然而，这个工作最重要的挑战之一在于识别具有类似消耗行为的最合适的消费者群体。本文提出了一种新颖的基于机器学习的框架，通过一个实际案例研究，利用伦敦近5000户家庭的数据实现最佳负荷轮廓。具体应用了四种广泛使用的聚类算法，包括K-means、K-medoids、层次凝聚聚类和基于密度的空间聚类。还利用经验分析和多个评估指标来评估这些算法。之后，我们将问题重新定义为概率分类问题，分类器模拟聚类算法的行为，利用可解释的人工智能（xAI）来增强解释性。

    Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm,leveraging Explainable AI (xAI) to enhance the interpre
    
[^4]: 从循环检测器数据集中提取信息本质：网络级交通预测是否需要更多数据？

    Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?. (arXiv:2310.20366v1 [cs.LG])

    [http://arxiv.org/abs/2310.20366](http://arxiv.org/abs/2310.20366)

    本文研究了网络级交通预测中是否需要更多循环检测器数据的问题，提出了一个不确定性感知的交通预测框架，结合交通流理论和图神经网络，使用证据学习来量化不确定性。

    

    几十年来，网络级交通条件预测一直受到密切关注。尽管随着深度学习模型的出现和交通数据的不断扩大，预测准确性不断提高，但交通预测在实践中仍面临许多挑战。这些挑战包括数据驱动模型的鲁棒性、交通动态的固有不可预测性以及进一步改进交通预测是否需要更多传感器数据。本文针对后者的问题，特别关注循环检测器数据。为了回答这个问题，我们提出了一个不确定性感知的交通预测框架，探索循环数据样本在训练预测模型时的真正有效性。首先，模型设计将交通流理论与图神经网络相结合，确保预测和不确定性量化的鲁棒性。其次，使用证据学习来量化一次传递中不同来源的不确定性。

    Network-level traffic condition forecasting has been intensively studied for decades. Although prediction accuracy has been continuously improved with emerging deep learning models and ever-expanding traffic data, traffic forecasting still faces many challenges in practice. These challenges include the robustness of data-driven models, the inherent unpredictability of traffic dynamics, and whether further improvement of traffic forecasting requires more sensor data. In this paper, we focus on this latter question and particularly on data from loop detectors. To answer this, we propose an uncertainty-aware traffic forecasting framework to explore how many samples of loop data are truly effective for training forecasting models. Firstly, the model design combines traffic flow theory with graph neural networks, ensuring the robustness of prediction and uncertainty quantification. Secondly, evidential learning is employed to quantify different sources of uncertainty in a single pass. The e
    
[^5]: CAFE: 冲突感知的特征解释

    CAFE: Conflict-Aware Feature-wise Explanations. (arXiv:2310.20363v1 [cs.LG])

    [http://arxiv.org/abs/2310.20363](http://arxiv.org/abs/2310.20363)

    CAFE是一种冲突感知的特征解释方法，通过解决现有方法的局限性，提供了对特征冲突的增强鲁棒性和更好的解释能力。

    

    特征归因方法被广泛用于解释神经模型，通过确定单个输入特征对模型输出的影响来解释模型。我们提出了一种新颖的特征归因方法，CAFE（冲突感知的特征解释），它解决了现有方法的三个局限性：对冲突特征影响的忽视，对偏差项影响的缺乏考虑，以及对激活函数局部变化过于敏感。与其他方法不同，CAFE提供了防止高估神经元输入影响的保护机制，并单独追踪输入特征和偏差的正向和负向影响，从而增强了稳健性和发现特征冲突的能力。实验证明，CAFE在合成表格数据上能更好地识别冲突特征，并在几个真实世界的表格数据集上展现了最佳的全面可信度，而计算性能也很高。

    Feature attribution methods are widely used to explain neural models by determining the influence of individual input features on the models' outputs. We propose a novel feature attribution method, CAFE (Conflict-Aware Feature-wise Explanations), that addresses three limitations of the existing methods: their disregard for the impact of conflicting features, their lack of consideration for the influence of bias terms, and an overly high sensitivity to local variations in the underpinning activation functions. Unlike other methods, CAFE provides safeguards against overestimating the effects of neuron inputs and separately traces positive and negative influences of input features and biases, resulting in enhanced robustness and increased ability to surface feature conflicts. We show experimentally that CAFE is better able to identify conflicting features on synthetic tabular data and exhibits the best overall fidelity on several real-world tabular datasets, while being highly computation
    
[^6]: 深度学习的数学介绍：方法、实现和理论

    Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory. (arXiv:2310.20360v1 [cs.LG])

    [http://arxiv.org/abs/2310.20360](http://arxiv.org/abs/2310.20360)

    本书提供了对深度学习算法的数学介绍，包括不同的神经网络架构和优化算法，并涵盖了深度学习算法的理论方面。此外，还介绍了深度学习逼近偏微分方程的方法。希望对学生和科学家们有所帮助。

    

    本书旨在介绍深度学习算法的主题。我们详细介绍了深度学习算法的基本组成部分，包括不同的人工神经网络架构（如全连接前馈神经网络、卷积神经网络、循环神经网络、残差神经网络和带有批归一化的神经网络）以及不同的优化算法（如基本的随机梯度下降法、加速方法和自适应方法）。我们还涵盖了深度学习算法的几个理论方面，如人工神经网络的逼近能力（包括神经网络的微积分）、优化理论（包括Kurdyka-Lojasiewicz不等式）和泛化误差。在本书的最后一部分，我们还回顾了一些用于偏微分方程的深度学习逼近方法，包括物理信息神经网络（PINNs）和深度Galerkin方法。希望本书能对学生和科学家们有所帮助。

    This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet 
    
[^7]: 肌肉体积定量化：基于解剖先验指导的变压器

    Muscle volume quantification: guiding transformers with anatomical priors. (arXiv:2310.20355v1 [cs.CV])

    [http://arxiv.org/abs/2310.20355](http://arxiv.org/abs/2310.20355)

    本研究提出了一种在三维磁共振图像中自动分割下肢18个肌肉的方法，以辅助形态测量分析。该方法基于混合架构，结合了卷积和视觉变压器模块，解决了肌肉组织无法分辨和轮廓难以检测的问题。

    

    肌肉体积是一种有用的定量生物标志物，不仅用于运动领域，还用于退行性肌肉骨骼疾病的随访。除了体积外，通过从医学图像中分割感兴趣的肌肉，还可以提取其他形状标志物。尽管非常耗时，但在这类测量中，手工分割仍然是金标准。我们提出了一种方法来自动分割三维磁共振图像中下肢的18个肌肉，以辅助形态测量分析。由于在MR图像中观察时，不同肌肉的组织无法分辨。因此，肌肉分割算法不能依赖外观，只能依赖轮廓线索。然而，这样的轮廓很难检测，并且其厚度在受试者之间变化。为了应对以上挑战，我们基于混合架构提出了一种分割方法，结合了卷积和视觉变压器模块。我们首次研究了这种混合架构的行为特点。

    Muscle volume is a useful quantitative biomarker in sports, but also for the follow-up of degenerative musculo-skelletal diseases. In addition to volume, other shape biomarkers can be extracted by segmenting the muscles of interest from medical images. Manual segmentation is still today the gold standard for such measurements despite being very time-consuming. We propose a method for automatic segmentation of 18 muscles of the lower limb on 3D Magnetic Resonance Images to assist such morphometric analysis. By their nature, the tissue of different muscles is undistinguishable when observed in MR Images. Thus, muscle segmentation algorithms cannot rely on appearance but only on contour cues. However, such contours are hard to detect and their thickness varies across subjects. To cope with the above challenges, we propose a segmentation approach based on a hybrid architecture, combining convolutional and visual transformer blocks. We investigate for the first time the behaviour of such hy
    
[^8]: 将形状完成和抓取预测结合，实现快速灵活的多指抓取

    Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand. (arXiv:2310.20350v1 [cs.RO])

    [http://arxiv.org/abs/2310.20350](http://arxiv.org/abs/2310.20350)

    本文提出了一种结合形状完成和抓取预测的方法，实现了快速灵活的多指抓取。通过使用基于深度图像的形状完成模块和基于预测的抓取预测器，实现了在具有有限或无先验知识的情况下，对物体进行抓取的任务。

    

    在辅助机器人中，对于具有有限或无先验知识的物体进行抓取是一项非常重要的技能。然而，在这种普适情况下，尤其是在观测能力有限和利用多指手进行灵活抓取时，仍然存在一个开放的问题。我们提出了一种新颖、快速和高保真度的深度学习流程，由基于单个深度图像的形状完成模块和基于预测的物体形状的抓取预测器组成。形状完成网络基于VQDIF，在任意查询点上预测空间占用值。作为抓取预测器，我们使用了两阶段架构，首先使用自回归模型生成手姿势，然后回归每个姿势的手指关节配置。关键因素是足够的数据真实性和增强，以及在训练过程中对困难情况的特殊关注。在物理机器人平台上进行的实验表明，成功地实现了抓取。

    Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful gras
    
[^9]: 使用预训练视觉-语言模型的类增量学习

    Class Incremental Learning with Pre-trained Vision-Language Models. (arXiv:2310.20348v1 [cs.CV])

    [http://arxiv.org/abs/2310.20348](http://arxiv.org/abs/2310.20348)

    本文提出了一种利用预训练视觉-语言模型进行类增量学习的方法，通过在CLIP模型中增加适配器层，并使用参数保留的方法，取得了最佳结果。

    

    随着大规模预训练模型的出现，人们对将其适应和利用于连续学习场景的兴趣不断增加。本文提出了一种利用预训练视觉-语言模型（如CLIP）的方法，使其能够进一步适应新任务，而不仅仅是使用零样本学习。我们在经过预训练的CLIP模型中增加了额外的层，可以在图像编码器之后或文本编码器之前进行操作。我们研究了三种不同的策略：线性适配器、自注意力适配器，分别作用于图像嵌入，并且使用Prompt Tuning修改CLIP文本编码器的输入。我们还提出了一种参数保留的适配器层方法，使用参数重要性的度量来更好地保持增量学习过程中的稳定性和可塑性。我们的实验证明，最简单的解决方案--只有一个线性适配器层且具有参数保留--可以产生最好的结果。

    With the advent of large-scale pre-trained models, interest in adapting and exploiting them for continual learning scenarios has grown.  In this paper, we propose an approach to exploiting pre-trained vision-language models (e.g. CLIP) that enables further adaptation instead of only using zero-shot learning of new tasks. We augment a pre-trained CLIP model with additional layers after the Image Encoder or before the Text Encoder. We investigate three different strategies: a Linear Adapter, a Self-attention Adapter, each operating on the image embedding, and Prompt Tuning which instead modifies prompts input to the CLIP text encoder. We also propose a method for parameter retention in the adapter layers that uses a measure of parameter importance to better maintain stability and plasticity during incremental learning. Our experiments demonstrate that the simplest solution -- a single Linear Adapter layer with parameter retention -- produces the best results. Experiments on several conve
    
[^10]: GACE: 基于几何感知的LiDAR数据黑盒3D物体检测器的置信度增强

    GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object Detectors on LiDAR-Data. (arXiv:2310.20319v1 [cs.CV])

    [http://arxiv.org/abs/2310.20319](http://arxiv.org/abs/2310.20319)

    GACE是一种基于几何感知的方法，通过聚合几何线索和空间关系来改进黑盒3D物体检测器的置信度估计，从而在各种检测器中实现了持续的性能提升。

    

    广泛使用的基于LiDAR的3D物体检测器在进行置信度估计时往往忽略了从物体提议中获得的基本几何信息。这主要是由于架构设计选择，往往采用了从2D图像领域采纳的方法，而在2D图像领域中几何上下文很少可用。然而，在3D中，综合考虑物体属性及其周围环境对于区分真正的和错误的检测结果，例如一组遮挡的行人，是很重要的。为了解决这个问题，我们提出了GACE，一种直观且高效的方法来改进给定黑盒3D物体检测器的置信度估计。我们聚合检测结果的几何线索和它们之间的空间关系，从而能够准确评估它们的可行性，并因此改进置信度估计。这样就实现了在多种最先进的检测器上持续的性能提升。在所有评估的检测器中，GACE被证明特别有效。

    Widely-used LiDAR-based 3D object detectors often neglect fundamental geometric information readily available from the object proposals in their confidence estimation. This is mostly due to architectural design choices, which were often adopted from the 2D image domain, where geometric context is rarely available. In 3D, however, considering the object properties and its surroundings in a holistic way is important to distinguish between true and false positive detections, e.g. occluded pedestrians in a group. To address this, we present GACE, an intuitive and highly efficient method to improve the confidence estimation of a given black-box 3D object detector. We aggregate geometric cues of detections and their spatial relationships, which enables us to properly assess their plausibility and consequently, improve the confidence estimation. This leads to consistent performance gains over a variety of state-of-the-art detectors. Across all evaluated detectors, GACE proves to be especially
    
[^11]: 自训练Transformer中自注意力的因果解释

    Causal Interpretation of Self-Attention in Pre-Trained Transformers. (arXiv:2310.20307v1 [cs.AI])

    [http://arxiv.org/abs/2310.20307](http://arxiv.org/abs/2310.20307)

    本研究提出了一种对自注意力进行因果解释的方法，并利用已有的预训练Transformer进行零样本因果发现。通过计算最深注意层中相应表示之间的偏相关，我们可以学习输入序列上的因果结构。该方法在两个任务中为Transformer的结果提供了因果解释。

    

    我们提出了一种对Transformer神经网络架构中自注意力进行因果解释的方法。我们将自注意力解释为一种估计给定输入符号序列的结构方程模型的机制。结构方程模型可以解释为在特定上下文中输入序列上的因果结构。重要的是，在存在潜在混淆变量的情况下，这种解释仍然有效。根据这种解释，我们通过计算最深注意层中相应表示之间的偏相关来估计输入符号之间的条件独立关系。这使得可以使用现有的基于约束的算法学习输入序列上的因果结构。从这个意义上讲，现有的预训练Transformer可以用于零样本因果发现。我们通过为两个任务中Transformer的结果提供因果解释来示范这种方法。

    We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks:
    
[^12]: 神经网络局部差分分类隐私的验证

    Verification of Neural Networks Local Differential Classification Privacy. (arXiv:2310.20299v1 [cs.LG])

    [http://arxiv.org/abs/2310.20299](http://arxiv.org/abs/2310.20299)

    该论文提出了局部差分分类隐私（LDCP）的概念，扩展了局部鲁棒性的差分隐私设置，旨在验证神经网络对隐私的保护能力。作者提出了Sphynx算法，通过计算网络的抽象来验证LDCP，解决了传统算法中需要训练大量网络和逐个验证的问题。

    

    神经网络容易受到隐私攻击。至今没有验证器能够推断参与训练集的个体的隐私。我们提出了一种新的隐私属性，称为局部差分分类隐私（LDCP），将局部鲁棒性扩展到适用于黑盒分类器的差分隐私设置中。给定一个输入邻域，如果一个分类器是LDCP的，无论它是使用完整数据集训练还是省略任何一个条目训练，它都能对所有输入进行相同的分类。一个天真的算法是非常不切实际的，因为它涉及到训练大量的网络，并且对给定邻域的局部鲁棒性进行单独验证。我们提出了Sphynx算法，它可以从一小组网络中以高概率计算出所有网络的抽象，并直接在抽象网络上验证LDCP。挑战是双重的：网络参数不遵循已知的分布概率。

    Neural networks are susceptible to privacy attacks. To date, no verifier can reason about the privacy of individuals participating in the training set. We propose a new privacy property, called local differential classification privacy (LDCP), extending local robustness to a differential privacy setting suitable for black-box classifiers. Given a neighborhood of inputs, a classifier is LDCP if it classifies all inputs the same regardless of whether it is trained with the full dataset or whether any single entry is omitted. A naive algorithm is highly impractical because it involves training a very large number of networks and verifying local robustness of the given neighborhood separately for every network. We propose Sphynx, an algorithm that computes an abstraction of all networks, with a high probability, from a small set of networks, and verifies LDCP directly on the abstract network. The challenge is twofold: network parameters do not adhere to a known distribution probability, ma
    
[^13]: 通过重置深度集合代理实现高样本效率和安全的深度强化学习

    Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents. (arXiv:2310.20287v1 [cs.LG])

    [http://arxiv.org/abs/2310.20287](http://arxiv.org/abs/2310.20287)

    这项研究提出一种基于重置的深度集合学习方法，以提高深度强化学习的样本效率和解决复位方法的局限性。实验结果表明，该方法在安全强化学习领域取得了良好的性能。

    

    深度强化学习通过与深度神经网络集成作为函数逼近器，在解决复杂任务方面取得了显著的成功。然而，对深度神经网络的依赖引入了一种被称为“优先级偏见”的新挑战，即这些函数逼近器倾向于优先考虑早期的经验，导致过拟合。为了缓解这种优先级偏见，已经提出了一种重置方法，该方法以保留回放缓冲区的方式对深度强化学习代理的部分或全部进行周期性重置。然而，使用重置方法后可能出现性能崩溃，这从安全强化学习和遗憾最小化的角度来看可能是有害的。在本文中，我们提出了一种新的基于重置的方法，利用深度集合学习来解决普通重置方法的局限性，并增强样本效率。通过包括安全强化学习领域在内的各种实验对所提出的方法进行了评估。

    Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To mitigate this primacy bias, a reset method has been proposed, which performs periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of the reset method can result in performance collapses after executing the reset, which can be detrimental from the perspective of safe RL and regret minimization. In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments including those in the domain of safe RL. Numer
    
[^14]: 通过以计算为代价加速广义线性模型

    Accelerating Generalized Linear Models by Trading off Computation for Uncertainty. (arXiv:2310.20285v1 [cs.LG])

    [http://arxiv.org/abs/2310.20285](http://arxiv.org/abs/2310.20285)

    本论文提出了一种迭代方法，通过增加不确定性来降低计算量，并显著提高广义线性模型的训练速度。

    

    贝叶斯广义线性模型（GLMs）定义了一个灵活的概率框架，用于建模分类、有序和连续数据，并且在实践中被广泛使用。然而，对于大型数据集，GLMs的精确推断代价太高，因此需要在实践中进行近似。造成的近似误差对模型的可靠性产生不利影响，并且没有被考虑在预测的不确定性中。在这项工作中，我们引入了一系列迭代方法，明确地对这个误差建模。它们非常适合并行计算硬件，有效地回收计算并压缩信息，以减少GLMs的时间和内存需求。正如我们在一个实际的大型分类问题上展示的那样，我们的方法通过明确地将减少计算与增加不确定性进行权衡来显著加速训练。

    Bayesian Generalized Linear Models (GLMs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in GLMs is prohibitively expensive for large datasets, thus requiring approximations in practice. The resulting approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. In this work, we introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for GLMs. As we demonstrate on a realistically large classification problem, our method significantly accelerates training by explicitly trading off reduced computation for increased uncertainty.
    
[^15]: 改进的多变量时间序列预测的自动混合模型在BizITOps数据上的应用

    AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data. (arXiv:2310.20280v1 [cs.LG])

    [http://arxiv.org/abs/2310.20280](http://arxiv.org/abs/2310.20280)

    这篇论文提出了AutoMixer，一个基于时间序列基础模型的自动混合模型，通过通道压缩预训练和微调工作流技术，有效解耦了BizITOps数据中有用和嘈杂的跨通道交互，提高了多变量时间序列预测的性能。

    

    业务过程的效率依赖于业务关键绩效指标（Biz-KPIs），而IT故障可能对其产生负面影响。BizITOps数据将Biz-KPIs和IT事件通道融合成多变量时间序列数据。提前预测Biz-KPIs可以通过主动的纠正措施提高效率和收益。然而，BizITOps数据通常展示出Biz-KPIs和IT事件之间有用和嘈杂的跨通道交互，需要有效解耦。当使用现有的多变量预测模型时，这导致预测性能不佳。为了解决这个问题，我们引入了一个称为AutoMixer的时间序列基础模型（FM）方法，该方法基于新颖的通道压缩预训练和微调工作流技术。AutoMixer利用自动编码器进行通道压缩的预训练，并将其与先进的TSMixer模型集成，用于多变量时间序列预测。这种融合极大地增强了TSM

    The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSM
    
[^16]: 从比较性产品评论中提取感兴趣的实体

    Extracting Entities of Interest from Comparative Product Reviews. (arXiv:2310.20274v1 [cs.IR])

    [http://arxiv.org/abs/2310.20274](http://arxiv.org/abs/2310.20274)

    本文提出了一种基于深度学习的方法，用于从比较性产品评论中提取产品比较信息，并通过实验证明其在这个任务中优于现有的语义角色标注框架。

    

    本文提出了一种基于深度学习的方法，用于从各种电子商务网站的用户评论中提取产品比较信息。任何一个比较性产品评论都有三个重要的信息实体：被比较产品的名称，用户观点（谓词）以及被比较的特征或方面。所有这些信息实体彼此依赖并受到评论语言规则的约束。我们观察到，这些相互依赖关系可以很好地通过LSTM进行捕捉。我们在现有的手动标记数据集上评估了我们的系统，并观察到其在这个任务中表现优于现有的语义角色标注（SRL）框架。

    This paper presents a deep learning based approach to extract product comparison information out of user reviews on various e-commerce websites. Any comparative product review has three major entities of information: the names of the products being compared, the user opinion (predicate) and the feature or aspect under comparison. All these informing entities are dependent on each other and bound by the rules of the language, in the review. We observe that their inter-dependencies can be captured well using LSTMs. We evaluate our system on existing manually labeled datasets and observe out-performance over the existing Semantic Role Labeling (SRL) framework popular for this task.
    
[^17]: 通过学习相关的潜在空间推进贝叶斯优化

    Advancing Bayesian Optimization via Learning Correlated Latent Space. (arXiv:2310.20258v1 [cs.LG])

    [http://arxiv.org/abs/2310.20258](http://arxiv.org/abs/2310.20258)

    本文提出了一种通过学习相关的潜在空间来推进贝叶斯优化的方法。该方法引入了Lipschitz正则化、损失加权和信任区域重新协调，以减小在潜在空间和目标函数之间的差距，并在多个优化任务中展示了其有效性。

    

    贝叶斯优化是一种通过有限的函数评估来优化黑盒函数的强大方法。最近的研究表明，通过深度生成模型（如变分自动编码器）在潜在空间中进行优化，可以对结构化或离散数据进行有效和高效的贝叶斯优化。然而，由于优化不是在输入空间中进行，这导致了潜在的差距，可能导致次优解。为了减轻这种差距，我们提出了相关潜在空间贝叶斯优化（CoBO），它专注于学习相关的潜在空间，其特点是潜在空间中的距离和目标函数内的距离之间存在强相关性。具体来说，我们的方法引入了Lipschitz正则化、损失加权和信任区域重新协调，以最小化有希望区域周围的潜在差距。我们在几个优化任务中展示了我们方法的有效性。

    Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in disc
    
[^18]: Pose-to-Motion: 基于姿势先验的跨领域动作重定向

    Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior. (arXiv:2310.20249v1 [cs.CV])

    [http://arxiv.org/abs/2310.20249](http://arxiv.org/abs/2310.20249)

    本文提出了一种基于姿势先验的跨领域动作重定向方法，通过从另一个具有不同骨架的角色的现有动作数据集中转移动作，为仅具有姿势数据的角色生成合理的动作。实验证明该方法在小样本或噪声数据集情况下表现鲁棒性。

    

    在计算机图形学领域，创造逼真的角色动作一直是一个目标。目前的基于学习的动作合成方法依赖于大量的动作数据集，这些数据集往往很难甚至不可能获得。另一方面，姿势数据更易获取，因为静态的角色姿势更容易创建，并且可以通过最新的计算机视觉技术从图像中提取出来。在本文中，我们利用这个替代数据源，引入了一种神经动作合成方法通过重定向。我们的方法通过从另一个具有极其不同骨架的角色的现有运动捕捉数据集中转移动作，为仅具有姿势数据的角色生成合理的动作。实验证明我们的方法有效地将源角色的动作特征与目标角色的姿势特征相结合，并且在姿势数据集很小或带有噪声的情况下表现出鲁棒性，从几个艺术家创建的数据集到大规模真实数据集等各种情况的实验都取得了良好的结果。

    Creating believable motions for various characters has long been a goal in computer graphics. Current learning-based motion synthesis methods depend on extensive motion datasets, which are often challenging, if not impossible, to obtain. On the other hand, pose data is more accessible, since static posed characters are easier to create and can even be extracted from images using recent advancements in computer vision. In this paper, we utilize this alternative data source and introduce a neural motion synthesis approach through retargeting. Our method generates plausible motions for characters that have only pose data by transferring motion from an existing motion capture dataset of another character, which can have drastically different skeletons. Our experiments show that our method effectively combines the motion features of the source character with the pose features of the target character, and performs robustly with small or noisy pose data sets, ranging from a few artist-created
    
[^19]: 选择一个表：基于图的张量狄利克雷过程多项式混合模型用于乘客轨迹聚类

    Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering. (arXiv:2310.20224v1 [stat.ML])

    [http://arxiv.org/abs/2310.20224](http://arxiv.org/abs/2310.20224)

    本论文提出了一种新的张量狄利克雷过程多项式混合模型，利用图形结构和空间语义图对基于轨迹记录的乘客聚类进行了改进，能在一步中自动确定聚类数量，并保留了多维出行信息的分层结构。

    

    基于轨迹记录的乘客聚类对于交通运营商至关重要。然而，现有方法由于乘客出行信息的分层结构，包括每个乘客内部的多次出行以及每次出行的多维信息，无法轻松地聚类乘客。此外，现有方法依赖于准确指定聚类数量的起始值。最后，现有方法未考虑空间语义图，如地理邻近性和位置间的功能相似性。在本文中，我们提出了一种新颖的基于图的张量狄利克雷过程多项式混合模型，它能够保留多维出行信息的分层结构，并能以统一的一步方式对其进行聚类，具有自动确定聚类数量的能力。空间图被用于社区检测以连接语义邻居。我们进一步提出了张量版本的Coll...

    Passenger clustering based on trajectory records is essential for transportation operators. However, existing methods cannot easily cluster the passengers due to the hierarchical structure of the passenger trip information, including multiple trips within each passenger and multi-dimensional information about each trip. Furthermore, existing approaches rely on an accurate specification of the clustering number to start. Finally, existing methods do not consider spatial semantic graphs such as geographical proximity and functional similarity between the locations. In this paper, we propose a novel tensor Dirichlet Process Multinomial Mixture model with graphs, which can preserve the hierarchical structure of the multi-dimensional trip information and cluster them in a unified one-step manner with the ability to determine the number of clusters automatically. The spatial graphs are utilized in community detection to link the semantic neighbors. We further propose a tensor version of Coll
    
[^20]: STDA-Meta: 一种用于少样本交通预测的元学习框架

    STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction. (arXiv:2310.20223v1 [cs.LG])

    [http://arxiv.org/abs/2310.20223](http://arxiv.org/abs/2310.20223)

    STDA-Meta是一种用于少样本交通预测的元学习框架，能够解决传统方法在数据稀缺的情况下性能较差的问题。

    

    随着城市的发展，交通拥堵成为一个日益严重的问题，交通预测是缓解这个问题的经典方法。交通预测是空间-时间预测学习的一种特定应用，如出租车调度、天气预测和船舶轨迹预测。然而，传统的空间-时间预测学习方法，包括深度学习，在一些传感器不足的新兴城市并不适用，并且数据稀缺使预测性能更差。在这种情况下，对于少样本数据的学习被称为少样本学习，而交通预测的少样本学习仍然面临挑战。一方面，图结构的不规则性和图的动态性无法满足空间-时间学习方法的性能要求。另一方面，传统的领域自适应方法在数据不足的情况下很难发挥作用。

    As the development of cities, traffic congestion becomes an increasingly pressing issue, and traffic prediction is a classic method to relieve that issue. Traffic prediction is one specific application of spatio-temporal prediction learning, like taxi scheduling, weather prediction, and ship trajectory prediction. Against these problems, classical spatio-temporal prediction learning methods including deep learning, require large amounts of training data. In reality, some newly developed cities with insufficient sensors would not hold that assumption, and the data scarcity makes predictive performance worse. In such situation, the learning method on insufficient data is known as few-shot learning (FSL), and the FSL of traffic prediction remains challenges. On the one hand, graph structures' irregularity and dynamic nature of graphs cannot hold the performance of spatio-temporal learning method. On the other hand, conventional domain adaptation methods cannot work well on insufficient tr
    
[^21]: 基于Transformer的长期系列预测的系统综述

    A Systematic Review for Transformer-based Long-term Series Forecasting. (arXiv:2310.20218v1 [cs.LG])

    [http://arxiv.org/abs/2310.20218](http://arxiv.org/abs/2310.20218)

    基于Transformer的长期系列预测的系统综述，介绍了Transformer架构及其改进、公开可用的数据集和评估指标、有效训练Transformer的最佳实践和技术，并提出了潜在研究方向。

    

    深度学习的出现在时间序列预测方面取得了显著进展。特别是Transformer架构在时间序列预测任务中得到了广泛的应用和采用。Transformer被证明是提取长序列内部元素之间语义相关性最成功的解决方案。各种变体使得Transformer架构能够有效处理长期时间序列预测任务。在本文中，我们首先对Transformer架构及其后续改进进行了全面概述，以解决各种长期时间序列预测任务。然后，我们总结了公开可用的长期时间序列预测数据集和相关的评估指标。此外，我们提供了关于在时间序列分析背景下有效训练Transformer的最佳实践和技术的有价值见解。最后，我们提出了这个快速发展领域的潜在研究方向。

    The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.
    
[^22]: LEO卫星网络的切换协议学习：访问延迟和碰撞最小化

    Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization. (arXiv:2310.20215v1 [cs.IT])

    [http://arxiv.org/abs/2310.20215](http://arxiv.org/abs/2310.20215)

    本研究提出了一种面向LEO卫星网络的深度强化学习切换协议(DHO)，通过预测能力跳过测量报告阶段，简化切换过程并消除访问延迟，同时在网络条件下表现优越，展示了实际应用价值。

    

    本研究介绍了一种新颖的基于深度强化学习 (DRL) 的切换 (HO) 协议，称为DHO，专门针对低轨道卫星网络的长传播延迟所带来的挑战。DHO在预定的LEO卫星轨迹模式下进行训练，在HO过程中利用其预测能力跳过测量报告(MR)阶段，简化了过程并消除了MR阶段产生的传播延迟，同时仍能提供有效的HO决策。所提出的DHO在不同网络条件下表现优于传统HO协议，包括访问延迟、碰撞率和切换成功率，展示了DHO在实际网络中的实际适用性。此外，本研究还探讨了访问延迟和碰撞率之间的平衡，并评估了使用各种DRL算法对DHO进行训练的性能和收敛性。

    This study presents a novel deep reinforcement learning (DRL)-based handover (HO) protocol, called DHO, specifically designed to address the persistent challenge of long propagation delays in low-Earth orbit (LEO) satellite networks' HO procedures. DHO skips the Measurement Report (MR) in the HO procedure by leveraging its predictive capabilities after being trained with a pre-determined LEO satellite orbital pattern. This simplification eliminates the propagation delay incurred during the MR phase, while still providing effective HO decisions. The proposed DHO outperforms the legacy HO protocol across diverse network conditions in terms of access delay, collision rate, and handover success rate, demonstrating the practical applicability of DHO in real-world networks. Furthermore, the study examines the trade-off between access delay and collision rate and also evaluates the training performance and convergence of DHO using various DRL algorithms.
    
[^23]: 分布匹配校准：可训练的核校准度量

    Calibration by Distribution Matching: Trainable Kernel Calibration Metrics. (arXiv:2310.20211v1 [cs.LG])

    [http://arxiv.org/abs/2310.20211](http://arxiv.org/abs/2310.20211)

    该论文提出了基于核的校准度量方法，统一和推广了分类和回归中常见的校准形式。这些度量可以产生可微的样本估计，易于纳入经验风险最小化，并通过定制校准度量来优化决策任务。

    

    校准确保概率预测能够有效地捕捉不确定性，要求预测概率与经验频率相吻合。然而，许多现有的校准方法专门用于事后再校准，可能会恶化预测的尖锐性。基于将校准视为分布匹配任务的洞察，我们引入了基于核的校准度量，统一和推广了分类和回归中常见的校准形式。这些度量可以产生可微的样本估计，可以很容易地将校准目标纳入经验风险最小化中。此外，我们提供了直观的机制来定制决策任务的校准度量，并强制准确的损失估计和无遗憾决策。我们的实证评估表明，使用这些度量作为正则化项可以提高在一系列回归和分类任务中的校准性、尖锐性和决策性能。

    Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification 
    
[^24]: 利用近无限历史的通用检索增强医学预测模型

    General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])

    [http://arxiv.org/abs/2310.20204](http://arxiv.org/abs/2310.20204)

    基于电子健康记录，我们提出了一种称为REMed的检索增强医学预测模型，通过无限评估临床事件并自动选择相关事件进行预测，消除了人工特征选择和观察窗口的限制，并在实验中表现出优异的效果。

    

    基于电子健康记录（EHRs）开发临床预测模型（例如死亡预测）通常依赖于专家意见进行特征选择和调整观测窗口大小。这给专家带来负担并在开发过程中造成瓶颈。我们提出了一种检索增强的医学预测模型（REMed），以应对这些挑战。REMed可以基本评估无限量的临床事件，选择相关的事件并进行预测。这种方法有效地消除了需要手动进行特征选择并实时观察的需要。我们通过对27个临床任务和两个公开可用的EHR数据集的独立队列实验验证了这些特性，结果显示REMed优于其他现代架构，它们旨在处理尽可能多的事件。值得注意的是，我们发现REMed的偏好与医学专家的偏好密切相似。我们期望我们的方法能显著加速该领域的发展。

    Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the d
    
[^25]: 使用随机梯度进行神经网络剪枝的重要性估计

    Importance Estimation with Random Gradient for Neural Network Pruning. (arXiv:2310.20203v1 [cs.LG])

    [http://arxiv.org/abs/2310.20203](http://arxiv.org/abs/2310.20203)

    本文提出了一种使用随机梯度进行神经网络剪枝的重要性估计方法，该方法通过启发式推导出Taylor一阶近似方法。此外，通过在传播过程中使用随机梯度和归一化处理，可以避免对有标签样本的需求，并使得所有样本对重要性得分的贡献相似。在实验中，该方法相比于先前方法在不同数据集和架构上表现更好。

    

    全局神经元重要性估计被用于剪枝神经网络以提高效率。大多数现有方法要么使用激活或梯度信息，要么两者兼有，以确定每个神经元或卷积核的全局重要性，这要求大量有标签的样本。在这项工作中，我们使用启发式方法推导出类似于Taylor一阶（TaylorFO）近似方法的重要性估计。我们将我们的方法命名为TaylorFO-abs和TaylorFO-sq。我们提出了两种额外的方法来改进这些重要性估计方法。首先，我们从网络的最后一层传播随机梯度，从而避免了对有标签的样本的需求。其次，我们对最后一层的梯度幅度进行归一化，使所有样本对重要性得分的贡献相似。我们的方法结合额外的技术在CIFAR-100和STL-10数据集上测试时表现优于先前的方法。

    Global Neuron Importance Estimation is used to prune neural networks for efficiency reasons. To determine the global importance of each neuron or convolutional kernel, most of the existing methods either use activation or gradient information or both, which demands abundant labelled examples. In this work, we use heuristics to derive importance estimation similar to Taylor First Order (TaylorFO) approximation based methods. We name our methods TaylorFO-abs and TaylorFO-sq. We propose two additional methods to improve these importance estimation methods. Firstly, we propagate random gradients from the last layer of a network, thus avoiding the need for labelled examples. Secondly, we normalize the gradient magnitude of the last layer output before propagating, which allows all examples to contribute similarly to the importance score. Our methods with additional techniques perform better than previous methods when tested on ResNet and VGG architectures on CIFAR-100 and STL-10 datasets. F
    
[^26]: FedRec+:增强隐私性和解决异质性问题的联邦推荐系统

    FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems. (arXiv:2310.20193v1 [cs.LG])

    [http://arxiv.org/abs/2310.20193](http://arxiv.org/abs/2310.20193)

    FedRec+是一个集成框架，旨在增强隐私性和解决联邦推荐系统中的异质性问题。它利用特征相似性来生成伪项目的虚拟评分，减少噪声，并通过使用Wasserstein距离来估计异质性问题。

    

    在推荐系统中，保护隐私和降低边缘用户的通信成本是一个重要的挑战。尽管联邦学习在避免客户端和服务器之间的数据交换方面被证明是有效的，但研究表明，服务器可以根据两轮用户上传的梯度的非零梯度变化来推断用户的评分。此外，联邦推荐系统面临异质性问题，导致推荐性能下降。在本文中，我们提出了一种名为FedRec+的联邦推荐系统的集成框架，旨在增强隐私性并解决异质性问题。FedRec+利用基于特征相似性的最优子集选择来生成伪项目的近似最佳虚拟评分，仅利用用户的本地信息。这种方法可以减少噪声而不增加额外的通信成本。此外，我们利用Wasserstein距离来估计异质性问题。

    Preserving privacy and reducing communication costs for edge users pose significant challenges in recommendation systems. Although federated learning has proven effective in protecting privacy by avoiding data exchange between clients and servers, it has been shown that the server can infer user ratings based on updated non-zero gradients obtained from two consecutive rounds of user-uploaded gradients. Moreover, federated recommendation systems (FRS) face the challenge of heterogeneity, leading to decreased recommendation performance. In this paper, we propose FedRec+, an ensemble framework for FRS that enhances privacy while addressing the heterogeneity challenge. FedRec+ employs optimal subset selection based on feature similarity to generate near-optimal virtual ratings for pseudo items, utilizing only the user's local information. This approach reduces noise without incurring additional communication costs. Furthermore, we utilize the Wasserstein distance to estimate the heterogene
    
[^27]: 低光条件下改善视觉任务的可见光到热红外图像翻译

    Visible to Thermal image Translation for improving visual task in low light conditions. (arXiv:2310.20190v1 [cs.CV])

    [http://arxiv.org/abs/2310.20190](http://arxiv.org/abs/2310.20190)

    本文提出了一种新的方法来改善低光条件下的视觉任务，利用可见光图像到热红外图像的翻译。通过使用生成网络和检测网络，实现了可见光图像到热红外图像的转换，并展示了该方法的可行性和效果。这对于安防和监控应用具有重要意义。

    

    在低光条件下，通过使用可见光图像很难完成一些视觉任务，如行人检测和图像翻译。热红外图像中物体的热变化可以用来克服这个问题。本文提出了一个端到端的框架，包括一个生成网络和一个检测网络，用于将可见光图像翻译成热红外图像，并将生成的热红外图像与真实数据进行比较。我们使用Parrot Anafi Thermal无人机在两个不同位置收集了图像。然后，我们创建了一个双流网络，对图像数据进行了预处理、增强，从头开始训练了生成器和鉴别器模型。研究结果表明，使用GAN将可见光训练数据转换为热红外数据是可行的。因此，现在可以更快速、更经济地生成热红外数据，这对于安防和监控应用非常有用。

    Several visual tasks, such as pedestrian detection and image-to-image translation, are challenging to accomplish in low light using RGB images. Heat variation of objects in thermal images can be used to overcome this. In this work, an end-to-end framework, which consists of a generative network and a detector network, is proposed to translate RGB image into Thermal ones and compare generated thermal images with real data. We have collected images from two different locations using the Parrot Anafi Thermal drone. After that, we created a two-stream network, preprocessed, augmented, the image data, and trained the generator and discriminator models from scratch. The findings demonstrate that it is feasible to translate RGB training data to thermal data using GAN. As a result, thermal data can now be produced more quickly and affordably, which is useful for security and surveillance applications.
    
[^28]: 自监督预训练用于降水后处理

    Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])

    [http://arxiv.org/abs/2310.20187](http://arxiv.org/abs/2310.20187)

    该论文提出了一种基于深度学习的降水后处理方法，使用自监督预训练和转移学习来提高数值天气预报模型的准确性。实验结果表明该方法在区域降水校正方面表现优于其他方法。

    

    为了预防危险天气事件，确保充足的局地降水预报提前时间至关重要。然而，全球变暖引起的气候变化增加了准确预测严重降水事件（如暴雨）的挑战。本工作提出了一种基于深度学习的降水后处理方法，用于数值天气预报（NWP）模型。降水后处理包括（i）自监督预训练，其中编码器的参数在大气物理领域的遮蔽变量重构上进行预训练，以及（ii）从预训练的编码器中转移学习到降水分割任务（目标领域）。我们还引入了一种启发式标记方法，以有效地训练类别不平衡的数据集。我们在区域NWP中的降水校正实验结果表明，所提出的方法优于其他方法。

    Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
    
[^29]: 通过引导学习发现技能

    Learning to Discover Skills through Guidance. (arXiv:2310.20178v1 [cs.LG])

    [http://arxiv.org/abs/2310.20178](http://arxiv.org/abs/2310.20178)

    提出了一种名为DISCO-DANCE的无监督技能发现算法，通过引导学习提高探索效果，并在具有挑战性的环境中优于其他方法。

    

    在无监督技能发现领域，主要挑战是有限的探索，主要是因为技能偏离其初始轨迹会受到重大惩罚。为了增强探索，最近的方法使用辅助奖励来最大化状态的认知不确定性或熵。然而，我们发现这些奖励的效果随着环境复杂性的增加而下降。因此，我们提出了一种新的无监督技能发现算法，DISCO-DANCE，它选择具有达到未探索状态潜力最高的引导技能，引导其他技能遵循引导技能，然后分散引导技能以最大化在未探索状态中的可区分性。实证评估表明，在具有挑战性的环境中，包括两个导航基准和一个连续控制基准，DISCO-DANCE优于其他无监督技能发现基线。DISCO-DANCE的定性可视化和代码。

    In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE
    
[^30]: 用生成预训练Transformer生成紧凑二进制系统波形

    Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer. (arXiv:2310.20172v1 [gr-qc])

    [http://arxiv.org/abs/2310.20172](http://arxiv.org/abs/2310.20172)

    提出了一种叫做CBS-GPT的生成预训练Transformer模型，用于紧凑二进制系统波形生成，在预测准确性上达到了较高的准确率，并且具有显著的解释性能。

    

    空间引力波探测是未来十年最受期待的引力波探测项目之一，将探测到丰富的紧凑二进制系统。然而，对于空间引力波波形的精确预测仍未被探索。为了解决探测器响应和二代时延干涉（TDI 2.0）引起的波形复杂性增加而带来的数据处理困难，提出了一种名为CBS-GPT（Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer）的可解释预训练大模型。对于紧凑二进制系统波形，训练了三个模型来预测超大质量黑洞二进制（MBHB）、极端质量比融合（EMRIs）和星系二进制（GB）的波形，分别实现了98%、91%和99%的预测准确性。CBS-GPT模型具有显著的解释性，其隐藏参数能够有效捕捉波形的复杂信息，即使是复杂的不连续。

    Space-based gravitational wave detection is one of the most anticipated gravitational wave (GW) detection projects in the next decade, which will detect abundant compact binary systems. However, the precise prediction of space GW waveforms remains unexplored. To solve the data processing difficulty in the increasing waveform complexity caused by detectors' response and second-generation time-delay interferometry (TDI 2.0), an interpretable pre-trained large model named CBS-GPT (Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer) is proposed. For compact binary system waveforms, three models were trained to predict the waveforms of massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%, respectively. The CBS-GPT model exhibits notable interpretability, with its hidden parameters effectively capturing the intricate information of waveforms, even with complex ins
    
[^31]: 理解和可视化浅云模拟中的液滴分布

    Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds. (arXiv:2310.20168v1 [cs.LG])

    [http://arxiv.org/abs/2310.20168](http://arxiv.org/abs/2310.20168)

    通过利用变分自编码器（VAEs）的紧凑潜在表示，我们能够产生新颖且直观的可视化效果，展示了浅云模拟中液滴尺寸的组织和演化特征。对比不同气溶胶浓度的模拟，我们发现液滴谱的演化在不同的气溶胶水平下很相似，但速度不同，这为气溶胶与云的相互作用提供了新的认识。

    

    对局部液滴级别交互的深入分析对于更好地理解云中微物理过程及其对全球气候的影响至关重要。基于大涡模拟（Large Eddy Simulations，LES）中液滴尺寸分布的高精度模拟挑战当前的分析技术，由于它们涉及三个空间维度、时间和连续范围的液滴尺寸，导致维度非常高。利用变分自编码器（Variational Autoencoders，VAEs）的紧凑潜在表示，我们能够产生新颖且直观的可视化效果，展示液滴尺寸的组织和其随时间的演化特征。这极大地提高了解释能力，并且让我们能够通过对比不同气溶胶浓度的模拟来研究气溶胶与云的相互作用。我们发现，液滴谱的演化在不同的气溶胶水平下很相似，但速度不同。这种相似性表明降水可能受到气溶胶浓度的影响，同时还提供了对云微物理过程的新认识。

    Thorough analysis of local droplet-level interactions is crucial to better understand the microphysical processes in clouds and their effect on the global climate. High-accuracy simulations of relevant droplet size distributions from Large Eddy Simulations (LES) of bin microphysics challenge current analysis techniques due to their high dimensionality involving three spatial dimensions, time, and a continuous range of droplet sizes. Utilizing the compact latent representations from Variational Autoencoders (VAEs), we produce novel and intuitive visualizations for the organization of droplet sizes and their evolution over time beyond what is possible with clustering techniques. This greatly improves interpretation and allows us to examine aerosol-cloud interactions by contrasting simulations with different aerosol concentrations. We find that the evolution of the droplet spectrum is similar across aerosol levels but occurs at different paces. This similarity suggests that precipitation 
    
[^32]: 高效robust Bayesian Optimization对于任意不确定输入的应用

    Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs. (arXiv:2310.20145v1 [cs.LG])

    [http://arxiv.org/abs/2310.20145](http://arxiv.org/abs/2310.20145)

    本文介绍了一种新颖的robust Bayesian Optimization算法，AIRBO，它能够在任意输入不确定性下有效识别出表现一致良好的鲁棒最优解。

    

    Bayesian Optimization (BO) 是一种广泛应用于各种应用中的高效优化算法。在一些具有挑战性的BO任务中，由于优化过程中的不可避免的随机性，如加工误差、执行噪声或上下文变异，输入不确定性会出现。这种不确定性会使输入在评估之前偏离预期值，导致最终结果的性能波动较大。在本文中，我们引入了一种新颖的robust Bayesian Optimization算法，AIRBO，它能有效地识别在任意输入不确定性下表现一致良好的鲁棒最优解。我们的方法通过使用最大均值差(MMD)赋能高斯过程，直接建模任意分布的不确定输入，并通过Nystrom逼近加速后验推断。我们在MMD估计误差下建立了严格的理论遗憾界，并在合成函数上进行了广泛的实验。

    Bayesian Optimization (BO) is a sample-efficient optimization algorithm widely employed across various applications. In some challenging BO tasks, input uncertainty arises due to the inevitable randomness in the optimization process, such as machining errors, execution noise, or contextual variability. This uncertainty deviates the input from the intended value before evaluation, resulting in significant performance fluctuations in the final result. In this paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO, which can effectively identify a robust optimum that performs consistently well under arbitrary input uncertainty. Our method directly models the uncertain inputs of arbitrary distributions by empowering the Gaussian Process with the Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference via Nystrom approximation. Rigorous theoretical regret bound is established under MMD estimation error and extensive experiments on synthetic function
    
[^33]: EELBERT:通过动态嵌入实现微型模型

    EELBERT: Tiny Models through Dynamic Embeddings. (arXiv:2310.20144v1 [cs.CL])

    [http://arxiv.org/abs/2310.20144](http://arxiv.org/abs/2310.20144)

    EELBERT是一种通过动态嵌入实现微型模型的方法，具有最小的准确性回归和显著的模型尺寸缩小。最小的模型UNO-EELBERT在GLUE得分上与完全训练的BERT-tiny相差4%，并且体积只有其15倍之一（1.2MB）。

    

    我们介绍了EELBERT，一种用于压缩基于Transformer的模型（例如BERT）的方法，对下游任务的准确性影响最小。这是通过将模型的输入嵌入层替换为动态的，即即时计算的嵌入实现来实现的。由于输入嵌入层占模型大小的重要部分，特别是对于较小的BERT变体，用嵌入计算函数替换该层有助于显著减小模型大小。在GLUE基准测试中的实证评估显示，我们的BERT变体（EELBERT）与传统BERT模型相比仅具有最小的回归。通过这种方法，我们能够开发出我们最小的模型UNO-EELBERT，其GLUE得分比完全训练的BERT-tiny高4％，同时体积小15倍（1.2MB）。

    We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
    
[^34]: 对比差异性预测编码

    Contrastive Difference Predictive Coding. (arXiv:2310.20141v1 [cs.LG])

    [http://arxiv.org/abs/2310.20141](http://arxiv.org/abs/2310.20141)

    本文介绍了一种时间差异版本的对比预测编码，通过将不同时间序列数据的片段组合在一起，来减少学习预测未来事件所需的数据量。实验证明，与先前的方法相比，我们的方法在成功率上提高了2倍，并且对于随机环境有更好的适应能力。

    

    预测和推理未来是许多时间序列问题的核心。例如，目标导向的强化学习可以被看作是学习表示以预测未来可能访问的状态。虽然先前的方法已经使用对比性预测编码来建模时间序列数据，但学习编码长期依赖通常需要大量的数据。在本文中，我们引入了一种时间差异版本的对比预测编码，将不同时间序列数据的片段组合在一起，以减少学习未来事件预测所需的数据量。我们将这种表示学习方法应用于导出目标导向的强化学习的离策略算法。实验证明，与先前的强化学习方法相比，我们的方法在成功率上实现了中位数提高2倍，并且可以更好地应对随机环境。在表格设置中，我们展示了我们的方法约为20倍。

    Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20
    
[^35]: 样本条件的假设稳定性提升了信息论的泛化界限

    Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds. (arXiv:2310.20102v1 [stat.ML])

    [http://arxiv.org/abs/2310.20102](http://arxiv.org/abs/2310.20102)

    通过构建"相邻假设"矩阵和引入样本条件的假设稳定性，本文提出了新的信息论泛化保证，改进了先前信息论界限，并解决了随机凸优化问题中信息论界限的局限性。

    

    我们通过一种新的"相邻假设"矩阵的构造和一种新的稳定性概念——样本条件的假设稳定性（SCH稳定性），提出了新的信息论泛化保证。我们的方法提供了比先前信息论界限更准确的界限，在各种学习场景中有所改善。值得注意的是，这些界限解决了最近Haghifam等人在随机凸优化（SCO）问题上的研究中存在的信息论界限的局限性。

    We present new information-theoretic generalization guarantees through the a novel construction of the "neighboring-hypothesis" matrix and a new family of stability notions termed sample-conditioned hypothesis (SCH) stability. Our approach yields sharper bounds that improve upon previous information-theoretic bounds in various learning scenarios. Notably, these bounds address the limitations of existing information-theoretic bounds in the context of stochastic convex optimization (SCO) problems, as explored in the recent work by Haghifam et al. (2023).
    
[^36]: 平滑在线凸优化问题中具有反馈延迟的鲁棒学习

    Robust Learning for Smoothed Online Convex Optimization with Feedback Delay. (arXiv:2310.20098v1 [cs.LG])

    [http://arxiv.org/abs/2310.20098](http://arxiv.org/abs/2310.20098)

    这项研究提出了一种鲁棒学习算法RCL，用于解决具有多步切换成本和反馈延迟的平滑在线凸优化问题。该算法将不可信的机器学习预测与可信的专家算法相结合，通过约束投影来提高机器学习预测的鲁棒性。研究结果证明了RCL能够在任何给定的专家算法下实现$(1+\lambda)$-竞争性，并通过鲁棒化感知的方式训练机器学习模型以提高平均性能。实验证明了RCL在电动交通的电池管理中的应用表现出了鲁棒性和平均性能的提升。

    

    我们研究了一种具有多步非线性切换成本和反馈延迟的平滑在线凸优化问题，即SOCO。我们提出了一种新颖的机器学习（ML）增强在线算法，称为鲁棒约束学习（RCL），该算法通过约束投影将不可信的ML预测与可信的专家在线算法相结合，以提高ML预测的鲁棒性。具体来说，我们证明了RCL能够保证对于任何给定的专家，满足$(1+\lambda)$-竞争性，其中$\lambda>0$，同时以鲁棒化感知的方式明确训练ML模型以改善平均情况性能。重要的是，RCL是第一个能够在多步切换成本和反馈延迟的情况下提供可证明鲁棒性保证的ML增强算法。我们以电动交通的电池管理为案例研究，展示了RCL在鲁棒性和平均性能上的提升。

    We study a challenging form of Smoothed Online Convex Optimization, a.k.a. SOCO, including multi-step nonlinear switching costs and feedback delay. We propose a novel machine learning (ML) augmented online algorithm, Robustness-Constrained Learning (RCL), which combines untrusted ML predictions with a trusted expert online algorithm via constrained projection to robustify the ML prediction. Specifically,we prove that RCL is able to guarantee$(1+\lambda)$-competitiveness against any given expert for any$\lambda>0$, while also explicitly training the ML model in a robustification-aware manner to improve the average-case performance. Importantly,RCL is the first ML-augmented algorithm with a provable robustness guarantee in the case of multi-step switching cost and feedback delay.We demonstrate the improvement of RCL in both robustness and average performance using battery management for electrifying transportationas a case study.
    
[^37]: 超越U：使扩散模型更快更轻

    Beyond U: Making Diffusion Models Faster & Lighter. (arXiv:2310.20092v1 [cs.LG])

    [http://arxiv.org/abs/2310.20092](http://arxiv.org/abs/2310.20092)

    本文介绍了一种利用连续动力系统设计扩散模型的新型去噪网络，该网络具有更高的参数效率、快速收敛和更好的噪声稳健性。实验证明，与标准U-Net相比，我们的模型在参数和计算成本方面显著减少，并且在推理速度和质量解方面都取得了优于基准模型的结果。

    

    扩散模型是一类生成模型，在图像合成、视频生成和分子设计等任务中取得了创纪录的性能。尽管具备这些能力，但其效率，特别是在逆向去噪过程中，仍然面临着慢收敛速度和高计算成本的挑战。在这项工作中，我们引入了一种利用连续动力系统来设计扩散模型的新型去噪网络的方法，该方法更具参数效率，收敛速度更快，并且具有更高的噪声稳健性。通过对去噪概率扩散模型进行实验，我们的框架使用的参数约为标准去噪概率扩散模型（DDPM）中标准U-Net的四分之一，浮点运算（FLOPs）约为标准U-Net的30%。此外，在相等条件下测量时，我们的模型的推理速度比基准模型快70％，同时收敛到更好的质量解。

    Diffusion models are a family of generative models that yield record-breaking performance in tasks such as image synthesis, video generation, and molecule design. Despite their capabilities, their efficiency, especially in the reverse denoising process, remains a challenge due to slow convergence rates and high computational costs. In this work, we introduce an approach that leverages continuous dynamical systems to design a novel denoising network for diffusion models that is more parameter-efficient, exhibits faster convergence, and demonstrates increased noise robustness. Experimenting with denoising probabilistic diffusion models, our framework operates with approximately a quarter of the parameters and 30% of the Floating Point Operations (FLOPs) compared to standard U-Nets in Denoising Diffusion Probabilistic Models (DDPMs). Furthermore, our model is up to 70% faster in inference than the baseline models when measured in equal conditions while converging to better quality solutio
    
[^38]: 消除变分推断与Wasserstein梯度流之间的鸿沟

    Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows. (arXiv:2310.20090v1 [stat.ML])

    [http://arxiv.org/abs/2310.20090](http://arxiv.org/abs/2310.20090)

    本文研究了消除变分推断与Wasserstein梯度流之间的差距，证明了布雷斯-瓦瑟斯坦梯度流可以重新表示为欧氏梯度流，提出了路径导数梯度估计器和蒸馏过程来拓展该方法，同时可以适用于f-散度和非高斯变分族。

    

    变分推断是一种通过在变分族参数空间内进行优化来近似目标分布的技术。而Wasserstein梯度流描述了在概率测度的空间内进行优化，其中不一定存在参数密度函数。在本文中，我们消除了这两种方法之间的差距。我们证明，在一定条件下，布雷斯-瓦瑟斯坦梯度流可以重新表示为欧氏梯度流，其前向欧拉方案是标准的黑盒变分推断算法。具体而言，梯度流的向量场通过路径导数梯度估计器生成。我们还提供了一个关于路径导数梯度的替代视角，将其框架化为对Wasserstein梯度流的蒸馏过程。蒸馏可以扩展到包含f-散度和非高斯变分族。这种扩展产生了一个新的梯度估计器

    Variational inference is a technique that approximates a target distribution by optimizing within the parameter space of variational families. On the other hand, Wasserstein gradient flows describe optimization within the space of probability measures where they do not necessarily admit a parametric density function. In this paper, we bridge the gap between these two methods. We demonstrate that, under certain conditions, the Bures-Wasserstein gradient flow can be recast as the Euclidean gradient flow where its forward Euler scheme is the standard black-box variational inference algorithm. Specifically, the vector field of the gradient flow is generated via the path-derivative gradient estimator. We also offer an alternative perspective on the path-derivative gradient, framing it as a distillation procedure to the Wasserstein gradient flow. Distillations can be extended to encompass $f$-divergences and non-Gaussian variational families. This extension yields a new gradient estimator fo
    
[^39]: 通过学习有效的选择策略提高子图图神经网络的效率

    Efficient Subgraph GNNs by Learning Effective Selection Policies. (arXiv:2310.20082v1 [cs.LG])

    [http://arxiv.org/abs/2310.20082](http://arxiv.org/abs/2310.20082)

    本文通过学习有效的选择策略提高了子图图神经网络的效率，并且实验证明该方法优于现有的基准方法。

    

    子图图神经网络是一种可证明表达力的神经架构，可以从一组子图中学习图表示。然而，由于在许多子图上执行信息传递所带来的计算复杂性，它们的适用性受到了限制。本文考虑以数据驱动方式学习如何选择一个小的子图子集的问题。我们首先通过证明存在一些WL-难区分的图族，这些图族存在可以识别该族中所有图的有效子图选择策略，来解释这个问题。然后，我们提出了一种名为Policy-Learn的新方法，以迭代方式学习如何选择子图。我们证明，与常用的随机策略和解决相同问题的先前工作不同，我们的架构能够学习到上述高效策略。我们的实验结果表明，Policy-Learn的性能优于现有的基准方法。

    Subgraph GNNs are provably expressive neural architectures that learn graph representations from sets of subgraphs. Unfortunately, their applicability is hampered by the computational complexity associated with performing message passing on many subgraphs. In this paper, we consider the problem of learning to select a small subset of the large set of possible subgraphs in a data-driven fashion. We first motivate the problem by proving that there are families of WL-indistinguishable graphs for which there exist efficient subgraph selection policies: small subsets of subgraphs that can already identify all the graphs within the family. We then propose a new approach, called Policy-Learn, that learns how to select subgraphs in an iterative manner. We prove that, unlike popular random policies and prior work addressing the same problem, our architecture is able to learn the efficient policies mentioned above. Our experimental results demonstrate that Policy-Learn outperforms existing basel
    
[^40]: 混合物理和神经ODE用于预测托卡马克聚变反应堆等离子体感应动力学

    Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance Dynamics in Tokamak Fusion Reactors. (arXiv:2310.20079v1 [physics.plasm-ph])

    [http://arxiv.org/abs/2310.20079](http://arxiv.org/abs/2310.20079)

    本研究将神经常微分方程（Neural ODE）框架应用于预测聚变反应堆等离子体的耦合等离子体电流和内部感应动力学，以解决物理模型和数据驱动模型的瓶颈问题。

    

    尽管被称为托卡马克的聚变反应堆作为一种可靠的能源来源而备受期待，但为了使其经济可行，需要在等离子体控制和处理失控等事件方面取得进展。应用更先进的控制算法面临一个重大瓶颈，即需要更好的等离子体模拟，目前基于物理和数据驱动的方法都存在问题。前者受到计算成本和建模等难题的限制，后者受到数据相对匮乏的限制。为了解决这个问题，本研究将神经常微分方程（Neural ODE）框架应用于预测等离子体动力学的子集，即耦合等离子体电流和内部感应动力学。由于神经ODE框架能够自然地包含基于物理的偏置，我们在Alcator C-Mod聚变反应堆的数据上训练了基于物理和神经网络模型，并发现神经ODE模型可以更好地预测等离子体动力学。

    While fusion reactors known as tokamaks hold promise as a firm energy source, advances in plasma control, and handling of events where control of plasmas is lost, are needed for them to be economical. A significant bottleneck towards applying more advanced control algorithms is the need for better plasma simulation, where both physics-based and data-driven approaches currently fall short. The former is bottle-necked by both computational cost and the difficulty of modelling plasmas, and the latter is bottle-necked by the relative paucity of data. To address this issue, this work applies the neural ordinary differential equations (ODE) framework to the problem of predicting a subset of plasma dynamics, namely the coupled plasma current and internal inductance dynamics. As the neural ODE framework allows for the natural inclusion of physics-based inductive biases, we train both physics-based and neural network models on data from the Alcator C-Mod fusion reactor and find that a model tha
    
[^41]: 部分张量化变压器用于自然语言处理

    Partial Tensorized Transformers for Natural Language Processing. (arXiv:2310.20077v1 [cs.CL])

    [http://arxiv.org/abs/2310.20077](http://arxiv.org/abs/2310.20077)

    论文研究了在自然语言处理中应用部分张量化的变压器架构，通过对BERT和ViT等神经网络的嵌入层压缩和部分张量化，显著提高了模型的准确性，并打破了张量分解领域的新局面。

    

    变压器架构由于其前所未有的准确性在自然语言处理和其他机器学习任务中开创了新局面。然而，它们广泛的存储和参数需求通常阻碍了它们的实际应用。在本研究中，我们研究了张量列分解对提高BERT和ViT等变压器视觉语言神经网络的准确性和压缩性的影响。我们专注于嵌入层压缩和神经网络的部分张量化（PTNN）通过一种算法方法。我们的新颖PTNN方法在不需要后期调整的情况下显著提高了现有模型的准确性，打破了张量分解领域的新局面。

    The transformer architecture has revolutionized Natural Language Processing (NLP) and other machine-learning tasks, due to its unprecedented accuracy. However, their extensive memory and parameter requirements often hinder their practical applications. In this work, we study the effect of tensor-train decomposition to improve the accuracy and compress transformer vision-language neural networks, namely BERT and ViT. We focus both on embedding-layer compression and partial tensorization of neural networks (PTNN) through an algorithmic approach. Our novel PTNN approach significantly improves the accuracy of existing models by up to 5%, all without the need for post-training adjustments, breaking new ground in the field of tensor decomposition.
    
[^42]: Meek分离器及其在目标因果发现中的应用

    Meek Separators and Their Applications in Targeted Causal Discovery. (arXiv:2310.20075v1 [cs.LG])

    [http://arxiv.org/abs/2310.20075](http://arxiv.org/abs/2310.20075)

    本论文研究了Meek分离器及其在目标因果发现中的应用。通过引入Meek分离器，我们可以设计出高效的算法来寻找小规模的分离器，从而实现对目标因果发现问题的优化。

    

    从干预数据中学习因果结构是一个具有广泛应用的基本问题。尽管许多之前的工作都集中于恢复整个因果图，但实际上存在一些场景，仅学习因果图的部分即可满足需求。这被称为“目标因果发现”。在我们的工作中，我们关注两个这样的问题：子集搜索和因果匹配。我们的目标是在这两种情况下尽量减少干预的次数。为此，我们引入了Meek分离器，它是一个子集，在干预时将剩余的未定向边分解为较小的连通分量。然后，我们提出了一种高效的算法来寻找小规模的Meek分离器。这样的过程有助于设计各种基于分而治之的方法。特别地，我们提出了两个随机算法，分别对子集搜索和因果匹配实现对数近似。我们的结果表明，

    Learning causal structures from interventional data is a fundamental problem with broad applications across various fields. While many previous works have focused on recovering the entire causal graph, in practice, there are scenarios where learning only part of the causal graph suffices. This is called $targeted$ causal discovery. In our work, we focus on two such well-motivated problems: subset search and causal matching. We aim to minimize the number of interventions in both cases.  Towards this, we introduce the $Meek~separator$, which is a subset of vertices that, when intervened, decomposes the remaining unoriented edges into smaller connected components. We then present an efficient algorithm to find Meek separators that are of small sizes. Such a procedure is helpful in designing various divide-and-conquer-based approaches. In particular, we propose two randomized algorithms that achieve logarithmic approximation for subset search and causal matching, respectively. Our results 
    
[^43]: 用指导调整实现对生成模型的自动评估

    Automatic Evaluation of Generative Models with Instruction Tuning. (arXiv:2310.20072v1 [cs.CL])

    [http://arxiv.org/abs/2310.20072](http://arxiv.org/abs/2310.20072)

    本论文提出了一种基于指导调整的学习度量方法，通过对预训练语言模型进行微调来实现对生成模型的自动评估。实验结果表明，这种方法在各种评估任务上取得了良好的性能，并且多任务联合训练可以进一步提高性能。

    

    自动评估自然语言生成一直以来都是NLP领域一个难以达到的目标。最近的一种方法是通过对预训练语言模型进行微调，来模拟人类在特定任务和评估标准上的判断。受到指导调整模型的泛化能力的启发，我们提出了一种基于指导调整的学习度量方法。为了测试我们的方法，我们收集了HEAP数据集，其中包含了各种自然语言生成任务和评估标准的人类判断。我们的研究结果表明，在HEAP上通过指导调整语言模型可以取得良好的评估性能，尽管有些评估标准的学习并不那么容易。此外，多任务联合训练可以进一步提高性能，对于未来缺乏人工标注数据的任务是有益的。

    Automatic evaluation of natural language generation has long been an elusive goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate human judgements for a particular task and evaluation criterion. Inspired by the generalization ability of instruction-tuned models, we propose a learned metric based on instruction tuning. To test our approach, we collected HEAP, a dataset of human judgements across various NLG tasks and evaluation criteria. Our findings demonstrate that instruction tuning language models on HEAP yields good performance on many evaluation tasks, though some criteria are less trivial to learn than others. Further, jointly training on multiple tasks can yield additional performance improvements, which can be beneficial for future tasks with little to no human annotated data.
    
[^44]: FOCAL: 在因子化正交潜空间中的多模时间序列感知信号中的对比学习

    FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space. (arXiv:2310.20071v1 [cs.AI])

    [http://arxiv.org/abs/2310.20071](http://arxiv.org/abs/2310.20071)

    本文提出了FOCAL框架，可以通过自监督训练从多模时间序列感知信号中提取全面的特征。它通过使每个模态都编码到一个因子化的潜空间中，同时突出共享特征和专属特征，从而有效处理感知模态之间的共享信息和专属信息。

    

    本文提出了一个名为FOCAL的新型对比学习框架，通过自监督训练从多模时间序列感知信号中提取全面的特征。现有的多模对比框架主要依赖于感知模态之间的共享信息，但没有明确考虑对理解底层感知物理学至关重要的专属模态信息。此外，针对时间序列的对比框架没有适当处理时间信息局部性。FOCAL解决了这些挑战，具体贡献如下：首先，对于给定的多模时间序列，将每个模态编码到一个因子化的潜空间中，该潜空间由共享特征和彼此正交的专属特征组成。共享空间通过模态匹配目标强调跨感知模态的特征模式一致性。相反，专属空间通过一个目标提取模态独占信息。

    This paper proposes a novel contrastive learning framework, called FOCAL, for extracting comprehensive features from multimodal time-series sensing signals through self-supervised training. Existing multimodal contrastive frameworks mostly rely on the shared information between sensory modalities, but do not explicitly consider the exclusive modality information that could be critical to understanding the underlying sensing physics. Besides, contrastive frameworks for time series have not handled the temporal information locality appropriately. FOCAL solves these challenges by making the following contributions: First, given multimodal time series, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective. In contrast, the private space extracts modality-exclusive information through a tr
    
[^45]: LinFlo-Net: 一种生成心脏模拟网格的两阶段深度学习方法

    LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart. (arXiv:2310.20065v1 [cs.CV])

    [http://arxiv.org/abs/2310.20065](http://arxiv.org/abs/2310.20065)

    本文提出了一种新的深度学习模型，通过两阶段形变和新型损失函数的应用，实现了自动生成心脏薄壁结构的计算机模型。该模型在准确性和网格质量方面表现出色，可以直接用于物理模拟，减少后处理的需求。

    

    本文提出了一种深度学习模型，可以从患者成像数据中自动生成人类心脏的计算机模型，并且特别强调其能够生成薄壁心脏结构。我们的方法通过将一个模板网格形变到给定图像的心脏结构上。与以前采用这种方法的深度学习方法相比，我们的框架旨在最小化网格自穿透，这通常发生在形变的表面网格之间的小距离。我们通过使用两阶段的形变过程和一种基于运动动力学导出的新型损失函数来实现这一点，该损失函数惩罚表面接触和相互渗透。我们的模型在与最先进方法相比的同时，还能产生不自相交的网格。产生的网格在基于物理的模拟中可直接使用，最大限度地减少了后处理和清理的需求。

    We present a deep learning model to automatically generate computer models of the human heart from patient imaging data with an emphasis on its capability to generate thin-walled cardiac structures. Our method works by deforming a template mesh to fit the cardiac structures to the given image. Compared with prior deep learning methods that adopted this approach, our framework is designed to minimize mesh self-penetration, which typically arises when deforming surface meshes separated by small distances. We achieve this by using a two-stage diffeomorphic deformation process along with a novel loss function derived from the kinematics of motion that penalizes surface contact and interpenetration. Our model demonstrates comparable accuracy with state-of-the-art methods while additionally producing meshes free of self-intersections. The resultant meshes are readily usable in physics based simulation, minimizing the need for post-processing and cleanup.
    
[^46]: 一种可扩展的训练策略用于盲目的多分布噪声去除

    A Scalable Training Strategy for Blind Multi-Distribution Noise Removal. (arXiv:2310.20064v1 [cs.CV])

    [http://arxiv.org/abs/2310.20064](http://arxiv.org/abs/2310.20064)

    提出了一种使用自适应采样/主动学习策略来训练去噪网络的方法，解决了通用去噪网络在不同噪声分布下表现差的问题。

    

    尽管最近取得了一些进展，但是开发通用的去噪和去伪影网络仍然是一个尚未解决的问题：给定固定的网络权重，一个任务（例如去除泊松噪声）的专门化与另一个任务（例如去除斑点噪声）的性能之间存在天然的权衡。此外，由于维度的诅咒，训练这样的网络是具有挑战性的：随着规格空间的维度增加（即需要描述噪声分布所需的参数数量增加），需要训练的唯一规格数量呈指数增长。均匀采样这个空间会导致网络在非常具有挑战性的问题规格上表现良好，但在简单的问题规格上表现不佳，即使大误差也对总体均方误差的影响很小。本文提出了一种使用自适应采样/主动学习策略来训练去噪网络的方法。我们的工作改进了最近提出的一种方法。

    Despite recent advances, developing general-purpose universal denoising and artifact-removal networks remains largely an open problem: Given fixed network weights, one inherently trades-off specialization at one task (e.g.,~removing Poisson noise) for performance at another (e.g.,~removing speckle noise). In addition, training such a network is challenging due to the curse of dimensionality: As one increases the dimensions of the specification-space (i.e.,~the number of parameters needed to describe the noise distribution) the number of unique specifications one needs to train for grows exponentially. Uniformly sampling this space will result in a network that does well at very challenging problem specifications but poorly at easy problem specifications, where even large errors will have a small effect on the overall mean squared error.  In this work we propose training denoising networks using an adaptive-sampling/active-learning strategy. Our work improves upon a recently proposed un
    
[^47]: 去中心化、可扩展且保护隐私的合成数据生成

    Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation. (arXiv:2310.20062v1 [cs.CR])

    [http://arxiv.org/abs/2310.20062](http://arxiv.org/abs/2310.20062)

    这篇论文介绍了一种去中心化、可扩展且保护隐私的合成数据生成系统，使真实数据的贡献者能够参与差分隐私合成数据生成，从而提供更好的隐私和统计保证，并在机器学习流程中更好地利用合成数据。

    

    合成数据作为一种有潜力的方式在降低隐私风险的同时发挥数据价值。合成数据的潜力不仅局限于隐私友好的数据发布，还包括在培训机器学习算法等使用案例中补充真实数据，使其更公平、更能抵抗分布转变等。对于提供更好的隐私和统计保证以及更好地在机器学习流程中利用合成数据的算法进展引起了广泛兴趣。然而，对于负责任和值得信赖的合成数据生成来说，仅关注这些算法方面是不够的，而应该考虑合成数据生成流程的整体视角。我们构建了一个新的系统，允许真实数据的贡献者在没有依赖于值得信赖的中心的情况下自主参与差分隐私合成数据生成。我们的模块化、通用化和可扩展的解决方案基于...

    Synthetic data is emerging as a promising way to harness the value of data, while reducing privacy risks. The potential of synthetic data is not limited to privacy-friendly data release, but also includes complementing real data in use-cases such as training machine learning algorithms that are more fair and robust to distribution shifts etc. There is a lot of interest in algorithmic advances in synthetic data generation for providing better privacy and statistical guarantees and for its better utilisation in machine learning pipelines. However, for responsible and trustworthy synthetic data generation, it is not sufficient to focus only on these algorithmic aspects and instead, a holistic view of the synthetic data generation pipeline must be considered. We build a novel system that allows the contributors of real data to autonomously participate in differentially private synthetic data generation without relying on a trusted centre. Our modular, general and scalable solution is based
    
[^48]: AdaSub：使用低维子空间中的二阶信息进行随机优化

    AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces. (arXiv:2310.20060v1 [math.OC])

    [http://arxiv.org/abs/2310.20060](http://arxiv.org/abs/2310.20060)

    AdaSub是一种使用低维子空间中的二阶信息进行随机优化的算法，通过选择搜索的子空间维度来管理计算开销和算法效率。初步数值结果显示，AdaSub在时间和迭代次数方面优于其他随机优化器。

    

    我们介绍了AdaSub，一种基于低维自适应定义的二阶信息的随机优化算法。与一阶方法相比，二阶方法具有更好的收敛特性，但在每次迭代中计算Hessian矩阵会导致过高的计算开销，使其不实用。为解决这个问题，我们的方法通过选择搜索的子空间维度来管理计算开销和算法效率。我们的代码在GitHub上免费提供，我们的初步数值结果表明，AdaSub在达到给定精度所需的时间和迭代次数方面超过了流行的随机优化器。

    We introduce AdaSub, a stochastic optimization algorithm that computes a search direction based on second-order information in a low-dimensional subspace that is defined adaptively based on available current and past information. Compared to first-order methods, second-order methods exhibit better convergence characteristics, but the need to compute the Hessian matrix at each iteration results in excessive computational expenses, making them impractical. To address this issue, our approach enables the management of computational expenses and algorithm efficiency by enabling the selection of the subspace dimension for the search. Our code is freely available on GitHub, and our preliminary numerical results demonstrate that AdaSub surpasses popular stochastic optimizers in terms of time and number of iterations required to reach a given accuracy.
    
[^49]: 使用Hamiltonian Monte Carlo方法估计最优PAC-Bayes界限

    Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo. (arXiv:2310.20053v1 [stat.ML])

    [http://arxiv.org/abs/2310.20053](http://arxiv.org/abs/2310.20053)

    本研究使用Hamiltonian Monte Carlo方法估计最优PAC-Bayes界限，研究了通过限制后验分布为因子化高斯分布在优化PAC-Bayes界限方面所失去的紧致度，并提出了三种方法来获得高概率界限。实验证明在MNIST数据集上存在显著的紧致度差距，高达5-6％。

    

    PAC-Bayes文献中一个重要但未被充分研究的问题是在优化PAC-Bayes界限时，通过限制后验分布为因子化高斯分布，我们失去了多少紧致度。我们通过估计独立于数据的PAC-Bayes界限来调查此问题，使用最优的后验与使用MFVI得到的界限进行比较。具体而言，我们采用Hamiltonian Monte Carlo从最优的Gibbs后验中进行采样，用热力学积分估计其与先验的KL散度，并提出了三种在不同假设下获得高概率界限的方法。我们在MNIST数据集上的实验揭示了显著的紧致度差距，在某些情况下可达5-6％。

    An important yet underexplored question in the PAC-Bayes literature is how much tightness we lose by restricting the posterior family to factorized Gaussian distributions when optimizing a PAC-Bayes bound. We investigate this issue by estimating data-independent PAC-Bayes bounds using the optimal posteriors, comparing them to bounds obtained using MFVI. Concretely, we (1) sample from the optimal Gibbs posterior using Hamiltonian Monte Carlo, (2) estimate its KL divergence from the prior with thermodynamic integration, and (3) propose three methods to obtain high-probability bounds under different assumptions. Our experiments on the MNIST dataset reveal significant tightness gaps, as much as 5-6\% in some cases.
    
[^50]: 基于多项式的注意力机制的表达能力

    The Expressibility of Polynomial based Attention Scheme. (arXiv:2310.20051v1 [cs.LG])

    [http://arxiv.org/abs/2310.20051](http://arxiv.org/abs/2310.20051)

    本文研究了基于多项式的注意力机制的表达能力，揭示了其与传统softmax的差异。

    

    大型语言模型（LLM）显著改善了我们日常生活的各个方面。这些模型影响了许多领域，从医疗保健到教育，提高了生产力、决策过程和可访问性。然而，在扩展这些模型以处理长文本内容时，transformer架构中的注意力的二次复杂度提出了挑战。这个问题使得在训练非常大的模型或在推断过程中高效地使用它们变得不切实际。最近由[KMZ23]提出了一种使用多项式函数和多项式草图替换softmax以加速注意力机制的技术，但对这种新方法的理论理解尚不完善。在本文中，我们对多项式注意力的表达能力进行了理论分析。我们的研究揭示了多项式注意力能力的差异性。

    Large language models (LLMs) have significantly improved various aspects of our daily lives. These models have impacted numerous domains, from healthcare to education, enhancing productivity, decision-making processes, and accessibility. As a result, they have influenced and, to some extent, reshaped people's lifestyles. However, the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference. While a recent study by [KMZ23] introduced a technique that replaces the softmax with a polynomial function and polynomial sketching to speed up attention mechanisms, the theoretical understandings of this new approach are not yet well understood.  In this paper, we offer a theoretical analysis of the expressive capabilities of polynomial attention. Our study reveals a disparity in the ability o
    
[^51]: SURF: GNN预测流体动力学的泛化性能评估

    SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v1 [cs.LG])

    [http://arxiv.org/abs/2310.20049](http://arxiv.org/abs/2310.20049)

    提出了一个名为SURF的基准测试，用于评估和比较基于图的学习流体模拟器的泛化能力。SURF包括各种数据集和具体的性能和泛化度量指标。通过深入研究两种最先进的模型，我们证明了SURF的适用性。

    

    模拟流体动力学对于设计和开发过程至关重要，涵盖了从简单阀门到复杂涡轮机械的范围。准确求解潜在的物理方程具有计算成本高的特点。因此，基于学习的求解器在网格上建模相互作用并具有显著的加速优势。然而，目前尚不清楚这些模型在多大程度上真正理解潜在的物理原理，并能够实现泛化而非插值。泛化是通用流体模拟器的关键要求，它应该能够适应不同的拓扑结构、分辨率或热力学范围。我们提出了SURF，这是一个旨在测试学习的基于图的流体模拟器的泛化能力的基准测试。SURF包括各个数据集，并提供用于评估和比较不同模型的具体性能和泛化度量指标。我们通过深入研究两种最先进的模型，实证地证明了SURF的适用性。

    Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the
    
[^52]: 缩放黎曼扩散模型

    Scaling Riemannian Diffusion Models. (arXiv:2310.20030v1 [cs.LG])

    [http://arxiv.org/abs/2310.20030](http://arxiv.org/abs/2310.20030)

    本文提出了一种改进的黎曼扩散模型，通过重新审视近似方法和利用对称空间的性质，实现了高精度计算和在高维任务上的扩展能力。

    

    黎曼扩散模型从标准欧几里得空间扩散模型中汲取灵感，学习在一般流形上的分布。不幸的是，额外的几何复杂性使得扩散过渡项无法用闭式表达式表示，因此先前的方法只能使用粗略的近似方法来降低性能并阻止在高维度下的应用。在这项工作中，我们重新审视这些近似方法，并提出几个实用的改进方法。我们的关键观察是，大多数相关流形是对称空间，这些对称空间在计算上更容易处理。通过利用和组合各种方法，我们可以快速计算相关的量到高精度。在低维数据集上，我们的校正产生了明显的改进，使扩散能够与其他方法竞争。此外，我们展示了我们的方法使我们能够在非平凡流形上扩展到高维任务。

    Riemannian diffusion models draw inspiration from standard Euclidean space diffusion models to learn distributions on general manifolds. Unfortunately, the additional geometric complexity renders the diffusion transition term inexpressible in closed form, so prior methods resort to imprecise approximations of the score matching training objective that degrade performance and preclude applications in high dimensions. In this work, we reexamine these approximations and propose several practical improvements. Our key observation is that most relevant manifolds are symmetric spaces, which are much more amenable to computation. By leveraging and combining various ans\"{a}tze, we can quickly compute relevant quantities to high precision. On low dimensional datasets, our correction produces a noticeable improvement, allowing diffusion to compete with other methods. Additionally, we show that our method enables us to scale to high dimensional tasks on nontrivial manifolds. In particular, we mo
    
[^53]: GOPlan:通过学习模型进行计划的目标条件下的离线强化学习

    GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])

    [http://arxiv.org/abs/2310.20025](http://arxiv.org/abs/2310.20025)

    GOPlan是一个使用学习模型进行计划的目标条件下的离线强化学习方法，通过预训练先验策略和使用重新分析方法生成虚构轨迹，用以提高性能和处理有限数据预算和未见目标泛化的能力。

    

    离线目标条件下的强化学习（GCRL）为从多样化和多任务的离线数据集中学习通用策略提供了可行的范例。尽管近期取得了显著进展，但主导的离线GCRL方法仍然受限于无模型方法，限制了它们应对有限数据预算和未见目标泛化的能力。在这项工作中，我们提出了一种新的两阶段模型为基础的框架，Goal-conditioned Offline Planning（GOPlan），包括（1）预训练一个能够捕捉多目标数据集中多模态动作分布的先验策略；（2）利用规划的重新分析方法为微调策略生成虚构轨迹。具体而言，先验策略基于一个具有明显模式分离的带优势权重的条件生成对抗网络，以克服超出分布（OOD）动作的缺点。为进一步优化策略，重新分析方法通过规划生成高质量的虚构数据。

    Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by
    
[^54]: 自组织机器人网络的拓扑可恢复性预测：一种数据驱动的容错方法

    Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach. (arXiv:2310.20024v1 [cs.RO])

    [http://arxiv.org/abs/2310.20024](http://arxiv.org/abs/2310.20024)

    本论文提出了一种基于数据驱动的容错方法来预测自组织机器人网络的拓扑可恢复性。通过将问题转化为二分类问题，并使用贝叶斯高斯混合模型，该方法通过前故障和后故障的两个不同预测路径预测典型问题的解决方案。结果表明，与当前文献中最佳策略相比，该方法在解决拓扑可恢复性预测问题上取得了成功。

    

    自组织机器人网络中的故障可能会严重扰乱其拓扑结构，导致其子集之间失去连接。通常情况下，对于大规模自组织机器人网络，进行最佳拓扑合成是资源密集型且耗时的，难以实时完成。只有当任何故障发生后拓扑恢复的概率超过不可恢复的概率时，才应该执行拓扑重新计算。我们将此问题定义为一个二分类问题，并基于贝叶斯高斯混合模型开发了一个基于数据驱动的模型，通过两个不同的故障前和故障后预测路径来预测典型问题的解决方案。通过整合这些路径的预测结果，与文献中最佳的当前策略相比，明确显示了我们的模型在解决拓扑（不）可恢复性预测问题方面的成功。

    Faults occurring in ad-hoc robot networks may fatally perturb their topologies leading to disconnection of subsets of those networks. Optimal topology synthesis is generally resource-intensive and time-consuming to be done in real time for large ad-hoc robot networks. One should only perform topology re-computations if the probability of topology recoverability after the occurrence of any fault surpasses that of its irrecoverability. We formulate this problem as a binary classification problem. Then, we develop a two-pathway data-driven model based on Bayesian Gaussian mixture models that predicts the solution to a typical problem by two different pre-fault and post-fault prediction pathways. The results, obtained by the integration of the predictions of those pathways, clearly indicate the success of our model in solving the topology (ir)recoverability prediction problem compared to the best of current strategies found in the literature.
    
[^55]: 多尺度特征归因的异常点检测

    Multiscale Feature Attribution for Outliers. (arXiv:2310.20012v1 [cs.LG])

    [http://arxiv.org/abs/2310.20012](http://arxiv.org/abs/2310.20012)

    本论文提出了一种逆多尺度遮挡的特征归因方法，用于识别异常点，并且在Dark Energy Survey Instrument中的星系光谱异常点上取得了更易解释的结果。

    

    机器学习技术能够快速且可靠地识别海量数据集中的异常点，比人工检查要快得多。但是，一旦发现这些异常点，很快就会产生一个问题：哪些特征使得这个输入成为异常？我们提出了一种新的特征归因方法，称为逆多尺度遮挡，专为异常点设计，因为我们对要识别的特征类型了解很少，并且预计模型性能可能不可靠，因为异常的测试数据很可能超过了训练数据的限制。我们在Dark Energy Survey Instrument检测到的星系光谱异常点上展示了我们的方法，并发现其结果比其他归因方法更易解释。

    Machine learning techniques can automatically identify outliers in massive datasets, much faster and more reproducible than human inspection ever could. But finding such outliers immediately leads to the question: which features render this input anomalous? We propose a new feature attribution method, Inverse Multiscale Occlusion, that is specifically designed for outliers, for which we have little knowledge of the type of features we want to identify and expect that the model performance is questionable because anomalous test data likely exceed the limits of the training data. We demonstrate our method on outliers detected in galaxy spectra from the Dark Energy Survey Instrument and find its results to be much more interpretable than alternative attribution approaches.
    
[^56]: 提升强化学习中汤普森采样的贝叶斯遗憾界

    Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. (arXiv:2310.20007v1 [stat.ML])

    [http://arxiv.org/abs/2310.20007](http://arxiv.org/abs/2310.20007)

    本文在多种情境下证明了汤普森采样在强化学习中的贝叶斯遗憾界，并通过对信息比的精确分析提出了一个基于时间不均匀强化学习问题的上界估计。同时，本文找到了各种设置中具体的界限，并讨论了这些结果是第一个其类别或改进了最先进方法的情况。

    

    本文证明了在多种情境下，汤普森采样在强化学习中的第一个贝叶斯遗憾界。我们利用离散的代理环境简化学习问题，并通过后验一致性对信息比进行了精确分析。这导致了一个基于时间不均匀强化学习问题的上界估计为$\widetilde{O}(H\sqrt{d_{l_1}T})$，其中$H$为回合长度，$d_{l_1}$为环境空间的Kolmogorov $l_1$维度。然后，我们在各种设置中找到了$d_{l_1}$的具体界限，比如表格、线性和有限混合，讨论了我们的结果是第一个其类别或改进了最先进方法的情况。

    In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
    
[^57]: PolyThrottle: 边缘设备上的节能神经网络推理

    PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices. (arXiv:2310.19991v1 [cs.LG])

    [http://arxiv.org/abs/2310.19991](http://arxiv.org/abs/2310.19991)

    PolyThrottle是一种在边缘设备上进行节能神经网络推理的解决方案，通过优化设备上的硬件元素配置，可以达到高达36%的能量节省，并且能够快速收敛到近乎最优的设置。

    

    随着神经网络在各个领域的部署，其能量需求也相应增长。尽管一些先前的工作专注于在训练过程中减少能量消耗，但是ML驱动系统的连续运行在推理过程中会导致显著的能量消耗。本文调查了设备上的硬件元素配置（如GPU、内存和CPU频率）在常规微调的神经网络推理中如何影响能量消耗，这在先前的研究中经常被忽视。我们提出了一种名为PolyThrottle的解决方案，通过受约束的贝叶斯优化以节能的方式对各个硬件组件的配置进行优化。我们的实证评估揭示了能量性能平衡的新颖方面，表明我们可以为流行模型节省高达36%的能量。我们还验证了PolyThrottle可以在满足应用约束条件的同时快速收敛到接近最优的设置。

    As neural networks (NN) are deployed across diverse sectors, their energy demand correspondingly grows. While several prior works have focused on reducing energy consumption during training, the continuous operation of ML-powered systems leads to significant energy use during inference. This paper investigates how the configuration of on-device hardware-elements such as GPU, memory, and CPU frequency, often neglected in prior studies, affects energy consumption for NN inference with regular fine-tuning. We propose PolyThrottle, a solution that optimizes configurations across individual hardware components using Constrained Bayesian Optimization in an energy-conserving manner. Our empirical evaluation uncovers novel facets of the energy-performance equilibrium showing that we can save up to 36 percent of energy for popular models. We also validate that PolyThrottle can quickly converge towards near-optimal settings while satisfying application constraints.
    
[^58]: 揭示学习的局部搜索启发式的局限性: 你是最强大的温顺者吗？

    Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?. (arXiv:2310.19990v1 [cs.AI])

    [http://arxiv.org/abs/2310.19990](http://arxiv.org/abs/2310.19990)

    本研究对学习的局部搜索启发式的限制进行了全面调查，结果表明，基于禁忌搜索的简单学习启发式超越了最先进的学习启发式方法。

    

    近年来，将神经网络与局部搜索启发式相结合已经在组合优化领域变得流行。尽管这种方法需要相当大的计算需求，但它在最小的人工工程投入下展现出了很有希望的结果。然而，我们发现了这些整合尝试的实证评估中存在三个关键限制。首先，中等复杂性和弱基线的情况在准确评估基于学习的方法的有效性时是一个挑战。其次，缺乏消融研究使得准确地量化和归因于深度学习架构的改进变得困难。最后，在不同分布下学习启发式的泛化性仍未被充分探索。在这项研究中，我们对这些被识别出的限制进行了全面的调查。令人惊讶的是，我们证明了基于禁忌搜索的简单学习启发式超越了最先进的学习启发式方法。

    In recent years, combining neural networks with local search heuristics has become popular in the field of combinatorial optimization. Despite its considerable computational demands, this approach has exhibited promising outcomes with minimal manual engineering. However, we have identified three critical limitations in the empirical evaluation of these integration attempts. Firstly, instances with moderate complexity and weak baselines pose a challenge in accurately evaluating the effectiveness of learning-based approaches. Secondly, the absence of an ablation study makes it difficult to quantify and attribute improvements accurately to the deep learning architecture. Lastly, the generalization of learned heuristics across diverse distributions remains underexplored. In this study, we conduct a comprehensive investigation into these identified limitations. Surprisingly, we demonstrate that a simple learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA) learned heurist
    
[^59]: 通过利用网络搜索和生成模型解决图像分类中的弱决策边界问题

    Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models. (arXiv:2310.19986v1 [cs.LG])

    [http://arxiv.org/abs/2310.19986](http://arxiv.org/abs/2310.19986)

    本研究针对机器学习模型在图像分类中对弱势群体的表现不一致问题，提出了利用网络搜索和生成模型的方法。通过在具有代表性的子类上进行实验，证明了该方法在增强模型鲁棒性和减轻偏见方面的有效性。

    

    机器学习（ML）技术众所周知存在伦理和运营问题，然而我们目睹到越来越多的企业致力于在敏感应用中部署它们。其中一个主要问题是ML模型对于代表少数群体的样本表现不一致。这使得弱势群体处于更不利和不公平的位置。我们提出了一种利用网络搜索和生成模型的方法，以减轻判别模型的一些缺点。我们在一个图像分类问题上演示了我们的方法，使用ImageNet的人类子树子集，并展示它在增强鲁棒性和减轻偏见方面对某些代表弱势群体（例如，有色人种女性医生）的类别是有效的。我们的新方法能够：（1）识别这些类别的弱决策边界；（2）为谷歌构建搜索查询，并通过DALL-E 2和稳定扩散生成图像的文本。

    Machine learning (ML) technologies are known to be riddled with ethical and operational problems, however, we are witnessing an increasing thrust by businesses to deploy them in sensitive applications. One major issue among many is that ML models do not perform equally well for underrepresented groups. This puts vulnerable populations in an even disadvantaged and unfavorable position. We propose an approach that leverages the power of web search and generative models to alleviate some of the shortcomings of discriminative models. We demonstrate our method on an image classification problem using ImageNet's People Subtree subset, and show that it is effective in enhancing robustness and mitigating bias in certain classes that represent vulnerable populations (e.g., female doctor of color). Our new method is able to (1) identify weak decision boundaries for such classes; (2) construct search queries for Google as well as text for generating images through DALL-E 2 and Stable Diffusion; a
    
[^60]: 通过更快的Frank-Wolfe迭代方法提高差分隐私LASSO正则化逻辑回归的扩展能力

    Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations. (arXiv:2310.19978v1 [cs.LG])

    [http://arxiv.org/abs/2310.19978](http://arxiv.org/abs/2310.19978)

    本论文提出了一种通过改进Frank-Wolfe算法来训练差分隐私回归模型的方法，并在稀疏输入数据上有效。通过这种方法，可以显著减少算法的训练时间，并在实验中取得了显著的性能提升。

    

    据我们所知，目前没有方法可以在稀疏输入数据上训练差分隐私回归模型。为了解决这个问题，我们将$ L_1 $惩罚线性回归的Frank-Wolfe算法适应于稀疏输入，并有效地利用它们。通过这样做，我们将算法的训练时间从$ \mathcal {O}（TDS + TNS）$减少到$ \mathcal {O}（NS + T \sqrt {D} \log {D} + TS ^ 2）$，其中$ T $是迭代次数，而$ N $是数据集的行数，$ D $是特征数，$ S $是稀疏率。我们的结果表明，这个过程可以将运行时间缩短多达$ 2,200 \times $，这取决于隐私参数$ \epsilon $的值和数据集的稀疏程度。

    To the best of our knowledge, there are no methods today for training differentially private regression models on sparse input data. To remedy this, we adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to be aware of sparse inputs and to use them effectively. In doing so, we reduce the training time of the algorithm from $\mathcal{O}( T D S + T N S)$ to $\mathcal{O}(N S + T \sqrt{D} \log{D} + T S^2)$, where $T$ is the number of iterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features. Our results demonstrate that this procedure can reduce runtime by a factor of up to $2,200\times$, depending on the value of the privacy parameter $\epsilon$ and the sparsity of the dataset.
    
[^61]: 通过$f$-差分隐私统一增强混合机制的隐私边界

    Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy. (arXiv:2310.19973v1 [stat.ML])

    [http://arxiv.org/abs/2310.19973](http://arxiv.org/abs/2310.19973)

    本文通过$f$-差分隐私方法改进了洗牌模型和DP-GD中随机初始化的隐私边界，折衷函数的闭式表达式优于$(\epsilon,\delta)$-DP的结果，并且随机初始化可以增强DP-GD的隐私性。

    

    差分隐私（DP）机器学习算法会产生许多随机性，如随机初始化、随机批次抽样和洗牌。然而，由于这些随机性会导致难以分析的混合分布，所以在证明差分隐私边界时很难将其纳入考虑。本文旨在改进洗牌模型和一次迭代的差分隐私梯度下降（DP-GD）中用于随机初始化的隐私边界，采用$f$-DP方法。我们导出了洗牌模型的折衷函数的闭式表达式，优于基于$(\epsilon,\delta)$-DP的最新结果。此外，我们还对随机初始化对一次迭代的DP-GD的隐私性进行了研究。我们对折衷函数的数值计算表明，随机初始化可以增强DP-GD的隐私性。我们对这些混合机制的$f$-DP保证的分析依赖于一种不等式。

    Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of $f$-DP guarantees for these mixture mechanisms relies on an ine
    
[^62]: 早期检测炎症性关节炎以改善转诊的多模态机器学习从血液测试、半结构化和非结构化病历中学习

    Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records. (arXiv:2310.19967v1 [cs.LG])

    [http://arxiv.org/abs/2310.19967](http://arxiv.org/abs/2310.19967)

    通过多模态机器学习从血液测试、半结构化和非结构化病历中学习，早期检测炎症性关节炎可以改善转诊，为及时治疗和预防病情恶化提供决策支持。

    

    早期检测炎症性关节炎（IA）对于高效准确的医院转诊分流以及及时治疗和预防IA疾病进展至关重要，尤其是在有限的医疗资源下。手动评估流程是目前实践中最常见的早期检测IA的方法，但这种方法的劳动密集型和低效率。每个从一般实践（GP）到医院的转诊都需要评估大量的临床信息。机器学习在自动化重复评估任务和提供决策支持方面展示了巨大的潜力，用于早期检测IA。然而，大多数基于机器学习的IA检测方法依赖于血液测试结果。然而，在实践中，血液测试数据并不总是在转诊时可用，因此我们需要利用半结构化和非结构化数据等多模态数据进行早期检测IA的方法。在这项研究中，我们提出了融合和

    Early detection of inflammatory arthritis (IA) is critical to efficient and accurate hospital referral triage for timely treatment and preventing the deterioration of the IA disease course, especially under limited healthcare resources. The manual assessment process is the most common approach in practice for the early detection of IA, but it is extremely labor-intensive and inefficient. A large amount of clinical information needs to be assessed for every referral from General Practice (GP) to the hospitals. Machine learning shows great potential in automating repetitive assessment tasks and providing decision support for the early detection of IA. However, most machine learning-based methods for IA detection rely on blood testing results. But in practice, blood testing data is not always available at the point of referrals, so we need methods to leverage multimodal data such as semi-structured and unstructured data for early detection of IA. In this research, we present fusion and en
    
[^63]: ExPT: 用于少样本实验设计的合成预训练方法

    ExPT: Synthetic Pretraining for Few-Shot Experimental Design. (arXiv:2310.19961v1 [cs.LG])

    [http://arxiv.org/abs/2310.19961](http://arxiv.org/abs/2310.19961)

    ExPT是一种用于少样本实验设计的合成预训练方法，通过将条件生成任务应用于少量带标签的样本和期望输出，生成最优的输入设计。这种方法在不依赖主动数据收集或大规模标记数据集的情况下，在现实场景中具有实用性。

    

    实验设计是许多科学和工程领域中的一个基本问题。在这个问题中，由于现实世界设计评估的时间、金钱和安全成本，样本效率非常重要。现有的方法要么依赖主动数据收集，要么依赖对过去实验的大规模标记数据集的访问，这使得它们在许多现实场景中不可行。在这项工作中，我们解决了少样本实验设计的更具挑战性、现实的环境，其中只有少量带标签的输入设计样本及其相应的数值可用。我们将这个问题看作是一个条件生成任务，在这个任务中，模型根据少量带标签的样本和期望的输出条件生成最优的输入设计。为此，我们引入了实验预训练变换器（ExPT），这是一个用于少样本实验设计的基础模型，它采用合成预训练与上下文学习的新颖组合。在ExPT中，我们只假设对有限的上下文知识有了解。

    Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite co
    
[^64]: 通过混合坐标为运动数据进行拓扑学习

    Topological Learning for Motion Data via Mixed Coordinates. (arXiv:2310.19960v1 [cs.LG])

    [http://arxiv.org/abs/2310.19960](http://arxiv.org/abs/2310.19960)

    本文通过混合坐标和拓扑诱导的聚类在一种多输出高斯过程模型中引入了拓扑信息，以提取结构信息，并能在时间和运动序列分析中应用。

    

    拓扑学可以高效地提取数据集中的结构信息。本文试图将拓扑信息引入多输出高斯过程模型中，以实现迁移学习。为了达到这个目标，我们将环形坐标的框架扩展为一种新颖的混合值坐标框架，以考虑时间序列中的线性趋势。通过拓扑诱导的聚类构建一个基于聚类的核心在多输出高斯过程模型中，是从多个时间序列中有效学习的主要挑战之一。这个核心不仅融入了拓扑结构信息，还允许我们提出一个统一的框架，利用拓扑信息来处理时间和运动序列。

    Topology can extract the structural information in a dataset efficiently. In this paper, we attempt to incorporate topological information into a multiple output Gaussian process model for transfer learning purposes. To achieve this goal, we extend the framework of circular coordinates into a novel framework of mixed valued coordinates to take linear trends in the time series into consideration.  One of the major challenges to learn from multiple time series effectively via a multiple output Gaussian process model is constructing a functional kernel. We propose to use topologically induced clustering to construct a cluster based kernel in a multiple output Gaussian process model. This kernel not only incorporates the topological structural information, but also allows us to put forward a unified framework using topological information in time and motion series.
    
[^65]: PriPrune: 在剪枝联邦学习中量化和保护隐私

    PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning. (arXiv:2310.19958v1 [cs.LG])

    [http://arxiv.org/abs/2310.19958](http://arxiv.org/abs/2310.19958)

    本文针对剪枝联邦学习中的隐私问题进行了调查，推导出了泄露信息的上限，并进行了理论验证和实验验证。

    

    联邦学习（FL）是一种允许多个客户设备和服务器通过仅交换模型更新而共同训练全局模型的范例，而不需要设备共享他们的局部训练数据的方法。这些设备在通信和计算资源方面往往受到限制，并且可以进一步从模型剪枝中受益 - 这是一种广泛用于减小模型大小和复杂度的范例。直观地说，通过使局部模型更粗糙，剪枝预计在FL环境中也会提供一定的隐私攻击保护。然而，目前尚未对此保护进行正式或实验性的特征化，并且不清楚它是否足以抵御最先进的攻击。在这篇论文中，我们对剪枝FL模型的隐私保障进行了首次调查。我们推导出剪枝FL模型泄露的信息论上界。我们通过理论发现进行了补充和验证

    Federated learning (FL) is a paradigm that allows several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks.  In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings, with compreh
    
[^66]: 深度学习在时空大数据中的应用：机遇和挑战展望

    Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges. (arXiv:2310.19957v1 [cs.LG])

    [http://arxiv.org/abs/2310.19957](http://arxiv.org/abs/2310.19957)

    本文介绍了深度学习在时空大数据中的应用，讨论了机遇和挑战，并指出了一些未来研究方向。

    

    随着全球定位系统、遥感和计算模拟的进步，来自各个应用领域的大量时空数据正在以越来越快的速度被收集。涵盖的应用领域包括地球科学、农业、智慧城市和公共安全等。这种新兴的地理空间和时空大数据，结合深度学习技术的最新进展，为解决以往无法实现的问题提供了新机会。例如，遥感研究人员可以利用地球图像大数据训练基础模型，用于众多土地覆盖和土地利用建模任务。沿海模拟学家可以训练人工智能替代模型加速数值模拟。然而，时空大数据的独特特征给深度学习技术带来了新的挑战。本文展望了时空大数据的各种类型，讨论了深度学习在时空大数据中的新研究机会，并列举了一些未来研究方向。

    With advancements in GPS, remote sensing, and computational simulation, an enormous volume of spatiotemporal data is being collected at an increasing speed from various application domains, spanning Earth sciences, agriculture, smart cities, and public safety. Such emerging geospatial and spatiotemporal big data, coupled with recent advances in deep learning technologies, foster new opportunities to solve problems that have not been possible before. For instance, remote sensing researchers can potentially train a foundation model using Earth imagery big data for numerous land cover and land use modeling tasks. Coastal modelers can train AI surrogates to speed up numerical simulations. However, the distinctive characteristics of spatiotemporal big data pose new challenges for deep learning technologies. This vision paper introduces various types of spatiotemporal big data, discusses new research opportunities in the realm of deep learning applied to spatiotemporal big data, lists the un
    
[^67]: 条件非线性自动编码器用于轨迹预测

    Conditional Unscented Autoencoders for Trajectory Prediction. (arXiv:2310.19944v1 [cs.RO])

    [http://arxiv.org/abs/2310.19944](http://arxiv.org/abs/2310.19944)

    本文提出了使用条件非线性自动编码器(CVAE)进行轨迹预测的方法，通过利用变分自动编码器(VAE)中的非线性采样过程和其他改进，超越了现有技术，为自动驾驶领域的轨迹预测提供了更好的性能。

    

    条件变分自动编码器(CVAE)是自动驾驶轨迹预测中最常用的模型之一。它将驾驶环境和真实未来关系建立在概率隐空间中，并利用此空间生成预测。本文对CVAE的关键组件提出了挑战。我们利用变分自动编码器(VAE)领域的最新进展，发现变化采样过程可以极大地提高性能。我们发现非线性采样能够更适合于轨迹预测，而随机采样可能带来潜在的问题。我们进一步提出了其他改进，包括更结构化的混合隐空间，以及一种新颖、可能更具表达力的CVAE推理方法。我们通过在INTERACTION预测数据集上进行评估，展示了我们模型的广泛适用性，超过了现有技术的表现。

    The \ac{CVAE} is one of the most widely-used models in trajectory prediction for \ac{AD}. It captures the interplay between a driving context and its ground-truth future into a probabilistic latent space and uses it to produce predictions. In this paper, we challenge key components of the CVAE. We leverage recent advances in the space of the VAE, the foundation of the CVAE, which show that a simple change in the sampling procedure can greatly benefit performance. We find that unscented sampling, which draws samples from any learned distribution in a deterministic manner, can naturally be better suited to trajectory prediction than potentially dangerous random sampling. We go further and offer additional improvements, including a more structured mixture latent space, as well as a novel, potentially more expressive way to do inference with CVAEs. We show wide applicability of our models by evaluating them on the INTERACTION prediction dataset, outperforming the state of the art, as well 
    
[^68]: 生成式神经网络在物质知识获取中的应用研究

    The Acquisition of Physical Knowledge in Generative Neural Networks. (arXiv:2310.19943v1 [cs.LG])

    [http://arxiv.org/abs/2310.19943](http://arxiv.org/abs/2310.19943)

    本文研究了生成式神经网络在物质知识获取中的应用，并发现其学习轨迹与儿童的发展轨迹不一致。

    

    随着孩子们的成长，他们对周围物理过程的直观理解逐渐发展起来。他们的物理理解以发展轨迹的方式呈现，这在先前的实证研究中被广泛映射出来。在这里，我们通过将物质理解作为测试平台，研究深度生成式神经网络的学习轨迹与儿童发展轨迹的比较。我们提出了一种方法，允许我们检验人类发展的两个不同假设 - 随机优化和复杂性增加。我们发现，虽然我们的模型能够准确预测一些物理过程，但它们在两个假设下的学习轨迹并不与儿童的发展轨迹一致。

    As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development - stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.
    
[^69]: Split-NER: 通过两个基于问答的分类解决命名实体识别问题

    Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications. (arXiv:2310.19942v1 [cs.CL])

    [http://arxiv.org/abs/2310.19942](http://arxiv.org/abs/2310.19942)

    本研究提出了一种名为Split-NER的系统，通过将命名实体识别问题分成提取实体提及跨度和跨度分类两个子任务，然后利用问答模型解决这两个子任务，实现了高效和准确的命名实体识别。

    

    本研究将命名实体识别问题分成两个逻辑子任务：（1）提取实体提及跨度，无论实体类型如何；（2）将跨度分类为实体类型。进一步，我们将这两个子任务都形式化为问答问题，并产生两个可以分别为每个子任务进行优化的更轻的模型。在四个跨领域数据集上的实验表明，这个两步法既有效又节省时间。我们的系统SplitNER在OntoNotes5.0、WNUT17和一个网络安全数据集上的性能超过了基线，并在BioNLP13CG上表现相当。在所有情况下，与QA基线对照相比，它在训练时减少了显著的时间。我们系统的有效性源于分别对跨度检测和分类进行BERT模型的微调。源代码可在https://github.com/c3sr/split-ner找到。

    In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both effective and time efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17 and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all cases, it achieves a significant reduction in training time compared to its QA baseline counterpart. The effectiveness of our system stems from fine-tuning the BERT model twice, separately for span detection and classification. The source code can be found at https://github.com/c3sr/split-ner.
    
[^70]: 基于Lyapunov的Dropout深度神经网络（Lb-DDNN）控制器

    Lyapunov-Based Dropout Deep Neural Network (Lb-DDNN) Controller. (arXiv:2310.19938v1 [eess.SY])

    [http://arxiv.org/abs/2310.19938](http://arxiv.org/abs/2310.19938)

    本文提出了一种基于Dropout的DNN自适应控制器，通过引入基于Lyapunov的实时权重调整定律和Dropout技术，实现了在线无监督学习，并取得了显著的跟踪误差改进和精度提升。

    

    深度神经网络（DNN）基于自适应控制器可以用来补偿非线性动态系统中的非结构化不确定性。然而，DNN也很容易过拟合和共适应。Dropout正则化是一种在训练过程中随机丢弃节点的方法，以减轻过拟合和共适应等问题。本文提出了一种基于Dropout的DNN自适应控制器。该Dropout技术允许对DNN每个层中的权重进行随机选择的去激活。同时，引入了一种基于Lyapunov的实时权重调整定律，用于更新DNN所有层级的权重以实现在线无监督学习。进行了非光滑的基于Lyapunov的稳定性分析，以确保跟踪误差的渐近收敛。实验结果表明，该基于Dropout的DNN自适应控制器的跟踪误差改进了38.32%，精度改进了53.67%。

    Deep neural network (DNN)-based adaptive controllers can be used to compensate for unstructured uncertainties in nonlinear dynamic systems. However, DNNs are also very susceptible to overfitting and co-adaptation. Dropout regularization is an approach where nodes are randomly dropped during training to alleviate issues such as overfitting and co-adaptation. In this paper, a dropout DNN-based adaptive controller is developed. The developed dropout technique allows the deactivation of weights that are stochastically selected for each individual layer within the DNN. Simultaneously, a Lyapunov-based real-time weight adaptation law is introduced to update the weights of all layers of the DNN for online unsupervised learning. A non-smooth Lyapunov-based stability analysis is performed to ensure asymptotic convergence of the tracking error. Simulation results of the developed dropout DNN-based adaptive controller indicate a 38.32% improvement in the tracking error, a 53.67% improvement in th
    
[^71]: 面向少标注学习的目标检测：基于Transformer的模型更有效吗？

    Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?. (arXiv:2310.19936v1 [cs.CV])

    [http://arxiv.org/abs/2310.19936](http://arxiv.org/abs/2310.19936)

    本文提出了一种针对当前最先进目标检测器的半监督方法，使用师生架构在少标注学习设置中避免了依赖敏感的后处理步骤，并在标注稀缺的情况下表现优异。

    

    对于专门的和密集的下游任务，如目标检测，标记数据需要专业知识，成本较高，因此少样本和半监督模型成为更具吸引力的替代方案。在少样本的设置中，我们观察到，在相似数量的参数下，基于Transformer的目标检测器比基于卷积的两阶段模型表现更好，但在最新的半监督设置中，它们的效果没有那么好。在本文中，我们提出了一种针对当前最先进的Deformable DETR目标检测器的半监督方法，使用师生架构在少标注学习设置中避免了依赖敏感的后处理步骤。我们在半监督目标检测基准COVO和Pascal VOC上评估了我们的方法，它在特别是标注稀缺的情况下优于先前的方法。我们相信我们的贡献将开启新的可能性。

    For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilitie
    
[^72]: 环境神经过程的模拟到真实转换

    Sim2Real for Environmental Neural Processes. (arXiv:2310.19932v1 [cs.LG])

    [http://arxiv.org/abs/2310.19932](http://arxiv.org/abs/2310.19932)

    "Sim2Real"提出了一种解决深度学习模型与真实环境观测数据稀疏性之间差距的方法，通过在再分析数据上进行预训练，然后在观测数据上进行微调，实现了对环境神经过程的模拟到真实转换。

    

    近年来，基于机器学习的天气模型已经有了快速的进展。这些模型通常是在数值数据同化系统的网格化再分析数据上训练的。然而，再分析数据存在一些限制，比如对物理定律的假设和低时空分辨率。再分析结果与真实情况之间的差距引发了直接在天气观测数据上训练机器学习模型的兴趣。对分散和稀疏的环境观测进行建模需要可扩展和灵活的机器学习架构，其中之一是卷积条件神经过程(ConvCNP)。ConvCNP可以学习在网格化和非网格化上下文数据上进行条件训练，以在目标位置上进行不确定性感知的预测。然而，真实观测的稀疏性对于深度学习模型（如ConvCNP）来说是个挑战。一种潜在的解决方案是“Sim2Real”：在再分析数据上进行预训练，然后在观测数据上进行微调。

    Machine learning (ML)-based weather models have recently undergone rapid improvements. These models are typically trained on gridded reanalysis data from numerical data assimilation systems. However, reanalysis data comes with limitations, such as assumptions about physical laws and low spatiotemporal resolution. The gap between reanalysis and reality has sparked growing interest in training ML models directly on observations such as weather stations. Modelling scattered and sparse environmental observations requires scalable and flexible ML architectures, one of which is the convolutional conditional neural process (ConvCNP). ConvCNPs can learn to condition on both gridded and off-the-grid context data to make uncertainty-aware predictions at target locations. However, the sparsity of real observations presents a challenge for data-hungry deep learning models like the ConvCNP. One potential solution is 'Sim2Real': pre-training on reanalysis and fine-tuning on observational data. We an
    
[^73]: 基于模型的重新参数化策略梯度方法：理论和实践算法

    Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms. (arXiv:2310.19927v1 [cs.LG])

    [http://arxiv.org/abs/2310.19927](http://arxiv.org/abs/2310.19927)

    研究者通过对基于模型的重新参数化策略梯度方法进行理论研究，发现在长期强化学习问题中可能会遇到优化困难，导致收敛速度较慢。他们提出了一种谱归一化方法来缓解梯度方差爆炸问题。

    

    在机器人学和计算机图形学中，重新参数化策略梯度方法已被广泛应用于连续控制任务。然而，最近的研究发现，当应用于长期强化学习问题时，基于模型的重新参数化策略梯度方法可能会遇到混乱和非平滑的优化景观，导致梯度方差爆炸，从而导致收敛速度较慢。这与传统观念相反，即重新参数化方法在训练深度生成模型等问题中具有较低的梯度估计方差。为了理解这一现象，我们对基于模型的重新参数化策略梯度方法进行了理论研究，并寻找解决优化困难的方法。具体而言，我们分析了基于模型的重新参数化策略梯度方法的收敛性，并指出函数逼近器的平滑性是影响梯度估计质量的主要因素。根据我们的分析，我们提出了一种谱归一化方法来缓解爆炸方差问题。

    ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding var
    
[^74]: Jina Embeddings 2: 面向长篇文档的8192-Token通用文本嵌入模型

    Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])

    [http://arxiv.org/abs/2310.19923](http://arxiv.org/abs/2310.19923)

    Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。

    

    文本嵌入模型已经成为将句子转化为固定大小特征向量的强大工具，这些向量包含了语义信息。尽管这些模型对于信息检索、语义聚类和文本重排序等任务至关重要，但大多数现有的开源模型，尤其是基于BERT等架构构建的模型，难以表示长篇文档，并且常常会进行截断。为了缓解这个挑战，一种常见的方法是将文档分割成更小的段落进行嵌入。然而，这种策略会导致更大的向量集合，进而增加内存消耗，并且在向量搜索时会出现计算密集和延迟升高的问题。为了解决这些挑战，我们介绍了Jina Embeddings 2，这是一个开源的文本嵌入模型，可以容纳高达8192个标记。该模型旨在突破传统的512个标记限制，能够灵活处理长篇文档。

    Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
    
[^75]: 通过机器学习解决一类通过线性规划生成割平面问题的方法

    Solving a Class of Cut-Generating Linear Programs via Machine Learning. (arXiv:2310.19920v1 [math.OC])

    [http://arxiv.org/abs/2310.19920](http://arxiv.org/abs/2310.19920)

    本文通过机器学习提出了一种新的框架，可以近似确定一类通过线性规划生成割平面问题的最优解，解决了在分支界限算法中运行CGLP的计算困难的问题。

    

    生成割平面的线性规划（CGLP）在产生混合整数规划的可行区域的有效不等式时起着关键作用。当将其插入分支界限算法中时，从CGLP获得的割平面有助于加紧松弛并改进对偶下界。然而，由于节点候选数目庞大且缺乏关于哪些节点可生成有效割平面的先验知识，在分支界限树的节点上运行CGLP在计算上是困难的。因此，尽管CGLP对改进对偶下界有潜在影响，但在默认设置的分支割算法中经常避免使用。在本文中，我们提出了一种基于机器学习的新框架，用于近似确定一个CGLP类的最优值，以确定一个割平面是否可以在分支界限树的节点上生成。我们将CGLP转化为目标向量的指示函数，并展示了它可以通过合作伙伴来近似。

    Cut-generating linear programs (CGLPs) play a key role as a separation oracle to produce valid inequalities for the feasible region of mixed-integer programs. When incorporated inside branch-and-bound, the cutting planes obtained from CGLPs help to tighten relaxations and improve dual bounds. However, running the CGLPs at the nodes of the branch-and-bound tree is computationally cumbersome due to the large number of node candidates and the lack of a priori knowledge on which nodes admit useful cutting planes. As a result, CGLPs are often avoided at default settings of branch-and-cut algorithms despite their potential impact on improving dual bounds. In this paper, we propose a novel framework based on machine learning to approximate the optimal value of a CGLP class that determines whether a cutting plane can be generated at a node of the branch-and-bound tree. Translating the CGLP as an indicator function of the objective function vector, we show that it can be approximated through co
    
[^76]: 神经网络中基于价值最大化的元学习策略

    Meta-Learning Strategies through Value Maximization in Neural Networks. (arXiv:2310.19919v1 [cs.NE])

    [http://arxiv.org/abs/2310.19919](http://arxiv.org/abs/2310.19919)

    本文理论上研究了在神经网络中的元学习最优策略，并提出了一个学习努力的框架，可以高效地优化控制信号，从而提升学习性能。

    

    生物和人工学习代理面临诸多学习选择，包括超参数选择和任务分布的各个方面，如课程。了解如何进行这些元学习选择可以提供对生物学习者的认知控制功能的规范解释，并改进工程系统。然而，由于优化整个学习过程的复杂性，目前仍然挑战着计算现代深度网络中的最优策略。在这里，我们在一个可处理的环境中从理论上研究最优策略。我们提出了一个学习努力的框架，能够在完全规范化的目标上高效地优化控制信号：在学习过程中的折现累积性能。通过使用估计梯度下降的平均动力方程，我们获得了计算的可行性，该方程适用于简单的神经网络架构。我们的框架包容了一系列元学习和自动课程学习方法，形成了统一的框架。

    Biological and artificial learning agents face numerous choices about how to learn, ranging from hyperparameter selection to aspects of task distributions like curricula. Understanding how to make these meta-learning choices could offer normative accounts of cognitive control functions in biological learners and improve engineered systems. Yet optimal strategies remain challenging to compute in modern deep networks due to the complexity of optimizing through the entire learning process. Here we theoretically investigate optimal strategies in a tractable setting. We present a learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning. We obtain computational tractability by using average dynamical equations for gradient descent, available for simple neural network architectures. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified n
    
[^77]: 揭示偏见和不平等：利用电子健康记录的医疗人工智能中偏见检测和缓解的系统综述

    Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])

    [http://arxiv.org/abs/2310.19917](http://arxiv.org/abs/2310.19917)

    本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。

    

    目的：利用电子健康记录的人工智能应用在医疗领域越来越受到欢迎，但也引入了各种类型的偏见。本研究旨在系统综述涉及利用电子健康记录数据的人工智能研究中的偏见。方法：遵循Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)准则进行了系统综述。从PubMed、Web of Science和电气和电子工程师学会中检索了2010年1月1日至2022年10月31日期间发表的文章。我们定义了六种主要的偏见类型，并总结了现有的偏见处理方法。结果：在检索到的252篇文章中，有20篇符合最终综述的纳入标准。本综述涵盖了六种偏见中的五种：八项研究分析了选择偏见；六项研究针对隐性偏见；五项研究对混杂偏见进行了研究；四项研究对测量偏见进行了研究；两项研究对算法偏见进行了研究。在偏见处理方法方面，有十项研究进行了探讨。

    Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
    
[^78]: GPCR-BERT:使用蛋白质语言模型解释G蛋白偶联受体(GPCR)的顺序设计

    GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models. (arXiv:2310.19915v1 [cs.LG])

    [http://arxiv.org/abs/2310.19915](http://arxiv.org/abs/2310.19915)

    本文提出了一种名为GPCR-BERT的模型，使用蛋白质语言模型解释了G蛋白偶联受体(GPCR)的顺序设计。通过利用预训练的蛋白质模型和微调预测任务，我们揭示了结合口袋中残基之间的关系和一些保守的基序。

    

    随着化学和生物学中Transformer和大型语言模型(LLMs)的兴起，对治疗方法的设计和理解的科学界开辟了新的途径。蛋白质序列可以被建模为语言，并可以利用最近在LLMs中取得的进展，特别是在我们对蛋白质序列数据集的广泛访问方面。在本文中，我们开发了GPCR-BERT模型，用于理解G蛋白偶联受体(GPCRs)的顺序设计。GPCRs是FDA批准的药物中超过三分之一的靶点。然而，关于氨基酸序列、配体选择性和构象基序(如NPxxY、CWxP、E/DRY)之间的关系缺乏全面的理解。通过利用预训练的蛋白质模型(Prot-Bert)并通过变异预测任务对基序的微调，我们能够揭示结合口袋中残基之间的几种关系和一些保守的基序。

    With the rise of Transformers and Large Language Models (LLMs) in Chemistry and Biology, new avenues for the design and understanding of therapeutics have opened up to the scientific community. Protein sequences can be modeled as language and can take advantage of recent advances in LLMs, specifically with the abundance of our access to the protein sequence datasets. In this paper, we developed the GPCR-BERT model for understanding the sequential design of G Protein-Coupled Receptors (GPCRs). GPCRs are the target of over one-third of FDA-approved pharmaceuticals. However, there is a lack of comprehensive understanding regarding the relationship between amino acid sequence, ligand selectivity, and conformational motifs (such as NPxxY, CWxP, E/DRY). By utilizing the pre-trained protein model (Prot-Bert) and fine-tuning with prediction tasks of variations in the motifs, we were able to shed light on several relationships between residues in the binding pocket and some of the conserved mot
    
[^79]: 基于贝叶斯仿真推断的宇宙初始条件研究

    Bayesian Simulation-based Inference for Cosmological Initial Conditions. (arXiv:2310.19910v1 [astro-ph.CO])

    [http://arxiv.org/abs/2310.19910](http://arxiv.org/abs/2310.19910)

    本论文提出了一种基于仿真推断的贝叶斯场重建算法，结合自回归建模，可以从观测数据中恢复宇宙初始条件。

    

    从观测数据中重建天体物理学和宇宙学领域的场是具有挑战性的。它需要考虑到非线性转换、空间结构的混合和噪声。相比之下，将场映射到观测数据的正向模拟器在许多应用中是可用的。我们提出了一种基于仿真推断的多功能贝叶斯场重建算法，并通过自回归建模进行改进。所提出的技术适用于通用的（非可微分）正向模拟器，并且允许从潜在场的后验分布中采样。我们首次展示了在一个概念验证应用中的一些有希望的结果：从晚期密度场恢复宇宙初始条件。

    Reconstructing astrophysical and cosmological fields from observations is challenging. It requires accounting for non-linear transformations, mixing of spatial structure, and noise. In contrast, forward simulators that map fields to observations are readily available for many applications. We present a versatile Bayesian field reconstruction algorithm rooted in simulation-based inference and enhanced by autoregressive modeling. The proposed technique is applicable to generic (non-differentiable) forward simulators and allows sampling from the posterior for the underlying field. We show first promising results on a proof-of-concept application: the recovery of cosmological initial conditions from late-time density fields.
    
[^80]: 骨干网络之战：计算机视觉任务中预训练模型的大规模比较

    Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks. (arXiv:2310.19909v1 [cs.CV])

    [http://arxiv.org/abs/2310.19909](http://arxiv.org/abs/2310.19909)

    本研究通过对多种预训练模型进行大规模比较，帮助从业者更好地选择骨干网络，从而提升计算机视觉系统的性能和研究进展的方向。

    

    基于神经网络的计算机视觉系统通常由一个骨干网络构成，即预训练或随机初始化的特征提取器。几年前，使用ImageNet训练的卷积神经网络是默认选择。然而，最近的发展出现了使用各种算法和数据集进行预训练的众多骨干网络。虽然这种选择丰富性提高了一系列系统的性能，但对于从业者来说，很难做出明智的选择。骨干网络之战（BoB）通过对多种预训练模型进行基准测试，包括视觉-语言模型、自监督学习训练的模型和稳定扩散骨干等等，以及针对从分类到目标检测到OOD泛化等多样的计算机视觉任务进行了比较，从而简化了这个选择过程。此外，BoB通过揭示有希望的研究方向，为研究社区推动计算机视觉进一步发展提供了启示。

    Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating
    
[^81]: 可解释的基于原型的图信息瓶颈

    Interpretable Prototype-based Graph Information Bottleneck. (arXiv:2310.19906v1 [cs.LG])

    [http://arxiv.org/abs/2310.19906](http://arxiv.org/abs/2310.19906)

    这项工作提出了一种新颖的可解释的GNN框架，通过在信息瓶颈框架中将原型学习与输入图的关键子图相结合，为模型的解释能力和性能提供了改进。

    

    图神经网络（GNN）的成功导致了对其决策过程的理解和对其预测的解释的需求，这催生了可解释的人工智能（XAI），为黑盒模型提供透明的解释。最近，原型的使用成功提高了模型的可解释性，通过学习原型来暗示影响预测的训练图。然而，这些方法往往会给原型提供来自整个图的过多信息，导致关键子结构的排除或无关子结构的包含，这可以限制模型在下游任务中的解释能力和性能。在这项工作中，我们提出了一种新颖的可解释的GNN框架，称为解释性的基于原型的图信息瓶颈 (PGIB)，将原型学习纳入信息瓶颈框架，为原型提供输入图的关键子图。

    The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph
    
[^82]: MIST: 具有卷积注意力混合（CAM）解码器的医学图像分割Transformer

    MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder. (arXiv:2310.19898v1 [cs.CV])

    [http://arxiv.org/abs/2310.19898](http://arxiv.org/abs/2310.19898)

    提出了一种医学图像分割Transformer（MIST），利用Convolutional Attention Mixing（CAM）解码器解决了Transformer在多模态尺寸中捕捉像素局部上下文的局限性。

    

    医学图像分割常用的深度学习方法之一是Transformer，通过利用自注意力机制，可以捕捉像素之间的长程依赖关系。尽管Transformer在医学图像分割方面取得了成功，但在多模态尺寸中捕捉像素的局部上下文方面存在局限性。我们提出了一种医学图像分割Transformer（MIST），其中包含了一种新颖的卷积注意力混合（CAM）解码器，以解决此问题。MIST由两部分组成：首先使用预训练的多轴视觉Transformer（MaxViT）作为编码器，然后通过CAM解码器对编码的特征表示进行图像分割。在CAM解码器中，引入了一个注意力混合器，结合了多头自注意力、空间注意力和压缩激励注意力模块，以捕捉所有空间维度中的长程依赖关系。此外，为了增强空间信息的获取，还使用了深层和浅层卷积。

    One of the common and promising deep learning approaches used for medical image segmentation is transformers, as they can capture long-range dependencies among the pixels by utilizing self-attention. Despite being successful in medical image segmentation, transformers face limitations in capturing local contexts of pixels in multimodal dimensions. We propose a Medical Image Segmentation Transformer (MIST) incorporating a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST has two parts: a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for
    
[^83]: 探索视觉模型中的盲点几何

    Exploring Geometry of Blind Spots in Vision Models. (arXiv:2310.19889v1 [cs.CV])

    [http://arxiv.org/abs/2310.19889](http://arxiv.org/abs/2310.19889)

    本研究探索了在视觉模型中存在的不敏感现象，并提出了一种通过研究网络的等置信度级别集合的几何形状和范围的技术。通过提出的级别集遍历算法，可以找到与给定源图像相似但属于相同等置信度级别集的输入图像。

    

    尽管深度神经网络在各种应用中取得了显著的成功，但一些研究表明它们对接近无法察觉的扰动非常敏感，即所谓的对抗攻击。另一方面，先前的研究还观察到，深度网络也可能出现对输入空间中的大幅扰动不敏感的情况，而这并不会导致网络激活发生明显改变。在本研究中，我们详细研究了在CNNs和Transformers等视觉模型中的不敏感现象，并提出了研究这些网络“等置信度”级别集合的几何形状和范围的技术。我们提出了一种级别集遍历算法，通过使用局部梯度的正交分量来迭代地探索与输入空间中高置信度区域。给定一个源图像，我们使用该算法来识别与源图像属于相同等置信度级别集的输入，尽管它们在感知上与任意图像相似。

    Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of "equi-confidence" level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary image
    
[^84]: BTRec: 基于BERT的个性化旅游轨迹推荐

    BTRec: BERT-Based Trajectory Recommendation for Personalized Tours. (arXiv:2310.19886v1 [cs.LG])

    [http://arxiv.org/abs/2310.19886](http://arxiv.org/abs/2310.19886)

    BTRec是一种基于BERT的轨迹推荐算法，它利用用户的人口统计信息和过去的POI访问信息，通过修改后的BERT语言模型生成个性化的POI行程预测，从而为旅游者提供有针对性的推荐行程。

    

    对于旅游者来说，拥有一个精心规划的行程和相关推荐是度过愉快假期的关键，尤其是当他们访问陌生城市时。许多旅游推荐工具只考虑了有限的因素，如热门景点和路径限制。因此，它们提供的解决方案可能不总是与系统的个体用户保持一致。本文提出了一种名为BTREC（基于BERT的轨迹推荐）的迭代算法，它从POIBERT嵌入算法扩展到使用BERT框架推荐个性化POI行程。我们的BTREC算法将用户的人口统计信息和过去的POI访问信息合并到修改后的BERT语言模型中，以给出一对出发地和目的地POI的个性化POI行程预测。我们的推荐系统可以创建一个最大化访问POI的旅行行程，同时考虑用户的偏好。

    An essential task for tourists having a pleasant holiday is to have a well-planned itinerary with relevant recommendations, especially when visiting unfamiliar cities. Many tour recommendation tools only take into account a limited number of factors, such as popular Points of Interest (POIs) and routing constraints. Consequently, the solutions they provide may not always align with the individual users of the system. We propose an iterative algorithm in this paper, namely: BTREC (BERT-based Trajectory Recommendation), that extends from the POIBERT embedding algorithm to recommend personalized itineraries on POIs using the BERT framework. Our BTREC algorithm incorporates users' demographic information alongside past POI visits into a modified BERT language model to recommend a personalized POI itinerary prediction given a pair of source and destination POIs. Our recommendation system can create a travel itinerary that maximizes POIs visited, while also taking into account user preferenc
    
[^85]: 学习有界门复杂度的量子态和酉算符

    Learning quantum states and unitaries of bounded gate complexity. (arXiv:2310.19882v1 [quant-ph])

    [http://arxiv.org/abs/2310.19882](http://arxiv.org/abs/2310.19882)

    本文证明了学习具有有界门复杂度的量子态和酉算符的样本复杂度与相应的门复杂度线性相关，并且在计算复杂度上存在指数关系，这一结果限制了量子机器学习模型的表达能力。

    

    尽管量子状态重构非常困难，但对于实际应用中的重构者来说，大多数状态的兴趣不大。鉴于自然界中出现的状态和酉算符都具有有界的门复杂度，自然而然地想问是否可以实现高效的学习。在这项工作中，我们证明了为了将由$G$个两量子比特门生成的状态学习到小的迹距离，需要和充分的样本复杂度与$G$线性比例。我们还证明了学习由$G$个门生成的酉算符到小的平均误差的最优查询复杂度与$G$线性比例。虽然可以实现样本高效的学习，但我们展示了在合理的密码学猜想下，学习门复杂度为$G$的状态和酉算符的计算复杂度必须与$G$指数比例。我们阐明了这些结果如何确定了量子机器学习模型的表达能力的基本限制，并提供了对无免费午餐定理的新视角。

    While quantum state tomography is notoriously hard, most states hold little interest to practically-minded tomographers. Given that states and unitaries appearing in Nature are of bounded gate complexity, it is natural to ask if efficient learning becomes possible. In this work, we prove that to learn a state generated by a quantum circuit with $G$ two-qubit gates to a small trace distance, a sample complexity scaling linearly in $G$ is necessary and sufficient. We also prove that the optimal query complexity to learn a unitary generated by $G$ gates to a small average-case error scales linearly in $G$. While sample-efficient learning can be achieved, we show that under reasonable cryptographic conjectures, the computational complexity for learning states and unitaries of gate complexity $G$ must scale exponentially in $G$. We illustrate how these results establish fundamental limitations on the expressivity of quantum machine learning models and provide new perspectives on no-free-lun
    
[^86]: 用神经网络实现的度量流

    Metric Flows with Neural Networks. (arXiv:2310.19870v1 [hep-th])

    [http://arxiv.org/abs/2310.19870](http://arxiv.org/abs/2310.19870)

    本论文开发了一种基于神经网络梯度下降的度量流理论，实现了在黎曼度量空间中的流动。其应用于数值Calabi-Yau度量，并探讨了特征学习的重要性。

    

    我们发展了一种由神经网络梯度下降诱导的黎曼度量空间中流动的理论。这部分是受到近期用神经网络逼近Calabi-Yau度量的进展的推动，也是由于对神经网络空间中流动的理解的最新进展的能力。我们推导了相应的度量流动方程，其由度量神经切向核定义，这是一个复杂的非局部对象，会随时间演化。然而，许多结构在无穷宽度极限下核将变得固定且动态简化。附加假设可导致流动中的局部性，使得我们能够实现Perelman关于解决3D Poincaré猜想中使用的Ricci流的形式化。我们将这些思想应用于数值Calabi-Yau度量，包括关于特征学习重要性的讨论。

    We develop a theory of flows in the space of Riemannian metrics induced by neural network gradient descent. This is motivated in part by recent advances in approximating Calabi-Yau metrics with neural networks and is enabled by recent advances in understanding flows in the space of neural networks. We derive the corresponding metric flow equations, which are governed by a metric neural tangent kernel, a complicated, non-local object that evolves in time. However, many architectures admit an infinite-width limit in which the kernel becomes fixed and the dynamics simplify. Additional assumptions can induce locality in the flow, which allows for the realization of Perelman's formulation of Ricci flow that was used to resolve the 3d Poincar\'e conjecture. We apply these ideas to numerical Calabi-Yau metrics, including a discussion on the importance of feature learning.
    
[^87]: 后验采样在竞争性强化学习中的应用：函数近似和部分观测

    Posterior Sampling for Competitive RL: Function Approximation and Partial Observation. (arXiv:2310.19861v1 [cs.LG])

    [http://arxiv.org/abs/2310.19861](http://arxiv.org/abs/2310.19861)

    本文研究了使用后验采样算法在竞争性强化学习中的应用。通过引入自对弈和对抗性广义艾略特系数，提出了用于探索-利用平衡的模型方法，并且成功处理了状态的部分可观测性。同时，提出了学习具有潜在部分可观测性的对抗性博弈模型的后验采样方法，并给出了低遗憾界限。

    

    本文研究了在一般函数近似的背景下，用于竞争性强化学习的后验采样算法。针对零和马尔可夫博弈中的自对弈和对抗学习两个关键情景，我们首先提出了自对弈和对抗性广义艾略特系数(GEC)作为函数近似的复杂度度量，并捕捉博弈中的探索-利用平衡。基于自对弈GEC，我们提出了一种基于模型的自对弈后验采样方法，以控制两个玩家学习纳什均衡，可以成功处理状态的部分可观测性。此外，我们确定了一套与对手的对抗策略相适应的部分可观测博弈模型。结合对抗GEC，我们提出了一种基于模型的后验采样方法，用于学习具有潜在部分可观测性的对抗性博弈模型。我们进一步为所提出的算法提供了低遗憾界限。

    This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms
    
[^88]: 节点属性随机块模型的精确恢复与Bregman硬聚类

    Exact Recovery and Bregman Hard Clustering of Node-Attributed Stochastic Block Model. (arXiv:2310.19854v1 [cs.SI])

    [http://arxiv.org/abs/2310.19854](http://arxiv.org/abs/2310.19854)

    该论文介绍了一种精确恢复节点属性随机块模型的聚类算法，该算法利用网络信息和节点属性信息交换，实现更可靠的网络信息需要更少可靠的属性信息的精确恢复。

    

    网络聚类解决了识别具有相似连接模式的节点集（社区）的问题。然而，在许多情况下，节点还具有与聚类结构相关的属性。因此，可以联合利用网络信息（边）和节点信息（属性）来设计高性能的聚类算法。在网络和节点属性的一般模型下，该工作建立了一个信息论准则，用于精确恢复社区标签，并确定了由模型的Chernoff-Hellinger散度确定的相变。这个准则显示了网络和属性信息如何交换，以实现精确恢复（例如，更可靠的网络信息需要更不可靠的属性信息）。该工作还提出了一种迭代聚类算法，最大化联合似然，假设网络交互和节点属性的概率分布。

    Network clustering tackles the problem of identifying sets of nodes (communities) that have similar connection patterns. However, in many scenarios, nodes also have attributes that are correlated with the clustering structure. Thus, network information (edges) and node information (attributes) can be jointly leveraged to design high-performance clustering algorithms. Under a general model for the network and node attributes, this work establishes an information-theoretic criterion for the exact recovery of community labels and characterizes a phase transition determined by the Chernoff-Hellinger divergence of the model. The criterion shows how network and attribute information can be exchanged in order to have exact recovery (e.g., more reliable network information requires less reliable attribute information). This work also presents an iterative clustering algorithm that maximizes the joint likelihood, assuming that the probability distribution of network interactions and node attrib
    
[^89]: 通过侧链扩散概率模型预测蛋白质相互作用的突变效应

    Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model. (arXiv:2310.19849v1 [q-bio.BM])

    [http://arxiv.org/abs/2310.19849](http://arxiv.org/abs/2310.19849)

    本文提出了一种基于表示学习的方法来预测蛋白质相互作用中氨基酸突变的效应。该方法利用黎曼扩散模型学习侧链构象的生成过程，并可以给出蛋白质相互作用界面上突变的结构上下文表示。通过利用学到的表示，实现了在预测蛋白质相互作用的突变效应方面的最先进性能。

    

    很多关键生物过程依赖于蛋白质相互作用网络。预测氨基酸突变对蛋白质相互作用的影响对于蛋白质工程和药物发现至关重要。然而，关于结合能的实验数据的稀缺性给基于深度学习的计算方法带来了重大挑战。在这项工作中，我们提出了一种基于表示学习的方法——SidechainDiff，它利用未标记的实验蛋白质结构。SidechainDiff利用黎曼扩散模型学习侧链构象的生成过程，并可以给出蛋白质相互作用界面上突变的结构上下文表示。通过利用学到的表示，我们在预测蛋白质相互作用的突变效应方面取得了最先进的性能。此外，SidechainDiff是第一个基于扩散的侧链生成模型，可以区分不同的侧链。

    Many crucial biological processes rely on networks of protein-protein interactions. Predicting the effect of amino acid mutations on protein-protein binding is vital in protein engineering and therapeutic discovery. However, the scarcity of annotated experimental data on binding energy poses a significant challenge for developing computational approaches, particularly deep learning-based methods. In this work, we propose SidechainDiff, a representation learning-based approach that leverages unlabelled experimental protein structures. SidechainDiff utilizes a Riemannian diffusion model to learn the generative process of side-chain conformations and can also give the structural context representations of mutations on the protein-protein interface. Leveraging the learned representations, we achieve state-of-the-art performance in predicting the mutational effects on protein-protein binding. Furthermore, SidechainDiff is the first diffusion-based generative model for side-chains, distingui
    
[^90]: 连续时间模型驱动的高效探索在强化学习中

    Efficient Exploration in Continuous-time Model-based Reinforcement Learning. (arXiv:2310.19848v1 [cs.LG])

    [http://arxiv.org/abs/2310.19848](http://arxiv.org/abs/2310.19848)

    连续时间模型驱动的强化学习算法使用非线性常微分方程（ODEs）表示连续时间动态，并通过使用乐观原则进行探索来降低遗憾，同时提出了一种自适应、数据相关的测量选择策略，具有更少的样本和次线性遗憾。

    

    强化学习算法通常考虑离散时间动态，即使底层系统通常是连续时间的。本文介绍了一种模型驱动的强化学习算法，使用非线性常微分方程（ODE）表示连续时间动态。我们使用良好校准的概率模型来捕捉认识不确定性，并使用乐观原则进行探索。我们的遗憾界表明了测量选择策略（MSS）的重要性，因为在连续时间中，我们不仅必须决定如何进行探索，还必须决定何时观察底层系统。我们的分析证明，使用高斯过程（GP）对ODE进行建模，并选择常见的MSS，如等距采样，遗憾是次线性的。此外，我们提出了一种自适应的、数据相关的实用MSS，在与GP动力学相结合时，也能以更少的样本实现次线性遗憾。我们展示了协同作用的好处。

    Reinforcement learning algorithms typically consider discrete-time dynamics, even though the underlying systems are often continuous in time. In this paper, we introduce a model-based reinforcement learning algorithm that represents continuous-time dynamics using nonlinear ordinary differential equations (ODEs). We capture epistemic uncertainty using well-calibrated probabilistic models, and use the optimistic principle for exploration. Our regret bounds surface the importance of the measurement selection strategy(MSS), since in continuous time we not only must decide how to explore, but also when to observe the underlying system. Our analysis demonstrates that the regret is sublinear when modeling ODEs with Gaussian Processes (GP) for common choices of MSS, such as equidistant sampling. Additionally, we propose an adaptive, data-dependent, practical MSS that, when combined with GP dynamics, also achieves sublinear regret with significantly fewer samples. We showcase the benefits of co
    
[^91]: 修改的遗传算法用于特征选择和超参数优化：以XGBoost在垃圾邮件预测中为例

    Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction. (arXiv:2310.19845v1 [cs.LG])

    [http://arxiv.org/abs/2310.19845](http://arxiv.org/abs/2310.19845)

    本文提出了一种修改的遗传算法，用于同时降低维度和优化超参数，在不平衡数据集上进行垃圾邮件预测。实验结果表明，该模型在几何平均和准确率上表现良好。

    

    近期，在在线社交网络上的垃圾邮件问题引起了研究界和商业界的关注。Twitter已成为传播垃圾邮件内容的首选媒介。许多研究努力试图应对社交网络垃圾邮件。Twitter带来了额外的挑战，包括特征空间的大小和不平衡的数据分布。通常，相关研究工作关注其中的一部分主要挑战，或者产生黑盒模型。在本文中，我们提出了一种修改的遗传算法，用于同时降低维度和优化超参数在不平衡数据集上。该算法初始化了一个eXtreme Gradient Boosting分类器，并减少了推文数据集的特征空间，以生成一个垃圾邮件预测模型。该模型使用50次重复的10倍分层交叉验证进行验证，并使用非参数统计检验进行分析。结果预测模型在几何平均和准确率上平均达到82.32％和92.67％。

    Recently, spam on online social networks has attracted attention in the research and business world. Twitter has become the preferred medium to spread spam content. Many research efforts attempted to encounter social networks spam. Twitter brought extra challenges represented by the feature space size, and imbalanced data distributions. Usually, the related research works focus on part of these main challenges or produce black-box models. In this paper, we propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization over imbalanced datasets. The algorithm initialized an eXtreme Gradient Boosting classifier and reduced the features space of tweets dataset; to generate a spam prediction model. The model is validated using a 50 times repeated 10-fold stratified cross-validation, and analyzed using nonparametric statistical tests. The resulted prediction model attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy r
    
[^92]: 使用遗传算法和极限增强模型对电话营销过程进行建模：特征选择和成本敏感的分析方法

    Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach. (arXiv:2310.19843v1 [cs.LG])

    [http://arxiv.org/abs/2310.19843](http://arxiv.org/abs/2310.19843)

    本研究提出了一个使用遗传算法和极限增强模型进行电话营销过程建模的方法，其中包括特征选择和成本敏感的分析方法。研究通过利用电话营销数据和社会经济指标对客户意愿进行建模，并构建出可解释的预测模型。

    

    目前，几乎所有的直接营销活动都是通过虚拟方式而不是面对面进行的，这加快了人际交往技巧的衰退速度。此外，企业一直在努力感知和促进客户接受营销提案的倾向。数字转型和增加的虚拟存在迫使企业寻求新的营销研究方法。本研究旨在利用电话营销数据建模客户办理定期存款的意愿，并找出客户的最重要特征。使用葡萄牙银行的真实数据和国家社会经济指标来建模电话营销决策过程。本研究提供了两个重要的贡献。首先，提出了一种基于遗传算法的分类器，可以同时选择最佳的区分特征和调整分类器参数。其次，构建了一个可解释的预测模型。

    Currently, almost all direct marketing activities take place virtually rather than in person, weakening interpersonal skills at an alarming pace. Furthermore, businesses have been striving to sense and foster the tendency of their clients to accept a marketing offer. The digital transformation and the increased virtual presence forced firms to seek novel marketing research approaches. This research aims at leveraging the power of telemarketing data in modeling the willingness of clients to make a term deposit and finding the most significant characteristics of the clients. Real-world data from a Portuguese bank and national socio-economic metrics are used to model the telemarketing decision-making process. This research makes two key contributions. First, propose a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously. Second, build an explainable prediction model. The best-generated classification models were 
    
[^93]: 音乐形式生成

    Musical Form Generation. (arXiv:2310.19842v1 [cs.SD])

    [http://arxiv.org/abs/2310.19842](http://arxiv.org/abs/2310.19842)

    本论文介绍了一种生成结构化、任意长音乐作品的方法，通过使用条件生成模型创建音乐片段，并利用大型语言模型来建议音乐的形式。

    

    尽管最近的生成模型可以产生引人入胜的音乐，但它们的实用性有限。音乐中的变化通常是随机的，导致作品缺乏结构。超过一分钟的作品可能变得不连贯或重复。本文介绍了一种生成结构化、任意长音乐作品的方法。该方法的核心是使用条件生成模型创建音乐片段，并在这些片段之间进行过渡。确定音乐的高层构成的生成与创建细节的低层次细节的生成是不同的。然后使用大型语言模型来建议音乐的形式。

    While recent generative models can produce engaging music, their utility is limited. The variation in the music is often left to chance, resulting in compositions that lack structure. Pieces extending beyond a minute can become incoherent or repetitive. This paper introduces an approach for generating structured, arbitrarily long musical pieces. Central to this approach is the creation of musical segments using a conditional generative model, with transitions between these segments. The generation of prompts that determine the high-level composition is distinct from the creation of finer, lower-level details. A large language model is then used to suggest the musical form.
    
[^94]: 可解释的聚类方法在安全氛围分析中的应用：研究司机群体在安全氛围感知中的差异

    An interpretable clustering approach to safety climate analysis: examining driver group distinction in safety climate perceptions. (arXiv:2310.19841v1 [cs.LG])

    [http://arxiv.org/abs/2310.19841](http://arxiv.org/abs/2310.19841)

    该研究引入了可解释的聚类方法来分析安全氛围，通过聚类司机群体的安全氛围感知，可以更好地了解组织的员工特点，并制定更有效的干预措施。

    

    运输行业，特别是卡车部门，容易发生工作场所事故和死亡。涉及大型卡车的事故占到了整体交通事故死亡人数的相当比例。认识到安全氛围在事故预防中的关键作用，研究人员努力了解其因素并在组织中测量其影响。虽然现有的数据驱动型安全氛围研究取得了显著进展，但基于安全氛围感知对员工进行聚类的方法是创新的，并且尚未在研究中广泛应用。根据司机的安全氛围感知来识别不同的聚类群体，可以帮助组织对其员工进行画像，并制定更有影响力的干预措施。没有广泛应用聚类方法可能是因为难以解释影响员工聚类成员资格的因素。此外，现有的安全相关研究并未比较多个聚类算法，

    The transportation industry, particularly the trucking sector, is prone to workplace accidents and fatalities. Accidents involving large trucks accounted for a considerable percentage of overall traffic fatalities. Recognizing the crucial role of safety climate in accident prevention, researchers have sought to understand its factors and measure its impact within organizations. While existing data-driven safety climate studies have made remarkable progress, clustering employees based on their safety climate perception is innovative and has not been extensively utilized in research. Identifying clusters of drivers based on their safety climate perception allows the organization to profile its workforce and devise more impactful interventions. The lack of utilizing the clustering approach could be due to difficulties interpreting or explaining the factors influencing employees' cluster membership. Moreover, existing safety-related studies did not compare multiple clustering algorithms, r
    
[^95]: CrossEAI: 使用可解释的人工智能生成更好的胸部X射线图像边界框

    CrossEAI: Using Explainable AI to generate better bounding boxes for Chest X-ray images. (arXiv:2310.19835v1 [eess.IV])

    [http://arxiv.org/abs/2310.19835](http://arxiv.org/abs/2310.19835)

    这篇论文利用可解释的人工智能方法生成胸部X射线图像的边界框，提供了对医学成像诊断的解释，并试图改进现有方法中边界框生成的问题。

    

    可解释性对于医疗保健中的深度学习应用非常重要，这些应用根据法律法规和责任要求向患者和医生提供解释。可解释的人工智能方法，如使用集成梯度的特征重要性、使用LIME的模型逼近，或者使用神经元激活和层导电来提供对某些健康风险预测的解释。在医学成像诊断中，疾病分类通常能够达到很高的准确性，但生成的边界框的交并比（IoU）要低得多。已经提出了不同的自监督或半监督学习策略的方法，但在边界框生成方面很少有改进。以前的工作表明，通过这些方法生成的边界框通常比真实值更大，并包含主要的非疾病区域。本文利用事后可解释的人工智能方法，为胸部X射线图像生成边界框。

    Explainability is critical for deep learning applications in healthcare which are mandated to provide interpretations to both patients and doctors according to legal regulations and responsibilities. Explainable AI methods, such as feature importance using integrated gradients, model approximation using LIME, or neuron activation and layer conductance to provide interpretations for certain health risk predictions. In medical imaging diagnosis, disease classification usually achieves high accuracy, but generated bounding boxes have much lower Intersection over Union (IoU). Different methods with self-supervised or semi-supervised learning strategies have been proposed, but few improvements have been identified for bounding box generation. Previous work shows that bounding boxes generated by these methods are usually larger than ground truth and contain major non-disease area. This paper utilizes the advantages of post-hoc AI explainable methods to generate bounding boxes for chest x-ray
    
[^96]: 模仿解释：通过可解释的策略学习理解决策

    Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning. (arXiv:2310.19831v1 [stat.ML])

    [http://arxiv.org/abs/2310.19831](http://arxiv.org/abs/2310.19831)

    本文提出了一种新的模型基于的贝叶斯方法，通过可解释的策略学习来理解决策，该方法具有透明度、适应部分可观测性和完全离线运行的特点。通过对阿尔茨海默病诊断问题的实验验证，展示了该方法作为审计和分析工具的潜力。

    

    从观察数据中理解人类行为对于透明度和决策的问责是至关重要的。在现实世界中，如医疗保健领域，建模决策者的策略具有挑战性——没有访问底层状态的权限，没有了解环境动态的知识，也没有进行实时实验的容错能力。我们希望学习一个数据驱动的决策行为表示，它具有（1）设计上的透明度，（2）适应部分可观测性，（3）完全离线运行。为了满足这些关键条件，我们提出了一种新颖的基于模型的贝叶斯方法来进行可解释的策略学习（"Interpole"），它同时估计一个代理人的（可能有偏差的）置信更新过程以及他们（可能次优的）信念动作映射。通过对模拟和真实世界的阿尔茨海默病诊断问题的实验，我们展示了我们的方法作为审计和分析工具的潜力。

    Understanding human behavior from observed data is critical for transparency and accountability in decision-making. Consider real-world settings such as healthcare, in which modeling a decision-maker's policy is challenging -- with no access to underlying states, no knowledge of environment dynamics, and no allowance for live experimentation. We desire learning a data-driven representation of decision-making behavior that (1) inheres transparency by design, (2) accommodates partial observability, and (3) operates completely offline. To satisfy these key criteria, we propose a novel model-based Bayesian method for interpretable policy learning ("Interpole") that jointly estimates an agent's (possibly biased) belief-update process together with their (possibly suboptimal) belief-action mapping. Through experiments on both simulated and real-world data for the problem of Alzheimer's disease diagnosis, we illustrate the potential of our approach as an investigative device for auditing, qua
    
[^97]: FuXi-Extreme: 使用扩散模型改进极端降雨和风暴预报

    FuXi-Extreme: Improving extreme rainfall and wind forecasts with diffusion model. (arXiv:2310.19822v1 [cs.LG])

    [http://arxiv.org/abs/2310.19822](http://arxiv.org/abs/2310.19822)

    FuXi-Extreme模型通过使用去噪扩散概率模型（DDPM）来恢复精细尺度的细节，提高了极端降雨和风暴预报的性能。

    

    机器学习（ML）模型在天气预报方面取得了显著的进展，FuXi等最先进的基于ML的天气预报模型展示了优越的统计预报性能。然而，ML模型面临一个共同的挑战：随着预报时间的增加，它们往往会生成越来越平滑的预测结果，导致对极端天气事件的强度低估。为了解决这个挑战，我们开发了FuXi-Extreme模型，该模型使用去噪扩散概率模型（DDPM）来恢复由FuXi模型在5天预报中生成的表面预报数据中的更细节尺度的细节。对极端降水（$\textrm{TP}$）、10米风速（$\textrm{WS10}$）和2米温度（$\textrm{T2M}$）的评估说明了其优越的性能。

    Significant advancements in the development of machine learning (ML) models for weather forecasting have produced remarkable results. State-of-the-art ML-based weather forecast models, such as FuXi, have demonstrated superior statistical forecast performance in comparison to the high-resolution forecasts (HRES) of the European Centre for Medium-Range Weather Forecasts (ECMWF). However, ML models face a common challenge: as forecast lead times increase, they tend to generate increasingly smooth predictions, leading to an underestimation of the intensity of extreme weather events. To address this challenge, we developed the FuXi-Extreme model, which employs a denoising diffusion probabilistic model (DDPM) to restore finer-scale details in the surface forecast data generated by the FuXi model in 5-day forecasts. An evaluation of extreme total precipitation ($\textrm{TP}$), 10-meter wind speed ($\textrm{WS10}$), and 2-meter temperature ($\textrm{T2M}$) illustrates the superior performance 
    
[^98]: 一种面向非平稳随机多臂老虎机的风险规避框架

    A Risk-Averse Framework for Non-Stationary Stochastic Multi-Armed Bandits. (arXiv:2310.19821v1 [cs.LG])

    [http://arxiv.org/abs/2310.19821](http://arxiv.org/abs/2310.19821)

    我们提出了一种适用于非平稳环境的自适应风险感知策略框架，通过结合各种风险度量和重启贝叶斯在线变点检测算法，解决了高波动性领域中简单奖励最大化方法的不可靠性问题。

    

    在典型的随机多臂老虎机问题中，目标通常是在一定的时间范围内最大化预期奖励总和。然而，当提供额外的环境特定知识时，选择一个能够达到最优的策略不再适用。尤其是在医疗或金融等高波动性领域，简单的奖励最大化方法往往不能准确捕捉学习问题的复杂性，导致不可靠的解决方案。为解决这类问题，我们提出了一种自适应风险感知策略的框架，用于在非平稳环境中操作。我们的框架结合了文献中普遍存在的各种风险度量，将多个多臂老虎机算法族映射到风险敏感设置中。此外，我们将结果算法配备了重启贝叶斯在线变点检测（R-BOCPD）算法，并施加了（可调节的）强制终止机制。

    In a typical stochastic multi-armed bandit problem, the objective is often to maximize the expected sum of rewards over some time horizon $T$. While the choice of a strategy that accomplishes that is optimal with no additional information, it is no longer the case when provided additional environment-specific knowledge. In particular, in areas of high volatility like healthcare or finance, a naive reward maximization approach often does not accurately capture the complexity of the learning problem and results in unreliable solutions. To tackle problems of this nature, we propose a framework of adaptive risk-aware strategies that operate in non-stationary environments. Our framework incorporates various risk measures prevalent in the literature to map multiple families of multi-armed bandit algorithms into a risk-sensitive setting. In addition, we equip the resulting algorithms with the Restarted Bayesian Online Change-Point Detection (R-BOCPD) algorithm and impose a (tunable) forced ex
    
[^99]: NetDistiller: 通过原地蒸馏增强微型深度学习

    NetDistiller: Empowering Tiny Deep Learning via In-Situ Distillation. (arXiv:2310.19820v1 [cs.LG])

    [http://arxiv.org/abs/2310.19820](http://arxiv.org/abs/2310.19820)

    NetDistiller提出了一个框架，通过将微型神经网络作为权重共享教师模型的子网络，通过联合训练和不确定性-aware的蒸馏来提高微型神经网络的准确性。

    

    在边缘设备上部署微型神经网络 (TNNs) 面临的一个基本挑战是提高任务准确性，这些设备在内存、计算、带宽和电源方面都有严格的限制。为了解决这个问题，我们提出了一个名为NetDistiller的框架，通过将TNNs看作是通过扩展TNN通道数而构造的权重共享教师模型的子网络，提高TNNs的可实现准确性。具体来说，目标TNN模型与权重共享教师模型一起进行联合训练，方法包括(1)通过梯度手术解决它们之间的梯度冲突，以及(2)通过考虑不确定性的蒸馏来减轻教师模型的过拟合。广泛的实验验证了NetDistiller在提高TNNs可实现准确性方面的有效性，超过了现有方法的表现。我们的代码可在https://github.com/GATECH-EIC/NetDistiller找到。

    Boosting the task accuracy of tiny neural networks (TNNs) has become a fundamental challenge for enabling the deployments of TNNs on edge devices which are constrained by strict limitations in terms of memory, computation, bandwidth, and power supply. To this end, we propose a framework called NetDistiller to boost the achievable accuracy of TNNs by treating them as sub-networks of a weight-sharing teacher constructed by expanding the number of channels of the TNN. Specifically, the target TNN model is jointly trained with the weight-sharing teacher model via (1) gradient surgery to tackle the gradient conflicts between them and (2) uncertainty-aware distillation to mitigate the overfitting of the teacher model. Extensive experiments across diverse tasks validate NetDistiller's effectiveness in boosting TNNs' achievable accuracy over state-of-the-art methods. Our code is available at https://github.com/GATECH-EIC/NetDistiller.
    
[^100]: 机器学习和知识：为什么鲁棒性很重要

    Machine Learning and Knowledge: Why Robustness Matters. (arXiv:2310.19819v1 [cs.LG])

    [http://arxiv.org/abs/2310.19819](http://arxiv.org/abs/2310.19819)

    机器学习算法的鲁棒性对于信任和知识的形成至关重要，只有在正确特征下跨情景良好运行的算法才能提供可靠的知识。

    

    信任机器学习算法需要对其输出有信心。信心通常以模型的可靠性来解释，即模型产生高比例正确输出时可靠。然而，模型可靠性不能解决机器学习模型鲁棒性的担忧，例如模型依赖错误特征或性能在不同情境下的变化。我认为，对信任的认识维度可以通过知识的概念来理解，算法的可信度取决于其用户是否能够确认其输出的正确性。知识要求信念基于正确的原因形成，并且对错误具有鲁棒性，因此机器学习算法只有在跨反事实情景中良好运行，并基于正确特征做出决策时，才能提供知识。我认为，这可以解释为什么我们应该关心像可解释性这样的模型属性。

    Trusting machine learning algorithms requires having confidence in their outputs. Confidence is typically interpreted in terms of model reliability, where a model is reliable if it produces a high proportion of correct outputs. However, model reliability does not address concerns about the robustness of machine learning models, such as models relying on the wrong features or variations in performance based on context. I argue that the epistemic dimension of trust can instead be understood through the concept of knowledge, where the trustworthiness of an algorithm depends on whether its users are in the position to know that its outputs are correct. Knowledge requires beliefs to be formed for the right reasons and to be robust to error, so machine learning algorithms can only provide knowledge if they work well across counterfactual scenarios and if they make decisions based on the right features. This, I argue, can explain why we should care about model properties like interpretability
    
[^101]: 基于SVBRDF提取模型的GPU性能评估

    Benchmarking GPUs on SVBRDF Extractor Model. (arXiv:2310.19816v1 [cs.PF])

    [http://arxiv.org/abs/2310.19816](http://arxiv.org/abs/2310.19816)

    本文针对操作更大输入图像（256x256）的神经网络模型，尝试区分不同GPU的性能。为了用户选择合适的GPU以实现特定任务的最佳性能提供了参考。

    

    随着深度学习的成熟，其在各个领域的应用愈发广泛。同时，市场上不同类型的GPU也越来越多，这为用户选择合适的GPU带来了困扰。用户如何选择GPU以实现特定任务的最佳性能呢？GPU架构的分析已经得到了广泛研究，但现有的GPU性能评估工作并未研究对于处理更大输入图像（256x256）的神经网络模型的任务。本研究旨在区分不同GPU在操作较大输入图像的神经网络模型上的性能差异。

    With the maturity of deep learning, its use is emerging in every field. Also, as different types of GPUs are becoming more available in the markets, it creates a difficult decision for users. How can users select GPUs to achieve optimal performance for a specific task? Analysis of GPU architecture is well studied, but existing works that benchmark GPUs do not study tasks for networks with significantly larger input. In this work, we tried to differentiate the performance of different GPUs on neural network models that operate on bigger input images (256x256).
    
[^102]: 训练无需浮点精度的二进制神经网络

    Training binary neural networks without floating point precision. (arXiv:2310.19815v1 [cs.LG])

    [http://arxiv.org/abs/2310.19815](http://arxiv.org/abs/2310.19815)

    本研究提出了两种解决方案，通过拓扑变化和策略训练，实现了高效训练的二进制神经网络，具有低延迟和低能耗的特点。

    

    本研究的主要目标是提高训练二进制神经网络的效率，这些网络具有低延迟和低能耗的特点。本研究的主要贡献是提出了两种解决方案，包括拓扑变化和策略训练，使得网络能够达到接近最先进性能和高效的训练。训练所需的时间和内存是促进高效训练的两个因素。

    The main goal of this work is to improve the efficiency of training binary neural networks, which are low latency and low energy networks. The main contribution of this work is the proposal of two solutions comprised of topology changes and strategy training that allow the network to achieve near the state-of-the-art performance and efficient training. The time required for training and the memory required in the process are two factors that contribute to efficient training.
    
[^103]: 学习梯度场用于可扩展和通用的不规则装箱问题

    Learning Gradient Fields for Scalable and Generalizable Irregular Packing. (arXiv:2310.19814v1 [cs.LG])

    [http://arxiv.org/abs/2310.19814](http://arxiv.org/abs/2310.19814)

    本论文提出了一种基于机器学习的方法来解决不规则装箱问题。通过学习梯度场，该方法能够处理对象有效性约束和碰撞避免等挑战，并生成最优的装箱解决方案。

    

    装箱问题在物流、制造、布局设计和图集生成等领域具有多样化的应用。它涉及将不规则形状的物品排列好，以最小化浪费并避免重叠。最近，机器学习，特别是强化学习在解决装箱问题方面显示出潜力。在这项工作中，我们深入研究了一种新颖的基于机器学习的方法，将装箱问题制定为条件生成建模。为了解决不规则装箱的挑战，包括对象有效性约束和碰撞避免，我们的方法使用基于分数的扩散模型来学习一系列梯度场。这些梯度场编码了约束满足与多边形空间关系之间的相关性，从教师案例中学习。在测试阶段，通过学习的梯度场引导，使用细粒度到粗粒度的改进机制生成装箱解决方案。

    The packing problem, also known as cutting or nesting, has diverse applications in logistics, manufacturing, layout design, and atlas generation. It involves arranging irregularly shaped pieces to minimize waste while avoiding overlap. Recent advances in machine learning, particularly reinforcement learning, have shown promise in addressing the packing problem. In this work, we delve deeper into a novel machine learning-based approach that formulates the packing problem as conditional generative modeling. To tackle the challenges of irregular packing, including object validity constraints and collision avoidance, our method employs the score-based diffusion model to learn a series of gradient fields. These gradient fields encode the correlations between constraint satisfaction and the spatial relationships of polygons, learned from teacher examples. During the testing phase, packing solutions are generated using a coarse-to-fine refinement mechanism guided by the learned gradient field
    
[^104]: 利用大型语言模型增强遗传改进突变

    Enhancing Genetic Improvement Mutations Using Large Language Models. (arXiv:2310.19813v1 [cs.SE])

    [http://arxiv.org/abs/2310.19813](http://arxiv.org/abs/2310.19813)

    本文研究了利用大型语言模型作为遗传改进的变异操作符来提高搜索过程，发现使用LLM编辑的补丁通过单元测试的数量高达75％，但相比较标准编辑，LLMs找到的补丁较少多样化。尽管LLM增强的GI找到了许多改进的补丁，但最好的改进补丁是通过标准GI找到的。

    

    大型语言模型（LLMs）已成功应用于软件工程任务，包括程序修复。然而，它们在遗传改进（GI）等基于搜索的技术中的应用仍然很少被探索。在本文中，我们评估了将LLMs作为GI的变异操作符以改进搜索过程的使用。我们扩展了Gin Java GI工具包，以调用OpenAI的API为JCodec工具生成编辑。我们使用5种不同的编辑类型随机抽样编辑空间。我们发现，通过LLM编辑，通过单元测试的补丁数量高于使用标准插入编辑的补丁数量高达75％。此外，我们观察到与标准编辑相比，LLMs找到的补丁通常较少多样化。我们使用局部搜索运行GI以寻找运行时改进。尽管LLM增强的GI找到了许多改进的补丁，但最好的改进补丁是通过标准GI找到的。

    Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as mutation operators for GI to improve the search process. We expand the Gin Java GI toolkit to call OpenAI's API to generate edits for the JCodec tool. We randomly sample the space of edits using 5 different edit types. We find that the number of patches passing unit tests is up to 75% higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits. We ran GI with local search to find runtime improvements. Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.
    
[^105]: 脑解码：走向实时重建视觉知觉

    Brain decoding: toward real-time reconstruction of visual perception. (arXiv:2310.19812v1 [eess.IV])

    [http://arxiv.org/abs/2310.19812](http://arxiv.org/abs/2310.19812)

    本研究提出了一种基于脑磁图（MEG）的脑解码方法，通过训练一个具有预训练嵌入、MEG模块和图像生成器的模型，在实时应用中实现了对视觉知觉的高时间分辨率解码，并在图像检索上取得了7倍的改进。

    

    在过去的五年中，生成式和基础性人工智能系统的使用极大地提高了对大脑活动的解码能力。特别是对于视觉知觉，现在可以从功能性磁共振成像（fMRI）中解码出令人瞩目的准确度。然而，这种神经影像技术的时间分辨率有限（约为0.5 Hz），因此在实时应用方面存在根本性的限制。在这里，我们提出了一种基于脑磁图（MEG）的替代方法，这是一种能够以高时间分辨率（约为5000 Hz）测量脑活动的神经影像设备。为此，我们开发了一个MEG解码模型，该模型通过对比和回归目标进行训练，并由三个模块组成：i）从图像中获得的预训练嵌入、ii）端到端训练的MEG模块以及iii）预训练的图像生成器。我们的结果有三个方面：首先，我们的MEG解码器在经典线性解码器上显示出7倍的图像检索改进。其次，后期脑部

    In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain 
    
[^106]: 数据流的历史背景

    A Historical Context for Data Streams. (arXiv:2310.19811v1 [cs.LG])

    [http://arxiv.org/abs/2310.19811](http://arxiv.org/abs/2310.19811)

    本文回顾了数据流研究的历史背景，并将机器学习对数据流的常见假设放置在其历史背景中。

    

    数据流的机器学习是一个活跃且不断发展的研究领域。关于从数据流中学习的研究通常对与计算资源限制相关的严格假设进行探讨，包括对流挖掘算法每个实例只能检查一次，并且随时准备好给出预测的要求。本文回顾了数据流研究的历史背景，并将机器学习对数据流的常见假设放置在其历史背景中。

    Machine learning from data streams is an active and growing research area. Research on learning from streaming data typically makes strict assumptions linked to computational resource constraints, including requirements for stream mining algorithms to inspect each instance not more than once and be ready to give a prediction at any time. Here we review the historical context of data streams research placing the common assumptions used in machine learning over data streams in their historical context.
    
[^107]: 机器学习在公共汽车运输分析中的优势

    Advantages of Machine Learning in Bus Transport Analysis. (arXiv:2310.19810v1 [cs.LG])

    [http://arxiv.org/abs/2310.19810](http://arxiv.org/abs/2310.19810)

    本研究利用监督机器学习算法分析了影响德黑兰快速巴士系统准点性的因素，并构建了准确的模型来预测公交线路是否符合准时性标准。通过深入研究算法的决策过程，发现了影响公交线路有效性的关键特征。

    

    监督机器学习是一种创新的方法，旨在通过利用过去的经验来模拟人类的学习过程。在本研究中，我们利用监督机器学习算法分析了影响德黑兰快速巴士系统准点性的因素。我们收集了2020年到2022年的德黑兰市政府公开可用的数据集，用于训练和测试我们的模型。通过利用各种算法和Python的Sci Kit Learn和Stats Models库，我们构建了准确的模型，能够预测任何给定日期公交线路是否符合准时性标准。此外，我们深入研究了每个算法的决策过程，以确定它所考虑的最具影响力的因素。这项调查使我们能够发现显著影响公交线路有效性的关键特征，为改进其性能提供了宝贵的见解。

    Supervised Machine Learning is an innovative method that aims to mimic human learning by using past experiences. In this study, we utilize supervised machine learning algorithms to analyze the factors that contribute to the punctuality of Tehran BRT bus system. We gather publicly available datasets of 2020 to 2022 from Municipality of Tehran to train and test our models. By employing various algorithms and leveraging Python's Sci Kit Learn and Stats Models libraries, we construct accurate models capable of predicting whether a bus route will meet the prescribed standards for on-time performance on any given day. Furthermore, we delve deeper into the decision-making process of each algorithm to determine the most influential factor it considers. This investigation allows us to uncover the key feature that significantly impacts the effectiveness of bus routes, providing valuable insights for improving their performance.
    
[^108]: MgNO:通过多重网格有效参数化线性算子

    MgNO: Efficient Parameterization of Linear Operators via Multigrid. (arXiv:2310.19809v1 [cs.LG])

    [http://arxiv.org/abs/2310.19809](http://arxiv.org/abs/2310.19809)

    本文提出了一个简洁的神经算子架构，通过多重网格结构有效参数化线性算子，实现了算子学习的数学严密性和实用性。

    

    本文提出了一个简洁的神经算子架构来进行算子学习。将其与传统的全连接神经网络进行类比，将神经算子定义为非线性算子层中第$i$个神经元的输出，记作$\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$。其中，$\mathcal W_{ij}$表示连接第$j$个输入神经元和第$i$个输出神经元的有界线性算子，而偏差$\mathcal B_{ij}$采用函数形式而非标量形式。通过在两个神经元（Banach空间）之间有效参数化有界线性算子，MgNO引入了多重网格结构。这种方法既具备了数学严密性，又具备了实用性。此外，MgNO消除了对传统的lifting和projecting操作的需求。

    In this work, we propose a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the $i$-th neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. Here, $\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input neuron to $i$-th output neuron, and the bias $\mathcal B_{ij}$ takes the form of a function rather than a scalar. Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role. As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons. This approach offers both mathematical rigor and practical expressivity. Additionally, MgNO obviates the need for conventional lifting and projecting ope
    
[^109]: SERA：离线到在线强化学习中的样本高效奖励增强

    SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])

    [http://arxiv.org/abs/2310.19805](http://arxiv.org/abs/2310.19805)

    这篇论文提出了一种称为SERA的奖励增强框架，用于改善离线到在线强化学习中的探索能力。它通过设计内在奖励来鼓励agent进行探索，并实现更好的在线微调效果。

    

    离线强化学习的一个潜在应用是使用现有的静态数据集来初始化预训练策略，然后进行后续在线微调。然而，直接对离线预训练策略进行微调往往会导致次优性能。主要原因是离线保守方法降低了agent的探索能力，从而影响了在线微调的性能。为了增强在线微调过程中的探索能力，从而提高整体的在线微调性能，我们引入了一种称为样本高效奖励增强（SERA）的通用奖励增强框架。SERA旨在通过设计鼓励agent进行探索的内在奖励来改善在线微调的性能。具体来说，它隐式地实现了状态边缘匹配（SMM）并惩罚超出分布范围的状态行动，从而鼓励agent覆盖目标状态密度，并实现更好的在线微调结果。

    A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
    
[^110]: 从核的角度看马尔科夫决策过程的行为度量

    A Kernel Perspective on Behavioural Metrics for Markov Decision Processes. (arXiv:2310.19804v1 [cs.LG])

    [http://arxiv.org/abs/2310.19804](http://arxiv.org/abs/2310.19804)

    本文从核的角度论述了马尔科夫决策过程行为度量的新视角，并提出了一种新的度量与MICo距离等价。此外，核的视角还使我们能够提供新的理论结果，包括界定价值函数差异和嵌入到低失真误差的欧氏空间中。这些结果对于使用行为度量构建强化学习表示至关重要。同时，我们通过实证结果证明了这些方法的实际有效性。

    

    研究表明，行为度量是构建强化学习表示的有效机制。本文通过使用正定核，提出了一种关于马尔科夫决策过程行为度量的新视角。我们利用这个新视角定义了一个新的度量，可以被证明与最近引入的MICo距离（Castro等人，2021年）等价。核的视角进一步使我们能够提供新的理论结果，这在之前的工作中一直存在困难。这些结果包括通过我们的度量来界定价值函数差异，并证明我们的度量可以被证明嵌入到具有低失真误差的有限维欧氏空间中。这是在使用行为度量进行强化学习表示时的两个关键属性。我们通过强有力的实证结果来补充我们的理论，证明了这些方法在实践中的有效性。

    Behavioural metrics have been shown to be an effective mechanism for constructing representations in reinforcement learning. We present a novel perspective on behavioural metrics for Markov decision processes via the use of positive definite kernels. We leverage this new perspective to define a new metric that is provably equivalent to the recently introduced MICo distance (Castro et al., 2021). The kernel perspective further enables us to provide new theoretical results, which has so far eluded prior work. These include bounding value function differences by means of our metric, and the demonstration that our metric can be provably embedded into a finite-dimensional Euclidean space with low distortion error. These are two crucial properties when using behavioural metrics for reinforcement learning representations. We complement our theory with strong empirical results that demonstrate the effectiveness of these methods in practice.
    
[^111]: 学习生成参数概率模型的随机热力学

    Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models. (arXiv:2310.19802v1 [cs.LG])

    [http://arxiv.org/abs/2310.19802](http://arxiv.org/abs/2310.19802)

    本文将生成式机器学习问题视为参数概率模型的时间演化过程，通过研究模型参数与生成样本之间的热力学交换，发现模型通过耗散热量来学习，参数子系统充当热库存储学到的信息。这为超参数模型的泛化能力提供了有价值的热力学洞察。

    

    我们将生成式机器学习问题形式化为参数化概率模型（PPM）的时间演化，从本质上来说，这是一个热力学过程。然后，我们研究了模型参数（记为$\Theta$）与模型生成样本（记为$X$）之间的热力学交换。我们证明了训练数据集和随机梯度下降（SGD）优化器的作用是驱动这两个子系统的时间演化的能源。我们的发现表明，在生成样本$X$的过程中，模型通过耗散热量来学习，导致模型参数$\Theta$的熵增加。因此，参数子系统充当了一个热库，有效地存储了学到的信息。此外，模型参数作为热库的角色为超参数模型的泛化能力提供了有价值的热力学洞察。这种方法提供了一个明确且一致的方式来理解生成模型学习过程中的热力学行为。

    We have formulated generative machine learning problems as the time evolution of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic process. Then, we have studied the thermodynamic exchange between the model's parameters, denoted as $\Theta$, and the model's generated samples, denoted as $X$. We demonstrate that the training dataset and the action of the Stochastic Gradient Descent (SGD) optimizer serve as a work source that governs the time evolution of these two subsystems. Our findings reveal that the model learns through the dissipation of heat during the generation of samples $X$, leading to an increase in the entropy of the model's parameters, $\Theta$. Thus, the parameter subsystem acts as a heat reservoir, effectively storing the learned information. Furthermore, the role of the model's parameters as a heat reservoir provides valuable thermodynamic insights into the generalization power of over-parameterized models. This approach offers an unambiguous 
    
[^112]: 基于症状的自动猴痘检测系统SyMPox：使用XGBoost算法

    SyMPox: An Automated Monkeypox Detection System Based on Symptoms Using XGBoost. (arXiv:2310.19801v1 [cs.LG])

    [http://arxiv.org/abs/2310.19801](http://arxiv.org/abs/2310.19801)

    SyMPox是一个基于症状的自动猴痘检测系统，利用XGBoost算法分析症状模式并提供准确的猴痘诊断，为用户提供了一个用户友好的平台。

    

    猴痘是一种人畜共患病。到2023年6月10日，世界卫生组织确认了约87000例猴痘病例。目前最常用的鉴定方法是基于图像识别技术，然而这些方法速度较慢，且仅供少数人使用。本研究提出了一种名为SyMPox的独立应用，基于症状诊断猴痘病例。SyMPox利用强大的XGBoost算法分析症状模式，并提供准确的评估结果。SyMPox使用Gradio框架开发，为个人提供一个用户友好的平台，用于评估症状并获得可靠的猴痘诊断结果。

    Monkeypox is a zoonotic disease. About 87000 cases of monkeypox were confirmed by the World Health Organization until 10th June 2023. The most prevalent methods for identifying this disease are image-based recognition techniques. Still, they are not too fast and could only be available to a few individuals. This study presents an independent application named SyMPox, developed to diagnose Monkeypox cases based on symptoms. SyMPox utilizes the robust XGBoost algorithm to analyze symptom patterns and provide accurate assessments. Developed using the Gradio framework, SyMPox offers a user-friendly platform for individuals to assess their symptoms and obtain reliable Monkeypox diagnoses.
    
[^113]: 从外部到Swap遗憾2.0：针对大动作空间的高效约化和无知对手

    From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.19786](http://arxiv.org/abs/2310.19786)

    通过新的约化方法，我们改进了经典的Swap遗憾最小化算法，并提供了一个无外部遗憾算法的替代方法。对于学习专家建议问题，我们的算法可以在较少的轮数和更低的复杂度下达到相同的Swap遗憾限制。

    

    我们提供了一种新颖的从Swap遗憾最小化到外部遗憾最小化的约化方法，改进了Blum-Mansour和Stolz-Lugosi的经典约化方法，不需要行为空间的有限性。我们证明，只要存在某个假设类的无外部遗憾算法，就必然存在相同类别的无Swap遗憾算法。对于学习专家建议问题，我们的结果意味着可以保证在$\log(N)^{O(1/\epsilon)}$轮后，每次迭代复杂度为$O(N)$的情况下，Swap遗憾被限定为$\epsilon$，而Blum-Mansour和Stolz-Lugosi的经典约化方法需要$O(N/\epsilon^2)$轮和至少$\Omega(N^2)$的每次迭代复杂度。我们的结果伴随着一个相关的下界，与[BM07]不同，这个下界适用于无知和$\ell_1$-受限的对手和可以利用这个下界的学习者。

    We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can emplo
    
[^114]: 结合语言模型的领域专用方法：一种丰富多彩的途径

    Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19708](http://arxiv.org/abs/2310.19708)

    该论文提出了一种将领域特定语言模型与通用语言模型结合的新颖方法，通过对词语进行标记或“上色”来有效处理领域术语，显著降低了领域专用任务的错误率。

    

    通用目的的语言模型在处理领域特定术语和术语时遇到困难，这些术语经常在医学或工业领域等专业领域中使用。此外，他们通常很难解释将通用语言与专门术语混合使用的混合语音。这对于在这些特定领域内操作的自动语音识别系统构成了挑战。在这项工作中，我们介绍了一种新颖的方法，将领域特定或次级语言模型集成到通用的语言模型中。该策略涉及对每个单词进行标记或“上色”，以指示其与通用或领域特定的语言模型的关联。我们开发了一种优化算法，可增强波束搜索算法，以有效处理涉及上色单词的推理。我们的评估表明，这种方法在集成术语到语言任务中非常有效。值得注意的是，我们的方法显著降低了领域专用任务的错误率。

    General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or ``coloring'', each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-sp
    
[^115]: 机器人学习中的非参数回归方法在流形上的应用

    Non-parametric regression for robot learning on manifolds. (arXiv:2310.19561v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2310.19561](http://arxiv.org/abs/2310.19561)

    本文提出了一种在机器人学习中针对流形值数据的非参数回归方法，通过在流形上操作概率分布参数来直接估计函数，以改善预测准确性和简化算法。

    

    许多机器人学习工具都是针对欧几里得数据设计的。然而，在机器人学中，许多应用涉及到流形值数据。一个常见的例子是姿态；它可以表示为3x3的旋转矩阵或四元数，其空间是非欧几里得流形。在机器人学习中，流形值数据通常通过将流形与合适的欧几里得空间相关联来处理，可以通过嵌入流形或将数据投影到一个或多个切空间来实现。这些方法可能导致预测准确性不高和算法复杂。在本文中，我们提出了一种在流形内直接进行回归的“固有”方法。它涉及对流形上的适当概率分布进行操作，将其参数作为预测变量（如时间）的函数，并通过“局部似然”方法来非参数估计该函数，其中包含核函数。我们将该方法命名为核化似然估计。

    Many of the tools available for robot learning were designed for Euclidean data. However, many applications in robotics involve manifold-valued data. A common example is orientation; this can be represented as a 3-by-3 rotation matrix or a quaternion, the spaces of which are non-Euclidean manifolds. In robot learning, manifold-valued data are often handled by relating the manifold to a suitable Euclidean space, either by embedding the manifold or by projecting the data onto one or several tangent spaces. These approaches can result in poor predictive accuracy, and convoluted algorithms. In this paper, we propose an "intrinsic" approach to regression that works directly within the manifold. It involves taking a suitable probability distribution on the manifold, letting its parameter be a function of a predictor variable, such as time, then estimating that function non-parametrically via a "local likelihood" method that incorporates a kernel. We name the method kernelised likelihood esti
    
[^116]: Hodge-Compositional 边缘高斯过程

    Hodge-Compositional Edge Gaussian Processes. (arXiv:2310.19450v1 [stat.ML])

    [http://arxiv.org/abs/2310.19450](http://arxiv.org/abs/2310.19450)

    本论文提出了一种新的方法用于对边缘集合上的函数进行建模，该方法基于Hodge分解开发了适用于不同应用场景的无散度和无旋度的高斯过程，并通过组合它们来表示任意边缘函数。实验结果表明这种方法在流动数据推断中具有潜在的实际应用价值。

    

    我们提出了一种基于边缘集合的2-复形结构（类似于图形，其中边缘可形成三角面）的函数建模的有原则的高斯过程（GPs）。这种方法适用于学习网络上的流动类型数据，其中边缘流可以通过离散的散度和旋度来表征。借鉴Hodge分解，我们首先开发了适用于各种应用的无散度和无旋游的边缘GPs。然后将它们组合起来创建Hodge-组合边缘GPs，这些GPs足够表达任何边缘函数。这些GPs便于对边缘函数的不同Hodge分量进行直接和独立的学习，使我们能够在超参数优化过程中捕捉它们的相关性。为了突显它们的实际潜力，我们将它们应用于货币兑换、海洋流动和供水网络中的流动数据推断，并将其与替代模型进行比较。

    We propose principled Gaussian processes (GPs) for modeling functions defined over the edge set of a simplicial 2-complex, a structure similar to a graph in which edges may form triangular faces. This approach is intended for learning flow-type data on networks where edge flows can be characterized by the discrete divergence and curl. Drawing upon the Hodge decomposition, we first develop classes of divergence-free and curl-free edge GPs, suitable for various applications. We then combine them to create \emph{Hodge-compositional edge GPs} that are expressive enough to represent any edge function. These GPs facilitate direct and independent learning for the different Hodge components of edge functions, enabling us to capture their relevance during hyperparameter optimization. To highlight their practical potential, we apply them for flow data inference in currency exchange, ocean flows and water supply networks, comparing them to alternative models.
    
[^117]: BERT失去耐心对抗恶意减速不能保持稳健性

    BERT Lost Patience Won't Be Robust to Adversarial Slowdown. (arXiv:2310.19152v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.19152](http://arxiv.org/abs/2310.19152)

    本文评估了多出口语言模型对抗恶意减速的稳健性, 发现复杂的机制更易受到恶意减速的攻击。此外，对抗训练无效，但对话模型的输入清洗是有效的。

    

    本文系统评估多出口语言模型对抗恶意减速的稳健性。为了审核其稳健性，我们设计了一种减速攻击，生成绕过早期退出点的自然恶意文本。我们使用所得到的WAFFLE攻击作为工具，对三种多出口机制在GLUE基准测试中对抗恶意减速进行全面评估。然后，我们展示了我们的攻击显著降低了这三种方法在白盒和黑盒设置下提供的计算节省效果。机制越复杂，越容易受到恶意减速的攻击。我们还对扰动的文本输入进行了语言分析，识别出我们的攻击生成的常见扰动模式，并将其与标准的恶意文本攻击进行了比较。此外，我们证明对抗训练在打败我们的减速攻击方面是无效的，但使用对话模型（如ChatGPT）进行输入清洗是有效的。

    In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown. To audit their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown. We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings. The more complex a mechanism is, the more vulnerable it is to adversarial slowdown. We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks. Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can r
    
[^118]: 行为一致性的奖励函数优化

    Behavior Alignment via Reward Function Optimization. (arXiv:2310.19007v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.19007](http://arxiv.org/abs/2310.19007)

    本论文介绍了一种通过优化奖励函数来实现行为一致性的新框架。该框架通过将设计师的启发和领域知识与环境的主要奖励相结合，自动确定了最有效的奖励结构，以避免引起不希望的行为。

    

    设计奖励函数来有效地引导强化学习代理向特定行为的方向发展是一项复杂的任务。这是具有挑战性的，因为它要求确定非稀疏的奖励结构，并避免无意中引起不希望的行为。简单地修改奖励结构以提供更密集和更频繁的反馈可能会导致意外的结果，并促使行为与设计者的预期目标不一致。虽然通常建议使用基于潜力的奖励设计来解决这个问题，但我们系统地研究了它在许多情况下会严重影响性能的问题。为了解决这些问题，我们提出了一个新的框架，使用双层目标来学习"行为一致性奖励函数"。这些函数通过将反映设计师启发和领域知识的辅助奖励与环境的主要奖励相结合，自动确定了最有效的混合方式。

    Designing reward functions for efficiently guiding reinforcement learning (RL) agents toward specific behaviors is a complex task. This is challenging since it requires the identification of reward structures that are not sparse and that avoid inadvertently inducing undesirable behaviors. Naively modifying the reward structure to offer denser and more frequent feedback can lead to unintended outcomes and promote behaviors that are not aligned with the designer's intended goal. Although potential-based reward shaping is often suggested as a remedy, we systematically investigate settings where deploying it often significantly impairs performance. To address these issues, we introduce a new framework that uses a bi-level objective to learn \emph{behavior alignment reward functions}. These functions integrate auxiliary rewards reflecting a designer's heuristics and domain knowledge with the environment's primary rewards. Our approach automatically determines the most effective way to blend
    
[^119]: 基于核的多图信号学习和图信号聚类的联合方法

    Kernel-based Joint Multiple Graph Learning and Clustering of Graph Signals. (arXiv:2310.19005v1 [eess.SP] CROSS LISTED)

    [http://arxiv.org/abs/2310.19005](http://arxiv.org/abs/2310.19005)

    在图信号处理中，我们提出了一种基于核的联合多图学习和图信号聚类算法，可以有效地将节点侧信息整合到信号的分区和图学习中。

    

    在图信号处理（GSP）的背景下，图学习（GL）研究从节点观测（即图信号）中推断出图的拓扑结构。然而，数据通常是混合形式，涉及不同的基础结构。这种异质性需要联合聚类和学习多个图。在许多实际应用中，有可用的节点侧协变量（即核函数），必须加以合并，而这在现有的图信号聚类方法中尚未解决。为此，受丰富的K-means框架启发，我们提出了一种新的基于核的算法，将节点侧信息整合到信号的分区和每个集群的图学习中。数值实验证明了该方法在现有方法之上的有效性。

    Within the context of Graph Signal Processing (GSP), Graph Learning (GL) is concerned with the inference of a graph's topology from nodal observations, i.e., graph signals. However, data is often in mixed form, relating to different underlying structures. This heterogeneity necessitates the joint clustering and learning of multiple graphs. In many real-life applications, there are available node-side covariates (i.e., kernels) that imperatively should be incorporated, which has not been addressed by the rare graph signal clustering approaches. To this end and inspired by the rich K-means framework, we propose a novel kernel-based algorithm to incorporate this node-side information as we jointly partition the signals and learn a graph for each cluster. Numerical experiments demonstrate its effectiveness over the state-of-the-art.
    
[^120]: LoRAShear: 高效的大型语言模型结构剪枝和知识恢复

    LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery. (arXiv:2310.18356v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.18356](http://arxiv.org/abs/2310.18356)

    LoRAShear是一种高效的大型语言模型结构剪枝和知识恢复方法，通过逐步剪枝和动态微调，有效减少LLMs的占用空间并且保持性能。

    

    大型语言模型（LLMs）已经改变了人工智能的格局，但其庞大的规模在计算成本方面带来了重大挑战。我们引入了LoRAShear，一种新颖的高效方法，用于结构化剪枝LLMs并恢复知识。LoRAShear首先在LoRA模块上创建依赖图，以发现最小删除结构并分析知识分布。然后，它在LoRA适配器上进行渐进式结构剪枝，并实现内在的知识转移，以更好地保留冗余结构中的信息。为了恢复剪枝期间丢失的知识，LoRAShear仔细研究并提出了一种动态微调方案，使用动态数据适配器，以有效缩小与完整模型的性能差距。数值结果表明，只使用一块GPU在几天内，LoRAShear将LLMs的占用空间有效减少了20%，仅有1.0%的性能损失。

    Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with only 1.0% performance d
    
[^121]: 无数据蒸馏提高了联邦胸部疾病分析的效率和隐私

    Data-Free Distillation Improves Efficiency and Privacy in Federated Thorax Disease Analysis. (arXiv:2310.18346v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2310.18346](http://arxiv.org/abs/2310.18346)

    我们提出了一种名为FedKDF的无数据蒸馏联邦学习方法，可以在保护隐私的前提下提高胸部疾病分析的效率和准确性。通过使用轻量级生成器聚合来自不同客户端的知识，FedKDF能够生成一个统一的预测器，并在其基础上进行进一步优化。

    

    在大规模、多中心和多扫描仪环境中，胸部疾病分析受到严格的隐私政策的限制。联邦学习（FL）提供了一种潜在的解决方案，而传统的基于参数的FL可能受到高通信成本、数据泄漏和异质性等问题的限制。基于蒸馏的FL可以提高效率，但依赖于一个代理数据集，在临床实践中通常不实用。为了解决这些挑战，我们引入了一种无数据蒸馏的FL方法FedKDF。在FedKDF中，服务器使用轻量级生成器从不同客户端聚合知识，而无需访问其私有数据或代理数据集。FedKDF将客户端的预测器组合成一个统一的预测器，并使用轻量级生成器中学到的知识进行进一步优化。我们的实证实验表明，FedKDF提供了一个稳健的解决方案，用于高效、隐私保护的联邦胸部疾病分析。

    Thorax disease analysis in large-scale, multi-centre, and multi-scanner settings is often limited by strict privacy policies. Federated learning (FL) offers a potential solution, while traditional parameter-based FL can be limited by issues such as high communication costs, data leakage, and heterogeneity. Distillation-based FL can improve efficiency, but it relies on a proxy dataset, which is often impractical in clinical practice. To address these challenges, we introduce a data-free distillation-based FL approach FedKDF. In FedKDF, the server employs a lightweight generator to aggregate knowledge from different clients without requiring access to their private data or a proxy dataset. FedKDF combines the predictors from clients into a single, unified predictor, which is further optimized using the learned knowledge in the lightweight generator. Our empirical experiments demonstrate that FedKDF offers a robust solution for efficient, privacy-preserving federated thorax disease analys
    
[^122]: AllTogether：使用大型语言模型对使用拼接提示进行Web导航的效果进行研究

    AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models. (arXiv:2310.18331v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.18331](http://arxiv.org/abs/2310.18331)

    AllTogether是一个标准化的提示模板，通过增强任务背景表示，提高了大型语言模型（LLMs）在基于HTML的Web导航中的性能。研究结果显示，像GPT-4这样的模型在这类任务中优于较小的模型，并且HTML代码片段的长度和历史轨迹对性能有显著影响。同时，在实时环境反馈方面，优于之前的逐步指导。这项工作为未来LLM驱动的Web代理研究提供了宝贵的见解。

    

    大型语言模型（LLMs）已经成为用于Web导航任务的有前景的代理，它们解释目标并与Web页面进行交互。然而，这种任务中使用拼接提示的效果仍未得到充分探索。我们引入了AllTogether，一个标准化的提示模板，增强任务背景表示，从而提高LLMs在基于HTML的Web导航中的性能。我们通过来自开源Llama-2和可访问的GPT模型的提示学习和指令微调来评估这种方法的效果。我们的结果表明，像GPT-4这样的模型在Web导航任务中优于较小的模型。此外，我们发现HTML代码片段的长度和历史轨迹显著影响性能，并且之前的逐步指导比实时环境反馈更有效。总体而言，我们相信我们的工作为未来LLM驱动的Web代理研究提供了宝贵的见解。

    Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.
    
[^123]: Causal Q-Aggregation for CATE Model Selection（CATE模型选择中的因果Q集成）

    Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v1 [stat.ML])

    [http://arxiv.org/abs/2310.16945](http://arxiv.org/abs/2310.16945)

    该论文提出了一种基于Q集成的CATE模型选择方法，其通过使用双重鲁棒损失实现了统计上的最佳预测模型选择遗憾率

    

    准确估计条件平均处理效应（CATE）是个性化决策的核心。尽管有大量用于CATE估计的模型，但由于因果推断的基本问题，模型选择是一项非常棘手的任务。最近的实证工作提供了有利于具有双重鲁棒性质的代理损失度量和模型集成的证据。然而，对于这些模型的理论理解还不够。直接应用先前的理论工作会由于模型选择问题的非凸性而导致次优的预测模型选择率。我们提供了现有主要CATE集成方法的遗憾率，并提出了一种基于双重鲁棒损失的Q集成的新的CATE模型集成方法。我们的主要结果表明，因果Q集成在预测模型选择的遗憾率上达到了统计上的最优值为$\frac{\log(M)}{n}$（其中$M$为模型数，$n$为样本数），加上高阶估计误差项

    Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error term
    
[^124]: 关于负责任的机器学习数据集和公平性、隐私和法规准则的论文

    On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms. (arXiv:2310.15848v1 [cs.LG])

    [http://arxiv.org/abs/2310.15848](http://arxiv.org/abs/2310.15848)

    这篇论文讨论了负责任的机器学习数据集的重要性，并提出了一个通过负责任评价标准来评估数据集的框架。

    

    人工智能已经进入各个科学领域，在各种任务上比现有算法有了惊人的改进。近年来，人们对人工智能技术的可信性存在严重担忧。科学界致力于开发可信的人工智能算法。然而，目前在人工智能社区中流行的机器学习和深度学习算法在其开发过程中严重依赖使用的数据。这些学习算法通过识别数据中的模式来学习行为目标。数据中的任何缺陷都有可能直接转化为算法的缺陷。在这项研究中，我们讨论了负责任的机器学习数据集的重要性，并提出了一个通过负责任评价标准来评估数据集的框架。现有的工作侧重于对算法的后期评估以确保其可信性，而我们提供了一个框架，单独考虑数据组件以理解其特性。

    Artificial Intelligence (AI) has made its way into various scientific fields, providing astonishing improvements over existing algorithms for a wide variety of tasks. In recent years, there have been severe concerns over the trustworthiness of AI technologies. The scientific community has focused on the development of trustworthy AI algorithms. However, machine and deep learning algorithms, popular in the AI community today, depend heavily on the data used during their development. These learning algorithms identify patterns in the data, learning the behavioral objective. Any flaws in the data have the potential to translate directly into algorithms. In this study, we discuss the importance of Responsible Machine Learning Datasets and propose a framework to evaluate the datasets through a responsible rubric. While existing work focuses on the post-hoc evaluation of algorithms for their trustworthiness, we provide a framework that considers the data component separately to understand it
    
[^125]: 学习可解释的规则以实现可扩展的数据表示和分类

    Learning Interpretable Rules for Scalable Data Representation and Classification. (arXiv:2310.14336v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14336](http://arxiv.org/abs/2310.14336)

    这项研究提出了一种名为RRL的新型分类器，通过自动学习可解释的非模糊规则，实现了数据表示和分类的良好可扩展性和解释性。

    

    基于规则的模型（如决策树）在需要高模型解释性的场景中被广泛使用，因为它们具有透明的内部结构和良好的模型表达能力。然而，由于离散的参数和结构，基于规则的模型在优化方面很难应对大规模的数据集。集成方法和模糊/软规则通常用于提高性能，但会牺牲模型的解释性。为了获得良好的可扩展性和可解释性，我们提出了一种新的分类器，称为基于规则的表示学习器（RRL），它可以自动学习用于数据表示和分类的可解释的非模糊规则。为了有效训练不可微分的RRL，我们将其映射到连续空间，并提出一种称为梯度嵌入的新的训练方法，可以使用梯度下降直接优化离散模型。此外，还设计了一种新颖的逻辑激活函数，以增加RRL的可扩展性，并使其能够进行判别。

    Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discr
    
[^126]: AI反馈促进的质量-多样性算法

    Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])

    [http://arxiv.org/abs/2310.13032](http://arxiv.org/abs/2310.13032)

    基于AI反馈的质量-多样性（QDAIF）算法利用语言模型来生成和评估创造性写作，比传统算法更广泛地覆盖高质量样本的搜索空间。

    

    在许多文本生成问题中，用户可能不仅偏好单一回复，而是希望得到多样性的高质量输出以供选择。质量-多样性（QD）搜索算法旨在通过不断改进和多样化候选人群来实现这一目标。然而，QD在创作性写作等质性领域的应用受到算法指定质量和多样性度量的困难的限制。有趣的是，最近语言模型（LMs）的发展使得通过AI反馈指导搜索成为可能，其中LMs在自然语言中被提示来评估文本的质性方面。借助这一进展，我们引入了通过AI反馈实现的质量-多样性算法（QDAIF），其中进化算法应用LMs来生成变异并评估候选文本的质量和多样性。在创作性写作领域的评估中，与非QDAIF算法相比，QDAIF更广泛地覆盖高质量样本的指定搜索空间。

    In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
    
[^127]: 流形学习的规范化正态流

    Canonical normalizing flows for manifold learning. (arXiv:2310.12743v1 [stat.ML])

    [http://arxiv.org/abs/2310.12743](http://arxiv.org/abs/2310.12743)

    该论文介绍了一种规范化正态流方法，用于流形学习。通过可学习的可逆变换将数据嵌入到高维空间中，从而实现了在流形上计算概率密度并优化网络参数的目标。然而，当前的方法在学习到的流形表示中存在着与流形关联且退化的内在基函数的问题。

    

    流形学习流是一类生成建模技术，假设数据具有低维流形描述。通过可学习的可逆变换将这种流形嵌入到数据的高维空间中。因此，一旦通过重构损失正确对齐流形，流形上的概率密度就是可计算的，并且可以使用最大似然来优化网络参数。自然地，数据的低维表示需要是单射映射。最近的方法能够在建模的流形上对密度进行对准，并在嵌入到高维空间时高效计算密度体积变化项。然而，除非单射映射在解析上预定义，否则学习到的流形不一定是数据的有效表示。也就是说，这种模型的潜在维度经常会学习到与流形相关并且退化的内在基函数。

    Manifold learning flows are a class of generative modelling techniques that assume a low-dimensional manifold description of the data. The embedding of such manifold into the high-dimensional space of the data is achieved via learnable invertible transformations. Therefore, once the manifold is properly aligned via a reconstruction loss, the probability density is tractable on the manifold and maximum likelihood can be used optimize the network parameters. Naturally, the lower-dimensional representation of the data requires an injective-mapping. Recent approaches were able to enforce that density aligns with the modelled manifold, while efficiently calculating the density volume-change term when embedding to the higher-dimensional space. However, unless the injective-mapping is analytically predefined, the learned manifold is not necessarily an efficient representation of the data. Namely, the latent dimensions of such models frequently learn an entangled intrinsic basis with degenerat
    
[^128]: 对于零样本神经机器翻译性能变化的更好理解

    Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance. (arXiv:2310.10385v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10385](http://arxiv.org/abs/2310.10385)

    该论文研究了零样本神经机器翻译性能变化的原因，发现目标语言的翻译质量、词汇重叠和语言特性是影响性能变化的关键因素。

    

    多语言神经机器翻译（MNMT）促进了知识分享，但往往在零样本（ZS）翻译质量上表现不佳。虽然之前的研究已经探讨了整体ZS性能低下的原因，但我们的工作引入了一个新的视角：ZS性能存在较大的变化。这表明MNMT的ZS能力并不是均匀地差，而是在特定的翻译方向上产生了合理的结果。通过系统性的实验，涵盖40种语言的1,560个语言方向，我们确定了三个影响ZS NMT性能变化的关键因素：1）目标语言的翻译能力，2）词汇重叠，3）语言特性。我们的研究结果表明，目标语言的翻译质量是最有影响力的因素，词汇重叠始终影响ZS性能。此外，语言特性，如语言家族和书写系统，尤其在较小的模型中起到了一定作用。

    Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing but often suffers from poor zero-shot (ZS) translation qualities. While prior work has explored the causes of overall low ZS performance, our work introduces a fresh perspective: the presence of high variations in ZS performance. This suggests that MNMT does not uniformly exhibit poor ZS capability; instead, certain translation directions yield reasonable results. Through systematic experimentation involving 1,560 language directions spanning 40 languages, we identify three key factors contributing to high variations in ZS NMT performance: 1) target side translation capability 2) vocabulary overlap 3) linguistic properties. Our findings highlight that the target side translation quality is the most influential factor, with vocabulary overlap consistently impacting ZS performance. Additionally, linguistic properties, such as language family and writing system, play a role, particularly with smaller models. Furt
    
[^129]: 联邦多目标学习

    Federated Multi-Objective Learning. (arXiv:2310.09866v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.09866](http://arxiv.org/abs/2310.09866)

    本研究提出了一种新的联邦多目标学习（FMOL）框架，在满足多代理多任务学习应用的分布式性质和数据隐私需求的同时，支持不同客户端上的不同目标函数集合。通过引入联邦学习的范式，将多目标优化（MOO）推广到联邦学习领域。

    

    在最近几年中，多目标优化（MOO）作为许多多代理多任务学习应用的基础问题出现。然而，现有的MOO算法仍局限于集中式学习环境，无法满足这些多代理多任务学习应用的分布式性质和数据隐私需求。这激发了我们提出一种新的联邦多目标学习（FMOL）框架，其中多个客户端在保持他们的训练数据私密的同时，分布式协作解决一个MOO问题。值得注意的是，我们的FMOL框架允许不同客户端上的不同目标函数集合，以支持广泛的应用，这首次将MOO形式化推广到联邦学习范式中。对于这个FMOL框架，我们提出了两种新的联邦多目标优化（FMOO）算法，称为联邦多梯度下降平均（FMGDA）和联邦随机梯度下降（Federated SGD）。

    In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stoc
    
[^130]: MIR2:面向通过互信息正则化进行可证明鲁棒多智能体强化学习

    MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization. (arXiv:2310.09833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.09833](http://arxiv.org/abs/2310.09833)

    MIR2提出了一种针对鲁棒多智能体强化学习的方法，通过在常规情况下训练策略并最小化互信息作为鲁棒正则化，实现了在不准备每种可能的最坏情况的情况下提升鲁棒性的目标。

    

    鲁棒多智能体强化学习(MARL)对于未知盟友的不确定或最坏情况行动需要具备弹性。现有的鲁棒MARL中的最大最小优化技术通过训练智能体抵抗最坏情况的对手来增强鲁棒性，但随着智能体数量的增加，这种方法变得难以操作，导致最坏情况的数量呈指数级增长。试图简化这种复杂性往往会导致过于悲观的策略、在各种情况下鲁棒性不足和高计算需求。与这些方法不同，人类在学习适应性和鲁棒行为时自然而然地不需要准备每种可能的最坏情况。受此启发，我们提出了MIR2，它在常规情况下训练策略，并将互信息最小化作为鲁棒正则化。从理论上讲，我们将鲁棒性视为一个推理问题，并证明了在历史和行动之间最小化互信息隐含地最大化了鲁棒性的下界。

    Robust multi-agent reinforcement learning (MARL) necessitates resilience to uncertain or worst-case actions by unknown allies. Existing max-min optimization techniques in robust MARL seek to enhance resilience by training agents against worst-case adversaries, but this becomes intractable as the number of agents grows, leading to exponentially increasing worst-case scenarios. Attempts to simplify this complexity often yield overly pessimistic policies, inadequate robustness across scenarios and high computational demands. Unlike these approaches, humans naturally learn adaptive and resilient behaviors without the necessity of preparing for every conceivable worst-case scenario. Motivated by this, we propose MIR2, which trains policy in routine scenarios and minimize Mutual Information as Robust Regularization. Theoretically, we frame robustness as an inference problem and prove that minimizing mutual information between histories and actions implicitly maximizes a lower bound on robust
    
[^131]: 进化动态优化与机器学习

    Evolutionary Dynamic Optimization and Machine Learning. (arXiv:2310.08748v1 [cs.NE])

    [http://arxiv.org/abs/2310.08748](http://arxiv.org/abs/2310.08748)

    进化计算和机器学习的结合为优化复杂的机器学习任务提供了有价值的机会，并通过利用进化计算算法生成的数据来提供对搜索空间和种群动态的洞察。

    

    进化计算(EC)作为一种受自然渐进发展机制启发的强大的人工智能领域已经出现。然而，EC方法常常面临停滞、多样性损失、计算复杂性、种群初始化和过早收敛等挑战。为了克服这些限制，研究人员将学习算法与进化技术相结合。这种集成利用了EC算法在迭代搜索过程中生成的有价值的数据，提供了对搜索空间和种群动态的洞察。类似地，进化算法与机器学习(ML)之间的关系是相互的，因为EC方法为优化噪声、不准确和动态目标函数所描述的复杂ML任务提供了极好的机会。这些混合技术被称为进化机器学习(EML)，已在ML过程的各个阶段应用。

    Evolutionary Computation (EC) has emerged as a powerful field of Artificial Intelligence, inspired by nature's mechanisms of gradual development. However, EC approaches often face challenges such as stagnation, diversity loss, computational complexity, population initialization, and premature convergence. To overcome these limitations, researchers have integrated learning algorithms with evolutionary techniques. This integration harnesses the valuable data generated by EC algorithms during iterative searches, providing insights into the search space and population dynamics. Similarly, the relationship between evolutionary algorithms and Machine Learning (ML) is reciprocal, as EC methods offer exceptional opportunities for optimizing complex ML tasks characterized by noisy, inaccurate, and dynamic objective functions. These hybrid techniques, known as Evolutionary Machine Learning (EML), have been applied at various stages of the ML process. EC techniques play a vital role in tasks such
    
[^132]: 使用合成任务教授语言模型更少幻觉

    Teaching Language Models to Hallucinate Less with Synthetic Tasks. (arXiv:2310.06827v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.06827](http://arxiv.org/abs/2310.06827)

    通过设计合成任务，我们的研究表明减少合成任务上的幻觉可以帮助减少现实世界的抽象概括任务上的幻觉。

    

    大型语言模型（LLMs）在抽象概括任务（如基于文档的问答、会议概述和临床报告生成）中经常产生幻觉，即使所有必要信息都在上下文中。然而，在这些任务上优化LLMs以减少幻觉是具有挑战性的，因为在每个优化步骤中有效评估幻觉是困难的。在这项工作中，我们展示了通过减少合成任务上的幻觉也可以减少现实世界下游任务上的幻觉。我们的方法，SynTra，首先设计了一个合成任务，其中易于诱发和衡量幻觉。然后，通过对合成任务进行前缀调优来优化LLM的系统消息，并最终将系统消息转移到现实中难以优化的任务中。通过对三个现实的抽象概括任务，使用仅合成检索任务进行监督，SynTra减少了两个具有13B参数的LLMs的幻觉。我们还发现通过优化合成任务上的系统消息，可以最大限度地减少现实任务上的幻觉。

    Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the sy
    
[^133]: RetSeg: 基于保留机制的结直肠息肉分割网络

    RetSeg: Retention-based Colorectal Polyps Segmentation Network. (arXiv:2310.05446v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2310.05446](http://arxiv.org/abs/2310.05446)

    本研究探索将保留机制整合到结直肠息肉分割中，解决了视觉变换器在资源受限设备上实时疾病检测中的内存和并行性挑战。

    

    视觉变换器（ViTs）在医学图像分析方面取得了重大突破，与传统的卷积神经网络（CNNs）相比，在息肉分类、检测和分割等关键任务中展现出了更高的效能。通过利用注意机制聚焦于特定图像区域，ViTs在处理视觉数据时表现出上下文感知能力，从而在处理复杂医学图像时实现了强大且精确的预测。此外，变换器中固有的自注意机制适应了不同的输入尺寸和分辨率，为传统的CNNs所不具备的提供了前所未有的灵活性。然而，变换器由于自注意机制而面临着过多的内存使用和有限的训练并行性等挑战，从而使其在资源受限设备上实时疾病检测变得不切实际。在本研究中，我们通过探究将最近引入的保留机制整合到息肉分割中，来解决这些难题。

    Vision Transformers (ViTs) have revolutionized medical imaging analysis, showcasing superior efficacy compared to conventional Convolutional Neural Networks (CNNs) in vital tasks such as polyp classification, detection, and segmentation. Leveraging attention mechanisms to focus on specific image regions, ViTs exhibit contextual awareness in processing visual data, culminating in robust and precise predictions, even for intricate medical images. Moreover, the inherent self-attention mechanism in Transformers accommodates varying input sizes and resolutions, granting an unprecedented flexibility absent in traditional CNNs. However, Transformers grapple with challenges like excessive memory usage and limited training parallelism due to self-attention, rendering them impractical for real-time disease detection on resource-constrained devices. In this study, we address these hurdles by investigating the integration of the recently introduced retention mechanism into polyp segmentation, intr
    
[^134]: BioBridge: 通过知识图谱桥接生物医学基础模型

    BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph. (arXiv:2310.03320v1 [cs.LG])

    [http://arxiv.org/abs/2310.03320](http://arxiv.org/abs/2310.03320)

    BioBridge是一种通过知识图谱桥接单模态生物医学基础模型的参数高效学习框架。实验证明，BioBridge在跨模态检索任务中胜过最佳基线KG嵌入方法，具有泛化能力。

    

    基础模型(FMs)能够利用大量的无标签数据，在各种任务上展现出优秀的性能。然而，用于生物医学领域的FMs主要仍处于单模态状态，即独立训练并用于处理蛋白质序列、小分子结构或临床数据等单一任务。为了克服生物医学FMs的这种局限性，我们提出了一种新颖的参数高效学习框架BioBridge，通过利用知识图谱(KG)来学习不需要微调任何底层单模态FMs的转换，从而桥接独立训练的单模态FMs以建立多模态行为。我们的实证结果表明，BioBridge在跨模态检索任务中可以击败最佳基线KG嵌入方法（平均提高约76.3%）。我们还发现，BioBridge表现出领域外的泛化能力，可以推广到未见的模态或关系中。

    Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relation
    
[^135]: 实现稳健的心脏分割：使用图卷积网络

    Towards Robust Cardiac Segmentation using Graph Convolutional Networks. (arXiv:2310.01210v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2310.01210](http://arxiv.org/abs/2310.01210)

    这项研究使用了图卷积网络来实现心脏分割，通过预测轮廓点而不是标记每个像素，消除了心脏分割中的解剖学错误。同时还对图卷积网络进行了消融研究，并评估了临床测量指标的性能。

    

    全自动化的心脏分割可以快速、可重复地从超声心动图检查中提取临床测量指标。U-Net结构是目前医学分割领域的深度学习架构，可以实时分割心脏结构，并且平均误差可与观测者间变异性相媲美。然而，该架构仍然会生成许多解离异常的结构。本研究使用图卷积神经网络的概念，预测出感兴趣结构的轮廓点，而不是对每个像素进行标记。我们提出了一个基于心脏解剖学的图结构，并证明这消除了公开可获得的CAMUS数据集上的多结构分割中的解剖学错误。此外，本研究还对图卷积网络进行了消融研究，并在临床HUNT4数据集上评估了临床测量指标。

    Fully automatic cardiac segmentation can be a fast and reproducible method to extract clinical measurements from an echocardiography examination. The U-Net architecture is the current state-of-the-art deep learning architecture for medical segmentation and can segment cardiac structures in real-time with average errors comparable to inter-observer variability. However, this architecture still generates large outliers that are often anatomically incorrect. This work uses the concept of graph convolutional neural networks that predict the contour points of the structures of interest instead of labeling each pixel. We propose a graph architecture that uses two convolutional rings based on cardiac anatomy and show that this eliminates anatomical incorrect multi-structure segmentations on the publicly available CAMUS dataset. Additionally, this work contributes with an ablation study on the graph convolutional architecture and an evaluation of clinical measurements on the clinical HUNT4 dat
    
[^136]: DataDAM: 基于注意力匹配的高效数据集精炼

    DataDAM: Efficient Dataset Distillation with Attention Matching. (arXiv:2310.00093v1 [cs.CV])

    [http://arxiv.org/abs/2310.00093](http://arxiv.org/abs/2310.00093)

    本研究提出了一种基于注意力匹配的高效数据集精炼(DataDAM)方法。通过匹配空间注意力来学习合成图像，从而实现了最新技术水平的性能，同时减少了训练成本。

    

    研究人员长期以来一直在尽量减少深度学习的训练成本，同时保持在多样化数据集上的强大泛化能力。最近的数据集精炼研究旨在通过创建一个包含更大真实数据集信息的小型合成数据集来减少训练成本，并最终实现与整个数据集训练的模型相当的测试准确性。然而，之前方法生成的合成数据并不能像原始训练数据那样分布和区分，而且会带来显著的计算成本。尽管取得了令人期待的结果，但精炼合成数据集上训练的模型与整个数据集上训练的模型之间仍然存在明显的性能差距。在本文中，我们通过使用基于注意力匹配的高效数据集精炼(DataDAM)来应对这些挑战，实现了最新技术水平的性能，同时减少了训练成本。具体而言，我们通过匹配空间注意力来学习合成图像。

    Researchers have long tried to minimize training costs in deep learning while maintaining strong generalization across diverse datasets. Emerging research on dataset distillation aims to reduce training costs by creating a small synthetic set that contains the information of a larger real dataset and ultimately achieves test accuracy equivalent to a model trained on the whole dataset. Unfortunately, the synthetic data generated by previous methods are not guaranteed to distribute and discriminate as well as the original training data, and they incur significant computational costs. Despite promising results, there still exists a significant performance gap between models trained on condensed synthetic sets and those trained on the whole dataset. In this paper, we address these challenges using efficient Dataset Distillation with Attention Matching (DataDAM), achieving state-of-the-art performance while reducing training costs. Specifically, we learn synthetic images by matching the spa
    
[^137]: 可重复性报告：多样的生物信息化神经结构对前列腺癌分层的影响

    Reusability report: Prostate cancer stratification with diverse biologically-informed neural architectures. (arXiv:2309.16645v1 [cs.LG])

    [http://arxiv.org/abs/2309.16645](http://arxiv.org/abs/2309.16645)

    该研究通过验证和重新实现作者提出的生物信息化神经网络P-NET的方法，量化了使用Reactome生物通路进行网络稀疏化的贡献，并探索了其他神经架构和方法。研究结果表明，不同结构的深度神经网络对个体患者进行了错误的预测。

    

    在Elmarakeby等人的研究中，提出了一种具有生物信息化、稀疏连接的前馈神经网络（P-NET）来模拟前列腺癌的状态。我们使用原始代码和我们自己使用更现代化的库重新实现的方法来验证Elmarakeby等人研究的可复现性。我们量化了通过Reactome生物通路进行网络稀疏化的贡献，并确认其对P-NET的卓越性能的重要性。此外，我们还探索了将生物信息纳入网络的其他神经架构和方法。我们在相同的训练数据上尝试了三种类型的图神经网络，并研究了不同模型之间的临床预测一致性。我们的分析表明，具有不同结构的深度神经网络对个体患者进行错误的预测。

    In, Elmarakeby et al., "Biologically informed deep neural network for prostate cancer discovery", a feedforward neural network with biologically informed, sparse connections (P-NET) was presented to model the state of prostate cancer. We verified the reproducibility of the study conducted by Elmarakeby et al., using both their original codebase, and our own re-implementation using more up-to-date libraries. We quantified the contribution of network sparsification by Reactome biological pathways, and confirmed its importance to P-NET's superior performance. Furthermore, we explored alternative neural architectures and approaches to incorporating biological information into the networks. We experimented with three types of graph neural networks on the same training data, and investigated the clinical prediction agreement between different models. Our analyses demonstrated that deep neural networks with distinct architectures make incorrect predictions for individual patient that are pers
    
[^138]: COCO-Counterfactuals:自动构建图像-文本对的反事实例

    COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v1 [cs.LG])

    [http://arxiv.org/abs/2309.14356](http://arxiv.org/abs/2309.14356)

    COCO-Counterfactuals是一个自动构建图像-文本对的反事实例的框架，通过使用文本到图像扩散模型来自动生成多模态反事实例。通过人工评估，我们验证了COCO-Counterfactuals的质量，并展示了其对于改善域外泛化能力的实用性。

    

    反事实例在自然语言处理(NLP)领域中已证明对于评估和改进语言模型对数据集中的虚假相关性的鲁棒性非常有价值。尽管反事实例在NLP领域具有显著的效用，但由于创建最小反事实变化的图像-文本配对数据的难度，多模态反事实例的研究相对较少。为了解决这一挑战，我们引入了一个可扩展的框架，利用文本到图像扩散模型自动生成反事实例。我们使用这个框架来创建COCO-Counterfactuals，这是一个基于MS-COCO数据集的多模态反事实数据集，包括图像和文本标题的配对。我们通过人工评估验证了COCO-Counterfactuals的质量，并展示了现有的多模态模型在我们的反事实图像-文本配对中面临的挑战。此外，我们展示了COCO-Counterfactuals在改善域外泛化能力方面的实用性。

    Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of 
    
[^139]: 使用深度学习和药动学先验预测治疗反应

    Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors. (arXiv:2309.13135v1 [cs.LG])

    [http://arxiv.org/abs/2309.13135](http://arxiv.org/abs/2309.13135)

    该研究提出了一种使用深度学习和药动学先验预测治疗反应的方法。研究者通过一个新颖的编码器提供药物的药动学信息，从而实现对时间序列的精确预测。实验结果显示，在逼真模拟和真实世界数据上，该方法比基准模型的预测准确性提高了约11%和8%。这种方法在临床实践中具有多种有益应用，如发出早期警告和定量特定患者的治疗效果。

    

    对医疗时间序列的预测对于早期检测不良结果和患者监测至关重要。然而，由于数据嘈杂和间歇性，实际中预测可能很困难。这些挑战通常通过外部因素诱导的变化点（如药物使用）而加剧。我们提出了一种新颖的编码器，以向深度学习模型提供药物的药动学效应信息，从而实现对受治疗影响的时间序列的准确预测。我们展示了我们方法在使用逼真模拟和真实世界数据预测血糖的任务中的有效性。我们的药动学编码器使深度学习模型在模拟数据上超过基准约11％，在真实世界数据上超过8％。所提出的方法可以在临床实践中具有多种有益应用，例如发出关于意外治疗反应的早期警告，或帮助表征特定于患者的治疗效果。

    Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. We propose a novel encoder that informs deep learning models of the pharmacokinetic effects of drugs to allow for accurate forecasting of time series affected by treatment. We showcase the effectiveness of our approach in a task to forecast blood glucose using both realistically simulated and real-world data. Our pharmacokinetic encoder helps deep learning models surpass baselines by approximately 11% on simulated data and 8% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in te
    
[^140]: 锐度感知最小化和稳定性边界。

    Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])

    [http://arxiv.org/abs/2309.12488](http://arxiv.org/abs/2309.12488)

    本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。

    

    最近的实验表明，当使用梯度下降(GD)训练神经网络时，损失函数的Hessian矩阵的操作符范数会增长，直到接近$2/\eta$，之后会在该值周围波动。根据对损失函数的局部二次逼近，$2/\eta$被称为“稳定性边界”。我们使用类似的计算方法，为锐度感知最小化(SAM)确定了一个“稳定性边界”，SAM是一种改进泛化性能的GD变种。与GD不同，SAM的稳定性边界取决于梯度的范数。通过三个深度学习任务的实证，我们观察到SAM在这个分析中确定的稳定性边界上运行。

    Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
    
[^141]: 评估潮起潮落：对不同平台间问答趋势的深入分析

    Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])

    [http://arxiv.org/abs/2309.05961](http://arxiv.org/abs/2309.05961)

    本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。

    

    社区问答平台因其快速回答用户查询的能力而越来越受欢迎。这些回答速度的快慢取决于查询特定和用户相关的因素的综合。本文通过研究六个高度流行的社区问答平台，分析了这些因素在其中的作用。我们的调查揭示了问题的第一个回答所花费的时间与元数据、问题的构成方式和用户之间的互动水平之间的关联。此外，通过使用传统的机器学习模型分析这些元数据和用户互动模式，我们试图预测哪些查询将迅速获得初始回答。

    Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
    
[^142]: 在物理系统中的紧急学习作为基于反馈的玻璃景观中的老化过程

    Emergent learning in physical systems as feedback-based aging in a glassy landscape. (arXiv:2309.04382v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2309.04382](http://arxiv.org/abs/2309.04382)

    通过训练线性物理网络学习线性变换，我们发现其学习行为与无序和玻璃状系统中的老化和记忆形成过程相似。学习动态类似于老化过程，通过重复施加反馈边界力来放松并编码输入-输出关系的记忆。

    

    通过训练线性物理网络来学习线性变换，我们发现它们的物理特性是由权重更新规则导致的。我们的研究结果突出了这样的网络学习行为与无序和玻璃状系统中老化和记忆形成过程之间的惊人相似之处。我们展示了学习动态类似于老化过程，系统在面对输入力的反馈边界力的重复施加时放松并编码了输入-输出关系的记忆。随着这种放松，相关长度增加，这可以通过网络的分量的两点相关函数来指示。我们还观察到误差均方根的平方根作为时间的函数呈非指数形式，这是玻璃系统的典型特征。这种物理解释表明通过将更详细的信息编码到输入和反馈边界中。

    By training linear physical networks to learn linear transformations, we discern how their physical properties evolve due to weight update rules. Our findings highlight a striking similarity between the learning behaviors of such networks and the processes of aging and memory formation in disordered and glassy systems. We show that the learning dynamics resembles an aging process, where the system relaxes in response to repeated application of the feedback boundary forces in presence of an input force, thus encoding a memory of the input-output relationship. With this relaxation comes an increase in the correlation length, which is indicated by the two-point correlation function for the components of the network. We also observe that the square root of the mean-squared error as a function of epoch takes on a non-exponential form, which is a typical feature of glassy systems. This physical interpretation suggests that by encoding more detailed information into input and feedback boundar
    
[^143]: 学习改进样本复杂性的零和线性二次博弈

    Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity. (arXiv:2309.04272v1 [eess.SY])

    [http://arxiv.org/abs/2309.04272](http://arxiv.org/abs/2309.04272)

    这项研究提出了改进样本复杂性的零和线性二次博弈，并发现了自然策略梯度方法的隐式正则化属性。在无模型参数知识的情况下，他们还提出了第一个多项式样本复杂性算法来达到Nash均衡。

    

    零和线性二次（LQ）博弈在最优控制中是基础性的，可以用于（i）风险敏感或鲁棒控制的动态博弈形式，或者（ii）作为连续状态-控制空间中两个竞争智能体的多智能体强化学习的基准设置。与广泛研究的单智能体线性二次调节器问题不同，零和LQ博弈涉及解决一个具有缺乏强制性的目标函数的具有挑战性的非凸非凹最小-最大问题。最近，张等人发现了自然策略梯度方法的隐式正则化属性，这对于安全关键的控制系统非常重要，因为它在学习过程中保持了控制器的鲁棒性。此外，在没有模型参数知识的模型无关设置中，张等人提出了第一个多项式样本复杂性算法，以达到Nash均衡的ε-邻域，同时保持理想的隐式正则化属性。

    Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit 
    
[^144]: 使用深度对称保持注意力网络重建不稳定的重粒子

    Reconstruction of Unstable Heavy Particles Using Deep Symmetry-Preserving Attention Networks. (arXiv:2309.01886v2 [hep-ex] UPDATED)

    [http://arxiv.org/abs/2309.01886](http://arxiv.org/abs/2309.01886)

    使用对称保持注意力网络 (Spa-Net) 对不稳定的重粒子进行重建，并扩展其能力以处理多种输入物体类型和全局事件特征。在顶夸克对的半轻子衰变和与希格斯玻色子共同产生的顶夸克对背景下，我们发现了显著的性能改进。

    

    重建不稳定的重粒子需要精密的技术来筛选出将探测器物体分配给底层粒子的大量可能排列。一种基于广义注意机制的方法，称为对称保持注意力网络 (Spa-Net)，先前已应用于只产生强子喷注的大型强子对撞机中的顶夸克对衰变。在这里，我们扩展了 Spa-Net 架构来考虑多种输入物体类型，如轻子，以及全局事件特征，如失去的横向动量。此外，我们提供回归和分类输出以补充粒子分配。我们在顶夸克对的半轻子衰变以及与希格斯玻色子共同产生的顶夸克对的背景下探讨了 Spa-Net 扩展能力的性能。我们发现在三个代表性研究的能力方面有显著的改进：ttH 搜索、顶夸克对的测量等等。

    Reconstructing unstable heavy particles requires sophisticated techniques to sift through the large number of possible permutations for assignment of detector objects to the underlying partons. An approach based on a generalized attention mechanism, symmetry preserving attention networks (Spa-Net), has been previously applied to top quark pair decays at the Large Hadron Collider which produce only hadronic jets. Here we extend the Spa-Net architecture to consider multiple input object types, such as leptons, as well as global event features, such as the missing transverse momentum. In addition, we provide regression and classification outputs to supplement the parton assignment. We explore the performance of the extended capability of Spa-Net in the context of semi-leptonic decays of top quark pairs as well as top quark pairs produced in association with a Higgs boson. We find significant improvements in the power of three representative studies: a search for ttH, a measurement of the 
    
[^145]: SGMM: 广义矩方法的随机近似

    SGMM: Stochastic Approximation to Generalized Method of Moments. (arXiv:2308.13564v1 [econ.EM])

    [http://arxiv.org/abs/2308.13564](http://arxiv.org/abs/2308.13564)

    我们提出了一种新的随机广义矩方法（SGMM），用于估计和推断矩限制模型。该方法具有快速和可扩展的实时处理能力，并且能够处理大规模和在线数据集。

    

    我们引入了一种新的算法类，随机广义矩方法（SGMM），用于估计和推断（超识别）矩限制模型。我们的SGMM是一种新颖的随机逼近方法，替代了流行的Hansen（1982年）的（离线）GMM，并提供了快速和可扩展的实时流数据处理能力。我们证明了SGMM对于效率不高的在线2SLS和高效的SGMM具有几乎确定的收敛性和（函数）中心极限定理。此外，我们提出了Durbin-Wu-Hausman和Sargan-Hansen测试的在线版本，可以无缝集成到SGMM框架中。广泛的蒙特卡洛模拟结果表明，随着样本量的增加，SGMM在估计准确性和计算效率方面与标准（离线）GMM相匹配，并显示出在大规模和在线数据集上的实际价值。我们通过使用两个示例证明了我们方法的有效性。

    We introduce a new class of algorithms, Stochastic Generalized Method of Moments (SGMM), for estimation and inference on (overidentified) moment restriction models. Our SGMM is a novel stochastic approximation alternative to the popular Hansen (1982) (offline) GMM, and offers fast and scalable implementation with the ability to handle streaming datasets in real time. We establish the almost sure convergence, and the (functional) central limit theorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we propose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that can be seamlessly integrated within the SGMM framework. Extensive Monte Carlo simulations show that as the sample size increases, the SGMM matches the standard (offline) GMM in terms of estimation accuracy and gains over computational efficiency, indicating its practical value for both large-scale and online datasets. We demonstrate the efficacy of our approach by a proof of concept using two we
    
[^146]: 超越文档页分类：设计、数据集和挑战

    Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])

    [http://arxiv.org/abs/2308.12896](http://arxiv.org/abs/2308.12896)

    本文强调了将文档分类基准测试更接近于现实世界应用的需求，通过提出多页文档分类数据集和不同分类任务，以及高效的多页文档表示，来解决现有基准测试不适用于实际完整文档评估的问题。

    

    本文强调了将文档分类基准测试更接近于现实世界应用的需求，即在测试数据的性质上（$X$：多通道、多页、多行业；$Y$：类别分布和标签集的多样性）和考虑的分类任务上（$f$：多页文档、页面流和文档捆绑分类，...）。我们确定了公共的多页文档分类数据集的缺乏，并规范了应用场景中产生的不同分类任务，并激发了以高效的多页文档表示为目标的价值。对提出的多页文档分类数据集进行的实验研究表明，当前的基准测试已经变得无关紧要，并需要更新以评估实际中自然发生的完整文档。这个现实情况检查也呼吁更成熟的评估方法，涵盖校准评估、推理复杂性（时间-内存）和一系列现实分散情况。

    This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distr
    
[^147]: 使用最优输运解决图像分类中的公平性和可解释性问题

    Addressing Fairness and Explainability in Image Classification Using Optimal Transport. (arXiv:2308.11090v1 [cs.CV])

    [http://arxiv.org/abs/2308.11090](http://arxiv.org/abs/2308.11090)

    本论文提出了使用最优输运理论解决图像分类中公平性和可解释性问题的综合方法，通过发现和解释模型中偏见区域的原因和影响来提供细粒度的解释。

    

    在诸如医疗保健和执法等领域中，算法公平性和对潜在不公平结果的解释是建立人工智能系统信任和问责性的关键。尽管在每个领域分别取得了重大进展，但在使用深度神经网络的领域中实现公平性应用的可解释性仍然具有挑战性。与此同时，伦理数据挖掘变得越来越重要，因为多次研究表明不关注公平性的算法会导致有偏差的结果。目前的方法主要关注在模型的输出中减轻偏见，但很少有尝试解释模型为什么存在偏见。为了弥补这一差距，我们提出了一种综合方法，利用最优输运理论来发现图像中有偏差区域的原因和影响，这种方法也可以轻松地扩展到表格数据。通过使用Wasserstein barycenters，我们可以以细粒度的方式解释模型的偏见。

    Algorithmic Fairness and the explainability of potentially unfair outcomes are crucial for establishing trust and accountability of Artificial Intelligence systems in domains such as healthcare and policing. Though significant advances have been made in each of the fields separately, achieving explainability in fairness applications remains challenging, particularly so in domains where deep neural networks are used. At the same time, ethical data-mining has become ever more relevant, as it has been shown countless times that fairness-unaware algorithms result in biased outcomes. Current approaches focus on mitigating biases in the outcomes of the model, but few attempts have been made to try to explain \emph{why} a model is biased. To bridge this gap, we propose a comprehensive approach that leverages optimal transport theory to uncover the causes and implications of biased regions in images, which easily extends to tabular data as well. Through the use of Wasserstein barycenters, we o
    
[^148]: SE(3)等变增强耦合流

    SE(3) Equivariant Augmented Coupling Flows. (arXiv:2308.10364v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10364](http://arxiv.org/abs/2308.10364)

    本文提出了一种SE(3)等变增强耦合流，可以快速采样和密度评估，通过在坐标分割中保持等变性。

    

    耦合标准化流能够快速采样和密度评估，使其成为物理系统概率建模的首选工具。然而，标准耦合架构无法赋予操作原子笛卡尔坐标的流SE(3)和置换不变性。本文提出了一种通过沿附加增强维度进行坐标分割的耦合流，以保持SE(3)和置换等变性。在每一层中，流将原子的位置映射到学习得到的SE(3)不变基上，我们在返回到原始基之前应用标准流变换，如单调分子有理二次样条。关键是，我们的流保持了快速采样和密度评估，并且可以通过重要性采样产生对目标分布的期望的无偏估计。在DW4、LJ13和QM9位置数据集上训练时，我们的流与等变流有竞争力。

    Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13, and QM9-positional datasets, our flow is competitive with equivari
    
[^149]: 机器人抓取中的稀疏奖励和稀疏交互下的优质多样性：应用于机器人学中的抓取任务

    Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics. (arXiv:2308.05483v1 [cs.RO])

    [http://arxiv.org/abs/2308.05483](http://arxiv.org/abs/2308.05483)

    这项研究针对机器人抓取任务中的奖励稀疏性、行为稀疏性和行为空间错位等挑战，探讨了优质多样性方法解决该问题的能力。通过在多个实验领域中进行实验，结果表明...

    

    优质多样性（Quality-Diversity，QD）方法旨在生成一组多样性和高性能的解决方案来解决给定问题。在进化机器人学中最初开发，大多数QD研究都是在有限的一组领域中进行的，主要应用于运动，其中适应度和行为信号是密集的。抓取是机器人操作中的关键任务。尽管许多研究社区的努力，但该任务尚未得到解决。抓取在QD文献中面临着前所未有的挑战：奖励稀疏性，行为稀疏性和行为空间错位。本研究探讨了QD如何解决抓取问题。在10个抓取领域上进行了15种不同方法的实验，对应于2种不同的机器人夹持器设置和5种标准物体。还提出了一个评估框架，以区分算法的评估与其内部组件的评估，以便进行公平比较。得到的结果表明：

    Quality-Diversity (QD) methods are algorithms that aim to generate a set of diverse and high-performing solutions to a given problem. Originally developed for evolutionary robotics, most QD studies are conducted on a limited set of domains - mainly applied to locomotion, where the fitness and the behavior signal are dense. Grasping is a crucial task for manipulation in robotics. Despite the efforts of many research communities, this task is yet to be solved. Grasping cumulates unprecedented challenges in QD literature: it suffers from reward sparsity, behavioral sparsity, and behavior space misalignment. The present work studies how QD can address grasping. Experiments have been conducted on 15 different methods on 10 grasping domains, corresponding to 2 different robot-gripper setups and 5 standard objects. An evaluation framework that distinguishes the evaluation of an algorithm from its internal components has also been proposed for a fair comparison. The obtained results show that 
    
[^150]: 一个用于土木工程结构的数字孪生框架

    A digital twin framework for civil engineering structures. (arXiv:2308.01445v1 [math.NA])

    [http://arxiv.org/abs/2308.01445](http://arxiv.org/abs/2308.01445)

    本研究提出了一个用于土木工程结构的预测数字孪生方法，它采用概率图模型编码资产孪生耦合动态系统，通过深度学习模型同化感测数据来提供实时的结构健康诊断，不断更新数字孪生状态并用于优化维护和管理规划。

    

    数字孪生概念为土木工程系统的基于条件和预测维护范式的推进提供了有吸引力的机会，从而实现降低生命周期成本、增加系统安全性和提高系统可用性。本文提出了一种预测性的数字孪生方法，用于土木工程结构的健康监测、维护和管理规划。资产孪生耦合动态系统采用概率图模型进行编码，从而可以考虑到所有相关的不确定性来源。特别是，采用动态贝叶斯网络对时间重复的观测-决策流进行建模。通过将感测数据与深度学习模型进行同化，实时的结构健康诊断得以提供。数字孪生状态以顺序贝叶斯推理的方式不断更新。然后利用这些信息来动态决策下的维护和管理行动的最优规划。

    The digital twin concept represents an appealing opportunity to advance condition-based and predictive maintenance paradigms for civil engineering systems, thus allowing reduced lifecycle costs, increased system safety, and increased system availability. This work proposes a predictive digital twin approach to the health monitoring, maintenance, and management planning of civil engineering structures. The asset-twin coupled dynamical system is encoded employing a probabilistic graphical model, which allows all relevant sources of uncertainty to be taken into account. In particular, the time-repeating observations-to-decisions flow is modeled using a dynamic Bayesian network. Real-time structural health diagnostics are provided by assimilating sensed data with deep learning models. The digital twin state is continually updated in a sequential Bayesian inference fashion. This is then exploited to inform the optimal planning of maintenance and management actions within a dynamic decision-
    
[^151]: ChatGPT的行为随时间变化如何？

    How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])

    [http://arxiv.org/abs/2307.09009](http://arxiv.org/abs/2307.09009)

    本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。

    

    GPT-3.5和GPT-4是两种广泛使用的大型语言模型（LLM）服务。然而，这些模型何时以及如何进行更新是不透明的。在这里，我们对GPT-3.5和GPT-4的2023年3月和2023年6月版本进行了评估，涉及四项不同的任务：1）解决数学问题，2）回答敏感/危险问题，3）生成代码和4）视觉推理。我们发现，GPT-3.5和GPT-4的性能和行为在时间上可以有很大的变化。例如，GPT-4（2023年3月）在识别质数方面表现非常出色（准确率为97.6%），但GPT-4（2023年6月）在相同的问题上表现非常差（准确率为2.4%）。有趣的是，GPT-3.5（2023年6月）在这个任务上比GPT-3.5（2023年3月）要好得多。GPT-4在6月份对回答敏感问题的意愿较3月份要低，而无论是GPT-4还是GPT-3.5在6月份的代码生成中都有更多的格式错误。总体而言，我们的发现表明相同LLM服务的行为在相对较短的时间内可以发生重大变化。

    GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
    
[^152]: 借鉴熵最优输运、镜像下降和共轭梯度的文献，我们设计了一种高效准确的最优输运算法

    Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients. (arXiv:2307.08507v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08507](http://arxiv.org/abs/2307.08507)

    本文设计了一种借鉴了多个文献的算法，通过使用镜像下降和共轭梯度的技术，能够高效准确地计算Wasserstein距离，并且在高维问题上比其他算法具有快速收敛的优势。

    

    本文设计了一种新颖的最优输运算法，通过借鉴熵最优输运、镜像下降和共轭梯度的文献。我们的算法可扩展且可在GPU上并行计算，能够以极高的精度计算Wasserstein距离，使相对误差达到$10^{-8}$，并且没有数值稳定性问题。实证上，与包括对数域稳定Sinkhorn算法在内的多种算法相比，我们的算法能够更快地达到高精度解，具有更短的墙钟时间。我们详细地分析了算法和问题参数，并在MNIST图像上进行了基准测试，与各种最新的高维问题算法进行了比较。结果表明我们的算法可以成为从业人员最优输运工具包中有用的补充。

    We design a novel algorithm for optimal transport by drawing from the entropic optimal transport, mirror descent and conjugate gradients literatures. Our scalable and GPU parallelizable algorithm is able to compute the Wasserstein distance with extreme precision, reaching relative error rates of $10^{-8}$ without numerical stability issues. Empirically, the algorithm converges to high precision solutions more quickly in terms of wall-clock time than a variety of algorithms including log-domain stabilized Sinkhorn's Algorithm. We provide careful ablations with respect to algorithm and problem parameters, and present benchmarking over upsampled MNIST images, comparing to various recent algorithms over high-dimensional problems. The results suggest that our algorithm can be a useful addition to the practitioner's optimal transport toolkit.
    
[^153]: 手术阶段和器械识别：如何识别适当的数据集划分

    Surgical Phase and Instrument Recognition: How to identify appropriate Dataset Splits. (arXiv:2306.16879v1 [cs.LG])

    [http://arxiv.org/abs/2306.16879](http://arxiv.org/abs/2306.16879)

    本研究开发了一个基于Web的应用程序，用于手术工作流和器械识别的机器学习模型。通过可视化框架，能够评估手术工作流识别的数据集划分，特别是识别次优的划分。

    

    目的：从时间数据中开发用于手术工作流和器械识别的机器学习模型是一项具有挑战性的任务，由于手术工作流的复杂性质，数据的不平衡分布是手术工作流识别领域面临的主要挑战之一。为了获取有意义的结果，将数据仔细划分为训练集、验证集和测试集，以及选择合适的评估指标是至关重要的。方法：在这项工作中，我们提出了一个开放的基于Web的应用程序，可以进行数据集划分的交互式探索。所提出的可视化框架有助于评估手术工作流识别的数据集划分，特别是识别次优的数据集划分。目前，它支持手术阶段和器械注释的可视化。结果：为了验证专用的交互式可视化，我们使用了一个数据集划分

    Purpose: The development of machine learning models for surgical workflow and instrument recognition from temporal data represents a challenging task due to the complex nature of surgical workflows. In particular, the imbalanced distribution of data is one of the major challenges in the domain of surgical workflow recognition. In order to obtain meaningful results, careful partitioning of data into training, validation, and test sets, as well as the selection of suitable evaluation metrics are crucial. Methods: In this work, we present an openly available web-based application that enables interactive exploration of dataset partitions. The proposed visual framework facilitates the assessment of dataset splits for surgical workflow recognition, especially with regard to identifying sub-optimal dataset splits. Currently, it supports visualization of surgical phase and instrument annotations. Results: In order to validate the dedicated interactive visualizations, we use a dataset split of
    
[^154]: 长期信用归因通过反事实贡献分析的方式

    Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis. (arXiv:2306.16803v1 [cs.LG])

    [http://arxiv.org/abs/2306.16803](http://arxiv.org/abs/2306.16803)

    本文提出了一种新的基于模型的信用分配算法，通过量化反事实查询来测量动作对未来奖励的影响。与现有方法不同的是，我们通过测量对奖励或奖励对象表示的贡献，获得了具有更低方差的梯度估计。

    

    为了使强化学习更加样本高效，我们需要更好的信用归因方法来衡量动作对未来奖励的影响。在悔棋信用归因（HCA）的基础上，我们引入了反事实贡献分析（COCOA），这是一种新的基于模型的信用归因算法系列。我们的算法通过量化一个反事实查询来实现精确的信用分配：“如果代理选择另一个动作，它仍然会获得这个奖励吗？”通过测量动作对获得后续奖励的贡献，我们展示了对于奖励状态测量贡献（即HCA中所做的）会导致贡献的错误估计，使得HCA在许多相关环境中向高方差的REINFORCE估计器退化。相反，我们通过测量对奖励或所学习的奖励对象的表示的贡献，得到具有更低方差的梯度估计。我们在一系列特定问题上进行了实验

    To make reinforcement learning more sample efficient, we need better credit assignment methods that measure an action's influence on future rewards. Building upon Hindsight Credit Assignment (HCA), we introduce Counterfactual Contribution Analysis (COCOA), a new family of model-based credit assignment algorithms. Our algorithms achieve precise credit assignment by measuring the contribution of actions upon obtaining subsequent rewards, by quantifying a counterfactual query: "Would the agent still have reached this reward if it had taken another action?". We show that measuring contributions w.r.t. rewarding states, as is done in HCA, results in spurious estimates of contributions, causing HCA to degrade towards the high-variance REINFORCE estimator in many relevant environments. Instead, we measure contributions w.r.t. rewards or learned representations of the rewarding objects, resulting in gradient estimates with lower variance. We run experiments on a suite of problems specifically 
    
[^155]: 可分离的物理信息神经网络

    Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])

    [http://arxiv.org/abs/2306.15969](http://arxiv.org/abs/2306.15969)

    这项研究提出了一种可分离的物理信息神经网络（SPINN），通过逐个处理轴来显著减少了多维 PDE 中的网络传播数量，并使用正向模式自动微分降低了计算成本，使得可以在单个普通 GPU 上使用大量的配点。

    

    物理信息神经网络(PINNs)最近已经成为有希望的基于数据的PDE求解器，在各种PDE上显示出令人鼓舞的结果。然而，训练PINNs来解决多维PDE和逼近高度复杂解函数存在根本限制。在这些具有挑战性的PDE上所需的训练点数量(配点)大大增加，但由于昂贵的计算成本和庞大的内存开销，其受到严重限制。为了解决这个问题，我们提出了一种用于PINNs的网络架构和训练算法。所提出的方法，可分离的PINN (SPINN)，在多维PDE中按轴逐个处理，从而显著减少了网络传播的数量，不同于传统PINNs中的逐点处理。我们还提出使用正向模式自动微分来降低计算PDE残差的计算成本，从而在单个普通GPU上可以使用大量的配点(>10^7)。

    Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (>10^7) on a single commodity GPU. The
    
[^156]: 为大型图表示简化和增强Transformer

    Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10759](http://arxiv.org/abs/2306.10759)

    本文通过实验证明，在大型图上使用一层注意力即可获得令人惊讶的竞争性能，挑战了在语言和视觉任务中复杂模型的应用。这促使我们重新思考在大型图上设计Transformer的理念，以提高可扩展性。

    

    在大型图上学习表示是一个长期存在的挑战，因为其中涉及了大量数据点之间的相互依赖关系。Transformer作为一种新兴的用于图结构数据的基本编码器类别，由于其全局注意力可以捕捉到邻节点之外的所有对影响，因此在小型图上表现出了有希望的性能。尽管如此，现有方法往往继承了Transformer在语言和视觉任务中的思想，并通过堆叠深层多头注意力来采用复杂的模型。本文通过关于节点属性预测基准的实验证明，即使只使用一层注意力也能在节点数量从千级到十亿级的范围内带来令人惊讶的竞争性能。这鼓励我们重新思考在大型图上设计Transformer的理念，其中全局注意力是一个阻碍可扩展性的计算开销。我们将提出的方案称为简化图Transformer。

    Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
    
[^157]: PLASTIC: 改善样本高效强化学习的输入和标签可塑性

    PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning. (arXiv:2306.10711v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10711](http://arxiv.org/abs/2306.10711)

    PLASTIC算法通过改善模型的输入和标签可塑性，提高样本高效强化学习的效果。

    

    在强化学习（RL）中，提高样本效率是至关重要的，特别是在数据获取成本高昂且风险高的情况下。原则上，离策略RL算法可以通过允许每个环境交互进行多次更新来提高样本效率。然而，这些多次更新往往导致模型过度拟合之前的交互，这被称为可塑性的丧失。我们的研究将可塑性分为两个方面进行调查。输入可塑性，指的是模型对变化的输入数据的适应能力，标签可塑性，指的是模型对不断演化的输入输出关系的适应能力。对CIFAR-10数据集进行的合成实验表明，在损失概览中寻找更平滑的最小值可以增强输入可塑性，而细化的梯度传播可以提高标签可塑性。基于这些发现，我们提出了PLASTIC算法，它融合了这两方面的技术来解决这个问题。

    In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier interactions, which is referred to as the loss of plasticity. Our study investigates the underlying causes of this phenomenon by dividing plasticity into two aspects. Input plasticity, which denotes the model's adaptability to changing input data, and label plasticity, which denotes the model's adaptability to evolving input-output relationships. Synthetic experiments on the CIFAR-10 dataset reveal that finding smoother minima of loss landscape enhances input plasticity, whereas refined gradient propagation improves label plasticity. Leveraging these findings, we introduce the PLASTIC algorithm, which harmoniously combines techniques to address both
    
[^158]: 基于评分的数据同化

    Score-based Data Assimilation. (arXiv:2306.10574v1 [cs.LG])

    [http://arxiv.org/abs/2306.10574](http://arxiv.org/abs/2306.10574)

    本文介绍了基于评分的数据同化方法，通过对状态轨迹模型的训练，实现了无需依赖传统推断方法和满足高维系统与长时间跨度下进行推断。

    

    在最全面的形式下，数据同化解决了鉴定随机动态系统中的可能状态轨迹的贝叶斯逆问题，从而解释尽管存在噪声或不完整观测的内容。已经提出了各种方法来解决这个问题，包括基于粒子的和可变方法。然而，大多数算法依赖于转移动态进行推断，这在长时间跨度或具有复杂动态的高维系统中变得棘手，如海洋或大气。本文介绍了基于评分的数据同化来实现轨迹推断。我们学习了基于评分的生成状态轨迹模型，这是基于一个关键洞察，即任意长轨迹的得分可以分解为短部分的得分系列。在训练完成后，运用得分模型进行无自回归的推断，通过同时生成所有状态。与众不同的是，我们解耦了观测。

    Data assimilation, in its most comprehensive form, addresses the Bayesian inverse problem of identifying plausible state trajectories that explain noisy or incomplete observations of stochastic dynamical systems. Various approaches have been proposed to solve this problem, including particle-based and variational methods. However, most algorithms depend on the transition dynamics for inference, which becomes intractable for long time horizons or for high-dimensional systems with complex dynamics, such as oceans or atmospheres. In this work, we introduce score-based data assimilation for trajectory inference. We learn a score-based generative model of state trajectories based on the key insight that the score of an arbitrarily long trajectory can be decomposed into a series of scores over short segments. After training, inference is carried out using the score model, in a non-autoregressive manner by generating all states simultaneously. Quite distinctively, we decouple the observation 
    
[^159]: 学习受限动力学的稳定神经微分方程

    Stabilized Neural Differential Equations for Learning Constrained Dynamics. (arXiv:2306.09739v1 [cs.LG])

    [http://arxiv.org/abs/2306.09739](http://arxiv.org/abs/2306.09739)

    本文提出了一种稳定神经微分方程（SNDEs）的方法，可以强制使用任意流形约束。该方法通过添加稳定项使约束流形成为渐进稳定的，并且在实验中表现优于现有方法。

    

    最近出现了许多成功的从数据学习动态系统的方法。然而，确保推断出的动态系统保留已知约束条件（例如守恒定律或对允许的系统状态的限制）仍然具有挑战性。我们提出了稳定神经微分方程（SNDEs）的方法，这是一种用于神经微分方程强制使用任意流形约束的方法。我们的方法基于一个稳定项，当添加到原始动态系统中时，可以将约束流形成为渐进稳定的。由于其简单性，我们的方法与所有常见的神经常微分方程（NODE）模型兼容并广泛适用。在广泛的经验评估中，我们证明SNDE在扩展可纳入NODE训练的约束类型方面胜过现有方法。

    Many successful methods to learn dynamical systems from data have recently been introduced. However, assuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose stabilized neural differential equations (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural ordinary differential equation (NODE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while extending the scope of which types of constraints can be incorporated into NODE training.
    
[^160]: QH9：QM9分子的量子哈密顿预测基准测试

    QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.09549](http://arxiv.org/abs/2306.09549)

    该论文提出了一种新的量子哈密顿数据集QH9，用于为各种分子提供精确的哈密顿矩阵。通过设计基准任务，展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。

    

    监督式机器学习方法越来越被用于加速电子结构预测，作为第一性原理计算方法（如密度泛函理论（DFT））的替代品。虽然许多量子化学数据集侧重于化学性质和原子力，但准确且高效地预测哈密顿矩阵的能力是非常重要和基本的物理量，它确定了物理系统和化学性质的量子状态。在这项工作中，我们生成了一个新的量子哈密顿数据集，命名为QH9，基于QM9数据集为2,399个分子动力学轨迹和130,831个稳定分子几何形态提供精确的哈密顿矩阵。通过设计各种分子的基准任务，我们展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。QH9数据集和基准模型都提供。

    Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
    
[^161]: 非线性潜变量层次模型的识别

    Identification of Nonlinear Latent Hierarchical Models. (arXiv:2306.07916v1 [cs.LG])

    [http://arxiv.org/abs/2306.07916](http://arxiv.org/abs/2306.07916)

    本文提出了一种方法，可以在观测变量由因果相关的潜变量生成的非线性潜变量层次因果模型中实现因果结构和潜变量的可识别性。

    

    从观测数据中识别潜变量和因果结构对于许多涉及生物数据、医学数据和非结构化数据（如图像和语言）的实际应用至关重要。然而，当观测变量由因果相关的潜变量生成，并且关系是非线性的时，这项任务可能非常具有挑战性。在这项工作中，我们研究了非线性潜变量层次因果模型的识别问题，在这种模型中，观测变量由一组因果相关的潜变量生成，有些潜变量可能没有观察到的后代。我们证明，在温和的假设下可以实现因果结构和潜变量的可识别性：对于因果结构，我们允许图中任意两个变量之间存在多条路径，这放宽了先前工作中的潜变量树假设；对于结构函数，我们没有进行参数假设，因此可以允许基因

    Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of both causal structure and latent variables can be achieved under mild assumptions: on causal structures, we allow for the existence of multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we do not make parametric assumptions, thus permitting gene
    
[^162]: 关于Removal-Based特征归因的鲁棒性研究

    On the Robustness of Removal-Based Feature Attributions. (arXiv:2306.07462v1 [cs.LG])

    [http://arxiv.org/abs/2306.07462](http://arxiv.org/abs/2306.07462)

    本文研究了Removal-Based特征归因的鲁棒性，提供了全面的理论和实验分析，并证明了所提方法的实际有效性。

    

    为了解释基于输入的复杂模型，开发了许多特征归因方法来分配输入特征的重要性分数。然而，最近的一些研究挑战了特征归因的鲁棒性，指出这些方法对输入和模型扰动敏感，而其他研究通过提出鲁棒归因方法和模型修改来解决这个问题。然而，以往的归因鲁棒性研究主要侧重于基于梯度的特征归因。相比之下，Removal-Based归因方法的鲁棒性质尚未全面地得到理解。为了弥补这一差距，我们从理论上对Removal-Based特征归因的鲁棒性进行了全面的阐述。具体而言，我们对这种方法进行了统一的分析，并在输入和模型扰动的情况下证明了完好和受扰动的归因之间的差异的上限。我们在合成和真实数据集上的实验验证了我们的理论结果，并证明了所提出方法的实际有效性。

    To explain complex models based on their inputs, many feature attribution methods have been developed that assign importance scores to input features. However, some recent work challenges the robustness of feature attributions by showing that these methods are sensitive to input and model perturbations, while other work addresses this robustness issue by proposing robust attribution methods and model modifications. Nevertheless, previous work on attribution robustness has focused primarily on gradient-based feature attributions. In contrast, the robustness properties of removal-based attribution methods are not comprehensively well understood. To bridge this gap, we theoretically characterize the robustness of removal-based feature attributions. Specifically, we provide a unified analysis of such methods and prove upper bounds for the difference between intact and perturbed attributions, under settings of both input and model perturbations. Our empirical experiments on synthetic and re
    
[^163]: TrojPrompt：基于黑盒方式的预训练语言模型木马攻击

    TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)

    [http://arxiv.org/abs/2306.06815](http://arxiv.org/abs/2306.06815)

    本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。

    

    Prompt学习被证明在提高预训练语言模型（PLM）适应性方面非常有效，超越了传统的微调范式，并在专为少样本学习场景量身定制的应用程序和API中展现了杰出的前景。但是，尽管prompt学习的API越来越受欢迎，但它们的安全问题仍未得到充分探索。本文在prompt学习的PLM API的特洛伊易感性方面进行了开创性研究。我们发现，离散提示，少样本和黑盒设置是几个关键挑战，限制了现有后门攻击的适用性。为了解决这些挑战，我们提出了TrojPrompt，这是一种自动的黑盒框架，可有效生成通用的和隐秘的触发器，并将特洛伊木马插入硬提示。具体而言，我们提出了一种API驱动的通用触发器发现算法，通过查询受害者PLM API，为各种输入生成通用触发器。

    Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
    
[^164]: 变分不平衡回归(Variational Imbalanced Regression)

    Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])

    [http://arxiv.org/abs/2306.06599](http://arxiv.org/abs/2306.06599)

    本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。

    

    当标签分布不平衡时，现有的回归模型往往在准确性和不确定性估计方面表现不佳。本文提出了一种概率深度学习模型——变分不平衡回归（VIR），它不仅在不平衡回归方面表现出色，而且自然地产生合理的不确定性估计。与典型的变分自编码器假设I.I.D.表示（数据点的表示不直接受其他数据点的影响）不同，我们的VIR借用具有类似回归标签的数据来计算潜在表示的变分分布；此外，不同于产生点估计的确定性回归模型， VIR预测整个正态反-伽玛分布并调节相关联的共轭分布，对不平衡数据施加概率重新加权，从而提供更好的不确定性估计。在几个真实世界的数据集上进行了实验。

    Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
    
[^165]: 从随机过程中学习先验知识的差分隐私图像分类

    Differentially Private Image Classification by Learning Priors from Random Processes. (arXiv:2306.06076v1 [cs.CV])

    [http://arxiv.org/abs/2306.06076](http://arxiv.org/abs/2306.06076)

    本文提出了一种名为DP-RandP的方法，从随机过程中学习先验知识，并将其传递给私有数据，以改进差分隐私的图像分类，实现了新的最先进的结果，提高了CIFAR-10的精度。

    

    在隐私保护的机器学习中，不同ially私有的随机梯度下降（DP-SGD）由于每个样本梯度剪辑和噪声添加而表现不佳。隐私学习研究的一个最近重点是通过将在真实世界公共数据上学习的先验知识纳入这些数据，从而提高DP-SGD在私有数据上的性能。在这项工作中，我们探讨了如何通过从由随机过程生成的图像中学习先验知识并将这些先验知识转移到私有数据来改进DP-SGD的隐私-效用折衷。我们提出了DP-RandP，这是一个三阶段的方法。在CIFAR10、CIFAR100和MedMNIST上从头开始训练时，我们获得了新的最先进的结果，并适用于一系列隐私预算ε∈[1，8]。特别地，我们将在ε=1时在CIFAR10上报告的最佳准确性从60.6%提高到72.3%。我们的代码可在https://github.com/inspire-group/DP-RandP上找到。

    In privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) performs worse than SGD due to per-sample gradient clipping and noise addition. A recent focus in private learning research is improving the performance of DP-SGD on private data by incorporating priors that are learned on real-world public data. In this work, we explore how we can improve the privacy-utility tradeoff of DP-SGD by learning priors from images generated by random processes and transferring these priors to private data. We propose DP-RandP, a three-phase approach. We attain new state-of-the-art accuracy when training from scratch on CIFAR10, CIFAR100, and MedMNIST for a range of privacy budgets $\varepsilon \in [1, 8]$. In particular, we improve the previous best reported accuracy on CIFAR10 from $60.6 \%$ to $72.3 \%$ for $\varepsilon=1$. Our code is available at https://github.com/inspire-group/DP-RandP.
    
[^166]: 用信息理论的Shapley值解释预测的不确定性

    Explaining Predictive Uncertainty with Information Theoretic Shapley Values. (arXiv:2306.05724v1 [stat.ML])

    [http://arxiv.org/abs/2306.05724](http://arxiv.org/abs/2306.05724)

    本文提出了一种新的方法，通过Shapley值解释不确定性预测，可以量化每个特征对个别模型输出条件熵的贡献，适用于协变量转移检测、主动学习、特征选择和活动特征价值评估等方面。

    

    可解释人工智能研究人员开发了大量方法来帮助用户理解复杂监督学习模型的预测结果。相比之下，解释模型输出的$\textit{不确定性}$却受到了相对较少的关注。我们将广泛使用的Shapley值框架用于解释各种类型的预测不确定性，量化每个特征对个别模型输出条件熵的贡献。我们考虑了修改特征函数的博弈，并发现了由此产生的Shapley值与信息论和条件独立性测试中的基本量之间的深刻联系。我们概述了有证明保证的有限样本误差率控制的推理过程，并实现了一种高效的算法，在真实和模拟数据的一系列实验中表现良好。我们的方法适用于协变量转移检测、主动学习、特征选择和活动特征价值评估等方面。

    Researchers in explainable artificial intelligence have developed numerous methods for helping users understand the predictions of complex supervised learning models. By contrast, explaining the $\textit{uncertainty}$ of model outputs has received relatively little attention. We adapt the popular Shapley value framework to explain various types of predictive uncertainty, quantifying each feature's contribution to the conditional entropy of individual model outputs. We consider games with modified characteristic functions and find deep connections between the resulting Shapley values and fundamental quantities from information theory and conditional independence testing. We outline inference procedures for finite sample error rate control with provable guarantees, and implement an efficient algorithm that performs well in a range of experiments on real and simulated data. Our method has applications to covariate shift detection, active learning, feature selection, and active feature-val
    
[^167]: 多体SE（3）等变性用于无监督的刚体分割和运动估计

    Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation. (arXiv:2306.05584v1 [cs.CV])

    [http://arxiv.org/abs/2306.05584](http://arxiv.org/abs/2306.05584)

    本文提出了一种基于SE（3）等变结构和非监督训练策略的方法，可以实现刚性分割和运动估计，不需要类别信息且具有极高的模型效率。

    

    实现刚体分割和运动估计的真正通用方法对于理解关节物体和移动场景的三维影像至关重要。鉴于分割和运动估计之间密切的关系，我们提出了一种SE（3）等变体系结构和培训策略，以无监督的方式解决这个任务。我们的体系结构包括两个轻量级和相互连接的头部，使用点级不变特征和来自SE（3）等变特征的运动估计来预测分割掩模，而不需要类别信息。我们的统一培训策略可以在线执行，通过利用场景流，分割掩模和刚性变换之间的相互关系来同时优化两个预测。我们在四个数据集上的实验表明，我们的方法在模型性能和计算效率方面均表现出优越性，只有0.25M参数和0.92G FLOPs。

    A truly generalizable approach to rigid segmentation and motion estimation is fundamental to 3D understanding of articulated objects and moving scenes. In view of the tightly coupled relationship between segmentation and motion estimates, we present an SE(3) equivariant architecture and a training strategy to tackle this task in an unsupervised manner. Our architecture comprises two lightweight and inter-connected heads that predict segmentation masks using point-level invariant features and motion estimates from SE(3) equivariant features without the prerequisites of category information. Our unified training strategy can be performed online while jointly optimizing the two predictions by exploiting the interrelations among scene flow, segmentation mask, and rigid transformations. We show experiments on four datasets as evidence of the superiority of our method both in terms of model performance and computational efficiency with only 0.25M parameters and 0.92G FLOPs. To the best of ou
    
[^168]: 使用大型语言模型注释进行社会科学中的有效下游统计推断: 基于设计的半监督学习

    Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])

    [http://arxiv.org/abs/2306.04746](http://arxiv.org/abs/2306.04746)

    该论文提出了一种新算法，使用大型语言模型（LLMs）输出进行下游统计分析，以实现有效的下游统计推断，并降低标签获取的研究成本80％，同时保证CSS研究的统计属性。

    

    在计算社会科学（CSS）中，研究人员通过分析文档来解释社会和政治现象。在大多数情况下，CSS研究人员首先获取文档的标签，然后使用可解释的回归分析来解释标签。大型语言模型（LLMs）的最近进展可以通过在规模上便宜地注释文档来降低CSS研究成本，但这些替代标签通常是不完美和有偏的。我们提出了一种新算法，用于使用LLMs的输出进行下游统计分析，同时保证与CSS研究基本相关的统计属性-如渐近无偏性和正确的不确定性量化。我们表明，直接在下游统计分析中使用LLM预测的替代标签会导致实质性偏差和无效置信区间，即使替代准确性高达80-90％。为了解决这个问题，我们基于无偏机器学习提出了基于设计的半监督学习（D-SSL）算法，该算法将LLM注释与有针对性的采样相结合，以实现有效的下游统计推断。我们的方法可以将标签获取的CSS研究成本降低80％，而不影响统计分析的有效性。模拟研究和实际数据示例表明，与直接使用LLM预测标签相比，D-SSL可以将回归估计的准确性提高多达40％。

    In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
    
[^169]: 揭示生成模型评价度量的缺陷及其不公平对待扩散模型的现象

    Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. (arXiv:2306.04675v1 [cs.LG])

    [http://arxiv.org/abs/2306.04675](http://arxiv.org/abs/2306.04675)

    系统地研究了图像生成模型的评估，发现常见的评价指标如FID等不能很好地体现扩散模型的感知真实性，建议使用SwAV特征提取器结合FID进行评估。

    

    我们系统地研究了许多种基于图像的生成模型，包括语义多样的数据集，以理解和改进用于评估它们的特征提取器和度量。使用心理物理学的最佳实践，我们进行了迄今为止最大规模的生成模型评估实验，通过对生成样本进行人类感知图像真实性的测量，发现没有任何现有的度量能与人类评估强相关。我们比较了用于评估生成模型整体性能、保真度、多样性和记忆能力的16个现代指标，发现以人类为基准的扩散模型的最先进的感知真实性不反映在常见的度量指标，如FID中。这种差异并不能通过生成样本的多样性来解释，尽管其中一个原因是过度依赖于Inception-V3。通过研究替代的自监督特征提取器，我们解决了这些缺陷，发现个别弱Downstream任务编码的语义信息最能解释图像真实性，建议在评估生成模型时使用SwAV特征提取器结合FID。

    We systematically study a wide variety of image-based generative models spanning semantically-diverse datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 16 modern metrics for evaluating the overall performance, fidelity, diversity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individu
    
[^170]: 分布式学习系统中用更少的代价获得更多。（arXiv:2306.04377v1 [cs.DC]）

    Get More for Less in Decentralized Learning Systems. (arXiv:2306.04377v1 [cs.DC])

    [http://arxiv.org/abs/2306.04377](http://arxiv.org/abs/2306.04377)

    本文提出了一种名为JWINS的分布式学习系统，它仅通过稀疏化的方式共享部分模型参数，使用小波变换来补偿由稀疏化引起的信息损失，并通过随机通信截断来减少通信用量。实验证明，JWINS可以在发送更少的字节的情况下实现与完全共享分布式学习相似的准确性。

    

    分布式学习系统因为能够避免原始数据共享而仅通过交流模型参数保护了数据的机密性，在机器学习领域越来越受欢迎。但是，深度神经网络的庞大规模对分布式训练提出了重要挑战——每个节点需要交换数千万个数据，容易导致网络超负荷。本文提出JWINS：一种通信效率高且完全分布式的机器学习系统，仅通过稀疏化来共享部分参数。JWINS使用小波变换来限制因稀疏化导致的信息损失，并采用随机通信切断来降低通信用量，而不会影响训练模型的性能。通过96个非独立同分布数据集的分布式学习节点的实现效果，证明JWINS可以在发送64％更少的字节时实现与完全共享分布式学习的相似准确性。此外，在低通信预算下，JWINS胜过了最先进的计算方法。

    Decentralized learning (DL) systems have been gaining popularity because they avoid raw data sharing by communicating only model parameters, hence preserving data confidentiality. However, the large size of deep neural networks poses a significant challenge for decentralized training, since each node needs to exchange gigabytes of data, overloading the network. In this paper, we address this challenge with JWINS, a communication-efficient and fully decentralized learning system that shares only a subset of parameters through sparsification. JWINS uses wavelet transform to limit the information loss due to sparsification and a randomized communication cut-off that reduces communication usage without damaging the performance of trained models. We demonstrate empirically with 96 DL nodes on non-IID datasets that JWINS can achieve similar accuracies to full-sharing DL while sending up to 64% fewer bytes. Additionally, on low communication budgets, JWINS outperforms the state-of-the-art com
    
[^171]: 多模态融合交互: 人类和自动量化研究

    Multimodal Fusion Interactions: A Study of Human and Automatic Quantification. (arXiv:2306.04125v1 [cs.LG])

    [http://arxiv.org/abs/2306.04125](http://arxiv.org/abs/2306.04125)

    本文比较研究了两种人类注释者可以用于注释多模态交互的分类，并提出了一种基于信息分解的分类学。

    

    在几乎所有多模态问题和应用中，多模态融合多种异构和互联的信号是一个基本挑战。为了进行多模态融合，我们需要理解模态可以展现的交互类型：每种模态如何单独提供对任务有用的信息，以及当存在其他模态时这些信息如何变化。在本文中，我们对人类注释者如何被利用来注释多模态交互的两种分类进行了比较研究：(1) 部分标签，其中不同随机分配的注释者注释给定第一个、第二个和两个模态的标签，以及(2) 反事实标签，其中同一注释者被要求在给出第一个模态之前注释标签，然后给出第二个模态，并要求他们明确地推理他们的答案如何改变，然后提出基于信息分解的另一种分类学。

    Multimodal fusion of multiple heterogeneous and interconnected signals is a fundamental challenge in almost all multimodal problems and applications. In order to perform multimodal fusion, we need to understand the types of interactions that modalities can exhibit: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how human annotators can be leveraged to annotate two categorizations of multimodal interactions: (1) partial labels, where different randomly assigned annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator is tasked to annotate the label given the first modality before giving them the second modality and asking them to explicitly reason about how their answer changes, before proposing an alternative taxonomy based on (3) information decomposition, where annotator
    
[^172]: 变分高斯过程扩散过程

    Variational Gaussian Process Diffusion Processes. (arXiv:2306.02066v1 [cs.LG])

    [http://arxiv.org/abs/2306.02066](http://arxiv.org/abs/2306.02066)

    本文提出一种高斯变分过程参数化方法来更好地学习具有非线性扩散过程的潜在过程，此方法采用具有连续指数族描述的算法实现凸优化，可以代替缓慢的具有固定点迭代的算法。

    

    扩散过程是一类随机微分方程，提供了一系列表现丰富的模型，自然地出现在动态建模任务中。概率推理和生成模型下具有非线性扩散过程的潜在过程的学习都是棘手的问题。本文在变分推理的基础上构建高斯过程扩散过程的参数化，指出方法中的病态，并提出一种使用连续指数族描述的高斯变分过程的替代参数化方法。这使我们可以用凸优化的快速算法代替具有固定点迭代的缓慢算法，这种算法类似于自然梯度下降，同时提供更好的目标来学习模型参数。

    Diffusion processes are a class of stochastic differential equations (SDEs) providing a rich family of expressive models that arise naturally in dynamic modelling tasks. Probabilistic inference and learning under generative models with latent processes endowed with a non-linear diffusion process prior are intractable problems. We build upon work within variational inference approximating the posterior process as a linear diffusion process, point out pathologies in the approach, and propose an alternative parameterization of the Gaussian variational process using a continuous exponential family description. This allows us to trade a slow inference algorithm with fixed-point iterations for a fast algorithm for convex optimization akin to natural gradient descent, which also provides a better objective for the learning of model parameters.
    
[^173]: 训练数据归因的贝叶斯视角

    A Bayesian Perspective On Training Data Attribution. (arXiv:2305.19765v1 [cs.LG])

    [http://arxiv.org/abs/2305.19765](http://arxiv.org/abs/2305.19765)

    本文介绍了一种TDA任务的贝叶斯视角，从中发现个别训练样本的影响常被噪声掩盖，TDA只能用于解释对模型预测影响稳定、独立于其他噪声因素的训练数据。

    

    训练数据归因（TDA）技术可找出影响模型对所关注的测试数据的预测的重要的训练数据，并估计减少或增加特定训练样本的影响。本文引入了一种TDA任务的贝叶斯视角，将学习的模型视为贝叶斯后验，TDA估计作为随机变量。从这个新的视角出发，我们发现个别训练样本的影响常会被模型初始化和SGD批次组合产生的噪声掩盖。基于这个发现，我们认为TDA只能可靠地用于解释一些训练数据对模型预测的影响是稳定的，独立于其他噪声因素。我们的实验证实了这种独立于噪声的训练-测试数据是很罕见的。

    Training data attribution (TDA) techniques find influential training data for the model's prediction on the test data of interest. They approximate the impact of down- or up-weighting a particular training sample. While conceptually useful, they are hardly applicable in practice, particularly because of their sensitivity to different model initialisation. In this paper, we introduce a Bayesian perspective on the TDA task, where the learned model is treated as a Bayesian posterior and the TDA estimates as random variables. From this novel viewpoint, we observe that the influence of an individual training sample is often overshadowed by the noise stemming from model initialisation and SGD batch composition. Based on this observation, we argue that TDA can only be reliably used for explaining model predictions that are consistently influenced by certain training data, independent of other noise factors. Our experiments demonstrate the rarity of such noise-independent training-test data pa
    
[^174]: 信仰与命运：Transformer在组合性方面的局限性。

    Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18654](http://arxiv.org/abs/2305.18654)

    研究了Transformer模型在三个代表性组合型任务中的表现，发现其通过线性子图匹配解决多步组合推理问题。

    

    Transformer大型语言模型在需要复杂多步推理的任务上表现卓越，但同时在一些简单问题上也会出现失败。这引发了疑问：这些错误是偶然的，还是它们表明了更实质性的限制？为了揭示Transformer的神秘面纱，我们研究了这些模型在三个代表性的组合型任务中的极限 - 多位数乘法、逻辑网格谜题和一个经典的动态规划问题。 这些任务需要将问题分解为子步骤，并将这些步骤综合成精确的答案。我们将组合型任务转化为计算图，以系统地量化其复杂性，并将推理步骤分解为中间子程序。我们的实证结果表明，Transformer通过将多步组合推理转化为线性子图匹配来解决组合型任务。

    Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
    
[^175]: 只使用前向传递微调语言模型

    Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v1 [cs.LG])

    [http://arxiv.org/abs/2305.17333](http://arxiv.org/abs/2305.17333)

    本论文提出了一种内存高效的零阶优化器，可以使用与推理相同的存储空间微调语言模型，其可以在大规模模型下更快地优化，具有更好的实验结果。

    

    微调语言模型已经在各种下游任务中取得了成功，但随着语言模型的增大，反向传播需要的存储空间数量变得过高。零阶（ZO）方法理论上仅使用两次前向传递就可以估计梯度，但通常情况下对大型模型进行优化的速度非常慢。在本文中，我们提出了一种内存高效的零阶优化器（MeZO），将经典的ZO-SGD方法适应于原地操作，从而使用与推理相同的存储空间微调语言模型。例如，只使用一张A100 80GB GPU，MeZO就可以训练一个300亿参数的模型，而使用反向传播可以在相同的预算下仅训练一个27亿个参数的语言模型。我们在各种模型类型（掩码和自回归语言模型）、模型规模（高达66B）和下游任务（分类、多项选择和生成）进行了全面的实验。我们的结果表明，（1）MeZO明显优于上下文学习和线性PR模型。

    Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear pr
    
[^176]: DPOK: 强化学习用于微调文本到图像扩散模型

    DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. (arXiv:2305.16381v1 [cs.LG])

    [http://arxiv.org/abs/2305.16381](http://arxiv.org/abs/2305.16381)

    本论文提出了DPOK，一种使用在线强化学习（RL）微调文本到图像扩散模型的方法。该方法在COCO数据集上实现了最先进的性能。

    

    已经证明，从人类反馈中学习可以改善文本到图像模型。这些技术首先学习一个捕捉任务中人类关心的特征的奖励函数，然后根据学习到的奖励函数改进模型。虽然已经研究了相对简单的方法（例如基于奖励得分的拒绝采样），但使用奖励函数微调文本到图像模型仍然具有挑战性。在这项工作中，我们提出使用在线强化学习（RL）来微调文本到图像模型。我们专注于扩散模型，将微调任务定义为RL问题，并使用策略梯度更新预训练文本到图像扩散模型，以最大化反馈训练奖励。我们的方法DPOK集成了KL正则化的策略优化。我们对RL微调和监督微调的KL正则化进行了分析。在我们的实验中，我们展示了DPOK通常优于使用交叉熵损失的监督微调和以前的RL微调技术。DPOK在COCO数据集上实现了最先进的性能，IS和FID得分显著优于现有方法。

    Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning w
    
[^177]: 最优化传输和概率扩散模型：反偏差，条件采样下的统计降尺度

    Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models. (arXiv:2305.15618v1 [cs.LG])

    [http://arxiv.org/abs/2305.15618](http://arxiv.org/abs/2305.15618)

    该论文提出了一种统计降尺度的概率框架，能够在不使用成对数据的情况下，通过反偏置和概率扩散模型来恢复存在偏见样本的真实物理统计信息。

    

    我们介绍了一种针对不成对数据的统计降尺度的两阶段概率框架。统计降尺度通过一个概率映射来将低分辨率的数据从（可能存在偏见的）粗粒度数值方案转换为与高保真度方案一致的高分辨率数据。我们的框架通过串联两个转换来解决这个问题：一个由最优传输图实现的反偏置步骤，以及一个由概率扩散模型实现的上采样步骤，并将条件采样后的概率分布纳入该方法，这种方法能够在不使用成对数据的前提下确定条件分布，并从存在偏见的样本中真实地恢复相关的物理统计信息。我们用一维和二维流体流动问题证明了该方法的效用，这些问题代表了天气和气候数值模拟中的核心困难。我们的方法可以从低分辨率的输入中生成真实的高分辨率输出。

    We introduce a two-stage probabilistic framework for statistical downscaling between unpaired data. Statistical downscaling seeks a probabilistic map to transform low-resolution data from a (possibly biased) coarse-grained numerical scheme to high-resolution data that is consistent with a high-fidelity scheme. Our framework tackles the problem by tandeming two transformations: a debiasing step that is performed by an optimal transport map, and an upsampling step that is achieved by a probabilistic diffusion model with \textit{a posteriori} conditional sampling. This approach characterizes a conditional distribution without the need for paired data, and faithfully recovers relevant physical statistics from biased samples. We demonstrate the utility of the proposed approach on one- and two-dimensional fluid flow problems, which are representative of the core difficulties present in numerical simulations of weather and climate. Our method produces realistic high-resolution outputs from lo
    
[^178]: 神经网络重构宇宙学与Pantheon编译

    Neural network reconstruction of cosmology using the Pantheon compilation. (arXiv:2305.15499v2 [gr-qc] CROSS LISTED)

    [http://arxiv.org/abs/2305.15499](http://arxiv.org/abs/2305.15499)

    本研究使用神经网络重构宇宙学和Pantheon编译中的Hubble图。通过扩展ReFANN算法以处理非高斯数据点和具有协方差矩阵的数据集，并与高斯过程的结果进行对比，还进行了零测试来验证宇宙学的协调模型的有效性。

    

    在这项工作中，我们使用各种数据集，包括相关数据集，在人工神经网络（ANN）中重构哈勃图。我们使用ReFANN来处理具有独立不确定性的数据集，并扩展它以包括非高斯数据点以及具有协方差矩阵等数据集。此外，我们将我们的结果与使用高斯过程得出的现有结果进行比较，并进行零测试以验证和谐宇宙模型的有效性。

    In this work, we reconstruct the Hubble diagram using various data sets, including correlated ones, in Artificial Neural Networks (ANN). Using ReFANN, that was built for data sets with independent uncertainties, we expand it to include non-Guassian data points, as well as data sets with covariance matrices among others. Furthermore, we compare our results with the existing ones derived from Gaussian processes and we also perform null tests in order to test the validity of the concordance model of cosmology.
    
[^179]: 可复现强化学习

    Replicable Reinforcement Learning. (arXiv:2305.15284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15284](http://arxiv.org/abs/2305.15284)

    本篇论文提供了可复制的强化学习算法，是控制问题的第一个正式的可复制性结果

    

    在社会、行为和数据科学中，可重复性危机导致了算法框架的形成，即要求算法在从相同的底层分布提取的两个不同样本上运行时产生相同的输出（概率高）。虽然仍处于初期阶段，但在机器学习和统计学中的许多基本任务，包括统计查询学习、重要项问题和分布测试，都已经开发出了可证明可复现算法。在这项工作中，我们开始研究可复现强化学习，并提供了并行值迭代的可证复制算法以及一个在连续设置中可证复制的R-max。这是控制问题的第一个正式可复制性结果，这些问题在批量学习环境中提出了不同的复制挑战。

    The replicability crisis in the social, behavioral, and data sciences has led to the formulation of algorithm frameworks for replicability -- i.e., a requirement that an algorithm produce identical outputs (with high probability) when run on two different samples from the same underlying distribution. While still in its infancy, provably replicable algorithms have been developed for many fundamental tasks in machine learning and statistics, including statistical query learning, the heavy hitters problem, and distribution testing. In this work we initiate the study of replicable reinforcement learning, providing a provably replicable algorithm for parallel value iteration, and a provably replicable version of R-max in the episodic setting. These are the first formal replicability results for control problems, which present different challenges for replication than batch learning settings.
    
[^180]: 具有函数逼近和理论保证的决策感知演员-评论家算法

    Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees. (arXiv:2305.15249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15249](http://arxiv.org/abs/2305.15249)

    提出了一种具有函数逼近和理论保证的决策感知演员-评论家算法，通过设计联合目标来解决演员和评论家之间的不匹配，并且无论策略和评论家参数化的选择如何，该算法都保证单调策略改进。

    

    演员评论家 (AC) 方法广泛应用于强化学习 (RL) 中，并从使用任何策略梯度方法作为演员和基于值方法作为评论家的灵活性中受益。评论家通常通过最小化 TD 误差来训练，这是一个与实现高奖励的真实目标可能脱钩的客观标准。我们通过设计一个决策感知的联合目标来解决这种不匹配。我们使用提出的目标来设计一个通用的 AC 算法，可以轻松处理任何函数逼近。我们明确表征了在选择策略和评论家参数化的情况下，所得算法保证单调策略改进的条件。实例化通用算法将导致涉及最大化一系列替代函数 (类似于 TRPO、PPO) 的演员和涉及最小化一个密切相关目标的评论家。使用简单的方法加速了证明的过程，同时还引入了新的研究方向。

    Actor-critic (AC) methods are widely used in reinforcement learning (RL) and benefit from the flexibility of using any policy gradient method as the actor and value-based method as the critic. The critic is usually trained by minimizing the TD error, an objective that is potentially decorrelated with the true goal of achieving a high reward with the actor. We address this mismatch by designing a joint objective for training the actor and critic in a decision-aware fashion. We use the proposed objective to design a generic, AC algorithm that can easily handle any function approximation. We explicitly characterize the conditions under which the resulting algorithm guarantees monotonic policy improvement, regardless of the choice of the policy and critic parameterization. Instantiating the generic algorithm results in an actor that involves maximizing a sequence of surrogate functions (similar to TRPO, PPO) and a critic that involves minimizing a closely connected objective. Using simple 
    
[^181]: 大型语言模型能力的可预测性如何？对BIG-bench的案例研究

    How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench. (arXiv:2305.14947v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14947](http://arxiv.org/abs/2305.14947)

    本研究通过对BIG-bench实验记录的研究，发现大型语言模型（LLM）的能力具有可预测性，并提出了寻找信息丰富的子集以最大程度恢复整个集合性能的问题。

    

    我们研究了大型语言模型（LLM）能力的可预测性：在使用不同模型家族、参数数量、任务数量和上下文示例数量的过去实验记录的基础上，我们能否准确预测LLM在新实验配置上的性能？回答这个问题对LLM用户（例如，决定尝试哪些模型）、开发者（例如，优先评估代表性任务）和研究社区（例如，识别需要进一步调查的难以预测的能力）具有实际意义。我们在BIG-bench的实验记录上研究了性能预测问题。在随机的训练-测试分离中，基于多层感知器（MLP）的预测器的R^2得分超过95%，表明实验记录中存在可学习的模式。然后，我们提出了寻找“small-bench”的问题，即从BIG-bench任务中寻找信息丰富的子集，可以从中最大程度地恢复整个集合的性能。

    We investigate the predictability of large language model (LLM) capabilities: given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations? Answering this question has practical implications for LLM users (e.g., deciding which models to try), developers (e.g., prioritizing evaluation on representative tasks), and the research community (e.g., identifying hard-to-predict capabilities that warrant further investigation).  We study the performance prediction problem on experiment records from BIG-bench. On a random train-test split, an MLP-based predictor achieves an $R^2$ score greater than 95%, indicating the presence of learnable patterns within the experiment records. We then formulate the problem of searching for "small-bench," an informative subset of BIG-bench tasks from which the performance on the full set can be maximally recovered. We
    
[^182]: 迈向可靠的假新闻缓解：泛化，不确定性和GPT-4

    Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. (arXiv:2305.14928v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14928](http://arxiv.org/abs/2305.14928)

    本研究提出利用泛化、不确定性和最新的大型语言模型，寻求解决假新闻问题。实验证明GPT-4在多语言环境下表现优于之前的方法。研究还探讨了泛化和不确定性处理技术，并在其他语言模型、温度、提示、版本控制、可解释性和网络检索方面取得了实际见解和未来研究方向。此外，研究还发布了新颖的英法配对假新闻数据集LIAR-New，为信息真实性评估提供了可行性标签。

    

    假新闻构成了一个重要的社会挑战，目前的方法尚未找到有效的解决方案。我们提出关注泛化，不确定性以及如何利用最新的大型语言模型，以便在无法完美分类的情况下创建更实用的工具来评估信息真实性。我们首先证明了GPT-4在多个设定和语言中可以胜过之前的方法。接下来，我们探索泛化，揭示了GPT-4和RoBERTa-large在失效模式上的差异。第三，我们提出了处理不确定性的技术，可以检测到不可能的例子并显著改进结果。我们还讨论了其他语言模型，温度，提示，版本控制，可解释性和网络检索的结果，每个结果都提供了实际的见解和未来研究的方向。最后，我们发布了具有新颖的英法配对假新闻数据和可行性标签的LIAR-New数据集。

    Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indic
    
[^183]: 实现大型语言模型生成带引文的文本

    Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v1 [cs.CL])

    [http://arxiv.org/abs/2305.14627](http://arxiv.org/abs/2305.14627)

    本文提出ALCE，是首个自动LLMs引文评估基准，实现大型语言模型生成带引文的文本，提高其事实正确性和可验证性；提示LLMs特定的关键词或利用外部知识源可以显著提高其引文准确性。

    

    大型语言模型（LLMs）已成为广泛使用的信息寻找工具，但生成的输出容易出现幻觉。本文旨在实现LLMs生成带引文的文本，提高其事实正确性和可验证性。我们提出了ALCE，这是首个自动LLMs引文评估基准。ALCE收集了各种问题和检索语料库，并要求建立端到端系统以检索支持证据并生成带有引文的答案。我们沿着流畅性、正确性和引文质量三个维度构建自动指标，并展示了它们与人类判断的强相关性。我们使用最先进的LLMs和新的提示策略进行实验，结果表明当前系统仍有相当大的提升空间--例如，提示LLMs特定的关键词或利用外部知识源可以显著提高其引文准确性。我们的工作为未来研究发展能够生成可验证和可信赖输出的LLMs提供了坚实基础。

    Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, we aim to enable LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare with different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We build automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvements -for example,
    
[^184]: 用基于符合性的图神经网络对图上不确定性进行量化

    Uncertainty Quantification over Graph with Conformalized Graph Neural Networks. (arXiv:2305.14535v1 [cs.LG])

    [http://arxiv.org/abs/2305.14535](http://arxiv.org/abs/2305.14535)

    本文提出了一种基于符合性的图神经网络模型（CF-GNN），通过将符合性预测（CP）扩展到基于图的模型中，对GNN不确定性进行了有效估计。CF-GNN生成的预测集/区间可根据预定义的覆盖概率保证包含真实标签，并且提供了一种减少预测集大小/区间长度的拓扑意识输出校正方法。

    

    图神经网络（GNN）是一种强大的用于图结构数据预测的机器学习模型。然而，GNN缺乏严格的不确定性估计，限制了它们在错误成本显著的环境中的可靠部署。我们提出了一种符合性GNN（CF-GNN），将符合性预测（CP）扩展到基于图的模型中，以获得可靠的不确定性估计。给定图中的实体，CF-GNN生成一个预测集/区间，以先验覆盖概率（例如90%）的方式保证包含真实标签。我们建立了一个排列不变条件，使得CP在图数据上成立，并提供了测试时间覆盖率的精确特征。此外，除了有效的覆盖，减少预测集大小/区间长度对于实际使用至关重要。我们发现非符合性得分和网络结构之间存在关键联系，这促使我们开发具有拓扑意识的输出校正模型来学习更新预测。

    Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN), extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates. Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation invariance condition that enables the validity of CP on graph data and provide an exact characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to reduce the prediction set size/interval length for practical use. We observe a key connection between non-conformity scores and network structures, which motivates us to develop a topology-aware output correction model that learns to update the predicti
    
[^185]: 对于表示偏见下的多模态学习的反事实增强

    Counterfactual Augmentation for Multimodal Learning Under Presentation Bias. (arXiv:2305.14083v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14083](http://arxiv.org/abs/2305.14083)

    本文提出了一种用于纠正表示偏见的新颖方法，即反事实增强。实证评估表明，反事实增强相比于未修正的模型和现有的偏见校正方法，可以获得更好的下游性能。模型分析进一步指出，在理想情况下，生成的反事实与真实反事实密切相关。

    

    在现实世界的机器学习系统中，标签通常是从系统希望鼓励的用户行为中得出的。随着时间的推移，随着新的训练样本和特征的提供，需要训练新模型。然而，用户和模型之间的反馈循环可能导致未来用户行为的偏见，进而导致标签中的表示偏见，这损害了训练新模型的能力。在本文中，我们提出了一种新颖的因果方法，即反事实增强，通过生成的反事实标签来纠正表示偏见。我们的实证评估表明，相比未修正的模型和现有的偏见校正方法，反事实增强可以产生更好的下游性能。模型分析进一步表明，在理想情况下，生成的反事实与真实反事实密切相关。

    In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a presentation bias in the labels that compromises the ability to train new models. In this paper, we propose counterfactual augmentation, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting.
    
[^186]: 基于语法约束的语言模型灵活解码技术

    Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])

    [http://arxiv.org/abs/2305.13971](http://arxiv.org/abs/2305.13971)

    本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。

    

    LLM在许多任务中展现出了惊人的少量样本表现，但在生成信息提取所需的复杂输出结构时仍存在困难。这个限制源于LLM在没有微调的情况下倾向于生成自由文本而不是遵循特定语法的精确结构。在本文中，我们提出在解码步骤中使用形式语法约束来丰富模型。在搜索过程中，只有符合语法产生规则的有效令牌能被考虑到。这样就强制只产生有效的序列。我们的框架非常通用和灵活，允许任何上下文无关语法(CFG)集成到我们的自定义约束beam搜索实现中。我们展示了许多NLP任务的输出可以被表示为形式语言，使它们适合在我们的框架中直接使用。对于输出空间取决于输入的任务，我们提出了基于输入的CFG，根据特定于输入的特征更新产生规则。实验证明了我们的方法在生成复杂输出结构方面的有效性，并在四个信息提取任务上实现了最先进的性能。

    LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
    
[^187]: 论文标题：《感知测试：多模态视频模型的诊断基准》

    Perception Test: A Diagnostic Benchmark for Multimodal Video Models. (arXiv:2305.13786v1 [cs.CV])

    [http://arxiv.org/abs/2305.13786](http://arxiv.org/abs/2305.13786)

    该论文提出了一个名为“感知测试”的多模态视频基准测试，可以评估预训练模型的感知和推理能力，测试涵盖了记忆、抽象、物理、语义等技能和描述性、解释性、预测性、反事实性等推理类型。

    

    我们提出了一种新颖的多模态视频基准——感知测试，用于评估预训练的多模态模型（例如 Flamingo、BEiT-3 或 GPT-4）的感知和推理技能。与现有的基准侧重于计算任务（例如分类、检测或跟踪）不同，感知测试侧重于视频、音频和文本模态跨越记忆、抽象、物理、语义等技能和推理类型（描述性、解释性、预测性、反事实性），以提供全面而高效的评估工具。该基准测试通过零样本/少样本或有限微调下挑选预训练模型的转移能力。为实现这些目的，感知测试介绍了11.6k种真实世界视频，平均长度为23秒，旨在展示感知上有趣的情境，由全球约100名参与者拍摄。这些视频密集地带有六种标签（多项选择和基于视频问题回答，对象a）

    We propose a novel multimodal video benchmark - the Perception Test - to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, BEiT-3, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the Perception Test introduces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object a
    
[^188]: 一种针对过度平滑的分数图拉普拉斯方法

    A Fractional Graph Laplacian Approach to Oversmoothing. (arXiv:2305.13084v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13084](http://arxiv.org/abs/2305.13084)

    本文提出了一种针对过度平滑问题的分数图拉普拉斯方法，通过采用神经图ODE框架来描述非局部动力学，允许在远处节点之间传播信息，同时保持低概率的远距离跳跃，进而减轻过度平滑问题，并在合成和实际图上进行了广泛实验验证。

    

    图神经网络在各种应用中显示出最先进的性能。然而，由于过度平滑，GNN经常难以捕捉图中的长距离依赖关系。在本文中，我们将过度平滑的概念从无向图推广到有向图。为此，我们通过考虑一个有向对称归一化拉普拉斯来扩展Dirichlet能量的概念。由于传统的图卷积网络容易出现过度平滑的问题，我们采用了神经图ODE框架。具体地，我们提出了分数图拉普拉斯神经ODE，描述了非局部动力学。我们证明了我们的方法允许在远处节点之间传播信息，同时保持低概率的远距离跳跃。而且，我们展示了我们的方法在图的Dirichlet能量收敛方面更加灵活，从而减轻了过度平滑的问题。我们在合成和实际图上进行了大量实验，包括有向和无向图。

    Graph neural networks (GNNs) have shown state-of-the-art performances in various applications. However, GNNs often struggle to capture long-range dependencies in graphs due to oversmoothing. In this paper, we generalize the concept of oversmoothing from undirected to directed graphs. To this aim, we extend the notion of Dirichlet energy by considering a directed symmetrically normalized Laplacian. As vanilla graph convolutional networks are prone to oversmooth, we adopt a neural graph ODE framework. Specifically, we propose fractional graph Laplacian neural ODEs, which describe non-local dynamics. We prove that our approach allows propagating information between distant nodes while maintaining a low probability of long-distance jumps. Moreover, we show that our method is more flexible with respect to the convergence of the graph's Dirichlet energy, thereby mitigating oversmoothing. We conduct extensive experiments on synthetic and real-world graphs, both directed and undirected, demons
    
[^189]: 一种可扩展的神经网络用于DSIC仿射极大价拍卖设计

    A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v1 [cs.GT])

    [http://arxiv.org/abs/2305.12162](http://arxiv.org/abs/2305.12162)

    该论文提出了一种可扩展的神经网络AMenuNet来构造AMAs参数和生成候选分配，解决了现有方法在占优策略激励兼容性和可扩展性方面的限制，其在协商一致的价值和社会残余价值方面优于强基线模型。

    

    自动拍卖设计旨在通过机器学习寻找经验上高收入的机制。现有的多物品拍卖情景的工作可以粗略地分为RegretNet类和仿射极大价（AMAs）方法。然而，前者不能严格保证占优策略激励兼容性（DSIC），而后者因为分配候选人数过多而面临可扩展性问题。为解决这些限制，我们提出了AMenuNet，一种可扩展的神经网络，它从出价人和物品表示中构造AMA参数（甚至包括分配菜单）。由于AMA的属性，AMenuNet始终是DSIC和个人理性（IR）的，通过神经网络生成候选分配来增强可伸缩性。此外，AMenuNet是置换等变的，其参数数量不受拍卖规模的影响。我们进行了大量实验，证明AMenuNet在协商一致的价值和社会残余价值方面优于强基线模型。

    Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both co
    
[^190]: ToolkenGPT：通过工具嵌入扩充冻结语言模型

    ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])

    [http://arxiv.org/abs/2305.11554](http://arxiv.org/abs/2305.11554)

    本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。

    

    将大型语言模型与外部工具结合起来解决复杂问题已成为一种有前途的方法。然而，传统方法需要用工具演示数据对LLM进行微调，既费时又受限于预定义的工具集。最近的上下文学习范例缓解了这些问题，但是有限的上下文长度只允许演示几次，导致对工具的理解不够充分。此外，当有大量工具可供选择时，上下文学习可能完全无法正常工作。在本文中，我们提出了一种$\textbf{ToolkenGPT}$的替代方法，将两种方法的优点结合起来。我们的方法将每个$\underline{工具}$表示为一个$\underline{token}$（$\textit{toolken}$），并为其学习一个嵌入，使得工具调用与生成常规单词标记的方式相同。一旦触发了toolken，LLM被提示完成工具执行所需的参数。ToolkenGPT提供了以下贡献：1）引入了toolken的概念，以扩充LLM与外部工具的交互，2）提出了一种新的学习范例，利用tool embeddings实现无缝交互，3）在各种下游任务上展示了我们方法的有效性。

    Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
    
[^191]: 三层神经网络中非线性特征学习的可证保证

    Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks. (arXiv:2305.06986v1 [cs.LG])

    [http://arxiv.org/abs/2305.06986](http://arxiv.org/abs/2305.06986)

    本文研究了三层神经网络的特征学习能力，相比之下，它具有比两层网络更丰富的可证的特征学习能力，并提出了一个通用定理，限制了目标结构的样本复杂度和宽度，以实现低测试误差。

    

    深度学习理论中的一个核心问题是理解神经网络如何学习分层特征。深度网络提取显著特征的能力对其卓越的泛化能力和现代深度学习范式的预训练和微调至关重要。然而，从理论角度来看，这种特征学习过程仍然不够清晰，现有的分析主要局限于两层网络。在本文中，我们展示了三层神经网络具有证明的比两层网络更丰富的特征学习能力。我们分析了通过逐层梯度下降训练的三层网络学习的特征，并提出了一个通用定理，它上界了目标具有特定层次结构时实现低测试错误所需的样本复杂度和宽度。我们将我们的框架实例化到特定的统计学学习设置中——单指数模型和二次函数。

    One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -- single-index models and functions of quadratic 
    
[^192]: 基于流形正则化 Tucker 分解的时空交通数据填充方法

    Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation. (arXiv:2305.06563v1 [stat.ML])

    [http://arxiv.org/abs/2305.06563](http://arxiv.org/abs/2305.06563)

    本文提出了一种基于流形正则化Tucker分解的时空交通数据填充方法，该方法利用稀疏正则化项改善了Tucker核的稀疏性，并引入流形正则化和时间约束项来优化张量的填充性能。

    

    时空交通数据填充(STDI)是数据驱动智能交通系统中不可避免和具有挑战性的任务，在部分观测到的交通数据中估计丢失数据。由于交通数据具有多维和时空性质，我们将丢失数据填充视为张量完成问题。过去十年中，许多关于基于张量分解的 STDI 的研究已经展开。然而，如何利用时空相关性和核张量稀疏性来改善填充性能仍然需要解决。本文重新构造了3/4阶汉克尔张量，并提出了一种创新的流形正则化 Tucker 分解(maniRTD)模型用于STDI。明确地，我们通过引入多维延迟嵌入变换将传感交通状态数据表示为3/4阶张量。然后，ManiRTD使用稀疏正则化项改善了Tucker核的稀疏性，并使用流形正则化和时间约束项来优化张量的填充性能。

    Spatiotemporal traffic data imputation (STDI), estimating the missing data from partially observed traffic data, is an inevitable and challenging task in data-driven intelligent transportation systems (ITS). Due to traffic data's multidimensional and spatiotemporal properties, we treat the missing data imputation as a tensor completion problem. Many studies have been on STDI based on tensor decomposition in the past decade. However, how to use spatiotemporal correlations and core tensor sparsity to improve the imputation performance still needs to be solved. This paper reshapes a 3rd/4th order Hankel tensor and proposes an innovative manifold regularized Tucker decomposition (ManiRTD) model for STDI. Expressly, we represent the sensory traffic state data as the 3rd/4th tensors by introducing Multiway Delay Embedding Transforms. Then, ManiRTD improves the sparsity of the Tucker core using a sparse regularization term and employs manifold regularization and temporal constraint terms of f
    
[^193]: 从子采样时间序列中使用代理变量进行因果推断

    Causal Discovery from Subsampled Time Series with Proxy Variables. (arXiv:2305.05276v1 [cs.LG])

    [http://arxiv.org/abs/2305.05276](http://arxiv.org/abs/2305.05276)

    本研究提出了一种使用代理变量方法的无模型算法，可以从子采样时间序列中无需参数约束地识别整个因果结构。

    

    从时间序列数据推断因果结构是许多科学研究的核心兴趣。采样频率远低于因果影响频率是此类推断的主要障碍。为了克服这个问题，已经提出了许多基于模型和非模型的方法，但是要么局限于线性情况，要么无法建立可识别性。在本研究中，我们提出了一种无模型的算法，可以在没有任何参数约束的情况下从子采样时间序列识别整个因果结构。该方法的思想是，子采样的挑战主要来自于“未观察到”的时间步，因此应使用为未观察到变量设计的工具处理此问题。在这些工具中，我们发现代理变量方法特别适合，因为未观察到变量的代理变量自然是在观察到的时间步上本身。根据这种直觉，我们建立了全面的结构可识别性。

    Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, i.e., the frequency of measurements is much lower than that of causal influence. To overcome this problem, numerous model-based and model-free methods have been proposed, yet either limited to the linear case or failed to establish identifiability. In this work, we propose a model-free algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. The idea is that the challenge of subsampling arises mainly from \emph{unobserved} time steps and therefore should be handled with tools designed for unobserved variables. Among these tools, we find the proxy variable approach particularly fits, in the sense that the proxy of an unobserved variable is naturally itself at the observed time step. Following this intuition, we establish comprehensive structural identifiabili
    
[^194]: 神经网络何时在表格数据上胜过增强树？

    When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])

    [http://arxiv.org/abs/2305.02997](http://arxiv.org/abs/2305.02997)

    这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。

    

    表格数据是机器学习中最常用的数据类型之一。尽管神经网络（NN）在表格数据上取得了最近的进展，但人们仍在积极讨论NN是否通常优于梯度提升决策树（GBDT）在表格数据上的表现，一些最近的工作要么认为GBDT在表格数据上一贯优于NN，要么认为NN优于GBDT。在这项工作中，我们退一步问：'这重要吗？'我们通过对176个数据集比较19种算法，进行了迄今为止最大的表格数据分析，并发现'NN vs. GBDT'争论被过分强调：令人惊讶的是，在相当多的数据集中，GBDT和NN之间的性能差异要么可以忽略不计，要么GBDT的轻微超参数调整比选择最佳算法更重要。接下来，我们分析了965个元特征，以确定数据集的哪些特性使NN或GBDT更适合表现良好。例如，我们发现GBDT要比NN在高维稀疏数据上表现更好。

    Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
    
[^195]: ChatGPT生成的代码真的正确吗？对大型语言模型在代码生成方面的严格评估

    Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v1 [cs.SE])

    [http://arxiv.org/abs/2305.01210](http://arxiv.org/abs/2305.01210)

    本论文提出了一个严格的代码综合基准评估框架EvalPlus，用于评估利用大型语言模型生成的代码的功能正确性。

    

    程序综合一直以来都是被长期研究的领域，最近的方法集中于直接利用大型语言模型(LLMs)根据自然语言中用户的意图生成代码。代码评估数据集，包含策划好的综合问题和各种输入/输出测试用例，被用来衡量各种LLMs在代码综合上的性能。然而，这些数据集中的测试用例在完全评估生成代码的功能正确性方面，数量和质量都可能有所限制。这种现有基准中的限制引出了以下问题：在LLMs时代，生成的代码真的正确吗？为了回答这个问题，我们提出了EvalPlus——一个评估LLM-synthesized代码功能正确性的严格基准评估框架。EvalPlus接受基础评估数据集，并利用自动输入生成步骤，使用LLM-based和基于变异的方法生成和多样化大量新的测试输入。

    Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various LLMs on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. In short, EvalPlus takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both LLM-based and mutation-based
    
[^196]: 学习使用自注记进行推理和记忆

    Learning to Reason and Memorize with Self-Notes. (arXiv:2305.00833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.00833](http://arxiv.org/abs/2305.00833)

    该论文提出了一种学习使用自注记进行推理和记忆的方法，通过允许模型明确思考、记录自己的想法，并整合先前的推理步骤，从而提高了多步推理的能力。

    

    大型语言模型在多步推理方面表现不佳，且不能保留以供将来使用的先前推理步骤。我们提出了一种解决这两个问题的简单方法，即允许模型进行自注记。与最近的思维链或草稿本方法不同，该模型可以随时偏离输入上下文来明确思考和记录自己的想法。这使得模型可以在阅读上下文时即时推理，并整合先前的推理步骤，从而增强其记忆并进行多步推理。广泛的实验表明，通过使用交织输入文本的自注记方法，我们的方法可以胜过思维链和草稿本方法。

    Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use. We propose a simple method for solving both of these problems by allowing the model to take Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts. This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning. Experiments across a wide variety of tasks demonstrate that our method can outperform chain-of-thought and scratchpad methods by taking Self-Notes that interleave the input text.
    
[^197]: 学习轨迹是泛化指标

    Learning Trajectories are Generalization Indicators. (arXiv:2304.12579v1 [cs.LG])

    [http://arxiv.org/abs/2304.12579](http://arxiv.org/abs/2304.12579)

    本文研究了DNN的学习轨迹与其在优化后的泛化能力的联系，提出了一种基于学习轨迹复杂性和训练集偏置和多样性比率的新的泛化上界，并通过实验验证了其有效性。

    

    本文旨在研究深度神经网络（DNN）的学习轨迹与其在广泛使用的梯度下降和随机梯度下降算法优化时对应的泛化能力之间的联系。本文构建了线性近似函数来模拟轨迹信息，并在此基础上提出了一种基于更丰富轨迹信息的新的泛化上界。我们提出的泛化上界依赖于学习轨迹的复杂性以及训练集的偏置和多样性比之间的比率。实验结果表明，该方法可以有效地捕捉不同训练步骤、学习率和标签噪声水平下的泛化趋势。

    The aim of this paper is to investigate the connection between learning trajectories of the Deep Neural Networks (DNNs) and their corresponding generalization capabilities when being optimized with broadly used gradient descent and stochastic gradient descent algorithms. In this paper, we construct Linear Approximation Function to model the trajectory information and we propose a new generalization bound with richer trajectory information based on it. Our proposed generalization bound relies on the complexity of learning trajectory and the ratio between the bias and diversity of training set. Experimental results indicate that the proposed method effectively captures the generalization trend across various training steps, learning rates, and label noise levels.
    
[^198]: 深度Calderón方法的电阻抗层析成像

    Electrical Impedance Tomography with Deep Calder\'on Method. (arXiv:2304.09074v1 [math.NA])

    [http://arxiv.org/abs/2304.09074](http://arxiv.org/abs/2304.09074)

    Calderón方法是一种快速的EIT成像算法，但图像模糊且低估电导率值。该论文基于U-net模型对Calderón方法的图像进行后处理，提高了图像分辨率和电导率估计的准确性。

    

    电阻抗层析成像(EIT)是一种利用在对象表面上测量的电流密度/电压数据的非侵入式医学成像方式。Calderon的方法是一种相对较新的EIT成像算法，它是非迭代的、快速的，并且能够重建复值电阻抗。然而，由于正则化通过低通滤波和线性化，重建的图像遭受严重的模糊和低估确切的电导率值。在这项工作中，我们开发了Calderón方法的增强版本，使用卷积神经网络（即U-net）通过后处理步骤。具体来说，我们学习一个U-net来后处理由Calderón方法生成的EIT图像，以获得更好的分辨率和更准确的电导率估计。我们模拟胸部配置，通过Calderón方法生成电流密度/电压边界测量和相应的重建图像。

    Electrical impedance tomography (EIT) is a noninvasive medical imaging modality utilizing the current-density/voltage data measured on the surface of the subject. Calder\'on's method is a relatively recent EIT imaging algorithm that is non-iterative, fast, and capable of reconstructing complex-valued electric impedances. However, due to the regularization via low-pass filtering and linearization, the reconstructed images suffer from severe blurring and underestimation of the exact conductivity values. In this work, we develop an enhanced version of Calder\'on's method, using convolution neural networks (i.e., U-net) via a postprocessing step. Specifically, we learn a U-net to postprocess the EIT images generated by Calder\'on's method so as to have better resolutions and more accurate estimates of conductivity values. We simulate chest configurations with which we generate the current-density/voltage boundary measurements and the corresponding reconstructed images by Calder\'on's metho
    
[^199]: 一种具有物体感知等变基元反应扩散模型的精确过渡态生成方法

    Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model. (arXiv:2304.06174v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.06174](http://arxiv.org/abs/2304.06174)

    本文开发了一种物体感知 SE(3) 等变扩散模型，可以在几秒钟内精确地生成过渡态结构，与基于量子化学的优化相比，计算时间大大缩短，其生成的过渡态结构与真实结构的平均误差为 0.13 A 根均方差，可以实现反应速率估计所需的精度。

    

    过渡态搜索在化学中具有重要作用，可用于阐明反应机理和探索反应网络。但搜索精确的三维过渡态结构需要大量的量子化学计算，因为势能面的复杂性。本文开发了一种物体感知 SE(3) 等变扩散模型，满足生成反应物、过渡态和生成物三种结构的所有物理对称性和约束条件。在已知反应物和生成物的情况下，该模型可以在几秒钟内生成过渡态结构，而不需要进行基于量子化学的优化，从而大大缩短了计算时间。生成的过渡态结构与真实结构的平均误差为 0.13 A 根均方差。通过对不确定性进行评估，并进行置信度评分，可以实现反应速率估计所需的精度 (2.6 kcal/mol)，并且只需对 14% 的结果进行基于量子化学的优化。

    Transition state (TS) search is key in chemistry for elucidating reaction mechanisms and exploring reaction networks. The search for accurate 3D TS structures, however, requires numerous computationally intensive quantum chemistry calculations due to the complexity of potential energy surfaces. Here, we developed an object-aware SE(3) equivariant diffusion model that satisfies all physical symmetries and constraints for generating pairs of structures, i.e., reactant, TS, and product, in an elementary reaction. Provided reactant and product, this model generates a TS structure in seconds instead of the hours required when performing quantum chemistry-based optimizations. The generated TS structures achieve an average error of 0.13 A root mean square deviation compared to true TS. With a confidence scoring model for uncertainty quantification, we approach an accuracy required for reaction rate estimation (2.6 kcal/mol) by only performing quantum chemistry-based optimizations on 14% of th
    
[^200]: 高维超统计特征的分类方法

    Classification of Superstatistical Features in High Dimensions. (arXiv:2304.02912v1 [stat.ML])

    [http://arxiv.org/abs/2304.02912](http://arxiv.org/abs/2304.02912)

    本文利用经验风险最小化的方法，对高维超统计特征下的数据进行分类，并分析了正则化和分布尺度参数对分类的影响。

    

    在高维情况下，我们通过经验风险最小化的方法，对具有一般中心点的两个数据云的混合进行了学习，假设具有通用的凸损失和凸正则化。每个数据云是通过从可能是不可数的高斯分布叠加中进行采样来获得的，其方差具有通用的概率密度$\varrho$。我们的分析涵盖了大量的数据分布，包括没有协方差的幂律尾部分布的情况。我们研究了所得估计器的泛化性能，分析了正则化的作用以及分离转换与分布尺度参数的相关性。

    We characterise the learning of a mixture of two clouds of data points with generic centroids via empirical risk minimisation in the high dimensional regime, under the assumptions of generic convex loss and convex regularisation. Each cloud of data points is obtained by sampling from a possibly uncountable superposition of Gaussian distributions, whose variance has a generic probability density $\varrho$. Our analysis covers therefore a large family of data distributions, including the case of power-law-tailed distributions with no covariance. We study the generalisation performance of the obtained estimator, we analyse the role of regularisation, and the dependence of the separability transition on the distribution scale parameters.
    
[^201]: 无碰撞运输图在流行学习中的应用

    Applications of No-Collision Transportation Maps in Manifold Learning. (arXiv:2304.00199v1 [cs.LG])

    [http://arxiv.org/abs/2304.00199](http://arxiv.org/abs/2304.00199)

    本文研究了在流形学习中应用无碰撞运输图的方法，其可以比OT图更便宜地计算距离，并提供单个概率测度的平移和伸缩的等距性。

    

    本文研究了引入于[Nurbekyan et al.，2020]的无碰撞运输图在图像数据的流形学习中的应用。近年来，在表示类似运动或变形现象的数据中，应用基于运输的距离和特征的研究大幅增加。事实上，固定位置比较强度通常无法显示数据结构。在[Nurbekyan et al.，2020]中开发的无碰撞图和距离类似于最优传输(OT)图的几何特征但由于无需优化，计算成本要便宜得多。本文证明无碰撞距离提供单个概率测度的平移(分别是伸缩)和装备欧几里得距离的平移(分别是伸缩)向量之间的等距性。此外，我们证明，无碰撞运输图以及OT和线性OT图，一般来说不能为旋转提供等距性。

    In this work, we investigate applications of no-collision transportation maps introduced in [Nurbekyan et. al., 2020] in manifold learning for image data. Recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. Indeed, comparing intensities at fixed locations often does not reveal the data structure. No-collision maps and distances developed in [Nurbekyan et. al., 2020] are sensitive to geometric features similar to optimal transportation (OT) maps but much cheaper to compute due to the absence of optimization. In this work, we prove that no-collision distances provide an isometry between translations (respectively dilations) of a single probability measure and the translation (respectively dilation) vectors equipped with a Euclidean distance. Furthermore, we prove that no-collision transportation maps, as well as OT and linearized OT maps, do not in general provide an isometry for rotatio
    
[^202]: BERT4ETH：用于以太坊欺诈检测的预训练Transformer模型

    BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection. (arXiv:2303.18138v1 [cs.CR])

    [http://arxiv.org/abs/2303.18138](http://arxiv.org/abs/2303.18138)

    BERT4ETH是一个用于以太坊欺诈检测的预训练Transformer编码器，它通过三种实用有效策略来解决针对高度重复、偏斜分布和异构以太坊交易这些挑战，并且比现有技术更为优秀。

    

    随着以太坊上各种欺诈行为的激增，保护易受攻击用户免受受害是非常必要的。虽然目前的研究仅依赖于基于图的欺诈检测方法，但有人认为它们可能不适合处理高度重复、偏斜分布和异构以太坊交易。为了应对这些挑战，我们提出了BERT4ETH，这是一个通用的预训练Transformer编码器，用于检测以太坊上的各种欺诈行为。BERT4ETH具有Transformer的优秀建模能力，能够捕捉以太坊交易中固有的动态顺序模式，并通过减少重复性、减轻偏斜和建模异构性等三种实用有效策略来解决为以太坊预训练BERT模型带来的挑战。我们的实证评估表明，BERT4ETH在欺诈检测方面优于现有技术。

    As various forms of fraud proliferate on Ethereum, it is imperative to safeguard against these malicious activities to protect susceptible users from being victimized. While current studies solely rely on graph-based fraud detection approaches, it is argued that they may not be well-suited for dealing with highly repetitive, skew-distributed and heterogeneous Ethereum transactions. To address these challenges, we propose BERT4ETH, a universal pre-trained Transformer encoder that serves as an account representation extractor for detecting various fraud behaviors on Ethereum. BERT4ETH features the superior modeling capability of Transformer to capture the dynamic sequential patterns inherent in Ethereum transactions, and addresses the challenges of pre-training a BERT model for Ethereum with three practical and effective strategies, namely repetitiveness reduction, skew alleviation and heterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETH outperforms state-of-the-ar
    
[^203]: 信息最大化课程：一种基于课程的模仿多样技能方法

    Information Maximizing Curriculum: A Curriculum-Based Approach for Imitating Diverse Skills. (arXiv:2303.15349v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.15349](http://arxiv.org/abs/2303.15349)

    本文提出了一种信息最大化课程的模仿学习方法，通过为每个数据点分配权重，让模型专注于能够表示的数据，从而解决模平均问题。为了实现多样性行为，该方法采用了专家混合策略，并引入了最大熵约束。

    

    模仿学习通过训练策略来解决复杂任务，但当训练数据来自人类示范者时，由于人类行为的变异性，通常会导致多模态分布。大多数模仿学习方法依赖于最大似然（ML）目标来学习参数化策略，但由于ML目标的模平均特性，这可能导致次优或不安全的行为。在这项工作中，我们提出了信息最大化课程，一种基于课程的方法，为每个数据点分配权重，并鼓励模型专注于它能够表示的数据，通过允许模型忽略不能表示的模态数据来有效缓解模平均问题。为了涵盖所有模态并能够实现多样的行为，我们将我们的方法扩展到了专家混合（MoE）策略，其中每个混合成分为自己选择一部分训练数据进行学习。一种新颖的基于最大熵的方法被用来约束策略的多样性。

    Imitation learning uses data for training policies to solve complex tasks. However, when the training data is collected from human demonstrators, it often leads to multimodal distributions because of the variability in human actions. Most imitation learning methods rely on a maximum likelihood (ML) objective to learn a parameterized policy, but this can result in suboptimal or unsafe behavior due to the mode-averaging property of the ML objective. In this work, we propose Information Maximizing Curriculum, a curriculum-based approach that assigns a weight to each data point and encourages the model to specialize in the data it can represent, effectively mitigating the mode-averaging problem by allowing the model to ignore data from modes it cannot represent. To cover all modes and thus, enable diverse behavior, we extend our approach to a mixture of experts (MoE) policy, where each mixture component selects its own subset of the training data for learning. A novel, maximum entropy-base
    
[^204]: 时间序列视为图像：用视觉transformer处理不规则采样时间序列

    Time Series as Images: Vision Transformer for Irregularly Sampled Time Series. (arXiv:2303.12799v1 [cs.LG])

    [http://arxiv.org/abs/2303.12799](http://arxiv.org/abs/2303.12799)

    本文提出了一种新颖的方法，将不规则采样的时间序列转换为线图像，并适应强大的视觉transformer进行时间序列分类。该方法简化了算法设计，具有通用性，并展示了在多个医疗和人体活动数据集上明显优于最先进的专业算法的表现。

    

    在各个领域中，尤其是在医疗应用中，不规则抽样的时间序列越来越普遍。尽管已经提出了不同的高度定制化方法来解决不规则性问题，但如何有效地模拟它们的复杂动态和高稀疏性仍然是一个开放的问题。本文从全新的角度研究了这个问题：将不规则采样的时间序列转换为线图像，并调整强大的视觉transformer以执行与图像分类相同的时间序列分类。我们的方法在不假设先前知识的情况下大大简化了算法设计，并且可以被潜在地扩展为一个通用框架。尽管其简单性，我们展示了它在几个流行的医疗保健和人体活动数据集上明显优于最先进的专业算法。特别是在具有挑战性的离传感器设置中，即在测试期间屏蔽变量的子集中，性能比最佳基准提高了高达11％。

    Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance impr
    
[^205]: FedML-HE:一种基于高效同态加密的隐私保护联邦学习系统

    FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving Federated Learning System. (arXiv:2303.10837v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.10837](http://arxiv.org/abs/2303.10837)

    FedML-HE是一种基于高效同态加密的实用联邦学习系统，它通过选择性加密敏感参数来显著减少计算和通信开销，并提供可定制的隐私保护。

    

    联邦学习通过聚合本地模型更新而不是本地数据，在分布式设备上训练机器学习模型。然而，隐私问题产生了，因为服务端上聚合的本地模型可能通过逆向攻击揭示敏感个人信息。隐私保护方法，如同态加密（HE），因此成为FL训练的必要手段。尽管HE具有隐私优势，但其应用受到不实际的开销限制，尤其是对基础模型而言。本文提出了FedML-HE，第一个具有高效HE安全模型聚合的实用联邦学习系统。FedML-HE提出了选择性加密敏感参数，显著减少训练过程中的计算和通信开销，同时提供可定制的隐私保护。我们优化的系统显示出相当大的开销降低，尤其是对于大型基础模型（例如，ResNet-50减少了约10倍，而B模型减少了约40倍）。

    Federated Learning trains machine learning models on distributed devices by aggregating local model updates instead of local data. However, privacy concerns arise as the aggregated local models on the server may reveal sensitive personal information by inversion attacks. Privacy-preserving methods, such as homomorphic encryption (HE), then become necessary for FL training. Despite HE's privacy advantages, its applications suffer from impractical overheads, especially for foundation models. In this paper, we present FedML-HE, the first practical federated learning system with efficient HE-based secure model aggregation. FedML-HE proposes to selectively encrypt sensitive parameters, significantly reducing both computation and communication overheads during training while providing customizable privacy preservation. Our optimized system demonstrates considerable overhead reduction, particularly for large foundation models (e.g., ~10x reduction for ResNet-50, and up to ~40x reduction for B
    
[^206]: 基于对象中心的槽扩散

    Object-Centric Slot Diffusion. (arXiv:2303.10834v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10834](http://arxiv.org/abs/2303.10834)

    基于对象中心的槽扩散是一种将扩散模型整合到对象中心学习的新颖方法，它使用Latent Slot Diffusion (LSD)模型替换了传统的槽解码器，同时也是第一个能够无监督进行组合条件扩散的模型。

    

    最近基于Transformer的图像生成模型在处理复杂场景的对象中心学习中取得了成功，这突显了强大的图像生成器的重要性。然而，尽管扩散模型在图像生成方面具有较高的表达能力，但它们在对象中心学习中的整合在这个领域中仍然较少探索。在本文中，我们探讨了将扩散模型整合到对象中心学习中的可行性和潜力，并研究了这种方法的优点和缺点。我们引入了Latent Slot Diffusion (LSD)，这是一种新颖的模型，它具有两个目标：首先，它是第一个将传统的槽解码器替换为以对象槽为条件的潜在扩散模型的对象中心学习模型；其次，它也是第一个不需要像文本这样的监督注释而能够无监督地进行组合条件扩散的模型。通过对各种对象中心任务的实验，包括首次在FFHQ数据集中的应用。

    The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in t
    
[^207]: 你的表示在网络中：可组合和并行适应大型模型

    Your representations are in the network: composable and parallel adaptation for large scale models. (arXiv:2303.04105v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04105](http://arxiv.org/abs/2303.04105)

    InCA是一种轻量级的迁移学习方法，可以在预训练模型的任何激活层进行交互式关注。与其他形式的适应方法相比，InCA的性能接近全面微调，但计算成本仅为基线的51%。

    

    我们提出了InCA，一种轻量级的迁移学习方法，在预训练模型的任何激活层中进行交互式关注。在训练过程中，InCA使用单次前向传递来提取多个激活，并将其传递给外部交叉注意力适配器进行重新训练和组合或选择用于下游任务。我们表明，即使选择一个最高分的适配器，InCA的性能也可以与全面微调相媲美，而成本仅相当于仅微调最后一层。例如，对于一个与预训练ViT-L/16模型大小相比仅为1.3%的交叉注意力探针，在平均11个下游分类任务中，我们的性能接近全面微调的参照，并且计算训练成本仅为基线的51%。与其他形式的高效适应不同，InCA不需要通过预训练模型进行反向传播，从而在训练和推理过程中保持其执行不变。InCA的多功能性在fine-tuning方面表现最好。

    We propose InCA, a lightweight method for transfer learning that cross-attends to any activation layer of a pre-trained model. During training, InCA uses a single forward pass to extract multiple activations, which are passed to external cross-attention adapters, trained anew and combined or selected for downstream tasks. We show that, even when selecting a single top-scoring adapter, InCA achieves performance comparable to full fine-tuning, at a cost comparable to fine-tuning just the last layer. For example, with a cross-attention probe 1.3% the size of a pre-trained ViT-L/16 model, we achieve performance within 0.2% of the full fine-tuning paragon at a computational training cost of 51% of the baseline, on average across 11 downstream classification. Unlike other forms of efficient adaptation, InCA does not require backpropagating through the pre-trained model, thus leaving its execution unaltered at both training and inference. The versatility of InCA is best illustrated in fine-gr
    
[^208]: GlucoSynth：生成差分私有合成血糖轨迹

    GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces. (arXiv:2303.01621v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01621](http://arxiv.org/abs/2303.01621)

    本文提出了一种新颖的GAN框架-GlucoSynth，采用差分隐私机制来生成合成血糖轨迹，保证数据隐私安全的同时能生成高质量的合成数据.

    

    本论文研究生成高质量、私有合成血糖轨迹的问题，这个任务可推广到许多其他时间序列数据。现有的时间序列数据合成方法，如使用生成对抗网络（GANs）的方法，无法捕捉血糖数据的先天特征，也无法在不严重降低合成数据效用的情况下提供任何形式隐私保证。本文提出了GlucoSynth，一种新颖的保护隐私的GAN框架，可用于生成合成血糖轨迹。我们方法的核心思想是在考虑时序动态的同时，保留轨迹中motif（血糖事件）之间的关系。我们的框架采用差分隐私机制，提供了强有力的形式隐私保证。我们使用120万条血糖轨迹进行了全面评估；GlucoSynth在生成高质量合成数据方面表现优异。

    We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synt
    
[^209]: 隐含偏见的双刃剑：ReLU网络中的泛化与鲁棒性比较

    The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness in ReLU Networks. (arXiv:2303.01456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01456](http://arxiv.org/abs/2303.01456)

    这项研究研究了ReLU网络中梯度流的隐式偏差对泛化和对抗鲁棒性的影响，发现梯度流倾向于泛化能力强但对抗性高的解决方案，并且这种偏差还导致非鲁棒性解决方案的出现。

    

    在这项工作中，我们研究了梯度流的隐式偏差对ReLU网络中泛化和对抗鲁棒性的影响。我们关注的是数据由簇组成且簇之间的相关性较小的情况，并且发现在两层ReLU网络中，梯度流在偏向泛化能力强的解决方案的同时也对小规模对抗性例子高度脆弱。我们的结果即使在网络参数远远多余训练样本的情况下也成立。尽管在这种过度参数化的设置中有潜在的有害过拟合可能性，我们证明梯度流的隐式偏差可以防止这种情况发生。然而，隐式偏差也会导致非鲁棒性的解决方案（容易受到小的对抗性$\ell_2$扰动），尽管也存在能够拟合数据的鲁棒网络。

    In this work, we study the implications of the implicit bias of gradient flow on generalization and adversarial robustness in ReLU networks. We focus on a setting where the data consists of clusters and the correlations between cluster means are small, and show that in two-layer ReLU networks gradient flow is biased towards solutions that generalize well, but are highly vulnerable to adversarial examples. Our results hold even in cases where the network has many more parameters than training examples. Despite the potential for harmful overfitting in such overparameterized settings, we prove that the implicit bias of gradient flow prevents it. However, the implicit bias also leads to non-robust solutions (susceptible to small adversarial $\ell_2$-perturbations), even though robust networks that fit the data exist.
    
[^210]: 分布模型和半监督学习器在少量标签上互相受益

    Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels. (arXiv:2302.10586v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10586](http://arxiv.org/abs/2302.10586)

    本文介绍了一种名为双伪训练（DPT）的训练策略，该策略结合了强大的半监督学习器和扩散模型来进一步推进半监督生成和分类任务。实验结果表明，DPT在各种情况下都能实现半监督生成和分类任务的SOTA性能，特别是在每个类别只有一个或两个标签的情况下，超过了其他一些模型。

    

    为了进一步推进半监督生成和分类任务，本文提出了一种简单而有效的训练策略——双伪训练（DPT），该策略建立在强大的半监督学习器和扩散模型之上。DPT分为三个阶段：使用部分标记数据训练分类器以预测伪标签；使用这些伪标签训练条件生成模型以生成伪图像；并使用真实和伪造的图像混合重新训练分类器。实验结果表明，在各种情况下，DPT始终实现了半监督生成和分类的SOTA性能。特别是，在每个类别只有一个或两个标签的情况下，在ImageNet 256x256上，DPT的Fr\'echet Inception Distance（FID）得分分别为3.08或2.52，超过了具有完整标签的强扩散模型（如IDDPM，CDM，ADM和LDM）。此外，DPT在ImageNet分类任务上显著优于竞争性的半监督基线，实现了顶级1的准确性。

    In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called dual pseudo training (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet 256x256, surpassing strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies o
    
[^211]: 机器学习在整数规划中的切割平面：一项调查

    Machine Learning for Cutting Planes in Integer Programming: A Survey. (arXiv:2302.09166v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.09166](http://arxiv.org/abs/2302.09166)

    该论文调查了机器学习技术在混合整数线性规划中选择切割平面的最新研究，提出了使用数据进行优化切割选择的有前景的方法。

    

    我们对机器学习技术在混合整数线性规划中选择切割平面（或切割）的最新研究进行了调查。尽管存在各种各样的切割类别，但在分支定界树的给定节点的线性规划放松中选择一组要添加的切割平面的任务迄今为止无法得到正式和启发式解法。机器学习通过使用数据来识别加速解决混合整数线性规划实例的有前景的切割，为改进切割选择过程提供了一种有希望的方法。本文概述了该主题，重点介绍了文献中的最新进展，数据收集、评估和机器学习模型架构的常见方法。我们分析了文献中的实证结果，试图量化已取得的进展，并在最后提出了未来研究的方向。

    We survey recent work on machine learning (ML) techniques for selecting cutting planes (or cuts) in mixed-integer linear programming (MILP). Despite the availability of various classes of cuts, the task of choosing a set of cuts to add to the linear programming (LP) relaxation at a given node of the branch-and-bound (B&B) tree has defied both formal and heuristic solutions to date. ML offers a promising approach for improving the cut selection process by using data to identify promising cuts that accelerate the solution of MILP instances. This paper presents an overview of the topic, highlighting recent advances in the literature, common approaches to data collection, evaluation, and ML model architectures. We analyze the empirical results in the literature in an attempt to quantify the progress that has been made and conclude by suggesting avenues for future research.
    
[^212]: 自监督的时间图学习与时间和结构强度对齐

    Self-Supervised Temporal Graph learning with Temporal and Structural Intensity Alignment. (arXiv:2302.07491v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07491](http://arxiv.org/abs/2302.07491)

    该论文提出了一种自监督的时间图学习方法，通过提取时间和结构信息来学习更具信息量的节点表示。

    

    时间图学习旨在生成用于基于图的任务的高质量表示，同时包含动态信息，最近引起了越来越多的关注。与静态图不同，时间图通常以连续时间上的节点交互序列的形式组织，而不是邻接矩阵。大多数时间图学习方法通过在时间上组合历史信息来建模当前的交互。然而，这些方法仅考虑了一阶时间信息，而忽视了重要的高阶结构信息，导致性能不佳。为了解决这个问题，我们提出了一种自监督方法，名为S2T，通过提取时间和结构信息来学习更具信息量的节点表示。

    Temporal graph learning aims to generate high-quality representations for graph-based tasks along with dynamic information, which has recently drawn increasing attention. Unlike the static graph, a temporal graph is usually organized in the form of node interaction sequences over continuous time instead of an adjacency matrix. Most temporal graph learning methods model current interactions by combining historical information over time. However, such methods merely consider the first-order temporal information while ignoring the important high-order structural information, leading to sub-optimal performance. To solve this issue, by extracting both temporal and structural information to learn more informative node representations, we propose a self-supervised method termed S2T for temporal graph learning. Note that the first-order temporal information and the high-order structural information are combined in different ways by the initial node representations to calculate two conditional 
    
[^213]: 数据修剪和神经缩放定律：基于评分的算法的基本限制

    Data pruning and neural scaling laws: fundamental limitations of score-based algorithms. (arXiv:2302.06960v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06960](http://arxiv.org/abs/2302.06960)

    评分数据修剪算法在高压缩区域失败，通过随机化的校准协议可以提高现有修剪算法在该区域的性能。

    

    数据修剪算法常用于减少优化过程的内存和计算成本。最近的实证结果表明，随机数据修剪仍然是一个强大的基准，并在高压缩区域优于大多数现有的数据修剪方法，即保留了不到数据的30％的部分。这种压缩区域最近引起了很多关注，因为数据修剪在提高所谓的神经缩放定律中的作用；在[Sorscher et al.]中，作者展示了需要高质量的数据修剪算法才能击败样本势律。在这项工作中，我们关注评分数据修剪算法，并在理论上和实际上展示了为什么这样的算法在高压缩区域失败。我们证明了数据修剪的“没有免费午餐”定理，并通过随机化提出了校准协议，以提高现有修剪算法在高压缩区域的性能。

    Data pruning algorithms are commonly used to reduce the memory and computational cost of the optimization process. Recent empirical results reveal that random data pruning remains a strong baseline and outperforms most existing data pruning methods in the high compression regime, i.e., where a fraction of $30\%$ or less of the data is kept. This regime has recently attracted a lot of interest as a result of the role of data pruning in improving the so-called neural scaling laws; in [Sorscher et al.], the authors showed the need for high-quality data pruning algorithms in order to beat the sample power law.  In this work, we focus on score-based data pruning algorithms and show theoretically and empirically why such algorithms fail in the high compression regime. We demonstrate ``No Free Lunch" theorems for data pruning and present calibration protocols that enhance the performance of existing pruning algorithms in this high compression regime using randomization.
    
[^214]: 消息传递与图神经网络相结合：大规模MIMO系统的新范式

    Message Passing Meets Graph Neural Networks: A New Paradigm for Massive MIMO Systems. (arXiv:2302.06896v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2302.06896](http://arxiv.org/abs/2302.06896)

    消息传递与图神经网络的结合提出了一个新的范式，用于大规模MIMO系统的设计，解决了现有算法在面对6G系统部署大量天线时的计算复杂度问题。

    

    作为5G系统的核心技术之一，大规模多输入多输出（MIMO）引入了巨大的容量提升以及非常高的波束赋形和空间复用增益。在为大规模MIMO系统开发高效的物理层算法时，消息传递是一个有希望的候选者，因为其具有卓越的性能。然而，随着问题规模的增加，其计算复杂度大幅增加，现有的消息传递算法无法直接应用于未来的6G系统，其中将部署大量天线。为了解决这个问题，我们提出了一个基于模型驱动的深度学习（DL）框架，即AMP-GNN，用于大规模MIMO收发器设计，考虑到AMP算法的低复杂度和GNN的适应性。具体而言，AMP-GNN网络的结构通过展开近似消息传递（AMP）算法和引入图神经网络（

    As one of the core technologies for 5G systems, massive multiple-input multiple-output (MIMO) introduces dramatic capacity improvements along with very high beamforming and spatial multiplexing gains. When developing efficient physical layer algorithms for massive MIMO systems, message passing is one promising candidate owing to the superior performance. However, as their computational complexity increases dramatically with the problem size, the state-of-the-art message passing algorithms cannot be directly applied to future 6G systems, where an exceedingly large number of antennas are expected to be deployed. To address this issue, we propose a model-driven deep learning (DL) framework, namely the AMP-GNN for massive MIMO transceiver design, by considering the low complexity of the AMP algorithm and adaptability of GNNs. Specifically, the structure of the AMP-GNN network is customized by unfolding the approximate message passing (AMP) algorithm and introducing a graph neural network (
    
[^215]: 几何深度学习仅依靠距离矩阵足够吗？

    Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05743](http://arxiv.org/abs/2302.05743)

    本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。

    

    图神经网络（GNN）常用于涉及图形几何的任务，例如分子动力学模拟。虽然几何图的距离矩阵包含完整的几何信息，但已经证明消息传递神经网络（MPNNs）无法学习这种几何信息。本文通过构造新颖的对称几何图的家族，扩展了MPNN无法区分其距离矩阵的反例家族，并提出$k$-DisGNNs，可以有效地利用距离矩阵中丰富的几何结构。我们证明了模型的高表达能力，并证明了一些现有的精心设计的几何模型可以作为$k$-DisGNNs的特殊情况统一起来。最重要的是，我们建立了几何深度学习和传统图表示学习之间的联系，展示了那些最初为低度表达能力的GNN模型设计的高度表达力的GNN模型。

    Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
    
[^216]: CodeBERTScore: 使用预训练的代码模型评估代码生成

    CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. (arXiv:2302.05527v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2302.05527](http://arxiv.org/abs/2302.05527)

    CodeBERTScore是一种用于代码生成的评估指标，通过对生成的代码以及其所给定的自然语言上下文进行编码，能够更可靠地评估代码生成模型的输出。

    

    自然语言到代码模型（NL->Code）的崛起使得能够生成长表达式和语句而不仅仅是一个单独的下一个标记的模型成为可能，但是其中一个主要问题是可靠地评估它们所生成的输出。在本文中，我们提出了CodeBERTScore：一种用于代码生成的评估指标，它建立在BERTScore (Zhang et al., 2020) 的基础上。与BERTScore只对生成的标记进行编码不同，CodeBERTScore还对生成代码之前的自然语言输入进行编码，从而建模生成的代码与其所给定的自然语言上下文之间的一致性。我们对CodeBERTScore在四种编程语言上进行了广泛的评估。我们发现，CodeBERTScore与人类偏好和功能正确性的相关性比所有现有指标都更高。也就是说，CodeBERTScore评分更高的生成代码更有可能被人类青睐，并且在执行时更可能正确运行。

    Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We
    
[^217]: 一种用于量化搔抓强度的多模态感应手环

    A Multimodal Sensing Ring for Quantification of Scratch Intensity. (arXiv:2302.03813v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03813](http://arxiv.org/abs/2302.03813)

    本文介绍了一种用于量化搔抓强度的多模态感应手环，通过加速度计和接触麦克风等设备的使用，以及机器学习算法的运用，实现了对搔抓强度的估计，并展示了其在临床相关鉴别上的成果。

    

    对于许多医疗状况，客观测量慢性瘙痒非常必要以提高患者护理水平。尽管可穿戴设备已经显示出检测搔抓的潜力，但目前无法估计搔抓强度，从而无法全面了解瘙痒对个体的影响。在这项工作中，我们提出了一个框架，用于估计搔抓强度以及检测搔抓。这通过一个多模态手环设备实现，包括一个加速度计和一个接触麦克风，一个压力敏感的平板用于捕获真实强度值，以及使用机器学习算法进行回归，将搔抓强度映射到0-10连续刻度的0-600毫瓦（mW）功率刻度。我们使用留一体外交叉验证评估了我们算法在20名个体上的性能，并使用来自其他14名参与者的数据，展示了我们的算法在临床相关鉴别上的成果。

    An objective measurement of chronic itch is necessary for improvements in patient care for numerous medical conditions. While wearables have shown promise for scratch detection, they are currently unable to estimate scratch intensity, preventing a comprehensive understanding of the effect of itch on an individual. In this work, we present a framework for the estimation of scratch intensity in addition to the detection of scratch. This is accomplished with a multimodal ring device, consisting of an accelerometer and a contact microphone, a pressure-sensitive tablet for capturing ground truth intensity values, and machine learning algorithms for regression of scratch intensity on a 0-600 milliwatts (mW) power scale that can be mapped to a 0-10 continuous scale. We evaluate the performance of our algorithms on 20 individuals using leave one subject out cross-validation and using data from 14 additional participants, we show that our algorithms achieve clinically-relevant discrimination of
    
[^218]: 自洽的概率流速度匹配方法

    Self-Consistent Velocity Matching of Probability Flows. (arXiv:2301.13737v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13737](http://arxiv.org/abs/2301.13737)

    我们提出了一种无网格的框架，用于求解守恒性偏微分方程，包括时间相关的Fokker-Planck方程和Wasserstein梯度流。通过自洽的速度匹配方法和迭代形式，我们的方法绕过了计算障碍，并在高维情况下表现出优越性能。

    

    我们提出了一种无网格的可扩展框架，用于求解一类包括时间相关的Fokker-Planck方程和Wasserstein梯度流在内的守恒性偏微分方程。主要观察是PDE解的时间变化速度场需要是自洽的：它必须满足一个包含相同速度场的概率流的不动点方程。我们使用迭代形式和有偏梯度估计器的方法，绕过了具有强大实证性能的重大计算障碍，而不是直接最小化不动点方程的残差。与现有方法相比，我们的方法不受时间或空间离散化的限制，涵盖了更广泛的PDEs，并且可以在高维情况下进行扩展。实验表明，当解析解可用时，我们的方法可以准确恢复解析解，并在高维情况下取得优越的性能。

    We present a discretization-free scalable framework for solving a large class of mass-conserving partial differential equations (PDEs), including the time-dependent Fokker-Planck equation and the Wasserstein gradient flow. The main observation is that the time-varying velocity field of the PDE solution needs to be self-consistent: it must satisfy a fixed-point equation involving the probability flow characterized by the same velocity field. Instead of directly minimizing the residual of the fixed-point equation with neural parameterization, we use an iterative formulation with a biased gradient estimator that bypasses significant computational obstacles with strong empirical performance. Compared to existing approaches, our method does not suffer from temporal or spatial discretization, covers a wider range of PDEs, and scales to high dimensions. Experimentally, our method recovers analytical solutions accurately when they are available and achieves superior performance in high dimensi
    
[^219]: 偶然性和认知性歧视：公平干预的基本限制

    Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions. (arXiv:2301.11781v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11781](http://arxiv.org/abs/2301.11781)

    该论文研究了机器学习模型中的偶然性和认知性歧视，将其分类为数据分布中固有的歧视和模型开发过程中的决策导致的歧视。通过量化偶然性歧视的性能限制和刻画认知性歧视，揭示了公平干预的基本限制。研究还应用这种方法评估了现有的公平干预措施，并探究了在存在缺失值的数据中的公平风险。

    

    机器学习模型在某些人群中可能表现不佳，原因是在模型开发过程中做出的选择和数据中固有的偏见。我们将机器学习流程中的歧视来源分为两类：偶然性歧视，即数据分布中固有的歧视，和认知性歧视，即模型开发过程中做出的决策导致的歧视。我们通过确定在完全了解数据分布的情况下，在公平约束下模型的性能限制来量化偶然性歧视。我们通过应用布莱克韦尔对比统计实验的结果来刻画偶然性歧视。然后，我们将认知性歧视定义为在应用公平约束时模型的准确性与偶然性歧视所限定的界限之间的差距。我们将这种方法应用于评估现有的公平干预措施，并调查具有缺失值的数据中的公平风险。我们的结果表明...

    Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate th
    
[^220]: 深度集成的联合训练因学习者勾结而失败

    Joint Training of Deep Ensembles Fails Due to Learner Collusion. (arXiv:2301.11323v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11323](http://arxiv.org/abs/2301.11323)

    深度神经网络的深度集成联合训练通常会导致基础学习者勾结，因此直接优化整个集成的损失很少被应用。

    

    机器学习模型的集成已被广泛认可为一种提高性能的强大方法。传统上，集成算法独立或顺序地训练它们的基础学习者，目的是优化它们的联合性能。然而，在深度神经网络的集成中，我们有机会直接优化真正的目标：整个集成的联合性能。然而令人惊讶的是，在实践中很少直接最小化集成损失。相反，大多数先前的研究是独立地训练个别模型，并在训练后进行集成。在这项工作中，我们展示了这样做的原因是很好的 - 集成损失的联合优化会导致退化行为。我们通过将集成目标分解为基础学习者的强度和它们之间的差异来解决这个问题。我们发现联合优化导致了一种基础学习者勾结的现象。

    Ensembles of machine learning models have been well established as a powerful method of improving performance over a single model. Traditionally, ensembling algorithms train their base learners independently or sequentially with the goal of optimizing their joint performance. In the case of deep ensembles of neural networks, we are provided with the opportunity to directly optimize the true objective: the joint performance of the ensemble as a whole. Surprisingly, however, directly minimizing the loss of the ensemble appears to rarely be applied in practice. Instead, most previous research trains individual models independently with ensembling performed post hoc. In this work, we show that this is for good reason - joint optimization of ensemble loss results in degenerate behavior. We approach this problem by decomposing the ensemble objective into the strength of the base learners and the diversity between them. We discover that joint optimization results in a phenomenon in which base
    
[^221]: 学习用于排名的列表级别领域不变表示

    Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2212.10764](http://arxiv.org/abs/2212.10764)

    本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。

    

    领域适应旨在将在（数据丰富）源领域学到的知识转移到（资源有限）目标领域，一种常用的方法是不变表示学习，它匹配并对齐特征空间上的数据分布。尽管这种方法在分类和回归问题上得到了广泛研究和应用，但在排名问题上的应用却是零散的，并且现有的几种实现缺乏理论上的证明。本文重新审视了用于排名的不变表示学习。在审查之前的工作时，我们发现他们实施了我们称之为项目级别对齐的方法，该方法在聚合的所有列表中对进行排名的项目分布进行对齐，但忽略了列表的结构。然而，列表的结构应该被利用，因为它是排名问题的固有特性，其中数据和度量是在列表上定义和计算的，而不是在项目本身上。为了解决这一不一致，我们提出了列表级别对齐的学习

    Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
    
[^222]: 无视觉基线的多模式语法归纳

    A Vision-free Baseline for Multimodal Grammar Induction. (arXiv:2212.10564v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10564](http://arxiv.org/abs/2212.10564)

    本论文研究了在多模式设置下，只使用文本进行训练的大型语言模型（LLMs）是否能够提供强大的辅助来进行语法归纳。结果显示，基于LLM的纯文本方法在多种多模式数据集上优于先前的方法，并且在性能、参数数量和训练速度方面取得了最先进的结果。

    

    过去的研究表明，配对的视觉与语言信号能够显著改善多模式数据集（如MSCOCO）中的语法归纳。我们研究了只使用文本进行训练的大型语言模型（LLMs）在多模式设置下是否能够提供强大的辅助来进行语法归纳。我们发现，我们的纯文本方法，即基于LLM的C-PCFG（LC-PCFG），在各种多模式数据集上优于先前的多模式方法，并且获得了最先进的语法归纳性能。与带图像的语法归纳相比，LC-PCFG在语料库F1得分上超过了先前的最先进方法7.9个点，参数数量减少了85％，训练速度加快了1.7倍。在三个辅助视频的语法归纳基准中，LC-PCFG在语料库F1上优于先前的最先进方法最多7.7个点，训练速度加快了8.8倍。

    Past work has shown that paired vision-language signals substantially improve grammar induction in multimodal datasets such as MSCOCO. We investigate whether advancements in large language models (LLMs) that are only trained with text could provide strong assistance for grammar induction in multimodal settings. We find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms previous multi-modal methods, and achieves state-of-the-art grammar induction performance for various multimodal datasets. Compared to image-aided grammar induction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1 points, with an 85% reduction in parameter count and 1.7x faster training speed. Across three video-assisted grammar induction benchmarks, LC-PCFG outperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster training. These results shed light on the notion that text-only language models might include visually grounded cues that aid in grammar induction in mult
    
[^223]: Spuriosity Rankings: 使用排序数据来测量和减少偏见的方法

    Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases. (arXiv:2212.02648v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02648](http://arxiv.org/abs/2212.02648)

    这个论文提出了一种使用排序数据来测量和减少模型对虚假线索的偏见的简单有效方法。通过排名图像的虚假性，可以识别出少数子群体，并通过准确率差来评估模型的偏见。此外，通过在虚假性较低的图像上微调模型，可以在几乎不损失准确率的情况下消除模型的偏见，实现对样本的公正处理。

    

    我们提出了一种简单但有效的方法，通过排序数据来测量和减少模型对虚假线索的依赖所引起的偏见。我们的方法不需要对数据或模型训练进行昂贵的改变，而是更好地利用已有的数据。具体而言，我们基于通过可解释网络的深度神经特征来对图像进行类内排序，以衡量其虚假性（即常见虚假线索的存在程度）。通过虚假性排名，可以很容易地识别出少数子群体（即虚假性较低的图像），并通过准确率差来评估模型的偏见。甚至可以通过在虚假性较低的图像上微调分类头部，以极少的准确率损失来有效消除模型的偏见，从而实现对样本的更公正处理，无论虚假性如何。我们在ImageNet上展示了我们的方法，注释了5000个类特征依赖关系（其中630个是虚假的），并生成了一个包含325k个软分割数据的数据集。

    We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft seg
    
[^224]: 稀疏加性模型的分类

    Classification by sparse additive models. (arXiv:2212.01792v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2212.01792](http://arxiv.org/abs/2212.01792)

    这篇论文研究了非参数的稀疏加性模型用于分类，通过对分量系数施加稀疏组Lasso和稀疏组Slope型惩罚来设计分类器，实验证明了分类器在未知稀疏性和平滑性上的自适应性能。

    

    我们考虑了用于分类的非参数稀疏加性模型（SpAM）。SpAM分类器的设计基于最小化logistic损失，通过对分量展开系数施加稀疏组Lasso和更一般的稀疏组Slope型惩罚（例如，傅里叶或小波）。所得的分类器对未知的稀疏性和平滑性具有固有的自适应性。我们证明，在某些稀疏组受限特征值条件下，稀疏组Lasso分类器在整个解析、Sobolev和Besov类范围内几乎是最小化极小（加上对数因子），而稀疏组Slope分类器在稀疏和适度稠密设定下达到了确切的最小化极小阶数（不含额外的对数因子）。该分类器的性能在实际数据例子中得到了证明。

    We consider (nonparametric) sparse additive models (SpAM) for classification. The design of a SpAM classifier is based on minimizing the logistic loss with a sparse group Lasso and more general sparse group Slope-type penalties on the coefficients of univariate components' expansions in orthonormal series (e.g., Fourier or wavelets). The resulting classifiers are inherently adaptive to the unknown sparsity and smoothness. We show that under certain sparse group restricted eigenvalue condition the sparse group Lasso classifier is nearly-minimax (up to log-factors) within the entire range of analytic, Sobolev and Besov classes while the sparse group Slope classifier achieves the exact minimax order (without the extra log-factors) for sparse and moderately dense setups. The performance of the proposed classifier is illustrated on the real-data example.
    
[^225]: 通过基于抽象的训练驯服DNN控制系统的可达性分析

    Taming Reachability Analysis of DNN-Controlled Systems via Abstraction-Based Training. (arXiv:2211.11127v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11127](http://arxiv.org/abs/2211.11127)

    本文提出了一种基于抽象的方法，用于绕过在DNN控制系统中对DNN进行过度近似的可达性分析问题。通过在传统的DNN中插入一个抽象层，将实数抽象化为一个区间进行训练，进而实现对DNN控制系统的黑盒可达性分析。

    

    深度神经网络(DNNs)的内在复杂性使得验证网络本身和托管DNN控制系统变得具有挑战性。这些系统的可达性分析面临相同的挑战。现有的方法依赖于使用更简单的多项式模型对DNN进行过度近似。然而，它们效率低下并且过度估计较大，并且限于特定类型的DNNs。本文提出了一种新颖的基于抽象的方法，绕过在可达性分析中对DNNs进行过度近似的关键问题。具体而言，我们通过插入一个额外的抽象层来扩展传统的DNNs，该抽象层将实数抽象化为一个区间进行训练。插入的抽象层确保区间表示的值在训练和决策过程中对网络不可区分。利用这一点，我们设计了第一个适用于DNN控制系统的黑盒可达性分析方法，其中只对训练过的DNN进行查询。

    The intrinsic complexity of deep neural networks (DNNs) makes it challenging to verify not only the networks themselves but also the hosting DNN-controlled systems. Reachability analysis of these systems faces the same challenge. Existing approaches rely on over-approximating DNNs using simpler polynomial models. However, they suffer from low efficiency and large overestimation, and are restricted to specific types of DNNs. This paper presents a novel abstraction-based approach to bypass the crux of over-approximating DNNs in reachability analysis. Specifically, we extend conventional DNNs by inserting an additional abstraction layer, which abstracts a real number to an interval for training. The inserted abstraction layer ensures that the values represented by an interval are indistinguishable to the network for both training and decision-making. Leveraging this, we devise the first black-box reachability analysis approach for DNN-controlled systems, where trained DNNs are only querie
    
[^226]: 使用神经切线核对图神经网络中的卷积，非线性和深度进行分析

    Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel. (arXiv:2210.09809v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09809](http://arxiv.org/abs/2210.09809)

    本研究通过理论方法分析了图神经网络中的卷积、非线性和深度对网络性能的影响，同时对基于图拉普拉斯和邻接矩阵的归一化方法进行了比较，并揭示了线性GNN与非线性ReLU-GNN性能相当的现象缺乏严格的理论解释。

    

    图神经网络（GNN）的基本原理是通过使用“图卷积”来聚合相邻节点的结构信息，并选择合适的网络架构（例如深度和激活函数）。因此，理解每个设计选择对网络性能的影响至关重要。基于图拉普拉斯的卷积已成为主流选择，其中对邻接矩阵进行对称归一化是最广泛采用的方法。然而，一些经验研究表明，行归一化的邻接矩阵在节点分类方面表现更好。尽管GNN的使用非常广泛，但目前尚无严格的理论研究关于这些卷积的表示能力，无法解释这种行为。同样，线性GNN的性能与非线性ReLU-GNN的性能相当的经验观察也缺乏严格的理论支持。

    The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory.  In this work, we theoretically a
    
[^227]: 信息度量反映记忆模式

    Measures of Information Reflect Memorization Patterns. (arXiv:2210.09404v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09404](http://arxiv.org/abs/2210.09404)

    本研究通过信息论度量神经网络中神经元激活的多样性，发现其与模型的泛化能力和记忆之间存在关联。实验证明，即使在未标记的分布内部计算神经激活时，信息的组织也可以指向两种形式的记忆。

    

    神经网络已被证明利用与目标标签共现的错误假象（或捷径）来展示启发式记忆。另一方面，网络已被证明会记忆训练样本，产生基于示例的记忆。这些记忆类型阻碍了网络在超出训练分布的泛化能力。检测此类记忆可能具有挑战性，通常需要研究人员策划定制的测试集。在这项工作中，我们假设——并随后展示了——不同神经元激活模式的多样性反映了模型的泛化和记忆。我们通过信息论度量量化神经激活的多样性，并在涵盖了几个自然语言和视觉任务的实验中得到了对我们假设的支持。重要的是，我们发现信息的组织指向了这两种形式的记忆，即使是在未标记的分布内部计算的神经激活上也是如此。

    Neural networks are known to exploit spurious artifacts (or shortcuts) that co-occur with a target label, exhibiting heuristic memorization. On the other hand, networks have been shown to memorize training examples, resulting in example-level memorization. These kinds of memorization impede generalization of networks beyond their training distributions. Detecting such memorization could be challenging, often requiring researchers to curate tailored test sets. In this work, we hypothesize -- and subsequently show -- that the diversity in the activation patterns of different neurons is reflective of model generalization and memorization. We quantify the diversity in the neural activations through information-theoretic measures and find support for our hypothesis on experiments spanning several natural language and vision tasks. Importantly, we discover that information organization points to the two forms of memorization, even for neural activations computed on unlabelled in-distribution
    
[^228]: 多组分的稀疏主成分分析

    Sparse PCA With Multiple Components. (arXiv:2209.14790v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.14790](http://arxiv.org/abs/2209.14790)

    本研究提出了一种新的方法来解决稀疏主成分分析问题，通过将正交性条件重新表述为秩约束，并同时对稀疏性和秩约束进行优化。我们设计了紧凑的半正定松弛来提供高质量的上界，当每个主成分的个体稀疏性被指定时，我们通过额外的二阶锥不等式加强上界。

    

    稀疏主成分分析是一种用于以可解释的方式解释高维数据集方差的基本技术。这涉及解决一个稀疏性和正交性约束的凸最大化问题，其计算复杂度非常高。大多数现有的方法通过迭代计算一个稀疏主成分并缩减协方差矩阵来解决稀疏主成分分析，但在寻找多个相互正交的主成分时，这些方法不能保证所得解的正交性和最优性。我们挑战这种现状，通过将正交性条件重新表述为秩约束，并同时对稀疏性和秩约束进行优化。我们设计了紧凑的半正定松弛来提供高质量的上界，当每个主成分的个体稀疏性被指定时，我们通过额外的二阶锥不等式加强上界。此外，我们采用另一种方法来加强上界，我们使用额外的二阶锥不等式来加强上界。

    Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we de
    
[^229]: EffEval:一种全面评估机器翻译评价指标效率的方法

    EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics. (arXiv:2209.09593v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.09593](http://arxiv.org/abs/2209.09593)

    EffEval是一种对机器翻译评价指标效率进行全面评估的方法，其中TinyBERT在质量和效率之间提供了最佳平衡，CPU加速比GPU更显著，WMD近似没有提高效率但降低了质量，适配器提高了训练效率并在某些情况下提高了指标的质量。

    

    效率是促进包容性和减少环境成本的关键特性，特别是在LLM时代。在这项工作中，我们对机器翻译评价指标的效率进行了全面评估。我们的方法是用轻量级替代计算密集型的transformers，并在LLM表示之上采用线性和二次近似的对齐算法。我们评估了六个（无参考和有参考）指标在三个机器翻译数据集上，并检查了16个轻量级transformers。此外，我们通过使用适配器来研究COMET等指标的训练效率。我们的结果表明：（a）TinyBERT在质量和效率之间提供了最佳平衡，（b）CPU加速比GPU更显著，（c）WMD近似没有提高效率，但降低了质量，（d）适配器提高了训练效率（关于反向传播速度和内存要求），在某些情况下提高了指标的质量。

    Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs. In this work, we provide a comprehensive evaluation of efficiency for MT evaluation metrics. Our approach involves replacing computation-intensive transformers with lighter alternatives and employing linear and quadratic approximations for alignment algorithms on top of LLM representations. We evaluate six (reference-free and reference-based) metrics across three MT datasets and examine 16 lightweight transformers. In addition, we look into the training efficiency of metrics like COMET by utilizing adapters. Our results indicate that (a) TinyBERT provides the optimal balance between quality and efficiency, (b) CPU speed-ups are more substantial than those on GPU; (c) WMD approximations yield no efficiency gains while reducing quality and (d) adapters enhance training efficiency (regarding backward pass speed and memory requirements) as well as, in some cases, metric qualit
    
[^230]: 基于双逻辑回归的有偏正无标记数据方法

    Double logistic regression approach to biased positive-unlabeled data. (arXiv:2209.07787v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.07787](http://arxiv.org/abs/2209.07787)

    该论文提出了一种基于双逻辑回归的方法，用于处理存在偏差的正向无标记数据。通过避免假设倾向得分函数为常数，作者共同估计后验概率和倾向得分函数，并提出了两种估计方法。实验结果显示，所提出的方法与现有的基于期望最大化方案的方法相比是可比较或更优的。

    

    正向和无标记学习是许多应用中自然产生的重要问题。几乎所有现有方法的一个显著限制在于假设倾向得分函数是常数（SCAR假设），这在许多实际情况下是不现实的。为了避免这种假设，我们考虑了参数化方法来共同估计后验概率和倾向得分函数的问题。我们证明，在温和的假设下，当两个函数具有相同的参数形式时（例如具有不同参数的逻辑函数），相应的参数是可识别的。在此基础上，我们提出了两种估计方法：联合最大似然方法和基于两个Fisher一致表达式的交替最大化的第二种方法。我们的实验结果表明，所提出的方法与基于期望最大化方案的现有方法可比较或更优。

    Positive and unlabelled learning is an important problem which arises naturally in many applications. The significant limitation of almost all existing methods lies in assuming that the propensity score function is constant (SCAR assumption), which is unrealistic in many practical situations. Avoiding this assumption, we consider parametric approach to the problem of joint estimation of posterior probability and propensity score functions. We show that under mild assumptions when both functions have the same parametric form (e.g. logistic with different parameters) the corresponding parameters are identifiable. Motivated by this, we propose two approaches to their estimation: joint maximum likelihood method and the second approach based on alternating maximization of two Fisher consistent expressions. Our experimental results show that the proposed methods are comparable or better than the existing methods based on Expectation-Maximisation scheme.
    
[^231]: 分布式OMP的恢复保证

    Recovery Guarantees for Distributed-OMP. (arXiv:2209.07230v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.07230](http://arxiv.org/abs/2209.07230)

    该论文研究了基于正交匹配追踪的分布式方案在高维稀疏线性回归中的应用。通过适当的假设，分布式OMP方案能够以较低的信噪比下实现线性通信复杂度，并能与更复杂的方法相竞争。

    

    我们研究了基于正交匹配追踪(OMP)的高维稀疏线性回归的分布式方案。这种方案特别适用于有计算和通信限制的末端机器连接到中央融合中心的设置。我们证明，在适当的假设下，分布式OMP方案能够以与稀疏度线性和维度对数成比例的通信恢复回归向量的支持。值得注意的是，即使在信噪比低的情况下，单个机器也无法检测到支持时，这仍然成立。我们的模拟结果表明，分布式OMP方案与更计算密集的方法竞争，并在某些情况下甚至表现更好。

    We study distributed schemes for high-dimensional sparse linear regression, based on orthogonal matching pursuit (OMP). Such schemes are particularly suited for settings where a central fusion center is connected to end machines, that have both computation and communication limitations. We prove that under suitable assumptions, distributed-OMP schemes recover the support of the regression vector with communication per machine linear in its sparsity and logarithmic in the dimension. Remarkably, this holds even at low signal-to-noise-ratios, where individual machines are unable to detect the support. Our simulations show that distributed-OMP schemes are competitive with more computationally intensive methods, and in some cases even outperform them.
    
[^232]: 关于动态输出反馈的优化景观: 基于线性二次调节器的案例研究

    On the Optimization Landscape of Dynamic Output Feedback: A Case Study for Linear Quadratic Regulator. (arXiv:2209.05042v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05042](http://arxiv.org/abs/2209.05042)

    本文研究了线性二次调节器中动态输出反馈策略的优化景观，推导了最优变换并证明了当其可观测时静止点的唯一性，从而为使用策略梯度方法解决动态控制器提供了最优性证明。

    

    策略梯度算法的收敛取决于基础最优控制问题的优化景观。通过分析线性二次控制的优化景观，我们可以获得这些算法的理论洞见。然而，现有文献大多只考虑静态全状态或输出反馈策略（控制器）的优化景观。我们研究了线性二次调节器中动态输出反馈策略（简称 dLQR）的更具挑战性的情况，这在实践中普遍存在，但其优化景观相对复杂。我们首先展示了 dLQR 成本如何随动态控制器的坐标变换而变化，然后推导了给定可观控稳定控制器的最优变换。我们的一个核心结果是 dLQR 在可观测时静止点的唯一性，这为使用策略梯度方法解决动态控制器提供了最优性证明。

    The convergence of policy gradient algorithms hinges on the optimization landscape of the underlying optimal control problem. Theoretical insights into these algorithms can often be acquired from analyzing those of linear quadratic control. However, most of the existing literature only considers the optimization landscape for static full-state or output feedback policies (controllers). We investigate the more challenging case of dynamic output-feedback policies for linear quadratic regulation (abbreviated as dLQR), which is prevalent in practice but has a rather complicated optimization landscape. We first show how the dLQR cost varies with the coordinate transformation of the dynamic controller and then derive the optimal transformation for a given observable stabilizing controller. One of our core results is the uniqueness of the stationary point of dLQR when it is observable, which provides an optimality certificate for solving dynamic controllers using policy gradient methods. More
    
[^233]: 高分辨率卫星图像中基于对比学习的渐进域自适应对象检测

    Progressive Domain Adaptation with Contrastive Learning for Object Detection in the Satellite Imagery. (arXiv:2209.02564v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.02564](http://arxiv.org/abs/2209.02564)

    本文提出了一种在高分辨率卫星图像中改进对象检测的小物体检测管线，通过对特征提取过程的改进和对图像难度评分的应用，以及利用对比学习和渐进域自适应生成域不变的特征，有效提高了对象识别效果。

    

    如今在卫星和无人机图像上应用的最先进的对象检测方法往往无法识别小而密集的对象。其中一个原因是由于拍摄的地面区域和采集条件的高变异性，导致航拍图像中的内容变异性大。另一个原因是航拍图像中的对象数量和大小与消费者数据大不相同。本文提出了一种小物体检测管线，通过空间金字塔池化、跨层部分网络、基于热图的区域候选网络以及通过新颖的图像难度评分来适应整体焦点损失度量的对象定位和识别，改进了特征提取过程。接下来，我们提出了一种新颖的对比学习方法，通过渐进域自适应在航拍数据集中生成域不变的特征，使用局部和全局组件。我们证明我们可以减轻对象识别效果的衰减。

    State-of-the-art object detection methods applied to satellite and drone imagery largely fail to identify small and dense objects. One reason is the high variability of content in the overhead imagery due to the terrestrial region captured and the high variability of acquisition conditions. Another reason is that the number and size of objects in aerial imagery are very different than in the consumer data. In this work, we propose a small object detection pipeline that improves the feature extraction process by spatial pyramid pooling, cross-stage partial networks, heatmap-based region proposal network, and object localization and identification through a novel image difficulty score that adapts the overall focal loss measure based on the image difficulty. Next, we propose novel contrastive learning with progressive domain adaptation to produce domain-invariant features across aerial datasets using local and global components. We show we can alleviate the degradation of object identifi
    
[^234]: R\'enyi和近似差分隐私中通过洗牌实现更强隐私放大效果

    Stronger Privacy Amplification by Shuffling for R\'enyi and Approximate Differential Privacy. (arXiv:2208.04591v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2208.04591](http://arxiv.org/abs/2208.04591)

    本研究在R\'enyi和近似差分隐私中通过洗牌提出了更强隐私放大的方法，并对理论和数值进行了改进和优化。

    

    差分隐私的洗牌模型作为标准本地和集中模型之间的一种中间信任模型，引起了极大的关注。在该模型中的一个关键结果是，随机洗牌本地随机化数据可以放大差分隐私的保证。这种放大效果意味着对于匿名贡献数据的系统而言，隐私保证会更加强大。本研究在理论和数值上改进了洗牌导致的隐私放大效果。首先，我们首次提出了LDP随机化器洗牌输出的R\'enyi差分隐私参数的渐近最优分析。其次，我们对隐私放大效果进行了新的分析，改进了[FMT20]的技术，并在所有参数设置下得到了更紧密的数值界限。

    The shuffle model of differential privacy has gained significant interest as an intermediate trust model between the standard local and central models [EFMRTT19; CSUZZ19]. A key result in this model is that randomly shuffling locally randomized data amplifies differential privacy guarantees. Such amplification implies substantially stronger privacy guarantees for systems in which data is contributed anonymously [BEMMRLRKTS17].  In this work, we improve the state of the art privacy amplification by shuffling results both theoretically and numerically. Our first contribution is the first asymptotically optimal analysis of the R\'enyi differential privacy parameters for the shuffled outputs of LDP randomizers. Our second contribution is a new analysis of privacy amplification by shuffling. This analysis improves on the techniques of [FMT20] and leads to tighter numerical bounds in all parameter settings.
    
[^235]: 带有丢失销售和不确定供应的库存系统的学习排序

    Learning to Order for Inventory Systems with Lost Sales and Uncertain Supplies. (arXiv:2207.04550v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2207.04550](http://arxiv.org/abs/2207.04550)

    本论文提出了一种计算有效的在线学习算法，用于解决在计划时间内无法处理的库存控制问题。该算法实现了 $O(L+\sqrt{T})$ 的后悔。

    

    本文考虑在计划时间 $T$ 内具有导向时间 $L$ 的随机丢失销售库存控制系统。由于随机产量/产能等原因，供应是不确定的，是订单数量的函数。我们旨在最小化 $T$ 期成本，但即使在需求和供应分布已知的情况下，这个问题也是计算无法处理的。在本文中，我们假设需求和供应分布都未知，并发展了一种计算有效的在线学习算法。我们证明了当 $L\geq\log(T)$ 时，我们的算法实现了 $O(L+\sqrt{T})$ 的后悔（即我们的算法成本和 $T$ 期最优策略之间的性能差距）。我们是通过 1）展示我们的算法成本比完全信息下一个最优常数订单策略高至多 $O(L+\sqrt{T})$（一个众所周知的广泛使用的算法）; 2）利用其已知的性能保证从现有的文学中实现的。

    We consider a stochastic lost-sales inventory control system with a lead time $L$ over a planning horizon $T$. Supply is uncertain, and is a function of the order quantity (due to random yield/capacity, etc). We aim to minimize the $T$-period cost, a problem that is known to be computationally intractable even under known distributions of demand and supply. In this paper, we assume that both the demand and supply distributions are unknown and develop a computationally efficient online learning algorithm. We show that our algorithm achieves a regret (i.e. the performance gap between the cost of our algorithm and that of an optimal policy over $T$ periods) of $O(L+\sqrt{T})$ when $L\geq\log(T)$. We do so by 1) showing our algorithm cost is higher by at most $O(L+\sqrt{T})$ for any $L\geq 0$ compared to an optimal constant-order policy under complete information (a well-known and widely-used algorithm) and 2) leveraging its known performance guarantee from the existing literature. To the 
    
[^236]: 在具有不确定性集合正则化的连续控制任务中的强化学习

    Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization. (arXiv:2207.02016v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02016](http://arxiv.org/abs/2207.02016)

    该论文提出了一种新的正则化器USR，通过构建转换函数参数空间上的不确定性集合来提高连续控制任务中的强化学习性能。通过对值函数进行对抗生成未知不确定性集合，进一步增强了USR的灵活性。在真实世界强化学习基准测试中得到了改进的结果。

    

    强化学习（RL）被认为在环境扰动下缺乏泛化性和鲁棒性，这严重限制了其在实际机器人领域的应用。以前的研究声称，在值函数中添加正则化等价于学习具有不确定转换的鲁棒策略。尽管正则化-鲁棒性转换因其简单和高效而具有吸引力，但在连续控制任务中仍然存在不足。在本文中，我们提出了一种新的正则化器，名为不确定性集合正则化器（USR），通过在转换函数的参数空间上构建不确定性集合来实现。特别是，USR足够灵活，可以插入到任何现有的RL框架中。为了处理未知的不确定性集合，我们进一步提出了一种基于值函数生成的新颖对抗方法来生成它们。我们在真实世界强化学习（RWRL）基准测试上评估了USR，展示了改进的结果。

    Reinforcement learning (RL) is recognized as lacking generalization and robustness under environmental perturbations, which excessively restricts its application for real-world robotics. Prior work claimed that adding regularization to the value function is equivalent to learning a robust policy with uncertain transitions. Although the regularization-robustness transformation is appealing for its simplicity and efficiency, it is still lacking in continuous control tasks. In this paper, we propose a new regularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer (USR), by formulating the uncertainty set on the parameter space of the transition function. In particular, USR is flexible enough to be plugged into any existing RL framework. To deal with unknown uncertainty sets, we further propose a novel adversarial approach to generate them based on the value function. We evaluate USR on the Real-world Reinforcement Learning (RWRL) benchmark, demonstrating improvements i
    
[^237]: 受Universum学习启发的监督对比学习

    Universum-inspired Supervised Contrastive Learning. (arXiv:2204.10695v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.10695](http://arxiv.org/abs/2204.10695)

    Mixup is a popular data augmentation method that synthesizes extra samples. This paper explores the potential of Mixup to generate in-domain samples as universum negatives in supervised contrastive learning, providing high-quality hard negatives and reducing the need for large batch sizes. The proposed UniCon incorporates Mixup strategy to generate Mixup-induced universum as negatives.

    

    作为一种有效的数据增强方法，Mixup通过线性插值合成了大量的样本。尽管 Mixup 在理论上依赖于数据属性，但据报道，它作为一种规则化器和校准器在深度模型训练中表现良好，为深度模型的鲁棒性和泛化性提供了可靠的贡献。在本文中，受到使用带有课外样本辅助目标任务的 Universum 学习的启发，我们从一个被广泛忽视的角度对 Mixup 进行了研究 - 利用其生成不属于目标类别的域内样本，即 universum。我们发现，在监督对比学习的框架下，通过 Mixup 产生的 universum 可以作为高质量的困难负样本，极大地减轻了对大批次大小在对比学习中的需要。基于这些发现，我们提出了受Universum学习启发的监督对比学习（UniCon），它将Mixup策略应用于产生Mixup诱导的universum作为负向样本。

    As an effective data augmentation method, Mixup synthesizes an extra amount of samples through linear interpolations. Despite its theoretical dependency on data properties, Mixup reportedly performs well as a regularizer and calibrator contributing reliable robustness and generalization to deep model training. In this paper, inspired by Universum Learning which uses out-of-class samples to assist the target tasks, we investigate Mixup from a largely under-explored perspective - the potential to generate in-domain samples that belong to none of the target classes, that is, universum. We find that in the framework of supervised contrastive learning, Mixup-induced universum can serve as surprisingly high-quality hard negatives, greatly relieving the need for large batch sizes in contrastive learning. With these findings, we propose Universum-inspired supervised Contrastive learning (UniCon), which incorporates Mixup strategy to generate Mixup-induced universum as universum negatives and p
    
[^238]: DAMNETS：一种用于生成马尔可夫网络时间序列的深度自回归模型

    DAMNETS: A Deep Autoregressive Model for Generating Markovian Network Time Series. (arXiv:2203.15009v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.15009](http://arxiv.org/abs/2203.15009)

    DAMNETS是一种用于生成马尔可夫网络时间序列的深度自回归模型，通过在真实数据和合成数据集上的表现超过竞争方法，达到了设计灵活且可扩展的生成模型的目标。

    

    生成模型在网络时间序列（也称为动态图）中具有巨大的潜力，可以应用于流行病学、生物学和经济学等领域，其中复杂的基于图的动态是核心研究对象。由于数据的高维度以及表示时间依赖性和边际网络结构的需要，设计灵活且可扩展的生成模型是一项非常具有挑战性的任务。在这里，我们介绍了一种名为DAMNETS的可扩展深度生成模型，它在真实数据和合成数据集上的所有样本质量指标上表现优于竞争方法。

    Generative models for network time series (also known as dynamic graphs) have tremendous potential in fields such as epidemiology, biology and economics, where complex graph-based dynamics are core objects of study. Designing flexible and scalable generative models is a very challenging task due to the high dimensionality of the data, as well as the need to represent temporal dependencies and marginal network structure. Here we introduce DAMNETS, a scalable deep generative model for network time series. DAMNETS outperforms competing methods on all of our measures of sample quality, over both real and synthetic data sets.
    
[^239]: 标签层级转换：深入研究类层次结构以增强深度分类器

    Label Hierarchy Transition: Delving into Class Hierarchies to Enhance Deep Classifiers. (arXiv:2112.02353v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.02353](http://arxiv.org/abs/2112.02353)

    本文提出了Label Hierarchy Transition (LHT)框架，基于深度学习，用于改进层次分类。LHT框架主要包括转换网络和混淆损失两个部分，通过显式学习标签层次转换矩阵和鼓励分类网络处理混淆情况，有效地利用类层次结构的相关性。

    

    层次分类旨在将对象按照类别的层次结构进行排序。现有方法通常通过将其解耦为一系列多类别分类任务来处理层次分类。然而，这种多任务学习策略未能充分利用层次结构不同层级之间各个类别之间的相关性。在本文中，我们提出了一种基于深度学习的统一概率框架Label Hierarchy Transition (LHT)，以应对层次分类的挑战。LHT框架由一个转换网络和一个混淆损失组成。转换网络专注于显式学习标签层次转换矩阵，这有助于有效地编码嵌入在类层次结构中的潜在相关性。混淆损失鼓励分类网络学习更好地处理类别之间的混淆情况。

    Hierarchical classification aims to sort the object into a hierarchical structure of categories. For example, a bird can be categorized according to a three-level hierarchy of order, family, and species. Existing methods commonly address hierarchical classification by decoupling it into a series of multi-class classification tasks. However, such a multi-task learning strategy fails to fully exploit the correlation among various categories across different levels of the hierarchy. In this paper, we propose Label Hierarchy Transition (LHT), a unified probabilistic framework based on deep learning, to address the challenges of hierarchical classification. The LHT framework consists of a transition network and a confusion loss. The transition network focuses on explicitly learning the label hierarchy transition matrices, which has the potential to effectively encode the underlying correlations embedded within class hierarchies. The confusion loss encourages the classification network to le
    
[^240]: Pharmacoprint -- 一种将药效固指纹和人工智能结合为计算辅助药物设计工具的方法

    Pharmacoprint -- a combination of pharmacophore fingerprint and artificial intelligence as a tool for computer-aided drug design. (arXiv:2110.01339v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2110.01339](http://arxiv.org/abs/2110.01339)

    提出了一种将药效固指纹和人工智能结合的新方法 Pharmacoprint，通过编码分子的药效固特征，提供了高分辨率的药物设计工具，优于其他常用的分子指纹方法。

    

    结构指纹和药效固建模是在化学信息学的各个领域中使用了至少二十年的方法学：从相似性搜索到机器学习。随着体外技术的进步，将这两种方法结合成一种称为药效固指纹的新方法。本文提出了一个高分辨率的药效固指纹 Pharmacoprint，它编码了分子的药效固特征的存在、类型和关系。通过使用机器学习算法（逻辑回归、支持向量机、线性支持向量机和神经网络）对 Pharmacoprint 进行了分类实验，并优于其他流行的分子指纹（即 Estate、MACCS、PubChem、亚结构、Klekotha-Roth、CDK、扩展和 GraphOnly）和 ChemAxon 药效固特征指纹。Pharmacoprint 包含了 39973 个位；使用了多种方法进行了降维处理，

    Structural fingerprints and pharmacophore modeling are methodologies that have been used for at least two decades in various fields of cheminformatics: from similarity searching to machine learning (ML). Advances in silico techniques consequently led to combining both these methodologies into a new approach known as pharmacophore fingerprint. Herein, we propose a high-resolution, pharmacophore fingerprint called Pharmacoprint that encodes the presence, types, and relationships between pharmacophore features of a molecule. Pharmacoprint was evaluated in classification experiments by using ML algorithms (logistic regression, support vector machines, linear support vector machines, and neural networks) and outperformed other popular molecular fingerprints (i.e., Estate, MACCS, PubChem, Substructure, Klekotha-Roth, CDK, Extended, and GraphOnly) and ChemAxon Pharmacophoric Features fingerprint. Pharmacoprint consisted of 39973 bits; several methods were applied for dimensionality reduction,
    
[^241]: 一类稳定高效的强化学习用替代函数的普适框架

    A general class of surrogate functions for stable and efficient reinforcement learning. (arXiv:2108.05828v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.05828](http://arxiv.org/abs/2108.05828)

    本研究提出了一个基于函数镜像上升的普适框架(FMA-PG)，构建了一系列替代函数，这些函数可以实现策略改进，并且不受策略参数化选择的影响。通过实验证实，该方法具有良好的性能和理论保证。

    

    传统的策略梯度方法依赖于一系列替代函数的最大化。近年来，提出了许多这样的替代函数，大多数没有强有力的理论保证，从而导致了TRPO、PPO或MPO等算法的出现。我们不是设计另一个替代函数，而是提出了一个基于函数镜像上升的普适框架（FMA-PG），从而产生了一整套替代函数。我们构建了替代函数，使其能够保证策略改进，这是大多数现有替代函数所没有的特性。关键是，这些保证不受策略参数化选择的影响。此外，FMA-PG的特定实例恢复了重要的实现启发式方法（例如，使用前向和反向KL散度），从而产生了具有额外理想性质的TRPO变种。通过在简单贝叶斯问题上进行实验，我们评估了FMA-PG产生的算法实例。该框架也支持其他应用。

    Common policy gradient methods rely on the maximization of a sequence of surrogate functions. In recent years, many such surrogate functions have been proposed, most without strong theoretical guarantees, leading to algorithms such as TRPO, PPO or MPO. Rather than design yet another surrogate function, we instead propose a general framework (FMA-PG) based on functional mirror ascent that gives rise to an entire family of surrogate functions. We construct surrogate functions that enable policy improvement guarantees, a property not shared by most existing surrogate functions. Crucially, these guarantees hold regardless of the choice of policy parameterization. Moreover, a particular instantiation of FMA-PG recovers important implementation heuristics (e.g., using forward vs reverse KL divergence) resulting in a variant of TRPO with additional desirable properties. Via experiments on simple bandit problems, we evaluate the algorithms instantiated by FMA-PG. The proposed framework also su
    
[^242]: Bures-Wasserstein流形上的平均：梯度下降的无维收敛。

    Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent. (arXiv:2106.08502v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2106.08502](http://arxiv.org/abs/2106.08502)

    本文研究了用于计算高斯分布相对于最优输运度量的重心的算法，在Bures-Wasserstein流形上证明了新的测地凸性结果，提供了更强的迭代控制，实现了无维收敛率，同时还提供了用于这些问题的Riemannian GD 的第一个收敛保证。

    

    我们研究了用于计算高斯分布相对于最优输运度量的重心的一阶优化算法。尽管目标是测地不凸的，但测地梯度下降在经验上快速收敛，实际上比诸如欧几里德梯度下降和SDP求解器等现成方法更快。这与Riemannian GD的已知的最佳理论结果形成了鲜明对比，后者是以指数方式依赖于维度的。在这项工作中，我们证明了新的测地凸性结果，提供了更强的迭代控制，从而得到了无维收敛率。我们的技术还使得对两种相关的平均概念 - 熵正则化的重心和几何中位数进行分析，为这些问题的Riemannian GD提供了第一个收敛保证。

    We study first-order optimization algorithms for computing the barycenter of Gaussian distributions with respect to the optimal transport metric. Although the objective is geodesically non-convex, Riemannian GD empirically converges rapidly, in fact faster than off-the-shelf methods such as Euclidean GD and SDP solvers. This stands in stark contrast to the best-known theoretical results for Riemannian GD, which depend exponentially on the dimension. In this work, we prove new geodesic convexity results which provide stronger control of the iterates, yielding a dimension-free convergence rate. Our techniques also enable the analysis of two related notions of averaging, the entropically-regularized barycenter and the geometric median, providing the first convergence guarantees for Riemannian GD for these problems.
    
[^243]: Helmholtzian特征图：从点云数据中发现拓扑特征和边缘流学习

    Helmholtzian Eigenmap: Topological feature discovery & edge flow learning from point cloud data. (arXiv:2103.07626v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2103.07626](http://arxiv.org/abs/2103.07626)

    本论文提出了一种从点云数据中估计流形Helmholtzian的方法，通过加权1-Laplacian构建了图Helmholtzian作为连续算子的一致估计器，并利用Helmholtz-Hodge定理对流和向量场进行分析。通过该方法，可以对流进行平滑、预测和特征提取。

    

    流形Helmholtzian（1-Laplacian）算子$ \Delta_1 $将Laplace-Beltrami算子优雅地广义化到流形$ \mathcal M $上的向量场。在本工作中，我们提出了通过加权1-Laplacian $ \mathcal L_1 $从点云数据估计流形Helmholtzian的方法。虽然已经引入和研究了高阶Laplacian，但本工作是首次提出了从单纯复合物构建的图Helmholtzian作为连续算子在非参数设置中的一致估计器。具备关于$ \mathcal M $的几何和拓扑信息的Helmholtzian是通过Helmholtz-Hodge定理对$ \mathcal M $上的流和向量场进行分析的有用工具。此外，$ \mathcal L_1 $允许对流进行平滑、预测和特征提取。我们在具有非平凡拓扑结构的大量合成和真实点云数据集上展示了这些可能性，并提供了理论依据。

    The manifold Helmholtzian (1-Laplacian) operator $\Delta_1$ elegantly generalizes the Laplace-Beltrami operator to vector fields on a manifold $\mathcal M$. In this work, we propose the estimation of the manifold Helmholtzian from point cloud data by a weighted 1-Laplacian $\mathcal L_1$. While higher order Laplacians have been introduced and studied, this work is the first to present a graph Helmholtzian constructed from a simplicial complex as a consistent estimator for the continuous operator in a non-parametric setting. Equipped with the geometric and topological information about $\mathcal M$, the Helmholtzian is a useful tool for the analysis of flows and vector fields on $\mathcal M$ via the Helmholtz-Hodge theorem. In addition, the $\mathcal L_1$ allows the smoothing, prediction, and feature extraction of the flows. We demonstrate these possibilities on substantial sets of synthetic and real point cloud datasets with non-trivial topological structures; and provide theoretical r
    
[^244]: 合成干预

    Synthetic Interventions. (arXiv:2006.07691v6 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2006.07691](http://arxiv.org/abs/2006.07691)

    提出了一个称为合成干预的因果框架，能够在观察到每个单元最多两个干预措施的情况下推断每个单元对每个干预措施的预期潜在结果，具有有限样本一致性和渐近正态性。

    

    考虑一个拥有$N$个异质单元（例如个体或子群体）和$D$个干预措施（例如社会经济政策）的情景。我们的目标是学习每个单元对每个干预措施的预期潜在结果，总共有$N \times D$个因果参数。为此，我们提出了一个因果框架——合成干预（SI），以推断这$N \times D$个因果参数，同时仅观察每个单元在最多两个干预措施下的情况，与$D$无关。当个性化水平增加时，这将具有重要意义。在一个新的张量因子模型下，跨单元、结果和干预措施，我们证明了这$N \times D$个因果参数的识别结果，并在附加条件下证明了我们估计值的有限样本一致性和渐近正态性。重要的是，我们的估计器还允许存在决定干预分配方式的潜在混淆因素。

    Consider a setting with $N$ heterogeneous units (e.g., individuals, sub-populations) and $D$ interventions (e.g., socio-economic policies). Our goal is to learn the expected potential outcome associated with every intervention on every unit, totaling $N \times D$ causal parameters. Towards this, we present a causal framework, synthetic interventions (SI), to infer these $N \times D$ causal parameters while only observing each of the $N$ units under at most two interventions, independent of $D$. This can be significant as the number of interventions, i.e., level of personalization, grows. Under a novel tensor factor model across units, outcomes, and interventions, we prove an identification result for each of these $N \times D$ causal parameters, establish finite-sample consistency of our estimator along with asymptotic normality under additional conditions. Importantly, our estimator also allows for latent confounders that determine how interventions are assigned. The estimator is furt
    
[^245]: 重访多智能体深度强化学习中的参数共享

    Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning. (arXiv:2005.13625v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.13625](http://arxiv.org/abs/2005.13625)

    本研究重访了多智能体深度强化学习中的参数共享方法。我们通过引入智能体指示信号实现了在不同策略网络共享参数的同时学习不同策略或任务的能力，并且证明了这些方法在异构观测和行动空间学习中可以收敛到最优策略。

    

    参数共享是多智能体深度强化学习中一种常用的基准方法，每个智能体都独立学习一个策略，并且所有策略之间共享参数。然而，由于所有智能体共享同一策略网络，它们无法学习不同的策略或任务。为了解决这个问题，我们通过向观测中添加智能体特定的指示信号（称为“智能体指示”）来进行实验性的改进。然而，智能体指示的局限在于，如果不进行修改，它无法应用于行动空间和/或观测空间不同质的环境。本研究正式定义了智能体指示的概念，并证明了它首次实现了收敛到最优策略。接下来，我们正式介绍了扩展参数共享到异构观测和行动空间学习的方法，并证明了这些方法可以实现收敛到最优策略。最后，我们进行了实验验证并对比了各种方法的性能。

    Parameter sharing, where each agent independently learns a policy with fully shared parameters between all policies, is a popular baseline method for multi-agent deep reinforcement learning. Unfortunately, since all agents share the same policy network, they cannot learn different policies or tasks. This issue has been circumvented experimentally by adding an agent-specific indicator signal to observations, which we term "agent indication". Agent indication is limited, however, in that without modification it does not allow parameter sharing to be applied to environments where the action spaces and/or observation spaces are heterogeneous. This work formalizes the notion of agent indication and proves that it enables convergence to optimal policies for the first time. Next, we formally introduce methods to extend parameter sharing to learning in heterogeneous observation and action spaces, and prove that these methods allow for convergence to optimal policies. Finally, we experimentally
    
[^246]: 利用未标记数据扩展类别的开放集学习（Open-LACU）

    Open-set learning with augmented category by exploiting unlabeled data (Open-LACU). (arXiv:2002.01368v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2002.01368](http://arxiv.org/abs/2002.01368)

    Open-LACU是一种新的开放式学习策略，它可以将分类器推广到观察到的和未观察到的新颖类别之间，并通过定义不同的背景和未知类别来提高训练成本效益性，确保在存在未观察到的新颖类别时进行安全分类。

    

    对于半监督学习（SSL）和开放式识别（OSR），已经进行了许多尝试以合成单个训练策略。然而，每次尝试都违反了开放集定义，因为这些方法在未标记的训练集中包含新颖的类别。本研究提出了一种新的学习策略，其中分类器能够在观察到的和未观察到的新颖类别之间进行推广，从而定义了观察到新颖类别的背景类别和未观察到新颖类别的未知类别。通过分类这两种新颖类别的方式，Open-LACU能够提高训练的成本效益性，并确保在存在未观察到的新颖类别时进行安全分类。

    Several efforts have been made to synthesize semi-supervised learning (SSL) and open set recognition (OSR) within a single training policy. However, each attempt violated the definition of an open set by incorporating novel categories within the unlabeled training set. Although such \textit{observed} novel categories are undoubtedly prevalent in application-grade datasets, they should not be conflated with the OSR-defined \textit{unobserved} novel categories, which only emerge during testing. This study proposes a new learning policy wherein classifiers generalize between observed and unobserved novel categories. Specifically, our open-set learning with augmented category by exploiting unlabeled data (Open-LACU) policy defines a background category for observed novel categories and an unknown category for unobserved novel categories. By separating these novel category types, Open-LACU promotes cost-efficient training by eliminating the need to label every category and ensures safe clas
    
[^247]: 加权赌博机或者：赌博机如何学习预期之外的扭曲价值

    Weighted bandits or: How bandits learn distorted values that are not expected. (arXiv:1611.10283v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1611.10283](http://arxiv.org/abs/1611.10283)

    本论文研究了带有扭曲概率的随机多臂赌博机问题，并提出了以UCB算法为基础、考虑了奖励扭曲并具有次线性后悔的算法。

    

    受到用于解释常见偏离传统预期价值偏好的人类决策模型的启发，我们提出了两个带有扭曲概率的随机多臂赌博机问题：经典的K臂赌博机和线性参数化赌博机设置。我们在对多臂赌博机的后悔最小化和最佳臂识别框架下研究了上述问题。对于K臂赌博机以及线性赌博机问题的后悔最小化设置，我们提出了受到上置信界(UCB)算法启发、包含奖励扭曲并且具有次线性后悔的算法。对于K臂赌博机设置，我们得出了对我们提出的算法的预期后悔的上界，然后我们证明了一个匹配的下界，以验证我们算法的次线性优化顺序。对于线性参数化设置，我们的算法实现了一个后悔上界，该上界是次线性的。

    Motivated by models of human decision making proposed to explain commonly observed deviations from conventional expected value preferences, we formulate two stochastic multi-armed bandit problems with distorted probabilities on the reward distributions: the classic $K$-armed bandit and the linearly parameterized bandit settings. We consider the aforementioned problems in the regret minimization as well as best arm identification framework for multi-armed bandits. For the regret minimization setting in $K$-armed as well as linear bandit problems, we propose algorithms that are inspired by Upper Confidence Bound (UCB) algorithms, incorporate reward distortions, and exhibit sublinear regret. For the $K$-armed bandit setting, we derive an upper bound on the expected regret for our proposed algorithm, and then we prove a matching lower bound to establish the order-optimality of our algorithm. For the linearly parameterized setting, our algorithm achieves a regret upper bound that is of the 
    

