# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective.](http://arxiv.org/abs/2306.13092) | SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。 |
| [^2] | [Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces.](http://arxiv.org/abs/2306.13091) | 该论文提出了一种基于属性条件的GAN方法，可以生成具有指定属性的对抗性伪造人脸图像，这些图像可以规避取证分类器的检测，同时保留所需的属性。 |
| [^3] | [GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning.](http://arxiv.org/abs/2306.13089) | 本研究提出了一种名为GIMLET的统一图文模型，用于在零样本设置下使用自然语言指令完成分子相关任务。我们解决了现有模型的指令处理不足和图形容量有限的问题，并证明了使用GIMLET能够增强图形特征的泛化能力。 |
| [^4] | [Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting.](http://arxiv.org/abs/2306.13085) | 该论文提出了一种混合离线强化学习数据集的改进方法，可以通过重新加权采样数据集来充分利用高表现的轨迹，以提高目标策略的表现。 |
| [^5] | [A Comparison of Time-based Models for Multimodal Emotion Recognition.](http://arxiv.org/abs/2306.13076) | 本研究比较了不同序列模型在多模态情感识别中的表现，发现将声音数据转录成文本并与图像数据一起分析可以提高情感识别的准确性。 |
| [^6] | [Auditing Predictive Models for Intersectional Biases.](http://arxiv.org/abs/2306.13064) | 本研究提出了一种灵活的审计框架——条件偏差扫描（CBS），用于检测分类模型中的交叉偏见。与审计子组公平性的类似方法相比，CBS能够检测到更多未曾发现的交叉和情境偏见。 |
| [^7] | [SQ Lower Bounds for Learning Bounded Covariance GMMs.](http://arxiv.org/abs/2306.13057) | 本文研究了在有界协方差情况下学习分离高斯混合模型问题的复杂性，并证明了使用SQ算法的复杂度下限为$d^{\Omega(1/\epsilon)}$。 |
| [^8] | [Quantum Pufferfish Privacy: A Flexible Privacy Framework for Quantum Systems.](http://arxiv.org/abs/2306.13054) | 本文提出了一种称为量子河豚隐私(QPP)的通用隐私框架，通过在指定私有信息、可行的测量和域知识方面提供灵活性，实现了量子差分隐私的概括和克服限制。同时，首次提供了QPP的操作解释，并证明了QPP的凸性、组合性和后处理，推导了保证去极化机制QPP的参数，并将QPP框架应用于隐私审计，以识别隐私侵犯。 |
| [^9] | [Context-lumpable stochastic bandits.](http://arxiv.org/abs/2306.13053) | 本文研究了具有 $S$ 个上下文和 $A$ 种行动的情境赌徒问题，并提出了一种可分组的随机赌徒问题算法。 |
| [^10] | [Data augmentation for recommender system: A semi-supervised approach using maximum margin matrix factorization.](http://arxiv.org/abs/2306.13050) | 本研究提出了一种基于最大边际矩阵分解的半监督方法来增广和细化协同过滤算法的评级预测。该方法利用自我训练来评估评分的置信度，并通过系统的数据增广策略来提高算法性能。 |
| [^11] | [Multi-Task Learning with Loop Specific Attention for CDR Structure Prediction.](http://arxiv.org/abs/2306.13045) | 本文提出了带有圈特定注意机制的多任务学习模型（MLSA），用于共同学习三个CDR环的结构预测，其中H3环的CDR结构由于长度和灵活的结构具有更大挑战性。在实验评估中，该方法显著降低了预测误差。 |
| [^12] | [Towards Explainable Evaluation Metrics for Machine Translation.](http://arxiv.org/abs/2306.13041) | 本研究探索机器翻译可解释性评估指标，提供综合综述和最新方法，并贡献下一代方法的愿景。 |
| [^13] | [Online Self-Supervised Learning in Machine Learning Intrusion Detection for the Internet of Things.](http://arxiv.org/abs/2306.13030) | 提出了一种在线自监督学习的机器学习入侵检测框架，能够实现完全在线的IDS，无需离线学习或人工干预，该框架能够快速适应流量的时变特性，消除了离线数据收集的需要，并避免了数据标记中的人为误差，具有准确率高、成本低等优势。 |
| [^14] | [Decentralized Online Federated G-Network Learning for Lightweight Intrusion Detection.](http://arxiv.org/abs/2306.13029) | 提出了一种分布式和在线联邦学习入侵检测（DOF-ID）架构，它允许每个用于网络系统的入侵检测系统从其他网络系统中获得经验以及本地数据进行学习而不违反其他系统的数据隐私，显著提高了所有正在协作的系统的入侵检测性能。 |
| [^15] | [Transferable Curricula through Difficulty Conditioned Generators.](http://arxiv.org/abs/2306.13028) | 本文提出了一种名为PERM的方法，可以直接对环境难度和RL代理的能力进行建模，通过匹配环境的难度生成课程，实现了在参数化环境中训练RL代理的有前途的结果。 |
| [^16] | [Can Differentiable Decision Trees Learn Interpretable Reward Functions?.](http://arxiv.org/abs/2306.13004) | 本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。 |
| [^17] | [Can a single image processing algorithm work equally well across all phases of DCE-MRI?.](http://arxiv.org/abs/2306.12988) | 本文研究了使用不同比例的增强对比（CE）数据训练对DCE-MRI图像分割和注册任务的影响。结果表明，使用CE数据进行预训练并使用非CE数据进行微调可以创建一个通用的模型，以达到最佳结果。 |
| [^18] | [Inferring the finest pattern of mutual independence from data.](http://arxiv.org/abs/2306.12984) | 通过引入二分独立性，利用i.i.d.正态分布数据估计随机变量最精细的互相独立模式，并在模拟数据和实验数据上进行测试。 |
| [^19] | [Towards More Realistic Membership Inference Attacks on Large Diffusion Models.](http://arxiv.org/abs/2306.12983) | 本文研究了成员推断攻击问题，以确定图像是否在训练集中使用。研究集中于稳定扩散模型，提出了一种公平的评估框架，并进行了成员攻击，揭示了先前提出的评估设置不能很好地模拟真实世界中的成员攻击。 |
| [^20] | [Sum-Rate Maximization of RSMA-based Aerial Communications with Energy Harvesting: A Reinforcement Learning Approach.](http://arxiv.org/abs/2306.12977) | 本文提出了一种基于强化学习的能量收集的 RSMA 空中通信的和速率最大化方法，通过限制每个时间的最大传输功率和使用序列最小二乘规划等方法，实现了对机器人和其他设备提供高效的服务。 |
| [^21] | [Adaptive Bernstein Change Detector for High-Dimensional Data Streams.](http://arxiv.org/abs/2306.12974) | 本文提出了一个适用于高维数据流的自适应伯恩斯坦变化检测器，具有准确地识别变化发生的时间与子空间，并能够量化严重程度的特性。 |
| [^22] | [Stock Price Prediction using Dynamic Neural Networks.](http://arxiv.org/abs/2306.12969) | 本文针对股票价格预测，使用一种新的动态神经网络模型，提供了比许多现有技术更准确的预测方法，并驳斥了有效市场假说，支持混沌理论。 |
| [^23] | [Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model.](http://arxiv.org/abs/2306.12968) | 本论文提出了一种算法，名为实例自适应聚类（IAC），它能够在标记随机块模型（LSBM）中恢复隐藏的群集。IAC包括一次谱聚类和一个迭代的基于似然的簇分配改进，不需要任何模型参数，是高效的。 |
| [^24] | [Improved Financial Forecasting via Quantum Machine Learning.](http://arxiv.org/abs/2306.12965) | 本研究利用量子机器学习提升了金融预测的表现，包括使用行列式点过程来增强随机森林模型进行流失预测并设计了量子神经网络架构用于信用风险评估，比传统方法使用更少的参数达到相似的性能。 |
| [^25] | [Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning.](http://arxiv.org/abs/2306.12964) | 本文提出了一种新的Alpha生成框架，它通过强化学习挖掘协同的公式化Alpha集合以优化它们作为组合模型的性能。 |
| [^26] | [PyKoopman: A Python Package for Data-Driven Approximation of the Koopman Operator.](http://arxiv.org/abs/2306.12962) | PyKoopman是一个用于数据驱动的Koopman算子逼近的Python包，旨在实现对强非线性动态的预测、估计和控制。该包提供了数据驱动系统识别工具和不需要方程的动态模态分解（DMD）以及其变种。 |
| [^27] | [Siamese SIREN: Audio Compression with Implicit Neural Representations.](http://arxiv.org/abs/2306.12957) | 该研究利用Siamese SIREN 建立了一种新的方法，使用了较少的网络参数来实现卓越的音频重建保真度，拓展了隐式神经表示压缩的应用领域。 |
| [^28] | [Triggering Dark Showers with Conditional Dual Auto-Encoders.](http://arxiv.org/abs/2306.12955) | 该论文通过使用条件双自编码器，对探测器图像进行暗版强力信号新物理搜索，实现了对撞机新物理搜索的有效和通用工具，证明了AE具有出色的判别能力。 |
| [^29] | [Evolving Computation Graphs.](http://arxiv.org/abs/2306.12943) | 本论文提出了一种进化计算图算法（ECGs），用于提高针对异质性数据的图神经网络（GNN）性能，通过重连GNN的计算图增加连接同一类节点的边缘以提升性能。 |
| [^30] | [Robust Semantic Segmentation: Strong Adversarial Attacks and Fast Training of Robust Models.](http://arxiv.org/abs/2306.12941) | 本文提出了针对语义分割模型的解决方案，使得可以对其进行攻击并提供了更好的评估协议。同时，通过微调鲁棒的主干，可以有限的计算代价训练对抗性鲁棒的分割模型。 |
| [^31] | [Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing.](http://arxiv.org/abs/2306.12929) | 本文提出了一种称为“Helper-Head”的方法，可以通过教授注意头忽略输入和输出的某些部分来消除离群值，从而实现对transformer的可量化。实验结果表明，该方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。 |
| [^32] | [Decentralized Multi-Agent Reinforcement Learning with Global State Prediction.](http://arxiv.org/abs/2306.12926) | 本文研究了分散式多智能体强化学习中的一个关键挑战：如何在没有全局信息的情况下有效训练机器人。我们提出了一种基于状态预测的方法，在不需要显式通信的情况下使机器人能够协调行动，实现更快更好的学习效果和任务执行性能。 |
| [^33] | [An Interactive Interface for Novel Class Discovery in Tabular Data.](http://arxiv.org/abs/2306.12919) | 这篇论文提出了一种交互界面，可以帮助专业领域的专家快速运行最先进的表格数据新类别发现算法，生成易于解释的结果。 |
| [^34] | [Multi-Objective Hull Form Optimization with CAD Engine-based Deep Learning Physics for 3D Flow Prediction.](http://arxiv.org/abs/2306.12915) | 本研究提出了一个基于CAD引擎和深度学习物理的优化框架，可以自动端到端地评估设计迭代并进行多目标船型优化，其中蕴含着能够提供精确的三维流预测的DLP模型。 |
| [^35] | [Mitigating Discrimination in Insurance with Wasserstein Barycenters.](http://arxiv.org/abs/2306.12912) | 本研究针对保险业依赖个人敏感特征预测风险容易造成歧视的问题，提出使用Wasserstein重心缓解偏见的方法。 |
| [^36] | [In Situ Framework for Coupling Simulation and Machine Learning with Application to CFD.](http://arxiv.org/abs/2306.12900) | 本文提出了一种原位框架，在异构集群上启用训练和推理工作流程。采用SmartSim部署数据库，存储数据和ML模型，避免了文件系统问题。在Polaris超级计算机上展示了完美的扩展效率，并通过训练湍流流动模拟中的自动编码器，验证了框架开销的可忽略性。 |
| [^37] | [Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots.](http://arxiv.org/abs/2306.12898) | 该论文提出了一种基于机器学习的实时反馈控制InAs/GaAs量子点生长方法。 |
| [^38] | [FuXi: A cascade machine learning forecasting system for 15-day global weather forecast.](http://arxiv.org/abs/2306.12873) | 逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。 |
| [^39] | [Wind Noise Reduction with a Diffusion-based Stochastic Regeneration Model.](http://arxiv.org/abs/2306.12867) | 本文介绍了一种基于扩散的随机再生模型的风噪声降噪方法，该方法使得风噪声降噪效果优于其他基于神经网络的方法和纯预测和生成模型，在使用模拟和真实记录的风噪声数据集上进行了测试，并在真实记录的风噪声数据集上具有很好的泛化性能。 |
| [^40] | [Learning from Visual Observation via Offline Pretrained State-to-Go Transformer.](http://arxiv.org/abs/2306.12860) | 该论文提出了一个学习框架，其中使用离线预训练和内在奖励来解决视觉观察学习的挑战性问题，在Atari和Minecraft上取得了优异的实验结果。 |
| [^41] | [Reinforcement Federated Learning Method Based on Adaptive OPTICS Clustering.](http://arxiv.org/abs/2306.12859) | 本文提出了一种基于自适应OPTICS聚类的强化联邦学习方法，旨在缓解不同用户终端上的数据分布不同所带来的负面影响，并有效提高了联邦学习方法的性能。 |
| [^42] | [Efficient Partitioning Method of Large-Scale Public Safety Spatio-Temporal Data based on Information Loss Constraints.](http://arxiv.org/abs/2306.12857) | 本文提出了一种基于信息丢失约束的大规模公共安全时空数据高效划分方法(IFL-LSTP)，可以显著减小数据规模，同时保持模型的准确性，确保分布式存储的负载平衡，同时保持数据划分的时空接近性。 |
| [^43] | [MultiTASC: A Multi-Tenancy-Aware Scheduler for Cascaded DNN Inference at the Consumer Edge.](http://arxiv.org/abs/2306.12830) | 本文提出了 MultiTASC，一种多租户感知调度程序，它能够自适应地控制设备的转发决策函数，以优化系统吞吐量和提高延迟服务水平目标的满足率。 |
| [^44] | [StrainNet: Predicting crystal structure elastic properties using SE(3)-equivariant graph neural networks.](http://arxiv.org/abs/2306.12818) | 该论文介绍了一种使用SE(3)-等变图神经网络预测晶体结构弹性性能的方法，可以高效地产生准确的弹性模量，并且能够预测应变能密度和相关的弹性常数，具有一定的可拓展性。 |
| [^45] | [XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance.](http://arxiv.org/abs/2306.12816) | XAI-TRIS提供了用于测试机器学习解释性能的具有挑战性的非线性基准数据集，并揭示了现有XAI方法的局限性。 |
| [^46] | [Conditional Generators for Limit Order Book Environments: Explainability, Challenges, and Robustness.](http://arxiv.org/abs/2306.12806) | 本文研究了在订单簿模拟中使用条件生成模型的方法，并探索了其对输入特征的依赖性及其优点和缺点，提高了CGAN的逼真度和强健性。 |
| [^47] | [Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement.](http://arxiv.org/abs/2306.12803) | 本研究提出了对于具有局部可变测量尺度的随机变量的广义随机优势（GSD）顺序，并通过线性优化和不精确概率模型提出了正则化的统计检验，解决了在这些非标准空间中如何正确利用全部信息来进行比较的问题。 |
| [^48] | [Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery.](http://arxiv.org/abs/2306.12802) | 本研究在药物发现方面使用了多模态知识图谱表示学习，获得了最先进的结果。他们整合了来自不同来源的数据，并提供了基于这些数据的预训练模型。 |
| [^49] | [HypeRS: Building a Hypergraph-driven ensemble Recommender System.](http://arxiv.org/abs/2306.12800) | 本文提出了一个新的集成推荐系统，将不同模型的预测结果结合成一个超图排名框架，是第一个使用超图排名建模集成推荐系统的。超图可以建模高阶关系。 |
| [^50] | [A prior regularized full waveform inversion using generative diffusion models.](http://arxiv.org/abs/2306.12776) | 本文提出了一种基于生成扩散模型的先验正则化全波形反演方法，在保持速度模型维度的同时适应于地震观测，可以在只有少量观测数据的情况下实现高分辨率反演。 |
| [^51] | [Pure Exploration in Bandits with Linear Constraints.](http://arxiv.org/abs/2306.12774) | 本文提出了两种相对于线性约束下的多臂赌博问题都是渐进最优的算法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。 |
| [^52] | [Concept-aware clustering for decentralized deep learning under temporal shift.](http://arxiv.org/abs/2306.12768) | 本研究提出的算法可解决分布式深度学习中在时间偏移下的非独立同分布数据问题，并自动适应网络中不断演化的概念，相对以往方法更优。 |
| [^53] | [Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields.](http://arxiv.org/abs/2306.12760) | Blended-NeRF是一种鲁棒而灵活的编辑NeRF场景中感兴趣的区域的框架，在保持自然性与一致性的情况下，它可以将用户提供的文本提示或图像补丁的物体合成并混合到原始场景中。 |
| [^54] | [On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective.](http://arxiv.org/abs/2306.12756) | 本论文研究了生成式检索模型在超出分布（OOD）泛化方面的鲁棒性，定义了从三个方面衡量OOD鲁棒性，并分析了其与密集检索模型的比较。实验结果表明，生成式检索模型的OOD鲁棒性较弱，特别是在面向任务的超出分布场景中更为明显。针对造成鲁棒性较弱的原因，提出了潜在的解决方案。 |
| [^55] | [Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement Learning.](http://arxiv.org/abs/2306.12755) | 该论文提出了一种交叉领域离线RL模型，以应对线下数据效率问题，并通过支持约束目标解决了OOD状态操作和OOD转换动态的挑战。 |
| [^56] | [Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models.](http://arxiv.org/abs/2306.12747) | 本文研究了在超参数模型中解决随机梯度下降（SGD）和Adam的速度问题，提出了一种非单调线搜索方法，取得更快的收敛速度和泛化性能，并结合同步的Polyak初始化步伐实现。 |
| [^57] | [On Exploring Node-feature and Graph-structure Diversities for Node Drop Graph Pooling.](http://arxiv.org/abs/2306.12726) | 该论文提出了一种名为MID的新的得分方案，通过有效维持不同的节点特征和迫使模型注意到多样的图结构，提高了节点降采样图池化技术的图层次表示效果。 |
| [^58] | [Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies.](http://arxiv.org/abs/2306.12714) | 本文研究了利用自监督学习模型（SSL模型）进行歌唱声音理解任务的有效性，并展示了将自监督前端转移到目标任务可以取得更好性能的潜力。此外，SSL模型在所有任务中均优于常规监督学习模型。 |
| [^59] | [OptIForest: Optimal Isolation Forest for Anomaly Detection.](http://arxiv.org/abs/2306.12703) | 本论文针对隔离森林算法中分支因子的最优取值问题，基于隔离效率提出创新算法OptIForest，该算法结构简洁、检测性能优秀，可应用于各种异常检测场景。 |
| [^60] | [Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation.](http://arxiv.org/abs/2306.12700) | 本文提出一个新的神经网络增长方法，采用动态稳定化的参数化方案和学习率适应机制，节约计算预算的同时，在准确性上也取得了可比较或更好的表现。 |
| [^61] | [Slimmable Encoders for Flexible Split DNNs in Bandwidth and Resource Constrained IoT Systems.](http://arxiv.org/abs/2306.12691) | 本文提出了一种基于可精简编码器的分层DNN分割计算方法，在数量宏大的数据传输的情况下实现了数据量的降低，并以最小的开销和时间调整计算负载和传输数据大小。 |
| [^62] | [Towards quantum enhanced adversarial robustness in machine learning.](http://arxiv.org/abs/2306.12688) | 本文讨论了将量子计算与机器学习相结合的新领域，即量子对抗机器学习，它有望提供更好的准确性，更高的计算效率和更强的对抗攻击鲁棒性。但在实现稳健的实际QAML工具方面仍存在挑战。 |
| [^63] | [Explainable Representations for Relation Prediction in Knowledge Graphs.](http://arxiv.org/abs/2306.12687) | 提出一种新的可解释表示方法SEEK用于知识图谱关系预测，通过识别实体之间相关的共享语义方面生成一个多方面和可解释的表示，并在真实世界的蛋白质相互作用预测和基因-疾病关联性预测任务中进行了验证。 |
| [^64] | [Outlier-robust Estimation of a Sparse Linear Model Using Invexity.](http://arxiv.org/abs/2306.12678) | 本文提出了一种组合版本的离群值鲁棒拉索方法，能够有效识别干净的样本并估计具有正确支持的稀疏回归向量。采用了一种新颖的凸松弛，具有可证明的理论保证，并在实验证实了理论有效性。 |
| [^65] | [Identifying and Disentangling Spurious Features in Pretrained Image Representations.](http://arxiv.org/abs/2306.12673) | 本文探究了如何从预训练图像表示中识别虚假特征，并提出了一个有效的线性自编码器训练方法来分离核心特征、虚假特征和其他特征。进一步，本文提出的两种虚假特征去除方法显著提高了分类性能。 |
| [^66] | [Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models.](http://arxiv.org/abs/2306.12659) | 本研究提出了一种简单而有效的指令调整方法，将少量监督式金融情感分析数据转化为指令数据，并用此方法对通用的LLM进行微调，从而在金融情感分析方面取得了显着进展。 |
| [^67] | [Fitted Value Iteration Methods for Bicausal Optimal Transport.](http://arxiv.org/abs/2306.12658) | 本文提出了一种适用于双因果最优传输问题的拟合值迭代方法，能够在保证精度的同时具有良好的可扩展性，数值实验结果也证明了该方法的优越性。 |
| [^68] | [Learnability and Algorithm for Continual Learning.](http://arxiv.org/abs/2306.12646) | 本文证明了类增量学习是可学习的，并提出了一种新的算法，实验结果表明其有效性。 |
| [^69] | [On Addressing the Limitations of Graph Neural Networks.](http://arxiv.org/abs/2306.12640) | 本文讨论了图神经网络的两个挑战：过度平滑和异质性，提出了解决方案并展望了未来的研究方向。 |
| [^70] | [Targeted collapse regularized autoencoder for anomaly detection: black hole at the center.](http://arxiv.org/abs/2306.12627) | 本文提出一种在自编码器的损失函数中添加一个轻量级的约束项，用于解决传统自编码器在异常检测中的不足，并取得了良好的表现。 |
| [^71] | [Communication-Efficient Federated Learning through Importance Sampling.](http://arxiv.org/abs/2306.12625) | 本文提出了一种通过重要性抽样实现有效通信的联邦学习方法，大大降低了发送模型更新的高通信成本，利用服务器端客户端分布和附加信息的接近关系，只需要较少的通信量即可实现。 |
| [^72] | [Class-Incremental Learning based on Label Generation.](http://arxiv.org/abs/2306.12619) | 本文提出了一种基于标签生成方法的增量分类学习（CIL）方法（VAG），大幅减少了灾难性遗忘（CF），并更好地保留了预训练模型的可推广表示。 |
| [^73] | [RobustNeuralNetworks.jl: a Package for Machine Learning and Data-Driven Control with Certified Robustness.](http://arxiv.org/abs/2306.12612) | RobustNeuralNetworks.jl是一个用Julia编写的机器学习和数据驱动控制包，它通过自然满足用户定义的鲁棒性约束条件，实现了神经网络模型的构建。 |
| [^74] | [Constant Memory Attention Block.](http://arxiv.org/abs/2306.12599) | 本文提出了一种常数内存注意力块（CMAB），用于在常数内存中计算输出并执行更新，从而实现更高的内存效率。实验证明该方法具有与现有最先进技术相当的结果。 |
| [^75] | [Rapid building damage assessment workflow: An implementation for the 2023 Rolling Fork, Mississippi tornado event.](http://arxiv.org/abs/2306.12589) | 本文介绍了一种人在循环工作流，用于自然灾害后快速训练建筑损伤评估模型，并通过在2023年密西西比州滚动叉口龙卷风事件中的案例研究获得了较高的精度和召回率。 |
| [^76] | [Hierarchical Neural Simulation-Based Inference Over Event Ensembles.](http://arxiv.org/abs/2306.12584) | 本文介绍了一种基于层级神经模拟的方法，可以在似然函数不可计算但可以通过前向模拟实现的情况下，对整个数据集进行最优概率推断，着重考虑了模型的层级结构，可以导致更紧凑的参数约束。 |
| [^77] | [Adversarial Training with Generated Data in High-Dimensional Regression: An Asymptotic Study.](http://arxiv.org/abs/2306.12582) | 该论文研究了在高维回归中将生成数据与对抗训练相结合的方法，发现该方法可通过两阶段训练实现更好的性能表现。 |
| [^78] | [An efficient and straightforward online quantization method for a data stream through remove-birth updating.](http://arxiv.org/abs/2306.12574) | 本文提出了一种在线量化数据流的方法，通过移除生胎更新快速适应概念漂移，可以产生最小化的死单元，并为漂移检测提供了一些有用的度量指标。 |
| [^79] | [Improving Long-Horizon Imitation Through Instruction Prediction.](http://arxiv.org/abs/2306.12554) | 本文探讨了基于语言的指令预测损失的辅助监督方式，展示了在演示数量受限的情况下，指令建模在复杂推理任务中提高了表现。 |
| [^80] | [Finite-time Lyapunov exponents of deep neural networks.](http://arxiv.org/abs/2306.12548) | 本文研究了深度神经网络的有限时间李雅普诺夫指数，发现正指数的脊线将输入空间分成不同区域，并揭示了深度网络学习能力的机制。 |
| [^81] | [Neural Multigrid Memory For Computational Fluid Dynamics.](http://arxiv.org/abs/2306.12545) | 本文提出了一种新的数据驱动湍流流动模拟方法MGxTransformer，结合了VPTR和多重网格架构的优点，使得模拟结果更为准确和高效。 |
| [^82] | [Memory-Query Tradeoffs for Randomized Convex Optimization.](http://arxiv.org/abs/2306.12534) | 随机凸优化需要在内存和查询之间权衡，使用 $\tilde{O}(d^2)$ 比特内存和 $\tilde{O}(d)$ 次查询的割平面方法是最优的。 |
| [^83] | [Semi-Implicit Denoising Diffusion Models (SIDDMs).](http://arxiv.org/abs/2306.12511) | SIDDMs是一种新方法，通过匹配隐式和显式因子，实现在生成模型中快速收敛且一定程度上保证样本多样性和质量。 |
| [^84] | [Comparative Analysis of Segment Anything Model and U-Net for Breast Tumor Detection in Ultrasound and Mammography Images.](http://arxiv.org/abs/2306.12510) | 研究采用U-Net和pretrained SAM两种深度学习架构，针对乳腺超声和乳腺X线图像，进行肿瘤区域的识别和分割。结果表明，U-Net模型对于不同类型的良性和恶性肿瘤的识别和分割效果更好。 |
| [^85] | [Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference.](http://arxiv.org/abs/2306.12509) | 本文提出了一种称为深度语言网络（DLN）的架构，通过联合训练叠加的语言模型层（LLMs），使用变分推断算法进行提示训练，使得DLN-2的性能甚至可以与少量训练数据的GPT-4相媲美。 |
| [^86] | [Investigating Poor Performance Regions of Black Boxes: LIME-based Exploration in Sepsis Detection.](http://arxiv.org/abs/2306.12507) | 本文提出了一种利用LIME方法探索黑盒分类模型不佳性能区域的方法，并应用于高风险的败血症检测中。通过分析错误分类实例，确定造成模型性能差的重要特征，并识别出分类器性能不佳的区域，并计算出其错误率，这对于实现谨慎的决策具有重要意义，同时增强了机器学习模型在临床实践中的可解释性，降低关键应用场景下的风险。 |
| [^87] | [Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective and Improved Bounds.](http://arxiv.org/abs/2306.12498) | 本文提出了带有Shuffled SGD的经验风险最小化的原始-对偶视角和改进界限，旨在解决理论和实践之间的差距。 |
| [^88] | [Precision psychiatry: predicting predictability.](http://arxiv.org/abs/2306.12462) | 本文讨论了精准精神医学领域的十个挑战，包括对真实世界人群进行研究、对临床结果定义的现实考虑、考虑治疗相关因素以及公平性等，提出了将重点从回顾研究转移到远景性实施的观点。 |
| [^89] | [Deep Dynamic Epidemiological Modelling for COVID-19 Forecasting in Multi-level Districts.](http://arxiv.org/abs/2306.12457) | 该文提出一种深度动态流行病学的模型，将流行病学方程和深度学习相结合，以实现对COVID-19在多级区域的准确预测，同时保证模型可视化机制 |
| [^90] | [Learning Conditional Instrumental Variable Representation for Causal Effect Estimation.](http://arxiv.org/abs/2306.12453) | 本文提出了一种名为 DVAE.CIV 的方法，通过分离表示学习，从带有潜在混淆因素的数据中学习和分解条件 IV 和其条件集的表示，用于因果效应估计。 |
| [^91] | [Comparing deep learning models for volatility prediction using multivariate data.](http://arxiv.org/abs/2306.12446) | 本研究比较了使用不同深度学习模型预测多项资产波动率的效果，发现时间融合变压器及时间卷积神经网络的变体最优，可以用于实践。 |
| [^92] | [Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials.](http://arxiv.org/abs/2306.12444) | 本文研究了自动化说话人验证技术在阿尔茨海默症临床试验中应用的可行性，人口统计特征、音频质量标准和AD的严重程度对ASV性能会产生影响。 |
| [^93] | [DEPAC: a Corpus for Depression and Anxiety Detection from Speech.](http://arxiv.org/abs/2306.12443) | 本文介绍了一份新颖的语音数据集DEPAC，该数据集标记了抑郁症和焦虑症标准筛查工具上的门槛。此外，作者还提出了一组手工筛选的声学和语言特征，可以有效地识别人类语音中的精神疾病迹象。该研究为自动诊断系统的开发提供了信息丰富且平衡的语料库。 |
| [^94] | [Knowledge Distillation via Token-level Relationship Graph.](http://arxiv.org/abs/2306.12442) | 本文提出了一种新方法，基于词级别关系图(TRG)，用于提高知识蒸馏的性能。通过利用TRG，学生模型可以模拟教师模型中更高级别的语义信息。同时，还引入了一种上下文损失以进一步增强学习过程。 |
| [^95] | [Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback.](http://arxiv.org/abs/2306.12438) | 本文提出了一个为了生成临床合理的医学图像而引入病理学家反馈的方法。 |
| [^96] | [MPSTAN: Metapopulation-based Spatio-Temporal Attention Network for Epidemic Forecasting.](http://arxiv.org/abs/2306.12436) | 提出了一种名为MPSTAN的混合模型，通过将多斑点流行病学知识融入时空模型并自适应地定义斑点间相互作用，提高流行病预测的准确性，并且使用元群体理论将斑点间相互作用纳入模型，为构建多斑点知识提供了可行的方法。该模型在真实世界数据集上表现最先进。 |
| [^97] | [Modeling T1 Resting-State MRI Variants Using Convolutional Neural Networks in Diagnosis of OCD.](http://arxiv.org/abs/2306.12435) | 本研究利用计算建模方法开发了一个卷积神经网络模型，通过T1静息态磁共振成像(TRS-MRI)扫描识别生物标志物，有效地区分出患有OCD和其他精神障碍的患者，准确率超过90%。 |
| [^98] | [Interpretation of immunofluorescence slides by deep learning techniques: anti-nuclear antibodies case study.](http://arxiv.org/abs/2306.12432) | 本文介绍了一种利用深度学习技术检测免疫荧光幻灯片的方法，旨在提供高效的工具帮助医生早期检测异常，解决免疫疾病方面的医疗难题。 |
| [^99] | [Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms.](http://arxiv.org/abs/2306.12383) | 本文针对随机零阶优化的问题，提出了二次型目标函数及其局部几何结构的最优Hessian相关样本复杂度，从信息论角度提供Hessian相关复杂度的下界，并提供了一种Hessian无关的算法可普遍实现所有Hessian实例的渐近最优样本复杂度。 |
| [^100] | [Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift.](http://arxiv.org/abs/2306.12306) | 该论文在多个具有挑战性的分类和回归任务上对现代BDL算法进行了系统性评估，重点关注了在分布偏移下的泛化能力和校准能力，并研究了一种带符号的期望校准误差版本。 |
| [^101] | [Corrector Operator to Enhance Accuracy and Reliability of Neural Operator Surrogates of Nonlinear Variational Boundary-Value Problems.](http://arxiv.org/abs/2306.12047) | 本文提供了一种基于Corrector操作符的框架，以增强神经算子代理非线性变分边界值问题的准确度和可靠性。使用该方案对于PCANet型神经算子的二维非线性扩散模型的数值实验结果显示，逼近的准确度近乎提高了两个数量级，并且还在涉及非线性d之上的拓扑优化问题中得到了探讨。 |
| [^102] | [AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization.](http://arxiv.org/abs/2306.11971) | AdCraft是一种高级强化学习基准环境，用于模拟出价和预算变化的搜索引擎营销(SEM)活动，可用于评估和提高SEM出价和预算管理相关的RL算法的鲁棒性。 |
| [^103] | [A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design.](http://arxiv.org/abs/2306.11768) | 本文在系统回顾几何深度学习在结构药物设计中的最新进展，分别讨论了不同任务并按不同的几何深度学习方法进行组织。该领域的前景看好，但仍存在挑战。 |
| [^104] | [FDINet: Protecting against DNN Model Extraction via Feature Distortion Index.](http://arxiv.org/abs/2306.11338) | FDINet是一种新颖的防御机制，该机制利用特征失真指数来保护DNN模型免受模型提取攻击，并利用FDI相似性来识别分布式提取攻击中的勾结敌人。 |
| [^105] | [Quilt-1M: One Million Image-Text Pairs for Histopathology.](http://arxiv.org/abs/2306.11207) | 本文介绍了一个名为 Quilt-1M 的癌症组织学图像和文字对的百万数据集，并利用 YouTube 上的专家医生教程视频为主要来源。这个数据集将使得癌症组织学领域的表征学习取得类似于其他领域的进展。 |
| [^106] | [Enhancing variational quantum state diagonalization using reinforcement learning techniques.](http://arxiv.org/abs/2306.11086) | 本研究采用强化学习技术，通过新的编码方法来优化量子状态对角化所需的电路深度，从而提高其在近期量子硬件上的应用性能。 |
| [^107] | [Text-Driven Foley Sound Generation With Latent Diffusion Model.](http://arxiv.org/abs/2306.10359) | 本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。我们通过迁移学习对系统进行微调，并引入可训练的层来改善文本嵌入，同时也改进了生成的波形。 |
| [^108] | [Federated Few-shot Learning.](http://arxiv.org/abs/2306.10234) | 本研究提出了一种名为“联邦少样本学习”的新问题，旨在解决联邦学习在少样本数据上的性能问题。我们提出了一个简单而有效的框架，使用特征提取和任务适应模块以及注意力机制来提高模型对于少样本客户端的泛化能力，在各种数据集上取得了最先进的联邦少样本学习性能。 |
| [^109] | [The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement.](http://arxiv.org/abs/2306.09633) | 谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。 |
| [^110] | [Explore, Establish, Exploit: Red Teaming Language Models from Scratch.](http://arxiv.org/abs/2306.09442) | 本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。 |
| [^111] | [RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows.](http://arxiv.org/abs/2306.06034) | 本研究提出了RANS-PINN模型，通过引入2方程涡粘度模型，可预测高雷诺数湍流流动中的流场，从而提高流体动力学模拟计算效率。 |
| [^112] | [Self-Interpretable Time Series Prediction with Counterfactual Explanations.](http://arxiv.org/abs/2306.06024) | 本文提出了一种自我解释的时间序列预测模型CounTS，该模型可以生成反事实和可操作的解释，适用于关键领域如医疗和自动驾驶等。与现有方法不同，该模型为可解释性建模做出了贡献。 |
| [^113] | [Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition.](http://arxiv.org/abs/2306.05021) | 本文提出了一种基于层特定张量分解的神经网络加速器Mixed-TD，采用混合方式的SVD和CPD方法，实现了高压缩率，同时保持了与原始神经网络类似的准确性，并通过动态映射方法实现了对可用片上存储器和计算资源的有效利用。 |
| [^114] | [Complex Preferences for Different Convergent Priors in Discrete Graph Diffusion.](http://arxiv.org/abs/2306.02957) | 本研究探讨了离散扩散核如何影响图的扩散模型的性能，结果表明选择正确的收敛先验对于扩散模型的生成性能至关重要。 |
| [^115] | [Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification.](http://arxiv.org/abs/2306.01726) | 本文提出了两种代数评估器来估计未标记数据中有噪声二元分类器的性能。其中，第二种评估器的正确性被保证。作者通过利用独立评估器无法返回合理估计的失败，缓解了委托/代理监控悖论，并通过搜索来寻找几乎无误差的三元组。 |
| [^116] | [Balanced Training of Energy-Based Models with Adaptive Flow Sampling.](http://arxiv.org/abs/2306.00684) | 本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。 |
| [^117] | [Multi-BVOC Super-Resolution Exploiting Compounds Inter-Connection.](http://arxiv.org/abs/2305.14180) | 本文提出了一种利用多种化合物贡献对粗糙BVOC排放地图进行超分辨的策略，实验结果表明该方法可以提高超分辨率性能。 |
| [^118] | [ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings.](http://arxiv.org/abs/2305.11554) | 本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。 |
| [^119] | [CosmoPower-JAX: high-dimensional Bayesian inference with differentiable cosmological emulators.](http://arxiv.org/abs/2305.06347) | CosmoPower-JAX使用可微的宇宙模拟器进行高维贝叶斯推断，其采用JAX的特性以及GPU技术加速参数估计，可以有效地探索高维参数空间并在短时间内获得准确的参数和后验分布。 |
| [^120] | [Investigating the effect of sub-word segmentation on the performance of transformer language models.](http://arxiv.org/abs/2305.05480) | 本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。 |
| [^121] | [Classification Tree Pruning Under Covariate Shift.](http://arxiv.org/abs/2305.04335) | 本文提出了一种基于协变量转移的分类树剪枝方法，可以访问大部分来自分布 $P_{X，Y}$ 的数据，但是只能获得来自拥有不同 $X$-边缘的目标分布 $Q_{X，Y}$ 的少量数据。使用的优化标准是一个关于分布 $P_{X} \to Q_{X}$ 的 \emph{平均差异}，该标准可以显著放宽最近提出的 \emph{转移指数}，最终可以得到最优的剪枝结果。 |
| [^122] | [Reinforcement learning for optimization of energy trading strategy.](http://arxiv.org/abs/2303.16266) | 本文使用强化学习算法优化了一种黑盒交易策略，该策略通过在马尔可夫决策过程中使用真实数据进行优化，在 DA 能源市场上由中型生产者自动进行交易。 |
| [^123] | [TSMixer: An all-MLP Architecture for Time Series Forecasting.](http://arxiv.org/abs/2303.06053) | TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。 |
| [^124] | [Visual Abstraction and Reasoning through Language.](http://arxiv.org/abs/2303.04091) | 本论文提出了一种通过自然语言描述任务的通用框架来解决Abstraction and Reasoning Corpus（ARC）问题，虽然还没有在ARC上击败最先进的DSL模型，但我们展示了我们的方法具有巨大的潜力，可以解决先前未解决的任务。 |
| [^125] | [Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces.](http://arxiv.org/abs/2303.00028) | 本文提出了一种用稀疏高斯过程解决连续空间下传感器放置问题的方法，避免离散化环境并降低了计算复杂度，可通过贪婪算法找到好的解决方案。 |
| [^126] | [IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research.](http://arxiv.org/abs/2302.13522) | IGB是一个研究数据集工具，包含同质和异质性学术图形，规模巨大，并提供工具用于生成不同特性的合成图形，为GNN研究人员提供解决公共图形数据集在标记、特征、异质性和大小方面差距的有价值资源。 |
| [^127] | [Sharp analysis of EM for learning mixtures of pairwise differences.](http://arxiv.org/abs/2302.10066) | 该论文研究了使用成对比较设计的随机样本的线性回归的对称混合，通过分析EM算法的序列收敛性和极限值，得出了$\ell_\infty$范数和$\ell_2$范数中的估计尖锐度。研究表明EM算法可以展现出多个独特的行为。 |
| [^128] | [A One-Sample Decentralized Proximal Algorithm for Non-Convex Stochastic Composite Optimization.](http://arxiv.org/abs/2302.09766) | 本文提出了两种单时间规模的算法：Prox-DASA和Prox-DASA-GT，它们可以用常量批量大小找到复合目标函数的$\epsilon$-静止点，并且不需要大批量大小、更复杂的操作或更强的假设。 |
| [^129] | [Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation.](http://arxiv.org/abs/2302.03673) | 本研究提出了一种新的独立线性Markov博弈模型，针对多智体强化学习中的大状态空间和大量代理问题，设计了新算法以学习Markov粗略相关均衡和Markov相关均衡。相比于现有的Markov博弈函数逼近技术，我们的方法能够大大降低样本复杂度并取得更高的精度。 |
| [^130] | [Prior Density Learning in Variational Bayesian Phylogenetic Parameters Inference.](http://arxiv.org/abs/2302.02522) | 本文提出了一种使用学习参数的灵活先验方法，通过多个马尔可夫链替代模型的模拟得出，该方法在估计系统发育参数方面非常有效。 |
| [^131] | [The Power of Linear Combinations: Learning with Random Convolutions.](http://arxiv.org/abs/2301.11360) | 本研究质疑了卷积神经网络中学习到的卷积核的重要性，提出了简单的线性组合方法，从随机卷积核中创建出表达能力强的网络运算符，通过隐含的正则化技术，可以提高整体性能。 |
| [^132] | [A Semi-supervised Sensing Rate Learning based CMAB Scheme to Combat COVID-19 by Trustful Data Collection in the Crowd.](http://arxiv.org/abs/2301.08563) | 本文提出了一种基于半监督学习的组合多臂赌博反向拍卖方案，用于解决移动众包系统中在招募多个未知和有策略的工作者时出现的数据可信问题。 |
| [^133] | [The quantum cost function concentration dependency on the parametrization expressivity.](http://arxiv.org/abs/2301.06883) | 本文分析了参数表达能力对代价函数的影响，证明了参数表达能力越强，代价函数越趋于集中在一个取决于选择的可观察量和使用的量子比特数的值上。 |
| [^134] | [A Survey of Deep Learning for Mathematical Reasoning.](http://arxiv.org/abs/2212.10535) | 本文综述了过去十年中深度学习在数学推理领域的关键任务、数据集和方法，并评估了现有的基准和方法，探讨了未来的研究方向。 |
| [^135] | [\{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning.](http://arxiv.org/abs/2212.01793) | 本文提出了一种新的\{kappa}HGCN模型，在双曲空间内实现树状结构建模，通过结合连续和离散曲率来学习输入图的基础几何结构，并在多个基准测试和数据集上取得了最先进的性能。 |
| [^136] | [Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning.](http://arxiv.org/abs/2211.15359) | 本论文的主要贡献在于提出了一种新的方法，通过将社交和任务相关特征考虑在对话中，来优化主动对话策略，使其在任务效率高的同时，也能促进用户信任，从而提高人机交互的成功率和效率。 |
| [^137] | [PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design.](http://arxiv.org/abs/2211.12020) | 提出了PhAST方法来快速发现更有效的催化剂来驱动电化学反应, 该方法适用于大多数体系结构, 可以增加计算效率和精度 |
| [^138] | [scikit-fda: A Python Package for Functional Data Analysis.](http://arxiv.org/abs/2211.02566) | scikit-fda是一个用于函数数据分析的Python包，提供了全面的工具，并于scikit-learn兼容，采用三条款BSD许可证发布，对FDA社区贡献开放。 |
| [^139] | [Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories.](http://arxiv.org/abs/2210.06518) | 该论文提出了一种半监督离线强化学习的新设置，利用有标记的轨迹数据和无动作的轨迹数据训练反动力学模型以获取代理标签，最终使用任何离线强化学习算法以实现高成功率的表现。 |
| [^140] | [Efficient Learning of Locomotion Skills through the Discovery of Diverse Environmental Trajectory Generator Priors.](http://arxiv.org/abs/2210.04819) | 本论文提出了一种名为EETG的方法，使用“质量-多样性”算法学习一组多样化的专业化运动先验知识，能够帮助四足机器人成功地穿越各种环境，比使用单个轨迹生成器的方法更加有效。 |
| [^141] | [Algorithmic decision making methods for fair credit scoring.](http://arxiv.org/abs/2209.07912) | 本论文研究了12种偏差缓解方法在5个不同的公平性指标下的有效性，评估了它们对金融机构准确性和潜在盈利能力的影响，并指出了最成功和最不成功缓解方法。 |
| [^142] | [Taming Multi-Agent Reinforcement Learning with Estimator Variance Reduction.](http://arxiv.org/abs/2209.01054) | 本文提出了一种名为“PERLA”的增强工具，它通过将智能体的联合策略采样技术引入评论家中，减少了联合动作单一样本造成的梯度估计方差，使得CT-DE MARL算法的学习更加稳定且更快速。 |
| [^143] | [Finding neural signatures for obesity through feature selection on source-localized EEG.](http://arxiv.org/abs/2208.14007) | 这项研究使用脑电图数据开发了一个新的机器学习模型，成功识别出肥胖女性的脑网络，表明肥胖大脑的特征是具有功能障碍的网络。 |
| [^144] | [SNAP: Efficient Extraction of Private Properties with Poisoning.](http://arxiv.org/abs/2208.12348) | SNAP 是一种高效的带毒素特征提取攻击方法，相比同类方法需要更少的毒素攻击，并获得更高的攻击成功率。 |
| [^145] | [Frouros: A Python library for drift detection in machine learning systems.](http://arxiv.org/abs/2208.06868) | Frouros是一个开源的Python库，可以检测任何机器学习框架中的概念和数据漂移，易于维护和扩展。 |
| [^146] | [Towards Antisymmetric Neural Ansatz Separation.](http://arxiv.org/abs/2208.03264) | 本文研究了反对称函数模型的分离，提出了一个既可通过Jastrow形式有效表示的反对称函数，却无法用Slater行列式进行逼近的结论。 |
| [^147] | [VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations.](http://arxiv.org/abs/2207.00221) | 本研究提出VL-CheckList，使用物体、属性和关系评估预训练的视觉语言模型，通过对七种流行的VLP模型进行全面研究分析，揭示出不同模型之间的细微差异。 |
| [^148] | [Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities.](http://arxiv.org/abs/2204.07321) | 本文提供了有关图神经网络中图池化方法的广泛回顾和分类，以及与之相关的库的概述，展示了这个充满希望和快速发展的研究领域的现有挑战和未来方向。 |
| [^149] | [Learning topological defects formation with neural networks in a quantum phase transition.](http://arxiv.org/abs/2204.06769) | 本文利用神经网络和机器学习算法研究了一维横场量子伊辛模型中的拓扑缺陷，发现激发能与缺陷数量成比例关系，并建立了缺陷数量的前三个累积量之间的普适幂律关系。 |
| [^150] | [Scrutinizing XAI using linear ground-truth data with suppressor variables.](http://arxiv.org/abs/2111.07473) | 研究提出了一个关于可解释人工智能(XAI)的方法，该方法提出了特征重要性的客观初步定义，以避免由于方法的行为引起的错误解读。 |
| [^151] | [Distributed Sparse Regression via Penalization.](http://arxiv.org/abs/2111.06530) | 本文研究了基于惩罚项的分布式稀疏线性回归问题，通过建立统计一致性证明了最优解在$\ell_2$-损失中以接近最优最小率获得。 |
| [^152] | [CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations.](http://arxiv.org/abs/2109.07557) | 本文提出了一个新的端到端学习框架CounterNet，将机器学习模型训练和相应的因果解释生成结合在一起。CounterNet通过一起训练ML模型和生成CF解释来提供预测感知的CF解释，并减少计算开销。 |
| [^153] | [Controlling Privacy Loss in Sampling Schemes: an Analysis of Stratified and Cluster Sampling.](http://arxiv.org/abs/2007.12674) | 本文扩展了隐私扩大结果的研究，对更为复杂、依赖于数据的抽样方案进行了分析，发现这些方案可能导致隐私损害；文章分析了聚类抽样和分层抽样范例的隐私影响。 |

# 详细

[^1]: 从新的角度压缩ImageNet规模数据集：SRe$^2$L

    Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])

    [http://arxiv.org/abs/2306.13092](http://arxiv.org/abs/2306.13092)

    SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。

    

    我们提出了一个新的数据集压缩框架，称为Squeeze、Recover和Relabel（SRe$^2$L），它在训练期间分离了模型和合成数据的双层优化，以处理不同规模的数据集、模型体系结构和图像分辨率，从而实现有效的数据集压缩。所提出的方法展示了在不同数据集规模上的灵活性，并在合成图像任意分辨率、高分辨率训练的情况下具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力。我们在Tiny-ImageNet和完整的ImageNet-1K数据集上进行了广泛的实验。在50IPC下，我们的方法在Tiny-ImageNet和ImageNet-1K上分别实现了42.5％和60.8％的最高验证精度，较之前所有最先进方法提高了14.5％和32.9％。我们的方法在Res上也比MTT快约52倍(ConvNet-4)和16倍。

    We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
    
[^2]: 带属性条件的对抗人脸生成器能够逃避取证分类器

    Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces. (arXiv:2306.13091v1 [cs.CV])

    [http://arxiv.org/abs/2306.13091](http://arxiv.org/abs/2306.13091)

    该论文提出了一种基于属性条件的GAN方法，可以生成具有指定属性的对抗性伪造人脸图像，这些图像可以规避取证分类器的检测，同时保留所需的属性。

    

    生成模型生成高度逼真的合成人脸图像的能力引起了安全和伦理问题。为了防止这种假面孔的第一道防线，基于深度学习的取证分类器已经被开发出来。虽然这些取证模型可以高精度地检测出面部图像是否为合成的或真实的，但它们也容易受到对抗攻击。虽然这样的攻击在逃避鉴定分类器方面非常成功，但它们会引入可通过仔细的人类检查进行检测的可见噪声模式。此外，这些攻击假设有访问目标模型的权限，这并不总是正确的。已经尝试直接扰动GAN的潜在空间以产生对抗性的伪造面孔，可以规避取证分类器。在这项研究中，我们更进一步，展示了可以成功生成带有指定属性（例如，头发颜色、眼睛大小、种族、性别）的对抗人脸，它们可以规避取证分类器，同时保持所需的属性。我们提出了一种基于属性条件的GAN方法，其中GAN生成器是以所需伪造面孔的属性作为条件，生成逃避取证分类器的对抗性面孔图像，并紧密匹配所需的属性。

    The ability of generative models to produce highly realistic synthetic face images has raised security and ethical concerns. As a first line of defense against such fake faces, deep learning based forensic classifiers have been developed. While these forensic models can detect whether a face image is synthetic or real with high accuracy, they are also vulnerable to adversarial attacks. Although such attacks can be highly successful in evading detection by forensic classifiers, they introduce visible noise patterns that are detectable through careful human scrutiny. Additionally, these attacks assume access to the target model(s) which may not always be true. Attempts have been made to directly perturb the latent space of GANs to produce adversarial fake faces that can circumvent forensic classifiers. In this work, we go one step further and show that it is possible to successfully generate adversarial fake faces with a specified set of attributes (e.g., hair color, eye size, race, gend
    
[^3]: GIMLET：一种用于基于指令分子零样本学习的统一图文模型

    GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])

    [http://arxiv.org/abs/2306.13089](http://arxiv.org/abs/2306.13089)

    本研究提出了一种名为GIMLET的统一图文模型，用于在零样本设置下使用自然语言指令完成分子相关任务。我们解决了现有模型的指令处理不足和图形容量有限的问题，并证明了使用GIMLET能够增强图形特征的泛化能力。

    

    分子属性预测近年来受到了广泛关注，但由于昂贵的实验造成的标签不足问题将是其主要瓶颈。为了缓解这个问题并更好地利用文本知识进行任务，本研究探讨了在零样本设置下使用自然语言指令完成分子相关任务的可行性。我们发现现有的分子-文本模型在这种情况下表现不佳，原因是处理指令不足以及图形容量有限。为了克服这些问题，我们提出了GIMLET，它统一了图形和文本数据的语言模型。通过采用广义位置嵌入，我们的模型被扩展以编码图形结构和指令文本，而无需额外的图形编码模块。GIMLET还在注意机制中解耦了图形的编码和任务指令，增强了跨新任务的图形特征的泛化能力。我们构建了一个数据集...

    Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset 
    
[^4]: 通过轨迹加权利用混合的离线强化学习数据集

    Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting. (arXiv:2306.13085v1 [cs.LG])

    [http://arxiv.org/abs/2306.13085](http://arxiv.org/abs/2306.13085)

    该论文提出了一种混合离线强化学习数据集的改进方法，可以通过重新加权采样数据集来充分利用高表现的轨迹，以提高目标策略的表现。

    

    多数离线强化学习算法返回一个最大化预期表现与诱导状态-动作占用的分布差异所带来的风险之间的权衡的目标策略，因此，目标策略的表现与数据集收集的行为策略的表现密切相关。我们发现，在由大多数低回报轨迹和少数高回报轨迹组成的混合数据集中，现有的离线强化学习算法受到低回报轨迹的过度制约，未能充分利用高表现轨迹。为了解决这个问题，我们表明，在带有随机初始状态的确定性MDPs中，可以通过重新加权采样数据集来诱导具有更高回报的行为策略的人工数据集。这种重新加权的采样策略可以与任何算法结合使用。

    Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any
    
[^5]: 多模态情感识别中基于时间的模型比较

    A Comparison of Time-based Models for Multimodal Emotion Recognition. (arXiv:2306.13076v1 [cs.LG])

    [http://arxiv.org/abs/2306.13076](http://arxiv.org/abs/2306.13076)

    本研究比较了不同序列模型在多模态情感识别中的表现，发现将声音数据转录成文本并与图像数据一起分析可以提高情感识别的准确性。

    

    情感识别已成为人机交互领域中的重要研究课题。关于声音和视频以了解情绪的研究主要集中在分析面部表情，并将其分类为6种基本情绪。本文比较了不同序列模型在多模态情感识别中的表现。首先，使用多层CNN模型对声音和图像进行处理，然后将这些模型的输出馈送到各种序列模型中。序列模型包括GRU、Transformer、LSTM和Max Pooling。计算了所有模型的准确度、精确度和F1 Score值。实验中使用了多模态CREMA-D数据集。比较CREMA-D数据集的结果表明，基于GRU的架构在F1分数方面表现最佳，得分为0.640，基于LSTM的架构在精度指标方面表现最佳，得分为0.699，而基于Max Pooling的架构随着时间的推移显示出最佳的敏感度，得分为0.620。因此，观察到将声音数据转录成文本并与图像数据一起分析可以提高情感识别的准确性。

    Emotion recognition has become an important research topic in the field of human-computer interaction. Studies on sound and videos to understand emotions focused mainly on analyzing facial expressions and classified 6 basic emotions. In this study, the performance of different sequence models in multi-modal emotion recognition was compared. The sound and images were first processed by multi-layered CNN models, and the outputs of these models were fed into various sequence models. The sequence model is GRU, Transformer, LSTM and Max Pooling. Accuracy, precision, and F1 Score values of all models were calculated. The multi-modal CREMA-D dataset was used in the experiments. As a result of the comparison of the CREMA-D dataset, GRU-based architecture with 0.640 showed the best result in F1 score, LSTM-based architecture with 0.699 in precision metric, while sensitivity showed the best results over time with Max Pooling-based architecture with 0.620. As a result, it has been observed that t
    
[^6]: 审计预测模型中的交叉偏见

    Auditing Predictive Models for Intersectional Biases. (arXiv:2306.13064v1 [cs.LG])

    [http://arxiv.org/abs/2306.13064](http://arxiv.org/abs/2306.13064)

    本研究提出了一种灵活的审计框架——条件偏差扫描（CBS），用于检测分类模型中的交叉偏见。与审计子组公平性的类似方法相比，CBS能够检测到更多未曾发现的交叉和情境偏见。

    

    满足受保护类别成员群体公平准则的预测模型，但不保证子组公平，可能会为两个或多个保护类别的个体产生偏差的预测。为了解决这个风险，我们提出了一种灵活的审计框架——条件偏差扫描（CBS），用于检测分类模型中的交叉偏见。CBS识别了对受保护类别最有偏差的子组，相比于非受保护类别中的等价子组，以及可以包括多种通常用于概率和二元预测的公平定义。我们展示了这种方法可以检测到COMPAS预审风险评估工具中先前未识别的交叉和情境偏见，并且与审计子组公平性的类似方法相比，具有更高的偏见检测能力。

    Predictive models that satisfy group fairness criteria in aggregate for members of a protected class, but do not guarantee subgroup fairness, could produce biased predictions for individuals at the intersection of two or more protected classes. To address this risk, we propose Conditional Bias Scan (CBS), a flexible auditing framework for detecting intersectional biases in classification models. CBS identifies the subgroup for which there is the most significant bias against the protected class, as compared to the equivalent subgroup in the non-protected class, and can incorporate multiple commonly used fairness definitions for both probabilistic and binarized predictions. We show that this methodology can detect previously unidentified intersectional and contextual biases in the COMPAS pre-trial risk assessment tool and has higher bias detection power compared to similar methods that audit for subgroup fairness.
    
[^7]: 用于学习有界协方差高斯混合模型的SQ下限

    SQ Lower Bounds for Learning Bounded Covariance GMMs. (arXiv:2306.13057v1 [cs.LG])

    [http://arxiv.org/abs/2306.13057](http://arxiv.org/abs/2306.13057)

    本文研究了在有界协方差情况下学习分离高斯混合模型问题的复杂性，并证明了使用SQ算法的复杂度下限为$d^{\Omega(1/\epsilon)}$。

    

    本文研究了具有相同未知有界协方差矩阵的分离高斯混合模型的复杂性。 具体来说，我们关注形式为$P = \sum_{i=1}^k w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$的$\mathbb{R}^d$上的高斯混合模型（GMMs），其中$\mathbf \Sigma_i = \mathbf \Sigma \preceq \mathbf I$且$\min_{i \neq j} \| \boldsymbol \mu_i \boldsymbol \mu_j\|_2 \geq k^\epsilon$对于某些$\epsilon> 0$。已知的学习算法的复杂度为$(dk)^{O(1/\epsilon)}$。在本文中，我们证明了任何用于此问题的统计查询（SQ）算法的复杂度至少需要$d^{\Omega(1/\epsilon)}$。当分离在$k^{1/2}$数量级上时，我们另外获得了具有正确指数的细粒度SQ下限。我们的SQ下限意味着低次多项式测试的类似下限。从概念上讲，我们的结果表明已知算法几乎是最优的。

    We study the complexity of learning mixtures of separated Gaussians with common unknown bounded covariance matrix. Specifically, we focus on learning Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$, where $\mathbf \Sigma_i = \mathbf \Sigma \preceq \mathbf I$ and $\min_{i \neq j} \| \boldsymbol \mu_i \boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon>0$. Known learning algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In this work, we prove that any Statistical Query (SQ) algorithm for this problem requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case where the separation is on the order of $k^{1/2}$, we additionally obtain fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds imply similar lower bounds for low-degree polynomial tests. Conceptually, our results provide evidence that known algorithms for this problem are nearly be
    
[^8]: 量子河豚隐私：一种灵活的量子系统隐私框架

    Quantum Pufferfish Privacy: A Flexible Privacy Framework for Quantum Systems. (arXiv:2306.13054v1 [quant-ph])

    [http://arxiv.org/abs/2306.13054](http://arxiv.org/abs/2306.13054)

    本文提出了一种称为量子河豚隐私(QPP)的通用隐私框架，通过在指定私有信息、可行的测量和域知识方面提供灵活性，实现了量子差分隐私的概括和克服限制。同时，首次提供了QPP的操作解释，并证明了QPP的凸性、组合性和后处理，推导了保证去极化机制QPP的参数，并将QPP框架应用于隐私审计，以识别隐私侵犯。

    

    我们提出了一种用于量子系统的通用隐私框架，称为量子河豚隐私（QPP）。受经典河豚隐私的启发，我们的公式概括并解决了量子差分隐私的局限性，通过在指定私有信息、可行的测量和域知识方面提供灵活性。我们展示了QPP可以等效地用Datta-Leditzky信息谱散度来表示，从而首次提供了它的操作解释。我们将这种差异重新表述为半定规划，并推导出它的几个属性，然后用这些属性证明了QPP机制的凸性、组合性和后处理。还推导了保证去极化机制QPP的参数。我们分析了普通QPP机制的隐私效用权衡，并再次以去极化机制为例研究其明确实例。然后将QPP框架应用于隐私审计，以识别隐私侵犯。

    We propose a versatile privacy framework for quantum systems, termed quantum pufferfish privacy (QPP). Inspired by classical pufferfish privacy, our formulation generalizes and addresses limitations of quantum differential privacy by offering flexibility in specifying private information, feasible measurements, and domain knowledge. We show that QPP can be equivalently formulated in terms of the Datta-Leditzky information spectrum divergence, thus providing the first operational interpretation thereof. We reformulate this divergence as a semi-definite program and derive several properties of it, which are then used to prove convexity, composability, and post-processing of QPP mechanisms. Parameters that guarantee QPP of the depolarization mechanism are also derived. We analyze the privacy-utility tradeoff of general QPP mechanisms and, again, study the depolarization mechanism as an explicit instance. The QPP framework is then applied to privacy auditing for identifying privacy violati
    
[^9]: 可分组的随机赌徒问题

    Context-lumpable stochastic bandits. (arXiv:2306.13053v1 [cs.LG])

    [http://arxiv.org/abs/2306.13053](http://arxiv.org/abs/2306.13053)

    本文研究了具有 $S$ 个上下文和 $A$ 种行动的情境赌徒问题，并提出了一种可分组的随机赌徒问题算法。

    

    本文研究了具有 $S$ 个上下文和 $A$ 种行动的情境赌徒问题。在每一轮 $t=1,2,\dots$ 中，学习者观察一个随机上下文，并根据其以往的经验选择一个行动。然后，学习者观察一个随机奖励，其平均值是该轮上下文和行动的一个函数。在假设上下文可以分为 $r\leq \min\{S,A\}$ 组，使得任意两个在同一组内的上下文的各种行动的平均奖励相同的情况下，我们设计了一个算法，它在使用 $\widetilde O(r(S + A)/\epsilon^2)$ 个样本后可以生成一个 $\epsilon$-最优策略，并且具有较高的置信度提供了匹配的 $\widetilde\Omega(r (S + A )/\epsilon^2)$ 下限。在遗憾最小化设置下，我们提供了一个算法，其累积遗憾在时间 $T$ 内有界，即 $\widetilde O(\sqrt{r^3(S +A)T})$。据我们所知，我们是第一个展示在可近似正确设置中接近最优样本复杂度和在遗憾最小化设置下的 $\widetilde O(\sqrt{r^3(S +A)T})$ 累积遗憾。

    We consider a contextual bandit problem with $S $ contexts and $A $ actions. In each round $t=1,2,\dots$ the learner observes a random context and chooses an action based on its past experience. The learner then observes a random reward whose mean is a function of the context and the action for the round. Under the assumption that the contexts can be lumped into $r\le \min\{S ,A \}$ groups such that the mean reward for the various actions is the same for any two contexts that are in the same group, we give an algorithm that outputs an $\epsilon$-optimal policy after using at most $\widetilde O(r (S +A )/\epsilon^2)$ samples with high probability and provide a matching $\widetilde\Omega(r (S +A )/\epsilon^2)$ lower bound. In the regret minimization setting, we give an algorithm whose cumulative regret up to time $T$ is bounded by $\widetilde O(\sqrt{r^3(S +A )T})$. To the best of our knowledge, we are the first to show the near-optimal sample complexity in the PAC setting and $\widetild
    
[^10]: 推荐系统的数据增广：一种基于最大边际矩阵分解的半监督方法

    Data augmentation for recommender system: A semi-supervised approach using maximum margin matrix factorization. (arXiv:2306.13050v1 [cs.IR])

    [http://arxiv.org/abs/2306.13050](http://arxiv.org/abs/2306.13050)

    本研究提出了一种基于最大边际矩阵分解的半监督方法来增广和细化协同过滤算法的评级预测。该方法利用自我训练来评估评分的置信度，并通过系统的数据增广策略来提高算法性能。

    

    协同过滤已成为推荐系统开发的常用方法，其中，根据用户的过去喜好和其他用户的可用偏好信息预测其对新物品的评分。尽管CF方法很受欢迎，但其性能通常受观察到的条目的稀疏性的极大限制。本研究探讨最大边际矩阵分解（MMMF）的数据增广和细化方面，该方法是广泛接受的用于评级预测的CF技术，之前尚未进行研究。我们利用CF算法的固有特性来评估单个评分的置信度，并提出了一种基于自我训练的半监督评级增强方法。我们假设任何CF算法的预测低置信度是由于训练数据的某些不足，因此，通过采用系统的数据增广策略，可以提高算法的性能。

    Collaborative filtering (CF) has become a popular method for developing recommender systems (RS) where ratings of a user for new items is predicted based on her past preferences and available preference information of other users. Despite the popularity of CF-based methods, their performance is often greatly limited by the sparsity of observed entries. In this study, we explore the data augmentation and refinement aspects of Maximum Margin Matrix Factorization (MMMF), a widely accepted CF technique for the rating predictions, which have not been investigated before. We exploit the inherent characteristics of CF algorithms to assess the confidence level of individual ratings and propose a semi-supervised approach for rating augmentation based on self-training. We hypothesize that any CF algorithm's predictions with low confidence are due to some deficiency in the training data and hence, the performance of the algorithm can be improved by adopting a systematic data augmentation strategy
    
[^11]: 带有圈特定注意机制的多任务学习用于CDR结构预测

    Multi-Task Learning with Loop Specific Attention for CDR Structure Prediction. (arXiv:2306.13045v1 [cs.LG])

    [http://arxiv.org/abs/2306.13045](http://arxiv.org/abs/2306.13045)

    本文提出了带有圈特定注意机制的多任务学习模型（MLSA），用于共同学习三个CDR环的结构预测，其中H3环的CDR结构由于长度和灵活的结构具有更大挑战性。在实验评估中，该方法显著降低了预测误差。

    

    抗体工程中的互补决定区（CDR）结构预测已经引起了研究者的极大关注。在设计抗体时，主要的挑战是预测H3环的CDR结构，与其他CDR环（H1和H2 环）相比，H3环的CDR结构由于长度和灵活的结构而更具挑战性。本文提出了一种带有圈特定注意力的多任务学习模型，即MLSA。我们是首位通过新颖的多任务学习策略共同学习三个CDR环。此外，为了考虑三个CDR环的结构和功能上的相似性和差异性，我们提出了一个圈特定的注意机制来控制MLSA模型训练中每个CDR环的影响。我们在广泛使用的基准数据集上进行了实验评估，结果显示，所提出的MLSA方法显著降低了预测误差。

    The Complementarity Determining Region (CDR) structure prediction of loops in antibody engineering has gained a lot of attraction by researchers. When designing antibodies, a main challenge is to predict the CDR structure of the H3 loop. Compared with the other CDR loops, that is the H1 and H2 loops, the CDR structure of the H3 loop is more challenging due to its varying length and flexible structure. In this paper, we propose a Multi-task learning model with Loop Specific Attention, namely MLSA. In particular, to the best of our knowledge we are the first to jointly learn the three CDR loops, via a novel multi-task learning strategy. In addition, to account for the structural and functional similarities and differences of the three CDR loops, we propose a loop specific attention mechanism to control the influence of each CDR loop on the training of MLSA. Our experimental evaluation on widely used benchmark data shows that the proposed MLSA method significantly reduces the prediction e
    
[^12]: 机器翻译可解释性评估指标的探索

    Towards Explainable Evaluation Metrics for Machine Translation. (arXiv:2306.13041v1 [cs.CL])

    [http://arxiv.org/abs/2306.13041](http://arxiv.org/abs/2306.13041)

    本研究探索机器翻译可解释性评估指标，提供综合综述和最新方法，并贡献下一代方法的愿景。

    

    与传统的词汇重叠度量（如BLEU）不同，大多数当前用于机器翻译评估的指标（例如COMET或BERTScore）基于黑盒子的大型语言模型。它们通常与人类判断具有强相关性，但是最近的研究表明，较低质量的传统指标仍然占主导地位，其中一个潜在原因是它们的决策过程更透明。因此，为了促进新的高质量指标的更广泛接受，解释性变得至关重要。在这篇概念论文中，我们确定了可解释机器翻译指标的关键属性和目标，并提供了最近技术的综合综述，将它们与我们确立的目标和属性联系起来。在这个背景下，我们还讨论基于生成模型（如ChatGPT和GPT4）的可解释指标的最新先进方法。最后，我们贡献了下一代方法的愿景，包括自然语言e。

    Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics for machine translation (for example, COMET or BERTScore) are based on black-box large language models. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent. To foster more widespread acceptance of novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties as well as key goals of explainable machine translation metrics and provide a comprehensive synthesis of recent techniques, relating them to our established goals and properties. In this context, we also discuss the latest state-of-the-art approaches to explainable metrics based on generative models such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation approaches, including natural language e
    
[^13]: 互联网物联网中基于在线自监督学习的机器学习入侵检测

    Online Self-Supervised Learning in Machine Learning Intrusion Detection for the Internet of Things. (arXiv:2306.13030v1 [cs.CR])

    [http://arxiv.org/abs/2306.13030](http://arxiv.org/abs/2306.13030)

    提出了一种在线自监督学习的机器学习入侵检测框架，能够实现完全在线的IDS，无需离线学习或人工干预，该框架能够快速适应流量的时变特性，消除了离线数据收集的需要，并避免了数据标记中的人为误差，具有准确率高、成本低等优势。

    

    本文提出了一种新颖的自我监督入侵检测（SSID）框架，它使得完全在线的基于机器学习的入侵检测系统成为可能，而无需人工干预或离线学习。该框架仅基于IDS本身的决策和在线估计的统计可信度，使用自联想深度随机神经网络分析和标记传入的流量数据包。SSID框架使IDS能够快速适应网络流量的时变特性，并消除了离线数据收集的需求。这种方法避免了数据标记中的人为误差，以及模型训练和数据收集的人力和计算成本。该方法在公共数据集上进行了实验评估，并与知名机器学习模型进行了比较，结果表明，作为准确和在线学习的基于机器学习的IDS，该SSID框架非常有用且具有优势，可以应用于物联网系统中。

    This paper proposes a novel Self-Supervised Intrusion Detection (SSID) framework, which enables a fully online Machine Learning (ML) based Intrusion Detection System (IDS) that requires no human intervention or prior off-line learning. The proposed framework analyzes and labels incoming traffic packets based only on the decisions of the IDS itself using an Auto-Associative Deep Random Neural Network, and on an online estimate of its statistically measured trustworthiness. The SSID framework enables IDS to adapt rapidly to time-varying characteristics of the network traffic, and eliminates the need for offline data collection. This approach avoids human errors in data labeling, and human labor and computational costs of model training and data collection. The approach is experimentally evaluated on public datasets and compared with well-known ML models, showing that this SSID framework is very useful and advantageous as an accurate and online learning ML-based IDS for IoT systems.
    
[^14]: 分布式在线联邦 G 网络学习用于轻量入侵检测

    Decentralized Online Federated G-Network Learning for Lightweight Intrusion Detection. (arXiv:2306.13029v1 [cs.CR])

    [http://arxiv.org/abs/2306.13029](http://arxiv.org/abs/2306.13029)

    提出了一种分布式和在线联邦学习入侵检测（DOF-ID）架构，它允许每个用于网络系统的入侵检测系统从其他网络系统中获得经验以及本地数据进行学习而不违反其他系统的数据隐私，显著提高了所有正在协作的系统的入侵检测性能。

    

    随着新型未知（零日）攻击的出现和易受攻击的设备的兴起，网络系统面临着日益增加的网络攻击威胁。虽然基于机器学习的入侵检测系统在检测这些攻击方面已经显示出极大的潜力，但需要学习大量标记数据的需求经常限制了仅有私有本地数据访问的网络系统应用基于机器学习的入侵检测系统的可行性。为解决这个问题，本文提出了一种新颖的分布式和在线联邦学习入侵检测（DOF-ID）架构。DOF-ID 是一种协作学习系统，允许每个用于网络系统的入侵检测系统从其他网络系统中获得的经验以及本地数据进行学习而不违反其他系统的数据隐私。通过使用公共 Kitsune 和 Bot-IoT 数据集的性能评估结果表明，DOF-ID 显著提高了所有正在协作的系统的入侵检测性能。

    Cyberattacks are increasingly threatening networked systems, often with the emergence of new types of unknown (zero-day) attacks and the rise of vulnerable devices. While Machine Learning (ML)-based Intrusion Detection Systems (IDSs) have been shown to be extremely promising in detecting these attacks, the need to learn large amounts of labelled data often limits the applicability of ML-based IDSs to cybersystems that only have access to private local data. To address this issue, this paper proposes a novel Decentralized and Online Federated Learning Intrusion Detection (DOF-ID) architecture. DOF-ID is a collaborative learning system that allows each IDS used for a cybersystem to learn from experience gained in other cybersystems in addition to its own local data without violating the data privacy of other systems. As the performance evaluation results using public Kitsune and Bot-IoT datasets show, DOF-ID significantly improves the intrusion detection performance in all collaborating 
    
[^15]: 基于难度条件生成器的可转移课程

    Transferable Curricula through Difficulty Conditioned Generators. (arXiv:2306.13028v1 [cs.AI])

    [http://arxiv.org/abs/2306.13028](http://arxiv.org/abs/2306.13028)

    本文提出了一种名为PERM的方法，可以直接对环境难度和RL代理的能力进行建模，通过匹配环境的难度生成课程，实现了在参数化环境中训练RL代理的有前途的结果。

    

    强化学习在复杂任务中展示了超人类水平的表现，如星际争霸、围棋、国际象棋等等。然而，将人工“专家”的知识传递给人类仍然是一个重大挑战。在课程中使用课程是一种有前途的转移途径。然而，最近的课程生成方法侧重于高效训练RL代理，然而这种方法依赖于代理人进展的代理测量标准，不能用于在现实世界中培训机器人（或更有雄心的是人类）。在本文中，我们介绍了一种名为参数化环境响应模型（PERM）的方法，该方法在参数化环境中训练RL代理表现出了有前途的结果。受项目反应理论的启发，PERM试图直接对环境难度和RL代理的能力建模。鉴于RL代理和人类在“发展的邻域”中更有效地接受训练，我们的方法通过匹配环境的难度以产生课程。

    Advancements in reinforcement learning (RL) have demonstrated superhuman performance in complex tasks such as Starcraft, Go, Chess etc. However, knowledge transfer from Artificial "Experts" to humans remain a significant challenge. A promising avenue for such transfer would be the use of curricula. Recent methods in curricula generation focuses on training RL agents efficiently, yet such methods rely on surrogate measures to track student progress, and are not suited for training robots in the real world (or more ambitiously humans). In this paper, we introduce a method named Parameterized Environment Response Model (PERM) that shows promising results in training RL agents in parameterized environments. Inspired by Item Response Theory, PERM seeks to model difficulty of environments and ability of RL agents directly. Given that RL agents and humans are trained more efficiently under the "zone of proximal development", our method generates a curriculum by matching the difficulty of an e
    
[^16]: 可微分决策树是否能够学习可解释的奖励函数?

    Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])

    [http://arxiv.org/abs/2306.13004](http://arxiv.org/abs/2306.13004)

    本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。

    

    学习人的意图和偏好的奖励函数越来越受到关注，但许多框架使用黑盒学习方法，难以解释。本文提出并评估了一种新颖方法，使用可微分决策树（DDT）从偏好中学习具有表达能力和可解释性的奖励函数，适用于低维和高维状态输入。我们在Cartpole、视觉网格世界环境和Atari游戏上评估了我们的算法，探讨了使用DDT学习可解释奖励函数的可行性。我们提供证据表明，学习到的奖励函数的树形结构有助于确定奖励函数与人类偏好的一致程度。我们可视化了学习到的奖励DDT，发现它们能够学习可解释的奖励函数，但树的离散性会影响强化学习在测试时的表现。

    There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
    
[^17]: 一种图像处理算法是否能在DCE-MRI的所有阶段中同等有效？

    Can a single image processing algorithm work equally well across all phases of DCE-MRI?. (arXiv:2306.12988v1 [eess.IV])

    [http://arxiv.org/abs/2306.12988](http://arxiv.org/abs/2306.12988)

    本文研究了使用不同比例的增强对比（CE）数据训练对DCE-MRI图像分割和注册任务的影响。结果表明，使用CE数据进行预训练并使用非CE数据进行微调可以创建一个通用的模型，以达到最佳结果。

    

    图像分割和配准在应用于动态对比增强MRI序列（DCE-MRI）时被认为是具有挑战性的。对比剂会导致感兴趣区域和其他区域的强度快速变化，这可能会导致分割任务的假阳性预测并混淆图像配准相似度度量。虽然广泛认为对比度变化会增加这些任务的难度，但据我们所知，没有任何研究量化这些影响。在本文中，我们研究了使用不同比例的增强对比（CE）数据训练对两个流行任务的影响：使用nnU-Net和Mask R-CNN进行分割以及使用VoxelMorph和VTN进行配准。我们进一步通过使用可用数据集来进行预训练和微调，进行了策略性实验。我们发现，为了创建一个通用模型，使用CE数据进行预训练并使用非CE数据进行微调可以获得最佳结果。这个有趣的发现可能可以扩展...

    Image segmentation and registration are said to be challenging when applied to dynamic contrast enhanced MRI sequences (DCE-MRI). The contrast agent causes rapid changes in intensity in the region of interest and elsewhere, which can lead to false positive predictions for segmentation tasks and confound the image registration similarity metric. While it is widely assumed that contrast changes increase the difficulty of these tasks, to our knowledge no work has quantified these effects. In this paper we examine the effect of training with different ratios of contrast enhanced (CE) data on two popular tasks: segmentation with nnU-Net and Mask R-CNN and registration using VoxelMorph and VTN. We experimented further by strategically using the available datasets through pretraining and fine tuning with different splits of data. We found that to create a generalisable model, pretraining with CE data and fine tuning with non-CE data gave the best result. This interesting find could be expande
    
[^18]: 从数据中推断最精细的互相独立模式

    Inferring the finest pattern of mutual independence from data. (arXiv:2306.12984v1 [stat.ML])

    [http://arxiv.org/abs/2306.12984](http://arxiv.org/abs/2306.12984)

    通过引入二分独立性，利用i.i.d.正态分布数据估计随机变量最精细的互相独立模式，并在模拟数据和实验数据上进行测试。

    

    对于随机变量X，我们对其最精细的互相独立模式μ(X)的盲目提取感兴趣。我们引入了一种特定的独立性，称为二分的独立性。如果Δ(X)代表所有适用于X的二分独立性模式集，则我们证明μ(X)可以作为Δ(X)的所有元素的交集来获得。然后，我们提出了一种方法，在数据独立且服从多元正态分布的条件下，估计Δ(X)。如果^Δ(X)是有效的二分独立模式的估计集，则我们将μ(X)估计为^Δ(X)的所有模式的交集。该方法在模拟数据上进行了测试，表明了其优点和局限性。我们还考虑了一个玩具例子和实验数据的应用。

    For a random variable $X$, we are interested in the blind extraction of its finest mutual independence pattern $\mu ( X )$. We introduce a specific kind of independence that we call dichotomic. If $\Delta ( X )$ stands for the set of all patterns of dichotomic independence that hold for $X$, we show that $\mu ( X )$ can be obtained as the intersection of all elements of $\Delta ( X )$. We then propose a method to estimate $\Delta ( X )$ when the data are independent and identically (i.i.d.) realizations of a multivariate normal distribution. If $\hat{\Delta} ( X )$ is the estimated set of valid patterns of dichotomic independence, we estimate $\mu ( X )$ as the intersection of all patterns of $\hat{\Delta} ( X )$. The method is tested on simulated data, showing its advantages and limits. We also consider an application to a toy example as well as to experimental data.
    
[^19]: 面向大型扩散模型的更真实成员推断攻击

    Towards More Realistic Membership Inference Attacks on Large Diffusion Models. (arXiv:2306.12983v1 [cs.LG])

    [http://arxiv.org/abs/2306.12983](http://arxiv.org/abs/2306.12983)

    本文研究了成员推断攻击问题，以确定图像是否在训练集中使用。研究集中于稳定扩散模型，提出了一种公平的评估框架，并进行了成员攻击，揭示了先前提出的评估设置不能很好地模拟真实世界中的成员攻击。

    

    生成扩散模型，包括稳定扩散和Midjourney，可以为各种应用程序生成具有视觉吸引力、多样性和高分辨率的图像。这些模型是在数十亿个互联网来源的图像上进行训练的，引发了关于潜在未经授权使用受版权保护的图像的重要担忧。本文研究了如何确定特定图像是否在训练集中使用了，这在网络安全社区中被称为成员推断攻击问题。我们的研究重点是稳定扩散，并解决了设计一个公平的评估框架来回答这个成员问题的挑战。我们提出了一种方法来建立一个公平的评估设置，并将其应用于稳定扩散，使潜在的扩展到其他生成模型成为可能。利用这个评估设置，我们执行成员攻击（包括已知和新引入的攻击）。我们的研究揭示了先前提出的评估设置不能很好地模拟真实世界中的成员攻击。

    Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community and referred to as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a methodology to establish a fair evaluation setup and apply it to Stable Diffusion, enabling potential extensions to other generative models. Utilizing this evaluation setup, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not
    
[^20]: 一种基于强化学习的能量收集的 RSMA 空中通信的和速率最大化方法

    Sum-Rate Maximization of RSMA-based Aerial Communications with Energy Harvesting: A Reinforcement Learning Approach. (arXiv:2306.12977v1 [cs.IT])

    [http://arxiv.org/abs/2306.12977](http://arxiv.org/abs/2306.12977)

    本文提出了一种基于强化学习的能量收集的 RSMA 空中通信的和速率最大化方法，通过限制每个时间的最大传输功率和使用序列最小二乘规划等方法，实现了对机器人和其他设备提供高效的服务。

    

    本文研究了一种基于能量收集的自持空中基站为多个用户提供服务的速率分裂多址 (RSMA) 空中通信的联合功率和波束赋形设计问题。为了从长期角度最大化总速率，我们使用深度强化学习算法（软演员-评论家算法），基于信道环境、收集能量和电池电量信息的随机属性，限制每个时间的最大传输功率。此外，为了设计 RSMA 的所有私有/公共流的预编码器和功率分配，我们采用序列最小二乘规划 (SLSQP)，使用 Han-Powell 拟牛顿方法通过 DRL 最大化给定传输功率下的总速率。数值结果显示了所提出方案在平均总速率性能方面优于几种基线方法。

    In this letter, we investigate a joint power and beamforming design problem for rate-splitting multiple access (RSMA)-based aerial communications with energy harvesting, where a self-sustainable aerial base station serves multiple users by utilizing the harvested energy. Considering maximizing the sum-rate from the long-term perspective, we utilize a deep reinforcement learning (DRL) approach, namely the soft actor-critic algorithm, to restrict the maximum transmission power at each time based on the stochastic property of the channel environment, harvested energy, and battery power information. Moreover, for designing precoders and power allocation among all the private/common streams of the RSMA, we employ sequential least squares programming (SLSQP) using the Han-Powell quasi-Newton method to maximize the sum-rate for the given transmission power via DRL. Numerical results show the superiority of the proposed scheme over several baseline methods in terms of the average sum-rate perf
    
[^21]: 高维数据流自适应伯恩斯坦变化检测器

    Adaptive Bernstein Change Detector for High-Dimensional Data Streams. (arXiv:2306.12974v1 [cs.LG])

    [http://arxiv.org/abs/2306.12974](http://arxiv.org/abs/2306.12974)

    本文提出了一个适用于高维数据流的自适应伯恩斯坦变化检测器，具有准确地识别变化发生的时间与子空间，并能够量化严重程度的特性。

    

    当分析数据流时，变化检测至关重要。快速和准确地检测到变化可以使监测和预测系统做出反应，例如发出警报或更新学习算法。然而，在高维数据中检测变化是具有挑战性的。在高维数据中，变化检测器不仅应能够识别变化发生的时间，还应能够识别发生在哪个子空间中，并且最好还应能量化它们的严重程度。我们提出的ABCd方法具备这些特性。ABCD学习一个编码器-解码器模型，并监测其在自适应大小的窗口内的准确性。ABCD根据伯恩斯坦不等式计算变化得分，以检测准确性方面的偏差，指示变化。我们的实验表明，ABCD在平均F1-score中至少比其最佳竞争对手表现优越了8％，最多可提高23％。它还可以准确地估计变化的子空间，以及与变化大小相关的严重程度指标。

    Change detection is of fundamental importance when analyzing data streams. Detecting changes both quickly and accurately enables monitoring and prediction systems to react, e.g., by issuing an alarm or by updating a learning algorithm. However, detecting changes is challenging when observations are high-dimensional. In high-dimensional data, change detectors should not only be able to identify when changes happen, but also in which subspace they occur. Ideally, one should also quantify how severe they are. Our approach, ABCD, has these properties. ABCD learns an encoder-decoder model and monitors its accuracy over a window of adaptive size. ABCD derives a change score based on Bernstein's inequality to detect deviations in terms of accuracy, which indicate changes. Our experiments demonstrate that ABCD outperforms its best competitor by at least 8% and up to 23% in F1-score on average. It can also accurately estimate changes' subspace, together with a severity measure that correlates w
    
[^22]: 动态神经网络预测股票价格

    Stock Price Prediction using Dynamic Neural Networks. (arXiv:2306.12969v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.12969](http://arxiv.org/abs/2306.12969)

    本文针对股票价格预测，使用一种新的动态神经网络模型，提供了比许多现有技术更准确的预测方法，并驳斥了有效市场假说，支持混沌理论。

    

    本文分析并实现了一种时间序列动态神经网络来预测每日收盘股票价格。神经网络在识别混沌、非线性和看似随机的数据中具有无与伦比的能力，因此提供了比许多当前技术更精准地预测股票价格变动的机制。文章讨论了包括基本面、技术面和回归技术在内的现代股票分析方法，并将其与神经网络的表现进行了对比。此外，本文还提出了有效市场假说（EMH）并使用神经网络与混沌理论进行了对比。本文驳斥了EMH并支持混沌理论。最后，本文提出了关于如何使用神经网络进行股票价格预测的建议。

    This paper will analyze and implement a time series dynamic neural network to predict daily closing stock prices. Neural networks possess unsurpassed abilities in identifying underlying patterns in chaotic, non-linear, and seemingly random data, thus providing a mechanism to predict stock price movements much more precisely than many current techniques. Contemporary methods for stock analysis, including fundamental, technical, and regression techniques, are conversed and paralleled with the performance of neural networks. Also, the Efficient Market Hypothesis (EMH) is presented and contrasted with Chaos theory using neural networks. This paper will refute the EMH and support Chaos theory. Finally, recommendations for using neural networks in stock price prediction will be presented.
    
[^23]: 标记随机块模型中的最优簇恢复问题

    Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model. (arXiv:2306.12968v1 [cs.SI])

    [http://arxiv.org/abs/2306.12968](http://arxiv.org/abs/2306.12968)

    本论文提出了一种算法，名为实例自适应聚类（IAC），它能够在标记随机块模型（LSBM）中恢复隐藏的群集。IAC包括一次谱聚类和一个迭代的基于似然的簇分配改进，不需要任何模型参数，是高效的。

    

    本文考虑在有限数量的簇的情况下，用标记随机块模型（LSBM）恢复隐藏的社群，其中簇大小随着物品总数$n$的增长而线性增长。在LSBM中，为每对物品（独立地）观测到一个标签。我们的目标是设计一种有效的算法，利用观测到的标签来恢复簇。为此，我们重新审视了关于期望被任何聚类算法误分类的物品数量的实例特定下界。我们提出了实例自适应聚类（IAC），这是第一个在期望和高概率下都能匹配这些下界表现的算法。IAC由一次谱聚类算法和一个迭代的基于似然的簇分配改进组成。这种方法基于实例特定的下界，不需要任何模型参数，包括簇的数量。通过仅执行一次谱聚类，IAC在计算和存储方面都是高效的。

    We consider the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters, where cluster sizes grow linearly with the total number $n$ of items. In the LSBM, a label is (independently) observed for each pair of items. Our objective is to devise an efficient algorithm that recovers clusters using the observed labels. To this end, we revisit instance-specific lower bounds on the expected number of misclassified items satisfied by any clustering algorithm. We present Instance-Adaptive Clustering (IAC), the first algorithm whose performance matches these lower bounds both in expectation and with high probability. IAC consists of a one-time spectral clustering algorithm followed by an iterative likelihood-based cluster assignment improvement. This approach is based on the instance-specific lower bound and does not require any model parameters, including the number of clusters. By performing the spectral clustering only once, IAC m
    
[^24]: 基于量子机器学习的金融预测的改进

    Improved Financial Forecasting via Quantum Machine Learning. (arXiv:2306.12965v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.12965](http://arxiv.org/abs/2306.12965)

    本研究利用量子机器学习提升了金融预测的表现，包括使用行列式点过程来增强随机森林模型进行流失预测并设计了量子神经网络架构用于信用风险评估，比传统方法使用更少的参数达到相似的性能。

    

    量子算法有潜力提高机器学习在各领域和应用中的表现。在本文中，我们展示了如何使用量子机器学习来改进金融预测。首先，我们使用经典和量子行列式点过程来增强随机森林模型以进行流失预测，提高了近6％的精度。其次，我们设计了具有正交和复合层的量子神经网络架构，用较少的参数达到了与经典性能相当的信用风险评估效果。我们的结果表明，利用量子思想可以有效提升机器学习的表现，无论是现在作为量子启发式的经典ML解决方案，还是在未来更好的量子硬件的到来时。

    Quantum algorithms have the potential to enhance machine learning across a variety of domains and applications. In this work, we show how quantum machine learning can be used to improve financial forecasting. First, we use classical and quantum Determinantal Point Processes to enhance Random Forest models for churn prediction, improving precision by almost 6%. Second, we design quantum neural network architectures with orthogonal and compound layers for credit risk assessment, which match classical performance with significantly fewer parameters. Our results demonstrate that leveraging quantum ideas can effectively enhance the performance of machine learning, both today as quantum-inspired classical ML solutions, and even more in the future, with the advent of better quantum hardware.
    
[^25]: 通过强化学习生成协同公式化的Alpha集合

    Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning. (arXiv:2306.12964v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.12964](http://arxiv.org/abs/2306.12964)

    本文提出了一种新的Alpha生成框架，它通过强化学习挖掘协同的公式化Alpha集合以优化它们作为组合模型的性能。

    

    在量化交易领域中，将原始股票历史数据转化为市场趋势指示信号是常见的实践。这些信号被称为Alpha因子。公式形式的Alpha因子更易于解释，因此风险意识强的实践者更喜欢它们。在实践中，往往同时使用一组公式化的Alpha因子进行更好的建模精度，因此我们需要找到能够良好协同的公式化Alpha集。但是，大多数传统的Alpha生成器分别挖掘单个Alpha，忽略了后续的Alpha组合重要性。本文提出了一种新的Alpha挖掘框架，它优先挖掘协同的Alpha集合，即直接使用下游组合模型的性能来优化Alpha生成器。我们的框架还利用了强化学习的强大探索能力，以更好地探索公式形式的Alpha因子的大量搜索空间。本文的贡献在于提出了使用强化学习生成协同的公式化Alpha集合的框架，优化它们作为组合模型的性能，这与传统的Alpha生成器分别挖掘单个Alpha的方法形成对比。

    In the field of quantitative trading, it is common practice to transform raw historical stock data into indicative signals for the market trend. Such signals are called alpha factors. Alphas in formula forms are more interpretable and thus favored by practitioners concerned with risk. In practice, a set of formulaic alphas is often used together for better modeling precision, so we need to find synergistic formulaic alpha sets that work well together. However, most traditional alpha generators mine alphas one by one separately, overlooking the fact that the alphas would be combined later. In this paper, we propose a new alpha-mining framework that prioritizes mining a synergistic set of alphas, i.e., it directly uses the performance of the downstream combination model to optimize the alpha generator. Our framework also leverages the strong exploratory capabilities of reinforcement learning~(RL) to better explore the vast search space of formulaic alphas. The contribution to the combina
    
[^26]: PyKoopman: 用于数据驱动的Koopman算子逼近的Python包

    PyKoopman: A Python Package for Data-Driven Approximation of the Koopman Operator. (arXiv:2306.12962v1 [eess.SY])

    [http://arxiv.org/abs/2306.12962](http://arxiv.org/abs/2306.12962)

    PyKoopman是一个用于数据驱动的Koopman算子逼近的Python包，旨在实现对强非线性动态的预测、估计和控制。该包提供了数据驱动系统识别工具和不需要方程的动态模态分解（DMD）以及其变种。

    

    PyKoopman是一个用于动态系统的数据驱动的Koopman算子逼近的Python包。Koopman算子是非线性动态的原理线性嵌入, 使用线性系统理论可实现对强非线性动态的预测、估计和控制。特别地，PyKoopman提供了用于建模自由和驱动动态系统的数据驱动的系统识别工具, 这些工具建立在不需要方程的动态模态分解（DMD）及其变种上。本文简要介绍了Koopman算子的数学基础，概述并演示了PyKoopman包中实现的功能（包括代码示例），提供了给用户的实用建议，以及PyKoopman的扩展列表。软件可在此网址获得：http URL。

    PyKoopman is a Python package for the data-driven approximation of the Koopman operator associated with a dynamical system. The Koopman operator is a principled linear embedding of nonlinear dynamics and facilitates the prediction, estimation, and control of strongly nonlinear dynamics using linear systems theory. In particular, PyKoopman provides tools for data-driven system identification for unforced and actuated systems that build on the equation-free dynamic mode decomposition (DMD) and its variants. In this work, we provide a brief description of the mathematical underpinnings of the Koopman operator, an overview and demonstration of the features implemented in PyKoopman (with code examples), practical advice for users, and a list of potential extensions to PyKoopman. Software is available at this http URL
    
[^27]: 用隐式神经表示进行音频压缩的连锁SIREN模型

    Siamese SIREN: Audio Compression with Implicit Neural Representations. (arXiv:2306.12957v1 [cs.SD])

    [http://arxiv.org/abs/2306.12957](http://arxiv.org/abs/2306.12957)

    该研究利用Siamese SIREN 建立了一种新的方法，使用了较少的网络参数来实现卓越的音频重建保真度，拓展了隐式神经表示压缩的应用领域。

    

    隐式神经表示(INRs)已成为表示多种数据形式(包括3D形状、图像和音频)的一种有前途的方法。虽然最近研究已经成功地将INRs应用于图像和3D形状的压缩中，但它们在音频压缩方面的潜力仍然未被充分探索。受此启发，我们对INRs进行了初步的音频压缩研究。我们的研究介绍了基于流行的SIREN架构的一种新方法: 连锁SIREN。我们的实验结果表明，与先前的INRs架构相比，连锁SIREN在利用更少的网络参数的同时实现了更高的音频重建保真度。

    Implicit Neural Representations (INRs) have emerged as a promising method for representing diverse data modalities, including 3D shapes, images, and audio. While recent research has demonstrated successful applications of INRs in image and 3D shape compression, their potential for audio compression remains largely unexplored. Motivated by this, we present a preliminary investigation into the use of INRs for audio compression. Our study introduces Siamese SIREN, a novel approach based on the popular SIREN architecture. Our experimental results indicate that Siamese SIREN achieves superior audio reconstruction fidelity while utilizing fewer network parameters compared to previous INR architectures.
    
[^28]: 条件双自编码器触发暗淋浴

    Triggering Dark Showers with Conditional Dual Auto-Encoders. (arXiv:2306.12955v1 [hep-ex])

    [http://arxiv.org/abs/2306.12955](http://arxiv.org/abs/2306.12955)

    该论文通过使用条件双自编码器，对探测器图像进行暗版强力信号新物理搜索，实现了对撞机新物理搜索的有效和通用工具，证明了AE具有出色的判别能力。

    

    自编码器(AEs)有潜力成为对撞机新物理搜索的有效和通用工具，需要很少或不需要模型依赖的假设。新的理论物理信号可以作为与通常期望用来描述整个数据集的已知背景过程偏离的异常值来考虑。本文提出了一种用AE定义判定事件物理本质的准则的异常检测(AD)方法。在这项工作中，我们使用原始探测器图像进行AD搜索，对于无法利用任何基于物理的预处理或对信号的假设的大而稀疏的数据，进行暗版强力的表现形式的搜索。我们提出了一种可通过条件学习紧凑潜在空间的双编码器设计。在多个AD指标的背景下，我们提出了相比竞争基线和先前方法的明显改进。这是第一次展示AE具有出色的判别能力。

    Auto-encoders (AEs) have the potential to be effective and generic tools for new physics searches at colliders, requiring little to no model-dependent assumptions. New hypothetical physics signals can be considered anomalies that deviate from the well-known background processes generally expected to describe the whole dataset. We present a search formulated as an anomaly detection (AD) problem, using an AE to define a criterion to decide about the physics nature of an event. In this work, we perform an AD search for manifestations of a dark version of strong force using raw detector images, which are large and very sparse, without leveraging any physics-based pre-processing or assumption on the signals. We propose a dual-encoder design which can learn a compact latent space through conditioning. In the context of multiple AD metrics, we present a clear improvement over competitive baselines and prior approaches. It is the first time that an AE is shown to exhibit excellent discriminati
    
[^29]: 进化计算图算法

    Evolving Computation Graphs. (arXiv:2306.12943v1 [cs.LG])

    [http://arxiv.org/abs/2306.12943](http://arxiv.org/abs/2306.12943)

    本论文提出了一种进化计算图算法（ECGs），用于提高针对异质性数据的图神经网络（GNN）性能，通过重连GNN的计算图增加连接同一类节点的边缘以提升性能。

    

    在关系数据建模方面，图神经网络（GNN）已经展现出了成功，尤其是对于那些表现出同质性的数据：当节点之间的连接往往暗示它们属于同一类时，这一点更为明显。然而，虽然这个假设在许多相关情况下是成立的，但在重要的现实场景中也有违反这个假设的情况，这促进了对GNN在这些情况下进行改进的研究。在这项工作中，我们提出了一种新颖的方法——进化计算图算法（ECGs），用于增强针对异质性数据的GNN。我们的方法建立在先前的理论洞见之上，将节点度、高同质性和内部与类间嵌入相似性联系在一起，通过重连GNN的计算图来增加连接同一类节点的边缘。我们利用较弱的分类器来识别这些边缘，从而最终提高了GNN对非同质性数据的性能。我们评估ECGs在一组多样化的最近提出的异质数据集上的表现。

    Graph neural networks (GNNs) have demonstrated success in modeling relational data, especially for data that exhibits homophily: when a connection between nodes tends to imply that they belong to the same class. However, while this assumption is true in many relevant situations, there are important real-world scenarios that violate this assumption, and this has spurred research into improving GNNs for these cases. In this work, we propose Evolving Computation Graphs (ECGs), a novel method for enhancing GNNs on heterophilic datasets. Our approach builds on prior theoretical insights linking node degree, high homophily, and inter vs intra-class embedding similarity by rewiring the GNNs' computation graph towards adding edges that connect nodes that are likely to be in the same class. We utilise weaker classifiers to identify these edges, ultimately improving GNN performance on non-homophilic data as a result. We evaluate ECGs on a diverse set of recently-proposed heterophilous datasets a
    
[^30]: 鲁棒语义分割：强鲁棒性攻击和快速训练鲁棒性模型

    Robust Semantic Segmentation: Strong Adversarial Attacks and Fast Training of Robust Models. (arXiv:2306.12941v1 [cs.CV])

    [http://arxiv.org/abs/2306.12941](http://arxiv.org/abs/2306.12941)

    本文提出了针对语义分割模型的解决方案，使得可以对其进行攻击并提供了更好的评估协议。同时，通过微调鲁棒的主干，可以有限的计算代价训练对抗性鲁棒的分割模型。

    

    虽然大量的工作已经集中在设计针对图像分类器的对抗性攻击上，但只有少数方法存在用于攻击语义分割模型。我们展示了攻击分割模型的任务特定挑战，并提出了新的解决方案。我们的最终评估协议优于现有方法，并表明这些方法可能高估了模型的鲁棒性。此外，至今最成功的获得鲁棒图像分类器的对抗性训练无法成功应用于语义分割。我们认为这是因为要学习的任务更具挑战性，需要比图像分类更高的计算量。作为解决方法，我们展示了通过利用最近在鲁棒ImageNet分类器方面的进展，可以通过微调鲁棒的主干，以有限的计算代价训练对抗性鲁棒的分割模型。

    While a large amount of work has focused on designing adversarial attacks against image classifiers, only a few methods exist to attack semantic segmentation models. We show that attacking segmentation models presents task-specific challenges, for which we propose novel solutions. Our final evaluation protocol outperforms existing methods, and shows that those can overestimate the robustness of the models. Additionally, so far adversarial training, the most successful way for obtaining robust image classifiers, could not be successfully applied to semantic segmentation. We argue that this is because the task to be learned is more challenging, and requires significantly higher computational effort than for image classification. As a remedy, we show that by taking advantage of recent advances in robust ImageNet classifiers, one can train adversarially robust segmentation models at limited computational cost by fine-tuning robust backbones.
    
[^31]: 可量化Transformer：通过帮助注意力头“什么也不做”去除离群值

    Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])

    [http://arxiv.org/abs/2306.12929](http://arxiv.org/abs/2306.12929)

    本文提出了一种称为“Helper-Head”的方法，可以通过教授注意头忽略输入和输出的某些部分来消除离群值，从而实现对transformer的可量化。实验结果表明，该方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。

    

    过去几年里，Transformer模型已经被广泛应用于各个领域，特别是大型语言模型已经显著推进了人工智能领域的发展。由于其规模，这些网络的能力已经大大增强，但这是以极大的计算成本为代价的。量化是减少神经网络计算时间和存储器消耗的最有效方法之一。然而，许多研究表明，现代transformer模型往往学习到其激活中的强离群值，这使得它们难以量化。为保持可接受的性能，这些离群值的存在需要将激活置于更高的比特宽度或使用不同的数字格式，进行额外的微调或其他变通方法。本文展示了强离群值与特定注意头行为的相关性，这些头试图学习“无操作”或仅仅是部分残差更新。为了实现注意力头中需要的精确零位，我们引入了一个称为“Helper-Head”的方法，教授注意力头忽略输入和输出的某些部分。我们还引入了一种利用这些额外信息的量化技术，可以使用低精度量化甚至是强离群数据。在几个基准数据集上的实验证明，我们的方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。

    Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention 
    
[^32]: 具有全局状态预测的分散式多智能体强化学习

    Decentralized Multi-Agent Reinforcement Learning with Global State Prediction. (arXiv:2306.12926v1 [cs.RO])

    [http://arxiv.org/abs/2306.12926](http://arxiv.org/abs/2306.12926)

    本文研究了分散式多智能体强化学习中的一个关键挑战：如何在没有全局信息的情况下有效训练机器人。我们提出了一种基于状态预测的方法，在不需要显式通信的情况下使机器人能够协调行动，实现更快更好的学习效果和任务执行性能。

    

    深度强化学习（DRL）在控制单个机器人方面取得了显着的成功。然而，将DRL应用于机器人群体存在重大挑战。其中一个关键挑战是非静态性，即当两个或更多机器人同时更新个体或共享政策时，会进入一个相互依存的培训过程，并且不保证收敛。克服非静态性通常涉及使用其他智能体的全局信息来训练机器人，例如其他智能体的状态和/或行动。相比之下，本文探讨了如何消除全局信息的需求。由于缺乏其他信息体的全局知识，我们将问题描述为部分可观察的马尔可夫决策过程。在以集体运输为测试场景的情况下，我们研究了两种多智能体培训方法。在第一种方法中，机器人不交换信息，并且被训练依靠通过推（push）和拉（pull）物体进行隐式通信。在第二种方法中，机器人彼此共享状态预测，使他们能够在没有显式通信的情况下协调行动。我们的实验表明，共享预测可以使智能体更有效地学习，同时在需要更少与环境交互的情况下实现更好的任务执行性能。

    Deep reinforcement learning (DRL) has seen remarkable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second appro
    
[^33]: 一种基于交互界面的表格数据新类别发现方法

    An Interactive Interface for Novel Class Discovery in Tabular Data. (arXiv:2306.12919v1 [cs.LG])

    [http://arxiv.org/abs/2306.12919](http://arxiv.org/abs/2306.12919)

    这篇论文提出了一种交互界面，可以帮助专业领域的专家快速运行最先进的表格数据新类别发现算法，生成易于解释的结果。

    

    新类别发现(Novel Class Discovery, NCD) 是在未标记的数据集合中，基于一个标记的不同但相关的数据集合进行寻找新类别的问题。目前大多数的 NCD 方法只适用于图像数据，而表格数据则是实际应用中最广泛使用的数据类型之一。为了解释聚类或 NCD 算法的结果，数据科学家需要理解表格数据的领域和应用特定属性，这项任务往往只能由专业领域的专家来完成。因此，这个交互界面允许专业领域的专家轻松运行应用于表格数据 NCD 的最先进算法，即使只有很少的数据科学知识，也可以生成易于解释的结果。

    Novel Class Discovery (NCD) is the problem of trying to discover novel classes in an unlabeled set, given a labeled set of different but related classes. The majority of NCD methods proposed so far only deal with image data, despite tabular data being among the most widely used type of data in practical applications. To interpret the results of clustering or NCD algorithms, data scientists need to understand the domain- and application-specific attributes of tabular data. This task is difficult and can often only be performed by a domain expert. Therefore, this interface allows a domain expert to easily run state-of-the-art algorithms for NCD in tabular data. With minimal knowledge in data science, interpretable results can be generated.
    
[^34]: 基于CAD引擎的深度学习物理的多目标船型优化与三维流预测

    Multi-Objective Hull Form Optimization with CAD Engine-based Deep Learning Physics for 3D Flow Prediction. (arXiv:2306.12915v1 [cs.LG])

    [http://arxiv.org/abs/2306.12915](http://arxiv.org/abs/2306.12915)

    本研究提出了一个基于CAD引擎和深度学习物理的优化框架，可以自动端到端地评估设计迭代并进行多目标船型优化，其中蕴含着能够提供精确的三维流预测的DLP模型。

    

    本研究提出了一种Deep Learning Physics Optimization (DLPO)框架来建立杜伊斯堡测试案例（DTC）集装箱船的形状优化研究。我们提出了两个应用：（1）敏感性分析以检测最有前途的基础船体形状，以及（2）多目标优化以量化最佳船体形式之间的权衡。DLPO框架允许自动端到端地评估设计迭代。我们通过将Extrality的Deep Learning Physics（DLP）模型与CAD引擎和优化器耦合实现了这些结果。我们提出的DLP模型是通过来自RANS模拟的完整三维体积数据进行训练的，可以提供精确而高质量的实时三维流预测，使其成为相对于水动力效率进行新集装箱船设计优化的优秀评估器。特别地，它能够通过对船体表面的高精度积分来恢复作用在船上的力。

    In this work, we propose a built-in Deep Learning Physics Optimization (DLPO) framework to set up a shape optimization study of the Duisburg Test Case (DTC) container vessel. We present two different applications: (1) sensitivity analysis to detect the most promising generic basis hull shapes, and (2) multi-objective optimization to quantify the trade-off between optimal hull forms. DLPO framework allows for the evaluation of design iterations automatically in an end-to-end manner. We achieved these results by coupling Extrality's Deep Learning Physics (DLP) model to a CAD engine and an optimizer. Our proposed DLP model is trained on full 3D volume data coming from RANS simulations, and it can provide accurate and high-quality 3D flow predictions in real-time, which makes it a good evaluator to perform optimization of new container vessel designs w.r.t the hydrodynamic efficiency. In particular, it is able to recover the forces acting on the vessel by integration on the hull surface wi
    
[^35]: 使用Wasserstein重心缓解保险中的歧视

    Mitigating Discrimination in Insurance with Wasserstein Barycenters. (arXiv:2306.12912v1 [stat.ML])

    [http://arxiv.org/abs/2306.12912](http://arxiv.org/abs/2306.12912)

    本研究针对保险业依赖个人敏感特征预测风险容易造成歧视的问题，提出使用Wasserstein重心缓解偏见的方法。

    

    保险业严重依赖于根据潜在客户的特征预测风险的模型。虽然这样的模型很常见，但研究人员长期以来一直指出，这种做法会因基于敏感特征（如性别或种族）而产生歧视。鉴于这种歧视通常可以归因于历史数据偏见，因此消除或至少缓解歧视是可取的。随着从更传统的模型转向基于机器学习的预测，对更大的缓解呼声也在增加，因为仅仅排除价格过程中的敏感变量被证明是无效的。在本文中，我们首先研究为什么预测在行业内是必要的，以及为什么纠正偏见并不像简单地识别敏感变量那么直接。然后，我们提出使用Wasserstein重心来缓解偏见，而不是简单地缩放。为了展示这种方法的效果和有效性

    The insurance industry is heavily reliant on predictions of risks based on characteristics of potential customers. Although the use of said models is common, researchers have long pointed out that such practices perpetuate discrimination based on sensitive features such as gender or race. Given that such discrimination can often be attributed to historical data biases, an elimination or at least mitigation is desirable. With the shift from more traditional models to machine-learning based predictions, calls for greater mitigation have grown anew, as simply excluding sensitive variables in the pricing process can be shown to be ineffective. In this article, we first investigate why predictions are a necessity within the industry and why correcting biases is not as straightforward as simply identifying a sensitive variable. We then propose to ease the biases through the use of Wasserstein barycenters instead of simple scaling. To demonstrate the effects and effectiveness of the approach 
    
[^36]: 一种联合模拟和机器学习的原位框架及其在CFD中的应用

    In Situ Framework for Coupling Simulation and Machine Learning with Application to CFD. (arXiv:2306.12900v1 [cs.LG])

    [http://arxiv.org/abs/2306.12900](http://arxiv.org/abs/2306.12900)

    本文提出了一种原位框架，在异构集群上启用训练和推理工作流程。采用SmartSim部署数据库，存储数据和ML模型，避免了文件系统问题。在Polaris超级计算机上展示了完美的扩展效率，并通过训练湍流流动模拟中的自动编码器，验证了框架开销的可忽略性。

    

    近年来，机器学习在促进流体动力学计算方面有许多成功的应用。随着模拟的增长，为传统的离线学习生成新的训练数据集会造成I/O和存储瓶颈。此外，在运行时执行推理需要将ML框架库与模拟代码进行非平凡的耦合。本文通过简化这种耦合并在异构集群上启用原位训练和推理工作流程来解决这两个限制。利用SmartSim，所提出的框架部署了一个数据库将数据和ML模型存储在内存中，从而避免了文件系统问题。在Polaris超级计算机上，我们展示了将数据库部署在新位置从而取得成倍增长的数据传输和推断成本的完美扩展效率。此外，我们从湍流流动模拟中实时研究了自动编码器的训练，显示出框架开销相对于传统方法的可忽略性。

    Recent years have seen many successful applications of machine learning (ML) to facilitate fluid dynamic computations. As simulations grow, generating new training datasets for traditional offline learning creates I/O and storage bottlenecks. Additionally, performing inference at runtime requires non-trivial coupling of ML framework libraries with simulation codes. This work offers a solution to both limitations by simplifying this coupling and enabling in situ training and inference workflows on heterogeneous clusters. Leveraging SmartSim, the presented framework deploys a database to store data and ML models in memory, thus circumventing the file system. On the Polaris supercomputer, we demonstrate perfect scaling efficiency to the full machine size of the data transfer and inference costs thanks to a novel co-located deployment of the database. Moreover, we train an autoencoder in situ from a turbulent flow simulation, showing that the framework overhead is negligible relative to a 
    
[^37]: 基于机器学习的实时反馈控制InAs/GaAs量子点生长

    Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots. (arXiv:2306.12898v1 [cond-mat.mes-hall])

    [http://arxiv.org/abs/2306.12898](http://arxiv.org/abs/2306.12898)

    该论文提出了一种基于机器学习的实时反馈控制InAs/GaAs量子点生长方法。

    

    自组装的InAs / GaAs量子点（QDs）具有用于开发各种光电子器件的极高价值。建立特定密度的QDs的过程参数是一个多维优化挑战，通常通过耗时和迭代的试错来解决。在此，作者使用基于3D ResNet的机器学习（ML）模型，专门训练RHEED视频，并提供有关表面形貌的实时反馈。

    Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuable for developing various optoelectronic devices such as QD lasers and single photon sources. The applications strongly rely on the density and quality of these dots, which has motivated studies of the growth process control to realize high-quality epi-wafers and devices. Establishing the process parameters in molecular beam epitaxy (MBE) for a specific density of QDs is a multidimensional optimization challenge, usually addressed through time-consuming and iterative trial-and-error. Meanwhile, reflective high-energy electron diffraction (RHEED) has been widely used to capture a wealth of growth information in situ. However, it still faces the challenges of extracting information from noisy and overlapping images. Here, based on 3D ResNet, we developed a machine learning (ML) model specially designed for training RHEED videos instead of static images and providing real-time feedback on surface morphologies for pro
    
[^38]: FuXi: 一个15天全球天气预报级联机器学习系统

    FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])

    [http://arxiv.org/abs/2306.12873](http://arxiv.org/abs/2306.12873)

    逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。

    

    近年来，随着机器学习模型在天气预报中的快速发展，最先进的机器学习模型在0.25度空间分辨率下的10天天气预报中已经表现出比欧洲中期天气预报中心(ECMWF)的高分辨率预报(HRES)更优越的性能。然而，挑战在于在15天预报中表现与ECMWF集合平均(EM)相当。以前的研究表明，缓解预报误差的积累对于有效的长期预报非常重要。尽管有许多减少积累误差的努力，包括自回归多时间步长损失，但使用单个模型发现无法在短和长导出时间上达到最佳性能。因此，我们提出了FuXi，这是一个级联机器学习天气预测系统，提供了分辨率为0.25度、时间分辨率为6小时的15天全球预测。FuXi基于级联集合模型开发，它集成了多种模型的优势，并减少了预测误差的积累。使用空气温度，比湿度和位势高度的均方根误差(RMSE)和异常相关系数(ACC)评估了FuXi的性能。结果表明，与ECMWF HRES相比，FuXi在15天预报中表现出更好的性能，并显著减少了积累误差。

    Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
    
[^39]: 基于扩散的随机再生模型的风噪声降噪方法

    Wind Noise Reduction with a Diffusion-based Stochastic Regeneration Model. (arXiv:2306.12867v1 [eess.AS])

    [http://arxiv.org/abs/2306.12867](http://arxiv.org/abs/2306.12867)

    本文介绍了一种基于扩散的随机再生模型的风噪声降噪方法，该方法使得风噪声降噪效果优于其他基于神经网络的方法和纯预测和生成模型，在使用模拟和真实记录的风噪声数据集上进行了测试，并在真实记录的风噪声数据集上具有很好的泛化性能。

    

    本文介绍了一种单通道风噪音降低方法，使用了我们先前提出的结合预测和生成建模的基于扩散的随机再生模型。我们引入了一个非加性的语音噪声模型来解释膜的非线性变形，这种变形是由风流和可能的剪裁引起的。我们证明了我们的随机再生模型在使用模拟和真实记录的风噪声数据集上表现优于其他基于神经网络的风噪声降低方法以及纯预测和生成模型。我们进一步展示了所提出的方法在未见过的真实记录的风噪声数据集上具有很好的泛化性能。所提出方法的音频样本，数据生成脚本和代码可以在线上找到(https://uhh.de/inf-sp-storm-wind)。

    In this paper we present a method for single-channel wind noise reduction using our previously proposed diffusion-based stochastic regeneration model combining predictive and generative modelling. We introduce a non-additive speech in noise model to account for the non-linear deformation of the membrane caused by the wind flow and possible clipping. We show that our stochastic regeneration model outperforms other neural-network-based wind noise reduction methods as well as purely predictive and generative models, on a dataset using simulated and real-recorded wind noise. We further show that the proposed method generalizes well by testing on an unseen dataset with real-recorded wind noise. Audio samples, data generation scripts and code for the proposed methods can be found online (https://uhh.de/inf-sp-storm-wind).
    
[^40]: 通过离线预训练状态到目标Transformer视觉观察学习

    Learning from Visual Observation via Offline Pretrained State-to-Go Transformer. (arXiv:2306.12860v1 [cs.LG])

    [http://arxiv.org/abs/2306.12860](http://arxiv.org/abs/2306.12860)

    该论文提出了一个学习框架，其中使用离线预训练和内在奖励来解决视觉观察学习的挑战性问题，在Atari和Minecraft上取得了优异的实验结果。

    

    视觉观察学习旨在仅从视觉观察数据中恢复策略，是一个有前途但具有挑战性的问题。为了解决这些问题，我们提出了一个学习框架，其中第一阶段引入并离线预训练状态到目标(STG) Transformer以预测和区分演示的潜在转换。随后，在第二阶段中，STG Transformer为下游强化学习任务提供内在奖励，其中代理仅从内在奖励中学习。实验结果表明，我们提出的方法在Atari和Minecraft上优于基线，甚至在某些任务中可以达到与从环境奖励学习的策略相当的性能。

    Learning from visual observation (LfVO), aiming at recovering policies from only visual observation data, is promising yet a challenging problem. Existing LfVO approaches either only adopt inefficient online learning schemes or require additional task-specific information like goal states, making them not suited for open-ended tasks. To address these issues, we propose a two-stage framework for learning from visual observation. In the first stage, we introduce and pretrain State-to-Go (STG) Transformer offline to predict and differentiate latent transitions of demonstrations. Subsequently, in the second stage, the STG Transformer provides intrinsic rewards for downstream reinforcement learning tasks where an agent learns merely from intrinsic rewards. Empirical results on Atari and Minecraft show that our proposed method outperforms baselines and in some tasks even achieves performance comparable to the policy learned from environmental rewards. These results shed light on the potentia
    
[^41]: 基于自适应OPTICS聚类的强化联邦学习方法

    Reinforcement Federated Learning Method Based on Adaptive OPTICS Clustering. (arXiv:2306.12859v1 [cs.LG])

    [http://arxiv.org/abs/2306.12859](http://arxiv.org/abs/2306.12859)

    本文提出了一种基于自适应OPTICS聚类的强化联邦学习方法，旨在缓解不同用户终端上的数据分布不同所带来的负面影响，并有效提高了联邦学习方法的性能。

    

    联邦学习是一种分布式机器学习技术，它实现了数据隐私保护和数据共享计算之间的平衡。为了保护数据隐私，联邦学习通过在参与设备上本地执行分布式训练并将本地模型聚合成全局模型来学习共享模型。联邦学习存在的问题是，由于数据在不同用户终端上的非独立和相同分布所导致的负面影响。为了缓解这个问题，本文提出了一种基于自适应OPTICS聚类的增强型联邦聚合方法。具体来说，该方法将聚类环境视为马尔科夫决策过程，并对参数搜索方向的调整过程进行建模，以找到最佳聚类参数以达到最佳联邦聚合方法。本文的核心贡献是提出了一种适用于联邦学习的自适应OPTICS聚类算法，可有效提高联邦学习方法的性能。

    Federated learning is a distributed machine learning technology, which realizes the balance between data privacy protection and data sharing computing. To protect data privacy, feder-ated learning learns shared models by locally executing distributed training on participating devices and aggregating local models into global models. There is a problem in federated learning, that is, the negative impact caused by the non-independent and identical distribu-tion of data across different user terminals. In order to alleviate this problem, this paper pro-poses a strengthened federation aggregation method based on adaptive OPTICS clustering. Specifically, this method perceives the clustering environment as a Markov decision process, and models the adjustment process of parameter search direction, so as to find the best clus-tering parameters to achieve the best federated aggregation method. The core contribution of this paper is to propose an adaptive OPTICS clustering algorithm for federated
    
[^42]: 基于信息丢失约束的大规模公共安全时空数据高效划分方法

    Efficient Partitioning Method of Large-Scale Public Safety Spatio-Temporal Data based on Information Loss Constraints. (arXiv:2306.12857v1 [cs.LG])

    [http://arxiv.org/abs/2306.12857](http://arxiv.org/abs/2306.12857)

    本文提出了一种基于信息丢失约束的大规模公共安全时空数据高效划分方法(IFL-LSTP)，可以显著减小数据规模，同时保持模型的准确性，确保分布式存储的负载平衡，同时保持数据划分的时空接近性。

    

    大规模时空数据的存储、管理和应用在各种实际场景中广泛应用，包括公共安全。然而，由于现实世界数据的独特时空分布特征，大多数现有方法在数据时空接近度和分布式存储负载平衡方面存在限制。因此，本文提出了一种基于信息丢失约束的大规模公共安全时空数据高效划分方法(IFL-LSTP)。该IFL-LSTP模型针对大规模时空点数据，将时空划分模块(STPM)和图划分模块(GPM)相结合。该方法可以显著减小数据规模，同时保持模型的准确性，以提高划分效率。它还可以确保分布式存储的负载平衡，同时保持数据划分的时空接近性。

    The storage, management, and application of massive spatio-temporal data are widely applied in various practical scenarios, including public safety. However, due to the unique spatio-temporal distribution characteristics of re-al-world data, most existing methods have limitations in terms of the spatio-temporal proximity of data and load balancing in distributed storage. There-fore, this paper proposes an efficient partitioning method of large-scale public safety spatio-temporal data based on information loss constraints (IFL-LSTP). The IFL-LSTP model specifically targets large-scale spatio-temporal point da-ta by combining the spatio-temporal partitioning module (STPM) with the graph partitioning module (GPM). This approach can significantly reduce the scale of data while maintaining the model's accuracy, in order to improve the partitioning efficiency. It can also ensure the load balancing of distributed storage while maintaining spatio-temporal proximity of the data partitioning res
    
[^43]: MultiTASC: 面向消费者边缘级联 DNN 推理的多租户感知调度程序

    MultiTASC: A Multi-Tenancy-Aware Scheduler for Cascaded DNN Inference at the Consumer Edge. (arXiv:2306.12830v1 [cs.LG])

    [http://arxiv.org/abs/2306.12830](http://arxiv.org/abs/2306.12830)

    本文提出了 MultiTASC，一种多租户感知调度程序，它能够自适应地控制设备的转发决策函数，以优化系统吞吐量和提高延迟服务水平目标的满足率。

    

    级联系统由轻量级模型和重型高精度模型组成，通过在设备端放置轻型模型和在服务器上放置重型模型，级联模型构成了一种广泛使用的分布式推理方法。本文提出了 MultiTASC，一种多租户感知调度程序，它自适应地控制设备的转发决策函数，以最大化系统吞吐量，同时保持高准确性和低延迟。通过明确考虑设备异构性，我们的调度程序改善了延迟服务水平目标 (SLO) 满足率。

    Cascade systems comprise a two-model sequence, with a lightweight model processing all samples and a heavier, higher-accuracy model conditionally refining harder samples to improve accuracy. By placing the light model on the device side and the heavy model on a server, model cascades constitute a widely used distributed inference approach. With the rapid expansion of intelligent indoor environments, such as smart homes, the new setting of Multi-Device Cascade is emerging where multiple and diverse devices are to simultaneously use a shared heavy model on the same server, typically located within or close to the consumer environment. This work presents MultiTASC, a multi-tenancy-aware scheduler that adaptively controls the forwarding decision functions of the devices in order to maximize the system throughput, while sustaining high accuracy and low latency. By explicitly considering device heterogeneity, our scheduler improves the latency service-level objective (SLO) satisfaction rate 
    
[^44]: StrainNet: 使用SE(3)-等变图神经网络预测晶体结构弹性性能

    StrainNet: Predicting crystal structure elastic properties using SE(3)-equivariant graph neural networks. (arXiv:2306.12818v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2306.12818](http://arxiv.org/abs/2306.12818)

    该论文介绍了一种使用SE(3)-等变图神经网络预测晶体结构弹性性能的方法，可以高效地产生准确的弹性模量，并且能够预测应变能密度和相关的弹性常数，具有一定的可拓展性。

    

    准确预测晶态固体的弹性性质对于计算材料科学至关重要。然而，传统的从头算方法在计算上非常耗时，特别是对于研究具有大量原子的复杂材料时如此。我们引入了一种新颖的数据驱动方法，使用SE(3)-等变图神经网络(GNN)高效预测晶体结构的弹性性质。该方法产生了与最近的数据驱动研究相当准确的重要标量弹性模量。重要的是，我们的对称感知GNN模型还能够预测应变能密度(SED)和相关弹性常数，这些是受晶体学群影响显著的基本张量量。该模型一致区分SED张量的独立元素，符合晶体结构的对称性。最后，我们的深度学习模型具有一定的可拓展性，可用于不同类型的晶体。

    Accurately predicting the elastic properties of crystalline solids is vital for computational materials science. However, traditional atomistic scale ab initio approaches are computationally intensive, especially for studying complex materials with a large number of atoms in a unit cell. We introduce a novel data-driven approach to efficiently predict the elastic properties of crystal structures using SE(3)-equivariant graph neural networks (GNNs). This approach yields important scalar elastic moduli with the accuracy comparable to recent data-driven studies. Importantly, our symmetry-aware GNNs model also enables the prediction of the strain energy density (SED) and the associated elastic constants, the fundamental tensorial quantities that are significantly influenced by a material's crystallographic group. The model consistently distinguishes independent elements of SED tensors, in accordance with the symmetry of the crystal structures. Finally, our deep learning model possesses mea
    
[^45]: XAI-TRIS：用于量化机器学习解释性能的非线性基准测试

    XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance. (arXiv:2306.12816v1 [cs.LG])

    [http://arxiv.org/abs/2306.12816](http://arxiv.org/abs/2306.12816)

    XAI-TRIS提供了用于测试机器学习解释性能的具有挑战性的非线性基准数据集，并揭示了现有XAI方法的局限性。

    

    “可解释的”人工智能（XAI）领域已经产生了高度引用的方法，旨在使复杂的机器学习（ML）方法的决策“可理解”给人类，例如通过对输入特征进行“重要性”评分来实现。然而，缺乏正式的基础，使得无法从给定XAI方法的结果中安全地得出结论，迄今为止，也阻碍了XAI方法的理论验证和实证验证。这意味着，目前缺乏适当的解决方法来解决通常由深度神经网络解决的具有挑战性的非线性问题。本文针对三种不同的非线性分类情景制作基准数据集，其中通过设计已知重要的类条件特征，作为地面实况解释。利用新的定量指标，我们在三个深度学习模型架构上测试了广泛的XAI方法的解释性能。我们展示了流行的XAI方法的很多局限性。

    The field of 'explainable' artificial intelligence (XAI) has produced highly cited methods that seek to make the decisions of complex machine learning (ML) methods 'understandable' to humans, for example by attributing 'importance' scores to input features. Yet, a lack of formal underpinning leaves it unclear as to what conclusions can safely be drawn from the results of a given XAI method and has also so far hindered the theoretical verification and empirical validation of XAI methods. This means that challenging non-linear problems, typically solved by deep neural networks, presently lack appropriate remedies. Here, we craft benchmark datasets for three different non-linear classification scenarios, in which the important class-conditional features are known by design, serving as ground truth explanations. Using novel quantitative metrics, we benchmark the explanation performance of a wide set of XAI methods across three deep learning model architectures. We show that popular XAI met
    
[^46]: 条件生成器在限价订单簿环境中的应用：可解释性、挑战和强健性探索

    Conditional Generators for Limit Order Book Environments: Explainability, Challenges, and Robustness. (arXiv:2306.12806v1 [q-fin.TR])

    [http://arxiv.org/abs/2306.12806](http://arxiv.org/abs/2306.12806)

    本文研究了在订单簿模拟中使用条件生成模型的方法，并探索了其对输入特征的依赖性及其优点和缺点，提高了CGAN的逼真度和强健性。

    

    限价订单簿是一种基本而广泛使用的市场机制。本文研究了在订单簿模拟中使用条件生成模型的方法。这种方法因为其能够对交易代理的存在做出反应而受到关注，被认为是传统回测的替代方案。本文使用了来自Coletta等人（2022）的最先进的CGAN，探索了它对输入特征的依赖性，强调了其优点和缺点。为此，我们对模型特征及其机制进行了“对抗攻击”。然后展示了如何利用这些认识来提高CGAN的逼真度和强健性。最后，我们提出了未来工作的路线图。

    Limit order books are a fundamental and widespread market mechanism. This paper investigates the use of conditional generative models for order book simulation. For developing a trading agent, this approach has drawn recent attention as an alternative to traditional backtesting due to its ability to react to the presence of the trading agent. Using a state-of-the-art CGAN (from Coletta et al. (2022)), we explore its dependence upon input features, which highlights both strengths and weaknesses. To do this, we use "adversarial attacks" on the model's features and its mechanism. We then show how these insights can be used to improve the CGAN, both in terms of its realism and robustness. We finish by laying out a roadmap for future work.
    
[^47]: 具有局部可变测量尺度的随机变量的鲁棒统计比较

    Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement. (arXiv:2306.12803v1 [stat.ML])

    [http://arxiv.org/abs/2306.12803](http://arxiv.org/abs/2306.12803)

    本研究提出了对于具有局部可变测量尺度的随机变量的广义随机优势（GSD）顺序，并通过线性优化和不精确概率模型提出了正则化的统计检验，解决了在这些非标准空间中如何正确利用全部信息来进行比较的问题。

    

    具有局部可变测量尺度的空间，在统计学和机器学习中是相当普遍的，比如说，具有不同缩放维度的多维结构。然而，如何正确地利用这些空间中编码的全部信息，仍然被认为是一个开放性问题。我们通过考虑一个基于随机变量期望的（集合）偏序关系来解决这个问题，这些随机变量映射到这些非标准空间中。当没有或完全的基数结构时，这个偏序关系包含随机优势和期望顺序作为极端情况。我们通过线性优化导出了一个适用于我们提出的广义随机优势（GSD）顺序的（正则化的）统计检验，并通过不精确概率模型使其更为鲁棒。我们的发现用多维贫困度量、金融和医学数据进行说明。

    Spaces with locally varying scale of measurement, like multidimensional structures with differently scaled dimensions, are pretty common in statistics and machine learning. Nevertheless, it is still understood as an open question how to exploit the entire information encoded in them properly. We address this problem by considering an order based on (sets of) expectations of random variables mapping into such non-standard spaces. This order contains stochastic dominance and expectation order as extreme cases when no, or respectively perfect, cardinal structure is given. We derive a (regularized) statistical test for our proposed generalized stochastic dominance (GSD) order, operationalize it by linear optimization, and robustify it by imprecise probability models. Our findings are illustrated with data from multidimensional poverty measurement, finance, and medicine.
    
[^48]: Otter-Knowledge：不同来源的多模态知识图谱表示学习在药物发现中的基准测试。

    Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])

    [http://arxiv.org/abs/2306.12802](http://arxiv.org/abs/2306.12802)

    本研究在药物发现方面使用了多模态知识图谱表示学习，获得了最先进的结果。他们整合了来自不同来源的数据，并提供了基于这些数据的预训练模型。

    

    最近，表示学习的研究利用大量的蛋白质或分子数据库，通过无监督学习技术获得药物和蛋白质结构的知识。这些预训练表示已被证明可以显著提高后续任务的准确性，如预测药物和靶蛋白之间的亲和力。在本研究中，我们展示了通过将来自不同来源和模态的知识图谱整合到序列或SMILES表示中，可以进一步丰富表示，并在已建立的基准测试数据集上实现最先进的结果。我们提供了来自7个公共来源的预处理和整合数据，其中包括超过30M个三元组。此外，我们还提供了基于这些数据的预训练模型，以及它们在Therapeutic Data Commons (TDC)基准测试中性能报告的结果。

    Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
    
[^49]: HypeRS：构建基于超图驱动的集成推荐系统

    HypeRS: Building a Hypergraph-driven ensemble Recommender System. (arXiv:2306.12800v1 [cs.IR])

    [http://arxiv.org/abs/2306.12800](http://arxiv.org/abs/2306.12800)

    本文提出了一个新的集成推荐系统，将不同模型的预测结果结合成一个超图排名框架，是第一个使用超图排名建模集成推荐系统的。超图可以建模高阶关系。

    

    推荐系统旨在预测用户对物品的偏好。这篇论文提出一种新的集成推荐系统，将不同模型的预测结果结合成一个统一的超图排名框架，这是第一次使用超图排名建模推荐系统的集成。超图是图的一种推广，可以有效地建模高阶关系。我们通过对不同的推荐系统分配不同的超边权重来区分用户和物品之间的实际和预测连接，并在电影、音乐和新闻领域的四个数据集上进行了实验。

    Recommender systems are designed to predict user preferences over collections of items. These systems process users' previous interactions to decide which items should be ranked higher to satisfy their desires. An ensemble recommender system can achieve great recommendation performance by effectively combining the decisions generated by individual models. In this paper, we propose a novel ensemble recommender system that combines predictions made by different models into a unified hypergraph ranking framework. This is the first time that hypergraph ranking has been employed to model an ensemble of recommender systems. Hypergraphs are generalizations of graphs where multiple vertices can be connected via hyperedges, efficiently modeling high-order relations. We differentiate real and predicted connections between users and items by assigning different hyperedge weights to individual recommender systems. We perform experiments using four datasets from the fields of movie, music and news 
    
[^50]: 基于生成扩散模型的先验正则化全波形反演

    A prior regularized full waveform inversion using generative diffusion models. (arXiv:2306.12776v1 [physics.geo-ph])

    [http://arxiv.org/abs/2306.12776](http://arxiv.org/abs/2306.12776)

    本文提出了一种基于生成扩散模型的先验正则化全波形反演方法，在保持速度模型维度的同时适应于地震观测，可以在只有少量观测数据的情况下实现高分辨率反演。

    

    全波形反演（FWI）具有提供高分辨率地下模型估计的潜力，但由于观测限制（例如区域噪声、有限的震源或接收器以及带限数据），很难使用FWI得到所需的高分辨率模型。为了应对这个挑战，我们提出了一种使用生成扩散模型正则化FWI的新范式。具体而言，我们以完全无监督的方式在先前的速度模型分布上预先训练扩散模型，该分布代表了我们对地下的预期，并将FWI并入生成扩散模型的采样过程中，将其适应于地震观测。扩散模型之所以独特适合这种实现，是因为其生成过程保留了速度模型的形式和尺寸。数值实例证明，我们的方法可以在仅有微不足道的额外计算成本的情况下优于传统的FWI。即使在只有少量观测数据的情况下，我们的方法也可以提供高分辨率。

    Full waveform inversion (FWI) has the potential to provide high-resolution subsurface model estimations. However, due to limitations in observation, e.g., regional noise, limited shots or receivers, and band-limited data, it is hard to obtain the desired high-resolution model with FWI. To address this challenge, we propose a new paradigm for FWI regularized by generative diffusion models. Specifically, we pre-train a diffusion model in a fully unsupervised manner on a prior velocity model distribution that represents our expectations of the subsurface and then adapt it to the seismic observations by incorporating the FWI into the sampling process of the generative diffusion models. What makes diffusion models uniquely appropriate for such an implementation is that the generative process retains the form and dimensions of the velocity model. Numerical examples demonstrate that our method can outperform the conventional FWI with only negligible additional computational cost. Even in case
    
[^51]: 线性约束下的多臂赌博纯探索算法

    Pure Exploration in Bandits with Linear Constraints. (arXiv:2306.12774v1 [cs.LG])

    [http://arxiv.org/abs/2306.12774](http://arxiv.org/abs/2306.12774)

    本文提出了两种相对于线性约束下的多臂赌博问题都是渐进最优的算法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。

    

    本文解决多臂赌博问题中存在线性约束的情况下，如何在一定置信度下确定最优策略的问题。与标准的最优臂识别问题不同，这种情况下的最优策略可能不是确定性的，而是可能在多个臂之间进行混合。这种情况改变了问题的几何形状，我们通过信息论下界进行了描述。我们提出了两种相对于此设置都是渐进最优的算法，其中一个基于“跟踪停止”方法，另一个基于博弈理论的方法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。最后，我们提供了实证结果，验证了我们的界限，并展示了约束如何改变问题的难度。

    We address the problem of identifying the optimal policy with a fixed confidence level in a multi-armed bandit setup, when \emph{the arms are subject to linear constraints}. Unlike the standard best-arm identification problem which is well studied, the optimal policy in this case may not be deterministic and could mix between several arms. This changes the geometry of the problem which we characterize via an information-theoretic lower bound. We introduce two asymptotically optimal algorithms for this setting, one based on the Track-and-Stop method and the other based on a game-theoretic approach. Both these algorithms try to track an optimal allocation based on the lower bound and computed by a weighted projection onto the boundary of a normal cone. Finally, we provide empirical results that validate our bounds and visualize how constraints change the hardness of the problem.
    
[^52]: 基于概念意识的分布式深度学习在时间偏移下的聚类

    Concept-aware clustering for decentralized deep learning under temporal shift. (arXiv:2306.12768v1 [cs.LG])

    [http://arxiv.org/abs/2306.12768](http://arxiv.org/abs/2306.12768)

    本研究提出的算法可解决分布式深度学习中在时间偏移下的非独立同分布数据问题，并自动适应网络中不断演化的概念，相对以往方法更优。

    

    分布式深度学习需要处理来自不同客户端、可能因为时间变化而发生变化的非独立同分布的数据。尽管非独立同分布数据在分布式环境下已得到广泛研究，但时间变化却一直没有得到关注。在我们所知道的情况下，我们是首次尝试解决具有非独立同分布和动态数据的分布式学习的新颖且具有挑战性的问题。我们提出了一种新颖算法，它可以自动发现和适应网络中不断演化的概念，而无需任何先前的知识或概念数量的估计。我们在标准基准数据集上评估了我们的算法，并证明它优于以前的分布式学习方法。

    Decentralized deep learning requires dealing with non-iid data across clients, which may also change over time due to temporal shifts. While non-iid data has been extensively studied in distributed settings, temporal shifts have received no attention. To the best of our knowledge, we are first with tackling the novel and challenging problem of decentralized learning with non-iid and dynamic data. We propose a novel algorithm that can automatically discover and adapt to the evolving concepts in the network, without any prior knowledge or estimation of the number of concepts. We evaluate our algorithm on standard benchmark datasets and demonstrate that it outperforms previous methods for decentralized learning.
    
[^53]: 混合式神经辐射场：零样本物体生成与混合

    Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields. (arXiv:2306.12760v1 [cs.CV])

    [http://arxiv.org/abs/2306.12760](http://arxiv.org/abs/2306.12760)

    Blended-NeRF是一种鲁棒而灵活的编辑NeRF场景中感兴趣的区域的框架，在保持自然性与一致性的情况下，它可以将用户提供的文本提示或图像补丁的物体合成并混合到原始场景中。

    

    在3D场景中编辑局部区域或特定物体并混合到已有的NeRF场景中，由于场景表示的隐式属性，这是一个具有挑战性的问题。我们提出了Blended-NeRF，一种基于文本提示或图像补丁以及3D ROI包围盒的NeRF场景中感兴趣的区域的编辑的鲁棒而灵活的框架。我们的方法利用预先训练的语言-图像模型来引导合成朝向用户提供的文本提示或图像补丁，还利用一个已存在的NeRF场景上初始化的3D MLP模型来生成物体并将其混合到原始场景中的指定区域。我们通过将3D ROI盒局部化以实现局部编辑，并利用新颖的体积混合技术将内部合成内容无缝混合到现有场景中，以获得自然而一致的结果。

    Editing a local region or a specific object in a 3D scene represented by a NeRF is challenging, mainly due to the implicit nature of the scene representation. Consistently blending a new realistic object into the scene adds an additional level of difficulty. We present Blended-NeRF, a robust and flexible framework for editing a specific region of interest in an existing NeRF scene, based on text prompts or image patches, along with a 3D ROI box. Our method leverages a pretrained language-image model to steer the synthesis towards a user-provided text prompt or image patch, along with a 3D MLP model initialized on an existing NeRF scene to generate the object and blend it into a specified region in the original scene. We allow local editing by localizing a 3D ROI box in the input scene, and seamlessly blend the content synthesized inside the ROI with the existing scene using a novel volumetric blending technique. To obtain natural looking and view-consistent results, we leverage existin
    
[^54]: 关于生成式检索模型的鲁棒性:基于超出分布视角的研究

    On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective. (arXiv:2306.12756v1 [cs.IR])

    [http://arxiv.org/abs/2306.12756](http://arxiv.org/abs/2306.12756)

    本论文研究了生成式检索模型在超出分布（OOD）泛化方面的鲁棒性，定义了从三个方面衡量OOD鲁棒性，并分析了其与密集检索模型的比较。实验结果表明，生成式检索模型的OOD鲁棒性较弱，特别是在面向任务的超出分布场景中更为明显。针对造成鲁棒性较弱的原因，提出了潜在的解决方案。

    

    最近，生成式检索在信息检索领域日益受到关注，它通过直接生成标识符来检索文档。迄今为止，人们已经付出了很多努力来开发有效的生成式检索模型。然而，在鲁棒性方面却得到的关注较少。当一个新的检索范式进入到真实世界应用中时，衡量超出分布（OOD）泛化也是至关重要的，即生成式检索模型如何泛化到新的分布中。为了回答这个问题，我们首先从检索问题的三个方面定义OOD鲁棒性：1）查询变化；2）未知的查询类型；3）未知任务。基于这个分类法，我们进行实证研究，分析了几个代表性生成式检索模型与密集检索模型在OOD鲁棒性方面的比较。实证结果表明，生成式检索模型的OOD鲁棒性比密集检索模型弱，特别是在面向任务的OOD场景中更明显。我们进一步研究了造成生成式检索模型鲁棒性较弱的原因，并提出了改善它们OOD泛化性能的潜在解决方法。

    Recently, we have witnessed generative retrieval increasingly gaining attention in the information retrieval (IR) field, which retrieves documents by directly generating their identifiers. So far, much effort has been devoted to developing effective generative retrieval models. There has been less attention paid to the robustness perspective. When a new retrieval paradigm enters into the real-world application, it is also critical to measure the out-of-distribution (OOD) generalization, i.e., how would generative retrieval models generalize to new distributions. To answer this question, firstly, we define OOD robustness from three perspectives in retrieval problems: 1) The query variations; 2) The unforeseen query types; and 3) The unforeseen tasks. Based on this taxonomy, we conduct empirical studies to analyze the OOD robustness of several representative generative retrieval models against dense retrieval models. The empirical results indicate that the OOD robustness of generative re
    
[^55]: 超越OOD状态行动：支持交叉领域离线强化学习

    Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement Learning. (arXiv:2306.12755v1 [cs.LG])

    [http://arxiv.org/abs/2306.12755](http://arxiv.org/abs/2306.12755)

    该论文提出了一种交叉领域离线RL模型，以应对线下数据效率问题，并通过支持约束目标解决了OOD状态操作和OOD转换动态的挑战。

    

    离线强化学习旨在仅使用预先收集且固定的数据来学习策略。尽管避免了RL中耗时的在线交互，但它对于OOD状态操作提出了挑战，并且通常在训练中存在数据效率问题。该论文提出了交叉领域离线RL模型，以解决跨领域离线数据的OOD转换动态问题，并期望其提高线下数据的效率。

    Offline reinforcement learning (RL) aims to learn a policy using only pre-collected and fixed data. Although avoiding the time-consuming online interactions in RL, it poses challenges for out-of-distribution (OOD) state actions and often suffers from data inefficiency for training. Despite many efforts being devoted to addressing OOD state actions, the latter (data inefficiency) receives little attention in offline RL. To address this, this paper proposes the cross-domain offline RL, which assumes offline data incorporate additional source-domain data from varying transition dynamics (environments), and expects it to contribute to the offline data efficiency. To do so, we identify a new challenge of OOD transition dynamics, beyond the common OOD state actions issue, when utilizing cross-domain offline data. Then, we propose our method BOSA, which employs two support-constrained objectives to address the above OOD issues. Through extensive experiments in the cross-domain offline RL sett
    
[^56]: 不要太单调：放宽超参数模型中的随机线搜索

    Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models. (arXiv:2306.12747v1 [math.OC])

    [http://arxiv.org/abs/2306.12747](http://arxiv.org/abs/2306.12747)

    本文研究了在超参数模型中解决随机梯度下降（SGD）和Adam的速度问题，提出了一种非单调线搜索方法，取得更快的收敛速度和泛化性能，并结合同步的Polyak初始化步伐实现。

    

    近期的工作表明，线搜索方法可以提高现代超参数设置下的随机梯度下降（SGD）和Adam的速度。但是，由于需要（小批量）目标函数的单调减少，现有的线搜索可能会采取比必要更小的步骤。我们探索了非单调线搜索方法来放宽这个条件，并可能接受更大的步长。尽管缺乏单调递减，但我们证明与单调情况相同的快速收敛速度。我们的实验表明，非单调方法在SGD / Adam的收敛速度和泛化性能方面甚至超越了先前的单调线搜索。我们提出了一种POlyak NOnmonotone随机（PoNoS）方法，通过将非单调线搜索与Polyak初始步长结合而得到。此外，我们开发了一种新的重置技术，在大多数迭代中将回溯的数量减少到零，同时仍保持较大的初始s。

    Recent works have shown that line search methods can speed up Stochastic Gradient Descent (SGD) and Adam in modern over-parameterized settings. However, existing line searches may take steps that are smaller than necessary since they require a monotone decrease of the (mini-)batch objective function. We explore nonmonotone line search methods to relax this condition and possibly accept larger step sizes. Despite the lack of a monotonic decrease, we prove the same fast rates of convergence as in the monotone case. Our experiments show that nonmonotone methods improve the speed of convergence and generalization properties of SGD/Adam even beyond the previous monotone line searches. We propose a POlyak NOnmonotone Stochastic (PoNoS) method, obtained by combining a nonmonotone line search with a Polyak initial step size. Furthermore, we develop a new resetting technique that in the majority of the iterations reduces the amount of backtracks to zero while still maintaining a large initial s
    
[^57]: 探索节点特征和图结构多样性用于节点降采样图池化

    On Exploring Node-feature and Graph-structure Diversities for Node Drop Graph Pooling. (arXiv:2306.12726v1 [cs.LG])

    [http://arxiv.org/abs/2306.12726](http://arxiv.org/abs/2306.12726)

    该论文提出了一种名为MID的新的得分方案，通过有效维持不同的节点特征和迫使模型注意到多样的图结构，提高了节点降采样图池化技术的图层次表示效果。

    

    池化操作对于有效的图层次表示学习至关重要，其中节点降采样池化已成为一种主流的图池化技术。然而，当前节点降采样方法通常根据节点的重要性分数保留前k个节点，忽略了节点特征和图结构的多样性，因此导致子优的图层次表示。为了解决此问题，我们提出了一种新的即插即用得分方案，并称之为MID，它由一个具有两个操作的多维得分空间组成，即flipscore和Dropscore。具体而言，多维得分空间通过多个标准描述节点的重要性；flipscore鼓励维护不同的节点特征；Dropscore则迫使模型注意到多样的图结构而不是停留在显著的本地结构。

    A pooling operation is essential for effective graph-level representation learning, where the node drop pooling has become one mainstream graph pooling technology. However, current node drop pooling methods usually keep the top-k nodes according to their significance scores, which ignore the graph diversity in terms of the node features and the graph structures, thus resulting in suboptimal graph-level representations. To address the aforementioned issue, we propose a novel plug-and-play score scheme and refer to it as MID, which consists of a \textbf{M}ultidimensional score space with two operations, \textit{i.e.}, fl\textbf{I}pscore and \textbf{D}ropscore. Specifically, the multidimensional score space depicts the significance of nodes through multiple criteria; the flipscore encourages the maintenance of dissimilar node features; and the dropscore forces the model to notice diverse graph structures instead of being stuck in significant local structures. To evaluate the effectiveness
    
[^58]: 利用预训练的自监督前端实现歌唱声音自动理解任务：三个案例研究

    Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies. (arXiv:2306.12714v1 [cs.SD])

    [http://arxiv.org/abs/2306.12714](http://arxiv.org/abs/2306.12714)

    本文研究了利用自监督学习模型（SSL模型）进行歌唱声音理解任务的有效性，并展示了将自监督前端转移到目标任务可以取得更好性能的潜力。此外，SSL模型在所有任务中均优于常规监督学习模型。

    

    采用深度学习技术的数据驱动方法对自动歌唱声音理解任务，如歌手识别、歌声转录和歌唱技巧分类等方面具有表征能力，即使在具有丰富人声和噪声样本的情况下也能发挥良好的作用。然而，有限的标注数据仍然是实现令人满意的性能的重要障碍。近年来，自监督学习模型（SSL模型）在语音处理和音乐分类领域经过大量未标注数据的训练。通过微调这些模型以用于目标任务，即使在有限的训练数据下也能实现与常规监督学习相当的性能。因此，本文研究了SSL模型在各种歌唱声音识别任务中的有效性。我们报告了比较三个不同任务（即歌手识别、歌声转录和歌唱技巧分类）的SSL模型和常规监督学习模型的实验结果。我们的实验表明，SSL模型在所有任务中均优于有监督学习模型。此外，我们表明，将在大量未标注数据上训练的自监督前端转移到目标任务可以比从头开始训练取得更好的性能。这些结果说明了自监督学习在提高歌唱声音理解任务的数据效率和性能方面的潜力。

    Automatic singing voice understanding tasks, such as singer identification, singing voice transcription, and singing technique classification, benefit from data-driven approaches that utilize deep learning techniques. These approaches work well even under the rich diversity of vocal and noisy samples owing to their representation ability. However, the limited availability of labeled data remains a significant obstacle to achieving satisfactory performance. In recent years, self-supervised learning models (SSL models) have been trained using large amounts of unlabeled data in the field of speech processing and music classification. By fine-tuning these models for the target tasks, comparable performance to conventional supervised learning can be achieved with limited training data. Therefore, in this paper, we investigate the effectiveness of SSL models for various singing voice recognition tasks. We report the results of experiments comparing SSL models for three different tasks (i.e.,
    
[^59]: OptIForest: 用于异常检测的最优隔离森林算法

    OptIForest: Optimal Isolation Forest for Anomaly Detection. (arXiv:2306.12703v1 [cs.LG])

    [http://arxiv.org/abs/2306.12703](http://arxiv.org/abs/2306.12703)

    本论文针对隔离森林算法中分支因子的最优取值问题，基于隔离效率提出创新算法OptIForest，该算法结构简洁、检测性能优秀，可应用于各种异常检测场景。

    

    异常检测在诸多领域扮演着重要角色，诸如网络安全中的入侵检测、金融风险监控、人类健康监测等。根据隔离森林机制提出的一类异常检测方法由于其简洁、有效、高效而备受青睐，例如针对实际部署，iForest是最常用的检测器之一。虽然大多数隔离森林采用二进制结构，但框架LSHiForest已经证明了多叉隔离树结构可以带来更好的检测性能。然而，尚无理论工作回答关于隔离森林的最优树结构的根本和实践重要问题，即何种分支因子的隔离树结构最优。本文提出隔离效率理论来解答该问题，进而确定了一个隔离树的最优分支因子。

    Anomaly detection plays an increasingly important role in various fields for critical tasks such as intrusion detection in cybersecurity, financial risk detection, and human health monitoring. A variety of anomaly detection methods have been proposed, and a category based on the isolation forest mechanism stands out due to its simplicity, effectiveness, and efficiency, e.g., iForest is often employed as a state-of-the-art detector for real deployment. While the majority of isolation forests use the binary structure, a framework LSHiForest has demonstrated that the multi-fork isolation tree structure can lead to better detection performance. However, there is no theoretical work answering the fundamentally and practically important question on the optimal tree structure for an isolation forest with respect to the branching factor. In this paper, we establish a theory on isolation efficiency to answer the question and determine the optimal branching factor for an isolation tree. Based on
    
[^60]: 利用方差传递和学习率调节通过递增成长神经网络的加速训练

    Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation. (arXiv:2306.12700v1 [cs.LG])

    [http://arxiv.org/abs/2306.12700](http://arxiv.org/abs/2306.12700)

    本文提出一个新的神经网络增长方法，采用动态稳定化的参数化方案和学习率适应机制，节约计算预算的同时，在准确性上也取得了可比较或更好的表现。

    

    我们提出了一种高效增长神经网络的方法，其中考虑了参数化和优化策略对训练动态的影响。与现有的增长方法不同，该方法采用动态稳定化权重、激活和梯度缩放的参数化方案，并保持网络推理功能。为了解决由于不同生长阶段的子网络分布不均衡而导致的优化困难，我们提出了一种学习率适应机制，以重新平衡这些独立子组件的梯度贡献。实验结果表明，我们的方法在节约原始计算预算的同时，实现了与训练大型固定模型相当或更好的准确性。

    We develop an approach to efficiently grow neural networks, within which parameterization and optimization strategies are designed by considering their effects on the training dynamics. Unlike existing growing methods, which follow simple replication heuristics or utilize auxiliary gradient-based local optimization, we craft a parameterization scheme which dynamically stabilizes weight, activation, and gradient scaling as the architecture evolves, and maintains the inference functionality of the network. To address the optimization difficulty resulting from imbalanced training effort distributed to subnetworks fading in at different growth phases, we propose a learning rate adaption mechanism that rebalances the gradient contribution of these separate subcomponents. Experimental results show that our method achieves comparable or better accuracy than training large fixed-size models, while saving a substantial portion of the original computation budget for training. We demonstrate that
    
[^61]: 基于可精简编码器的分层DNN在带宽和资源受限的物联网系统中的灵活应用

    Slimmable Encoders for Flexible Split DNNs in Bandwidth and Resource Constrained IoT Systems. (arXiv:2306.12691v1 [cs.LG])

    [http://arxiv.org/abs/2306.12691](http://arxiv.org/abs/2306.12691)

    本文提出了一种基于可精简编码器的分层DNN分割计算方法，在数量宏大的数据传输的情况下实现了数据量的降低，并以最小的开销和时间调整计算负载和传输数据大小。

    

    在移动边缘设备上执行大型深度神经网络(DNN)需要消耗大量的关键资源，例如能量，同时也对硬件能力提出要求。在基于边缘计算的方法中，模型的执行被卸载到位于5G基础设施边缘的计算能力设备上。后一类方法的主要问题是需要在带宽受限且时变的无线链路上传输信息丰富的信号。最近的分割计算范例试图通过分发DNN模型的执行到系统的各个层上来解决这个僵局，以减少要传输的数据量，并且在移动设备上施加最小的计算负载。在这种情况下，我们提出了一种基于可精简集合编码器的新型分割计算方法。我们设计的主要优点是能够以最小的开销和时间实现实时地调整计算负载和传输数据大小的能力。

    The execution of large deep neural networks (DNN) at mobile edge devices requires considerable consumption of critical resources, such as energy, while imposing demands on hardware capabilities. In approaches based on edge computing the execution of the models is offloaded to a compute-capable device positioned at the edge of 5G infrastructures. The main issue of the latter class of approaches is the need to transport information-rich signals over wireless links with limited and time-varying capacity. The recent split computing paradigm attempts to resolve this impasse by distributing the execution of DNN models across the layers of the systems to reduce the amount of data to be transmitted while imposing minimal computing load on mobile devices. In this context, we propose a novel split computing approach based on slimmable ensemble encoders. The key advantage of our design is the ability to adapt computational load and transmitted data size in real-time with minimal overhead and time
    
[^62]: 量子增强机器学习中的对抗鲁棒性研究

    Towards quantum enhanced adversarial robustness in machine learning. (arXiv:2306.12688v1 [quant-ph])

    [http://arxiv.org/abs/2306.12688](http://arxiv.org/abs/2306.12688)

    本文讨论了将量子计算与机器学习相结合的新领域，即量子对抗机器学习，它有望提供更好的准确性，更高的计算效率和更强的对抗攻击鲁棒性。但在实现稳健的实际QAML工具方面仍存在挑战。

    

    机器学习算法是进行图像分类和特征检测等数据驱动任务的强大工具，然而它们对于对抗性样本的脆弱性——即对于被篡改以欺骗算法的输入样本——仍然是一个严峻的挑战。将机器学习与量子计算相结合，有可能提供不仅更好的准确性和计算效率，而且还能够在对抗性攻击方面具有优越的鲁棒性的工具。确实，最近的工作利用了量子机械现象来防御对抗性攻击，推动了量子对抗机器学习（QAML）领域的快速发展，并可能产生新的量子优势来源。尽管有着有希望的初步结果，但在构建稳健的实际QAML工具方面仍然存在挑战。在本综述中，我们讨论了QAML的最新进展，并确定了关键挑战。同时，我们还提出了可能确定实现QA ML路线的未来研究方向。

    Machine learning algorithms are powerful tools for data driven tasks such as image classification and feature detection, however their vulnerability to adversarial examples - input samples manipulated to fool the algorithm remains a serious challenge. The integration of machine learning with quantum computing has the potential to yield tools offering not only better accuracy and computational efficiency, but also superior robustness against adversarial attacks. Indeed, recent work has employed quantum mechanical phenomena to defend against adversarial attacks, spurring the rapid development of the field of quantum adversarial machine learning (QAML) and potentially yielding a new source of quantum advantage. Despite promising early results, there remain challenges towards building robust real-world QAML tools. In this review we discuss recent progress in QAML and identify key challenges. We also suggest future research directions which could determine the route to practicality for QA
    
[^63]: 知识图谱关系预测中的可解释表示

    Explainable Representations for Relation Prediction in Knowledge Graphs. (arXiv:2306.12687v1 [cs.LG])

    [http://arxiv.org/abs/2306.12687](http://arxiv.org/abs/2306.12687)

    提出一种新的可解释表示方法SEEK用于知识图谱关系预测，通过识别实体之间相关的共享语义方面生成一个多方面和可解释的表示，并在真实世界的蛋白质相互作用预测和基因-疾病关联性预测任务中进行了验证。

    

    知识图谱以本体论支持的语义丰富结构表示实体及其关系。基于机器学习方法探索这个数据通常依赖于知识图谱嵌入，它可产生实体的潜在表示，保留结构和本地图邻域特性，但牺牲了可解释性。然而，在像链接或关系预测等任务中，了解哪些具体特征更好地解释关系对于支持复杂或关键应用至关重要。本文提出了一种名为SEEK的新型方法，用于解释支持知识图谱关系预测的表示。它基于识别实体之间相关的共享语义方面（即子图），并学习每个子图的表示，生成一个多方面和可解释的表示。我们将SEEK评估于两个真实世界中高度复杂的关系预测任务：蛋白质相互作用预测和基因-疾病关联性预测。

    Knowledge graphs represent real-world entities and their relations in a semantically-rich structure supported by ontologies. Exploring this data with machine learning methods often relies on knowledge graph embeddings, which produce latent representations of entities that preserve structural and local graph neighbourhood properties, but sacrifice explainability. However, in tasks such as link or relation prediction, understanding which specific features better explain a relation is crucial to support complex or critical applications.  We propose SEEK, a novel approach for explainable representations to support relation prediction in knowledge graphs. It is based on identifying relevant shared semantic aspects (i.e., subgraphs) between entities and learning representations for each subgraph, producing a multi-faceted and explainable representation.  We evaluate SEEK on two real-world highly complex relation prediction tasks: protein-protein interaction prediction and gene-disease associ
    
[^64]: 利用凸性的离群值鲁棒稀疏线性模型估计

    Outlier-robust Estimation of a Sparse Linear Model Using Invexity. (arXiv:2306.12678v1 [cs.LG])

    [http://arxiv.org/abs/2306.12678](http://arxiv.org/abs/2306.12678)

    本文提出了一种组合版本的离群值鲁棒拉索方法，能够有效识别干净的样本并估计具有正确支持的稀疏回归向量。采用了一种新颖的凸松弛，具有可证明的理论保证，并在实验证实了理论有效性。

    

    本文研究了在存在离群样本的情况下估计具有正确支持的稀疏回归向量的问题。我们提出了离群值鲁棒拉索的组合版本，也可以识别出干净的样本。随后，我们使用这些干净样本进行良好的估计。我们还为组合问题提供了一种新颖的凸松弛，并为此松弛提供了可证明的理论保证。最后，我们进行实验证实了我们的理论，并将结果与标准的拉索进行了比较。

    In this paper, we study problem of estimating a sparse regression vector with correct support in the presence of outlier samples. The inconsistency of lasso-type methods is well known in this scenario. We propose a combinatorial version of outlier-robust lasso which also identifies clean samples. Subsequently, we use these clean samples to make a good estimation. We also provide a novel invex relaxation for the combinatorial problem and provide provable theoretical guarantees for this relaxation. Finally, we conduct experiments to validate our theory and compare our results against standard lasso.
    
[^65]: 预训练图像表示中虚假特征的识别和分离

    Identifying and Disentangling Spurious Features in Pretrained Image Representations. (arXiv:2306.12673v1 [cs.LG])

    [http://arxiv.org/abs/2306.12673](http://arxiv.org/abs/2306.12673)

    本文探究了如何从预训练图像表示中识别虚假特征，并提出了一个有效的线性自编码器训练方法来分离核心特征、虚假特征和其他特征。进一步，本文提出的两种虚假特征去除方法显著提高了分类性能。

    

    神经网络在预测中使用虚假的相关性，当这些相关性不成立时，导致性能下降。最近的研究表明，固定预训练表示并训练一个不使用虚假特征的分类头。我们研究预训练表示中如何表示虚假特征，并探索去除虚假特征信息的策略。考虑水鸟数据集和一些预训练表示，我们发现即使知道虚假特征的全部信息，由于表示纠缠，其去除也不是一件容易的事情。为了解决这个问题，我们提出了一种线性自编码器训练方法，将表示分为核心，虚假和其他特征。我们提出了两种有效的虚假特征去除方法，应用于编码并显着提高了最差组准确度衡量的分类性能。

    Neural networks employ spurious correlations in their predictions, resulting in decreased performance when these correlations do not hold. Recent works suggest fixing pretrained representations and training a classification head that does not use spurious features. We investigate how spurious features are represented in pretrained representations and explore strategies for removing information about spurious features. Considering the Waterbirds dataset and a few pretrained representations, we find that even with full knowledge of spurious features, their removal is not straightforward due to entangled representation. To address this, we propose a linear autoencoder training method to separate the representation into core, spurious, and other features. We propose two effective spurious feature removal approaches that are applied to the encoding and significantly improve classification performance measured by worst group accuracy.
    
[^66]: Instruct-FinGPT: 通过指令调整普适大型语言模型的金融情感分析

    Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models. (arXiv:2306.12659v1 [cs.CL])

    [http://arxiv.org/abs/2306.12659](http://arxiv.org/abs/2306.12659)

    本研究提出了一种简单而有效的指令调整方法，将少量监督式金融情感分析数据转化为指令数据，并用此方法对通用的LLM进行微调，从而在金融情感分析方面取得了显着进展。

    

    情感分析是发现金融文章、新闻和社交媒体洞察的重要工具，塑造我们对市场走向的理解。尽管大型语言模型（LLM）在金融自然语言处理（NLP）方面具有惊人的能力，但它们仍然难以准确解读数字值并抓住金融背景，从而限制了它们在预测金融情感方面的有效性。在本文中，我们介绍了一种简单而有效的指令调整方法来解决这些问题。通过将少量的监督式金融情感分析数据转化为指令数据，并用此方法对通用的LLM进行微调，我们在金融情感分析方面取得了显着的进展。在实验中，我们的方法优于最先进的监督式情感分析模型，以及广泛使用的LLMs，如ChatGPT和LLaMAs，特别是在数字理解和背景理解是关键因素的情况下。

    Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and fine-tuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension 
    
[^67]: 拟合值迭代方法求解适应结构双因果最优传输问题

    Fitted Value Iteration Methods for Bicausal Optimal Transport. (arXiv:2306.12658v1 [stat.ML])

    [http://arxiv.org/abs/2306.12658](http://arxiv.org/abs/2306.12658)

    本文提出了一种适用于双因果最优传输问题的拟合值迭代方法，能够在保证精度的同时具有良好的可扩展性，数值实验结果也证明了该方法的优越性。

    

    本文提出一种拟合值迭代方法(FVI)用于计算具有适应结构的双因果最优传输(OT)。基于动态规划的形式化表述，FVI采用函数类用于近似双因果OT中的值函数。在可集中条件和近似完备性假设下，我们使用（局部）Rademacher复杂度证明了样本复杂度。此外，我们证明了深度多层神经网络具有适当结构，满足样本复杂度证明所需的关键假设条件。数值实验表明，FVI在时间跨度增加时优于线性规划和适应性Sinkhorn方法，在保持可接受精度的同时具有很好的可扩展性。

    We develop a fitted value iteration (FVI) method to compute bicausal optimal transport (OT) where couplings have an adapted structure. Based on the dynamic programming formulation, FVI adopts a function class to approximate the value functions in bicausal OT. Under the concentrability condition and approximate completeness assumption, we prove the sample complexity using (local) Rademacher complexity. Furthermore, we demonstrate that multilayer neural networks with appropriate structures satisfy the crucial assumptions required in sample complexity proofs. Numerical experiments reveal that FVI outperforms linear programming and adapted Sinkhorn methods in scalability as the time horizon increases, while still maintaining acceptable accuracy.
    
[^68]: 学习能力和持续学习算法

    Learnability and Algorithm for Continual Learning. (arXiv:2306.12646v1 [cs.LG])

    [http://arxiv.org/abs/2306.12646](http://arxiv.org/abs/2306.12646)

    本文证明了类增量学习是可学习的，并提出了一种新的算法，实验结果表明其有效性。

    

    本文研究了类增量学习（CIL）这一具有挑战性的持续学习（CL）设置。CIL 学习了一系列由不同概念或类别组成的任务序列。任何时候，都将构建一个单一的模型，可以用于预测或分类到目前为止学习的任何类的测试实例，而不需要提供每个测试实例的任务相关信息。虽然许多技术已提出用于 CIL，但它们大多是经验性的。最近显示了，强的 CIL 系统需要强的任务内预测 (WP) 和对每个任务的强的分布外 (OOD) 检测。然而，CIL 是否真正可学习仍未知。本文表明了 CIL 是可学习的，基于此，也提出了一种新的 CIL 算法。实验结果表明了其有效性。

    This paper studies the challenging continual learning (CL) setting of Class Incremental Learning (CIL). CIL learns a sequence of tasks consisting of disjoint sets of concepts or classes. At any time, a single model is built that can be applied to predict/classify test instances of any classes learned thus far without providing any task related information for each test instance. Although many techniques have been proposed for CIL, they are mostly empirical. It has been shown recently that a strong CIL system needs a strong within-task prediction (WP) and a strong out-of-distribution (OOD) detection for each task. However, it is still not known whether CIL is actually learnable. This paper shows that CIL is learnable. Based on the theory, a new CIL algorithm is also proposed. Experimental results demonstrate its effectiveness.
    
[^69]: 解决图神经网络的局限性

    On Addressing the Limitations of Graph Neural Networks. (arXiv:2306.12640v1 [cs.LG])

    [http://arxiv.org/abs/2306.12640](http://arxiv.org/abs/2306.12640)

    本文讨论了图神经网络的两个挑战：过度平滑和异质性，提出了解决方案并展望了未来的研究方向。

    

    本文总结了关于图卷积网络（GCNs）的两个问题：过度平滑和异质性挑战，并概述了未来要探索的方向。

    This report gives a summary of two problems about graph convolutional networks (GCNs): over-smoothing and heterophily challenges, and outlines future directions to explore.
    
[^70]: 针对异常检测的目标塌缩正则化自编码器：中心的黑洞

    Targeted collapse regularized autoencoder for anomaly detection: black hole at the center. (arXiv:2306.12627v1 [cs.LG])

    [http://arxiv.org/abs/2306.12627](http://arxiv.org/abs/2306.12627)

    本文提出一种在自编码器的损失函数中添加一个轻量级的约束项，用于解决传统自编码器在异常检测中的不足，并取得了良好的表现。

    

    自编码器已广泛用于最近的异常检测技术开发中。它们的应用前提是在正常训练数据上训练自编码器后，异常输入将表现出显著的重构误差。因此，这使得正常和异常样本之间有了明显的区别。然而，在实践中观察到，自编码器可以一定程度上泛化到正常类之外，并在一些异常样本上实现较小的重构误差。为了改善性能，各种技术提出了其他组件和更复杂的训练程序。在这项工作中，我们提出了一个非常简单的替代方法：不是添加神经网络组件、涉及计算和繁琐的训练，而是通过在潜在空间中调节表示的范数，用一个计算简单的项来补充重构损失。我们方法的简单性最小化了复杂性。

    Autoencoders have been extensively used in the development of recent anomaly detection techniques. The premise of their application is based on the notion that after training the autoencoder on normal training data, anomalous inputs will exhibit a significant reconstruction error. Consequently, this enables a clear differentiation between normal and anomalous samples. In practice, however, it is observed that autoencoders can generalize beyond the normal class and achieve a small reconstruction error on some of the anomalous samples. To improve the performance, various techniques propose additional components and more sophisticated training procedures. In this work, we propose a remarkably straightforward alternative: instead of adding neural network components, involved computations, and cumbersome training, we complement the reconstruction loss with a computationally light term that regulates the norm of representations in the latent space. The simplicity of our approach minimizes th
    
[^71]: 通过重要性抽样实现有效通信的联邦学习

    Communication-Efficient Federated Learning through Importance Sampling. (arXiv:2306.12625v1 [cs.LG])

    [http://arxiv.org/abs/2306.12625](http://arxiv.org/abs/2306.12625)

    本文提出了一种通过重要性抽样实现有效通信的联邦学习方法，大大降低了发送模型更新的高通信成本，利用服务器端客户端分布和附加信息的接近关系，只需要较少的通信量即可实现。

    

    客户端向服务器发送模型更新的高通信成本是可扩展联邦学习（FL）的重要瓶颈。现有方法中，使用随机压缩方法实现了最先进的比特率-准确性折衷——其中客户端n发送来自仅为该客户端的概率分布qφ（n）的样本，服务器使用这些样本估计客户端分布的平均值。然而，这种方法没有充分利用FL的设置，其中服务器在整个训练过程中具有预数据分布pθ的附加信息，该分布与客户端分布qφ（n）在Kullback-Leibler（KL）发散方面接近。在本文中，我们利用服务器端客户端分布qφ（n)与附加信息pθ之间的这种接近关系，并提出了一种框架，该框架需要大约Dkl（qφ（n）|| pθ）位的通信量。

    The high communication cost of sending model updates from the clients to the server is a significant bottleneck for scalable federated learning (FL). Among existing approaches, state-of-the-art bitrate-accuracy tradeoffs have been achieved using stochastic compression methods -- in which the client $n$ sends a sample from a client-only probability distribution $q_{\phi^{(n)}}$, and the server estimates the mean of the clients' distributions using these samples. However, such methods do not take full advantage of the FL setup where the server, throughout the training process, has side information in the form of a pre-data distribution $p_{\theta}$ that is close to the client's distribution $q_{\phi^{(n)}}$ in Kullback-Leibler (KL) divergence. In this work, we exploit this closeness between the clients' distributions $q_{\phi^{(n)}}$'s and the side information $p_{\theta}$ at the server, and propose a framework that requires approximately $D_{KL}(q_{\phi^{(n)}}|| p_{\theta})$ bits of com
    
[^72]: 基于标签生成的增量分类学习方法

    Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])

    [http://arxiv.org/abs/2306.12619](http://arxiv.org/abs/2306.12619)

    本文提出了一种基于标签生成方法的增量分类学习（CIL）方法（VAG），大幅减少了灾难性遗忘（CF），并更好地保留了预训练模型的可推广表示。

    

    尽管预训练语言模型取得了巨大成功，但对于类别增量学习（CIL）设置，由于灾难性遗忘（CF），使用这些模型进行连续学习仍然是一个挑战。本文发现，如果将CIL定式为一个连续的标签生成问题，则可以大幅减少CF并更好地保留预训练模型的可推广表示。因此，我们提出了一种新的CIL方法（VAG），该方法还利用了词汇表的稀疏性以便于生成，并使用标签语义创建伪重播样本。实验结果表明，VAG的性能比基线大幅优越。

    Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.
    
[^73]: RobustNeuralNetworks.jl：带有认证鲁棒性的机器学习和数据驱动控制包。

    RobustNeuralNetworks.jl: a Package for Machine Learning and Data-Driven Control with Certified Robustness. (arXiv:2306.12612v1 [cs.LG])

    [http://arxiv.org/abs/2306.12612](http://arxiv.org/abs/2306.12612)

    RobustNeuralNetworks.jl是一个用Julia编写的机器学习和数据驱动控制包，它通过自然满足用户定义的鲁棒性约束条件，实现了神经网络模型的构建。

    

    神经网络通常对于微小的输入扰动非常敏感，导致出现意外或脆弱的行为。本文介绍了RobustNeuralNetworks.jl：一个Julia包，用于构建神经网络模型，该模型自然地满足一组用户定义的鲁棒性约束条件。该包基于最近提出的Recurrent Equilibrium Network（REN）和Lipschitz-Bounded Deep Network（LBDN）模型类，并旨在直接与Julia最广泛使用的机器学习包Flux.jl接口。我们讨论了模型参数化背后的理论，概述了该包，并提供了一个教程，演示了其在图像分类、强化学习和非线性状态观测器设计中的应用。

    Neural networks are typically sensitive to small input perturbations, leading to unexpected or brittle behaviour. We present RobustNeuralNetworks.jl: a Julia package for neural network models that are constructed to naturally satisfy a set of user-defined robustness constraints. The package is based on the recently proposed Recurrent Equilibrium Network (REN) and Lipschitz-Bounded Deep Network (LBDN) model classes, and is designed to interface directly with Julia's most widely-used machine learning package, Flux.jl. We discuss the theory behind our model parameterization, give an overview of the package, and provide a tutorial demonstrating its use in image classification, reinforcement learning, and nonlinear state-observer design.
    
[^74]: 常数内存注意力块

    Constant Memory Attention Block. (arXiv:2306.12599v1 [cs.LG])

    [http://arxiv.org/abs/2306.12599](http://arxiv.org/abs/2306.12599)

    本文提出了一种常数内存注意力块（CMAB），用于在常数内存中计算输出并执行更新，从而实现更高的内存效率。实验证明该方法具有与现有最先进技术相当的结果。

    

    现代基础模型体系结构依赖于注意力机制来有效捕获上下文。然而，这些方法在输入/数据点数量方面需要线性或二次内存，限制了它们在低计算领域中的适用性。在这项工作中，我们提出了常数内存注意力块（CMAB），一种新颖的通用注意力块，其在常数内存中计算其输出，并在常数计算中执行更新。我们强调了CMAB的有效性，并引入了神经过程和时间点过程的方法。在实证方面，我们展示了我们的提议方法取得了与现有最先进技术相当的结果，同时内存效率显著提高。

    Modern foundation model architectures rely on attention mechanisms to effectively capture context. However, these methods require linear or quadratic memory in terms of the number of inputs/datapoints, limiting their applicability in low-compute domains. In this work, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that computes its output in constant memory and performs updates in constant computation. Highlighting CMABs efficacy, we introduce methods for Neural Processes and Temporal Point Processes. Empirically, we show our proposed methods achieve results competitive with state-of-the-art while being significantly more memory efficient.
    
[^75]: 快速建筑损伤评估工作流：针对2023年密西西比州滚动叉口龙卷风事件的实现

    Rapid building damage assessment workflow: An implementation for the 2023 Rolling Fork, Mississippi tornado event. (arXiv:2306.12589v1 [cs.CV])

    [http://arxiv.org/abs/2306.12589](http://arxiv.org/abs/2306.12589)

    本文介绍了一种人在循环工作流，用于自然灾害后快速训练建筑损伤评估模型，并通过在2023年密西西比州滚动叉口龙卷风事件中的案例研究获得了较高的精度和召回率。

    

    自然灾害后通过高分辨率卫星图像进行快速准确的建筑损伤评估对于指导和优化第一应答者的努力至关重要。然而，由于不同的灾害损伤、卫星图像的多样性以及缺乏广泛的标记数据集等问题，以自动化方式执行此类建筑损伤评估并不容易。为了解决这些问题，本文介绍了一种人在循环工作流，用于在自然灾害后快速训练建筑损伤评估模型。该文章详细介绍了一个使用此工作流的案例研究，该工作流是与美国红十字会合作执行的，针对2023年3月密西西比州滚动叉口龙卷风事件。根据后灾情收集的地面真实数据，人在循环模型过程的输出在受损建筑方面实现了0.86的精度和0.80的召回率。这个工作流的端到端实现时间不到2个小时。

    Rapid and accurate building damage assessments from high-resolution satellite imagery following a natural disaster is essential to inform and optimize first responder efforts. However, performing such building damage assessments in an automated manner is non-trivial due to the challenges posed by variations in disaster-specific damage, diversity in satellite imagery, and the dearth of extensive, labeled datasets. To circumvent these issues, this paper introduces a human-in-the-loop workflow for rapidly training building damage assessment models after a natural disaster. This article details a case study using this workflow, executed in partnership with the American Red Cross during a tornado event in Rolling Fork, Mississippi in March, 2023. The output from our human-in-the-loop modeling process achieved a precision of 0.86 and recall of 0.80 for damaged buildings when compared to ground truth data collected post-disaster. This workflow was implemented end-to-end in under 2 hours per s
    
[^76]: 基于层级神经模拟的事件集推断

    Hierarchical Neural Simulation-Based Inference Over Event Ensembles. (arXiv:2306.12584v1 [stat.ML])

    [http://arxiv.org/abs/2306.12584](http://arxiv.org/abs/2306.12584)

    本文介绍了一种基于层级神经模拟的方法，可以在似然函数不可计算但可以通过前向模拟实现的情况下，对整个数据集进行最优概率推断，着重考虑了模型的层级结构，可以导致更紧凑的参数约束。

    

    在实际数据分析中，事件集是常见的观测值集合，它们共同约束了感兴趣的模型参数。这些模型通常具有层级结构，其中“局部”参数影响单个事件，“全局”参数影响整个数据集。我们引入了实用的方法，用于处理似然函数不可计算但可以通过前向模拟实现的情况下，对整个数据集进行最优概率推断。我们构建了似然函数（比）或后验概率的神经估计器，并展示了明确考虑模型层级结构可以导致更紧凑的参数约束。我们以物理科学为例研究了本文讨论的内容，着重于粒子物理学（粒子对撞机数据）和天体物理学（强引力透镜观测）的案例。

    When analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. Such models often have a hierarchical structure, where "local" parameters impact individual events and "global" parameters influence the entire dataset. We introduce practical approaches for optimal dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via forward modeling. We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to tighter parameter constraints. We ground our discussion using case studies from the physical sciences, focusing on examples from particle physics (particle collider data) and astrophysics (strong gravitational lensing observations).
    
[^77]: 生成数据在高维回归中的对抗训练：一项渐近研究

    Adversarial Training with Generated Data in High-Dimensional Regression: An Asymptotic Study. (arXiv:2306.12582v1 [stat.ML])

    [http://arxiv.org/abs/2306.12582](http://arxiv.org/abs/2306.12582)

    该论文研究了在高维回归中将生成数据与对抗训练相结合的方法，发现该方法可通过两阶段训练实现更好的性能表现。

    

    近年来，许多研究（例如\cite{carmon2019unlabeled,gowal2021improving,xing2022artificial}）表明通过两阶段训练方法，在对抗训练中加入带伪标签的额外真实或生成数据可以增强模型性能。本文在高维线性回归模型中对该方法的渐近行为进行了理论分析。我们发现，虽然在无岭训练中存在双峰现象，但在适当的$\mathcal{L}_2$正则化中，两阶段对抗训练可以实现更好的性能。最后，我们导出了一个特别针对两阶段训练方法的快速交叉验证公式。

    In recent years, studies such as \cite{carmon2019unlabeled,gowal2021improving,xing2022artificial} have demonstrated that incorporating additional real or generated data with pseudo-labels can enhance adversarial training through a two-stage training approach. In this paper, we perform a theoretical analysis of the asymptotic behavior of this method in high-dimensional linear regression. While a double-descent phenomenon can be observed in ridgeless training, with an appropriate $\mathcal{L}_2$ regularization, the two-stage adversarial training achieves a better performance. Finally, we derive a shortcut cross-validation formula specifically tailored for the two-stage training method.
    
[^78]: 一种通过移除生胎更新流数据在线量化的高效简单方法

    An efficient and straightforward online quantization method for a data stream through remove-birth updating. (arXiv:2306.12574v1 [cs.LG])

    [http://arxiv.org/abs/2306.12574](http://arxiv.org/abs/2306.12574)

    本文提出了一种在线量化数据流的方法，通过移除生胎更新快速适应概念漂移，可以产生最小化的死单元，并为漂移检测提供了一些有用的度量指标。

    

    网络设备的增长正在创造出大量数据，即所谓的大数据，并对有效数据分析提出了重要挑战。这些数据不断地产生，形成了动态流数据。流数据的特征可能会动态变化，这种变化被称为概念漂移。因此，处理流数据的方法必须在动态适应这些变化的同时，高效地缩减它们的体积。本文提出了一种简单的概念漂移在线向量量化方法。该方法通过移除生胎更新识别并替换概率低的单元，从而快速适应概念漂移。此外，研究结果表明，即使在概念漂移的情况下，该方法也可以产生最小化的死单元。本研究还表明一些通过所提出方法计算出来的度量指标对于漂移检测将是有益的。

    The growth of network-connected devices is creating an explosion of data, known as big data, and posing significant challenges to efficient data analysis. This data is generated continuously, creating a dynamic flow known as a data stream. The characteristics of a data stream may change dynamically, and this change is known as concept drift. Consequently, a method for handling data streams must efficiently reduce their volume while dynamically adapting to these changing characteristics. This paper proposes a simple online vector quantization method for concept drift. The proposed method identifies and replaces units with low win probability through remove-birth updating, thus achieving a rapid adaptation to concept drift. Furthermore, the results of this study show that the proposed method can generate minimal dead units even in the presence of concept drift. This study also suggests that some metrics calculated from the proposed method will be helpful for drift detection.
    
[^79]: 通过指令预测提高长期模仿的表现

    Improving Long-Horizon Imitation Through Instruction Prediction. (arXiv:2306.12554v1 [cs.LG])

    [http://arxiv.org/abs/2306.12554](http://arxiv.org/abs/2306.12554)

    本文探讨了基于语言的指令预测损失的辅助监督方式，展示了在演示数量受限的情况下，指令建模在复杂推理任务中提高了表现。

    

    复杂的长期计划及其组合性质对于基于学习的智能体来说是巨大的挑战。在低数据环境中，过度拟合抑制了泛化，并且累积误差损害了准确性。本文中，我们探讨了一种通常未使用的辅助监督方式：语言。受最近基于Transformer模型的进展的启发，我们使用指令预测损失来训练代理，以鼓励学习在高层次上操作的具有时间扩展的表示。具体而言，我们证明了在BabyAI和Crafter基准测试中，指令建模显著提高了规划环境中受限的演示数量时的表现。在进一步的分析中，我们发现指令建模对需要复杂推理的任务最为重要，而在需要简单计划的环境中则可以理解地获得更小的收益。更多细节和代码可在[https://github.com/facebookresearch/long-term-fairness]中找到。

    Complex, long-horizon planning and its combinatorial nature pose steep challenges for learning-based agents. Difficulties in such settings are exacerbated in low data regimes where over-fitting stifles generalization and compounding errors hurt accuracy. In this work, we explore the use of an often unused source of auxiliary supervision: language. Inspired by recent advances in transformer-based models, we train agents with an instruction prediction loss that encourages learning temporally extended representations that operate at a high level of abstraction. Concretely, we demonstrate that instruction modeling significantly improves performance in planning environments when training with a limited number of demonstrations on the BabyAI and Crafter benchmarks. In further analysis we find that instruction modeling is most important for tasks that require complex reasoning, while understandably offering smaller gains in environments that require simple plans. More details and code can be 
    
[^80]: 深度神经网络的有限时间李雅普诺夫指数

    Finite-time Lyapunov exponents of deep neural networks. (arXiv:2306.12548v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2306.12548](http://arxiv.org/abs/2306.12548)

    本文研究了深度神经网络的有限时间李雅普诺夫指数，发现正指数的脊线将输入空间分成不同区域，并揭示了深度网络学习能力的机制。

    

    我们计算了小的输入扰动如何影响深度神经网络的输出，探索深度网络与动力系统之间的类比，其中局部扰动的增长或衰减由有限时间李雅普诺夫指数来描述。我们显示最大指数在输入空间中形成几何结构，类似于动力系统中的相干结构。大正指数的脊线将输入空间分成网络将其与不同类别相关联的不同区域。这些脊线可视化深度网络在输入空间中构建的几何形状，揭示了其学习能力背后的基本机制。

    We compute how small input perturbations affect the output of deep neural networks, exploring an analogy between deep networks and dynamical systems, where the growth or decay of local perturbations is characterised by finite-time Lyapunov exponents. We show that the maximal exponent forms geometrical structures in input space, akin to coherent structures in dynamical systems. Ridges of large positive exponents divide input space into different regions that the network associates with different classes. These ridges visualise the geometry that deep networks construct in input space, shedding light on the fundamental mechanisms underlying their learning capabilities.
    
[^81]: 神经多重网格内存用于计算流体力学

    Neural Multigrid Memory For Computational Fluid Dynamics. (arXiv:2306.12545v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2306.12545](http://arxiv.org/abs/2306.12545)

    本文提出了一种新的数据驱动湍流流动模拟方法MGxTransformer，结合了VPTR和多重网格架构的优点，使得模拟结果更为准确和高效。

    

    湍流流动模拟在多个应用中都扮演着至关重要的角色，包括飞机和船舶设计、工业流程优化和天气预报等。本文提出了一种先进的数据驱动湍流流动模拟方法，相较于现有方法有了显著的改进。我们的方法结合了视频预测变换器（VPTR）（Ye & Bilodeau, 2022）和多重网格架构（MgConv，MgResnet）（Ke等，2017）的优点。VPTR擅长捕捉复杂的时空依赖关系和处理大型输入数据，是湍流流动预测的一个有前途的选择。与此同时，多重网格架构利用多个具有不同分辨率的网格来捕捉湍流流动的多尺度特性，从而实现更精确和高效的模拟。通过我们的实验，我们展示了我们提出的方法MGxTransformer在准确预测速度、温度和湍流方面的有效性。

    Turbulent flow simulation plays a crucial role in various applications, including aircraft and ship design, industrial process optimization, and weather prediction. In this paper, we propose an advanced data-driven method for simulating turbulent flow, representing a significant improvement over existing approaches.  Our methodology combines the strengths of Video Prediction Transformer (VPTR) (Ye & Bilodeau, 2022) and Multigrid Architecture (MgConv, MgResnet) (Ke et al., 2017). VPTR excels in capturing complex spatiotemporal dependencies and handling large input data, making it a promising choice for turbulent flow prediction. Meanwhile, Multigrid Architecture utilizes multiple grids with different resolutions to capture the multiscale nature of turbulent flows, resulting in more accurate and efficient simulations.  Through our experiments, we demonstrate the effectiveness of our proposed approach, named MGxTransformer, in accurately predicting velocity, temperature, and turbulence in
    
[^82]: 随机凸优化的内存和查询权衡

    Memory-Query Tradeoffs for Randomized Convex Optimization. (arXiv:2306.12534v1 [cs.DS])

    [http://arxiv.org/abs/2306.12534](http://arxiv.org/abs/2306.12534)

    随机凸优化需要在内存和查询之间权衡，使用 $\tilde{O}(d^2)$ 比特内存和 $\tilde{O}(d)$ 次查询的割平面方法是最优的。

    

    我们证明了任何随机一阶算法，用于在单位球上最小化一个 $d$ 维、$1$-Lipschitz 凸函数，必须使用 $\Omega(d^{2-\delta})$ 比特的内存或者进行 $\Omega(d^{1+\delta/6-o(1)})$ 次查询，其中 $\delta\in (0,1)$ 是任意常数，精度 $\epsilon$ 在 $d$ 中是准多项式小的。我们的结果意味着，使用 $\tilde{O}(d^2)$ 比特内存和 $\tilde{O}(d)$ 次查询的割平面方法，在随机一阶算法中是帕累托最优，而对于凸优化，需要二次内存才能实现最佳查询复杂度。

    We show that any randomized first-order algorithm which minimizes a $d$-dimensional, $1$-Lipschitz convex function over the unit ball must either use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$ queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$ is quasipolynomially small in $d$. Our result implies that cutting plane methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries, are Pareto-optimal among randomized first-order algorithms, and quadratic memory is required to achieve optimal query complexity for convex optimization.
    
[^83]: 半隐式去噪扩散模型（SIDDMs）

    Semi-Implicit Denoising Diffusion Models (SIDDMs). (arXiv:2306.12511v1 [cs.LG])

    [http://arxiv.org/abs/2306.12511](http://arxiv.org/abs/2306.12511)

    SIDDMs是一种新方法，通过匹配隐式和显式因子，实现在生成模型中快速收敛且一定程度上保证样本多样性和质量。

    

    尽管生成模型的数量正在增加，但在推理过程中实现快速采样而不牺牲样本多样性和质量仍然具有挑战性。现有的模型（如 DDPMS）可以提供高质量，丰富多样的样本，但受迭代步骤数量的固有限制而速度较慢。Denoising Diffusion Generative Adversarial Networks (DDGAN) 试图通过集成 GAN 模型用于扩散过程的较大跳跃来规避此限制。然而，当应用于大型数据集时，DDGAN 遇到了可扩展性限制。为了解决这些限制，我们引入了一种新的方法，通过匹配隐式和显式因子来解决问题。更具体地说，我们的方法涉及利用隐式模型来匹配嘈杂数据的边缘分布和前向扩散的显式条件分布。这种组合使我们能够有效地匹配联合去噪分布。与 DDPMS 不同，我们的半隐式去噪扩散模型（SIDDMs）可以在不影响所生成样本的多样性和质量的情况下快速收敛。

    Despite the proliferation of generative models, achieving fast sampling during inference without compromising sample diversity and quality remains challenging. Existing models such as Denoising Diffusion Probabilistic Models (DDPM) deliver high-quality, diverse samples but are slowed by an inherently high number of iterative steps. The Denoising Diffusion Generative Adversarial Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN model for larger jumps in the diffusion process. However, DDGAN encountered scalability limitations when applied to large datasets. To address these limitations, we introduce a novel approach that tackles the problem by matching implicit and explicit factors. More specifically, our approach involves utilizing an implicit model to match the marginal distributions of noisy data and the explicit conditional distribution of the forward diffusion. This combination allows us to effectively match the joint denoising distributions. Unlike DDPM
    
[^84]: 基于U-Net和Segment Anything Model的乳腺肿瘤在超声和乳腺X线图像中的对比分析

    Comparative Analysis of Segment Anything Model and U-Net for Breast Tumor Detection in Ultrasound and Mammography Images. (arXiv:2306.12510v1 [eess.IV])

    [http://arxiv.org/abs/2306.12510](http://arxiv.org/abs/2306.12510)

    研究采用U-Net和pretrained SAM两种深度学习架构，针对乳腺超声和乳腺X线图像，进行肿瘤区域的识别和分割。结果表明，U-Net模型对于不同类型的良性和恶性肿瘤的识别和分割效果更好。

    

    本研究的主要目标是开发一种能够在乳腺超声（BUS）和乳腺X线图像中识别和描绘肿瘤区域的算法。该技术采用了两种先进的深度学习架构，即U-Net和pretrained SAM，用于肿瘤分割。U-Net模型专门设计用于医学图像分割，利用其深卷积神经网络框架从输入图像中提取有意义的特征。另一方面，pretrained SAM架构引入了一种机制来捕捉空间依赖关系并生成分割结果。评估在不同良性和恶性肿瘤的注释肿瘤区域的多样化数据集上进行。结果表明，U-Net模型能够在BUS和乳腺X线图像中准确地识别和分割乳腺肿瘤，且优于pretrained SAM架构。

    In this study, the main objective is to develop an algorithm capable of identifying and delineating tumor regions in breast ultrasound (BUS) and mammographic images. The technique employs two advanced deep learning architectures, namely U-Net and pretrained SAM, for tumor segmentation. The U-Net model is specifically designed for medical image segmentation and leverages its deep convolutional neural network framework to extract meaningful features from input images. On the other hand, the pretrained SAM architecture incorporates a mechanism to capture spatial dependencies and generate segmentation results. Evaluation is conducted on a diverse dataset containing annotated tumor regions in BUS and mammographic images, covering both benign and malignant tumors. This dataset enables a comprehensive assessment of the algorithm's performance across different tumor types. Results demonstrate that the U-Net model outperforms the pretrained SAM architecture in accurately identifying and segment
    
[^85]: 深度语言网络：使用变分推断联合训练叠加LLM的提示层

    Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])

    [http://arxiv.org/abs/2306.12509](http://arxiv.org/abs/2306.12509)

    本文提出了一种称为深度语言网络（DLN）的架构，通过联合训练叠加的语言模型层（LLMs），使用变分推断算法进行提示训练，使得DLN-2的性能甚至可以与少量训练数据的GPT-4相媲美。

    

    我们将大型语言模型（LLMs）视为网络中的随机“语言层”，其中可学习的参数是每个层的自然语言“提示”。我们将两个这样的层叠加在一起，将一个层的输出馈送到下一个层。我们将这种堆叠的结构称为“深度语言网络”（DLN）。首先，我们展示如何有效地针对单层语言网络（DLN-1）执行提示优化。然后，我们展示如何训练2层DLNs（DLN-2），其中必须学习两个提示。我们认为第一层的输出是一个潜在变量，需要进行边缘化，并设计了一种联合提示训练的变分推断算法。DLN-2比单层达到更高的性能，有时即使网络中的每个LLM更小且更弱，也可以与少量训练数据的GPT-4相媲美。DLN代码是开源的：https://github.com/microsoft/deep-language-networks。

    We view large language models (LLMs) as stochastic \emph{language layers} in a network, where the learnable parameters are the natural language \emph{prompts} at each layer. We stack two such layers, feeding the output of one layer to the next. We call the stacked architecture a \emph{Deep Language Network} (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs (DLN-2), where two prompts must be learnt. We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training. A DLN-2 reaches higher performance than a single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful. The DLN code is open source: https://github.com/microsoft/deep-language-networks .
    
[^86]: 基于LIME的探索方法研究黑匣子的性能不佳区域：以败血症检测为例

    Investigating Poor Performance Regions of Black Boxes: LIME-based Exploration in Sepsis Detection. (arXiv:2306.12507v1 [cs.LG])

    [http://arxiv.org/abs/2306.12507](http://arxiv.org/abs/2306.12507)

    本文提出了一种利用LIME方法探索黑盒分类模型不佳性能区域的方法，并应用于高风险的败血症检测中。通过分析错误分类实例，确定造成模型性能差的重要特征，并识别出分类器性能不佳的区域，并计算出其错误率，这对于实现谨慎的决策具有重要意义，同时增强了机器学习模型在临床实践中的可解释性，降低关键应用场景下的风险。

    

    解释机器学习模型仍然是一个挑战，阻碍了它们在临床环境中的应用。本文提出利用局部可解释的模型不可知解释（LIME）来提供解释性地描述高风险败血症检测黑盒分类模型。通过分析被错误分类的实例，识别贡献于次优性能的重要特征。分析揭示出分类器性能不佳的区域，从而计算出这些区域内的错误率，这对于在败血症检测等关键应用场景下进行谨慎决策至关重要。本文使用eICU数据集展示了所提出方法的有效性，能够识别和可视化分类器性能不佳的区域。通过增强可解释性，我们的方法促进了机器学习模型在临床实践中的应用，实现了明智的决策，并在关键时刻降低风险。

    Interpreting machine learning models remains a challenge, hindering their adoption in clinical settings. This paper proposes leveraging Local Interpretable Model-Agnostic Explanations (LIME) to provide interpretable descriptions of black box classification models in high-stakes sepsis detection. By analyzing misclassified instances, significant features contributing to suboptimal performance are identified. The analysis reveals regions where the classifier performs poorly, allowing the calculation of error rates within these regions. This knowledge is crucial for cautious decision-making in sepsis detection and other critical applications. The proposed approach is demonstrated using the eICU dataset, effectively identifying and visualizing regions where the classifier underperforms. By enhancing interpretability, our method promotes the adoption of machine learning models in clinical practice, empowering informed decision-making and mitigating risks in critical scenarios.
    
[^87]: 带有Shuffled SGD的经验风险最小化：原始-对偶视角和改进界限

    Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective and Improved Bounds. (arXiv:2306.12498v1 [math.OC])

    [http://arxiv.org/abs/2306.12498](http://arxiv.org/abs/2306.12498)

    本文提出了带有Shuffled SGD的经验风险最小化的原始-对偶视角和改进界限，旨在解决理论和实践之间的差距。

    

    随机梯度下降（SGD）是现代机器学习中最普遍的优化方法。与每个时期从数据集中无替换随机抽样和与（可能的）重排练的经验惯例相反，SGD的理论对应通常依赖于带替换的抽样假设。仅最近才分析了采用无替换抽样的Shuffled SGD。对于具有$n$个组件和对于每个组件函数$L$-平滑性假设的凸有限和问题，在足够小的步长（$\mathcal{O}(\frac{1}{nL})$）下，存在匹配的上下界。然而，这些界限似乎过于悲观 - 实际上，预测的性能通常不比全梯度下降更好 - 并且与经验观察不符。为了缩小理论和实践之间的差距，本文将焦点从一般有限和问题集中到了...

    Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning. Contrary to the empirical practice of sampling from the datasets without replacement and with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption of sampling with replacement. It is only very recently that SGD with sampling without replacement -- shuffled SGD -- has been analyzed. For convex finite sum problems with $n$ components and under the $L$-smoothness assumption for each component function, there are matching upper and lower bounds, under sufficiently small -- $\mathcal{O}(\frac{1}{nL})$ -- step sizes. Yet those bounds appear too pessimistic -- in fact, the predicted performance is generally no better than for full gradient descent -- and do not agree with the empirical observations. In this work, to narrow the gap between the theory and practice of shuffled SGD, we sharpen the focus from general finite sum problem
    
[^88]: 精准精神医学：预测的预测(arXiv:2306.12462v1 [q-bio.QM])

    Precision psychiatry: predicting predictability. (arXiv:2306.12462v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.12462](http://arxiv.org/abs/2306.12462)

    本文讨论了精准精神医学领域的十个挑战，包括对真实世界人群进行研究、对临床结果定义的现实考虑、考虑治疗相关因素以及公平性等，提出了将重点从回顾研究转移到远景性实施的观点。

    

    精准精神医学是一个新兴领域，旨在为精神健康护理提供个性化方法。多元分析和机器学习用于创建基于临床数据(如人口统计学、症状评估、基因信息和脑成像)的结果预测模型。虽然技术创新受到了很大的重视，但由于精神健康的复杂和多样性，这些模型的成功实施面临着重大挑战。从这个角度来看，本文回顾了精准精神医学领域中的十个挑战，包括需对真实世界人群进行研究以及对临床结果定义进行现实考虑，需考虑治疗相关因素，如安慰剂效应和不遵从处方。公平性、与现行实践的前瞻性验证和预测模型的实施研究等其他关键问题目前被低估了。提出了将重点从回顾研究转移到远景性实施的观点。

    Precision psychiatry is an ermerging field that aims to provide individualized approaches to mental health care. Multivariate analysis and machine learning are used to create outcome prediction models based on clinical data such as demographics, symptom assessments, genetic information, and brain imaging. While much emphasis has been placed on technical innovation, the complex and varied nature of mental health presents significant challenges to the successful implementation of these models. From this perspective, I review ten challenges in the field of precision psychiatry, including the need for studies on real-world populations and realistic clinical outcome definitions, consideration of treatment-related factors such as placebo effects and non-adherence to prescriptions. Fairness, prospective validation in comparison to current practice and implementation studies of prediction models are other key issues that are currently understudied. A shift is proposed from retrospective studie
    
[^89]: 多级区域COVID-19深度动态流行病学建模和预测

    Deep Dynamic Epidemiological Modelling for COVID-19 Forecasting in Multi-level Districts. (arXiv:2306.12457v1 [cs.LG])

    [http://arxiv.org/abs/2306.12457](http://arxiv.org/abs/2306.12457)

    该文提出一种深度动态流行病学的模型，将流行病学方程和深度学习相结合，以实现对COVID-19在多级区域的准确预测，同时保证模型可视化机制

    

    目的：COVID-19已经全球蔓延并对整个世界产生了巨大的影响。模拟COVID-19传播状况对了解当前状态和制定干预措施至关重要。基于SEIR模型的流行病学方程模拟疾病的发展。传统的参数估计方法不能精确地拟合真实世界的数据，可能由于不同的情况，如社交距离政策和干预策略等。此外，基于学习的模型可以实现出色的拟合性能，但无法可视化机制。方法：因此，我们提出了一种深度动态流行病学（DDE）方法，将流行病学方程和深度学习优势相结合，以获得高精度和可视化。 DDE包含深度网络以适应效应函数，以基于神经ODE方法解决变量方程来模拟不断变化的情况，确保拟合性能

    Objective: COVID-19 has spread worldwide and made a huge influence across the world. Modeling the infectious spread situation of COVID-19 is essential to understand the current condition and to formulate intervention measurements. Epidemiological equations based on the SEIR model simulate disease development. The traditional parameter estimation method to solve SEIR equations could not precisely fit real-world data due to different situations, such as social distancing policies and intervention strategies. Additionally, learning-based models achieve outstanding fitting performance, but cannot visualize mechanisms. Methods: Thus, we propose a deep dynamic epidemiological (DDE) method that combines epidemiological equations and deep-learning advantages to obtain high accuracy and visualization. The DDE contains deep networks to fit the effect function to simulate the ever-changing situations based on the neural ODE method in solving variants' equations, ensuring the fitting performance o
    
[^90]: 学习条件工具变量表示用于因果效应估计

    Learning Conditional Instrumental Variable Representation for Causal Effect Estimation. (arXiv:2306.12453v1 [cs.LG])

    [http://arxiv.org/abs/2306.12453](http://arxiv.org/abs/2306.12453)

    本文提出了一种名为 DVAE.CIV 的方法，通过分离表示学习，从带有潜在混淆因素的数据中学习和分解条件 IV 和其条件集的表示，用于因果效应估计。

    

    因果推断中的一个基本挑战是从观察数据中估计治疗对感兴趣结果的因果效应。然而，因果效应估计经常受到混淆偏差的影响，这是由于未测量的混淆因素影响了治疗和结果。 工具变量 (IV) 方法是消除潜在混淆因素的混淆偏差的有效方法。然而，现有的基于 IV 的估计器需要有一个被提名的 IV，而对于条件 IV (CIV) 进行因果效应估计需要其相应的条件集。 基于这种限制，我们提出了一种名为 DVAE.CIV 的新方法，通过利用分离表示学习的优势，从具有混淆因素的数据中学习并分解 CIV 的表示和其条件集的表示，用于因果效应估计。在合成和实际数据集上的大量实验结果证明了所提出方法的有效性，包括准确性和对混淆偏差的鲁棒性。

    One of the fundamental challenges in causal inference is to estimate the causal effect of a treatment on its outcome of interest from observational data. However, causal effect estimation often suffers from the impacts of confounding bias caused by unmeasured confounders that affect both the treatment and the outcome. The instrumental variable (IV) approach is a powerful way to eliminate the confounding bias from latent confounders. However, the existing IV-based estimators require a nominated IV, and for a conditional IV (CIV) the corresponding conditioning set too, for causal effect estimation. This limits the application of IV-based estimators. In this paper, by leveraging the advantage of disentangled representation learning, we propose a novel method, named DVAE.CIV, for learning and disentangling the representations of CIV and the representations of its conditioning set for causal effect estimations from data with latent confounders. Extensive experimental results on both synthet
    
[^91]: 比较多元数据下使用深度学习模型进行波动率预测的效果

    Comparing deep learning models for volatility prediction using multivariate data. (arXiv:2306.12446v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.12446](http://arxiv.org/abs/2306.12446)

    本研究比较了使用不同深度学习模型预测多项资产波动率的效果，发现时间融合变压器及时间卷积神经网络的变体最优，可以用于实践。

    

    本研究旨在比较使用多种深度学习模型预测波动率的效果，从简单浅层的模型到更深层、更复杂的模型，并将它们与天真预测和经典GARCH模型的变化相比较。具体而言，基于GARCH模型、多层感知器、循环神经网络、时间卷积神经网络和时间融合变压器预测了五种资产（即S\&P500、纳斯达克100、黄金、白银和石油）的波动率。在大多数情况下，时间融合变压器以及时间卷积神经网络的变体胜过了经典的方法和浅层网络。这些实验被重复进行，并且竞争模型之间的差异被证明是具有统计显著性的，因此鼓励在实践中使用它们。

    This study aims at comparing several deep learning-based forecasters in the task of volatility prediction using multivariate data, proceeding from simpler or shallower to deeper and more complex models and compare them to the naive prediction and variations of classical GARCH models. Specifically, the volatility of five assets (i.e., S\&P500, NASDAQ100, gold, silver, and oil) was predicted with the GARCH models, Multi-Layer Perceptrons, recurrent neural networks, Temporal Convolutional Networks, and the Temporal Fusion Transformer. In most cases the Temporal Fusion Transformer followed by variants of Temporal Convolutional Network outperformed classical approaches and shallow networks. These experiments were repeated, and the difference between competing models was shown to be statistically significant, therefore encouraging their use in practice.
    
[^92]: 自动化说话人验证在阿尔茨海默病临床试验中的表现受哪些因素影响？

    Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials. (arXiv:2306.12444v1 [eess.AS])

    [http://arxiv.org/abs/2306.12444](http://arxiv.org/abs/2306.12444)

    本文研究了自动化说话人验证技术在阿尔茨海默症临床试验中应用的可行性，人口统计特征、音频质量标准和AD的严重程度对ASV性能会产生影响。

    

    在临床试验中检测重复的患者参与是一个重大的挑战，因为重复的患者可能会破坏试验结果的可信度和准确性，导致重大的健康和财务风险。开发准确的自动化说话人验证（ASV）模型对于验证已注册个体的身份并去除重复项来说至关重要，但是数据的大小和质量会影响ASV的表现。然而，迄今为止对于影响临床环境中ASV能力的因素的调查有限。在本文中，我们通过对从多个说话任务中获取的659个具有不同AD感染程度的参与者的语音记录数据集进行分析，探讨参与者的人口统计特征、音频质量标准和AD的严重程度对ASV性能的影响。我们的结果表明，ASV的性能：1）在男性说话者上略优于女性说话者；2）

    Detecting duplicate patient participation in clinical trials is a major challenge because repeated patients can undermine the credibility and accuracy of the trial's findings and result in significant health and financial risks. Developing accurate automated speaker verification (ASV) models is crucial to verify the identity of enrolled individuals and remove duplicates, but the size and quality of data influence ASV performance. However, there has been limited investigation into the factors that can affect ASV capabilities in clinical environments. In this paper, we bridge the gap by conducting analysis of how participant demographic characteristics, audio quality criteria, and severity level of Alzheimer's disease (AD) impact the performance of ASV utilizing a dataset of speech recordings from 659 participants with varying levels of AD, obtained through multiple speech tasks. Our results indicate that ASV performance: 1) is slightly better on male speakers than on female speakers; 2)
    
[^93]: DEPAC：一份针对抑郁症和焦虑症检测的语音语料库

    DEPAC: a Corpus for Depression and Anxiety Detection from Speech. (arXiv:2306.12443v1 [eess.AS])

    [http://arxiv.org/abs/2306.12443](http://arxiv.org/abs/2306.12443)

    本文介绍了一份新颖的语音数据集DEPAC，该数据集标记了抑郁症和焦虑症标准筛查工具上的门槛。此外，作者还提出了一组手工筛选的声学和语言特征，可以有效地识别人类语音中的精神疾病迹象。该研究为自动诊断系统的开发提供了信息丰富且平衡的语料库。

    

    心理困扰，比如抑郁症和焦虑症，对全球疾病负担的贡献最大。受到人工智能领域最新技术的影响，这些障碍的自动诊断系统可以为受影响的人们减少痛苦。这种系统的开发需要信息丰富且平衡的语料库。在这项工作中，我们介绍了一份新颖的心理困扰分析音频数据集DEPAC，基于抑郁症和焦虑症标准筛查工具上的已建立门槛进行标记。这个大型数据集包括每个个体的多个语音任务以及相关的人口统计信息。同时，我们提出了一个特征集，包括手工筛选的声学和语言特征，在人类语音中识别精神疾病迹象方面发挥了效果。最后，我们通过比较基线的性能来证明我们提出的音频语料库和特征集在预测抑郁症严重程度方面的质量和有效性。

    Mental distress like depression and anxiety contribute to the largest proportion of the global burden of diseases. Automated diagnosis systems of such disorders, empowered by recent innovations in Artificial Intelligence, can pave the way to reduce the sufferings of the affected individuals. Development of such systems requires information-rich and balanced corpora. In this work, we introduce a novel mental distress analysis audio dataset DEPAC, labeled based on established thresholds on depression and anxiety standard screening tools. This large dataset comprises multiple speech tasks per individual, as well as relevant demographic information. Alongside, we present a feature set consisting of hand-curated acoustic and linguistic features, which were found effective in identifying signs of mental illnesses in human speech. Finally, we justify the quality and effectiveness of our proposed audio corpus and feature set in predicting depression severity by comparing the performance of bas
    
[^94]: 基于词级别关系图的知识蒸馏

    Knowledge Distillation via Token-level Relationship Graph. (arXiv:2306.12442v1 [cs.LG])

    [http://arxiv.org/abs/2306.12442](http://arxiv.org/abs/2306.12442)

    本文提出了一种新方法，基于词级别关系图(TRG)，用于提高知识蒸馏的性能。通过利用TRG，学生模型可以模拟教师模型中更高级别的语义信息。同时，还引入了一种上下文损失以进一步增强学习过程。

    

    知识蒸馏是将预先训练好的教师模型中的知识传递给学生模型的一种有效技术。然而，知识传递的真正潜力还没有被充分挖掘。现有的方法主要集中在蒸馏单个信息或实例级别的关系，忽略了嵌入在词级别关系中的有价值的信息，这可能会受到长尾效应的影响。为了解决上述限制，我们提出了一种称为基于词级别关系图(TRG)的知识蒸馏的新方法，利用词级别的关系知识来提高知识蒸馏的性能。通过使用TRG，学生模型可以有效地模拟教师模型中更高级别的语义信息，从而提高了蒸馏结果。为了进一步增强学习过程，我们引入了一种称为上下文损失的词级别上下文损失，鼓励学生模型捕捉

    Knowledge distillation is a powerful technique for transferring knowledge from a pre-trained teacher model to a student model. However, the true potential of knowledge transfer has not been fully explored. Existing approaches primarily focus on distilling individual information or instance-level relationships, overlooking the valuable information embedded in token-level relationships, which may be particularly affected by the long-tail effects. To address the above limitations, we propose a novel method called Knowledge Distillation with Token-level Relationship Graph (TRG) that leverages the token-wise relational knowledge to enhance the performance of knowledge distillation. By employing TRG, the student model can effectively emulate higher-level semantic information from the teacher model, resulting in improved distillation results. To further enhance the learning process, we introduce a token-wise contextual loss called contextual loss, which encourages the student model to capture
    
[^95]: 利用人类反馈将合成医学图像与临床知识对齐

    Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback. (arXiv:2306.12438v1 [eess.IV])

    [http://arxiv.org/abs/2306.12438](http://arxiv.org/abs/2306.12438)

    本文提出了一个为了生成临床合理的医学图像而引入病理学家反馈的方法。

    

    能够捕获医学图像中微妙临床特征的生成模型，为促进临床数据共享、增强罕见病数据集、并在规模上高效地合成标注医学图像方面具有巨大潜力。然而，评估合成医学图像的质量仍然是一个挑战。虽然现代生成模型可以合成视觉逼真的医学图像，但这些图像的临床有效性可能受到质疑。领域不可知分数，如FID分数、精确度和召回率，无法融入临床知识，因此不适合评估临床意义。此外，生成模型可能以许多不可预测的方式无法合成临床合理的图像，从而难以预见潜在的失败并手动设计评分以进行检测。为了解决这些问题，本文提出了一种病理学家参与的框架，用于生成临床合理的医学图像。

    Generative models capable of capturing nuanced clinical features in medical images hold great promise for facilitating clinical data sharing, enhancing rare disease datasets, and efficiently synthesizing annotated medical images at scale. Despite their potential, assessing the quality of synthetic medical images remains a challenge. While modern generative models can synthesize visually-realistic medical images, the clinical validity of these images may be called into question. Domain-agnostic scores, such as FID score, precision, and recall, cannot incorporate clinical knowledge and are, therefore, not suitable for assessing clinical sensibility. Additionally, there are numerous unpredictable ways in which generative models may fail to synthesize clinically plausible images, making it challenging to anticipate potential failures and manually design scores for their detection. To address these challenges, this paper introduces a pathologist-in-the-loop framework for generating clinical
    
[^96]: 基于元群体和时空关注网络的流行病预测模型(MPSTAN)

    MPSTAN: Metapopulation-based Spatio-Temporal Attention Network for Epidemic Forecasting. (arXiv:2306.12436v1 [cs.LG])

    [http://arxiv.org/abs/2306.12436](http://arxiv.org/abs/2306.12436)

    提出了一种名为MPSTAN的混合模型，通过将多斑点流行病学知识融入时空模型并自适应地定义斑点间相互作用，提高流行病预测的准确性，并且使用元群体理论将斑点间相互作用纳入模型，为构建多斑点知识提供了可行的方法。该模型在真实世界数据集上表现最先进。

    

    准确的流行病预测对政府制定有效的防控措施至关重要。当前大多数时空模型不能提供稳定、准确预测多样化流行趋势的常规框架。将传染病领域知识从单个斑点到多个斑点融入神经网络，有望提高预测准确性。然而，仅依赖单个斑点知识忽视了斑点间的相互作用，而构建多斑点知识在缺乏人口流动性数据的情况下具有挑战性。为解决上述问题，我们提出了一种名为基于元群体和时空关注网络(MPSTAN)的混合模型。该模型旨在通过将多斑点流行病学知识融入时空模型并自适应地定义斑点间相互作用，提高流行病预测的准确性。此外，我们利用元群体理论将斑点间相互作用纳入模型，为构建多斑点知识提供了可行的方法。对真实世界数据集的广泛实验表明，与现有方法相比，我们的模型具有最先进的性能。

    Accurate epidemic forecasting plays a vital role for governments in developing effective prevention measures for suppressing epidemics. Most of the present spatio-temporal models cannot provide a general framework for stable, and accurate forecasting of epidemics with diverse evolution trends. Incorporating epidemiological domain knowledge ranging from single-patch to multi-patch into neural networks is expected to improve forecasting accuracy. However, relying solely on single-patch knowledge neglects inter-patch interactions, while constructing multi-patch knowledge is challenging without population mobility data. To address the aforementioned problems, we propose a novel hybrid model called Metapopulation-based Spatio-Temporal Attention Network (MPSTAN). This model aims to improve the accuracy of epidemic forecasting by incorporating multi-patch epidemiological knowledge into a spatio-temporal model and adaptively defining inter-patch interactions. Moreover, we incorporate inter-pat
    
[^97]: 应用卷积神经网络对T1静息态磁共振图像进行建模，帮助诊断强迫症

    Modeling T1 Resting-State MRI Variants Using Convolutional Neural Networks in Diagnosis of OCD. (arXiv:2306.12435v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.12435](http://arxiv.org/abs/2306.12435)

    本研究利用计算建模方法开发了一个卷积神经网络模型，通过T1静息态磁共振成像(TRS-MRI)扫描识别生物标志物，有效地区分出患有OCD和其他精神障碍的患者，准确率超过90%。

    

    强迫症是一种高度令人痛苦的疾病，其常常与前额皮层和代谢型谷氨酸受体5(mGluR5)有关。研究发现，通过测量小鼠正电子发射断层扫描的分布容积比，发现该受体信号水平更高。然而，由于需要更多实证数据，尚无法完全验证mGluR5的参与。因此，本研究使用计算建模方法，利用患有精神分裂症、抑郁症和强迫症患者的T1静息态磁共振成像(TRS-MRI)扫描，以回答有关OCD的因果因素的不足。通过这些疾病之间的交叉比较，寻找特定疾病的显著特征。本研究开发了一个卷积神经网络模型，用于识别区分患有OCD和其他精神障碍的生物标志物。结果表明，该模型可以高精度地识别患有OCD 的患者，准确率超过90%。这种方法可以作为一种有效的诊断OCD的工具，为传统诊断流程提供可靠的替代方案。

    Obsessive-compulsive disorder (OCD) presents itself as a highly debilitating disorder. The disorder has common associations with the prefrontal cortex and the glutamate receptor known as Metabotropic Glutamate Receptor 5 (mGluR5). This receptor has been observed to demonstrate higher levels of signaling from positron emission tomography scans measured by its distribution volume ratios in mice. Despite this evidence, studies are unable to fully verify the involvement of mGluR5 as more empirical data is needed. Computational modeling methods were used as a means of validation for previous hypotheses involving mGluR5. The inadequacies in relation to the causal factor of OCD were answered by utilizing T1 resting-state magnetic resonance imaging (TRS-MRI) scans of patients suffering from schizophrenia, major depressive disorder, and obsessive-compulsive disorder. Because comorbid cases often occur within these disorders, cross-comparative abilities become necessary to find distinctive chara
    
[^98]: 深度学习技术解读免疫荧光幻灯片：抗核抗体案例研究

    Interpretation of immunofluorescence slides by deep learning techniques: anti-nuclear antibodies case study. (arXiv:2306.12432v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.12432](http://arxiv.org/abs/2306.12432)

    本文介绍了一种利用深度学习技术检测免疫荧光幻灯片的方法，旨在提供高效的工具帮助医生早期检测异常，解决免疫疾病方面的医疗难题。

    

    如今，疾病数量和严重程度都在不断增加，而免疫疾病是医学领域中值得关注的领域之一，据世界卫生组织（WHO）2017年的数据显示，免疫疾病影响着全球8％的人口。本研究提供了最新的免疫疾病医疗解决方案的综述，侧重于利用现代解决方案如深度学习来在早期检测异常，为健康医生提供有效的工具。我们依赖于卷积神经网络（CNN）等先进的深度学习技术来实现我们的目标。所提出的解决方案已经在突尼斯总军事医院的免疫学部门进行了测试和评估，他们认为它是一个非常有用的工具。

    Nowadays, diseases are increasing in numbers and severity by the hour. Immunity diseases, affecting 8\% of the world population in 2017 according to the World Health Organization (WHO), is a field in medicine worth attention due to the high rate of disease occurrence classified under this category. This work presents an up-to-date review of state-of-the-art immune diseases healthcare solutions. We focus on tackling the issue with modern solutions such as Deep Learning to detect anomalies in the early stages hence providing health practitioners with efficient tools. We rely on advanced deep learning techniques such as Convolutional Neural Networks (CNN) to fulfill our objective of providing an efficient tool while providing a proficient analysis of this solution. The proposed solution was tested and evaluated by the immunology department in the Principal Military Hospital of Instruction of Tunis, which considered it a very helpful tool.
    
[^99]: 二次型赌臂机的样本复杂度：Hessian相关性界限和最优算法

    Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms. (arXiv:2306.12383v1 [cs.LG])

    [http://arxiv.org/abs/2306.12383](http://arxiv.org/abs/2306.12383)

    本文针对随机零阶优化的问题，提出了二次型目标函数及其局部几何结构的最优Hessian相关样本复杂度，从信息论角度提供Hessian相关复杂度的下界，并提供了一种Hessian无关的算法可普遍实现所有Hessian实例的渐近最优样本复杂度。

    

    在随机零阶优化中，了解如何充分利用底层目标函数的局部几何结构是一个实际相关的问题。我们考虑一种基本情况，即目标函数是二次型的，并且提供了最优Hessian相关样本复杂度的第一个紧密刻画。我们的贡献具有双重性质。首先，从信息论的角度出发，通过引入一种称为能量分配的概念来捕捉搜索算法和目标函数几何结构之间的交互，证明了Hessian相关复杂度的紧密下界。通过解决最优能量谱，得到了配套的上限。其次，算法方面，我们展示了存在一种Hessian无关的算法，能够普遍实现所有Hessian实例的渐近最优样本复杂度。我们算法能够实现的渐近最优样本复杂度对于重尾噪声分布仍然有效。

    In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called energy allocation, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributio
    
[^100]: 超越深度集成——基于分布偏移下贝叶斯深度学习的大规模评估

    Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift. (arXiv:2306.12306v1 [cs.LG])

    [http://arxiv.org/abs/2306.12306](http://arxiv.org/abs/2306.12306)

    该论文在多个具有挑战性的分类和回归任务上对现代BDL算法进行了系统性评估，重点关注了在分布偏移下的泛化能力和校准能力，并研究了一种带符号的期望校准误差版本。

    

    贝叶斯深度学习（BDL）是实现在分布偏移数据上进行良好校准预测的有前途的方法。然而，缺乏大规模调查，以系统方式评估最近的 SOTA 方法在多样、现实和具有挑战性的基准任务上的表现。为了清晰了解BDL研究的当前状况，我们在来自WILDS集合的现实世界数据集上评估现代BDL算法，包含具有挑战性的分类和回归任务，重点关注在分布偏移下的泛化性能和校准能力。我们比较了一系列大型，卷积和基于 transformer 的神经网络结构上的算法，并研究了一个带符号的期望校准误差版本，揭示出方法是过度自信还是低振幅，进一步深入研究方法的行为。此外，我们为了首次系统评估BDL在微调大型预训练模型上表现，做了更多的工作。

    Bayesian deep learning (BDL) is a promising approach to achieve well-calibrated predictions on distribution-shifted data. Nevertheless, there exists no large-scale survey that evaluates recent SOTA methods on diverse, realistic, and challenging benchmark tasks in a systematic manner. To provide a clear picture of the current state of BDL research, we evaluate modern BDL algorithms on real-world datasets from the WILDS collection containing challenging classification and regression tasks, with a focus on generalization capability and calibration under distribution shift. We compare the algorithms on a wide range of large, convolutional and transformer-based neural network architectures. In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or under-confident, providing further insight into the behavior of the methods. Further, we provide the first systematic evaluation of BDL for fine-tuning large pre-trained models, 
    
[^101]: 使用Corrector操作符增强神经算子代理非线性变分边界值问题的准确度和可靠性

    Corrector Operator to Enhance Accuracy and Reliability of Neural Operator Surrogates of Nonlinear Variational Boundary-Value Problems. (arXiv:2306.12047v1 [math.NA])

    [http://arxiv.org/abs/2306.12047](http://arxiv.org/abs/2306.12047)

    本文提供了一种基于Corrector操作符的框架，以增强神经算子代理非线性变分边界值问题的准确度和可靠性。使用该方案对于PCANet型神经算子的二维非线性扩散模型的数值实验结果显示，逼近的准确度近乎提高了两个数量级，并且还在涉及非线性d之上的拓扑优化问题中得到了探讨。

    

    本文旨在开发一类参数偏微分方程解算符的逼近方法，即通过神经算子的方式。神经算子有几个挑战，包括生成适当的训练数据、成本-准确度权衡和非平凡的超参数调整问题。神经算子准确度的不可预测性影响了它们在推理、优化和控制等后续问题中的应用。本文提出了一个基于线性变分问题的框架，给出了神经算子预测结果的校正值。与校正问题相关的算子称为校正算子。通过使用提出的方案对采用PCANet型神经算子的二维非线性扩散模型进行的数值实验结果显示，当使用校正方法对神经算子进行校正时，逼近的准确度近乎提高了两个数量级。此外，涉及非线性d之上的拓扑优化问题也得到了探讨。

    This work focuses on developing methods for approximating the solution operators of a class of parametric partial differential equations via neural operators. Neural operators have several challenges, including the issue of generating appropriate training data, cost-accuracy trade-offs, and nontrivial hyperparameter tuning. The unpredictability of the accuracy of neural operators impacts their applications in downstream problems of inference, optimization, and control. A framework is proposed based on the linear variational problem that gives the correction to the prediction furnished by neural operators. The operator associated with the corrector problem is referred to as the corrector operator. Numerical results involving a nonlinear diffusion model in two dimensions with PCANet-type neural operators show almost two orders of increase in the accuracy of approximations when neural operators are corrected using the proposed scheme. Further, topology optimization involving a nonlinear d
    
[^102]: AdCraft：一种用于搜索引擎营销优化的高级强化学习基准环境

    AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])

    [http://arxiv.org/abs/2306.11971](http://arxiv.org/abs/2306.11971)

    AdCraft是一种高级强化学习基准环境，用于模拟出价和预算变化的搜索引擎营销(SEM)活动，可用于评估和提高SEM出价和预算管理相关的RL算法的鲁棒性。

    

    本文介绍了一种新的强化学习基准环境—— AdCraft，其具有随机和非静态特性。该环境模拟了搜索引擎营销中出价和预算的动态变化。SEM是一种利用付费广告来增加网站在搜索引擎结果页面上的可见性的数字营销技术。SEM广告活动的表现取决于多个因素，包括关键字选择、广告设计、出价管理、预算调整和表现监控。深度强化学习最近被认为是一种优化SEM广告投放活动的潜在策略，但需要大量数据，在实践中可能成本高昂或不可行。我们的可定制环境使从业者能够评估和提高与SEM出价和预算管理相关的RL算法的鲁棒性，而无需付出这些成本。通过在AdCraft环境下进行一系列实验，

    We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
    
[^103]: 基于几何深度学习的结构药物设计系统综述

    A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design. (arXiv:2306.11768v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.11768](http://arxiv.org/abs/2306.11768)

    本文在系统回顾几何深度学习在结构药物设计中的最新进展，分别讨论了不同任务并按不同的几何深度学习方法进行组织。该领域的前景看好，但仍存在挑战。

    

    结构药物设计利用蛋白质的三维几何结构来识别潜在的药物候选物，在药物发现中变得越来越重要。然而，基于物理化学建模和专家领域知识的传统方法费时费力。近年来，几何深度学习的发展，可以处理和整合三维几何数据，加上类似AlphaFold的工具提供准确的蛋白质三维结构预测，极大地推动了结构药物设计的进展。在本文中，我们系统地回顾了几何深度学习在结构药物设计中的最新进展。我们从结构药物设计中的主流任务、常用的3D蛋白质表示和预测/生成模型入手，然后详细介绍每个任务的回顾（例如结合位点预测、结合构象生成、\emph{de novo} 分子设计等），并按不同的几何深度学习方法进行组织。最后，我们总结了该领域未来研究的挑战和前景。

    Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, \emph{de novo} mo
    
[^104]: FDINet：利用特征失真指数保护 DNN 模型免受模型提取攻击

    FDINet: Protecting against DNN Model Extraction via Feature Distortion Index. (arXiv:2306.11338v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.11338](http://arxiv.org/abs/2306.11338)

    FDINet是一种新颖的防御机制，该机制利用特征失真指数来保护DNN模型免受模型提取攻击，并利用FDI相似性来识别分布式提取攻击中的勾结敌人。

    

    机器学习即服务（MLaaS）平台由于其易用性、成本效益、可扩展性和快速开发能力而变得越来越受欢迎。然而，最近的研究强调了 MLaaS 中基于云的模型对模型提取攻击的脆弱性。本文介绍了 FDINET，一种利用深度神经网络（DNN）模型特征分布的新颖防御机制。具体地，通过分析对手的查询的特征分布，我们揭示了这些查询的特征分布与模型的训练集不同。基于这个关键观察，我们提出了特征失真指数（FDI），这是一种度量设计，用于定量测量接收到的查询的特征分布偏差。所提出的 FDINET 利用 FDI 训练一个二进制检测器，并利用 FDI 相似性识别分布式提取攻击中的勾结敌人。我们进行了广泛的实验来评估 FDINET 对抗模型提取攻击的效果。

    Machine Learning as a Service (MLaaS) platforms have gained popularity due to their accessibility, cost-efficiency, scalability, and rapid development capabilities. However, recent research has highlighted the vulnerability of cloud-based models in MLaaS to model extraction attacks. In this paper, we introduce FDINET, a novel defense mechanism that leverages the feature distribution of deep neural network (DNN) models. Concretely, by analyzing the feature distribution from the adversary's queries, we reveal that the feature distribution of these queries deviates from that of the model's training set. Based on this key observation, we propose Feature Distortion Index (FDI), a metric designed to quantitatively measure the feature distribution deviation of received queries. The proposed FDINET utilizes FDI to train a binary detector and exploits FDI similarity to identify colluding adversaries from distributed extraction attacks. We conduct extensive experiments to evaluate FDINET against
    
[^105]: Quilt-1M: 癌症组织学图像文字对的百万数据集

    Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11207](http://arxiv.org/abs/2306.11207)

    本文介绍了一个名为 Quilt-1M 的癌症组织学图像和文字对的百万数据集，并利用 YouTube 上的专家医生教程视频为主要来源。这个数据集将使得癌症组织学领域的表征学习取得类似于其他领域的进展。

    

    多模态应用的加速使得在线图像和文字数据大量涌现，但医学领域（特别是癌症组织学）类似的数据却很稀少，这阻碍了医学领域的进展。本文利用YouTube上的专家医生教程视频，从中选择了 1,087 小时的医学组织学视频，以此自动筛选出共包含 768,826 个癌症组织学图像及其对应的文字对的 Quilt 数据集。

    Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has halted comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of $768,826$ image and text pairs. Quilt was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine Quilt with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even l
    
[^106]: 采用强化学习技术增强变分量子状态对角化

    Enhancing variational quantum state diagonalization using reinforcement learning techniques. (arXiv:2306.11086v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2306.11086](http://arxiv.org/abs/2306.11086)

    本研究采用强化学习技术，通过新的编码方法来优化量子状态对角化所需的电路深度，从而提高其在近期量子硬件上的应用性能。

    

    变分量子算法的发展对于 NISQ 计算机的应用至关重要。这种算法需要短的量子电路，这种电路更易于在近期硬件上实现，也已经开发出了许多这样的方法。其中一个特别有趣的算法是所谓的变分对角化方法，它是一种重要的算法子例程，可以直接用于处理以量子状态编码的数据。特别地，它可以用于分辨量子态的特征，例如系统的纠缠性质，或者在量子机器学习算法中使用。在本研究中，我们利用强化学习解决在量子状态对角化任务中所需电路非常浅的设计问题。为了实现这一点，我们利用一种新的编码方法，可以利用强化学习方法解决电路深度优化问题。我们证明，我们的方法有可能显著减少所需的电路深度，从而使其更适用于近期量子硬件。

    The development of variational quantum algorithms is crucial for the application of NISQ computers. Such algorithms require short quantum circuits, which are more amenable to implementation on near-term hardware, and many such methods have been developed. One of particular interest is the so-called the variational diagonalization method, which constitutes an important algorithmic subroutine, and it can be used directly for working with data encoded in quantum states. In particular, it can be applied to discern the features of quantum states, such as entanglement properties of a system, or in quantum machine learning algorithms. In this work, we tackle the problem of designing a very shallow quantum circuit, required in the quantum state diagonalization task, by utilizing reinforcement learning. To achieve this, we utilize a novel encoding method that can be used to tackle the problem of circuit depth optimization using a reinforcement learning approach. We demonstrate that our approach
    
[^107]: 基于潜在扩散模型的文本驱动Foley音效生成

    Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.10359](http://arxiv.org/abs/2306.10359)

    本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。我们通过迁移学习对系统进行微调，并引入可训练的层来改善文本嵌入，同时也改进了生成的波形。

    

    Foley音效生成旨在为多媒体内容生成背景音效。先前的模型通常使用大量有标签的开发集作为输入（例如，单个数字或one-hot向量）。本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。为了缓解数据稀缺问题，我们的模型首先使用大规模数据集进行预训练，然后通过对比语言-音频配对（CLAP）技术进行迁移学习来对该任务进行微调。我们观察到，文本编码器提取的特征嵌入可以显著影响生成模型的性能。因此，我们在编码器之后引入可训练的层来改善编码器产生的文本嵌入。此外，我们通过同时生成多个候选音频片段并选择最佳片段来进一步改进生成的波形，最佳片段是根据嵌入之间相似性得分确定的。

    Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
    
[^108]: 联邦少样本学习

    Federated Few-shot Learning. (arXiv:2306.10234v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10234](http://arxiv.org/abs/2306.10234)

    本研究提出了一种名为“联邦少样本学习”的新问题，旨在解决联邦学习在少样本数据上的性能问题。我们提出了一个简单而有效的框架，使用特征提取和任务适应模块以及注意力机制来提高模型对于少样本客户端的泛化能力，在各种数据集上取得了最先进的联邦少样本学习性能。

    

    联邦学习使得多个客户端可以在不交换本地数据的情况下协作学习一个机器学习模型。然而，现有的方法通常假设每个客户端都有足够的数据用于训练，而事实上某些客户端可能只有有限数量的样本，即少样本数据。这种情况下，现有的联邦学习方法可能在这些客户端上遇到显著的性能下降。本文提出了一个处理这一问题的框架，命名为联邦少样本学习。我们设计了特征提取模块和任务适应模块来提高模型对于少样本客户端的泛化能力，并利用注意力机制来动态地调整每个客户端在训练过程中的重要性。实验结果表明，我们的方法在各种数据集上均取得了最先进的联邦少样本学习性能。

    Federated Learning (FL) enables multiple clients to collaboratively learn a machine learning model without exchanging their own local data. In this way, the server can exploit the computational power of all clients and train the model on a larger set of data samples among all clients. Although such a mechanism is proven to be effective in various fields, existing works generally assume that each client preserves sufficient data for training. In practice, however, certain clients may only contain a limited number of samples (i.e., few-shot samples). For example, the available photo data taken by a specific user with a new mobile device is relatively rare. In this scenario, existing FL efforts typically encounter a significant performance drop on these clients. Therefore, it is urgent to develop a few-shot model that can generalize to clients with limited data under the FL scenario. In this paper, we refer to this novel problem as federated few-shot learning. Nevertheless, the problem re
    
[^109]: 虚假黎明：重新评估谷歌强化学习在芯片宏观布局中的应用

    The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])

    [http://arxiv.org/abs/2306.09633](http://arxiv.org/abs/2306.09633)

    谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。

    

    谷歌2021年在《自然》杂志上发表的有关使用强化学习设计芯片的论文，因为所声称的结果缺乏充分的文件记录和关键步骤的说明，引发争议并受到媒体的批评报道。 而两项独立的评估填补了空白，证明谷歌强化学习落后于人类设计师、落后于一种众所周知的算法（模拟退火），并且还落后于普遍可用的商业软件。交叉检查的数据表明，由于行为、分析和报告中的错误，该《自然》文章的完整性受到了严重的损害。

    Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
    
[^110]: 从零开始实现红队对抗语言模型的探索与建立

    Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])

    [http://arxiv.org/abs/2306.09442](http://arxiv.org/abs/2306.09442)

    本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。

    

    部署大型语言模型（LLMs）可能会产生有害输出，例如有毒或不诚实陈述。先前的研究已经引入了工具以调查有害输出，以识别和减轻这些风险。虽然这是确保语言模型安全的有价值步骤，但这些方法通常依赖于现有的针对不希望的输出的分类器。这限制了它们在只有预先知道有害行为类型的情况下的应用。然而，这跳过了红队行动的核心挑战：开发模型可能展示的行为的上下文理解。此外，当这样的分类器已经存在时，红队行动的边际价值有限，因为分类器可以用于过滤训练数据或模型输出。本文考虑在假设对手从高级、抽象的不良行为规范出发的情况下进行红队行动。红队应该在精化/扩展此规范的同时对抗该模型。

    Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
    
[^111]: 基于RANS-PINN的模拟代理预测湍流流动

    RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows. (arXiv:2306.06034v1 [cs.LG])

    [http://arxiv.org/abs/2306.06034](http://arxiv.org/abs/2306.06034)

    本研究提出了RANS-PINN模型，通过引入2方程涡粘度模型，可预测高雷诺数湍流流动中的流场，从而提高流体动力学模拟计算效率。

    

    物理知识引导的神经网络（PINN）为建立由微分方程控制的动态系统的代理模型提供了框架。 PINN在学习过程中会通过损失函数中的物理基础正则化项来增强泛化性能。由于模拟由偏微分方程（PDEs）控制的动态可能计算成本过高，PINN已经在学习由Navier-Stokes方程控制的液体流动问题的参数代理方面广受欢迎。在这项工作中，我们介绍了RANS-PINN，一种修改后的PINN框架，用于预测高雷诺数湍流流动中的流场（即速度和压力）。为了考虑湍流引入的额外复杂性，RANS-PINN采用基于雷诺平均Navier-Stokes（RANS）的2方程涡粘度模型。此外，我们采用了一种新的训练方法，确保各个组成部分的有效初始化和平衡。

    Physics-informed neural networks (PINNs) provide a framework to build surrogate models for dynamical systems governed by differential equations. During the learning process, PINNs incorporate a physics-based regularization term within the loss function to enhance generalization performance. Since simulating dynamics controlled by partial differential equations (PDEs) can be computationally expensive, PINNs have gained popularity in learning parametric surrogates for fluid flow problems governed by Navier-Stokes equations. In this work, we introduce RANS-PINN, a modified PINN framework, to predict flow fields (i.e., velocity and pressure) in high Reynolds number turbulent flow regime. To account for the additional complexity introduced by turbulence, RANS-PINN employs a 2-equation eddy viscosity model based on a Reynolds-averaged Navier-Stokes (RANS) formulation. Furthermore, we adopt a novel training approach that ensures effective initialization and balance among the various component
    
[^112]: 可自我解释的时间序列预测与反事实解释

    Self-Interpretable Time Series Prediction with Counterfactual Explanations. (arXiv:2306.06024v1 [cs.LG])

    [http://arxiv.org/abs/2306.06024](http://arxiv.org/abs/2306.06024)

    本文提出了一种自我解释的时间序列预测模型CounTS，该模型可以生成反事实和可操作的解释，适用于关键领域如医疗和自动驾驶等。与现有方法不同，该模型为可解释性建模做出了贡献。

    

    可解释的时间序列预测对于像医疗和自动驾驶等安全关键领域至关重要。本文提出了一种不同于现有方法的思路，旨在开发出一种自我解释的模型，被称为Counterfactual Time Series（CounTS），该模型针对时间序列预测生成反事实和可操作的解释。具体而言，我们形式化了时间序列反事实解释的问题，建立了相应的评估协议，并提出了一种带有时间序列绑架、行动和预测反事实推理能力的变分贝叶斯深度学习模型。与最先进的基线相比，我们的自我解释模型可以生成更好的反事实解释，同时保持相当的预测准确性。

    Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.
    
[^113]: Mixed-TD: 基于层特定张量分解的高效神经网络加速器

    Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition. (arXiv:2306.05021v1 [cs.LG])

    [http://arxiv.org/abs/2306.05021](http://arxiv.org/abs/2306.05021)

    本文提出了一种基于层特定张量分解的神经网络加速器Mixed-TD，采用混合方式的SVD和CPD方法，实现了高压缩率，同时保持了与原始神经网络类似的准确性，并通过动态映射方法实现了对可用片上存储器和计算资源的有效利用。

    

    神经网络设计相当多样化，从VGG到ResNet，从卷积神经网络到变换器。为了设计效率高的加速器，许多工作采用了基于数据流的、层间流水线结构的体系结构，针对每一层进行了自定义硬件设计，实现了超高的吞吐量和低延迟。神经网络部署到此类数据流体系结构加速器上通常受可用片上内存的限制，因为预加载神经网络的权重到片上以最大化系统性能是理想的。为了解决这个问题，网络通常会通过修剪、量化和张量分解等方法进行压缩。本文提出了一种将CNN映射到FPGA上的框架，基于一种新颖的张量分解方法Mixed-TD。该方法采用了混合方式的层特定奇异值分解（SVD）和典型多项式分解（CPD），在保持原始神经网络准确性的同时实现了高压缩率。Mixed-TD框架采用动态映射方法，以实现有效地利用可用的片上存储器和计算资源。实验结果表明，Mixed-TD框架在减少内存占用和计算周期方面能够显著提高性能，同时保持与原始未压缩神经网络相似的准确性。

    Neural Network designs are quite diverse, from VGG-style to ResNet-style, and from Convolutional Neural Networks to Transformers. Towards the design of efficient accelerators, many works have adopted a dataflow-based, inter-layer pipelined architecture, with a customised hardware towards each layer, achieving ultra high throughput and low latency. The deployment of neural networks to such dataflow architecture accelerators is usually hindered by the available on-chip memory as it is desirable to preload the weights of neural networks on-chip to maximise the system performance. To address this, networks are usually compressed before the deployment through methods such as pruning, quantization and tensor decomposition. In this paper, a framework for mapping CNNs onto FPGAs based on a novel tensor decomposition method called Mixed-TD is proposed. The proposed method applies layer-specific Singular Value Decomposition (SVD) and Canonical Polyadic Decomposition (CPD) in a mixed manner, achi
    
[^114]: 离散图扩散中不同收敛先验的复杂偏好

    Complex Preferences for Different Convergent Priors in Discrete Graph Diffusion. (arXiv:2306.02957v1 [cs.LG])

    [http://arxiv.org/abs/2306.02957](http://arxiv.org/abs/2306.02957)

    本研究探讨了离散扩散核如何影响图的扩散模型的性能，结果表明选择正确的收敛先验对于扩散模型的生成性能至关重要。

    

    扩散模型已经取得了在生成许多不同类型的数据，包括图像、文本和视频方面的最先进表现。尽管它们很成功，但对于基础扩散过程和最终收敛先验如何影响生成的性能进行的研究有限；此研究也仅限于连续数据类型和基于分数的扩散框架。我们探讨了不同离散扩散核（收敛到不同的先验分布）如何影响图的扩散模型的性能。为此，我们开发了一种新的离散扩散核系列公式，可以轻松调整以收敛到不同的伯努利先验，并研究这些不同的核对生成性能的影响。我们表明，生成的图的质量对使用的先验很敏感，最优选择不能用明显的统计数据或指标来解释，这挑战了扩散模型的直觉假设。我们的结果表明，在离散数据上，选择正确的收敛先验对于扩散模型的生成性能至关重要。

    Diffusion models have achieved state-of-the-art performance in generating many different kinds of data, including images, text, and videos. Despite their success, there has been limited research on how the underlying diffusion process and the final convergent prior can affect generative performance; this research has also been limited to continuous data types and a score-based diffusion framework. To fill this gap, we explore how different discrete diffusion kernels (which converge to different prior distributions) affect the performance of diffusion models for graphs. To this end, we developed a novel formulation of a family of discrete diffusion kernels which are easily adjustable to converge to different Bernoulli priors, and we study the effect of these different kernels on generative performance. We show that the quality of generated graphs is sensitive to the prior used, and that the optimal choice cannot be explained by obvious statistics or metrics, which challenges the intuiti
    
[^115]: 评估有噪声判别器对未标记数据的流式算法 -- 二元分类

    Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification. (arXiv:2306.01726v1 [stat.ML])

    [http://arxiv.org/abs/2306.01726](http://arxiv.org/abs/2306.01726)

    本文提出了两种代数评估器来估计未标记数据中有噪声二元分类器的性能。其中，第二种评估器的正确性被保证。作者通过利用独立评估器无法返回合理估计的失败，缓解了委托/代理监控悖论，并通过搜索来寻找几乎无误差的三元组。

    

    本文将对未标记数据中的有噪声二元分类器的评估作为流式任务进行研究: 给定一个分类器决策的数据草图，估计标签的真实流行度以及每个分类器对它们的准确度。本文构建了两种完全代数化的评估器来实现这一目标。两种评估器都基于分类器产生独立错误的假设。第一种是基于多数投票的。而第二种则是本文的主要贡献，并被保证是正确的。但是如何确保分类器在任何给定的测试中是独立的呢？本文通过利用独立评估器无法返回合理估计的失败来缓解这个委托/代理监控悖论。通过利用代数故障模式来拒绝太相关的评估集合，使用 \texttt{adult}，\texttt{mushroom} 和 \texttt{two-norm} 数据集对一组几乎无误差三元组进行了实证搜索。这些搜索通过构建评估空间中的表面来进行精细化。

    The evaluation of noisy binary classifiers on unlabeled data is treated as a streaming task: given a data sketch of the decisions by an ensemble, estimate the true prevalence of the labels as well as each classifier's accuracy on them. Two fully algebraic evaluators are constructed to do this. Both are based on the assumption that the classifiers make independent errors. The first is based on majority voting. The second, the main contribution of the paper, is guaranteed to be correct. But how do we know the classifiers are independent on any given test? This principal/agent monitoring paradox is ameliorated by exploiting the failures of the independent evaluator to return sensible estimates. A search for nearly error independent trios is empirically carried out on the \texttt{adult}, \texttt{mushroom}, and \texttt{two-norm} datasets by using the algebraic failure modes to reject evaluation ensembles as too correlated. The searches are refined by constructing a surface in evaluation spa
    
[^116]: 使用自适应流采样平衡训练能量基模型

    Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])

    [http://arxiv.org/abs/2306.00684](http://arxiv.org/abs/2306.00684)

    本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。

    

    能量基模型 (EBM) 是一种直接参数化未标准化对数密度的多功能密度估计模型。EBM 非常灵活，但缺乏模型的规范化常量，使模型的似然函数计算不可行。近年来，已经提出了许多近似采样器和变分推理技术来估计似然函数梯度进行训练。这些技术在生成样本方面表现出色，但对于估计密度的统计精度，例如确定数据集中不同类的相对重要性，却付出了很少的关注。在本文中，我们提出了一种新的最大似然训练算法，使用一种不同类型的生成模型，归一化流 (NF)，这种模型最近被提出以便于采样。我们的方法在训练过程中将 NF 拟合到 EBM 上，以便 NF 辅助下的采样方案能够始终为 EBM 提供准确的梯度，最终提高模型的统计精度。实验结果表明，与传统 EBM 训练技术相比，我们的方法产生了更高质量的样本和更好的生成性能。

    Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim
    
[^117]: 利用化合物互连的多种挥发性有机化合物的超分辨率

    Multi-BVOC Super-Resolution Exploiting Compounds Inter-Connection. (arXiv:2305.14180v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.14180](http://arxiv.org/abs/2305.14180)

    本文提出了一种利用多种化合物贡献对粗糙BVOC排放地图进行超分辨的策略，实验结果表明该方法可以提高超分辨率性能。

    

    陆地生态系统排放到地球大气中的生物挥发性有机化合物（BVOC）是大气化学的重要组成部分。由于测量的稀缺性，可靠的BVOC排放地图可以提供更密集的数据，以供大气化学、气候和空气质量模型使用。在本研究中，我们提出了一种利用不同化合物贡献同时超分辨粗糙BVOC排放地图的策略。为此，我们首先准确地调查几种BVOC物种之间的空间相互作用。然后，我们利用发现的相似性建立了一个Multi-Image Super-Resolution（MISR）系统，其中与不同化合物相关联的多个排放地图被集成以提高超分辨率（SR）性能。我们比较了不同配置的物种和合并BVOC数量的方法。我们的实验结果表明，将BVOC关系纳入过程中可以提高SR性能。

    Biogenic Volatile Organic Compounds (BVOCs) emitted from the terrestrial ecosystem into the Earth's atmosphere are an important component of atmospheric chemistry. Due to the scarcity of measurement, a reliable enhancement of BVOCs emission maps can aid in providing denser data for atmospheric chemical, climate, and air quality models. In this work, we propose a strategy to super-resolve coarse BVOC emission maps by simultaneously exploiting the contributions of different compounds. To this purpose, we first accurately investigate the spatial inter-connections between several BVOC species. Then, we exploit the found similarities to build a Multi-Image Super-Resolution (MISR) system, in which a number of emission maps associated with diverse compounds are aggregated to boost Super-Resolution (SR) performance. We compare different configurations regarding the species and the number of joined BVOCs. Our experimental results show that incorporating BVOCs' relationship into the process can 
    
[^118]: ToolkenGPT：通过工具嵌入扩充冻结语言模型

    ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])

    [http://arxiv.org/abs/2305.11554](http://arxiv.org/abs/2305.11554)

    本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。

    

    将大型语言模型与外部工具结合起来解决复杂问题已成为一种有前途的方法。然而，传统方法需要用工具演示数据对LLM进行微调，既费时又受限于预定义的工具集。最近的上下文学习范例缓解了这些问题，但是有限的上下文长度只允许演示几次，导致对工具的理解不够充分。此外，当有大量工具可供选择时，上下文学习可能完全无法正常工作。在本文中，我们提出了一种$\textbf{ToolkenGPT}$的替代方法，将两种方法的优点结合起来。我们的方法将每个$\underline{工具}$表示为一个$\underline{token}$（$\textit{toolken}$），并为其学习一个嵌入，使得工具调用与生成常规单词标记的方式相同。一旦触发了toolken，LLM被提示完成工具执行所需的参数。ToolkenGPT提供了以下贡献：1）引入了toolken的概念，以扩充LLM与外部工具的交互，2）提出了一种新的学习范例，利用tool embeddings实现无缝交互，3）在各种下游任务上展示了我们方法的有效性。

    Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
    
[^119]: CosmoPower-JAX:利用可微的宇宙模拟器进行高维贝叶斯推断

    CosmoPower-JAX: high-dimensional Bayesian inference with differentiable cosmological emulators. (arXiv:2305.06347v1 [astro-ph.CO])

    [http://arxiv.org/abs/2305.06347](http://arxiv.org/abs/2305.06347)

    CosmoPower-JAX使用可微的宇宙模拟器进行高维贝叶斯推断，其采用JAX的特性以及GPU技术加速参数估计，可以有效地探索高维参数空间并在短时间内获得准确的参数和后验分布。

    

    我们介绍了CosmoPower-JAX，这是CosmoPower框架的一个基于JAX的实现，通过构建宇宙功率谱的神经仿真器来加速宇宙学推断。我们展示了如何使用JAX的自动微分、批量评估和即时编译特性，并在图形处理器（GPU）上运行推断流水线，通过先进的基于梯度的采样技术将参数估计加速数倍。这些可以用于高效地探索高维参数空间，例如用于下一代宇宙学调查分析的空间。我们展示了CosmoPower-JAX在两个模拟的第四阶段配置中的准确性和计算效率。我们首先考虑了一个进行37个模型参数的宇宙剪切分析的单个调查。我们使用CosmoPower-JAX和哈密顿蒙特卡罗取样器派生的等高线进行了验证，结果与未使用仿真似然的嵌套取样器派生的等高线相符。接下来，我们考虑了宇宙剪切和星系聚类的联合分析，将参数空间增加到167个维度。我们展示了我们的方法在计算速度和准确性方面优于现有的实现，使我们能够在几分钟而不是几天内产生后验和参数限制。

    We present CosmoPower-JAX, a JAX-based implementation of the CosmoPower framework, which accelerates cosmological inference by building neural emulators of cosmological power spectra. We show how, using the automatic differentiation, batch evaluation and just-in-time compilation features of JAX, and running the inference pipeline on graphics processing units (GPUs), parameter estimation can be accelerated by orders of magnitude with advanced gradient-based sampling techniques. These can be used to efficiently explore high-dimensional parameter spaces, such as those needed for the analysis of next-generation cosmological surveys. We showcase the accuracy and computational efficiency of CosmoPower-JAX on two simulated Stage IV configurations. We first consider a single survey performing a cosmic shear analysis totalling 37 model parameters. We validate the contours derived with CosmoPower-JAX and a Hamiltonian Monte Carlo sampler against those derived with a nested sampler and without em
    
[^120]: 探究子词分割对transformer语言模型性能的影响

    Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])

    [http://arxiv.org/abs/2305.05480](http://arxiv.org/abs/2305.05480)

    本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。

    

    我们想研究词段如何影响语言模型的性能。我们使用了一种单语词段算法StateMorph，在芬兰语和俄语中训练了GPT-2和BERT模型。作为比较，我们还训练了一个使用BPE和Morfessor分割算法的模型。我们的初步结果表明，StateMorph可以帮助模型更有效地收敛并获得更好的验证分数。

    We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
    
[^121]: 基于协变量转移的分类树剪枝

    Classification Tree Pruning Under Covariate Shift. (arXiv:2305.04335v1 [stat.ML])

    [http://arxiv.org/abs/2305.04335](http://arxiv.org/abs/2305.04335)

    本文提出了一种基于协变量转移的分类树剪枝方法，可以访问大部分来自分布 $P_{X，Y}$ 的数据，但是只能获得来自拥有不同 $X$-边缘的目标分布 $Q_{X，Y}$ 的少量数据。使用的优化标准是一个关于分布 $P_{X} \to Q_{X}$ 的 \emph{平均差异}，该标准可以显著放宽最近提出的 \emph{转移指数}，最终可以得到最优的剪枝结果。

    

    本文考虑在训练数据不均匀的情况下，选择适当的子树以平衡偏差和方差的分类树剪枝问题。我们提出了一种针对这种情况的最优剪枝的高效程序，该程序可以访问大部分来自分布 $P_{X，Y}$ 的数据，但是只能获得来自拥有不同 $X$-边缘的目标分布 $Q_{X，Y}$ 的少量数据。在基本交叉验证和其他进行惩罚的变体，如基于信息度量的剪枝方法非常不理想的情况下，我们提供了一种最优剪枝的方法。使用的优化标准是一个关于分布 $P_{X} \to Q_{X}$ 的 \emph{平均差异}（在 $X$ 空间上平均），该标准可以显著放宽最近提出的 \emph{转移指数} 这一统计学概念，该概念被证明能够紧密地捕捉这种分布转移情况下分类的极限限制。我们放宽的标准可以被看作是分布之间的\emph{相对维度}度量，因为它涉及到信息的现有度量概念，例如闵可夫斯基和Rényi维度。

    We consider the problem of \emph{pruning} a classification tree, that is, selecting a suitable subtree that balances bias and variance, in common situations with inhomogeneous training data. Namely, assuming access to mostly data from a distribution $P_{X, Y}$, but little data from a desired distribution $Q_{X, Y}$ with different $X$-marginals, we present the first efficient procedure for optimal pruning in such situations, when cross-validation and other penalized variants are grossly inadequate. Optimality is derived with respect to a notion of \emph{average discrepancy} $P_{X} \to Q_{X}$ (averaged over $X$ space) which significantly relaxes a recent notion -- termed \emph{transfer-exponent} -- shown to tightly capture the limits of classification under such a distribution shift. Our relaxed notion can be viewed as a measure of \emph{relative dimension} between distributions, as it relates to existing notions of information such as the Minkowski and Renyi dimensions.
    
[^122]: 强化学习用于能源交易策略的优化

    Reinforcement learning for optimization of energy trading strategy. (arXiv:2303.16266v1 [cs.LG])

    [http://arxiv.org/abs/2303.16266](http://arxiv.org/abs/2303.16266)

    本文使用强化学习算法优化了一种黑盒交易策略，该策略通过在马尔可夫决策过程中使用真实数据进行优化，在 DA 能源市场上由中型生产者自动进行交易。

    

    越来越多的能源来自大量小型生产者的可再生能源，这些来源的效率是不稳定的，在某种程度上也是随机的，加剧了能源市场平衡问题。在许多国家，这种平衡是在预测日（DA）能源市场上完成的。本文考虑由中型生产者在DA能源市场上的自动化交易。我们将此活动建模为马尔可夫决策过程，并规范了一个框架，其中可以使用现实数据优化即用策略。我们合成参数化交易策略，并使用进化算法优化它们。我们还使用最先进的强化学习算法优化一个黑盒交易策略，该策略利用来自环境的可用信息来影响未来价格。

    An increasing part of energy is produced from renewable sources by a large number of small producers. The efficiency of these sources is volatile and, to some extent, random, exacerbating the energy market balance problem. In many countries, that balancing is performed on day-ahead (DA) energy markets. In this paper, we consider automated trading on a DA energy market by a medium size prosumer. We model this activity as a Markov Decision Process and formalize a framework in which a ready-to-use strategy can be optimized with real-life data. We synthesize parametric trading strategies and optimize them with an evolutionary algorithm. We also use state-of-the-art reinforcement learning algorithms to optimize a black-box trading strategy fed with available information from the environment that can impact future prices.
    
[^123]: TSMixer：一种全MLP架构用于时间序列预测

    TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06053](http://arxiv.org/abs/2303.06053)

    TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。

    

    实际时间序列数据集通常是多变量且具有复杂的动态。为了捕获这种复杂性，像循环或基于注意力的顺序深度学习模型这样的高容量结构变得受欢迎。然而，最近的研究表明，简单的单变量线性模型可以在几个常用的学术基准测试中胜过这样的深度学习模型。扩展它们，本文研究线性模型在时间序列预测中的能力，并提出了时序混合器（TSMixer），这是一种通过堆叠多层感知器（MLP）设计的新型结构。 TSMixer基于沿时间和特征维度的混合操作，以有效地提取信息。在流行的学术基准测试上，简单易行的TSMixer与利用特定基准的归纳偏差的专业先进模型相媲美。在具有挑战性和大规模的M5基准测试中，即一个实际的零售数据集上，TSMixer表现出非常出色的性能。

    Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
    
[^124]: 通过语言实现视觉抽象和推理技术

    Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04091](http://arxiv.org/abs/2303.04091)

    本论文提出了一种通过自然语言描述任务的通用框架来解决Abstraction and Reasoning Corpus（ARC）问题，虽然还没有在ARC上击败最先进的DSL模型，但我们展示了我们的方法具有巨大的潜力，可以解决先前未解决的任务。

    

    尽管人工智能（AI）模型在局限应用中已经达到了人类甚至超越人类的性能，但它们仍然难以展现更广泛和更灵活的智能。Abstraction and Reasoning Corpus（ARC）旨在评估AI系统与人类类似的认知能力。目前大多数方法依赖于精心设计的特定领域语言（DSL），用于暴力解决ARC中的任务。在这项工作中，我们提出了一个基于任务自然语言描述的通用框架来解决ARC问题。虽然还没有在ARC上击败最先进的DSL模型，但我们展示了我们的方法具有巨大的潜力，可以解决先前未解决的任务。

    While Artificial Intelligence (AI) models have achieved human or even superhuman performance in narrowly defined applications, they still struggle to show signs of broader and more flexible intelligence. The Abstraction and Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how close AI systems are to human-like cognitive abilities. Most current approaches rely on carefully handcrafted domain-specific languages (DSLs), which are used to brute-force solutions to the tasks present in ARC. In this work, we propose a general framework for solving ARC based on natural language descriptions of the tasks. While not yet beating state-of-the-art DSL models on ARC, we demonstrate the immense potential of our approach hinted at by the ability to solve previously unsolved tasks.
    
[^125]: 基于稀疏高斯过程的连续和离散空间的回归传感器放置优化

    Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.00028](http://arxiv.org/abs/2303.00028)

    本文提出了一种用稀疏高斯过程解决连续空间下传感器放置问题的方法，避免离散化环境并降低了计算复杂度，可通过贪婪算法找到好的解决方案。

    

    本文提出了一种基于稀疏高斯过程方法的传感器放置方案，用于监测温度、降水等空间（或时空）相关现象。与现有的基于高斯过程的传感器放置方法不同，我们将已知内核函数参数的稀疏高斯过程拟合到环境中随机采样的未标记位置，并通过学习得到的诱导点来解决连续空间的传感器放置问题。使用稀疏高斯过程避免了对环境进行离散化，并将计算复杂度从立方级别降低到线性级别。在候选传感器放置点集合的限制下，我们可以使用贪婪顺序选择算法来找到较好的解决方案。

    We present a novel approach based on sparse Gaussian processes (SGPs) to address the sensor placement problem for monitoring spatially (or spatiotemporally) correlated phenomena such as temperature and precipitation. Existing Gaussian process (GP) based sensor placement approaches use GPs with known kernel function parameters to model a phenomenon and subsequently optimize the sensor locations in a discretized representation of the environment. In our approach, we fit an SGP with known kernel function parameters to randomly sampled unlabeled locations in the environment and show that the learned inducing points of the SGP inherently solve the sensor placement problem in continuous spaces. Using SGPs avoids discretizing the environment and reduces the computation cost from cubic to linear complexity. When restricted to a candidate set of sensor placement locations, we can use greedy sequential selection algorithms on the SGP's optimization bound to find good solutions. We also present a
    
[^126]: IGB: 针对公共图形数据集在标记、特征、异质性和大小方面的差距为深度学习研究提供了解决方案

    IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research. (arXiv:2302.13522v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13522](http://arxiv.org/abs/2302.13522)

    IGB是一个研究数据集工具，包含同质和异质性学术图形，规模巨大，并提供工具用于生成不同特性的合成图形，为GNN研究人员提供解决公共图形数据集在标记、特征、异质性和大小方面差距的有价值资源。

    

    图形神经网络 (GNNs) 已经展示了对于各种具有挑战性的真实应用的巨大潜力，但 GNN 研究中的一个主要障碍是缺乏大规模灵活的数据集。现有的大部分公共GNN数据集都相对较小，这限制了 GNN 的推广到未知数据的能力。很少有大型的图形数据集提供丰富的标记数据，这使得难以确定 GNN 模型在未知数据上的低准确性是由于训练数据不足还是模型无法推广。此外，训练 GNN 的数据集需要提供灵活性，以便深入研究各种因素对 GNN 模型训练的影响。在这项工作中，我们介绍了伊利诺伊图形基准 (IGB)，这是一个研究数据集工具，开发人员可以使用它来高精度地训练、审查和系统地评估GNN模型。IGB 包括同质和异质性学术图形，规模巨大，并且可以标记和操作，以模拟真实场景。该数据集还包括用于生成具有不同特性的合成图形的工具，使研究人员能够探索各种图形特性对 GNN 的影响。我们相信，伊利诺伊图形基准将为 GNN 研究团体提供有价值的资源，以解决公共图形数据集在标记、特征、异质性和大小方面的差距，以用于深度学习研究。

    Graph neural networks (GNNs) have shown high potential for a variety of real-world, challenging applications, but one of the major obstacles in GNN research is the lack of large-scale flexible datasets. Most existing public datasets for GNNs are relatively small, which limits the ability of GNNs to generalize to unseen data. The few existing large-scale graph datasets provide very limited labeled data. This makes it difficult to determine if the GNN model's low accuracy for unseen data is inherently due to insufficient training data or if the model failed to generalize. Additionally, datasets used to train GNNs need to offer flexibility to enable a thorough study of the impact of various factors while training GNN models.  In this work, we introduce the Illinois Graph Benchmark (IGB), a research dataset tool that the developers can use to train, scrutinize and systematically evaluate GNN models with high fidelity. IGB includes both homogeneous and heterogeneous academic graphs of enorm
    
[^127]: 学习成对差分混合的EM算法的尖锐分析。

    Sharp analysis of EM for learning mixtures of pairwise differences. (arXiv:2302.10066v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2302.10066](http://arxiv.org/abs/2302.10066)

    该论文研究了使用成对比较设计的随机样本的线性回归的对称混合，通过分析EM算法的序列收敛性和极限值，得出了$\ell_\infty$范数和$\ell_2$范数中的估计尖锐度。研究表明EM算法可以展现出多个独特的行为。

    

    我们考虑使用成对比较设计的随机样本的线性回归的对称混合，这可以看作是一种欧几里得距离几何问题的噪声版本。我们在真实值周围局部分析期望最大化（EM）算法，并建立起它的序列线性收敛性，从而为迭代估计误差提供一个$\ell_\infty$-范数保证。此外，我们表明，EM序列的极限实现了在$\ell_2$-范数中的估计尖锐度，匹配信息理论上最优的常数。我们还通过模拟论证了在这种情况下从随机初始化收敛的问题更为微妙，通常不会发生。我们的结果表明，当协变量分布被适当地结构化时，EM算法可以表现出几个独特的行为。

    We consider a symmetric mixture of linear regressions with random samples from the pairwise comparison design, which can be seen as a noisy version of a type of Euclidean distance geometry problem. We analyze the expectation-maximization (EM) algorithm locally around the ground truth and establish that the sequence converges linearly, providing an $\ell_\infty$-norm guarantee on the estimation error of the iterates. Furthermore, we show that the limit of the EM sequence achieves the sharp rate of estimation in the $\ell_2$-norm, matching the information-theoretically optimal constant. We also argue through simulation that convergence from a random initialization is much more delicate in this setting, and does not appear to occur in general. Our results show that the EM algorithm can exhibit several unique behaviors when the covariate distribution is suitably structured.
    
[^128]: 一种单样本去中心化近端算法用于非凸随机复合优化

    A One-Sample Decentralized Proximal Algorithm for Non-Convex Stochastic Composite Optimization. (arXiv:2302.09766v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.09766](http://arxiv.org/abs/2302.09766)

    本文提出了两种单时间规模的算法：Prox-DASA和Prox-DASA-GT，它们可以用常量批量大小找到复合目标函数的$\epsilon$-静止点，并且不需要大批量大小、更复杂的操作或更强的假设。

    

    本文研究了去中心化随机非凸优化问题，其中$n$个代理共同优化由光滑项和非光滑凸项相加的复合目标函数。为了解决这个问题，我们提出了两个单时间规模算法：Prox-DASA和Prox-DASA-GT。这些算法可以使用常量批量大小（即$\mathcal{O}(1)$）在$\mathcal{O}(n^{-1}\epsilon^{-2})$次迭代中找到$\epsilon$-静止点。与以前的工作不同，我们的算法在不需要大批量大小、更复杂的每次迭代操作（如双重循环）或更强的假设的情况下实现了可比拟的复杂度。我们的理论发现得到了广泛的数值实验支持，这些实验证明了我们的算法优于以前的方法。我们的代码可在https://github.com/xuxingc/ProxDASA找到。

    We focus on decentralized stochastic non-convex optimization, where $n$ agents work together to optimize a composite objective function which is a sum of a smooth term and a non-smooth convex term. To solve this problem, we propose two single-time scale algorithms: Prox-DASA and Prox-DASA-GT. These algorithms can find $\epsilon$-stationary points in $\mathcal{O}(n^{-1}\epsilon^{-2})$ iterations using constant batch sizes (i.e., $\mathcal{O}(1)$). Unlike prior work, our algorithms achieve comparable complexity without requiring large batch sizes, more complex per-iteration operations (such as double loops), or stronger assumptions. Our theoretical findings are supported by extensive numerical experiments, which demonstrate the superiority of our algorithms over previous approaches. Our code is available at https://github.com/xuxingc/ProxDASA.
    
[^129]: 在大状态空间中打破多智体的诅咒：带独立线性函数逼近的Markov博弈中的强化学习

    Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation. (arXiv:2302.03673v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03673](http://arxiv.org/abs/2302.03673)

    本研究提出了一种新的独立线性Markov博弈模型，针对多智体强化学习中的大状态空间和大量代理问题，设计了新算法以学习Markov粗略相关均衡和Markov相关均衡。相比于现有的Markov博弈函数逼近技术，我们的方法能够大大降低样本复杂度并取得更高的精度。

    

    我们提出了一种新的模型——独立线性Markov博弈，用于具有大状态空间和大量代理的多智体强化学习。这是一类带有独立线性函数逼近的Markov博弈，每个代理都有自己的函数逼近，用于被其他玩家的策略边缘化的状态-动作值函数。我们设计了学习Markov粗略相关均衡和Markov相关均衡的新算法，并提供了样本复杂度界限，这些界限仅与每个代理自己的函数类复杂度成多项式比例，从而打破了多智体的诅咒。相比之下，现有的用于函数逼近的Markov博弈的研究，在特化于标准表格状况的Markov博弈设置时，其样本复杂度界限会随着\emph{联合行动空间}的大小成指数级增长，而该联合行动空间在代理的数量上呈指数级增长。我们的算法依赖于两个关键的技术创新：(1) 利用策略重放来降低样本复杂度；(2) 利用独立线性函数逼近来获得计算上的有效性和统计上的高精度。

    We propose a new model, independent linear Markov game, for multi-agent reinforcement learning with a large state space and a large number of agents. This is a class of Markov games with independent linear function approximation, where each agent has its own function approximation for the state-action value functions that are marginalized by other players' policies. We design new algorithms for learning the Markov coarse correlated equilibria (CCE) and Markov correlated equilibria (CE) with sample complexity bounds that only scale polynomially with each agent's own function class complexity, thus breaking the curse of multiagents. In contrast, existing works for Markov games with function approximation have sample complexity bounds scale with the size of the \emph{joint action space} when specialized to the canonical tabular Markov game setting, which is exponentially large in the number of agents. Our algorithms rely on two key technical innovations: (1) utilizing policy replay to tac
    
[^130]: 变分贝叶斯系统发育参数推断中的先验密度学习

    Prior Density Learning in Variational Bayesian Phylogenetic Parameters Inference. (arXiv:2302.02522v2 [q-bio.PE] UPDATED)

    [http://arxiv.org/abs/2302.02522](http://arxiv.org/abs/2302.02522)

    本文提出了一种使用学习参数的灵活先验方法，通过多个马尔可夫链替代模型的模拟得出，该方法在估计系统发育参数方面非常有效。

    

    变分推断在贝叶斯估计问题中提供了有希望的路径。这些进步使得变分系统发育推断成为逼近系统发育后验的蒙特卡罗马尔科夫链方法的替代方法。然而，这种方法的主要缺点之一是通过固定分布来建模先验，如果它们远离当前数据分布，可能会偏倚后验逼近。在本文中，我们提出了一种方法和实施框架，通过学习它们的参数，使用基于梯度的方法和基于神经网络的参数化来放松先验密度的严格性。我们将这种方法应用于多个马尔可夫链替代模型下的支长度和进化参数估计。所进行的模拟结果显示出这种方法在估计支长度和进化模型参数方面非常有效。它们还表明灵活的先验可以提高推断的准确性。

    The advances in variational inference are providing promising paths in Bayesian estimation problems. These advances make variational phylogenetic inference an alternative approach to Markov Chain Monte Carlo methods for approximating the phylogenetic posterior. However, one of the main drawbacks of such approaches is the modelling of the prior through fixed distributions, which could bias the posterior approximation if they are distant from the current data distribution. In this paper, we propose an approach and an implementation framework to relax the rigidity of the prior densities by learning their parameters using a gradient-based method and a neural network-based parameterization. We applied this approach for branch lengths and evolutionary parameters estimation under several Markov chain substitution models. The results of performed simulations show that the approach is powerful in estimating branch lengths and evolutionary model parameters. They also show that a flexible prior m
    
[^131]: 线性组合的威力：随机卷积学习

    The Power of Linear Combinations: Learning with Random Convolutions. (arXiv:2301.11360v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.11360](http://arxiv.org/abs/2301.11360)

    本研究质疑了卷积神经网络中学习到的卷积核的重要性，提出了简单的线性组合方法，从随机卷积核中创建出表达能力强的网络运算符，通过隐含的正则化技术，可以提高整体性能。

    

    现代卷积神经网络通过增加模型深度、宽度和卷积核大小来保持与更先进的模型（如基于变换器的模型）的竞争力，导致有大量的可训练模型参数需要在训练过程中进行处理。本文质疑卷积神经网络中学习到的卷积核的重要性。实验证明，很多当代的卷积神经网络结构，甚至在不更新初始化的随机卷积核的情况下就可以达到高的测试准确率。实际上，简单的线性组合可以有效地将随机卷积核组合成表达能力强的网络运算符，其通过高效的 $1 \times 1$ 卷积来实现。此外，这些随机卷积核的组合可以隐式地正则化结果运算符，减轻过拟合，提高整体性能。

    Following the traditional paradigm of convolutional neural networks (CNNs), modern CNNs manage to keep pace with more recent, for example transformer-based, models by not only increasing model depth and width but also the kernel size. This results in large amounts of learnable model parameters that need to be handled during training. While following the convolutional paradigm with the according spatial inductive bias, we question the significance of \emph{learned} convolution filters. In fact, our findings demonstrate that many contemporary CNN architectures can achieve high test accuracies without ever updating randomly initialized (spatial) convolution filters. Instead, simple linear combinations (implemented through efficient $1\times 1$ convolutions) suffice to effectively recombine even random filters into expressive network operators. Furthermore, these combinations of random filters can implicitly regularize the resulting operations, mitigating overfitting and enhancing overall 
    
[^132]: 一种基于半监督感知率学习的CMAB方案，通过可信数据收集在人群中抗击COVID-19

    A Semi-supervised Sensing Rate Learning based CMAB Scheme to Combat COVID-19 by Trustful Data Collection in the Crowd. (arXiv:2301.08563v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2301.08563](http://arxiv.org/abs/2301.08563)

    本文提出了一种基于半监督学习的组合多臂赌博反向拍卖方案，用于解决移动众包系统中在招募多个未知和有策略的工作者时出现的数据可信问题。

    

    对于移动众包系统，招募可信和高质量的工作者是一个重要的研究问题。先前的研究要么假设工作者的能力是事先已知的，要么假设平台一旦接收到他们收集的数据就知道他们的能力。实际上，为了降低成本并最大程度地提高收入，许多有策略的工作者不诚实地执行其感知任务，并向平台报告虚假数据，这被称为虚假数据攻击。对于平台来说，评估所收到的数据的真实性十分困难。本文提出了一种名为基于半监督组合多臂赌博反向拍卖（SCMABA）的激励机制来解决MCS中多个未知和有策略的工作者的招聘问题。首先，我们将工作者招募建模为多臂赌博反向拍卖问题，并设计了一种基于UCB的算法来分离探索和开发，将已招募的工作者的感知率视为“臂“。

    The recruitment of trustworthy and high-quality workers is an important research issue for MCS. Previous studies either assume that the qualities of workers are known in advance, or assume that the platform knows the qualities of workers once it receives their collected data. In reality, to reduce costs and thus maximize revenue, many strategic workers do not perform their sensing tasks honestly and report fake data to the platform, which is called False data attacks. And it is very hard for the platform to evaluate the authenticity of the received data. In this paper, an incentive mechanism named Semi-supervision based Combinatorial Multi-Armed Bandit reverse Auction (SCMABA) is proposed to solve the recruitment problem of multiple unknown and strategic workers in MCS. First, we model the worker recruitment as a multi-armed bandit reverse auction problem and design an UCB-based algorithm to separate the exploration and exploitation, regarding the Sensing Rates (SRs) of recruited worke
    
[^133]: 参数表达能力对于量子代价函数集中性的影响分析

    The quantum cost function concentration dependency on the parametrization expressivity. (arXiv:2301.06883v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2301.06883](http://arxiv.org/abs/2301.06883)

    本文分析了参数表达能力对代价函数的影响，证明了参数表达能力越强，代价函数越趋于集中在一个取决于选择的可观察量和使用的量子比特数的值上。

    

    尽管我们目前处于嘈杂中间规模的量子设备时代，但是人们正在进行多项研究，旨在把机器学习引入到量子领域。目前，量子变分电路是构建这种模型的主要策略之一。然而，尽管被广泛使用，我们仍不知道创建量子机器学习模型所需的最小资源。在本文中，我们分析了参数表达能力如何影响代价函数。我们在理论上证明，参数表达能力越强，代价函数就越倾向于集中于一个值，这个值既取决于所选择的可观察量，也取决于所使用的量子比特数。为此，我们首先得出了参数表达能力和代价函数均值之间的关系。之后，我们将参数表达能力与代价函数的方差相关联。最后，我们展示了一些数值结果。

    Although we are currently in the era of noisy intermediate scale quantum devices, several studies are being conducted with the aim of bringing machine learning to the quantum domain. Currently, quantum variational circuits are one of the main strategies used to build such models. However, despite its widespread use, we still do not know what are the minimum resources needed to create a quantum machine learning model. In this article, we analyze how the expressiveness of the parametrization affects the cost function. We analytically show that the more expressive the parametrization is, the more the cost function will tend to concentrate around a value that depends both on the chosen observable and on the number of qubits used. For this, we initially obtain a relationship between the expressiveness of the parametrization and the mean value of the cost function. Afterwards, we relate the expressivity of the parametrization with the variance of the cost function. Finally, we show some nume
    
[^134]: 深度学习在数学推理中的应用综述

    A Survey of Deep Learning for Mathematical Reasoning. (arXiv:2212.10535v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.10535](http://arxiv.org/abs/2212.10535)

    本文综述了过去十年中深度学习在数学推理领域的关键任务、数据集和方法，并评估了现有的基准和方法，探讨了未来的研究方向。

    

    数学推理是人类智能的基本组成部分，并且应用广泛，包括科学、工程、金融和日常生活。发展能够解决数学问题和证明定理的人工智能系统在机器学习和自然语言处理领域引起了极大关注。本文回顾了过去十年中在数学推理和深度学习交叉领域的关键任务、数据集和方法，并评估了现有的基准和方法，讨论了未来的研究方向。

    Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.
    
[^135]: \{kappa}HGCN: 通过连续和离散曲率学习实现树状结构建模

    \{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning. (arXiv:2212.01793v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01793](http://arxiv.org/abs/2212.01793)

    本文提出了一种新的\{kappa}HGCN模型，在双曲空间内实现树状结构建模，通过结合连续和离散曲率来学习输入图的基础几何结构，并在多个基准测试和数据集上取得了最先进的性能。

    

    树状结构在现实世界中广泛存在，包括层次结构和幂律分布。最近，利用双曲空间进行树状结构建模受到了广泛关注，由于其呈指数增长，相比于平坦的欧几里得空间，曲面双曲空间提供了更易处理和嵌入的空间，特别适用于展现隐含树状结构的数据集。然而，真实世界树状数据的复杂性提出了一个重要挑战，因为它经常展示出树状、平坦和圆形区域的异质组成。将这样异质的结构直接嵌入一个同质化的嵌入空间（即双曲空间）必然导致重大失真。为了缓解上述缺点，本研究致力于探索双曲空间的曲率，以实现灵活准确地建模树状结构。具体而言，我们提出了一种新的\{kappa}HGCN模型，将连续和离散曲率相结合，学习输入图的基础几何结构。我们的模型在不同的基准测试和数据集上均取得了最先进的性能，证明了其在捕捉输入数据的树状结构方面的有效性。

    The prevalence of tree-like structures, encompassing hierarchical structures and power law distributions, exists extensively in real-world applications, including recommendation systems, ecosystems, financial networks, social networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness modeling has garnered considerable attention owing to its exponential growth volume. Compared to the flat Euclidean space, the curved hyperbolic space provides a more amenable and embeddable room, especially for datasets exhibiting implicit tree-like architectures. However, the intricate nature of real-world tree-like data presents a considerable challenge, as it frequently displays a heterogeneous composition of tree-like, flat, and circular regions. The direct embedding of such heterogeneous structures into a homogeneous embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions. To mitigate the aforementioned shortage, this study endeavors to explore the curvatur
    
[^136]: 使用社交感知强化学习提高主动对话代理的性能

    Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning. (arXiv:2211.15359v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15359](http://arxiv.org/abs/2211.15359)

    本论文的主要贡献在于提出了一种新的方法，通过将社交和任务相关特征考虑在对话中，来优化主动对话策略，使其在任务效率高的同时，也能促进用户信任，从而提高人机交互的成功率和效率。

    

    智能对话代理的下一个步骤是从旁观者的角色中解脱出来，变得更加主动。明确定义的主动行为可以改善人机合作，因为代理在交互过程中扮演更积极的角色并解除了用户的责任。然而，主动性是一把双刃剑，因为执行不当的预防性行动可能不仅对任务结果产生破坏性影响，而且还会对与用户的关系产生影响。为了设计合适的主动对话策略，我们提出了一种新的方法，将社交和任务相关特征都考虑在对话中。这里的主要目标是优化主动行为，使其任务导向——这意味着高任务成功率和效率——同时在促进用户信任时也具有社交效益。将这两个方面包含在用强化学习训练主动对话代理的奖励函数中，显示出我们的方法对于更加成功的人机交互的益处。

    The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive. Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user. However, proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user. For designing adequate proactive dialog strategies, we propose a novel approach including both social as well as task-relevant features in the dialog. Here, the primary goal is to optimize proactive behavior so that it is task-oriented - this implies high task success and efficiency - while also being socially effective by fostering user trust. Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for more successful human-mach
    
[^137]: PhAST：物理感知、可扩展、任务特定的GNN在加速催化剂设计中的应用

    PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design. (arXiv:2211.12020v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12020](http://arxiv.org/abs/2211.12020)

    提出了PhAST方法来快速发现更有效的催化剂来驱动电化学反应, 该方法适用于大多数体系结构, 可以增加计算效率和精度

    

    缓解气候危机需要快速向低碳能源转变。催化剂材料在许多工业过程中的电化学反应中起着至关重要的作用，如可再生能源储存和电荷合成。为了减少在这些过程中消耗的能量，我们必须快速发现更有效的催化剂来驱动电化学反应。机器学习（ML）有潜力从大量数据中高效地模拟材料的性质，从而加速电催化剂的设计。为此，Open Catalyst Project OC20数据集已经被构建。然而，大多数已经在OC20上训练的现有ML模型仍然无法满足实际应用的可扩展性和准确性要求。在这里，我们提出了几个任务特定的创新，适用于大多数体系结构，可以增加计算效率和精度。特别是我们在图翻译层、图注意力层和池化层中提出了改进。我们将这种方法称为Physical Attribute Scaling Transformer (PhAST)。我们证明了PhAST在生成准确数据的同时，具有低计算成本，适用于几个相关应用，包括电催化剂的发现和设计。

    Mitigating the climate crisis requires a rapid transition towards lower carbon energy. Catalyst materials play a crucial role in the electrochemical reactions involved in a great number of industrial processes key to this transition, such as renewable energy storage and electrofuel synthesis. To reduce the amount of energy spent on such processes, we must quickly discover more efficient catalysts to drive the electrochemical reactions. Machine learning (ML) holds the potential to efficiently model the properties of materials from large amounts of data, and thus to accelerate electrocatalyst design. The Open Catalyst Project OC20 data set was constructed to that end. However, most existing ML models trained on OC20 are still neither scalable nor accurate enough for practical applications. Here, we propose several task-specific innovations, applicable to most architectures, which increase both computational efficiency and accuracy. In particular, we propose improvements in (1) the graph 
    
[^138]: scikit-fda：用于函数数据分析的Python包

    scikit-fda: A Python Package for Functional Data Analysis. (arXiv:2211.02566v2 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2211.02566](http://arxiv.org/abs/2211.02566)

    scikit-fda是一个用于函数数据分析的Python包，提供了全面的工具，并于scikit-learn兼容，采用三条款BSD许可证发布，对FDA社区贡献开放。

    

    库scikit-fda是用于函数数据分析（FDA）的Python软件包。它提供了一套全面的工具，用于函数数据的表示、预处理和探索性分析。该库建立在Python科学计算生态系统之上，特别是采用了scikit-learn应用程序接口，以利用该软件包提供的机器学习功能：包括管道、模型选择和超参数调整等。这个scikit-fda软件包已经以三条款BSD许可证的形式发布为自由和开源软件，并对FDA社区的贡献持开放态度。该库的广泛文档包括逐步教程和详细的使用示例。

    The library scikit-fda is a Python package for Functional Data Analysis (FDA). It provides a comprehensive set of tools for representation, preprocessing, and exploratory analysis of functional data. The library is built upon and integrated in Python's scientific ecosystem. In particular, it conforms to the scikit-learn application programming interface so as to take advantage of the functionality for machine learning provided by this package: pipelines, model selection, and hyperparameter tuning, among others. The scikit-fda package has been released as free and open-source software under a 3-Clause BSD license and is open to contributions from the FDA community. The library's extensive documentation includes step-by-step tutorials and detailed examples of use.
    
[^139]: 基于无动作轨迹的半监督离线强化学习

    Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories. (arXiv:2210.06518v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06518](http://arxiv.org/abs/2210.06518)

    该论文提出了一种半监督离线强化学习的新设置，利用有标记的轨迹数据和无动作的轨迹数据训练反动力学模型以获取代理标签，最终使用任何离线强化学习算法以实现高成功率的表现。

    

    自然智能体可以有效地从不同大小、质量和测量类型的多个数据源中学习。我们通过引入一个新的、实际上受到启发的半监督设置来研究强化学习中的这种异质性。在这里，智能体可以访问两个轨迹集：一个包含每个时间步的状态、行为和奖励三元组的标记轨迹集，以及一个仅包含状态和奖励信息的未标记轨迹集。针对这种情况，我们开发和研究了一个简单的元算法流水线，该算法在标记数据上学习反动力学模型，以获得未标记数据的代理标签，然后将任何离线强化学习算法用于真实和代理标记轨迹。经验表明，这个简单的流水线非常成功——在几个D4RL基准测试中，某些离线RL算法可以与在完全标记数据集上训练的变体的表现相匹配，即使后者拥有更多的标记数据。

    Natural agents can effectively learn from multiple data sources that differ in size, quality, and types of measurements. We study this heterogeneity in the context of offline reinforcement learning (RL) by introducing a new, practically motivated semi-supervised setting. Here, an agent has access to two sets of trajectories: labelled trajectories containing state, action and reward triplets at every timestep, along with unlabelled trajectories that contain only state and reward information. For this setting, we develop and study a simple meta-algorithmic pipeline that learns an inverse dynamics model on the labelled data to obtain proxy-labels for the unlabelled data, followed by the use of any offline RL algorithm on the true and proxy-labelled trajectories. Empirically, we find this simple pipeline to be highly successful -- on several D4RL benchmarks~\cite{fu2020d4rl}, certain offline RL algorithms can match the performance of variants trained on a fully labelled dataset even when w
    
[^140]: 通过发现多样的环境轨迹生成器先验知识，高效学习运动技能

    Efficient Learning of Locomotion Skills through the Discovery of Diverse Environmental Trajectory Generator Priors. (arXiv:2210.04819v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2210.04819](http://arxiv.org/abs/2210.04819)

    本论文提出了一种名为EETG的方法，使用“质量-多样性”算法学习一组多样化的专业化运动先验知识，能够帮助四足机器人成功地穿越各种环境，比使用单个轨迹生成器的方法更加有效。

    

    最近数据驱动的学习方法在学习各种不规则地形的鲁棒运动控制器方面表现出色。以轨迹生成器（TG）形式加入良好的运动先验知识，可以有效地学习复杂的运动技能。然而，随着任务/环境变得越来越复杂，在单个良好的TG中定义它仍然是一个具有挑战性的问题，因为它需要大量的调整，同时还有可能降低先验知识的有效性。本文提出一种名为EETG的方法，它使用“质量-多样性（Quality-Diversity）”算法学习一组多样化的专业化运动先验知识，同时在Policies Modulating TG（PMTG）框架内维持单一策略。实验结果表明，EETG使四足机器人能够成功地穿越各种环境，如斜坡、台阶、崎岖地形和平衡横杆。我们的实验结果表明，EETG比使用单个TG的方法更加有效。

    Data-driven learning based methods have recently been particularly successful at learning robust locomotion controllers for a variety of unstructured terrains. Prior work has shown that incorporating good locomotion priors in the form of trajectory generators (TGs) is effective at efficiently learning complex locomotion skills. However, defining a good, single TG as tasks/environments become increasingly more complex remains a challenging problem as it requires extensive tuning and risks reducing the effectiveness of the prior. In this paper, we present Evolved Environmental Trajectory Generators (EETG), a method that learns a diverse set of specialised locomotion priors using Quality-Diversity algorithms while maintaining a single policy within the Policies Modulating TG (PMTG) architecture. The results demonstrate that EETG enables a quadruped robot to successfully traverse a wide range of environments, such as slopes, stairs, rough terrain, and balance beams. Our experiments show th
    
[^141]: 公平信用评分的算法决策方法研究

    Algorithmic decision making methods for fair credit scoring. (arXiv:2209.07912v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07912](http://arxiv.org/abs/2209.07912)

    本论文研究了12种偏差缓解方法在5个不同的公平性指标下的有效性，评估了它们对金融机构准确性和潜在盈利能力的影响，并指出了最成功和最不成功缓解方法。

    

    机器学习在评估贷款申请人的信用价值方面的有效性已经被证明了很长一段时间。然而，使用自动决策过程可能导致对某些群体或个人进行不平等对待，进而导致歧视性结果，这引起了人们的关注。本文通过评估12种主要的偏差缓解方法在5种不同的公平性指标下的有效性以及评估它们对于金融机构的准确性和潜在盈利能力，来解决这个问题。通过我们的分析，我们已经确定了在保持准确性和盈利能力的同时实现公平所面临的挑战，并突出了最成功和最不成功的缓解方法。最终，我们的研究旨在弥合实验机器学习与其在金融行业中的实际应用之间的差距。

    The effectiveness of machine learning in evaluating the creditworthiness of loan applicants has been demonstrated for a long time. However, there is concern that the use of automated decision-making processes may result in unequal treatment of groups or individuals, potentially leading to discriminatory outcomes. This paper seeks to address this issue by evaluating the effectiveness of 12 leading bias mitigation methods across 5 different fairness metrics, as well as assessing their accuracy and potential profitability for financial institutions. Through our analysis, we have identified the challenges associated with achieving fairness while maintaining accuracy and profitabiliy, and have highlighted both the most successful and least successful mitigation methods. Ultimately, our research serves to bridge the gap between experimental machine learning and its practical applications in the finance industry.
    
[^142]: 使用评估器方差降低控制多智能体强化学习

    Taming Multi-Agent Reinforcement Learning with Estimator Variance Reduction. (arXiv:2209.01054v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2209.01054](http://arxiv.org/abs/2209.01054)

    本文提出了一种名为“PERLA”的增强工具，它通过将智能体的联合策略采样技术引入评论家中，减少了联合动作单一样本造成的梯度估计方差，使得CT-DE MARL算法的学习更加稳定且更快速。

    

    集中式训练和分散式执行（CT-DE）是许多领先的多智能体强化学习算法的基础。尽管它很受欢迎，但由于其依赖于在给定状态下对联合动作的单个样本进行学习，因此存在一个关键缺陷。随着智能体在训练过程中探索和更新策略，这些单个样本可能较差地代表系统代理的实际联合策略，导致梯度估计方差高，阻碍学习。为了解决这个问题，我们提出了一个增强工具，该工具适用于任何演员-评论员多智能体强化学习方法。我们的框架Performance Enhancing Reinforcement Learning Apparatus（PERLA）在代理训练时引入了一种代理的联合策略采样技术到评论家中。这将导致TD更新更接近当前联合策略的真正期望值，而不是在给定状态下对联合动作的单个样本的估计值。这产生了方差减少的梯度估计，从而为CT-DE MARL算法提供更稳定和更快的学习。

    Centralised training with decentralised execution (CT-DE) serves as the foundation of many leading multi-agent reinforcement learning (MARL) algorithms. Despite its popularity, it suffers from a critical drawback due to its reliance on learning from a single sample of the joint-action at a given state. As agents explore and update their policies during training, these single samples may poorly represent the actual joint-policy of the system of agents leading to high variance gradient estimates that hinder learning. To address this problem, we propose an enhancement tool that accommodates any actor-critic MARL method. Our framework, Performance Enhancing Reinforcement Learning Apparatus (PERLA), introduces a sampling technique of the agents' joint-policy into the critics while the agents train. This leads to TD updates that closely approximate the true expected value under the current joint-policy rather than estimates from a single sample of the joint-action at a given state. This prod
    
[^143]: 通过选择源定位脑电图特征，找到肥胖症的神经标志

    Finding neural signatures for obesity through feature selection on source-localized EEG. (arXiv:2208.14007v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14007](http://arxiv.org/abs/2208.14007)

    这项研究使用脑电图数据开发了一个新的机器学习模型，成功识别出肥胖女性的脑网络，表明肥胖大脑的特征是具有功能障碍的网络。

    

    肥胖是现代社会一个严重的问题，与显著降低的生活质量密切相关。目前，用脑电图数据探索肥胖相关的神经学证据的研究局限于传统方法。本研究开发了一种新的机器学习模型，使用从脑电图数据派生的 alpha 波段功能连接特征来识别肥胖女性的脑网络。获得了总体分类准确度为 0.937。我们的发现表明，肥胖大脑的特征是具有功能障碍的网络，其中负责处理自我参照信息和环境上下文信息的区域受损。

    Obesity is a serious issue in the modern society and is often associated to significantly reduced quality of life. Current research conducted to explore obesity-related neurological evidences using electroencephalography (EEG) data are limited to traditional approaches. In this study, we developed a novel machine learning model to identify brain networks of obese females using alpha band functional connectivity features derived from EEG data. An overall classification accuracy of 0.937 is achieved. Our finding suggests that the obese brain is characterized by a dysfunctional network in which the areas that responsible for processing self-referential information and environmental context information are impaired.
    
[^144]: SNAP: 带毒素的高效私密特征提取

    SNAP: Efficient Extraction of Private Properties with Poisoning. (arXiv:2208.12348v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12348](http://arxiv.org/abs/2208.12348)

    SNAP 是一种高效的带毒素特征提取攻击方法，相比同类方法需要更少的毒素攻击，并获得更高的攻击成功率。

    

    特征推理攻击允许对手从机器学习模型中提取训练数据集的全局特征。这些攻击对于共享数据集以训练机器学习模型的数据所有者具有隐私影响。针对深度神经网络的特征推理攻击存在一些已有方法，但它们都依赖于攻击者训练大量的阴影模型，从而引起了巨大的计算开销。本文考虑了攻击者可以对训练数据集的子集进行毒素攻击并查询训练目标模型的特征推理攻击设置。在对毒素下模型置信度的理论分析基础上，我们设计出了一种有效的特征推理攻击，SNAP，它比 Mahloujifar et al. 的最新毒素攻击特征推理攻击获得更高的攻击成功率，并且需要更少的毒素攻击。例如，在人口普查数据集上，SNAP 攻击成功率可达到 34％。

    Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. Such attacks have privacy implications for data owners sharing their datasets to train machine learning models. Several existing approaches for property inference attacks against deep neural networks have been proposed, but they all rely on the attacker training a large number of shadow models, which induces a large computational overhead.  In this paper, we consider the setting of property inference attacks in which the attacker can poison a subset of the training dataset and query the trained target model. Motivated by our theoretical analysis of model confidences under poisoning, we design an efficient property inference attack, SNAP, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-based property inference attack by Mahloujifar et al. For example, on the Census dataset, SNAP achieves 34% high
    
[^145]: Frouros: 一个用于机器学习系统中漂移检测的Python库

    Frouros: A Python library for drift detection in machine learning systems. (arXiv:2208.06868v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06868](http://arxiv.org/abs/2208.06868)

    Frouros是一个开源的Python库，可以检测任何机器学习框架中的概念和数据漂移，易于维护和扩展。

    

    Frouros是一个开源的Python库，能够检测机器学习系统中的漂移。它提供了传统和最近算法的组合来检测概念和数据漂移。我们的设计目标是使它与任何机器学习框架兼容，并轻松适应实际应用场景。该库遵循一系列最佳开发和持续集成实践，以确保易于维护和扩展。源代码可在https://github.com/IFCA/frouros上获取。

    Frouros is an open-source Python library capable of detecting drift in machine learning systems. It provides a combination of classical and more recent algorithms for drift detection: both concept and data drift. We have designed it with the objective of making it compatible with any machine learning framework and easily adaptable to real-world use cases. The library is developed following a set of best development and continuous integration practices to ensure ease of maintenance and extensibility. The source code is available at https://github.com/IFCA/frouros.
    
[^146]: 走向反对称神经近似函数分离

    Towards Antisymmetric Neural Ansatz Separation. (arXiv:2208.03264v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.03264](http://arxiv.org/abs/2208.03264)

    本文研究了反对称函数模型的分离，提出了一个既可通过Jastrow形式有效表示的反对称函数，却无法用Slater行列式进行逼近的结论。

    

    我们研究了两个反对称函数模型（或\emph{Ans\"atze}）之间的区别，即形式为$f(x_{\sigma(1)},\ldots,x_{\sigma(N)}) = \text{sign}(\sigma)f(x_1,\ldots,x_N)$的函数$f$，其中$\sigma$是任意排列。它们出现在量子化学的背景下，是费米子系统的波函数的基本建模工具。具体而言，我们考虑了两个常见的反对称Ans\"atze：利用行列式交替结构的Slater表示和将Slater行列式增加一个任意对称函数的Jastrow近似。我们构造了一个在Jastrow形式下可以被有效表示的$N$维反对称函数，但可以证明除非有指数级的（$N^2$），否则它不能用Slater行列式进行逼近。这代表了这两个Ans\"atze之间的第一个明确的定量差异。

    We study separations between two fundamental models (or \emph{Ans\"atze}) of antisymmetric functions, that is, functions $f$ of the form $f(x_{\sigma(1)}, \ldots, x_{\sigma(N)}) = \text{sign}(\sigma)f(x_1, \ldots, x_N)$, where $\sigma$ is any permutation. These arise in the context of quantum chemistry, and are the basic modeling tool for wavefunctions of Fermionic systems. Specifically, we consider two popular antisymmetric Ans\"atze: the Slater representation, which leverages the alternating structure of determinants, and the Jastrow ansatz, which augments Slater determinants with a product by an arbitrary symmetric function. We construct an antisymmetric function in $N$ dimensions that can be efficiently expressed in Jastrow form, yet provably cannot be approximated by Slater determinants unless there are exponentially (in $N^2$) many terms. This represents the first explicit quantitative separation between these two Ans\"atze.
    
[^147]: VL-CheckList: 使用物体、属性和关系评估预训练的视觉语言模型

    VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.00221](http://arxiv.org/abs/2207.00221)

    本研究提出VL-CheckList，使用物体、属性和关系评估预训练的视觉语言模型，通过对七种流行的VLP模型进行全面研究分析，揭示出不同模型之间的细微差异。

    

    最近，视觉语言预训练（VLP）模型已经成功地促进了许多跨模态的下游任务。然而，现有的大多数工作都是通过比较下游任务的性能来评估它们的系统。然而，仅有的下游任务平均准确性提供很少关于每种VLP方法的优缺点的信息，更不用说为社区在未来如何改进系统提供见解了。受自然语言处理测试CheckList的启发，我们提出了VL-CheckList，这是一个新颖的框架，用于了解VLP模型的能力。所提出的方法将VLP模型的图像-文本能力分为三类：物体、属性和关系，并使用新颖的分类法进一步分解这三个方面。我们通过该框架对七种最近流行的VLP模型进行全面研究分析。结果通过揭示比较模型之间的细微差异来确认所提出方法的有效性。

    Vision-Language Pretraining (VLP) models have recently successfully facilitated many cross-modal downstream tasks. Most existing works evaluated their systems by comparing the fine-tuned downstream task performance. However, only average downstream task accuracy provides little information about the pros and cons of each VLP method, let alone provides insights on how the community can improve the systems in the future. Inspired by the CheckList for testing natural language processing, we exploit VL-CheckList, a novel framework to understand the capabilities of VLP models. The proposed method divides the image-texting ability of a VLP model into three categories: objects, attributes, and relations, and uses a novel taxonomy to further break down these three aspects. We conduct comprehensive studies to analyze seven recently popular VLP models via the proposed framework. Results confirm the effectiveness of the proposed method by revealing fine-grained differences among the compared mode
    
[^148]: 图神经网络的图池化：进展，挑战和机遇

    Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities. (arXiv:2204.07321v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07321](http://arxiv.org/abs/2204.07321)

    本文提供了有关图神经网络中图池化方法的广泛回顾和分类，以及与之相关的库的概述，展示了这个充满希望和快速发展的研究领域的现有挑战和未来方向。

    

    图神经网络已成为许多图级任务的主要架构，例如图分类和图生成。作为架构的重要组成部分，图池化对于获得整体的图级表示非常重要。虽然在这个充满希望和快速发展的研究领域中提出了许多方法，但据我们所知，很少有人努力系统地总结这些工作。为了为未来的工作奠定基础，本文试图通过提供最近关于图池化的方法的广泛回顾来填补这一空白。具体而言，1）我们首先提出了现有图池化方法的分类法，并为每个类别提供了数学概述；2）然后，我们提供了与图池化相关的库的概述，包括常用的数据集，下游任务的模型架构和开源实现；3）接下来，我们进一步概述了未来研究的方向和挑战。我们期望这个综述可以为进一步发展提供有益的参考。

    Graph neural networks have emerged as a leading architecture for many graph-level tasks, such as graph classification and graph generation. As an essential component of the architecture, graph pooling is indispensable for obtaining a holistic graph-level representation of the whole graph. Although a great variety of methods have been proposed in this promising and fast-developing research field, to the best of our knowledge, little effort has been made to systematically summarize these works. To set the stage for the development of future works, in this paper, we attempt to fill this gap by providing a broad review of recent methods for graph pooling. Specifically, 1) we first propose a taxonomy of existing graph pooling methods with a mathematical summary for each category; 2) then, we provide an overview of the libraries related to graph pooling, including the commonly used datasets, model architectures for downstream tasks, and open-source implementations; 3) next, we further outlin
    
[^149]: 利用神经网络学习量子相变中的拓扑缺陷生成

    Learning topological defects formation with neural networks in a quantum phase transition. (arXiv:2204.06769v2 [cond-mat.dis-nn] CROSS LISTED)

    [http://arxiv.org/abs/2204.06769](http://arxiv.org/abs/2204.06769)

    本文利用神经网络和机器学习算法研究了一维横场量子伊辛模型中的拓扑缺陷，发现激发能与缺陷数量成比例关系，并建立了缺陷数量的前三个累积量之间的普适幂律关系。

    

    神经网络在求解复杂量子多体系统方面拥有强大的表示能力。然而，在处理非平衡过程时，包括量子相变中的关键动力学，神经网络面临更大的挑战。为了解决这个问题，我们利用神经网络和机器学习算法研究了一维横场量子伊辛模型中的拓扑缺陷的时间演化、普适统计和相关性，并计算了系统的能量。我们发现，激发能满足幂律关系，表明激发能与缺陷数量成比例关系。此外，我们建立了缺陷数量的前三个累积量之间的普适幂律关系。

    Neural networks possess formidable representational power, rendering them invaluable in solving complex quantum many-body systems. While they excel at analyzing static solutions, nonequilibrium processes, including critical dynamics during a quantum phase transition, pose a greater challenge for neural networks. To address this, we utilize neural networks and machine learning algorithms to investigate the time evolutions, universal statistics, and correlations of topological defects in a one-dimensional transverse-field quantum Ising model. Specifically, our analysis involves computing the energy of the system during a quantum phase transition following a linear quench of the transverse magnetic field strength. The excitation energies satisfy a power-law relation to the quench rate, indicating a proportional relationship between the excitation energy and the kink numbers. Moreover, we establish a universal power-law relationship between the first three cumulants of the kink numbers and
    
[^150]: 利用抑制变量的线性基础数据审查可解释人工智能(XAI)

    Scrutinizing XAI using linear ground-truth data with suppressor variables. (arXiv:2111.07473v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.07473](http://arxiv.org/abs/2111.07473)

    研究提出了一个关于可解释人工智能(XAI)的方法，该方法提出了特征重要性的客观初步定义，以避免由于方法的行为引起的错误解读。

    

    机器学习(ML)越来越常用于高风险决策中。由于复杂的ML模型(如深度神经网络)通常被认为是黑匣子，因此已经开发出大量程序来阐明其内部运作和预测方式，定义了"可解释人工智能"(XAI)领域。显著性方法根据某种"重要性"度量对输入特征进行排序。由于迄今为止缺乏特征重要性的正式定义，这些方法很难验证。已经证实，一些显著性方法可以突出显示与预测目标没有统计联系的特征(抑制变量)。为了避免由于这种行为引起的错误解读，我们提出了确保此类联系存在是特征重要性的必要条件和客观初步定义。我们精心制作了一个基础数据集，其中所有统计依赖关系都是明确定义的和线性的，

    Machine learning (ML) is increasingly often used to inform high-stakes decisions. As complex ML models (e.g., deep neural networks) are often considered black boxes, a wealth of procedures has been developed to shed light on their inner workings and the ways in which their predictions come about, defining the field of 'explainable AI' (XAI). Saliency methods rank input features according to some measure of 'importance'. Such methods are difficult to validate since a formal definition of feature importance is, thus far, lacking. It has been demonstrated that some saliency methods can highlight features that have no statistical association with the prediction target (suppressor variables). To avoid misinterpretations due to such behavior, we propose the actual presence of such an association as a necessary condition and objective preliminary definition for feature importance. We carefully crafted a ground-truth dataset in which all statistical dependencies are well-defined and linear, se
    
[^151]: 基于惩罚项的分布式稀疏回归

    Distributed Sparse Regression via Penalization. (arXiv:2111.06530v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.06530](http://arxiv.org/abs/2111.06530)

    本文研究了基于惩罚项的分布式稀疏线性回归问题，通过建立统计一致性证明了最优解在$\ell_2$-损失中以接近最优最小率获得。

    

    本文研究了在模拟为无中心节点的无向图的代理网络上进行的稀疏线性回归。估计问题被制定为将局部LASSO损失函数的总和与共识约束的二次惩罚相结合的最小化--后者对于获得分布式解决方法至关重要。虽然基于惩罚项的共识方法在优化文献中得到了广泛研究，但在高维设置中它们的统计和计算保证还不清楚。 这项工作提供了这个开放问题的答案。我们的贡献有两个。首先，我们建立了估计器的统计一致性：在合适的惩罚参数选择下，惩罚问题的最优解在$\ell_2$- 损失中以接近最优最小率 $\mathcal{O}(s \log d/N)$ 获得 ，其中 $s$是稀疏值，$d$是环境维数，$N$是网络中的总样本大小--这与集中式的结果相匹配。

    We study sparse linear regression over a network of agents, modeled as an undirected graph (with no centralized node). The estimation problem is formulated as the minimization of the sum of the local LASSO loss functions plus a quadratic penalty of the consensus constraint -- the latter being instrumental to obtain distributed solution methods. While penalty-based consensus methods have been extensively studied in the optimization literature, their statistical and computational guarantees in the high dimensional setting remain unclear. This work provides an answer to this open problem. Our contribution is two-fold. First, we establish statistical consistency of the estimator: under a suitable choice of the penalty parameter, the optimal solution of the penalized problem achieves near optimal minimax rate $\mathcal{O}(s \log d/N)$ in $\ell_2$-loss, where $s$ is the sparsity value, $d$ is the ambient dimension, and $N$ is the total sample size in the network -- this matches centralized s
    
[^152]: CounterNet：具有预测感知因果解释的端到端训练

    CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations. (arXiv:2109.07557v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.07557](http://arxiv.org/abs/2109.07557)

    本文提出了一个新的端到端学习框架CounterNet，将机器学习模型训练和相应的因果解释生成结合在一起。CounterNet通过一起训练ML模型和生成CF解释来提供预测感知的CF解释，并减少计算开销。

    

    本文提出了一个新的端到端学习框架CounterNet，将机器学习模型训练和相应的因果解释生成结合在一起。因果解释提供了对照案例，即试图找到最小的修改特征值的实例，将ML模型对该实例的预测改变为预定义输出。之前生成因果解释的技术存在两个主要限制：它们都是针对专有ML模型的后续方法，并且它们的生成过程缺乏对ML模型的训练的启示，这导致了模型预测和解释之间的错位；大多数技术依赖于解决针对每个输入数据点的分离时限制的优化问题来找到CF解释(这对他们的运行时间有负面影响)。本文通过引入端到端框架CounterNet做出了新贡献，将机器学习模型训练与生成CF解释相结合。通过一起训练ML模型和生成CF解释，CounterNet解决了之前技术的限制，并提供了与模型预测和解释更好对齐的预测感知的CF解释。此外，CounterNet避免了对每个输入数据点解决分离的优化问题，从而减少了计算开销。

    This work presents CounterNet, a novel end-to-end learning framework which integrates Machine Learning (ML) model training and the generation of corresponding counterfactual (CF) explanations into a single end-to-end pipeline. Counterfactual explanations offer a contrastive case, i.e., they attempt to find the smallest modification to the feature values of an instance that changes the prediction of the ML model on that instance to a predefined output. Prior techniques for generating CF explanations suffer from two major limitations: (i) all of them are post-hoc methods designed for use with proprietary ML models -- as a result, their procedure for generating CF explanations is uninformed by the training of the ML model, which leads to misalignment between model predictions and explanations; and (ii) most of them rely on solving separate time-intensive optimization problems to find CF explanations for each input data point (which negatively impacts their runtime). This work makes a nove
    
[^153]: 控制抽样方案中的隐私损失: 分层抽样和聚类抽样的分析。

    Controlling Privacy Loss in Sampling Schemes: an Analysis of Stratified and Cluster Sampling. (arXiv:2007.12674v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2007.12674](http://arxiv.org/abs/2007.12674)

    本文扩展了隐私扩大结果的研究，对更为复杂、依赖于数据的抽样方案进行了分析，发现这些方案可能导致隐私损害；文章分析了聚类抽样和分层抽样范例的隐私影响。

    

    抽样方案是统计学、调查设计和算法设计中的基本工具。在差分隐私中的一个基本结果是，对于一个人群的简单随机样本运行的差异私密机制比对整个人群运行相同算法提供更强的隐私保证。然而，在实践中，抽样设计往往比在先前的工作中所涉及到的简单的独立于数据的抽样方案更加复杂。在本文中，我们将隐私扩大结果的研究扩展到更复杂的依赖于数据的抽样方案。我们发现，这些抽样方案不仅通常无法扩大隐私，而且可能会导致隐私损害。我们分析了流行的聚类抽样和分层抽样范例的隐私影响，同时为更普遍的抽样设计的研究提供了一些见解。

    Sampling schemes are fundamental tools in statistics, survey design, and algorithm design. A fundamental result in differential privacy is that a differentially private mechanism run on a simple random sample of a population provides stronger privacy guarantees than the same algorithm run on the entire population. However, in practice, sampling designs are often more complex than the simple, data-independent sampling schemes that are addressed in prior work. In this work, we extend the study of privacy amplification results to more complex, data-dependent sampling schemes. We find that not only do these sampling schemes often fail to amplify privacy, they can actually result in privacy degradation. We analyze the privacy implications of the pervasive cluster sampling and stratified sampling paradigms, as well as provide some insight into the study of more general sampling designs.
    

