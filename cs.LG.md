# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets.](http://arxiv.org/abs/2311.01588) | 该论文研究了通过领域适应图神经网络对宇宙学参数进行约束的方法。通过利用GNNs捕捉宇宙学信息和使用最大均值差异进行领域适应，该方法在不同数据集上具有较好的泛化能力。 |
| [^2] | [FedSN: A General Federated Learning Framework over LEO Satellite Networks.](http://arxiv.org/abs/2311.01483) | FedSN是一个通用的联邦学习框架，用于解决在LEO卫星网络中的异构计算和存储能力、有限的上行速率以及模型陈旧等关键挑战。 |
| [^3] | [Diffusion models for probabilistic programming.](http://arxiv.org/abs/2311.00474) | 我们提出了一种新的扩散模型变分推断（DMVI）方法，用于在概率编程语言中进行自动近似推断。DMVI可以更准确地进行后验推断，而且易于实现和使用，对神经网络模型没有任何约束。 |
| [^4] | [Fast and Reliable Generation of EHR Time Series via Diffusion Models.](http://arxiv.org/abs/2310.15290) | 本研究通过使用扩散模型提出了一种快速可靠生成EHR时间序列数据的新方法，该方法在数据效用方面明显优于现有方法，并且对训练工作的需求更少。同时，该方法还提供了多样化和真实的合成EHR数据，增强了下游医疗数据分析。 |
| [^5] | [Correspondence learning between morphologically different robots through task demonstrations.](http://arxiv.org/abs/2310.13458) | 本论文提出了一种学习不同形态机器人间对应关系的方法，通过演示实现了共同的潜变量表示，使得不同机器人可以更直接地转移技能。 |
| [^6] | [On the Representational Capacity of Recurrent Neural Language Models.](http://arxiv.org/abs/2310.12942) | 本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。 |
| [^7] | [A General Theoretical Paradigm to Understand Learning from Human Preferences.](http://arxiv.org/abs/2310.12036) | 本文研究了学习从人类偏好中学习的实际算法的理论基础，推导出一个新的一般目标，绕过了两个重要的近似。这种方法允许直接从收集的数据中学习策略而无需奖励模型的训练。 |
| [^8] | [Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography.](http://arxiv.org/abs/2310.08897) | 本研究提出了一种自监督卷积核手工特征融合方法，用于增强超声心动图左室高血压病变的识别。通过将卷积滤波器应用于自监督学习预处理中，将图像转换为特征图，实现了在不同成像设备和协议下的一致特征提取。 |
| [^9] | [Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks.](http://arxiv.org/abs/2310.03789) | 本研究将自适应核方法应用于两个师生模型，预测了特征学习和 Grokking 的性质，并展示了 Grokking 与相变理论之间的映射关系。 |
| [^10] | [Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients.](http://arxiv.org/abs/2310.01012) | 本论文提出了一个新颖的无约束目标，通过应用随机梯度下降（SGD）到CCA目标，实现了一系列快速算法，包括随机PLS、随机CCA和深度CCA。这些方法在各种基准测试中表现出比先前最先进方法更快的收敛速度和更高的相关性恢复。 |
| [^11] | [GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization.](http://arxiv.org/abs/2309.16020) | GeoCLIP是一种受Clip启发的图像到GPS检索方法，用于全球地理定位。它通过对齐图像和其对应的GPS位置来提高定位精度，并克服了传统方法中固定分类的局限性。 |
| [^12] | [Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling.](http://arxiv.org/abs/2309.11983) | 本论文将连接主义时间分类（CTC）与变分模型相结合，提出了两个版本的新型变分CTC，用于训练更具普适性的保序序列模型。这些方法允许直接优化模型对数似然的变分下界，并解决了计算上的挑战。 |
| [^13] | [HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks.](http://arxiv.org/abs/2309.08549) | 本论文提出了一种名为健康影响力噪声训练的高效稳健训练方法，该方法使用影响函数制造了有助于加强分类模型对抗数据污染攻击的健康噪声，并且在仅修改训练数据的子集时也能有效运行。 |
| [^14] | [Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis.](http://arxiv.org/abs/2309.07675) | 本文提出了一种基于集合的可达性分析方法，通过 emergent representation 实现目标空间抽象，在分层强化学习中自主发现符号目标表示，并引入封建HRL算法来同时学习目标表示和分层策略。 |
| [^15] | [BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions.](http://arxiv.org/abs/2308.09936) | BLIVA是一个简单的多模态LLM模型，用于更好地处理嵌入了文本的图像上下文场景。它通过引入InstructBLIP的查询嵌入和LLaVA的编码补丁嵌入技术来改进视觉语言模型，从而有效解决了文本丰富的视觉问题。 |
| [^16] | [Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space.](http://arxiv.org/abs/2307.13390) | 本文介绍了一种通过在自编码器的潜空间中进行高斯混合分布搜索来生成反事实解释的方法。 |
| [^17] | [Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting.](http://arxiv.org/abs/2307.11494) | 本研究提出了一种面向概率时间序列预测的自引导扩散模型，称为TSDiff。该模型不需要辅助网络或训练过程的改变，在预测、改进和合成数据生成等时间序列任务上展现出了竞争力。 |
| [^18] | [Epsilon*: Privacy Metric for Machine Learning Models.](http://arxiv.org/abs/2307.11280) | Epsilon*是一种用于测量机器学习模型隐私风险的新度量方法，不需要访问训练数据或模型训练算法，能与成员推断攻击中的假设检验相结合，提供对经过训练的模型实例隐私损失的下界，避免数值和噪声放大不稳定性。 |
| [^19] | [Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms.](http://arxiv.org/abs/2307.03357) | 本文通过统计学习理论的算法稳定性，分析了随机组合梯度下降算法的稳定性和泛化性，引入了组合一致稳定性概念并与SCO问题的泛化性建立了定量关系。 |
| [^20] | [Why Shallow Networks Struggle with Approximating and Learning High Frequency: A Numerical Study.](http://arxiv.org/abs/2306.17301) | 本文通过数值研究探讨了浅层神经网络在逼近和学习高频率方面的困难，重点是通过分析激活函数的谱分析来理解问题的原因。 |
| [^21] | [DNA-TEQ: An Adaptive Exponential Quantization of Tensors for DNN Inference.](http://arxiv.org/abs/2306.16430) | 本文介绍了一种在DNN推理中使用的自适应指数量化张量方法DNA-TEQ，该方法通过发现大量张量符合指数分布来实现最佳的数值精度和准确性损失平衡。 |
| [^22] | [Differentially Private Wireless Federated Learning Using Orthogonal Sequences.](http://arxiv.org/abs/2306.08280) | 本文提出了一种使用正交序列的FLORAS方法，可消除发送端的信道状态信息，同时提供了项目级和客户级的差分隐私保证。FLORAS可以灵活地实现不同的差分隐私等级，并且通过推导收敛界限，实现了收敛速度和隐私保证之间的平稳权衡。 |
| [^23] | [NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics.](http://arxiv.org/abs/2306.06202) | 本文介绍了神经成像领域的图机器学习基准测试NeuroGraph，并探讨了数据集生成的搜索空间。 |
| [^24] | [ShaDDR: Real-Time Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering.](http://arxiv.org/abs/2306.04889) | 本文介绍了一种基于示例的深度生成神经网络 ShaDDR，可以通过几何细节化和条件纹理生成应用于输入的粗略体素形状，生成高分辨率贴图的 3D 形状。生成实时且精度高，风格可以通过学习的潜在代码进行控制。 |
| [^25] | [Efficient Vision Transformer for Human Pose Estimation via Patch Selection.](http://arxiv.org/abs/2306.04225) | 该论文提出了一种基于补丁选择的高效视觉Transformer方法，大幅度提高了处理速度和降低计算复杂度，用于2D人体姿态估计方面。 |
| [^26] | [Hinge-Wasserstein: Mitigating Overconfidence in Regression by Classification.](http://arxiv.org/abs/2306.00560) | 该论文提出了一种基于Wasserstein距离的损失函数hinge-Wasserstein，用于缓解回归任务中由于过度自信导致的不确定性问题。这种损失函数有效提高了aleatoric和epistemic不确定性的质量。 |
| [^27] | [On sampling determinantal and Pfaffian point processes on a quantum computer.](http://arxiv.org/abs/2305.15851) | 本文总结了在量子计算机上采样确定性行列式和Pfaffian点过程的状态及其优化方式。 |
| [^28] | [Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification.](http://arxiv.org/abs/2305.14032) | 本研究提出了一种新的通过在音频数据上进行对比学习的方法，在呼吸音分类任务中取得了最先进的性能表现。 |
| [^29] | [Representing Input Transformations by Low-Dimensional Parameter Subspaces.](http://arxiv.org/abs/2305.13536) | 本文提出配置子空间假设，为连续参数化变换最优模型权重可以存在于低维线性子空间中。我们引入自定义网络学习这些子空间，并观察到它们的低维结构可以在所有测试变换中使用。 |
| [^30] | [A principled deep learning approach for geological facies generation.](http://arxiv.org/abs/2305.13318) | 本研究使用基于深度学习原理的生成对抗网络和深度变分推理应用于地质岩相生成，针对地下渠道进行了有条件模拟，并且比传统地质统计模型具有更高水平的准确性和物理逼真性。 |
| [^31] | [Sensor Fault Detection and Isolation in Autonomous Nonlinear Systems Using Neural Network-Based Observers.](http://arxiv.org/abs/2304.08837) | 本文介绍了一种基于神经网络的观测器方法，可用于检测和隔离工业系统中的传感器故障，适用于一般的自主非线性系统，通过学习实现Lueneberger观察器的设计，通过残留生成检测传感器故障并实现故障隔离。 |
| [^32] | [LASER: Neuro-Symbolic Learning of Semantic Video Representations.](http://arxiv.org/abs/2304.07647) | LASER提出了一种神经符号学习方法来学习语义视频表示，通过逻辑规范捕捉视频数据中的时空属性，能够对齐原始视频和规范，有效地训练低级感知模型以提取符合所需高级规范的视频表示。 |
| [^33] | [Randomized Adversarial Style Perturbations for Domain Generalization.](http://arxiv.org/abs/2304.01959) | 本文提出了一种随机对抗风格扰动技术，它能够通过对抗性扰动特征风格达到域泛化的效果，同时结合混合原始特征的方法缓解扰动带来的挑战。 |
| [^34] | [Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes.](http://arxiv.org/abs/2302.11381) | 本论文展示了一个不需要对目标函数正则化的未正则化PMD算法族，可以实现PI的维度自由的线性$\gamma$收敛速率，这个速率是最优的，且自适应步长是实现这个速率所必需的。 |
| [^35] | [Scalable Real-Time Recurrent Learning Using Sparse Connections and Selective Learning.](http://arxiv.org/abs/2302.05326) | 本文提出了两个限制使得实时循环学习算法具有可扩展性，分别是将网络分解为独立模块或逐步学习网络。与其他可扩展算法不同的是，这些算法不会向梯度估计添加噪声或偏差，而是通过权衡网络的功能能力以实现可扩展学习。 |
| [^36] | [GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition.](http://arxiv.org/abs/2207.12261) | 本文提出了一种基于有向图的跨模态特征补充方法，可以提取多模态上下文信息和交互信息，缓解了多模态融合中的异构性差距问题。 |
| [^37] | [Degree-Preserving Randomized Response for Graph Neural Networks under Local Differential Privacy.](http://arxiv.org/abs/2202.10209) | 本文提出了一种保持度数不变的随机化响应算法，用于在无属性图中提供高准确性的边的局部差分隐私保护。通过使用Warner的随机化响应和策略性边采样，我们的算法能够保护用户的隐私同时保留图结构。 |

# 详细

[^1]: 针对多个数据集约束宇宙学参数的领域适应图神经网络

    Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v1 [astro-ph.CO])

    [http://arxiv.org/abs/2311.01588](http://arxiv.org/abs/2311.01588)

    该论文研究了通过领域适应图神经网络对宇宙学参数进行约束的方法。通过利用GNNs捕捉宇宙学信息和使用最大均值差异进行领域适应，该方法在不同数据集上具有较好的泛化能力。

    

    研究表明，与依赖于摘要统计量（如功率谱）的方法相比，深度学习模型在从复杂宇宙学数据集中提取信息方面表现更好。然而，由于不同模拟套件中的子网格物理实现和数值逼近的差异，模型在一个宇宙学模拟的数据上训练后，在另一个模拟数据上的表现会下降。同样，对于任何模拟数据训练的模型，在应用于观测数据时也可能出现性能下降。通过在两个不同套件的CAMELS水动力宇宙学模拟数据上进行训练，我们研究了领域适应图神经网络（DA-GNNs）的泛化能力。通过利用GNNs，我们可以利用它们捕捉来自星系分布的结构无标度宇宙学信息的能力。此外，通过包括无监督的领域适配最大均值差异（MMD），我们使模型能够自适应地学习两个模拟数据之间的差异。

    Deep learning models have been shown to outperform methods that rely on summary statistics, like the power spectrum, in extracting information from complex cosmological data sets. However, due to differences in the subgrid physics implementation and numerical approximations across different simulation suites, models trained on data from one cosmological simulation show a drop in performance when tested on another. Similarly, models trained on any of the simulations would also likely experience a drop in performance when applied to observational data. Training on data from two different suites of the CAMELS hydrodynamic cosmological simulations, we examine the generalization capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing GNNs, we capitalize on their capacity to capture structured scale-free cosmological information from galaxy distributions. Moreover, by including unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable our models to ex
    
[^2]: FedSN：一个适用于LEO卫星网络的通用联邦学习框架

    FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])

    [http://arxiv.org/abs/2311.01483](http://arxiv.org/abs/2311.01483)

    FedSN是一个通用的联邦学习框架，用于解决在LEO卫星网络中的异构计算和存储能力、有限的上行速率以及模型陈旧等关键挑战。

    

    最近，许多低地球轨道（LEO）卫星已经由商业公司成功地发射和部署到太空中，如SpaceX。由于LEO卫星配备了多模传感器，它们不仅用于通信，还用于各种机器学习应用，如空间调制识别、遥感图像分类等。然而，由于与LEO卫星的有限接触时间（例如5分钟），地面站（GS）可能无法下载如此大量的原始感测数据进行集中模型训练。因此，联邦学习（FL）已经成为解决这个问题的有希望的解决方案，通过在设备上进行训练。不幸的是，要在LEO卫星上使用FL，我们仍然面临三个关键挑战，即i）异构计算和存储能力，ii）有限的上行速率，以及iii）模型陈旧问题。为此，我们提出了一种名为FedSN的通用FL框架来解决上述挑战，一

    Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
    
[^3]: 概率编程的扩散模型

    Diffusion models for probabilistic programming. (arXiv:2311.00474v1 [cs.LG])

    [http://arxiv.org/abs/2311.00474](http://arxiv.org/abs/2311.00474)

    我们提出了一种新的扩散模型变分推断（DMVI）方法，用于在概率编程语言中进行自动近似推断。DMVI可以更准确地进行后验推断，而且易于实现和使用，对神经网络模型没有任何约束。

    

    我们提出了扩散模型变分推断（DMVI），这是一种在概率编程语言（PPL）中进行自动近似推断的新方法。DMVI利用扩散模型作为对真实后验分布的变分近似，通过导出贝叶斯建模中使用的边际似然目标的新约束。DMVI易于实现，在PPL中进行无障碍推断，不像使用归一化流的变分推断那样具有缺点，并且对基础神经网络模型不做任何约束。我们在一组常见的贝叶斯模型上评估了DMVI，并表明它的后验推断一般比PPL中使用的现代方法更准确，同时具有类似的计算成本并且需要较少的手动调整。

    We propose Diffusion Model Variational Inference (DMVI), a novel method for automated approximate inference in probabilistic programming languages (PPLs). DMVI utilizes diffusion models as variational approximations to the true posterior distribution by deriving a novel bound to the marginal likelihood objective used in Bayesian modelling. DMVI is easy to implement, allows hassle-free inference in PPLs without the drawbacks of, e.g., variational inference using normalizing flows, and does not make any constraints on the underlying neural network model. We evaluate DMVI on a set of common Bayesian models and show that its posterior inferences are in general more accurate than those of contemporary methods used in PPLs while having a similar computational cost and requiring less manual tuning.
    
[^4]: 通过扩散模型快速可靠地生成电子健康记录时间序列

    Fast and Reliable Generation of EHR Time Series via Diffusion Models. (arXiv:2310.15290v1 [cs.LG])

    [http://arxiv.org/abs/2310.15290](http://arxiv.org/abs/2310.15290)

    本研究通过使用扩散模型提出了一种快速可靠生成EHR时间序列数据的新方法，该方法在数据效用方面明显优于现有方法，并且对训练工作的需求更少。同时，该方法还提供了多样化和真实的合成EHR数据，增强了下游医疗数据分析。

    

    电子健康记录（EHR）是丰富的患者级数据来源，包括实验室检验、药物和诊断，为医疗数据分析提供了宝贵资源。然而，对隐私的担忧常常限制了对EHR的访问，阻碍了下游分析。研究人员已经探索了各种方法来生成保护隐私的EHR数据。在本研究中，我们引入了一种使用去噪扩散概率模型（DDPM）生成多样化和真实的合成EHR时间序列数据的新方法。我们对六个数据集进行了实验证明，我们的方法在数据效用方面明显优于七种现有方法，并且需要更少的训练工作。我们的方法还通过提供多样化和真实的合成EHR数据来增强下游医疗数据分析。

    Electronic Health Records (EHRs) are rich sources of patient-level data, including laboratory tests, medications, and diagnoses, offering valuable resources for medical data analysis. However, concerns about privacy often restrict access to EHRs, hindering downstream analysis. Researchers have explored various methods for generating privacy-preserving EHR data. In this study, we introduce a new method for generating diverse and realistic synthetic EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We conducted experiments on six datasets, comparing our proposed method with seven existing methods. Our results demonstrate that our approach significantly outperforms all existing methods in terms of data utility while requiring less training effort. Our approach also enhances downstream medical data analysis by providing diverse and realistic synthetic EHR data.
    
[^5]: 不同形态机器人间的任务演示学习与对应关系

    Correspondence learning between morphologically different robots through task demonstrations. (arXiv:2310.13458v1 [cs.RO])

    [http://arxiv.org/abs/2310.13458](http://arxiv.org/abs/2310.13458)

    本论文提出了一种学习不同形态机器人间对应关系的方法，通过演示实现了共同的潜变量表示，使得不同机器人可以更直接地转移技能。

    

    我们观察到机器人在其机身、传感器和执行器方面有着各种各样的差异。考虑到机器人领域的巨大多样性，独立地教导每个不同机器人的每个技能是低效且不可扩展的。如果我们能够学习不同机器人的感官运动空间之间的对应关系，那么我们可以期望在一个机器人上学习的技能可以更直接有效地转移到其他机器人上。本文提出了一种方法，在具有关节控制的固定基座操纵机器人和差动驱动移动机器人之间学习对应关系。为此，首先让两个机器人进行执行相同任务的演示。在学习对应策略的同时形成一个共同的潜变量表示。在这个初始学习阶段之后，通过观察一个机器人的新任务执行就足以生成一个潜变量的表示，从而实现对应关系的学习。

    We observe a large variety of robots in terms of their bodies, sensors, and actuators. Given the commonalities in the skill sets, teaching each skill to each different robot independently is inefficient and not scalable when the large variety in the robotic landscape is considered. If we can learn the correspondences between the sensorimotor spaces of different robots, we can expect a skill that is learned in one robot can be more directly and easily transferred to the other robots. In this paper, we propose a method to learn correspondences between robots that have significant differences in their morphologies: a fixed-based manipulator robot with joint control and a differential drive mobile robot. For this, both robots are first given demonstrations that achieve the same tasks. A common latent representation is formed while learning the corresponding policies. After this initial learning stage, the observation of a new task execution by one robot becomes sufficient to generate a lat
    
[^6]: 关于循环神经网络语言模型的表示能力的研究

    On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.12942](http://arxiv.org/abs/2310.12942)

    本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。

    

    本研究调查了基于循环神经网络(RNNs)的语言模型(LMs)的计算表达性。Siegelmann和Sontag(1992)曾经展示了具有有理权重和隐藏状态以及无限计算时间的RNNs是图灵完备的。然而，LMs不仅定义了字符串上的加权，还定义了(非加权)语言成员关系，对RNN LMs（RLMs）的计算能力分析应该反映这一点。我们将图灵完备性结果扩展到概率情况，展示了如何使用有理权重的RLM和无限计算时间来模拟任何概率图灵机(PTM)。由于在实践中，RLMs实时工作，每个时间步骤处理一个符号，因此我们将上述结果作为RLMs表达性的上界。我们还通过展示在实时计算限制下，这些模型可以模拟确定性实时有理PTMs来提供下界。

    This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
    
[^7]: 一个理论框架来理解从人类偏好中学习的一般方法

    A General Theoretical Paradigm to Understand Learning from Human Preferences. (arXiv:2310.12036v1 [cs.AI])

    [http://arxiv.org/abs/2310.12036](http://arxiv.org/abs/2310.12036)

    本文研究了学习从人类偏好中学习的实际算法的理论基础，推导出一个新的一般目标，绕过了两个重要的近似。这种方法允许直接从收集的数据中学习策略而无需奖励模型的训练。

    

    目前从人类偏好中学习的流行方法依赖于两个重要的近似：第一假设可以用逐点奖励替代成对偏好。第二个假设是在这些逐点奖励上训练的奖励模型可以从收集到的数据泛化到策略采样的超出分布的数据。最近，提出了一种称为直接偏好优化(DPO)的方法，该方法绕过了第二个近似，并直接从收集的数据中学习策略而无需奖励模型阶段。然而，这种方法仍然严重依赖于第一个近似。在本文中，我们试图对这些实际算法进行更深入的理论理解。特别地，我们推导出了一个新的一般目标，称为ΨPO，用于从人类偏好中学习，该目标以成对偏好的形式表达，因此绕过了这两个近似。这个新的一般目标使我们能够进行一种新的从训练数据直接学习策略的方法而无需进行奖励模型的训练。

    The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.  In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an 
    
[^8]: 自监督卷积核手工特征的特色融合：增强超声心动图左室高血压病变表型的识别

    Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography. (arXiv:2310.08897v1 [eess.IV])

    [http://arxiv.org/abs/2310.08897](http://arxiv.org/abs/2310.08897)

    本研究提出了一种自监督卷积核手工特征融合方法，用于增强超声心动图左室高血压病变的识别。通过将卷积滤波器应用于自监督学习预处理中，将图像转换为特征图，实现了在不同成像设备和协议下的一致特征提取。

    

    放射学特征学是一种通过图像提取定量手工特征来预测疾病的医学成像技术。在这些特征中进行融合，可以确保在不同的成像设备和协议中进行一致的特征提取。融合的方法包括标准化成像协议、统计调整和评估特征的稳健性。通过超声心动图可以诊断心肌疾病，如左室肥厚(LVH)和高血压心脏病(HHD)，但不同的成像设置会带来挑战。在这种情况下，特征融合技术对于在疾病诊断中应用手工特征至关重要。自监督学习(SSl)通过限制的数据集增强数据理解，并适应多样的数据设置。ConvNeXt-V2将卷积层集成到SSL中，在各种任务中展现出优越的性能。本研究侧重于SSL中的卷积滤波器，将它们用作预处理，将图像转换为特征图。

    Radiomics, a medical imaging technique, extracts quantitative handcrafted features from images to predict diseases. Harmonization in those features ensures consistent feature extraction across various imaging devices and protocols. Methods for harmonization include standardized imaging protocols, statistical adjustments, and evaluating feature robustness. Myocardial diseases such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD) are diagnosed via echocardiography, but variable imaging settings pose challenges. Harmonization techniques are crucial for applying handcrafted features in disease diagnosis in such scenario. Self-supervised learning (SSL) enhances data understanding within limited datasets and adapts to diverse data settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying superior performance in various tasks. This study focuses on convolutional filters within SSL, using them as preprocessing to convert images into feature maps for h
    
[^9]: 好表示的液滴：在两层网络中 grokking 作为一阶相变

    Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks. (arXiv:2310.03789v1 [stat.ML])

    [http://arxiv.org/abs/2310.03789](http://arxiv.org/abs/2310.03789)

    本研究将自适应核方法应用于两个师生模型，预测了特征学习和 Grokking 的性质，并展示了 Grokking 与相变理论之间的映射关系。

    

    深度神经网络 (DNN) 的一个关键特性是在训练过程中能够学习新的特征。这种深度学习的有趣方面在最近报道的 Grokking 现象中表现得最为明显。虽然主要体现为测试准确性的突变增加，但 Grokking 也被认为是一种超越懒惰学习/高斯过程 (GP) 的现象，涉及特征学习。在这里，我们将特征学习理论的最新发展，自适应核方法，应用于具有立方多项式和模加法教师的两个师生模型。我们在这些模型上提供了关于特征学习和 Grokking 性质的分析预测，并展示了 Grokking 与相变理论之间的映射关系。我们表明，在 Grokking 之后，DNN 的状态类似于一阶相变后的混合相。在这个混合相中，DNN 生成了与之前明显不同的教师的有用内部表示。

    A key property of deep neural networks (DNNs) is their ability to learn new features during training. This intriguing aspect of deep learning stands out most clearly in recently reported Grokking phenomena. While mainly reflected as a sudden increase in test accuracy, Grokking is also believed to be a beyond lazy-learning/Gaussian Process (GP) phenomenon involving feature learning. Here we apply a recent development in the theory of feature learning, the adaptive kernel approach, to two teacher-student models with cubic-polynomial and modular addition teachers. We provide analytical predictions on feature learning and Grokking properties of these models and demonstrate a mapping between Grokking and the theory of phase transitions. We show that after Grokking, the state of the DNN is analogous to the mixed phase following a first-order phase transition. In this mixed phase, the DNN generates useful internal representations of the teacher that are sharply distinct from those before the 
    
[^10]: CCA家族的高效算法：无约束目标与无偏梯度

    Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])

    [http://arxiv.org/abs/2310.01012](http://arxiv.org/abs/2310.01012)

    本论文提出了一个新颖的无约束目标，通过应用随机梯度下降（SGD）到CCA目标，实现了一系列快速算法，包括随机PLS、随机CCA和深度CCA。这些方法在各种基准测试中表现出比先前最先进方法更快的收敛速度和更高的相关性恢复。

    

    典型相关分析（CCA）方法在多视角学习中具有基础性作用。正则化线性CCA方法可以看作是偏最小二乘（PLS）的推广，并与广义特征值问题（GEP）框架统一。然而，这些线性方法的传统算法在大规模数据上计算上是不可行的。深度CCA的扩展显示出很大的潜力，但目前的训练过程缓慢且复杂。我们首先提出了一个描述GEPs的顶级子空间的新颖无约束目标。我们的核心贡献是一系列快速算法，用随机梯度下降（SGD）应用于相应的CCA目标，从而获得随机PLS、随机CCA和深度CCA。这些方法在所有标准CCA和深度CCA基准测试中显示出比先前最先进方法更快的收敛速度和更高的相关性恢复。这样的速度使我们能够首次进行大规模生物数据的PLS分析。

    The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
    
[^11]: GeoCLIP：受Clip启发的地点和图像对齐方法，用于有效的全球地理定位

    GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization. (arXiv:2309.16020v1 [cs.CV])

    [http://arxiv.org/abs/2309.16020](http://arxiv.org/abs/2309.16020)

    GeoCLIP是一种受Clip启发的图像到GPS检索方法，用于全球地理定位。它通过对齐图像和其对应的GPS位置来提高定位精度，并克服了传统方法中固定分类的局限性。

    

    全球地理定位旨在确定遥感图像的精确位置。由于地理景观的巨大变化，这项任务面临着巨大的挑战。 基于图像检索的方法无法在全球范围内解决这个问题，因为构建涵盖整个世界的大型图像库是不可行的。 相反，现有方法将全球划分为离散的地理单元，将问题转化为分类任务。 然而，这些方法的性能受到预定义类别的限制，并且当图像的位置与其类别中心显著偏离时，往往导致不准确的定位。 为了克服这些限制，我们提出了GeoCLIP，一种新颖的受Clip启发的图像到GPS检索方法，强制进行图像与其对应的GPS位置之间的对齐。 GeoCLIP的位置编码器通过使用随机傅里叶特征进行位置编码，将地球建模为连续函数。

    Worldwide Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth. This task has considerable challenges due to immense variation in geographic landscapes. The image-to-image retrieval-based approaches fail to solve this problem on a global scale as it is not feasible to construct a large gallery of images covering the entire world. Instead, existing approaches divide the globe into discrete geographic cells, transforming the problem into a classification task. However, their performance is limited by the predefined classes and often results in inaccurate localizations when an image's location significantly deviates from its class center. To overcome these limitations, we propose GeoCLIP, a novel CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between the image and its corresponding GPS locations. GeoCLIP's location encoder models the Earth as a continuous function by employing positional encoding through random Fourier features an
    
[^12]: 变分连接主义时间分类用于保序序列建模

    Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling. (arXiv:2309.11983v1 [cs.LG])

    [http://arxiv.org/abs/2309.11983](http://arxiv.org/abs/2309.11983)

    本论文将连接主义时间分类（CTC）与变分模型相结合，提出了两个版本的新型变分CTC，用于训练更具普适性的保序序列模型。这些方法允许直接优化模型对数似然的变分下界，并解决了计算上的挑战。

    

    连接主义时间分类（CTC）常被用于保序序列建模任务，比如语音识别，其中保持输入和目标序列的顺序是必要的。然而，CTC仅应用于确定性序列模型，其中潜在空间是不连续且稀疏的，这使得它们在处理数据的变异性方面比变分模型能力更弱。在本文中，我们将CTC与变分模型相结合，并导出了可以用于训练更具普适性的序列模型的损失函数，以保持顺序。具体而言，我们根据两个合理的假设导出了两个版本的新型变分CTC，第一个假设是每个时间步的变分潜在变量在条件下是独立的；第二个假设是这些潜在变量是马尔可夫的。我们展示了这两个损失函数都允许直接优化模型对数似然的变分下界，并且展示了计算上的一些挑战和解决方案。

    Connectionist temporal classification (CTC) is commonly adopted for sequence modeling tasks like speech recognition, where it is necessary to preserve order between the input and target sequences. However, CTC is only applied to deterministic sequence models, where the latent space is discontinuous and sparse, which in turn makes them less capable of handling data variability when compared to variational models. In this paper, we integrate CTC with a variational model and derive loss functions that can be used to train more generalizable sequence models that preserve order. Specifically, we derive two versions of the novel variational CTC based on two reasonable assumptions, the first being that the variational latent variables at each time step are conditionally independent; and the second being that these latent variables are Markovian. We show that both loss functions allow direct optimization of the variational lower bound for the model log-likelihood, and present computationally t
    
[^13]: 基于健康影响力噪声的训练来抵御数据污染攻击的方法

    HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v1 [cs.LG])

    [http://arxiv.org/abs/2309.08549](http://arxiv.org/abs/2309.08549)

    本论文提出了一种名为健康影响力噪声训练的高效稳健训练方法，该方法使用影响函数制造了有助于加强分类模型对抗数据污染攻击的健康噪声，并且在仅修改训练数据的子集时也能有效运行。

    

    虽然已经提出了许多防御方法来防止来自不可信数据源的潜在污染攻击，但大多数研究仅针对特定攻击进行防御，这给了攻击者许多可利用的机会。在本论文中，我们提出了一种基于影响函数的高效稳健训练方法，名为健康影响力噪声训练。通过使用影响函数，我们制造了有助于加强分类模型对抗污染攻击的健康噪声，同时不会对测试数据的泛化能力产生显著影响。此外，我们的方法可以在仅修改训练数据的子集时有效运行，而不是如几种之前的方法中那样向所有示例添加噪声。我们在两个图像数据集上进行了全面评估，并考虑不同的实际攻击场景下的最新攻击技术。我们的实证结果表明，H

    While numerous defense methods have been proposed to prohibit potential poisoning attacks from untrusted data sources, most research works only defend against specific attacks, which leaves many avenues for an adversary to exploit. In this work, we propose an efficient and robust training approach to defend against data poisoning attacks based on influence functions, named Healthy Influential-Noise based Training. Using influence functions, we craft healthy noise that helps to harden the classification model against poisoning attacks without significantly affecting the generalization ability on test data. In addition, our method can perform effectively when only a subset of the training data is modified, instead of the current method of adding noise to all examples that has been used in several previous works. We conduct comprehensive evaluations over two image datasets with state-of-the-art poisoning attacks under different realistic attack scenarios. Our empirical results show that H
    
[^14]: 通过基于集合的可达性分析，在分层强化学习中实现目标空间抽象

    Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis. (arXiv:2309.07675v1 [cs.LG])

    [http://arxiv.org/abs/2309.07675](http://arxiv.org/abs/2309.07675)

    本文提出了一种基于集合的可达性分析方法，通过 emergent representation 实现目标空间抽象，在分层强化学习中自主发现符号目标表示，并引入封建HRL算法来同时学习目标表示和分层策略。

    

    开放式学习通过使用符号方法进行目标表示而获益良多，因为它们提供了一种结构化知识以进行高效和可传递的学习。然而，现有的依赖符号推理的分层强化学习(HRL)方法通常受限于需要手动设置目标表示。自主发现符号目标表示的挑战在于它必须保留关键信息，例如环境动力学。在本文中，我们提出了一种通过新出现的表示来实现目标发现的发展机制，该表示将具有类似任务中的角色的环境状态集合进行抽象（即分组）。我们引入了一个同时学习目标表示和分层策略的封建HRL算法。该算法使用神经网络的符号可达性分析来近似状态集合之间的过渡关系，并改进目标表示。

    Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this paper, we propose a developmental mechanism for goal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We introduce a Feudal HRL algorithm that concurrently learns both the goal representation and a hierarchical policy. The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation. We eval
    
[^15]: BLIVA: 一个简单的多模态LLM用于更好地处理文本丰富的视觉问题

    BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])

    [http://arxiv.org/abs/2308.09936](http://arxiv.org/abs/2308.09936)

    BLIVA是一个简单的多模态LLM模型，用于更好地处理嵌入了文本的图像上下文场景。它通过引入InstructBLIP的查询嵌入和LLaVA的编码补丁嵌入技术来改进视觉语言模型，从而有效解决了文本丰富的视觉问题。

    

    视觉语言模型（VLM）通过整合视觉理解能力扩展了大规模语言模型（LLM），在解决开放式视觉问答（VQA）任务方面取得了显著进展。然而，这些模型无法准确解释嵌入文本的图像，这在现实场景中经常发生。从图像中提取信息的标准流程通常涉及学习一组固定的查询嵌入。这些嵌入被设计为封装图像上下文，并随后用作LLM中的软提示输入。然而，这个过程受令牌数量的限制，可能限制对文本丰富的上下文场景的识别。为了改进这一点，本研究引入了BLIVA：InstructBLIP with Visual Assistant的增强版本。BLIVA集成了来自InstructBLIP的查询嵌入，并将编码的补丁嵌入直接投影到LLM中，这是受到LLaVA的启发的一种技术。这种方法有助于处理文本丰富的视觉问题。

    Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
    
[^16]: 通过高斯混合分布潜空间的搜索生成反事实解释

    Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space. (arXiv:2307.13390v1 [cs.LG])

    [http://arxiv.org/abs/2307.13390](http://arxiv.org/abs/2307.13390)

    本文介绍了一种通过在自编码器的潜空间中进行高斯混合分布搜索来生成反事实解释的方法。

    

    反事实解释（CEs）是用于解决算法补救中的两个问题的重要工具：1. 是什么关键因素导致了自动预测/决策？2. 如何改变这些因素以从用户角度获得更有利的结果？因此，通过提供易于理解的解释和易于实现的可行变化来引导用户与AI系统的交互对于可信赖的采用和长期接受AI系统是至关重要的。在文献中，已经提出了各种方法来生成CEs，并建议使用不同的质量度量来评估这些方法。然而，CEs的生成通常需要大量计算，并且生成的建议是不切实际的，因此不可操作。在本文中，我们介绍了一种新的方法，通过首先将自编码器的潜空间形成为高斯分布的混合，为预先训练的二分类器生成CEs。

    Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generat
    
[^17]: 预测、改进、合成：面向概率时间序列预测的自引导扩散模型

    Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])

    [http://arxiv.org/abs/2307.11494](http://arxiv.org/abs/2307.11494)

    本研究提出了一种面向概率时间序列预测的自引导扩散模型，称为TSDiff。该模型不需要辅助网络或训练过程的改变，在预测、改进和合成数据生成等时间序列任务上展现出了竞争力。

    

    扩散模型在各个领域的生成建模任务中取得了最先进的性能。之前关于时间序列扩散模型的研究主要集中在开发针对特定预测或填补任务的条件模型。在这项工作中，我们探索了面向多种时间序列应用的任务不可知条件下的扩散模型的潜力。我们提出了TSDiff，一种面向时间序列的无条件训练的扩散模型。我们的自引导机制在推理过程中使得TSDiff能够为下游任务进行条件设置，而无需辅助网络或改变训练过程。我们在三个不同的时间序列任务上展示了我们方法的有效性：预测、改进和合成数据生成。首先，我们表明TSDiff与几种任务特定的条件预测方法相竞争（预测）。其次，我们利用TSDiff学到的隐性概率密度来迭代地改进p

    Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the p
    
[^18]: Epsilon*: 机器学习模型的隐私度量

    Epsilon*: Privacy Metric for Machine Learning Models. (arXiv:2307.11280v1 [cs.LG])

    [http://arxiv.org/abs/2307.11280](http://arxiv.org/abs/2307.11280)

    Epsilon*是一种用于测量机器学习模型隐私风险的新度量方法，不需要访问训练数据或模型训练算法，能与成员推断攻击中的假设检验相结合，提供对经过训练的模型实例隐私损失的下界，避免数值和噪声放大不稳定性。

    

    我们引入了Epsilon*，一种新的隐私度量方法，用于在隐私减轻策略部署之前、期间或之后，测量单个模型实例的隐私风险。该度量不需要访问训练数据采样或模型训练算法。Epsilon*是一个关于真阳性和假阳性率的函数，用于敌手在成员推断攻击中使用的假设检验中。我们区分了量化经过训练的模型实例的隐私损失和量化产生该模型实例的训练机制的隐私损失。现有的隐私审计文献中的方法为后者提供了下界，而我们的度量方法通过依赖于训练模型实例的隐私的（ε，δ）型量化，为前者提供了下界。我们建立了这些下界之间的关系，并展示了如何实现Epsilon*以避免数值和噪声放大不稳定性。

    We introduce Epsilon*, a new privacy metric for measuring the privacy risk of a single model instance prior to, during, or after deployment of privacy mitigation strategies. The metric does not require access to the training data sampling or model training algorithm. Epsilon* is a function of true positive and false positive rates in a hypothesis test used by an adversary in a membership inference attack. We distinguish between quantifying the privacy loss of a trained model instance and quantifying the privacy loss of the training mechanism which produces this model instance. Existing approaches in the privacy auditing literature provide lower bounds for the latter, while our metric provides a lower bound for the former by relying on an (${\epsilon}$,${\delta}$)-type of quantification of the privacy of the trained model instance. We establish a relationship between these lower bounds and show how to implement Epsilon* to avoid numerical and noise amplification instability. We further 
    
[^19]: 随机组合梯度下降算法的稳定性和泛化

    Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms. (arXiv:2307.03357v1 [cs.LG])

    [http://arxiv.org/abs/2307.03357](http://arxiv.org/abs/2307.03357)

    本文通过统计学习理论的算法稳定性，分析了随机组合梯度下降算法的稳定性和泛化性，引入了组合一致稳定性概念并与SCO问题的泛化性建立了定量关系。

    

    许多机器学习任务可以被形式化为随机组合优化（SCO）问题，例如强化学习、AUC最大化和元学习，其中目标函数涉及与期望相关的嵌套组合。虽然已经有大量研究致力于研究SCO算法的收敛行为，但对于它们的泛化性能如何，即从训练示例构建的学习算法在未来的测试示例上的行为如何，却很少有研究。在本文中，我们通过统计学习理论框架下的算法稳定性，提供了随机组合梯度下降算法的稳定性和泛化性分析。首先，我们引入了一种稳定性概念，称为组合一致稳定性，并建立了它与SCO问题的泛化性之间的定量关系。然后，我们为两种流行的随机组合优化问题建立了组合一致稳定性结果。

    Many machine learning tasks can be formulated as a stochastic compositional optimization (SCO) problem such as reinforcement learning, AUC maximization, and meta-learning, where the objective function involves a nested composition associated with an expectation. While a significant amount of studies has been devoted to studying the convergence behavior of SCO algorithms, there is little work on understanding their generalization, i.e., how these learning algorithms built from training examples would behave on future test examples. In this paper, we provide the stability and generalization analysis of stochastic compositional gradient descent algorithms through the lens of algorithmic stability in the framework of statistical learning theory. Firstly, we introduce a stability concept called compositional uniform stability and establish its quantitative relation with generalization for SCO problems. Then, we establish the compositional uniform stability results for two popular stochastic
    
[^20]: 浅层网络在逼近和学习高频率方面的困难：一个数值研究

    Why Shallow Networks Struggle with Approximating and Learning High Frequency: A Numerical Study. (arXiv:2306.17301v1 [cs.LG])

    [http://arxiv.org/abs/2306.17301](http://arxiv.org/abs/2306.17301)

    本文通过数值研究探讨了浅层神经网络在逼近和学习高频率方面的困难，重点是通过分析激活函数的谱分析来理解问题的原因。

    

    本研究通过对分析和实验的综合数值研究，解释了为什么两层神经网络在机器精度和计算成本等实际因素中，处理高频率的逼近和学习存在困难。具体而言，研究了以下基本计算问题：（1）在有限的机器精度下可以达到的最佳精度，（2）实现给定精度所需的计算成本，以及（3）对扰动的稳定性。研究的关键是相应激活函数的格拉姆矩阵的谱分析，该分析还显示了激活函数属性在这个问题中的作用。

    In this work, a comprehensive numerical study involving analysis and experiments shows why a two-layer neural network has difficulties handling high frequencies in approximation and learning when machine precision and computation cost are important factors in real practice. In particular, the following fundamental computational issues are investigated: (1) the best accuracy one can achieve given a finite machine precision, (2) the computation cost to achieve a given accuracy, and (3) stability with respect to perturbations. The key to the study is the spectral analysis of the corresponding Gram matrix of the activation functions which also shows how the properties of the activation function play a role in the picture.
    
[^21]: DNA-TEQ：一种用于DNN推理的自适应指数量化张量的方法

    DNA-TEQ: An Adaptive Exponential Quantization of Tensors for DNN Inference. (arXiv:2306.16430v1 [cs.LG])

    [http://arxiv.org/abs/2306.16430](http://arxiv.org/abs/2306.16430)

    本文介绍了一种在DNN推理中使用的自适应指数量化张量方法DNA-TEQ，该方法通过发现大量张量符合指数分布来实现最佳的数值精度和准确性损失平衡。

    

    量化是深度神经网络（DNN）中常用的技术，通过降低激活和权重（即张量）的算术精度来减少存储和计算复杂度。高效的硬件架构采用线性量化，以便将最新的DNN部署到嵌入式系统和移动设备上。然而，线性均匀量化通常无法将数值精度降低到小于8位而不牺牲模型准确性。这是因为张量并不服从均匀分布。本文中，我们展示了大量张量符合指数分布。然后，我们提出了DNA-TEQ，通过自适应方案指数量化DNN张量，以实现数值精度和准确性损失之间的最佳平衡。实验结果表明，与先前的方案相比，DNA-TEQ提供了更低的量化位宽，从而提高了性能。

    Quantization is commonly used in Deep Neural Networks (DNNs) to reduce the storage and computational complexity by decreasing the arithmetical precision of activations and weights, a.k.a. tensors. Efficient hardware architectures employ linear quantization to enable the deployment of recent DNNs onto embedded systems and mobile devices. However, linear uniform quantization cannot usually reduce the numerical precision to less than 8 bits without sacrificing high performance in terms of model accuracy. The performance loss is due to the fact that tensors do not follow uniform distributions. In this paper, we show that a significant amount of tensors fit into an exponential distribution. Then, we propose DNA-TEQ to exponentially quantize DNN tensors with an adaptive scheme that achieves the best trade-off between numerical precision and accuracy loss. The experimental results show that DNA-TEQ provides a much lower quantization bit-width compared to previous proposals, resulting in an av
    
[^22]: 使用正交序列的差分隐私无线联合学习方法

    Differentially Private Wireless Federated Learning Using Orthogonal Sequences. (arXiv:2306.08280v1 [cs.IT])

    [http://arxiv.org/abs/2306.08280](http://arxiv.org/abs/2306.08280)

    本文提出了一种使用正交序列的FLORAS方法，可消除发送端的信道状态信息，同时提供了项目级和客户级的差分隐私保证。FLORAS可以灵活地实现不同的差分隐私等级，并且通过推导收敛界限，实现了收敛速度和隐私保证之间的平稳权衡。

    

    本文提出了一种新的隐私保护上行空中计算方法FLORAS，用于单输入单输出（SISO）无线联合学习（FL）系统。FLORAS从通信设计的角度出发，利用正交序列的性质消除了发送端的信道状态信息（CSIT）要求。从隐私保护的角度来看，我们证明FLORAS可以提供项目级和客户级差分隐私（DP）保证。此外，通过调整系统参数，FLORAS可以在不增加成本的情况下灵活地实现不同的DP等级。我们推导出了一个新的FL收敛界限，结合隐私保证，可以在收敛速度和差分隐私级别之间实现平稳的权衡。数值结果证明了FLORAS相对于基准AirComp方法的优势，并验证了我们的分析结果可以指导不同权衡条件下的隐私保护FL的设计。

    We propose a novel privacy-preserving uplink over-the-air computation (AirComp) method, termed FLORAS, for single-input single-output (SISO) wireless federated learning (FL) systems. From the communication design perspective, FLORAS eliminates the requirement of channel state information at the transmitters (CSIT) by leveraging the properties of orthogonal sequences. From the privacy perspective, we prove that FLORAS can offer both item-level and client-level differential privacy (DP) guarantees. Moreover, by adjusting the system parameters, FLORAS can flexibly achieve different DP levels at no additional cost. A novel FL convergence bound is derived which, combined with the privacy guarantees, allows for a smooth tradeoff between convergence rate and differential privacy levels. Numerical results demonstrate the advantages of FLORAS compared with the baseline AirComp method, and validate that our analytical results can guide the design of privacy-preserving FL with different tradeoff 
    
[^23]: NeuroGraph:面向脑连接组学的图机器学习基准测试

    NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])

    [http://arxiv.org/abs/2306.06202](http://arxiv.org/abs/2306.06202)

    本文介绍了神经成像领域的图机器学习基准测试NeuroGraph，并探讨了数据集生成的搜索空间。

    

    机器学习为分析高维功能性神经成像数据提供了有价值的工具，已被证明对预测各种神经疾病、精神障碍和认知模式有效。在功能磁共振成像研究中，大脑区域之间的相互作用通常使用基于图的表示进行建模。图机器学习方法的有效性已在多个领域得到证实，标志着数据解释和预测建模中的一个转变步骤。然而，尽管有前景，但由于图形数据集构建的广泛预处理流水线和大参数搜索空间，在神经成像领域中应用这些技术的转换仍然受到意外的限制。本文介绍了NeuroGraph(一个基于图的神经成像数据集)，它涵盖了多个行为和认知特征类别。我们深入探讨了数据集生成搜索空间

    Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
    
[^24]: ShaDDR: 基于示例的实时几何和纹理生成

    ShaDDR: Real-Time Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering. (arXiv:2306.04889v1 [cs.CV])

    [http://arxiv.org/abs/2306.04889](http://arxiv.org/abs/2306.04889)

    本文介绍了一种基于示例的深度生成神经网络 ShaDDR，可以通过几何细节化和条件纹理生成应用于输入的粗略体素形状，生成高分辨率贴图的 3D 形状。生成实时且精度高，风格可以通过学习的潜在代码进行控制。

    

    本文介绍了 ShaDDR，一种基于示例的深度生成神经网络，通过几何细节化和条件纹理生成应用于输入的粗略体素形状，生成高分辨率贴图的 3D 形状。在少量详细和纹理的范例形状上训练，我们的方法通过多分辨率体素上采样学习几何细节化，并且通过与几个视图的范例纹理图像进行可微渲染，在体素表面生成纹理。生成是实时的，仅需不到 1 秒即可生成分辨率高达 512^3 的 3D 模型。生成的形状保留了输入粗略体素模型的整体结构，而生成的几何细节和纹理的风格可以通过学习的潜在代码进行操纵。在实验中，我们展示了我们的方法可以生成比以前的作品更真实且几何细节和纹理更干净的高分辨率形状。

    We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is real-time, taking less than 1 second to produce a 3D model with voxel resolutions up to 512^3. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase th
    
[^25]: 基于补丁选择的高效视觉Transformer在人体姿态估计中的应用

    Efficient Vision Transformer for Human Pose Estimation via Patch Selection. (arXiv:2306.04225v1 [cs.CV])

    [http://arxiv.org/abs/2306.04225](http://arxiv.org/abs/2306.04225)

    该论文提出了一种基于补丁选择的高效视觉Transformer方法，大幅度提高了处理速度和降低计算复杂度，用于2D人体姿态估计方面。

    

    虽然卷积神经网络在2D人体姿态估计方面已经取得了广泛成功，但是视觉Transformer作为卷积神经网络的有力替代者，通过提高最先进的性能而崭露头角。 然而，视觉Transformer的二次计算复杂度限制了其在处理高分辨率图像和长视频方面的适用性。为了解决这一挑战，我们提出了一种简单的方法来减少视觉Transformer的计算复杂度，基于选择和处理少量最具信息的补丁，而忽略其他地方的补丁。我们利用轻量级姿态估计网络来指导补丁选择过程，确保所选补丁包含最重要的信息。我们在三个广泛使用的2D姿态估计基准（即COCO、MPII和OCHuman）上的实验结果表明，我们提出的方法在显著提高速度和降低计算复杂度方面具有很好的效果，虽然性能略有下降。

    While Convolutional Neural Networks (CNNs) have been widely successful in 2D human pose estimation, Vision Transformers (ViTs) have emerged as a promising alternative to CNNs, boosting state-of-the-art performance. However, the quadratic computational complexity of ViTs has limited their applicability for processing high-resolution images and long videos. To address this challenge, we propose a simple method for reducing ViT's computational complexity based on selecting and processing a small number of most informative patches while disregarding others. We leverage a lightweight pose estimation network to guide the patch selection process, ensuring that the selected patches contain the most important information. Our experimental results on three widely used 2D pose estimation benchmarks, namely COCO, MPII and OCHuman, demonstrate the effectiveness of our proposed methods in significantly improving speed and reducing computational complexity with a slight drop in performance.
    
[^26]: Hinge-Wasserstein: 通过分类避免回归中的过度自信

    Hinge-Wasserstein: Mitigating Overconfidence in Regression by Classification. (arXiv:2306.00560v1 [cs.LG])

    [http://arxiv.org/abs/2306.00560](http://arxiv.org/abs/2306.00560)

    该论文提出了一种基于Wasserstein距离的损失函数hinge-Wasserstein，用于缓解回归任务中由于过度自信导致的不确定性问题。这种损失函数有效提高了aleatoric和epistemic不确定性的质量。

    

    现代深度神经网络在性能方面得到了巨大的提高，但它们容易产生过度自信。在模糊甚至不可预测的现实世界场景中，这种过度自信可能对应用程序的安全性构成重大风险。针对回归任务，采用回归-分类方法有潜力缓解这些歧义，因为它可以预测所需输出的离散概率密度。然而，密度估计仍然倾向于过度自信，尤其是在使用常见的NLL损失函数训练时。为了缓解这种过度自信的问题，我们提出了一种基于Wasserstein距离的损失函数，即hinge-Wasserstein。与以前的工作相比，此损失显着提高了两种不确定性的质量： aleatoric不确定性和epistemic不确定性。我们在合成数据集上展示了新损失的能力，其中两种类型的不确定性可以分别控制。此外，作为现实世界场景的演示，我们在基准数据集上评估了我们的方法。

    Modern deep neural networks are prone to being overconfident despite their drastically improved performance. In ambiguous or even unpredictable real-world scenarios, this overconfidence can pose a major risk to the safety of applications. For regression tasks, the regression-by-classification approach has the potential to alleviate these ambiguities by instead predicting a discrete probability density over the desired output. However, a density estimator still tends to be overconfident when trained with the common NLL loss. To mitigate the overconfidence problem, we propose a loss function, hinge-Wasserstein, based on the Wasserstein Distance. This loss significantly improves the quality of both aleatoric and epistemic uncertainty, compared to previous work. We demonstrate the capabilities of the new loss on a synthetic dataset, where both types of uncertainty are controlled separately. Moreover, as a demonstration for real-world scenarios, we evaluate our approach on the benchmark dat
    
[^27]: 关于在量子计算机上采样确定性行列式和Pfaffian点过程

    On sampling determinantal and Pfaffian point processes on a quantum computer. (arXiv:2305.15851v1 [stat.CO])

    [http://arxiv.org/abs/2305.15851](http://arxiv.org/abs/2305.15851)

    本文总结了在量子计算机上采样确定性行列式和Pfaffian点过程的状态及其优化方式。

    

    确定性点过程(DPP) 最早被 Macchi 作为量子光学模型引入，自那以后，它们已广泛用作统计学和计算机科学中的模型和子抽样工具。大多数应用需要从DPP抽样，考虑到其量子起源，自然会想知道在量子计算机上抽样DPP是否比在经典计算机上更容易。本文关注于有限状态空间上的DPP，这是一个在$\{1,\dots,N\}$子集上的分布，由一个$N\times N$的Hermite内核矩阵参数化。最基本的采样包括两个步骤，在经典计算机上分别需要 $\mathcal{O}(N^3)$ 和 $\mathcal{O}(Nr^2)$ 的操作成本，其中$r$是内核矩阵的秩。本文旨在讨论量子计算机上的DPP采样算法的状态及其优化方式。

    DPPs were introduced by Macchi as a model in quantum optics the 1970s. Since then, they have been widely used as models and subsampling tools in statistics and computer science. Most applications require sampling from a DPP, and given their quantum origin, it is natural to wonder whether sampling a DPP on a quantum computer is easier than on a classical one. We focus here on DPPs over a finite state space, which are distributions over the subsets of $\{1,\dots,N\}$ parametrized by an $N\times N$ Hermitian kernel matrix. Vanilla sampling consists in two steps, of respective costs $\mathcal{O}(N^3)$ and $\mathcal{O}(Nr^2)$ operations on a classical computer, where $r$ is the rank of the kernel matrix. A large first part of the current paper consists in explaining why the state-of-the-art in quantum simulation of fermionic systems already yields quantum DPP sampling algorithms. We then modify existing quantum circuits, and discuss their insertion in a full DPP sampling pipeline that start
    
[^28]: 带有音频光谱变换器的 Patch-Mix 对比学习在呼吸音分类中的应用

    Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification. (arXiv:2305.14032v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.14032](http://arxiv.org/abs/2305.14032)

    本研究提出了一种新的通过在音频数据上进行对比学习的方法，在呼吸音分类任务中取得了最先进的性能表现。

    

    呼吸声包含早期诊断致命肺部疾病的重要信息。自 COVID-19 疫情以来，基于电子听诊器的无接触医疗越来越受关注。为此，开发了先进的深度学习模型来诊断肺部疾病；然而，由于医学数据的稀缺，仍然存在挑战。本研究证明了在大规模视觉和音频数据集上预训练的模型可以推广到呼吸音分类任务。此外，我们引入了一种简单的 Patch-Mix 数据增强方法，通过随机混合不同样本之间的补丁，与 Audio Spectrogram Transformer (AST) 相结合。我们进一步提出了一种新颖而有效的 Patch-Mix 对比学习方法，以区分潜在空间中的混合表示。我们的方法在 ICBHI 数据集上取得了最先进的性能，优于先前的最高得分 4.08%。

    Respiratory sound contains crucial information for the early diagnosis of fatal lung diseases. Since the COVID-19 pandemic, there has been a growing interest in contact-free medical care based on electronic stethoscopes. To this end, cutting-edge deep learning models have been developed to diagnose lung diseases; however, it is still challenging due to the scarcity of medical data. In this study, we demonstrate that the pretrained model on large-scale visual and audio datasets can be generalized to the respiratory sound classification task. In addition, we introduce a straightforward Patch-Mix augmentation, which randomly mixes patches between different samples, with Audio Spectrogram Transformer (AST). We further propose a novel and effective Patch-Mix Contrastive Learning to distinguish the mixed representations in the latent space. Our method achieves state-of-the-art performance on the ICBHI dataset, outperforming the prior leading score by an improvement of 4.08%.
    
[^29]: 用低维参数子空间表示输入变换

    Representing Input Transformations by Low-Dimensional Parameter Subspaces. (arXiv:2305.13536v1 [cs.LG])

    [http://arxiv.org/abs/2305.13536](http://arxiv.org/abs/2305.13536)

    本文提出配置子空间假设，为连续参数化变换最优模型权重可以存在于低维线性子空间中。我们引入自定义网络学习这些子空间，并观察到它们的低维结构可以在所有测试变换中使用。

    

    深度模型对于简单的输入变换（如旋转、缩放和平移）缺乏鲁棒性，除非它们具有特定的不变结构或经过特定的训练后（例如从数据增强中学习所需的鲁棒性）。本文提出了配置子空间假设，即为连续参数化变换最优模型权重可以存在于低维线性子空间中。我们引入子空间可配置网络来学习这些子空间，并观察它们在来自计算机视觉和音频信号处理领域的所有测试变换、数据集和架构中的结构和惊人的低维度。我们的发现有助于跨不同领域有效地表示和转移输入变换知识，并可能导致更健壮和可解释的模型。

    Deep models lack robustness to simple input transformations such as rotation, scaling, and translation, unless they feature a particular invariant architecture or undergo specific training, e.g., learning the desired robustness from data augmentations. Alternatively, input transformations can be treated as a domain shift problem, and solved by post-deployment model adaptation. Although a large number of methods deal with transformed inputs, the fundamental relation between input transformations and optimal model weights is unknown. In this paper, we put forward the configuration subspace hypothesis that model weights optimal for parameterized continuous transformations can reside in low-dimensional linear subspaces. We introduce subspace-configurable networks to learn these subspaces and observe their structure and surprisingly low dimensionality on all tested transformations, datasets and architectures from computer vision and audio signal processing domains. Our findings enable effic
    
[^30]: 用基于深度学习原理的方法进行地质岩相生成

    A principled deep learning approach for geological facies generation. (arXiv:2305.13318v1 [physics.geo-ph])

    [http://arxiv.org/abs/2305.13318](http://arxiv.org/abs/2305.13318)

    本研究使用基于深度学习原理的生成对抗网络和深度变分推理应用于地质岩相生成，针对地下渠道进行了有条件模拟，并且比传统地质统计模型具有更高水平的准确性和物理逼真性。

    

    在各种地球科学应用中，模拟不可观测体积中的地质相是至关重要的。考虑到该问题的复杂性，深度生成学习是克服传统地质统计模型局限性（特别是缺乏物理逼真性）的一种有前途的方法。本研究旨在探索对生成对抗网络和深度变分推理进行应用，以便有条件地对地下渠道进行模拟。本文回顾了生成深度学习方法，特别是对抗性方法和旨在促进其训练的稳定化技术。本文提出的方法在以随机过程为基础的Flumy模型生成的二维和三维模拟上进行了测试。利用形态学指标比较我们提出的方法与以前的对抗生成网络迭代的结果。结果表明，通过利用最近的稳定技术，生成对抗网络可以成功地应用于地质岩相生成，并表现出比传统地质统计模型更高水平的准确性和物理逼真性。

    The simulation of geological facies in an unobservable volume is essential in various geoscience applications. Given the complexity of the problem, deep generative learning is a promising approach to overcome the limitations of traditional geostatistical simulation models, in particular their lack of physical realism. This research aims to investigate the application of generative adversarial networks and deep variational inference for conditionally simulating meandering channels in underground volumes. In this paper, we review the generative deep learning approaches, in particular the adversarial ones and the stabilization techniques that aim to facilitate their training. The proposed approach is tested on 2D and 3D simulations generated by the stochastic process-based model Flumy. Morphological metrics are utilized to compare our proposed method with earlier iterations of generative adversarial networks. The results indicate that by utilizing recent stabilization techniques, generati
    
[^31]: 基于神经网络观测器的自主非线性系统传感器故障检测与隔离

    Sensor Fault Detection and Isolation in Autonomous Nonlinear Systems Using Neural Network-Based Observers. (arXiv:2304.08837v1 [math.OC])

    [http://arxiv.org/abs/2304.08837](http://arxiv.org/abs/2304.08837)

    本文介绍了一种基于神经网络的观测器方法，可用于检测和隔离工业系统中的传感器故障，适用于一般的自主非线性系统，通过学习实现Lueneberger观察器的设计，通过残留生成检测传感器故障并实现故障隔离。

    

    本文介绍了一种基于观测器的新方法，用于检测和隔离工业系统中的传感器故障。考虑了两种类型的传感器故障：完全故障和传感器劣化。所提出的方法适用于一般的自主非线性系统，而不需对其三角形和/或正常形式进行任何假设，这通常在观察者设计文献中考虑。我们方法的关键是Lueneberger观察器的基于学习的设计，其中涉及使用神经网络来近似将非线性系统转化为具有输出注入的稳定线性系统的单射映射。这种基于学习的Lueneberger观察器准确估计了系统的状态，从而通过残留生成实现传感器故障的检测。残差是通过计算系统测量输出和观察者预测输出向量之间差值的范数而得出的。故障隔离是通过将每个传感器的测量输出与残差信号进行比较来实现的。该方法的有效性和鲁棒性通过对双罐系统的模拟结果进行演示。

    This paper presents a new observer-based approach to detect and isolate faulty sensors in industrial systems. Two types of sensor faults are considered: complete failure and sensor deterioration. The proposed method is applicable to general autonomous nonlinear systems without making any assumptions about its triangular and/or normal form, which is usually considered in the observer design literature. The key aspect of our approach is a learning-based design of the Luenberger observer, which involves using a neural network to approximate the injective map that transforms the nonlinear system into a stable linear system with output injection. This learning-based Luenberger observer accurately estimates the system's state, allowing for the detection of sensor faults through residual generation. The residual is computed as the norm of the difference between the system's measured output and the observer's predicted output vectors. Fault isolation is achieved by comparing each sensor's meas
    
[^32]: LASER：神经符号学习语义视频表示

    LASER: Neuro-Symbolic Learning of Semantic Video Representations. (arXiv:2304.07647v1 [cs.CV])

    [http://arxiv.org/abs/2304.07647](http://arxiv.org/abs/2304.07647)

    LASER提出了一种神经符号学习方法来学习语义视频表示，通过逻辑规范捕捉视频数据中的时空属性，能够对齐原始视频和规范，有效地训练低级感知模型以提取符合所需高级规范的视频表示。

    

    现代涉及视频的AI应用（如视频-文本对齐、视频搜索和视频字幕）受益于对视频语义的细致理解。现有的视频理解方法要么需要大量注释，要么基于不可解释的通用嵌入，可能会忽略重要细节。我们提出了LASER，这是一种神经符号方法，通过利用能够捕捉视频数据中丰富的时空属性的逻辑规范来学习语义视频表示。特别地，我们通过原始视频与规范之间的对齐来公式化问题。对齐过程有效地训练了低层感知模型，以提取符合所需高层规范的细粒度视频表示。我们的流程可以端到端地训练，并可纳入从规范导出的对比和语义损失函数。我们在两个具有丰富空间和时间信息的数据集上评估了我们的方法。

    Modern AI applications involving video, such as video-text alignment, video search, and video captioning, benefit from a fine-grained understanding of video semantics. Existing approaches for video understanding are either data-hungry and need low-level annotation, or are based on general embeddings that are uninterpretable and can miss important details. We propose LASER, a neuro-symbolic approach that learns semantic video representations by leveraging logic specifications that can capture rich spatial and temporal properties in video data. In particular, we formulate the problem in terms of alignment between raw videos and specifications. The alignment process efficiently trains low-level perception models to extract a fine-grained video representation that conforms to the desired high-level specification. Our pipeline can be trained end-to-end and can incorporate contrastive and semantic loss functions derived from specifications. We evaluate our method on two datasets with rich sp
    
[^33]: 随机对抗风格扰动技术用于域泛化

    Randomized Adversarial Style Perturbations for Domain Generalization. (arXiv:2304.01959v1 [cs.CV])

    [http://arxiv.org/abs/2304.01959](http://arxiv.org/abs/2304.01959)

    本文提出了一种随机对抗风格扰动技术，它能够通过对抗性扰动特征风格达到域泛化的效果，同时结合混合原始特征的方法缓解扰动带来的挑战。

    

    本文提出了一种新颖的域泛化技术，称为随机对抗风格扰动技术（RASP），其动机在于特征统计学捕捉到每个域的特征。该算法在对抗性方向上扰动一个特征的风格，朝着一个随机选择的类别方向并使模型学习避免被在未知目标域中观察到的意外风格所误导。虽然RASP能有效处理域漂移，但它的简单融合到训练过程中可能会降低从源域学习知识的能力，因为它并不限制表征的扰动。这个挑战由规一化特征Mixup（NFM）缓解，它通过训练过程中的特征混合来促进原始特征的学习，同时实现了对扰动表示的鲁棒性。我们通过广泛的实验评估了所提出的算法。

    We propose a novel domain generalization technique, referred to as Randomized Adversarial Style Perturbation (RASP), which is motivated by the observation that the characteristics of each domain are captured by the feature statistics corresponding to style. The proposed algorithm perturbs the style of a feature in an adversarial direction towards a randomly selected class, and makes the model learn against being misled by the unexpected styles observed in unseen target domains. While RASP is effective to handle domain shifts, its naive integration into the training procedure might degrade the capability of learning knowledge from source domains because it has no restriction on the perturbations of representations. This challenge is alleviated by Normalized Feature Mixup (NFM), which facilitates the learning of the original features while achieving robustness to perturbed representations via their mixup during training. We evaluate the proposed algorithm via extensive experiments on var
    
[^34]: 折扣马尔可夫决策过程中精确策略镜像下降算法的最优收敛速率

    Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes. (arXiv:2302.11381v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.11381](http://arxiv.org/abs/2302.11381)

    本论文展示了一个不需要对目标函数正则化的未正则化PMD算法族，可以实现PI的维度自由的线性$\gamma$收敛速率，这个速率是最优的，且自适应步长是实现这个速率所必需的。

    

    策略镜像下降(Policy Mirror Descent，PMD)是一种广泛的算法族，包括强化学习中的一系列新颖和基础方法。本文旨在通过精确的策略评价来建立PI和PMD之间的联系，并展示一个不需要对目标函数正则化就可以对PI中的策略改进步骤进行算法正则化的未正则化PMD算法族来实现PI的维度自由的线性$\gamma$-收敛速率。我们展示了PMD的这种速率和步长都是无法改进的：我们提供的匹配下界表明$\gamma$-率对于PMD方法和PI方法都是最优的，并且自适应步长是实现它所必需的。我们的工作是首次在精确策略评价下研究PMD方法的最优误差速率。

    Policy Mirror Descent (PMD) is a general family of algorithms that covers a wide range of novel and fundamental methods in reinforcement learning. Motivated by the instability of policy iteration (PI) with inexact policy evaluation, unregularised PMD algorithmically regularises the policy improvement step of PI without regularising the objective function. With exact policy evaluation, PI is known to converge linearly with a rate given by the discount factor $\gamma$ of a Markov Decision Process. In this work, we bridge the gap between PI and PMD with exact policy evaluation and show that the dimension-free $\gamma$-rate of PI can be achieved by the general family of unregularised PMD algorithms under an adaptive step-size. We show that both the rate and step-size are unimprovable for PMD: we provide matching lower bounds that demonstrate that the $\gamma$-rate is optimal for PMD methods as well as PI and that the adaptive step-size is necessary to achieve it. Our work is the first to r
    
[^35]: 使用稀疏连接和选择性学习的可扩展实时循环学习

    Scalable Real-Time Recurrent Learning Using Sparse Connections and Selective Learning. (arXiv:2302.05326v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05326](http://arxiv.org/abs/2302.05326)

    本文提出了两个限制使得实时循环学习算法具有可扩展性，分别是将网络分解为独立模块或逐步学习网络。与其他可扩展算法不同的是，这些算法不会向梯度估计添加噪声或偏差，而是通过权衡网络的功能能力以实现可扩展学习。

    

    从感知观察中构建状态是强化学习代理的重要组成部分。一种用于状态构建的解决方案是使用循环神经网络。 BPTT和实时循环学习（RTRL）是两种流行的基于梯度的循环学习方法。 BPTT在计算梯度之前需要完整的观察序列，不适合在线实时更新。 RTRL可以进行在线更新，但不适用于大型网络。 在本文中，我们提出了两个限制，使RTRL具有可扩展性。我们表明，通过将网络分解为独立模块或逐步学习网络，我们可以使RTRL与参数数量呈线性比例关系。与先前的可扩展梯度估计算法（例如UORO和Truncated-BPTT）不同，我们的算法不会向梯度估计添加噪声或偏差。相反，它们权衡了网络的功能能力以实现可扩展学习。

    State construction from sensory observations is an important component of a reinforcement learning agent. One solution for state construction is to use recurrent neural networks. Back-propagation through time (BPTT), and real-time recurrent learning (RTRL) are two popular gradient-based methods for recurrent learning. BPTT requires the complete sequence of observations before computing gradients and is unsuitable for online real-time updates. RTRL can do online updates but scales poorly to large networks. In this paper, we propose two constraints that make RTRL scalable. We show that by either decomposing the network into independent modules, or learning the network incrementally, we can make RTRL scale linearly with the number of parameters. Unlike prior scalable gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms do not add noise or bias to the gradient estimate. Instead, they trade-off the functional capacity of the network to achieve scalable learning. W
    
[^36]: 基于有向图的跨模态特征补充方法用于多模态对话情感识别

    GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.12261](http://arxiv.org/abs/2207.12261)

    本文提出了一种基于有向图的跨模态特征补充方法，可以提取多模态上下文信息和交互信息，缓解了多模态融合中的异构性差距问题。

    

    对话情感识别在人机交互系统中起着重要作用，因为它可以提供有共情心理的服务。多模态对话情感识别可以缓解单模态方法的缺点。最近，由于关系建模方面的卓越性能，图神经网络已被广泛用于各种领域。在多模态对话情感识别中，图神经网络能够提取远距离的上下文信息和跨模态的交互信息。不幸的是，由于现有方法（如MMGCN）直接融合多个模态，可能会产生冗余信息，且可能丢失多样化的信息。在本文中，我们提出了一种基于有向图的跨模态特征补充（GraphCFC）模块，可以有效地模拟上下文和互动信息。GraphCFC通过利用多个子空间提取器和成对跨模态补充（PairCC）策略，缓解了多模态融合中的异构性差距问题。

    Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. 
    
[^37]: 保持度数不变的随机化响应在局部差分隐私下的图神经网络

    Degree-Preserving Randomized Response for Graph Neural Networks under Local Differential Privacy. (arXiv:2202.10209v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2202.10209](http://arxiv.org/abs/2202.10209)

    本文提出了一种保持度数不变的随机化响应算法，用于在无属性图中提供高准确性的边的局部差分隐私保护。通过使用Warner的随机化响应和策略性边采样，我们的算法能够保护用户的隐私同时保留图结构。

    

    最近，研究了差分隐私图神经网络(Differentially private GNNs)来在图数据上提供高准确性同时强力保护用户隐私。特别地，最近的一项研究提出了一种算法，用局部差分隐私(LDP)保护有属性的图中每个用户的特征向量，而局部差分隐私是一种强隐私概念，不需要可信第三方。然而，该算法不保护社交图中的边（友谊），因此无法保护无属性图中的用户隐私。如何在无属性图中提供高准确性的强隐私保护仍然是一个开放问题。本文提出了一种新颖的LDP算法，名为DPRR（Degree-Preserving Randomized Response），用于在GNN中提供边的LDP。我们的DPRR在提供边的LDP的同时保留了每个用户的度数，从而保持了图结构。从技术上讲，我们的DPRR使用了Warner的RR（Randomized Response）和策略性边采样，其中每个用户的采样概率是通过Lapla进行自动调整的。

    Differentially private GNNs (Graph Neural Networks) have been recently studied to provide high accuracy in various tasks on graph data while strongly protecting user privacy. In particular, a recent study proposes an algorithm to protect each user's feature vector in an attributed graph with LDP (Local Differential Privacy), a strong privacy notion without a trusted third party. However, this algorithm does not protect edges (friendships) in a social graph, hence cannot protect user privacy in unattributed graphs. How to provide strong privacy with high accuracy in unattributed graphs remains open.  In this paper, we propose a novel LDP algorithm called the DPRR (Degree-Preserving Randomized Response) to provide LDP for edges in GNNs. Our DPRR preserves each user's degree hence a graph structure while providing edge LDP. Technically, our DPRR uses Warner's RR (Randomized Response) and strategic edge sampling, where each user's sampling probability is automatically tuned using the Lapla
    

