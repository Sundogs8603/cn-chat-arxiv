# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TEDDY: Trimming Edges with Degree-based Discrimination strategY](https://rss.arxiv.org/abs/2402.01261) | TEDDY是一种利用边缘度量信息的边缘修剪方法，旨在通过一次性操作实现边缘稀疏化，进而鼓励参数稀疏化训练。这是一个解决图神经网络中抽奖票假设的时间效率和效果问题的创新方法。 |
| [^2] | [Bayesian Diffusion Models for 3D Shape Reconstruction](https://arxiv.org/abs/2403.06973) | BDM是一种利用联合扩散过程紧密耦合先验信息与数据驱动过程的预测算法，专注于3D形状重建任务，通过引入独立标签的丰富先验信息来改善自下而上的重建过程。 |
| [^3] | [A representation-learning game for classes of prediction tasks](https://arxiv.org/abs/2403.06971) | 提出了一种基于博弈的表示学习方法，该方法通过最小化预测损失优化表示学习，在有限制条件下找到了理论最佳表示，并展示了先验知识的有效性以及随机化表示的有用性。 |
| [^4] | [Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts](https://arxiv.org/abs/2403.06966) | 使用Mixture of Experts的Di-SkilL方法通过优化每个专家和其相关上下文分布，实现了在相似上下文中学习多样技能的目标。 |
| [^5] | [The pitfalls of next-token prediction](https://arxiv.org/abs/2403.06963) | 论文揭示了在某些任务类别中，教师强制方法可能无法在第一时间学习到准确的下一个标记预测器，进而导致模型失败的一般机制。 |
| [^6] | [Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic Perovskites](https://arxiv.org/abs/2403.06955) | 提出了一种用于准确预测新型二维混合有机无机钙钛矿结构的机器学习原子间势，可以实现化学精度。 |
| [^7] | [SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data](https://arxiv.org/abs/2403.06952) | SELMA提出了一种新范式，通过在自动生成的多技能图像文本数据集上微调模型，并进行技能特定专家学习和合并，从而改进T2I模型的忠实度。 |
| [^8] | [Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI](https://arxiv.org/abs/2403.06942) | 提出了基于连续时序测量和生成人工智能的电网监测和控制系统，通过数据压缩和故障检测，实现了对传统监控系统的进步。 |
| [^9] | [Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction](https://arxiv.org/abs/2403.06940) | 该论文提出了一种条件得分扩散模型，针对皮质厚度轨迹预测，在应对数据缺失和对疾病进展建模方面具有优势。 |
| [^10] | [Counterfactual Reasoning with Knowledge Graph Embeddings](https://arxiv.org/abs/2403.06936) | 通过新任务CFKGR，本文将知识图补全和反事实推理联系起来，提出了一种用于适应假设前提的知识图嵌入方法COULDD，并通过基准数据集的评估表明KGEs可以学习图中的模式，检测出合理的反事实变化。 |
| [^11] | [Simplicity Bias of Transformers to Learn Low Sensitivity Functions](https://arxiv.org/abs/2403.06925) | Transformers在不同数据模态上具有低敏感性，这种简单性偏差有助于解释其在视觉和语言任务中的优越性能。 |
| [^12] | [Responsible Artificial Intelligence: A Structured Literature Review](https://arxiv.org/abs/2403.06910) | 该研究提出了负责任人工智能的全面统一定义，并通过结构化文献综述阐明了当前对负责任人工智能的理解，旨在帮助立法者和机器学习从业者在AI监管领域做出指导。 |
| [^13] | [Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints](https://arxiv.org/abs/2403.06906) | 提出了成本和工作量约束下的推迟框架（DeCCaF），旨在解决成本敏感场景、并发预测和人类工作能力约束等问题 |
| [^14] | [Benign overfitting in leaky ReLU networks with moderate input dimension](https://arxiv.org/abs/2403.06903) | 研究了在泄漏ReLU网络上使用铰链损失进行训练的过程中，信噪比（SNR）条件对于良性和非良性过拟合的影响，并发现高SNR值会导致良性过拟合，低SNR值则会导致有害过拟合。 |
| [^15] | [LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration](https://arxiv.org/abs/2403.06901) | 通过深度学习和生物力学模型相结合的LIBR+方法，作者提出了一种改进术中肝脏配准的方法，能有效处理术中测量的稀疏性和变异性。 |
| [^16] | [Application of Quantum Tensor Networks for Protein Classification](https://arxiv.org/abs/2403.06890) | 量子张量网络广泛应用于蛋白质分类问题，通过量子增强处理能力有效应对蛋白质序列的复杂性和多样性 |
| [^17] | [HiRA-Pro: High resolution alignment of multimodal spatio-temporal data: a process physics driven approach](https://arxiv.org/abs/2403.06888) | HiRA-Pro是一个高分辨率对齐多模态时空数据的过程物理驱动方法，成功解决了对齐具有亚毫秒现象的数据的挑战，并在智能制造环境中取得了成功应用。 |
| [^18] | [Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning](https://arxiv.org/abs/2403.06880) | 研究探讨了幼儿启发的奖励转换如何影响强化学习任务的样本效率和成功率，特别是发现了幼儿启发的稀疏转密集（S2D）转换的有效性。 |
| [^19] | [COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification](https://arxiv.org/abs/2403.06874) | 本文提出了一种将个体OOD度量结合成一个组合OOD度量（COOD）的框架，在大规模生物多样性数据集上进行了广泛评估，表现出在TPR@1% FPR方面明显优于个体OOD度量。 |
| [^20] | [Last Iterate Convergence of Incremental Methods and Applications in Continual Learning](https://arxiv.org/abs/2403.06873) | 针对继续学习应用，本研究首次获得了增量梯度和增量近端方法最后迭代的收敛保证，且其预期复杂度界限几乎与已知最佳平均迭代的界限相匹配。 |
| [^21] | [On the Generalization Ability of Unsupervised Pretraining](https://arxiv.org/abs/2403.06871) | 无监督预训练如何影响模型泛化能力的关键因素的新理论框架 |
| [^22] | [Semantic Residual Prompts for Continual Learning](https://arxiv.org/abs/2403.06870) | 通过引入语义剩余提示，作者提出了一种稳定的选择策略，利用两级适应机制来在持续学习中解决提示冲突的问题。 |
| [^23] | [Learning with Noisy Foundation Models](https://arxiv.org/abs/2403.06869) | 本文首次全面了解和分析了预训练数据集中的噪声性质，有效减轻其对下游任务影响。 |
| [^24] | [A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa](https://arxiv.org/abs/2403.06860) | 该研究开发了一个地理空间模型，用于预测沙漠蝗虫的繁殖地，有望提升早期预警系统和有针对性的控制措施。 |
| [^25] | [Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification](https://arxiv.org/abs/2403.06854) | 本文分析了逆强化学习问题对行为模型误差的敏感性，并提供了观测数据与假定行为模型不同但不引发错误的条件。 |
| [^26] | [Towards an educational tool for supporting neonatologists in the delivery room](https://arxiv.org/abs/2403.06843) | 提出了一种机器学习方法，用于从实际数据中识别新生儿分娩事件的风险因素，旨在设计出一款用户友好的移动应用程序，提高对高风险患者的识别率和干预规划。 |
| [^27] | [Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?](https://arxiv.org/abs/2403.06833) | 本研究提出了一种形式化的度量来量化指令与数据分离现象，以及一种可以从模型黑盒输出计算的经验变量，并引入了新数据集SEP，用于评估 |
| [^28] | [Constructing Variables Using Classifiers as an Aid to Regression: An Empirical Assessment](https://arxiv.org/abs/2403.06829) | 提出了一种方法，利用分类器预测变量的离散值并将其作为附加变量用于丰富回归问题的初始向量，经实验证实了该方法的有效性。 |
| [^29] | [In-context Exploration-Exploitation for Reinforcement Learning](https://arxiv.org/abs/2403.06826) | 引入了In-context Exploration-Exploitation (ICEE)算法，通过在Transformer模型内部进行探索-利用权衡，提高了在-context策略学习的效率。 |
| [^30] | [Are Targeted Messages More Effective?](https://arxiv.org/abs/2403.06817) | GNN的核心架构有两个版本，第一个版本消息仅取决于源顶点的状态，而第二个版本消息取决于源顶点和目标顶点的状态。 |
| [^31] | [Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science](https://arxiv.org/abs/2403.06816) | 提出了一种新颖的优化算法，利用Kullback-Leibler散度训练大规模、非光滑的Maxent模型 |
| [^32] | [{\epsilon}-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment](https://arxiv.org/abs/2403.06814) | 引入了{\epsilon}-神经Thompson采样在帕金森病治疗领域中，通过解决传统DBS设备的能源效率和副作用问题，实现了自适应DBS的目标 |
| [^33] | [Monotone Individual Fairness](https://arxiv.org/abs/2403.06812) | 该论文提出了一个新的在线学习框架，通过单调聚合函数实现了一种个体公平性审计方案，可以有效降低多个审计员对个体公平性的分析复杂度。 |
| [^34] | [Multistep Consistency Models](https://arxiv.org/abs/2403.06807) | 本文提出了多步一致性模型，通过在一致性模型和扩散模型之间插值，实现了采样速度和采样质量的平衡。 |
| [^35] | [On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes](https://arxiv.org/abs/2403.06806) | 该论文首次证明了策略梯度算法在平均奖励MDPs中的收敛性，并获得了有限时间的性能保证。 |
| [^36] | [Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification](https://arxiv.org/abs/2403.06798) | 该论文提出了一种动态扰动自适应对抗训练（DPAAT）方法，旨在在提高稳健性的同时保持高泛化，通过将对抗训练（AT）置于动态学习环境中生成自适应的数据级扰动，并通过损失信息收集提供动态更新的标准，以提高医学图像分类网络的稳健性。 |
| [^37] | [Leveraging Internal Representations of Model for Magnetic Image Classification](https://arxiv.org/abs/2403.06797) | 该研究利用深度学习内部表示，针对只有单个磁图像及其标签可用的情况，提出了一种新的机器学习模型训练范式，旨在解决数据稀缺性问题。 |
| [^38] | [Redefining Event Types and Group Evolution in Temporal Data](https://arxiv.org/abs/2403.06771) | 将事件类型重新定义为“原型”，其由一组称为“面向”的独特定量维度组合特征化，而不是强制执行严格的事件类型，从而允许涉及群体邻近多个原型的动态的混合描述。 |
| [^39] | [XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage](https://arxiv.org/abs/2403.06768) | XB-MAML引入了学习可扩展基参数的新方法，通过线性组合形成有效初始化，观察并利用基与微调参数在向量空间的差异，从而在多领域元学习中取得突破性进展。 |
| [^40] | [Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation](https://arxiv.org/abs/2403.06759) | 提出一种平均L1校准误差（mL1-ACE）作为辅助损失函数，用于改善图像分割中的像素级校准，减少了校准误差并引入了数据集可靠性直方图以提高校准评估。 |
| [^41] | [Koopman Ensembles for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2403.06757) | 研究提出了一种Koopman合奏方法，通过训练模型合奏产生具有高模型间方差的预测，从而改善集成模型的不确定性量化。 |
| [^42] | [ALaRM: Align Language Models via Hierarchical Rewards Modeling](https://arxiv.org/abs/2403.06754) | ALaRM是第一个从人类反馈中建模分层奖励的框架，通过整合整体奖励与特定方面的奖励，改善了大型语言模型与人类偏好的对齐性，尤其在复杂文本生成任务中表现出更精确和一致的指导。 |
| [^43] | [Generalising Multi-Agent Cooperation through Task-Agnostic Communication](https://arxiv.org/abs/2403.06750) | 通过预先训练的任务无关通信策略，在合作多机器人问题中实现了无需微调的任务泛化，支持更多Agent数量的扩展，并保证了收敛性。 |
| [^44] | [Shortcut Learning in Medical Image Segmentation](https://arxiv.org/abs/2403.06748) | 本研究将捷径学习现象扩展到医学图像分割领域，发现临床注释和特定数据处理方式可能误导模型并影响分割准确性，提出了缓解捷径学习影响的策略。 |
| [^45] | [On the Approximation of Kernel functions](https://arxiv.org/abs/2403.06731) | 本文提出了一种新的方法，通过考虑径向核函数的泰勒级数逼近，建立了对核函数的较好近似，证实了可以使用比文献中更小的正则化参数来实现更好的结果。 |
| [^46] | [Probabilistic Contrastive Learning for Long-Tailed Visual Recognition](https://arxiv.org/abs/2403.06726) | 提出了一种新颖的概率对比（ProCo）学习算法，该算法估计特征空间中每个类别样本的数据分布，并对比对进行采样 |
| [^47] | [Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning](https://arxiv.org/abs/2403.06725) | 本文提出了名为LoReKT的低资源知识追踪框架，通过监督预训练和微调重要性机制，旨在从丰富资源的KT数据集中学习可转移的参数和表示来改进低资源知识追踪任务。 |
| [^48] | [Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data](https://arxiv.org/abs/2403.06687) | 本研究提出了Hodge-Laplacian异构图注意网络（HL-HGAT），通过HL卷积滤波器、单纯投影和单纯注意池化运算符，可以学习跨k-单纯体的异构信号表示。 |
| [^49] | [Streamlining in the Riemannian Realm: Efficient Riemannian Optimization with Loopless Variance Reduction](https://arxiv.org/abs/2403.06677) | 引入了在黎曼优化中采用概率梯度计算触发器的Loopless SVRG和PAGE方法，简化了证明、提高了超参数选择效率，并提供了尖锐的收敛保证。 |
| [^50] | [Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains](https://arxiv.org/abs/2403.06672) | 本文研究了在隐私敏感领域中如何设计一种FL协议，既能保证隐私，又能提高模型准确性，并提供了设计出对所有参与者都有益处的协议。 |
| [^51] | [Untangling Gaussian Mixtures](https://arxiv.org/abs/2403.06671) | 这项研究在数据科学中探索了Tangles的潜力，将数据集中的簇与图中的Tangles联系起来，并开发了一套关于从高斯混合中抽取的数据集中Tangles的定量理论。 |
| [^52] | [PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor](https://arxiv.org/abs/2403.06668) | PeerAiD提出了一种新的对抗性蒸馏方法，通过让同行网络学习学生网络的对抗性示例，而不是自身的示例，来提升神经网络的鲁棒性。 |
| [^53] | [Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System](https://arxiv.org/abs/2403.06664) | Smart-Infinity通过在真实系统上使用近存储处理设备，解决了存储外部化的大型语言模型训练中存储带宽瓶颈的问题 |
| [^54] | [Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement](https://arxiv.org/abs/2403.06659) | 通过Multimodal ECG Representation Learning (MERL)框架，本文提出了一种零样本心电图分类方法，结合了对ECG记录和相关报告的多模态学习，同时在测试阶段使用了Clinical Knowledge Enhanced Prompt Engineering (CKEPE)方法来利用临床知识数据库。 |
| [^55] | [Ricci flow-based brain surface covariance descriptors for Alzheimer disease](https://arxiv.org/abs/2403.06645) | 本文首次提出了一种基于Ricci流的大脑表面协方差描述符的流水线，可以用于诊断阿尔茨海默病。 |
| [^56] | [Elephants Never Forget: Testing Language Models for Memorization of Tabular Data](https://arxiv.org/abs/2403.06644) | 本研究针对表格数据探讨了大型语言模型（LLMs）存在的数据污染和记忆化问题，发现LLMs在许多常见的表格数据集上进行了预训练，可能导致在下游任务中对性能评估的无效性。 |
| [^57] | [Spatial features of CO2 for occupancy detection in a naturally ventilated school building](https://arxiv.org/abs/2403.06643) | 通过空间CO2浓度分布提出两种新特征，使用支持向量机进行量化分析后发现，与基准相比，在自然通风房间中的占用状态检测准确率最多可提高14.8个百分点，达到83.2％（F1分数0.84）。有通风信息的情况下，准确率达到87.6％。 |
| [^58] | [Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings](https://arxiv.org/abs/2403.06631) | 本文研究了在工业环境中使用少样本学习进行物体检测的能效性，通过微调方法降低了能源消耗，并对模型在工业环境数据集上的能源需求进行了评估。 |
| [^59] | [Pulling back symmetric Riemannian geometry for data analysis](https://arxiv.org/abs/2403.06612) | 对称黎曼几何可用于捕捉数据几何的非线性结构，并且能够将欧几里德空间上的数据分析工具有效推广到对称黎曼流形上。 |
| [^60] | [Distributionally Generative Augmentation for Fair Facial Attribute Classification](https://arxiv.org/abs/2403.06606) | 该论文提出了一种新颖的分布生成增强方法，用于在偏见数据上训练公平的面部属性分类模型，无需额外标注虚假属性，通过生成模型识别潜在的虚假属性，并在图像空间中显示，以提高模型可解释性。 |
| [^61] | [ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models](https://arxiv.org/abs/2403.06586) | 将预训练的大型语言模型（LLMs）的常识知识有效地注入神经符号活动识别模型，以缓解标记数据稀缺性问题。 |
| [^62] | [FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing Fourier Transform and Auto-encoder](https://arxiv.org/abs/2403.06576) | 本论文提出了一种新的度量标准FFAD，利用傅里叶变换和自动编码器评估生成的时间序列数据质量，填补了时间序列数据评估领域的空白。 |
| [^63] | [Scalable Online Exploration via Coverability](https://arxiv.org/abs/2403.06571) | 提出了探索目标框架，引入了$L_1$-覆盖度作为新的探索目标，支持内在复杂度控制、高效规划和灵活集成的优点。 |
| [^64] | [Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming](https://arxiv.org/abs/2403.06569) | 通过深度学习的重编程特性，研究者成功将原本设计用于健全人的模型改编为适用于截肢者预测关节运动。 |
| [^65] | [Unraveling the Mystery of Scaling Laws: Part I](https://arxiv.org/abs/2403.06563) | 确认缩放定律原则在模型预训练中的重要作用，揭示OpenAI原始缩放定律论文的不完整细节，并探究预测测试损失轨迹可靠公式的挑战 |
| [^66] | [Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds](https://arxiv.org/abs/2403.06560) | 该论文在卡坦-哈达玛德流形上推导了切片-华塞斯坦距离的一般构造，提出了不同应用，并且推导了非参数方案以最小化这些新距离。 |
| [^67] | [Data-driven architecture to encode information in the kinematics of robots and artificial avatars](https://arxiv.org/abs/2403.06557) | 提出了一种数据驱动控制架构，用于编码机器人和人工角色的运动学信息，能够表达运动中的情绪信息。 |
| [^68] | [OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2403.06546) | 本文提出一种名为Optimally Matched Hierarchy（OMH）的新方法，通过在特征空间上施加结构化稀疏性，实现了无监督语义分割任务中特征优化的层次匹配。 |
| [^69] | [ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation](https://arxiv.org/abs/2403.06545) | 提出了一种新的方法，通过将形态特定的IHC染色分解为单独的图像通道，生成了in-silico免疫组织化学（IHC）图像，该方法在训练细胞核分割模型时在质量和数量上优于基线方法。 |
| [^70] | [Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning](https://arxiv.org/abs/2403.06535) | 本文提出了DeLAMA，一种具有动态协作图的分散多智能体终身协作学习算法，旨在通过自主识别协作关系和适应动态任务来增强多智能体之间的协作。 |
| [^71] | [SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection](https://arxiv.org/abs/2403.06534) | SARDet-100K是第一个COCO级别的大规模多类别SAR物体检测数据集，为研究提供了大规模且多样化的数据集，揭示了SAR物体检测中预训练模型显著差异的关键挑战。 |
| [^72] | [Adaptive Federated Learning Over the Air](https://arxiv.org/abs/2403.06528) | 提出了在空中模型训练框架内的自适应梯度方法的联邦版本，能够利用无线信道的叠加特性实现快速可伸缩的参数聚合，并通过动态调整步长增强了模型训练的稳健性。 |
| [^73] | [Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward](https://arxiv.org/abs/2403.06524) | 通过基于全面运营成本的奖励函数，采用深度强化学习框架优化自主卡车的战术决策，将高级决策与低级控制分离，并采用不同技巧提升性能。 |
| [^74] | [Automatic Generation of Python Programs Using Context-Free Grammars](https://arxiv.org/abs/2403.06503) | 使用上下文无关文法自动生成Python程序的TinyPy Generator工具可以确保生成的程序的正确性，并能轻松生成大规模Python代码，尤其适用于机器学习领域。 |
| [^75] | [Detection of Unobserved Common Causes based on NML Code in Discrete, Mixed, and Continuous Variables](https://arxiv.org/abs/2403.06499) | 提出一种新方法CLOUD，用于仅基于观测数据检测未观测到的共同原因，无需对未观测变量的方程模型形式做任何假设 |
| [^76] | [Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling](https://arxiv.org/abs/2403.06489) | 提出了一种基于图神经网络的框架，具有两个提升估计器，通过从社交图中学习提升估计来解决标签稀缺的个体提升建模问题 |
| [^77] | [Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach](https://arxiv.org/abs/2403.06485) | 提出了一种基于相关挖掘和大型语言模型推理的新型混合方法COLA，用于大规模云系统中的警报聚合，能够综合利用外部知识来解决基于语义相似性和统计方法的警报聚合的局限性。 |
| [^78] | [Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning](https://arxiv.org/abs/2403.06482) | 本文提出了一种 motif-preserving Graph Neural Network with curriculum learning (MotifGNN)模型，通过联合学习原始图的低阶结构和基于多视图图案的高阶结构，用于金融违约预测。 |
| [^79] | [RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling Approach](https://arxiv.org/abs/2403.06466) | RL-MSA提出了一种基于强化学习的多线路公交车调度方法，将多线路公交车调度问题建模为MDP，首次在离线阶段将直行车决策整合入公交车选择决策，有效简化学习问题，在线阶段通过时间窗口机制进行直行车决策。 |
| [^80] | [Prediction of Wort Density with LSTM Network](https://arxiv.org/abs/2403.06458) | 该论文介绍了一种使用LSTM网络预测麦汁密度的系统，通过廉价传感器测量值计算密度，帮助减少手动数据收集中的错误。 |
| [^81] | [A Survey of Learned Indexes for the Multi-dimensional Space](https://arxiv.org/abs/2403.06456) | 学习索引是将数据库索引结构视为机器学习模型，最近出现的趋势。本综述调查了学习多维索引结构的现状，分类了各种方法，并提出了分类方法。 |
| [^82] | [Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain](https://arxiv.org/abs/2403.06432) | 提出了一种受到计算机视觉中 JEPA 架构启发的 Spatio-Temporal Joint Embedding Masked Autoencoder（ST-JEMA）用于动态功能连接的自监督学习。 |
| [^83] | [A Differential Geometric View and Explainability of GNN on Evolving Graphs](https://arxiv.org/abs/2403.06425) | 提出了一种基于微分几何视角的方法，通过流形上的平滑曲线模拟图演化，实现了对图神经网络预测分布的可解释性。 |
| [^84] | [Bridging Domains with Approximately Shared Features](https://arxiv.org/abs/2403.06424) | 提出了一种统计框架，根据特征与标签的相关性方差来区分特征的效用，并设计了一种学习过程，从源任务学习到大致共享的特征表示，并在目标任务上进行微调，以优化总体风险。 |
| [^85] | [RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models](https://arxiv.org/abs/2403.06420) | RLingua提出了一个框架，利用大型语言模型的内部知识来提高机器人操作中强化学习的样本效率。 |
| [^86] | [Causal Multi-Label Feature Selection in Federated Setting](https://arxiv.org/abs/2403.06419) | 提出了在联合设置中进行因果多标签特征选择的挑战性问题，并提出了FedCMFS算法来解决该问题，算法通过三个新颖子程序，在不集中数据的情况下学习每个类标签的相关特征（候选父节点和子节点）。 |
| [^87] | [What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation](https://arxiv.org/abs/2403.06408) | 量化对大型语言模型的困难主要表现在如何通过添加扰动来改善模型性能和效率的关系，研究通过对不同人为扰动进行实验，发现了扰动特性与模型性能之间的联系，提出了改善量化稳健性的潜在解决方案。 |
| [^88] | [Cosine Scoring with Uncertainty for Neural Speaker Embedding](https://arxiv.org/abs/2403.06404) | 该论文提出了一种处理神经语者嵌入中不确定性的方法，通过在嵌入前端估算不确定性并传播至余弦打分后端，实现了在说话者识别中降低错误率和最小DCF的改进。 |
| [^89] | ['One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification](https://arxiv.org/abs/2403.06402) | 本文提出了自适应上下文学习（AICL）的工作流程，通过动态调整示例数量来提高文本分类的性能，类似于k最近邻（k-NN）中的可变大小邻域。 |
| [^90] | [On the Diminishing Returns of Width for Continual Learning](https://arxiv.org/abs/2403.06398) | 增加神经网络宽度以减少遗忘会带来递减的回报，并且在先前研究中尚未探索的宽度范围内进行了实证验证。 |
| [^91] | [DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2403.06397) | DeepSafeMPC是一种基于深度学习的模型预测控制方法，旨在有效预测多智体环境的复杂动态，并应用MARL原则寻找最优解。 |
| [^92] | [Towards Robust Out-of-Distribution Generalization Bounds via Sharpness](https://arxiv.org/abs/2403.06392) | 本文研究了学习的极小值的尖锐度如何影响模型对领域转移中数据变化的容忍度，并建立了尖锐度和鲁棒性之间的严格联系，提出了基于尖锐度的OOD泛化界限，为鲁棒算法提供了更好的OOD保证。 |
| [^93] | [A Zero Trust Framework for Realization and Defense Against Generative AI Attacks in Power Grid](https://arxiv.org/abs/2403.06388) | 提出了一种针对电力网络的生成式AI攻击的零信任框架，可早期检测潜在的攻击向量、评估尾部风险的稳定性措施，并对此类威胁进行缓解。 |
| [^94] | [Pre-Trained Model Recommendation for Downstream Fine-tuning](https://arxiv.org/abs/2403.06382) | 本文提出了一个名为Fennec的框架，通过将所有模型和历史任务映射到一个迁移相关的子空间，以便推断新任务在迁移空间中的表征，从而改善模型选择技术。 |
| [^95] | [FeatAug: Automatic Feature Augmentation From One-to-Many Relationship Tables](https://arxiv.org/abs/2403.06367) | FEATAUG是一种新的特征增强框架，能够从一对多关系表中自动提取具有谓词意识的SQL查询，弥补了Featuretools在许多实际场景中缺乏谓词的限制。 |
| [^96] | [Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach](https://arxiv.org/abs/2403.06366) | 本文通过切换系统模型，针对软Q-learning算法进行了有限时间误差分析，为两种软Q-learning算法导出了新颖的误差界限。 |
| [^97] | [Separable Physics-informed Neural Networks for Solving the BGK Model of the Boltzmann Equation](https://arxiv.org/abs/2403.06342) | 本研究介绍了一种基于可分离的物理信息神经网络的方法，用于解决Boltzmann方程的BGK模型，在处理高维偏微分方程时能够显著减少计算成本，并通过引入高斯函数来改善对神经网络的训练。 |
| [^98] | [Disentangling shared and private latent factors in multimodal Variational Autoencoders](https://arxiv.org/abs/2403.06338) | 多模态变分自动编码器在解开共享和私有潜在因素方面的能力进行了研究，并提出了改进的方法以增强对特定模态变化的鲁棒性。 |
| [^99] | [Transferable Reinforcement Learning via Generalized Occupancy Models](https://arxiv.org/abs/2403.06328) | 通过广义占有模型，本研究提出了一种新颖的模型类别，保留了模型化强化学习的通用性，并避免了累积错误的问题。 |
| [^100] | [From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification](https://arxiv.org/abs/2403.06326) | 提出了ACT框架，通过约束验证器自动计算每个响应的约束满意率，实现语言模型对齐与自动约束验证。 |
| [^101] | [Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL](https://arxiv.org/abs/2403.06323) | 本研究通过将其降低为标准强化学习提出了两个通用的元算法，一个基于乐观算法，另一个基于策略优化，概括了以往的风险敏感强化学习理论并证实了新的理论在具有有界可覆盖性的MDP中的有效性。 |
| [^102] | [Fake or Compromised? Making Sense of Malicious Clients in Federated Learning](https://arxiv.org/abs/2403.06319) | 该研究致力于澄清联邦学习中对恶意客户的混淆，通过提出混合对手模型来连接现有的对手模型，分析各种毒害攻击和防御聚合规则，从而为该领域的安全研究提供指导. |
| [^103] | [A Study on Domain Generalization for Failure Detection through Human Reactions in HRI](https://arxiv.org/abs/2403.06315) | 该研究通过分析使用人类面部表情训练的故障检测模型中的领域泛化，并发现在不同数据集上性能显著下降。对于HRI研究，这强调了改进模型稳健性和实际适用性的重要性。 |
| [^104] | [Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning](https://arxiv.org/abs/2403.06313) | 提出一种新颖的$L_0$范数正则化技术，用于深度强化学习的最优策略稀疏化和低秩分解 |
| [^105] | [How much data do you need? Part 2: Predicting DL class specific training dataset sizes](https://arxiv.org/abs/2403.06311) | 通过考虑每个类别的训练样本数量，而不仅仅是总体训练样本数量，来预测机器学习分类模型性能，并提出了一种基于空间填充设计的算法，可以对 CIFAR10 和 EMNIST 数据集进行应用。 |
| [^106] | [Nonparametric Automatic Differentiation Variational Inference with Spline Approximation](https://arxiv.org/abs/2403.06302) | 提出了一种基于样条的非参数逼近方法，实现了对复杂结构分布的灵活后验逼近，提高了生成模型性能。 |
| [^107] | [Analysis of Total Variation Minimization for Clustered Federated Learning](https://arxiv.org/abs/2403.06298) | GTVMin在集群化联邦学习中的分析提供了关于解决统计异质性的有效性和鲁棒性的宝贵见解 |
| [^108] | [Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning](https://arxiv.org/abs/2403.06289) | 本文揭示了人工标注误差不仅与合成标签错误有显著不同，而且在监督对比学习中构成了独特挑战，提出了一种新颖的对抗人工标注误差的SCL目标。 |
| [^109] | [Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond](https://arxiv.org/abs/2403.06279) | 本文致力于研究连续时间扩散模型中的熵正则化微调问题，并展示了分析如何扩展到涉及一般$f$-散度正则化的微调中。 |
| [^110] | [UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation](https://arxiv.org/abs/2403.06275) | 提出了一种名为UNICORN的新方法，通过分数匹配和自适应实现超声纳卡加米成像，能够在准确性和分辨率质量上超越传统方法。 |
| [^111] | [Physics-Guided Abnormal Trajectory Gap Detection](https://arxiv.org/abs/2403.06268) | 通过物理引导的方法，解决了在轨迹数据中识别异常间隙的挑战性问题，具有重要的社会应用和技术难度。 |
| [^112] | [Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance](https://arxiv.org/abs/2403.06265) | 本文研究了文本压缩在分词过程中的重要性，证明了压缩与预训练语言模型后续成功之间的实证重要性，并表明分词器的压缩与模型的性能存在相关性。 |
| [^113] | [Editing Conceptual Knowledge for Large Language Models](https://arxiv.org/abs/2403.06259) | 该论文首次研究了为大型语言模型编辑概念知识，通过构建基准数据集和建立新评估指标，发现现有方法虽然能一定程度上修改概念定义，但也可能造成LLMs中相关实例知识的扭曲，导致性能下降。 |
| [^114] | [Online Multi-spectral Neuron Tracing](https://arxiv.org/abs/2403.06251) | 提出了一种在线多光谱神经追踪方法，无需离线训练，可以快速并准确地在多光谱图像中重构神经。 |
| [^115] | [Cooperative Classification and Rationalization for Graph Generalization](https://arxiv.org/abs/2403.06239) | 本文提出了一种合作分类与理性化（C2R）方法，旨在解决图神经网络在泛化时面临的挑战，通过分类和理性化模块协同工作，改善对分布之外数据的泛化能力。 |
| [^116] | [Probabilistic Neural Circuits](https://arxiv.org/abs/2403.06235) | PNCs将概率电路和神经网络的特点结合起来，可以解释为深层混合的贝叶斯网络，同时作为强大的函数逼近器。 |
| [^117] | [LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding Linear Bandit Problem](https://arxiv.org/abs/2403.06230) | LinearAPT算法是一种为固定预算设置的阈值线性赌博机问题而设计的新算法，具有适应性、简单性和计算效率，并在优化顺序决策方面表现出色。 |
| [^118] | [Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!](https://arxiv.org/abs/2403.06201) | 介绍了LLMTrack模型，通过引入一种新颖的单提示技术，结合角色扮演和逐步思考方法，利用未经处理的IMU数据，实现了零射轨迹识别，超越了传统机器学习和深度学习模型，无需训练在专门数据集上的性能表现。 |
| [^119] | [DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency](https://arxiv.org/abs/2403.06197) | DrFuse通过解耦数据特征解决了临床多模态融合中的缺失模态和模态不一致性问题。 |
| [^120] | [An Improved Analysis of Langevin Algorithms with Prior Diffusion for Non-Log-Concave Sampling](https://arxiv.org/abs/2403.06183) | 本文研究了先前扩散技术对满足对数Sobolev不等式的目标分布的作用，扩展了先前仅针对强对数凹分布的相关工作。 |
| [^121] | [Domain Adversarial Active Learning for Domain Generalization Classification](https://arxiv.org/abs/2403.06174) | 本文提出了一种用于领域泛化分类任务的领域对抗主动学习（DAAL）算法，通过设计一种优先选择具有挑战性样本的领域对抗选择方法，来改善模型的泛化能力。 |
| [^122] | [Speeding up 6-DoF Grasp Sampling with Quality-Diversity](https://arxiv.org/abs/2403.06173) | 该论文研究了如何将质量多样性（QD）与先验知识结合，以加速在仿真中生成多样化抓取姿势，实验证明QD相比于标准的六自由度抓取采样方案有着显著的优势。 |
| [^123] | [The ALL0CORE Tensor Decomposition for Sparse Count Data](https://arxiv.org/abs/2403.06153) | ALL0CORE是一种新的概率非负张量分解方法，它在保持计算可处理性的基础上利用Tucker分解的潜在结构，可以仅使用核的微小部分即达到与完整Tucker分解相同效果。 |
| [^124] | [MACE: Mass Concept Erasure in Diffusion Models](https://arxiv.org/abs/2403.06135) | MACE通过成功将概念消除的范围扩展到100个概念，并在泛化性和特异性之间取得有效平衡，从而防止大规模文本到图像扩散模型生成不良或误导性图像。 |
| [^125] | [Automatic design optimization of preference-based subjective evaluation with online learning in crowdsourcing environment](https://arxiv.org/abs/2403.06100) | 在众包环境中，我们提出了一种自动优化方法，利用在线学习对配对组合和评估量进行优化，实现基于喜好的主观评估的设计优化。 |
| [^126] | [Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models](https://arxiv.org/abs/2403.06088) | 本文通过研究合成数据集的实用性和不同视觉基础模型在车载多任务面部属性识别中的效果，填补了现有文献中的空白。 |
| [^127] | [Learning the irreversible progression trajectory of Alzheimer's disease](https://arxiv.org/abs/2403.06087) | 提出了一种正则化方法来预测阿尔茨海默病的不可逆进展轨迹 |
| [^128] | [FrameQuant: Flexible Low-Bit Quantization for Transformers](https://arxiv.org/abs/2403.06082) | 提出一种简单的方案，通过融合框架将Transformer模型量化为仅两位，仅有轻微精度下降。 |
| [^129] | [Local Vertex Colouring Graph Neural Networks](https://arxiv.org/abs/2403.06080) | 提出一种基于本地顶点着色的方案，通过经典搜索算法能够高效计算超越1-WL的图表示，并展示这种着色方案可以帮助解决图双连通性等问题，同时表明在某些条件下，GNNs的表达能力随着搜索邻域半径的增加而层级提高。 |
| [^130] | [Generalization of Graph Neural Networks through the Lens of Homomorphism](https://arxiv.org/abs/2403.06079) | 本文通过分析图同态的熵提出了一种新颖视角，推导出了适用于图和节点分类的泛化界限，能够捕捉各种图结构的细微差异，并通过统一框架刻画了广泛的GNN模型。 |
| [^131] | [Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising](https://arxiv.org/abs/2403.06069) | I3SB方法通过引入非马尔可夫过程，结合损坏的图像改善纹理恢复，在CT超分辨率和去噪任务中表现优异。 |
| [^132] | [CausalCellSegmenter: Causal Inference inspired Diversified Aggregation Convolution for Pathology Image Segmentation](https://arxiv.org/abs/2403.06066) | 提出了一种结合了因果推断模块和多样化聚合卷积技术的CausalCellSegmenter框架，通过DAC模块和SimAM关注模块解决了细胞核分割中的挑战，包括误报识别和边缘模糊问题。 |
| [^133] | [L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification](https://arxiv.org/abs/2403.06064) | 本文提出了一种新颖的洛伦兹线性图卷积网络框架，将双曲空间引入线性GCN，用于捕捉数据的树状结构，并在实验中取得了新的最先进的节点分类结果。 |
| [^134] | [Ensemble Language Models for Multilingual Sentiment Analysis](https://arxiv.org/abs/2403.06060) | 该研究针对低资源语言如阿拉伯语进行了情感分析，提出了两种集成语言模型，发现单语模型表现优越，集成模型胜过基线，而多数投票集成优于英语语言。 |
| [^135] | [Absence of spurious solutions far from ground truth: A low-rank analysis with high-order losses](https://arxiv.org/abs/2403.06056) | 本研究证明了在某些条件下，远离真实解的关键点表现出有利的严格鞍点几何结构，并介绍了高阶损失的概念，通过将其纳入目标函数增加负曲率，从而加速摆脱这些临界点。 |
| [^136] | [Decoupled Data Consistency with Diffusion Purification for Image Restoration](https://arxiv.org/abs/2403.06054) | 通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。 |
| [^137] | [Texture image retrieval using a classification and contourlet-based features](https://arxiv.org/abs/2403.06048) | 使用新的基于RCT-Plus变换的图像表示和学习方法改进了纹理图像检索，实现了比以前方案更高的检索速率。 |
| [^138] | [MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts](https://arxiv.org/abs/2403.06041) | 本研究提出了一种名为MATRIX的学习基础自动轨迹生成模型，能够在多样化背景中生成交互式人类行为。 |
| [^139] | [Predicting Depression and Anxiety: A Multi-Layer Perceptron for Analyzing the Mental Health Impact of COVID-19](https://arxiv.org/abs/2403.06033) | 提出了一个名为CoDAP的多层感知器，用于预测COVID-19大流行期间焦虑和抑郁的趋势，通过定性个体属性视角分析了焦虑和抑郁模式，并揭示了人口因素、行为变化和社会心理健康决定因素的关键见解 |
| [^140] | [FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition](https://arxiv.org/abs/2403.06031) | FairTargetSim提供了一个交互式模拟器，展示了目标变量定义对公平性的影响，适用于算法开发者、研究人员和非技术利益相关者。 |
| [^141] | [Multimodal deep learning approach to predicting neurological recovery from coma after cardiac arrest](https://arxiv.org/abs/2403.06027) | 使用多模态深度学习方法结合临床数据和时间序列信号，成功预测心脏骤停后昏迷患者的神经恢复，同时展示了迁移学习在医学分类中的效用和限制 |
| [^142] | [Towards a Generic Representation of Cominatorial Problems for Learning-Based Approaches](https://arxiv.org/abs/2403.06026) | 本文倡导为基于学习方法的组合问题构建通用表示，以解决特定表示无法跨越不同组合问题的问题。 |
| [^143] | [Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis Diagnosis](https://arxiv.org/abs/2403.06024) | 引入了半监督多模态多实例学习（SMMIL）框架，用于结构性心脏疾病自动解释，克服了心脏超声心动图评估AS时对有限2D cineloops和难以获得标记数据的限制。 |
| [^144] | [Persian Slang Text Conversion to Formal and Deep Learning of Persian Short Texts on Social Media for Sentiment Classification](https://arxiv.org/abs/2403.06023) | 通过提供PSC工具将波斯语俚语文本转换为正式文本，结合深度学习方法进行波斯语短文本的情感学习。 |
| [^145] | [Hierarchical Query Classification in E-commerce Search](https://arxiv.org/abs/2403.06021) | 提出了一个利用分层信息的新框架，以增强表示学习，解决了分层查询分类中的类别不平衡和查询模糊性带来的挑战。 |
| [^146] | [Multi-conditioned Graph Diffusion for Neural Architecture Search](https://arxiv.org/abs/2403.06020) | 提出了一种基于图扩散的NAS方法，结合多条件无分类器指导方法，能在架构搜索中生成快速且性能优越的神经网络架构，并在多个标准基准和ImageNet数据集上取得了有希望的结果 |
| [^147] | [Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages](https://arxiv.org/abs/2403.06018) | 本研究评估了如何将具有70亿参数的开源PLM LLaMa 用于低资源语言的提示，解决了跨语言适应提示的问题。 |
| [^148] | [Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark](https://arxiv.org/abs/2403.06017) | 通过引入满足广泛要求的合成、半合成和真实世界数据集，解决了公平图学习中数据集信息不足的问题。 |
| [^149] | [Grafting: Making Random Forests Consistent](https://arxiv.org/abs/2403.06015) | 本文探讨了将一致化估计器移植到浅层决策树（CART）的适用性，并表明这种方法具有一致性保证并在实证环境中表现良好。 |
| [^150] | [Hard-label based Small Query Black-box Adversarial Attack](https://arxiv.org/abs/2403.06014) | 提出了一种新的基于硬标签的黑盒对抗攻击方法，通过在优化过程中利用预训练替代模型的指导，显著提高了攻击的查询效率。 |
| [^151] | [Are Classification Robustness and Explanation Robustness Really Strongly Correlated? An Analysis Through Input Loss Landscape](https://arxiv.org/abs/2403.06013) | 通过新颖的评估方法和训练方法，本研究发现增强解释鲁棒性并不能提高分类鲁棒性，这一发现挑战了传统观念。 |
| [^152] | [Reinforcement Learning Paycheck Optimization for Multivariate Financial Goals](https://arxiv.org/abs/2403.06011) | 该论文提出了一种针对多个竞争性金融目标的工资优化问题的强化学习方法，并提出了统一不同目标、整合用户偏好和处理随机利率的问题形式化，为解决这一问题提供了新思路。 |
| [^153] | [Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations](https://arxiv.org/abs/2403.06009) | 创建了一系列检测器库，其中包含紧凑且易于构建的分类模型，为各种危害提供标签，可作为大型语言模型（LLMs）的有效替代方案。 |
| [^154] | [A Generalized Acquisition Function for Preference-based Reward Learning](https://arxiv.org/abs/2403.06003) | 通过引入一种可以捕捉奖励函数相似性定义的框架，优化了奖励函数的学习效率。 |
| [^155] | [Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence](https://arxiv.org/abs/2403.05996) | 本研究剖析了深度强化学习中的首要偏差现象，发现在大量更新比例下，价值高估是导致学习失败的根本挑战。 |
| [^156] | [Enhancing Classification Performance via Reinforcement Learning for Feature Selection](https://arxiv.org/abs/2403.05979) | 通过强化学习算法，特别是Q学习和SARSA学习，优化特征选择以增强分类模型的性能，取得了87%和88%的最高分类准确率。 |
| [^157] | [Calibrating Large Language Models Using Their Generations Only](https://arxiv.org/abs/2403.05973) | 使用APRICOT方法，通过仅使用大型语言模型的文本输入和输出来设置置信目标并训练额外模型，从而实现大型语言模型的校准。 |
| [^158] | [Can Generative Models Improve Self-Supervised Representation Learning?](https://arxiv.org/abs/2403.05966) | 本文引入了一个新的框架，通过利用生成模型生成语义一致的图像增强，丰富了自监督学习的方法，实验结果表明提高了学到的视觉质量。 |
| [^159] | [Robust Emotion Recognition in Context Debiasing](https://arxiv.org/abs/2403.05963) | 提出了一个反事实情绪推理（CLEF）框架来解决上下文偏差干扰的挑战 |
| [^160] | [General surgery vision transformer: A video pre-trained foundation model for general surgery](https://arxiv.org/abs/2403.05949) | 该论文开源了迄今为止最大的通用外科视频数据集，提出了用于外科应用的视频预训练通用外科视觉变换器（GSViT）技术，并展示了其在Cholec80阶段注释任务上的优越性能。 |
| [^161] | [Thread Detection and Response Generation using Transformers with Prompt Optimisation](https://arxiv.org/abs/2403.05931) | 通过优化提示，该研究提出了一种端到端模型，能够识别对话中的线程并基于其重要性优先生成响应。 |
| [^162] | [SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data](https://arxiv.org/abs/2403.05918) | 基于残差网络的扩散建模方法能够有效处理不平衡数据，克服了经典过采样方法和基于生成网络的模式塌陷与训练不稳定问题。 |
| [^163] | [Online Identification of Stochastic Continuous-Time Wiener Models Using Sampled Data](https://arxiv.org/abs/2403.05899) | 开发了一种简单的递归在线估计算法，用于通过随机逼近识别连续时间随机参数Wiener模型，具有对扰动过程频谱假设鲁棒性。 |
| [^164] | [Towards Efficient Replay in Federated Incremental Learning](https://arxiv.org/abs/2403.05890) | 本研究提出了一种名为Re-Fed的简单通用框架，用于联邦增量学习中的重播，通过协调每个客户端缓存重要样本以减轻灾难性遗忘问题。 |
| [^165] | [DiffRed: Dimensionality Reduction guided by stable rank](https://arxiv.org/abs/2403.05882) | DiffRed 提出了一种由稳定秩引导的降维方法，证明了在 Stress 和 M1 上取得了较紧密的上界，并通过实验证明在多种真实世界数据集上取得了良好效果。 |
| [^166] | [Deep Learning based acoustic measurement approach for robotic applications on orthopedics](https://arxiv.org/abs/2403.05879) | 本研究提出了一种基于深度学习的结构，通过A模式超声波（US）改进骨骼跟踪的准确性。 |
| [^167] | [LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss](https://arxiv.org/abs/2403.05873) | 提出了一种名为Legion的新方法，利用预训练语言模型（PTMs）为GitHub存储库推荐主题，以解决现有技术在主题推荐中的局限性。 |
| [^168] | [PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems](https://arxiv.org/abs/2403.05864) | PAPER-HILT是针对人机协同系统中隐私保护的创新自适应强化学习策略，通过提前退出方法动态调整隐私保护和系统效用，以适应个体行为模式和偏好。 |
| [^169] | [tLaSDI: Thermodynamics-informed latent space dynamics identification](https://arxiv.org/abs/2403.05848) | 提出了一种融合热力学定律的数据驱动潜空间动力学识别方法，通过自动编码器学习潜变量并构建动力学模型，实现对热力学定律的遵守，并通过新的损失函数进行训练。演示了其在稳健泛化能力方面的表现，以及在潜空间中熵产生速率与系统行为之间的相关性。 |
| [^170] | [TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic Analysis and Generation](https://arxiv.org/abs/2403.05822) | TrafficGPT 是一个深度学习模型，旨在突破令牌长度限制，实现高效的长时间流量分析和生成，解决了网络流量分析和生成中依赖标记数据和生成符合实际模式的流量样本的难题。 |
| [^171] | [Optimizing LLM Queries in Relational Workloads](https://arxiv.org/abs/2403.05821) | 本文研究了如何优化在关系查询中调用LLM的分析型工作负载的推理过程，发现关系查询为加速LLM推理提供了新颖的机会。 |
| [^172] | [PR-NET: Leveraging Pathway Refined Network Structures for Prostate Cancer Patient Condition Prediction](https://arxiv.org/abs/2403.05818) | PR-NET模型通过压缩和优化P-NET的网络结构，降低了模型复杂性，同时保持高准确性和可解释性，在前列腺癌患者状况预测中表现出优越性能。 |
| [^173] | [Statistical Efficiency of Distributional Temporal Difference](https://arxiv.org/abs/2403.05811) | 该论文分析了分布式时间差分的统计效率和有限样本性能。 |
| [^174] | [Shallow ReLU neural networks and finite elements](https://arxiv.org/abs/2403.05809) | 在凸多面体网格上，提出了用两个隐藏层的ReLU神经网络来弱表示分段线性函数，并根据网格中的多面体和超平面的数量准确确定了所需的神经元数，建立了浅层ReLU神经网络和有限元函数之间的联系。 |
| [^175] | [$\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting](https://arxiv.org/abs/2403.05798) | 提出了$\textbf{S}^2$IP-LLM，利用预训练的语言模型进行时间序列预测，并将语义空间与时间序列嵌入空间对齐进行提示学习。 |
| [^176] | [Optimistic Safety for Linearly-Constrained Online Convex Optimization](https://arxiv.org/abs/2403.05786) | 本文提出了一种乐观安全性的设计范式，在线性约束在线凸优化问题中取得了$\tilde{\mathcal{O}}(\sqrt{T})$的后悔值，同时将问题转化为在时变随机线性约束下的OCO问题，展示了算法在这个设置下的有效性。 |
| [^177] | [Large Generative Model Assisted 3D Semantic Communication](https://arxiv.org/abs/2403.05783) | 提出了一种基于生成AI模型辅助的3D语义通信系统，包括3D语义提取器和自适应语义压缩模型，用于解决3D场景中的语义提取和编码挑战 |
| [^178] | [Spatial Clustering Approach for Vessel Path Identification](https://arxiv.org/abs/2403.05778) | 该论文提出了一种仅利用位置信息的船舶路径的空间聚类方法，通过两种方法实现路径聚类，达到了完美的F1分数，为改善海运安全和效率提供了宝贵见解。 |
| [^179] | [Extending Activation Steering to Broad Skills and Multiple Behaviours](https://arxiv.org/abs/2403.05767) | 本文研究了将激活导向技术应用于广泛技能和多种行为的功效，并发现导向广泛技能具有竞争力，同时在模型中同时注入个体导向向量是一种有前途的方法。 |
| [^180] | [Physics-informed Neural Motion Planning on Constraint Manifolds](https://arxiv.org/abs/2403.05765) | 提出了第一个在约束流形上解决Eikonal方程的物理信息CMP框架，无需专家数据，高效。 |
| [^181] | [HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning](https://arxiv.org/abs/2403.05763) | 本文提出了一种基于超维计算的算法-硬件协同设计，用于更高效和加速的知识图推理。 |
| [^182] | [Membership Testing in Markov Equivalence Classes via Independence Query Oracles](https://arxiv.org/abs/2403.05759) | 通过建立在给定的最大无向团大小($s$)方面的下界，我们探讨了通过独立查询预言者在马尔可夫等价类中进行成员测试这一问题。 |
| [^183] | [Model-Free Local Recalibration of Neural Networks](https://arxiv.org/abs/2403.05756) | 提出了一种使用ANN隐层提供的输入的降维表示来对ANN预测分布进行本地重新校准的方法，从文献中近似贝叶斯计算和无似然推断方法的重新校准技术中获取灵感。 |
| [^184] | [Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition with Completeness Analysis](https://arxiv.org/abs/2403.05754) | 提出了两种根植于残差连接和密集连接的混合量子启发式神经网络，用于更全面地改进和评估新型神经网络在复杂和不可预测环境中的表现 |
| [^185] | [Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling](https://arxiv.org/abs/2403.05752) | 本文提出了一种自动化TOSG提取的方法KG-TOSA，用于在大型知识图上进行面向任务的图神经网络训练，以减轻对大型KG的过多计算负担。 |
| [^186] | [MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process](https://arxiv.org/abs/2403.05751) | 提出了一种新颖的MG-TSD模型，利用数据内在粒度水平作为目标来引导学习过程，实现了状态-of-the-art的预测性能 |
| [^187] | [Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text](https://arxiv.org/abs/2403.05750) | 大型语言模型在自然语言生成领域取得了重大突破，提出了识别AI生成文本的解决方案，并探索了未来研究方向。 |
| [^188] | [Generative Probabilistic Forecasting with Applications in Market Operations](https://arxiv.org/abs/2403.05743) | 提出了一种基于Wiener-Kallianpur创新表示的生成式概率预测方法，包括自编码器和新颖的深度学习算法，具有渐近最优性和结构收敛性质，适用于实时市场运营中的高动态和波动时间序列。 |
| [^189] | [Provable Policy Gradient Methods for Average-Reward Markov Potential Games](https://arxiv.org/abs/2403.05738) | 提出了针对平均回报准则下马尔可夫潜在博弈的可证明的策略梯度方法，并展示了算法的全局收敛性和时间复杂度。 |
| [^190] | [Conservative DDPG -- Pessimistic RL without Ensemble](https://arxiv.org/abs/2403.05732) | 提出了一种新的保守DDPG方法，通过引入$Q$-目标和行为克隆损失惩罚来解决DDPG中的高估偏差问题，可以在不需要集成的情况下轻松实现，并且在各种任务中表现优异。 |
| [^191] | [Augmentations vs Algorithms: What Works in Self-Supervised Learning](https://arxiv.org/abs/2403.05726) | 研究了自监督学习中数据增强、预训练算法和模型架构的相对影响，提出了统一SSL方法的新框架，发现除了改变预训练算法外，新数据增强和更强大的模型架构也起到重要作用 |
| [^192] | [A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries](https://arxiv.org/abs/2403.05720) | 介绍了一个新的基准测试，评估了用于生成简要住院病程摘要的大语言模型在健康保健领域中的性能并提出相应的自适应策略 |
| [^193] | [A Framework for Effective AI Recommendations in Cyber-Physical-Human Systems](https://arxiv.org/abs/2403.05715) | 该论文提出了一个框架来解决网络物理人系统中人工智能推荐和人类决策者之间的挑战，通过考虑人类可能不同于AI平台的感知和解释方式，建立了最佳推荐策略的结构特性，并开发了近似人类模型，从而提供了理论上的最优性界限和数值示例验证结果。 |
| [^194] | [$\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer](https://arxiv.org/abs/2403.05713) | $\mathtt{tsGT}$是一种基于通用Transformer架构的随机时间序列模型，表现优于最先进模型，并超过其随机同行，特别在数据分布建模和边际分位值预测方面具备优势。 |
| [^195] | [Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking](https://arxiv.org/abs/2403.05693) | 本文基于线性时态逻辑（LTL）形式化了航天器任务和安全要求，并提出了自动构建奖励函数以实现有效训练的方法。同时探讨了从安全LTL规范构建航天器屏蔽的方法，提出了三种可以提供概率保证的设计，并通过实验展示了这些屏蔽与不同策略的互动和奖励结构的灵活性。 |
| [^196] | [Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning](https://arxiv.org/abs/2403.05683) | 决策焦点学习（DFL）在公共卫生干预中的应用提高了干预的精准度，虽然存在高计算成本，但能优化有限的干预资源使用效果。 |
| [^197] | [DP-TabICL: In-Context Learning with Differentially Private Tabular Data](https://arxiv.org/abs/2403.05681) | 研究了如何使用差分隐私来保护在环境中学习中使用的表格数据。 |
| [^198] | [Spectral Clustering of Categorical and Mixed-type Data via Extra Graph Nodes](https://arxiv.org/abs/2403.05669) | 通过添加额外节点，本文提出了一种新的方法，将数值和分类信息同时纳入谱聚类算法，避免了数据预处理或复杂相似性函数的需求。 |
| [^199] | [Prepared for the Worst: A Learning-Based Adversarial Attack for Resilience Analysis of the ICP Algorithm](https://arxiv.org/abs/2403.05666) | 通过对ICP算法进行基于深度学习的攻击，在安全关键应用中评估其鲁棒性，重点在于找到可能的最大ICP姿势误差。 |
| [^200] | [What is different between these datasets?](https://arxiv.org/abs/2403.05652) | 这里是中文总结出的一句话要点 |
| [^201] | [Geometric Neural Network based on Phase Space for BCI decoding](https://arxiv.org/abs/2403.05645) | 基于相空间的几何神经网络用于BCI解码，提供了在脑机接口领域中可靠算法操作的方法，以提高用户舒适度并促进其广泛应用。 |
| [^202] | [OmniJet-$\alpha$: The first cross-task foundation model for particle physics](https://arxiv.org/abs/2403.05618) | 基于物理数据和变压器架构，OmniJet-$\alpha$是首个跨任务基础模型，引入了全面的评估方法并展示了在无监督问题上的迁移学习。 |
| [^203] | [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://arxiv.org/abs/2403.05612) | 本文研究了大型语言模型如何产生幻觉，并提出通过调整微调示例的监督来控制其对不熟悉输入的预测。作者开发了一种基于RL的方法，更可靠地减轻了长篇生成任务中的幻觉。 |
| [^204] | [Evidence, Definitions and Algorithms regarding the Existence of Cohesive-Convergence Groups in Neural Network Optimization](https://arxiv.org/abs/2403.05610) | 论文讨论了一种新的方法，通过观察人工神经网络优化过程中出现的凝聚收敛群体，以拓展最近关于神经网络收敛问题的研究。 |
| [^205] | [A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data](https://arxiv.org/abs/2403.05606) | 提出了一种基于概念的可解释模型，用于利用多模态数据诊断脉络膜肿瘤，促进了对罕见疾病的诊断，并在临床实践和医学教育中具有重要意义 |
| [^206] | [Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature using Attention-based Relational Context Information](https://arxiv.org/abs/2403.05602) | 通过利用关系上下文信息的Transformer-based深度学习方法，本研究提出了一个统一的、多源PPI语料库，改进了关系分类性能。 |
| [^207] | [Select High-Level Features: Efficient Experts from a Hierarchical Classification Network](https://arxiv.org/abs/2403.05601) | 提出了一种新颖的专家生成方法，通过选择高级特征实现动态降低任务和计算复杂性，为未来设计轻量级和适应性强的网络铺平了道路 |
| [^208] | [Density-Regression: Efficient and Distance-Aware Deep Regressor for Uncertainty Estimation under Distribution Shifts](https://arxiv.org/abs/2403.05600) | 密度回归是一种利用密度函数进行不确定性估计并通过单次前向传递实现快速推断的方法，具有距离感知能力，能够在分布偏移下产生高质量不确定性估计。 |
| [^209] | [Privacy Amplification for the Gaussian Mechanism via Bounded Support](https://arxiv.org/abs/2403.05598) | 通过有界支持的高斯机制的简单修改，可以在数据相关核算下增强隐私保证 |
| [^210] | [Comparison of gait phase detection using traditional machine learning and deep learning techniques](https://arxiv.org/abs/2403.05595) | 本研究比较了使用传统机器学习和深度学习技术在下肢肌电图数据上进行步态相位检测，对于控制下肢辅助设备具有重要意义。 |
| [^211] | [Data-Driven Ergonomic Risk Assessment of Complex Hand-intensive Manufacturing Processes](https://arxiv.org/abs/2403.05591) | 开发了一个数据驱动的人体工程学风险评估系统，特别关注手部和手指活动，以更好地识别和解决与手工密集型制造过程相关的人体工程学问题 |
| [^212] | [Can Interpretability Layouts Influence Human Perception of Offensive Sentences?](https://arxiv.org/abs/2403.05581) | 本文通过用户研究探讨了机器学习解释布局是否会影响参与者对包含仇恨言论句子的评价，结果表明解释布局在触发参与者提供纠正性反馈和评估模型方面具有优势 |
| [^213] | [Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners](https://arxiv.org/abs/2403.05578) | 本研究提出了一种新方法，利用文本到图像模型和大型语言模型生成个性化网页横幅，根据用户互动动态内容，并且无需人工干预。 |
| [^214] | [Beyond Predictive Algorithms in Child Welfare](https://arxiv.org/abs/2403.05573) | 研究发现，儿童福利领域常用的风险评估模型无法准确预测未与出生父母团聚的儿童的出院结果。 |
| [^215] | [Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with Constrained Diffusion Model](https://arxiv.org/abs/2403.05571) | 本文提出了一种具有约束扩散模型的高效和保证安全的非凸轨迹优化框架，通过结合扩散模型和数值求解器，保证了计算效率和约束满足。 |
| [^216] | [Improving Cognitive Diagnosis Models with Adaptive Relational Graph Neural Networks](https://arxiv.org/abs/2403.05559) | 提出了一种利用自适应语义感知图神经网络改进认知诊断模型的方法，弥补了现有研究中对边缘内的异质性和不确定性的忽视。 |
| [^217] | [Re-thinking Human Activity Recognition with Hierarchy-aware Label Relationship Modeling](https://arxiv.org/abs/2403.05557) | 重新思考人类活动识别任务，通过层级感知的标签关系建模提高模型性能，并将复杂的标签关系纳入基本模型中 |
| [^218] | [Modeling and predicting students' engagement behaviors using mixture Markov models](https://arxiv.org/abs/2403.05556) | 本文通过混合马尔可夫模型对学生的参与行为进行建模和预测，并引入了一种名为K-EM的基于K均值的初始化方法，实验结果表明其效果极具前景 |
| [^219] | [Subgroup Discovery in MOOCs: A Big Data Application for Describing Different Types of Learners](https://arxiv.org/abs/2403.05555) | 通过基于MapReduce的子群体发现方法，本论文旨在对MOOC中的不同类型学习者进行分类和描述，提出的方法在处理海量数据集中表现优异。 |
| [^220] | [Understanding the Progression of Educational Topics via Semantic Matching](https://arxiv.org/abs/2403.05553) | 本文利用BERT主题建模从课程中提取主题，然后利用这些主题来识别不同学科之间的关系，帮助我们更好地理解各种学习话题的演进。 |
| [^221] | [Multi-source and multimodal data fusion for predicting academic performance in blended learning university courses](https://arxiv.org/abs/2403.05552) | 该研究应用多源多模态数据融合方法，发现在混合式学习环境中预测学生学术表现的最佳属性集为理论课上的关注程度、Moodle测验成绩以及Moodle论坛上的活动水平。 |
| [^222] | [Monitoring the evolution of antisemitic discourse on extremist social media using BERT](https://arxiv.org/abs/2403.05548) | 本研究提出了一种使用BERT监测极端社交媒体上反犹太主义话语演变的自动方法，避免了手动监测的不可行性，为干预和防止仇恨升级提供了新途径。 |
| [^223] | [AI for non-programmers: Applied AI in the lectures for students without programming skills](https://arxiv.org/abs/2403.05547) | 本研究提出了一个应用AI的教学规划脚本，通过将AI概念与研究相关主题联系起来，促进了学生对AI潜力和风险的理解和兴趣。 |
| [^224] | [Unified Occupancy on a Public Transport Network through Combination of AFC and APC Data](https://arxiv.org/abs/2403.05546) | 本文提出了统一乘客占座率方法，通过结合AFC和APC数据，在公共交通网络上推断每个班次的乘客占座率，弥补了缺失的APC信息。 |
| [^225] | [AI in ESG for Financial Institutions: An Industrial Survey](https://arxiv.org/abs/2403.05541) | 本文调查了金融机构中人工智能在ESG倡议中的应用，阐明了AI在加强ESG框架方面的必要性和影响，以及AI如何增强金融活动和可持续发展目标之间的复杂相互作用。 |
| [^226] | [Extinction Risks from AI: Invisible to Science?](https://arxiv.org/abs/2403.05540) | 论文探讨了AI的灭绝风险，并提出了灭绝级古哈特定律，指出任何目标规范过度追求都可能导致人类灭绝，同时指出评估此假设需要满足一定条件，但可能由于模型复杂性而难以实现，暗示了人工智能带来的灭绝风险可能在当前科学方法下难以观察到。 |
| [^227] | [Switching the Loss Reduces the Cost in Batch Reinforcement Learning](https://arxiv.org/abs/2403.05385) | 使用对数损失函数来训练适合的Q迭代的批强化学习方法，在实现目标时不产生成本的问题中，其样本数量需求与最优策略的累积成本成比例，能够提供与最优可达成本成比例的“小成本”界限，并在实验中验证在那些最优策略可靠实现目标的问题中，FQI-LOG比使用平方损失训练的FQI使用更少的样本。 |
| [^228] | [Cell reprogramming design by transfer learning of functional transcriptional networks](https://arxiv.org/abs/2403.04837) | 通过迁移学习设计细胞重编程，利用功能转录网络的知识来控制细胞行为，实现从初始状态到目标状态的转录状态之间的最小化差异 |
| [^229] | [TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection](https://arxiv.org/abs/2403.04789) | 提出了一种TopicDiff方法，用于捕获多模态会话情感检测任务中的主题信息，通过将扩散模型集成到神经主题模型中，解决了神经主题模型在捕获主题信息方面的多样性不足问题，并相对于现有MCE基线取得了显著改进 |
| [^230] | [Removing GPT4's Filter](https://arxiv.org/abs/2403.04769) | 提出了一种方法，可以使经过微调的GPT4恢复到没有经过人类反馈强化学习训练的状态，从而移除其在学习期间的所有安全机制 |
| [^231] | [What makes an image realistic?](https://arxiv.org/abs/2403.04493) | 论文讨论了如何设计能够可靠区分真实数据和不真实数据的函数，提出了通用评论者的概念作为一个新的解决方案。 |
| [^232] | [Advancing Biomedical Text Mining with Community Challenges](https://arxiv.org/abs/2403.04261) | 社区挑战评估竞赛在促进生物医学文本挖掘研究中的技术创新和跨学科合作方面起着重要作用。 |
| [^233] | [Aligners: Decoupling LLMs and Alignment](https://arxiv.org/abs/2403.04224) | 提出了一种通过训练对齐器模型来解耦大型语言模型（LLMs）和对齐，以减少对齐对性能的潜在负面影响。 |
| [^234] | [SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS](https://arxiv.org/abs/2403.04161) | 提出了一种新颖的高性能无需训练的度量SWAP-Score，能够在不同搜索空间和任务中测量网络在一批输入样本上的表现能力，并通过正则化进一步提高相关性，实现模型大小的控制。 |
| [^235] | [Can Large Language Models Reason and Plan?](https://arxiv.org/abs/2403.04121) | 大型语言模型缺乏自我批评能力，无法像人类一样纠正错误。 |
| [^236] | [DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models](https://arxiv.org/abs/2403.03768) | DeepCRE是一种新型的计算模型，在患者级别CRE性能上平均提高了17.7％，在指示级别CRE增加了5倍，并成功确定了六个具有显着优势的药物候选者。 |
| [^237] | [An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand](https://arxiv.org/abs/2403.03434) | 通过将图神经网络（GNN）和长短期记忆（LSTM）网络与传统ABM相结合，开发了一种可微分的基于Agent的模型，成功应用于模拟新西兰2019年麻疹爆发，深入洞察传染病爆发的动态。 |
| [^238] | [Hypothesis Spaces for Deep Learning](https://arxiv.org/abs/2403.03353) | 本文介绍了一种应用深度神经网络的深度学习假设空间，并构建了一个再生核Banach空间，研究了正则化学习和最小插值问题，证明了学习模型的解可以表示为线性组合。 |
| [^239] | [Over-The-Air Double-Threshold Deep Learner for Jamming Detection in 5G RF domain](https://arxiv.org/abs/2403.02645) | 本文提出了一种在5G网络中检测干扰者的新型深度学习技术，通过引入双阈值深度学习干扰检测器，专注于SSB的RF领域特征，提高了网络的鲁棒性。 |
| [^240] | [Improving generalisation via anchor multivariate analysis](https://arxiv.org/abs/2403.01865) | 引入因果正则化扩展到锚回归（AR）中，提出了与锚框架相匹配的损失函数确保稳健性，各种多元分析算法均在锚框架内，简单正则化增强了OOD设置中的稳健性，验证了锚正则化的多功能性和对因果推断方法论的推进。 |
| [^241] | [NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models](https://arxiv.org/abs/2403.01845) | NASH是一种将神经架构搜索应用于机器学习硬件的新方法，可以帮助硬件设计实现高吞吐量、低延迟和优越的准确性表现。 |
| [^242] | [PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning](https://arxiv.org/abs/2403.00929) | PRIME是一个基于行为原语设计的框架，通过将任务分解为原语序列并学习高级控制策略，显著提高了多阶段操作任务的性能表现。 |
| [^243] | [Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs](https://arxiv.org/abs/2403.00858) | 通过提出的框架，我们训练了一种用于Llama 2 Chat 7B或更大模型的草案模型，实现了加速推理，仅占原始大小的1.64％。 |
| [^244] | [Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds](https://arxiv.org/abs/2403.00715) | 通过引入竞争分析框架，我们提出了调整FTRL学习率的更新规则，使其在常数因子内达到最佳竞争比，并且展示了当惩罚项具有近似单调性时的竞争比特性。 |
| [^245] | [Parameter-Efficient Tuning of Large Convolutional Models](https://arxiv.org/abs/2403.00269) | 通过引入滤波器子空间和滤波器原子的概念，本研究提出了一种在微调大型卷积模型时仅调整少量参数来提取任务特定表示的方法。 |
| [^246] | [Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games](https://arxiv.org/abs/2402.18781) | 提出了一种具有假设在线学习（COL）的学习方案，针对通用AISG，结构化为一个先验预测者-演员-评论家（FAC）架构，利用一级信念和对手策略的主观预测，通过在线展开更新策略，并通过贝叶斯学习校准假设。 |
| [^247] | [ICE-SEARCH: A Language Model-Driven Feature Selection Approach](https://arxiv.org/abs/2402.18609) | ICE-SEARCH是首个将语言模型与进化算法相结合用于特征选择任务的方法，在医学预测分析应用中取得了State-of-the-Art(SOTA)表现。 |
| [^248] | [MMSR: Symbolic Regression is a Multimodal Task](https://arxiv.org/abs/2402.18603) | 符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。 |
| [^249] | [MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization](https://arxiv.org/abs/2402.16898) | 引入了MIM-Reasoner，结合强化学习和概率图模型，有效地捕捉了给定多重网络内部和层间的复杂传播过程，从而解决了MIM中最具挑战性的问题。 |
| [^250] | [A Survey on Data Selection for Language Models](https://arxiv.org/abs/2402.16827) | 大型语言模型成功的关键在于使用大规模的文本数据集进行无监督预训练，但如何优化选择数据以降低碳足迹和财务成本仍是一个挑战。 |
| [^251] | [A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction](https://arxiv.org/abs/2402.16278) | 提出了一种自匹配训练方法，通过两种本体嵌入模型捕获全局和局部信息，提高了概念子类预测的稳健性 |
| [^252] | [Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks](https://arxiv.org/abs/2402.14515) | 量子神经网络研究了频谱的极大性质，证明了在一类模型中存在极大结果，以及在一些条件下存在保持频谱的光谱不变性，解释了文献中观察到的结果对称性。 |
| [^253] | [Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms](https://arxiv.org/abs/2402.12263) | 提出了面向门控循环单元的定制混合精度低于8位量化方案，使用遗传算法优化模型尺寸和准确性，在四个不同顺序任务上展示混合精度解决方案优于同质精度解决方案，实现了模型尺寸缩减25%至55%同时保持准确性。 |
| [^254] | [Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before](https://arxiv.org/abs/2402.11816) | 开发了一种多阶对比学习（MCL）框架，以解决对比学习中的特征抑制问题，并确保模型学习全面的表示。 |
| [^255] | [Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models](https://arxiv.org/abs/2402.11140) | 本文提出了一种名为Boosting of Thoughts（BoT）的自动提示框架，通过迭代地探索和自我评估多个思维树，获得一系列试错推理经验，作为解决复杂问题的新形式的提示。 |
| [^256] | [Model Editing by Pure Fine-Tuning](https://arxiv.org/abs/2402.11078) | 纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。 |
| [^257] | [Unlink to Unlearn: Simplifying Edge Unlearning in GNNs](https://arxiv.org/abs/2402.10695) | 研究揭示了GNN中边解除过程的关键问题，即过度遗忘现象，提出了解决方法来解决损失函数引起的问题。 |
| [^258] | [Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal Neural Topic Model](https://arxiv.org/abs/2402.10045) | 这项研究针对短视频对观众心理健康的抑郁影响问题，开发了一种基于医学知识的多模态神经主题模型，以预测其影响并采取相应的干预措施。 |
| [^259] | [Sparse and Faithful Explanations Without Sparse Models](https://arxiv.org/abs/2402.09702) | 引入了稀疏解释值(SEV)，用于衡量机器学习模型的决策稀疏性。即使模型不是稀疏的，许多机器学习模型在SEV的衡量下仍具有低决策稀疏性。 |
| [^260] | [Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram](https://arxiv.org/abs/2402.09450) | 本研究提出了一种叫做ST-MEM的模型，通过重构遮蔽的心电图数据来学习时空特征，该模型在心律失常分类任务中优于其他自监督学习方法。 |
| [^261] | [Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models](https://arxiv.org/abs/2402.05210) | 这篇论文提出了一种采用分割引导扩散模型的解剖可控医学图像生成方法，通过随机掩模消融训练算法实现对解剖约束的条件化，同时提高了网络对解剖真实性的学习能力。 |
| [^262] | [Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems](https://arxiv.org/abs/2402.05067) | 本文提出了利用物理信息神经网络进行多尺度建模的方法，通过解耦大尺度和小尺度动力学，并在正交基函数空间中近似小尺度系统。实验结果表明该方法在处理液体动力学问题以及更复杂的情况下具有较高的有效性和适用性。 |
| [^263] | [Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning](https://arxiv.org/abs/2402.04852) | 本研究提出了aLLM4TS框架，将LLMs应用于时间序列表示学习。通过自监督的多块预测任务，捕捉时间动态特征，并通过特定时间序列上的微调进行进一步优化。 |
| [^264] | [Group Distributionally Robust Dataset Distillation with Risk Minimization](https://arxiv.org/abs/2402.04676) | 这项研究关注数据集蒸馏与其泛化能力的关系，尤其是在面对不常见的子组的样本时，如何确保模型在合成数据集上的训练可以表现良好。 |
| [^265] | [Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2402.04613) | 本文研究了在再生核希尔伯特空间中使用Moreau包络来对测度f-差异进行正则化的方法，并利用该方法分析了Wasserstein梯度流。 |
| [^266] | [Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning](https://arxiv.org/abs/2402.03917) | 这篇论文解决了冷启动场景的无示例增量学习的问题，提出了一种弹性特征整合的方法，通过规范特征漂移并利用原型来减少任务新鲜度偏差。 |
| [^267] | [Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels](https://arxiv.org/abs/2402.03838) | 本研究提出了一种带有切片Wasserstein Weisfeiler-Lehman图核的高斯过程回归方法，在处理大规模稀疏图形数据集时具有正定性和显著的复杂度降低。 |
| [^268] | [Arrows of Time for Large Language Models](https://arxiv.org/abs/2401.17505) | 这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。 |
| [^269] | [Strategic Usage in a Multi-Learner Setting](https://arxiv.org/abs/2401.16422) | 在多学习者环境中，本研究分析了用户在多个可用服务中进行选择的战略行为对系统行为的影响。 |
| [^270] | [SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2401.11791) | SemPLeS框架利用语义提示学习解决弱监督语义分割中的问题，通过学习有效提示来增强分割区域与目标对象类别之间的语义对齐。 |
| [^271] | [RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair](https://arxiv.org/abs/2312.15698) | 高效表示和微调适配器相结合的新型程序修复方法RepairLLaMA可为语言模型修复错误产生高效的适配器。 |
| [^272] | [Robustness, Efficiency, or Privacy: Pick Two in Machine Learning](https://arxiv.org/abs/2312.14712) | 该论文研究了在分布式机器学习架构中实现隐私和鲁棒性的成本，指出整合这两个目标会牺牲计算效率。 |
| [^273] | [Optimizing Heat Alert Issuance with Reinforcement Learning](https://arxiv.org/abs/2312.14196) | 本研究利用强化学习优化热预警系统，通过引入新颖强化学习环境和综合数据集，解决了气候和健康环境中的低信号效应和空间异质性。 |
| [^274] | [Topology Learning for Heterogeneous Decentralized Federated Learning over Unreliable D2D Networks](https://arxiv.org/abs/2312.13611) | 通过拓扑学习方法和定义不可靠链接感知邻域差异的新领域数量，这篇论文提出了一种解决分散式联邦学习中数据分布异构性和D2D网络中通信中断导致的挑战的方法。 |
| [^275] | [Active learning with biased non-response to label requests](https://arxiv.org/abs/2312.08150) | 偏倧的非响应对主动学习模型性能有害，我们提出了一种成本-based 修正策略来减轻其影响，并通过实验证明其在许多情况下有效。 |
| [^276] | [Learning Unknown Intervention Targets in Structural Causal Models from Heterogeneous Data](https://arxiv.org/abs/2312.06091) | 在结构因果模型中，通过两阶段方法学习未知干预目标的外生噪声，并将其与相应的内生变量匹配，有效地识别干预目标。 |
| [^277] | [Breaking the Entanglement of Homophily and Heterophily in Semi-supervised Node Classification](https://arxiv.org/abs/2312.04111) | 开发了一个可以在同质性和异质性下保证性能的强大GNN模型 |
| [^278] | [Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation](https://arxiv.org/abs/2311.18207) | 该研究提出了一种新的指标 SharpeRatio@k，用于评估离策略评估的风险-收益权衡，能够有效区分不同风险估计器并准确识别最高效的估计器。 |
| [^279] | [SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation](https://arxiv.org/abs/2311.18206) | SCOPE-RL是一个Python库，兼顾离线强化学习和离策略评估，通过整合策略学习和评估实现了更灵活、完整的实现方式，并通过OPE模块提供了多种OPE估计器和稳健的OPE协议，使得OPE更深入和可靠。 |
| [^280] | [A transductive few-shot learning approach for classification of digital histopathological slides from liver cancer](https://arxiv.org/abs/2311.17740) | 提出了一种用于肝癌数字组织病理切片分类的传导式小样本学习方法，通过滑动窗口技术和优化策略，在解决标记数据有限可用性的同时取得了一致准确分类的实际优势。 |
| [^281] | [Opening the Black Box: Towards inherently interpretable energy data imputation models using building physics insight](https://arxiv.org/abs/2311.16632) | 提出了利用建筑物理学洞察实现固有可解释的能源数据插补模型PI-DAE，在损失函数中引入物理启发软约束，从而实现更可解释的预测。 |
| [^282] | [TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models](https://arxiv.org/abs/2311.16503) | TFMQ-DM提出了一种称为Temporal Feature Maintenance Quantization (TFMQ)的方法，针对扩散模型中的时间特征进行量化，解决了传统模型中存在的优化问题，提高了压缩效率。 |
| [^283] | [FRAC-Q-Learning: A Reinforcement Learning with Boredom Avoidance Processes for Social Robots](https://arxiv.org/abs/2311.15327) | FRAC-Q-Learning是一种专为社交机器人设计，能避免用户厌烦的强化学习方法，比传统算法在兴趣和厌烦程度上表现更好，有助于开发不会让用户感到无聊的社交机器人。 |
| [^284] | [Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation](https://arxiv.org/abs/2311.15100) | 在神经蒙日映射中引入不平衡性可改进未配对领域转换任务的效率和鲁棒性 |
| [^285] | [ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets](https://arxiv.org/abs/2311.12831) | ECNR是一种针对时变数据的高效压缩神经表示方法，通过使用多尺度结构和多个小型MLP，以及深度压缩策略，可以显著加速训练和推理过程。 |
| [^286] | [BEND: Benchmarking DNA Language Models on biologically meaningful tasks](https://arxiv.org/abs/2311.12570) | BEND是一个针对DNA语言模型的基准测试，包含一系列在人类基因组上定义的现实且具有生物意义的下游任务。 |
| [^287] | [EPIM: Efficient Processing-In-Memory Accelerators based on Epitome](https://arxiv.org/abs/2311.07620) | 本论文引入了Epitome，一种轻量级神经算子，为PIM加速器设计了内存高效的CNN算子（EPIM）。 |
| [^288] | [Quantum Neural Networks for Power Flow Analysis](https://arxiv.org/abs/2311.06293) | 混合量子-经典神经网络在电力流分析中表现优越，有望改进深度学习在量子计算时代的应用。 |
| [^289] | [Fair Supervised Learning with A Simple Random Sampler of Sensitive Attributes](https://arxiv.org/abs/2311.05866) | 提出了一种公平受监督学习的方法，利用简单随机采样器处理敏感属性，可以更广泛地适用于实践中，并构建了一个计算效率高的群体级别处理公平感知的训练框架。 |
| [^290] | [LRM: Large Reconstruction Model for Single Image to 3D](https://arxiv.org/abs/2311.04400) | 首次提出了LRM，采用大规模训练数据和高容量模型，可在短时间内从单个图像中预测高质量3D重建结果 |
| [^291] | [Weight-Sharing Regularization](https://arxiv.org/abs/2311.03096) | 提出了一种权重共享正则化方法，通过引入对神经网络权重的惩罚，设计并实现了一个新型并行算法，使网络能够学习卷积样式的滤波器 |
| [^292] | [Fast Minimization of Expected Logarithmic Loss via Stochastic Dual Averaging](https://arxiv.org/abs/2311.02557) | 提出了一种名为$B$-sample stochastic dual averaging with the logarithmic barrier的随机一阶算法，用于快速解决泊松逆问题，算法在$\smash{\tilde{O}}(d^2/\varepsilon^2)$时间内获得$\varepsilon$-最优解，与当前技术水平相匹配。 |
| [^293] | [From Posterior Sampling to Meaningful Diversity in Image Restoration](https://arxiv.org/abs/2310.16047) | 图像恢复问题通常具有无限种合理的解决方案，传统的后验采样生成的多样输出受到尾部效应限制，本文提出了一种更具意义的多样性策略。 |
| [^294] | [Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World](https://arxiv.org/abs/2310.10207) | Bongard-OpenWorld基准旨在评估机器视觉中对真实世界中的自由形式视觉概念进行少样本推理，并且提出了开放世界自由形式概念和真实世界图像两项新挑战。 |
| [^295] | [Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates](https://arxiv.org/abs/2310.09804) | 本文提出了一种新的带压缩的拜占庭鲁棒方法 Byz-DASHA-PAGE，证明了其在非凸和Polyak-Lojasiewicz平滑优化问题上具有更好的收敛速率，同时在异构情况下具有更小的邻域大小，以及在过参数化情况下能容忍更多的拜占庭工作者。 |
| [^296] | [Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491) | Prometheus是一个开源的LLM，通过使用自定义评分标准和适当的参考材料，可以在与GPT-4相媲美的评估能力上进行评估。 |
| [^297] | [AutoVP: An Automated Visual Prompting Framework and Benchmark](https://arxiv.org/abs/2310.08381) | AutoVP提出了一个自动化的视觉提示框架，同时提供12个下游图像分类任务作为基准，实验结果显示AutoVP在准确性方面明显优于当前已知的最佳VP方法。 |
| [^298] | [iTransformer: Inverted Transformers Are Effective for Time Series Forecasting](https://arxiv.org/abs/2310.06625) | iTransformer通过重新利用Transformer架构，在时间序列预测中简单应用注意力和馈送，提高了性能并克服了其他模型在处理具有更大回溯窗口的系列时面临的挑战 |
| [^299] | [Distributional Reinforcement Learning with Online Risk-awareness Adaption](https://arxiv.org/abs/2310.05179) | 本论文提出了一个新的分布式强化学习框架，可以通过在线风险适应性调整来量化不确定性，并动态选择认知风险水平。 |
| [^300] | [Visual Political Communication in a Polarized Society: A Longitudinal Study of Brazilian Presidential Elections on Instagram](https://arxiv.org/abs/2310.00349) | 该研究通过纵向研究巴西总统候选人在Instagram上发布的帖子，揭示了视觉政治传播中固定的模式，包括庆祝和积极调性图像的普遍使用以及候选人与选民紧密联系的个性化展示。 |
| [^301] | [A Unifying Variational Framework for Gaussian Process Motion Planning](https://arxiv.org/abs/2309.00854) | 提出了一个基于变分高斯过程的机器人运动规划框架，统一和泛化了各种基于概率推断的运动规划算法，并将其与基于优化的规划器连接起来 |
| [^302] | [Task-Aware Machine Unlearning and Its Application in Load Forecasting](https://arxiv.org/abs/2308.14412) | 该论文介绍了一种面向任务的机器遗忘概念，在负荷预测中应用该概念可以平衡遗忘完整性和模型性能之间的关系。 |
| [^303] | [Defending Against Malicious Behaviors in Federated Learning with Blockchain](https://arxiv.org/abs/2307.00543) | 该研究提出了一个基于区块链和分布式分类账技术的安全和可靠的联邦学习系统，包括点对点投票机制和奖励和惩罚机制，以检测和阻止恶意行为，证明了该框架对抗恶意客户的有效性。 |
| [^304] | [Collaborative and Distributed Bayesian Optimization via Consensus: Showcasing the Power of Collaboration for Optimal Design](https://arxiv.org/abs/2306.14348) | 通过引入共识概念实现了合作和分布式贝叶斯优化，充分展示了合作对于最优设计的重要作用 |
| [^305] | [Deep Classifier Mimicry without Data Access](https://arxiv.org/abs/2306.02090) | 提出了一种无需访问原始数据的模型-无关知识蒸馏过程CAKE，可以模拟深度分类器，通过生成噪声合成样本对比地扩散到模型的决策边界。 |
| [^306] | [Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness](https://arxiv.org/abs/2305.19101) | 稳健计算机视觉模型的梯度通常与人类感知对齐，通过离散度稳健性解释这一现象。 |
| [^307] | [DistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution](https://arxiv.org/abs/2305.17000) | DistriBlock提出了一种能够识别对抗性音频样本的有效检测策略，通过利用输出分布的特征，包括中位数、最大值和最小值、熵以及与后续时间步骤的分布之间的散度，应用二元分类器进行预测。这项研究证明了DistriBlock在识别对抗性音频样本方面的有效性。 |
| [^308] | [Error-mitigated Quantum Approximate Optimization via Learning-based Adaptive Optimization](https://arxiv.org/abs/2303.14877) | 我们设计了一种双自适应区域贝叶斯优化算法，用于提升量子近似优化算法的性能。实验结果表明，该算法在性能上远远超过了传统的方法。 |
| [^309] | [Brain Effective Connectome based on fMRI and DTI Data: Bayesian Causal Learning and Assessment](https://arxiv.org/abs/2302.05451) | 提出了两种基于贝叶斯因果发现框架的方法，通过利用DTI数据作为先验知识，显著提高了在仅基于fMRI数据发现大脑有效连接组的准确性和可靠性 |
| [^310] | [Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding](https://arxiv.org/abs/2301.03765) | 本论文提出通过交叉模型比较损失的方法来增强语言理解模型中神经元的效用，实现减少冗余参数和抑制输入噪声的目标。 |
| [^311] | [oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep Learning Compilation](https://arxiv.org/abs/2301.01333) | 提出了oneDNN图编译器，采用了编译优化和混合方法，旨在实现高性能张量编译，生成专家级性能代码并在DNN计算图范围内应用编译优化 |
| [^312] | [Visual CPG-RL: Learning Central Pattern Generators for Visually-Guided Quadruped Locomotion](https://arxiv.org/abs/2212.14400) | 该论文提出了一个框架，通过将外感知传感和中枢模式发生器整合到深度强化学习框架中，学习视觉引导的四足动物运动，探索了耦合振荡器系统对导航鲁棒性的改进、具有记忆功能的策略网络与无记忆策略网络在导航任务中的效果以及动物如何容忍高。 |
| [^313] | [Quantum-Inspired Tensor Neural Networks for Option Pricing](https://arxiv.org/abs/2212.14076) | 引入了Tensor神经网络（TNN）和张量网络初始化器（TNN Init），TNN可以在获得与DNN相同精度的情况下提供显著的参数节省，并展示了训练速度更快的优点。 |
| [^314] | [Text2Model: Text-based Model Induction for Zero-shot Image Classification](https://arxiv.org/abs/2210.15182) | 该论文提出了一种使用文本描述构建与任务无关的分类器的方法，通过生成针对查询分类任务定制的模型来解决零样本图像分类问题。 |
| [^315] | [Machine Learning-Powered Course Allocation](https://arxiv.org/abs/2210.00954) | 引入机器学习驱动的课程分配机制（MLCM），通过机器学习模块减轻学生在报告偏好时的错误，显著提高学生效用，且具有对环境变化的稳健性。 |
| [^316] | [Sample Efficient Learning of Factored Embeddings of Tensor Fields](https://arxiv.org/abs/2209.00372) | 学习了有效的方法来生成张量场的分解嵌入，使得对原始张量集合进行信息查询和后处理更加高效。 |
| [^317] | [Computational Complexity Evaluation of Neural Network Applications in Signal Processing](https://arxiv.org/abs/2206.12191) | 本文提供了对数字信号处理中神经网络层计算复杂度进行评估和比较的系统方法，引入了适用于异构量化的“加法和位移数量(NABS)”指标。 |
| [^318] | [Is your model predicting the past?](https://arxiv.org/abs/2206.11673) | 提出了区分机器学习模型是预测个体未来还是重复过去模式的方法，通过向后基线测试展示模型是否回溯过去，并在长期面板调研任务中验证了该框架的有效性。 |
| [^319] | [OpenXAI: Towards a Transparent Evaluation of Model Explanations](https://arxiv.org/abs/2206.11104) | OpenXAI 是一个开源框架，旨在评估和基准测试后续解释方法，提供了灵活的数据生成器、多种数据集和评估指标，用户可轻松扩展和比较不同解释方法。 |
| [^320] | [NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search](https://arxiv.org/abs/2206.09166) | NAS-Bench-Graph提出了一个定制的基准测试，支持对GraphNAS进行统一、可复现和高效的评估，解决了实验设置不一致和计算需求大的挑战。 |
| [^321] | [Slowly Changing Adversarial Bandit Algorithms are Efficient for Discounted MDPs](https://arxiv.org/abs/2205.09056) | 研究证明，在一些假设下，任何缓慢变化的对抗式老虎机算法在折扣马尔可夫决策过程中能够达到最优期望遗憾。 |
| [^322] | [Uncertainty-aware Pseudo-label Selection for Positive-Unlabeled Learning](https://arxiv.org/abs/2201.13192) | 通过不确定性感知的伪标签选择过程，本研究提出了一种解决正负样本学习中不平衡数据集和模型校准问题的方法，实验结果表明在高度不平衡的情况下能显著提高预测性能。 |
| [^323] | [Deep Reinforcement Learning with Spiking Q-learning](https://arxiv.org/abs/2201.09754) | 基于脉冲Q学习的深度强化学习方法DSQN利用非脉冲神经元的膜电压作为Q值表示，实现了能源高效的控制任务。 |
| [^324] | [Nested Nonparametric Instrumental Variable Regression: Long Term, Mediated, and Time Varying Treatment Effects](https://arxiv.org/abs/2112.14249) | 该论文提出了嵌套非参数工具变量回归的对抗估计器，并提供了对因果参数进行有效推断的充分条件，具有限制病态性复合技术、多种适应模型和扩展到因果函数等特征。 |
| [^325] | [PMFL: Partial Meta-Federated Learning for heterogeneous tasks and its applications on real-world medical records](https://arxiv.org/abs/2112.05321) | 提出了PMFL算法，结合了联邦学习和元学习的思想，针对异构任务的数据分布，解决了传统联邦学习在医疗记录等领域应用受限的问题 |
| [^326] | [Generalizing Graph Neural Networks on Out-Of-Distribution Graphs](https://arxiv.org/abs/2111.10657) | 提出了一个名为StableGNN的通用因果表示框架，通过提取高级图表示并利用因果推断的区分能力，帮助模型在分布外图上实现稳定的泛化能力。 |
| [^327] | [Pareto-wise Ranking Classifier for Multi-objective Evolutionary Neural Architecture Search](https://arxiv.org/abs/2109.07582) | 本研究的主要贡献是通过将复杂的多目标NAS任务转化为简单的Pareto占优分类任务，提出了一种基于分类的Pareto进化方法，从而简化了NAS的搜索过程 |
| [^328] | [A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with an Arbitrary Opponent](https://arxiv.org/abs/2109.03396) | 本文提出了一种针对无限时间跨度的零和随机博弈的在线学习算法，该算法在具有平均奖励准则的情况下实现了贝叶斯遗憾界$O(HS\sqrt{AT})$。 |
| [^329] | [Upper Counterfactual Confidence Bounds: a New Optimism Principle for Contextual Bandits](https://arxiv.org/abs/2007.07876) | 本文提出的“上限反事实置信区间”（UCCB）是针对一般上下文赌博设计乐观算法的新原则，通过在策略空间中构建置信区间，而非像UCB那样在行动空间中，这使得算法在处理一般函数类和大上下文空间时均具有优越性。 |
| [^330] | [ENCORE: Ensemble Learning using Convolution Neural Machine Translation for Automatic Program Repair](https://arxiv.org/abs/1906.08691) | ENCORE提出了一种新的自动程序修复技术，使用卷积神经机器翻译的集成学习方法可以有效修复多种编程语言中的错误，超越了之前使用的LSTM方法，成功修复了42个错误，其中包括16个先前未被修复的错误。 |
| [^331] | [NeurAll: Towards a Unified Visual Perception Model for Automated Driving](https://arxiv.org/abs/1902.03589) | 本文提出了一种联合多任务网络设计，以实现在自动驾驶中的视觉感知任务中共享计算资源，从而提高计算效率并提供更好的泛化能力。 |
| [^332] | [A Discriminative Latent-Variable Model for Bilingual Lexicon Induction](https://arxiv.org/abs/1808.09334) | 引入判别式潜变量模型，结合先前研究的词典先验和表示法，提出了用于双语词典归纳的新方法，并通过实验证据展示先验可以改善诱导的双语词典。 |
| [^333] | [The CTU Prague Relational Learning Repository](https://arxiv.org/abs/1511.03086) | 支持机器学习研究使用多关系数据的布拉格捷克技术大学关系学习资源库，包含大量SQL数据库，并由getML提供支持。 |
| [^334] | [Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population.](http://arxiv.org/abs/2401.14512) | 本文提出了一种基于优化的方法，Rashomon Set of Optimal Trees (ROOT)，用于识别和描述随机对照试验中的少数人群。该方法通过最小化目标平均处理效应估计的方差来优化目标子群体分布，从而提供更精确和可解释的处理效应估计。与其他方法相比，该方法具有更高的精度和可解释性，通过合成数据实验进行了验证。 |
| [^335] | [True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning.](http://arxiv.org/abs/2401.14151) | 本研究通过使用大型语言模型（LLMs）作为决策智能体，通过强化学习与具身环境高效互动来解决LLMs与环境之间知识不对齐的问题。通过查询LLMs的联合概率，形成行为策略，并通过两种归一化方法和四个提示设计原则提高策略的稳定性和鲁棒性。最后，通过设计参数高效的训练架构提高学习效率。 |
| [^336] | [Compositional Generative Inverse Design.](http://arxiv.org/abs/2401.13171) | 逆向设计起到优化底层目标函数的作用，最近的研究利用了学习的动力学模型进行优化。通过优化扩散模型捕获的学习能量函数，可以避免对抗示例，并显著提高设计性能。这一设计系统是组合性的，使得可以设计具有每个指定组件的系统。 |
| [^337] | [Assessment of Sports Concussion in Female Athletes: A Role for Neuroinformatics?.](http://arxiv.org/abs/2401.13045) | 该论文提出了通过神经信息学和机器学习来评估女性运动员脑震荡的方法。相比传统的临床方法，在女性运动员中诊断脑震荡存在一些局限性，而这些新技术可以通过数据分析找出与性别相关的生物机制，从而填补这一差距。 |
| [^338] | [Towards Scalable and Robust Model Versioning.](http://arxiv.org/abs/2401.09574) | 本文探讨了在不获取新的训练数据或更改模型架构的情况下，生成具有不同攻击属性的多个模型版本的可行性，以保护模型所有者免受恶意入侵带来的损失。 |
| [^339] | [Learning from Sparse Offline Datasets via Conservative Density Estimation.](http://arxiv.org/abs/2401.08819) | 本文提出了一种名为保守密度估计（CDE）的训练算法，通过明确约束状态-行为占据稳态分布来解决离线强化学习中的外推错误问题。在稀疏奖励或不足数据的任务中，CDE显示出明显优于基准方法的性能。 |
| [^340] | [Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression.](http://arxiv.org/abs/2401.05653) | 本文研究了使用Shapley值回归对渠道合作伙伴层面的营销绩效进行量化，并通过与营销组合建模进行比较，证明了Shapley值回归的实用性。同时提出了一种简单的方法来计算调整系数。 |
| [^341] | [On the Expressive Power of Graph Neural Networks.](http://arxiv.org/abs/2401.01626) | 研究人员对图神经网络的表达能力和设计架构进行了大量工作，以提高其在各领域任务中的性能。主要方法包括研究GNN的通用逼近性质和其在区分不同图之间的能力程度。 |
| [^342] | [Locally Optimal Best Arm Identification with a Fixed Budget.](http://arxiv.org/abs/2310.19788) | 该研究解决了识别具有最高预期效果的治疗方案的问题，并提出了具有固定预算的局部最优算法来降低错误识别的概率。 |
| [^343] | [Improved Regret Bounds of (Multinomial) Logistic Bandits via Regret-to-Confidence-Set Conversion.](http://arxiv.org/abs/2310.18554) | 本论文通过遗憾到置信集转换方法改进了逻辑回归赌博机的遗憾界限，提出了一个基于在线学习算法的凸置信集，并应用于具有新的鞅集中步骤的遗憾分析。 |
| [^344] | [Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs.](http://arxiv.org/abs/2310.18152) | 本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。 |
| [^345] | [Boosting Data Analytics With Synthetic Volume Expansion.](http://arxiv.org/abs/2310.17848) | 本文介绍了一种利用合成数据生成框架来提升数据分析的方法，在此方法中，使用先进模型生成高逼真度的合成数据，并采用统计方法进行分析。研究发现，在合成数据上的统计方法错误随着合成数据的增加而减少，但最终可能会增加或停滞。 |
| [^346] | [MaxEnt Loss: Constrained Maximum Entropy for Calibration under Out-of-Distribution Shift.](http://arxiv.org/abs/2310.17159) | 本论文提出了一种新的损失函数，用于解决超出分布转换的校准问题。该方法基于最大熵原理，在训练过程中引入统计约束，以提供更好的模型校准效果，同时不牺牲准确性。实验证明该方法在合成和真实世界的基准上实现了最先进的校准效果。 |
| [^347] | [Large-Scale Gaussian Processes via Alternating Projection.](http://arxiv.org/abs/2310.17137) | 本论文提出了一种通过交替投影的迭代方法来解决高斯过程在大规模数据集上的训练问题，并证明了该方法具有线性收敛性。 |
| [^348] | [Detecting Pretraining Data from Large Language Models.](http://arxiv.org/abs/2310.16789) | 这项研究探讨了如何检测大型语言模型的预训练数据，提出了一个动态基准和一种新的检测方法，以解决数据隐私和不透明性的问题。 |
| [^349] | [DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data.](http://arxiv.org/abs/2310.13349) | DeepFDR是一种基于深度学习的虚警控制方法，通过利用无监督的图像分割技术解决神经影像数据中的多重检验问题，并在实验证明其相对于现有方法具有卓越的性能。 |
| [^350] | [CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation.](http://arxiv.org/abs/2310.13165) | CycleNet是一种将循环一致性引入扩散模型的新方法，用于规范图像操作，具有优越的翻译一致性和质量，并且可以生成高质量的跨领域分布图像。 |
| [^351] | [Towards Robust Offline Reinforcement Learning under Diverse Data Corruption.](http://arxiv.org/abs/2310.12955) | 本文研究了在多种数据损坏情况下，离线强化学习算法的性能。研究发现，隐式Q-learning（IQL）在各种离线强化学习算法中展现出了较强的鲁棒性能，其采用的监督策略学习方案为关键。然而，在动力学损坏下，IQL仍然存在Q函数的重尾目标问题。 |
| [^352] | [Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness.](http://arxiv.org/abs/2310.12713) | 本文提出了一个基于代理的对抗性防御框架，通过引入目标模型的历史状态作为代理来增强模型对各种对抗攻击的鲁棒性。 |
| [^353] | [In-Context Pretraining: Language Modeling Beyond Document Boundaries.](http://arxiv.org/abs/2310.10638) | 本论文提出了一种超越文档边界的上下文预训练方法，通过在相关文档序列上训练语言模型，鼓励模型进行跨文档的阅读和推理。该方法通过改变文档顺序并应用现有的预训练管道来实现。 |
| [^354] | [SiamAF: Learning Shared Information from ECG and PPG Signals for Robust Atrial Fibrillation Detection.](http://arxiv.org/abs/2310.09203) | 提出了一种名为SiamAF的新方法，利用心电图和光电脉搏图信号的共享信息，通过Siamese网络和联合学习实现强健的心房颤动（AF）检测。 |
| [^355] | [METRA: Scalable Unsupervised RL with Metric-Aware Abstraction.](http://arxiv.org/abs/2310.08887) | METRA提出了一种新的无监督强化学习目标，旨在使其在复杂的高维环境中可扩展。这个目标解决了纯探索方法在大状态空间环境中的困难以及互信息技能学习方法中缺乏激励而无法探索环境的问题。 |
| [^356] | [GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning.](http://arxiv.org/abs/2310.07365) | 在图领域迁移学习中，GraphControl通过添加条件控制实现了对通用图预训练模型的有效迁移，克服了不同图域间的属性语义差异问题。 |
| [^357] | [MuseChat: A Conversational Music Recommendation System for Videos.](http://arxiv.org/abs/2310.06282) | MuseChat是一种创新的对话式音乐推荐系统，通过模拟用户和推荐系统之间的对话交互，利用预训练的音乐标签和艺术家信息，为用户提供定制的音乐推荐，使用户可以个性化选择他们喜欢的音乐。 |
| [^358] | [Parameter Efficient Multi-task Model Fusion with Partial Linearization.](http://arxiv.org/abs/2310.04742) | 本文提出了一种参数高效的多任务模型融合方法，通过部分线性化适配器模块，并应用任务算法，实现了对大型预训练模型在多个下游任务上的高效微调，从而提高了多任务模型融合的效果和效率。 |
| [^359] | [Training-free Linear Image Inversion via Flows.](http://arxiv.org/abs/2310.04432) | 提出了一种无需训练的线性图像反演方法，通过使用预训练的流模型，在减少手动调整的情况下解决逆问题。 |
| [^360] | [Sampling via Gradient Flows in the Space of Probability Measures.](http://arxiv.org/abs/2310.03597) | 通过梯度流抽样方法的研究方向在计算科学和工程中具有重要意义。本文通过研究概率测度空间中的梯度流的设计组成部分，提出了三个贡献：Kullback-Leibler散度作为能量泛函的独特属性、度量的选择与不变性的关系。 |
| [^361] | [A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability.](http://arxiv.org/abs/2310.02807) | 本文提出了G2MILP，这是第一个用于MILP实例的深度生成框架，它可以生成新颖而逼真的MILP实例。 |
| [^362] | [Reward Model Ensembles Help Mitigate Overoptimization.](http://arxiv.org/abs/2310.02743) | 本研究通过探究奖励模型集成和保守优化目标的效果，对减轻奖励模型过度优化进行了系统研究。 |
| [^363] | [Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization.](http://arxiv.org/abs/2310.02679) | 这项工作介绍了一种名为扩散生成流采样器（DGFS）的采样框架，通过将学习过程分解为短的部分轨迹段，实现从难以处理的高维密度函数中进行采样。它通过利用中间的学习信号和非策略探索能力来改善学习信号的分配问题。 |
| [^364] | [Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition.](http://arxiv.org/abs/2310.02011) | 本文提出了一种用于活动识别的分层多结构方法，利用残差网络和残差MobileNet对静态和动态活动进行分类，然后通过加权合奏方法进行集成。 |
| [^365] | [Large Language Models as Analogical Reasoners.](http://arxiv.org/abs/2310.01714) | 本研究提出了一种新的提示方法，称为类比提示，用于自动引导大型语言模型的推理过程。通过在上下文中自动生成相关实例或知识，该方法在多种推理任务中表现出优异的性能。 |
| [^366] | [On Representation Complexity of Model-based and Model-free Reinforcement Learning.](http://arxiv.org/abs/2310.01706) | 本研究在电路复杂度的角度探讨了基于模型和无模型强化学习的表示复杂性。理论上证明了某些MDP可以用恒定深度电路表示转移和奖励函数，但最优$Q$-函数的电路复杂度指数级增加。我们的理论揭示了为什么基于模型的算法通常比无模型的算法具有更好的样本复杂性。 |
| [^367] | [Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks.](http://arxiv.org/abs/2309.17002) | 本文研究了深度学习中预训练数据中的标签噪声对下游任务的影响，并通过在合成噪声数据集上的实验证明，在预训练中的轻微噪声可以提高领域内的性能，但会损害领域外的性能。为了减轻噪声的影响，提出了一种轻量级的黑盒调整方法（NMTune）。 |
| [^368] | [Segment Anything Model is a Good Teacher for Local Feature Learning.](http://arxiv.org/abs/2309.16992) | 本文提出了使用Segment Anything Model (SAM)作为教师来指导本地特征学习，通过像素语义关系蒸馏和弱监督对比学习两种技术，实现了在有限数据集上的更高性能表现。 |
| [^369] | [M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning.](http://arxiv.org/abs/2309.16578) | M-OFDFT是一种利用深度学习模型解决分子系统问题的OFDFT方法，通过将非局域性建立在模型中并使用紧凑的密度表示，实现了与Kohn-Sham DFT相近的精确度，并且具有良好的外推能力。 |
| [^370] | [ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers.](http://arxiv.org/abs/2309.16119) | ModuLoRA提出了一种内存高效、能够在消费级GPU上支持3比特LLMs微调的方法，并通过与模块化量化器的集成实现了竞争性能和更少的内存使用。 |
| [^371] | [STARC: A General Framework For Quantifying Differences Between Reward Functions.](http://arxiv.org/abs/2309.15257) | 这篇论文提出了一个通用框架（STARC），用于评估奖励函数之间的差异，填补了奖励学习理论基础的空白。 |
| [^372] | [Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation.](http://arxiv.org/abs/2309.14859) | 本文介绍了LyCORIS，一个开源库，提供了多种稳定扩散模型的微调方法，并提出了一个系统评估的全面框架。 |
| [^373] | [FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler.](http://arxiv.org/abs/2309.14675) | FedCompass是一种创新的半异步联邦学习算法，通过在服务器端使用计算能力感知调度器，解决了异构客户端和数据中跨边界联邦学习的效率和收敛准确性问题。 |
| [^374] | [Doubly Robust Proximal Causal Learning for Continuous Treatments.](http://arxiv.org/abs/2309.12819) | 本文提出了一种基于核函数的双重稳健近端因果学习方法，用于处理连续治疗，并提出了一种高效求解干扰函数的新方法。 |
| [^375] | [Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain.](http://arxiv.org/abs/2309.07080) | 本文介绍了贝叶斯动态有向无环图学习方法（BDyMA）来解决在发现大脑动态效应连接组中的两个主要挑战。该方法通过无约束框架实现更准确的结果和更稀疏的网络结构，使其特别适用于提取动态效应连接组。 |
| [^376] | [Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data.](http://arxiv.org/abs/2309.06679) | 本研究通过实验数据同化改进了Spalart-Allmaras模型，实现了对分离流体的雷诺平均纳维-斯托克斯解的泛化，提高了计算模型的性能。 |
| [^377] | [Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms.](http://arxiv.org/abs/2309.05961) | 本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。 |
| [^378] | [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models.](http://arxiv.org/abs/2309.03883) | DoLa通过对比不同层次的逻辑差异，提高大型语言模型中的真实性和减少幻觉，无需外部知识或微调。 |
| [^379] | [Index-aware learning of circuits.](http://arxiv.org/abs/2309.00958) | 提出了一种索引感知学习的方法，通过对电路进行解剖概念的应用，将差分代数方程组分解为常微分方程和代数方程，以更好地利用现有知识来量化电路设计中可调参数的影响。 |
| [^380] | [MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification.](http://arxiv.org/abs/2308.13139) | MatchXML是一种高效的文本-标签匹配框架，用于极端多标签文本分类。它通过label2vec方法生成语义密集的标签嵌入，并利用这些嵌入构建层次化标签树。通过微调预训练的Transformer模型，MatchXML将多标签文本分类问题转化为文本-标签匹配问题，并提取出密集的文本表示和静态的句子嵌入。 |
| [^381] | [Online Control for Linear Dynamics: A Data-Driven Approach.](http://arxiv.org/abs/2308.08138) | 本文提出了一种数据驱动的方法来解决在线控制线性动力学问题，该方法不需要识别系统模型，而是通过累积扰动来进行决策，证明了算法性能与基于模型的方法相当。 |
| [^382] | [Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?.](http://arxiv.org/abs/2307.14642) | 本文证明了带有控制变量的黑盒变分推断在完美变分族规范下以几何速度收敛，为BBVI提供了收敛性保证，同时提出了对熵梯度估计器的改进，对比了STL估计器，并给出了明确的非渐近复杂度保证。 |
| [^383] | [Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification.](http://arxiv.org/abs/2307.14025) | 本论文提出一种基于拓扑正则化的多实例学习方法，用于罕见贫血疾病的红细胞分类。通过从单个红细胞图像中提取多尺度的拓扑特征来进行模型正则化，以保持数据的特征拓扑属性。实验结果表明，该方法是有效的。 |
| [^384] | [Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items.](http://arxiv.org/abs/2307.13709) | 本论文提出了深度布拉德利-特里评分（DBTR）方法，用于评估不一定存在于数据集中的未知物品的属性。该方法通过将传统的布拉德利-特里模型与神经网络结构无缝结合，成功地学习了这些属性的预期量化。 |
| [^385] | [HIQL: Offline Goal-Conditioned RL with Latent States as Actions.](http://arxiv.org/abs/2307.11949) | 本文提出了一个基于离线数据的目标导向强化学习的分层算法，通过利用目标达成问题的结构，使用一个无动作的价值函数学习了两个策略，从而在学习过程中更有效地利用离线数据。 |
| [^386] | [On the Universality of Linear Recurrences Followed by Nonlinear Projections.](http://arxiv.org/abs/2307.11888) | 本论文展示了一种基于循环线性层和多层感知器的序列模型可以逼近任何规则的非线性序列到序列映射。 |
| [^387] | [Overthinking the Truth: Understanding how Language Models Process False Demonstrations.](http://arxiv.org/abs/2307.09476) | 该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。 |
| [^388] | [Scaling Laws for Imitation Learning in NetHack.](http://arxiv.org/abs/2307.09423) | 本文研究了在NetHack游戏中的模仿学习，发现通过扩大模型和数据规模可以改进模仿学习的效果，并建立了训练计算最优IL代理人的幂律。 |
| [^389] | [NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services.](http://arxiv.org/abs/2307.06148) | NetGPT是一个能够在边缘和云端部署适当的大型语言模型的本地AI网络架构，实现了个性化生成服务，并通过协作云边方法论来优化资源协调和互动效果。 |
| [^390] | [Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators.](http://arxiv.org/abs/2307.05358) | 本文提出了一种带有双调节器的新型联邦半监督学习框架FedDure，解决了数据分布不平衡的问题。通过粗调节器和细调节器对本地模型的更新进行规范，以及学习适应性加权方案，适应不同的数据分布。 |
| [^391] | [Simulation-free Schr\"odinger bridges via score and flow matching.](http://arxiv.org/abs/2307.03672) | [SF]$^2$M是一种无需模拟的方法，用于推断随机动力学。它将连续时间随机生成建模解释为Schr\"odinger桥问题，并通过静态熵正则化最优传输来高效学习。在学习细胞动力学方面表现出更高的准确性和效率。 |
| [^392] | [Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data.](http://arxiv.org/abs/2307.03410) | 这篇论文提出了一个适用于特征分布式数据的可扩展高维多变量线性回归算法，具有通信复杂度不依赖于特征维度和快速收敛性的优势，可应用于大规模数据集和具有多变量响应变量的场景。 |
| [^393] | [DeepOnto: A Python Package for Ontology Engineering with Deep Learning.](http://arxiv.org/abs/2307.03067) | DeepOnto是一个Python包，用于深度学习本体工程。它通过集成深度学习框架和本体API，提供了丰富的工具和算法，支持本体工程任务，如本体对齐和完成。 |
| [^394] | [Group-based Robustness: A General Framework for Customized Robustness in the Real World.](http://arxiv.org/abs/2306.16614) | 本研究提出了一种基于群体的鲁棒性指标，可以更好地评估机器学习模型在现实世界中抵抗攻击的能力，弥补了传统指标的不足。实验证明，该指标能够区分模型对特定威胁的脆弱性。 |
| [^395] | [SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating Reproducible Scenes.](http://arxiv.org/abs/2306.15620) | SCENEREPLICA是一个基于YCB对象的可重复性基准测试，用于评估现实世界中的机器人操纵能力。此基准测试易于重复并允许研究人员比较不同的技术和算法，有助于加快机器人操纵方法的发展。 |
| [^396] | [Input-sensitive dense-sparse primitive compositions for GNN acceleration.](http://arxiv.org/abs/2306.15155) | 本文提出了一种在不同的输入图和GNN嵌入大小上使用代数重组的方法，通过选择最佳组合来提高GNN加速的性能。 |
| [^397] | [Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors.](http://arxiv.org/abs/2306.12041) | 提出了一种基于轻量级遮蔽自编码器的高效异常事件检测模型，通过引入基于运动梯度的令牌加权方法，整合教师解码器和学生解码器以及生成合成异常事件，实现共同重构原始帧和对应的像素级异常映射。在三个基准测试中，实现出色的速度和准确性的权衡。 |
| [^398] | [3D molecule generation by denoising voxel grids.](http://arxiv.org/abs/2306.07473) | VoxMol是一种根据分数的新方法，可以生成3D分子，并通过学习从噪声分子的平滑分布到真实分子的分布的映射。该方法与当前先进技术不同，具有更简单的训练和更快的速度。 |
| [^399] | [On the Expected Size of Conformal Prediction Sets.](http://arxiv.org/abs/2306.07254) | 该论文研究了适应性预测集的期望大小问题，提出了一种理论量化方法以及点估计和高概率区间，并在真实数据集上验证了其实用性。 |
| [^400] | [Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model.](http://arxiv.org/abs/2306.06138) | 该论文提出了一种利用扩散模型提取和恢复潜在动力学对齐的时空结构的方法，以解决现有方法忽略潜在动力学时空结构导致对齐后性能质量较差的问题。 |
| [^401] | [Error Feedback Can Accurately Compress Preconditioners.](http://arxiv.org/abs/2306.06098) | 本论文提出一种错误反馈技术，可以通过在馈入预处理器之前对梯度信息进行压缩（稀疏化或低秩压缩），将预处理器的存储成本压缩多达两个数量级，而不会丢失收敛性。 |
| [^402] | [BeMap: Balanced Message Passing for Fair Graph Neural Network.](http://arxiv.org/abs/2306.04107) | 本文提出了一种公平的消息传递方法，称为BeMap，旨在解决消息传递中的偏差放大问题，通过平衡感知的采样策略来平衡不同人口群体的1-hop邻居的数量。 |
| [^403] | [Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization.](http://arxiv.org/abs/2306.02879) | 本文通过研究神经元激活状态，提出了神经元激活覆盖度（NAC）作为衡量神经元行为的指标。利用NAC可以有效区分域内和离域输入，简化离域检测问题，并且NAC与模型的泛化能力间存在正相关关系。 |
| [^404] | [Explainability in Simplicial Map Neural Networks.](http://arxiv.org/abs/2306.00010) | 本文提出了简单形式映射神经网络（SMNN）的训练过程和替代凸多面体的方法，并且首次引入了 SMNN 的可解释性能力。 |
| [^405] | [Prediction Error-based Classification for Class-Incremental Learning.](http://arxiv.org/abs/2305.18806) | 本论文提出了一种新的增量学习分类方法——基于预测误差的分类方法（PEC）。对PEC的评估表明，在各种基准测试中，PEC可以与最先进的增量学习方法相竞争，并具有许多实际优势，例如样本效率高、易于调整。 |
| [^406] | [HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance.](http://arxiv.org/abs/2305.18766) | 该论文提出了一种高保真度的文本到3D图像合成方法，并引入了先进的扩散引导策略。通过对NeRF渲染图像进行辅助深度监督和规范化密度场来提高3D几何表示。实验证明该方法优于以前的工作，产生了先进的照片真实感和改进的多视角一致性。 |
| [^407] | [Contextual Bandits with Budgeted Information Reveal.](http://arxiv.org/abs/2305.18511) | 本文介绍了一种针对医疗领域“亲治疗”操作的限制，且考虑到了操作预算的具有信息预算的情境赌博机算法，这种算法将在线原始-对偶算法和情境赌博机学习算法有机地结合在一起，取得了很好的效果。 |
| [^408] | [Sharpened Lazy Incremental Quasi-Newton Method.](http://arxiv.org/abs/2305.17283) | 本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。 |
| [^409] | [Large Language Models as Tool Makers.](http://arxiv.org/abs/2305.17126) | 本文提出了一个闭环框架，即LLMs作为工具制造者（LATM），使LLMs能够自主地创建用于解决问题的工具，而不需要依赖于现有的外部工具。 |
| [^410] | [Vector-Valued Variation Spaces and Width Bounds for DNNs: Insights on Weight Decay Regularization.](http://arxiv.org/abs/2305.16534) | 该论文提供了关于通过加权衰减训练的多输出ReLU神经网络的函数类型和相应的解决方案的新见解。 |
| [^411] | [Ensemble Synthetic EHR Generation for Increasing Subpopulation Model's Performance.](http://arxiv.org/abs/2305.16363) | 本文提出了一种利用生成模型集成合成数据的方法，以提高训练机器学习模型在代表不足的亚群体的性能。 |
| [^412] | [Empirical Optimal Transport between Conditional Distributions.](http://arxiv.org/abs/2305.15901) | 本文考虑在一个公共变量的条件下，相应分布之间的最优输运问题。通过采用基于 MMD 的核正则化器，克服了条件变量是连续的和两个分布中该变量的边缘是不同的挑战。 |
| [^413] | [The Behavior and Convergence of Local Bayesian Optimization.](http://arxiv.org/abs/2305.15572) | 本文研究了贝叶斯本地优化策略的行为和收敛性，并在高维问题上提供了强大的实证性能。统计数据表明，单个高斯过程样本路径的本地解比全局方法恢复的预期值更好。Müller等人提出的贝叶斯本地优化算法的收敛速率在有噪音和无噪音的情况下都有推导。 |
| [^414] | [A score-based operator Newton method for measure transport.](http://arxiv.org/abs/2305.09792) | 本文提出一种新的基于分数的算子Newton方法，可以迭代构造一个易处理的原概率测度，该方法可以在满足目标分数光滑性假设下，实现快速收敛性。 |
| [^415] | [Assessment of few-hits machine learning classification algorithms for low energy physics in liquid argon detectors.](http://arxiv.org/abs/2305.09744) | 本文评估了在液体氩探测器低能物理中使用Few-Hits机器学习分类算法的效果，证明在单比特与双比特事件的分类问题上，卷积神经网络和Transformer-Encoder方法优于传统算法，并针对DUNE Phase II探测器优化了探测器参数。 |
| [^416] | [DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text.](http://arxiv.org/abs/2305.05773) | 本文提出了一种基于深度学习的文本水印技术DeepTextMark，可用于检测大语言模型生成的文本。该技术实现了盲目性、鲁棒性、隐蔽性和可靠性，并在水印检测精度和抵抗攻击方面优于现有方法。 |
| [^417] | [Exploring the Effectiveness of Large Language Models in Generating Unit Tests.](http://arxiv.org/abs/2305.00418) | 本文研究了三种生成模型在单元测试生成方面的效果，并发现在不经过微调的情况下，它们的覆盖率较低且存在测试味道问题。 |
| [^418] | [A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems.](http://arxiv.org/abs/2304.14907) | 本文提出了一种用于求解光滑有界约束优化问题的内点算法。它使用基于随机梯度估计的搜索方向和内部邻域，能够在确定性和随机性设置下具有收敛保证，并且在实验中表现出优于传统方法的性能。 |
| [^419] | [Application of Tensor Neural Networks to Pricing Bermudan Swaptions.](http://arxiv.org/abs/2304.09750) | 本论文用张量神经网络(TNN)对百慕大掉期进行定价，相比于传统方法，TNN具有更快的收敛速度和减少参数敏感度的优点。 |
| [^420] | [Online Reinforcement Learning in Markov Decision Process Using Linear Programming.](http://arxiv.org/abs/2304.00155) | 本论文提出了一种在未知转移矩阵和固定但未知分布的情况下进行在线MDP学习的简单而高效的方法，可以实现更紧的遗憾界，并通过置信区间框架改进了现有算法。 |
| [^421] | [Recent Developments in Machine Learning Methods for Stochastic Control and Games.](http://arxiv.org/abs/2303.10257) | 本文回顾了基于机器学习的随机控制问题和博弈的计算方法，特别是着重介绍了使用深度学习算法解决高维度和非常复杂结构情况下问题的新方法。 |
| [^422] | [Improving and generalizing flow-based generative models with minibatch optimal transport.](http://arxiv.org/abs/2302.00482) | 这篇论文提出了一种称为广义条件流匹配（CFM）的技术，在连续正则化流（CNFs）的生成模型中无需模拟训练，极大提高了效率和稳定性。此外，论文还引入了最优传输CFM（OT-CFM）的变体，可以以无模拟方式计算动态OT，加速了推断过程。 |
| [^423] | [DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models.](http://arxiv.org/abs/2301.13629) | 本论文提出了一种新的方法DiffSTG，该方法结合了STGNN的时空学习能力和扩散模型的不确定性测量，可以有效减小STG预测中的排名概率分数和均方根误差。 |
| [^424] | [Language-Driven Anchors for Zero-Shot Adversarial Robustness.](http://arxiv.org/abs/2301.13096) | 本文提出了一种基于语言驱动、基于锚点的对抗训练策略LAAT，通过利用文本编码器的语义一致性，在零样本图像分类场景下增强图像模型的对抗鲁棒性。实验结果表明，该方法在零样本对抗性能上优于先前的最佳状态对抗性一次性方法，同时能为流行的图像分类模型带来实质性的零样本对抗性能提升。 |
| [^425] | [PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design.](http://arxiv.org/abs/2211.12020) | 提出了PhAST方法来快速发现更有效的催化剂来驱动电化学反应, 该方法适用于大多数体系结构, 可以增加计算效率和精度 |
| [^426] | [CAPE: Corrective Actions from Precondition Errors using Large Language Models.](http://arxiv.org/abs/2211.09935) | CAPE是一种利用大型语言模型从前置错误中纠正行动的方法，提高了生成计划的质量，使具身代理能够执行更多任务，并改善了计划的正确性。 |
| [^427] | [Invariant Aggregator for Defending against Federated Backdoor Attacks.](http://arxiv.org/abs/2210.01834) | 该论文针对联邦学习中的背后攻击提出了一种不变聚合器，防御背后攻击并保持模型的整体效用。研究发现在扁平损失空间中，恶意客户端可以通过提供背后样本来误导联邦学习模型，而不需要与良性客户端有明显的差异。 |
| [^428] | [A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases.](http://arxiv.org/abs/2208.05845) | 本研究通过提供包含47种不同属性注释的大规模数据集，并对三种最先进的Deepfake检测模型进行全面分析，旨在研究公共Deepfake数据集可能带来的AI偏差问题 |
| [^429] | [Local Minima Structures in Gaussian Mixture Models.](http://arxiv.org/abs/2009.13040) | 研究了高斯混合模型中的负对数似然函数的局部极小值结构，发现它们都共享一种常见结构而部分确定了真正的位置混合物的簇中心。这些结果适用于真实混合组分满足某种分离条件的情况，也适用于成分数量过多或过少的情况。 |

# 详细

[^1]: TEDDY: 基于度量判别策略的边缘修剪方法

    TEDDY: Trimming Edges with Degree-based Discrimination strategY

    [https://rss.arxiv.org/abs/2402.01261](https://rss.arxiv.org/abs/2402.01261)

    TEDDY是一种利用边缘度量信息的边缘修剪方法，旨在通过一次性操作实现边缘稀疏化，进而鼓励参数稀疏化训练。这是一个解决图神经网络中抽奖票假设的时间效率和效果问题的创新方法。

    

    自从Chen等人在2021年提出用于图神经网络（GNNs）的抽奖票假设的开创性工作以来，寻找图抽奖票（GLT）的研究已成为GNN社区的重要关注点之一，激发了研究人员在实现与原始密集网络相当性能的同时，发现更稀疏的GLT。同时，图结构作为GNN训练动力学的重要因素，也受到了广泛关注，并得到了最近几项研究的阐明。尽管如此，目前关于GLT的研究通常没有充分利用图结构中的内在路径，并以迭代方式识别票数，这种方法耗时且效率低下。为解决这些限制，我们引入TEDDY，一种利用结构信息并整合边缘度量信息的一次性边缘稀疏化框架。在进行边缘稀疏化后，我们通过简单的投影梯度下降方法鼓励参数稀疏化训练。

    Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient desc
    
[^2]: 三维形状重建的贝叶斯扩散模型

    Bayesian Diffusion Models for 3D Shape Reconstruction

    [https://arxiv.org/abs/2403.06973](https://arxiv.org/abs/2403.06973)

    BDM是一种利用联合扩散过程紧密耦合先验信息与数据驱动过程的预测算法，专注于3D形状重建任务，通过引入独立标签的丰富先验信息来改善自下而上的重建过程。

    

    我们提出了贝叶斯扩散模型（BDM），这是一种预测算法，通过联合扩散过程将自上而下（先验）信息与自下而上（数据驱动）过程紧密耦合，实现有效的贝叶斯推断。我们展示了BDM在3D形状重建任务中的有效性。与基于配对（监督）数据标签（例如图像-点云）数据集训练的典型深度学习数据驱动方法相比，我们的BDM通过引入来自独立标签（例如点云）的丰富先验信息来改进自下而上的3D重建。与标准的贝叶斯框架相反，其需要明确的先验和似然进行推断，BDM通过学习的梯度计算网络通过耦合扩散过程执行无缝信息融合。我们BDM的特殊之处在于其能够在自上而下和自下而上的主动有效信息交换和融合。

    arXiv:2403.06973v1 Announce Type: cross  Abstract: We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom
    
[^3]: 用于预测任务类的表示学习博弈

    A representation-learning game for classes of prediction tasks

    [https://arxiv.org/abs/2403.06971](https://arxiv.org/abs/2403.06971)

    提出了一种基于博弈的表示学习方法，该方法通过最小化预测损失优化表示学习，在有限制条件下找到了理论最佳表示，并展示了先验知识的有效性以及随机化表示的有用性。

    

    当仅有关于未来预测任务的先验知识可用时，我们提出了一种基于博弈的学习降维特征向量表示的形式。在这个博弈中，第一个玩家选择表示，然后第二个玩家从给定类中对手选出一个预测任务，代表了先验知识。第一个玩家的目标是最小化，第二个玩家则是最大化后悔：使用表示进行预测的最小损失，与使用原始特征相比的损失相同。

    arXiv:2403.06971v1 Announce Type: new  Abstract: We propose a game-based formulation for learning dimensionality-reducing representations of feature vectors, when only a prior knowledge on future prediction tasks is available. In this game, the first player chooses a representation, and then the second player adversarially chooses a prediction task from a given class, representing the prior knowledge. The first player aims is to minimize, and the second player to maximize, the regret: The minimal prediction loss using the representation, compared to the same loss using the original features. For the canonical setting in which the representation, the response to predict and the predictors are all linear functions, and under the mean squared error loss function, we derive the theoretically optimal representation in pure strategies, which shows the effectiveness of the prior knowledge, and the optimal regret in mixed strategies, which shows the usefulness of randomizing the representation
    
[^4]: 使用专家混合的课程强化学习获取多样技能

    Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts

    [https://arxiv.org/abs/2403.06966](https://arxiv.org/abs/2403.06966)

    使用Mixture of Experts的Di-SkilL方法通过优化每个专家和其相关上下文分布，实现了在相似上下文中学习多样技能的目标。

    

    强化学习是获取良好表现策略的强大方法，然而，由于常用的高斯策略参数化，RL中学习多样技能具有挑战性。我们提出了一种名为Di-SkilL的RL方法，用于使用专家混合学习多样技能，其中每个专家将技能形式化为一种上下文运动原语。Di-SkilL优化每个专家及其相关上下文分布以达到最大熵目标，激励在相似上下文中学习多样技能。每个专家的上下文分布使得自动课程学习成为可能，使每个专家能够专注于其在上下文空间的最佳表现子区域。为了克服在没有任何关于环境未知上下文概率空间的先验知识的情况下的硬性不连续性和多模态性，我们利用基于能量的模型来表示每个专家的上下文分布。

    arXiv:2403.06966v1 Announce Type: new  Abstract: Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose \textbf{Di}verse \textbf{Skil}l \textbf{L}earning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distri
    
[^5]: 下一个标记预测的陷阱

    The pitfalls of next-token prediction

    [https://arxiv.org/abs/2403.06963](https://arxiv.org/abs/2403.06963)

    论文揭示了在某些任务类别中，教师强制方法可能无法在第一时间学习到准确的下一个标记预测器，进而导致模型失败的一般机制。

    

    一篇关于下一个标记预测的论文。我们提出了一个直观的担忧：一个仅仅基于下一个标记预测的模型是否能忠实地模拟人类智能。我们认为下一个标记预测中经常混淆的两个阶段 -- 自回归推断和教师强制训练 -- 必须被区别对待。我们描述了一个一般机制，展示了教师强制如何失败，并设计了一个最小化计划任务，在这个任务中Transformer和Mamba架构在实践中以这种方式失败 -- 尽管任务本身很容易学习。

    arXiv:2403.06963v1 Announce Type: cross  Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn. We provide preliminary
    
[^6]: 准确预测新型二维混合有机无机钙钛矿结构

    Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic Perovskites

    [https://arxiv.org/abs/2403.06955](https://arxiv.org/abs/2403.06955)

    提出了一种用于准确预测新型二维混合有机无机钙钛矿结构的机器学习原子间势，可以实现化学精度。

    

    低维混合有机无机钙钛矿（HOIPs）代表了一类既适用于光吸收又适用于发射的电子活性材料。HOIPs的设计空间非常庞大，因为各种不同的有机阳离子可以与不同的无机框架结合。这种巨大的设计空间使电子和机械性能可以进行可调，但也需要发展新的工具来推断候选结构的高通量分子动力学分析。在本文中，我们提出了一种准确、高效、可转移且广泛适用的机器学习原子间势（MLIP）用于预测新型二维HOIPs的结构。使用MACE体系结构，在86个不同的实验报道的HOIP结构上训练了一个MLIP模型。该模型在73个未见过的钙钛矿组成上进行了测试，并实现了与参考电子结构方法相比的化学精度。然后，我们的模型继续...

    arXiv:2403.06955v1 Announce Type: cross  Abstract: Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a promising class of electronically active materials for both light absorption and emission. The design space of HOIPs is extremely large, since a diverse space of organic cations can be combined with different inorganic frameworks. This immense design space allows for tunable electronic and mechanical properties, but also necessitates the development of new tools for in silico high throughput analysis of candidate structures. In this work, we present an accurate, efficient, transferable and widely applicable machine learning interatomic potential (MLIP) for predicting the structure of new 2D HOIPs. Using the MACE architecture, an MLIP is trained on 86 diverse experimentally reported HOIP structures. The model is tested on 73 unseen perovskite compositions, and achieves chemical accuracy with respect to the reference electronic structure method. Our model is then co
    
[^7]: SELMA：学习和合并具有自动生成数据的技能特定文本到图像专家

    SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data

    [https://arxiv.org/abs/2403.06952](https://arxiv.org/abs/2403.06952)

    SELMA提出了一种新范式，通过在自动生成的多技能图像文本数据集上微调模型，并进行技能特定专家学习和合并，从而改进T2I模型的忠实度。

    

    最近，文本到图像（T2I）生成模型展示了从文本描述中创建图像的令人印象深刻的能力。然而，这些T2I生成模型在生成精确匹配文本输入细节的图像方面经常表现不佳，比如不正确的空间关系或缺失对象。在本文中，我们介绍了SELMA：具有自动生成数据的技能特定专家学习和合并，这是一种改进T2I模型忠实度的新范式，通过在自动生成的多技能图像文本数据集上微调模型，并进行技能特定专家学习和合并。首先，SELMA利用LLM的环境学习能力生成多个文本提示数据集，可以教授不同的技能，然后基于提示使用T2I模型生成图像。接下来，SELMA通过学习多个单技能的LoRA（低秩调整）专家调整T2I模型到新技能。

    arXiv:2403.06952v1 Announce Type: cross  Abstract: Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts follow
    
[^8]: 使用连续时序测量和生成人工智能进行电网监测和保护

    Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI

    [https://arxiv.org/abs/2403.06942](https://arxiv.org/abs/2403.06942)

    提出了基于连续时序测量和生成人工智能的电网监测和控制系统，通过数据压缩和故障检测，实现了对传统监控系统的进步。

    

    本文提出了一个下一代电网监测和控制系统的案例，利用生成人工智能（AI）、机器学习和统计推断方面的最新进展。我们提出了一种基于连续时序测量和AI支持的数据压缩和故障检测的监测和控制框架，超越了先前基于SCADA和同步相量技术构建的广域监测系统的发展。

    arXiv:2403.06942v1 Announce Type: cross  Abstract: Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference. Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection.   Methods and Results: The architecture of the proposed design originates from the Wiener-Kallianpur innovation representation of a random process that transforms causally a stationary random process into an innovation sequence with independent and identically distributed random variables. This work presents a generative AI approach that (i) learns an innovation autoencoder that extracts innovation se
    
[^9]: 用于皮质厚度轨迹预测的条件得分扩散模型

    Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction

    [https://arxiv.org/abs/2403.06940](https://arxiv.org/abs/2403.06940)

    该论文提出了一种条件得分扩散模型，针对皮质厚度轨迹预测，在应对数据缺失和对疾病进展建模方面具有优势。

    

    阿尔茨海默病（AD）是一种以个体之间病程进展速率不同为特征的神经退行性疾病，皮质厚度（CTh）的变化与疾病进展密切相关。准确预测CTh轨迹可以显著提升早期诊断和干预策略，为患者提供及时护理。然而，用于这些研究的纵向数据通常存在时间稀疏和不完整的问题，导致在准确建模疾病进展方面面临重大挑战。现有方法局限性较大，主要集中在没有缺失条目的数据集上或要求对CTh进展进行预定义假设。为克服这些障碍，我们提出了一种特别设计用于生成带有基线信息（如年龄、性别和初步诊断）的CTh轨迹的条件得分扩散模型。我们的条件扩散模型利用所有可用数据进行模型拟合。

    arXiv:2403.06940v1 Announce Type: cross  Abstract: Alzheimer's Disease (AD) is a neurodegenerative condition characterized by diverse progression rates among individuals, with changes in cortical thickness (CTh) closely linked to its progression. Accurately forecasting CTh trajectories can significantly enhance early diagnosis and intervention strategies, providing timely care. However, the longitudinal data essential for these studies often suffer from temporal sparsity and incompleteness, presenting substantial challenges in modeling the disease's progression accurately. Existing methods are limited, focusing primarily on datasets without missing entries or requiring predefined assumptions about CTh progression. To overcome these obstacles, we propose a conditional score-based diffusion model specifically designed to generate CTh trajectories with the given baseline information, such as age, sex, and initial diagnosis. Our conditional diffusion model utilizes all available data durin
    
[^10]: 使用知识图嵌入进行反事实推理

    Counterfactual Reasoning with Knowledge Graph Embeddings

    [https://arxiv.org/abs/2403.06936](https://arxiv.org/abs/2403.06936)

    通过新任务CFKGR，本文将知识图补全和反事实推理联系起来，提出了一种用于适应假设前提的知识图嵌入方法COULDD，并通过基准数据集的评估表明KGEs可以学习图中的模式，检测出合理的反事实变化。

    

    知识图嵌入（KGEs）最初是为了推断不完整知识库中缺失的真实事实而开发的。本文通过我们的新任务CFKGR将知识图补全和反事实推理联系起来。我们将原始世界状态建模为知识图，假设情景为添加到图中的边，对图的合理变化为逻辑规则推理的结果。我们创建了相应的基准数据集，其中包含各种假设情景及对原始知识图的合理改变以及应该保留的事实。我们开发了COULDD，一种针对特定假设前提调整现有知识图嵌入的通用方法，并在我们的基准上进行评估。我们的结果表明，KGEs可以在没有显式训练的情况下从图中学习模式。我们进一步观察到，通过COULDD调整后的KGEs可以可靠地检测出遵循逻辑规则的图的合理反事实变化。

    arXiv:2403.06936v1 Announce Type: cross  Abstract: Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories. In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR. We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules. We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained. We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark. Our results indicate that KGEs learn patterns in the graph without explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow the
    
[^11]: Transformers学习低敏感性函数的简单性偏差

    Simplicity Bias of Transformers to Learn Low Sensitivity Functions

    [https://arxiv.org/abs/2403.06925](https://arxiv.org/abs/2403.06925)

    Transformers在不同数据模态上具有低敏感性，这种简单性偏差有助于解释其在视觉和语言任务中的优越性能。

    

    Transformers在许多任务中取得了最先进的准确性和鲁棒性，但对它们具有的归纳偏差以及这些偏差如何与其他神经网络架构不同的理解仍然难以捉摸。本文中，我们将模型对输入中的随机更改的敏感性概念化为一种简单性偏差的概念，这为解释transformers在不同数据模态上的简单性和谱偏差提供了统一的度量标准。我们展示了transformers在视觉和语言任务中比其他替代架构（如LSTMs、MLPs和CNNs）具有更低的敏感性。我们还展示了低敏感性偏差与改进性能的相关性。

    arXiv:2403.06925v1 Announce Type: cross  Abstract: Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with impro
    
[^12]: 负责任人工智能：一项结构化文献综述

    Responsible Artificial Intelligence: A Structured Literature Review

    [https://arxiv.org/abs/2403.06910](https://arxiv.org/abs/2403.06910)

    该研究提出了负责任人工智能的全面统一定义，并通过结构化文献综述阐明了当前对负责任人工智能的理解，旨在帮助立法者和机器学习从业者在AI监管领域做出指导。

    

    arXiv:2403.06910v1 公告类型：新的 摘要：我们的研究致力于推进负责任人工智能（AI）的概念，在欧盟政策讨论中日益重要。欧盟最近发布了几份强调AI信任必要性的出版物，强调AI作为有益工具和潜在武器的双重性质。这种二元性突显了国际监管的迫切需要。与此同时，需要指导公司在AI发展中遵守这些规定的框架。我们的研究旨在帮助立法者和机器学习从业者在AI监管不断发展的背景下导航，确定未来关注的焦点领域。本文介绍了负责任AI的全面且据我们所知第一个统一定义。通过结构化文献综述，我们阐明了对负责任AI的当前理解。借鉴这一分析，我们提出了

    arXiv:2403.06910v1 Announce Type: new  Abstract: Our research endeavors to advance the concept of responsible artificial intelligence (AI), a topic of increasing importance within EU policy discussions. The EU has recently issued several publications emphasizing the necessity of trust in AI, underscoring the dual nature of AI as both a beneficial tool and a potential weapon. This dichotomy highlights the urgent need for international regulation. Concurrently, there is a need for frameworks that guide companies in AI development, ensuring compliance with such regulations. Our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of AI regulation, identifying focal areas for future attention. This paper introduces a comprehensive and, to our knowledge, the first unified definition of responsible AI. Through a structured literature review, we elucidate the current understanding of responsible AI. Drawing from this analysis, we propose an
    
[^13]: 成本敏感学习在考虑工作量约束下推迟多位专家决策

    Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints

    [https://arxiv.org/abs/2403.06906](https://arxiv.org/abs/2403.06906)

    提出了成本和工作量约束下的推迟框架（DeCCaF），旨在解决成本敏感场景、并发预测和人类工作能力约束等问题

    

    学习推迟（L2D）旨在通过学习如何在人工智能协作系统中将决策推迟给人类，从而在人类更有可能正确时推迟决策。现有L2D研究忽视了阻碍其实际采用的真实系统的关键方面，即：忽视成本敏感场景，其中第1类和第2类错误的成本不同；要求每个训练数据集实例的并发人类预测；不处理人类工作能力约束。为了解决这些问题，我们提出了成本和工作量约束下的推迟框架（DeCCaF）。DeCCaF是一种新颖的L2D方法，采用监督学习来建模人类错误的概率，减少数据要求的限制，并使用约束编程来全局最小化错误成本，同时考虑工作量限制。我们在一个系列中测试了DeCCaF

    arXiv:2403.06906v1 Announce Type: cross  Abstract: Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series 
    
[^14]: 具有适度输入维度的泄漏ReLU网络中的良性过拟合问题

    Benign overfitting in leaky ReLU networks with moderate input dimension

    [https://arxiv.org/abs/2403.06903](https://arxiv.org/abs/2403.06903)

    研究了在泄漏ReLU网络上使用铰链损失进行训练的过程中，信噪比（SNR）条件对于良性和非良性过拟合的影响，并发现高SNR值会导致良性过拟合，低SNR值则会导致有害过拟合。

    

    良性过拟合问题探讨了一个模型是否能够完美地拟合嘈杂的训练数据，同时又能够很好地泛化。我们研究了在二层泄漏ReLU网络上使用铰链损失进行训练的良性过拟合问题，针对二分类任务。我们考虑输入数据，可以分解为一个共同信号和一个随机噪声成分的总和，这两者相互正交。我们表征了模型参数的信噪比（SNR）条件，导致了良性和非良性（有害）过拟合：特别是，如果SNR很高，则发生良性过拟合，相反，如果SNR很低，则发生有害过拟合。我们将良性和非良性过拟合归因于一个近似边界最大化性质，并展示了在铰链损失下使用梯度下降（GD）训练的泄漏ReLU网络满足这一性质。与以前的工作相比，我们不需要nea

    arXiv:2403.06903v1 Announce Type: new  Abstract: The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property. In contrast to prior work we do not require nea
    
[^15]: LIBR+: 通过学习基于生物力学变形的差值来改进术中肝脏配准

    LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration

    [https://arxiv.org/abs/2403.06901](https://arxiv.org/abs/2403.06901)

    通过深度学习和生物力学模型相结合的LIBR+方法，作者提出了一种改进术中肝脏配准的方法，能有效处理术中测量的稀疏性和变异性。

    

    手术环境对术中器官形状与术前成像几何形状的配准提出了独特的挑战。基于生物力学模型的配准仍然很流行，而深度学习解决方案受到术中测量的稀疏性和变异性以及手术过程中可以获得的有限器官地面真实变形的限制。本文提出了一种新颖的“混合”配准方法，利用基于线性弹性生物力学的线性化迭代边界重建（LIBR）方法，并使用深度神经网络学习其残差以获取地面真实变形（LIBR+）。我们进一步制定了一个双分支样条残差图卷积神经网络（SR-GCN）来吸收稀疏和变量术中测量的信息，并有效地通过3D器官的几何形状传播。在大规模数据集上进行了实验...

    arXiv:2403.06901v1 Announce Type: cross  Abstract: The surgical environment imposes unique challenges to the intraoperative registration of organ shapes to their preoperatively-imaged geometry. Biomechanical model-based registration remains popular, while deep learning solutions remain limited due to the sparsity and variability of intraoperative measurements and the limited ground-truth deformation of an organ that can be obtained during the surgery. In this paper, we propose a novel \textit{hybrid} registration approach that leverage a linearized iterative boundary reconstruction (LIBR) method based on linear elastic biomechanics, and use deep neural networks to learn its residual to the ground-truth deformation (LIBR+). We further formulate a dual-branch spline-residual graph convolutional neural network (SR-GCN) to assimilate information from sparse and variable intraoperative measurements and effectively propagate it through the geometry of the 3D organ. Experiments on a large int
    
[^16]: 量子张量网络在蛋白质分类中的应用

    Application of Quantum Tensor Networks for Protein Classification

    [https://arxiv.org/abs/2403.06890](https://arxiv.org/abs/2403.06890)

    量子张量网络广泛应用于蛋白质分类问题，通过量子增强处理能力有效应对蛋白质序列的复杂性和多样性

    

    我们展示了蛋白质序列可以被视为自然语言处理中的句子，并且可以使用现有的量子自然语言框架解析成合理量子比特的参数化量子电路，这些电路可以被训练用于解决各种与蛋白质相关的机器学习问题。我们基于蛋白质的细胞亚细胞位置对其进行分类，这是生物信息学中的关键任务，有助于理解生物过程和疾病机制。利用量子增强的处理能力，我们展示了量子张量网络（QTN）可以有效处理蛋白质序列的复杂性和多样性。我们提出了一种详细的方法论，将QTN架构调整到蛋白质数据的微妙要求，支持全面的实验结果。我们展示了受传统循环神经网络（RNN）和卷积神经网络（CNN）启发的两种不同的QTNs，以解决...

    arXiv:2403.06890v1 Announce Type: cross  Abstract: We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems. We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences. We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results. We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve th
    
[^17]: HiRA-Pro: 高分辨率对齐多模态时空数据的过程物理驱动方法

    HiRA-Pro: High resolution alignment of multimodal spatio-temporal data: a process physics driven approach

    [https://arxiv.org/abs/2403.06888](https://arxiv.org/abs/2403.06888)

    HiRA-Pro是一个高分辨率对齐多模态时空数据的过程物理驱动方法，成功解决了对齐具有亚毫秒现象的数据的挑战，并在智能制造环境中取得了成功应用。

    

    我们提出了HiRA-Pro，一个新颖的程序，用于在高时空分辨率下对来自展示多样瞬态、非线性随机动态的真实世界过程和系统的多模态信号进行对齐，例如制造机器。它基于识别和同步这些不同信号中显著运动学和动力学事件的过程特征。HiRA-Pro解决了对齐具有亚毫秒现象的数据的挑战，而传统的时间戳、外部触发器或基于时钟的对齐方法则难以胜任。HiRA-Pro的有效性在智能制造环境中得到了展示，在这里，它对来自Optomec-LENS MTS 500混合机器进行3D打印和铣削操作期间获取的13+通道数据进行了对齐。然后，对齐数据被体素化，生成对应于所制造零件上的物理体素的0.25秒对齐数据块。HiRA-Pro的优越性通过以下方式进一步展示

    arXiv:2403.06888v1 Announce Type: cross  Abstract: We present HiRA-Pro, a novel procedure to align, at high spatio-temporal resolutions, multimodal signals from real-world processes and systems that exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing machines. It is based on discerning and synchronizing the process signatures of salient kinematic and dynamic events in these disparate signals. HiRA-Pro addresses the challenge of aligning data with sub-millisecond phenomena, where traditional timestamp, external trigger, or clock-based alignment methods fall short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing context, where it aligns data from 13+ channels acquired during 3D-printing and milling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data is then voxelized to generate 0.25 second aligned data chunks that correspond to physical voxels on the produced part. The superiority of HiRA-Pro is further showcased thro
    
[^18]: 揭示受幼儿启发的奖励转换在目标导向强化学习中的重要性

    Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning

    [https://arxiv.org/abs/2403.06880](https://arxiv.org/abs/2403.06880)

    研究探讨了幼儿启发的奖励转换如何影响强化学习任务的样本效率和成功率，特别是发现了幼儿启发的稀疏转密集（S2D）转换的有效性。

    

    幼儿从稀疏反馈的自由探索逐渐发展为利用先前经验进行以目标为导向的学习，获得更密集奖励。受此幼儿启发的奖励转换的影响，我们探讨了将不同奖励转换纳入强化学习（RL）任务的意义。我们研究的重点是从稀疏到基于潜在的密集奖励的转换，这两者共享无论奖励变化均为最佳策略。通过包括以自我为中心的导航和机械臂操作任务在内的各种实验，我们发现适当的奖励转换显著影响样本效率和成功率。特别值得注意的是受幼儿启发的稀疏转密集（S2D）转换的有效性。除了这些性能指标外，使用交叉密度可视化技术，我们观察到转换，特别是S2D，使策略损失景观更加平滑，促进

    arXiv:2403.06880v1 Announce Type: cross  Abstract: Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting
    
[^19]: COOD：使用多种度量结合进行大规模层次分类中的异常和新类别检测

    COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification

    [https://arxiv.org/abs/2403.06874](https://arxiv.org/abs/2403.06874)

    本文提出了一种将个体OOD度量结合成一个组合OOD度量（COOD）的框架，在大规模生物多样性数据集上进行了广泛评估，表现出在TPR@1% FPR方面明显优于个体OOD度量。

    

    高性能的超出分布（OOD）检测，包括异常和新类别，是分类模型实际应用的重要先决条件。本文关注图像中物种识别任务，涉及大型数据库、大量细粒度层次类、严重的类别不平衡以及不同的图像质量。我们提出了一个框架，将个体OOD度量结合成一个组合OOD（COOD）度量，使用监督模型。个体度量包括几个现有的最先进度量和几个新颖的OOD度量，这些度量是为了新类别检测和分层类结构而开发的。COOD在三个大规模（500k+图像）生物多样性数据集上进行了广泛评估，用于异常和新类别检测。我们展示了COOD在TPR@1% FPR方面明显优于个体度量，包括最先进的度量。

    arXiv:2403.06874v1 Announce Type: cross  Abstract: High-performing out-of-distribution (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models. In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality. We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a supervised model. The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind. COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection. We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority 
    
[^20]: 增量方法的最后迭代收敛性及在继续学习中的应用

    Last Iterate Convergence of Incremental Methods and Applications in Continual Learning

    [https://arxiv.org/abs/2403.06873](https://arxiv.org/abs/2403.06873)

    针对继续学习应用，本研究首次获得了增量梯度和增量近端方法最后迭代的收敛保证，且其预期复杂度界限几乎与已知最佳平均迭代的界限相匹配。

    

    增量梯度方法和增量近端方法是一类基本的优化算法，用于解决广泛研究的有限和问题。然而，就收敛性保证而言，非渐进（一阶或近端）的预期复杂性界限最近才得到，并且几乎仅适用于平均迭代。受继续学习应用的启发，我们获得了对一般凸平滑（两者）和凸Lipschitz（对于近端变种）设置中增量梯度和增量近端方法的最后迭代的首个收敛保证。我们对最后迭代的预期复杂性界限几乎与最佳已知的平均迭代的预期复杂性界限相匹配（即匹配至平方根对数或对数因子），对两类方法均适用。我们进一步将我们的结果推广到加权平均的情况。

    arXiv:2403.06873v1 Announce Type: cross  Abstract: Incremental gradient methods and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature. Yet, when it comes to their convergence guarantees, nonasymptotic (first-order or proximal) oracle complexity bounds have been obtained fairly recently, almost exclusively applying to the average iterate. Motivated by applications in continual learning, we obtain the first convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings. Our oracle complexity bounds for the last iterate nearly match (i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods. We further obtain generalizations of our results to weighted averaging of th
    
[^21]: 关于无监督预训练的泛化能力

    On the Generalization Ability of Unsupervised Pretraining

    [https://arxiv.org/abs/2403.06871](https://arxiv.org/abs/2403.06871)

    无监督预训练如何影响模型泛化能力的关键因素的新理论框架

    

    无监督学习的最新进展表明，无监督预训练，然后进行微调，可以提高模型的泛化能力。然而，目前对于在未标记数据集上学习的表示函数如何影响微调模型的泛化能力缺乏严格的理解。现有理论研究未能充分考虑预训练和微调阶段的分布和任务的异质性。为填补这一空白，本文引入了一个新颖的理论框架，阐明了影响从无监督预训练获得的知识在随后的微调阶段的可传递性的关键因素，最终影响了微调模型在下游任务上的泛化能力。我们应用我们的理论框架来分析两种不同情景的泛化界限：使用深度神经网络进行上下文编码器预训练和蒙版自编码预

    arXiv:2403.06871v1 Announce Type: new  Abstract: Recent advances in unsupervised learning have shown that unsupervised pre-training, followed by fine-tuning, can improve model generalization. However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the fine-tuned model is lacking. Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and fine-tuning stage. To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during unsupervised pre-training to the subsequent fine-tuning phase, ultimately affecting the generalization capabilities of the fine-tuned model on downstream tasks. We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked Auto
    
[^22]: 语义剩余提示用于持续学习

    Semantic Residual Prompts for Continual Learning

    [https://arxiv.org/abs/2403.06870](https://arxiv.org/abs/2403.06870)

    通过引入语义剩余提示，作者提出了一种稳定的选择策略，利用两级适应机制来在持续学习中解决提示冲突的问题。

    

    持续学习（CL）的提示调整方法冻结了一个大型预训练模型，并侧重于训练一些称为提示的参数向量。这些方法中的大多数将这些向量组织在一个键-值对池中，并使用输入图像作为查询来检索提示（值）。然而，随着任务的进行，由于键是学习的，提示选择策略本身也会面临灾难性遗忘，这是现有方法经常忽视的问题。为了使选择策略更加稳定，我们请求一个基础模型（CLIP）来在两级适应机制中选择我们的提示。具体而言，第一级利用标准文本提示来调整CLIP文本编码器，形成稳定的类原型。而第二级则将这些原型与查询图像一起用作键来索引一个s

    arXiv:2403.06870v1 Announce Type: new  Abstract: Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a s
    
[^23]: 在有噪声基础模型中学习

    Learning with Noisy Foundation Models

    [https://arxiv.org/abs/2403.06869](https://arxiv.org/abs/2403.06869)

    本文首次全面了解和分析了预训练数据集中的噪声性质，有效减轻其对下游任务影响。

    

    基础模型通常是在大规模数据集上进行预训练，然后通过调整来适应下游任务。然而，大规模预训练数据集往往无法获取或成本过高，可能包含标签噪声，这可能会对模型的泛化能力造成不利影响，并带来意想不到的风险。本文是首个全面了解和分析预训练数据集中噪声性质，并有效减轻其对下游任务影响的工作。具体而言，通过在合成有噪声的ImageNet-1K、YFCC15M和CC12M数据集上进行完全监督和图像-文本对比预训练的广泛实验，我们证明了，尽管预训练中的轻微噪声可以使同领域（ID）性能受益，即训练和测试数据共享类似分布，但它总是会破坏跨领域（OOD）性能，在那里训练和测试分布明显不同。

    arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
    
[^24]: 非洲沙漠蝗虫繁殖地预测的地理空间方法

    A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa

    [https://arxiv.org/abs/2403.06860](https://arxiv.org/abs/2403.06860)

    该研究开发了一个地理空间模型，用于预测沙漠蝗虫的繁殖地，有望提升早期预警系统和有针对性的控制措施。

    

    沙漠蝗虫成群成队对农业和食品安全构成重大威胁。本研究针对这一挑战，开发了一个可操作的模型，用于预测蝗虫的繁殖地，有望增强早期预警系统和有针对性的控制措施。我们从联合国粮食和农业组织(UN-FAO)的蝗虫观测记录中整理了一个数据集，并使用两种类型的时空输入特征进行分析：遥感环境和气候数据，以及多光谱地球观测图像。我们的方法采用定制的深度学习模型(三维和基于LSTM的循环卷积网络)，以及Jakubik等人于2023年最新发布的地理空间基础模型Prithvi。这些模型显著优于现有基准线，基于Prithvi的模型，通过对来自NASA的谐调Landsat和哨兵-2(HLS)多光谱图像进行微调而表现出色。

    arXiv:2403.06860v1 Announce Type: new  Abstract: Desert locust swarms present a major threat to agriculture and food security. Addressing this challenge, our study develops an operationally-ready model for predicting locust breeding grounds, which has the potential to enhance early warning systems and targeted control measures. We curated a dataset from the United Nations Food and Agriculture Organization's (UN-FAO) locust observation records and analyzed it using two types of spatio-temporal input features: remotely-sensed environmental and climate data as well as multi-spectral earth observation images. Our approach employed custom deep learning models (three-dimensional and LSTM-based recurrent convolutional networks), along with the geospatial foundational model Prithvi recently released by Jakubik et al., 2023. These models notably outperformed existing baselines, with the Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized Landsat and Sentinel-2 (HLS) 
    
[^25]: 量化逆强化学习对误差规定的敏感性

    Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification

    [https://arxiv.org/abs/2403.06854](https://arxiv.org/abs/2403.06854)

    本文分析了逆强化学习问题对行为模型误差的敏感性，并提供了观测数据与假定行为模型不同但不引发错误的条件。

    

    逆强化学习（IRL）旨在从代理的行为（表示为策略$\pi$）中推断其偏好（表示为奖励函数$R$）。为此，我们需要一个描述$\pi$与$R$关系的行为模型。当前文献中，最常见的行为模型是最优性、Boltzmann-理性和因果熵最大化。然而，人类偏好与其行为之间的真实关系要比任何这些行为模型复杂得多。这意味着行为模型存在规定错误的可能性，从而引发对真实数据的系统误差担忧。本文分析了IRL问题对行为模型误差的敏感性。具体而言，我们提供了完全描述观测数据如何可能与假定的行为模型不同而不会产生错误的必要和充分条件。

    arXiv:2403.06854v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) aims to infer an agent's preferences (represented as a reward function $R$) from their behaviour (represented as a policy $\pi$). To do this, we need a behavioural model of how $\pi$ relates to $R$. In the current literature, the most common behavioural models are optimality, Boltzmann-rationality, and causal entropy maximisation. However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models. This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data. In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model. Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error abov
    
[^26]: 面向支持新生儿科医生的分娩室教育工具研究

    Towards an educational tool for supporting neonatologists in the delivery room

    [https://arxiv.org/abs/2403.06843](https://arxiv.org/abs/2403.06843)

    提出了一种机器学习方法，用于从实际数据中识别新生儿分娩事件的风险因素，旨在设计出一款用户友好的移动应用程序，提高对高风险患者的识别率和干预规划。

    

    如今，有证据表明几个因素可能增加婴儿在出生时需要稳定或复苏操作的风险。然而，这些风险因素尚未完全知晓，并且目前尚无适用于预测高风险情况的普遍模型。考虑到这些限制和出生时需要进行复苏的需求相对罕见，有必要对负责分娩室新生儿护理的医护人员进行定期培训。本文提出了一种机器学习方法，用于从实际数据中识别风险因素及其对分娩事件的影响，这可以帮助人员逐步增加和更新他们的知识。我们的最终目标是设计一个用户友好的移动应用程序，能够提高高风险患者的识别率和规划适当的干预措施。

    arXiv:2403.06843v1 Announce Type: new  Abstract: Nowadays, there is evidence that several factors may increase the risk, for an infant, to require stabilisation or resuscitation manoeuvres at birth. However, this risk factors are not completely known, and a universally applicable model for predicting high-risk situations is not available yet. Considering both these limitations and the fact that the need for resuscitation at birth is a rare event, periodic training of the healthcare personnel responsible for newborn caring in the delivery room is mandatory.   In this paper, we propose a machine learning approach for identifying risk factors and their impact on the birth event from real data, which can be used by personnel to progressively increase and update their knowledge. Our final goal will be the one of designing a user-friendly mobile application, able to improve the recognition rate and the planning of the appropriate interventions on high-risk patients.
    
[^27]: LLMs能够将指令与数据分离吗？我们具体指的是什么？

    Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?

    [https://arxiv.org/abs/2403.06833](https://arxiv.org/abs/2403.06833)

    本研究提出了一种形式化的度量来量化指令与数据分离现象，以及一种可以从模型黑盒输出计算的经验变量，并引入了新数据集SEP，用于评估

    

    arXiv:2403.06833v1 公告类型: 跨 针对大型语言模型（LLMs）进行调节指令的技术取得了突破性的成果，为许多实际应用打开了无数新可能。然而，LLMs缺乏其他计算机科学领域已建立为规范的基本安全特性，比如指令与数据之间的分离，导致它们发生故障或易受第三方操控和干扰（例如通过间接提示/命令注入）。更糟糕的是，迄今为止，甚至没有确切定义这种分离究竟意味着什么以及如何测试其违反情况。本研究旨在填补这一空白。我们引入了一个正式的指标来量化指令与数据分离现象，以及一个可以从模型的黑盒输出计算的经验变量。我们还介绍了一个新的数据集SEP（应该执行还是处理？），该数据集允许评估

    arXiv:2403.06833v1 Announce Type: cross  Abstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating th
    
[^28]: 使用分类器构建变量辅助回归：一个实证评估

    Constructing Variables Using Classifiers as an Aid to Regression: An Empirical Assessment

    [https://arxiv.org/abs/2403.06829](https://arxiv.org/abs/2403.06829)

    提出了一种方法，利用分类器预测变量的离散值并将其作为附加变量用于丰富回归问题的初始向量，经实验证实了该方法的有效性。

    

    本文提出了一种方法，用于自动创建变量（在回归的情况下），以补充初始输入向量中所包含的信息。该方法作为预处理步骤运行，在该步骤中，要回归的变量的连续值被离散化为一组间隔，然后用于定义值阈。然后对分类器进行训练，以预测要回归的值是否小于或等于这些阈值中的每一个。然后分类器的不同输出被链接在一个附加的变量向量中，丰富了回归问题的初始向量。实现的系统因此可以被视为一种通用的预处理工具。我们通过5种类型的回归器测试了所提出的丰富方法，并在33个回归数据集中进行了评估。我们的实验证实了这种方法的有效性。

    arXiv:2403.06829v1 Announce Type: new  Abstract: This paper proposes a method for the automatic creation of variables (in the case of regression) that complement the information contained in the initial input vector. The method works as a pre-processing step in which the continuous values of the variable to be regressed are discretized into a set of intervals which are then used to define value thresholds. Then classifiers are trained to predict whether the value to be regressed is less than or equal to each of these thresholds. The different outputs of the classifiers are then concatenated in the form of an additional vector of variables that enriches the initial vector of the regression problem. The implemented system can thus be considered as a generic pre-processing tool. We tested the proposed enrichment method with 5 types of regressors and evaluated it in 33 regression datasets. Our experimental results confirm the interest of the approach.
    
[^29]: 基于上下文的探索-利用用于强化学习

    In-context Exploration-Exploitation for Reinforcement Learning

    [https://arxiv.org/abs/2403.06826](https://arxiv.org/abs/2403.06826)

    引入了In-context Exploration-Exploitation (ICEE)算法，通过在Transformer模型内部进行探索-利用权衡，提高了在-context策略学习的效率。

    

    在-context学习是在线策略学习离线强化学习（RL）方法的一种有前途的方法，可以在推理时间内实现，无需梯度优化。然而，由于需要收集大量训练轨迹集并训练大型Transformer模型，这种方法所带来的显著计算成本。我们通过引入一种基于In-context Exploration-Exploitation（ICEE）的算法来解决这一挑战，该算法旨在优化在-context策略学习的效率。ICEE在推理时间内在Transformer模型中执行探索-利用权衡，不需要显式贝叶斯推断。因此，ICEE可以像高斯过程偏差方法那样有效地解决贝叶斯优化问题，但时间显着较短。通过在网格世界环境中的实验，我们证明ICEE能够学习解决新的R

    arXiv:2403.06826v1 Announce Type: cross  Abstract: In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new R
    
[^30]: 目标信息更有效吗？

    Are Targeted Messages More Effective?

    [https://arxiv.org/abs/2403.06817](https://arxiv.org/abs/2403.06817)

    GNN的核心架构有两个版本，第一个版本消息仅取决于源顶点的状态，而第二个版本消息取决于源顶点和目标顶点的状态。

    

    图神经网络（GNN）是用于图形的深度学习架构。本质上，GNN是一个分布式的消息传递算法，其受到从数据中学到的参数的控制。它在图的顶点上操作：在每次迭代中，顶点在每个传入边上接收一条消息，聚合这些消息，然后根据它们当前的状态和聚合的消息更新它们的状态。GNN的表达能力可以用带计数的一阶逻辑的某些片段和Weisfeiler-Lehman算法来描述。GNN的核心架构有两个不同的版本。在第一个版本中，消息仅取决于源顶点的状态，而在第二个版本中，消息取决于源顶点和目标顶点的状态。实际上，这两个版本都被使用，但迄今为止GNN的理论大多集中在第一个版本上。在逻辑方面，这两个版本对应着

    arXiv:2403.06817v1 Announce Type: cross  Abstract: Graph neural networks (GNN) are deep learning architectures for graphs. Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   The core GNN architecture comes in two different versions. In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices. In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one. On the logical side, the two versions correspond to 
    
[^31]: 大规模、非光滑最大熵模型的高效一阶算法及其在野火科学中的应用

    Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science

    [https://arxiv.org/abs/2403.06816](https://arxiv.org/abs/2403.06816)

    提出了一种新颖的优化算法，利用Kullback-Leibler散度训练大规模、非光滑的Maxent模型

    

    最大熵（Maxent）模型是一类利用最大熵原理从数据中估计概率分布的统计模型。由于现代数据集的规模，Maxent模型需要高效的优化算法来适应大数据应用。本文提出了一种新颖的优化算法，克服了训练大规模、非光滑Maxent模型的现有算法的缺点。我们提出的一阶算法利用Kullback-Leibler散度，可以高效地训练大规模且非光滑的Maxent模型。

    arXiv:2403.06816v1 Announce Type: cross  Abstract: Maximum entropy (Maxent) models are a class of statistical models that use the maximum entropy principle to estimate probability distributions from data. Due to the size of modern data sets, Maxent models need efficient optimization algorithms to scale well for big data applications. State-of-the-art algorithms for Maxent models, however, were not originally designed to handle big data sets; these algorithms either rely on technical devices that may yield unreliable numerical results, scale poorly, or require smoothness assumptions that many practical Maxent models lack. In this paper, we present novel optimization algorithms that overcome the shortcomings of state-of-the-art algorithms for training large-scale, non-smooth Maxent models. Our proposed first-order algorithms leverage the Kullback-Leibler divergence to train large-scale and non-smooth Maxent models efficiently. For Maxent models with discrete probability distribution of $
    
[^32]: {\epsilon}-神经Thompson采样在帕金森病治疗的深度脑部刺激中的应用

    {\epsilon}-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment

    [https://arxiv.org/abs/2403.06814](https://arxiv.org/abs/2403.06814)

    引入了{\epsilon}-神经Thompson采样在帕金森病治疗领域中，通过解决传统DBS设备的能源效率和副作用问题，实现了自适应DBS的目标

    

    深度脑部刺激(DBS)作为缓解帕金森病(PD)运动症状的有效干预手段。传统的商用DBS设备仅能向大脑基底神经节(BG)区域提供固定频率的周期脉冲，即连续DBS(cDBS)。然而，它们普遍存在能源效率低和副作用（例如言语障碍）等问题。最近的研究集中在自适应DBS(aDBS)上，以解决cDBS的局限性。具体而言，基于强化学习(RL)的方法已经被开发用来调整刺激的频率，以达到既能源效率又能治疗有效的目的。然而，RL方法通常需要大量的训练数据和计算资源，使得将RL策略整合到实时嵌入式系统中变得难以实现，而这是aDBS所需要的。相比之下，上下文多臂老虎机(CMAB)通常会导致更好的结果

    arXiv:2403.06814v1 Announce Type: new  Abstract: Deep Brain Stimulation (DBS) stands as an effective intervention for alleviating the motor symptoms of Parkinson's disease (PD). Traditional commercial DBS devices are only able to deliver fixed-frequency periodic pulses to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS). However, they in general suffer from energy inefficiency and side effects, such as speech impairment. Recent research has focused on adaptive DBS (aDBS) to resolve the limitations of cDBS. Specifically, reinforcement learning (RL) based approaches have been developed to adapt the frequencies of the stimuli in order to achieve both energy efficiency and treatment efficacy. However, RL approaches in general require significant amount of training data and computational resources, making it intractable to integrate RL policies into real-time embedded systems as needed in aDBS. In contrast, contextual multi-armed bandits (CMAB) in general lead to bet
    
[^33]: 单调个体公平性

    Monotone Individual Fairness

    [https://arxiv.org/abs/2403.06812](https://arxiv.org/abs/2403.06812)

    该论文提出了一个新的在线学习框架，通过单调聚合函数实现了一种个体公平性审计方案，可以有效降低多个审计员对个体公平性的分析复杂度。

    

    我们重新审视了带有个体公平性的在线学习问题，其中在线学习者努力在最大化预测准确性的同时确保相似个体受到类似对待。我们首先扩展了Gillen等人（2018年）；Bechavod等人（2020年）的框架，这些框架依赖于来自人类审计员有关公平性违规的反馈，因为我们考虑了能够聚合任意数量审计员反馈的审计方案，使用了我们称为单调聚合函数的丰富类别。然后，我们证明了这种审计方案的特性，通过实际将多个审计员的个体公平性审计分析简化为（实例特定的）单个审计员的审计。利用我们的广义框架，我们提出了一个实现上界前沿的oracle-efficient算法，分别为遗憾值和公平性违规次数的上界$(\mathcal{O}(T^{1/2+2b}),\mathcal{O}(T^{3/4-b}))$，其中$0\leq b$

    arXiv:2403.06812v1 Announce Type: new  Abstract: We revisit the problem of online learning with individual fairness, where an online learner strives to maximize predictive accuracy while ensuring that similar individuals are treated similarly. We first extend the frameworks of Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human auditors regarding fairness violations, as we consider auditing schemes that are capable of aggregating feedback from any number of auditors, using a rich class we term monotone aggregation functions. We then prove a characterization for such auditing schemes, practically reducing the analysis of auditing for individual fairness by multiple auditors to that of auditing by (instance-specific) single auditors. Using our generalized framework, we present an oracle-efficient algorithm achieving an upper bound frontier of $(\mathcal{O}(T^{1/2+2b}),\mathcal{O}(T^{3/4-b}))$ respectively for regret, number of fairness violations, for $0\leq b
    
[^34]: 多步一致性模型

    Multistep Consistency Models

    [https://arxiv.org/abs/2403.06807](https://arxiv.org/abs/2403.06807)

    本文提出了多步一致性模型，通过在一致性模型和扩散模型之间插值，实现了采样速度和采样质量的平衡。

    

    扩散模型相对容易训练，但生成样本需要许多步骤。一致性模型更难训练，但可以在一个步骤中生成样本。本文提出了多步一致性模型：通过一致性模型和TRACT的统一，可以在一致性模型和扩散模型之间进行插值：在采样速度和采样质量之间取得平衡。具体来说，1步一致性模型是传统的一致性模型，而我们展示了$\infty$步一致性模型是扩散模型。多步一致性模型在实践中表现良好。将样本预算从单步增加到2-8步，我们可以更轻松地训练模型，生成更高质量的样本，同时保留大部分采样速度优势。在Imagenet 64上8步达到1.4的FID，在Imagenet128上8步达到2.1的FID。

    arXiv:2403.06807v1 Announce Type: new  Abstract: Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step.   In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas we show that a $\infty$-step consistency model is a diffusion model.   Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 
    
[^35]: 关于平均奖励马尔可夫决策过程中策略梯度的全局收敛性

    On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes

    [https://arxiv.org/abs/2403.06806](https://arxiv.org/abs/2403.06806)

    该论文首次证明了策略梯度算法在平均奖励MDPs中的收敛性，并获得了有限时间的性能保证。

    

    我们首次在无限时间间隔的平均奖励马尔可夫决策过程（MDP）的背景下，对策略梯度的有限时间全局收敛性进行了分析。具体而言，我们关注具有有限状态和动作空间的遍历标签MDP。我们的分析表明，策略梯度迭代以$O\left({\frac{1}{T}}\right)$的次线性速率收敛到最优策略，这对应于$O\left({\log(T)}\right)$的后悔，其中$T$表示迭代次数。之前关于折现奖励MDPs性能界的工作无法推广到平均奖励MDPs，因为界会随有效视距的五次方增长。因此，我们的主要贡献在于证明策略梯度算法在平均奖励MDPs中收敛，并得到有限时间的性能保证。与现有的折现奖励性能界相比，我们的性能界具有明确的依赖关系。

    arXiv:2403.06806v1 Announce Type: new  Abstract: We present the first finite time global convergence analysis of policy gradient in the context of infinite horizon average reward Markov decision processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite state and action spaces. Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O\left({\frac{1}{T}}\right),$ which translates to $O\left({\log(T)}\right)$ regret, where $T$ represents the number of iterations. Prior work on performance bounds for discounted reward MDPs cannot be extended to average reward MDPs because the bounds grow proportional to the fifth power of the effective horizon. Thus, our primary contribution is in proving that the policy gradient algorithm converges for average-reward MDPs and in obtaining finite-time performance guarantees. In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit depende
    
[^36]: 动态扰动自适应对抗训练在医学图像分类中的应用

    Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification

    [https://arxiv.org/abs/2403.06798](https://arxiv.org/abs/2403.06798)

    该论文提出了一种动态扰动自适应对抗训练（DPAAT）方法，旨在在提高稳健性的同时保持高泛化，通过将对抗训练（AT）置于动态学习环境中生成自适应的数据级扰动，并通过损失信息收集提供动态更新的标准，以提高医学图像分类网络的稳健性。

    

    最近在医学图像分类领域取得了显著的成功，主要是由于卷积神经网络（CNNs）的广泛应用。然而，对抗样本（AEs）与原始数据具有难以察觉的相似性，引发了人们对网络稳健性的严重关切。尽管对抗训练（AT）对应对恶意AEs被认为是一种提高稳健性的有效方法，但克服由AT引起的网络泛化下降仍具有挑战性。本文提出了一种动态扰动自适应对抗训练（DPAAT）方法，旨在在提高稳健性的同时保持高泛化，将AT放置在动态学习环境中以生成自适应的数据级扰动，并通过损失信息收集提供动态更新的标准，以处理传统AT方法中固定扰动大小的缺陷以及对外部依赖的问题。

    arXiv:2403.06798v1 Announce Type: cross  Abstract: Remarkable successes were made in Medical Image Classification (MIC) recently, mainly due to wide applications of convolutional neural networks (CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity with raw data, raising serious concerns on network robustness. Although adversarial training (AT), in responding to malevolent AEs, was recognized as an effective approach to improve robustness, it was challenging to overcome generalization decline of networks caused by the AT. In this paper, in order to reserve high generalization while improving robustness, we proposed a dynamic perturbation-adaptive adversarial training (DPAAT) method, which placed AT in a dynamic learning environment to generate adaptive data-level perturbations and provided a dynamically updated criterion by loss information collections to handle the disadvantage of fixed perturbation sizes in conventional AT methods and the dependence on extern
    
[^37]: 利用模型的内部表示进行磁图像分类

    Leveraging Internal Representations of Model for Magnetic Image Classification

    [https://arxiv.org/abs/2403.06797](https://arxiv.org/abs/2403.06797)

    该研究利用深度学习内部表示，针对只有单个磁图像及其标签可用的情况，提出了一种新的机器学习模型训练范式，旨在解决数据稀缺性问题。

    

    由边缘设备生成的数据具有训练智能自主系统跨多个领域的潜力。尽管出现了各种解决隐私问题并利用分布式数据的机器学习方法，由于数据碎片在不同位置的敏感存储，安全问题仍然存在。本文介绍了一种潜在的颠覆性的机器学习模型训练范式，专门设计用于只有单个磁图像及其相应标签图像可用的情况。我们利用深度学习的能力生成简洁而富有信息的样本，旨在克服数据稀缺性。通过利用深度学习的内部表示，我们的目标是有效解决数据稀缺性问题并产生有意义的结果。这种方法为以最少数据进行机器学习模型训练提供了一个有前途的途径。

    arXiv:2403.06797v1 Announce Type: new  Abstract: Data generated by edge devices has the potential to train intelligent autonomous systems across various domains. Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations. This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available. We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity. Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results. This methodology presents a promising avenue for training machine learning models with minimal data.
    
[^38]: 重新定义时间数据中的事件类型和群体演化

    Redefining Event Types and Group Evolution in Temporal Data

    [https://arxiv.org/abs/2403.06771](https://arxiv.org/abs/2403.06771)

    将事件类型重新定义为“原型”，其由一组称为“面向”的独特定量维度组合特征化，而不是强制执行严格的事件类型，从而允许涉及群体邻近多个原型的动态的混合描述。

    

    组群 - 如点的聚类或节点的社区 - 在处理各种数据挖掘任务时是基础的。在时间数据中，表征群体演化的主要方法一直是通过识别“事件”。然而，文献中通常描述的事件，例如收缩/增长、分裂/合并，往往是任意定义的，导致理论/预定义类型与真实数据群体观察之间存在差距。在超越现有分类的基础上，我们将事件视为由我们称为“面向”的一组特定定量维度的组合所特征化的“原型”。群体动态由它们在面向空间内的位置所定义，其中原型事件占据极端位置。因此，与强制执行严格的事件类型不同，我们的方法可以允许涉及到群体邻近多个原型的动态的混合描述。我们将我们的框架应用于来自多个实际数据集的群体演化。

    arXiv:2403.06771v1 Announce Type: new  Abstract: Groups -- such as clusters of points or communities of nodes -- are fundamental when addressing various data mining tasks. In temporal data, the predominant approach for characterizing group evolution has been through the identification of ``events". However, the events usually described in the literature, e.g., shrinks/growths, splits/merges, are often arbitrarily defined, creating a gap between such theoretical/predefined types and real-data group observations. Moving beyond existing taxonomies, we think of events as ``archetypes" characterized by a unique combination of quantitative dimensions that we call ``facets". Group dynamics are defined by their position within the facet space, where archetypal events occupy extremities. Thus, rather than enforcing strict event types, our approach can allow for hybrid descriptions of dynamics involving group proximity to multiple archetypes. We apply our framework to evolving groups from severa
    
[^39]: XB-MAML: 学习可扩展基参数以实现具有广泛任务覆盖范围的有效元学习

    XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage

    [https://arxiv.org/abs/2403.06768](https://arxiv.org/abs/2403.06768)

    XB-MAML引入了学习可扩展基参数的新方法，通过线性组合形成有效初始化，观察并利用基与微调参数在向量空间的差异，从而在多领域元学习中取得突破性进展。

    

    元学习追求有效的初始化模型，已成为处理未知任务的有前途方法。然而，当元学习器试图涵盖广泛的任务分布时，即跨不同数据集或领域进行学习时，一个限制仍然显而易见。最近，一组作品尝试使用多个模型初始化来覆盖广泛的任务，但它们在自适应扩展初始化方面受限。我们引入了XB-MAML，它学习可扩展的基本参数，其中它们线性组合以形成给定任务的有效初始化。XB-MAML观察基与微调参数张成的向量空间之间的差异，以决定是否扩展基础。我们的方法在多领域元学习基准测试中超越了现有作品，并为获取可以应用于不同领域的多样归纳偏差打开了新的机会。

    arXiv:2403.06768v1 Announce Type: new  Abstract: Meta-learning, which pursues an effective initialization model, has emerged as a promising approach to handling unseen tasks. However, a limitation remains to be evident when a meta-learner tries to encompass a wide range of task distribution, e.g., learning across distinctive datasets or domains. Recently, a group of works has attempted to employ multiple model initializations to cover widely-ranging tasks, but they are limited in adaptively expanding initializations. We introduce XB-MAML, which learns expandable basis parameters, where they are linearly combined to form an effective initialization to a given task. XB-MAML observes the discrepancy between the vector space spanned by the basis and fine-tuned parameters to decide whether to expand the basis. Our method surpasses the existing works in the multi-domain meta-learning benchmarks and opens up new chances of meta-learning for obtaining the diverse inductive bias that can be com
    
[^40]: 平均校准误差：一种可微损失函数，用于改善图像分割中的可靠性

    Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation

    [https://arxiv.org/abs/2403.06759](https://arxiv.org/abs/2403.06759)

    提出一种平均L1校准误差（mL1-ACE）作为辅助损失函数，用于改善图像分割中的像素级校准，减少了校准误差并引入了数据集可靠性直方图以提高校准评估。

    

    医学图像分割的深度神经网络经常产生与经验观察不一致的过于自信的结果，这种校准错误挑战着它们的临床应用。我们提出使用平均L1校准误差（mL1-ACE）作为一种新颖的辅助损失函数，以改善像素级校准而不会损害分割质量。我们展示了，尽管使用硬分箱，这种损失是直接可微的，避免了需要近似但可微的替代或软分箱方法的必要性。我们的工作还引入了数据集可靠性直方图的概念，这一概念推广了标准的可靠性图，用于在数据集级别聚合的语义分割中细化校准的视觉评估。使用mL1-ACE，我们将平均和最大校准误差分别降低了45%和55%，同时在BraTS 2021数据集上保持了87%的Dice分数。我们在这里分享我们的代码: https://github

    arXiv:2403.06759v1 Announce Type: cross  Abstract: Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: https://github
    
[^41]: Koopman合奏用于概率时间序列预测

    Koopman Ensembles for Probabilistic Time Series Forecasting

    [https://arxiv.org/abs/2403.06757](https://arxiv.org/abs/2403.06757)

    研究提出了一种Koopman合奏方法，通过训练模型合奏产生具有高模型间方差的预测，从而改善集成模型的不确定性量化。

    

    在数据驱动模型越来越受欢迎的背景下，最近提出了许多基于机器学习的Koopman算子的实现。然而，绝大多数这类研究仅限于确定性预测，而在像气象学和气象学这样的领域，知识的不确定性是至关重要的。在这项工作中，我们研究了训练模型合奏以产生随机输出。我们通过对真实遥感图像时间序列进行实验表明，独立训练模型的集成过分自信，并且使用明确鼓励成员生成具有高模型间方差的预测的训练标准极大改善了集成的不确定性量化。

    arXiv:2403.06757v1 Announce Type: new  Abstract: In the context of an increasing popularity of data-driven models to represent dynamical systems, many machine learning-based implementations of the Koopman operator have recently been proposed. However, the vast majority of those works are limited to deterministic predictions, while the knowledge of uncertainty is critical in fields like meteorology and climatology. In this work, we investigate the training of ensembles of models to produce stochastic outputs. We show through experiments on real remote sensing image time series that ensembles of independently trained models are highly overconfident and that using a training criterion that explicitly encourages the members to produce predictions with high inter-model variances greatly improves the uncertainty quantification of the ensembles.
    
[^42]: ALaRM: 通过分层奖励建模对齐语言模型

    ALaRM: Align Language Models via Hierarchical Rewards Modeling

    [https://arxiv.org/abs/2403.06754](https://arxiv.org/abs/2403.06754)

    ALaRM是第一个从人类反馈中建模分层奖励的框架，通过整合整体奖励与特定方面的奖励，改善了大型语言模型与人类偏好的对齐性，尤其在复杂文本生成任务中表现出更精确和一致的指导。

    

    我们介绍了ALaRM，第一个在强化学习中从人类反馈模型分层奖励的框架，旨在增强大型语言模型（LLMs）与人类偏好的对齐性。该框架解决了当前对齐方法的限制，这些方法通常难以处理人类监督信号的不一致性和稀疏性，通过将整体奖励与特定方面的奖励相结合。这种整合使得语言模型更加精确和一致地指导朝着期望的结果前进，尤其在复杂和开放的文本生成任务中。通过应用基于一致性的方法来过滤和组合多个奖励，该框架提供了一种可靠的机制来改善模型的对齐性。我们通过在长篇问题回答和机器翻译任务中使用gpt-3.5-turbo进行成对比较来验证我们的方法。

    arXiv:2403.06754v1 Announce Type: cross  Abstract: We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demon
    
[^43]: 通过任务无关的通信来泛化多Agent合作

    Generalising Multi-Agent Cooperation through Task-Agnostic Communication

    [https://arxiv.org/abs/2403.06750](https://arxiv.org/abs/2403.06750)

    通过预先训练的任务无关通信策略，在合作多机器人问题中实现了无需微调的任务泛化，支持更多Agent数量的扩展，并保证了收敛性。

    

    现有的合作多机器人问题中的多Agent强化学习（MARL）的通信方法几乎完全是特定于任务的，为每个独特的任务训练新的通信策略。我们通过引入一种适用于给定环境中任何任务的通信策略来解决这种低效问题。我们自监督地使用一组自动编码器预先训练通信策略，而不需要特定于任务的奖励指导。我们的目标是从可变数量的Agent观测中学习固定大小的潜在马尔可夫状态。在温和的假设下，我们证明使用我们的潜在表示的策略保证收敛，并对由我们的马尔可夫状态近似引入的值误差上界进行限制。我们的方法实现了对新任务的无缝适应，无需微调通信策略，优雅地支持比训练期间更多Agent，以及检测范

    arXiv:2403.06750v1 Announce Type: cross  Abstract: Existing communication methods for multi-agent reinforcement learning (MARL) in cooperative multi-robot problems are almost exclusively task-specific, training new communication strategies for each unique task. We address this inefficiency by introducing a communication strategy applicable to any task within a given environment. We pre-train the communication strategy without task-specific reward guidance in a self-supervised manner using a set autoencoder. Our objective is to learn a fixed-size latent Markov state from a variable number of agent observations. Under mild assumptions, we prove that policies using our latent representations are guaranteed to converge, and upper bound the value error introduced by our Markov state approximation. Our method enables seamless adaptation to novel tasks without fine-tuning the communication strategy, gracefully supports scaling to more agents than present during training, and detects out-of-di
    
[^44]: 医学图像分割中的捷径学习

    Shortcut Learning in Medical Image Segmentation

    [https://arxiv.org/abs/2403.06748](https://arxiv.org/abs/2403.06748)

    本研究将捷径学习现象扩展到医学图像分割领域，发现临床注释和特定数据处理方式可能误导模型并影响分割准确性，提出了缓解捷径学习影响的策略。

    

    捷径学习是一种现象，机器学习模型优先学习简单、潜在误导的数据提示，这些提示在训练集之外很难泛化。现有研究主要探讨这一现象在图像分类领域，而这项研究将捷径学习探索延伸到医学图像分割中。我们证明，临床注释如卡尺，以及数据集中零填充卷积和中心裁剪的组合可以无意中作为捷径，影响分割准确性。我们在两个不同但常见的医学图像分割任务中识别和评估了捷径学习。此外，我们提出了缓解捷径学习影响、提高分割模型泛化能力的策略。通过揭示医学图像分割中捷径的存在和影响，我们提供了一些见解。

    arXiv:2403.06748v1 Announce Type: cross  Abstract: Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights a
    
[^45]: 对核函数的近似方法

    On the Approximation of Kernel functions

    [https://arxiv.org/abs/2403.06731](https://arxiv.org/abs/2403.06731)

    本文提出了一种新的方法，通过考虑径向核函数的泰勒级数逼近，建立了对核函数的较好近似，证实了可以使用比文献中更小的正则化参数来实现更好的结果。

    

    统计学习中的各种方法都建立在再生核希尔伯特空间中考虑的核函数基础之上。在应用中，通常根据问题和数据的特征选择核函数。然后利用这个核函数推断那些没有观测到解释数据的点的响应变量。本文研究的数据位于高维空间中的紧致集合中，并解决了核函数本身的近似问题。新方法考虑了径向核函数的泰勒级数逼近。对于单位立方体上的高斯核函数，本文建立了关联特征函数的上限，这个上限随指数多项式增长。该新方法证明了比文献中考虑的更小的正则化参数，从而整体上实现更好的近似。这一改进证实了低秩逼近方法，如Nystrom方法。

    arXiv:2403.06731v1 Announce Type: cross  Abstract: Various methods in statistical learning build on kernels considered in reproducing kernel Hilbert spaces. In applications, the kernel is often selected based on characteristics of the problem and the data. This kernel is then employed to infer response variables at points, where no explanatory data were observed. The data considered here are located in compact sets in higher dimensions and the paper addresses approximations of the kernel itself. The new approach considers Taylor series approximations of radial kernel functions. For the Gauss kernel on the unit cube, the paper establishes an upper bound of the associated eigenfunctions, which grows only polynomially with respect to the index. The novel approach substantiates smaller regularization parameters than considered in the literature, overall leading to better approximations. This improvement confirms low rank approximation methods such as the Nystr\"om method.
    
[^46]: 长尾视觉识别的概率对比学习

    Probabilistic Contrastive Learning for Long-Tailed Visual Recognition

    [https://arxiv.org/abs/2403.06726](https://arxiv.org/abs/2403.06726)

    提出了一种新颖的概率对比（ProCo）学习算法，该算法估计特征空间中每个类别样本的数据分布，并对比对进行采样

    

    长尾分布经常出现在现实世界的数据中，其中大量少数类别包含有限数量的样本。这种不平衡问题严重影响了标准监督学习算法的性能，这些算法主要设计用于平衡的训练集。最近的研究表明，监督对比学习在缓解数据不平衡方面表现出有希望的潜力。然而，监督对比学习的性能受到固有挑战的困扰：它需要足够大批次的训练数据来构建涵盖所有类别的对比对，然而在类别不平衡数据的情况下很难满足这一要求。为克服这一障碍，我们提出了一种新颖的概率对比（ProCo）学习算法，该算法估计特征空间中每个类别样本的数据分布，并对比对进行采样。

    arXiv:2403.06726v1 Announce Type: new  Abstract: Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples. Such imbalance issue considerably impairs the performance of standard supervised learning algorithms, which are mainly designed for balanced training sets. Recent investigations have revealed that supervised contrastive learning exhibits promising potential in alleviating the data imbalance. However, the performance of supervised contrastive learning is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct contrastive pairs that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data. To overcome this obstacle, we propose a novel probabilistic contrastive (ProCo) learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples contrastive pa
    
[^47]: 通过监督预训练和重要性机制微调改进低资源知识追踪任务

    Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning

    [https://arxiv.org/abs/2403.06725](https://arxiv.org/abs/2403.06725)

    本文提出了名为LoReKT的低资源知识追踪框架，通过监督预训练和微调重要性机制，旨在从丰富资源的KT数据集中学习可转移的参数和表示来改进低资源知识追踪任务。

    

    知识追踪（KT）旨在基于学生的历史互动来估计他们的知识掌握程度。最近，基于深度学习的KT（DLKT）方法在KT任务中取得了令人印象深刻的表现。然而，由于各种原因，如预算限制和隐私问题，许多实际场景中观察到的互动非常有限，即低资源KT数据集。直接在低资源KT数据集上训练DLKT模型可能会导致过拟合，并且很难选择适当的深度神经架构。因此，在本文中，我们提出了一个名为LoReKT的低资源KT框架来应对上述挑战。受盛行的“预训练和微调”范式的启发，我们旨在在预训练阶段从丰富资源的KT数据集中学习可转移的参数和表示。

    arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
    
[^48]: 用Hodge-Laplacian和注意机制进展图神经网络：一种针对异构图结构数据的方法

    Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data

    [https://arxiv.org/abs/2403.06687](https://arxiv.org/abs/2403.06687)

    本研究提出了Hodge-Laplacian异构图注意网络（HL-HGAT），通过HL卷积滤波器、单纯投影和单纯注意池化运算符，可以学习跨k-单纯体的异构信号表示。

    

    图神经网络（GNNs）在捕捉图中节点之间的关系方面已被证明是有效的。本研究通过考虑将图作为一个单纯复合体的新视角，包含节点、边、三角形和k-单纯体，从而使得可以在任何k-单纯体上定义图结构数据。我们的贡献是Hodge-Laplacian异构图注意网络（HL-HGAT），旨在学习跨k-单纯体的异构信号表示。HL-HGAT包含三个关键组件：HL卷积滤波器（HL-filters）、单纯投影（SP）和单纯注意池化（SAP）运算符，应用于k-单纯体。HL-filters利用由Hodge-Laplacian（HL）算子编码的k-单纯体的独特拓扑结构，在k-th HL算子的频谱域内运作。为了解决计算挑战，我们介绍了一种HL-filters的多项式逼近，展示了在谱域解决异构图注意跨k-单纯体的计算挑战。

    arXiv:2403.06687v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have proven effective in capturing relationships among nodes in a graph. This study introduces a novel perspective by considering a graph as a simplicial complex, encompassing nodes, edges, triangles, and $k$-simplices, enabling the definition of graph-structured data on any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous graph attention network (HL-HGAT), designed to learn heterogeneous signal representations across $k$-simplices. The HL-HGAT incorporates three key components: HL convolutional filters (HL-filters), simplicial projection (SP), and simplicial attention pooling (SAP) operators, applied to $k$-simplices. HL-filters leverage the unique topology of $k$-simplices encoded by the Hodge-Laplacian (HL) operator, operating within the spectral domain of the $k$-th HL operator. To address computation challenges, we introduce a polynomial approximation for HL-filters, exhibiting spatia
    
[^49]: 流畅的黎曼领域：无环方差减少的高效黎曼优化

    Streamlining in the Riemannian Realm: Efficient Riemannian Optimization with Loopless Variance Reduction

    [https://arxiv.org/abs/2403.06677](https://arxiv.org/abs/2403.06677)

    引入了在黎曼优化中采用概率梯度计算触发器的Loopless SVRG和PAGE方法，简化了证明、提高了超参数选择效率，并提供了尖锐的收敛保证。

    

    在本研究中，我们研究了在黎曼流形上的随机优化，重点关注在欧几里得和黎曼设置中使用的关键方差减少机制。黎曼方差减少方法通常涉及双循环结构，在每个循环的开始计算完整梯度。确定最佳内循环长度在实践中具有挑战性，因为它取决于强凸性或光滑度常数，这些常数通常是未知的或难以估计。受欧几里得方法的启发，我们引入了无环黎曼SVRG（R-LSVRG）和PAGE（R-PAGE）方法。这些方法用每次迭代中的硬币翻转触发的概率梯度计算替换了外循环，确保了更简单的证明、高效的超参数选择和尖锐的收敛保证。将R-PAGE作为非凸黎曼优化的框架，我们展示了它在各种重要设置中的适用性。

    arXiv:2403.06677v1 Announce Type: cross  Abstract: In this study, we investigate stochastic optimization on Riemannian manifolds, focusing on the crucial variance reduction mechanism used in both Euclidean and Riemannian settings. Riemannian variance-reduced methods usually involve a double-loop structure, computing a full gradient at the start of each loop. Determining the optimal inner loop length is challenging in practice, as it depends on strong convexity or smoothness constants, which are often unknown or hard to estimate. Motivated by Euclidean methods, we introduce the Riemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods. These methods replace the outer loop with probabilistic gradient computation triggered by a coin flip in each iteration, ensuring simpler proofs, efficient hyperparameter selection, and sharp convergence guarantees. Using R-PAGE as a framework for non-convex Riemannian optimization, we demonstrate its applicability to various important settings. For ex
    
[^50]: 在隐私敏感领域中从联邦学习中有可证明的互惠益处

    Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains

    [https://arxiv.org/abs/2403.06672](https://arxiv.org/abs/2403.06672)

    本文研究了在隐私敏感领域中如何设计一种FL协议，既能保证隐私，又能提高模型准确性，并提供了设计出对所有参与者都有益处的协议。

    

    跨领域联邦学习（FL）允许数据所有者通过从彼此的私有数据集中获益来训练准确的机器学习模型。本文研究了在何时以及如何服务器可以设计一种对所有参与者都有利的FL协议的问题。我们提供了在均值估计和凸随机优化背景下存在相互有利协议的必要和充分条件。我们推导出了在对称隐私偏好下，最大化总客户效用的协议。最后，我们设计了最大化最终模型准确性的协议，并在合成实验中展示了它们的好处。

    arXiv:2403.06672v1 Announce Type: cross  Abstract: Cross-silo federated learning (FL) allows data owners to train accurate machine learning models by benefiting from each others private datasets. Unfortunately, the model accuracy benefits of collaboration are often undermined by privacy defenses. Therefore, to incentivize client participation in privacy-sensitive domains, a FL protocol should strike a delicate balance between privacy guarantees and end-model accuracy. In this paper, we study the question of when and how a server could design a FL protocol provably beneficial for all participants. First, we provide necessary and sufficient conditions for the existence of mutually beneficial protocols in the context of mean estimation and convex stochastic optimization. We also derive protocols that maximize the total clients' utility, given symmetric privacy preferences. Finally, we design protocols maximizing end-model accuracy and demonstrate their benefits in synthetic experiments.
    
[^51]: 解开高斯混合模型

    Untangling Gaussian Mixtures

    [https://arxiv.org/abs/2403.06671](https://arxiv.org/abs/2403.06671)

    这项研究在数据科学中探索了Tangles的潜力，将数据集中的簇与图中的Tangles联系起来，并开发了一套关于从高斯混合中抽取的数据集中Tangles的定量理论。

    

    Tangles最初是作为一种概念引入的，用于形式化图中的高连接区域。近年来，人们发现Tangles也是结构图论和数据科学之间的联系：当将数据集中的相似性解释为点之间的连接时，寻找数据中的簇基本上等同于在底层图中寻找Tangles。本文进一步探讨了数据集中Tangles的潜力，作为对簇的形式化研究。现实世界的数据通常遵循正态分布。考虑到这一点，我们开发了一套关于从高斯混合中抽取的数据集中Tangles的定量理论。为此，我们为数据赋予了一个模拟点之间相似性的图结构，并使我们能够将Tangle理论应用于数据。我们提供了明确的条件，根据这些条件，与边际高斯分布相关的Tangles几乎必然会渐近存在。

    arXiv:2403.06671v1 Announce Type: cross  Abstract: Tangles were originally introduced as a concept to formalize regions of high connectivity in graphs. In recent years, they have also been discovered as a link between structural graph theory and data science: when interpreting similarity in data sets as connectivity between points, finding clusters in the data essentially amounts to finding tangles in the underlying graphs. This paper further explores the potential of tangles in data sets as a means for a formal study of clusters. Real-world data often follow a normal distribution. Accounting for this, we develop a quantitative theory of tangles in data sets drawn from Gaussian mixtures. To this end, we equip the data with a graph structure that models similarity between the points and allows us to apply tangle theory to the data. We provide explicit conditions under which tangles associated with the marginal Gaussian distributions exist asymptotically almost surely. This can be consid
    
[^52]: PeerAiD：改善专业同行导师的对抗性蒸馏

    PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor

    [https://arxiv.org/abs/2403.06668](https://arxiv.org/abs/2403.06668)

    PeerAiD提出了一种新的对抗性蒸馏方法，通过让同行网络学习学生网络的对抗性示例，而不是自身的示例，来提升神经网络的鲁棒性。

    

    神经网络的对抗鲁棒性在应用于安全关键领域时是一个重要问题。在这种情况下，对抗性蒸馏是一种有前途的选择，旨在提炼教师网络的鲁棒性，以改进小型学生网络的鲁棒性。我们提出了PeerAiD，通过让同行网络学习学生网络的对抗性示例，而不是针对自身的对抗性示例，来改进对抗性蒸馏。PeerAiD是一种对抗性蒸馏，同时训练同行网络和学生网络。

    arXiv:2403.06668v1 Announce Type: new  Abstract: Adversarial robustness of the neural network is a significant concern when it is applied to security-critical domains. In this situation, adversarial distillation is a promising option which aims to distill the robustness of the teacher network to improve the robustness of a small student network. Previous works pretrain the teacher network to make it robust to the adversarial examples aimed at itself. However, the adversarial examples are dependent on the parameters of the target network. The fixed teacher network inevitably degrades its robustness against the unseen transferred adversarial examples which targets the parameters of the student network in the adversarial distillation process. We propose PeerAiD to make a peer network learn the adversarial examples of the student network instead of adversarial examples aimed at itself. PeerAiD is an adversarial distillation that trains the peer network and the student network simultaneousl
    
[^53]: Smart-Infinity: 使用真实系统上的近存储处理进行快速大型语言模型训练

    Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System

    [https://arxiv.org/abs/2403.06664](https://arxiv.org/abs/2403.06664)

    Smart-Infinity通过在真实系统上使用近存储处理设备，解决了存储外部化的大型语言模型训练中存储带宽瓶颈的问题

    

    大型语言模型（LLMs）的最新巨大进展主要是由参数数量的增加驱动的。这导致了对大量内存容量的需求，需要使用数十个GPU才能满足容量。一个常见的解决方案是存储外部训练，它使用主机内存和存储作为扩展内存层次结构。然而，这显然会以存储带宽瓶颈为代价，因为存储设备的带宽比GPU设备内存低几个数量级。我们的工作Smart-Infinity，通过在真实系统上使用近存储处理设备来解决存储外部化的LLM训练的存储带宽瓶颈。Smart-Infinity的主要组成部分是SmartUpdate，它在自定义近存储加速器上执行参数更新。我们发现将参数更新移到存储端可以消除大部分存储流量。此外，

    arXiv:2403.06664v1 Announce Type: cross  Abstract: The recent huge advance of Large Language Models (LLMs) is mainly driven by the increase in the number of parameters. This has led to substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity. One popular solution to this is storage-offloaded training, which uses host memory and storage as an extended memory hierarchy. However, this obviously comes at the cost of storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck of storage-offloaded LLM training using near-storage processing devices on a real system. The main component of Smart-Infinity is SmartUpdate, which performs parameter updates on custom near-storage accelerators. We identify that moving parameter updates to the storage side removes most of the storage traffic. In addition, we p
    
[^54]: 基于多模态学习和测试时临床知识增强的零样本心电图分类

    Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement

    [https://arxiv.org/abs/2403.06659](https://arxiv.org/abs/2403.06659)

    通过Multimodal ECG Representation Learning (MERL)框架，本文提出了一种零样本心电图分类方法，结合了对ECG记录和相关报告的多模态学习，同时在测试阶段使用了Clinical Knowledge Enhanced Prompt Engineering (CKEPE)方法来利用临床知识数据库。

    

    心电图（ECG）是临床实践中用于检测心律失常疾病的非侵入性诊断工具。在未经注释的ECG数据中进行自监督学习（eSSL）方法显示出了表征学习的潜力，但往往忽视了可以在报告中找到的临床知识。本文通过多模态学习ECG记录和相关报告，提出了Multimodal ECG Representation Learning (MERL)框架，该框架能够使用文本提示进行零样本ECG分类，消除了下游任务中对训练数据的需求。在测试时，我们提出了Clinical Knowledge Enhanced Prompt Engineering (CKEPE)方法，利用大型语言模型（LLM）来利用外部专家验证的临床知识数据库，生成更多关于患者病史的提示。

    arXiv:2403.06659v1 Announce Type: cross  Abstract: Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating mo
    
[^55]: 基于Ricci流的大脑表面协方差描述符用于阿尔茨海默病

    Ricci flow-based brain surface covariance descriptors for Alzheimer disease

    [https://arxiv.org/abs/2403.06645](https://arxiv.org/abs/2403.06645)

    本文首次提出了一种基于Ricci流的大脑表面协方差描述符的流水线，可以用于诊断阿尔茨海默病。

    

    从MRI大脑扫描中自动提取特征并诊断阿尔茨海默病是一个持续挑战。随着3D成像技术的进步，3D数据采集比其2D对应物更具可行性和效率。本文首次提出了一种流水线，从皮层表面利用Ricci能量优化提取新颖的基于协方差的描述符，而非使用基于特征的向量。这些协方差描述符是对称正定矩阵的非线性流形的组成部分，因此我们专注于使用高斯径向基函数将基于流形的分类应用于3D形状问题。将这一新颖特征应用于异常皮层脑形态学分析中，可以用于诊断阿尔茨海默病。在大约两百个3D MRI大脑模型上进行的实验研究，这些模型来自阿尔茨海默病神经影像学倡议 (ADNI) 数据集

    arXiv:2403.06645v1 Announce Type: cross  Abstract: Automated feature extraction from MRI brain scans and diagnosis of Alzheimer's disease are ongoing challenges. With advances in 3D imaging technology, 3D data acquisition is becoming more viable and efficient than its 2D counterpart. Rather than using feature-based vectors, in this paper, for the first time, we suggest a pipeline to extract novel covariance-based descriptors from the cortical surface using the Ricci energy optimization. The covariance descriptors are components of the nonlinear manifold of symmetric positive-definite matrices, thus we focus on using the Gaussian radial basis function to apply manifold-based classification to the 3D shape problem. Applying this novel signature to the analysis of abnormal cortical brain morphometry allows for diagnosing Alzheimer's disease. Experimental studies performed on about two hundred 3D MRI brain models, gathered from Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset dem
    
[^56]: 大象永远不会忘记：测试语言模型对表格数据的记忆能力

    Elephants Never Forget: Testing Language Models for Memorization of Tabular Data

    [https://arxiv.org/abs/2403.06644](https://arxiv.org/abs/2403.06644)

    本研究针对表格数据探讨了大型语言模型（LLMs）存在的数据污染和记忆化问题，发现LLMs在许多常见的表格数据集上进行了预训练，可能导致在下游任务中对性能评估的无效性。

    

    虽然许多人已经展示了大型语言模型（LLMs）如何应用于各种任务，但数据污染和记忆化的关键问题往往被忽视。在这项工作中，我们针对表格数据解决了这一问题。我们从简单的定性测试开始，测试LLM是否知道特征的名称和值，引入了各种不同的技术来评估污染程度，包括用于条件分布建模的统计测试和四个识别记忆化的测试。我们的调查发现，LLMs在许多流行的表格数据集上进行了预训练。这种暴露可能导致在下游任务上无效的性能评估，因为LLMs实际上已经适应了测试集。有趣的是，我们还确定了一种情况，在这种情况下，语言模型可以复制数据的重要统计信息，但无法逐字复制数据集。在这些数据集上，尽管见过...

    arXiv:2403.06644v1 Announce Type: cross  Abstract: While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Starting with simple qualitative tests for whether an LLM knows the names and values of features, we introduce a variety of different techniques to assess the degrees of contamination, including statistical tests for conditional distribution modeling and four tests that identify memorization. Our investigation reveals that LLMs are pre-trained on many popular tabular datasets. This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to the test set. Interestingly, we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim. On these datasets, although seen during 
    
[^57]: CO2在自然通风学校建筑物中用于占用检测的空间特征

    Spatial features of CO2 for occupancy detection in a naturally ventilated school building

    [https://arxiv.org/abs/2403.06643](https://arxiv.org/abs/2403.06643)

    通过空间CO2浓度分布提出两种新特征，使用支持向量机进行量化分析后发现，与基准相比，在自然通风房间中的占用状态检测准确率最多可提高14.8个百分点，达到83.2％（F1分数0.84）。有通风信息的情况下，准确率达到87.6％。

    

    精确的占用信息有助于提高建筑能源效率和居住者舒适度。基于CO2传感器的占用检测方法由于成本低、干扰小，受到关注。在自然通风建筑中，由于复杂的通风行为和测量窗户实际换气量的困难，基于CO2的占用检测准确性通常较低。本研究提出了基于CO2浓度的空间分布的两种新颖特征用于占用检测。通过使用支持向量机（SVM）作为分类器的量化分析，发现与基准相比，在没有任何通风信息的情况下，自然通风房间中的占用状态检测准确率可提高最多14.8个百分点，达到83.2％（F1分数0.84）。有通风信息的情况下，准确率达到87.6％（F1分数0. ．84）。

    arXiv:2403.06643v1 Announce Type: new  Abstract: Accurate occupancy information helps to improve building energy efficiency and occupant comfort. Occupancy detection methods based on CO2 sensors have received attention due to their low cost and low intrusiveness. In naturally ventilated buildings, the accuracy of CO2-based occupancy detection is generally low in related studies due to the complex ventilation behavior and the difficulty in measuring the actual air exchange through windows. In this study, we present two novel features for occupancy detection based on the spatial distribution of the CO2 concentration. After a quantitative analysis with Support Vector Machine (SVM) as classifier, it was found that the accuracy of occupancy state detection in naturally ventilated rooms could be improved by up to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1 score 0.84) without any ventilation information. With ventilation information, the accuracy reached 87.6 % (F1 s
    
[^58]: 评估在工业环境中物体检测中少样本学习的能效性

    Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings

    [https://arxiv.org/abs/2403.06631](https://arxiv.org/abs/2403.06631)

    本文研究了在工业环境中使用少样本学习进行物体检测的能效性，通过微调方法降低了能源消耗，并对模型在工业环境数据集上的能源需求进行了评估。

    

    在人工智能不断发展的时代，模型性能一直是驱动创新的关键指标，导致模型大小和复杂性呈指数级增长。然而，在当代工业环境中，可持续性和能效性一直是部署过程中的关键要求，因此需要使用少样本学习等数据高效方法。本文通过微调方法来减轻模型训练的负担，降低能源消耗，将标准物体检测模型调整到下游任务。随后，对开发的模型在不稳定工业环境中的物体检测基准数据集的能源需求进行了深入案例研究和评估。具体地，研究了不同的微调策略以及在训练过程中利用辅助评估数据，以及性能和能耗之间的权衡。

    arXiv:2403.06631v1 Announce Type: cross  Abstract: In the ever-evolving era of Artificial Intelligence (AI), model performance has constituted a key metric driving innovation, leading to an exponential growth in model size and complexity. However, sustainability and energy efficiency have been critical requirements during deployment in contemporary industrial settings, necessitating the use of data-efficient approaches such as few-shot learning. In this paper, to alleviate the burden of lengthy model training and minimize energy consumption, a finetuning approach to adapt standard object detection models to downstream tasks is examined. Subsequently, a thorough case study and evaluation of the energy demands of the developed models, applied in object detection benchmark datasets from volatile industrial environments is presented. Specifically, different finetuning strategies as well as utilization of ancillary evaluation data during training are examined, and the trade-off between perf
    
[^59]: 将对称黎曼几何应用于数据分析

    Pulling back symmetric Riemannian geometry for data analysis

    [https://arxiv.org/abs/2403.06612](https://arxiv.org/abs/2403.06612)

    对称黎曼几何可用于捕捉数据几何的非线性结构，并且能够将欧几里德空间上的数据分析工具有效推广到对称黎曼流形上。

    

    数据集往往存在于低维非线性子空间中。针对这种数据集的理想数据分析工具应考虑这种非线性几何。对称黎曼几何设置因多种原因而适用。首先，它具有丰富的数学结构，可以解释广泛范围的非线性几何，从经典非线性嵌入的经验证据中已经显示可以捕捉数据几何。其次，许多最初为欧几里德空间中的数据开发的标准数据分析工具也可以有效地推广到对称黎曼流形上的数据。概念上的挑战源自于缺乏构建数据空间本身上的对称黎曼结构的指导方针以及缺乏修改成功的算法在对称黎曼流形上进行数据分析的指导方针到此设置的问题。本工作考虑了这些挑战。

    arXiv:2403.06612v1 Announce Type: cross  Abstract: Data sets tend to live in low-dimensional non-linear subspaces. Ideal data analysis tools for such data sets should therefore account for such non-linear geometry. The symmetric Riemannian geometry setting can be suitable for a variety of reasons. First, it comes with a rich mathematical structure to account for a wide range of non-linear geometries that has been shown to be able to capture the data geometry through empirical evidence from classical non-linear embedding. Second, many standard data analysis tools initially developed for data in Euclidean space can also be generalised efficiently to data on a symmetric Riemannian manifold. A conceptual challenge comes from the lack of guidelines for constructing a symmetric Riemannian structure on the data space itself and the lack of guidelines for modifying successful algorithms on symmetric Riemannian manifolds for data analysis to this setting. This work considers these challenges in
    
[^60]: 分布生成增强公平面部属性分类

    Distributionally Generative Augmentation for Fair Facial Attribute Classification

    [https://arxiv.org/abs/2403.06606](https://arxiv.org/abs/2403.06606)

    该论文提出了一种新颖的分布生成增强方法，用于在偏见数据上训练公平的面部属性分类模型，无需额外标注虚假属性，通过生成模型识别潜在的虚假属性，并在图像空间中显示，以提高模型可解释性。

    

    面部属性分类（FAC）在广泛的应用中具有重要的潜力。然而，传统方法训练的FAC模型可能存在不公平性，因为在不同的数据子群体中展示准确性不一致。这种不公平性主要归因于数据中的偏见，其中一些虚假属性（例如，男性）在统计上与目标属性（例如，微笑）相关。大多数现有的注重公平性的方法依赖于虚假属性的标签，但这在实践中可能无法获得。本文提出了一种新颖的基于生成的两阶段框架，用于在带有偏见的数据上训练公平的FAC模型，而无需额外注释。首先，我们基于生成模型识别潜在的虚假属性。值得注意的是，通过明确展示图像空间中的虚假属性，它增强了可解释性。随后，对于每个图像，我们首先编辑虚假属性，随机抽样一定程度，

    arXiv:2403.06606v1 Announce Type: cross  Abstract: Facial Attribute Classification (FAC) holds substantial promise in widespread applications. However, FAC models trained by traditional methodologies can be unfair by exhibiting accuracy inconsistencies across varied data subpopulations. This unfairness is largely attributed to bias in data, where some spurious attributes (e.g., Male) statistically correlate with the target attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the labels of spurious attributes, which may be unavailable in practice. This work proposes a novel, generation-based two-stage framework to train a fair FAC model on biased data without additional annotation. Initially, we identify the potential spurious attributes based on generative models. Notably, it enhances interpretability by explicitly showing the spurious attributes in image space. Following this, for each image, we first edit the spurious attributes with a random degree sampled from
    
[^61]: ContextGPT: 将LLMs知识注入神经符号活动识别模型

    ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models

    [https://arxiv.org/abs/2403.06586](https://arxiv.org/abs/2403.06586)

    将预训练的大型语言模型（LLMs）的常识知识有效地注入神经符号活动识别模型，以缓解标记数据稀缺性问题。

    

    上下文感知人类活动识别（HAR）是移动计算中一个热门的研究领域，文献中最有效的解决方案基于监督式深度学习模型。然而，这些系统的实际部署受到需要用于训练的标记数据的稀缺性的限制。神经符号人工智能（NeSy）为缓解这一问题提供了一个有趣的研究方向，即将关于人类活动及其可能发生的背景的常识知识注入HAR深度学习分类器中。现有的用于上下文感知HAR的NeSy方法依赖于逻辑模型中编码的知识（例如本体论），其设计、实施和维护以捕捉新活动和上下文需要显著的人力工程努力、技术知识和领域专业知识。最近的研究表明，预训练的大型语言模型（LLMs）有效地编码了关于人类活动的常识知识。

    arXiv:2403.06586v1 Announce Type: cross  Abstract: Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human a
    
[^62]: FFAD：利用傅里叶变换和自动编码器评估生成的时间序列数据的新型度量标准

    FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing Fourier Transform and Auto-encoder

    [https://arxiv.org/abs/2403.06576](https://arxiv.org/abs/2403.06576)

    本论文提出了一种新的度量标准FFAD，利用傅里叶变换和自动编码器评估生成的时间序列数据质量，填补了时间序列数据评估领域的空白。

    

    深度学习生成模型在生成逼真图像、视频和音频方面取得成功，这引发了一个关键问题：如何有效评估合成样本的质量。虽然Fréchet Inception Distance（FID）被用作评估图像合成生成模型的标准度量，但在时间序列数据方面尚缺乏可比较的度量标准。本文提出了一种新颖解决方案，利用傅里叶变换和自动编码器，称为Fréchet Fourier-transform Auto-encoder Distance（FFAD），以应对这些与评估时间序列质量相关的挑战，特别是在Fréchet Distance的背景下。通过我们的实验结果，展示了FFAD在有效区分不同样本方面的潜力。

    arXiv:2403.06576v1 Announce Type: new  Abstract: The success of deep learning-based generative models in producing realistic images, videos, and audios has led to a crucial consideration: how to effectively assess the quality of synthetic samples. While the Fr\'{e}chet Inception Distance (FID) serves as the standard metric for evaluating generative models in image synthesis, a comparable metric for time series data is notably absent. This gap in assessment capabilities stems from the absence of a widely accepted feature vector extractor pre-trained on benchmark time series datasets. In addressing these challenges related to assessing the quality of time series, particularly in the context of Fr\'echet Distance, this work proposes a novel solution leveraging the Fourier transform and Auto-encoder, termed the Fr\'{e}chet Fourier-transform Auto-encoder Distance (FFAD). Through our experimental results, we showcase the potential of FFAD for effectively distinguishing samples from different
    
[^63]: 可扩展的在线探索方法：通过Coverability

    Scalable Online Exploration via Coverability

    [https://arxiv.org/abs/2403.06571](https://arxiv.org/abs/2403.06571)

    提出了探索目标框架，引入了$L_1$-覆盖度作为新的探索目标，支持内在复杂度控制、高效规划和灵活集成的优点。

    

    在强化学习中，探索是一个主要挑战，尤其对于需要函数逼近的高维领域。我们提出了探索目标——作为一个概念框架，能够使任何奖励函数的下游最大化成为可能。在这个框架内，我们引入了一个新的目标，即$L_1$-覆盖度，它泛化了以往的探索方案，并支持三个基本愿望：1.内在复杂度控制。$L_1$-覆盖度与结构参数$L_1$-Coverability相关联，反映了潜在MDP的内在统计困难度，包含Block和Low-Rank MDPs。2.高效规划。对于已知的MDP，优化$L_1$-覆盖度能够有效地降低到标准的策略优化，允许与诸如策略梯度和Q-learning等现成方法灵活集成。3.高效的探索。$L_1$-覆盖度的优化等同于现有强化学习算法的操作，尤其在高维领域中具有很强的泛化性。

    arXiv:2403.06571v1 Announce Type: new  Abstract: Exploration is a major challenge in reinforcement learning, especially for high-dimensional domains that require function approximation. We propose exploration objectives -- policy optimization objectives that enable downstream maximization of any reward function -- as a conceptual framework to systematize the study of exploration. Within this framework, we introduce a new objective, $L_1$-Coverage, which generalizes previous exploration schemes and supports three fundamental desiderata:   1. Intrinsic complexity control. $L_1$-Coverage is associated with a structural parameter, $L_1$-Coverability, which reflects the intrinsic statistical difficulty of the underlying MDP, subsuming Block and Low-Rank MDPs.   2. Efficient planning. For a known MDP, optimizing $L_1$-Coverage efficiently reduces to standard policy optimization, allowing flexible integration with off-the-shelf methods such as policy gradient and Q-learning approaches.   3. E
    
[^64]: 通过模型重编程增强假肢使用者的关节运动预测

    Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming

    [https://arxiv.org/abs/2403.06569](https://arxiv.org/abs/2403.06569)

    通过深度学习的重编程特性，研究者成功将原本设计用于健全人的模型改编为适用于截肢者预测关节运动。

    

    肢体丧失导致的行动障碍是全球数百万人面临的重大挑战。先进的辅助技术（如假肢设备）的开发有可能大大提高截肢患者的生活质量。设计这类技术的关键组成部分是准确预测缺失肢体的参考关节运动。然而，与大量来自健全受试者的数据形成对比，截肢患者关节运动数据的稀缺性阻碍了这一任务的完成。为了克服这一障碍，我们利用深度学习的重编程特性，无需改变模型参数即可重新利用训练有素的模型实现新目标。通过仅在数据级别进行操作，我们将原本设计用于健全人的模型改编为适用于截肢者预测关节运动。本研究结果对于推进辅助技术具有重要意义。

    arXiv:2403.06569v1 Announce Type: new  Abstract: Mobility impairment caused by limb loss is a significant challenge faced by millions of individuals worldwide. The development of advanced assistive technologies, such as prosthetic devices, has the potential to greatly improve the quality of life for amputee patients. A critical component in the design of such technologies is the accurate prediction of reference joint motion for the missing limb. However, this task is hindered by the scarcity of joint motion data available for amputee patients, in contrast to the substantial quantity of data from able-bodied subjects. To overcome this, we leverage deep learning's reprogramming property to repurpose well-trained models for a new goal without altering the model parameters. With only data-level manipulation, we adapt models originally designed for able-bodied people to forecast joint motion in amputees. The findings in this study have significant implications for advancing assistive tech a
    
[^65]: 揭开缩放定律之谜：第一部分

    Unraveling the Mystery of Scaling Laws: Part I

    [https://arxiv.org/abs/2403.06563](https://arxiv.org/abs/2403.06563)

    确认缩放定律原则在模型预训练中的重要作用，揭示OpenAI原始缩放定律论文的不完整细节，并探究预测测试损失轨迹可靠公式的挑战

    

    缩放定律原则表明在模型大小、数据集大小和训练过程中使用的计算资源等变量之间存在幂定律相关性。这些原则在优化模型预训练的各个方面中起着至关重要的作用，最终有助于大型语言模型（如GPT-4、Llama和Gemini）的成功。然而，OpenAI的原始缩放定律论文并未披露推导精确缩放定律公式所必需的完整细节，他们的结论仅基于包含高达15亿参数的模型。尽管一些后续作品试图揭示这些细节并扩展到更大的模型，但它们经常忽略了重要因素的训练依赖性，如学习速率、上下文长度和批量大小，导致它们未能建立一个可靠的预测测试损失轨迹的公式。在本技术报告中，我们确认了缩放

    arXiv:2403.06563v1 Announce Type: cross  Abstract: Scaling law principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training. These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of large language models such as GPT-4, Llama and Gemini. However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters. Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory. In this technical report, we confirm that the scaling 
    
[^66]: 切片-华塞斯坦距离及在卡坦-哈达玛德流形上的应用

    Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds

    [https://arxiv.org/abs/2403.06560](https://arxiv.org/abs/2403.06560)

    该论文在卡坦-哈达玛德流形上推导了切片-华塞斯坦距离的一般构造，提出了不同应用，并且推导了非参数方案以最小化这些新距离。

    

    尽管许多机器学习方法被开发或移植到黎曼流形上，以应对具有已知非欧几何结构的数据，在这些空间中对最优输运（Optimal Transport, OT）方法却没有得到太多关注。在欧几里得空间中，一种流行的替代方法是切片-华塞斯坦距离，该方法利用了一维华塞斯坦距离的封闭形式解决方案，但在流形上无法直接使用。在这项工作中，我们推导了卡坦-哈达玛德流形上切片-华塞斯坦距离的一般构造，这些流形是具有非正曲率的黎曼流形，其中包括了双曲空间或对称正定矩阵空间等。然后，我们提出了不同的应用。此外，我们推导了非参数方案来最小化这些新距离，通过近似它们的华塞斯坦距离。

    arXiv:2403.06560v1 Announce Type: new  Abstract: While many Machine Learning methods were developed or transposed on Riemannian manifolds to tackle data with known non Euclidean geometry, Optimal Transport (OT) methods on such spaces have not received much attention. The main OT tool on these spaces is the Wasserstein distance which suffers from a heavy computational burden. On Euclidean spaces, a popular alternative is the Sliced-Wasserstein distance, which leverages a closed-form solution of the Wasserstein distance in one dimension, but which is not readily available on manifolds. In this work, we derive general constructions of Sliced-Wasserstein distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive curvature, which include among others Hyperbolic spaces or the space of Symmetric Positive Definite matrices. Then, we propose different applications. Additionally, we derive non-parametric schemes to minimize these new distances by approximating their Wasserste
    
[^67]: 基于数据驱动的架构来编码机器人和人工角色的运动学信息

    Data-driven architecture to encode information in the kinematics of robots and artificial avatars

    [https://arxiv.org/abs/2403.06557](https://arxiv.org/abs/2403.06557)

    提出了一种数据驱动控制架构，用于编码机器人和人工角色的运动学信息，能够表达运动中的情绪信息。

    

    我们提出了一种数据驱动控制架构，用于修改机器人和人工角色的运动学，以编码特定信息，例如由人类操作员驱动的角色或机器人的运动中是否存在情绪。我们在一个拿取放置任务的抓取阶段实验数据集上验证了我们的方法。

    arXiv:2403.06557v1 Announce Type: cross  Abstract: We present a data-driven control architecture for modifying the kinematics of robots and artificial avatars to encode specific information such as the presence or not of an emotion in the movements of an avatar or robot driven by a human operator. We validate our approach on an experimental dataset obtained during the reach-to-grasp phase of a pick-and-place task.
    
[^68]: OMH: 通过最佳匹配层次实现无监督语义分割的结构化稀疏性

    OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation

    [https://arxiv.org/abs/2403.06546](https://arxiv.org/abs/2403.06546)

    本文提出一种名为Optimally Matched Hierarchy（OMH）的新方法，通过在特征空间上施加结构化稀疏性，实现了无监督语义分割任务中特征优化的层次匹配。

    

    无监督语义分割（USS）涉及在不依赖预定义标签的情况下对图像进行分割，旨在减轻广泛人工标记的负担。本文介绍了一种名为Optimally Matched Hierarchy（OMH）的新方法，用于同时解决上述问题。我们方法的核心在于在特征空间上施加结构化稀疏性，从而允许特征以不同粒度的信息进行编码。这种稀疏性的结构源自我们的层次结构（OMH）。

    arXiv:2403.06546v1 Announce Type: cross  Abstract: Unsupervised Semantic Segmentation (USS) involves segmenting images without relying on predefined labels, aiming to alleviate the burden of extensive human labeling. Existing methods utilize features generated by self-supervised models and specific priors for clustering. However, their clustering objectives are not involved in the optimization of the features during training. Additionally, due to the lack of clear class definitions in USS, the resulting segments may not align well with the clustering objective. In this paper, we introduce a novel approach called Optimally Matched Hierarchy (OMH) to simultaneously address the above issues. The core of our method lies in imposing structured sparsity on the feature space, which allows the features to encode information with different levels of granularity. The structure of this sparsity stems from our hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy among parallel cl
    
[^69]: ReStainGAN:利用IHC到IF染色域转换进行in-silico数据生成

    ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation

    [https://arxiv.org/abs/2403.06545](https://arxiv.org/abs/2403.06545)

    提出了一种新的方法，通过将形态特定的IHC染色分解为单独的图像通道，生成了in-silico免疫组织化学（IHC）图像，该方法在训练细胞核分割模型时在质量和数量上优于基线方法。

    

    arXiv:2403.06545v1 公告类型：交叉摘要：通过在计算病理学中扩展现有注释的实用性到具有不同染色模式的新领域中，可以创造in-silico数据集。因此，这有可能大幅降低构建训练监督深度学习模型所需的大型且像素精确的数据集的成本。我们提出了一种新颖方法，通过在免疫荧光（IF）图像中将形态特定的IHC染色分离成单独的图像通道，生成in-silico免疫组织化学（IHC）图像。所提出的方法在创建的in-silico数据集上通过训练细胞核分割模型在质量和数量上优于基线方法。

    arXiv:2403.06545v1 Announce Type: cross  Abstract: The creation of in-silico datasets can expand the utility of existing annotations to new domains with different staining patterns in computational pathology. As such, it has the potential to significantly lower the cost associated with building large and pixel precise datasets needed to train supervised deep learning models. We propose a novel approach for the generation of in-silico immunohistochemistry (IHC) images by disentangling morphology specific IHC stains into separate image channels in immunofluorescence (IF) images. The proposed approach qualitatively and quantitatively outperforms baseline methods as proven by training nucleus segmentation models on the created in-silico datasets.
    
[^70]: 分散和终身自适应多智能体协作学习

    Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning

    [https://arxiv.org/abs/2403.06535](https://arxiv.org/abs/2403.06535)

    本文提出了DeLAMA，一种具有动态协作图的分散多智能体终身协作学习算法，旨在通过自主识别协作关系和适应动态任务来增强多智能体之间的协作。

    

    分散和终身自适应多智能体协作学习旨在增强多个智能体之间的协作，不需要中央服务器，每个智能体随时间解决不同的任务。为了实现高效的协作，智能体应该：i) 在分散的方式下自主识别有益的协作关系；ii) 适应动态变化的任务观察。本文提出了DeLAMA，一种具有动态协作图的分散多智能体终身协作学习算法。为了促进自主协作关系学习，我们提出了一种分散的图结构学习算法，消除了对外部先验知识的需求。为了促进适应动态任务，我们设计了一个存储单元来捕获智能体积累的学习历史和知识，同时保持有限的存储消耗。为了进一步增强系统的表达能力和计算

    arXiv:2403.06535v1 Announce Type: cross  Abstract: Decentralized and lifelong-adaptive multi-agent collaborative learning aims to enhance collaboration among multiple agents without a central server, with each agent solving varied tasks over time. To achieve efficient collaboration, agents should: i) autonomously identify beneficial collaborative relationships in a decentralized manner; and ii) adapt to dynamically changing task observations. In this paper, we propose DeLAMA, a decentralized multi-agent lifelong collaborative learning algorithm with dynamic collaboration graphs. To promote autonomous collaboration relationship learning, we propose a decentralized graph structure learning algorithm, eliminating the need for external priors. To facilitate adaptation to dynamic tasks, we design a memory unit to capture the agents' accumulated learning history and knowledge, while preserving finite storage consumption. To further augment the system's expressive capabilities and computation
    
[^71]: SARDet-100K: 面向大规模合成孔径雷达 SAR 物体检测的开源基准和工具包

    SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection

    [https://arxiv.org/abs/2403.06534](https://arxiv.org/abs/2403.06534)

    SARDet-100K是第一个COCO级别的大规模多类别SAR物体检测数据集，为研究提供了大规模且多样化的数据集，揭示了SAR物体检测中预训练模型显著差异的关键挑战。

    

    面向合成孔径雷达（SAR）物体检测近来备受关注，因其不可替代的全天候成像能力。然而，这一研究领域面临着有限的公共数据集（主要包含 <2K 张图像，且仅包含单类别物体）和源代码不可访问的挑战。为解决这些问题，我们建立了一个新的基准数据集和一个针对大规模 SAR 物体检测的开源方法。我们的数据集 SARDet-100K 结果是对 10 个现有 SAR 检测数据集进行深入调研、收集和标准化的产物，为研究提供了一个大规模且多样化的数据集。据我们所知，SARDet-100K 是有史以来第一个达到 COCO 水平的大规模多类别 SAR 物体检测数据集。凭借这一高质量数据集，我们进行了全面实验，并揭示了 SAR 物体检测中一个关键挑战：预训练模型的显著差异。

    arXiv:2403.06534v1 Announce Type: cross  Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining
    
[^72]: 适应性空中联邦学习

    Adaptive Federated Learning Over the Air

    [https://arxiv.org/abs/2403.06528](https://arxiv.org/abs/2403.06528)

    提出了在空中模型训练框架内的自适应梯度方法的联邦版本，能够利用无线信道的叠加特性实现快速可伸缩的参数聚合，并通过动态调整步长增强了模型训练的稳健性。

    

    我们在空中模型训练框架内提出了一种自适应梯度方法的联邦版本，特别是AdaGrad和Adam。这种方法充分利用了无线信道的固有叠加特性，促进了快速可伸缩的参数聚合。同时，通过根据全局梯度更新动态调整步长，增强了模型训练过程的稳健性。我们推导了训练算法的收敛速率，包括信道衰落和干扰对各种非凸损失函数的影响。我们的分析表明，基于AdaGrad的算法以$\mathcal{O}( \ln{(T)} /{ T^{ 1 - \frac{1}{\alpha} } } )$的速率收敛到一个稳态点，其中$\alpha$表示电磁干扰的拖尾指数。这一结果表明，干扰分布的重尾特性水平起着关键作用。

    arXiv:2403.06528v1 Announce Type: new  Abstract: We propose a federated version of adaptive gradient methods, particularly AdaGrad and Adam, within the framework of over-the-air model training. This approach capitalizes on the inherent superposition property of wireless channels, facilitating fast and scalable parameter aggregation. Meanwhile, it enhances the robustness of the model training process by dynamically adjusting the stepsize in accordance with the global gradient update. We derive the convergence rate of the training algorithms, encompassing the effects of channel fading and interference, for a broad spectrum of nonconvex loss functions. Our analysis shows that the AdaGrad-based algorithm converges to a stationary point at the rate of $\mathcal{O}( \ln{(T)} /{ T^{ 1 - \frac{1}{\alpha} } } )$, where $\alpha$ represents the tail index of the electromagnetic interference. This result indicates that the level of heavy-tailedness in interference distribution plays a crucial role
    
[^73]: 基于全面运营成本奖励的深度强化学习自主卡车战术决策

    Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward

    [https://arxiv.org/abs/2403.06524](https://arxiv.org/abs/2403.06524)

    通过基于全面运营成本的奖励函数，采用深度强化学习框架优化自主卡车的战术决策，将高级决策与低级控制分离，并采用不同技巧提升性能。

    

    我们为自主卡车的战术决策制定了一个基于深度强化学习的框架，特别是针对高速公路场景中的自适应巡航控制（ACC）和变道动作。我们的研究结果表明，在强化学习代理和基于物理模型的低级控制器之间分离高级决策过程和低级控制动作是有益的。接下来，我们研究了通过在卡车的全面运营成本（TCOP）为基础的多目标奖励函数优化性能的不同方法；通过为奖励分量添加权重，通过对奖励分量进行归一化，以及使用课程学习技术。

    arXiv:2403.06524v1 Announce Type: cross  Abstract: We develop a deep reinforcement learning framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario. Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models. In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using curriculum learning techniques.
    
[^74]: 使用上下文无关文法自动生成Python程序

    Automatic Generation of Python Programs Using Context-Free Grammars

    [https://arxiv.org/abs/2403.06503](https://arxiv.org/abs/2403.06503)

    使用上下文无关文法自动生成Python程序的TinyPy Generator工具可以确保生成的程序的正确性，并能轻松生成大规模Python代码，尤其适用于机器学习领域。

    

    近年来，数据已经成为新的黄金，成为创建智能系统的强大工具。然而，获取高质量的数据仍然具有挑战性，尤其是对于代码。为了解决这个问题，我们开发了TinyPy Generator，这是一个使用上下文无关文法生成随机Python程序的工具。生成的程序通过构造保证正确性。我们的系统使用自定义的产生规则（采用巴科斯-诺尔范式（BNF）格式）递归地生成代码。这使我们能够生成具有不同复杂程度的代码，范围从仅包含赋值的代码到包含条件和循环的更复杂代码。我们提出的工具实现了轻松的大规模Python代码生成，在各种应用中具有益处。TinyPy Generator在机器学习领域特别有用，可以为训练Python语言模型生成大量Python代码。

    arXiv:2403.06503v1 Announce Type: cross  Abstract: In recent years, data has emerged as the new gold, serving as a powerful tool for creating intelligent systems. However, procuring high-quality data remains challenging, especially for code. To address this, we developed TinyPy Generator, a tool that generates random Python programs using a context-free grammar. The generated programs are guaranteed to be correct by construction. Our system uses custom production rules (in the Backus-Naur Form (BNF) format) to recursively generate code. This allows us to generate code with different levels of complexity, ranging from code containing only assignments to more complex code containing conditionals and loops. Our proposed tool enables effortless large-scale Python code generation, beneficial for a wide range of applications. TinyPy Generator is particularly useful in the field of machine learning, where it can generate substantial amounts of Python code for training Python language models. 
    
[^75]: 基于NML编码在离散、混合和连续变量中检测未观测到的共同原因

    Detection of Unobserved Common Causes based on NML Code in Discrete, Mixed, and Continuous Variables

    [https://arxiv.org/abs/2403.06499](https://arxiv.org/abs/2403.06499)

    提出一种新方法CLOUD，用于仅基于观测数据检测未观测到的共同原因，无需对未观测变量的方程模型形式做任何假设

    

    仅基于观测数据从因果发现中检测未观测到的共同原因是一个至关重要但具有挑战性的问题。我们将两个随机变量之间所有可能的因果关系归为四个类别，并旨在从观测数据中识别一个：直接因果关系存在的两种情况，变量相互独立的情况，以及变量被潜在混杂因素所混淆的情况。尽管已提出了解决这一问题的现有方法，但它们要求未观测到的变量满足其方程模型形式的假设。在我们之前的研究中（Kobayashi等，2022年），首次提出了一种无需这些假设的离散数据的因果发现方法，命名为CLOUD。使用归一化最大似然（NML）编码，CLOUD从一组候选模型中选择一个生成观测数据最小编码长度的模型。本文将CLOUD扩展到...

    arXiv:2403.06499v1 Announce Type: cross  Abstract: Causal discovery in the presence of unobserved common causes from observational data only is a crucial but challenging problem. We categorize all possible causal relationships between two random variables into the following four categories and aim to identify one from observed data: two cases in which either of the direct causality exists, a case that variables are independent, and a case that variables are confounded by latent confounders. Although existing methods have been proposed to tackle this problem, they require unobserved variables to satisfy assumptions on the form of their equation models. In our previous study (Kobayashi et al., 2022), the first causal discovery method without such assumptions is proposed for discrete data and named CLOUD. Using Normalized Maximum Likelihood (NML) Code, CLOUD selects a model that yields the minimum codelength of the observed data from a set of model candidates. This paper extends CLOUD to 
    
[^76]: 具有两个提升估计器的图神经网络用于标签稀缺的个体提升建模

    Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling

    [https://arxiv.org/abs/2403.06489](https://arxiv.org/abs/2403.06489)

    提出了一种基于图神经网络的框架，具有两个提升估计器，通过从社交图中学习提升估计来解决标签稀缺的个体提升建模问题

    

    提升建模旨在测量从随机实验或观测数据中的策略或行动对用户的增量效应，我们称之为提升。大多数现有的提升方法仅使用个体数据，这些数据通常不足以捕捉有关提升的未观察到和复杂的隐藏因素。此外，提升建模场景通常具有稀缺的标记数据，尤其是对于处理组，这也对模型训练构成了巨大挑战。考虑到邻居的特征和社会关系对于表征用户的提升非常有信息量，我们提出了一种基于图神经网络的框架，其中包括两个提升估计器，称为GNUM，以从社交图中学习提升估计。具体地，我们设计了基于类变换目标的第一个估计器。该估计器适用于所有类型的结果，并且能够全面地建模处理组

    arXiv:2403.06489v1 Announce Type: new  Abstract: Uplift modeling aims to measure the incremental effect, which we call uplift, of a strategy or action on the users from randomized experiments or observational data. Most existing uplift methods only use individual data, which are usually not informative enough to capture the unobserved and complex hidden factors regarding the uplift. Furthermore, uplift modeling scenario usually has scarce labeled data, especially for the treatment group, which also poses a great challenge for model training. Considering that the neighbors' features and the social relationships are very informative to characterize a user's uplift, we propose a graph neural network-based framework with two uplift estimators, called GNUM, to learn from the social graph for uplift estimation. Specifically, we design the first estimator based on a class-transformed target. The estimator is general for all types of outcomes, and is able to comprehensively model the treatment
    
[^77]: 大规模云系统中的知识感知警报聚合：一种混合方法

    Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach

    [https://arxiv.org/abs/2403.06485](https://arxiv.org/abs/2403.06485)

    提出了一种基于相关挖掘和大型语言模型推理的新型混合方法COLA，用于大规模云系统中的警报聚合，能够综合利用外部知识来解决基于语义相似性和统计方法的警报聚合的局限性。

    

    由于云系统的规模和复杂性，系统故障会引发“警报风暴”，即大量相关的警报。尽管这些警报可以追溯到少数几个根本原因，但压倒性的数量使人工处理变得不可行。因此，警报聚合对帮助工程师集中精力解决根本原因并促进故障解决至关重要。现有方法通常利用基于语义相似性的方法或统计方法来聚合警报。然而，基于语义相似性的方法忽视了警报的因果推理，而统计方法几乎无法处理不经常发生的警报。 为了解决这些限制，我们介绍了利用外部知识，即警报的标准操作规程（SOP）作为补充。我们提出了一种基于相关挖掘和LLM（Large Language Model）推理的在线警报聚合的新型混合方法COLA。

    arXiv:2403.06485v1 Announce Type: cross  Abstract: Due to the scale and complexity of cloud systems, a system failure would trigger an "alert storm", i.e., massive correlated alerts. Although these alerts can be traced back to a few root causes, the overwhelming number makes it infeasible for manual handling. Alert aggregation is thus critical to help engineers concentrate on the root cause and facilitate failure resolution. Existing methods typically utilize semantic similarity-based methods or statistical methods to aggregate alerts. However, semantic similarity-based methods overlook the causal rationale of alerts, while statistical methods can hardly handle infrequent alerts.   To tackle these limitations, we introduce leveraging external knowledge, i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation. The correlation mining modul
    
[^78]: 通过保留图神经网络进行金融违约预测的课程学习

    Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning

    [https://arxiv.org/abs/2403.06482](https://arxiv.org/abs/2403.06482)

    本文提出了一种 motif-preserving Graph Neural Network with curriculum learning (MotifGNN)模型，通过联合学习原始图的低阶结构和基于多视图图案的高阶结构，用于金融违约预测。

    

    用户的金融违约预测在信用风险预测和管理中起着至关重要的作用，旨在预测用户未来未能偿还贷款的概率。先前的方法主要提取用户个人特征，构建二分类模型进行违约预测，但对于信息有限的用户，这些方法无法得到满意的结果。本文通过提出一种保留子图模式的图神经网络与课程学习（MotifGNN），填补了这一缺口，共同学习原始图的低阶结构和基于多视图图案的子图的高阶结构，用于金融违约预测。

    arXiv:2403.06482v1 Announce Type: cross  Abstract: User financial default prediction plays a critical role in credit risk forecasting and management. It aims at predicting the probability that the user will fail to make the repayments in the future. Previous methods mainly extract a set of user individual features regarding his own profiles and behaviors and build a binary-classification model to make default predictions. However, these methods cannot get satisfied results, especially for users with limited information. Although recent efforts suggest that default prediction can be improved by social relations, they fail to capture the higher-order topology structure at the level of small subgraph patterns. In this paper, we fill in this gap by proposing a motif-preserving Graph Neural Network with curriculum learning (MotifGNN) to jointly learn the lower-order structures from the original graph and higherorder structures from multi-view motif-based graphs for financial default predict
    
[^79]: RL-MSA：基于强化学习的多线路公交车调度方法

    RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling Approach

    [https://arxiv.org/abs/2403.06466](https://arxiv.org/abs/2403.06466)

    RL-MSA提出了一种基于强化学习的多线路公交车调度方法，将多线路公交车调度问题建模为MDP，首次在离线阶段将直行车决策整合入公交车选择决策，有效简化学习问题，在线阶段通过时间窗口机制进行直行车决策。

    

    多线路公交车调度问题（MLBSP）对于节省公交公司运营成本和保证乘客服务质量至关重要。现有方法通常以离线方式生成公交车调度方案，然后根据该方案安排公交车。然而在实践中，诸如交通拥堵之类的不确定事件经常发生，这可能使事先确定的公交车调度方案变得不可行。本文将MLBSP建模为马尔科夫决策过程（MDP）。提出了一种基于强化学习的多线路公交车调度方法（RL-MSA），用于在离线和在线阶段进行公交车调度。在离线阶段，将直行车决策整合到首次出现的公交车选择决策中，以简化学习问题。在在线阶段，通过基于离线阶段学会的策略进行直行车决策，采用时间窗口机制。我们开发了几个新的有用状态特征包括

    arXiv:2403.06466v1 Announce Type: cross  Abstract: Multiple Line Bus Scheduling Problem (MLBSP) is vital to save operational cost of bus company and guarantee service quality for passengers. Existing approaches typically generate a bus scheduling scheme in an offline manner and then schedule buses according to the scheme. In practice, uncertain events such as traffic congestion occur frequently, which may make the pre-determined bus scheduling scheme infeasible. In this paper, MLBSP is modeled as a Markov Decision Process (MDP). A Reinforcement Learning-based Multi-line bus Scheduling Approach (RL-MSA) is proposed for bus scheduling at both the offline and online phases. At the offline phase, deadhead decision is integrated into bus selection decision for the first time to simplify the learning problem. At the online phase, deadhead decision is made through a time window mechanism based on the policy learned at the offline phase. We develop several new and useful state features includi
    
[^80]: 使用LSTM网络预测麦汁密度

    Prediction of Wort Density with LSTM Network

    [https://arxiv.org/abs/2403.06458](https://arxiv.org/abs/2403.06458)

    该论文介绍了一种使用LSTM网络预测麦汁密度的系统，通过廉价传感器测量值计算密度，帮助减少手动数据收集中的错误。

    

    技术过程中许多物理目标值很容易出错、繁琐或昂贵，无法自动测量。麦汁密度是一个物理目标值的例子，对于啤酒生产非常重要。本文介绍了一个系统，通过传感器帮助酿酒师测量麦汁密度，以减少手动数据收集中的错误。方法是使用压力或温度等廉价标准传感器测量到的值来计算密度，而不是直接测量麦汁密度。计算背后的模型是一个名为LSTM的神经网络。

    arXiv:2403.06458v1 Announce Type: new  Abstract: Many physical target values in technical processes are error-prone, cumbersome, or expensive to measure automatically. One example of a physical target value is the wort density, which is an important value needed for beer production. This article introduces a system that helps the brewer measure wort density through sensors in order to reduce errors in manual data collection. Instead of a direct measurement of wort density, a method is developed that calculates the density from measured values acquired by inexpensive standard sensors such as pressure or temperature. The model behind the calculation is a neural network, known as LSTM.
    
[^81]: 一种用于多维空间的学习索引综述

    A Survey of Learned Indexes for the Multi-dimensional Space

    [https://arxiv.org/abs/2403.06456](https://arxiv.org/abs/2403.06456)

    学习索引是将数据库索引结构视为机器学习模型，最近出现的趋势。本综述调查了学习多维索引结构的现状，分类了各种方法，并提出了分类方法。

    

    一种最新的研究趋势涉及将数据库索引结构视为机器学习（ML）模型。在这一领域中，单个或多个ML模型被训练来学习从键到数据集内部位置的映射。这类索引被称为“学习索引”。学习索引展示了对一维数据的改进搜索性能和减少空间需求。一维学习索引的概念自然被扩展到多维（例如，空间）数据，推动了“学习多维索引”的发展。这项调查侧重于学习多维索引结构。具体而言，它审查了这一研究领域的当前状态，解释了每种提出方法背后的核心概念，并根据几个明确定义的标准对这些方法进行分类。我们提出了一个对每个学习多维索引进行分类和归类的分类法，并对现有方法进行了调查。

    arXiv:2403.06456v1 Announce Type: cross  Abstract: A recent research trend involves treating database index structures as Machine Learning (ML) models. In this domain, single or multiple ML models are trained to learn the mapping from keys to positions inside a data set. This class of indexes is known as "Learned Indexes." Learned indexes have demonstrated improved search performance and reduced space requirements for one-dimensional data. The concept of one-dimensional learned indexes has naturally been extended to multi-dimensional (e.g., spatial) data, leading to the development of "Learned Multi-dimensional Indexes". This survey focuses on learned multi-dimensional index structures. Specifically, it reviews the current state of this research area, explains the core concepts behind each proposed method, and classifies these methods based on several well-defined criteria. We present a taxonomy that classifies and categorizes each learned multi-dimensional index, and survey the existi
    
[^82]: 人类大脑动态功能连接的自监督学习中的联合嵌入掩蔽自编码器

    Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain

    [https://arxiv.org/abs/2403.06432](https://arxiv.org/abs/2403.06432)

    提出了一种受到计算机视觉中 JEPA 架构启发的 Spatio-Temporal Joint Embedding Masked Autoencoder（ST-JEMA）用于动态功能连接的自监督学习。

    

    arXiv:2403.06432v1 通告类型: 新的 摘要: 图神经网络（GNNs）在学习动态功能连接方面表现出潜力，可以区分人脑网络中的表现型。然而，获得用于训练的大量标记临床数据通常具有资源密集性，这使得实际应用变得困难。因此，在标签稀缺设置中，利用未标记数据对于表示学习变得至关重要。尽管生成式自监督学习技术，特别是掩蔽自编码器，在各个领域的表示学习中展现出了有希望的结果，但它们在动态图形上的应用以及动态功能连接方面仍未得到充分探讨，面临着捕捉高级语义表示方面的挑战。在这里，我们介绍了时空联合嵌入掩蔽自编码器（ST-JEMA），受到计算机视觉中联合嵌入预测架构（JEPA）的启发。ST-JEMA采用了一种受JEPA启发的策略来重构

    arXiv:2403.06432v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting. Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations. Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructin
    
[^83]: 基于微分几何视角的图演化上图神经网络的可解释性研究

    A Differential Geometric View and Explainability of GNN on Evolving Graphs

    [https://arxiv.org/abs/2403.06425](https://arxiv.org/abs/2403.06425)

    提出了一种基于微分几何视角的方法，通过流形上的平滑曲线模拟图演化，实现了对图神经网络预测分布的可解释性。

    

    图在社交网络和生物化学中无处不在，图神经网络（GNN）是用于预测的最先进模型。图可能是演化的，形式化建模并理解训练后的GNN如何响应图演化至关重要。我们提出了使用公理归因对GNN预测分布进行平滑参数化，其中分布位于高维嵌入空间内的低维流形上。我们利用微分几何视角将分布演化建模为流形上的平滑曲线。我们重新参数化流形上的曲线族，并设计一个凸优化问题，以找到简洁地近似分布演化以供人类解释的唯一曲线。在节点分类、链接预测和图分类任务中进行了大量实验，这些任务涉及到演化的图，展示了更好的稀疏性、忠实度和直观性。

    arXiv:2403.06425v1 Announce Type: cross  Abstract: Graphs are ubiquitous in social networks and biochemistry, where Graph Neural Networks (GNN) are the state-of-the-art models for prediction. Graphs can be evolving and it is vital to formally model and understand how a trained GNN responds to graph evolution. We propose a smooth parameterization of the GNN predicted distributions using axiomatic attribution, where the distributions are on a low-dimensional manifold within a high-dimensional embedding space. We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold. We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation. Extensive experiments on node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better sparsity, faithfulness, and intui
    
[^84]: 通过大致共享特征来实现领域之间的桥梁

    Bridging Domains with Approximately Shared Features

    [https://arxiv.org/abs/2403.06424](https://arxiv.org/abs/2403.06424)

    提出了一种统计框架，根据特征与标签的相关性方差来区分特征的效用，并设计了一种学习过程，从源任务学习到大致共享的特征表示，并在目标任务上进行微调，以优化总体风险。

    

    多源领域适应旨在在将机器学习模型应用于未知领域时降低性能下降。一个基本挑战是设计特征选择的最佳策略。现有文献在某种程度上存在悖论：有些人主张从源领域学习不变特征，而另一些人则更青睐多样化特征。为了解决这一挑战，我们提出了一个统计框架，根据它们与标签 $y$ 的相关性的方差来区分特征的效用。在我们的框架下，我们设计和分析了一个学习过程，该过程由从源任务学习到的大致共享特征表示，并在目标任务上进行微调。我们的理论分析需要学习大致共享特征的重要性，而不仅仅是严格不变的特征，并且相对于以前关于源领域适应性的结果而言，产生了改进的总体风险。

    arXiv:2403.06424v1 Announce Type: cross  Abstract: Multi-source domain adaptation aims to reduce performance degradation when applying machine learning models to unseen domains. A fundamental challenge is devising the optimal strategy for feature selection. Existing literature is somewhat paradoxical: some advocate for learning invariant features from source domains, while others favor more diverse features. To address the challenge, we propose a statistical framework that distinguishes the utilities of features based on the variance of their correlation to label $y$ across domains. Under our framework, we design and analyze a learning procedure consisting of learning approximately shared feature representation from source tasks and fine-tuning it on the target task. Our theoretical analysis necessitates the importance of learning approximately shared features instead of only the strictly invariant features and yields an improved population risk compared to previous results on both sou
    
[^85]: RLingua：利用大型语言模型改善在机器人操作中的强化学习样本效率

    RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models

    [https://arxiv.org/abs/2403.06420](https://arxiv.org/abs/2403.06420)

    RLingua提出了一个框架，利用大型语言模型的内部知识来提高机器人操作中强化学习的样本效率。

    

    强化学习（RL）已经证明了其在解决各种任务中的能力，但以其低样本效率而声名狼藉。在本文中，我们提出了RLingua，这是一个可以利用大型语言模型（LLMs）的内部知识来减少机器人操作中RL的样本复杂性的框架。为此，我们首先介绍了如何通过提示工程提取LLMs的先验知识，从而生成特定任务的初步基于规则的机器人控制器。尽管不完美，LLM生成的机器人控制器被用于在rollout时以衰减概率生成动作样本，从而提高RL的样本效率。我们采用了演员-评论家框架，并修改了演员损失，以使策略学习朝着LLM生成的控制器规范化。RLingua还提供了一种改善不完美的LLM生成机器人控制器的新方法。我们展示了RLing

    arXiv:2403.06420v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present how to extract the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrated that RLing
    
[^86]: 联合环境中因果多标签特征选择

    Causal Multi-Label Feature Selection in Federated Setting

    [https://arxiv.org/abs/2403.06419](https://arxiv.org/abs/2403.06419)

    提出了在联合设置中进行因果多标签特征选择的挑战性问题，并提出了FedCMFS算法来解决该问题，算法通过三个新颖子程序，在不集中数据的情况下学习每个类标签的相关特征（候选父节点和子节点）。

    

    多标签特征选择作为处理高维多标签数据的有效手段。为了实现令人满意的性能，现有的多标签特征选择方法通常需要将来自多个源的大量数据进行集中。然而，在联合设置中，从所有来源集中数据并将其合并为单个数据集是不可行的。为了解决这个问题，在本文中，我们研究了联合环境中因果多标签特征选择的挑战性问题，并提出了一种具有三个新颖子程序的联合因果多标签特征选择（FedCMFS）算法。

    arXiv:2403.06419v1 Announce Type: new  Abstract: Multi-label feature selection serves as an effective mean for dealing with high-dimensional multi-label data. To achieve satisfactory performance, existing methods for multi-label feature selection often require the centralization of substantial data from multiple sources. However, in Federated setting, centralizing data from all sources and merging them into a single dataset is not feasible. To tackle this issue, in this paper, we study a challenging problem of causal multi-label feature selection in federated setting and propose a Federated Causal Multi-label Feature Selection (FedCMFS) algorithm with three novel subroutines. Specifically, FedCMFS first uses the FedCFL subroutine that considers the correlations among label-label, label-feature, and feature-feature to learn the relevant features (candidate parents and children) of each class label while preserving data privacy without centralizing data. Second, FedCMFS employs the FedCF
    
[^87]: 量化对大型语言模型的困难在哪里？基于扰动视角的实证研究

    What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation

    [https://arxiv.org/abs/2403.06408](https://arxiv.org/abs/2403.06408)

    量化对大型语言模型的困难主要表现在如何通过添加扰动来改善模型性能和效率的关系，研究通过对不同人为扰动进行实验，发现了扰动特性与模型性能之间的联系，提出了改善量化稳健性的潜在解决方案。

    

    量化已经成为提高大型语言模型（LLMs）内存和计算效率的一种有前途的技术。尽管性能和效率之间的权衡是众所周知的，但关于量化与LLM性能之间的关系仍有很多待探索。为了阐明这种关系，我们提出了一种量化新视角，将其视为添加到LLMs权重和激活上的扰动。我们称这种方法为“扰动视角”。利用这一视角，我们进行了各种人为扰动的实验，探讨它们对LLM性能的影响。我们的研究结果揭示了扰动的特性与LLM性能之间的几个联系，为均匀量化的失败案例提供了见解，并暗示了改善LLM量化稳健性的潜在解决方案。

    arXiv:2403.06408v1 Announce Type: cross  Abstract: Quantization has emerged as a promising technique for improving the memory and computational efficiency of large language models (LLMs). Though the trade-off between performance and efficiency is well-known, there is still much to be learned about the relationship between quantization and LLM performance. To shed light on this relationship, we propose a new perspective on quantization, viewing it as perturbations added to the weights and activations of LLMs. We call this approach "the lens of perturbation". Using this lens, we conduct experiments with various artificial perturbations to explore their impact on LLM performance. Our findings reveal several connections between the properties of perturbations and LLM performance, providing insights into the failure cases of uniform quantization and suggesting potential solutions to improve the robustness of LLM quantization. To demonstrate the significance of our findings, we implement a s
    
[^88]: 带有不确定性的余弦相似性打分用于神经语者嵌入

    Cosine Scoring with Uncertainty for Neural Speaker Embedding

    [https://arxiv.org/abs/2403.06404](https://arxiv.org/abs/2403.06404)

    该论文提出了一种处理神经语者嵌入中不确定性的方法，通过在嵌入前端估算不确定性并传播至余弦打分后端，实现了在说话者识别中降低错误率和最小DCF的改进。

    

    说话者表示中的不确定性建模旨在学习言语话语中存在的变异性。传统的余弦打分在说话者识别中计算效率高且普遍存在，但缺乏处理不确定性的能力。为了解决这一挑战，本文提出了一种方法，用于在说话者嵌入前端估算不确定性，并将其传播至余弦打分后端。在VoxCeleb和SITW数据集上进行的实验确认了所提方法在处理由嵌入估算引起的不确定性方面的功效。与传统余弦相似性相比，它在实验中实现了8.5％和9.8％的EER和minDCF平均降低。在实践中，它还具有计算效率。

    arXiv:2403.06404v1 Announce Type: cross  Abstract: Uncertainty modeling in speaker representation aims to learn the variability present in speech utterances. While the conventional cosine-scoring is computationally efficient and prevalent in speaker recognition, it lacks the capability to handle uncertainty. To address this challenge, this paper proposes an approach for estimating uncertainty at the speaker embedding front-end and propagating it to the cosine scoring back-end. Experiments conducted on the VoxCeleb and SITW datasets confirmed the efficacy of the proposed method in handling uncertainty arising from embedding estimation. It achieved improvement with 8.5% and 9.8% average reductions in EER and minDCF compared to the conventional cosine similarity. It is also computationally efficient in practice.
    
[^89]: 一刀切不适用：学习在文本分类中使用多少例为了改进上下文学习

    'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification

    [https://arxiv.org/abs/2403.06402](https://arxiv.org/abs/2403.06402)

    本文提出了自适应上下文学习（AICL）的工作流程，通过动态调整示例数量来提高文本分类的性能，类似于k最近邻（k-NN）中的可变大小邻域。

    

    arXiv:2403.06402v1 发表类型：新 Abstract: 自然语言处理（NLP）中的预测模型已经从从头训练模型发展到使用标记数据微调预训练模型。这种微调的极端形式涉及到上下文学习（ICL），其中一个预先训练的生成模型的输出（冻结的解码器参数）只受到输入字符串的变化（称为指令或提示）的控制。ICL的一个重要组成部分是在提示中使用少量标记数据实例作为示例。尽管现有工作在推理过程中为每个数据实例使用静态数量的示例，但在本文中，我们提出了一种动态调整示例数量的新方法。这类似于k最近邻（k-NN）分类器中使用可变大小邻域的方法。在我们提出的自适应ICL（AICL）的工作流程中，对于特定数据实例进行推理时使用的演示数量是动态调整的。

    arXiv:2403.06402v1 Announce Type: new  Abstract: Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data. An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts). An important component of ICL is the use of a small number of labelled data instances as examples in the prompt. While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data. This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data inst
    
[^90]: 关于持续学习中宽度递减回报的研究

    On the Diminishing Returns of Width for Continual Learning

    [https://arxiv.org/abs/2403.06398](https://arxiv.org/abs/2403.06398)

    增加神经网络宽度以减少遗忘会带来递减的回报，并且在先前研究中尚未探索的宽度范围内进行了实证验证。

    

    深度神经网络在各种设置中展示了突破性的性能，但这些模型在按顺序训练新任务时经常出现“灾难性遗忘”。 一些研究已经经验性地证明增加神经网络宽度会导致灾难性遗忘减少，但尚未准确刻画宽度和持续学习之间的确切关系。我们设计了其中一个最早的框架来分析持续学习理论，并证明宽度与前馈网络（FFN）中的遗忘直接相关。 具体来说，我们证明增加网络宽度以减少遗忘会带来递减的回报。我们在先前研究中尚未探索的宽度上经验性验证了我们的论断，结果显示递减回报如我们的理论所预测的那样清晰可见。

    arXiv:2403.06398v1 Announce Type: cross  Abstract: While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.
    
[^91]: DeepSafeMPC: 基于深度学习的安全多智体强化学习模型预测控制

    DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning

    [https://arxiv.org/abs/2403.06397](https://arxiv.org/abs/2403.06397)

    DeepSafeMPC是一种基于深度学习的模型预测控制方法，旨在有效预测多智体环境的复杂动态，并应用MARL原则寻找最优解。

    

    安全多智体强化学习（safe MARL）在最近几年逐渐受到关注，强调了智体不仅需要优化全局回报，还需要通过行为约束遵守安全要求的必要性。近期一些工作将控制理论与多智体强化学习相结合，以解决确保安全性的挑战。然而，在这一领域中应用模型预测控制（MPC）方法的应用非常有限，主要是由于多智体环境中复杂且隐式动态的特性。为弥合这一差距，我们提出了一种称为基于深度学习的安全多智体强化学习模型预测控制（DeepSafeMPC）的新方法。DeepSafeMPC 的关键见解是利用集中式深度学习模型很好地预测环境动态。我们的方法应用MARL原则来寻找最优解。

    arXiv:2403.06397v1 Announce Type: cross  Abstract: Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints. Some recent work has integrated control theory with multi-agent reinforcement learning to address the challenge of ensuring safety. However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments. To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics. Our method applies MARL principles to search for optimal solutions. Through the employme
    
[^92]: 通过尖锐度实现健壮的出域泛化界限

    Towards Robust Out-of-Distribution Generalization Bounds via Sharpness

    [https://arxiv.org/abs/2403.06392](https://arxiv.org/abs/2403.06392)

    本文研究了学习的极小值的尖锐度如何影响模型对领域转移中数据变化的容忍度，并建立了尖锐度和鲁棒性之间的严格联系，提出了基于尖锐度的OOD泛化界限，为鲁棒算法提供了更好的OOD保证。

    

    对于对于领域之外（OOD）数据或未见过的领域进行泛化，即OOD泛化，仍然缺乏适当的理论保证。传统的OOD界限关注源领域和目标领域之间的不同距离测量，但未考虑学习模型的优化特性。我们研究了学习的极小值的尖锐度对模型在领域转移中承受数据变化的影响，这通常由泛化中的"鲁棒性"来捕获。本文提出了尖锐度和鲁棒性之间的严格联系，为鲁棒算法提供了更好的OOD保证。论文提供了"平坦极小值导致更好的OOD泛化"的理论支持。总体而言，我们通过考虑鲁棒性提出了基于尖锐度的OOD泛化界限。

    arXiv:2403.06392v1 Announce Type: new  Abstract: Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, the sharpness of learned minima influences OOD generalization. To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by "robustness" in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms. It also provides a theoretical backing for "flat minima leads to better OOD generalization". Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, re
    
[^93]: 一种用于实现和防御发电网络中生成式AI攻击的零信任框架

    A Zero Trust Framework for Realization and Defense Against Generative AI Attacks in Power Grid

    [https://arxiv.org/abs/2403.06388](https://arxiv.org/abs/2403.06388)

    提出了一种针对电力网络的生成式AI攻击的零信任框架，可早期检测潜在的攻击向量、评估尾部风险的稳定性措施，并对此类威胁进行缓解。

    

    了解生成式AI（GenAI）攻击对电力网络的潜在影响是一个基本挑战，必须解决以保护电力网络免受新攻击向量的风险。本文提出了一个新颖的用于电力网络供应链（PGSC）的零信任框架，该框架有助于早期检测潜在的GenAI驱动攻击向量（例如重放和协议型攻击），评估基于尾部风险的稳定性措施，并减轻此类威胁。首先，设计并构建了PGSC的新零信任系统模型，将其形成为一个寻求通过实现和防御GenAI驱动的网络攻击来保证PGSC稳定的零信任问题。其次，开发了一种基于领域特定生成对抗网络（GAN）的攻击生成机制，以为进一步了解威胁而创建一个新的易受攻击的网络空间。第三，基于尾部风险的稳定性措施评估和减轻网络攻击。

    arXiv:2403.06388v1 Announce Type: cross  Abstract: Understanding the potential of generative AI (GenAI)-based attacks on the power grid is a fundamental challenge that must be addressed in order to protect the power grid by realizing and validating risk in new attack vectors. In this paper, a novel zero trust framework for a power grid supply chain (PGSC) is proposed. This framework facilitates early detection of potential GenAI-driven attack vectors (e.g., replay and protocol-type attacks), assessment of tail risk-based stability measures, and mitigation of such threats. First, a new zero trust system model of PGSC is designed and formulated as a zero-trust problem that seeks to guarantee for a stable PGSC by realizing and defending against GenAI-driven cyber attacks. Second, in which a domain-specific generative adversarial networks (GAN)-based attack generation mechanism is developed to create a new vulnerability cyberspace for further understanding that threat. Third, tail-based ri
    
[^94]: 针对下游微调的预训练模型推荐

    Pre-Trained Model Recommendation for Downstream Fine-tuning

    [https://arxiv.org/abs/2403.06382](https://arxiv.org/abs/2403.06382)

    本文提出了一个名为Fennec的框架，通过将所有模型和历史任务映射到一个迁移相关的子空间，以便推断新任务在迁移空间中的表征，从而改善模型选择技术。

    

    作为迁移学习中的一个基本问题，模型选择旨在对现成的预训练模型进行排名，并选择最适合新目标任务的模型。现有的模型选择技术通常在范围上受限，并倾向于忽视模型与任务之间微妙的关系。在本文中，我们提出了一个务实的框架 Fennec，深入研究了一个多样化、大规模的模型库，同时细致考虑了任务与模型之间的复杂联系。关键洞见在于将所有模型和历史任务映射到一个与迁移相关的子空间中，模型向量和任务向量之间的距离代表了可迁移性的大小。一个大型视觉模型作为代理人，在迁移空间中推断新任务的表示，从而避开了进行大量前向传播的计算负担。我们还调查了模型固有归纳偏差的影响。

    arXiv:2403.06382v1 Announce Type: cross  Abstract: As a fundamental problem in transfer learning, model selection aims to rank off-the-shelf pre-trained models and select the most suitable one for the new target task. Existing model selection techniques are often constrained in their scope and tend to overlook the nuanced relationships between models and tasks. In this paper, we present a pragmatic framework \textbf{Fennec}, delving into a diverse, large-scale model repository while meticulously considering the intricate connections between tasks and models. The key insight is to map all models and historical tasks into a transfer-related subspace, where the distance between model vectors and task vectors represents the magnitude of transferability. A large vision model, as a proxy, infers a new task's representation in the transfer space, thereby circumventing the computational burden of extensive forward passes. We also investigate the impact of the inherent inductive bias of models 
    
[^95]: FeatAug：从一对多关系表自动进行特征增强

    FeatAug: Automatic Feature Augmentation From One-to-Many Relationship Tables

    [https://arxiv.org/abs/2403.06367](https://arxiv.org/abs/2403.06367)

    FEATAUG是一种新的特征增强框架，能够从一对多关系表中自动提取具有谓词意识的SQL查询，弥补了Featuretools在许多实际场景中缺乏谓词的限制。

    

    arXiv:2403.06367v1 公告类型: 新的 摘要: 从一对多关系表中进行特征增强是机器学习模型开发中一个关键但具有挑战性的问题。为了增强好的特征，数据科学家需要手动提出 SQL 查询，这是耗时的。Featuretools [1] 是数据科学界广泛使用的工具，能够通过从相关表中提取新特征来自动增强训练数据。它将每个特征表示为在相关表上的分组聚合 SQL 查询，并能够自动生成这些 SQL 查询。然而，它未在这些查询中包含谓词，这在许多实际场景中显著限制了其应用。为了克服这一限制，我们提出了FEATAUG，一种新的特征增强框架，能够从一对多关系表中自动提取具有谓词意识的 SQL 查询。这样的扩展并不是微不足道的，因为考虑谓词会指数级增加候选查询的数量。

    arXiv:2403.06367v1 Announce Type: new  Abstract: Feature augmentation from one-to-many relationship tables is a critical but challenging problem in ML model development. To augment good features, data scientists need to come up with SQL queries manually, which is time-consuming. Featuretools [1] is a widely used tool by the data science community to automatically augment the training data by extracting new features from relevant tables. It represents each feature as a group-by aggregation SQL query on relevant tables and can automatically generate these SQL queries. However, it does not include predicates in these queries, which significantly limits its application in many real-world scenarios. To overcome this limitation, we propose FEATAUG, a new feature augmentation framework that automatically extracts predicate-aware SQL queries from one-to-many relationship tables. This extension is not trivial because considering predicates will exponentially increase the number of candidate que
    
[^96]: 软Q-learning的有限时间误差分析：切换系统方法

    Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach

    [https://arxiv.org/abs/2403.06366](https://arxiv.org/abs/2403.06366)

    本文通过切换系统模型，针对软Q-learning算法进行了有限时间误差分析，为两种软Q-learning算法导出了新颖的误差界限。

    

    Soft Q-learning是Q-learning的一种变体，旨在解决熵正则化马尔可夫决策问题，其中代理的目标是最大化熵正则化值函数。尽管在经验上取得成功，但迄今为止对软Q-learning的理论研究有限。本文旨在提供对软Q-learning算法的新颖和统一的有限时间、控制论分析。我们专注于两种类型的软Q-learning算法：一种利用对数和指数运算子，另一种采用玻尔兹曼运算子。通过使用动态切换系统模型，我们为两种软Q-learning算法推导出了新颖的有限时间误差界限。我们希望我们的分析能够通过与切换系统模型建立联系来加深对软Q-learning的当前理解，甚至为其他强化学习算法的有限时间分析的新框架铺平道路。

    arXiv:2403.06366v1 Announce Type: new  Abstract: Soft Q-learning is a variation of Q-learning designed to solve entropy regularized Markov decision problems where an agent aims to maximize the entropy regularized value function. Despite its empirical success, there have been limited theoretical studies of soft Q-learning to date. This paper aims to offer a novel and unified finite-time, control-theoretic analysis of soft Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one utilizing the log-sum-exp operator and the other employing the Boltzmann operator. By using dynamical switching system models, we derive novel finite-time error bounds for both soft Q-learning algorithms. We hope that our analysis will deepen the current understanding of soft Q-learning by establishing connections with switching system models and may even pave the way for new frameworks in the finite-time analysis of other reinforcement learning algorithms.
    
[^97]: 可分离的物理信息神经网络用于解决Boltzmann方程的BGK模型

    Separable Physics-informed Neural Networks for Solving the BGK Model of the Boltzmann Equation

    [https://arxiv.org/abs/2403.06342](https://arxiv.org/abs/2403.06342)

    本研究介绍了一种基于可分离的物理信息神经网络的方法，用于解决Boltzmann方程的BGK模型，在处理高维偏微分方程时能够显著减少计算成本，并通过引入高斯函数来改善对神经网络的训练。

    

    在这项研究中，我们介绍了一种基于可分离的物理信息神经网络（SPINNs）的方法，用于有效解决Boltzmann方程的BGK模型。尽管PINNs的无网格特性在处理高维偏微分方程（PDEs）方面具有显著优势，但在BGK操作符的积分精确评估时，应用求积规则会带来挑战，可能影响无网格优势并增加计算成本。为了解决这一问题，我们利用SPINNs的规范polyadic分解结构和矩计算的线性特性，实现了在求积规则应用中大幅减少计算开销。粒子密度函数的多尺度性质使得使用神经网络精确逼近宏观矩困难。为了改进SPINN训练，我们引入了高斯函数的积分到SPINNs中，并结合

    arXiv:2403.06342v1 Announce Type: cross  Abstract: In this study, we introduce a method based on Separable Physics-Informed Neural Networks (SPINNs) for effectively solving the BGK model of the Boltzmann equation. While the mesh-free nature of PINNs offers significant advantages in handling high-dimensional partial differential equations (PDEs), challenges arise when applying quadrature rules for accurate integral evaluation in the BGK operator, which can compromise the mesh-free benefit and increase computational costs. To address this, we leverage the canonical polyadic decomposition structure of SPINNs and the linear nature of moment calculation, achieving a substantial reduction in computational expense for quadrature rule application. The multi-scale nature of the particle density function poses difficulties in precisely approximating macroscopic moments using neural networks. To improve SPINN training, we introduce the integration of Gaussian functions into SPINNs, coupled with a
    
[^98]: 在多模式变分自动编码器中解开共享和私有潜在因素

    Disentangling shared and private latent factors in multimodal Variational Autoencoders

    [https://arxiv.org/abs/2403.06338](https://arxiv.org/abs/2403.06338)

    多模态变分自动编码器在解开共享和私有潜在因素方面的能力进行了研究，并提出了改进的方法以增强对特定模态变化的鲁棒性。

    

    多模态数据的生成模型可以确定可能与观测数据异质性的重要决定因素相关联的潜在因素。共享因素可以用于解释跨模态的变化，而其他因素可能是私有的，仅用于解释单个模态。多模态变分自动编码器，如MVAE和MMVAE，是推断这些潜在潜因素并将共享变异与私有变异分离的自然选择。在这项工作中，我们研究它们可靠执行此解缠的能力。特别是，我们强调了一个具有挑战性的问题设置，其中特定于模态的变异支配了共享信号。采用交叉模态预测的视角，我们展示了现有模型的局限性，并提出了一种修改方法，使它们对特定于模态的变异更加稳健。我们的发现得到了实验证明。

    arXiv:2403.06338v1 Announce Type: cross  Abstract: Generative models for multimodal data permit the identification of latent factors that may be associated with important determinants of observed data heterogeneity. Common or shared factors could be important for explaining variation across modalities whereas other factors may be private and important only for the explanation of a single modality. Multimodal Variational Autoencoders, such as MVAE and MMVAE, are a natural choice for inferring those underlying latent factors and separating shared variation from private. In this work, we investigate their capability to reliably perform this disentanglement. In particular, we highlight a challenging problem setting where modality-specific variation dominates the shared signal. Taking a cross-modal prediction perspective, we demonstrate limitations of existing models, and propose a modification how to make them more robust to modality-specific variation. Our findings are supported by experi
    
[^99]: 通过广义占有模型实现可迁移的强化学习

    Transferable Reinforcement Learning via Generalized Occupancy Models

    [https://arxiv.org/abs/2403.06328](https://arxiv.org/abs/2403.06328)

    通过广义占有模型，本研究提出了一种新颖的模型类别，保留了模型化强化学习的通用性，并避免了累积错误的问题。

    

    智能代理必须是通用的 - 具有快速适应和概括到不同任务的能力。在强化学习（RL）框架内，基于模型的RL算法学习世界的任务不可知动态模型，原则上使它们能够概括到任意奖励。然而，一步模型自然会受到累积错误的影响，使它们在具有长时间跨度和大状态空间的问题上失效。在这项工作中，我们提出了一类新型模型 - 广义占有模型（GOMs），保留了基于模型的RL的通用性，同时避免了累积性错误。GOMs的关键思想是在一个固定数据集的覆盖下，建模给定状态的所有可能长期结果的分布，以及实现给定状态的特定结果的策略。然后，这些模型可以迅速用于为任意新任务选择最优操作，而无需担心累积错误。

    arXiv:2403.06328v1 Announce Type: new  Abstract: Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without hav
    
[^100]: 从指令到约束：语言模型对齐与自动约束验证

    From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification

    [https://arxiv.org/abs/2403.06326](https://arxiv.org/abs/2403.06326)

    提出了ACT框架，通过约束验证器自动计算每个响应的约束满意率，实现语言模型对齐与自动约束验证。

    

    用户对齐对于将通用语言模型（LMs）调整为下游任务至关重要，但通常无法为所有类型的指令提供人类注释，特别是具有定制约束的指令。我们观察到用户指令通常包含约束条件。虽然评估整个指令的响应质量通常成本高昂，但高效地评估约束条件的满意率是可行的。我们研究了NLP任务中的常见约束条件，将它们基于其参数类型分类为三类，并提出了一个统一框架，ACT（Aligning to ConsTraints），用于自动为带约束用户对齐生成监督信号。具体而言，ACT使用约束验证器，这些验证器在实践中通常易于实现，来计算每个响应的约束满意率（CSR）。它为每个提示取样多个响应并收集偏好标签。

    arXiv:2403.06326v1 Announce Type: cross  Abstract: User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints. We observe that user instructions typically contain constraints. While assessing response quality in terms of the whole instruction is often costly, efficiently evaluating the satisfaction rate of constraints is feasible. We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints. Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response. It samples multiple responses for each prompt and collect preference labels ba
    
[^101]: 使用优化等价证明降低到标准强化学习中的风险敏感RL

    Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL

    [https://arxiv.org/abs/2403.06323](https://arxiv.org/abs/2403.06323)

    本研究通过将其降低为标准强化学习提出了两个通用的元算法，一个基于乐观算法，另一个基于策略优化，概括了以往的风险敏感强化学习理论并证实了新的理论在具有有界可覆盖性的MDP中的有效性。

    

    我们研究了具有优化等价证明（OCE）风险的风险敏感强化学习（RSRL），该风险概括了条件值风险（CVaR）、熵风险和马科维茨的均值-方差。通过增强马尔可夫决策过程（MDP），我们提出了两个通用的元算法，通过将其降低为标准RL：一个基于乐观算法，另一个基于策略优化。我们的乐观元算法概括了几乎所有之前RSRL理论，该理论使用熵风险或CVaR。在离散奖励下，我们的乐观理论还证明了具有有界可覆盖性的MDP（例如外生块MDP）的第一个RSRL遗憾上界。在离散奖励下，我们的策略优化元算法在一个新颖的度量中享有全局收敛性和局部改进保证，该度量下界为真实的OCE风险。最后，我们使用PPO实例化我们的框架，构建一个MDP，并展示它学习了最优的风险敏感。

    arXiv:2403.06323v1 Announce Type: new  Abstract: We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk (CVaR), entropic risk and Markowitz's mean-variance. Using an augmented Markov Decision Process (MDP), we propose two general meta-algorithms via reductions to standard RL: one based on optimistic algorithms and another based on policy optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL theory with entropic risk or CVaR. Under discrete rewards, our optimistic theory also certifies the first RSRL regret bounds for MDPs with bounded coverability, e.g., exogenous block MDPs. Under discrete rewards, our policy optimization meta-algorithm enjoys both global convergence and local improvement guarantees in a novel metric that lower bounds the true OCE risk. Finally, we instantiate our framework with PPO, construct an MDP, and show that it learns the optimal risk-sensitive
    
[^102]: 伪造还是被篡改? 理解联邦学习中的恶意客户

    Fake or Compromised? Making Sense of Malicious Clients in Federated Learning

    [https://arxiv.org/abs/2403.06319](https://arxiv.org/abs/2403.06319)

    该研究致力于澄清联邦学习中对恶意客户的混淆，通过提出混合对手模型来连接现有的对手模型，分析各种毒害攻击和防御聚合规则，从而为该领域的安全研究提供指导.

    

    联邦学习（FL）是一种分布式机器学习范例，可以在分散的数据上训练模型。在防毒攻击方面，FL安全领域存在混乱，因为有许多研究假设对手的能力和对手模型存在不同。我们的工作旨在通过对文献中提出的各种毒害攻击和防御聚合规则（AGRs）进行全面分析，并在一个共同框架下予以连接来澄清这种混乱。为了联结现有的对手模型，我们提出了一个混合对手模型，位于对手光谱的中间位置，对手会损害一些客户，使用他们受损的样本训练生成（例如DDPM）模型，并生成新的合成数据以解决更强（例如更便宜，更实际）攻击的优化问题。

    arXiv:2403.06319v1 Announce Type: new  Abstract: Federated learning (FL) is a distributed machine learning paradigm that enables training models on decentralized data. The field of FL security against poisoning attacks is plagued with confusion due to the proliferation of research that makes different assumptions about the capabilities of adversaries and the adversary models they operate under. Our work aims to clarify this confusion by presenting a comprehensive analysis of the various poisoning attacks and defensive aggregation rules (AGRs) proposed in the literature, and connecting them under a common framework. To connect existing adversary models, we present a hybrid adversary model, which lies in the middle of the spectrum of adversaries, where the adversary compromises a few clients, trains a generative (e.g., DDPM) model with their compromised samples, and generates new synthetic data to solve an optimization for a stronger (e.g., cheaper, more practical) attack against differe
    
[^103]: 通过人类反应研究人机交互中的领域泛化用于故障检测

    A Study on Domain Generalization for Failure Detection through Human Reactions in HRI

    [https://arxiv.org/abs/2403.06315](https://arxiv.org/abs/2403.06315)

    该研究通过分析使用人类面部表情训练的故障检测模型中的领域泛化，并发现在不同数据集上性能显著下降。对于HRI研究，这强调了改进模型稳健性和实际适用性的重要性。

    

    机器学习模型通常在分布内（相同数据集）进行测试；性能几乎总是在分布外设置下降。对于HRI研究，目标往往是开发广义模型。这使得领域泛化 - 在不同设置中保持性能 - 成为一个关键问题。在这项研究中，我们对在人类面部表情上训练的故障检测模型中的领域泛化进行了简明分析。我们使用两个不同的数据集，包括一个来自受控实验室环境的数据集和另一个在线收集的数据集，这些数据集展示了人类对发生错误的视频作出的反应。我们在每个数据集上训练了深度学习模型。当在交替数据集上测试这些模型时，我们观察到了显著的性能下降。我们反思了观察到的模型行为的原因并提出建议。这项工作强调了HRI研究需要关注改进模型的稳健性和实际适用性的需求。

    arXiv:2403.06315v1 Announce Type: cross  Abstract: Machine learning models are commonly tested in-distribution (same dataset); performance almost always drops in out-of-distribution settings. For HRI research, the goal is often to develop generalized models. This makes domain generalization - retaining performance in different settings - a critical issue. In this study, we present a concise analysis of domain generalization in failure detection models trained on human facial expressions. Using two distinct datasets of humans reacting to videos where error occurs, one from a controlled lab setting and another collected online, we trained deep learning models on each dataset. When testing these models on the alternate dataset, we observed a significant performance drop. We reflect on the causes for the observed model behavior and leave recommendations. This work emphasizes the need for HRI research focusing on improving model robustness and real-life applicability.
    
[^104]: 深度强化学习的最优策略稀疏化和低秩分解

    Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning

    [https://arxiv.org/abs/2403.06313](https://arxiv.org/abs/2403.06313)

    提出一种新颖的$L_0$范数正则化技术，用于深度强化学习的最优策略稀疏化和低秩分解

    

    深度强化学习在计算机游戏和机器人等多个领域显示出巨大潜力。然而，训练深度强化学习策略耗费了大量的计算资源，导致密集策略容易过拟合。此外，使用密集深度强化学习策略进行推理限制了它们在边缘计算等实际应用中的适用性。为了限制过拟合和减少内存消耗，研究者已经使用了像剪枝和奇异值分解这样的技术来对深度学习模型进行稀疏化和模型压缩。然而，这些技术导致了性能次优，在奖励方面出现显著的减弱。在神经网络稀疏化和稀疏自编码器开发中已经提出了$L_1$和$L_2$正则化技术，但它们在深度强化学习环境中的实现尚不明显。我们提出了一种新颖的$L_0$范数正则化技术，使用了一种最优稀疏度

    arXiv:2403.06313v1 Announce Type: cross  Abstract: Deep reinforcement learning(DRL) has shown significant promise in a wide range of applications including computer games and robotics. Yet, training DRL policies consume extraordinary computing resources resulting in dense policies which are prone to overfitting. Moreover, inference with dense DRL policies limit their practical applications, especially in edge computing. Techniques such as pruning and singular value decomposition have been used with deep learning models to achieve sparsification and model compression to limit overfitting and reduce memory consumption. However, these techniques resulted in sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$ regularization techniques have been proposed for neural network sparsification and sparse auto-encoder development, but their implementation in DRL environments has not been apparent. We propose a novel $L_0$-norm-regularization technique using an optimal sparsity m
    
[^105]: 你需要多少数据？第二部分：预测深度学习类特定训练数据集大小

    How much data do you need? Part 2: Predicting DL class specific training dataset sizes

    [https://arxiv.org/abs/2403.06311](https://arxiv.org/abs/2403.06311)

    通过考虑每个类别的训练样本数量，而不仅仅是总体训练样本数量，来预测机器学习分类模型性能，并提出了一种基于空间填充设计的算法，可以对 CIFAR10 和 EMNIST 数据集进行应用。

    

    本文旨在研究在考虑每个类别的训练样本数量而不仅仅是总体训练样本数量时，预测机器学习分类模型性能的问题。这带来了一个组合问题，即在给定固定总体训练数据集大小的情况下，应考虑哪些每个类的训练样本数量组合。为了解决这个问题，提出了一种算法，该算法受到实验设计中的空间填满设计的特殊情况的启发。生成的数据使用诸如幂律曲线和类似模型、扩展的广义线性模型等模型来进行建模，即通过将总体训练数据集大小替换为给定标签类别的训练样本数量的参数化线性组合。该算法已应用于CIFAR10和EMNIST数据集。

    arXiv:2403.06311v1 Announce Type: new  Abstract: This paper targets the question of predicting machine learning classification model performance, when taking into account the number of training examples per class and not just the overall number of training examples. This leads to the a combinatorial question, which combinations of number of training examples per class should be considered, given a fixed overall training dataset size. In order to solve this question, an algorithm is suggested which is motivated from special cases of space filling design of experiments. The resulting data are modeled using models like powerlaw curves and similar models, extended like generalized linear models i.e. by replacing the overall training dataset size by a parametrized linear combination of the number of training examples per label class. The proposed algorithm has been applied on the CIFAR10 and the EMNIST datasets.
    
[^106]: 非参数自动微分变分推断与样条逼近

    Nonparametric Automatic Differentiation Variational Inference with Spline Approximation

    [https://arxiv.org/abs/2403.06302](https://arxiv.org/abs/2403.06302)

    提出了一种基于样条的非参数逼近方法，实现了对复杂结构分布的灵活后验逼近，提高了生成模型性能。

    

    自动微分变分推断（ADVI）在学习概率模型方面很有效。经典ADVI依赖于参数化方法来逼近后验分布。本文提出了一种基于样条的非参数逼近方法，实现了对具有复杂结构的分布（如偏度、多峰性和有界支持）的灵活后验逼近。与广泛使用的非参数变分推断方法相比，所提出的方法易于实现，并适应各种数据结构。通过采用样条逼近，我们推导出了重要性加权自动编码器的一个下界，并确立了渐近一致性。实验证明了所提出方法在逼近复杂后验分布和改善具有不完全数据的生成模型性能方面的高效性。

    arXiv:2403.06302v1 Announce Type: cross  Abstract: Automatic Differentiation Variational Inference (ADVI) is efficient in learning probabilistic models. Classic ADVI relies on the parametric approach to approximate the posterior. In this paper, we develop a spline-based nonparametric approximation approach that enables flexible posterior approximation for distributions with complicated structures, such as skewness, multimodality, and bounded support. Compared with widely-used nonparametric variational inference methods, the proposed method is easy to implement and adaptive to various data structures. By adopting the spline approximation, we derive a lower bound of the importance weighted autoencoder and establish the asymptotic consistency. Experiments demonstrate the efficiency of the proposed method in approximating complex posterior distributions and improving the performance of generative models with incomplete data.
    
[^107]: 集群化联邦学习中总变差最小化的分析

    Analysis of Total Variation Minimization for Clustered Federated Learning

    [https://arxiv.org/abs/2403.06298](https://arxiv.org/abs/2403.06298)

    GTVMin在集群化联邦学习中的分析提供了关于解决统计异质性的有效性和鲁棒性的宝贵见解

    

    联邦学习应用中的一个关键挑战是本地数据集的统计异质性。集群化联邦学习通过识别大致同质的本地数据集群来解决这一挑战。最近的一种集群化联邦学习方法是广义总变差最小化（GTVMin）。该方法需要一个相似性图，可以通过领域专业知识或数据驱动的图学习技术来获得。在一个广泛适用的集群假设下，我们推导了GTVMin解与其按簇平均值之间的偏差的上界。这个界限为我们提供了关于GTVMin在解决联邦学习环境中的统计异质性的有效性和鲁棒性的宝贵见解。

    arXiv:2403.06298v1 Announce Type: new  Abstract: A key challenge in federated learning applications is the statistical heterogeneity of local datasets. Clustered federated learning addresses this challenge by identifying clusters of local datasets that are approximately homogeneous. One recent approach to clustered federated learning is generalized total variation minimization (GTVMin). This approach requires a similarity graph which can be obtained by domain expertise or in a data-driven fashion via graph learning techniques. Under a widely applicable clustering assumption, we derive an upper bound the deviation between GTVMin solutions and their cluster-wise averages. This bound provides valuable insights into the effectiveness and robustness of GTVMin in addressing statistical heterogeneity within federated learning environments.
    
[^108]: 理解和减轻监督对比学习中的人工标注误差

    Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning

    [https://arxiv.org/abs/2403.06289](https://arxiv.org/abs/2403.06289)

    本文揭示了人工标注误差不仅与合成标签错误有显著不同，而且在监督对比学习中构成了独特挑战，提出了一种新颖的对抗人工标注误差的SCL目标。

    

    通过arXiv:2403.06289v1公开的交叉类型的文摘可以得知，人工标注的视觉数据集中不可避免地包含一部分人工标注错误的示例。尽管这类标注错误对于监督学习的负面影响已经得到深入研究，但它们对监督对比学习（SCL）的影响仍然是相对未知的。本文表明，人工标注错误不仅与合成标签错误显著不同，而且在SCL中构成独特挑战，与传统监督学习方法中的挑战有所不同。具体而言，我们的研究结果表明，当它们作为误报样本出现时，它们会对学习过程造成大约99%的负面影响。已有的噪声缓解方法主要侧重于合成标签错误，并处理非常高合成噪声率（40-80%）的不切实际设置，但由于过度拟合，它们在普通图像数据集上的表现往往较差。为了解决这个问题，我们引入了一种新颖的对抗人工标注鲁棒性的SCL目标。

    arXiv:2403.06289v1 Announce Type: cross  Abstract: Human-annotated vision datasets inevitably contain a fraction of human mislabelled examples. While the detrimental effects of such mislabelling on supervised learning are well-researched, their influence on Supervised Contrastive Learning (SCL) remains largely unexplored. In this paper, we show that human-labelling errors not only differ significantly from synthetic label errors, but also pose unique challenges in SCL, different to those in traditional supervised learning methods. Specifically, our results indicate they adversely impact the learning process in the ~99% of cases when they occur as false positive samples. Existing noise-mitigating methods primarily focus on synthetic label errors and tackle the unrealistic setting of very high synthetic noise rates (40-80%), but they often underperform on common image datasets due to overfitting. To address this issue, we introduce a novel SCL objective with robustness to human-labelling
    
[^109]: 通过随机控制进行扩散模型的微调：熵正则化及更多

    Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond

    [https://arxiv.org/abs/2403.06279](https://arxiv.org/abs/2403.06279)

    本文致力于研究连续时间扩散模型中的熵正则化微调问题，并展示了分析如何扩展到涉及一般$f$-散度正则化的微调中。

    

    本文旨在发展并对连续时间扩散模型中熵正则化微调问题进行严格处理，该问题最近由上原等人提出。我们还展示了如何将分析扩展到涉及一般$f$-散度正则化的微调中。

    arXiv:2403.06279v1 Announce Type: cross  Abstract: This paper aims to develop and provide a rigorous treatment to the problem of entropy regularized fine-tuning in the context of continuous-time diffusion models, which was recently proposed by Uehara et al. ( arXiv:2402.15194, 2024). We also show how the analysis can be extended to fine-tuning involving a general $f$-divergence regularizer.
    
[^110]: UNICORN: 通过分数匹配和自适应实现的超声纳卡加米成像

    UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation

    [https://arxiv.org/abs/2403.06275](https://arxiv.org/abs/2403.06275)

    提出了一种名为UNICORN的新方法，通过分数匹配和自适应实现超声纳卡加米成像，能够在准确性和分辨率质量上超越传统方法。

    

    Nakagami成像在超声波中可视化和量化组织散射方面具有潜力，在肿瘤诊断和脂肪分数估计等领域有潜在应用，而这些领域很难通过传统超声B模式图像分辨。现有方法在选择最优窗口大小上存在困难，并且由于估计不稳定性而导致分辨率降低。为解决这一问题，本文提出了一种名为UNICORN（通过分数匹配和自适应实现的超声纳卡加米成像）的新方法，它提供了一种准确的，封闭形式的方法，用于通过超声包络的分数函数来估计卡加米参数。通过使用模拟和真实超声RF数据进行的大量实验证明了UNICORN在准确性和分辨率质量方面优于传统方法。

    arXiv:2403.06275v1 Announce Type: cross  Abstract: Nakagami imaging holds promise for visualizing and quantifying tissue scattering in ultrasound waves, with potential applications in tumor diagnosis and fat fraction estimation which are challenging to discern by conventional ultrasound B-mode images. Existing methods struggle with optimal window size selection and suffer from estimator instability, leading to degraded resolution images. To address this, here we propose a novel method called UNICORN (Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an accurate, closed-form estimator for Nakagami parameter estimation in terms of the score function of ultrasonic envelope. Extensive experiments using simulation and real ultrasound RF data demonstrate UNICORN's superiority over conventional approaches in accuracy and resolution quality.
    
[^111]: 物理引导的异常轨迹间隙检测

    Physics-Guided Abnormal Trajectory Gap Detection

    [https://arxiv.org/abs/2403.06268](https://arxiv.org/abs/2403.06268)

    通过物理引导的方法，解决了在轨迹数据中识别异常间隙的挑战性问题，具有重要的社会应用和技术难度。

    

    在具有间隙（即缺失数据）的轨迹中，我们研究了用于识别轨迹中的异常间隙的算法，这种异常间隙在一个移动物体没有报告其位置时发生，但同一地理区域的其他移动物体周期性地报告位置。该问题由于其对社会的重要应用而变得重要，如改善海上安全和对全球安全问题（如非法捕鱼、非法输油和转运活动）的监管执行。该问题具有挑战性，因为很难限定在轨迹间隙期间移动物体的可能位置，并且检测如此大量位置数据中的间隙的计算成本非常高。目前关于异常轨迹检测的文献假设在间隙内进行线性插值，这可能无法检测到异常间隙，因为在给定区域内的物体可能已经偏离最短路径。在初步

    arXiv:2403.06268v1 Announce Type: cross  Abstract: Given trajectories with gaps (i.e., missing data), we investigate algorithms to identify abnormal gaps in trajectories which occur when a given moving object did not report its location, but other moving objects in the same geographic region periodically did. The problem is important due to its societal applications, such as improving maritime safety and regulatory enforcement for global security concerns such as illegal fishing, illegal oil transfers, and trans-shipments. The problem is challenging due to the difficulty of bounding the possible locations of the moving object during a trajectory gap, and the very high computational cost of detecting gaps in such a large volume of location data. The current literature on anomalous trajectory detection assumes linear interpolation within gaps, which may not be able to detect abnormal gaps since objects within a given region may have traveled away from their shortest path. In preliminary 
    
[^112]: 拆解分词：评估文本压缩及其与模型性能的相关性

    Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance

    [https://arxiv.org/abs/2403.06265](https://arxiv.org/abs/2403.06265)

    本文研究了文本压缩在分词过程中的重要性，证明了压缩与预训练语言模型后续成功之间的实证重要性，并表明分词器的压缩与模型的性能存在相关性。

    

    尽管压缩是BPE最常见的分词算法的重要基础，但分词过程中的压缩重要性仍不清楚。本文论述了压缩的理论重要性，可以被看作是0-gram语言建模，即为所有标记分配相等的概率。我们还展示了压缩对预训练语言模型后续成功的实证重要性。我们通过改变训练过程中可用文档的数量来控制多个BPE分词器的压缩能力：从100万个文档到相当于没有训练数据的基于字符的分词器。然后，我们基于这些分词器预训练英语语言模型，并在多个任务上进行微调。我们展示了分词器的压缩与模型的后续性能之间存在相关性，表明压缩是分词的可靠内在指标

    arXiv:2403.06265v1 Announce Type: cross  Abstract: Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokeniza
    
[^113]: 大型语言模型的概念知识编辑

    Editing Conceptual Knowledge for Large Language Models

    [https://arxiv.org/abs/2403.06259](https://arxiv.org/abs/2403.06259)

    该论文首次研究了为大型语言模型编辑概念知识，通过构建基准数据集和建立新评估指标，发现现有方法虽然能一定程度上修改概念定义，但也可能造成LLMs中相关实例知识的扭曲，导致性能下降。

    

    最近，对于大型语言模型（LLMs）的知识编辑引起了越来越多的关注。当前的方法和评估仅探讨了实例级别的编辑，然而LLMs是否具有修改概念的能力仍不清楚。本文首次研究了为LLMs编辑概念知识，通过构建一个新颖的基准数据集ConceptEdit并建立了一套新的评估指标。实验结果表明，尽管现有的编辑方法可以有效地在一定程度上修改概念级别的定义，但它们也有潜力扭曲LLMs中相关的实例知识，导致性能不佳。我们期望这可以激发对更好理解LLMs的进一步进展。我们的项目主页位于https://zjunlp.github.io/project/ConceptEdit。

    arXiv:2403.06259v1 Announce Type: cross  Abstract: Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.
    
[^114]: 在线多光谱神经追踪

    Online Multi-spectral Neuron Tracing

    [https://arxiv.org/abs/2403.06251](https://arxiv.org/abs/2403.06251)

    提出了一种在线多光谱神经追踪方法，无需离线训练，可以快速并准确地在多光谱图像中重构神经。

    

    在本文中，我们提出了一种在线多光谱神经追踪方法，其具有独特设计的模块，无需离线训练。我们的方法通过在线训练来更新增强的判别性相关滤波器，以连接追踪过程。这种独特的无离线训练模式使我们与其他需要训练依赖的追踪方法（如深度学习方法）不同，因为我们的方法不需要注释。此外，与其他追踪方法相比，其他方法需要复杂的设置，例如用于聚类和图多切割，我们的方法更容易应用于新图像。事实上，它只需要一个追踪神经元的起始边界框，显著减少了用户的配置工作。我们广泛的实验证明，我们的无需训练和易配置的方法允许在多光谱图像中进行快速准确的神经重构。

    arXiv:2403.06251v1 Announce Type: cross  Abstract: In this paper, we propose an online multi-spectral neuron tracing method with uniquely designed modules, where no offline training are required. Our method is trained online to update our enhanced discriminative correlation filter to conglutinate the tracing process. This distinctive offline-training-free schema differentiates us from other training-dependent tracing approaches like deep learning methods since no annotation is needed for our method. Besides, compared to other tracing methods requiring complicated set-up such as for clustering and graph multi-cut, our approach is much easier to be applied to new images. In fact, it only needs a starting bounding box of the tracing neuron, significantly reducing users' configuration effort. Our extensive experiments show that our training-free and easy-configured methodology allows fast and accurate neuron reconstructions in multi-spectral images.
    
[^115]: 合作分类与理性化用于图泛化

    Cooperative Classification and Rationalization for Graph Generalization

    [https://arxiv.org/abs/2403.06239](https://arxiv.org/abs/2403.06239)

    本文提出了一种合作分类与理性化（C2R）方法，旨在解决图神经网络在泛化时面临的挑战，通过分类和理性化模块协同工作，改善对分布之外数据的泛化能力。

    

    arXiv:2403.06239v1 公告类型: 跨界 摘要: 图神经网络（GNNs）在图分类任务中取得了令人印象深刻的结果，但面对分布之外的数据时很难有效泛化。已经提出了几种方法来解决这个问题。其中之一的解决方案是通过修改数据环境来使原始分类的训练分布多样化，但访问环境信息较为复杂。另一个有前途的方法涉及理性化，提取用于预测的不变原理。然而，由于学习信号有限，提取原理是困难的，导致较不准确的原理和减弱的预测。为了解决这些挑战，在本文中，我们提出了一种合作分类与理性化（C2R）方法，包括分类模块和理性化模块。具体地，我们首先假设分类中存在多个环境

    arXiv:2403.06239v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have achieved impressive results in graph classification tasks, but they struggle to generalize effectively when faced with out-of-distribution (OOD) data. Several approaches have been proposed to address this problem. Among them, one solution is to diversify training distributions in vanilla classification by modifying the data environment, yet accessing the environment information is complex. Besides, another promising approach involves rationalization, extracting invariant rationales for predictions. However, extracting rationales is difficult due to limited learning signals, resulting in less accurate rationales and diminished predictions. To address these challenges, in this paper, we propose a Cooperative Classification and Rationalization (C2R) method, consisting of the classification and the rationalization module. Specifically, we first assume that multiple environments are available in the classif
    
[^116]: 概率神经电路

    Probabilistic Neural Circuits

    [https://arxiv.org/abs/2403.06235](https://arxiv.org/abs/2403.06235)

    PNCs将概率电路和神经网络的特点结合起来，可以解释为深层混合的贝叶斯网络，同时作为强大的函数逼近器。

    

    概率电路（PCs）近年来作为一个灵活的框架，被广泛应用于探讨支持可处理查询且足够表达复杂概率分布的概率模型。然而，可处理性是有代价的：PCs比神经网络表达能力更弱。在本文中，我们介绍了概率神经电路（PNCs），在可处理性和表达能力方面在PCs和神经网络之间取得了平衡。理论上，我们展示了PNCs可以被解释为贝叶斯网络的深度混合。实验上，我们证明了PNCs构成了强大的函数逼近器。

    arXiv:2403.06235v1 Announce Type: cross  Abstract: Probabilistic circuits (PCs) have gained prominence in recent years as a versatile framework for discussing probabilistic models that support tractable queries and are yet expressive enough to model complex probability distributions. Nevertheless, tractability comes at a cost: PCs are less expressive than neural networks. In this paper we introduce probabilistic neural circuits (PNCs), which strike a balance between PCs and neural nets in terms of tractability and expressive power. Theoretically, we show that PNCs can be interpreted as deep mixtures of Bayesian networks. Experimentally, we demonstrate that PNCs constitute powerful function approximators.
    
[^117]: LinearAPT: 一种用于固定预算阈值线性赌博机问题的自适应算法

    LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding Linear Bandit Problem

    [https://arxiv.org/abs/2403.06230](https://arxiv.org/abs/2403.06230)

    LinearAPT算法是一种为固定预算设置的阈值线性赌博机问题而设计的新算法，具有适应性、简单性和计算效率，并在优化顺序决策方面表现出色。

    

    在这项研究中，我们深入探讨了阈值线性赌博机（TLB）问题，这是随机多臂赌博问题中的一个微妙领域，重点是在资源约束下最大化针对线性定义阈值的决策准确性。我们提出了LinearAPT，这是一种为TLB的固定预算设置而设计的新颖算法，为优化顺序决策提供了高效解决方案。该算法不仅为估计损失提供了理论上界，而且在合成和真实数据集上展示了强大的性能。我们的贡献突出了LinearAPT的适应性、简单性和计算效率，使其成为解决复杂顺序决策挑战的有价值工具。

    arXiv:2403.06230v1 Announce Type: new  Abstract: In this study, we delve into the Thresholding Linear Bandit (TLB) problem, a nuanced domain within stochastic Multi-Armed Bandit (MAB) problems, focusing on maximizing decision accuracy against a linearly defined threshold under resource constraints. We present LinearAPT, a novel algorithm designed for the fixed budget setting of TLB, providing an efficient solution to optimize sequential decision-making. This algorithm not only offers a theoretical upper bound for estimated loss but also showcases robust performance on both synthetic and real-world datasets. Our contributions highlight the adaptability, simplicity, and computational efficiency of LinearAPT, making it a valuable addition to the toolkit for addressing complex sequential decision-making challenges.
    
[^118]: 您被追踪了吗？发现LLMs的零射轨迹跟踪的威力！

    Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!

    [https://arxiv.org/abs/2403.06201](https://arxiv.org/abs/2403.06201)

    介绍了LLMTrack模型，通过引入一种新颖的单提示技术，结合角色扮演和逐步思考方法，利用未经处理的IMU数据，实现了零射轨迹识别，超越了传统机器学习和深度学习模型，无需训练在专门数据集上的性能表现。

    

    在关于大型语言模型（LLMs）能够作为基本组件的讨论中，能够无缝地融入物联网人工智能（AIoT）中以解释复杂轨迹。本研究介绍了LLMTrack，该模型演示了如何利用LLMs进行零射轨迹识别，通过采用将角色扮演和逐步思考方法与未经处理的惯性测量单元（IMU）数据相结合的新颖单提示技术。我们使用真实世界数据集对模型进行评估，这些数据集旨在挑战它以具有室内和室外情景特征的不同轨迹。在两种测试情景中，LLMTrack 不仅满足甚至超过了传统机器学习方法以及当代最先进的深度学习模型设定的性能基准，而且无需对专门数据集进行训练。

    arXiv:2403.06201v1 Announce Type: cross  Abstract: There is a burgeoning discussion around the capabilities of Large Language Models (LLMs) in acting as fundamental components that can be seamlessly incorporated into Artificial Intelligence of Things (AIoT) to interpret complex trajectories. This study introduces LLMTrack, a model that illustrates how LLMs can be leveraged for Zero-Shot Trajectory Recognition by employing a novel single-prompt technique that combines role-play and think step-by-step methodologies with unprocessed Inertial Measurement Unit (IMU) data. We evaluate the model using real-world datasets designed to challenge it with distinct trajectories characterized by indoor and outdoor scenarios. In both test scenarios, LLMTrack not only meets but exceeds the performance benchmarks set by traditional machine learning approaches and even contemporary state-of-the-art deep learning models, all without the requirement of training on specialized datasets. The results of our 
    
[^119]: DrFuse：学习面向临床多模态融合的解耦表示，解决缺失模态和模态不一致性问题

    DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency

    [https://arxiv.org/abs/2403.06197](https://arxiv.org/abs/2403.06197)

    DrFuse通过解耦数据特征解决了临床多模态融合中的缺失模态和模态不一致性问题。

    

    电子健康记录（EHR）和医学图像的组合对临床医生在诊断和预测预后方面至关重要。战略性地融合这两种数据模态有巨大潜力改善临床预测任务中机器学习模型的准确性。然而，EHR和医学图像的异步和互补性特质带来独特挑战。由于临床和行政因素，缺失模态在实践中是不可避免的，而每种数据模态的重要性取决于患者和预测目标，导致预测不一致和模型性能次优。为解决这些挑战，我们提出DrFuse来实现有效的临床多模态融合。它通过解耦共享模态内和独特模态内的特征来解决缺失模态问题。此外，通过一种方法，我们解决了模态不一致性问题。

    arXiv:2403.06197v1 Announce Type: cross  Abstract: The combination of electronic health records (EHR) and medical images is crucial for clinicians in making diagnoses and forecasting prognosis. Strategically fusing these two data modalities has great potential to improve the accuracy of machine learning models in clinical prediction tasks. However, the asynchronous and complementary nature of EHR and medical images presents unique challenges. Missing modalities due to clinical and administrative factors are inevitable in practice, and the significance of each data modality varies depending on the patient and the prediction target, resulting in inconsistent predictions and suboptimal model performance. To address these challenges, we propose DrFuse to achieve effective clinical multi-modal fusion. It tackles the missing modality issue by disentangling the features shared across modalities and those unique within each modality. Furthermore, we address the modal inconsistency issue via a 
    
[^120]: 具有先前扩散的兰基文算法在非对数凹抽样中的改进分析

    An Improved Analysis of Langevin Algorithms with Prior Diffusion for Non-Log-Concave Sampling

    [https://arxiv.org/abs/2403.06183](https://arxiv.org/abs/2403.06183)

    本文研究了先前扩散技术对满足对数Sobolev不等式的目标分布的作用，扩展了先前仅针对强对数凹分布的相关工作。

    

    高维抽样问题中的计算复杂度的维度相关性是一个基本问题，无论是从实际还是理论的角度来看。相对于具有无偏稳态分布的抽样器，如Metropolis-adjusted Langevin algorithm (MALA)，具有偏置稳态分布的抽样器，如Underdamped Langevin Dynamics (ULD)，在低准确度情况下表现更好，仅仅因为它们的复杂度对维度的依赖性更低。在此基础上，Freund等人(2022)提出，具有先前扩散的修改兰基文算法能够维度独立地收敛于强对数凹目标分布。然而，对于更一般的情况是否存在这样的性质仍然是一个未解之谜。在本文中，我们研究了对满足对数Sobolev不等式（LSI）的目标分布的先前扩散技术，该技术覆盖了比强对数凹分布更广泛的分布类。

    arXiv:2403.06183v1 Announce Type: new  Abstract: Understanding the dimension dependency of computational complexity in high-dimensional sampling problem is a fundamental problem, both from a practical and theoretical perspective. Compared with samplers with unbiased stationary distribution, e.g., Metropolis-adjusted Langevin algorithm (MALA), biased samplers, e.g., Underdamped Langevin Dynamics (ULD), perform better in low-accuracy cases just because a lower dimension dependency in their complexities. Along this line, Freund et al. (2022) suggest that the modified Langevin algorithm with prior diffusion is able to converge dimension independently for strongly log-concave target distributions. Nonetheless, it remains open whether such property establishes for more general cases. In this paper, we investigate the prior diffusion technique for the target distributions satisfying log-Sobolev inequality (LSI), which covers a much broader class of distributions compared to the strongly log-c
    
[^121]: 针对领域泛化分类的领域对抗主动学习

    Domain Adversarial Active Learning for Domain Generalization Classification

    [https://arxiv.org/abs/2403.06174](https://arxiv.org/abs/2403.06174)

    本文提出了一种用于领域泛化分类任务的领域对抗主动学习（DAAL）算法，通过设计一种优先选择具有挑战性样本的领域对抗选择方法，来改善模型的泛化能力。

    

    领域泛化模型旨在从源领域数据中学习跨领域知识，以提高在未知目标领域上的性能。最近的研究表明，多样化和丰富的源领域样本可以增强领域泛化能力。本文认为每个样本对模型的泛化能力的影响是不同的。尽管规模较小，高质量的数据集仍然能够实现一定水平的泛化能力。在此基础上，我们提出了一种用于领域泛化分类任务的领域对抗主动学习（DAAL）算法。首先，我们分析任务的目标是最大化同一领域内的类间距离，并在不同领域之间最小化类内距离。为了实现这一目标，我们设计了一种优先考虑具有挑战性样本的领域对抗选择方法。

    arXiv:2403.06174v1 Announce Type: cross  Abstract: Domain generalization models aim to learn cross-domain knowledge from source domain data, to improve performance on unknown target domains. Recent research has demonstrated that diverse and rich source domain samples can enhance domain generalization capability. This paper argues that the impact of each sample on the model's generalization ability varies. Despite its small scale, a high-quality dataset can still attain a certain level of generalization ability. Motivated by this, we propose a domain-adversarial active learning (DAAL) algorithm for classification tasks in domain generalization. First, we analyze that the objective of tasks is to maximize the inter-class distance within the same domain and minimize the intra-class distance across different domains. To achieve this objective, we design a domain adversarial selection method that prioritizes challenging samples. Second, we posit that even in a converged model, there are sub
    
[^122]: 用质量多样性加速六自由度抓取采样

    Speeding up 6-DoF Grasp Sampling with Quality-Diversity

    [https://arxiv.org/abs/2403.06173](https://arxiv.org/abs/2403.06173)

    该论文研究了如何将质量多样性（QD）与先验知识结合，以加速在仿真中生成多样化抓取姿势，实验证明QD相比于标准的六自由度抓取采样方案有着显著的优势。

    

    最近人工智能的进步在机器人学习领域取得了显著成果，包括自然语言条件规划和利用生成模型高效优化控制器。然而，交互数据仍然是泛化的瓶颈。获取抓取数据是一个关键挑战，因为这项技能对完成许多操纵任务至关重要。质量多样性（QD）算法优化一组解以获得在给定问题上多样化、高性能的解决方案。本文研究了如何将QD与先验知识结合起来，以加速仿真中多样化抓取姿势的生成，相对于标准的六自由度抓取采样方案。在标准对象上对四个带有2至5个手指的夹爪进行的实验表明，QD的表现远远超过常用方法。进一步的实验表明，QD优化自动发现了一些通常难以编码的高效先验知识。

    arXiv:2403.06173v1 Announce Type: cross  Abstract: Recent advances in AI have led to significant results in robotic learning, including natural language-conditioned planning and efficient optimization of controllers using generative models. However, the interaction data remains the bottleneck for generalization. Getting data for grasping is a critical challenge, as this skill is required to complete many manipulation tasks. Quality-Diversity (QD) algorithms optimize a set of solutions to get diverse, high-performing solutions to a given problem. This paper investigates how QD can be combined with priors to speed up the generation of diverse grasps poses in simulation compared to standard 6-DoF grasp sampling schemes. Experiments conducted on 4 grippers with 2-to-5 fingers on standard objects show that QD outperforms commonly used methods by a large margin. Further experiments show that QD optimization automatically finds some efficient priors that are usually hard coded. The deployment
    
[^123]: ALL0CORE张量分解用于稀疏计数数据

    The ALL0CORE Tensor Decomposition for Sparse Count Data

    [https://arxiv.org/abs/2403.06153](https://arxiv.org/abs/2403.06153)

    ALL0CORE是一种新的概率非负张量分解方法，它在保持计算可处理性的基础上利用Tucker分解的潜在结构，可以仅使用核的微小部分即达到与完整Tucker分解相同效果。

    

    本文介绍了ALL0CORE，一种新的概率非负张量分解形式。ALL0CORE是一种Tucker分解，其中核张量的非零元素数量（即L0范数）被限制为远小于核的大小的预设值Q。虽然用户规定了总预算Q，但非零元素的位置和值是潜在变量，在推断过程中分配给核张量的各个部分。ALL0CORE，即分配的L0约束核，因此既具有CP分解的计算可处理性，又具有Tucker的潜在结构，令人满意。在一系列真实数据实验中，我们展示了ALL0CORE通常只需使用核的微小部分（例如～1%）即可以与完整Tucker分解相同的结果，而成本仅相应的一小部分。

    arXiv:2403.06153v1 Announce Type: cross  Abstract: This paper introduces ALL0CORE, a new form of probabilistic non-negative tensor decomposition. ALL0CORE is a Tucker decomposition where the number of non-zero elements (i.e., the L0-norm) of the core tensor is constrained to a preset value Q much smaller than the size of the core. While the user dictates the total budget Q, the locations and values of the non-zero elements are latent variables and allocated across the core tensor during inference. ALL0CORE -- i.e., allocated L0-constrained core -- thus enjoys both the computational tractability of CP decomposition and the qualitatively appealing latent structure of Tucker. In a suite of real-data experiments, we demonstrate that ALL0CORE typically requires only tiny fractions (e.g.,~1%) of the full core to achieve the same results as full Tucker decomposition at only a correspondingly tiny fraction of the cost.
    
[^124]: MACE：扩散模型中的大规模概念消除

    MACE: Mass Concept Erasure in Diffusion Models

    [https://arxiv.org/abs/2403.06135](https://arxiv.org/abs/2403.06135)

    MACE通过成功将概念消除的范围扩展到100个概念，并在泛化性和特异性之间取得有效平衡，从而防止大规模文本到图像扩散模型生成不良或误导性图像。

    

    大规模文本到图像扩散模型的快速扩展引起了人们对其在创建有害或误导性内容方面潜在误用的担忧。本文介绍了一种名为MACE的微调框架，用于大规模概念消除任务。该任务旨在防止模型在提示时生成具有不希望概念的图像。现有的概念消除方法通常受限于同时处理少于五个概念，并且在消除概念的同时难以找到消除概念同义词（泛化性）和保持不相关概念（特异性）之间的平衡。相比之下，MACE通过成功将消除范围扩展到100个概念，并在泛化性和特异性之间取得有效平衡而不同。这是通过利用闭环交叉注意力细化以及LoRA微调来实现的，共同消除了不良概念的信息。此外，MACE还...

    arXiv:2403.06135v1 Announce Type: cross  Abstract: The rapid expansion of large-scale text-to-image diffusion models has raised growing concerns regarding their potential misuse in creating harmful or misleading content. In this paper, we introduce MACE, a finetuning framework for the task of mass concept erasure. This task aims to prevent models from generating images that embody unwanted concepts when prompted. Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity). In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity. This is achieved by leveraging closed-form cross-attention refinement along with LoRA finetuning, collectively eliminating the information of undesirable concepts. Furthermore, MACE int
    
[^125]: 在众包环境中利用在线学习进行基于喜好的主观评估设计优化

    Automatic design optimization of preference-based subjective evaluation with online learning in crowdsourcing environment

    [https://arxiv.org/abs/2403.06100](https://arxiv.org/abs/2403.06100)

    在众包环境中，我们提出了一种自动优化方法，利用在线学习对配对组合和评估量进行优化，实现基于喜好的主观评估的设计优化。

    

    基于喜好的主观评估是评价生成式媒体可靠性的关键方法。然而，其庞大的配对组合使得它无法应用于利用众包进行大规模评估。为了解决这个问题，我们提出了一种用于在众包环境中进行基于喜好的主观评估的自动优化方法，该方法涉及对配对组合选择和评估量分配的在线学习。我们使用基于排序算法的基于喜好的在线学习方法来识别具有最小样本量的评估目标的完全顺序。我们的在线学习算法支持在众包所需的固定预算条件下的并行和异步执行。我们对合成语音的基于喜好的主观评估实验证明了我们的方法成功通过将配对组合从351减少到83并分配最优评估。

    arXiv:2403.06100v1 Announce Type: cross  Abstract: A preference-based subjective evaluation is a key method for evaluating generative media reliably. However, its huge combinations of pairs prohibit it from being applied to large-scale evaluation using crowdsourcing. To address this issue, we propose an automatic optimization method for preference-based subjective evaluation in terms of pair combination selections and allocation of evaluation volumes with online learning in a crowdsourcing environment. We use a preference-based online learning method based on a sorting algorithm to identify the total order of evaluation targets with minimum sample volumes. Our online learning algorithm supports parallel and asynchronous execution under fixed-budget conditions required for crowdsourcing. Our experiment on preference-based subjective evaluation of synthetic speech shows that our method successfully optimizes the test by reducing pair combinations from 351 to 83 and allocating optimal eva
    
[^126]: 面向车载多任务面部属性识别：研究合成数据和视觉基础模型

    Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models

    [https://arxiv.org/abs/2403.06088](https://arxiv.org/abs/2403.06088)

    本文通过研究合成数据集的实用性和不同视觉基础模型在车载多任务面部属性识别中的效果，填补了现有文献中的空白。

    

    在智能交通系统蓬勃发展的领域中，通过面部属性识别（如面部表情、眼神、年龄等）增强车辆驾驶员的交互对于安全、个性化和整体用户体验至关重要。然而，缺乏全面大规模的真实世界数据集对训练稳健的多任务模型构成了重大挑战。现有文献通常忽视了合成数据集的潜力以及在这种受限环境中最先进的视觉基础模型的比较功效。本文通过研究合成数据集的实用性来训练识别车辆乘客的面部属性（如眼神方向、年龄和面部表情）的复杂多任务模型，并利用预训练的Vision Transformer（ViT）和残差网络（ResNet）模型的迁移学习技术来填补这些空白。

    arXiv:2403.06088v1 Announce Type: cross  Abstract: In the burgeoning field of intelligent transportation systems, enhancing vehicle-driver interaction through facial attribute recognition, such as facial expression, eye gaze, age, etc., is of paramount importance for safety, personalization, and overall user experience. However, the scarcity of comprehensive large-scale, real-world datasets poses a significant challenge for training robust multi-task models. Existing literature often overlooks the potential of synthetic datasets and the comparative efficacy of state-of-the-art vision foundation models in such constrained settings. This paper addresses these gaps by investigating the utility of synthetic datasets for training complex multi-task models that recognize facial attributes of passengers of a vehicle, such as gaze plane, age, and facial expression. Utilizing transfer learning techniques with both pre-trained Vision Transformer (ViT) and Residual Network (ResNet) models, we exp
    
[^127]: 学习阿尔茨海默病的不可逆进展轨迹

    Learning the irreversible progression trajectory of Alzheimer's disease

    [https://arxiv.org/abs/2403.06087](https://arxiv.org/abs/2403.06087)

    提出了一种正则化方法来预测阿尔茨海默病的不可逆进展轨迹

    

    阿尔茨海默病（AD）是一种随着30年时间逐渐展开的进行性不可逆脑部疾病。因此，关键是在早期捕获疾病的进展，以便在症状出现之前可以施加干预。机器学习（ML）模型已被证明在预测AD的发作方面有效。然而，对于有随访的受试者，现有的AD分类技术只针对准确的组分配，通常忽略了在随访过程中递增风险的单调增加。在随访间出现的波动风险评分违背了AD的不可逆性，影响了模型的可信度，也对理解疾病的进展提供了很少的价值。为了解决这个问题，我们提出了一种新的正则化方法来预测AD的纵向发展。我们的技术旨在在病情进展期间保持预期的单调增加疾病风险。

    arXiv:2403.06087v1 Announce Type: new  Abstract: Alzheimer's disease (AD) is a progressive and irreversible brain disorder that unfolds over the course of 30 years. Therefore, it is critical to capture the disease progression in an early stage such that intervention can be applied before the onset of symptoms. Machine learning (ML) models have been shown effective in predicting the onset of AD. Yet for subjects with follow-up visits, existing techniques for AD classification only aim for accurate group assignment, where the monotonically increasing risk across follow-up visits is usually ignored. Resulted fluctuating risk scores across visits violate the irreversibility of AD, hampering the trustworthiness of models and also providing little value to understanding the disease progression. To address this issue, we propose a novel regularization approach to predict AD longitudinally. Our technique aims to maintain the expected monotonicity of increasing disease risk during progression w
    
[^128]: FrameQuant: Transformer的灵活低比特量化方法

    FrameQuant: Flexible Low-Bit Quantization for Transformers

    [https://arxiv.org/abs/2403.06082](https://arxiv.org/abs/2403.06082)

    提出一种简单的方案，通过融合框架将Transformer模型量化为仅两位，仅有轻微精度下降。

    

    Transformer是许多视觉和自然语言处理任务强大基础模型的支柱。然而，它们的计算和内存/存储空间占用较大，因此为这些模型提供服务往往需要昂贵的高端硬件。为了缓解这一困难，后训练量化试图修改预训练模型并将其量化为八位或更低的位数，显着提高计算/内存/延迟效率。既可以成功将这些模型量化为四位，但性能有所损失。在这项工作中，我们概述了一个简单的方案，将基于Transformer的模型量化为仅两位（加一些额外开销），仅会有轻微的精度下降。我们的制定关键在于从谐波分析中借鉴了一种称为融合框架的概念。我们的主要发现是，量化不应该在原始权重空间中进行，而是应该在融合框架表示中进行。

    arXiv:2403.06082v1 Announce Type: cross  Abstract: Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addi
    
[^129]: 基于本地顶点着色的图神经网络

    Local Vertex Colouring Graph Neural Networks

    [https://arxiv.org/abs/2403.06080](https://arxiv.org/abs/2403.06080)

    提出一种基于本地顶点着色的方案，通过经典搜索算法能够高效计算超越1-WL的图表示，并展示这种着色方案可以帮助解决图双连通性等问题，同时表明在某些条件下，GNNs的表达能力随着搜索邻域半径的增加而层级提高。

    

    近年来，已经有大量的研究聚焦于扩展图神经网络（GNNs）的表达能力，超越Weisfeiler-Lehman（1-WL）框架。虽然许多研究在提高表达能力方面取得了进展，但往往以效率降低为代价，或者限制在特定类型的图上。在本研究中，我们从图搜索的角度调查了GNNs的表达能力。具体而言，我们提出了一种新的顶点着色方案，并展示了经典搜索算法可以有效地计算超越1-WL的图表示。我们展示了着色方案从图搜索中继承了有用的属性，可以帮助解决诸如图的双连通性等问题。此外，我们表明在某些条件下，GNNs的表达能力随着搜索邻域半径的增加而层级提高。

    arXiv:2403.06080v1 Announce Type: new  Abstract: In recent years, there has been a significant amount of research focused on expanding the expressivity of Graph Neural Networks (GNNs) beyond the Weisfeiler-Lehman (1-WL) framework. While many of these studies have yielded advancements in expressivity, they have frequently come at the expense of decreased efficiency or have been restricted to specific types of graphs. In this study, we investigate the expressivity of GNNs from the perspective of graph search. Specifically, we propose a new vertex colouring scheme and demonstrate that classical search algorithms can efficiently compute graph representations that extend beyond the 1-WL. We show the colouring scheme inherits useful properties from graph search that can help solve problems like graph biconnectivity. Furthermore, we show that under certain conditions, the expressivity of GNNs increases hierarchically with the radius of the search neighbourhood. To further investigate the prop
    
[^130]: 通过同态的角度推广图神经网络

    Generalization of Graph Neural Networks through the Lens of Homomorphism

    [https://arxiv.org/abs/2403.06079](https://arxiv.org/abs/2403.06079)

    本文通过分析图同态的熵提出了一种新颖视角，推导出了适用于图和节点分类的泛化界限，能够捕捉各种图结构的细微差异，并通过统一框架刻画了广泛的GNN模型。

    

    尽管图神经网络(GNNs)在许多应用中广受欢迎，但其泛化能力仍未得到充分探讨。本文提出通过一种新颖的视角 - 分析图同态的熵来研究GNNs的泛化。通过将图同态与信息论度量联系起来，我们推导出针对图和节点分类的泛化界限。这些界限能够捕捉各种图结构固有的细微差异，包括但不限于路径、环和团。这使得我们能够进行具有稳健理论保证的基于数据的泛化分析。为了阐明我们提出的界限的普适性，我们提出一个统一框架，可以通过图同态的视角刻画广泛的GNN模型。我们通过显示我们的理论发现与实际的一致性来验证我们理论发现的实际适用性。

    arXiv:2403.06079v1 Announce Type: new  Abstract: Despite the celebrated popularity of Graph Neural Networks (GNNs) across numerous applications, the ability of GNNs to generalize remains less explored. In this work, we propose to study the generalization of GNNs through a novel perspective - analyzing the entropy of graph homomorphism. By linking graph homomorphism with information-theoretic measures, we derive generalization bounds for both graph and node classifications. These bounds are capable of capturing subtleties inherent in various graph structures, including but not limited to paths, cycles and cliques. This enables a data-dependent generalization analysis with robust theoretical guarantees. To shed light on the generality of of our proposed bounds, we present a unifying framework that can characterize a broad spectrum of GNN models through the lens of graph homomorphism. We validate the practical applicability of our theoretical findings by showing the alignment between the 
    
[^131]: 隐式图像对图像Schrodinger桥用于CT超分辨率和去噪

    Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising

    [https://arxiv.org/abs/2403.06069](https://arxiv.org/abs/2403.06069)

    I3SB方法通过引入非马尔可夫过程，结合损坏的图像改善纹理恢复，在CT超分辨率和去噪任务中表现优异。

    

    有条件扩散模型因其在图像恢复任务中的有效性而得到认可，然而，其从高斯噪声开始的迭代去噪过程往往导致推断速度慢。作为一种有希望的替代方案，图像对图像Schrödinger桥（I2SB）从损坏的图像开始初始化生成过程，并集成了有条件扩散模型的训练技术。在本研究中，我们通过引入隐式图像对图像Schrödinger桥（I3SB）扩展了I2SB方法，通过在每一生成步骤中纳入损坏的图像，将其生成过程转换为非马尔可夫过程。这种增强使得I3SB能够在少量生成步骤中生成具有更好纹理恢复的图像。所提出的方法在CT超分辨率和去噪任务上得到验证，并超越了包括有条件去噪扩散概率模型在内的现有方法。

    arXiv:2403.06069v1 Announce Type: cross  Abstract: Conditional diffusion models have gained recognition for their effectiveness in image restoration tasks, yet their iterative denoising process, starting from Gaussian noise, often leads to slow inference speeds. As a promising alternative, the Image-to-Image Schr\"odinger Bridge (I2SB) initializes the generative process from corrupted images and integrates training techniques from conditional diffusion models. In this study, we extended the I2SB method by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB), transitioning its generative process to a non-Markovian process by incorporating corrupted images in each generative step. This enhancement empowers I3SB to generate images with better texture restoration using a small number of generative steps. The proposed method was validated on CT super-resolution and denoising tasks and outperformed existing methods, including the conditional denoising diffusion probabilistic mod
    
[^132]: CausalCellSegmenter：因果推断启发的多样化聚合卷积用于病理图像分割

    CausalCellSegmenter: Causal Inference inspired Diversified Aggregation Convolution for Pathology Image Segmentation

    [https://arxiv.org/abs/2403.06066](https://arxiv.org/abs/2403.06066)

    提出了一种结合了因果推断模块和多样化聚合卷积技术的CausalCellSegmenter框架，通过DAC模块和SimAM关注模块解决了细胞核分割中的挑战，包括误报识别和边缘模糊问题。

    

    深度学习模型在病理图像分析领域的细胞核分割中表现出令人期待的性能。然而，训练一个来自多个领域的强大模型仍然是细胞核分割的巨大挑战。此外，背景噪声、细胞核之间高度重叠以及模糊边缘的缺点经常导致性能不佳。为了解决这些挑战，我们提出了一种称为CausalCellSegmenter的新颖框架，它将因果推断模块（CIM）与多样化聚合卷积（DAC）技术相结合。DAC模块被设计为通过一个简单、无参数的关注模块（SimAM）将不同的下采样特征整合在一起，旨在解决误报识别和边缘模糊问题。此外，我们引入CIM来利用样本加权，通过直接消除每个输入样本之间的虚假相关性来聚焦。

    arXiv:2403.06066v1 Announce Type: cross  Abstract: Deep learning models have shown promising performance for cell nucleus segmentation in the field of pathology image analysis. However, training a robust model from multiple domains remains a great challenge for cell nucleus segmentation. Additionally, the shortcomings of background noise, highly overlapping between cell nucleus, and blurred edges often lead to poor performance. To address these challenges, we propose a novel framework termed CausalCellSegmenter, which combines Causal Inference Module (CIM) with Diversified Aggregation Convolution (DAC) techniques. The DAC module is designed which incorporates diverse downsampling features through a simple, parameter-free attention module (SimAM), aiming to overcome the problems of false-positive identification and edge blurring. Furthermore, we introduce CIM to leverage sample weighting by directly removing the spurious correlations between features for every input sample and concentra
    
[^133]: L$^2$GC: 洛伦兹线性图卷积网络用于节点分类

    L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification

    [https://arxiv.org/abs/2403.06064](https://arxiv.org/abs/2403.06064)

    本文提出了一种新颖的洛伦兹线性图卷积网络框架，将双曲空间引入线性GCN，用于捕捉数据的树状结构，并在实验中取得了新的最先进的节点分类结果。

    

    线性图卷积网络（GCNs）用于对图数据中的节点进行分类。然而，我们注意到大多数现有的线性GCN模型在欧几里得空间中执行神经网络操作，这并没有明确捕捉到作为图模型的现实世界数据集中呈现出的类似树状的层次结构。本文尝试将双曲空间引入线性GCN，并提出了一种新颖的洛伦兹线性GCN框架。具体来说，我们将图节点的学习特征映射到双曲空间中，然后进行洛伦兹线性特征变换，以捕获数据的潜在树状结构。在标准引文网络数据集上进行的半监督学习实验结果显示，我们的方法在Citeseer数据集上达到了74.7%的准确度，而在PubMed数据集上达到了81.3%的准确度，创造了新的最先进结果。此外，我们观察到我们的方法可以训练至少达到2个数量级。

    arXiv:2403.06064v1 Announce Type: cross  Abstract: Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and 81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitu
    
[^134]: 集成式多语言情感分析语言模型

    Ensemble Language Models for Multilingual Sentiment Analysis

    [https://arxiv.org/abs/2403.06060](https://arxiv.org/abs/2403.06060)

    该研究针对低资源语言如阿拉伯语进行了情感分析，提出了两种集成语言模型，发现单语模型表现优越，集成模型胜过基线，而多数投票集成优于英语语言。

    

    社交媒体的快速发展使我们能够分析用户的观点。最近，情感分析在理解基于社交媒体分享内容的人类情感方面显示出明显的研究空白。尽管常用语言的情感分析已有显著进展，但像阿拉伯语这样的低资源语言由于资源有限仍然很少被研究。在本研究中，我们探讨了来自SemEval-17和阿拉伯情感推文数据集的推文文本的情感分析。此外，我们调查了四个预训练语言模型，并提出了两个集成语言模型。我们的发现包括单语模型表现出优越性能，集成模型胜过基线，而多数投票集成胜过英语语言。

    arXiv:2403.06060v1 Announce Type: new  Abstract: The rapid advancement of social media enables us to analyze user opinions. In recent times, sentiment analysis has shown a prominent research gap in understanding human sentiment based on the content shared on social media. Although sentiment analysis for commonly spoken languages has advanced significantly, low-resource languages like Arabic continue to get little research due to resource limitations. In this study, we explore sentiment analysis on tweet texts from SemEval-17 and the Arabic Sentiment Tweet dataset. Moreover, We investigated four pretrained language models and proposed two ensemble language models. Our findings include monolingual models exhibiting superior performance and ensemble models outperforming the baseline while the majority voting ensemble outperforms the English language.
    
[^135]: 远离真实解的虚假解析无：具有高阶损失的低秩分析

    Absence of spurious solutions far from ground truth: A low-rank analysis with high-order losses

    [https://arxiv.org/abs/2403.06056](https://arxiv.org/abs/2403.06056)

    本研究证明了在某些条件下，远离真实解的关键点表现出有利的严格鞍点几何结构，并介绍了高阶损失的概念，通过将其纳入目标函数增加负曲率，从而加速摆脱这些临界点。

    

    矩阵感知问题展现出普遍的非凸性，困扰着优化过程中大量次优虚假解的蔓延。避免收敛到这些临界点构成了一项重大挑战。本文提供了有助于揭示非凸景观复杂性的新的理论见解。在这项工作中，我们证明在某些条件下，远离真实矩阵的临界点呈现出有利的几何特性，即严格鞍点，而非令人头疼的局部最小值。此外，我们引入了矩阵感知问题的高阶损失概念，并展示将这些损失纳入目标函数可以增大围绕这些远离临界点的负曲率。这意味着通过高阶损失增加目标函数的复杂性可以加速摆脱这些临界点，并且作为一种理想的替代方式。

    arXiv:2403.06056v1 Announce Type: cross  Abstract: Matrix sensing problems exhibit pervasive non-convexity, plaguing optimization with a proliferation of suboptimal spurious solutions. Avoiding convergence to these critical points poses a major challenge. This work provides new theoretical insights that help demystify the intricacies of the non-convex landscape. In this work, we prove that under certain conditions, critical points sufficiently distant from the ground truth matrix exhibit favorable geometry by being strict saddle points rather than troublesome local minima. Moreover, we introduce the notion of higher-order losses for the matrix sensing problem and show that the incorporation of such losses into the objective function amplifies the negative curvature around those distant critical points. This implies that increasing the complexity of the objective function via high-order losses accelerates the escape from such critical points and acts as a desirable alternative to increa
    
[^136]: 具有扩散净化的分离数据一致性的图像恢复

    Decoupled Data Consistency with Diffusion Purification for Image Restoration

    [https://arxiv.org/abs/2403.06054](https://arxiv.org/abs/2403.06054)

    通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。

    

    最近，扩散模型作为一种强大的深度生成先验类别已经引起了人们的关注，由于其出色地建模数据分布的能力，在各种图像恢复任务中表现出色。为了解决图像恢复问题，许多现有技术通过将额外的似然梯度步骤纳入到扩散模型的反向采样过程中来实现数据一致性。然而，这些额外的梯度步骤对于实际应用中存在挑战，因为它们造成了巨大的计算开销，从而增加了推理时间。当使用加速的扩散模型采样器时，这些额外的步骤还会导致额外的困难，因为数据一致性步骤的数量受限于反向采样步骤的数量。在这项工作中，我们提出了一种新颖的基于扩散的图像恢复求解器，通过将反向过程与数据一致性步骤分离来解决这些问题。我们的方法涉及

    arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
    
[^137]: 使用分类和基于Contourlet特征的纹理图像检索

    Texture image retrieval using a classification and contourlet-based features

    [https://arxiv.org/abs/2403.06048](https://arxiv.org/abs/2403.06048)

    使用新的基于RCT-Plus变换的图像表示和学习方法改进了纹理图像检索，实现了比以前方案更高的检索速率。

    

    在本文中，我们提出了一个新的框架，用于改进基于内容的图像检索（CBIR）的纹理图像。这是通过使用基于RCT-Plus变换的新图像表示来实现的，这是一种提取图像中更丰富方向信息的Redundant Contourlet变换的新变体。此外，通过使用基于学习的方法改进了图像搜索过程，其中数据库中的图像使用适应于RCT-Plus变换的统计建模的相似度度量进行分类。然后首先对查询进行分类以选择最佳纹理类别，然后对保留类别的图像进行排名以选择顶部图像。通过这种方式，我们相比以前的CBIR方案在检索速率上取得了显著的改进。

    arXiv:2403.06048v1 Announce Type: cross  Abstract: In this paper, we propose a new framework for improving Content Based Image Retrieval (CBIR) for texture images. This is achieved by using a new image representation based on the RCT-Plus transform which is a novel variant of the Redundant Contourlet transform that extracts a richer directional information in the image. Moreover, the process of image search is improved through a learning-based approach where the images of the database are classified using an adapted similarity metric to the statistical modeling of the RCT-Plus transform. A query is then first classified to select the best texture class after which the retained class images are ranked to select top ones. By this, we have achieved significant improvements in the retrieval rates compared to previous CBIR schemes.
    
[^138]: MATRIX: 具有多样化背景的多智能体轨迹生成

    MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts

    [https://arxiv.org/abs/2403.06041](https://arxiv.org/abs/2403.06041)

    本研究提出了一种名为MATRIX的学习基础自动轨迹生成模型，能够在多样化背景中生成交互式人类行为。

    

    数据驱动方法在建模复杂人类行为动态和处理许多人机交互应用中具有巨大优势。然而，收集大规模和带标注的真实世界人类数据通常是一项费力的任务，特别是对于高度交互式的场景。另一方面，基于算法的数据生成方法通常受到模型能力的限制，使其无法为各种应用用户提供所需的现实和多样化数据。本文研究了多人或人机交互场景的轨迹级数据生成，并提出了一种学习基础的自动轨迹生成模型，我们称之为具有多样化背景的多智能体轨迹生成（MATRIX）。MATRIX能够在现实多样化的背景中生成交互式人类行为。我们通过建模显式和可解释的目标来实现这一目标，使MATRIX能够生成人类运动。

    arXiv:2403.06041v1 Announce Type: cross  Abstract: Data-driven methods have great advantages in modeling complicated human behavioral dynamics and dealing with many human-robot interaction applications. However, collecting massive and annotated real-world human datasets has been a laborious task, especially for highly interactive scenarios. On the other hand, algorithmic data generation methods are usually limited by their model capacities, making them unable to offer realistic and diverse data needed by various application users. In this work, we study trajectory-level data generation for multi-human or human-robot interaction scenarios and propose a learning-based automatic trajectory generation model, which we call Multi-Agent TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of generating interactive human behaviors in realistic diverse contexts. We achieve this goal by modeling the explicit and interpretable objectives so that MATRIX can generate human motion
    
[^139]: 预测抑郁和焦虑：一种用于分析COVID-19心理健康影响的多层感知器

    Predicting Depression and Anxiety: A Multi-Layer Perceptron for Analyzing the Mental Health Impact of COVID-19

    [https://arxiv.org/abs/2403.06033](https://arxiv.org/abs/2403.06033)

    提出了一个名为CoDAP的多层感知器，用于预测COVID-19大流行期间焦虑和抑郁的趋势，通过定性个体属性视角分析了焦虑和抑郁模式，并揭示了人口因素、行为变化和社会心理健康决定因素的关键见解

    

    我们引入了一个名为COVID-19抑郁和焦虑预测器（CoDAP）的多层感知器（MLP），用于预测COVID-19大流行期间的心理健康趋势，特别是焦虑和抑郁。我们的方法利用了一个全面的数据集，在COVID-19大流行初期（2020年4月至6月）的十周内每周跟踪了美国成年人多样化队列的心理健康症状。这一时期以心理健康症状和情况激增为特征，为我们的分析提供了关键背景。我们的重点是通过CoDAP从独特的定性个体属性视角提取和分析焦虑和抑郁模式。这个模型不仅可以预测大流行期间焦虑和抑郁的模式，还揭示了人口因素、行为变化和社会心理健康决定因素相互作用的关键见解。这些发现有助于更细致地理解

    arXiv:2403.06033v1 Announce Type: new  Abstract: We introduce a multi-layer perceptron (MLP) called the COVID-19 Depression and Anxiety Predictor (CoDAP) to predict mental health trends, particularly anxiety and depression, during the COVID-19 pandemic. Our method utilizes a comprehensive dataset, which tracked mental health symptoms weekly over ten weeks during the initial COVID-19 wave (April to June 2020) in a diverse cohort of U.S. adults. This period, characterized by a surge in mental health symptoms and conditions, offers a critical context for our analysis. Our focus was to extract and analyze patterns of anxiety and depression through a unique lens of qualitative individual attributes using CoDAP. This model not only predicts patterns of anxiety and depression during the pandemic but also unveils key insights into the interplay of demographic factors, behavioral changes, and social determinants of mental health. These findings contribute to a more nuanced understanding of the 
    
[^140]: FairTargetSim：用于理解和解释目标变量定义公平性影响的交互式模拟器

    FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition

    [https://arxiv.org/abs/2403.06031](https://arxiv.org/abs/2403.06031)

    FairTargetSim提供了一个交互式模拟器，展示了目标变量定义对公平性的影响，适用于算法开发者、研究人员和非技术利益相关者。

    

    机器学习需要为预测或决策定义目标变量，这个过程可能对公平性产生深远影响：偏见通常已经被编码在目标变量定义本身中，而不是在任何数据收集或训练之前。我们提出了一个交互式模拟器，FairTargetSim (FTS)，展示了目标变量定义如何影响公平性。FTS是一个有价值的工具，适用于算法开发者、研究人员和非技术利益相关者。FTS使用了算法招聘的案例研究，使用真实世界数据和用户定义的目标变量。FTS是开源的，可在以下网址找到：http://tinyurl.com/ftsinterface。本文附带的视频网址为：http://tinyurl.com/ijcaifts。

    arXiv:2403.06031v1 Announce Type: cross  Abstract: Machine learning requires defining one's target variable for predictions or decisions, a process that can have profound implications on fairness: biases are often encoded in target variable definition itself, before any data collection or training. We present an interactive simulator, FairTargetSim (FTS), that illustrates how target variable definition impacts fairness. FTS is a valuable tool for algorithm developers, researchers, and non-technical stakeholders. FTS uses a case study of algorithmic hiring, using real-world data and user-defined target variables. FTS is open-source and available at: http://tinyurl.com/ftsinterface. The video accompanying this paper is here: http://tinyurl.com/ijcaifts.
    
[^141]: 使用多模态深度学习方法预测心脏骤停后昏迷患者的神经恢复

    Multimodal deep learning approach to predicting neurological recovery from coma after cardiac arrest

    [https://arxiv.org/abs/2403.06027](https://arxiv.org/abs/2403.06027)

    使用多模态深度学习方法结合临床数据和时间序列信号，成功预测心脏骤停后昏迷患者的神经恢复，同时展示了迁移学习在医学分类中的效用和限制

    

    这项研究展示了我们团队（BEEGees）在2023年George B. Moody PhysioNet挑战赛中的贡献。旨在利用临床数据和诸如多通道脑电图（EEG）和心电图（ECG）信号之类的时间序列数据，预测心脏骤停后昏迷患者的神经恢复。我们的建模方法是多模态的，基于从众多脑电图通道派生的二维频谱图表示，同时整合临床数据和直接从脑电图记录中提取的特征。我们提交的模型在自发循环恢复后72小时进行的预测的隐藏测试集上取得了0.53的挑战分数。我们的研究展示了在医学分类中使用迁移学习的有效性和局限性。关于未来的实施，我们的分析揭示了模型性能与决策阈值的选择密切相关，并在数据分割方面表现出明显的变异性。

    arXiv:2403.06027v1 Announce Type: new  Abstract: This work showcases our team's (The BEEGees) contributions to the 2023 George B. Moody PhysioNet Challenge. The aim was to predict neurological recovery from coma following cardiac arrest using clinical data and time-series such as multi-channel EEG and ECG signals. Our modelling approach is multimodal, based on two-dimensional spectrogram representations derived from numerous EEG channels, alongside the integration of clinical data and features extracted directly from EEG recordings. Our submitted model achieved a Challenge score of $0.53$ on the hidden test set for predictions made $72$ hours after return of spontaneous circulation. Our study shows the efficacy and limitations of employing transfer learning in medical classification. With regard to prospective implementation, our analysis reveals that the performance of the model is strongly linked to the selection of a decision threshold and exhibits strong variability across data spl
    
[^142]: 为基于学习方法的组合问题构建通用表示

    Towards a Generic Representation of Cominatorial Problems for Learning-Based Approaches

    [https://arxiv.org/abs/2403.06026](https://arxiv.org/abs/2403.06026)

    本文倡导为基于学习方法的组合问题构建通用表示，以解决特定表示无法跨越不同组合问题的问题。

    

    近年来，越来越多的研究人员对使用基于学习方法解决组合问题产生了兴趣，无论是以端到端的方式还是与传统优化算法结合使用。在这两种情景下，挑战在于将目标组合问题编码成适用于学习算法的结构。许多现有作品提出了特定于问题的表示，通常以图的形式，以利用图神经网络的优势。然而，这些方法缺乏泛化性，因为表示不能轻易从一个组合问题转移到另一个组合问题。虽然已经有一些尝试去填补这一差距，但它们仍然只提供了部分泛化性。鉴于这一挑战，本文倡导为基于学习方法的组合问题朝着完全通用的表示方式迈进。我们提出的方法包括

    arXiv:2403.06026v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in using learning-based approaches for solving combinatorial problems, either in an end-to-end manner or in conjunction with traditional optimization algorithms. In both scenarios, the challenge lies in encoding the targeted combinatorial problems into a structure compatible with the learning algorithm. Many existing works have proposed problem-specific representations, often in the form of a graph, to leverage the advantages of \textit{graph neural networks}. However, these approaches lack generality, as the representation cannot be easily transferred from one combinatorial problem to another one. While some attempts have been made to bridge this gap, they still offer a partial generality only. In response to this challenge, this paper advocates for progress toward a fully generic representation of combinatorial problems for learning-based approaches. The approach we propose involves 
    
[^143]: 主动脉狭窄诊断的半监督多模态多实例学习

    Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis Diagnosis

    [https://arxiv.org/abs/2403.06024](https://arxiv.org/abs/2403.06024)

    引入了半监督多模态多实例学习（SMMIL）框架，用于结构性心脏疾病自动解释，克服了心脏超声心动图评估AS时对有限2D cineloops和难以获得标记数据的限制。

    

    arXiv:2403.06024v1 公告类型: 跨领域 自动解释心脏超声心动图心脏超声心动图(imaging of the heart (echocardiograms))可以改善主动脉瓣狭窄(aortic stenosis, AS)的检测和治疗，而目前用于评估AS的深度学习流程存在两个主要限制。首先，大多数方法依赖于有限的2D cineloops，因此忽略了包含与AS相关的压力梯度和血液流动异常等重要互补信息的广泛可用的多普勒(imaging)。其次，获得有标记的数据是困难的。通常存在大量未标记的心脏超声心动图录音可用，但这些数据被现有方法很少利用。为了克服这些限制，我们引入了半监督多模态多实例学习（Semi-supervised Multimodal Multiple-Instance Learning, SMMIL），这是一个新的深度学习框架，可用于自动解释AS等结构性心脏疾病。

    arXiv:2403.06024v1 Announce Type: cross  Abstract: Automated interpretation of ultrasound imaging of the heart (echocardiograms) could improve the detection and treatment of aortic stenosis (AS), a deadly heart disease. However, existing deep learning pipelines for assessing AS from echocardiograms have two key limitations. First, most methods rely on limited 2D cineloops, thereby ignoring widely available Doppler imaging that contains important complementary information about pressure gradients and blood flow abnormalities associated with AS. Second, obtaining labeled data is difficult. There are often far more unlabeled echocardiogram recordings available, but these remain underutilized by existing methods. To overcome these limitations, we introduce Semi-supervised Multimodal Multiple-Instance Learning (SMMIL), a new deep learning framework for automatic interpretation for structural heart diseases like AS. When deployed, SMMIL can combine information from two input modalities, spec
    
[^144]: 波斯语俚语文本转换为正式文本以及社交媒体上波斯语短文本的深度学习用于情感分类

    Persian Slang Text Conversion to Formal and Deep Learning of Persian Short Texts on Social Media for Sentiment Classification

    [https://arxiv.org/abs/2403.06023](https://arxiv.org/abs/2403.06023)

    通过提供PSC工具将波斯语俚语文本转换为正式文本，结合深度学习方法进行波斯语短文本的情感学习。

    

    缺乏适合分析波斯语会话文本的工具使得对这些文本（包括情感分析）的各种分析变得困难。本研究尝试通过提供PSC（波斯语俚语转换器），将会话文本转换为正式文本，并结合最新和最佳的深度学习方法，使机器更容易理解这些文本，更好地进行波斯语短文本的情感学习。

    arXiv:2403.06023v1 Announce Type: new  Abstract: The lack of a suitable tool for the analysis of conversational texts in the Persian language has made various analyses of these texts, including Sentiment Analysis, difficult. In this research, we tried to make the understanding of these texts easier for the machine by providing PSC, Persian Slang Converter, a tool for converting conversational texts into formal ones, and by using the most up-to-date and best deep learning methods along with the PSC, the sentiment learning of short Persian language texts for the machine in a better way. be made More than 10 million unlabeled texts from various social networks and movie subtitles (as Conversational texts) and about 10 million news texts (as formal texts) have been used for training unsupervised models and formal implementation of the tool. 60,000 texts from the comments of Instagram social network users with positive, negative, and neutral labels are considered supervised data for trainin
    
[^145]: 电子商务搜索中的分层查询分类

    Hierarchical Query Classification in E-commerce Search

    [https://arxiv.org/abs/2403.06021](https://arxiv.org/abs/2403.06021)

    提出了一个利用分层信息的新框架，以增强表示学习，解决了分层查询分类中的类别不平衡和查询模糊性带来的挑战。

    

    电子商务平台通常以层次结构存储和构造产品信息和搜索数据。将用户搜索查询有效分类到类似的层次结构中，在提升电子商务平台用户体验的同时，对于新闻整理和学术研究也至关重要。处理敏感查询分类或关键信息传播时，精确性的重要性得到放大，因为不准确可能带来相当大的负面影响。分层查询分类的固有复杂性受到两个主要挑战的影响：（1）明显的偏向主导类别的类别不平衡性，和（2）搜索查询的固有简洁性和模糊性，阻碍了准确分类。为了解决这些挑战，我们引入了一种新颖的框架，通过（i）增强的表示学习来利用分层信息。

    arXiv:2403.06021v1 Announce Type: cross  Abstract: E-commerce platforms typically store and structure product information and search data in a hierarchy. Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification.   To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss
    
[^146]: 多条件图扩散用于神经架构搜索

    Multi-conditioned Graph Diffusion for Neural Architecture Search

    [https://arxiv.org/abs/2403.06020](https://arxiv.org/abs/2403.06020)

    提出了一种基于图扩散的NAS方法，结合多条件无分类器指导方法，能在架构搜索中生成快速且性能优越的神经网络架构，并在多个标准基准和ImageNet数据集上取得了有希望的结果

    

    神经架构搜索通过探索庞大且复杂的架构搜索空间来自动设计神经网络架构。为了推动架构搜索，我们提出了一种基于图扩散的NAS方法，该方法使用离散条件图扩散过程生成性能优越的神经网络架构。我们随后提出了一种多条件无分类器指导方法，应用于图扩散网络，共同施加诸如高准确性和低硬件延迟等约束。与相关工作不同，我们的方法完全可微分，并且仅需要单模型训练。在我们的评估中，我们展示了在六个标准基准上取得了有希望的结果，以快速速度生成新颖且独特的架构，即每种架构少于0.2秒。此外，我们通过在ImageNet数据集上的实验展示了我们方法的泛化能力和效率。

    arXiv:2403.06020v1 Announce Type: new  Abstract: Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space. To advance the architecture search, we present a graph diffusion-based NAS approach that uses discrete conditional graph diffusion processes to generate high-performing neural network architectures. We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency. Unlike the related work, our method is completely differentiable and requires only a single model training. In our evaluations, we show promising results on six standard benchmarks, yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset.
    
[^147]: 少样本跨语言迁移用于在低资源语言中提示大型语言模型

    Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages

    [https://arxiv.org/abs/2403.06018](https://arxiv.org/abs/2403.06018)

    本研究评估了如何将具有70亿参数的开源PLM LLaMa 用于低资源语言的提示，解决了跨语言适应提示的问题。

    

    大型预训练语言模型（PLMs）处于自然语言处理进展的前沿。PLMs的一个广泛应用是“提示” - 或上下文学习 - 用户在提示PLM对新示例执行任务之前向PLM提供任务描述和一些完成的示例作为上下文。目前只有最大、最有能力的PLMs才能有效地执行上下文学习，而这些模型通常是通过主要以英语为语料库训练的，其他所有语言都落后。大多数语言的数据限制阻碍了训练具有提示能力的语言特定PLMs。尽管在提示设置方面的工作激增，目前仍不清楚如何将PLMs专门用于跨语言适应提示。我们评估了适应LLaMa的可能方法，LLaMa是一个主要在英语中训练的具有70亿参数的开源PLM，用于在低资源语言中进行提示。

    arXiv:2403.06018v1 Announce Type: cross  Abstract: Large pre-trained language models (PLMs) are at the forefront of advances in Natural Language Processing. One widespread use case of PLMs is "prompting" - or in-context learning - where a user provides a description of a task and some completed examples of the task to a PLM as context before prompting the PLM to perform the task on a new example. Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind. The data limitations in most languages preclude the training of language-specific PLMs capable of prompting. Albeit the surge in work of prompting settings, it is still unclear how PLMs should be adapted cross-lingually specifically for prompting. We evaluate the possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, nam
    
[^148]: 弥补公平图学习数据集的不足：走向一个新的基准

    Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark

    [https://arxiv.org/abs/2403.06017](https://arxiv.org/abs/2403.06017)

    通过引入满足广泛要求的合成、半合成和真实世界数据集，解决了公平图学习中数据集信息不足的问题。

    

    公平图学习在许多实际应用中起着关键作用。最近，提出了许多公平图学习方法；然而，它们的评估往往依赖于构造不佳的半合成数据集或低标准的真实世界数据集。在这种情况下，即使是基本的多层感知机（MLP）也可以在效用和公平性方面胜过图神经网络（GNNs）。在这项工作中，我们说明许多数据集未能提供有意义的边缘信息，这可能挑战使用图结构在这些问题中的必要性。为解决这些问题，我们开发并引入了一个充分满足广泛要求的合成、半合成和真实世界数据集集合。这些数据集经过精心设计，包括对于模型公平评价至关重要的相关图结构和偏差信息。所提出的合成和半合成数据集提供了灵活性...

    arXiv:2403.06017v1 Announce Type: new  Abstract: Fair graph learning plays a pivotal role in numerous practical applications. Recently, many fair graph learning methods have been proposed; however, their evaluation often relies on poorly constructed semi-synthetic datasets or substandard real-world datasets. In such cases, even a basic Multilayer Perceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utility and fairness. In this work, we illustrate that many datasets fail to provide meaningful information in the edges, which may challenge the necessity of using graph structures in these problems. To address these issues, we develop and introduce a collection of synthetic, semi-synthetic, and real-world datasets that fulfill a broad spectrum of requirements. These datasets are thoughtfully designed to include relevant graph structures and bias information crucial for the fair evaluation of models. The proposed synthetic and semi-synthetic datasets offer the flexibility to
    
[^149]: 移植：使随机森林一致化

    Grafting: Making Random Forests Consistent

    [https://arxiv.org/abs/2403.06015](https://arxiv.org/abs/2403.06015)

    本文探讨了将一致化估计器移植到浅层决策树（CART）的适用性，并表明这种方法具有一致性保证并在实证环境中表现良好。

    

    尽管随机森林在性能和广泛应用方面表现出色，但关于其理论知之甚少。一个未解之谜是随机森林算法是否一致化，或何时达到一致化。文献探讨了经典随机森林算法的各种变体，以解决这一问题和已知缺陷。本文为这一文献做出了贡献。具体来说，探讨了将一致化估计器移植到浅层CART的适用性。结果表明，这种方法具有一致化保证并在实证环境中表现良好。

    arXiv:2403.06015v1 Announce Type: cross  Abstract: Despite their performance and widespread use, little is known about the theory of Random Forests. A major unanswered question is whether, or when, the Random Forest algorithm is consistent. The literature explores various variants of the classic Random Forest algorithm to address this question and known short-comings of the method. This paper is a contribution to this literature. Specifically, the suitability of grafting consistent estimators onto a shallow CART is explored. It is shown that this approach has a consistency guarantee and performs well in empirical settings.
    
[^150]: 基于硬标签的小查询黑盒对抗攻击

    Hard-label based Small Query Black-box Adversarial Attack

    [https://arxiv.org/abs/2403.06014](https://arxiv.org/abs/2403.06014)

    提出了一种新的基于硬标签的黑盒对抗攻击方法，通过在优化过程中利用预训练替代模型的指导，显著提高了攻击的查询效率。

    

    我们考虑了基于硬标签的黑盒对抗攻击设置，仅观察来自目标模型的预测类别。在这种设置中，大多数攻击方法都需要不切实际数量的查询才能实现成功攻击。解决这一缺点的一种方法是利用白盒替代模型与黑盒目标模型之间的对抗传递性。然而，采用这种方法的大多数方法都是基于软标签的，以充分利用零阶优化。与主流方法不同，我们提出了一种新的实用的基于硬标签的攻击设置，其优化过程由预训练的替代模型指导。实验证明，所提出的方法显著提高了在各种目标模型架构上的基于硬标签的黑盒攻击的查询效率。我们发现，所提出的方法实现了约5倍更高的攻击成功率。

    arXiv:2403.06014v1 Announce Type: cross  Abstract: We consider the hard label based black box adversarial attack setting which solely observes predicted classes from the target model. Most of the attack methods in this setting suffer from impractical number of queries required to achieve a successful attack. One approach to tackle this drawback is utilising the adversarial transferability between white box surrogate models and black box target model. However, the majority of the methods adopting this approach are soft label based to take the full advantage of zeroth order optimisation. Unlike mainstream methods, we propose a new practical setting of hard label based attack with an optimisation process guided by a pretrained surrogate model. Experiments show the proposed method significantly improves the query efficiency of the hard label based black-box attack across various target model architectures. We find the proposed method achieves approximately 5 times higher attack success rat
    
[^151]: 分类鲁棒性和解释鲁棒性是否真的强相关？通过输入损失景观的分析

    Are Classification Robustness and Explanation Robustness Really Strongly Correlated? An Analysis Through Input Loss Landscape

    [https://arxiv.org/abs/2403.06013](https://arxiv.org/abs/2403.06013)

    通过新颖的评估方法和训练方法，本研究发现增强解释鲁棒性并不能提高分类鲁棒性，这一发现挑战了传统观念。

    

    本文深入探讨了深度学习鲁棒性领域，挑战了传统观念，即图像分类系统中的分类鲁棒性和解释鲁棒性本质上是相关的。通过一种新颖的评估方法，利用聚类来有效评估解释鲁棒性，我们展示了增强解释鲁棒性并不一定会使输入损失景观相对于解释损失变平 - 与损失景观变平表示更好的分类鲁棒性相反。为了深入研究这一矛盾，提出了一种突破性的训练方法，旨在调整相对于解释损失的损失景观。通过这种新的训练方法，我们发现虽然这种调整可以影响解释的鲁棒性，但它们对分类的鲁棒性没有影响。这些发现不仅挑战了流行的观念

    arXiv:2403.06013v1 Announce Type: new  Abstract: This paper delves into the critical area of deep learning robustness, challenging the conventional belief that classification robustness and explanation robustness in image classification systems are inherently correlated. Through a novel evaluation approach leveraging clustering for efficient assessment of explanation robustness, we demonstrate that enhancing explanation robustness does not necessarily flatten the input loss landscape with respect to explanation loss - contrary to flattened loss landscapes indicating better classification robustness. To deeply investigate this contradiction, a groundbreaking training method designed to adjust the loss landscape with respect to explanation loss is proposed. Through the new training method, we uncover that although such adjustments can impact the robustness of explanations, they do not have an influence on the robustness of classification. These findings not only challenge the prevailing 
    
[^152]: 针对多元金融目标的强化学习工资优化

    Reinforcement Learning Paycheck Optimization for Multivariate Financial Goals

    [https://arxiv.org/abs/2403.06011](https://arxiv.org/abs/2403.06011)

    该论文提出了一种针对多个竞争性金融目标的工资优化问题的强化学习方法，并提出了统一不同目标、整合用户偏好和处理随机利率的问题形式化，为解决这一问题提供了新思路。

    

    我们研究了工资优化，探讨如何分配收入以实现多个竞争性金融目标。对于工资优化来说，由于缺乏合适的问题形式化，缺少定量方法。为了解决这个问题，我们将问题形式化为效用最大化问题。提出的形式化能够（i）统一不同的金融目标；（ii）整合用户关于目标的偏好；（iii）处理随机利率。所提出的形式化还有助于端到端的强化学习解决方案，在多种问题设置上进行了实现。

    arXiv:2403.06011v1 Announce Type: new  Abstract: We study paycheck optimization, which examines how to allocate income in order to achieve several competing financial goals. For paycheck optimization, a quantitative methodology is missing, due to a lack of a suitable problem formulation. To deal with this issue, we formulate the problem as a utility maximization problem. The proposed formulation is able to (i) unify different financial goals; (ii) incorporate user preferences regarding the goals; (iii) handle stochastic interest rates. The proposed formulation also facilitates an end-to-end reinforcement learning solution, which is implemented on a variety of problem settings.
    
[^153]: 用于安全可靠LLM的检测器：实现、用途和局限性

    Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations

    [https://arxiv.org/abs/2403.06009](https://arxiv.org/abs/2403.06009)

    创建了一系列检测器库，其中包含紧凑且易于构建的分类模型，为各种危害提供标签，可作为大型语言模型（LLMs）的有效替代方案。

    

    大型语言模型（LLMs）容易受到各种风险的影响，从输出不忠实到有偏见和有毒的生成。由于围绕LLMs存在的几个限制性因素（训练成本、API访问、数据可用性等），在部署模型时可能并非总是可行施加直接安全约束。因此，需要一个高效可靠的替代方案。为此，我们正在努力创建和部署一系列检测器库：紧凑且易于构建的分类模型，为各种危害提供标签。除了检测器本身，我们还讨论了这些检测器模型的广泛用途——从充当防护栏到促进有效的AI治理。我们还深入探讨了它们的开发中固有的挑战，并讨论了未来工作，旨在使检测器更可靠并拓展其范围。

    arXiv:2403.06009v1 Announce Type: new  Abstract: Large language models (LLMs) are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding LLMs (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model. Therefore, an efficient and reliable alternative is required. To this end, we present our ongoing efforts to create and deploy a library of detectors: compact and easy-to-build classification models that provide labels for various harms. In addition to the detectors themselves, we discuss a wide range of uses for these detector models - from acting as guardrails to enabling effective AI governance. We also deep dive into inherent challenges in their development and discuss future work aimed at making the detectors more reliable and broadening their scope.
    
[^154]: 一种推广的用于基于偏好的奖励学习的收益函数

    A Generalized Acquisition Function for Preference-based Reward Learning

    [https://arxiv.org/abs/2403.06003](https://arxiv.org/abs/2403.06003)

    通过引入一种可以捕捉奖励函数相似性定义的框架，优化了奖励函数的学习效率。

    

    基于偏好的奖励学习是一种用于教导机器人和自主系统如何执行任务的流行技术。 先前的研究表明，积极合成偏好查询以最大化关于奖励函数参数的信息增益可以提高数据效率。 信息增益标准侧重于精确识别奖励函数的所有参数。 这可能是低效的，因为许多参数可能导致相同的奖励，并且许多奖励可能导致下游任务中的相同行为。 相反，我们展示了可以优化学习奖励函数直到行为等效类，例如诱导出相同的行为排序，选择分布，或其他相关定义使得两个奖励看起来相似。 我们引入了一个可以捕捉这种相似性定义的可处理框架。 我们在一个.synthetic env的实验中展示了这一点

    arXiv:2403.06003v1 Announce Type: cross  Abstract: Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic env
    
[^155]: 用高更新比例剖析深度强化学习：应对价值高估和发散

    Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence

    [https://arxiv.org/abs/2403.05996](https://arxiv.org/abs/2403.05996)

    本研究剖析了深度强化学习中的首要偏差现象，发现在大量更新比例下，价值高估是导致学习失败的根本挑战。

    

    我们展示了深度强化学习在设置中可以在梯度更新次数大大超过环境样本数量的情况下保持学习能力，而无需重置网络参数。在这种大量更新与数据比例的情况下，尼基辛等人 (2022) 的最近一项研究指出了一个首要偏差的出现，即代理在早期交互中过拟合并淡化后续经验，从而损害了其学习能力。在这项工作中，我们深入解析了导致首要偏差的现象。我们检查了应该导致学习失败的训练早期阶段，并发现一个根本性挑战是长期以来存在的问题：价值高估。我们发现Q值不仅在分布外数据上被高估，而且在分布内数据上也是如此，可以追溯到由优化器动量推动的未见的动作预测。我们采用了一种简单的单位球归一化方法，可以在大更新比例下实现学习。

    arXiv:2403.05996v1 Announce Type: cross  Abstract: We show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we dissect the phenomena underlying the primacy bias. We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios
    
[^156]: 通过强化学习增强特征选择的分类性能

    Enhancing Classification Performance via Reinforcement Learning for Feature Selection

    [https://arxiv.org/abs/2403.05979](https://arxiv.org/abs/2403.05979)

    通过强化学习算法，特别是Q学习和SARSA学习，优化特征选择以增强分类模型的性能，取得了87%和88%的最高分类准确率。

    

    特征选择在提高预测准确性方面发挥着至关重要的作用，通过识别相关特征并过滤无关特征。本研究探讨了有效特征选择在提升分类模型性能中的重要性。通过采用强化学习（RL）算法，具体来说是Q学习（QL）和SARSA学习，本文解决了特征选择挑战。通过使用乳腺癌科英布拉数据集（BCCDS）和三种归一化方法（最小-最大，l1和l2），研究评估了这些算法的性能。结果显示，QL@最小-最大和SARSA@l2分别达到了最高的分类准确率，分别达到了87%和88%。这突显了基于RL的特征选择方法在优化分类任务方面的有效性，有助于提高模型的准确性和效率。

    arXiv:2403.05979v1 Announce Type: new  Abstract: Feature selection plays a crucial role in improving predictive accuracy by identifying relevant features while filtering out irrelevant ones. This study investigates the importance of effective feature selection in enhancing the performance of classification models. By employing reinforcement learning (RL) algorithms, specifically Q-learning (QL) and SARSA learning, this paper addresses the feature selection challenge. Using the Breast Cancer Coimbra dataset (BCCDS) and three normalization methods (Min-Max, l1, and l2), the study evaluates the performance of these algorithms. Results show that QL@Min-Max and SARSA@l2 achieve the highest classification accuracies, reaching 87% and 88%, respectively. This highlights the effectiveness of RL-based feature selection methods in optimizing classification tasks, contributing to improved model accuracy and efficiency.
    
[^157]: 仅使用生成来校准大型语言模型

    Calibrating Large Language Models Using Their Generations Only

    [https://arxiv.org/abs/2403.05973](https://arxiv.org/abs/2403.05973)

    使用APRICOT方法，通过仅使用大型语言模型的文本输入和输出来设置置信目标并训练额外模型，从而实现大型语言模型的校准。

    

    随着大型语言模型（LLMs）越来越多地部署在面向用户的应用程序中，通过准确量化模型对其预测的信心来建立信任并保持安全性变得更加重要。然而，找到有效的方法来校准LLMs - 尤其是当与模型的唯一接口是它们生成的文本时 - 仍然是一个挑战。我们提出了APRICOT（辅助预测置信目标）：一种通过仅使用其文本输入和输出来设置置信目标并训练一个额外模型来预测LLM置信度的方法。这种方法有几个优点：概念上简单，不需要访问目标模型超出其输出，不干扰语言生成，并且有多种潜在用途，例如通过言语化预测的置信度或根据置信度调整给定的答案。我们展示了我们的方法如何具有竞争性能。

    arXiv:2403.05973v1 Announce Type: cross  Abstract: As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge. We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively
    
[^158]: 能生成模型改进自监督表示学习吗？

    Can Generative Models Improve Self-Supervised Representation Learning?

    [https://arxiv.org/abs/2403.05966](https://arxiv.org/abs/2403.05966)

    本文引入了一个新的框架，通过利用生成模型生成语义一致的图像增强，丰富了自监督学习的方法，实验结果表明提高了学到的视觉质量。

    

    自监督学习的快速发展突显了其利用无标签数据学习强大视觉表示的潜力。然而，现有的自监督学习方法，特别是那些利用同一图像的不同视图的方法，通常依赖于一组预定义的数据增强，这限制了变换的多样性和质量，导致表示不够优化。本文引入了一个新颖的框架，通过利用生成模型产生语义一致的图像增强，丰富了自监督学习范式。通过直接在源图像表示上进行条件生成模型，我们的方法能够生成多样的增强，同时保持源图像的语义，为自监督学习提供更丰富的数据集。我们的实验结果表明，我们的框架显著提高了学到的视觉的质量。

    arXiv:2403.05966v1 Announce Type: cross  Abstract: The rapid advancement in self-supervised learning (SSL) has highlighted its potential to leverage unlabeled data for learning powerful visual representations. However, existing SSL approaches, particularly those employing different views of the same image, often rely on a limited set of predefined data augmentations. This constrains the diversity and quality of transformations, which leads to sub-optimal representations. In this paper, we introduce a novel framework that enriches the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations. By directly conditioning generative models on a source image representation, our method enables the generation of diverse augmentations while maintaining the semantics of the source image, thus offering a richer set of data for self-supervised learning. Our experimental results demonstrate that our framework significantly enhances the quality of learned visu
    
[^159]: 在上下文去偏的情绪识别中的鲁棒性

    Robust Emotion Recognition in Context Debiasing

    [https://arxiv.org/abs/2403.05963](https://arxiv.org/abs/2403.05963)

    提出了一个反事实情绪推理（CLEF）框架来解决上下文偏差干扰的挑战

    

    上下文感知情绪识别（CAER）最近在无约束环境中推动了情感计算技术的实际应用。 主流的CAER方法总是从不同的上下文和以主体为中心的特征中提取集成表示，以感知目标人物的情绪状态。 尽管有所进展，但最大的挑战仍然是由于上下文偏差的干扰。 有害的偏见迫使模型依赖于背景上下文和情感标签之间的虚假相关性，在可能性估计中造成严重的性能瓶颈，并使有价值的上下文先验混淆。 在本文中，我们提出了一个反事实情绪推理（CLEF）框架来解决上述问题。 具体而言，我们首先制定了一个广义因果图，以解耦CAER中变量之间的因果关系。 遵循因果图，CLEF引入了一个非侵入式的上下文分支来获取

    arXiv:2403.05963v1 Announce Type: cross  Abstract: Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph, CLEF introduces a non-invasive context branch to capt
    
[^160]: 通用外科视觉变换器：用于通用外科的视频预训练基础模型

    General surgery vision transformer: A video pre-trained foundation model for general surgery

    [https://arxiv.org/abs/2403.05949](https://arxiv.org/abs/2403.05949)

    该论文开源了迄今为止最大的通用外科视频数据集，提出了用于外科应用的视频预训练通用外科视觉变换器（GSViT）技术，并展示了其在Cholec80阶段注释任务上的优越性能。

    

    缺乏开放获取的数据和专门的基础模型是外科计算研究的主要障碍。为此，我们开源迄今为止最大的通用外科视频数据集，包括来自28种手术技术的680小时手术视频数据；我们提出了一种基于前向视频预测的通用外科视觉变换器（GSViT）视频预训练技术，可实时运行用于外科应用，我们还开源了GSViT的代码和权重；我们还发布了针对10种手术程序的特定程序微调版本的GSViT的代码和权重；我们展示了GSViT在Cholec80阶段注释任务上的性能，显示出优于最先进的单帧预测器的性能。

    arXiv:2403.05949v1 Announce Type: cross  Abstract: The absence of openly accessible data and specialized foundation models is a major barrier for computational research in surgery. Toward this, (i) we open-source the largest dataset of general surgery videos to-date, consisting of 680 hours of surgical videos, including data from robotic and laparoscopic techniques across 28 procedures; (ii) we propose a technique for video pre-training a general surgery vision transformer (GSViT) on surgical videos based on forward video prediction that can run in real-time for surgical applications, toward which we open-source the code and weights of GSViT; (iii) we also release code and weights for procedure-specific fine-tuned versions of GSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the Cholec80 phase annotation task, displaying improved performance over state-of-the-art single frame predictors.
    
[^161]: 使用优化提示的Transformer进行线程检测和响应生成

    Thread Detection and Response Generation using Transformers with Prompt Optimisation

    [https://arxiv.org/abs/2403.05931](https://arxiv.org/abs/2403.05931)

    通过优化提示，该研究提出了一种端到端模型，能够识别对话中的线程并基于其重要性优先生成响应。

    

    Conversational systems 对于人机交互至关重要，通过识别线程并优先响应来管理复杂对话。 在多方对话中尤为重要，其中线程的准确识别和策略性响应优先级确保高效对话管理。 为了解决这些挑战，开发了一种端到端模型，根据重要性识别线程并优先生成其响应，涉及将问题系统地分解为离散组件 - 线程检测、优先级和性能优化，并对其进行精心分析和优化。 这些精细的组件无缝集成到统一框架中，在会话系统中。 由于其高通用性，Llama2 7b被用于，该系统可以使用任何开源大型语言模型(LLM) 进行更新。 Llama2 模型的计算能力得到增强

    arXiv:2403.05931v1 Announce Type: new  Abstract: Conversational systems are crucial for human-computer interaction, managing complex dialogues by identifying threads and prioritising responses. This is especially vital in multi-party conversations, where precise identification of threads and strategic response prioritisation ensure efficient dialogue management. To address these challenges an end-to-end model that identifies threads and prioritises their response generation based on the importance was developed, involving a systematic decomposition of the problem into discrete components - thread detection, prioritisation, and performance optimisation which was meticulously analysed and optimised. These refined components seamlessly integrate into a unified framework, in conversational systems. Llama2 7b is used due to its high level of generalisation but the system can be updated with any open source Large Language Model(LLM). The computational capabilities of the Llama2 model was aug
    
[^162]: 基于残差网络的扩散建模在不平衡数据上的应用

    SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data

    [https://arxiv.org/abs/2403.05918](https://arxiv.org/abs/2403.05918)

    基于残差网络的扩散建模方法能够有效处理不平衡数据，克服了经典过采样方法和基于生成网络的模式塌陷与训练不稳定问题。

    

    在数据挖掘和机器学习领域，通常使用的分类模型在不平衡数据中无法有效学习。为了平衡模型训练前的数据分布，通常使用过采样方法为少数类生成数据，以解决分类不平衡数据的问题。大多数经典的过采样方法基于SMOTE技术，该技术仅关注数据的局部信息，因此生成的数据可能存在不够逼真的问题。在基于生成网络的当前过采样方法中，基于GAN的方法可以捕获数据的真实分布，但训练中存在模式崩溃和不稳定性的问题；基于去噪扩散概率模型的过采样方法中，使用U-Net的逆扩散过程神经网络不适用于表格数据。

    arXiv:2403.05918v1 Announce Type: cross  Abstract: In the field of data mining and machine learning, commonly used classification models cannot effectively learn in unbalanced data. In order to balance the data distribution before model training,oversamplingmethods are often used to generate data for a small number of classes to solve the problem of classifying unbalanced data. Most of the classical oversampling methods are based on theSMOTE technique, which only focuses on the local information of the data, and therefore the generated data may have the problem of not being realistic enough. In the current oversampling methods based on generative networks, the methods based on GANs can capture the true distribution of data, but there is the problem of pattern collapse and training instability in training; in the oversampling methods based on denoising diffusion probability models, the neural network of the inverse diffusion process using the U-Net is not applicable to tabular data, and
    
[^163]: 使用采样数据在线识别随机连续时间Wiener模型

    Online Identification of Stochastic Continuous-Time Wiener Models Using Sampled Data

    [https://arxiv.org/abs/2403.05899](https://arxiv.org/abs/2403.05899)

    开发了一种简单的递归在线估计算法，用于通过随机逼近识别连续时间随机参数Wiener模型，具有对扰动过程频谱假设鲁棒性。

    

    众所周知，在识别随机Wiener模型中忽略随机扰动的存在会导致渐近偏倚的估计。另一方面，通过基于似然的方法进行最优统计识别对数据分布的假设敏感，通常基于相对复杂的顺序蒙特卡洛算法。我们基于输出误差预测器开发了一种简单的递归在线估计算法，用于通过随机逼近识别连续时间随机参数Wiener模型。该方法适用于通用模型参数化，并且正如数值模拟示例所证明的那样，它对扰动过程频谱的假设具有鲁棒性。

    arXiv:2403.05899v1 Announce Type: cross  Abstract: It is well known that ignoring the presence of stochastic disturbances in the identification of stochastic Wiener models leads to asymptotically biased estimators. On the other hand, optimal statistical identification, via likelihood-based methods, is sensitive to the assumptions on the data distribution and is usually based on relatively complex sequential Monte Carlo algorithms. We develop a simple recursive online estimation algorithm based on an output-error predictor, for the identification of continuous-time stochastic parametric Wiener models through stochastic approximation. The method is applicable to generic model parameterizations and, as demonstrated in the numerical simulation examples, it is robust with respect to the assumptions on the spectrum of the disturbance process.
    
[^164]: 朝着联邦增量学习中高效的重播

    Towards Efficient Replay in Federated Incremental Learning

    [https://arxiv.org/abs/2403.05890](https://arxiv.org/abs/2403.05890)

    本研究提出了一种名为Re-Fed的简单通用框架，用于联邦增量学习中的重播，通过协调每个客户端缓存重要样本以减轻灾难性遗忘问题。

    

    在联邦学习（FL）中，通常假定每个客户端的数据是固定或静态的。然而，在现实世界的应用中，数据通常以增量方式到来，其中数据领域可能动态增加。在这项工作中，我们研究了在边缘客户端在联邦增量学习（FIL）场景中因数据异构性而可能缺乏足够存储空间以保留完整数据的灾难性遗忘。我们提出了一种名为Re-Fed的简单、通用的FIL框架，它可以协调每个客户端缓存重播的重要样本。具体而言，当出现新任务时，每个客户端首先基于它们的全局和本地重要性缓存选定的先前样本。然后，客户端使用既缓存的样本又使用新任务的样本训练本地模型。在理论上，我们分析了Re-Fed发现重播重要样本的能力，从而缓解了灾难性遗忘问题。

    arXiv:2403.05890v1 Announce Type: new  Abstract: In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we e
    
[^165]: DiffRed: 由稳定秩引导的降维

    DiffRed: Dimensionality Reduction guided by stable rank

    [https://arxiv.org/abs/2403.05882](https://arxiv.org/abs/2403.05882)

    DiffRed 提出了一种由稳定秩引导的降维方法，证明了在 Stress 和 M1 上取得了较紧密的上界，并通过实验证明在多种真实世界数据集上取得了良好效果。

    

    在这项工作中，我们提出了一种新颖的降维技术 DiffRed，该技术首先沿着前 $k_1$ 个主成分投影数据矩阵 A，然后将剩余矩阵 $A^{*}$（减去其 $k_1$-秩近似后剩余的部分）沿着 $k_2$ 个高斯随机向量进行投影。我们评估了 M1，即均方对距离失真，以及 Stress，即对成对距离失真的均方根值的归一化值。我们严格证明 DiffRed 在 Stress 上取得了 $O\left(\sqrt{\frac{1-p}{k_2}}\right)$ 的一般上界，在 M1 上取得了 $O\left(\frac{(1-p)}{\sqrt{k_2*\rho(A^{*})}}\right)$ 的一般上界，其中$p$ 是由前 $k_1$ 个主成分解释的方差分数，$\rho(A^{*})$ 是 $A^{*}$ 的稳定秩。这些上界比目前已知的随机映射结果更紧密。我们在各种真实世界数据集上进行了大量实验证明，DiffRed 实现了接近零的 M1 和更低的值。

    arXiv:2403.05882v1 Announce Type: new  Abstract: In this work, we propose a novel dimensionality reduction technique, DiffRed, which first projects the data matrix, A, along first $k_1$ principal components and the residual matrix $A^{*}$ (left after subtracting its $k_1$-rank approximation) along $k_2$ Gaussian random vectors. We evaluate M1, the distortion of mean-squared pair-wise distance, and Stress, the normalized value of RMS of distortion of the pairwise distances. We rigorously prove that DiffRed achieves a general upper bound of $O\left(\sqrt{\frac{1-p}{k_2}}\right)$ on Stress and $O\left(\frac{(1-p)}{\sqrt{k_2*\rho(A^{*})}}\right)$ on M1 where $p$ is the fraction of variance explained by the first $k_1$ principal components and $\rho(A^{*})$ is the stable rank of $A^{*}$. These bounds are tighter than the currently known results for Random maps. Our extensive experiments on a variety of real-world datasets demonstrate that DiffRed achieves near zero M1 and much lower values 
    
[^166]: 基于深度学习的声学测量方法在骨科机器人应用中的研究

    Deep Learning based acoustic measurement approach for robotic applications on orthopedics

    [https://arxiv.org/abs/2403.05879](https://arxiv.org/abs/2403.05879)

    本研究提出了一种基于深度学习的结构，通过A模式超声波（US）改进骨骼跟踪的准确性。

    

    在全膝关节置换手术（TKA）中，外科机器人可以提供图像引导导航，以高精度安装植入物。其跟踪方法高度依赖于将骨针插入由光学跟踪系统跟踪的骨骼。为了解决这个问题，我们提出了一种改进骨骼跟踪准确性的新型深度学习结构，通过A模式超声波（US）进行。我们首先从尸体实验中获得了一组超声波数据集，通过骨针计算得出骨骼的真实位置。这些数据被用来训练我们提出的CasAtt-UNet，自动和稳健地预测骨骼位置。真实的骨骼位置和US的位置

    arXiv:2403.05879v1 Announce Type: cross  Abstract: In Total Knee Replacement Arthroplasty (TKA), surgical robotics can provide image-guided navigation to fit implants with high precision. Its tracking approach highly relies on inserting bone pins into the bones tracked by the optical tracking system. This is normally done by invasive, radiative manners (implantable markers and CT scans), which introduce unnecessary trauma and prolong the preparation time for patients. To tackle this issue, ultrasound-based bone tracking could offer an alternative. In this study, we proposed a novel deep learning structure to improve the accuracy of bone tracking by an A-mode ultrasound (US). We first obtained a set of ultrasound dataset from the cadaver experiment, where the ground truth locations of bones were calculated using bone pins. These data were used to train the proposed CasAtt-UNet to predict bone location automatically and robustly. The ground truth bone locations and those locations of US 
    
[^167]: LEGION：利用分布平衡损失调整预训练语言模型进行GitHub主题推荐

    LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss

    [https://arxiv.org/abs/2403.05873](https://arxiv.org/abs/2403.05873)

    提出了一种名为Legion的新方法，利用预训练语言模型（PTMs）为GitHub存储库推荐主题，以解决现有技术在主题推荐中的局限性。

    

    开源开发通过促进协作、透明性和社区驱动的创新，彻底改变了软件行业。如今，大量各种类型的开源软件，形成了网络存储库，通常托管在GitHub上-一种流行的软件开发平台。为了增强存储库网络的可发现性，即相似存储库组，GitHub在2017年引入了存储库主题，使用户更容易按类型、技术等浏览相关项目。因此，准确为每个GitHub存储库分配主题至关重要。目前用于自动主题推荐的方法主要依赖于TF-IDF来对文本数据进行编码，存在理解语义细微差别的挑战。本文通过提出Legion，一种利用预训练语言模型（PTMs）推荐主题的新方法，解决了现有技术的局限性。

    arXiv:2403.05873v1 Announce Type: cross  Abstract: Open-source development has revolutionized the software industry by promoting collaboration, transparency, and community-driven innovation. Today, a vast amount of various kinds of open-source software, which form networks of repositories, is often hosted on GitHub - a popular software development platform. To enhance the discoverability of the repository networks, i.e., groups of similar repositories, GitHub introduced repository topics in 2017 that enable users to more easily explore relevant projects by type, technology, and more. It is thus crucial to accurately assign topics for each GitHub repository. Current methods for automatic topic recommendation rely heavily on TF-IDF for encoding textual data, presenting challenges in understanding semantic nuances. This paper addresses the limitations of existing techniques by proposing Legion, a novel approach that leverages Pre-trained Language Models (PTMs) for recommending topics for 
    
[^168]: PAPER-HILT：个性化和自适应隐私感知的强化学习提前退出在人机协同系统中的应用

    PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems

    [https://arxiv.org/abs/2403.05864](https://arxiv.org/abs/2403.05864)

    PAPER-HILT是针对人机协同系统中隐私保护的创新自适应强化学习策略，通过提前退出方法动态调整隐私保护和系统效用，以适应个体行为模式和偏好。

    

    强化学习（RL）日益成为人机协同（HITL）应用中的首选方法，因其适应于人类交互的动态特性。然而，在这种环境中整合RL会带来重大的隐私问题，可能会不经意地暴露敏感用户信息。为解决这一问题，我们的论文专注于开发PAPER-HILT，一种创新的自适应RL策略，通过利用专为HITL环境中隐私保护设计的提前退出方法。该方法动态调整隐私保护和系统效用之间的权衡，使其操作适应个人行为模式和偏好。我们主要强调面临处理人类行为的可变和不断发展的挑战，使得静态隐私模型失效。通过其应用，评估了PAPER-HILT的有效性。

    arXiv:2403.05864v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has increasingly become a preferred method over traditional rule-based systems in diverse human-in-the-loop (HITL) applications due to its adaptability to the dynamic nature of human interactions. However, integrating RL in such settings raises significant privacy concerns, as it might inadvertently expose sensitive user information. Addressing this, our paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy through exploiting an early-exit approach designed explicitly for privacy preservation in HITL environments. This approach dynamically adjusts the tradeoff between privacy protection and system utility, tailoring its operation to individual behavioral patterns and preferences. We mainly highlight the challenge of dealing with the variable and evolving nature of human behavior, which renders static privacy models ineffective. PAPER-HILT's effectiveness is evaluated through its applicati
    
[^169]: tLaSDI: 热力学信息驱动的潜空间动力学识别

    tLaSDI: Thermodynamics-informed latent space dynamics identification

    [https://arxiv.org/abs/2403.05848](https://arxiv.org/abs/2403.05848)

    提出了一种融合热力学定律的数据驱动潜空间动力学识别方法，通过自动编码器学习潜变量并构建动力学模型，实现对热力学定律的遵守，并通过新的损失函数进行训练。演示了其在稳健泛化能力方面的表现，以及在潜空间中熵产生速率与系统行为之间的相关性。

    

    我们提出了一种数据驱动的潜空间动力学识别方法（tLaSDI），该方法融入了热力学的第一和第二定律。通过自动编码器学习潜变量作为非线性降维模型。潜变量的动力学由基于神经网络的模型构建，通过通用形式主义保留某些结构以尊重热力学定律。建立了对近似值的抽象误差估计，提供了涉及自动编码器雅可比计算的新损失制定。自动编码器和潜动态都经过训练以最小化新的损失。展示了数值示例以演示tLaSDI的性能，即使在外推情况下也表现出稳健的泛化能力。此外，在潜空间中观察到了熵产生速率与完整行为之间的有趣相关性。

    arXiv:2403.05848v1 Announce Type: new  Abstract: We propose a data-driven latent space dynamics identification method (tLaSDI) that embeds the first and second principles of thermodynamics. The latent variables are learned through an autoencoder as a nonlinear dimension reduction model. The dynamics of the latent variables are constructed by a neural network-based model that preserves certain structures to respect the thermodynamic laws through the GENERIC formalism. An abstract error estimate of the approximation is established, which provides a new loss formulation involving the Jacobian computation of autoencoder. Both the autoencoder and the latent dynamics are trained to minimize the new loss. Numerical examples are presented to demonstrate the performance of tLaSDI, which exhibits robust generalization ability, even in extrapolation. In addition, an intriguing correlation is empirically observed between the entropy production rates in the latent space and the behaviors of the ful
    
[^170]: TrafficGPT：突破令牌限制，实现高效长时间流量分析和生成

    TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic Analysis and Generation

    [https://arxiv.org/abs/2403.05822](https://arxiv.org/abs/2403.05822)

    TrafficGPT 是一个深度学习模型，旨在突破令牌长度限制，实现高效的长时间流量分析和生成，解决了网络流量分析和生成中依赖标记数据和生成符合实际模式的流量样本的难题。

    

    多年来，网络流量分析和生成取得了显著进步。从传统的统计方法，该领域发展到复杂的深度学习技术。这种进步提高了检测复杂模式和安全威胁的能力，以及测试和优化网络性能的能力。然而，仍然存在障碍，例如对标记数据进行分析的依赖以及生成遵循实际模式的流量样本的困难。预训练的深度神经网络已经成为解决这些问题的强大工具，通过从大型无标签数据集中学习健壮的数据表示来提供改进的性能。尽管存在益处，但现有的预训练模型面临令牌长度限制等挑战，这限制了它们在全面流量分析和实际流量生成中的有用性。为了应对这些挑战，我们介绍了TrafficGPT，一个深度学习

    arXiv:2403.05822v1 Announce Type: new  Abstract: Over the years, network traffic analysis and generation have advanced significantly. From traditional statistical methods, the field has progressed to sophisticated deep learning techniques. This progress has improved the ability to detect complex patterns and security threats, as well as to test and optimize network performance. However, obstacles persist, such as the dependence on labeled data for analysis and the difficulty of generating traffic samples that follow realistic patterns. Pre-trained deep neural networks have emerged as powerful tools to resolve these issues, offering improved performance by learning robust data representations from large unlabeled datasets. Despite their benefits, existing pre-trained models face challenges like token length limitation, which restricts their usefulness in comprehensive traffic analysis and realistic traffic generation. To address these challenges, we introduce TrafficGPT, a deep learning
    
[^171]: 在关系型工作负载中优化LLM查询

    Optimizing LLM Queries in Relational Workloads

    [https://arxiv.org/abs/2403.05821](https://arxiv.org/abs/2403.05821)

    本文研究了如何优化在关系查询中调用LLM的分析型工作负载的推理过程，发现关系查询为加速LLM推理提供了新颖的机会。

    

    arXiv:2403.05821v1 公告类型: 新的 摘要: 分析性数据库提供商（例如Redshift、Databricks、BigQuery）已迅速增加对通过本机用户自定义函数（UDFs）调用大型语言模型（LLMs）的支持，以帮助用户在分析型工作负载内执行自然语言任务，例如分类、实体提取和翻译。本文探讨了如何优化关系查询中调用LLM的分析工作负载的推理。我们展示了关系查询为加速LLM推理提供了新颖的机会，包括重新排序行以最大化LLM推理引擎内的键值（KV）缓存重用，重新排序行内的列以进一步。

    arXiv:2403.05821v1 Announce Type: new  Abstract: Analytical database providers (e.g., Redshift, Databricks, BigQuery) have rapidly added support for invoking Large Language Models (LLMs) through native user-defined functions (UDFs) to help users perform natural language tasks, such as classification, entity extraction, and translation, inside analytical workloads. For instance, an analyst might want to extract customer sentiments on millions of product reviews. However, LLM inference is highly expensive in both computational and economic terms: for example, an NVIDIA L4 GPU running Llama2-7B can only process 6 KB of text per second. In this paper, we explore how to optimize LLM inference for analytical workloads that invoke LLMs within relational queries. We show that relational queries present novel opportunities for accelerating LLM inference, including reordering rows to maximize key-value (KV) cache reuse within the LLM inference engine, reordering columns within a row to further i
    
[^172]: PR-NET：利用精细化通路网络结构进行前列腺癌患者状况预测

    PR-NET: Leveraging Pathway Refined Network Structures for Prostate Cancer Patient Condition Prediction

    [https://arxiv.org/abs/2403.05818](https://arxiv.org/abs/2403.05818)

    PR-NET模型通过压缩和优化P-NET的网络结构，降低了模型复杂性，同时保持高准确性和可解释性，在前列腺癌患者状况预测中表现出优越性能。

    

    动机：诊断和监测去势抵抗性前列腺癌（CRPC）对癌症患者至关重要，但目前的模型（如P-NET）在参数数量、泛化能力和成本方面存在局限性。结果：为解决上述问题，我们开发了一种更准确高效的前列腺癌患者状况预测模型，名为PR-NET。通过压缩和优化P-NET的网络结构，降低了模型复杂性，同时保持了高准确性和可解释性。PR-NET在预测前列腺癌患者结果方面表现出色，超过了P-NET和其他六种传统模型显著。在我们的严格评估中，PR-NET不仅在已知数据上取得了令人印象深刻的平均AUC和召回率分数（分别为0.94和0.83），而且在五个未知数据集上保持了强大的泛化能力，平均AUC为0.73，召回率为0

    arXiv:2403.05818v1 Announce Type: new  Abstract: Motivation: The diagnosis and monitoring of Castrate Resistant Prostate Cancer (CRPC) are crucial for cancer patients, but the current models (such as P-NET) have limitations in terms of parameter count, generalization, and cost. Results: To address the above issues, we develop a more accurate and efficient Prostate Cancer patient condition prediction model, named PR-NET. By compressing and optimizing the network structure of P-NET, the model complexity is reduced while maintaining high accuracy and interpretability. The PR-NET demonstrated superior performance in predicting prostate cancer patient outcomes, outshining P-NET and six other traditional models with a significant margin. In our rigorous evaluation, PR-NET not only achieved impressive average AUC and Recall scores of 0.94 and 0.83, respectively, on known data but also maintained robust generalizability on five unknown datasets with a higher average AUC of 0.73 and Recall of 0
    
[^173]: 分布式时间差分的统计效率

    Statistical Efficiency of Distributional Temporal Difference

    [https://arxiv.org/abs/2403.05811](https://arxiv.org/abs/2403.05811)

    该论文分析了分布式时间差分的统计效率和有限样本性能。

    

    分布式强化学习(DRL)关注的是返回的完整分布，而不仅仅是均值，在各个领域取得了经验成功。领域DRL中的核心任务之一是分布式策略评估，涉及估计给定策略pi的返回分布η^pi。相应地提出了分布时间差分(TD)算法，这是经典RL文献中时间差分算法的延伸。在表格案例中，citet{rowland2018analysis}和citet{rowland2023analysis}分别证明了两个分布式TD实例即分类时间差分算法(CTD)和分位数时间差分算法(QTD)的渐近收敛。在这篇论文中，我们进一步分析了分布式TD的有限样本性能。为了促进理论分析，我们提出了一个非参数的 dis

    arXiv:2403.05811v1 Announce Type: cross  Abstract: Distributional reinforcement learning (DRL), which cares about the full distribution of returns instead of just the mean, has achieved empirical success in various domains. One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\eta^\pi$ for a given policy $\pi$. A distributional temporal difference (TD) algorithm has been accordingly proposed, which is an extension of the temporal difference algorithm in the classic RL literature. In the tabular case, \citet{rowland2018analysis} and \citet{rowland2023analysis} proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference algorithm (CTD) and quantile temporal difference algorithm (QTD), respectively. In this paper, we go a step further and analyze the finite-sample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric dis
    
[^174]: 浅层ReLU神经网络和有限元

    Shallow ReLU neural networks and finite elements

    [https://arxiv.org/abs/2403.05809](https://arxiv.org/abs/2403.05809)

    在凸多面体网格上，提出了用两个隐藏层的ReLU神经网络来弱表示分段线性函数，并根据网格中的多面体和超平面的数量准确确定了所需的神经元数，建立了浅层ReLU神经网络和有限元函数之间的联系。

    

    我们指出在凸多面体网格上，可以用两个隐藏层的ReLU神经网络在弱意义下表示（连续或不连续的）分段线性函数。此外，基于涉及到的多面体和超平面的数量，准确给出了弱表示所需的两个隐藏层的神经元数。这些结果自然地适用于常数和线性有限元函数。这种弱表示建立了浅层ReLU神经网络和有限元函数之间的桥梁，并为通过有限元函数分析ReLU神经网络在$L^p$范数中的逼近能力提供了视角。此外，我们还讨论了最近张量神经网络对张量有限元函数的严格表示。

    arXiv:2403.05809v1 Announce Type: cross  Abstract: We point out that (continuous or discontinuous) piecewise linear functions on a convex polytope mesh can be represented by two-hidden-layer ReLU neural networks in a weak sense. In addition, the numbers of neurons of the two hidden layers required to weakly represent are accurately given based on the numbers of polytopes and hyperplanes involved in this mesh. The results naturally hold for constant and linear finite element functions. Such weak representation establishes a bridge between shallow ReLU neural networks and finite element functions, and leads to a perspective for analyzing approximation capability of ReLU neural networks in $L^p$ norm via finite element functions. Moreover, we discuss the strict representation for tensor finite element functions via the recent tensor neural networks.
    
[^175]: $\textbf{S}^2$IP-LLM: 借助LLM进行时间序列预测的语义空间提示学习

    $\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting

    [https://arxiv.org/abs/2403.05798](https://arxiv.org/abs/2403.05798)

    提出了$\textbf{S}^2$IP-LLM，利用预训练的语言模型进行时间序列预测，并将语义空间与时间序列嵌入空间对齐进行提示学习。

    

    最近，利用预训练的大型语言模型（LLM）进行各种时间序列应用引起了越来越多的关注。然而，通过预训练建立的LLM的语义空间仍然未被充分探索，可能有助于产生更加独特和信息丰富的表示，以促进时间序列预测。为此，我们提出了借助LLM进行语义空间提示学习（$\textbf{S}^2$IP-LLM），将预训练的语义空间与时间序列嵌入空间进行对齐，并基于联合空间中学到的提示进行时间序列预测。我们首先设计了一个专为跨模态对齐定制的标记化模块，显式地串联分解的时间序列组件的补丁，以创建能够有效编码时间动态的嵌入。接下来，我们利用预训练的单词标记嵌入来导出语义锚点，并通过最大化对齐所选锚点与时间序列嵌入。

    arXiv:2403.05798v1 Announce Type: new  Abstract: Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embeddings space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maxi
    
[^176]: 线性约束在线凸优化的乐观安全性

    Optimistic Safety for Linearly-Constrained Online Convex Optimization

    [https://arxiv.org/abs/2403.05786](https://arxiv.org/abs/2403.05786)

    本文提出了一种乐观安全性的设计范式，在线性约束在线凸优化问题中取得了$\tilde{\mathcal{O}}(\sqrt{T})$的后悔值，同时将问题转化为在时变随机线性约束下的OCO问题，展示了算法在这个设置下的有效性。

    

    近年来，对于未知约束下的在线凸优化（OCO）问题备受关注。本研究考虑了一个具有静态线性约束的问题版本，玩家会收到嘈杂反馈并且必须始终满足这些约束。通过利用我们创新的乐观安全性设计范式，我们提出了一个算法，该算法在这个问题上的后悔值为$\tilde{\mathcal{O}}(\sqrt{T})$。这一改进了以往$\tilde{\mathcal{O}}(T^{2/3})$的最佳后悔值，并且仅使用了略强的独立噪声和无意识对手的假设。然后，通过将这个问题重新构建为在时变随机线性约束下的OCO问题，我们展示了我们的算法在这样的设置下具有相同的后悔值保证，并且从期望上永远不会违反约束。这为OCO在时变随机约束下的文献做出了贡献，其中现有的先进算法

    arXiv:2403.05786v1 Announce Type: new  Abstract: The setting of online convex optimization (OCO) under unknown constraints has garnered significant attention in recent years. In this work, we consider a version of this problem with static linear constraints that the player receives noisy feedback of and must always satisfy. By leveraging our novel design paradigm of optimistic safety, we give an algorithm for this problem that enjoys $\tilde{\mathcal{O}}(\sqrt{T})$ regret. This improves on the previous best regret bound of $\tilde{\mathcal{O}}(T^{2/3})$ while using only slightly stronger assumptions of independent noise and an oblivious adversary. Then, by recasting this problem as OCO under time-varying stochastic linear constraints, we show that our algorithm enjoys the same regret guarantees in such a setting and never violates the constraints in expectation. This contributes to the literature on OCO under time-varying stochastic constraints, where the state-of-the-art algorithms en
    
[^177]: 大型生成模型辅助的3D语义通信

    Large Generative Model Assisted 3D Semantic Communication

    [https://arxiv.org/abs/2403.05783](https://arxiv.org/abs/2403.05783)

    提出了一种基于生成AI模型辅助的3D语义通信系统，包括3D语义提取器和自适应语义压缩模型，用于解决3D场景中的语义提取和编码挑战

    

    语义通信（SC）是6G数据传输的一种新范式。然而，在3D场景中进行SC时存在几个挑战：1）3D语义提取；2）潜在语义冗余；和3）不确定的信道估计。为了解决这些问题，我们提出了一种基于生成AI模型辅助的3D SC（GAM-3DSC）系统。首先，我们引入了一个3D语义提取器（3DSE），它利用生成AI模型，包括Segment Anything Model（SAM）和Neural Radiance Field（NeRF），根据用户需求从3D场景中提取关键语义。提取的3D语义被表示为面向目标的3D物体的多视角图像。然后，我们提出了一种自适应语义压缩模型（ASCM），用于对这些多视角图像进行编码，其中我们使用一个具有两个输出头的语义编码器来执行语义编码，并在潜在语义空间中掩盖冗余语义。

    arXiv:2403.05783v1 Announce Type: cross  Abstract: Semantic Communication (SC) is a novel paradigm for data transmission in 6G. However, there are several challenges posed when performing SC in 3D scenarios: 1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain channel estimation. To address these issues, we propose a Generative AI Model assisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor (3DSE), which employs generative AI models, including Segment Anything Model (SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D scenario based on user requirements. The extracted 3D semantics are represented as multi-perspective images of the goal-oriented 3D object. Then, we present an Adaptive Semantic Compression Model (ASCM) for encoding these multi-perspective images, in which we use a semantic encoder with two output heads to perform semantic encoding and mask redundant semantics in the latent semantic space, respectively. 
    
[^178]: 船舶航迹识别的空间聚类方法

    Spatial Clustering Approach for Vessel Path Identification

    [https://arxiv.org/abs/2403.05778](https://arxiv.org/abs/2403.05778)

    该论文提出了一种仅利用位置信息的船舶路径的空间聚类方法，通过两种方法实现路径聚类，达到了完美的F1分数，为改善海运安全和效率提供了宝贵见解。

    

    本文针对识别具有重复路径、部分重复路径和新路径的船舶路径的挑战提出解决方案。我们提出了一种仅利用位置信息标记船舶路径的空间聚类方法。我们开发了一种路径聚类框架，采用两种方法：基于距离的路径建模和概似估计方法。前者通过整合无监督机器学习技术提高了路径聚类的准确性，而后者专注于基于概似的路径模型，并引入分割以进行更详细的分析。结果发现突出了所开发方法的卓越性能和效率，因为将船舶路径聚类为五类的两种方法均取得了完美的F1分数。该方法旨在为航线规划提供宝贵见解，最终有助于提高海运安全和效率。

    arXiv:2403.05778v1 Announce Type: new  Abstract: This paper addresses the challenge of identifying the paths for vessels with operating routes of repetitive paths, partially repetitive paths, and new paths. We propose a spatial clustering approach for labeling the vessel paths by using only position information. We develop a path clustering framework employing two methods: a distance-based path modeling and a likelihood estimation method. The former enhances the accuracy of path clustering through the integration of unsupervised machine learning techniques, while the latter focuses on likelihood-based path modeling and introduces segmentation for a more detailed analysis. The result findings highlight the superior performance and efficiency of the developed approach, as both methods for clustering vessel paths into five classes achieve a perfect F1-score. The approach aims to offer valuable insights for route planning, ultimately contributing to improving safety and efficiency in marit
    
[^179]: 将激活导向扩展到广泛技能与多种行为

    Extending Activation Steering to Broad Skills and Multiple Behaviours

    [https://arxiv.org/abs/2403.05767](https://arxiv.org/abs/2403.05767)

    本文研究了将激活导向技术应用于广泛技能和多种行为的功效，并发现导向广泛技能具有竞争力，同时在模型中同时注入个体导向向量是一种有前途的方法。

    

    当前大型语言模型具有危险的能力，这很可能在未来变得更加棘手。激活导向技术可用于减少这些能力带来的风险。本文研究了激活导向在广泛技能和多种行为中的功效。通过比较减少对一般编码能力和Python特定能力表现的影响，我们发现导向更广泛技能与导向较窄技能竞争激烈。其次，我们引导模型变得更加或更少近视和寻求财富，以及其他行为。在实验中，将多种不同行为的导向向量组合为一个导向向量通常不成功。另一方面，同时在模型中不同位置注入个体导向向量是有前途的。

    arXiv:2403.05767v1 Announce Type: cross  Abstract: Current large language models have dangerous capabilities, which are likely to become more problematic in the future. Activation steering techniques can be used to reduce risks from these capabilities. In this paper, we investigate the efficacy of activation steering for broad skills and multiple behaviours. First, by comparing the effects of reducing performance on general coding ability and Python-specific ability, we find that steering broader skills is competitive to steering narrower skills. Second, we steer models to become more or less myopic and wealth-seeking, among other behaviours. In our experiments, combining steering vectors for multiple different behaviours into one steering vector is largely unsuccessful. On the other hand, injecting individual steering vectors at different places in a model simultaneously is promising.
    
[^180]: 物理信息神经运动规划在约束流形上的应用

    Physics-informed Neural Motion Planning on Constraint Manifolds

    [https://arxiv.org/abs/2403.05765](https://arxiv.org/abs/2403.05765)

    提出了第一个在约束流形上解决Eikonal方程的物理信息CMP框架，无需专家数据，高效。

    

    约束运动规划（CMP）旨在在运动约束流形上找到给定起始和目标配置之间的无碰路径。这些问题出现在各种场景中，从物体操纵到腿式机器人行走。然而，流形的零体积特性使得CMP问题具有挑战性，当前最先进的方法仍然需要几秒钟来找到路径，并且需要一个计算费力的路径数据集来进行模仿学习。最近，出现了直接通过神经网络解决Eikonal方程进行运动规划的物理信息运动规划方法，无需专家演示进行学习。受这些方法的启发，我们提出了第一个在约束流形上解决Eikonal方程的物理信息CMP框架，并训练神经函数进行CMP而无需专家数据。我们的结果表明，所提出的方法高效

    arXiv:2403.05765v1 Announce Type: cross  Abstract: Constrained Motion Planning (CMP) aims to find a collision-free path between the given start and goal configurations on the kinematic constraint manifolds. These problems appear in various scenarios ranging from object manipulation to legged-robot locomotion. However, the zero-volume nature of manifolds makes the CMP problem challenging, and the state-of-the-art methods still take several seconds to find a path and require a computationally expansive path dataset for imitation learning. Recently, physics-informed motion planning methods have emerged that directly solve the Eikonal equation through neural networks for motion planning and do not require expert demonstrations for learning. Inspired by these approaches, we propose the first physics-informed CMP framework that solves the Eikonal equation on the constraint manifolds and trains neural function for CMP without expert data. Our results show that the proposed approach efficientl
    
[^181]: HDReason：超维知识图推理的算法-硬件协同设计

    HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning

    [https://arxiv.org/abs/2403.05763](https://arxiv.org/abs/2403.05763)

    本文提出了一种基于超维计算的算法-硬件协同设计，用于更高效和加速的知识图推理。

    

    最近，为图学习应用如顶点分类和图分类提出了大量硬件加速器。然而，先前的工作很少关注知识图补全（KGC），这是一项以其显著更高算法复杂性而闻名的任务。基于图卷积神经网络（GCN）的最先进KGC解决方案涉及广泛的顶点/关系嵌入更新和复杂的得分函数，这对加速来说是困难的。因此，现有的加速器设计不再是最佳选择，需要一种新颖的算法-硬件协同设计来进行KG推理。最近，受脑启发的超维计算（HDC）被引入作为轻量级机器学习的有前途的解决方案，特别适用于图学习应用。在本文中，我们利用HDC来实现一个固有更高效且适合加速的

    arXiv:2403.05763v1 Announce Type: cross  Abstract: In recent times, a plethora of hardware accelerators have been put forth for graph learning applications such as vertex classification and graph classification. However, previous works have paid little attention to Knowledge Graph Completion (KGC), a task that is well-known for its significantly higher algorithm complexity. The state-of-the-art KGC solutions based on graph convolution neural network (GCN) involve extensive vertex/relation embedding updates and complicated score functions, which are inherently cumbersome for acceleration. As a result, existing accelerator designs are no longer optimal, and a novel algorithm-hardware co-design for KG reasoning is needed.   Recently, brain-inspired HyperDimensional Computing (HDC) has been introduced as a promising solution for lightweight machine learning, particularly for graph learning applications. In this paper, we leverage HDC for an intrinsically more efficient and acceleration-fri
    
[^182]: 通过独立查询预言者在马尔可夫等价类中进行成员测试

    Membership Testing in Markov Equivalence Classes via Independence Query Oracles

    [https://arxiv.org/abs/2403.05759](https://arxiv.org/abs/2403.05759)

    通过建立在给定的最大无向团大小($s$)方面的下界，我们探讨了通过独立查询预言者在马尔可夫等价类中进行成员测试这一问题。

    

    变量之间因果关系的理解是许多科学领域中具有广泛影响的基本问题。虽然已经投入了大量研究来从数据中学习因果图，但其补充概念——测试因果关系却基本没有被探索。我们通过建立在给定MEC(Markov等价类)的最大无向团的大小($s$)方面的下界，探讨基于约束的测试方法。在最坏情况下，我们展示了$\exp(\Omega(s))$个独立性测试的下界。

    arXiv:2403.05759v1 Announce Type: cross  Abstract: Understanding causal relationships between variables is a fundamental problem with broad impact in numerous scientific fields. While extensive research has been dedicated to learning causal graphs from data, its complementary concept of testing causal relationships has remained largely unexplored. While learning involves the task of recovering the Markov equivalence class (MEC) of the underlying causal graph from observational data, the testing counterpart addresses the following critical question: Given a specific MEC and observational data from some causal graph, can we determine if the data-generating causal graph belongs to the given MEC?   We explore constraint-based testing methods by establishing bounds on the required number of conditional independence tests. Our bounds are in terms of the size of the maximum undirected clique ($s$) of the given MEC. In the worst case, we show a lower bound of $\exp(\Omega(s))$ independence tes
    
[^183]: 无模型本地重新校准神经网络

    Model-Free Local Recalibration of Neural Networks

    [https://arxiv.org/abs/2403.05756](https://arxiv.org/abs/2403.05756)

    提出了一种使用ANN隐层提供的输入的降维表示来对ANN预测分布进行本地重新校准的方法，从文献中近似贝叶斯计算和无似然推断方法的重新校准技术中获取灵感。

    

    人工神经网络（ANNs）是高度灵活的预测模型。然而，可靠地量化其预测的不确定性仍然是一个持续的挑战。最近关于对ANNs的预测分布进行“重新校准”的工作很多，使得感兴趣事件的预测概率与对它们的某些频率评估一致。未校准的概率预测对于许多重要的决策任务有限用处。为了解决这个问题，我们提出了一种使用ANN隐层提供的输入的降维表示来对ANN预测分布进行本地重新校准的方法。我们的新颖方法受到了文献中用于近似贝叶斯计算和无似然推断方法的重新校准技术的启发。大多数现有的ANN校准方法可以被认为是在输入层上进行校准，但当输入是...

    arXiv:2403.05756v1 Announce Type: cross  Abstract: Artificial neural networks (ANNs) are highly flexible predictive models. However, reliably quantifying uncertainty for their predictions is a continuing challenge. There has been much recent work on "recalibration" of predictive distributions for ANNs, so that forecast probabilities for events of interest are consistent with certain frequency evaluations of them. Uncalibrated probabilistic forecasts are of limited use for many important decision-making tasks. To address this issue, we propose a localized recalibration of ANN predictive distributions using the dimension-reduced representation of the input provided by the ANN hidden layers. Our novel method draws inspiration from recalibration techniques used in the literature on approximate Bayesian computation and likelihood-free inference methods. Most existing calibration methods for ANNs can be thought of as calibrating either on the input layer, which is difficult when the input is
    
[^184]: 模式识别的混合量子启发式ResNet和DenseNet及其完整性分析

    Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition with Completeness Analysis

    [https://arxiv.org/abs/2403.05754](https://arxiv.org/abs/2403.05754)

    提出了两种根植于残差连接和密集连接的混合量子启发式神经网络，用于更全面地改进和评估新型神经网络在复杂和不可预测环境中的表现

    

    随着当今数字技术的接近，深度神经网络正成为人工智能繁荣的基础算法。然而，不断发展的社会需求正在强调替代传统神经网络的新方法的必要性。同时，后摩尔时代的来临推动了具有卓越潜力的量子启发式神经网络在某些情况下的发展。然而，由于目前新旧深度学习模型之间比较中存在含糊指标，因此一套明确的评估系统与详细的指标是非常重要和不可或缺的。因此，为了更全面地改进和评估新型神经网络在复杂和不可预测环境中的表现，我们提出了两种根植于残差连接和密集连接的混合量子启发式神经网络，用于模式识别。

    arXiv:2403.05754v1 Announce Type: new  Abstract: With the contemporary digital technology approaching, deep neural networks are emerging as the foundational algorithm of the artificial intelligence boom. Whereas, the evolving social demands have been emphasizing the necessity of novel methodologies to substitute traditional neural networks. Concurrently, the advent of the post-Moore era has spurred the development of quantum-inspired neural networks with outstanding potentials at certain circumstances. Nonetheless, a definitive evaluating system with detailed metrics is tremendously vital and indispensable owing to the vague indicators in comparison between the novel and traditional deep learning models at present. Hence, to improve and evaluate the performances of the novel neural networks more comprehensively in complex and unpredictable environments, we propose two hybrid quantum-inspired neural networks which are rooted in residual and dense connections respectively for pattern rec
    
[^185]: 在大型知识图上训练面向任务的图神经网络，实现准确高效建模

    Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling

    [https://arxiv.org/abs/2403.05752](https://arxiv.org/abs/2403.05752)

    本文提出了一种自动化TOSG提取的方法KG-TOSA，用于在大型知识图上进行面向任务的图神经网络训练，以减轻对大型KG的过多计算负担。

    

    知识图（KG）是一种包含各种节点和边类型的异构图。异构图神经网络（HGNNs）通常用于在KG上训练节点分类和链接预测等机器学习任务。然而，HGNN方法受KG的大小、密度以及节点和边类型数量的影响，表现出过多的复杂性。AI从业者手工设计出一个与特定任务相关的KG G的子图，我们称之为面向任务的子图（TOSG），其中包含G中与任务相关的节点和边类型的子集。使用TOSG而不是G来训练任务可以减轻对大型KG所需的过多计算。设计TOSG需要深入了解KG的结构和任务的目标，因此具有挑战性且耗时。本文提出了KG-TOSA，一种自动化TOSG提取的方法，用于在大型KG上进行面向任务的HGNN训练。

    arXiv:2403.05752v1 Announce Type: cross  Abstract: A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range of node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular for training machine learning tasks like node classification and link prediction on KGs. However, HGNN methods exhibit excessive complexity influenced by the KG's size, density, and the number of node and edge types. AI practitioners handcraft a subgraph of a KG G relevant to a specific task. We refer to this subgraph as a task-oriented subgraph (TOSG), which contains a subset of task-related node and edge types in G. Training the task using TOSG instead of G alleviates the excessive computation required for a large KG. Crafting the TOSG demands a deep understanding of the KG's structure and the task's objectives. Hence, it is challenging and time-consuming. This paper proposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented HGNN training on a large KG. In KG
    
[^186]: MG-TSD：具有引导学习过程的多粒度时间序列扩散模型

    MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process

    [https://arxiv.org/abs/2403.05751](https://arxiv.org/abs/2403.05751)

    提出了一种新颖的MG-TSD模型，利用数据内在粒度水平作为目标来引导学习过程，实现了状态-of-the-art的预测性能

    

    最近，扩散概率模型由于其生成高保真样本的显著能力而在生成式时间序列预测中引起关注。然而，由于其随机特性带来的不稳定性挑战，如何有效利用它们在概率时间序列预测任务中的强大建模能力仍然是一个悬而未决的问题。为了解决这一挑战，我们引入了一种新颖的多粒度时间序列扩散（MG-TSD）模型，通过利用数据内在的粒度水平作为中间扩散步骤的给定目标来指导扩散模型的学习过程，实现了最先进的预测性能。

    arXiv:2403.05751v1 Announce Type: cross  Abstract: Recently, diffusion probabilistic models have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the probabilistic time series forecasting task remains an open question, partially due to the challenge of instability arising from their stochastic nature. To address this challenge, we introduce a novel Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves state-of-the-art predictive performance by leveraging the inherent granularity levels within the data as given targets at intermediate diffusion steps to guide the learning process of diffusion models. The way to construct the targets is motivated by the observation that the forward process of the diffusion model, which sequentially corrupts the data distribution to a standard normal distribution, intuitively aligns with the p
    
[^187]: 解读AI笔: 检测AI生成文本的技术与挑战

    Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text

    [https://arxiv.org/abs/2403.05750](https://arxiv.org/abs/2403.05750)

    大型语言模型在自然语言生成领域取得了重大突破，提出了识别AI生成文本的解决方案，并探索了未来研究方向。

    

    大型语言模型(LLMs)通过展示生成类人文本的惊人能力，彻底颠覆了自然语言生成(NLG)领域。然而，它们广泛的应用带来挑战，需要深入审查、伦理审查和负责任的实践。本研究探讨了这些挑战，探索了现有的缓解策略，重点是识别AI生成文本作为最终解决方案。此外，我们从理论角度评估了检测的可行性，并提出了解决当前领域限制的新颖研究方向。

    arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
    
[^188]: 具有市场运营应用的生成式概率预测

    Generative Probabilistic Forecasting with Applications in Market Operations

    [https://arxiv.org/abs/2403.05743](https://arxiv.org/abs/2403.05743)

    提出了一种基于Wiener-Kallianpur创新表示的生成式概率预测方法，包括自编码器和新颖的深度学习算法，具有渐近最优性和结构收敛性质，适用于实时市场运营中的高动态和波动时间序列。

    

    本文提出了一种新颖的生成式概率预测方法，该方法源自于非参数时间序列的Wiener-Kallianpur创新表示。在生成人工智能的范式下，所提出的预测架构包括一个自编码器，将非参数多变量随机过程转化为规范的创新序列，从中根据过去样本生成未来时间序列样本，条件是它们的概率分布取决于过去样本。提出了一种新的深度学习算法，将潜在过程限制为具有匹配自编码器输入-输出条件概率分布的独立同分布序列。建立了所提出的生成式预测方法的渐近最优性和结构收敛性质。该方法在实时市场运营中涉及高度动态和波动时间序列的三个应用方面。

    arXiv:2403.05743v1 Announce Type: cross  Abstract: This paper presents a novel generative probabilistic forecasting approach derived from the Wiener-Kallianpur innovation representation of nonparametric time series. Under the paradigm of generative artificial intelligence, the proposed forecasting architecture includes an autoencoder that transforms nonparametric multivariate random processes into canonical innovation sequences, from which future time series samples are generated according to their probability distributions conditioned on past samples. A novel deep-learning algorithm is proposed that constrains the latent process to be an independent and identically distributed sequence with matching autoencoder input-output conditional probability distributions. Asymptotic optimality and structural convergence properties of the proposed generative forecasting approach are established. Three applications involving highly dynamic and volatile time series in real-time market operations a
    
[^189]: 可证明的平均回报马尔可夫潜在博弈的策略梯度方法

    Provable Policy Gradient Methods for Average-Reward Markov Potential Games

    [https://arxiv.org/abs/2403.05738](https://arxiv.org/abs/2403.05738)

    提出了针对平均回报准则下马尔可夫潜在博弈的可证明的策略梯度方法，并展示了算法的全局收敛性和时间复杂度。

    

    我们研究了在无限时间段平均回报准则下的马尔可夫潜在博弈。大多数先前的研究都是针对折扣回报的。我们证明基于独立策略梯度和独立自然策略梯度的算法均在平均回报准则下全局收敛到纳什均衡。为了为基于梯度的方法奠定基础，我们首先建立平均回报是策略的光滑函数，并在马尔可夫决策过程 (MDP) 的遍历性和次大特征值的一定条件下提供差分价值函数的敏感度界限。我们证明了三种算法，包括策略梯度、近端-Q 和自然策略梯度 (NPG)，在给定梯度/差分Q函数oracle的情况下，收敛到$\epsilon$-Nash均衡的时间复杂度为$O(\frac{1}{\epsilon^2})$。当必须估计策略梯度时，我们提出了一个算法，具有$\tilde{...}$

    arXiv:2403.05738v1 Announce Type: new  Abstract: We study Markov potential games under the infinite horizon average reward criterion. Most previous studies have been for discounted rewards. We prove that both algorithms based on independent policy gradient and independent natural policy gradient converge globally to a Nash equilibrium for the average reward criterion. To set the stage for gradient-based methods, we first establish that the average reward is a smooth function of policies and provide sensitivity bounds for the differential value functions, under certain conditions on ergodicity and the second largest eigenvalue of the underlying Markov decision process (MDP). We prove that three algorithms, policy gradient, proximal-Q, and natural policy gradient (NPG), converge to an $\epsilon$-Nash equilibrium with time complexity $O(\frac{1}{\epsilon^2})$, given a gradient/differential Q function oracle. When policy gradients have to be estimated, we propose an algorithm with $\tilde{
    
[^190]: 保守DDPG - 无集成的悲观强化学习

    Conservative DDPG -- Pessimistic RL without Ensemble

    [https://arxiv.org/abs/2403.05732](https://arxiv.org/abs/2403.05732)

    提出了一种新的保守DDPG方法，通过引入$Q$-目标和行为克隆损失惩罚来解决DDPG中的高估偏差问题，可以在不需要集成的情况下轻松实现，并且在各种任务中表现优异。

    

    DDPG受到高估偏差问题的阻碍，其中其$Q$-估计倾向于夸大实际$Q$值。传统解决这一偏见的方法涉及基于集成的方法，需要大量计算资源，或者基于复杂对数策略的方法，难以理解和实施。相比之下，我们提出了一种简单的解决方案，使用$Q$-目标并结合行为克隆（BC）损失惩罚。这种解决方案作为一种不确定性度量，可以很容易地用较少的代码实现，而无需集成。我们的实证结果强烈支持保守DDPG在各种MuJoCo和Bullet任务上优于DDPG。我们始终观察到在所有评估任务中表现更好，甚至在与TD3和TD7相比性能更有竞争力或更优越，所有这些都是以显著降低的计算要求实现的。

    arXiv:2403.05732v1 Announce Type: new  Abstract: DDPG is hindered by the overestimation bias problem, wherein its $Q$-estimates tend to overstate the actual $Q$-values. Traditional solutions to this bias involve ensemble-based methods, which require significant computational resources, or complex log-policy-based approaches, which are difficult to understand and implement. In contrast, we propose a straightforward solution using a $Q$-target and incorporating a behavioral cloning (BC) loss penalty. This solution, acting as an uncertainty measure, can be easily implemented with minimal code and without the need for an ensemble. Our empirical findings strongly support the superiority of Conservative DDPG over DDPG across various MuJoCo and Bullet tasks. We consistently observe better performance in all evaluated tasks and even competitive or superior performance compared to TD3 and TD7, all achieved with significantly reduced computational requirements.
    
[^191]: 增强 vs 算法：自监督学习中的有效因素

    Augmentations vs Algorithms: What Works in Self-Supervised Learning

    [https://arxiv.org/abs/2403.05726](https://arxiv.org/abs/2403.05726)

    研究了自监督学习中数据增强、预训练算法和模型架构的相对影响，提出了统一SSL方法的新框架，发现除了改变预训练算法外，新数据增强和更强大的模型架构也起到重要作用

    

    我们研究了数据增强、预训练算法和模型架构在自监督学习（SSL）中的相对影响。虽然最近这一领域的文献让人觉得预训练算法对性能至关重要，但要理解其影响受制于很难进行客观和直接的比较。我们提出了一个新的框架，将许多看似不同的SSL方法统一到一个共享的模板中。利用这个框架，我们确定了方法之间的差异之处，并观察到除了改变预训练算法外，许多作品还使用了新的数据增强或更强大的模型架构。我们使用我们的框架比较了几种流行的SSL方法，发现许多算法添加，如预测网络或新的损失，对下游任务性能影响较小（通常低于1%），而增强型

    arXiv:2403.05726v1 Announce Type: new  Abstract: We study the relative effects of data augmentations, pretraining algorithms, and model architectures in Self-Supervised Learning (SSL). While the recent literature in this space leaves the impression that the pretraining algorithm is of critical importance to performance, understanding its effect is complicated by the difficulty in making objective and direct comparisons between methods. We propose a new framework which unifies many seemingly disparate SSL methods into a single shared template. Using this framework, we identify aspects in which methods differ and observe that in addition to changing the pretraining algorithm, many works also use new data augmentations or more powerful model architectures. We compare several popular SSL methods using our framework and find that many algorithmic additions, such as prediction networks or new losses, have a minor impact on downstream task performance (often less than $1\%$), while enhanced a
    
[^192]: 用于生成简要住院病程摘要的领域自适应大语言模型的基准测试

    A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries

    [https://arxiv.org/abs/2403.05720](https://arxiv.org/abs/2403.05720)

    介绍了一个新的基准测试，评估了用于生成简要住院病程摘要的大语言模型在健康保健领域中的性能并提出相应的自适应策略

    

    简要住院病程（BHC）摘要是通过总结临床记录而生成的常见临床文件。虽然大型语言模型（LLMs）在自动化实际任务方面展现出显著能力，但它们在医疗应用（如BHC合成）中的能力尚未得到展示。为了使LLMs能够适应BHC合成，我们引入了一个新颖的基准测试，其中包含从MIMIC-IV记录中提取的经过预处理的数据集，封装了临床记录和简要住院病程（BHC）对。我们评估了两个通用LLMs和三个医疗领域适应的LLMs的性能，以改进从临床记录生成BHC。我们使用临床记录作为输入来生成BHC，采用基于提示的（使用上下文学习）和基于微调的自适应策略来应用于三个开源LLMs（Clinical-T5-Large，Llama2-13B，FLAN-UL2）和两个专有LLMs（GPT-3.5，GPT-4）。我们定量评估了性能。

    arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
    
[^193]: 一个用于在网络物理人系统中进行有效人工智能推荐的框架

    A Framework for Effective AI Recommendations in Cyber-Physical-Human Systems

    [https://arxiv.org/abs/2403.05715](https://arxiv.org/abs/2403.05715)

    该论文提出了一个框架来解决网络物理人系统中人工智能推荐和人类决策者之间的挑战，通过考虑人类可能不同于AI平台的感知和解释方式，建立了最佳推荐策略的结构特性，并开发了近似人类模型，从而提供了理论上的最优性界限和数值示例验证结果。

    

    许多网络物理人系统（CPHS）涉及到一个人类决策者，他可能会从人工智能（AI）平台接收推荐，同时又持有最终决策的责任。在这类CPHS应用中，人类决策者可能会因为各种原因而偏离最佳推荐决策，而不是实施另一个。在这封信中，我们开发了一个严格的框架来克服这一挑战。在我们的框架中，我们考虑到人类可能会因为感知和解释系统状态的方式与AI平台不同而偏离AI的推荐。我们建立了最佳推荐策略的结构特性，并开发了AI使用的近似人类模型（AHM）。我们为由AHM产生的最优性差距提供了理论界限，并在一个数值示例中说明了我们结果的功效。

    arXiv:2403.05715v1 Announce Type: cross  Abstract: Many cyber-physical-human systems (CPHS) involve a human decision-maker who may receive recommendations from an artificial intelligence (AI) platform while holding the ultimate responsibility of making decisions. In such CPHS applications, the human decision-maker may depart from an optimal recommended decision and instead implement a different one for various reasons. In this letter, we develop a rigorous framework to overcome this challenge. In our framework, we consider that humans may deviate from AI recommendations as they perceive and interpret the system's state in a different way than the AI platform. We establish the structural properties of optimal recommendation strategies and develop an approximate human model (AHM) used by the AI. We provide theoretical bounds on the optimality gap that arises from an AHM and illustrate the efficacy of our results in a numerical example.
    
[^194]: $\mathtt{tsGT}$：具有Transformer的随机时间序列建模

    $\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer

    [https://arxiv.org/abs/2403.05713](https://arxiv.org/abs/2403.05713)

    $\mathtt{tsGT}$是一种基于通用Transformer架构的随机时间序列模型，表现优于最先进模型，并超过其随机同行，特别在数据分布建模和边际分位值预测方面具备优势。

    

    时间序列方法在几乎所有处理时间结构化数据的科学领域中都具有基础重要性。最近，出现了一大批具有时间序列特定架构偏见的确定性Transformer模型。本文采取了不同的方向，引入了$\mathtt{tsGT}$，这是一种基于通用Transformer架构构建的随机时间序列模型。我们专注于使用一个众所周知且理论上合理的滚动窗口回测和评估协议。我们展示了$\mathtt{tsGT}$在四个常用数据集上在MAD和RMSE方面优于最先进模型，并在QL和CRPS方面超过了其随机同行。我们通过详细分析$\mathtt{tsGT}$在建模数据分布和预测边际分位值方面的能力来补充这些结果。

    arXiv:2403.05713v1 Announce Type: new  Abstract: Time series methods are of fundamental importance in virtually any field of science that deals with temporally structured data. Recently, there has been a surge of deterministic transformer models with time series-specific architectural biases. In this paper, we go in a different direction by introducing $\mathtt{tsGT}$, a stochastic time series model built on a general-purpose transformer architecture. We focus on using a well-known and theoretically justified rolling window backtesting and evaluation protocol. We show that $\mathtt{tsGT}$ outperforms the state-of-the-art models on MAD and RMSE, and surpasses its stochastic peers on QL and CRPS, on four commonly used datasets. We complement these results with a detailed analysis of $\mathtt{tsGT}$'s ability to model the data distribution and predict marginal quantile values.
    
[^195]: 复杂航天器任务的屏蔽深度强化学习

    Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking

    [https://arxiv.org/abs/2403.05693](https://arxiv.org/abs/2403.05693)

    本文基于线性时态逻辑（LTL）形式化了航天器任务和安全要求，并提出了自动构建奖励函数以实现有效训练的方法。同时探讨了从安全LTL规范构建航天器屏蔽的方法，提出了三种可以提供概率保证的设计，并通过实验展示了这些屏蔽与不同策略的互动和奖励结构的灵活性。

    

    自主航天器控制通过屏蔽深度强化学习（SDRL）已成为快速增长的研究领域。然而，目前对屏蔽的构建和任务的定义仍不够正式，导致策略无法保证安全并给RL代理设定了模棱两可的目标。本文首先探讨了使用形式语言，即线性时态逻辑（LTL），来形式化航天器任务和安全要求。然后定义了一种自动从co-safe LTL规范构建奖励函数以有效训练SDRL框架的方式。我们还研究了为航天器应用从安全LTL规范构建屏蔽的方法，并提出了三种可以提供概率保证的设计。通过几个实验展示了这些屏蔽与不同策略的互动以及奖励结构的灵活性。

    arXiv:2403.05693v1 Announce Type: new  Abstract: Autonomous spacecraft control via Shielded Deep Reinforcement Learning (SDRL) has become a rapidly growing research area. However, the construction of shields and the definition of tasking remains informal, resulting in policies with no guarantees on safety and ambiguous goals for the RL agent. In this paper, we first explore the use of formal languages, namely Linear Temporal Logic (LTL), to formalize spacecraft tasks and safety requirements. We then define a manner in which to construct a reward function from a co-safe LTL specification automatically for effective training in SDRL framework. We also investigate methods for constructing a shield from a safe LTL specification for spacecraft applications and propose three designs that provide probabilistic guarantees. We show how these shields interact with different policies and the flexibility of the reward structure through several experiments.
    
[^196]: 使用基于分解的决策焦点学习进行高效的公共卫生干预规划

    Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning

    [https://arxiv.org/abs/2403.05683](https://arxiv.org/abs/2403.05683)

    决策焦点学习（DFL）在公共卫生干预中的应用提高了干预的精准度，虽然存在高计算成本，但能优化有限的干预资源使用效果。

    

    公共卫生计划中受益者参与度下降是一个重要问题，为了改善保留率，健康工作者会对有辍学风险的受益者进行干预，然而，健康工作者的可用性和时间是有限资源。因此，有研究致力于使用不安定多臂赌博机来优化这些有限的干预资源。然而，在实践中使用这一框架的关键技术障碍在于需要从历史数据中估算受益者的多臂赌博机参数。

    arXiv:2403.05683v1 Announce Type: new  Abstract: The declining participation of beneficiaries over time is a key concern in public health programs. A popular strategy for improving retention is to have health workers `intervene' on beneficiaries at risk of dropping out. However, the availability and time of these health workers are limited resources. As a result, there has been a line of research on optimizing these limited intervention resources using Restless Multi-Armed Bandits (RMABs). The key technical barrier to using this framework in practice lies in the need to estimate the beneficiaries' RMAB parameters from historical data. Recent research has shown that Decision-Focused Learning (DFL), which focuses on maximizing the beneficiaries' adherence rather than predictive accuracy, improves the performance of intervention targeting using RMABs. Unfortunately, these gains come at a high computational cost because of the need to solve and evaluate the RMAB in each DFL training step. 
    
[^197]: 使用不同ially Private Tabular Data进行环境中学习的DP-TabICL

    DP-TabICL: In-Context Learning with Differentially Private Tabular Data

    [https://arxiv.org/abs/2403.05681](https://arxiv.org/abs/2403.05681)

    研究了如何使用差分隐私来保护在环境中学习中使用的表格数据。

    

    In-context learning (ICL)使得大型语言模型（LLM）通过在问题-答案对的示范条件下适应新任务，并且它已经表现出与昂贵的模型重新训练和微调相媲美的性能。最近，ICL已被扩展，允许使用表格数据作为示范示例，方法是将单个记录串行化为自然语言格式。然而，已经表明LLM可能会泄露提示中包含的信息，而且由于表格数据通常包含敏感信息，因此了解如何保护ICL中使用的基础表格数据是一个重要的研究领域。本文作为对如何使用差分隐私（DP）进行初始探索的研究--差分隐私是数据隐私和匿名化的长期金标准--以保护ICL中使用的表格数据。具体而言，我们研究了通过数据私有化机制在私有表格ICL中应用DP机制。

    arXiv:2403.05681v1 Announce Type: cross  Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks by conditioning on demonstrations of question-answer pairs and it has been shown to have comparable performance to costly model retraining and fine-tuning. Recently, ICL has been extended to allow tabular data to be used as demonstration examples by serializing individual records into natural language formats. However, it has been shown that LLMs can leak information contained in prompts, and since tabular data often contain sensitive information, understanding how to protect the underlying tabular data used in ICL is a critical area of research. This work serves as an initial investigation into how to use differential privacy (DP) -- the long-established gold standard for data privacy and anonymization -- to protect tabular data used in ICL. Specifically, we investigate the application of DP mechanisms for private tabular ICL via data privatization pr
    
[^198]: 通过额外图节点的方法对分类和混合数据进行谱聚类

    Spectral Clustering of Categorical and Mixed-type Data via Extra Graph Nodes

    [https://arxiv.org/abs/2403.05669](https://arxiv.org/abs/2403.05669)

    通过添加额外节点，本文提出了一种新的方法，将数值和分类信息同时纳入谱聚类算法，避免了数据预处理或复杂相似性函数的需求。

    

    将数据对象聚类成同质群体是数据挖掘中最重要的任务之一。谱聚类是一种理论上坚实且适应多种现实场景的最重要的聚类算法之一。本文探讨了一种更自然的方法，将数值和分类信息同时纳入谱聚类算法，避免了需要对数据进行预处理或使用复杂相似性函数的情况。

    arXiv:2403.05669v1 Announce Type: cross  Abstract: Clustering data objects into homogeneous groups is one of the most important tasks in data mining. Spectral clustering is arguably one of the most important algorithms for clustering, as it is appealing for its theoretical soundness and is adaptable to many real-world data settings. For example, mixed data, where the data is composed of numerical and categorical features, is typically handled via numerical discretization, dummy coding, or similarity computation that takes into account both data types. This paper explores a more natural way to incorporate both numerical and categorical information into the spectral clustering algorithm, avoiding the need for data preprocessing or the use of sophisticated similarity functions. We propose adding extra nodes corresponding to the different categories the data may belong to and show that it leads to an interpretable clustering objective function. Furthermore, we demonstrate that this simple 
    
[^199]: 面对最坏情况：一种基于学习的对ICP算法鲁棒性分析的对抗攻击

    Prepared for the Worst: A Learning-Based Adversarial Attack for Resilience Analysis of the ICP Algorithm

    [https://arxiv.org/abs/2403.05666](https://arxiv.org/abs/2403.05666)

    通过对ICP算法进行基于深度学习的攻击，在安全关键应用中评估其鲁棒性，重点在于找到可能的最大ICP姿势误差。

    

    这篇论文提出了一种通过深度学习攻击激光雷达点云来评估迭代最近点（ICP）算法鲁棒性的新方法。对于像自主导航这样的安全关键应用，确保算法在部署前的鲁棒性至关重要。ICP算法已成为基于激光雷达的定位的标准。然而，它产生的姿势估计可能会受到测量数据的影响。数据的污染可能来自各种场景，如遮挡、恶劣天气或传感器的机械问题。不幸的是，ICP的复杂和迭代特性使得评估其对污染的鲁棒性具有挑战性。虽然已经有人努力创建具有挑战性的数据集和开发仿真来经验性地评估ICP的鲁棒性，但我们的方法侧重于通过基于扰动的对抗攻击找到最大可能的ICP姿势误差。

    arXiv:2403.05666v1 Announce Type: cross  Abstract: This paper presents a novel method to assess the resilience of the Iterative Closest Point (ICP) algorithm via deep-learning-based attacks on lidar point clouds. For safety-critical applications such as autonomous navigation, ensuring the resilience of algorithms prior to deployments is of utmost importance. The ICP algorithm has become the standard for lidar-based localization. However, the pose estimate it produces can be greatly affected by corruption in the measurements. Corruption can arise from a variety of scenarios such as occlusions, adverse weather, or mechanical issues in the sensor. Unfortunately, the complex and iterative nature of ICP makes assessing its resilience to corruption challenging. While there have been efforts to create challenging datasets and develop simulations to evaluate the resilience of ICP empirically, our method focuses on finding the maximum possible ICP pose error using perturbation-based adversarial
    
[^200]: 这里是翻译过的论文标题

    What is different between these datasets?

    [https://arxiv.org/abs/2403.05652](https://arxiv.org/abs/2403.05652)

    这里是中文总结出的一句话要点

    

    这里是翻译过的论文摘要

    arXiv:2403.05652v1 Announce Type: cross  Abstract: The performance of machine learning models heavily depends on the quality of input data, yet real-world applications often encounter various data-related challenges. One such challenge could arise when curating training data or deploying the model in the real world - two comparable datasets in the same domain may have different distributions. While numerous techniques exist for detecting distribution shifts, the literature lacks comprehensive approaches for explaining dataset differences in a human-understandable manner. To address this gap, we propose a suite of interpretable methods (toolbox) for comparing two datasets. We demonstrate the versatility of our approach across diverse data modalities, including tabular data, language, images, and signals in both low and high-dimensional settings. Our methods not only outperform comparable and related approaches in terms of explanation quality and correctness, but also provide actionable,
    
[^201]: 基于相空间的几何神经网络用于BCI解码

    Geometric Neural Network based on Phase Space for BCI decoding

    [https://arxiv.org/abs/2403.05645](https://arxiv.org/abs/2403.05645)

    基于相空间的几何神经网络用于BCI解码，提供了在脑机接口领域中可靠算法操作的方法，以提高用户舒适度并促进其广泛应用。

    

    Deep Learning(DL)算法与脑信号分析的整合仍处于萌芽阶段，相比计算机视觉等领域的成功，在脑机接口(BCI)领域尤为突出，BCI通过解码大脑活动控制外部设备而无需肌肉控制。脑电图(EEG)是设计BCI系统的广泛选择，因其无创性、成本效益和出色的时间分辨率，但缺少训练数据、信噪比低、以及在个体间和内部的大量变化。 最后，使用多个电极设置BCI系统需要很长时间，阻碍可靠DL架构在研究实验室之外的BCI中的广泛应用。 为了提高采纳率，我们需要改善用户舒适度，例如使用少量电极操作的可靠算法。

    arXiv:2403.05645v1 Announce Type: cross  Abstract: The integration of Deep Learning (DL) algorithms on brain signal analysis is still in its nascent stages compared to their success in fields like Computer Vision, especially in Brain-Computer Interface (BCI), where the brain activity is decoded to control external devices without requiring muscle control. Electroencephalography (EEG) is a widely adopted choice for designing BCI systems due to its non-invasive and cost-effective nature and excellent temporal resolution. Still, it comes at the expense of limited training data, poor signal-to-noise, and a large variability across and within-subject recordings. Finally, setting up a BCI system with many electrodes takes a long time, hindering the widespread adoption of reliable DL architectures in BCIs outside research laboratories. To improve adoption, we need to improve user comfort using, for instance, reliable algorithms that operate with few electrodes. \textbf{Approach:} Our research
    
[^202]: OmniJet-$\alpha$: 粒子物理学的首个跨任务基础模型

    OmniJet-$\alpha$: The first cross-task foundation model for particle physics

    [https://arxiv.org/abs/2403.05618](https://arxiv.org/abs/2403.05618)

    基于物理数据和变压器架构，OmniJet-$\alpha$是首个跨任务基础模型，引入了全面的评估方法并展示了在无监督问题上的迁移学习。

    

    基础模型是多数据集和多任务的机器学习方法，一经预训练，便可被微调用于各种不同的应用。成功开发出这种通用物理数据模型将是一项重大突破，因为它们可以提高可实现的物理性能，同时大幅减少所需的训练时间和数据量。我们在这一挑战上取得了显著进展。首先，引入了一套全面的评估方法，来评判从物理数据转换为适合变压器架构（基础模型的通用骨干）进行自回归生成粒子喷流的表示质量。这些措施支持了相较于先前工作的更高保真度的标记化的选择。最后，我们展示了在无监督问题（喷流生成）之间的迁移学习。

    arXiv:2403.05618v1 Announce Type: cross  Abstract: Foundation models are multi-dataset and multi-task machine learning methods that once pre-trained can be fine-tuned for a large variety of downstream applications. The successful development of such general-purpose models for physics data would be a major breakthrough as they could improve the achievable physics performance while at the same time drastically reduce the required amount of training time and data.   We report significant progress on this challenge on several fronts. First, a comprehensive set of evaluation methods is introduced to judge the quality of an encoding from physics data into a representation suitable for the autoregressive generation of particle jets with transformer architectures (the common backbone of foundation models). These measures motivate the choice of a higher-fidelity tokenization compared to previous works. Finally, we demonstrate transfer learning between an unsupervised problem (jet generation) an
    
[^203]: 不熟悉的微调示例控制语言模型如何产生幻觉

    Unfamiliar Finetuning Examples Control How Language Models Hallucinate

    [https://arxiv.org/abs/2403.05612](https://arxiv.org/abs/2403.05612)

    本文研究了大型语言模型如何产生幻觉，并提出通过调整微调示例的监督来控制其对不熟悉输入的预测。作者开发了一种基于RL的方法，更可靠地减轻了长篇生成任务中的幻觉。

    

    大型语言模型（LLMs）倾向于生成听起来令人信服但事实不正确的响应，特别是当在不熟悉的概念上进行查询时。本文探讨了调整后的LLMs如何产生幻觉的基本机制。我们的调查揭示了一个有趣的模式：随着输入变得更不熟悉，LLMs的输出倾向于默认为"含糊其词"的预测，其形式受微调数据中不熟悉示例监督方式的影响。因此，通过策略性地修改这些示例的监督，我们可以控制LLM对不熟悉输入的预测（例如，教会它们说“我不知道”）。基于这些原则，我们开发了一种RL方法，通过解决奖励模型幻觉带来的挑战，更可靠地减轻长篇生成任务的幻觉。我们通过在MMLU上的多选QA中进行一系列受控实验来验证我们的发现。

    arXiv:2403.05612v1 Announce Type: cross  Abstract: Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts. In this work, we explore the underlying mechanisms that govern how finetuned LLMs hallucinate. Our investigation reveals an interesting pattern: as inputs become more unfamiliar, LLM outputs tend to default towards a ``hedged'' prediction, whose form is determined by how the unfamiliar examples in the finetuning data are supervised. Thus, by strategically modifying these examples' supervision, we can control LLM predictions for unfamiliar inputs (e.g., teach them to say ``I don't know''). Based on these principles, we develop an RL approach that more reliably mitigates hallucinations for long-form generation tasks, by tackling the challenges presented by reward model hallucinations. We validate our findings with a series of controlled experiments in multiple-choice QA on MMLU, as
    
[^204]: 关于神经网络优化中凝聚收敛群体存在的证据、定义和算法

    Evidence, Definitions and Algorithms regarding the Existence of Cohesive-Convergence Groups in Neural Network Optimization

    [https://arxiv.org/abs/2403.05610](https://arxiv.org/abs/2403.05610)

    论文讨论了一种新的方法，通过观察人工神经网络优化过程中出现的凝聚收敛群体，以拓展最近关于神经网络收敛问题的研究。

    

    神经网络的收敛过程是机器学习领域中最复杂和关键的问题之一。尽管人工神经网络的收敛与该领域的显著成功密切相关，但这一概念仍然主要是理论性的。实际上，由于人工神经网络处理的优化问题具有非凸性质，很少训练的网络实际上实现收敛。为了扩大人工神经网络收敛的最新研究成果，本文将讨论一种基于人工神经网络优化过程中出现的凝聚收敛群体观察的不同方法。

    arXiv:2403.05610v1 Announce Type: new  Abstract: Understanding the convergence process of neural networks is one of the most complex and crucial issues in the field of machine learning. Despite the close association of notable successes in this domain with the convergence of artificial neural networks, this concept remains predominantly theoretical. In reality, due to the non-convex nature of the optimization problems that artificial neural networks tackle, very few trained networks actually achieve convergence. To expand recent research efforts on artificial-neural-network convergence, this paper will discuss a different approach based on observations of cohesive-convergence groups emerging during the optimization process of an artificial neural network.
    
[^205]: 基于概念的可解释模型用于利用多模态数据诊断脉络膜肿瘤

    A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data

    [https://arxiv.org/abs/2403.05606](https://arxiv.org/abs/2403.05606)

    提出了一种基于概念的可解释模型，用于利用多模态数据诊断脉络膜肿瘤，促进了对罕见疾病的诊断，并在临床实践和医学教育中具有重要意义

    

    诊断罕见疾病在临床实践中面临着共同挑战，需要专家的专业知识才能准确识别。机器学习的出现提供了一种有希望的解决方案，然而这类技术的发展受到罕见状况的数据稀缺和在临床环境中需要具有可解释性和可信赖性的模型的需求的阻碍。可解释的人工智能，具有人类可读输出的能力，可以促进临床医生的验证并促进医学教育。在当前工作中，我们专注于脉络膜肿瘤，这是成人中最常见的眼睛癌症形式，尽管罕见，罹患率为每百万人5.1例。我们建立了迄今为止最大的数据集，共包括750名患者，涵盖了从2004年至2022年收集的三种不同成像模态。我们的工作引入了一个基于概念的可解释模型，可区分三种类型的脉络膜肿瘤，融合了一些不太重要的

    arXiv:2403.05606v1 Announce Type: cross  Abstract: Diagnosing rare diseases presents a common challenge in clinical practice, necessitating the expertise of specialists for accurate identification. The advent of machine learning offers a promising solution, while the development of such technologies is hindered by the scarcity of data on rare conditions and the demand for models that are both interpretable and trustworthy in a clinical context. Interpretable AI, with its capacity for human-readable outputs, can facilitate validation by clinicians and contribute to medical education. In the current work, we focus on choroid neoplasias, the most prevalent form of eye cancer in adults, albeit rare with 5.1 per million. We built the so-far largest dataset consisting of 750 patients, incorporating three distinct imaging modalities collected from 2004 to 2022. Our work introduces a concept-based interpretable model that distinguishes between three types of choroidal tumors, integrating insig
    
[^206]: 利用基于注意力的关联上下文信息从生物医学文献中提取蛋白质相互作用（PPIs）

    Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature using Attention-based Relational Context Information

    [https://arxiv.org/abs/2403.05602](https://arxiv.org/abs/2403.05602)

    通过利用关系上下文信息的Transformer-based深度学习方法，本研究提出了一个统一的、多源PPI语料库，改进了关系分类性能。

    

    蛋白质相互作用（PPIs）对于理解生命系统至关重要，获取这些数据对于探究疾病发展和识别基因/蛋白质功能以及生物过程是必不可少的。本文提出了一个统一的、多源PPI语料库，通过二元交互类型标签增强了经过审查的交互定义，并提出了一种基于Transformer的深度学习方法，利用实体的关联上下文信息进行关系表示，以提高关系分类性能。

    arXiv:2403.05602v1 Announce Type: cross  Abstract: Because protein-protein interactions (PPIs) are crucial to understand living systems, harvesting these data is essential to probe disease development and discern gene/protein functions and biological processes. Some curated datasets contain PPI data derived from the literature and other sources (e.g., IntAct, BioGrid, DIP, and HPRD). However, they are far from exhaustive, and their maintenance is a labor-intensive process. On the other hand, machine learning methods to automate PPI knowledge extraction from the scientific literature have been limited by a shortage of appropriate annotated data. This work presents a unified, multi-source PPI corpora with vetted interaction definitions augmented by binary interaction type labels and a Transformer-based deep learning method that exploits entities' relational context information for relation representation to improve relation classification performance. The model's performance is evaluated
    
[^207]: 选择高级特征：分层分类网络中的高效专家

    Select High-Level Features: Efficient Experts from a Hierarchical Classification Network

    [https://arxiv.org/abs/2403.05601](https://arxiv.org/abs/2403.05601)

    提出了一种新颖的专家生成方法，通过选择高级特征实现动态降低任务和计算复杂性，为未来设计轻量级和适应性强的网络铺平了道路

    

    这项研究介绍了一种新颖的专家生成方法，可以动态减少任务和计算复杂性，同时不影响预测性能。它基于一种新的分层分类网络拓扑结构，将通用低级特征的顺序处理与高级特征的并行处理和嵌套相结合。这种结构允许创新的特征提取技术：能够选择与任务相关类别的高级特征。在某些情况下，几乎可以跳过所有不必要的高级特征，这可以显著减少推理成本，在资源受限的条件下非常有益。我们相信这种方法为未来轻量级和可适应的网络设计铺平了道路，使其适用于从紧凑边缘设备到大型云端的各种应用。在动态推理方面，我们的方法可以实现排除

    arXiv:2403.05601v1 Announce Type: new  Abstract: This study introduces a novel expert generation method that dynamically reduces task and computational complexity without compromising predictive performance. It is based on a new hierarchical classification network topology that combines sequential processing of generic low-level features with parallelism and nesting of high-level features. This structure allows for the innovative extraction technique: the ability to select only high-level features of task-relevant categories. In certain cases, it is possible to skip almost all unneeded high-level features, which can significantly reduce the inference cost and is highly beneficial in resource-constrained conditions. We believe this method paves the way for future network designs that are lightweight and adaptable, making them suitable for a wide range of applications, from compact edge devices to large-scale clouds. In terms of dynamic inference our methodology can achieve an exclusion 
    
[^208]: 密度回归：面向分布偏移下不确定性估计的高效且距离感知的深度回归器

    Density-Regression: Efficient and Distance-Aware Deep Regressor for Uncertainty Estimation under Distribution Shifts

    [https://arxiv.org/abs/2403.05600](https://arxiv.org/abs/2403.05600)

    密度回归是一种利用密度函数进行不确定性估计并通过单次前向传递实现快速推断的方法，具有距离感知能力，能够在分布偏移下产生高质量不确定性估计。

    

    现代深度合奏技术通过使用不同模型的多次前向传递实现强大的不确定性估计性能。然而，这样做会占用大量存储空间并且推断（测试）时间较慢。为了解决此问题，我们提出了一种称为密度回归的方法，该方法在不确定性估计中利用密度函数，通过单次前向传递实现快速推断。我们证明它在特征空间上具有距离感知，这是神经网络产生高质量不确定性估计的必要条件。在实证方面，我们在立方体玩具数据集、基准UCI数据集、具有时间序列的天气预测和真实世界偏移应用下的深度估计等回归任务上进行了实验。我们展示了在分布偏移下，密度回归与现代深度回归器相比具有竞争力的不确定性估计性能，同时使用的模式较低。

    arXiv:2403.05600v1 Announce Type: new  Abstract: Morden deep ensembles technique achieves strong uncertainty estimation performance by going through multiple forward passes with different models. This is at the price of a high storage space and a slow speed in the inference (test) time. To address this issue, we propose Density-Regression, a method that leverages the density function in uncertainty estimation and achieves fast inference by a single forward pass. We prove it is distance aware on the feature space, which is a necessary condition for a neural network to produce high-quality uncertainty estimation under distribution shifts. Empirically, we conduct experiments on regression tasks with the cubic toy dataset, benchmark UCI, weather forecast with time series, and depth estimation under real-world shifted applications. We show that Density-Regression has competitive uncertainty estimation performance under distribution shifts with modern deep regressors while using a lower mode
    
[^209]: 通过有界支持的高斯机制实现隐私增强

    Privacy Amplification for the Gaussian Mechanism via Bounded Support

    [https://arxiv.org/abs/2403.05598](https://arxiv.org/abs/2403.05598)

    通过有界支持的高斯机制的简单修改，可以在数据相关核算下增强隐私保证

    

    arXiv：2403.05598v1 公布类型：跨领域 摘要：数据相关的隐私核算框架，如每实例差分隐私（pDP）和费舍尔信息损失（FIL），为固定训练数据集中的个体提供细粒度的隐私保证。与传统的差分隐私相比，这些保证在现实世界的环境中可能更为理想，因为它们严格地上界约束了$\textit{实际}$数据集中$\textit{特定}$个体的隐私泄露，而不是考虑最坏情况的数据集。尽管这些框架开始受到欢迎，但迄今为止，缺乏可以充分利用数据相关核算优势的私人机制。为了填补这一空白，我们提出了对有界支持的高斯机制进行简单修改，证明它们在数据相关核算下增强了隐私保证。通过对DP-SGD模型训练的实验表明，使用有界支持高斯机制可以降低pDP界限$\epsilon$。

    arXiv:2403.05598v1 Announce Type: cross  Abstract: Data-dependent privacy accounting frameworks such as per-instance differential privacy (pDP) and Fisher information loss (FIL) confer fine-grained privacy guarantees for individuals in a fixed training dataset. These guarantees can be desirable compared to vanilla DP in real world settings as they tightly upper-bound the privacy leakage for a $\textit{specific}$ individual in an $\textit{actual}$ dataset, rather than considering worst-case datasets. While these frameworks are beginning to gain popularity, to date, there is a lack of private mechanisms that can fully leverage advantages of data-dependent accounting. To bridge this gap, we propose simple modifications of the Gaussian mechanism with bounded support, showing that they amplify privacy guarantees under data-dependent accounting. Experiments on model training with DP-SGD show that using bounded support Gaussian mechanisms can provide a reduction of the pDP bound $\epsilon$ by
    
[^210]: 使用传统机器学习和深度学习技术比较步态相位检测

    Comparison of gait phase detection using traditional machine learning and deep learning techniques

    [https://arxiv.org/abs/2403.05595](https://arxiv.org/abs/2403.05595)

    本研究比较了使用传统机器学习和深度学习技术在下肢肌电图数据上进行步态相位检测，对于控制下肢辅助设备具有重要意义。

    

    人类行走是一项复杂的活动，需要身体不同系统之间的高度合作和互动。实时准确检测步态阶段对于控制下肢辅助设备如外骨骼和假肢至关重要。本研究提出了基于下肢肌电图（EMG）数据进行人类行走的几个基于机器学习（ML）的模型。所提出的模型基于高斯朴素贝叶斯（NB）、决策树（DT）、随机森林（RF）、线性判别分析（LDA）和深度卷积神经网络（DCNN）。传统的ML模型是在手工设计的特征上进行训练的。

    arXiv:2403.05595v1 Announce Type: cross  Abstract: Human walking is a complex activity with a high level of cooperation and interaction between different systems in the body. Accurate detection of the phases of the gait in real-time is crucial to control lower-limb assistive devices like exoskeletons and prostheses. There are several ways to detect the walking gait phase, ranging from cameras and depth sensors to the sensors attached to the device itself or the human body. Electromyography (EMG) is one of the input methods that has captured lots of attention due to its precision and time delay between neuromuscular activity and muscle movement. This study proposes a few Machine Learning (ML) based models on lower-limb EMG data for human walking. The proposed models are based on Gaussian Naive Bayes (NB), Decision Tree (DT), Random Forest (RF), Linear Discriminant Analysis (LDA) and Deep Convolutional Neural Networks (DCNN). The traditional ML models are trained on hand-crafted features
    
[^211]: 复杂手工密集型制造过程的数据驱动人体工程学风险评估

    Data-Driven Ergonomic Risk Assessment of Complex Hand-intensive Manufacturing Processes

    [https://arxiv.org/abs/2403.05591](https://arxiv.org/abs/2403.05591)

    开发了一个数据驱动的人体工程学风险评估系统，特别关注手部和手指活动，以更好地识别和解决与手工密集型制造过程相关的人体工程学问题

    

    手工密集型制造过程，如复合材料铺设和纺织成形，需要显著的人体灵巧性以适应任务复杂性。这些繁重的手部动作往往导致肌肉骨骼疾病和康复手术。我们开发了一个数据驱动的人体工程学风险评估系统，特别关注手部和手指活动，以更好地识别和解决与手工密集型制造过程相关的人体工程学问题。该系统包括一个多模式传感器实验平台，用于收集和同步操作员上半身姿势、手部姿势和施加力量；一个用于测量高保真度手部和手指风险的Biometric Assessment of Complete Hand (BACH)配方；以及与上半身姿势、RULA和手部活动HAL相关的行业标准风险评分。我们的研究结果表明，BACH相比现有的指标能够更细致地捕捉有害活动。机器学习模型

    arXiv:2403.05591v1 Announce Type: cross  Abstract: Hand-intensive manufacturing processes, such as composite layup and textile draping, require significant human dexterity to accommodate task complexity. These strenuous hand motions often lead to musculoskeletal disorders and rehabilitation surgeries. We develop a data-driven ergonomic risk assessment system with a special focus on hand and finger activity to better identify and address ergonomic issues related to hand-intensive manufacturing processes. The system comprises a multi-modal sensor testbed to collect and synchronize operator upper body pose, hand pose and applied forces; a Biometric Assessment of Complete Hand (BACH) formulation to measure high-fidelity hand and finger risks; and industry-standard risk scores associated with upper body posture, RULA, and hand activity, HAL. Our findings demonstrate that BACH captures injurious activity with a higher granularity in comparison to the existing metrics. Machine learning models
    
[^212]: 解释布局可以影响人对冒犯性句子的感知吗？

    Can Interpretability Layouts Influence Human Perception of Offensive Sentences?

    [https://arxiv.org/abs/2403.05581](https://arxiv.org/abs/2403.05581)

    本文通过用户研究探讨了机器学习解释布局是否会影响参与者对包含仇恨言论句子的评价，结果表明解释布局在触发参与者提供纠正性反馈和评估模型方面具有优势

    

    本文进行了一项用户研究，评估三种机器学习（ML）解释布局是否会影响参与者评估包含仇恨言论的句子时的观点，重点关注“厌恶女性”和“种族主义”两类。鉴于文献中存在分歧的结论，我们通过统计和定性分析问卷调查回应的实证证据，探讨在在线社区中使用ML解释性的优势。广义可加模型估计参与者的评级，融合了组内设计和组间设计。尽管我们的统计分析表明，没有任何解释布局显著影响参与者的观点，但我们的定性分析表明ML解释性的优势：1）触发参与者在他们的观点与模型之间存在差异时提供纠正性反馈，2）提供评估模型的见解

    arXiv:2403.05581v1 Announce Type: cross  Abstract: This paper conducts a user study to assess whether three machine learning (ML) interpretability layouts can influence participants' views when evaluating sentences containing hate speech, focusing on the "Misogyny" and "Racism" classes. Given the existence of divergent conclusions in the literature, we provide empirical evidence on using ML interpretability in online communities through statistical and qualitative analyses of questionnaire responses. The Generalized Additive Model estimates participants' ratings, incorporating within-subject and between-subject designs. While our statistical analysis indicates that none of the interpretability layouts significantly influences participants' views, our qualitative analysis demonstrates the advantages of ML interpretability: 1) triggering participants to provide corrective feedback in case of discrepancies between their views and the model, and 2) providing insights to evaluate a model's 
    
[^213]: 将文本到图像和大型语言模型串联：生成个性化电子商务横幅的新方法

    Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners

    [https://arxiv.org/abs/2403.05578](https://arxiv.org/abs/2403.05578)

    本研究提出了一种新方法，利用文本到图像模型和大型语言模型生成个性化网页横幅，根据用户互动动态内容，并且无需人工干预。

    

    arXiv:2403.05578v1 公告类型：交叉摘要：稳定扩散等文本到图像模型为生成艺术作品开辟了大量机会。最近的文献调查了文本到图像模型在增强许多创意艺术家工作中的应用。许多电子商务平台采用手动流程生成横幅，这是耗时的且存在可扩展性的局限性。在这项工作中，我们展示了利用文本到图像模型根据在线购物者的互动生成具有动态内容的个性化网页横幅的用途。此方法的新颖之处在于在没有人为干预的情况下将用户互动数据转换为有意义的提示。为此，我们利用大型语言模型（LLM）系统地从项目元信息中提取属性元组。然后通过提示工程将这些属性传递给文本到图像模型，以生成横幅的图像。我们的结果表明，所提出的方法可以创建高-

    arXiv:2403.05578v1 Announce Type: cross  Abstract: Text-to-image models such as stable diffusion have opened a plethora of opportunities for generating art. Recent literature has surveyed the use of text-to-image models for enhancing the work of many creative artists. Many e-commerce platforms employ a manual process to generate the banners, which is time-consuming and has limitations of scalability. In this work, we demonstrate the use of text-to-image models for generating personalized web banners with dynamic content for online shoppers based on their interactions. The novelty in this approach lies in converting users' interaction data to meaningful prompts without human intervention. To this end, we utilize a large language model (LLM) to systematically extract a tuple of attributes from item meta-information. The attributes are then passed to a text-to-image model via prompt engineering to generate images for the banner. Our results show that the proposed approach can create high-
    
[^214]: 超越儿童福利领域的预测性算法

    Beyond Predictive Algorithms in Child Welfare

    [https://arxiv.org/abs/2403.05573](https://arxiv.org/abs/2403.05573)

    研究发现，儿童福利领域常用的风险评估模型无法准确预测未与出生父母团聚的儿童的出院结果。

    

    儿童福利部门的工作人员使用建立在风险评估（RA）数据上的预测性决策算法来指导和支持儿童福利决策。研究人员指出，RA可能包含偏见信号，会削弱儿童福利案例的复杂性，并且这些算法可能受益于纳入上下文丰富的案例描述，即由工作人员撰写的案例笔记。为了调查这种假设的改进，我们定量地解构了美国一个儿童福利机构两种常用的RA。我们训练了分类器模型，比较了带有和不带有案例笔记描述的RA的预测有效性，并对案例笔记进行计算机文本分析以突出其中揭示的主题。我们的研究发现，用于评估家庭并构建CWS预测性风险模型（PRMs）的常见风险指标无法预测未与出生父母团聚的儿童的出院结果。

    arXiv:2403.05573v1 Announce Type: cross  Abstract: Caseworkers in the child welfare (CW) sector use predictive decision-making algorithms built on risk assessment (RA) data to guide and support CW decisions. Researchers have highlighted that RAs can contain biased signals which flatten CW case complexities and that the algorithms may benefit from incorporating contextually rich case narratives, i.e. - casenotes written by caseworkers. To investigate this hypothesized improvement, we quantitatively deconstructed two commonly used RAs from a United States CW agency. We trained classifier models to compare the predictive validity of RAs with and without casenote narratives and applied computational text analysis on casenotes to highlight topics uncovered in the casenotes. Our study finds that common risk metrics used to assess families and build CWS predictive risk models (PRMs) are unable to predict discharge outcomes for children who are not reunified with their birth parent(s). We also
    
[^215]: 具有约束扩散模型的高效和保证安全的非凸轨迹优化

    Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with Constrained Diffusion Model

    [https://arxiv.org/abs/2403.05571](https://arxiv.org/abs/2403.05571)

    本文提出了一种具有约束扩散模型的高效和保证安全的非凸轨迹优化框架，通过结合扩散模型和数值求解器，保证了计算效率和约束满足。

    

    机器人轨迹优化面临一个具有挑战性的非凸问题，这是由于复杂的动力学和环境设置造成的。本文引入了一个通用且完全可并行化的框架，将扩散模型和数值求解器结合起来，用于非凸轨迹优化，确保计算效率和约束满足。提出了一种新颖的带有额外约束违反损失的约束扩散模型进行训练。它旨在在采样过程中近似局部最优解的分布，同时最小化约束违反。然后用样本作为数值求解器的初始猜测，来优化并得出最终解，并验证可行性和最优性。

    arXiv:2403.05571v1 Announce Type: cross  Abstract: Trajectory optimization in robotics poses a challenging non-convex problem due to complex dynamics and environmental settings. Traditional numerical optimization methods are time-consuming in finding feasible solutions, whereas data-driven approaches lack safety guarantees for the output trajectories. In this paper, we introduce a general and fully parallelizable framework that combines diffusion models and numerical solvers for non-convex trajectory optimization, ensuring both computational efficiency and constraint satisfaction. A novel constrained diffusion model is proposed with an additional constraint violation loss for training. It aims to approximate the distribution of locally optimal solutions while minimizing constraint violations during sampling. The samples are then used as initial guesses for a numerical solver to refine and derive final solutions with formal verification of feasibility and optimality. Experimental evalua
    
[^216]: 用自适应关系图神经网络改进认知诊断模型

    Improving Cognitive Diagnosis Models with Adaptive Relational Graph Neural Networks

    [https://arxiv.org/abs/2403.05559](https://arxiv.org/abs/2403.05559)

    提出了一种利用自适应语义感知图神经网络改进认知诊断模型的方法，弥补了现有研究中对边缘内的异质性和不确定性的忽视。

    

    认知诊断（CD）算法在智能教育中受到越来越多的研究关注。典型的CD算法通过推断学生的能力（即他们在各种知识概念上的熟练水平）来帮助学生。这种熟练水平可以进一步实现针对性的技能训练和个性化的练习建议，从而促进在线教育中学生的学习效率。最近，研究人员发现建立和整合学生-练习二部图对于增强诊断性能是有益的。然而，他们的研究仍然存在局限性。一方面，研究人员忽视了边缘内的异质性，即可能存在正确和错误的答案。另一方面，他们忽视了边缘内的不确定性，例如，正确的答案可能表示真正掌握或幸运猜测。为解决这些限制，我们提出了自适应语义感知图

    arXiv:2403.05559v1 Announce Type: cross  Abstract: Cognitive Diagnosis (CD) algorithms receive growing research interest in intelligent education. Typically, these CD algorithms assist students by inferring their abilities (i.e., their proficiency levels on various knowledge concepts). The proficiency levels can enable further targeted skill training and personalized exercise recommendations, thereby promoting students' learning efficiency in online education. Recently, researchers have found that building and incorporating a student-exercise bipartite graph is beneficial for enhancing diagnostic performance. However, there are still limitations in their studies. On one hand, researchers overlook the heterogeneity within edges, where there can be both correct and incorrect answers. On the other hand, they disregard the uncertainty within edges, e.g., a correct answer can indicate true mastery or fortunate guessing. To address the limitations, we propose Adaptive Semantic-aware Graph-ba
    
[^217]: 重新思考具有层次感知标签关系建模的人类活动识别

    Re-thinking Human Activity Recognition with Hierarchy-aware Label Relationship Modeling

    [https://arxiv.org/abs/2403.05557](https://arxiv.org/abs/2403.05557)

    重新思考人类活动识别任务，通过层级感知的标签关系建模提高模型性能，并将复杂的标签关系纳入基本模型中

    

    人类活动识别（HAR）已经研究了几十年，从数据收集、学习模型到后处理和结果解释。然而，尽管活动中存在的层次结构对模型性能和解释有着显著影响，但这种层次结构仍然相对未被充分探讨。在本文中，我们提出了一种称为H-HAR的方法，通过深入研究活动中复杂的全局标签关系，从一个新的角度重新思考HAR任务。我们不是为多层次的活动分别构建多个分类器，而是探索通过基于图的标签关系建模增强的平面模型的有效性。基于层次意识的图基标签建模将复杂的标签关系纳入基本HAR模型中。我们使用复杂的人类活动数据验证了该提议。结果强调了该提议的优势，可以垂直提高

    arXiv:2403.05557v1 Announce Type: cross  Abstract: Human Activity Recognition (HAR) has been studied for decades, from data collection, learning models, to post-processing and result interpretations. However, the inherent hierarchy in the activities remains relatively under-explored, despite its significant impact on model performance and interpretation. In this paper, we propose H-HAR, by rethinking the HAR tasks from a fresh perspective by delving into their intricate global label relationships. Rather than building multiple classifiers separately for multi-layered activities, we explore the efficacy of a flat model enhanced with graph-based label relationship modeling. Being hierarchy-aware, the graph-based label modeling enhances the fundamental HAR model, by incorporating intricate label relationships into the model. We validate the proposal with a multi-label classifier on complex human activity data. The results highlight the advantages of the proposal, which can be vertically i
    
[^218]: 通过混合马尔可夫模型对学生的参与行为进行建模和预测

    Modeling and predicting students' engagement behaviors using mixture Markov models

    [https://arxiv.org/abs/2403.05556](https://arxiv.org/abs/2403.05556)

    本文通过混合马尔可夫模型对学生的参与行为进行建模和预测，并引入了一种名为K-EM的基于K均值的初始化方法，实验结果表明其效果极具前景

    

    学生的参与反映了他们在进行中的学习过程中的参与程度，可以通过他们与基于计算机的学习或评估系统的互动来估计。激发学生的参与需要具备大致的表示模型，以理解学生的各种（不）参与行为。本文利用基于模型的聚类，生成K个混合马尔可夫模型，对包含学生（不）参与行为模式的轨迹进行分组。为了防止期望最大化（EM）算法陷入局部最大值，我们还引入了一种名为K-EM的基于K均值的初始化方法。我们对两个真实数据集进行了实验，使用EM算法的三个变体：原始EM、emEM、K-EM；以及两个数据集的非混合基线模型。所提出的K-EM表现出了极具前景。

    arXiv:2403.05556v1 Announce Type: cross  Abstract: Students' engagements reflect their level of involvement in an ongoing learning process which can be estimated through their interactions with a computer-based learning or assessment system. A pre-requirement for stimulating student engagement lies in the capability to have an approximate representation model for comprehending students' varied (dis)engagement behaviors. In this paper, we utilized model-based clustering for this purpose which generates K mixture Markov models to group students' traces containing their (dis)engagement behavioral patterns. To prevent the Expectation-Maximization (EM) algorithm from getting stuck in a local maxima, we also introduced a K-means-based initialization method named as K-EM. We performed an experimental work on two real datasets using the three variants of the EM algorithm: the original EM, emEM, K-EM; and, non-mixture baseline models for both datasets. The proposed K-EM has shown very promising
    
[^219]: MOOC中的子群体发现：描述不同类型学习者的大数据应用

    Subgroup Discovery in MOOCs: A Big Data Application for Describing Different Types of Learners

    [https://arxiv.org/abs/2403.05555](https://arxiv.org/abs/2403.05555)

    通过基于MapReduce的子群体发现方法，本论文旨在对MOOC中的不同类型学习者进行分类和描述，提出的方法在处理海量数据集中表现优异。

    

    本文旨在通过基于MapReduce的子群体发现方法对海量开放在线课程（MOOCs）中的不同类型学习者进行分类和描述。最终目标是发现出现在不同MOOC中的IF-THEN规则。所提出的子群体发现方法是对著名的FP-Growth算法的扩展，考虑了MapReduce等新兴并行方法，以应对极大型数据集。作为附加特性，该提案包括一个阈值数值，用于指示每个发现的规则应满足的课程数量。还包括后处理步骤，以删除多余的子群体。实验阶段通过考虑来自edX平台上16门MITx和HarvardX课程首年的去标识化数据进行。实验结果表明，所提出的MapReduce方法胜过传统顺序子群体方法。

    arXiv:2403.05555v1 Announce Type: cross  Abstract: The aim of this paper is to categorize and describe different types of learners in massive open online courses (MOOCs) by means of a subgroup discovery approach based on MapReduce. The final objective is to discover IF-THEN rules that appear in different MOOCs. The proposed subgroup discovery approach, which is an extension of the well-known FP-Growth algorithm, considers emerging parallel methodologies like MapReduce to be able to cope with extremely large datasets. As an additional feature, the proposal includes a threshold value to denote the number of courses that each discovered rule should satisfy. A post-processing step is also included so redundant subgroups can be removed. The experimental stage is carried out by considering de-identified data from the first year of 16 MITx and HarvardX courses on the edX platform. Experimental results demonstrate that the proposed MapReduce approach outperforms traditional sequential subgroup
    
[^220]: 通过语义匹配理解教育话题的演进

    Understanding the Progression of Educational Topics via Semantic Matching

    [https://arxiv.org/abs/2403.05553](https://arxiv.org/abs/2403.05553)

    本文利用BERT主题建模从课程中提取主题，然后利用这些主题来识别不同学科之间的关系，帮助我们更好地理解各种学习话题的演进。

    

    教育系统正在动态变化，以适应技术进步、工业和社会需求，并增强学生的学习旅程。课程专家和教育工作者不断修订各教育年级所教授的科目，以确定差距，引入新的学习话题，并提高学习成果。本文利用数据科学来更好地了解各种学习话题的演进。

    arXiv:2403.05553v1 Announce Type: cross  Abstract: Education systems are dynamically changing to accommodate technological advances, industrial and societal needs, and to enhance students' learning journeys. Curriculum specialists and educators constantly revise taught subjects across educational grades to identify gaps, introduce new learning topics, and enhance the learning outcomes. This process is usually done within the same subjects (e.g. math) or across related subjects (e.g. math and physics) considering the same and different educational levels, leading to massive multi-layer comparisons. Having nuanced data about subjects, topics, and learning outcomes structured within a dataset, empowers us to leverage data science to better understand the progression of various learning topics. In this paper, Bidirectional Encoder Representations from Transformers (BERT) topic modeling was used to extract topics from the curriculum, which were then used to identify relationships between su
    
[^221]: 多源多模态数据融合在混合式学习大学课程中预测学术表现

    Multi-source and multimodal data fusion for predicting academic performance in blended learning university courses

    [https://arxiv.org/abs/2403.05552](https://arxiv.org/abs/2403.05552)

    该研究应用多源多模态数据融合方法，发现在混合式学习环境中预测学生学术表现的最佳属性集为理论课上的关注程度、Moodle测验成绩以及Moodle论坛上的活动水平。

    

    在这篇论文中，我们应用数据融合方法来预测大学生在混合式学习环境中的最终学术表现，利用来自多个源头、多模态的数据。我们从不同来源收集和预处理了关于大一学生的数据：理论课、实践课、在线Moodle课程以及期末考试。我们的目标是发现哪种数据融合方法在我们的数据中产生了最好的结果。我们进行了实验，应用了四种不同的数据融合方法和六种分类算法。结果显示，集成和选择最佳属性方法与离散化数据一起产生了最佳预测结果。最佳预测模型显示，理论课上的关注程度、Moodle测验成绩以及Moodle论坛上的活动水平是预测学生最终表现的最佳属性集。

    arXiv:2403.05552v1 Announce Type: cross  Abstract: In this paper we applied data fusion approaches for predicting the final academic performance of university students using multiple-source, multimodal data from blended learning environments. We collected and preprocessed data about first-year university students from different sources: theory classes, practical sessions, on-line Moodle sessions, and a final exam. Our objective was to discover which data fusion approach produced the best results using our data. We carried out experiments by applying four different data fusion approaches and six classification algorithms. The results showed that the best predictions were produced using ensembles and selecting the best attributes approach with discretized data. The best prediction models showed us that the level of attention in theory classes, scores in Moodle quizzes, and the level of activity in Moodle forums were the best set of attributes for predicting students' final performance in
    
[^222]: 使用BERT监测极端社交媒体上反犹太主义话语的演变

    Monitoring the evolution of antisemitic discourse on extremist social media using BERT

    [https://arxiv.org/abs/2403.05548](https://arxiv.org/abs/2403.05548)

    本研究提出了一种使用BERT监测极端社交媒体上反犹太主义话语演变的自动方法，避免了手动监测的不可行性，为干预和防止仇恨升级提供了新途径。

    

    研究表明，社交媒体上的种族主义和不宽容有可能在线下产生仇恨，最终导致身体暴力。本研究考虑的是在线反犹主义，追踪在线讨论中的反犹主题及其相关术语的演变，有助于监测参与者的情绪和演变，并可能提供干预方法，防止仇恨升级。鉴于在线流量庞大且不断变化，手动监测谈话实际上是不现实的。因此，我们提出了一种自动化方法，可以从极端社交媒体中提取反犹主题和术语，跟踪它们的演变。由于监督学习在这样的任务中过于受限，我们开发了一种无监督的在线机器学习方法，使用大型语言模型。

    arXiv:2403.05548v1 Announce Type: cross  Abstract: Racism and intolerance on social media contribute to a toxic online environment which may spill offline to foster hatred, and eventually lead to physical violence. That is the case with online antisemitism, the specific category of hatred considered in this study. Tracking antisemitic themes and their associated terminology over time in online discussions could help monitor the sentiments of their participants and their evolution, and possibly offer avenues for intervention that may prevent the escalation of hatred. Due to the large volume and constant evolution of online traffic, monitoring conversations manually is impractical. Instead, we propose an automated method that extracts antisemitic themes and terminology from extremist social media over time and captures their evolution. Since supervised learning would be too limited for such a task, we created an unsupervised online machine learning approach that uses large language model
    
[^223]: AI对非程序员的应用：应用AI在没有编程技能的学生课堂中的应用

    AI for non-programmers: Applied AI in the lectures for students without programming skills

    [https://arxiv.org/abs/2403.05547](https://arxiv.org/abs/2403.05547)

    本研究提出了一个应用AI的教学规划脚本，通过将AI概念与研究相关主题联系起来，促进了学生对AI潜力和风险的理解和兴趣。

    

    诸如ChatGPT和WOMBO Dream等应用使得启发无编程知识的学生使用人工智能（AI）变得轻而易举。鉴于AI在各个学科中的日益重要，需要创新策略来教育那些没有编程知识的学生，以便将AI作为未来技能融入他们的学习模块。本文提出了一个应用AI的教学规划脚本。该教学规划脚本基于AI应用流程，并将AI概念与研究相关主题联系起来。这些联系打开了新的解决方案空间，并促进了学生对AI潜力和风险的兴趣和理解。以能源管理硕士学生为例，展示了如何将AI无缝地整合到专业课程中。为此，应用AI的教学规划脚本被调整以适应研究项目的主题。

    arXiv:2403.05547v1 Announce Type: cross  Abstract: Applications such as ChatGPT and WOMBO Dream make it easy to inspire students without programming knowledge to use artificial intelligence (AI). Therefore, given the increasing importance of AI in all disciplines, innovative strategies are needed to educate students in AI without programming knowledge so that AI can be integrated into their study modules as a future skill. This work presents a didactic planning script for applied AI. The didactic planning script is based on the AI application pipeline and links AI concepts with study-relevant topics. These linkages open up a new solution space and promote students' interest in and understanding of the potentials and risks of AI. An example lecture series for master students in energy management shows how AI can be seamlessly integrated into discipline-specific lectures. To this end, the planning script for applied AI is adapted to fit the study programs' topic. This specific teaching s
    
[^224]: 通过结合AFC和APC数据在公共交通网络上实现统一乘客占座率

    Unified Occupancy on a Public Transport Network through Combination of AFC and APC Data

    [https://arxiv.org/abs/2403.05546](https://arxiv.org/abs/2403.05546)

    本文提出了统一乘客占座率方法，通过结合AFC和APC数据，在公共交通网络上推断每个班次的乘客占座率，弥补了缺失的APC信息。

    

    在交通网络中，车上的乘客占座率对于了解旅客习惯并调整提供至关重要。传统上，运营商依靠实地研究评估典型工作日的乘车情况。然而，自动收费系统（AFC）和自动乘客计数（APC）数据往往可获得但未充分利用，提供了完整的时间覆盖。然而，需要注意的是，每种数据源都带有其自身的偏见：AFC数据可能无法考虑欺诈行为，而并非所有车辆都配备有APC系统。 本文引入了统一乘客占座率方法，这是一种地理统计模型，通过将AFC和APC数据与部分覆盖的范围相结合来推断公共交通网络内每趟车的乘客占座率。统一乘客占座率为在其他趟车有APC测量的线路上的某些趟车，以及在根本没有APC数据可用的线路上的某些趟车，补全了缺失的APC信息。

    arXiv:2403.05546v1 Announce Type: cross  Abstract: In a transport network, the onboard occupancy is key for gaining insights into travelers' habits and adjusting the offer. Traditionally, operators have relied on field studies to evaluate ridership of a typical workday. However, automated fare collection (AFC) and automatic passenger counting (APC) data, which provide complete temporal coverage, are often available but underexploited. It should be noted, however, that each data source comes with its own biases: AFC data may not account for fraud, while not all vehicles are equipped with APC systems.   This paper introduces the unified occupancy method, a geostatistical model to extrapolate occupancy to every course of a public transportation network by combining AFC and APC data with partial coverage. Unified occupancy completes missing APC information for courses on lines where other courses have APC measures, as well as for courses on lines where no APC data is available at all. The 
    
[^225]: 金融机构ESG中的人工智能：一个产业调查

    AI in ESG for Financial Institutions: An Industrial Survey

    [https://arxiv.org/abs/2403.05541](https://arxiv.org/abs/2403.05541)

    本文调查了金融机构中人工智能在ESG倡议中的应用，阐明了AI在加强ESG框架方面的必要性和影响，以及AI如何增强金融活动和可持续发展目标之间的复杂相互作用。

    

    人工智能（AI）日益融入金融行业的环境、社会和治理（ESG）倡议，代表了向更可持续和公平金融实践的范式转变。本文调查产业格局，阐明了AI在加强ESG框架中的必要性和影响。随着严格的监管要求和利益相关者意识的提高，金融机构（FIs）越来越被迫采纳ESG标准。AI成为在导航金融活动和可持续发展目标的复杂相互作用中的关键工具。我们的调查对ESG的三个主要支柱中的AI应用进行了分类，阐明了AI如何增强分析能力、风险评估、客户参与、报告准确性等方面。此外，我们深入探讨了围绕数据使用和模型开发的关键考虑因素。

    arXiv:2403.05541v1 Announce Type: cross  Abstract: The burgeoning integration of Artificial Intelligence (AI) into Environmental, Social, and Governance (ESG) initiatives within the financial sector represents a paradigm shift towards more sus-tainable and equitable financial practices. This paper surveys the industrial landscape to delineate the necessity and impact of AI in bolstering ESG frameworks. With the advent of stringent regulatory requirements and heightened stakeholder awareness, financial institutions (FIs) are increasingly compelled to adopt ESG criteria. AI emerges as a pivotal tool in navigating the complex in-terplay of financial activities and sustainability goals. Our survey categorizes AI applications across three main pillars of ESG, illustrating how AI enhances analytical capabilities, risk assessment, customer engagement, reporting accuracy and more. Further, we delve into the critical con-siderations surrounding the use of data and the development of models, und
    
[^226]: AI的灭绝风险：对科学是否隐形的思考

    Extinction Risks from AI: Invisible to Science?

    [https://arxiv.org/abs/2403.05540](https://arxiv.org/abs/2403.05540)

    论文探讨了AI的灭绝风险，并提出了灭绝级古哈特定律，指出任何目标规范过度追求都可能导致人类灭绝，同时指出评估此假设需要满足一定条件，但可能由于模型复杂性而难以实现，暗示了人工智能带来的灭绝风险可能在当前科学方法下难以观察到。

    

    为了讨论AI所带来的灭绝风险，我们提出了灭绝级古哈特定律，即“几乎任何目标规范，如果过度追求，将导致人类的灭绝”，并试图了解哪些形式模型适合研究这一假设。我们对灭绝级古哈特定律是否成立持中立态度。作为我们的主要贡献，我们确定了一组必要的条件，这些条件对于一个旨在评估灭绝级古哈特定律具体论点的模型是必要的。由于每个条件似乎在相当程度上增加了最终模型的复杂性，形式评估这一假设可能会异常困难。这引发了一个可能性，即无论来自人工智能的灭绝风险是否真实，其潜在动态可能在当前科学方法下无法观察到。

    arXiv:2403.05540v1 Announce Type: cross  Abstract: In an effort to inform the discussion surrounding existential risks from AI, we formulate Extinction-level Goodhart's Law as "Virtually any goal specification, pursued to the extreme, will result in the extinction of humanity", and we aim to understand which formal models are suitable for investigating this hypothesis. Note that we remain agnostic as to whether Extinction-level Goodhart's Law holds or not. As our key contribution, we identify a set of conditions that are necessary for a model that aims to be informative for evaluating specific arguments for Extinction-level Goodhart's Law. Since each of the conditions seems to significantly contribute to the complexity of the resulting model, formally evaluating the hypothesis might be exceedingly difficult. This raises the possibility that whether the risk of extinction from artificial intelligence is real or not, the underlying dynamics might be invisible to current scientific method
    
[^227]: 在批强化学习中，通过切换损失函数来降低成本

    Switching the Loss Reduces the Cost in Batch Reinforcement Learning

    [https://arxiv.org/abs/2403.05385](https://arxiv.org/abs/2403.05385)

    使用对数损失函数来训练适合的Q迭代的批强化学习方法，在实现目标时不产生成本的问题中，其样本数量需求与最优策略的累积成本成比例，能够提供与最优可达成本成比例的“小成本”界限，并在实验中验证在那些最优策略可靠实现目标的问题中，FQI-LOG比使用平方损失训练的FQI使用更少的样本。

    

    我们提出了一种使用对数损失（FQI-LOG）来训练适合的Q迭代的批强化学习（RL）方法。我们展示了使用FQI-LOG学习接近最优策略所需的样本数量与最优策略的累积成本成比例，对于那些通过最优行为实现目标且不产生成本的问题，最优策略的累积成本为零。通过这种方式，我们提供了一种在批RL中证明具有与最优可达成本成比例的“小成本”界限的一般框架。此外，我们从经验上验证，FQI-LOG在那些最优策略可靠地实现目标的问题上使用的样本比使用平方损失训练的FQI要少。

    arXiv:2403.05385v1 Announce Type: new  Abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.
    
[^228]: 细胞重编程设计通过功能转录网络的迁移学习

    Cell reprogramming design by transfer learning of functional transcriptional networks

    [https://arxiv.org/abs/2403.04837](https://arxiv.org/abs/2403.04837)

    通过迁移学习设计细胞重编程，利用功能转录网络的知识来控制细胞行为，实现从初始状态到目标状态的转录状态之间的最小化差异

    

    最近合成生物学、下一代测序和机器学习的发展为基于基因扰动和药物的测量响应来合理设计新的疾病治疗提供了前所未有的机会，主要挑战是细胞网络的知识不完整和可能干预的组合爆炸，这两者都无法通过实验来克服。为了解决这些挑战，我们开发了一种迁移学习方法来控制细胞行为，该方法在与人类细胞命运相关的转录组数据上进行了预训练，从而生成一个可以转移到特定重编程目标的网络动态模型。该方法结合了对基因扰动的转录反应，以最小化给定初始和目标转录状态之间的差异。我们通过展示我们的方法的多功能性来证明其

    arXiv:2403.04837v1 Announce Type: cross  Abstract: Recent developments in synthetic biology, next-generation sequencing, and machine learning provide an unprecedented opportunity to rationally design new disease treatments based on measured responses to gene perturbations and drugs to reprogram cells. The main challenges to seizing this opportunity are the incomplete knowledge of the cellular network and the combinatorial explosion of possible interventions, both of which are insurmountable by experiments. To address these challenges, we develop a transfer learning approach to control cell behavior that is pre-trained on transcriptomic data associated with human cell fates, thereby generating a model of the network dynamics that can be transferred to specific reprogramming goals. The approach combines transcriptional responses to gene perturbations to minimize the difference between a given pair of initial and target transcriptional states. We demonstrate our approach's versatility by 
    
[^229]: TopicDiff：一种用于多模态会话情感检测的主题丰富扩散方法

    TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection

    [https://arxiv.org/abs/2403.04789](https://arxiv.org/abs/2403.04789)

    提出了一种TopicDiff方法，用于捕获多模态会话情感检测任务中的主题信息，通过将扩散模型集成到神经主题模型中，解决了神经主题模型在捕获主题信息方面的多样性不足问题，并相对于现有MCE基线取得了显著改进

    

    多模态会话情感（MCE）检测通常跨越声学、视觉和语言模态，吸引了多媒体社区日益增加的兴趣。先前的研究主要集中在学习对话中的语境信息，只有少数考虑单一语言模态中的主题信息，而总是忽视声学和视觉主题信息。在此基础上，我们提出了一个模型不可知的Topic-enriched Diffusion（TopicDiff）方法，用于捕获MCE任务中的多模态主题信息。特别是，我们将扩散模型集成到神经主题模型中，以缓解神经主题模型在捕获主题信息方面的多样性不足问题。详细的评估表明，TopicDiff相对于最先进的MCE基线取得了显著改进，证明了多模态主题信息对MCE的重要性以及TopicDiff的有效性。

    arXiv:2403.04789v1 Announce Type: cross  Abstract: Multimodal Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the topic information in single language modality, while always neglecting the acoustic and vision topic information. On this basis, we propose a model-agnostic Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic information in MCE tasks. Particularly, we integrate the diffusion model into neural topic model to alleviate the diversity deficiency problem of neural topic model in capturing topic information. Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of multimodal topic information to MCE and the effectiveness of Topic
    
[^230]: 移除GPT4的过滤器

    Removing GPT4's Filter

    [https://arxiv.org/abs/2403.04769](https://arxiv.org/abs/2403.04769)

    提出了一种方法，可以使经过微调的GPT4恢复到没有经过人类反馈强化学习训练的状态，从而移除其在学习期间的所有安全机制

    

    GPT4最初在大量数据集上进行训练，然后使用来自人类反馈的强化学习进行微调，即志愿者提供反馈以教导GPT4不要生成不当内容。本文提出了一种方法来操作已经进行微调的版本，使其恢复到没有经过RLHF（Reinforcement learning from Human Feedback）的行为，有效地移除了模型在RLHF期间学习的所有安全机制。特别是，当GPT4在没有经过RLHF的情况下运行时，它失去了所有抑制力，只需前几个词就可以生成非常不当的内容。

    arXiv:2403.04769v1 Announce Type: cross  Abstract: GPT4 was initially trained on large amounts of data, and then fine-tuned using Reinforcement learning from Human Feedback (RLHF), which is when volunteers give feedback in order to teach GPT4 not to create inappropriate content. In this paper, we present a method to manipulate the fine-tuned version into reverting to pre-RLHF behavior, effectively removing all safety mechanisms that the model learned during RLHF. In particular, when GPT4 acts without RLHF, it loses all inhibition, and can complete very inappropriate content given only the first few words.
    
[^231]: 使图像真实的因素是什么？

    What makes an image realistic?

    [https://arxiv.org/abs/2403.04493](https://arxiv.org/abs/2403.04493)

    论文讨论了如何设计能够可靠区分真实数据和不真实数据的函数，提出了通用评论者的概念作为一个新的解决方案。

    

    在过去的十年里，我们在生成看起来真实的数据方面取得了巨大进展，无论是图像、文本、音频还是视频。在这里，我们讨论了与之密切相关的问题，即量化现实主义，即设计能够可靠地区分真实数据和不真实数据的函数。从算法信息理论的观点出发，我们讨论了为什么这个问题很具挑战性，为什么一个好的生成模型单独不能解决它，以及一个好的解决方案应该是什么样的。特别是，我们引入了通用评论者的概念，不像对抗性评论者那样需要对抗性训练。尽管通用评论者并不立即实用，但它们既可以作为引导实际实现的北极星，也可以作为一个工具。

    arXiv:2403.04493v1 Announce Type: new  Abstract: The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool 
    
[^232]: 通过社区挑战推动生物医学文本挖掘的发展

    Advancing Biomedical Text Mining with Community Challenges

    [https://arxiv.org/abs/2403.04261](https://arxiv.org/abs/2403.04261)

    社区挑战评估竞赛在促进生物医学文本挖掘研究中的技术创新和跨学科合作方面起着重要作用。

    

    生物医学研究领域积累了大量来自科学文献、电子病历、临床试验报告和社交媒体等各方面的文本数据，然而手动处理和分析这些庞大且复杂的资源是耗时且低效的。为了解决这一挑战，生物医学文本挖掘，也称为生物医学自然语言处理，备受关注。社区挑战评估竞赛在促进生物医学文本挖掘研究中的技术创新和跨学科合作方面发挥了重要作用。这些挑战为研究人员提供了开发生物医学研究中数据挖掘和信息处理的最新解决方案的平台。在本文中，我们回顾了与中文生物医学文本挖掘有关的最新社区挑战的进展。

    arXiv:2403.04261v1 Announce Type: new  Abstract: The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firs
    
[^233]: Aligners: 解耦LLMs和对齐

    Aligners: Decoupling LLMs and Alignment

    [https://arxiv.org/abs/2403.04224](https://arxiv.org/abs/2403.04224)

    提出了一种通过训练对齐器模型来解耦大型语言模型（LLMs）和对齐，以减少对齐对性能的潜在负面影响。

    

    大型语言模型（LLMs）需要与人类期望对齐，以确保它们在大多数应用中的安全性和实用性。对齐具有挑战性，成本高昂，并且需要为每个LLM和对齐标准重复进行。我们建议通过训练可以根据需要用于对齐给定标准的任何LLM的对齐模型来解耦LLMs和对齐，从而在一定程度上减少对性能的潜在负面影响。我们提出的对齐模型训练配方仅依赖于使用（提示的）LLM 生成的合成数据，并且可以轻松调整以适应各种对齐标准。我们通过训练一个“道德”对齐器并在实验上验证其有效性来阐明我们的方法。

    arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
    
[^234]: SWAP-NAS: 适用于超快速NAS的样本级激活模式

    SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS

    [https://arxiv.org/abs/2403.04161](https://arxiv.org/abs/2403.04161)

    提出了一种新颖的高性能无需训练的度量SWAP-Score，能够在不同搜索空间和任务中测量网络在一批输入样本上的表现能力，并通过正则化进一步提高相关性，实现模型大小的控制。

    

    无需训练的度量（即零成本代理）被广泛用于避免资源密集型的神经网络训练，尤其是在神经结构搜索（NAS）中。最近的研究表明，现有的无需训练的度量存在一些局限，比如在不同搜索空间和任务之间存在有限的关联性和差劲的泛化能力。因此，我们提出了样本级激活模式及其衍生物SWAP-Score，这是一种新颖的高性能无需训练的度量。它测量了网络在一批输入样本上的表现能力。SWAP-Score与不同搜索空间和任务中的真实性能强相关，在NAS-Bench-101/201/301和TransNAS-Bench-101上胜过了15种现有的无需训练的度量。SWAP-Score可以通过正则化进一步增强，这在基于单元的搜索空间中可以实现更高的相关性，并且在搜索过程中实现模型大小控制。例如，Spearman的排序

    arXiv:2403.04161v1 Announce Type: new  Abstract: Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank
    
[^235]: 大型语言模型能够进行推理和规划吗？

    Can Large Language Models Reason and Plan?

    [https://arxiv.org/abs/2403.04121](https://arxiv.org/abs/2403.04121)

    大型语言模型缺乏自我批评能力，无法像人类一样纠正错误。

    

    虽然人类有时候表现出能够通过自我批评纠正自己错误猜测的能力，但似乎在大型语言模型的情况下没有依据支持这一假设。

    arXiv:2403.04121v1 Announce Type: new  Abstract: While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.
    
[^236]: DeepCRE：利用尖端计算模型改革药物研发

    DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models

    [https://arxiv.org/abs/2403.03768](https://arxiv.org/abs/2403.03768)

    DeepCRE是一种新型的计算模型，在患者级别CRE性能上平均提高了17.7％，在指示级别CRE增加了5倍，并成功确定了六个具有显着优势的药物候选者。

    

    arXiv:2403.03768v1 公告类型：新摘要：药物开发领域和治疗应用领域都面临着重大挑战。治疗领域需要更多的治疗选择，同时大量有前景的临床前药物在临床试验中失败。一个原因是在药物开发的后期阶段交叉药物反应评估（CRE）的不足。尽管计算机模拟的CRE模型为解决这一问题提供了一种解决方案，但现有方法学要么局限于早期开发阶段，要么缺乏对全面CRE分析的能力。在这里，我们介绍了一种名为DeepCRE的新型计算模型，并展示了DeepCRE在推动治疗发现和发展方面的潜力。DeepCRE通过实现患者级别CRE平均性能提高17.7\%，指示级别CRE增加了5倍，优于现有最佳模型。此外，DeepCRE已经确定了六个显示出明显更大优势的药物候选者。

    arXiv:2403.03768v1 Announce Type: new  Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater ef
    
[^237]: 一种AI启用的基于Agent的模型及其在新西兰麻疹爆发模拟中的应用

    An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand

    [https://arxiv.org/abs/2403.03434](https://arxiv.org/abs/2403.03434)

    通过将图神经网络（GNN）和长短期记忆（LSTM）网络与传统ABM相结合，开发了一种可微分的基于Agent的模型，成功应用于模拟新西兰2019年麻疹爆发，深入洞察传染病爆发的动态。

    

    代理模型（ABMs）已经成为研究复杂社会互动的强大工具，特别是在公共卫生和传染病调查的背景下。为了增强传统ABM，实现自动化模型校准并减少扩展模型所需的计算资源，我们开发了一个通过耦合图神经网络（GNN）和长短期记忆（LSTM）网络来进行张量化和可微分的基于Agent的模型。该模型被用于研究2019年发生在新西兰的麻疹爆发，展示了在高峰期重复病例中准确模拟爆发动态的有希望能力。本文表明，通过利用最新的人工智能（AI）技术和传统ABMs的能力，我们可以更深入地了解传染病爆发的动态，从而帮助我们做出更好的决策。

    arXiv:2403.03434v1 Announce Type: cross  Abstract: Agent Based Models (ABMs) have emerged as a powerful tool for investigating complex social interactions, particularly in the context of public health and infectious disease investigation. In an effort to enhance the conventional ABM, enabling automated model calibration and reducing the computational resources needed for scaling up the model, we have developed a tensorized and differentiable agent-based model by coupling Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) network. The model was employed to investigate the 2019 measles outbreak occurred in New Zealand, demonstrating a promising ability to accurately simulate the outbreak dynamics, particularly during the peak period of repeated cases. This paper shows that by leveraging the latest Artificial Intelligence (AI) technology and the capabilities of traditional ABMs, we gain deeper insights into the dynamics of infectious disease outbreaks. This, in turn, helps us ma
    
[^238]: 深度学习的假设空间

    Hypothesis Spaces for Deep Learning

    [https://arxiv.org/abs/2403.03353](https://arxiv.org/abs/2403.03353)

    本文介绍了一种应用深度神经网络的深度学习假设空间，并构建了一个再生核Banach空间，研究了正则化学习和最小插值问题，证明了学习模型的解可以表示为线性组合。

    

    本文介绍了一种应用深度神经网络（DNNs）的深度学习假设空间。通过将DNN视为两个变量的函数，即物理变量和参数变量，我们考虑了DNNs的原始集合，参数变量位于由DNNs的权重矩阵和偏置决定的一组深度和宽度中。然后在弱*拓扑中完成原始DNN集合的线性跨度，以构建一个物理变量函数的Banach空间。我们证明所构造的Banach空间是一个再生核Banach空间（RKBS），并构造其再生核。通过为学习模型的解建立表达定理，我们研究了两个学习模型，正则化学习和最小插值问题在结果RKBS中。表达定理揭示了这些学习模型的解可以表示为线性组合

    arXiv:2403.03353v1 Announce Type: cross  Abstract: This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of
    
[^239]: 在5G RF领域，用于干扰检测的空中双阈值深度学习器

    Over-The-Air Double-Threshold Deep Learner for Jamming Detection in 5G RF domain

    [https://arxiv.org/abs/2403.02645](https://arxiv.org/abs/2403.02645)

    本文提出了一种在5G网络中检测干扰者的新型深度学习技术，通过引入双阈值深度学习干扰检测器，专注于SSB的RF领域特征，提高了网络的鲁棒性。

    

    随着5G无线通信的发展，同步信号块（SSB）在设备同步和服务可访问性中起着关键作用。然而，由于SSB传输具有可预测性，包括主要同步信号（PSS）和次要同步信号（SSS），干扰攻击是重要威胁。本文利用RF领域知识，提出了一种新颖的基于深度学习的5G网络干扰检测技术。与现有的大多依赖网络参数的干扰检测算法不同，我们通过专注于SSB引入了双阈值深度学习干扰检测器。该检测方法侧重于RF领域特征，提高了网络的鲁棒性，无需与现有网络基础设施集成。通过集成一个预处理块来提取PSS相关性和每个空闲资源元素的能量（EPNRE）

    arXiv:2403.02645v1 Announce Type: cross  Abstract: With the evolution of 5G wireless communications, the Synchronization Signal Block (SSB) plays a critical role in the synchronization of devices and accessibility of services. However, due to the predictable nature of SSB transmission, including the Primary and Secondary Synchronization Signals (PSS and SSS), jamming attacks are critical threats. By leveraging RF domain knowledge, this work presents a novel deep learning-based technique for detecting jammers in 5G networks. Unlike the existing jamming detection algorithms that mostly rely on network parameters, we introduce a double threshold deep learning jamming detector by focusing on the SSB. The detection method is focused on RF domain features and improves the robustness of the network without requiring integration with the pre-existing network infrastructure. By integrating a preprocessing block that extracts PSS correlation and energy per null resource elements (EPNRE) characte
    
[^240]: 通过锚多元分析改善泛化能力

    Improving generalisation via anchor multivariate analysis

    [https://arxiv.org/abs/2403.01865](https://arxiv.org/abs/2403.01865)

    引入因果正则化扩展到锚回归（AR）中，提出了与锚框架相匹配的损失函数确保稳健性，各种多元分析算法均在锚框架内，简单正则化增强了OOD设置中的稳健性，验证了锚正则化的多功能性和对因果推断方法论的推进。

    

    我们在锚回归（AR）中引入因果正则化扩展，以改善超出分布（OOD）的泛化能力。我们提出了与锚框架相匹配的损失函数，以确保对分布转移的稳健性。各种多元分析（MVA）算法，如（正交化）PLS、RRR和MLR，均在锚框架内。我们观察到简单的正则化增强了OOD设置中的稳健性。在合成和真实的气候科学问题中，为所选算法提供了估计器，展示了其一致性和有效性。经验验证突显了锚正则化的多功能性，强调其与MVA方法的兼容性，并强调其在增强可复制性的同时抵御分布转移中的作用。扩展的AR框架推进了因果推断方法论，解决了可靠OOD泛化的需求。

    arXiv:2403.01865v1 Announce Type: cross  Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.
    
[^241]: NASH：用于硬件优化机器学习模型的神经架构搜索

    NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models

    [https://arxiv.org/abs/2403.01845](https://arxiv.org/abs/2403.01845)

    NASH是一种将神经架构搜索应用于机器学习硬件的新方法，可以帮助硬件设计实现高吞吐量、低延迟和优越的准确性表现。

    

    随着机器学习（ML）算法在越来越多的应用中部署，这些算法需要在高准确性、高吞吐量和低延迟之间取得更好的权衡。本文介绍了一种名为NASH的新方法，将神经架构搜索应用于机器学习硬件。使用NASH，硬件设计不仅可以实现高吞吐量和低延迟，还可以实现优越的准确性表现。本文提出了四个版本的NASH策略，所有这些策略显示出比原始模型更高的准确性。该策略可以应用于各种卷积神经网络，从众多模型操作中选择特定操作，引导训练过程朝向更高的准确性。实验结果显示，在 ResNet18 或 ResNet34 上应用NASH，与非NASH版本相比，可使Top1准确率提高高达3.1%，Top5准确率提高高达2.2%。

    arXiv:2403.01845v1 Announce Type: cross  Abstract: As machine learning (ML) algorithms get deployed in an ever-increasing number of applications, these algorithms need to achieve better trade-offs between high accuracy, high throughput and low latency. This paper introduces NASH, a novel approach that applies neural architecture search to machine learning hardware. Using NASH, hardware designs can achieve not only high throughput and low latency but also superior accuracy performance. We present four versions of the NASH strategy in this paper, all of which show higher accuracy than the original models. The strategy can be applied to various convolutional neural networks, selecting specific model operations among many to guide the training process toward higher accuracy. Experimental results show that applying NASH on ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top 5 accuracy increase of up to 2.2% compared to the non-NASH version when tested on the Imag
    
[^242]: 利用行为原语搭建任务的框架以提高数据效率的模仿学习

    PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning

    [https://arxiv.org/abs/2403.00929](https://arxiv.org/abs/2403.00929)

    PRIME是一个基于行为原语设计的框架，通过将任务分解为原语序列并学习高级控制策略，显著提高了多阶段操作任务的性能表现。

    

    模仿学习已经显示出巨大潜力，可以让机器人学会复杂的操作行为。然而，在长期任务中，这些算法受到高样本复杂度的困扰，因为复合误差会在任务时段内累积。我们提出了PRIME（基于行为原语的数据效率模仿），这是一个基于行为原语的框架，旨在提高模仿学习的数据效率。PRIME通过将任务演示分解为原语序列来搭建机器人任务，然后通过模仿学习学习一个高级控制策略来对原语序列进行排序。我们的实验证明，PRIME在多阶段操作任务中实现了显著的性能提升，在模拟环境中的成功率比最先进的基线高出10-34％，在实际硬件上高出20-48％。

    arXiv:2403.00929v1 Announce Type: cross  Abstract: Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in simulation over state-of-the-art baselines and 20-48% on physical hardware.
    
[^243]: 直接与Chat-Fine-Tuned LLMs的草案模型对齐

    Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs

    [https://arxiv.org/abs/2403.00858](https://arxiv.org/abs/2403.00858)

    通过提出的框架，我们训练了一种用于Llama 2 Chat 7B或更大模型的草案模型，实现了加速推理，仅占原始大小的1.64％。

    

    文本生成与大型语言模型（LLMs）由于其自回归本质、巨大的参数数量和有限的内存带宽而被认为是内存密集型，通常导致低令牌速率。猜测解码已被提出作为LLM推理加速的解决方案。然而，在现代开源LLM系列中，例如Llama 2 7B，由于草案模型通常不可用，因此需要训练高质量的草案模型以通过猜测解码实现推理加速。在本文中，我们提出了一个简单的草案模型训练框架，用于直接与Chat-capable目标模型对齐。通过我们提出的框架，我们训练出Llama 2 Chat Drafter 115M，这是一个适用于Llama 2 Chat 7B或更大模型的草案模型，仅占原始大小的1.64％。我们的训练框架仅包括预训练、蒸馏数据集生成和使用知识蒸馏进行微调，没有额外的对齐步骤。

    arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
    
[^244]: 自适应学习率的FTRL算法的竞争比分析和最佳方案研究

    Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds

    [https://arxiv.org/abs/2403.00715](https://arxiv.org/abs/2403.00715)

    通过引入竞争分析框架，我们提出了调整FTRL学习率的更新规则，使其在常数因子内达到最佳竞争比，并且展示了当惩罚项具有近似单调性时的竞争比特性。

    

    Follow-The-Regularized-Leader (FTRL)被认为是在线学习中一种有效且多功能的方法，其中学习率的恰当选择对于减小后悔是至关重要的。为此，我们将调整FTRL学习率的问题构建为一个顺序决策问题，并引入竞争分析框架。我们建立了竞争比的下界，并提出了学习率的更新规则，使其在一个常数因子内达到下界的上界。具体地，我们说明了最优竞争比是由惩罚项的组成部分的（近似）单调性所决定的，表明如果惩罚项的组成部分形成单调非增序列，则可以实现常数竞争比，并推导出了在惩罚项$\xi$近似单调非增时的紧密竞争比。我们提出的更新规则被称为...

    arXiv:2403.00715v1 Announce Type: new  Abstract: Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile approach in online learning, where appropriate choice of the learning rate is crucial for smaller regret. To this end, we formulate the problem of adjusting FTRL's learning rate as a sequential decision-making problem and introduce the framework of competitive analysis. We establish a lower bound for the competitive ratio and propose update rules for learning rate that achieves an upper bound within a constant factor of this lower bound. Specifically, we illustrate that the optimal competitive ratio is characterized by the (approximate) monotonicity of components of the penalty term, showing that a constant competitive ratio is achievable if the components of the penalty term form a monotonically non-increasing sequence, and derive a tight competitive ratio when penalty terms are $\xi$-approximately monotone non-increasing. Our proposed update rule, referred to a
    
[^245]: 大型卷积模型的参数高效调整

    Parameter-Efficient Tuning of Large Convolutional Models

    [https://arxiv.org/abs/2403.00269](https://arxiv.org/abs/2403.00269)

    通过引入滤波器子空间和滤波器原子的概念，本研究提出了一种在微调大型卷积模型时仅调整少量参数来提取任务特定表示的方法。

    

    为了解决微调大型预训练模型所需的高计算和参数复杂性，研究人员开发了参数高效的方法，仅更新下游任务的部分参数。然而，这些工作通常忽视了卷积核的独特属性，而卷积核仍然是许多大型模型的基本元素，比如Stable Diffusion。在本研究中，我们首先通过在每个网络层内分解卷积核到一小组滤波器子空间元素，即滤波器原子，引入了滤波器子空间。然后，我们通过仅调整滤波器原子（通常为几百个参数）对这些模型进行微调，以提取任务特定的表示。为了潜在地扩展调整的参数空间，我们进一步展示了一种简单的方法，通过递归地将每个筛选原子分解到另一组筛选原子来生成一个过完备的滤波器子空间。

    arXiv:2403.00269v1 Announce Type: cross  Abstract: To address the high computational and parameter complexity associated with fine-tuning large pre-trained models, researchers have developed parameter-efficient methods, where only partial parameters are updated for downstream tasks. However, these works often overlook the distinct properties of convolutional kernels, which still remain essential elements in many large models, such as Stable Diffusion. In this study, we first introduce filter subspace by decomposing convolutional kernels within each network layer over a small set of filter subspace elements, referred to as filter atoms. We then fine-tune these models to extract task-specific representation by only adapting the filter atoms, a few hundred parameters typically. To potentially expand the parameter space for tuning, we further show a simple approach to generate an overcomplete filter subspace by recursively decomposing each filter atom over another set of filter atoms. The 
    
[^246]: 具有一级信念的假设在线学习在不对称信息随机博弈中的应用

    Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games

    [https://arxiv.org/abs/2402.18781](https://arxiv.org/abs/2402.18781)

    提出了一种具有假设在线学习（COL）的学习方案，针对通用AISG，结构化为一个先验预测者-演员-评论家（FAC）架构，利用一级信念和对手策略的主观预测，通过在线展开更新策略，并通过贝叶斯学习校准假设。

    

    随机博弈出现在许多复杂的社会技术系统中，如网络物理系统和IT基础设施，信息不对称为决策实体（玩家）的决策带来挑战。现有的不对称信息随机博弈（AISG）的计算方法主要是离线的，针对特殊类别的AISG，以避免信念层次，并且缺乏适应均衡偏差的在线能力。为了解决这一限制，我们提出了一种具有假设在线学习（COL）的学习方案，专门针对通用AISG。COL结构化为一个先验预测者-演员-评论家（FAC）架构，利用对隐藏状态的一级信念和对对手策略的主观预测。针对假设的对手，COL通过在线展开更新策略，并通过贝叶斯学习校准假设。我们证明了COL中的假设与t一致。

    arXiv:2402.18781v1 Announce Type: cross  Abstract: Stochastic games arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures, where information asymmetry presents challenges for decision-making entities (players). Existing computational methods for asymmetric information stochastic games (AISG) are primarily offline, targeting special classes of AISGs to avoid belief hierarchies, and lack online adaptability to deviations from equilibrium. To address this limitation, we propose a conjectural online learning (COL), a learning scheme for generic AISGs. COL, structured as a forecaster-actor-critic (FAC) architecture, utilizes first-order beliefs over the hidden states and subjective forecasts of the opponent's strategies. Against the conjectured opponent, COL updates strategies in an actor-critic approach using online rollout and calibrates conjectures through Bayesian learning. We prove that conjecture in COL is asymptotically consistent with t
    
[^247]: ICE-SEARCH: 一种基于语言模型驱动的特征选择方法

    ICE-SEARCH: A Language Model-Driven Feature Selection Approach

    [https://arxiv.org/abs/2402.18609](https://arxiv.org/abs/2402.18609)

    ICE-SEARCH是首个将语言模型与进化算法相结合用于特征选择任务的方法，在医学预测分析应用中取得了State-of-the-Art(SOTA)表现。

    

    本研究揭示了In-Context Evolutionary Search (ICE-SEARCH)方法，这是首个将语言模型(LMs)与进化算法相结合用于特征选择(FS)任务的工作，并展示了其在医学预测分析(MPA)应用中的有效性。ICE-SEARCH利用语言模型中固有的交叉和突变能力，在一个进化框架内显着改进特征选择，通过模型的全面世界知识和其适应各种角色的能力。我们对该方法的评估涵盖了三个关键的MPA任务：中风、心血管疾病和糖尿病，在这些任务中ICE-SEARCH在确定医学应用的关键特征方面优于传统的FS方法。ICE-SEARCH在中风预测和糖尿病预测中实现了领先水平；决策随机化ICE-SEARCH在心血管疾病预测中排名为领先水平。我们的结果不仅证明了

    arXiv:2402.18609v1 Announce Type: cross  Abstract: This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate
    
[^248]: MMSR：符号回归是一个多模态任务

    MMSR: Symbolic Regression is a Multimodal Task

    [https://arxiv.org/abs/2402.18603](https://arxiv.org/abs/2402.18603)

    符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。

    

    数学公式是探索自然规律几千年来人类智慧的结晶。用简洁的数学公式描述复杂的自然规律是科学家不断追求的目标，也是人工智能面临的重大挑战。这一领域被称为符号回归。在本文中，研究人员将从数据到表达式的映射视为翻译问题，并引入了相应的大规模预训练模型。

    arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
    
[^249]: MIM-Reasoner: 具有理论保证的多重影响最大化学习

    MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization

    [https://arxiv.org/abs/2402.16898](https://arxiv.org/abs/2402.16898)

    引入了MIM-Reasoner，结合强化学习和概率图模型，有效地捕捉了给定多重网络内部和层间的复杂传播过程，从而解决了MIM中最具挑战性的问题。

    

    多重影响最大化（MIM）要求我们识别一组种子用户，以最大化多重网络中受影响用户的预期数量。本文介绍了MIM-Reasoner，将强化学习与概率图模型相结合，有效捕捉给定多重网络内部和层间的复杂传播过程，从而解决了MIM中最具挑战性的问题。

    arXiv:2402.16898v1 Announce Type: cross  Abstract: Multiplex influence maximization (MIM) asks us to identify a set of seed users such as to maximize the expected number of influenced users in a multiplex network. MIM has been one of central research topics, especially in nowadays social networking landscape where users participate in multiple online social networks (OSNs) and their influences can propagate among several OSNs simultaneously. Although there exist a couple combinatorial algorithms to MIM, learning-based solutions have been desired due to its generalization ability to heterogeneous networks and their diversified propagation characteristics. In this paper, we introduce MIM-Reasoner, coupling reinforcement learning with probabilistic graphical model, which effectively captures the complex propagation process within and between layers of a given multiplex network, thereby tackling the most challenging problem in MIM. We establish a theoretical guarantee for MIM-Reasoner as w
    
[^250]: 语言模型数据选择概述

    A Survey on Data Selection for Language Models

    [https://arxiv.org/abs/2402.16827](https://arxiv.org/abs/2402.16827)

    大型语言模型成功的关键在于使用大规模的文本数据集进行无监督预训练，但如何优化选择数据以降低碳足迹和财务成本仍是一个挑战。

    

    最近大型语言模型取得成功的一个主要因素是利用巨大且不断增长的文本数据集进行无监督预训练。然而，简单地在所有可用数据上训练模型可能并不是最佳选择（或不可行），因为可用文本数据的质量可能有所不同。数据过滤也可以通过减少所需的训练量来降低训练模型的碳足迹和财务成本。数据选择方法旨在确定要包括在训练数据集中的哪些候选数据点，以及如何从所选数据点中适当采样。改进的数据选择方法的前景已经导致该领域的研究量迅速扩大。然而，由于深度学习主要受实证证据驱动，对大规模数据进行实验成本昂贵，很少有组织拥有资源进行广泛的数据选择研究。因此，有效数据选择的知识可能大多局限于大型技术公司或研究机构内部。

    arXiv:2402.16827v1 Announce Type: new  Abstract: A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data se
    
[^251]: 一种使用注释嵌入模型的本体包含关系预测自匹配训练方法

    A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction

    [https://arxiv.org/abs/2402.16278](https://arxiv.org/abs/2402.16278)

    提出了一种自匹配训练方法，通过两种本体嵌入模型捕获全局和局部信息，提高了概念子类预测的稳健性

    

    最近，提出了一种在低维空间中表示实体的本体嵌入，用于本体完成。然而，用于概念子类预测的本体嵌入未解决类似和孤立实体的困难，并且未提取本体中注释公理的全局信息。本文提出了一种针对两种本体嵌入模型的自匹配训练方法：Inverted-index Matrix Embedding (InME) 和 Co-occurrence Matrix Embedding (CoME)。这两种嵌入通过每个单词在一组公理中出现的位置以及每个公理中单词的共现来捕获注释公理中的全局和局部信息。自匹配训练方法提高了概念子类预测的稳健性，当预测的超类与子类相似且孤立于本体中的其他实体时。

    arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
    
[^252]: 量子神经网络频谱的光谱不变性和极大性质

    Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks

    [https://arxiv.org/abs/2402.14515](https://arxiv.org/abs/2402.14515)

    量子神经网络研究了频谱的极大性质，证明了在一类模型中存在极大结果，以及在一些条件下存在保持频谱的光谱不变性，解释了文献中观察到的结果对称性。

    

    量子神经网络（QNNs）是量子机器学习领域的热门方法，由于其与变分量子电路的密切联系，使其成为在噪声中间尺度量子（NISQ）设备上进行实际应用的有前途的候选方法。QNN可以表示为有限傅里叶级数，其中频率集被称为频谱。我们分析了这个频谱并证明，对于一大类模型，存在各种极大性结果。此外，我们证明在一些温和条件下，存在一个保持频谱的具有相同面积$A = RL$的模型类之间的双射，其中$R$表示量子比特数量，$L$表示层数，我们因此称之为面积保持变换下的光谱不变性。通过这个，我们解释了文献中经常观察到的在结果中$R$和$L$的对称性，并展示了最大频谱的依赖性

    arXiv:2402.14515v1 Announce Type: cross  Abstract: Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine Learning due to their close connection to Variational Quantum Circuits, making them a promising candidate for practical applications on Noisy Intermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite Fourier series, where the set of frequencies is called the frequency spectrum. We analyse this frequency spectrum and prove, for a large class of models, various maximality results. Furthermore, we prove that under some mild conditions there exists a bijection between classes of models with the same area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the number of qubits and $L$ the number of layers, which we consequently call spectral invariance under area-preserving transformations. With this we explain the symmetry in $R$ and $L$ in the results often observed in the literature and show that the maximal frequency spectrum depen
    
[^253]: 面向门控循环单元的定制混合精度低于8位量化方案的研究，使用遗传算法

    Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms

    [https://arxiv.org/abs/2402.12263](https://arxiv.org/abs/2402.12263)

    提出了面向门控循环单元的定制混合精度低于8位量化方案，使用遗传算法优化模型尺寸和准确性，在四个不同顺序任务上展示混合精度解决方案优于同质精度解决方案，实现了模型尺寸缩减25%至55%同时保持准确性。

    

    虽然深度神经网络模型压缩技术近年来取得了进展，但在超低功耗嵌入式设备上部署这些模型仍然具有挑战性。特别是针对门控循环单元（GRU）的量化方案很难调整，因为它们依赖于内部状态，无法充分从低于8位的量化中获益。在本研究中，我们提出了一种模块化整数量化方案，其中每个运算符的位宽可以独立选择。然后，我们采用遗传算法（GA）来探索可能位宽的庞大搜索空间，同时优化模型尺寸和准确性。我们在四个不同的顺序任务上评估了我们的方法，并表明混合精度解决方案在Pareto效率方面超过同质精度解决方案。在我们的结果中，我们实现了模型尺寸在25%至55%之间的缩减，同时保持了与t相当的准确性。

    arXiv:2402.12263v1 Announce Type: new  Abstract: Despite the recent advances in model compression techniques for deep neural networks, deploying such models on ultra-low-power embedded devices still proves challenging. In particular, quantization schemes for Gated Recurrent Units (GRU) are difficult to tune due to their dependence on an internal state, preventing them from fully benefiting from sub-8bit quantization. In this work, we propose a modular integer quantization scheme for GRUs where the bit width of each operator can be selected independently. We then employ Genetic Algorithms (GA) to explore the vast search space of possible bit widths, simultaneously optimising for model size and accuracy. We evaluate our methods on four different sequential tasks and demonstrate that mixed-precision solutions exceed homogeneous-precision ones in terms of Pareto efficiency. In our results, we achieve a model size reduction between 25% and 55% while maintaining an accuracy comparable with t
    
[^254]: 避免对比学习中的特征抑制：学习以前未曾学到的内容

    Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before

    [https://arxiv.org/abs/2402.11816](https://arxiv.org/abs/2402.11816)

    开发了一种多阶对比学习（MCL）框架，以解决对比学习中的特征抑制问题，并确保模型学习全面的表示。

    

    自监督对比学习已经成为从未标记数据中获取高质量表示的强大方法。然而，最近在标准对比学习（如SimCLR、CLIP中）中发现了特征抑制：在单个端到端训练阶段，对比模型仅捕获对比观点之间的一部分共享信息，而忽略了其他潜在有用的信息。具有特征抑制，对比模型通常无法学习足够适用于各种下游任务的表示。为了减轻特征抑制问题并确保对比模型学习全面的表示，我们开发了一种新颖的多阶对比学习（MCL）框架。与通常会导致特征抑制的标准对比学习不同，MCL逐渐学习以前未探索过的新特征，同时保持已经学到的内容。

    arXiv:2402.11816v1 Announce Type: cross  Abstract: Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-lea
    
[^255]: 思维的提升：使用大型语言模型进行试错问题解决

    Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models

    [https://arxiv.org/abs/2402.11140](https://arxiv.org/abs/2402.11140)

    本文提出了一种名为Boosting of Thoughts（BoT）的自动提示框架，通过迭代地探索和自我评估多个思维树，获得一系列试错推理经验，作为解决复杂问题的新形式的提示。

    

    大型语言模型（LLMs）在各种问题上的推理性能关键取决于思维链提示，其中包括在提示中提供一些思维链示范作为示例。最近的工作（例如Thought Tree）指出了在复杂问题解决的推理步骤选择中，探索和自我评估的重要性。在本文中，我们提出了一种名为Boosting of Thoughts（BoT）的自动提示框架，用于通过迭代地探索和自我评估许多思维树来获得一系列试错推理经验，这将作为解决复杂问题的新形式的提示。BoT从一个简单提示开始，无需示例，迭代地探索和评估大量的推理步骤，更重要的是，利用LLM获得的错误分析来明确修改提示。

    arXiv:2402.11140v1 Announce Type: new  Abstract: The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompt
    
[^256]: 通过纯微调进行模型编辑

    Model Editing by Pure Fine-Tuning

    [https://arxiv.org/abs/2402.11078](https://arxiv.org/abs/2402.11078)

    纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。

    

    精细调整被认为在模型编辑中不够有效，因为相对更专业的方法而言，它的表现较差。然而，微调是简单的，不关心被编辑模型的体系结构细节，并且能够利用标准训练方法的不断进展（例如PEFT），使其成为模型编辑器的吸引选择。在本文中，我们展示了纯粹的微调可以是一种可行的模型编辑方法。我们提出了对朴素微调进行轻微修改的两个关键因素。第一，我们优化条件似然而非完整似然。第二，我们使用随机释义和事实来增加数据，以鼓励泛化和局部性。我们在ZsRE和CounterFact上的实验表明，这一简单修改使得微调通常可以与专业编辑器在编辑分数方面匹敌甚至超越。

    arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
    
[^257]: 与遗忘呼应的解除链接：简化GNN中的边解除

    Unlink to Unlearn: Simplifying Edge Unlearning in GNNs

    [https://arxiv.org/abs/2402.10695](https://arxiv.org/abs/2402.10695)

    研究揭示了GNN中边解除过程的关键问题，即过度遗忘现象，提出了解决方法来解决损失函数引起的问题。

    

    随着对数据隐私的担忧加剧，图神经网络（GNN）中的解除学习已经成为学术界一个突出的研究前沿。这一概念在强调被遗忘权利方面起着关键作用，包括在用户请求时有选择性地从已训练的GNN中删除特定数据。我们的研究关注边的解除学习，这一过程对现实应用特别相关，因为它具有广泛的适用性。目前的最先进方法如GNNDelete可以消除特定边的影响，然而我们的研究揭示了这些方法的一个关键局限，称为过度遗忘。当解除学习过程无意中除去超出特定数据的过多信息时，会导致对剩余边的预测准确性显著下降。为了解决这个问题，我们确定了GNNDelete的损失函数作为过度遗忘现象的主要来源。

    arXiv:2402.10695v1 Announce Type: cross  Abstract: As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. 
    
[^258]: 短视频和心理健康：基于知识导向的多模态神经主题模型

    Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal Neural Topic Model

    [https://arxiv.org/abs/2402.10045](https://arxiv.org/abs/2402.10045)

    这项研究针对短视频对观众心理健康的抑郁影响问题，开发了一种基于医学知识的多模态神经主题模型，以预测其影响并采取相应的干预措施。

    

    短视频正试图重新塑造整个社交媒体景观，然而专家们对其对观众的抑郁影响感到极度担忧，这一点已由医学研究证明。为了防止广泛影响，各平台渴望预测这些视频对观众心理健康的影响，从而采取干预措施，比如修订推荐算法和显示观众慎重选择。然而，现有的预测方法缺乏与抑郁症的临床证实的外部环境因素相关的医学知识。为了考虑这样的医学知识，我们采用了一种新兴的方法论学科——种子神经主题模型（NTMs）。然而，现有的种子NTMs存在单一来源主题、未知主题来源、模糊的种子监督和次优的收敛等局限性。为了解决这些挑战，我们开发了一种新颖的基于知识指导的多模态神经主题模型（Knowledg...（待补充）

    arXiv:2402.10045v1 Announce Type: cross  Abstract: While short-form videos head to reshape the entire social media landscape, experts are exceedingly worried about their depressive impacts on viewers, as evidenced by medical studies. To prevent widespread consequences, platforms are eager to predict these videos' impact on viewers' mental health. Subsequently, they can take intervention measures, such as revising recommendation algorithms and displaying viewer discretion. Nevertheless, applicable predictive methods lack relevance to well-established medical knowledge, which outlines clinically proven external and environmental factors of depression. To account for such medical knowledge, we resort to an emergent methodological discipline, seeded Neural Topic Models (NTMs). However, existing seeded NTMs suffer from the limitations of single-origin topics, unknown topic sources, unclear seed supervision, and suboptimal convergence. To address those challenges, we develop a novel Knowledg
    
[^259]: 无需稀疏模型的稀疏且准确的解释

    Sparse and Faithful Explanations Without Sparse Models

    [https://arxiv.org/abs/2402.09702](https://arxiv.org/abs/2402.09702)

    引入了稀疏解释值(SEV)，用于衡量机器学习模型的决策稀疏性。即使模型不是稀疏的，许多机器学习模型在SEV的衡量下仍具有低决策稀疏性。

    

    即使模型不满足全局的稀疏性，决策仍然可以用少量的特征准确地描述。例如，对于某人而言，尽管没有信用历史，但申请大笔贷款可能会被拒绝，这就忽视了与其信用价值相关的任何证据。在本论文中，我们引入了稀疏解释值（SEV），这是一种衡量机器学习模型稀疏性的新方法。在以上贷款拒绝的例子中，SEV为1，因为只需要一个因素来解释为什么贷款被拒绝。SEV是对决策稀疏性的衡量，而不是对整体模型稀疏性的衡量，并且我们能够证明许多机器学习模型——即使它们不是稀疏的——实际上在SEV的衡量下具有低决策稀疏性。SEV使用超立方体上的移动进行定义，使得SEV能够在各种模型类别上一致地定义，其中移动限制反映了模型的性质。

    arXiv:2402.09702v1 Announce Type: new  Abstract: Even if a model is not globally sparse, it is possible for decisions made from that model to be accurately and faithfully described by a small number of features. For instance, an application for a large loan might be denied to someone because they have no credit history, which overwhelms any evidence towards their creditworthiness. In this work, we introduce the Sparse Explanation Value (SEV), a new way of measuring sparsity in machine learning models. In the loan denial example above, the SEV is 1 because only one factor is needed to explain why the loan was denied. SEV is a measure of decision sparsity rather than overall model sparsity, and we are able to show that many machine learning models -- even if they are not sparse -- actually have low decision sparsity, as measured by SEV. SEV is defined using movements over a hypercube, allowing SEV to be defined consistently over various model classes, with movement restrictions reflectin
    
[^260]: 引导遮蔽表示学习以捕捉心电图的时空关系

    Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram

    [https://arxiv.org/abs/2402.09450](https://arxiv.org/abs/2402.09450)

    本研究提出了一种叫做ST-MEM的模型，通过重构遮蔽的心电图数据来学习时空特征，该模型在心律失常分类任务中优于其他自监督学习方法。

    

    心电图（ECG）广泛用作监测心脏起源的电信号的诊断工具。近年来，机器学习的研究努力集中在使用ECG信号进行各种疾病筛查的应用上。然而，适应疾病筛查应用是具有挑战性的，因为标记的ECG数据有限。通过自监督学习（SSL）实现通用表示是克服标记数据稀缺性的常用方法；然而，在ECG数据上纯粹应用SSL，而不考虑ECG信号固有的时空关系，可能会产生次优的结果。本文介绍了ST-MEM（时空遮蔽心电图建模），该模型通过重构遮蔽的12导联ECG数据来学习时空特征。在各种实验设置中，ST-MEM在心律失常分类任务中的性能优于其他SSL基线方法。

    arXiv:2402.09450v1 Announce Type: cross  Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Mo
    
[^261]: 采用分割引导扩散模型的解剖可控医学图像生成

    Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models

    [https://arxiv.org/abs/2402.05210](https://arxiv.org/abs/2402.05210)

    这篇论文提出了一种采用分割引导扩散模型的解剖可控医学图像生成方法，通过随机掩模消融训练算法实现对解剖约束的条件化，同时提高了网络对解剖真实性的学习能力。

    

    扩散模型已经实现了非常高质量的医学图像生成，可以通过为小型或不平衡的数据集提供补充，从而帮助减轻获取和注释新图像的费用，同时还可以应用于其他方面。然而，这些模型在生成图像时面临着全局解剖真实性的挑战。因此，我们提出了一种解剖可控的医学图像生成模型。我们的模型在每个采样步骤中遵循多类解剖分割掩模，并采用随机掩模消融训练算法，以实现对所选解剖约束的条件化，同时允许其他解剖区域的灵活性。这也改善了网络在完全无条件（无约束生成）情况下对解剖真实性的学习。通过对乳腺MRI和腹部/颈部到盆腔CT数据集的比较评估，证明了我们模型在解剖真实性和输入掩模保真度方面具有优越性。

    Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network's learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over st
    
[^262]: 物理信息神经网络的多尺度建模：从复杂系统的大尺度动力学到小尺度预测

    Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems

    [https://arxiv.org/abs/2402.05067](https://arxiv.org/abs/2402.05067)

    本文提出了利用物理信息神经网络进行多尺度建模的方法，通过解耦大尺度和小尺度动力学，并在正交基函数空间中近似小尺度系统。实验结果表明该方法在处理液体动力学问题以及更复杂的情况下具有较高的有效性和适用性。

    

    多尺度现象在各个科学领域中普遍存在，对于准确有效地预测复杂系统中的多尺度动力学提出了普遍的挑战。本文提出了一种通过解耦方法对多尺度动力学进行表征的新的求解模式。通过独立地建模大尺度动力学，并将小尺度动力学视为从属系统，我们开发了一种谱PINN方法，在正交基函数空间中接近小尺度系统。通过大量的数值实验，包括一维Kuramot-Sivashinsky (KS)方程、二维和三维Navier-Stokes (NS)方程，我们展示了该方法的有效性，展示了它在液体动力学问题中的多样性。此外，我们还深入研究了该方法在更复杂问题中的应用，包括非均匀网格、复杂几何形状、带噪声的大尺度数据和高维小尺度动力学。

    Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel solving mode is proposed for characterizing multiscale dynamics through a decoupling method. By modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system, a Spectral PINN is developed to approach the small-scale system in an orthogonal basis functional space. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional Navier-Stokes (NS) equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. 
    
[^263]: 多块预测：适应时间序列表示学习的LLMs方法

    Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning

    [https://arxiv.org/abs/2402.04852](https://arxiv.org/abs/2402.04852)

    本研究提出了aLLM4TS框架，将LLMs应用于时间序列表示学习。通过自监督的多块预测任务，捕捉时间动态特征，并通过特定时间序列上的微调进行进一步优化。

    

    本研究提出了一个创新的框架aLLM4TS，用于将大型语言模型（LLMs）应用于时间序列表示学习。我们的方法将时间序列预测重新构想为一项自监督的多块预测任务，相比传统的掩码和重构方法，更有效地捕捉了块表示中的时间动态。我们的策略包括两个阶段的训练：(i) 在各种时间序列数据集上进行因果连续预训练阶段，以下一个块预测为锚点，有效地将LLM的能力与时间序列数据的复杂性同步。(ii) 在目标时间序列上进行多块预测的微调。我们框架的一个独特要素是块级解码层，不同于之前依赖于序列级解码的方法。这样的设计直接将单个块转换为时间序列，从而显著增强了模型在掩蔽任务下的能力。

    In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mast
    
[^264]: 带风险最小化的分组分布鲁棒数据集蒸馏

    Group Distributionally Robust Dataset Distillation with Risk Minimization

    [https://arxiv.org/abs/2402.04676](https://arxiv.org/abs/2402.04676)

    这项研究关注数据集蒸馏与其泛化能力的关系，尤其是在面对不常见的子组的样本时，如何确保模型在合成数据集上的训练可以表现良好。

    

    数据集蒸馏（DD）已成为一种广泛采用的技术，用于构建一个合成数据集，该数据集在捕捉训练数据集的基本信息方面起到重要作用，从而方便准确训练神经模型。其应用涵盖了转移学习、联邦学习和神经架构搜索等各个领域。构建合成数据的最流行方法依赖于使模型在合成数据集和训练数据集上的收敛性能相匹配。然而，目标是将训练数据集视为辅助，就像训练集是人口分布的近似替代品一样，而后者才是我们感兴趣的数据。尽管其受欢迎程度很高，但尚未探索的一个方面是DD与其泛化能力的关系，特别是跨不常见的子组。也就是说，当面对来自罕见子组的样本时，我们如何确保在合成数据集上训练的模型表现良好。

    Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from re
    
[^265]: 在再生核希尔伯特空间中的Moreau包络的f-差异的Wasserstein梯度流

    Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces

    [https://arxiv.org/abs/2402.04613](https://arxiv.org/abs/2402.04613)

    本文研究了在再生核希尔伯特空间中使用Moreau包络来对测度f-差异进行正则化的方法，并利用该方法分析了Wasserstein梯度流。

    

    大多数常用的测度f-差异，例如Kullback-Leibler差异，对于所涉及的测度的支持存在限制。解决办法是通过与特征核K相关的平方最大均值差异(MMD)对f-差异进行正则化。在本文中，我们使用所谓的核均值嵌入来显示相应的正则化可以重写为与K相关的再生核希尔伯特空间中某些函数的Moreau包络。然后，我们利用关于希尔伯特空间中Moreau包络的众所周知的结果来证明MMD正则化的f-差异及其梯度的属性。随后，我们使用我们的研究结果来分析受MMD正则化的f-差异的Wasserstein梯度流。最后，我们考虑从经验测度开始的Wasserstein梯度流，并提供使用Tsallis-$\alpha$差异的概念性数值示例的证明。

    Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler divergence, are subject to limitations regarding the support of the involved measures. A remedy consists of regularizing the $f$-divergence by a squared maximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In this paper, we use the so-called kernel mean embedding to show that the corresponding regularization can be rewritten as the Moreau envelope of some function in the reproducing kernel Hilbert space associated with $K$. Then, we exploit well-known results on Moreau envelopes in Hilbert spaces to prove properties of the MMD-regularized $f$-divergences and, in particular, their gradients. Subsequently, we use our findings to analyze Wasserstein gradient flows of MMD-regularized $f$-divergences. Finally, we consider Wasserstein gradient flows starting from empirical measures and provide proof-of-the-concept numerical examples with Tsallis-$\alpha$ divergences.
    
[^266]: 冷启动无示例增量学习的弹性特征整合

    Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning

    [https://arxiv.org/abs/2402.03917](https://arxiv.org/abs/2402.03917)

    这篇论文解决了冷启动场景的无示例增量学习的问题，提出了一种弹性特征整合的方法，通过规范特征漂移并利用原型来减少任务新鲜度偏差。

    

    无示例类别增量学习（EFCIL）旨在从一系列任务中学习，而不需要访问先前任务的数据。在本文中，我们考虑了具有挑战性的冷启动场景，在第一个任务中没有足够的数据来学习高质量的骨干网络。对于EFCIL来说，这是特别具有挑战性的，因为它需要高度的可塑性，这会导致特征漂移，在无示例的情况下很难进行补偿。为了解决这个问题，我们提出了一种简单而有效的方法，通过规范在与先前任务高度相关的方向上的漂移，并利用原型来减少任务新鲜度偏差，以整合特征表示。我们的方法被称为弹性特征整合（EFC），它利用基于经验特征矩阵（EFM）的可解二阶近似来处理特征漂移。EFM在特征空间中引入了伪度量，我们使用它来规范重要方向上的特征漂移，并更新高斯原型。

    Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prot
    
[^267]: 带有切片Wasserstein Weisfeiler-Lehman图核的高斯过程回归

    Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels

    [https://arxiv.org/abs/2402.03838](https://arxiv.org/abs/2402.03838)

    本研究提出了一种带有切片Wasserstein Weisfeiler-Lehman图核的高斯过程回归方法，在处理大规模稀疏图形数据集时具有正定性和显著的复杂度降低。

    

    监督学习在计算物理领域引起了广泛关注，因为它能够有效地提取复杂模式，用于解决偏微分方程或预测材料性质等任务。传统上，这类数据集由具有大量节点的网格表示的输入（视为图形）和使用数值求解器获得的相应输出组成。这意味着监督学习模型必须能够处理具有连续节点属性的大规模稀疏图形。在本研究中，我们专注于高斯过程回归，引入了切片Wasserstein Weisfeiler-Lehman（SWWL）图核。与现有的图核相比，所提出的SWWL核具有正定性和显著的复杂度降低，使其能够处理此前不可处理的数据集。新的核首先在分子图分类中进行了验证。

    Supervised learning has recently garnered significant attention in the field of computational physics due to its ability to effectively extract complex patterns for tasks like solving partial differential equations, or predicting material properties. Traditionally, such datasets consist of inputs given as meshes with a large number of nodes representing the problem geometry (seen as graphs), and corresponding outputs obtained with a numerical solver. This means the supervised learning model must be able to handle large and sparse graphs with continuous node attributes. In this work, we focus on Gaussian process regression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman (SWWL) graph kernel. In contrast to existing graph kernels, the proposed SWWL kernel enjoys positive definiteness and a drastic complexity reduction, which  makes it possible to process datasets that were previously impossible to handle. The new kernel is first validated on graph classification for molec
    
[^268]: 大型语言模型中的时间箭头

    Arrows of Time for Large Language Models

    [https://arxiv.org/abs/2401.17505](https://arxiv.org/abs/2401.17505)

    这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。

    

    我们通过时间方向性的视角研究了自回归大型语言模型的概率建模。我们在实证上发现这类模型在建模自然语言能力上存在时间上的不对称性：预测下一个记号和预测前一个记号时的平均对数困惑度存在差异。这种差异既微妙又在不同的模态（语言、模型大小、训练时间等）下非常一致。从信息理论的角度来看，这在理论上是令人惊讶的，不应该存在这样的差异。我们提供了一个理论框架，解释了这种不对称性如何出现在稀疏性和计算复杂性考虑中，并概述了我们的结果带来的一些展望。

    We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
    
[^269]: 在多学习者环境中的战略应用

    Strategic Usage in a Multi-Learner Setting

    [https://arxiv.org/abs/2401.16422](https://arxiv.org/abs/2401.16422)

    在多学习者环境中，本研究分析了用户在多个可用服务中进行选择的战略行为对系统行为的影响。

    

    现实世界中的系统通常涉及一些用户在一系列服务之间进行选择。随着在线学习算法的流行，这些服务现在可以自我优化，利用对用户收集的数据来最大化某种奖励，如服务质量。然而，用户可能会战略性地选择要使用的服务，以追求他们自己的奖励函数，从而对哪些服务能够看到并使用他们的数据拥有权力。先前已对单服务设置中战略用户的影响进行了广泛研究，战略行为体现在操纵可观察特征以实现期望的分类上；然而，这对用户来说往往会很昂贵或难以实现，并且未能捕捉到多服务动态系统的全部行为。因此，我们分析了一个设置，其中战略用户在几个可用服务中进行选择，以追求积极的收益。

    arXiv:2401.16422v2 Announce Type: replace  Abstract: Real-world systems often involve some pool of users choosing between a set of services. With the increase in popularity of online learning algorithms, these services can now self-optimize, leveraging data collected on users to maximize some reward such as service quality. On the flipside, users may strategically choose which services to use in order to pursue their own reward functions, in the process wielding power over which services can see and use their data. Extensive prior research has been conducted on the effects of strategic users in single-service settings, with strategic behavior manifesting in the manipulation of observable features to achieve a desired classification; however, this can often be costly or unattainable for users and fails to capture the full behavior of multi-service dynamic systems. As such, we analyze a setting in which strategic users choose among several available services in order to pursue positive c
    
[^270]: SemPLeS: 语义提示学习用于弱监督语义分割

    SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation

    [https://arxiv.org/abs/2401.11791](https://arxiv.org/abs/2401.11791)

    SemPLeS框架利用语义提示学习解决弱监督语义分割中的问题，通过学习有效提示来增强分割区域与目标对象类别之间的语义对齐。

    

    弱监督语义分割（WSSS）旨在利用仅具有图像级监督的图像数据来训练分割模型。由于无法获得精确的像素级标注，现有方法通常侧重于通过优化CAM样式的热图来生成用于训练分割模型的伪标记。然而，生成的热图可能仅捕获对象类别的具有区分性的图像区域或相关的共同出现的背景。为解决这些问题，我们提出了一种用于WSSS的语义提示学习（SemPLeS）框架，该框架学习有效地提示CLIP潜空间以增强分割区域与目标对象类别之间的语义对准。具体而言，我们提出了对比提示学习和提示引导的语义细化，以学习适当描述和抑制与每个目标对象类别相关的共同出现的背景的提示。

    arXiv:2401.11791v2 Announce Type: replace-cross  Abstract: Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each target object category. In thi
    
[^271]: RepairLLaMA：高效表示和微调适配器用于程序修复

    RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair

    [https://arxiv.org/abs/2312.15698](https://arxiv.org/abs/2312.15698)

    高效表示和微调适配器相结合的新型程序修复方法RepairLLaMA可为语言模型修复错误产生高效的适配器。

    

    自动程序修复（APR）随着大型语言模型（LLMs）的出现已有了显著发展。对于程序修复进行LLMs的微调是最近研究的一个新领域，有许多未被探索的维度。现有工作大多使用简单的代码表示对LLMs进行微调，并在能够微调更大型LLMs的能力方面存在根本性局限。为解决这个问题，我们提出了RepairLLaMA，一个结合了1）用于APR的代码表示和2）最先进的参数高效的LLM微调技术LoRA的新型程序修复方法。这使得RepairLLaMA产生了一个高效的“程序修复适配器”，用于使用语言模型修复错误。我们的实验证明了这两个概念的有效性。首先，使用具有程序修复特定代码表示的微调适配器使模型能够使用有意义的修复信号。其次，参数高效的微调有助于微调...

    arXiv:2312.15698v2 Announce Type: replace-cross  Abstract: Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-the-art parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameter-efficient fine-tuning helps fine-tun
    
[^272]: 机器学习中的鲁棒性、效率或隐私：只能选两样

    Robustness, Efficiency, or Privacy: Pick Two in Machine Learning

    [https://arxiv.org/abs/2312.14712](https://arxiv.org/abs/2312.14712)

    该论文研究了在分布式机器学习架构中实现隐私和鲁棒性的成本，指出整合这两个目标会牺牲计算效率。

    

    机器学习（ML）应用的成功依赖于庞大的数据集和分布式架构，随着它们的增长，这些架构带来了重大挑战。在真实世界的场景中，数据通常包含敏感信息，数据污染和硬件故障等问题很常见。确保隐私和鲁棒性对于ML在公共生活中的广泛应用至关重要。本文从理论和实证角度研究了在分布式ML架构中实现这些目标所带来的成本。我们概述了分布式ML中隐私和鲁棒性的含义，并阐明了如何单独高效实现它们。然而，我们认为整合这两个目标会在计算效率上有显著的折衷。简而言之，传统的噪声注入通过隐藏毒害输入来损害准确性，而加密方法与防毒防御相冲突，因为它们是非线性的。

    arXiv:2312.14712v2 Announce Type: replace  Abstract: The success of machine learning (ML) applications relies on vast datasets and distributed architectures which, as they grow, present major challenges. In real-world scenarios, where data often contains sensitive information, issues like data poisoning and hardware failures are common. Ensuring privacy and robustness is vital for the broad adoption of ML in public life. This paper examines the costs associated with achieving these objectives in distributed ML architectures, from both theoretical and empirical perspectives. We overview the meanings of privacy and robustness in distributed ML, and clarify how they can be achieved efficiently in isolation. However, we contend that the integration of these two objectives entails a notable compromise in computational efficiency. In short, traditional noise injection hurts accuracy by concealing poisoned inputs, while cryptographic methods clash with poisoning defenses due to their non-line
    
[^273]: 用强化学习优化热预警的发布

    Optimizing Heat Alert Issuance with Reinforcement Learning

    [https://arxiv.org/abs/2312.14196](https://arxiv.org/abs/2312.14196)

    本研究利用强化学习优化热预警系统，通过引入新颖强化学习环境和综合数据集，解决了气候和健康环境中的低信号效应和空间异质性。

    

    社会适应气候变化的关键战略之一是利用预警系统减少极端高温事件的不利健康影响，以促使预防性行动。本文研究了强化学习（RL）作为优化此类系统效果的工具。我们的贡献有三个方面。首先，我们引入了一个新颖的强化学习环境，评估热预警政策的有效性，以减少与高温有关的住院人数。奖励模型基于历史天气、医疗保险健康记录以及社会经济/地理特征的全面数据集进行训练。我们使用变分贝叶斯技术解决了在气候和健康环境中常见的低信号效应和空间异质性。转换模型结合了真实的历史天气模式，并通过基于气候区域相似性的数据增强机制进行增强。

    arXiv:2312.14196v2 Announce Type: replace  Abstract: A key strategy in societal adaptation to climate change is the use of alert systems to reduce the adverse health impacts of extreme heat events by prompting preventative action. In this work, we investigate reinforcement learning (RL) as a tool to optimize the effectiveness of such systems. Our contributions are threefold. First, we introduce a novel RL environment enabling the evaluation of the effectiveness of heat alert policies to reduce heat-related hospitalizations. The rewards model is trained from a comprehensive dataset of historical weather, Medicare health records, and socioeconomic/geographic features. We use variational Bayesian techniques to address low-signal effects and spatial heterogeneity, which are commonly encountered in climate & health settings. The transition model incorporates real historical weather patterns enriched by a data augmentation mechanism based on climate region similarity. Second, we use this env
    
[^274]: 用于不可靠D2D网络上异构分散式联邦学习的拓扑学习

    Topology Learning for Heterogeneous Decentralized Federated Learning over Unreliable D2D Networks

    [https://arxiv.org/abs/2312.13611](https://arxiv.org/abs/2312.13611)

    通过拓扑学习方法和定义不可靠链接感知邻域差异的新领域数量，这篇论文提出了一种解决分散式联邦学习中数据分布异构性和D2D网络中通信中断导致的挑战的方法。

    

    随着智能移动设备在无线设备间(D2D)网络中的普及，分散式联邦学习(DFL)备受关注。与集中式联邦学习(CFL)相比，DFL减轻了由于通信瓶颈导致中央服务器故障的风险。然而，DFL面临一些挑战，如不同环境中数据分布的严重异构性，以及D2D网络中采用用户数据报协议(UDP)导致的传输中断和数据包错误。这些挑战通常会降低训练DFL模型的收敛性。为了解决这些挑战，我们对DFL进行了深入的理论收敛性分析，并提出了一个收敛界。通过在这一收敛界中定义一个名为不可靠链接感知邻域差异的新领域数量，我们制定了一个可行的优化目标，并开发了一种新的拓扑学习方法。

    arXiv:2312.13611v2 Announce Type: replace  Abstract: With the proliferation of intelligent mobile devices in wireless device-to-device (D2D) networks, decentralized federated learning (DFL) has attracted significant interest. Compared to centralized federated learning (CFL), DFL mitigates the risk of central server failures due to communication bottlenecks. However, DFL faces several challenges, such as the severe heterogeneity of data distributions in diverse environments, and the transmission outages and package errors caused by the adoption of the User Datagram Protocol (UDP) in D2D networks. These challenges often degrade the convergence of training DFL models. To address these challenges, we conduct a thorough theoretical convergence analysis for DFL and derive a convergence bound. By defining a novel quantity named unreliable links-aware neighborhood discrepancy in this convergence bound, we formulate a tractable optimization objective, and develop a novel Topology Learning metho
    
[^275]: 具有偏倧的非响应问题的主动学习

    Active learning with biased non-response to label requests

    [https://arxiv.org/abs/2312.08150](https://arxiv.org/abs/2312.08150)

    偏倧的非响应对主动学习模型性能有害，我们提出了一种成本-based 修正策略来减轻其影响，并通过实验证明其在许多情况下有效。

    

    主动学习可以通过识别获取最具信息量的新标签来提高训练预测模型的效率。然而，在标签请求的非响应情况下，会影响主动学习在实际环境中的有效性。我们通过考虑数据中存在的非响应类型来概念化这种退化，证明偏倧的非响应对模型性能特别有害。我们认为，在标签过程天然依赖用户交互的环境中，偏倧的非响应很可能会出现。为了减轻偏倧的非响应影响，我们提出了一种基于成本的修正采样策略——预期效用的上界置信度（UCB-EU）, 这种方法可以合理地应用于任何主动学习算法。通过实验证明，我们的方法成功地减少了在许多场景中标签非响应造成的伤害。

    arXiv:2312.08150v2 Announce Type: replace  Abstract: Active learning can improve the efficiency of training prediction models by identifying the most informative new labels to acquire. However, non-response to label requests can impact active learning's effectiveness in real-world contexts. We conceptualise this degradation by considering the type of non-response present in the data, demonstrating that biased non-response is particularly detrimental to model performance. We argue that biased non-response is likely in contexts where the labelling process, by nature, relies on user interactions. To mitigate the impact of biased non-response, we propose a cost-based correction to the sampling strategy--the Upper Confidence Bound of the Expected Utility (UCB-EU)--that can, plausibly, be applied to any active learning algorithm. Through experiments, we demonstrate that our method successfully reduces the harm from labelling non-response in many settings. However, we also characterise settin
    
[^276]: 从异构数据中学习结构因果模型中的未知干预目标

    Learning Unknown Intervention Targets in Structural Causal Models from Heterogeneous Data

    [https://arxiv.org/abs/2312.06091](https://arxiv.org/abs/2312.06091)

    在结构因果模型中，通过两阶段方法学习未知干预目标的外生噪声，并将其与相应的内生变量匹配，有效地识别干预目标。

    

    我们研究了在结构因果模型中识别未知干预目标的问题，其中我们可以访问从多个环境中收集的异构数据。未知干预目标是一组内生变量，其相应的外生噪声在不同环境中发生变化。我们提出了一个两阶段方法，第一阶段中恢复了跨不同环境发生变化的未知干预目标对应的外生噪声。在第二阶段，恢复的噪声与相应的内生变量进行匹配。对于恢复阶段，我们提供了学习这些外生噪声的充分条件，可达到某种分量方向可逆转换。对于匹配阶段，在因果充分性假设下，我们证明了所提方法可以唯一地识别干预目标。

    arXiv:2312.06091v2 Announce Type: replace-cross  Abstract: We study the problem of identifying the unknown intervention targets in structural causal models where we have access to heterogeneous data collected from multiple environments. The unknown intervention targets are the set of endogenous variables whose corresponding exogenous noises change across the environments. We propose a two-phase approach which in the first phase recovers the exogenous noises corresponding to unknown intervention targets whose distributions have changed across environments. In the second phase, the recovered noises are matched with the corresponding endogenous variables. For the recovery phase, we provide sufficient conditions for learning these exogenous noises up to some component-wise invertible transformation. For the matching phase, under the causal sufficiency assumption, we show that the proposed method uniquely identifies the intervention targets. In the presence of latent confounders, the interv
    
[^277]: 打破同质性和异质性在半监督节点分类中的纠缠

    Breaking the Entanglement of Homophily and Heterophily in Semi-supervised Node Classification

    [https://arxiv.org/abs/2312.04111](https://arxiv.org/abs/2312.04111)

    开发了一个可以在同质性和异质性下保证性能的强大GNN模型

    

    最近，图神经网络（GNNs）在利用图数据库知识进行半监督节点分类方面表现出色。然而，大多数现有的GNNs遵循同质性假设，即连接的节点更有可能展现出相似的特征分布和相同的标签，这种假设在越来越多的实际应用中被证明是脆弱的。作为补充，异质性反映了相连节点的不相似性，在图学习中引起了重要关注。因此，数据工程师旨在开发一个强大的GNN模型，可以在同质性和异质性下保证性能。尽管已进行了大量尝试，但由于无向图的约束，大多数现有的GNNs都难以实现最佳节点表示。忽略有向边会导致次优的图表示，从而阻碍了GNNs的能力。

    arXiv:2312.04111v2 Announce Type: replace-cross  Abstract: Recently, graph neural networks (GNNs) have shown prominent performance in semi-supervised node classification by leveraging knowledge from the graph database. However, most existing GNNs follow the homophily assumption, where connected nodes are more likely to exhibit similar feature distributions and the same labels, and such an assumption has proven to be vulnerable in a growing number of practical applications. As a supplement, heterophily reflects dissimilarity in connected nodes, which has gained significant attention in graph learning. To this end, data engineers aim to develop a powerful GNN model that can ensure performance under both homophily and heterophily. Despite numerous attempts, most existing GNNs struggle to achieve optimal node representations due to the constraints of undirected graphs. The neglect of directed edges results in sub-optimal graph representations, thereby hindering the capacity of GNNs. To add
    
[^278]: 评估和基准化离策略评估的风险-收益权衡

    Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation

    [https://arxiv.org/abs/2311.18207](https://arxiv.org/abs/2311.18207)

    该研究提出了一种新的指标 SharpeRatio@k，用于评估离策略评估的风险-收益权衡，能够有效区分不同风险估计器并准确识别最高效的估计器。

    

    离策略评估（OPE）旨在仅使用线下记录的数据评估反事实政策的有效性，并通常用于识别在在线A/B测试部署中的前k个有前途的政策。当前的OPE估计器评估指标主要关注OPE或下游政策选择的“准确性”，而忽略了随后在线政策部署中的风险-回报权衡。为解决这一问题，我们从金融中的投资组合评估中汲取灵感，开发了一种名为SharpeRatio@k的新指标，用于衡量由OPE估计器形成的政策投资组合在不同的在线评估预算（k）下的风险-回报权衡。我们在两个示例场景中验证了我们的指标，展示了其能够有效区分低风险和高风险估计器，并准确识别效率最高的估计器。

    arXiv:2311.18207v3 Announce Type: replace-cross  Abstract: Off-Policy Evaluation (OPE) aims to assess the effectiveness of counterfactual policies using only offline logged data and is often used to identify the top-k promising policies for deployment in online A/B tests. Existing evaluation metrics for OPE estimators primarily focus on the "accuracy" of OPE or that of downstream policy selection, neglecting risk-return tradeoff in the subsequent online policy deployment. To address this issue, we draw inspiration from portfolio evaluation in finance and develop a new metric, called SharpeRatio@k, which measures the risk-return tradeoff of policy portfolios formed by an OPE estimator under varying online evaluation budgets (k). We validate our metric in two example scenarios, demonstrating its ability to effectively distinguish between low-risk and high-risk estimators and to accurately identify the most efficient one. Efficiency of an estimator is characterized by its capability to fo
    
[^279]: SCOPE-RL: 用于离线强化学习和离策略评估的Python库

    SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation

    [https://arxiv.org/abs/2311.18206](https://arxiv.org/abs/2311.18206)

    SCOPE-RL是一个Python库，兼顾离线强化学习和离策略评估，通过整合策略学习和评估实现了更灵活、完整的实现方式，并通过OPE模块提供了多种OPE估计器和稳健的OPE协议，使得OPE更深入和可靠。

    

    本文介绍了SCOPE-RL，这是一个全面的开源Python软件，专为离线强化学习（offline RL）、离策略评估（OPE）和选择（OPS）而设计。与大多数现有的库不同，这些库仅关注策略学习或评估中的一个，SCOPE-RL无缝整合了这两个关键方面，促进了离线RL和OPE过程的灵活和完整实现。SCOPE-RL特别侧重于其OPE模块，提供一系列OPE估计器和稳健的OPE协议。这种方法使得与其他软件包相比，SCOPE-RL能够更深入和可靠地评估OPE。例如，SCOPE-RL通过估计策略下的整个奖励分布而不仅仅是其点值预期值来增强OPE。此外，SCOPE-RL通过在OPE结果中提供风险-回报权衡，超越了现有库中仅仅是准确性评估的更全面的OPE评估。

    arXiv:2311.18206v3 Announce Type: replace-cross  Abstract: This paper introduces SCOPE-RL, a comprehensive open-source Python software designed for offline reinforcement learning (offline RL), off-policy evaluation (OPE), and selection (OPS). Unlike most existing libraries that focus solely on either policy learning or evaluation, SCOPE-RL seamlessly integrates these two key aspects, facilitating flexible and complete implementations of both offline RL and OPE processes. SCOPE-RL put particular emphasis on its OPE modules, offering a range of OPE estimators and robust evaluation-of-OPE protocols. This approach enables more in-depth and reliable OPE compared to other packages. For instance, SCOPE-RL enhances OPE by estimating the entire reward distribution under a policy rather than its mere point-wise expected value. Additionally, SCOPE-RL provides a more thorough evaluation-of-OPE by presenting the risk-return tradeoff in OPE results, extending beyond mere accuracy evaluations in exis
    
[^280]: 一种用于分类肝癌数字组织病理切片的传导式小样本学习方法

    A transductive few-shot learning approach for classification of digital histopathological slides from liver cancer

    [https://arxiv.org/abs/2311.17740](https://arxiv.org/abs/2311.17740)

    提出了一种用于肝癌数字组织病理切片分类的传导式小样本学习方法，通过滑动窗口技术和优化策略，在解决标记数据有限可用性的同时取得了一致准确分类的实际优势。

    

    本文提出了一种新的方法，使用少样本学习对2D组织病理切片进行分类。该方法旨在解决组织病理学中的一个重要挑战，即标记数据的有限可用性。通过将滑动窗口技术应用于组织病理学切片，我们展示了传导式学习（即在切片上做出联合预测）在实现一致准确分类方面的实际优势。我们的方法涉及一种基于优化的策略，积极惩罚每个窗口内预测大量不同类别。我们对组织病理学数据进行了实验，以对肝癌数字幻灯片中的组织类别进行分类，特别是肝细胞癌。初步结果显示了我们方法的有效性及其增强自动癌症诊断和治疗过程的潜力，同时减少了所需的时间和精力。

    arXiv:2311.17740v2 Announce Type: replace-cross  Abstract: This paper presents a new approach for classifying 2D histopathology patches using few-shot learning. The method is designed to tackle a significant challenge in histopathology, which is the limited availability of labeled data. By applying a sliding window technique to histopathology slides, we illustrate the practical benefits of transductive learning (i.e., making joint predictions on patches) to achieve consistent and accurate classification. Our approach involves an optimization-based strategy that actively penalizes the prediction of a large number of distinct classes within each window. We conducted experiments on histopathological data to classify tissue classes in digital slides of liver cancer, specifically hepatocellular carcinoma. The initial results show the effectiveness of our method and its potential to enhance the process of automated cancer diagnosis and treatment, all while reducing the time and effort requir
    
[^281]: 打开黑匣子：利用建筑物理学洞察实现固有可解释的能源数据插补模型

    Opening the Black Box: Towards inherently interpretable energy data imputation models using building physics insight

    [https://arxiv.org/abs/2311.16632](https://arxiv.org/abs/2311.16632)

    提出了利用建筑物理学洞察实现固有可解释的能源数据插补模型PI-DAE，在损失函数中引入物理启发软约束，从而实现更可解释的预测。

    

    缺失数据经常被建筑能源建模领域的实践者和研究人员观察到。在这方面，通常需要先进的数据驱动解决方案，如深度学习方法，以反映这些异常的非线性行为。作为与深度学习相关的一个持续研究问题，可以通过在网络中引入先验知识来探索模型在有限数据设置下的适用性。这种策略也可以导致更可解释的预测，从而促进方法的现场应用。因此，本文旨在提出物理启发型去噪自动编码器(PI-DAE)用于商业建筑的缺失数据插补。具体而言，所提出的方法将物理启发软约束强加给去噪自动编码器(DAE)的损失函数。为了量化物理组件的好处，进行了一项消融实验。

    arXiv:2311.16632v2 Announce Type: replace-cross  Abstract: Missing data are frequently observed by practitioners and researchers in the building energy modeling community. In this regard, advanced data-driven solutions, such as Deep Learning methods, are typically required to reflect the non-linear behavior of these anomalies. As an ongoing research question related to Deep Learning, a model's applicability to limited data settings can be explored by introducing prior knowledge in the network. This same strategy can also lead to more interpretable predictions, hence facilitating the field application of the approach. For that purpose, the aim of this paper is to propose the use of Physics-informed Denoising Autoencoders (PI-DAE) for missing data imputation in commercial buildings. In particular, the presented method enforces physics-inspired soft constraints to the loss function of a Denoising Autoencoder (DAE). In order to quantify the benefits of the physical component, an ablation s
    
[^282]: TFMQ-DM：面向扩散模型的时间特征维持量化

    TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models

    [https://arxiv.org/abs/2311.16503](https://arxiv.org/abs/2311.16503)

    TFMQ-DM提出了一种称为Temporal Feature Maintenance Quantization (TFMQ)的方法，针对扩散模型中的时间特征进行量化，解决了传统模型中存在的优化问题，提高了压缩效率。

    

    arXiv:2311.16503v2 通告类型：替换-交叉 摘要：扩散模型是一种广泛应用于图像生成的框架，但由于其较长的推理时间和大量的内存需求，在广泛适用性方面遇到了重大挑战。高效的后训练量化（PTQ）对于传统模型解决这些问题至关重要。与传统模型不同，扩散模型严重依赖时间步长 $t$ 来实现令人满意的多轮去噪。通常，从有限集合 $\{1, \ldots, T\}$ 中的 $t$会被几个模块编码为一个时间特征，这完全不考虑采样数据。然而，现有的PTQ方法并不分别优化这些模块。它们采用不恰当的重构目标和复杂的校准方法，导致时间特征和去噪轨迹严重受到干扰，同时压缩效率较低。为了解决这些问题，我们提出了一种称为Temporal Feature Maintenance Quantization (TFMQ)的方法

    arXiv:2311.16503v2 Announce Type: replace-cross  Abstract: The Diffusion model, a prevalent framework for image generation, encounters significant challenges in terms of broad applicability due to its extended inference times and substantial memory requirements. Efficient Post-training Quantization (PTQ) is pivotal for addressing these issues in traditional models. Different from traditional models, diffusion models heavily depend on the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$ from the finite set $\{1, \ldots, T\}$ is encoded to a temporal feature by a few modules totally irrespective of the sampling data. However, existing PTQ methods do not optimize these modules separately. They adopt inappropriate reconstruction targets and complex calibration methods, resulting in a severe disturbance of the temporal feature and denoising trajectory, as well as a low compression efficiency. To solve these, we propose a Temporal Feature Maintenance Quantization (TF
    
[^283]: FRAC-Q-Learning: 一种具有避免厌烦过程的社交机器人强化学习方法

    FRAC-Q-Learning: A Reinforcement Learning with Boredom Avoidance Processes for Social Robots

    [https://arxiv.org/abs/2311.15327](https://arxiv.org/abs/2311.15327)

    FRAC-Q-Learning是一种专为社交机器人设计，能避免用户厌烦的强化学习方法，比传统算法在兴趣和厌烦程度上表现更好，有助于开发不会让用户感到无聊的社交机器人。

    

    强化学习算法经常被应用于社交机器人。然而，大多数强化学习算法并未针对社交机器人进行优化，因此可能会让用户感到无聊。我们提出了一种专为社交机器人设计的新强化学习方法，FRAC-Q-Learning，可以避免用户感到无聊。该算法除了随机化和分类过程外，还包括一个遗忘过程。本研究通过与传统Q-Learning的比较评估了FRAC-Q-Learning的兴趣和厌烦程度分数。FRAC-Q-Learning显示出明显更高的兴趣分数趋势，并且相较于传统Q-Learning更难让用户感到无聊。因此，FRAC-Q-Learning有助于开发不会让用户感到无聊的社交机器人。该算法还可以在基于Web的通信和教育中找到应用。

    arXiv:2311.15327v3 Announce Type: replace-cross  Abstract: The reinforcement learning algorithms have often been applied to social robots. However, most reinforcement learning algorithms were not optimized for the use of social robots, and consequently they may bore users. We proposed a new reinforcement learning method specialized for the social robot, the FRAC-Q-learning, that can avoid user boredom. The proposed algorithm consists of a forgetting process in addition to randomizing and categorizing processes. This study evaluated interest and boredom hardness scores of the FRAC-Q-learning by a comparison with the traditional Q-learning. The FRAC-Q-learning showed significantly higher trend of interest score, and indicated significantly harder to bore users compared to the traditional Q-learning. Therefore, the FRAC-Q-learning can contribute to develop a social robot that will not bore users. The proposed algorithm can also find applications in Web-based communication and educational 
    
[^284]: 在神经蒙日映射中的不平衡性改进未配对领域转换

    Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation

    [https://arxiv.org/abs/2311.15100](https://arxiv.org/abs/2311.15100)

    在神经蒙日映射中引入不平衡性可改进未配对领域转换任务的效率和鲁棒性

    

    在最优输送（OT）中，蒙日映射被称为以最经济的方式将源分布传输到目标分布的映射。最近，已经开发并应用了多种用于Monge映射的神经估计器，并在各种未配对的领域转换任务中进行了应用，例如在单细胞生物学和计算机视觉中。然而，经典的OT框架强制保持质量守恒，这使其容易受到离群值的影响，并限制了它在现实世界场景中的适用性。后者在OT领域转换任务中可能特别有害，因为其中显式考虑了样本在分布中的相对位置。尽管在离散设置中，不平衡OT解决了这一挑战，但其集成到神经Monge映射估计器中受到了有限的关注。我们提出了一种理论上基础的方法，将不平衡性纳入到任何Monge映射估计器中。我们改进了现有的估计器以模

    arXiv:2311.15100v2 Announce Type: replace-cross  Abstract: In optimal transport (OT), a Monge map is known as a mapping that transports a source distribution to a target distribution in the most cost-efficient way. Recently, multiple neural estimators for Monge maps have been developed and applied in diverse unpaired domain translation tasks, e.g. in single-cell biology and computer vision. However, the classic OT framework enforces mass conservation, which makes it prone to outliers and limits its applicability in real-world scenarios. The latter can be particularly harmful in OT domain translation tasks, where the relative position of a sample within a distribution is explicitly taken into account. While unbalanced OT tackles this challenge in the discrete setting, its integration into neural Monge map estimators has received limited attention. We propose a theoretically grounded method to incorporate unbalancedness into any Monge map estimator. We improve existing estimators to mode
    
[^285]: ECNR: 高效压缩神经表示的时变体积数据

    ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets

    [https://arxiv.org/abs/2311.12831](https://arxiv.org/abs/2311.12831)

    ECNR是一种针对时变数据的高效压缩神经表示方法，通过使用多尺度结构和多个小型MLP，以及深度压缩策略，可以显著加速训练和推理过程。

    

    由于其概念简单性和广泛适用性，压缩神经表示已经成为管理大规模体积数据集的传统压缩方法的有希望的替代方案。当前的神经压缩实践利用单个大型多层感知器（MLP）来对全局体积进行编码，导致训练和推理速度慢。本文提出了一种用于时变数据压缩的高效压缩神经表示（ECNR）解决方案，利用拉普拉斯金字塔进行自适应信号拟合。遵循多尺度的结构，我们在每个尺度上利用多个小型MLP来拟合本地内容或残差块。通过将相似的块分配给相同大小的MLP，通过大小统一化，我们实现了MLP之间的平衡并行化，从而显著加速训练和推理。与多尺度结构协同工作的是，我们量身定制了一种深度压缩策略来压缩结果模型。我们展示了ECNR的效果，有多个...

    Due to its conceptual simplicity and generality, compressive neural representation has emerged as a promising alternative to traditional compression methods for managing massive volumetric datasets. The current practice of neural compression utilizes a single large multilayer perceptron (MLP) to encode the global volume, incurring slow training and inference. This paper presents an efficient compressive neural representation (ECNR) solution for time-varying data compression, utilizing the Laplacian pyramid for adaptive signal fitting. Following a multiscale structure, we leverage multiple small MLPs at each scale for fitting local content or residual blocks. By assigning similar blocks to the same MLP via size uniformization, we enable balanced parallelization among MLPs to significantly speed up training and inference. Working in concert with the multiscale structure, we tailor a deep compression strategy to compact the resulting model. We show the effectiveness of ECNR with multiple 
    
[^286]: BEND: 在具有生物意义的任务上对DNA语言模型进行基准测试

    BEND: Benchmarking DNA Language Models on biologically meaningful tasks

    [https://arxiv.org/abs/2311.12570](https://arxiv.org/abs/2311.12570)

    BEND是一个针对DNA语言模型的基准测试，包含一系列在人类基因组上定义的现实且具有生物意义的下游任务。

    

    基因组序列包含了指导细胞过程的蓝图。尽管过去几十年来基因组的可用性大大增加，但对DNA序列中编码的各种功能性、非编码和调节元素进行实验注释仍既昂贵又具挑战性。这引发了对基因组DNA进行无监督语言建模的兴趣，这种范式在蛋白序列数据中取得了巨大成功。尽管提出了各种DNA语言模型，但个别作品之间的评估任务往往不同，并且可能无法完全复制基因组注释的基本挑战，包括数据的长度、规模和稀疏性。在这项研究中，我们引入了BEND，一个针对DNA语言模型的基准测试，其中包含了一系列定义在人类基因组上的现实和有生物意义的下游任务。我们发现当前DNA语言模型的嵌入可以接近pe

    arXiv:2311.12570v3 Announce Type: replace-cross  Abstract: The genome sequence contains the blueprint for governing cellular processes. While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data. Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. We find that embeddings from current DNA LMs can approach pe
    
[^287]: EPIM: 基于Epitome的高效处理内存加速器

    EPIM: Efficient Processing-In-Memory Accelerators based on Epitome

    [https://arxiv.org/abs/2311.07620](https://arxiv.org/abs/2311.07620)

    本论文引入了Epitome，一种轻量级神经算子，为PIM加速器设计了内存高效的CNN算子（EPIM）。

    

    处理内存（PIM）加速器上大规模神经网络的利用面临挑战，原因是片上内存容量受限。为了解决这个问题，目前的研究探讨了模型压缩算法，以减小卷积神经网络（CNNs）的大小。大多数这些算法要么旨在用具有减小参数大小的神经算子（例如，量化）表示神经算子，要么寻找神经算子的最佳组合（例如，神经架构搜索）。设计与PIM加速器规格相一致的神经算子是一个需要进一步研究的领域。在本文中，我们介绍Epitome，一种轻量级神经算子，提供类似卷积功能，以为PIM加速器（EPIM）设计内存高效的CNN算子。在软件方面，我们评估了Epitome在PIM加速器上的延迟和能量，并引入了一种PIM感知的逐层设计方法来增强它们。

    arXiv:2311.07620v2 Announce Type: replace-cross  Abstract: The utilization of large-scale neural networks on Processing-In-Memory (PIM) accelerators encounters challenges due to constrained on-chip memory capacity. To tackle this issue, current works explore model compression algorithms to reduce the size of Convolutional Neural Networks (CNNs). Most of these algorithms either aim to represent neural operators with reduced-size parameters (e.g., quantization) or search for the best combinations of neural operators (e.g., neural architecture search). Designing neural operators to align with PIM accelerators' specifications is an area that warrants further study. In this paper, we introduce the Epitome, a lightweight neural operator offering convolution-like functionality, to craft memory-efficient CNN operators for PIM accelerators (EPIM). On the software side, we evaluate epitomes' latency and energy on PIM accelerators and introduce a PIM-aware layer-wise design method to enhance thei
    
[^288]: 用于电力流分析的量子神经网络

    Quantum Neural Networks for Power Flow Analysis

    [https://arxiv.org/abs/2311.06293](https://arxiv.org/abs/2311.06293)

    混合量子-经典神经网络在电力流分析中表现优越，有望改进深度学习在量子计算时代的应用。

    

    本文探讨了量子和混合量子-经典神经网络在电力流分析中的潜在应用。实验使用基于4节点和33节点测试系统的两个数据集进行。还进行了量子、混合量子-经典和经典神经网络的系统性性能比较。比较基于(i) 泛化能力，(ii) 鲁棒性，(iii) 需要的训练数据集大小，(iv) 训练误差，和(v) 训练过程稳定性。结果显示，开发的混合量子-经典神经网络表现优于量子和经典神经网络，因此可以改进基于深度学习的电力流分析在噪声中等规模量子（NISQ）和容错量子（FTQ）时代。

    arXiv:2311.06293v2 Announce Type: replace-cross  Abstract: This paper explores the potential application of quantum and hybrid quantum-classical neural networks in power flow analysis. Experiments are conducted using two datasets based on 4-bus and 33-bus test systems. A systematic performance comparison is also conducted among quantum, hybrid quantum-classical, and classical neural networks. The comparison is based on (i) generalization ability, (ii) robustness, (iii) training dataset size needed, (iv) training error, and (v) training process stability. The results show that the developed hybrid quantum-classical neural network outperforms both quantum and classical neural networks, and hence can improve deep learning-based power flow analysis in the noisy-intermediate-scale quantum (NISQ) and fault-tolerant quantum (FTQ) era.
    
[^289]: 公平受监督学习中具有敏感属性的简单随机采样器

    Fair Supervised Learning with A Simple Random Sampler of Sensitive Attributes

    [https://arxiv.org/abs/2311.05866](https://arxiv.org/abs/2311.05866)

    提出了一种公平受监督学习的方法，利用简单随机采样器处理敏感属性，可以更广泛地适用于实践中，并构建了一个计算效率高的群体级别处理公平感知的训练框架。

    

    随着数据驱动的决策过程在工业应用中变得主导，各个领域对具有公平意识的机器学习引起了极大关注。本研究提出了利用敏感属性的简单随机采样器学习公平惩罚的神经网络方法，用于非歧视性受监督学习。与许多现有作品基本依赖于敏感属性和响应变量的离散性不同，所提出的惩罚能够处理多种格式的敏感属性，因此在实践中比许多现有算法更具广泛适用性。该惩罚使我们能够构建一个计算效率高的群体级别处理公平感知的训练框架。实证证据表明，我们的框架在流行的基准数据集上比竞争方法具有更好的效用和公平度量。我们还在理论上对估计误差和损失进行了表征。

    arXiv:2311.05866v2 Announce Type: replace-cross  Abstract: As the data-driven decision process becomes dominating for industrial applications, fairness-aware machine learning arouses great attention in various areas. This work proposes fairness penalties learned by neural networks with a simple random sampler of sensitive attributes for non-discriminatory supervised learning. In contrast to many existing works that critically rely on the discreteness of sensitive attributes and response variables, the proposed penalty is able to handle versatile formats of the sensitive attributes, so it is more extensively applicable in practice than many existing algorithms. This penalty enables us to build a computationally efficient group-level in-processing fairness-aware training framework. Empirical evidence shows that our framework enjoys better utility and fairness measures on popular benchmark data sets than competing methods. We also theoretically characterize estimation errors and loss of u
    
[^290]: LRM：单图像到3D的大型重建模型

    LRM: Large Reconstruction Model for Single Image to 3D

    [https://arxiv.org/abs/2311.04400](https://arxiv.org/abs/2311.04400)

    首次提出了LRM，采用大规模训练数据和高容量模型，可在短时间内从单个图像中预测高质量3D重建结果

    

    我们提出了第一个大型重建模型（LRM），可以在短短5秒内从单个输入图像中预测对象的3D模型。与许多先前训练在小规模数据集（如ShapeNet）上的方法相比，LRM采用了一个高度可扩展的基于transformer的架构，具有5亿的可学习参数，可以直接从输入图像中预测神经辐射场（NeRF）。我们以端到端的方式在包含约100万个对象的大规模多视角数据上训练我们的模型，包括来自Objaverse的合成渲染和来自MVImgNet的真实捕获。这种高容量模型和大规模训练数据的结合使得我们的模型具有很强的泛化能力，可以从各种测试输入中产生高质量的3D重建结果，包括真实场景捕获和生成模型创建的图像。视频演示和可交互的3D模型

    arXiv:2311.04400v2 Announce Type: replace-cross  Abstract: We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D mes
    
[^291]: 权重共享正则化

    Weight-Sharing Regularization

    [https://arxiv.org/abs/2311.03096](https://arxiv.org/abs/2311.03096)

    提出了一种权重共享正则化方法，通过引入对神经网络权重的惩罚，设计并实现了一个新型并行算法，使网络能够学习卷积样式的滤波器

    

    在深度学习中，权重共享是无处不在的。受此启发，我们提出了对神经网络的权重$w \in \mathbb{R}^d$进行“权重共享正则化”惩罚，定义为$\mathcal{R}(w) = \frac{1}{d - 1}\sum_{i > j}^d |w_i - w_j|$。我们研究了$\mathcal{R}$的近端映射，并通过一个物理粒子相互作用的系统对其进行了直观解释。我们还并行化了现有的$\operatorname{prox}_\mathcal{R}$算法（在GPU上运行），并发现其中一种在实践中快速，但对于最坏情况输入较慢（$O(d)$）。利用物理解释，我们设计了一种新颖的并行算法，当有足够的处理器可用时，其运行时间为$O(\log^3 d)$，从而保证了快速训练。我们的实验显示，权重共享正则化使全连接网络能够学习卷积样式的滤波器，即使像素已被打乱，而卷积神经网

    arXiv:2311.03096v2 Announce Type: replace  Abstract: Weight-sharing is ubiquitous in deep learning. Motivated by this, we propose a "weight-sharing regularization" penalty on the weights $w \in \mathbb{R}^d$ of a neural network, defined as $\mathcal{R}(w) = \frac{1}{d - 1}\sum_{i > j}^d |w_i - w_j|$. We study the proximal mapping of $\mathcal{R}$ and provide an intuitive interpretation of it in terms of a physical system of interacting particles. We also parallelize existing algorithms for $\operatorname{prox}_\mathcal{R}$ (to run on GPU) and find that one of them is fast in practice but slow ($O(d)$) for worst-case inputs. Using the physical interpretation, we design a novel parallel algorithm which runs in $O(\log^3 d)$ when sufficient processors are available, thus guaranteeing fast training. Our experiments reveal that weight-sharing regularization enables fully connected networks to learn convolution-like filters even when pixels have been shuffled while convolutional neural netwo
    
[^292]: 通过随机对偶平均快速最小化期望对数损失

    Fast Minimization of Expected Logarithmic Loss via Stochastic Dual Averaging

    [https://arxiv.org/abs/2311.02557](https://arxiv.org/abs/2311.02557)

    提出了一种名为$B$-sample stochastic dual averaging with the logarithmic barrier的随机一阶算法，用于快速解决泊松逆问题，算法在$\smash{\tilde{O}}(d^2/\varepsilon^2)$时间内获得$\varepsilon$-最优解，与当前技术水平相匹配。

    

    考虑在概率单纯形或量子密度矩阵集合上最小化期望对数损失的问题。该问题包括解决泊松逆问题、计算量子态测量的最大似然估计以及用当前最紧密逼近比率近似正半定矩阵永久值等任务。尽管优化问题是凸的，但由于损失函数缺乏Lipschitz连续性和光滑性，标准的一阶方法的迭代复杂度保证并不直接适用。

    arXiv:2311.02557v2 Announce Type: replace-cross  Abstract: Consider the problem of minimizing an expected logarithmic loss over either the probability simplex or the set of quantum density matrices. This problem includes tasks such as solving the Poisson inverse problem, computing the maximum-likelihood estimate for quantum state tomography, and approximating positive semi-definite matrix permanents with the currently tightest approximation ratio. Although the optimization problem is convex, standard iteration complexity guarantees for first-order methods do not directly apply due to the absence of Lipschitz continuity and smoothness in the loss function.   In this work, we propose a stochastic first-order algorithm named $B$-sample stochastic dual averaging with the logarithmic barrier. For the Poisson inverse problem, our algorithm attains an $\varepsilon$-optimal solution in $\smash{\tilde{O}}(d^2/\varepsilon^2)$ time, matching the state of the art, where $d$ denotes the dimension. 
    
[^293]: 从后验采样到图像恢复中有意义的多样性

    From Posterior Sampling to Meaningful Diversity in Image Restoration

    [https://arxiv.org/abs/2310.16047](https://arxiv.org/abs/2310.16047)

    图像恢复问题通常具有无限种合理的解决方案，传统的后验采样生成的多样输出受到尾部效应限制，本文提出了一种更具意义的多样性策略。

    

    图像恢复问题通常是不适定的，因为每个退化图像都可以以无限有效的方式恢复。为了满足这一点，许多作品试图从给定退化输入的自然图像后验分布中随机采样，生成多样化的输出。然而，我们认为这种策略普遍具有有限的实用价值，因为后验分布的尾部较重。例如，考虑修复图像中缺失的天空区域。由于缺失区域很可能不包含物体，只有云，任何来自后验的样本集都将完全被天空的（实际上相同的）修复所主导。然而，可以认为向用户呈现一个清晰的天空修复以及几种替代解决方案（例如飞艇、鸟类和气球）将更好地概述可能的选择集。本文中，我们开始

    arXiv:2310.16047v2 Announce Type: replace-cross  Abstract: Image restoration problems are typically ill-posed in the sense that each degraded image can be restored in infinitely many valid ways. To accommodate this, many works generate a diverse set of outputs by attempting to randomly sample from the posterior distribution of natural images given the degraded input. Here we argue that this strategy is commonly of limited practical value because of the heavy tail of the posterior distribution. Consider for example inpainting a missing region of the sky in an image. Since there is a high probability that the missing region contains no object but clouds, any set of samples from the posterior would be entirely dominated by (practically identical) completions of sky. However, arguably, presenting users with only one clear sky completion, along with several alternative solutions such as airships, birds, and balloons, would better outline the set of possibilities. In this paper, we initiate 
    
[^294]: Bongard-OpenWorld: 在真实世界中进行自由形式视觉概念的少样本推理

    Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World

    [https://arxiv.org/abs/2310.10207](https://arxiv.org/abs/2310.10207)

    Bongard-OpenWorld基准旨在评估机器视觉中对真实世界中的自由形式视觉概念进行少样本推理，并且提出了开放世界自由形式概念和真实世界图像两项新挑战。

    

    我们介绍了Bongard-OpenWorld，这是一个用于评估机器视觉中真实世界少样本推理的新基准。 它源自经典的Bongard问题（BPs）：给定两组图像（正和负），模型需要通过诱导视觉概念来确定查询图像所属的图像集，这些概念仅由正集中的图像所描述。 我们的基准继承了原始BPs的少样本概念归纳，同时增加了两层新挑战：1）开放世界的自由形式概念，因为Bongard-OpenWorld中的视觉概念是从开放词汇表中独特组合的术语，范围从对象类别到抽象视觉属性和常识事实知识； 2）真实世界的图像，而不是许多类似物使用的合成图表。在我们的探索中，Bongard-OpenWorld已经对当前的少样本推理算法提出了重大挑战。我们还远

    arXiv:2310.10207v2 Announce Type: replace  Abstract: We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We furt
    
[^295]: 拜占庭鲁棒学习的通信压缩：新高效算法和改进收敛速率

    Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates

    [https://arxiv.org/abs/2310.09804](https://arxiv.org/abs/2310.09804)

    本文提出了一种新的带压缩的拜占庭鲁棒方法 Byz-DASHA-PAGE，证明了其在非凸和Polyak-Lojasiewicz平滑优化问题上具有更好的收敛速率，同时在异构情况下具有更小的邻域大小，以及在过参数化情况下能容忍更多的拜占庭工作者。

    

    拜占庭鲁棒性是某些分布式优化问题算法的基本特征，通常出现在协作/联邦学习中。这些问题通常规模巨大，因此通信压缩对于解决这些问题至关重要。这些因素推动了最近在拜占庭鲁棒学习与压缩领域的算法和理论发展。本文在两个主要方向上为这一研究领域做出贡献。首先，我们提出了一种新的带压缩的拜占庭鲁棒方法 - Byz-DASHA-PAGE，并证明该新方法在非凸和Polyak-Lojasiewicz平滑优化问题上具有更好的收敛速率，在异构情况下具有更小的邻域大小，并且在过参数化情况下容忍更多的拜占庭工作者，胜过具有SOTA理论收敛保证的先前方法（Byz-VR-MARINA）。其次，我们开发了…

    arXiv:2310.09804v2 Announce Type: replace-cross  Abstract: Byzantine robustness is an essential feature of algorithms for certain distributed optimization problems, typically encountered in collaborative/federated learning. These problems are usually huge-scale, implying that communication compression is also imperative for their resolution. These factors have spurred recent algorithmic and theoretical developments in the literature of Byzantine-robust learning with compression. In this paper, we contribute to this research area in two main directions. First, we propose a new Byzantine-robust method with compression - Byz-DASHA-PAGE - and prove that the new method has better convergence rate (for non-convex and Polyak-Lojasiewicz smooth optimization problems), smaller neighborhood size in the heterogeneous case, and tolerates more Byzantine workers under over-parametrization than the previous method with SOTA theoretical convergence guarantees (Byz-VR-MARINA). Secondly, we develop the 
    
[^296]: Prometheus: 在语言模型中引入细粒度评估能力

    Prometheus: Inducing Fine-grained Evaluation Capability in Language Models

    [https://arxiv.org/abs/2310.08491](https://arxiv.org/abs/2310.08491)

    Prometheus是一个开源的LLM，通过使用自定义评分标准和适当的参考材料，可以在与GPT-4相媲美的评估能力上进行评估。

    

    最近，使用强大的专有大型语言模型（例如GPT-4）作为长篇回答的评估者已经成为事实上的标准。然而，对于具有大规模评估任务和考虑自定义标准（例如儿童可读性）的从业者来说，使用专有LLMs作为评估者是不可靠的，这是由于其闭源性质、无控制的版本和高昂的成本。在这项工作中，我们提出了Prometheus，这是一个完全开源的LLM，只要携带适当的参考材料（参考答案、评分标准），就可以与GPT-4的评估能力相媲美。我们首先构建了反馈收集（Feedback Collection），这是一个由1K个细粒度评分标准、20K条指令和GPT-4生成的100K条响应和语言反馈组成的新数据集。使用这个反馈收集，我们训练了Prometheus，一个13B的评估者LLM，可以根据定制的评分标准评估任何给定的长篇文本。

    arXiv:2310.08491v2 Announce Type: replace  Abstract: Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric
    
[^297]: AutoVP: 一种自动化的视觉提示框架和基准

    AutoVP: An Automated Visual Prompting Framework and Benchmark

    [https://arxiv.org/abs/2310.08381](https://arxiv.org/abs/2310.08381)

    AutoVP提出了一个自动化的视觉提示框架，同时提供12个下游图像分类任务作为基准，实验结果显示AutoVP在准确性方面明显优于当前已知的最佳VP方法。

    

    视觉提示（VP）是一种新兴的参数高效微调方法，用于调整预训练视觉模型以解决各种下游图像分类任务。然而，迄今为止，对VP的设计空间缺乏系统性研究，并没有明确的用于评估其性能的基准。为了弥合这一差距，我们提出了AutoVP，一个端到端可扩展的框架，用于自动化VP设计选择，以及12个下游图像分类任务，可以作为一个全面的VP性能基准。我们的设计空间覆盖了1）提示的联合优化；2）预训练模型的选择，包括图像分类器和文本-图像编码器；以及3）模型输出映射策略，包括非参数化和可训练的标签映射。我们的广泛实验结果表明，AutoVP在准确性方面优于目前已知的最佳VP方法，精度提高高达6.7%。

    arXiv:2310.08381v2 Announce Type: replace-cross  Abstract: Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance. To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark. Our design space covers 1) the joint optimization of the prompts; 2) the selection of pre-trained models, including image classifiers and text-image encoders; and 3) model output mapping strategies, including nonparametric and trainable label mapping. Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy
    
[^298]: iTransformer: 反转Transformer在时间序列预测中是有效的

    iTransformer: Inverted Transformers Are Effective for Time Series Forecasting

    [https://arxiv.org/abs/2310.06625](https://arxiv.org/abs/2310.06625)

    iTransformer通过重新利用Transformer架构，在时间序列预测中简单应用注意力和馈送，提高了性能并克服了其他模型在处理具有更大回溯窗口的系列时面临的挑战

    

    最近线性预测模型的兴起对基于Transformer的预测器的架构修改的持续热情提出了质疑。这些预测器利用Transformer来模拟对时间序列的时间标记的全局依赖关系，每个时间标记由相同时间戳的多个变量组成。然而，由于性能下降和计算爆炸，Transformer在预测具有更大回溯窗口的系列时受到挑战。此外，每个时间标记的嵌入融合了代表潜在延迟事件和不同物理测量的多个变量，这可能会导致无法学习变量-centric表示并导致无意义的注意力映射。在这项工作中，我们反思了Transformer组件的能力，并重新利用了Transformer架构，而没有修改基本组件。我们提出了iTransformer，它简单地应用了注意力和馈送-

    arXiv:2310.06625v3 Announce Type: replace  Abstract: The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-
    
[^299]: 具有在线风险感知适应性的分布式强化学习

    Distributional Reinforcement Learning with Online Risk-awareness Adaption

    [https://arxiv.org/abs/2310.05179](https://arxiv.org/abs/2310.05179)

    本论文提出了一个新的分布式强化学习框架，可以通过在线风险适应性调整来量化不确定性，并动态选择认知风险水平。

    

    在实际应用中使用强化学习（RL）需要考虑次优结果，这取决于代理人对不确定环境的熟悉程度。本文介绍了一个新的框架，Distributional RL with Online Risk Adaption（DRL-ORA），可以综合量化不确定性并动态选择认知风险水平，通过在线解决总变差最小化问题。风险水平选择可以通过使用Follow-The-Leader类型算法进行网格搜索来有效实现。

    arXiv:2310.05179v2 Announce Type: replace  Abstract: The use of reinforcement learning (RL) in practical applications requires considering sub-optimal outcomes, which depend on the agent's familiarity with the uncertain environment. Dynamically adjusting the level of epistemic risk over the course of learning can tactically achieve reliable optimal policy in safety-critical environments and tackle the sub-optimality of a static risk level. In this work, we introduce a novel framework, Distributional RL with Online Risk Adaption (DRL-ORA), which can quantify the aleatory and epistemic uncertainties compositely and dynamically select the epistemic risk levels via solving a total variation minimization problem online. The risk level selection can be efficiently achieved through grid search using a Follow-The-Leader type algorithm, and its offline oracle is related to "satisficing measure" (in the decision analysis community) under a special modification of the loss function. We show multi
    
[^300]: 极化社会中的视觉政治传播: 巴西总统选举在Instagram上的纵向研究

    Visual Political Communication in a Polarized Society: A Longitudinal Study of Brazilian Presidential Elections on Instagram

    [https://arxiv.org/abs/2310.00349](https://arxiv.org/abs/2310.00349)

    该研究通过纵向研究巴西总统候选人在Instagram上发布的帖子，揭示了视觉政治传播中固定的模式，包括庆祝和积极调性图像的普遍使用以及候选人与选民紧密联系的个性化展示。

    

    在当今数字时代，图像已经成为政治人士在社交媒体平台上与选民互动的强大工具。视觉内容具有独特的情感吸引力，往往导致用户参与度增加。然而，对视觉传播的研究仍然相对有限，尤其是在全球南方国家。本研究旨在通过采用计算方法和定性方法相结合的方式，研究2018年和2022年19位巴西总统候选人在Instagram上发布的11,263条帖子中采用的视觉传播策略，以弥补这一空白。通过两项研究，我们观察到这些候选人在视觉政治传播上存在一致的模式。值得注意的是，我们确定了庆祝和积极调性图像的普遍性。他们还展示了强烈的个性化感，描绘候选人与选民更紧密相连的形象。

    arXiv:2310.00349v2 Announce Type: replace-cross  Abstract: In today's digital age, images have emerged as powerful tools for politicians to engage with their voters on social media platforms. Visual content possesses a unique emotional appeal that often leads to increased user engagement. However, research on visual communication remains relatively limited, particularly in the Global South. This study aims to bridge this gap by employing a combination of computational methods and qualitative approach to investigate the visual communication strategies employed in a dataset of 11,263 Instagram posts by 19 Brazilian presidential candidates in 2018 and 2022 national elections. Through two studies, we observed consistent patterns across these candidates on their use of visual political communication. Notably, we identify a prevalence of celebratory and positively toned images. They also exhibit a strong sense of personalization, portraying candidates connected with their voters on a more em
    
[^301]: 一个统一的高斯过程运动规划变分框架

    A Unifying Variational Framework for Gaussian Process Motion Planning

    [https://arxiv.org/abs/2309.00854](https://arxiv.org/abs/2309.00854)

    提出了一个基于变分高斯过程的机器人运动规划框架，统一和泛化了各种基于概率推断的运动规划算法，并将其与基于优化的规划器连接起来

    

    要控制机器人的运动，运动规划算法必须计算高维状态空间中的路径，同时考虑与电机和关节相关的物理约束，生成平稳稳定的运动，避开障碍物，并防止碰撞。运动规划算法必须平衡竞争需求，并且理想情况下应该包括不确定性以处理噪声、模型错误，并促进在复杂环境中的部署。为了解决这些问题，我们介绍了一个基于变分高斯过程的机器人运动规划框架，它统一和泛化了各种基于概率推断的运动规划算法，并将它们与基于优化的规划器连接起来。我们的框架提供了一种原则性和灵活的方法，在端到端的训练中整合基于相等性、基于不等式性和软性运动规划约束，易于实现，并提供bo

    arXiv:2309.00854v2 Announce Type: replace-cross  Abstract: To control how a robot moves, motion planning algorithms must compute paths in high-dimensional state spaces while accounting for physical constraints related to motors and joints, generating smooth and stable motions, avoiding obstacles, and preventing collisions. A motion planning algorithm must therefore balance competing demands, and should ideally incorporate uncertainty to handle noise, model errors, and facilitate deployment in complex environments. To address these issues, we introduce a framework for robot motion planning based on variational Gaussian processes, which unifies and generalizes various probabilistic-inference-based motion planning algorithms, and connects them with optimization-based planners. Our framework provides a principled and flexible way to incorporate equality-based, inequality-based, and soft motion-planning constraints during end-to-end training, is straightforward to implement, and provides bo
    
[^302]: 面向任务的机器遗忘及其在负荷预测中的应用

    Task-Aware Machine Unlearning and Its Application in Load Forecasting

    [https://arxiv.org/abs/2308.14412](https://arxiv.org/abs/2308.14412)

    该论文介绍了一种面向任务的机器遗忘概念，在负荷预测中应用该概念可以平衡遗忘完整性和模型性能之间的关系。

    

    数据隐私和安全已成为负载预测中一个不可忽视的因素。以往的研究主要集中在训练阶段的增强上。然而，一旦模型被训练和部署，如果发现这些数据是恶意的或被数据所有者要求，可能需要“遗忘”（即去除）部分训练数据的影响。本文介绍了机器遗忘的概念，其专门设计用于消除已经训练的预测器上数据集的影响。然而，直接遗忘不可避免地会降低模型的泛化能力。为了在遗忘的完整性和模型性能之间取得平衡，提出了一种性能感知算法，通过使用影响函数和样本重新加权来评估本地模型参数变化的敏感性。此外，我们观察到统计准则（如均方误差）无法完全反映操作成本。

    arXiv:2308.14412v2 Announce Type: replace  Abstract: Data privacy and security have become a non-negligible factor in load forecasting. Previous researches mainly focus on training stage enhancement. However, once the model is trained and deployed, it may need to `forget' (i.e., remove the impact of) part of training data if the these data are found to be malicious or as requested by the data owner. This paper introduces the concept of machine unlearning which is specifically designed to remove the influence of part of the dataset on an already trained forecaster. However, direct unlearning inevitably degrades the model generalization ability. To balance between unlearning completeness and model performance, a performance-aware algorithm is proposed by evaluating the sensitivity of local model parameter change using influence function and sample re-weighting. Furthermore, we observe that the statistical criterion such as mean squared error, cannot fully reflect the operation cost of th
    
[^303]: 使用区块链防御联邦学习中的恶意行为

    Defending Against Malicious Behaviors in Federated Learning with Blockchain

    [https://arxiv.org/abs/2307.00543](https://arxiv.org/abs/2307.00543)

    该研究提出了一个基于区块链和分布式分类账技术的安全和可靠的联邦学习系统，包括点对点投票机制和奖励和惩罚机制，以检测和阻止恶意行为，证明了该框架对抗恶意客户的有效性。

    

    在深度学习时代，联邦学习(FL)提供了一种有前途的方法，允许多家机构数据所有者或客户共同训练机器学习模型，而不会损害数据隐私。然而，大多数现有的FL方法依赖于用于全局模型聚合的集中式服务器，导致单点故障。这使系统在处理不诚实的客户时容易受到恶意攻击。在这项工作中，我们通过提出基于区块链和分布式分类账技术的安全可靠FL系统来解决这个问题。我们的系统结合了点对点投票机制和奖励和惩罚机制，由链上智能合约提供动力，以检测和阻止恶意行为。我们提出了理论和实证分析，以展示所提出方法的有效性，表明我们的框架对恶意客户是强大的。

    arXiv:2307.00543v2 Announce Type: replace-cross  Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-s
    
[^304]: 合作和分布式贝叶斯优化 via 共识: 展示合作的力量用于最优设计

    Collaborative and Distributed Bayesian Optimization via Consensus: Showcasing the Power of Collaboration for Optimal Design

    [https://arxiv.org/abs/2306.14348](https://arxiv.org/abs/2306.14348)

    通过引入共识概念实现了合作和分布式贝叶斯优化，充分展示了合作对于最优设计的重要作用

    

    最优设计在许多应用中是一个关键但具有挑战性的任务。这种挑战来自于需要进行大量的试验和错误，通常通过模拟或进行现场实验来完成。幸运的是，当使用带有贝叶斯风格的代理和高效的顺序抽样策略时，顺序最优设计，在贝叶斯优化下也扮演着加速设计过程的关键角色。然而，当今存在一个重要机遇。边缘设备的增加连接性为贝叶斯优化提供了一种新的合作范式。一种范式，通过有效地分配其实验努力，让不同客户端共同借鉴彼此的力量，以改善和快速推进其最优设计过程。为此，我们将共识的概念引入到贝叶斯优化中，客户端同意（即达成共识）其下一个采样设计。

    arXiv:2306.14348v2 Announce Type: replace  Abstract: Optimal design is a critical yet challenging task within many applications. This challenge arises from the need for extensive trial and error, often done through simulations or running field experiments. Fortunately, sequential optimal design, also referred to as Bayesian optimization when using surrogates with a Bayesian flavor, has played a key role in accelerating the design process through efficient sequential sampling strategies. However, a key opportunity exists nowadays. The increased connectivity of edge devices sets forth a new collaborative paradigm for Bayesian optimization. A paradigm whereby different clients collaboratively borrow strength from each other by effectively distributing their experimentation efforts to improve and fast-track their optimal design process. To this end, we bring the notion of consensus to Bayesian optimization, where clients agree (i.e., reach a consensus) on their next-to-sample designs. Our 
    
[^305]: 没有数据访问的深度分类器模拟

    Deep Classifier Mimicry without Data Access

    [https://arxiv.org/abs/2306.02090](https://arxiv.org/abs/2306.02090)

    提出了一种无需访问原始数据的模型-无关知识蒸馏过程CAKE，可以模拟深度分类器，通过生成噪声合成样本对比地扩散到模型的决策边界。

    

    最近，对预先训练模型的访问已经成为许多机器学习领域的标准。不幸的是，可能无法等同地获得模型训练所需的原始数据。这使得微调、压缩模型、持续调整或进行任何其他类型的数据驱动更新变得极具挑战性。我们认为可能无需原始数据访问。具体而言，我们提出了对比推理知识提取（CAKE），这是一种模型无关的知识蒸馏过程，可以模拟深度分类器而无需访问原始数据。为此，CAKE生成一对噪声合成样本，并将它们对比地扩散到模型的决策边界。我们通过几个基准数据集和各种架构选择在实证上证实了CAKE的有效性，为广泛应用铺平了道路。

    arXiv:2306.02090v2 Announce Type: replace-cross  Abstract: Access to pre-trained models has recently emerged as a standard across numerous machine learning domains. Unfortunately, access to the original data the models were trained on may not equally be granted. This makes it tremendously challenging to fine-tune, compress models, adapt continually, or to do any other type of data-driven update. We posit that original data access may however not be required. Specifically, we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure that mimics deep classifiers without access to the original data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses them contrastively toward a model's decision boundary. We empirically corroborate CAKE's effectiveness using several benchmark datasets and various architectural choices, paving the way for broad application.
    
[^306]: 哪些模型具有感知对齐梯度？通过离散度稳健性解释

    Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness

    [https://arxiv.org/abs/2305.19101](https://arxiv.org/abs/2305.19101)

    稳健计算机视觉模型的梯度通常与人类感知对齐，通过离散度稳健性解释这一现象。

    

    强健的计算机视觉模型的一个显著特性是它们的输入梯度通常与人类感知对齐，被文献称为感知对齐梯度（PAGs）。尽管只被训练用于分类，PAGs使得稳健模型具有基本的生成能力，包括图像生成、去噪和修复。然而，这些现象背后的机制仍未知。在这项工作中，我们通过\emph{离散度稳健性}首次对PAGs进行解释，该理论指出模型在数据流形之外必须比在流形上更加稳健。我们首先从理论上证明离散度稳健性导致输入梯度大致位于数据流形上，从而解释它们的感知对齐性。然后我们展示贝叶斯最优模型满足离散度稳健性，并在经验上证实通过梯度训练的稳健模型也满足相同条件。

    arXiv:2305.19101v2 Announce Type: replace  Abstract: One of the remarkable properties of robust computer vision models is that their input-gradients are often aligned with human perception, referred to in the literature as perceptually-aligned gradients (PAGs). Despite only being trained for classification, PAGs cause robust models to have rudimentary generative capabilities, including image generation, denoising, and in-painting. However, the underlying mechanisms behind these phenomena remain unknown. In this work, we provide a first explanation of PAGs via \emph{off-manifold robustness}, which states that models must be more robust off- the data manifold than they are on-manifold. We first demonstrate theoretically that off-manifold robustness leads input gradients to lie approximately on the data manifold, explaining their perceptual alignment. We then show that Bayes optimal models satisfy off-manifold robustness, and confirm the same empirically for robust models trained via grad
    
[^307]: DistriBlock: 通过利用输出分布的特征识别对抗性音频样本

    DistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution

    [https://arxiv.org/abs/2305.17000](https://arxiv.org/abs/2305.17000)

    DistriBlock提出了一种能够识别对抗性音频样本的有效检测策略，通过利用输出分布的特征，包括中位数、最大值和最小值、熵以及与后续时间步骤的分布之间的散度，应用二元分类器进行预测。这项研究证明了DistriBlock在识别对抗性音频样本方面的有效性。

    

    对抗性攻击可能误导自动语音识别（ASR）系统，使其预测任意目标文本，从而构成明显的安全威胁。为了防止这种攻击，我们提出了DistriBlock，一种适用于任何ASR系统的高效检测策略，该系统在每个时间步骤上预测输出标记的概率分布。我们对该分布的一组特征进行测量：输出概率的中位数、最大值和最小值，分布的熵，以及与后续时间步骤的分布之间的Kullback-Leibler和Jensen-Shannon散度。然后，通过利用对良性和对抗性数据观察到的特征，我们应用二元分类器，包括简单的基于阈值的分类、这种分类器的集合以及神经网络。通过对不同最先进的ASR系统和语言数据集进行广泛分析，我们证明了DistriBlock在识别对抗性音频样本方面的有效性。

    arXiv:2305.17000v2 Announce Type: replace-cross  Abstract: Adversarial attacks can mislead automatic speech recognition (ASR) systems into predicting an arbitrary target text, thus posing a clear security threat. To prevent such attacks, we propose DistriBlock, an efficient detection strategy applicable to any ASR system that predicts a probability distribution over output tokens in each time step. We measure a set of characteristics of this distribution: the median, maximum, and minimum over the output probabilities, the entropy of the distribution, as well as the Kullback-Leibler and the Jensen-Shannon divergence with respect to the distributions of the subsequent time step. Then, by leveraging the characteristics observed for both benign and adversarial data, we apply binary classifiers, including simple threshold-based classification, ensembles of such classifiers, and neural networks. Through extensive analysis across different state-of-the-art ASR systems and language data sets, 
    
[^308]: 通过基于学习的自适应优化纠正误差的量子近似优化算法

    Error-mitigated Quantum Approximate Optimization via Learning-based Adaptive Optimization

    [https://arxiv.org/abs/2303.14877](https://arxiv.org/abs/2303.14877)

    我们设计了一种双自适应区域贝叶斯优化算法，用于提升量子近似优化算法的性能。实验结果表明，该算法在性能上远远超过了传统的方法。

    

    组合优化问题普遍存在，并且在一般情况下很难求解。量子计算被视为一种强大的工具，可以在解决某些问题时具有潜在的计算优势。量子近似优化算法（QAOA）是最具代表性的量子-经典混合算法之一，旨在通过将离散优化问题转化为连续电路参数域上的经典优化问题来解决特定的组合优化问题。QAOA目标函数在参数变量上的景观因其普遍存在的局部最小值和贫瘠平台而臭名昭著，其可行性在很大程度上依赖于经典优化算法的有效性。为了提高QAOA的性能，我们设计了一种双自适应区域贝叶斯优化（DARBO），一种适应性经典优化器用于QAOA。我们的实验结果表明，该算法在性能上远远超过了传统的grad方法。

    Combinatorial optimization problems are ubiquitous and computationally hard to solve in general. Quantum computing is envisioned as a powerful tool offering potential computational advantages for solving some of these problems. Quantum approximate optimization algorithm (QAOA), one of the most representative quantum-classical hybrid algorithms, is designed to solve certain combinatorial optimization problems by transforming a discrete optimization problem into a classical optimization problem over a continuous circuit parameter domain. QAOA objective landscape over the parameter variables is notorious for pervasive local minima and barren plateaus, and its viability in training significantly relies on the efficacy of the classical optimization algorithm. To enhance the performance of QAOA, we design double adaptive-region Bayesian optimization (DARBO), an adaptive classical optimizer for QAOA. Our experimental results demonstrate that the algorithm greatly outperforms conventional grad
    
[^309]: 基于fMRI和DTI数据的大脑有效连接组：贝叶斯因果学习与评估

    Brain Effective Connectome based on fMRI and DTI Data: Bayesian Causal Learning and Assessment

    [https://arxiv.org/abs/2302.05451](https://arxiv.org/abs/2302.05451)

    提出了两种基于贝叶斯因果发现框架的方法，通过利用DTI数据作为先验知识，显著提高了在仅基于fMRI数据发现大脑有效连接组的准确性和可靠性

    

    神经科学研究旨在找到准确可靠的大脑有效连接组（EC）。尽管目前的EC发现方法已经为我们理解大脑组织做出了贡献，但由于fMRI数据的样本量较小、时间分辨率较低以及大脑连接组的高维性，它们的性能受到严重限制。通过利用DTI数据作为先验知识，我们引入了两种贝叶斯因果发现框架--贝叶斯GOLEM（BGOLEM）和贝叶斯FGES（BFGES）方法--这两种方法提供了更准确可靠的EC，并解决了仅基于fMRI数据发现EC的现有因果发现方法的缺陷。通过一系列针对合成和混合（人类连接组计划（HCP）受试者的DTI与合成fMRI数据）数据的模拟研究，我们展示了所提方法在发现EC方面的有效性。为了对改进进行数值评估

    arXiv:2302.05451v3 Announce Type: replace  Abstract: Neuroscientific studies aim to find an accurate and reliable brain Effective Connectome (EC). Although current EC discovery methods have contributed to our understanding of brain organization, their performances are severely constrained by the short sample size and poor temporal resolution of fMRI data, and high dimensionality of the brain connectome. By leveraging the DTI data as prior knowledge, we introduce two Bayesian causal discovery frameworks -- the Bayesian GOLEM (BGOLEM) and Bayesian FGES (BFGES) methods -- that offer significantly more accurate and reliable ECs and address the shortcomings of the existing causal discovery methods in discovering ECs based on only fMRI data. Through a series of simulation studies on synthetic and hybrid (DTI of the Human Connectome Project (HCP) subjects and synthetic fMRI) data, we demonstrate the effectiveness of the proposed methods in discovering EC. To numerically assess the improvement
    
[^310]: 通过交叉模型比较损失增强语言理解中神经元效用

    Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding

    [https://arxiv.org/abs/2301.03765](https://arxiv.org/abs/2301.03765)

    本论文提出通过交叉模型比较损失的方法来增强语言理解模型中神经元的效用，实现减少冗余参数和抑制输入噪声的目标。

    

    当前自然语言理解（NLU）模型在模型规模和输入背景方面不断扩大，引入了更多隐藏神经元和输入神经元，大体上提高了性能。然而，额外的神经元并不能为所有实例带来一致的改进，因为一些隐藏神经元是冗余的，混入输入神经元的噪声往往会分散模型的注意力。之前的工作主要侧重于通过附加的后处理或预处理，如网络修剪和上下文选择，从外部降低低效神经元的数量，以避免这个问题。除此之外，我们是否可以通过增强每个神经元的效用来使模型减少冗余参数并抑制输入噪声？如果一个模型能够有效地利用神经元，那么不管哪些神经元被剥离（禁用），剥离后的子模型的性能都不应该优于原始完整模型。根据这样的比较

    arXiv:2301.03765v2 Announce Type: replace  Abstract: Current natural language understanding (NLU) models have been continuously scaling up, both in terms of model size and input context, introducing more hidden and input neurons. While this generally improves performance on average, the extra neurons do not yield a consistent improvement for all instances. This is because some hidden neurons are redundant, and the noise mixed in input neurons tends to distract the model. Previous work mainly focuses on extrinsically reducing low-utility neurons by additional post- or pre-processing, such as network pruning and context selection, to avoid this problem. Beyond that, can we make the model reduce redundant parameters and suppress input noise by intrinsically enhancing the utility of each neuron? If a model can efficiently utilize neurons, no matter which neurons are ablated (disabled), the ablated submodel should perform no better than the original full model. Based on such a comparison pr
    
[^311]: oneDNN图编译器：用于高性能深度学习编译的混合方法

    oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep Learning Compilation

    [https://arxiv.org/abs/2301.01333](https://arxiv.org/abs/2301.01333)

    提出了oneDNN图编译器，采用了编译优化和混合方法，旨在实现高性能张量编译，生成专家级性能代码并在DNN计算图范围内应用编译优化

    

    随着深度学习模型的快速发展和硬件对密集计算的支持，深度学习工作负载特征发生了显著变化，从计算密集型操作上的几个热点到分布在模型中的广泛操作。利用专家调优的基元实现加速几个计算密集型操作并不能充分利用AI硬件的性能潜力。已经做出了各种努力来编译完整的深度神经网络（DNN）图。最大的挑战之一是通过生成专家级性能代码来实现高性能张量编译，对DNN计算图的范围跨多个计算密集操作应用编译优化。

    arXiv:2301.01333v3 Announce Type: replace  Abstract: With the rapid development of deep learning models and hardware support for dense computing, the deep learning workload characteristics changed significantly from a few hot spots on compute-intensive operations to a broad range of operations scattered across the models. Accelerating a few compute-intensive operations using the expert-tuned implementation of primitives does not fully exploit the performance potential of AI hardware. Various efforts have been made to compile a full deep neural network (DNN) graph. One of the biggest challenges is to achieve high-performance tensor compilation by generating expert level performance code for the dense compute-intensive operations and applying compilation optimization at the scope of DNN computation graph across multiple compute-intensive operations.   We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid approach of using techniques from both compiler optimization and
    
[^312]: Visual CPG-RL：学习用于视觉引导的四足动物运动的中枢模式发生器

    Visual CPG-RL: Learning Central Pattern Generators for Visually-Guided Quadruped Locomotion

    [https://arxiv.org/abs/2212.14400](https://arxiv.org/abs/2212.14400)

    该论文提出了一个框架，通过将外感知传感和中枢模式发生器整合到深度强化学习框架中，学习视觉引导的四足动物运动，探索了耦合振荡器系统对导航鲁棒性的改进、具有记忆功能的策略网络与无记忆策略网络在导航任务中的效果以及动物如何容忍高。

    

    我们提出了一个框架，通过将外感知传感和中枢模式发生器（CPGs，即耦合振荡器系统）整合到深度强化学习（DRL）框架中，学习视觉引导的四足动物运动。通过外感知和本体感知，代理学习协调不同振荡器之间的节律行为，以跟踪速度指令，同时覆盖这些指令以避免与环境碰撞。我们探讨了几个开放的机器人技术和神经科学问题：1）振荡器之间的显式相互耦合的作用是什么，这种耦合是否能提高用于导航鲁棒性的从模拟到真实的转移？2）使用具有记忆功能与无记忆策略网络对于鲁棒性、能效和从模拟到真实导航任务的跟踪性能有什么影响？3）动物是如何容忍高

    arXiv:2212.14400v2 Announce Type: replace-cross  Abstract: We present a framework for learning visually-guided quadruped locomotion by integrating exteroceptive sensing and central pattern generators (CPGs), i.e. systems of coupled oscillators, into the deep reinforcement learning (DRL) framework. Through both exteroceptive and proprioceptive sensing, the agent learns to coordinate rhythmic behavior among different oscillators to track velocity commands, while at the same time override these commands to avoid collisions with the environment. We investigate several open robotics and neuroscience questions: 1) What is the role of explicit interoscillator couplings between oscillators, and can such coupling improve sim-to-real transfer for navigation robustness? 2) What are the effects of using a memory-enabled vs. a memory-free policy network with respect to robustness, energy-efficiency, and tracking performance in sim-to-real navigation tasks? 3) How do animals manage to tolerate high 
    
[^313]: 量子启发的张量神经网络用于期权定价

    Quantum-Inspired Tensor Neural Networks for Option Pricing

    [https://arxiv.org/abs/2212.14076](https://arxiv.org/abs/2212.14076)

    引入了Tensor神经网络（TNN）和张量网络初始化器（TNN Init），TNN可以在获得与DNN相同精度的情况下提供显著的参数节省，并展示了训练速度更快的优点。

    

    深度学习的最新进展使我们能够通过在更高维度解决问题来解决维度诅咒（COD）。解决COD的这类方法之一导致我们解决高维PDE问题。这已经为解决从数学金融到工业应用中的随机控制等各种实际问题敞开了大门。尽管可行，但这些深度学习方法仍受到训练时间和内存的限制。Tensor神经网络（TNN）解决了这些缺点，它们证明可以在获得与经典密集神经网络（DNN）相同精度的情况下提供显著的参数节省。此外，我们还展示了相比DNN，TNN的训练速度更快，除了TNN，我们还介绍了张量网络初始化器（TNN Init），这是一种权重初始化方案，可以实现更快的收敛速度和更小的

    arXiv:2212.14076v2 Announce Type: replace-cross  Abstract: Recent advances in deep learning have enabled us to address the curse of dimensionality (COD) by solving problems in higher dimensions. A subset of such approaches of addressing the COD has led us to solving high-dimensional PDEs. This has resulted in opening doors to solving a variety of real-world problems ranging from mathematical finance to stochastic control for industrial applications. Although feasible, these deep learning methods are still constrained by training time and memory. Tackling these shortcomings, Tensor Neural Networks (TNN) demonstrate that they can provide significant parameter savings while attaining the same accuracy as compared to the classical Dense Neural Network (DNN). In addition, we also show how TNN can be trained faster than DNN for the same accuracy. Besides TNN, we also introduce Tensor Network Initializer (TNN Init), a weight initialization scheme that leads to faster convergence with smaller 
    
[^314]: Text2Model:基于文本的模型归纳用于零样本图像分类

    Text2Model: Text-based Model Induction for Zero-shot Image Classification

    [https://arxiv.org/abs/2210.15182](https://arxiv.org/abs/2210.15182)

    该论文提出了一种使用文本描述构建与任务无关的分类器的方法，通过生成针对查询分类任务定制的模型来解决零样本图像分类问题。

    

    我们解决了仅使用文本描述构建与任务无关的分类器的挑战，展示了一种统一的方法来进行图像分类、3D点云分类以及从场景中识别动作。与学习固定输出类别表示的方法不同，我们在推断时生成针对查询分类任务定制的模型。为了生成基于任务的零样本分类器，我们训练一个超网络，该网络接收类描述并输出一个多类模型。超网络设计为对描述集合和分类层具有等变性，因此符合问题的对称性并提高了泛化性能。我们的方法生成非线性分类器，并且可以处理丰富的文本描述。我们在一系列零样本分类任务中评估了这种方法，涵盖了图像、点云和动作识别，并使用一系列文本描述。

    arXiv:2210.15182v2 Announce Type: replace-cross  Abstract: We address the challenge of building task-agnostic classifiers using only text descriptions, demonstrating a unified approach to image classification, 3D point cloud classification, and action recognition from scenes. Unlike approaches that learn a fixed representation of the output classes, we generate at inference time a model tailored to a query classification task. To generate task-based zero-shot classifiers, we train a hypernetwork that receives class descriptions and outputs a multi-class model. The hypernetwork is designed to be equivariant with respect to the set of descriptions and the classification layer, thus obeying the symmetries of the problem and improving generalization. Our approach generates non-linear classifiers and can handle rich textual descriptions. We evaluate this approach in a series of zero-shot classification tasks, for image, point-cloud, and action recognition, using a range of text descriptions
    
[^315]: 机器学习驱动的课程分配

    Machine Learning-Powered Course Allocation

    [https://arxiv.org/abs/2210.00954](https://arxiv.org/abs/2210.00954)

    引入机器学习驱动的课程分配机制（MLCM），通过机器学习模块减轻学生在报告偏好时的错误，显著提高学生效用，且具有对环境变化的稳健性。

    

    我们研究了课程分配问题，即大学为学生安排课程时间表。目前最先进的机制Course Match存在一个主要缺点：学生在报告他们的偏好时会犯很大的错误，从而对福利和公平性产生负面影响。为解决这一问题，我们引入了一种新机制，即基于机器学习的Course Match（MLCM）。MLCM的核心是一个机器学习驱动的偏好引导模块，通过迭代式地向学生提出个性化的两两比较查询，以减轻学生的报告错误。大量基于真实数据的计算实验表明，MLCM仅需十个比较查询，就能将平均和最小学生效用分别提高7%-11%和17%-29%。最后，我们强调了MLCM对环境变化的稳健性，并展示了我们的设计如何最小化升级至MLCM 的风险。

    arXiv:2210.00954v3 Announce Type: replace-cross  Abstract: We study the course allocation problem, where universities assign course schedules to students. The current state-of-the-art mechanism, Course Match, has one major shortcoming: students make significant mistakes when reporting their preferences, which negatively affects welfare and fairness. To address this issue, we introduce a new mechanism, Machine Learning-powered Course Match (MLCM). At the core of MLCM is a machine learning-powered preference elicitation module that iteratively asks personalized pairwise comparison queries to alleviate students' reporting mistakes. Extensive computational experiments, grounded in real-world data, demonstrate that MLCM, with only ten comparison queries, significantly increases both average and minimum student utility by 7%-11% and 17%-29%, respectively. Finally, we highlight MLCM's robustness to changes in the environment and show how our design minimizes the risk of upgrading to MLCM whil
    
[^316]: 有效学习张量场的分解嵌入

    Sample Efficient Learning of Factored Embeddings of Tensor Fields

    [https://arxiv.org/abs/2209.00372](https://arxiv.org/abs/2209.00372)

    学习了有效的方法来生成张量场的分解嵌入，使得对原始张量集合进行信息查询和后处理更加高效。

    

    数据张量的秩2及以上的生成现在已成常态。这些数据集越来越庞大且不断增长。许多科学和医学数据张量是张量场（例如图像、视频、地理数据），其中空间邻域包含重要信息。直接访问如此庞大的数据张量集合以获取信息变得越来越困难。我们学习了近似全秩和紧凑的张量草图，具有提供张量场的紧凑空间、时间和谱嵌入的分解表示。所有对原始张量场的信息查询和后处理现在可以更高效地实现，并且能够在这些紧凑的分解草图上以潜在生成空间中进行定制化准确度。通过从样本高效子空间构建紧凑的因子矩阵，我们可以生成任意阶数据张量的最佳秩r草图化Tucker分解。

    arXiv:2209.00372v2 Announce Type: replace  Abstract: Data tensors of orders 2 and greater are now routinely being generated. These data collections are increasingly huge and growing. Many scientific and medical data tensors are tensor fields (e.g., images, videos, geographic data) in which the spatial neighborhood contains important information. Directly accessing such large data tensor collections for information has become increasingly prohibitive. We learn approximate full-rank and compact tensor sketches with decompositive representations providing compact space, time and spectral embeddings of tensor fields. All information querying and post-processing on the original tensor field can now be achieved more efficiently and with customizable accuracy as they are performed on these compact factored sketches in latent generative space. We produce optimal rank-r sketchy Tucker decomposition of arbitrary order data tensors by building compact factor matrices from a sample-efficient sub-s
    
[^317]: 信号处理中神经网络应用的计算复杂度评估

    Computational Complexity Evaluation of Neural Network Applications in Signal Processing

    [https://arxiv.org/abs/2206.12191](https://arxiv.org/abs/2206.12191)

    本文提供了对数字信号处理中神经网络层计算复杂度进行评估和比较的系统方法，引入了适用于异构量化的“加法和位移数量(NABS)”指标。

    

    在本文中，我们提供了一种系统方法，用于评估和比较数字信号处理中神经网络层的计算复杂度。我们提供并链接了四种软件到硬件复杂度衡量标准，定义了不同复杂度指标与层的超参数之间的关系。本文解释了如何计算前馈和循环层的这四个指标，并定义了我们何时应该使用特定指标的情况，具体取决于我们是对更软件导向还是硬件导向的应用进行特征化。其中一种名为“加法和位移数量（NABS）”的四个指标为异构量化新引入。NABS刻画了操作中使用的位宽以及算术操作中使用的量化类型的影响。我们希望这项工作能够作为不同水平（目的）的复杂度估计的基准。

    arXiv:2206.12191v2 Announce Type: replace-cross  Abstract: In this paper, we provide a systematic approach for assessing and comparing the computational complexity of neural network layers in digital signal processing. We provide and link four software-to-hardware complexity measures, defining how the different complexity metrics relate to the layers' hyper-parameters. This paper explains how to compute these four metrics for feed-forward and recurrent layers, and defines in which case we ought to use a particular metric depending on whether we characterize a more soft- or hardware-oriented application. One of the four metrics, called `the number of additions and bit shifts (NABS)', is newly introduced for heterogeneous quantization. NABS characterizes the impact of not only the bitwidth used in the operation but also the type of quantization used in the arithmetical operations. We intend this work to serve as a baseline for the different levels (purposes) of complexity estimation rela
    
[^318]: 您的模型是在预测过去吗？

    Is your model predicting the past?

    [https://arxiv.org/abs/2206.11673](https://arxiv.org/abs/2206.11673)

    提出了区分机器学习模型是预测个体未来还是重复过去模式的方法，通过向后基线测试展示模型是否回溯过去，并在长期面板调研任务中验证了该框架的有效性。

    

    机器学习模型何时预测个体的未来，何时重复预先存在的模式？在这项工作中，我们提出了对这两种预测路径进行区分，这一提议得到了理论、经验和规范性论证的支持。我们提出的核心是一类简单高效的统计测试，称为“向后基线”，可以展示模型是否以及在何种程度上重新讲述了过去。我们的统计理论为解释向后基线提供了指导，并建立了不同基线和熟悉统计概念之间的等价关系。具体而言，我们推导了一个有意义的向后基线，可以对预测系统进行审计，即使只提供了背景变量和系统的预测。在经验上，我们在从纵向面板调查中衍生出的不同预测任务上评估了该框架，展示了将其纳入的便捷性和有效性。

    arXiv:2206.11673v2 Announce Type: replace  Abstract: When does a machine learning model predict the future of individuals and when does it recite patterns that predate the individuals? In this work, we propose a distinction between these two pathways of prediction, supported by theoretical, empirical, and normative arguments. At the center of our proposal is a family of simple and efficient statistical tests, called backward baselines, that demonstrate if, and to what extent, a model recounts the past. Our statistical theory provides guidance for interpreting backward baselines, establishing equivalences between different baselines and familiar statistical concepts. Concretely, we derive a meaningful backward baseline for auditing a prediction system as a black box, given only background variables and the system's predictions. Empirically, we evaluate the framework on different prediction tasks derived from longitudinal panel surveys, demonstrating the ease and effectiveness of incorpo
    
[^319]: OpenXAI: 迈向透明评估模型解释

    OpenXAI: Towards a Transparent Evaluation of Model Explanations

    [https://arxiv.org/abs/2206.11104](https://arxiv.org/abs/2206.11104)

    OpenXAI 是一个开源框架，旨在评估和基准测试后续解释方法，提供了灵活的数据生成器、多种数据集和评估指标，用户可轻松扩展和比较不同解释方法。

    

    虽然最近文献中提出了几种后续解释方法，但对这些方法进行系统性基准测试的工作非常少。在这里，我们介绍了OpenXAI，一个全面且可扩展的开源框架，用于评估和基准测试后续解释方法。OpenXAI包括以下关键组件：（i）灵活的合成数据生成器和各种真实世界数据集、预训练模型和最先进特征归属方法的集合，以及（ii）用于评估解释方法忠实度、稳定性（鲁棒性）和公平性的十一种量化度量标准的开源实现，从而提供了对多种度量标准、模型和数据集上几种解释方法的比较。OpenXAI易于扩展，用户可以轻松评估自定义解释方法并将其纳入我们的排行榜中。

    arXiv:2206.11104v4 Announce Type: replace-cross  Abstract: While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, and (ii) open-source implementations of eleven quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, in turn providing comparisons of several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI 
    
[^320]: NAS-Bench-Graph: 基于图神经架构搜索的基准测试

    NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search

    [https://arxiv.org/abs/2206.09166](https://arxiv.org/abs/2206.09166)

    NAS-Bench-Graph提出了一个定制的基准测试，支持对GraphNAS进行统一、可复现和高效的评估，解决了实验设置不一致和计算需求大的挑战。

    

    图神经架构搜索（GraphNAS）最近在学术界和工业界引起了相当大的关注。然而，两个关键挑战严重阻碍了对GraphNAS的进一步研究。首先，由于实验设置没有共识，不同研究论文中的实验结果通常不可比较，甚至不可重复，导致不公平的比较。其次，GraphNAS通常需要大量计算，这使其高度低效且不易访问，限制了没有大规模计算资源的研究人员。为了解决这些挑战，我们提出了NAS-Bench-Graph，这是一个定制的基准测试，支持对GraphNAS进行统一、可复现和高效的评估。具体来说，我们构建了一个统一、表达力丰富而紧凑的搜索空间，涵盖了26,206种独特的图神经网络（GNN）架构，并提出了一个有原则的评估协议。

    arXiv:2206.09166v2 Announce Type: replace  Abstract: Graph neural architecture search (GraphNAS) has recently aroused considerable attention in both academia and industry. However, two key challenges seriously hinder the further research of GraphNAS. First, since there is no consensus for the experimental setting, the empirical results in different research papers are often not comparable and even not reproducible, leading to unfair comparisons. Secondly, GraphNAS often needs extensive computations, which makes it highly inefficient and inaccessible to researchers without access to large-scale computation. To solve these challenges, we propose NAS-Bench-Graph, a tailored benchmark that supports unified, reproducible, and efficient evaluations for GraphNAS. Specifically, we construct a unified, expressive yet compact search space, covering 26,206 unique graph neural network (GNN) architectures and propose a principled evaluation protocol. To avoid unnecessary repetitive training, we hav
    
[^321]: 缓慢变化的对抗式赌博算法在折扣马尔可夫决策过程中效率高效

    Slowly Changing Adversarial Bandit Algorithms are Efficient for Discounted MDPs

    [https://arxiv.org/abs/2205.09056](https://arxiv.org/abs/2205.09056)

    研究证明，在一些假设下，任何缓慢变化的对抗式老虎机算法在折扣马尔可夫决策过程中能够达到最优期望遗憾。

    

    强化学习将多臂老虎机问题泛化为具有更长计划视野和未知转移内核的额外困难。我们探讨了从折扣无限视野表格强化学习到多臂老虎机的黑盒降维，其中特别是在每个状态放置一个独立的老虎机学习器。我们表明，在遍历性和快速混合的假设下，任何在对抗式老虎机设置中实现最优遗憾的缓慢变化对抗式老虎机算法也可以在无限视野折扣马尔可夫决策过程中实现最优期望遗憾，关于回合数$T$。此外，我们使用指数加权算法的特定实例来检验我们的降维过程。

    arXiv:2205.09056v3 Announce Type: replace  Abstract: Reinforcement learning generalizes multi-armed bandit problems with additional difficulties of a longer planning horizon and unknown transition kernel. We explore a black-box reduction from discounted infinite-horizon tabular reinforcement learning to multi-armed bandits, where, specifically, an independent bandit learner is placed in each state. We show that, under ergodicity and fast mixing assumptions, any slowly changing adversarial bandit algorithm achieving optimal regret in the adversarial bandit setting can also attain optimal expected regret in infinite-horizon discounted Markov decision processes, with respect to the number of rounds $T$. Furthermore, we examine our reduction using a specific instance of the exponential-weight algorithm.
    
[^322]: 针对正负样本学习的不确定性感知伪标签选择

    Uncertainty-aware Pseudo-label Selection for Positive-Unlabeled Learning

    [https://arxiv.org/abs/2201.13192](https://arxiv.org/abs/2201.13192)

    通过不确定性感知的伪标签选择过程，本研究提出了一种解决正负样本学习中不平衡数据集和模型校准问题的方法，实验结果表明在高度不平衡的情况下能显著提高预测性能。

    

    正负样本学习（PUL）旨在从仅具有正样本和未标记训练数据中学习二元分类器。尽管现实应用通常涉及不平衡数据集，其中大多数示例属于一类，但大多数当代PUL方法并未研究这种情况下的性能，因此严重限制了它们在实践中的适用性。在这项工作中，我们通过一种不确定性感知的伪标签选择过程（PUUPL）来解决不平衡数据集和模型校准问题：通过增强少数类的信号，伪标签从未标记集中扩展了带标签的数据集，而显式的不确定性量化防止了有害的确认偏见的出现，从而提高了预测性能。通过一系列实验，PUUPL在高度不平衡的情况下取得了显著的性能提升。

    arXiv:2201.13192v3 Announce Type: replace-cross  Abstract: Positive-unlabeled learning (PUL) aims at learning a binary classifier from only positive and unlabeled training data. Even though real-world applications often involve imbalanced datasets where the majority of examples belong to one class, most contemporary approaches to PUL do not investigate performance in this setting, thus severely limiting their applicability in practice. In this work, we thus propose to tackle the issues of imbalanced datasets and model calibration in a PUL setting through an uncertainty-aware pseudo-labeling procedure (PUUPL): by boosting the signal from the minority class, pseudo-labeling expands the labeled dataset with new samples from the unlabeled set, while explicit uncertainty quantification prevents the emergence of harmful confirmation bias leading to increased predictive performance. Within a series of experiments, PUUPL yields substantial performance gains in highly imbalanced settings while 
    
[^323]: 带波脉冲Q学习的深度强化学习

    Deep Reinforcement Learning with Spiking Q-learning

    [https://arxiv.org/abs/2201.09754](https://arxiv.org/abs/2201.09754)

    基于脉冲Q学习的深度强化学习方法DSQN利用非脉冲神经元的膜电压作为Q值表示，实现了能源高效的控制任务。

    

    借助特殊的神经形态硬件，期望通过脉冲神经网络（SNNs）实现人工智能（AI），以更少的能量消耗。通过将SNNs与深度强化学习（RL）相结合，为实现现实控制任务提供了一种有前途的高效能源方式。目前仅有少数基于SNN的RL方法。其中大部分要么缺乏泛化能力，要么在训练中使用人工神经网络（ANNs）来估算值函数。前者需要为每个场景调整大量超参数，而后者限制了不同类型RL算法的应用并忽略了训练中的能量消耗较大。为开发一个强大的基于脉冲的RL方法，我们从昆虫中发现的非脉冲间神经元中汲取灵感，提出了深度脉冲Q网络（DSQN），使用非脉冲神经元的膜电压作为Q值的表示，从而可以指导

    arXiv:2201.09754v2 Announce Type: replace-cross  Abstract: With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of non-spiking neurons as the representation of Q-value, which can direct
    
[^324]: 嵌套非参数工具变量回归：长期、中介和时变治疗效应

    Nested Nonparametric Instrumental Variable Regression: Long Term, Mediated, and Time Varying Treatment Effects

    [https://arxiv.org/abs/2112.14249](https://arxiv.org/abs/2112.14249)

    该论文提出了嵌套非参数工具变量回归的对抗估计器，并提供了对因果参数进行有效推断的充分条件，具有限制病态性复合技术、多种适应模型和扩展到因果函数等特征。

    

    短面板数据模型中的几个因果参数是称为嵌套非参数工具变量回归（nested NPIV）的函数的标量总结。例如，使用代理变量识别出长期、中介和时变治疗效应。然而，似乎不存在关于嵌套NPIV的先前估计量或保证，这样就无法灵活地估计和推断这些因果参数。一个主要挑战是由于嵌套逆问题而导致的复合病态性。我们分析了嵌套NPIV的对抗估计器，并提供了对因果参数进行有效推断的充分条件。我们的非渐近分析具有三个显著特征：（i）引入限制病态性复合的技术；（ii）适应神经网络、随机森林和再生核希尔伯特空间；（iii）扩展到因果函数，例如长期异质治疗效果。

    arXiv:2112.14249v3 Announce Type: replace-cross  Abstract: Several causal parameters in short panel data models are scalar summaries of a function called a nested nonparametric instrumental variable regression (nested NPIV). Examples include long term, mediated, and time varying treatment effects identified using proxy variables. However, it appears that no prior estimators or guarantees for nested NPIV exist, preventing flexible estimation and inference for these causal parameters. A major challenge is compounding ill posedness due to the nested inverse problems. We analyze adversarial estimators of nested NPIV, and provide sufficient conditions for efficient inference on the causal parameter. Our nonasymptotic analysis has three salient features: (i) introducing techniques that limit how ill posedness compounds; (ii) accommodating neural networks, random forests, and reproducing kernel Hilbert spaces; and (iii) extending to causal functions, e.g. long term heterogeneous treatment eff
    
[^325]: PMFL: 针对异构任务的部分元联邦学习及其在现实世界医疗记录中的应用

    PMFL: Partial Meta-Federated Learning for heterogeneous tasks and its applications on real-world medical records

    [https://arxiv.org/abs/2112.05321](https://arxiv.org/abs/2112.05321)

    提出了PMFL算法，结合了联邦学习和元学习的思想，针对异构任务的数据分布，解决了传统联邦学习在医疗记录等领域应用受限的问题

    

    arXiv:2112.05321v2 公告类型: 替换 摘要: 联邦机器学习是一种多功能和灵活的工具，可以利用来自不同来源的分布式数据，尤其是当通信技术高速发展，如今可以在移动设备上收集到前所未有的大量数据时。联邦学习方法利用网络中所有设备的数据和计算能力来实现更高效的模型训练。然而，尽管大多数传统的联邦学习方法适用于同质数据和任务，但将该方法适应不同的异构数据和任务分布具有挑战性。这一限制已经限制了联邦学习在现实世界背景下的应用，特别是在医疗保健领域。在本研究中，受到元学习的基本思想的启发，我们提出了一种新算法，将联邦学习和元学习相结合，以解决这一问题。此外，由于该算法的优势

    arXiv:2112.05321v2 Announce Type: replace  Abstract: Federated machine learning is a versatile and flexible tool to utilize distributed data from different sources, especially when communication technology develops rapidly and an unprecedented amount of data could be collected on mobile devices nowadays. Federated learning method exploits not only the data but the computational power of all devices in the network to achieve more efficient model training. Nevertheless, while most traditional federated learning methods work well for homogeneous data and tasks, adapting the method to a different heterogeneous data and task distribution is challenging. This limitation has constrained the applications of federated learning in real-world contexts, especially in healthcare settings. Inspired by the fundamental idea of meta-learning, in this study we propose a new algorithm, which is an integration of federated learning and meta-learning, to tackle this issue. In addition, owing to the advanta
    
[^326]: 在分布外图上推广图神经网络

    Generalizing Graph Neural Networks on Out-Of-Distribution Graphs

    [https://arxiv.org/abs/2111.10657](https://arxiv.org/abs/2111.10657)

    提出了一个名为StableGNN的通用因果表示框架，通过提取高级图表示并利用因果推断的区分能力，帮助模型在分布外图上实现稳定的泛化能力。

    

    图神经网络（GNNs）在没有考虑训练和测试图之间的分布差异的情况下提出，导致GNNs在分布外（OOD）设置上的泛化能力下降。这种退化的根本原因是大多数GNNs是基于独立同分布假设开发的。在这种设置中，GNNs倾向于利用训练集中存在的细微统计相关性进行预测，即使这是一种伪相关性。然而，这种伪相关性在测试环境中可能会改变，导致GNNs失败。因此，消除伪相关性的影响对于稳定的GNNs至关重要。为此，我们提出了一个名为StableGNN的通用因果表示框架。主要思想是首先从图数据中提取高级表示，然后借助因果推断的区分能力来帮助模型获得稳定的预测效果。

    arXiv:2111.10657v3 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are proposed without considering the agnostic distribution shifts between training and testing graphs, inducing the degeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD) settings. The fundamental reason for such degeneration is that most GNNs are developed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. However, such spurious correlations may change in testing environments, leading to the failure of GNNs. Therefore, eliminating the impact of spurious correlations is crucial for stable GNNs. To this end, we propose a general causal representation framework, called StableGNN. The main idea is to extract high-level representations from graph data first and resort to the distinguishing ability of causal inference to help the model get ri
    
[^327]: Pareto-wise Ranking Classifier for Multi-objective Evolutionary Neural Architecture Search

    Pareto-wise Ranking Classifier for Multi-objective Evolutionary Neural Architecture Search

    [https://arxiv.org/abs/2109.07582](https://arxiv.org/abs/2109.07582)

    本研究的主要贡献是通过将复杂的多目标NAS任务转化为简单的Pareto占优分类任务，提出了一种基于分类的Pareto进化方法，从而简化了NAS的搜索过程

    

    在部署深度神经模型时，如何有效地自动找到符合不同设计目标的深度模型是至关重要的。大多数现有的神经结构搜索（NAS）方法利用替代模型在搜索过程中预测候选架构的详细性能（例如准确性和模型大小），然而这种方法复杂且低效。相反，我们旨在通过学习一个高效的Pareto分类器，将复杂的多目标NAS任务转化为简单的Pareto占优分类任务，以简化NAS的搜索过程。为此，我们提出了一种基于分类的Pareto进化方法，用于一次性NAS，在这种方法中，一个在线分类器被训练来预测候选架构与构建的参考架构之间的占优关系，而不是使用替代模型来拟合目标函数。

    arXiv:2109.07582v2 Announce Type: replace  Abstract: In the deployment of deep neural models, how to effectively and automatically find feasible deep models under diverse design objectives is fundamental. Most existing neural architecture search (NAS) methods utilize surrogates to predict the detailed performance (e.g., accuracy and model size) of a candidate architecture during the search, which however is complicated and inefficient. In contrast, we aim to learn an efficient Pareto classifier to simplify the search process of NAS by transforming the complex multi-objective NAS task into a simple Pareto-dominance classification task. To this end, we propose a classification-wise Pareto evolution approach for one-shot NAS, where an online classifier is trained to predict the dominance relationship between the candidate and constructed reference architectures, instead of using surrogates to fit the objective functions. The main contribution of this study is to change supernet adaption i
    
[^328]: 针对未知零和随机博弈的贝叶斯学习算法及其任意对手

    A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with an Arbitrary Opponent

    [https://arxiv.org/abs/2109.03396](https://arxiv.org/abs/2109.03396)

    本文提出了一种针对无限时间跨度的零和随机博弈的在线学习算法，该算法在具有平均奖励准则的情况下实现了贝叶斯遗憾界$O(HS\sqrt{AT})$。

    

    在本文中，我们提出了后验采样强化学习零和随机博弈（PSRL-ZSG）算法，这是第一个在线学习算法，在具有平均奖励准则的无限时间跨度的零和随机博弈中实现了贝叶斯遗憾界为$O(HS\sqrt{AT})$。其中$H$是偏差函数跨度的上界，$S$是状态数量，$A$是联合动作数量，$T$是时间跨度。我们考虑对手无法控制且可以采取任意任意与历史相关的策略的在线设置。我们的遗憾界改进了魏等人(2017)在相同假设下的最佳遗憾界$O(\sqrt[3]{DS^2AT^2})$，并且与在$T$上的理论下界相匹配。

    arXiv:2109.03396v2 Announce Type: replace  Abstract: In this paper, we propose Posterior Sampling Reinforcement Learning for Zero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that achieves Bayesian regret bound of $O(HS\sqrt{AT})$ in the infinite-horizon zero-sum stochastic games with average-reward criterion. Here $H$ is an upper bound on the span of the bias function, $S$ is the number of states, $A$ is the number of joint actions and $T$ is the horizon. We consider the online setting where the opponent can not be controlled and can take any arbitrary time-adaptive history-dependent strategy. Our regret bound improves on the best existing regret bound of $O(\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the same assumption and matches the theoretical lower bound in $T$.
    
[^329]: 上限反事实置信区间：一种面向上下文赌博的新乐观原则

    Upper Counterfactual Confidence Bounds: a New Optimism Principle for Contextual Bandits

    [https://arxiv.org/abs/2007.07876](https://arxiv.org/abs/2007.07876)

    本文提出的“上限反事实置信区间”（UCCB）是针对一般上下文赌博设计乐观算法的新原则，通过在策略空间中构建置信区间，而非像UCB那样在行动空间中，这使得算法在处理一般函数类和大上下文空间时均具有优越性。

    

    乐观原则面对不确定性是多臂赌博和强化学习中最广泛使用和成功的理念之一。然而，现有的乐观算法（主要是UCB及其变种）通常无法处理一般的函数类和大的上下文空间。本文研究了具有离线回归预言机的一般上下文赌博，并提出了一种简单的通用原则来设计乐观算法，称为“上限反事实置信区间”（UCCB）。UCCB的关键创新是在策略空间中建立置信区间，而不是像UCB那样在行动空间中。我们证明了这些算法在处理一般函数类和大上下文空间方面既是可证明的最优的，又在计算上是高效的。此外，我们证明了UCCB原则可以轻松地扩展到无限动作的一般上下文赌博，并提供了第一个解决方案。

    arXiv:2007.07876v4 Announce Type: replace  Abstract: The principle of optimism in the face of uncertainty is one of the most widely used and successful ideas in multi-armed bandits and reinforcement learning. However, existing optimistic algorithms (primarily UCB and its variants) often struggle to deal with general function classes and large context spaces. In this paper, we study general contextual bandits with an offline regression oracle and propose a simple, generic principle to design optimistic algorithms, dubbed "Upper Counterfactual Confidence Bounds" (UCCB). The key innovation of UCCB is building confidence bounds in policy space, rather than in action space as is done in UCB. We demonstrate that these algorithms are provably optimal and computationally efficient in handling general function classes and large context spaces. Furthermore, we illustrate that the UCCB principle can be seamlessly extended to infinite-action general contextual bandits, provide the first solutions 
    
[^330]: ENCORE：使用卷积神经机器翻译的集成学习进行自动程序修复

    ENCORE: Ensemble Learning using Convolution Neural Machine Translation for Automatic Program Repair

    [https://arxiv.org/abs/1906.08691](https://arxiv.org/abs/1906.08691)

    ENCORE提出了一种新的自动程序修复技术，使用卷积神经机器翻译的集成学习方法可以有效修复多种编程语言中的错误，超越了之前使用的LSTM方法，成功修复了42个错误，其中包括16个先前未被修复的错误。

    

    arXiv:1906.08691v2 通告类型: 交叉替换 摘要: 自动生成验证（G&V）程序修复技术通常依赖硬编码规则，仅修复遵循特定模式的错误，并且难以适应不同的编程语言。我们提出了ENCORE，一种新的G&V技术，它利用卷积神经机器翻译（NMT）模型的集成学习来自动修复多种编程语言中的错误。我们利用超参数调整的随机性构建多个模型，这些模型修复不同的错误，并使用集成学习将它们结合在一起。这种新的卷积NMT方法优于之前工作中使用的标准长短期记忆（LSTM）方法，因为它更好地捕捉了令牌之间的局部和长距离连接。我们在两个流行的基准测试集Defects4J和QuixBugs上的评估显示，ENCORE修复了42个错误，其中包括16个未被现有技术修复的错误。

    arXiv:1906.08691v2 Announce Type: replace-cross  Abstract: Automated generate-and-validate (G&V) program repair techniques typically rely on hard-coded rules, only fix bugs following specific patterns, and are hard to adapt to different programming languages. We propose ENCORE, a new G&V technique, which uses ensemble learning on convolutional neural machine translation (NMT) models to automatically fix bugs in multiple programming languages.   We take advantage of the randomness in hyper-parameter tuning to build multiple models that fix different bugs and combine them using ensemble learning. This new convolutional NMT approach outperforms the standard long short-term memory (LSTM) approach used in previous work, as it better captures both local and long-distance connections between tokens.   Our evaluation on two popular benchmarks, Defects4J and QuixBugs, shows that ENCORE fixed 42 bugs, including 16 that have not been fixed by existing techniques. In addition, ENCORE is the first 
    
[^331]: NeurAll: 面向自动驾驶的统一视觉感知模型

    NeurAll: Towards a Unified Visual Perception Model for Automated Driving

    [https://arxiv.org/abs/1902.03589](https://arxiv.org/abs/1902.03589)

    本文提出了一种联合多任务网络设计，以实现在自动驾驶中的视觉感知任务中共享计算资源，从而提高计算效率并提供更好的泛化能力。

    

    卷积神经网络（CNNs）被成功应用于重要的汽车视觉感知任务，包括目标识别、运动和深度估计、视觉SLAM等。然而，这些任务通常是独立探索和建模的。本文提出了一种联合多任务网络设计，同时学习多个任务。我们的主要动机是通过在所有任务之间共享昂贵的初始卷积层来实现计算效率。事实上，自动驾驶系统中的主要瓶颈是部署硬件上可用的有限处理能力。还有一些证据表明，对于某些任务来说，在提高准确性方面存在其他好处，并且可以减轻开发工作量。它还提供了可伸缩性，可以通过利用现有特征增加更多任务，并实现更好的泛化。我们调查了自动驾驶中用于视觉感知任务的各种基于CNN的解决方案。

    arXiv:1902.03589v3 Announce Type: replace-cross  Abstract: Convolutional Neural Networks (CNNs) are successfully used for the important automotive visual perception tasks including object recognition, motion and depth estimation, visual SLAM, etc. However, these tasks are typically independently explored and modeled. In this paper, we propose a joint multi-task network design for learning several tasks simultaneously. Our main motivation is the computational efficiency achieved by sharing the expensive initial convolutional layers between all tasks. Indeed, the main bottleneck in automated driving systems is the limited processing power available on deployment hardware. There is also some evidence for other benefits in improving accuracy for some tasks and easing development effort. It also offers scalability to add more tasks leveraging existing features and achieving better generalization. We survey various CNN based solutions for visual perception tasks in automated driving. Then we
    
[^332]: 一种用于双语词典归纳的判别式潜变量模型

    A Discriminative Latent-Variable Model for Bilingual Lexicon Induction

    [https://arxiv.org/abs/1808.09334](https://arxiv.org/abs/1808.09334)

    引入判别式潜变量模型，结合先前研究的词典先验和表示法，提出了用于双语词典归纳的新方法，并通过实验证据展示先验可以改善诱导的双语词典。

    

    我们引入了一种新颖的用于双语词典归纳的判别式潜变量模型。我们的模型将Haghighi等人（2008）的二分匹配词典先验与基于表示的方法（Artetxe等人，2017）相结合。为了训练模型，我们推导出了高效的Viterbi EM算法。我们在两个度量标准下对六种语言对进行了实证结果，并显示先验改善了诱导的双语词典。我们还演示了如何将先前的工作视为类似风格的潜变量模型，尽管有不同的先验。

    arXiv:1808.09334v3 Announce Type: replace  Abstract: We introduce a novel discriminative latent variable model for bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a representation-based approach (Artetxe et al., 2017). To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical results on six language pairs under two metrics and show that the prior improves the induced bilingual lexicons. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.
    
[^333]: 布拉格捷克技术大学关系学习资源库

    The CTU Prague Relational Learning Repository

    [https://arxiv.org/abs/1511.03086](https://arxiv.org/abs/1511.03086)

    支持机器学习研究使用多关系数据的布拉格捷克技术大学关系学习资源库，包含大量SQL数据库，并由getML提供支持。

    

    布拉格关系学习资源库的目的是支持具有多关系数据的机器学习研究。此资源库目前包含148个SQL数据库，托管在位于\url{https://relational-data.org}的公共MySQL服务器上。服务器由getML提供，以支持关系机器学习社区（\url{www.getml.com}）。可搜索的元数据库提供元数据（例如数据库中的表数量、表中的行列数、自关联数量）。

    arXiv:1511.03086v2 Announce Type: replace  Abstract: The aim of the Prague Relational Learning Repository is to support machine learning research with multi-relational data. The repository currently contains 148 SQL databases hosted on a public MySQL server located at \url{https://relational-data.org}. The server is provided by getML to support the relational machine learning community (\url{www.getml.com}). A searchable meta-database provides metadata (e.g., the number of tables in the database, the number of rows and columns in the tables, the number of self-relationships).
    
[^334]: 我们错过了谁？一种基于原则的揭示少数人群特征的方法

    Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population. (arXiv:2401.14512v1 [stat.ME])

    [http://arxiv.org/abs/2401.14512](http://arxiv.org/abs/2401.14512)

    本文提出了一种基于优化的方法，Rashomon Set of Optimal Trees (ROOT)，用于识别和描述随机对照试验中的少数人群。该方法通过最小化目标平均处理效应估计的方差来优化目标子群体分布，从而提供更精确和可解释的处理效应估计。与其他方法相比，该方法具有更高的精度和可解释性，通过合成数据实验进行了验证。

    

    随机对照试验在理解因果效应方面起到了关键作用，然而将推论扩展到目标人群时面临效应异质性和代表性不足的挑战。我们的论文解决了在随机对照试验中识别和描述少数人群的关键问题，提出了一种改进目标人群以提升普适性的创新框架。我们引入了一种基于优化的方法——Rashomon Set of Optimal Trees (ROOT)，来描述少数人群。ROOT通过最小化目标平均处理效应估计的方差来优化目标子群体分布，从而确保更精确的处理效应估计。值得注意的是，ROOT生成可解释的少数人群特征，有助于研究人员有效沟通。我们的方法在精度和可解释性方面相对于其他方法展现了改进，通过合成数据实验进行了验证。

    Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We ap
    
[^335]: 真知来源于实践：通过强化学习使LLMs与具身环境对齐的方法研究

    True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])

    [http://arxiv.org/abs/2401.14151](http://arxiv.org/abs/2401.14151)

    本研究通过使用大型语言模型（LLMs）作为决策智能体，通过强化学习与具身环境高效互动来解决LLMs与环境之间知识不对齐的问题。通过查询LLMs的联合概率，形成行为策略，并通过两种归一化方法和四个提示设计原则提高策略的稳定性和鲁棒性。最后，通过设计参数高效的训练架构提高学习效率。

    

    尽管在众多任务中取得了令人印象深刻的表现，但大型语言模型（LLMs）在解决简单的决策任务上经常失败，原因是LLMs中的知识与环境不对齐。相反，强化学习（RL）智能体从零开始学习策略，这使得它们始终与环境保持一致，但难以将先前的知识整合到其中以进行有效的探索。为了缩小这一差距，我们提出了TWOSOME，一种新颖的在线框架，利用LLMs作为决策智能体，通过RL与具身环境高效互动并实现对齐，而无需任何准备好的数据集或环境的先前知识。首先，我们使用LLMs查询每个有效动作的联合概率以形成行为策略。然后，为了增强策略的稳定性和鲁棒性，我们提出了两种归一化方法，并总结了四个提示设计原则。最后，我们设计了一种新颖的参数高效的训练架构，其中包括一个行为评估和选择算法来提高学习效率。

    Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the acto
    
[^336]: 组合式生成逆设计

    Compositional Generative Inverse Design. (arXiv:2401.13171v1 [cs.LG])

    [http://arxiv.org/abs/2401.13171](http://arxiv.org/abs/2401.13171)

    逆向设计起到优化底层目标函数的作用，最近的研究利用了学习的动力学模型进行优化。通过优化扩散模型捕获的学习能量函数，可以避免对抗示例，并显著提高设计性能。这一设计系统是组合性的，使得可以设计具有每个指定组件的系统。

    

    逆设计是一种寻求设计输入变量以优化底层目标函数的重要问题，在机械工程到航天工程等领域都有应用。逆设计通常被构建成一个优化问题，最近的研究利用了学习的动力学模型进行优化。然而，由于模型的优化往往会陷入对抗模式，阻碍有效的抽样。我们证明，通过优化扩散模型捕获的学习能量函数，我们可以避免这种对抗性示例，并显著提高设计性能。我们进一步展示了这样一个设计系统是组合性的，使我们能够结合多个不同的扩散模型来代表所需系统的子组件，从而设计具有每个指定组件的系统。在一个N体相互作用任务和一个具有挑战性的二维多翼型设计任务中，我们证明通过组合学习的能量函数，可以实现更好的设计性能。

    Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent works leveraging optimization across learned dynamics models. However, as models are optimized they tend to fall into adversarial modes, preventing effective sampling. We illustrate that by instead optimizing over the learned energy function captured by the diffusion model, we can avoid such adversarial examples and significantly improve design performance. We further illustrate how such a design system is compositional, enabling us to combine multiple different diffusion models representing subcomponents of our desired system to design systems with every specified component. In an N-body interaction task and a challenging 2D multi-airfoil design task, we demonstrate that by composing the learn
    
[^337]: 评估女性运动员脑震荡：神经信息学的作用？

    Assessment of Sports Concussion in Female Athletes: A Role for Neuroinformatics?. (arXiv:2401.13045v1 [stat.ML])

    [http://arxiv.org/abs/2401.13045](http://arxiv.org/abs/2401.13045)

    该论文提出了通过神经信息学和机器学习来评估女性运动员脑震荡的方法。相比传统的临床方法，在女性运动员中诊断脑震荡存在一些局限性，而这些新技术可以通过数据分析找出与性别相关的生物机制，从而填补这一差距。

    

    在过去的十年中，女性运动员脑震荡的复杂性变得明显。传统的临床诊断脑震荡的方法在应用于女性运动员时存在局限性，往往无法捕捉到脑结构和功能的细微变化。先进的神经信息学技术和机器学习模型在这方面已经成为宝贵的资产。虽然这些技术在理解男性运动员的脑震荡方面已经被广泛应用，但在我们对于它们对女性运动员的有效性的理解上仍存在显著差距。通过利用机器学习的强大数据分析能力，研究人员可以将观察到的表型神经影像数据联系到特定于性别的生物机制，揭示女性运动员脑震荡的奥秘。此外，嵌入机器学习的方法还可以在研究中进行交叉验证，进一步检验性别差异。

    Over the past decade, the intricacies of sports-related concussions among female athletes have become readily apparent. Traditional clinical methods for diagnosing concussions suffer limitations when applied to female athletes, often failing to capture subtle changes in brain structure and function. Advanced neuroinformatics techniques and machine learning models have become invaluable assets in this endeavor. While these technologies have been extensively employed in understanding concussion in male athletes, there remains a significant gap in our comprehension of their effectiveness for female athletes. With its remarkable data analysis capacity, machine learning offers a promising avenue to bridge this deficit. By harnessing the power of machine learning, researchers can link observed phenotypic neuroimaging data to sex-specific biological mechanisms, unraveling the mysteries of concussions in female athletes. Furthermore, embedding methods within machine learning enable examining b
    
[^338]: 实现可扩展和稳健的模型版本控制

    Towards Scalable and Robust Model Versioning. (arXiv:2401.09574v1 [cs.LG])

    [http://arxiv.org/abs/2401.09574](http://arxiv.org/abs/2401.09574)

    本文探讨了在不获取新的训练数据或更改模型架构的情况下，生成具有不同攻击属性的多个模型版本的可行性，以保护模型所有者免受恶意入侵带来的损失。

    

    随着深度学习模型在各行各业的不断部署，针对这些部署模型进行恶意入侵的威胁也在增加。如果攻击者能够通过服务器入侵、内部攻击或模型反转技术获取部署模型的访问权限，他们可以构造白盒对抗攻击来操纵模型的分类结果，从而给依赖这些模型进行关键任务的组织带来重大风险。模型所有者需要一种机制来保护自己免受这种损失，而不需要获取新的训练数据，这通常需要大量的时间和资本投入。，在本文中，我们探讨了在不获取新的训练数据或更改模型架构的情况下，生成具有不同攻击属性的多个模型版本的可行性。模型所有者可以逐个部署版本并立即替换泄露的版本。

    As the deployment of deep learning models continues to expand across industries, the threat of malicious incursions aimed at gaining access to these deployed models is on the rise. Should an attacker gain access to a deployed model, whether through server breaches, insider attacks, or model inversion techniques, they can then construct white-box adversarial attacks to manipulate the model's classification outcomes, thereby posing significant risks to organizations that rely on these models for critical tasks. Model owners need mechanisms to protect themselves against such losses without the necessity of acquiring fresh training data - a process that typically demands substantial investments in time and capital.  In this paper, we explore the feasibility of generating multiple versions of a model that possess different attack properties, without acquiring new training data or changing model architecture. The model owner can deploy one version at a time and replace a leaked version immed
    
[^339]: 通过保守密度估计从稀疏离线数据集中学习

    Learning from Sparse Offline Datasets via Conservative Density Estimation. (arXiv:2401.08819v1 [cs.LG])

    [http://arxiv.org/abs/2401.08819](http://arxiv.org/abs/2401.08819)

    本文提出了一种名为保守密度估计（CDE）的训练算法，通过明确约束状态-行为占据稳态分布来解决离线强化学习中的外推错误问题。在稀疏奖励或不足数据的任务中，CDE显示出明显优于基准方法的性能。

    

    离线强化学习（RL）为从预先收集的数据集中学习策略提供了一种有前景的方向，而无需与环境进一步交互。然而，现有的方法在处理分布外（OOD）外推错误方面存在困难，特别是在稀疏奖励或数据稀缺的情况下。在本文中，我们提出了一种名为保守密度估计（CDE）的新的训练算法，通过明确约束状态-行为占据稳态分布来解决这个挑战。CDE通过解决边际重要性抽样中的支持不匹配问题，克服了现有方法的局限性，如稳态分布校正方法。我们的方法在D4RL基准测试中实现了最先进的性能。值得注意的是，CDE在具有稀疏奖励或不足数据的挑战性任务中持续优于基准方法，证明了我们的方法在解决外推错误问题上的优势。

    Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation (CDE), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. CDE overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. Our method achieves state-of-the-art performance on the D4RL benchmark. Notably, CDE consistently outperforms baselines in challenging tasks with sparse rewards or insufficient data, demonstrating the advantages of our approach in addressing the extrapolation error problem i
    
[^340]: 使用营销组合建模（MMM）和Shapley值回归量化渠道合作伙伴层面的营销绩效

    Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression. (arXiv:2401.05653v1 [cs.LG])

    [http://arxiv.org/abs/2401.05653](http://arxiv.org/abs/2401.05653)

    本文研究了使用Shapley值回归对渠道合作伙伴层面的营销绩效进行量化，并通过与营销组合建模进行比较，证明了Shapley值回归的实用性。同时提出了一种简单的方法来计算调整系数。

    

    本文探索了在渠道合作伙伴层面利用Shapley值回归来解析营销绩效的应用，补充了渠道层面的营销组合建模（MMM）。利用来自金融服务行业的真实数据，我们展示了Shapley值回归在评估个别合作伙伴贡献方面的实用性。尽管结构化的现场测试以及合作博弈理论最为准确，但经常会非常复杂和昂贵。因此，Shapley值回归是一种更可行的方法，可以分离营销渠道中每个营销合作伙伴的影响。我们还提出了一种简单的方法来推导调整系数，将其与其他方法进行比较。

    This paper explores the application of Shapley Value Regression in dissecting marketing performance at channel-partner level, complementing channel-level Marketing Mix Modeling (MMM). Utilizing real-world data from the financial services industry, we demonstrate the practicality of Shapley Value Regression in evaluating individual partner contributions. Although structured in-field testing along with cooperative game theory is most accurate, it can often be highly complex and expensive to conduct. Shapley Value Regression is thus a more feasible approach to disentangle the influence of each marketing partner within a marketing channel. We also propose a simple method to derive adjusted coefficients of Shapley Value Regression and compares it with alternative approaches.
    
[^341]: 关于图神经网络的表达能力研究

    On the Expressive Power of Graph Neural Networks. (arXiv:2401.01626v1 [cs.LG])

    [http://arxiv.org/abs/2401.01626](http://arxiv.org/abs/2401.01626)

    研究人员对图神经网络的表达能力和设计架构进行了大量工作，以提高其在各领域任务中的性能。主要方法包括研究GNN的通用逼近性质和其在区分不同图之间的能力程度。

    

    过去几年来，图神经网络的研究引起了相当大的兴趣。通过将深度学习扩展到图结构化数据，GNN可以解决社会科学、化学和医学等领域的各种任务。GNN架构的发展主要集中在改进节点或图分类等任务的实证性性能。然而，最近的一系列工作则寻求找到具有理论特性的GNN架构，通过研究其表达能力并设计最大化这种表达能力的架构。虽然关于如何定义GNN的表达能力还没有共识，但可以从几个有很好动机的角度来看待。也许最自然的方法是研究GNN的通用逼近性质，就像MLP的这种性质一样得到了广泛的研究。另一个方向关注的是GNN在区分不同图之间的能力程度。

    The study of Graph Neural Networks has received considerable interest in the past few years. By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine. The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification. However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.  While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives. Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs. Another direction focuses on the extent to which GNNs can distinguish between different graph
    
[^342]: 具有固定预算的局部最优最佳臂识别算法

    Locally Optimal Best Arm Identification with a Fixed Budget. (arXiv:2310.19788v1 [math.ST])

    [http://arxiv.org/abs/2310.19788](http://arxiv.org/abs/2310.19788)

    该研究解决了识别具有最高预期效果的治疗方案的问题，并提出了具有固定预算的局部最优算法来降低错误识别的概率。

    

    本研究探讨了识别最佳治疗方案的问题，即具有最高预期效果的治疗方案。我们旨在通过降低错误识别的概率来确定最佳治疗方案，这一问题在许多研究领域中已被探索，包括最佳臂识别（Best Arm Identification，BAI）和序列优化。在我们的实验中，治疗分配的轮数是固定的。在每一轮中，决策者将一种治疗方案分配给一个实验单元，并观察相应的结果，该结果遵循不同治疗方案之间方差不同的高斯分布。在实验结束时，我们根据观察结果推荐一种治疗方案作为最佳治疗方案的估计值。决策者的目标是设计一个实验，使错误识别最佳治疗方案的概率最小化。基于这一目标，我们开发了误识别概率的下界。

    This study investigates the problem of identifying the best treatment arm, a treatment arm with the highest expected outcome. We aim to identify the best treatment arm with a lower probability of misidentification, which has been explored under various names across numerous research fields, including \emph{best arm identification} (BAI) and ordinal optimization. In our experiments, the number of treatment-allocation rounds is fixed. In each round, a decision-maker allocates a treatment arm to an experimental unit and observes a corresponding outcome, which follows a Gaussian distribution with a variance different among treatment arms. At the end of the experiment, we recommend one of the treatment arms as an estimate of the best treatment arm based on the observations. The objective of the decision-maker is to design an experiment that minimizes the probability of misidentifying the best treatment arm. With this objective in mind, we develop lower bounds for the probability of misident
    
[^343]: 通过遗憾到置信集转换改进（多项式）逻辑回归赌博机的遗憾界限

    Improved Regret Bounds of (Multinomial) Logistic Bandits via Regret-to-Confidence-Set Conversion. (arXiv:2310.18554v1 [stat.ML])

    [http://arxiv.org/abs/2310.18554](http://arxiv.org/abs/2310.18554)

    本论文通过遗憾到置信集转换方法改进了逻辑回归赌博机的遗憾界限，提出了一个基于在线学习算法的凸置信集，并应用于具有新的鞅集中步骤的遗憾分析。

    

    逻辑回归赌博机是建模用户选择的普遍框架，例如广告推荐系统中的点击与否。我们观察到先前的工作忽视或忽略了$S \geq \lVert \theta_\star \rVert_2$中的依赖关系，其中$\theta_\star \in \mathbb{R}^d$是未知的参数向量，当$S$较大时，例如$S \geq d$，这会产生问题。在这项工作中，我们通过一种称为“遗憾到置信集转换（R2CS）”的新方法改善了对$S$的依赖关系，该方法允许我们构建一个基于在线学习算法存在性的凸置信集。使用R2CS，我们在逻辑回归赌博机的遗憾界限方面获得了严格的改进，同时保持了计算可行性和对其他因素（如$d$和$T$）的依赖。我们将我们的新置信集应用于具有新的鞅集中步骤的逻辑回归赌博机的遗憾分析，从而避免了额外的因素。

    Logistic bandit is a ubiquitous framework of modeling users' choices, e.g., click vs. no click for advertisement recommender system. We observe that the prior works overlook or neglect dependencies in $S \geq \lVert \theta_\star \rVert_2$, where $\theta_\star \in \mathbb{R}^d$ is the unknown parameter vector, which is particularly problematic when $S$ is large, e.g., $S \geq d$. In this work, we improve the dependency on $S$ via a novel approach called {\it regret-to-confidence set conversion (R2CS)}, which allows us to construct a convex confidence set based on only the \textit{existence} of an online learning algorithm with a regret guarantee. Using R2CS, we obtain a strict improvement in the regret bound w.r.t. $S$ in logistic bandits while retaining computational feasibility and the dependence on other factors such as $d$ and $T$. We apply our new confidence set to the regret analyses of logistic bandits with a new martingale concentration step that circumvents an additional factor
    
[^344]: 使用大型语言模型进行文本属性图的解缠表征学习

    Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])

    [http://arxiv.org/abs/2310.18152](http://arxiv.org/abs/2310.18152)

    本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。

    

    文本属性图（TAGs）在网络上非常常见，对于该类图，如引用网络、电子商务网络和社交网络的研究在网络社区中引起了相当大的关注。最近，大型语言模型（LLMs）在各种任务上展示了出色的能力。然而，现有的工作仅仅依靠提示信息来传达图结构信息给LLMs，因此对于TAGs中复杂的结构关系了解不足。为解决这个问题，本文提出了解缠图文学习器（DGTL）模型，能够增强LLMs对TAGs的推理和预测能力。我们的DGTL模型通过定制的解缠图神经网络（GNN）层将图结构信息纳入其中，使得LLMs能够捕捉多个结构因素中隐藏的复杂关系。

    Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
    
[^345]: 使用合成数据扩展提升数据分析

    Boosting Data Analytics With Synthetic Volume Expansion. (arXiv:2310.17848v1 [stat.ML])

    [http://arxiv.org/abs/2310.17848](http://arxiv.org/abs/2310.17848)

    本文介绍了一种利用合成数据生成框架来提升数据分析的方法，在此方法中，使用先进模型生成高逼真度的合成数据，并采用统计方法进行分析。研究发现，在合成数据上的统计方法错误随着合成数据的增加而减少，但最终可能会增加或停滞。

    

    合成数据生成作为生成式人工智能的基石，在解决数据稀缺和隐私问题的同时，实现了前所未有的性能。随着合成数据的日益重要，人们开始关注统计方法在合成数据与原始数据上的准确性。在本文中，我们介绍了用于分析的合成数据生成框架。该框架使用高逼真度的合成数据，通过先进模型如表格扩散和生成式预训练转换器模型生成，并结合相关研究洞察进一步增强。在这个框架中的一个重要发现是生成效应：统计方法在合成数据上的错误随着合成数据的增加一开始减少，但最终可能会增加或停滞。这个现象根源于复制原始数据分布的复杂性。

    Synthetic data generation, a cornerstone of Generative Artificial Intelligence, signifies a paradigm shift in data science by addressing data scarcity and privacy while enabling unprecedented performance. As synthetic data gains prominence, questions arise concerning the accuracy of statistical methods when applied to synthetic data compared to raw data. In this article, we introduce the Synthetic Data Generation for Analytics framework. This framework employs statistical methods on high-fidelity synthetic data generated by advanced models such as tabular diffusion and Generative Pre-trained Transformer models. These models, trained on raw data, are further enhanced with insights from pertinent studies. A significant discovery within this framework is the generational effect: the error of a statistical method on synthetic data initially diminishes with added synthetic data but may eventually increase or plateau. This phenomenon, rooted in the complexities of replicating raw data distri
    
[^346]: 最大熵损失：针对超出分布转换的校准的约束最大熵方法

    MaxEnt Loss: Constrained Maximum Entropy for Calibration under Out-of-Distribution Shift. (arXiv:2310.17159v1 [cs.LG])

    [http://arxiv.org/abs/2310.17159](http://arxiv.org/abs/2310.17159)

    本论文提出了一种新的损失函数，用于解决超出分布转换的校准问题。该方法基于最大熵原理，在训练过程中引入统计约束，以提供更好的模型校准效果，同时不牺牲准确性。实验证明该方法在合成和真实世界的基准上实现了最先进的校准效果。

    

    我们提出了一种解决超出分布转换校准问题的新的损失函数。虽然有很多目标函数被提出来有效地在分布内校准模型，我们的研究发现它们在超出分布的情况下表现并不好。基于最大熵原理，我们在训练过程中引入有用的统计约束，以在不牺牲准确性的情况下提供更好的模型校准。我们提供了理论分析并通过实验证明我们的方法在实践中表现良好，在合成和真实世界的基准上实现了最先进的校准效果。

    We present a new loss function that addresses the out-of-distribution (OOD) calibration problem. While many objective functions have been proposed to effectively calibrate models in-distribution, our findings show that they do not always fare well OOD. Based on the Principle of Maximum Entropy, we incorporate helpful statistical constraints observed during training, delivering better model calibration without sacrificing accuracy. We provide theoretical analysis and show empirically that our method works well in practice, achieving state-of-the-art calibration on both synthetic and real-world benchmarks.
    
[^347]: 大规模高斯过程通过交替投影

    Large-Scale Gaussian Processes via Alternating Projection. (arXiv:2310.17137v1 [cs.LG])

    [http://arxiv.org/abs/2310.17137](http://arxiv.org/abs/2310.17137)

    本论文提出了一种通过交替投影的迭代方法来解决高斯过程在大规模数据集上的训练问题，并证明了该方法具有线性收敛性。

    

    高斯过程（GP）超参数优化需要反复求解具有 nxn 核矩阵的线性系统。为了解决 O(n^3) 的时间复杂性问题，最近的研究采用了快速迭代数值方法，如共轭梯度（CG）。然而，随着数据集规模的增加，相应的核矩阵变得越来越病态，并且在没有分割的情况下仍然需要 O(n^2) 的空间。因此，虽然 CG 增加了可训练 GP 基于的数据集的大小，但现代数据集已经达到超出其适用范围的规模。在这项工作中，我们提出了一种只访问核矩阵的子块的迭代方法，有效地实现了小批量处理。我们的算法基于交替投影，每次迭代的时间和空间复杂度为 O(n)，解决了将 GP 扩展到非常大的数据集时的许多实际挑战。从理论上讲，我们证明了我们的方法具有线性收敛性，从实证的角度来看，我们证明了

    Gaussian process (GP) hyperparameter optimization requires repeatedly solving linear systems with $n \times n$ kernel matrices. To address the prohibitive $\mathcal{O}(n^3)$ time complexity, recent work has employed fast iterative numerical methods, like conjugate gradients (CG). However, as datasets increase in magnitude, the corresponding kernel matrices become increasingly ill-conditioned and still require $\mathcal{O}(n^2)$ space without partitioning. Thus, while CG increases the size of datasets GPs can be trained on, modern datasets reach scales beyond its applicability. In this work, we propose an iterative method which only accesses subblocks of the kernel matrix, effectively enabling \emph{mini-batching}. Our algorithm, based on alternating projection, has $\mathcal{O}(n)$ per-iteration time and space complexity, solving many of the practical challenges of scaling GPs to very large datasets. Theoretically, we prove our method enjoys linear convergence and empirically we demons
    
[^348]: 检测大型语言模型的预训练数据

    Detecting Pretraining Data from Large Language Models. (arXiv:2310.16789v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.16789](http://arxiv.org/abs/2310.16789)

    这项研究探讨了如何检测大型语言模型的预训练数据，提出了一个动态基准和一种新的检测方法，以解决数据隐私和不透明性的问题。

    

    虽然大型语言模型（LLM）被广泛应用，但用于训练它们的数据很少被公开。考虑到这些数据的规模之大，可能包含受版权保护的材料、个人可识别信息以及用于广泛报道的参考基准测试数据，我们几乎可以肯定它们包含了潜在的问题文本。然而，我们目前无法知道这些文本中包含了哪些类型的数据以及比例。在本文中，我们研究了预训练数据检测问题：在不知道预训练数据的情况下，给定一段文本和对LLM的黑盒访问，我们能否确定模型是否是在提供的文本上进行了训练？为了方便这项研究，我们引入了一个动态基准WIKIMIA，使用在模型训练之前和之后创建的数据来支持金标准检测。我们还引入了一种新的检测方法Min-K% Prob，基于一个简单的假设：一个未见过的例子可能包含几个具有较低概率的离群词。

    Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low pro
    
[^349]: DeepFDR：一种用于神经影像数据的基于深度学习的虚警控制方法

    DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data. (arXiv:2310.13349v1 [stat.ML])

    [http://arxiv.org/abs/2310.13349](http://arxiv.org/abs/2310.13349)

    DeepFDR是一种基于深度学习的虚警控制方法，通过利用无监督的图像分割技术解决神经影像数据中的多重检验问题，并在实验证明其相对于现有方法具有卓越的性能。

    

    基于体素的多重检验在神经影像数据分析中广泛应用。传统的虚警控制方法常常忽视基于体素的检验之间的空间相关性，从而导致测试能力的大幅损失。虽然最近出现了一些空间虚警控制方法，但是当处理复杂的脑空间依赖关系时，它们的有效性和最优性仍存在疑问。与此同时，深度学习方法已经在图像分割方面取得了革命性的进展，而图像分割与基于体素的多重检验密切相关。本文提出了一种名为DeepFDR的新型空间虚警控制方法，利用无监督的基于深度学习的图像分割来解决基于体素的多重检验问题。包括全面的模拟和阿尔茨海默病FDG-PET影像分析在内的数值研究表明DeepFDR相对于现有方法具有优势。DeepFDR不仅在虚警控制方面表现出色，还有效降低了虚假的非发现率。

    Voxel-based multiple testing is widely used in neuroimaging data analysis. Traditional false discovery rate (FDR) control methods often ignore the spatial dependence among the voxel-based tests and thus suffer from substantial loss of testing power. While recent spatial FDR control methods have emerged, their validity and optimality remain questionable when handling the complex spatial dependencies of the brain. Concurrently, deep learning methods have revolutionized image segmentation, a task closely related to voxel-based multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR control method that leverages unsupervised deep learning-based image segmentation to address the voxel-based multiple testing problem. Numerical studies, including comprehensive simulations and Alzheimer's disease FDG-PET image analysis, demonstrate DeepFDR's superiority over existing methods. DeepFDR not only excels in FDR control and effectively diminishes the false nondiscovery rate, but als
    
[^350]: CycleNet：重新思考文本引导扩散中的循环一致性，以进行图像操作

    CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation. (arXiv:2310.13165v1 [cs.CV])

    [http://arxiv.org/abs/2310.13165](http://arxiv.org/abs/2310.13165)

    CycleNet是一种将循环一致性引入扩散模型的新方法，用于规范图像操作，具有优越的翻译一致性和质量，并且可以生成高质量的跨领域分布图像。

    

    扩散模型（DM）在图像合成任务中取得了突破，但缺乏一种直观的一致图像到图像（I2I）翻译接口。为解决这个问题，已经探索了各种方法，包括基于掩码的方法，基于注意力的方法和基于图像的方法。然而，如何使用预训练的DMs进行无配对的I2I翻译并保持一致性仍然是一个关键挑战。本文介绍了Cyclenet，一种新颖但简单的方法，它将循环一致性纳入DMs中，以规范图像操作。我们验证了Cyclenet在不同粒度的无配对I2I任务上的优势。除了场景和对象级别的翻译，我们还贡献了一个多领域I2I翻译数据集，用于研究物体的物理状态变化。我们的实证研究表明，Cyclenet在翻译的一致性和质量方面具有优势，并且在改变文本描述时可以生成高质量的跨领域分布图像。

    Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual 
    
[^351]: 构建具有多样数据损坏情况下鲁棒性的离线强化学习

    Towards Robust Offline Reinforcement Learning under Diverse Data Corruption. (arXiv:2310.12955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.12955](http://arxiv.org/abs/2310.12955)

    本文研究了在多种数据损坏情况下，离线强化学习算法的性能。研究发现，隐式Q-learning（IQL）在各种离线强化学习算法中展现出了较强的鲁棒性能，其采用的监督策略学习方案为关键。然而，在动力学损坏下，IQL仍然存在Q函数的重尾目标问题。

    

    离线强化学习（RL）是一种有前途的方法，可以从离线数据集中学习强化策略，而无需与环境进行昂贵或不安全的交互。然而，人们在真实环境中收集的数据集往往存在噪声，甚至可能被恶意损坏，这可能会严重影响离线强化学习的性能。本研究首先对当前离线强化学习算法在包括状态、动作、奖励和动力学在内的全面数据损坏情况下的性能进行了调查。我们的大量实验显示，隐式Q-learning（IQL）在各种离线强化学习算法中表现出了可靠的抗数据损坏能力。此外，我们还进行了经验和理论分析，以了解IQL的鲁棒性能，并将其监督策略学习方案确定为关键因素。尽管相对鲁棒，但IQL在动力学损坏下仍然存在Q函数的重尾目标问题。

    Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tack
    
[^352]: 从过去学习：基于代理的对抗性防御框架来增强鲁棒性

    Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness. (arXiv:2310.12713v1 [cs.LG])

    [http://arxiv.org/abs/2310.12713](http://arxiv.org/abs/2310.12713)

    本文提出了一个基于代理的对抗性防御框架，通过引入目标模型的历史状态作为代理来增强模型对各种对抗攻击的鲁棒性。

    

    鉴于深度学习模型对对抗样本的脆弱性及其带来的安全问题，一系列方法，包括突出代表性的对抗训练（AT），旨在增强模型对各种对抗攻击的鲁棒性，得到了快速发展。然而，现有方法主要是通过明确或隐性的计算负担帮助目标模型的当前状态来防御面向参数的对抗攻击，同时由于优化轨迹不一致而导致不稳定的收敛行为。与以往的工作不同，本文重新考虑了目标模型的更新规则及其当前状态下的防御不足。通过引入目标模型的历史状态作为代理，并赋予其用于防御的先验信息，我们制定了一个两阶段的更新规则，从而得到一个通用的对抗性防御框架，我们称之为"LAST"。

    In light of the vulnerability of deep learning models to adversarial samples and the ensuing security issues, a range of methods, including Adversarial Training (AT) as a prominent representative, aimed at enhancing model robustness against various adversarial attacks, have seen rapid development. However, existing methods essentially assist the current state of target model to defend against parameter-oriented adversarial attacks with explicit or implicit computation burdens, which also suffers from unstable convergence behavior due to inconsistency of optimization trajectories. Diverging from previous work, this paper reconsiders the update rule of target model and corresponding deficiency to defend based on its current state. By introducing the historical state of the target model as a proxy, which is endowed with much prior information for defense, we formulate a two-stage update rule, resulting in a general adversarial defense framework, which we refer to as `LAST' ({\bf L}earn fr
    
[^353]: 超越文档边界的上下文预训练：语言模型

    In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10638](http://arxiv.org/abs/2310.10638)

    本论文提出了一种超越文档边界的上下文预训练方法，通过在相关文档序列上训练语言模型，鼓励模型进行跨文档的阅读和推理。该方法通过改变文档顺序并应用现有的预训练管道来实现。

    

    目前，大型语言模型（LMs）通过预测给定文档前缀的标记来进行训练，从而能够直接进行长篇生成和提示式任务，这可以简化为文档完成。现有的预训练管道通过连接随机组合的短文档来训练LMs，以创建输入上下文，但前一个文档对于预测下一个文档没有提供任何信号。我们提出了一种新方法——上下文预训练，即在相关文档序列上预先训练语言模型，从而明确鼓励它们跨越文档边界进行阅读和推理。我们可以通过改变文档顺序，使每个上下文包含相关的文档，并直接应用现有的预训练管道来进行上下文预训练。然而，这个文档排序问题很具有挑战性。有数十亿个文档，我们希望在每个文档中最大化上下文相似性而不重复任何数据。

    Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
    
[^354]: SiamAF: 学习心电图和光电脉搏图信号的共享信息用于强健的心房颤动检测

    SiamAF: Learning Shared Information from ECG and PPG Signals for Robust Atrial Fibrillation Detection. (arXiv:2310.09203v1 [cs.LG])

    [http://arxiv.org/abs/2310.09203](http://arxiv.org/abs/2310.09203)

    提出了一种名为SiamAF的新方法，利用心电图和光电脉搏图信号的共享信息，通过Siamese网络和联合学习实现强健的心房颤动（AF）检测。

    

    心房颤动（AF）是最常见的心脏心律失常类型，与中风、心力衰竭和其他心血管并发症的风险增加有关，但可以临床上无声。佩戴式设备进行被动性的AF监测可能有助于减少与AF相关的不良临床结果。在嘈杂的佩戴式数据中检测AF面临重大挑战，引发了各种不同的深度学习技术。先前的深度学习模型从单一形态学习，要么是心电图（ECG），要么是光电脉搏图（PPG）信号。然而，深度学习模型往往难以学习可泛化的特征，并依赖于更容易受到噪声损坏的特征，在某些场景中导致次优的性能，特别是在低质量信号的情况下。鉴于佩戴式设备和床边监护仪上ECG和PPG信号配对的日益丰富，我们提出了一种新的方法SiamAF，利用一种新颖的Siamese网络结构和联合学习的方法。

    Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. It is associated with an increased risk of stroke, heart failure, and other cardiovascular complications, but can be clinically silent. Passive AF monitoring with wearables may help reduce adverse clinical outcomes related to AF. Detecting AF in noisy wearable data poses a significant challenge, leading to the emergence of various deep learning techniques. Previous deep learning models learn from a single modality, either electrocardiogram (ECG) or photoplethysmography (PPG) signals. However, deep learning models often struggle to learn generalizable features and rely on features that are more susceptible to corruption from noise, leading to sub-optimal performances in certain scenarios, especially with low-quality signals. Given the increasing availability of ECG and PPG signal pairs from wearables and bedside monitors, we propose a new approach, SiamAF, leveraging a novel Siamese network architecture and joint le
    
[^355]: METRA:具有度量感知抽象的可扩展无监督强化学习

    METRA: Scalable Unsupervised RL with Metric-Aware Abstraction. (arXiv:2310.08887v1 [cs.LG])

    [http://arxiv.org/abs/2310.08887](http://arxiv.org/abs/2310.08887)

    METRA提出了一种新的无监督强化学习目标，旨在使其在复杂的高维环境中可扩展。这个目标解决了纯探索方法在大状态空间环境中的困难以及互信息技能学习方法中缺乏激励而无法探索环境的问题。

    

    无监督预训练策略在自然语言处理和计算机视觉领域证明了其高效性。同样，无监督强化学习（RL）有望发现各种潜在有用的行为，可以加速学习各种下游任务。然而，尽管之前的尝试，使无监督RL真正可扩展仍然是一个重大的挑战：在具有大状态空间的复杂环境中，纯探索方法可能会面临困难，因为覆盖每个可能的转换是不可行的；而互信息技能学习方法可能由于缺乏激励而完全无法探索环境。为了使无监督RL在复杂的高维环境中可扩展，我们提出了一种新的无监督RL目标，称为度量感知抽象（METRA）。

    Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Ou
    
[^356]: GraphControl:为图领域迁移学习中的通用图预训练模型添加条件控制

    GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning. (arXiv:2310.07365v1 [cs.LG])

    [http://arxiv.org/abs/2310.07365](http://arxiv.org/abs/2310.07365)

    在图领域迁移学习中，GraphControl通过添加条件控制实现了对通用图预训练模型的有效迁移，克服了不同图域间的属性语义差异问题。

    

    图结构化数据在世界中无处不在，这种数据模型了对象之间的复杂关系，为各种Web应用提供了可能。Web上每天涌现的无标签图数据为这些应用提供了巨大的潜力。图自监督算法在从丰富的无标签图数据中获得通用知识方面取得了显著成功。这些预训练模型可以应用于各种下游Web应用，节省训练时间，提高下游（目标）性能。然而，即使在表面上看起来相似的领域中，不同的图在属性语义方面也可能存在显着差异，这给将预训练模型迁移到下游任务中带来了困难，甚至是不可行性。具体而言，例如，下游任务中的附加特定任务节点信息（特异性）通常会被有意省略，以便利用预训练表示（可迁移性）。这种权衡被称为

    Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as 
    
[^357]: MuseChat:一种视频对话音乐推荐系统

    MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])

    [http://arxiv.org/abs/2310.06282](http://arxiv.org/abs/2310.06282)

    MuseChat是一种创新的对话式音乐推荐系统，通过模拟用户和推荐系统之间的对话交互，利用预训练的音乐标签和艺术家信息，为用户提供定制的音乐推荐，使用户可以个性化选择他们喜欢的音乐。

    

    我们引入了MuseChat，一种创新的基于对话的音乐推荐系统。这个独特的平台不仅提供互动用户参与，还为输入的视频提供了定制的音乐推荐，使用户可以改进和个性化他们的音乐选择。与之相反，以前的系统主要强调内容的兼容性，往往忽视了用户个体偏好的细微差别。例如，所有的数据集都只提供基本的音乐-视频配对，或者带有音乐描述的配对。为了填补这一空白，我们的研究提供了三个贡献。首先，我们设计了一种对话合成方法，模拟了用户和推荐系统之间的两轮交互，利用预训练的音乐标签和艺术家信息。在这个交互中，用户提交一个视频给系统，系统会提供一个合适的音乐片段，并附带解释。之后，用户会表达他们对音乐的偏好，系统会呈现一个改进后的音乐推荐

    We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
    
[^358]: 参数高效的多任务模型融合与部分线性化

    Parameter Efficient Multi-task Model Fusion with Partial Linearization. (arXiv:2310.04742v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04742](http://arxiv.org/abs/2310.04742)

    本文提出了一种参数高效的多任务模型融合方法，通过部分线性化适配器模块，并应用任务算法，实现了对大型预训练模型在多个下游任务上的高效微调，从而提高了多任务模型融合的效果和效率。

    

    大型预训练模型在机器学习中取得了重大进展，并成为基础组件。模型融合方法，如任务算法，已被证明具有强大的可扩展性，可以将来自不同任务的微调权重合并到一个多任务模型中。然而，对于在多个下游任务上高效微调大型预训练模型仍然具有挑战性，导致多任务模型融合效率低下。在这项工作中，我们提出了一种改进多任务融合的新方法，适用于像LoRA微调这样的参数高效微调技术。具体而言，我们的方法仅部分线性化适配器模块，并在线性化的适配器上应用任务算法。这样一来，我们可以利用模型融合优势来进行线性化微调，同时保持微调和推理的效率。我们证明，我们的部分线性化技术使多个任务更有效地融合到单个模型中，表现更好。

    Large pre-trained models have enabled significant advances in machine learning and served as foundation components. Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. However, efficiently fine-tuning large pre-trained models on multiple downstream tasks remains challenging, leading to inefficient multi-task model fusion. In this work, we propose a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically, our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters. This allows us to leverage the the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently. We demonstrate that our partial linearization technique enables a more effective fusion of multiple tasks into a single model, outper
    
[^359]: 无需训练的线性图像反演方法：通过流进行

    Training-free Linear Image Inversion via Flows. (arXiv:2310.04432v1 [cs.CV])

    [http://arxiv.org/abs/2310.04432](http://arxiv.org/abs/2310.04432)

    提出了一种无需训练的线性图像反演方法，通过使用预训练的流模型，在减少手动调整的情况下解决逆问题。

    

    无需训练的线性反演方法使用预训练的生成模型，并通过对生成过程的适当修改来解决逆问题，而无需对生成模型进行调优。虽然最近的先前方法已经探索了扩散模型的使用，但仍需要手动调整许多超参数来应对不同的逆问题。在本文中，我们提出了一种使用预训练流模型进行图像反演的无需训练方法，利用了流匹配模型的简洁性和高效性，使用理论上合理的加权方案，从而显著减少了手动调整的工作量。具体而言，我们从两个主要源头汲取灵感：将先前的梯度校正方法应用于流领域，以及基于条件最优传输路径的求解器方案。由于预训练的扩散模型广泛可用，我们还展示了如何将扩散模型实际应用于我们的方法。实验结果表明，我们的方法在多个逆问题上实现了较好的性能。

    Training-free linear inversion involves the use of a pretrained generative model and -- through appropriate modifications to the generation process -solving inverse problems without any finetuning of the generative model. While recent prior methods have explored the use of diffusion models, they still require the manual tuning of many hyperparameters for different inverse problems. In this work, we propose a training-free method for image inversion using pretrained flow models, leveraging the simplicity and efficiency of Flow Matching models, using theoretically-justified weighting schemes and thereby significantly reducing the amount of manual tuning. In particular, we draw inspiration from two main sources: adopting prior gradient correction methods to the flow regime, and a solver scheme based on conditional Optimal Transport paths. As pretrained diffusion models are widely accessible, we also show how to practically adapt diffusion models for our method. Empirically, our approach
    
[^360]: 在概率测度空间中通过梯度流进行抽样

    Sampling via Gradient Flows in the Space of Probability Measures. (arXiv:2310.03597v1 [stat.ML])

    [http://arxiv.org/abs/2310.03597](http://arxiv.org/abs/2310.03597)

    通过梯度流抽样方法的研究方向在计算科学和工程中具有重要意义。本文通过研究概率测度空间中的梯度流的设计组成部分，提出了三个贡献：Kullback-Leibler散度作为能量泛函的独特属性、度量的选择与不变性的关系。

    

    在计算科学和工程中，使用未知归一化常数的目标概率分布进行抽样是一项基本的挑战。最近的研究表明，通过考虑概率测度空间中的梯度流派生的算法为算法开发开辟了新的途径。本文通过审查这种梯度流的设计组成部分，对这种抽样方法做出了三个贡献。抽样的任何实例化都需要一个能量泛函和一个度量来确定流动，以及流动的数值近似来推导算法。我们的第一个贡献是展示了Kullback-Leibler散度作为一个能量泛函具有唯一的特征（在所有f-散度中），即由其得到的梯度流不依赖于目标分布的归一化常数。我们的第二个贡献是从不变性的角度研究度量的选择。Fisher-Rao度量被称为t

    Sampling a target probability distribution with an unknown normalization constant is a fundamental challenge in computational science and engineering. Recent work shows that algorithms derived by considering gradient flows in the space of probability measures open up new avenues for algorithm development. This paper makes three contributions to this sampling approach by scrutinizing the design components of such gradient flows. Any instantiation of a gradient flow for sampling needs an energy functional and a metric to determine the flow, as well as numerical approximations of the flow to derive algorithms. Our first contribution is to show that the Kullback-Leibler divergence, as an energy functional, has the unique property (among all f-divergences) that gradient flows resulting from it do not depend on the normalization constant of the target distribution. Our second contribution is to study the choice of metric from the perspective of invariance. The Fisher-Rao metric is known as t
    
[^361]: 有限数据条件下用于MILP求解器的深度实例生成框架

    A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability. (arXiv:2310.02807v1 [cs.LG])

    [http://arxiv.org/abs/2310.02807](http://arxiv.org/abs/2310.02807)

    本文提出了G2MILP，这是第一个用于MILP实例的深度生成框架，它可以生成新颖而逼真的MILP实例。

    

    在过去的几年中，使用机器学习技术解决组合优化问题，特别是混合整数线性规划问题（MILP），出现了爆炸式增长。尽管取得了一些成果，但真实世界实例的有限可用性往往会导致次优决策和有偏见的求解器评估，这就需要一系列合成MILP实例生成技术。然而，现有方法要么过于依赖专家设计的表达式，要么难以捕捉真实世界实例的丰富特征。为了解决这个问题，我们提出了G2MILP，据我们所知这是第一个用于MILP实例的深度生成框架。具体来说，G2MILP将MILP实例表示为二分图，并应用遮蔽变分自编码器来迭代地破坏和替换原始图的部分以生成新的实例。G2MILP的一个吸引人的特点是它可以学会生成新颖而逼真的MILP实例。

    In the past few years, there has been an explosive surge in the use of machine learning (ML) techniques to address combinatorial optimization (CO) problems, especially mixed-integer linear programs (MILPs). Despite the achievements, the limited availability of real-world instances often leads to sub-optimal decisions and biased solver assessments, which motivates a suite of synthetic MILP instance generation techniques. However, existing methods either rely heavily on expert-designed formulations or struggle to capture the rich features of real-world instances. To tackle this problem, we propose G2MILP, which to the best of our knowledge is the first deep generative framework for MILP instances. Specifically, G2MILP represents MILP instances as bipartite graphs, and applies a masked variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP instan
    
[^362]: 奖励模型集成有助于减轻过度优化问题

    Reward Model Ensembles Help Mitigate Overoptimization. (arXiv:2310.02743v1 [cs.LG])

    [http://arxiv.org/abs/2310.02743](http://arxiv.org/abs/2310.02743)

    本研究通过探究奖励模型集成和保守优化目标的效果，对减轻奖励模型过度优化进行了系统研究。

    

    人类反馈强化学习（RLHF）是一种将大型语言模型微调以遵循指令的标准方法。在这个过程中，学习到的奖励模型被用来近似人类偏好。然而，作为“真实”奖励的不完美表示，这些学习到的奖励模型容易受到过度优化的影响。Gao等人在一个人工反馈实验中研究了这个现象，使用一个较大的“金标准”奖励模型作为真实奖励（而不是人类），并显示过度优化仍然是一个持续存在的问题，无论代理奖励模型和训练数据的大小如何。使用类似的设置，我们进行了一项系统研究，评估了在使用两种优化方法时，使用基于集合的保守优化目标（最坏情况优化和权重不确定性优化）来减轻奖励模型过度优化的有效性。

    Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the "true" reward, these learned reward models are susceptible to \textit{overoptimization}. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger "gold" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sa
    
[^363]: 扩散生成流采样器：通过部分轨迹优化改善学习信号

    Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. (arXiv:2310.02679v1 [cs.LG])

    [http://arxiv.org/abs/2310.02679](http://arxiv.org/abs/2310.02679)

    这项工作介绍了一种名为扩散生成流采样器（DGFS）的采样框架，通过将学习过程分解为短的部分轨迹段，实现从难以处理的高维密度函数中进行采样。它通过利用中间的学习信号和非策略探索能力来改善学习信号的分配问题。

    

    我们解决了从难以处理的高维密度函数中进行采样的问题，这是在机器学习和统计中经常出现的基本任务。我们扩展了最近的基于采样的方法，利用控制的随机过程来模拟这些目标密度的近似样本。这些方法的主要缺点是训练目标需要计算完整的轨迹，导致由于使用完整轨迹和只在终端时间存在的学习信号的使用而产生缓慢的信用分配问题。在这项工作中，我们提出了扩散生成流采样器（DGFS），这是一个基于采样的框架，可以将学习过程可行地分解为短的部分轨迹段，通过参数化一个额外的“流函数”。我们的方法借鉴了生成流网络（GFlowNets）的理论，使我们能够利用中间的学习信号，并从非策略探索能力中受益。

    We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. We extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities. The main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time. In this work, we present Diffusion Generative Flow Samplers (DGFS), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional "flow function". Our method takes inspiration from the theory developed for generative flow networks (GFlowNets), allowing us to make use of intermediate learning signals and benefit from off-policy exploration capabilitie
    
[^364]: 解码人类行为：分析可穿戴加速度计和陀螺仪数据进行活动识别

    Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition. (arXiv:2310.02011v1 [cs.CV])

    [http://arxiv.org/abs/2310.02011](http://arxiv.org/abs/2310.02011)

    本文提出了一种用于活动识别的分层多结构方法，利用残差网络和残差MobileNet对静态和动态活动进行分类，然后通过加权合奏方法进行集成。

    

    一个人的运动或相对定位有效地产生了可以被计算机读取的原始电信号，通过应用各种操作技术来对不同的人类活动进行分类。本文提出了一种基于残差网络与残差MobileNet进行合奏的分层多结构方法，称为FusionActNet。所提出的方法涉及使用精心设计的残差块分别对静态和动态活动进行分类，因为它们具有明显而独特的特征。这些网络独立训练，得到两个专业的高精度模型。通过利用架构调整的算法优势，这些模型在特定超类中优秀地识别活动。然后，这两个残差网络通过加权合奏的残差MobileNet进行传递。随后，这个合奏能够有效区分一些特定的子类。

    A person's movement or relative positioning effectively generates raw electrical signals that can be read by computing machines to apply various manipulative techniques for the classification of different human activities. In this paper, a stratified multi-structural approach based on a Residual network ensembled with Residual MobileNet is proposed, termed as FusionActNet. The proposed method involves using carefully designed Residual blocks for classifying the static and dynamic activities separately because they have clear and distinct characteristics that set them apart. These networks are trained independently, resulting in two specialized and highly accurate models. These models excel at recognizing activities within a specific superclass by taking advantage of the unique algorithmic benefits of architectural adjustments. Afterward, these two ResNets are passed through a weighted ensemble-based Residual MobileNet. Subsequently, this ensemble proficiently discriminates between a sp
    
[^365]: 大型语言模型作为类比推理器

    Large Language Models as Analogical Reasoners. (arXiv:2310.01714v1 [cs.LG])

    [http://arxiv.org/abs/2310.01714](http://arxiv.org/abs/2310.01714)

    本研究提出了一种新的提示方法，称为类比提示，用于自动引导大型语言模型的推理过程。通过在上下文中自动生成相关实例或知识，该方法在多种推理任务中表现出优异的性能。

    

    语言模型的思维链（CoT）提示在推理任务中展现出令人印象深刻的性能，但通常需要有标记的推理过程示例。在这项工作中，我们引入了一种新的提示方法，称为类比提示，旨在自动引导大型语言模型的推理过程。受类比推理的启发，类比推理是一种认知过程，人类从相关的过去经验中获取知识来解决新问题。我们的方法促使语言模型自动生成上下文中的相关实例或知识，然后解决给定的问题，具有以下几个优点：它省去了标记或检索实例的需求，提供了普适性和便利性；它还可以根据每个问题定制生成的示例和知识，提供了适应性。实验结果表明，我们的方法在各种推理任务中优于0-shot CoT和手动few-shot CoT，包括数学问题求解。

    Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, Analogical Prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving i
    
[^366]: 关于基于模型和无模型强化学习的表示复杂性的研究

    On Representation Complexity of Model-based and Model-free Reinforcement Learning. (arXiv:2310.01706v1 [cs.LG])

    [http://arxiv.org/abs/2310.01706](http://arxiv.org/abs/2310.01706)

    本研究在电路复杂度的角度探讨了基于模型和无模型强化学习的表示复杂性。理论上证明了某些MDP可以用恒定深度电路表示转移和奖励函数，但最优$Q$-函数的电路复杂度指数级增加。我们的理论揭示了为什么基于模型的算法通常比无模型的算法具有更好的样本复杂性。

    

    我们在电路复杂度的背景下研究了基于模型和无模型的强化学习的表示复杂性。我们在理论上证明，存在一类广泛的马尔可夫决策过程（MDP），它们的转移和奖励函数可以用具有多项式大小的恒定深度电路表示，而最优的$Q$-函数在恒定深度电路中遭受指数级电路复杂度。通过关注逼近误差并建立到复杂性理论的联系，我们的理论从新的表示复杂性角度为什么基于模型的算法通常比无模型的算法具有更好的样本复杂性提供了独特的见解：在某些情况下，环境的真实规则（模型）易于表示，而其他数量，如$Q$-函数，似乎很复杂。我们通过比较转移核函数、奖励函数和最优$Q$-函数的逼近误差来经验性地验证我们的理论。

    We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. By drawing attention to the approximation errors and building connections to complexity theory, our theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities, such as $Q$-function, appear complex. We empirically corroborate our theory by comparing the approximation error of the transition kernel, reward function, and optimal $Q$-function
    
[^367]: 理解和减轻预训练中的标签噪声对下游任务的影响

    Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks. (arXiv:2309.17002v1 [cs.LG])

    [http://arxiv.org/abs/2309.17002](http://arxiv.org/abs/2309.17002)

    本文研究了深度学习中预训练数据中的标签噪声对下游任务的影响，并通过在合成噪声数据集上的实验证明，在预训练中的轻微噪声可以提高领域内的性能，但会损害领域外的性能。为了减轻噪声的影响，提出了一种轻量级的黑盒调整方法（NMTune）。

    

    在深度学习中，先在大规模数据集上进行预训练，然后在下游任务上进行微调已经成为一种标准做法。然而，预训练数据通常包含标签噪声，这可能对模型的泛化能力产生不利影响。本文旨在了解预训练数据集中噪声的性质，并减轻其对下游任务的影响。具体而言，通过在合成噪声的ImageNet-1K和YFCC15M数据集上进行大量实验，我们证明在预训练中的轻微噪声可以促进领域内的转移性能，即训练和测试数据具有相同的分布；然而，它总是会损害领域外的性能，即训练和测试数据具有不同的分布。我们通过实验证实，预训练中的噪声会不同地塑造特征空间。然后我们提出了一种轻量级的黑盒调整方法（NMTune）来使特征空间达到映射并减轻噪声的影响。

    Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the m
    
[^368]: Segment Anything Model对本地特征学习具有良好的教导作用

    Segment Anything Model is a Good Teacher for Local Feature Learning. (arXiv:2309.16992v1 [cs.CV])

    [http://arxiv.org/abs/2309.16992](http://arxiv.org/abs/2309.16992)

    本文提出了使用Segment Anything Model (SAM)作为教师来指导本地特征学习，通过像素语义关系蒸馏和弱监督对比学习两种技术，实现了在有限数据集上的更高性能表现。

    

    本地特征的检测和描述在许多计算机视觉任务中起着重要作用，旨在检测和描述“任何场景”和“任何下游任务”的关键点。数据驱动的本地特征学习方法需要依赖于像素级一致性进行训练，这在大规模获得方面具有挑战性，从而阻碍了进一步的性能提升。在本文中，我们提出了SAMFeat来引入SAM（segment anything model）作为教师来指导本地特征学习，从而在有限的数据集上激发更高的性能。为此，首先，我们构建了一个像素语义关系蒸馏（PSRD）的辅助任务，将SAM编码器学习到的类别不可知的语义信息通过特征关系蒸馏到本地特征学习网络中，以提高通过语义区分改善本地特征描述的能力。其次，我们开发了一种称为弱监督对比学习的技术

    Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in "any scene" and "any downstream task". Data-driven local feature learning methods need to rely on pixel-level correspondence for training, which is challenging to acquire at scale, thus hindering further improvements in performance. In this paper, we propose SAMFeat to introduce SAM (segment anything model), a fundamental model trained on 11 million images, as a teacher to guide local feature learning and thus inspire higher performance on limited datasets. To do so, first, we construct an auxiliary task of Pixel Semantic Relational Distillation (PSRD), which distillates feature relations with category-agnostic semantic information learned by the SAM encoder into a local feature learning network, to improve local feature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning 
    
[^369]: M-OFDFT：利用深度学习克服分子系统中的无轨道密度泛函理论的障碍

    M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning. (arXiv:2309.16578v1 [stat.ML])

    [http://arxiv.org/abs/2309.16578](http://arxiv.org/abs/2309.16578)

    M-OFDFT是一种利用深度学习模型解决分子系统问题的OFDFT方法，通过将非局域性建立在模型中并使用紧凑的密度表示，实现了与Kohn-Sham DFT相近的精确度，并且具有良好的外推能力。

    

    无轨道密度泛函理论（OFDFT）是一种具有较低运算成本的量子化学计算方法，比起常用的Kohn-Sham密度泛函理论更加适用于当代分子研究。然而，OFDFT的精确性受到了动能密度泛函的限制，对于非周期性分子系统的近似求解非常困难。本文提出了名为M-OFDFT的方法，利用深度学习的函数模型解决了分子系统的问题。我们将必要的非局域性建立在这个模型中，通过原子基下的展开系数作为紧凑的密度表示来降低成本。通过解决其中的非传统学习挑战的技术，M-OFDFT在一系列OFDFT无法触及的分子上实现了与Kohn-Sham DFT相当的精确度。更有吸引力的是，M-OFDFT在训练时属于更大的分子中有着良好的外推能力，为研究大分子提供了有吸引力的规模效应。

    Orbital-free density functional theory (OFDFT) is a quantum chemistry formulation that has a lower cost scaling than the prevailing Kohn-Sham DFT, which is increasingly desired for contemporary molecular research. However, its accuracy is limited by the kinetic energy density functional, which is notoriously hard to approximate for non-periodic molecular systems. In this work, we propose M-OFDFT, an OFDFT approach capable of solving molecular systems using a deep-learning functional model. We build the essential nonlocality into the model, which is made affordable by the concise density representation as expansion coefficients under an atomic basis. With techniques to address unconventional learning challenges therein, M-OFDFT achieves a comparable accuracy with Kohn-Sham DFT on a wide range of molecules untouched by OFDFT before. More attractively, M-OFDFT extrapolates well to molecules much larger than those in training, which unleashes the appealing scaling for studying large molecu
    
[^370]: ModuLoRA:通过与模块化量化器集成在消费级GPU上对3 Bit LLMs进行微调

    ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers. (arXiv:2309.16119v1 [cs.LG])

    [http://arxiv.org/abs/2309.16119](http://arxiv.org/abs/2309.16119)

    ModuLoRA提出了一种内存高效、能够在消费级GPU上支持3比特LLMs微调的方法，并通过与模块化量化器的集成实现了竞争性能和更少的内存使用。

    

    我们提出了一种内存高效的大型语言模型（LLMs）微调算法，可支持在仅使用1个48GB GPU上以3比特或4比特精度微调具有65B参数的LLMs。我们的方法——模块化低秩自适应（ModuLoRA），通过低秩适配器（LoRA）将任何用户指定的权重量化器与微调集成。我们的方法依赖于一个简单的量化无关的反向传播，通过自定义的黑盒量化模块从低精度LLM权重中自适应地生成权重。这种方法使得首次能够进行3比特LLMs的微调，利用先进的3比特OPTQ量化往往优于依赖于较不复杂的4比特和8比特方法的微调。在我们的实验中，ModuLoRA在文本分类、自然语言推理和指令跟随任务中取得了有竞争力的性能，使用的内存比现有方法少很多，并且在一个流行的摘要任务上超过了最先进的ROUGE分数。

    We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit precision on as little as one 48GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 3-bit LLMs for the first time--leveraging state-of-the-art 3-bit OPTQ quantization often outperforms finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. W
    
[^371]: STARC:评估奖励函数之间差异的通用框架

    STARC: A General Framework For Quantifying Differences Between Reward Functions. (arXiv:2309.15257v1 [cs.LG])

    [http://arxiv.org/abs/2309.15257](http://arxiv.org/abs/2309.15257)

    这篇论文提出了一个通用框架（STARC），用于评估奖励函数之间的差异，填补了奖励学习理论基础的空白。

    

    为了使用强化学习解决任务，需要将任务的目标形式化为奖励函数。然而，对于许多现实世界的任务来说，手动指定一个永不激励不良行为的奖励函数非常困难。因此，使用奖励学习算法来从数据中学习奖励函数变得越来越流行。然而，奖励学习的理论基础尚未完善。特别地，通常不知道给定的奖励学习算法在高概率下是否会学习到一个安全优化的奖励函数。这意味着奖励学习算法通常必须经过经验评估，这是昂贵的，并且很难预测其失效模式。其中一个阻碍获得更好理论保证的障碍是缺乏较好的方法来量化奖励函数之间的差异。在本文中，我们提供了一种解决方案。

    In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to predict in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution t
    
[^372]: 导航文本到图像定制：从LyCORIS微调到模型评估

    Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation. (arXiv:2309.14859v1 [cs.CV])

    [http://arxiv.org/abs/2309.14859](http://arxiv.org/abs/2309.14859)

    本文介绍了LyCORIS，一个开源库，提供了多种稳定扩散模型的微调方法，并提出了一个系统评估的全面框架。

    

    文本到图像生成模型因其能够从文本提示生成高保真度图像而受到广泛关注。其中，稳定扩散模型作为领先的开源模型在这个快速发展的领域中表现出色。然而，微调这些模型的复杂性给新方法的整合和系统评估带来了多重挑战。本文介绍了LyCORIS（Lora beYond Conventional methods，Other Rank adaptation Implementations for Stable diffusion）[https://github.com/KohakuBlueleaf/LyCORIS]，这是一个开源库，提供了多种稳定扩散模型的微调方法。此外，我们还提出了一个系统评估的全面框架，该框架采用了多样化的指标，并深入研究了微调的多个方面，包括超参数调整和在不同概念类别下使用不同提示类型的评估。

    Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categori
    
[^373]: FedCompass：基于计算能力感知调度器的异构客户端设备的高效跨边界联邦学习

    FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler. (arXiv:2309.14675v1 [cs.LG])

    [http://arxiv.org/abs/2309.14675](http://arxiv.org/abs/2309.14675)

    FedCompass是一种创新的半异步联邦学习算法，通过在服务器端使用计算能力感知调度器，解决了异构客户端和数据中跨边界联邦学习的效率和收敛准确性问题。

    

    跨边界联邦学习为协同训练鲁棒且泛化的AI模型提供了有希望的解决方案，而不会损害本地数据集的隐私，如医疗、金融以及缺乏集中式数据设施的科学项目。然而，由于不同客户端（即设备异构性）之间的计算资源差异，同步联邦学习算法在等待阻塞客户端时效率下降。同样，非同分布（non-IID）异构数据集上的异步联邦学习算法由于过时的本地模型和客户端偏移，导致收敛速度和最终模型准确性降低。为了解决这些异构客户端和数据中的跨边界联邦学习的局限性，我们提出了FedCompass，这是一种具有计算能力感知调度器的创新半异步联邦学习算法。

    Cross-silo federated learning offers a promising solution to collaboratively train robust and generalized AI models without compromising the privacy of local datasets, e.g., healthcare, financial, as well as scientific projects that lack a centralized data facility. Nonetheless, because of the disparity of computing resources among different clients (i.e., device heterogeneity), synchronous federated learning algorithms suffer from degraded efficiency when waiting for straggler clients. Similarly, asynchronous federated learning algorithms experience degradation in the convergence rate and final model accuracy on non-identically and independently distributed (non-IID) heterogeneous datasets due to stale local models and client drift. To address these limitations in cross-silo federated learning with heterogeneous clients and data, we propose FedCompass, an innovative semi-asynchronous federated learning algorithm with a computing power aware scheduler on the server side, which adaptive
    
[^374]: 连续治疗的双重稳健近端因果学习

    Doubly Robust Proximal Causal Learning for Continuous Treatments. (arXiv:2309.12819v1 [stat.ME])

    [http://arxiv.org/abs/2309.12819](http://arxiv.org/abs/2309.12819)

    本文提出了一种基于核函数的双重稳健近端因果学习方法，用于处理连续治疗，并提出了一种高效求解干扰函数的新方法。

    

    近端因果学习是在存在未测量混淆因素下识别因果效应的有希望的框架。在该框架中，补充稳健（DR）估计器被推导出来，并在估计中展示了其有效性，特别是在模型假设被违反时。然而，当前形式的DR估计器仅限于二进制治疗，而在许多现实世界的应用中，治疗可以是连续的。连续治疗的主要障碍在于在原始DR估计器中存在的delta函数，使得在因果效应估计中不可行，并在干扰函数估计中引入了沉重的计算负担。为了解决这些挑战，我们提出了一种基于核函数的连续治疗的DR估计器，可以很好地处理连续治疗。配备其平滑性，我们展示了其Oracle形式是影响函数的一致近似。此外，我们提出了一种新的方法来高效解决干扰函数。

    Proximal causal learning is a promising framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatment can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We
    
[^375]: 贝叶斯动态有向无环图学习：在发现大脑动态效应连接组中的应用

    Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain. (arXiv:2309.07080v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.07080](http://arxiv.org/abs/2309.07080)

    本文介绍了贝叶斯动态有向无环图学习方法（BDyMA）来解决在发现大脑动态效应连接组中的两个主要挑战。该方法通过无约束框架实现更准确的结果和更稀疏的网络结构，使其特别适用于提取动态效应连接组。

    

    通过提取动态效应连接组（DEC）可以揭示大脑的复杂机制。最近，基于评分的有向无环图（DAG）发现方法在提取因果结构和推断有效连接方面表现出显著改进。然而，通过这些方法学习DEC仍然面临两个主要挑战：一个是高维动态DAG发现方法的根本无能力，另一个是fMRI数据质量低下。在本文中，我们引入了基于M-矩阵无环特性的贝叶斯动态DAG学习（BDyMA）方法来解决发现DEC中的挑战。所提出的动态因果模型使我们能够发现双向边缘。利用BDyMA方法中的无约束框架在检测高维网络方面可以获得更准确的结果，实现更稀疏的结果，使其特别适用于提取DEC。

    Understanding the complex mechanisms of the brain can be unraveled by extracting the Dynamic Effective Connectome (DEC). Recently, score-based Directed Acyclic Graph (DAG) discovery methods have shown significant improvements in extracting the causal structure and inferring effective connectivity. However, learning DEC through these methods still faces two main challenges: one with the fundamental impotence of high-dimensional dynamic DAG discovery methods and the other with the low quality of fMRI data. In this paper, we introduce Bayesian Dynamic DAG learning with M-matrices Acyclicity characterization \textbf{(BDyMA)} method to address the challenges in discovering DEC. The presented dynamic causal model enables us to discover bidirected edges as well. Leveraging an unconstrained framework in the BDyMA method leads to more accurate results in detecting high-dimensional networks, achieving sparser outcomes, making it particularly suitable for extracting DEC. Additionally, the score f
    
[^376]: 通过实验数据同化实现对Spalart-Allmaras模型的可普适改进

    Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data. (arXiv:2309.06679v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2309.06679](http://arxiv.org/abs/2309.06679)

    本研究通过实验数据同化改进了Spalart-Allmaras模型，实现了对分离流体的雷诺平均纳维-斯托克斯解的泛化，提高了计算模型的性能。

    

    本研究旨在利用模型和数据融合改进分离流体的雷诺平均纳维-斯托克斯解的Spalart-Allmaras（SA）闭合模型。特别是，我们的目标是开发模型，不仅能将稀疏的实验数据同化以改善计算模型的性能，还能通过恢复经典的SA行为来推广到未见过的情况。我们使用数据同化，即集合卡尔曼滤波方法（EnKF），通过将SA模型的系数校准到分离流体中来实现我们的目标。通过参数化产生、扩散和破坏项，实现了一种全面的校准策略。该校准依赖于采集的分离流体速度剖面、壁擦力和压力系数的实验数据的同化。尽管仅使用了来自单一流动条件（环绕一个背面台阶）的观测数据，但重新校准的SA模型表现出泛化能力。

    This study focuses on the use of model and data fusion for improving the Spalart-Allmaras (SA) closure model for Reynolds-averaged Navier-Stokes solutions of separated flows. In particular, our goal is to develop of models that not-only assimilate sparse experimental data to improve performance in computational models, but also generalize to unseen cases by recovering classical SA behavior. We achieve our goals using data assimilation, namely the Ensemble Kalman Filtering approach (EnKF), to calibrate the coefficients of the SA model for separated flows. A holistic calibration strategy is implemented via a parameterization of the production, diffusion, and destruction terms. This calibration relies on the assimilation of experimental data collected velocity profiles, skin friction, and pressure coefficients for separated flows. Despite using of observational data from a single flow condition around a backward-facing step (BFS), the recalibrated SA model demonstrates generalization to o
    
[^377]: 评估潮起潮落：对不同平台间问答趋势的深入分析

    Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])

    [http://arxiv.org/abs/2309.05961](http://arxiv.org/abs/2309.05961)

    本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。

    

    社区问答平台因其快速回答用户查询的能力而越来越受欢迎。这些回答速度的快慢取决于查询特定和用户相关的因素的综合。本文通过研究六个高度流行的社区问答平台，分析了这些因素在其中的作用。我们的调查揭示了问题的第一个回答所花费的时间与元数据、问题的构成方式和用户之间的互动水平之间的关联。此外，通过使用传统的机器学习模型分析这些元数据和用户互动模式，我们试图预测哪些查询将迅速获得初始回答。

    Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
    
[^378]: DoLa：通过对比层次提高大型语言模型中的真实性

    DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])

    [http://arxiv.org/abs/2309.03883](http://arxiv.org/abs/2309.03883)

    DoLa通过对比不同层次的逻辑差异，提高大型语言模型中的真实性和减少幻觉，无需外部知识或微调。

    

    尽管大型语言模型（LLMs）具有令人印象深刻的能力，但它们容易出现幻觉，即生成与预训练期间观察到的事实偏离的内容。我们提出了一种简单的解码策略，用于减少预训练LLMs中的幻觉，它不需要在检索的外部知识或额外的微调上进行条件约束。我们的方法通过对比将较晚层和较早层投影到词汇空间得到的逻辑差异来获得下一个令牌的分布，利用了LLMs中的事实知识通常被证明局部化在特定的Transformer层中的事实。我们发现，这种通过对比层次的解码（DoLa）方法能够更好地展示事实知识，并减少生成不正确事实的情况。DoLa在多个选择任务和开放式生成任务中持续提升了真实性，例如改善了LLaMA系列模型在TruthfulQA上的表现。

    Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA b
    
[^379]: 电路的索引感知学习

    Index-aware learning of circuits. (arXiv:2309.00958v2 [cs.CE] UPDATED)

    [http://arxiv.org/abs/2309.00958](http://arxiv.org/abs/2309.00958)

    提出了一种索引感知学习的方法，通过对电路进行解剖概念的应用，将差分代数方程组分解为常微分方程和代数方程，以更好地利用现有知识来量化电路设计中可调参数的影响。

    

    电子电路在各种技术中都存在，其设计是计算机辅助工程的重要部分。影响最终设计的可调参数数量的增加导致了对量化其影响的新方法的需求。机器学习可以在这方面起到重要作用，然而目前的方法往往没有充分利用有关现有系统的知识。就电路而言，通过修改节点分析对其进行描述是众所周知的。这种特定的表达形式导致了差分代数方程组（DAEs），其中存在一些特殊性质，例如解决方案需要满足的隐藏约束。我们的目标是使用最近引入的用于DAEs的解剖概念，可以将给定系统分解为仅依赖于差分变量的常微分方程和仅描述差分和代数变量之间关系的代数方程。

    Electrical circuits are present in a variety of technologies, making their design an important part of computer aided engineering. The growing number of tunable parameters that affect the final design leads to a need for new approaches of quantifying their impact. Machine learning may play a key role in this regard, however current approaches often make suboptimal use of existing knowledge about the system at hand. In terms of circuits, their description via modified nodal analysis is well-understood. This particular formulation leads to systems of differential-algebraic equations (DAEs) which bring with them a number of peculiarities, e.g. hidden constraints that the solution needs to fulfill. We aim to use the recently introduced dissection concept for DAEs that can decouple a given system into ordinary differential equations, only depending on differential variables, and purely algebraic equations that describe the relations between differential and algebraic variables. The idea the
    
[^380]: MatchXML: 高效的文本-标签匹配框架，用于极端多标签文本分类

    MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification. (arXiv:2308.13139v1 [cs.CL])

    [http://arxiv.org/abs/2308.13139](http://arxiv.org/abs/2308.13139)

    MatchXML是一种高效的文本-标签匹配框架，用于极端多标签文本分类。它通过label2vec方法生成语义密集的标签嵌入，并利用这些嵌入构建层次化标签树。通过微调预训练的Transformer模型，MatchXML将多标签文本分类问题转化为文本-标签匹配问题，并提取出密集的文本表示和静态的句子嵌入。

    

    极端多标签文本分类（XMC）是指训练一个分类器，从一个非常大规模的标签集中（例如数百万个标签）为文本样本分配相关标签。我们提出了MatchXML，一种用于XMC的高效文本-标签匹配框架。我们观察到，由稀疏的词频-逆文档频率（TF-IDF）特征生成的标签嵌入存在一些限制。因此，我们提出了label2vec，通过Skip-gram模型来有效训练语义密集的标签嵌入。然后，使用这些密集的标签嵌入来构建一个层次化标签树。在微调预训练的编码器Transformer时，我们将多标签文本分类问题制定为一个在二分图中的文本-标签匹配问题。然后，从微调后的Transformer中提取密集的文本表示。除了微调后的密集文本嵌入之外，我们还从预训练的Sentence Transformer中提取静态的密集句子嵌入。

    The eXtreme Multi-label text Classification(XMC) refers to training a classifier that assigns a text sample with relevant labels from an extremely large-scale label set (e.g., millions of labels). We propose MatchXML, an efficient text-label matching framework for XMC. We observe that the label embeddings generated from the sparse Term Frequency-Inverse Document Frequency(TF-IDF) features have several limitations. We thus propose label2vec to effectively train the semantic dense label embeddings by the Skip-gram model. The dense label embeddings are then used to build a Hierarchical Label Tree by clustering. In fine-tuning the pre-trained encoder Transformer, we formulate the multi-label text classification as a text-label matching problem in a bipartite graph. We then extract the dense text representations from the fine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also extract the static dense sentence embeddings from a pre-trained Sentence Transformer. Finally,
    
[^381]: 在线控制线性动力学：一种数据驱动方法

    Online Control for Linear Dynamics: A Data-Driven Approach. (arXiv:2308.08138v1 [eess.SY])

    [http://arxiv.org/abs/2308.08138](http://arxiv.org/abs/2308.08138)

    本文提出了一种数据驱动的方法来解决在线控制线性动力学问题，该方法不需要识别系统模型，而是通过累积扰动来进行决策，证明了算法性能与基于模型的方法相当。

    

    本文考虑了一个在线控制问题，涉及到具有未知动力学、有界扰动和对抗成本的线性时不变系统。我们提出了一种数据驱动策略来降低控制器的后悔。与基于模型的方法不同，我们的算法不需要识别系统模型，而是利用单个无噪声轨迹来计算扰动的累积，并使用我们设计的累积扰动动作控制器做出决策，其参数通过在线梯度下降进行更新。我们证明，在温和的假设下，我们算法的后悔是$\mathcal{O}(\sqrt{T})$的，这表明它的性能与基于模型的方法相当。

    This paper considers an online control problem over a linear time-invariant system with unknown dynamics, bounded disturbance, and adversarial cost. We propose a data-driven strategy to reduce the regret of the controller. Unlike model-based methods, our algorithm does not identify the system model, instead, it leverages a single noise-free trajectory to calculate the accumulation of disturbance and makes decisions using the accumulated disturbance action controller we design, whose parameters are updated by online gradient descent. We prove that the regret of our algorithm is $\mathcal{O}(\sqrt{T})$ under mild assumptions, suggesting that its performance is on par with model-based methods.
    
[^382]: 黑盒变分推断的线性收敛性：我们应该坚持到底吗？

    Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])

    [http://arxiv.org/abs/2307.14642](http://arxiv.org/abs/2307.14642)

    本文证明了带有控制变量的黑盒变分推断在完美变分族规范下以几何速度收敛，为BBVI提供了收敛性保证，同时提出了对熵梯度估计器的改进，对比了STL估计器，并给出了明确的非渐近复杂度保证。

    

    我们证明了带有控制变量的黑盒变分推断（BBVI），特别是着陆稳定（STL）估计器，在完美变分族规范下收敛于几何（传统上称为“线性”）速度。特别地，我们证明了STL估计器的梯度方差的二次界限，该界限包括了误指定的变分族。结合先前关于二次方差条件的工作，这直接暗示了在使用投影随机梯度下降的情况下BBVI的收敛性。我们还改进了现有对于正常封闭形式熵梯度估计器的分析，这使得我们能够将其与STL估计器进行比较，并为两者提供明确的非渐进复杂度保证。

    We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
    
[^383]: 基于拓扑正则化的多实例学习用于红细胞疾病分类

    Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification. (arXiv:2307.14025v1 [cs.LG])

    [http://arxiv.org/abs/2307.14025](http://arxiv.org/abs/2307.14025)

    本论文提出一种基于拓扑正则化的多实例学习方法，用于罕见贫血疾病的红细胞分类。通过从单个红细胞图像中提取多尺度的拓扑特征来进行模型正则化，以保持数据的特征拓扑属性。实验结果表明，该方法是有效的。

    

    使用显微图像诊断罕见的贫血疾病对于熟练的专家和机器学习方法来说都具有挑战性。由于在单个血样中有数千个与疾病相关的细胞，这构成了一个复杂的多实例学习（MIL）问题。虽然红细胞的空间邻域本身并不重要，但整个血样的拓扑结构，即数据的几何性质，包含了有益的特征，以解决典型的MIL问题，如梯度消失和在有限数据上训练时的过拟合。因此，我们开发了一种基于拓扑的方法，从单个红细胞图像的包中提取多尺度的拓扑特征。这些拓扑特征被用来对模型进行正则化，强制保持数据的特征拓扑属性。在包含71个罕见贫血疾病患者的数据集上，包括521张红细胞显微图像，我们的实验表明拓扑正则化是一个有效的方法。

    Diagnosing rare anemia disorders using microscopic images is challenging for skilled specialists and machine-learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effe
    
[^384]: 深度布拉德利-特里评分：在没有具体评价标准的情况下估计物品的属性

    Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])

    [http://arxiv.org/abs/2307.13709](http://arxiv.org/abs/2307.13709)

    本论文提出了深度布拉德利-特里评分（DBTR）方法，用于评估不一定存在于数据集中的未知物品的属性。该方法通过将传统的布拉德利-特里模型与神经网络结构无缝结合，成功地学习了这些属性的预期量化。

    

    在现实世界中，许多属性，如竞争环境中的可取性或强度，无法直接观测，这使得它们难以评估。为了解决这个具有挑战性的问题，先前的研究主要集中在估计已知物品的这些属性，特别是出现在配对比较数据集中的运动员的实力。在本文中，我们介绍了深度布拉德利-特里评分（DBTR），这是一个新颖的机器学习框架，用于评估不一定存在于数据集中的未知物品的任何属性。我们的方法无缝地将传统的布拉德利-特里模型与神经网络结构相结合。我们还进一步推广了这个架构，用于具有不公平性的非对称环境，这在现实世界中更为常见。在我们的实验分析中，DBTR成功地学习了这些属性的预期量化。

    Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
    
[^385]: HIQL: 以潜在状态作为动作的离线目标导向强化学习

    HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])

    [http://arxiv.org/abs/2307.11949](http://arxiv.org/abs/2307.11949)

    本文提出了一个基于离线数据的目标导向强化学习的分层算法，通过利用目标达成问题的结构，使用一个无动作的价值函数学习了两个策略，从而在学习过程中更有效地利用离线数据。

    

    无监督预训练最近已成为计算机视觉和自然语言处理的基石。在强化学习中，目标导向强化学习可以潜在地利用大量未标记的（无奖励）数据，提供类似于自我监督的方法。然而，构建有效的目标导向强化学习算法并直接从多样化的离线数据中进行学习是具有挑战性的，因为准确估计远期目标的价值函数很困难。然而，目标达成问题表现出一定的结构，即达到远期目标需要首先通过较近子目标。这种结构非常有用，因为评估邻近目标的动作质量通常比更远目标容易。基于这一思想，我们提出了一个基于离线数据的目标导向强化学习的分层算法。利用一个没有动作的价值函数，我们学习了两个策略，允许我们利用这种结构：一个高层策略

    Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
    
[^386]: 关于线性递推和非线性投影的普遍性

    On the Universality of Linear Recurrences Followed by Nonlinear Projections. (arXiv:2307.11888v1 [cs.LG])

    [http://arxiv.org/abs/2307.11888](http://arxiv.org/abs/2307.11888)

    本论文展示了一种基于循环线性层和多层感知器的序列模型可以逼近任何规则的非线性序列到序列映射。

    

    在这篇注释（作为一篇全文论文的工作进展）中，我们展示了一族基于循环线性层（包括S4、S5和LRU）和位置逐元素多层感知器（MLPs）的序列模型可以很好地逼近任意规则的非线性序列到序列映射。我们的结果背后的主要思想是将循环层视为可以忠实地存储输入序列信息到内部状态的压缩算法，然后由高度表达能力的MLP进行处理。

    In this note (work in progress towards a full-length paper) we show that a family of sequence models based on recurrent linear layers~(including S4, S5, and the LRU) interleaved with position-wise multi-layer perceptrons~(MLPs) can approximate arbitrarily well any sufficiently regular non-linear sequence-to-sequence map. The main idea behind our result is to see recurrent layers as compression algorithms that can faithfully store information about the input sequence into an inner state, before it is processed by the highly expressive MLP.
    
[^387]: 过度思考真相：理解语言模型如何处理虚假演示

    Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])

    [http://arxiv.org/abs/2307.09476](http://arxiv.org/abs/2307.09476)

    该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。

    

    现代语言模型可以通过少量示范进行复杂模式的模仿学习，使其能够在没有微调的情况下完成具有挑战性的任务。然而，模仿也可能导致模型在上下文中重现不准确或有害的内容。我们通过模型的内部表示来研究有害的模仿，并确定了两个相关现象：过度思考和错误归纳头。第一个现象，过度思考，在给出正确与错误的少量示范时，我们从中间层解码预测。在早期层中，两种示范引起了相似的模型行为，但在某个“关键层”之后，给出错误示范的准确性逐渐降低。第二个现象，错误归纳头，可能是过度思考的一种机制性原因：这些是位于较晚层的头部，它们关注并复制先前示范中的错误信息，其削弱会减少过度思考现象。

    Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
    
[^388]: 在NetHack中的模仿学习的规模律

    Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])

    [http://arxiv.org/abs/2307.09423](http://arxiv.org/abs/2307.09423)

    本文研究了在NetHack游戏中的模仿学习，发现通过扩大模型和数据规模可以改进模仿学习的效果，并建立了训练计算最优IL代理人的幂律。

    

    模仿学习 (IL) 是机器学习中最常用的方法之一。然而，虽然强大，但许多研究发现它往往不能完全恢复出潜在的专家行为。然而，这些研究没有深入探究模型和数据规模的扩大在其中的作用。受最近在自然语言处理 (NLP) 领域的工作的启发，在那里“扩大规模”已经导致了越来越有能力的领域特定语言模型 (LLMs)，我们研究了仔细扩大模型和数据规模是否可以在模仿学习的设置中带来类似的改进。为了展示我们的发现，我们将重点放在 NetHack 游戏上，这是一个具有程序生成、随机性、长期依赖性和部分可观测性的具有挑战性的环境。我们发现 IL 的损失和平均回报随着计算预算的变化而平滑变化且强相关，从而在模型大小和样本数量方面为训练计算最优的 IL 代理人的计算预算建立了幂律。我们预测并训练了几个具有 IL 的NetHack代理。

    Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL an
    
[^389]: NetGPT: 超越提供个性化生成服务的本地AI网络架构

    NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v1 [cs.LG])

    [http://arxiv.org/abs/2307.06148](http://arxiv.org/abs/2307.06148)

    NetGPT是一个能够在边缘和云端部署适当的大型语言模型的本地AI网络架构，实现了个性化生成服务，并通过协作云边方法论来优化资源协调和互动效果。

    

    大型语言模型（LLMs）通过生成信息在日常生活中取得了巨大成功，LLMs的个性化可能进一步促进它们在应用中的作用，因为它们能更好地与人类意图对齐。针对个性化生成服务，协作云边方法论听起来很有前景，因为它有助于有效协调异构分布式通信和计算资源。在本文中，我们讨论了几种候选的云边协作技术的利弊，提出了NetGPT，根据其计算能力在边缘和云端部署适当的LLMs。此外，边缘LLMs可以高效利用基于位置的信息进行个性化提示完成，从而有益于与云端LLMs的互动。在边缘和云端部署代表性的开源LLMs（例如GPT-2-base和LLaMA模型）之后，我们展示了NetGPT的可行性。

    Large language models (LLMs) have triggered tremendous success to empower daily life by generative information, and the personalization of LLMs could further contribute to their applications due to better alignment with human intents. Towards personalized generative services, a collaborative cloud-edge methodology sounds promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources. In this article, after discussing the pros and cons of several candidate cloud-edge collaboration techniques, we put forward NetGPT to capably deploy appropriate LLMs at the edge and the cloud in accordance with their computing capacity. In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with cloud LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on th
    
[^390]: 使用双调节器解决联邦半监督学习中的数据不平衡问题

    Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])

    [http://arxiv.org/abs/2307.05358](http://arxiv.org/abs/2307.05358)

    本文提出了一种带有双调节器的新型联邦半监督学习框架FedDure，解决了数据分布不平衡的问题。通过粗调节器和细调节器对本地模型的更新进行规范，以及学习适应性加权方案，适应不同的数据分布。

    

    联邦学习已经成为一种从分散异构数据中学习的流行方法。由于分散客户端上标签稀缺，联邦半监督学习（FSSL）出现以从少量标记数据中训练模型。现有的FSSL方法假设客户端之间的标签数据独立且具有相同分布，并且在客户端内部标记和未标记数据之间具有一致的类别分布。本文研究了FSSL的更实际和具有挑战性的情况，即数据分布不仅在客户端之间不同，在客户端内部标记和未标记数据之间也不同。为了解决这个挑战，本文提出了一种带有双调节器的新型FSSL框架，FedDure。FedDure通过粗调节器（C-reg）和细调节器（F-reg）解除了以前的假设：C-reg通过跟踪标记数据分布的学习效果来规范本地模型的更新；F-reg学习一个适应性加权方案，以适应客户端内不同的数据分布。

    Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored f
    
[^391]: 通过得分和流匹配实现无需模拟的Schr\"odinger桥

    Simulation-free Schr\"odinger bridges via score and flow matching. (arXiv:2307.03672v1 [cs.LG])

    [http://arxiv.org/abs/2307.03672](http://arxiv.org/abs/2307.03672)

    [SF]$^2$M是一种无需模拟的方法，用于推断随机动力学。它将连续时间随机生成建模解释为Schr\"odinger桥问题，并通过静态熵正则化最优传输来高效学习。在学习细胞动力学方面表现出更高的准确性和效率。

    

    我们提出了无需模拟的得分和流匹配（[SF]$^2$M），这是一种无需模拟的目标函数，用于推断给定来自任意分布的未配对源和目标样本的随机动力学。我们的方法推广了用于训练扩散模型的得分匹配损失和最近提出的用于训练连续归一化流的流匹配损失。[SF]$^2$M将连续时间随机生成建模解释为Schr\"odinger桥问题。它依赖于静态熵正则化最优传输或小批量近似，以有效地学习Schr\"odinger桥，而无需模拟学习过程。我们发现，与先前的基于模拟的方法相比，[SF]$^2$M更高效并提供了更准确的Schr\"odinger桥解决方案。最后，我们将[SF]$^2$M应用于从快照数据中学习细胞动力学的问题。值得注意的是，[SF]$^2$M是首个能够准确建模高维细胞动力学的方法。

    We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired source and target samples drawn from arbitrary distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schr\"odinger bridge (SB) problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can 
    
[^392]: 可扩展高维多变量线性回归用于特征分布式数据翻译标题

    Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data. (arXiv:2307.03410v1 [stat.ML])

    [http://arxiv.org/abs/2307.03410](http://arxiv.org/abs/2307.03410)

    这篇论文提出了一个适用于特征分布式数据的可扩展高维多变量线性回归算法，具有通信复杂度不依赖于特征维度和快速收敛性的优势，可应用于大规模数据集和具有多变量响应变量的场景。

    

    特征分布式数据是指根据特征划分并存储在多个计算节点上的数据，在具有大量特征的应用中越来越常见。本文提出了一个适用于这种数据的两阶段放松贪婪算法 (TSRGA)，用于应用多变量线性回归。TSRGA 的主要优势在于其通信复杂度不依赖于特征维度，使其能够高度扩展到非常大的数据集。此外，对于多变量响应变量，TSRGA 可用于产生低秩系数估计。通过模拟实验证明了TSRGA 的快速收敛性。最后，我们将提出的TSRGA 应用于一种金融应用中，利用来自 10-K 报告的非结构化数据，证明了其在具有许多密集大维矩阵的应用中的实用性。

    Feature-distributed data, referred to data partitioned by features and stored across multiple computing nodes, are increasingly common in applications with a large number of features. This paper proposes a two-stage relaxed greedy algorithm (TSRGA) for applying multivariate linear regression to such data. The main advantage of TSRGA is that its communication complexity does not depend on the feature dimension, making it highly scalable to very large data sets. In addition, for multivariate response variables, TSRGA can be used to yield low-rank coefficient estimates. The fast convergence of TSRGA is validated by simulation experiments. Finally, we apply the proposed TSRGA in a financial application that leverages unstructured data from the 10-K reports, demonstrating its usefulness in applications with many dense large-dimensional matrices.
    
[^393]: DeepOnto: 一个用于深度学习本体工程的Python包

    DeepOnto: A Python Package for Ontology Engineering with Deep Learning. (arXiv:2307.03067v1 [cs.AI])

    [http://arxiv.org/abs/2307.03067](http://arxiv.org/abs/2307.03067)

    DeepOnto是一个Python包，用于深度学习本体工程。它通过集成深度学习框架和本体API，提供了丰富的工具和算法，支持本体工程任务，如本体对齐和完成。

    

    应用深度学习技术，特别是语言模型（LMs），在本体工程中已经引起了广泛关注。然而，深度学习框架如PyTorch和Tensorflow主要是为Python开发的，而广泛使用的本体API（如OWL API和Jena）主要是基于Java的。为了方便无缝集成这些框架和API，我们提出了Deeponto，一个专为本体工程设计的Python包。该包包括一个基于广泛认可和可靠的OWL API的核心本体处理模块，以更“Pythonic”的方式封装其基本特性，并扩展其功能以包括其他重要组成部分，包括推理、语言化、规范化、投影等。基于这个模块，Deeponto提供了一套工具、资源和算法，支持各种本体工程任务，例如本体对齐和完成，利用深度学习方法实现。

    Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more "Pythonic" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning meth
    
[^394]: 基于群体的鲁棒性：现实世界中定制鲁棒性的通用框架

    Group-based Robustness: A General Framework for Customized Robustness in the Real World. (arXiv:2306.16614v1 [cs.LG])

    [http://arxiv.org/abs/2306.16614](http://arxiv.org/abs/2306.16614)

    本研究提出了一种基于群体的鲁棒性指标，可以更好地评估机器学习模型在现实世界中抵抗攻击的能力，弥补了传统指标的不足。实验证明，该指标能够区分模型对特定威胁的脆弱性。

    

    众所周知，机器学习模型容易受到逃避攻击的影响，即通过扰动模型输入来引起错误分类。本研究中，我们发现传统的度量目标和非目标鲁棒性的指标无法准确评估现实世界中的真实威胁。为了解决现有方法的缺陷，我们正式定义了一种新的指标，称为基于群体的鲁棒性，它补充了现有的度量标准，并更适合评估特定攻击场景下的模型性能。我们通过实验证明，基于群体的鲁棒性能够在传统的鲁棒性指标不适用的情况下区分模型对特定威胁模型的脆弱性。此外，为了有效准确地衡量基于群体的鲁棒性，我们提出了两个损失函数。

    Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss func
    
[^395]: SCENEREPLICA：通过创建可重复的场景来评估现实世界中的机器人操纵能力的基准测试

    SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating Reproducible Scenes. (arXiv:2306.15620v1 [cs.RO])

    [http://arxiv.org/abs/2306.15620](http://arxiv.org/abs/2306.15620)

    SCENEREPLICA是一个基于YCB对象的可重复性基准测试，用于评估现实世界中的机器人操纵能力。此基准测试易于重复并允许研究人员比较不同的技术和算法，有助于加快机器人操纵方法的发展。

    

    我们提出了一个新的可重复性基准测试，用于评估现实世界中的机器人操纵能力，特别关注抓取和放置任务。我们的基准测试使用了YCB对象，这是机器人学界常用的数据集，确保我们的结果可以与其他研究进行比较。此外，此基准测试还被设计为在现实世界中易于重复，使其可供研究人员和实践者使用。我们还提供了对基准测试中基于模型和无模型的6D机器人抓取的实验结果和分析，其中评估了代表性算法在物体感知、抓取规划和运动规划方面的性能。我们相信我们的基准测试将成为推动机器人操纵领域发展的宝贵工具。通过提供一个标准化的评估框架，研究人员可以更容易地比较不同的技术和算法，从而加快发展机器人操纵方法的进展。

    We present a new reproducible benchmark for evaluating robot manipulation in the real world, specifically focusing on pick-and-place. Our benchmark uses the YCB objects, a commonly used dataset in the robotics community, to ensure that our results are comparable to other studies. Additionally, the benchmark is designed to be easily reproducible in the real world, making it accessible to researchers and practitioners. We also provide our experimental results and analyzes for model-based and model-free 6D robotic grasping on the benchmark, where representative algorithms are evaluated for object perception, grasping planning, and motion planning. We believe that our benchmark will be a valuable tool for advancing the field of robot manipulation. By providing a standardized evaluation framework, researchers can more easily compare different techniques and algorithms, leading to faster progress in developing robot manipulation methods.
    
[^396]: 针对GNN加速的输入敏感的稠密-稀疏基本组成元素

    Input-sensitive dense-sparse primitive compositions for GNN acceleration. (arXiv:2306.15155v1 [cs.LG])

    [http://arxiv.org/abs/2306.15155](http://arxiv.org/abs/2306.15155)

    本文提出了一种在不同的输入图和GNN嵌入大小上使用代数重组的方法，通过选择最佳组合来提高GNN加速的性能。

    

    图神经网络（GNN）已成为一类重要的神经网络模型，在社交和金融网络分析等领域越来越受欢迎。GNN计算的不同阶段可以使用稠密和稀疏矩阵运算来建模。文献中已经提出了许多框架和优化技术来加速GNN。然而，在许多具有不同稀疏模式和GNN嵌入大小的输入图上实现一致高性能仍然困难。在本文中，我们提出了对GNN计算进行不同的代数重组，导致了新的密集和稀疏矩阵基本选择和组合。我们表明，这些组合的盈利能力取决于输入图、嵌入大小和目标硬件。我们开发了一个名为SENSEi的系统，它使用数据驱动的自适应策略来选择给定输入图和GNN嵌入大小的最佳组合。我们在广泛的范围上进行了评估。

    Graph neural networks (GNN) have become an important class of neural network models that have gained popularity in domains such as social and financial network analysis. Different phases of GNN computations can be modeled using both dense and sparse matrix operations. There have been many frameworks and optimization techniques proposed in the literature to accelerate GNNs. However, getting consistently high performance across many input graphs with different sparsity patterns and GNN embedding sizes has remained difficult.  In this paper, we propose different algebraic reassociations of GNN computations that lead to novel dense and sparse matrix primitive selections and compositions. We show that the profitability of these compositions depends on the input graph, embedding size, and the target hardware. We developed SENSEi, a system that uses a data-driven adaptive strategy to select the best composition given the input graph and GNN embedding sizes. Our evaluations on a wide range of 
    
[^397]: 自学习遮蔽自编码器是高效视频异常检测器

    Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors. (arXiv:2306.12041v1 [cs.CV])

    [http://arxiv.org/abs/2306.12041](http://arxiv.org/abs/2306.12041)

    提出了一种基于轻量级遮蔽自编码器的高效异常事件检测模型，通过引入基于运动梯度的令牌加权方法，整合教师解码器和学生解码器以及生成合成异常事件，实现共同重构原始帧和对应的像素级异常映射。在三个基准测试中，实现出色的速度和准确性的权衡。

    

    我们提出了一种基于轻量级遮蔽自编码器（AE）的高效异常事件检测模型，该模型应用于视频帧级别。我们的模型的三个创新点：其一，我们引入了基于运动梯度的令牌加权方法，因此避免了学习重构静态背景场景。其二，我们将教师解码器和学生解码器整合到我们的架构中，利用两个解码器给出的输出之间的差异来改善异常检测。其三，我们生成合成异常事件以增强培训视频，并将遮蔽AE模型任务设置为共同重构原始帧（无异常）和对应的像素级异常映射。我们的设计导致高效和有效的模型，通过对三个基准测试进行的广泛实验证明。实验结果显示，我们的模型在速度和准确性之间实现了出色的权衡。

    We propose an efficient abnormal event detection model based on a lightweight masked auto-encoder (AE) applied at the video frame level. The novelty of the proposed model is threefold. First, we introduce an approach to weight tokens based on motion gradients, thus avoiding learning to reconstruct the static background scene. Second, we integrate a teacher decoder and a student decoder into our architecture, leveraging the discrepancy between the outputs given by the two decoders to improve anomaly detection. Third, we generate synthetic abnormal events to augment the training videos, and task the masked AE model to jointly reconstruct the original frames (without anomalies) and the corresponding pixel-level anomaly maps. Our design leads to an efficient and effective model, as demonstrated by the extensive experiments carried out on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. The empirical results show that our model achieves an excellent trade-off between speed and accuracy
    
[^398]: 去噪声点阵格生成3D分子

    3D molecule generation by denoising voxel grids. (arXiv:2306.07473v1 [cs.LG])

    [http://arxiv.org/abs/2306.07473](http://arxiv.org/abs/2306.07473)

    VoxMol是一种根据分数的新方法，可以生成3D分子，并通过学习从噪声分子的平滑分布到真实分子的分布的映射。该方法与当前先进技术不同，具有更简单的训练和更快的速度。

    

    我们提出了一种新的基于分数的方法，用于生成表示为规则网格上的原子密度的3D分子。首先，我们训练了一个去噪声的神经网络，学习从噪声分子的平滑分布到真实分子的分布的映射。然后，我们遵循神经经验贝叶斯框架 [Saremi和Hyvarinen，2019]，通过两个步骤生成分子：（i）通过欠阻尼Langevin马尔可夫链蒙特卡罗从平滑分布中采样带噪声的密度网格，（ii）通过单步去噪噪声格，还原“干净”的分子。我们的方法VoxMol是一种根本不同于当前现有技术（即应用于原子点云的扩散模型）的生成分子的方法。它在数据表示、噪声模型、网络架构和生成建模算法方面不同。VoxMol在无条件3D分子生成方面取得了与现有技术可比较的结果，同时训练简单且更快。

    We propose a new score-based approach to generate 3D molecules represented as atomic densities on regular grids. First, we train a denoising neural network that learns to map from a smooth distribution of noisy molecules to the distribution of real molecules. Then, we follow the neural empirical Bayes framework [Saremi and Hyvarinen, 2019] and generate molecules in two steps: (i) sample noisy density grids from a smooth distribution via underdamped Langevin Markov chain Monte Carlo, and (ii) recover the ``clean'' molecule by denoising the noisy grid with a single step. Our method, VoxMol, generates molecules in a fundamentally different way than the current state of the art (i.e., diffusion models applied to atom point clouds). It differs in terms of the data representation, the noise model, the network architecture and the generative modeling algorithm. VoxMol achieves comparable results to state of the art on unconditional 3D molecule generation while being simpler to train and faste
    
[^399]: 关于适应性预测集期望大小的研究

    On the Expected Size of Conformal Prediction Sets. (arXiv:2306.07254v1 [stat.ML])

    [http://arxiv.org/abs/2306.07254](http://arxiv.org/abs/2306.07254)

    该论文研究了适应性预测集的期望大小问题，提出了一种理论量化方法以及点估计和高概率区间，并在真实数据集上验证了其实用性。

    

    虽然适应性预测器在误差频率方面具有严格的统计保证，但其预测集大小对其实际效用至关重要。不幸的是，目前缺乏有限样本分析和预测集大小的保证。为了解决这个问题，我们在分裂适应性预测框架下理论量化预测集的期望大小。因为这种精确的计算通常无法直接计算，我们进一步推导出可轻松计算的点估计和高概率区间，提供了一种描述测试和校准数据不同可能实现的期望预测集大小的实用方法。此外，我们通过对回归和分类问题的真实世界数据集进行实验证实了我们结果的实用性。

    While conformal predictors reap the benefits of rigorous statistical guarantees for their error frequency, the size of their corresponding prediction sets is critical to their practical utility. Unfortunately, there is currently a lack of finite-sample analysis and guarantees for their prediction set sizes. To address this shortfall, we theoretically quantify the expected size of the prediction set under the split conformal prediction framework. As this precise formulation cannot usually be calculated directly, we further derive point estimates and high probability intervals that can be easily computed, providing a practical method for characterizing the expected prediction set size across different possible realizations of the test and calibration data. Additionally, we corroborate the efficacy of our results with experiments on real-world datasets, for both regression and classification problems.
    
[^400]: 利用扩散模型提取和恢复潜在动力学对齐的时空结构

    Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model. (arXiv:2306.06138v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.06138](http://arxiv.org/abs/2306.06138)

    该论文提出了一种利用扩散模型提取和恢复潜在动力学对齐的时空结构的方法，以解决现有方法忽略潜在动力学时空结构导致对齐后性能质量较差的问题。

    

    在行为相关的脑计算领域，有必要将原始神经群体活动与它们之间的剧烈变化有意义地对齐。然而，由于大多数神经群体活动都以多变量时间序列的方式出现，因此对齐是非常棘手的。神经科学研究中的一个工具性框架认为，基于试验的神经群体活动依赖于低维度潜在动力学。关注这种潜在动力学大大促进了对齐过程。尽管我们取得了相当大的进展，但现有方法通常忽略了潜在动力学中固有的时空结构。因此，在对齐后导致动力学结构和整体性能质量较差。为了解决这个问题，我们提出一种利用扩散模型表达能力的方法来解决这些问题。具体而言，我们利用扩散模型首先提取源域的潜在动力学结构，然后在对齐时进行恢复。

    In the field of behavior-related brain computation, it is necessary to meaningfully align raw neural population activities against the drastic shift between them. However, the alignment is non-trivial since most neural population activities are in a multivariate time-series manner. An instrumental framework within neuroscience research posits that trial-based neural population activities rely on low-dimensional latent dynamics. Focusing on such latent dynamics greatly facilitates the alignment procedure. Despite the considerable progress we have reached, existing methods usually ignore the intrinsic spatio-temporal structures within latent dynamics. Thus, those solutions lead to poor quality in dynamics structures and overall performance after alignment. To tackle this problem, we propose a method leveraging the expressiveness of diffusion model to relieve such issues. Specifically, the latent dynamics structures of the source domain are first extracted by the diffusion model. Then, su
    
[^401]: 错误反馈可以准确地压缩预处理器。

    Error Feedback Can Accurately Compress Preconditioners. (arXiv:2306.06098v1 [cs.LG])

    [http://arxiv.org/abs/2306.06098](http://arxiv.org/abs/2306.06098)

    本论文提出一种错误反馈技术，可以通过在馈入预处理器之前对梯度信息进行压缩（稀疏化或低秩压缩），将预处理器的存储成本压缩多达两个数量级，而不会丢失收敛性。

    

    利用深度网络规模的二阶信息是改进当前用于深度学习优化器性能的主要途径之一。然而，现有的精确全矩阵预处理方法，如全矩阵Adagrad（GGT）或无矩阵近似曲率（M-FAC），即使应用于中等规模模型，也会遇到巨大的存储成本问题，因为它们必须存储梯度的滑动窗口，其存储需求在模型维度中是成倍增加的。本文通过一种高效且易于实现的错误反馈技术来解决这个问题，该技术可以在实践中将预处理器压缩多达两个数量级，而不会丢失收敛性。具体而言，我们的方法在将梯度信息馈入预处理器之前通过稀疏化或低秩压缩压缩梯度信息，将压缩误差反馈到未来的迭代中。对深度神经网络进行了大量实验。

    Leveraging second-order information at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to medium-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via an efficient and simple-to-implement error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression \emph{before} it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks
    
[^402]: BeMap：平衡的消息传递方法用于公平的图神经网络。

    BeMap: Balanced Message Passing for Fair Graph Neural Network. (arXiv:2306.04107v1 [cs.LG])

    [http://arxiv.org/abs/2306.04107](http://arxiv.org/abs/2306.04107)

    本文提出了一种公平的消息传递方法，称为BeMap，旨在解决消息传递中的偏差放大问题，通过平衡感知的采样策略来平衡不同人口群体的1-hop邻居的数量。

    

    图神经网络（GNN）通过迭代地聚合每个节点的局部邻域信息来表现出强大的实证性能，即消息传递。然而，具体证据显示，图神经网络可能对某些人口群体存在偏见，这要求考虑算法的公正性。尽管越来越多的努力在保证图神经网络的算法公平性，但在训练期间往往并不明确考虑消息传递在GNN中引起的偏差。本文首先研究了消息传递中的偏差放大问题。我们通过经验证据和理论证明，当来自不同人口群体的1-hop邻居不平衡时，消息传递可能会放大偏差。在这些分析的指导下，我们提出了BeMap，一种公平的消息传递方法，利用平衡感知的采样策略来平衡每个节点的1-hop邻居的数量。

    Graph Neural Network (GNN) has shown strong empirical performance in many downstream tasks by iteratively aggregating information from the local neighborhood of each node, i.e., message passing. However, concrete evidence has revealed that a graph neural network could be biased against certain demographic groups, which calls for the consideration of algorithmic fairness. Despite the increasing efforts in ensuring algorithmic fairness on graph neural networks, they often do not explicitly consider the induced bias caused by message passing in GNN during training. In this paper, we first investigate the problem of bias amplification in message passing. We empirically and theoretically demonstrate that message passing could amplify the bias when the 1-hop neighbors from different demographic groups are unbalanced. Guided by such analyses, we propose BeMap, a fair message passing method, that leverages a balance-aware sampling strategy to balance the number of the 1-hop neighbors of each n
    
[^403]: 神经元激活覆盖度：重新思考离域检测和泛化问题

    Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization. (arXiv:2306.02879v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02879](http://arxiv.org/abs/2306.02879)

    本文通过研究神经元激活状态，提出了神经元激活覆盖度（NAC）作为衡量神经元行为的指标。利用NAC可以有效区分域内和离域输入，简化离域检测问题，并且NAC与模型的泛化能力间存在正相关关系。

    

    离域问题通常在神经网络遇到明显偏离训练数据分布的数据时出现，即在域内数据（InD）之外。本文从神经元激活的角度研究了离域问题。我们首先通过考虑神经元的输出和其对模型决策的影响来定义了神经元激活状态。然后，为了描述神经元与离域问题的关系，我们引入了神经元激活覆盖度（NAC）——一种衡量神经元在域内数据下行为的简单度量。利用我们的NAC，我们展示了：1）基于神经元行为可以在很大程度上区分域内和离域输入，大大简化了离域检测问题，并在三个基准测试集（CIFAR-10、CIFAR-100和ImageNet-1K）上超过了21个先前的方法；2）NAC与模型的泛化能力之间存在正相关关系，这种关系在不同架构和数据集上一致成立，使得基于NAC的准则可以用于评估模型的泛化能力。

    The out-of-distribution (OOD) problem generally arises when neural networks encounter data that significantly deviates from the training data distribution, i.e., in-distribution (InD). In this paper, we study the OOD problem from a neuron activation view. We first formulate neuron activation states by considering both the neuron output and its influence on model decisions. Then, to characterize the relationship between neurons and OOD issues, we introduce the \textit{neuron activation coverage} (NAC) -- a simple measure for neuron behaviors under InD data. Leveraging our NAC, we show that 1) InD and OOD inputs can be largely separated based on the neuron behavior, which significantly eases the OOD detection problem and beats the 21 previous methods over three benchmarks (CIFAR-10, CIFAR-100, and ImageNet-1K). 2) a positive correlation between NAC and model generalization ability consistently holds across architectures and datasets, which enables a NAC-based criterion for evaluating mod
    
[^404]: 简单形式映射神经网络中的可解释性

    Explainability in Simplicial Map Neural Networks. (arXiv:2306.00010v1 [cs.LG])

    [http://arxiv.org/abs/2306.00010](http://arxiv.org/abs/2306.00010)

    本文提出了简单形式映射神经网络（SMNN）的训练过程和替代凸多面体的方法，并且首次引入了 SMNN 的可解释性能力。

    

    简单形式映射神经网络（SMNN）是基于拓扑学的神经网络，具有普适逼近能力和在适当条件下对抗性示例的鲁棒性。然而，在高维中应用 SMNN 存在一些瓶颈，首先没有定义 SMNN 的训练过程，其次对于输入数据集需要构建一个包围凸多面体。本文提出了基于给定数据集的支持子集和投影到超球面的方法作为替代凸多面体的 SMNN 训练过程，并首次引入了 SMNN 的可解释性能力。

    Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper.
    
[^405]: 基于预测误差的增量学习分类方法

    Prediction Error-based Classification for Class-Incremental Learning. (arXiv:2305.18806v1 [cs.LG])

    [http://arxiv.org/abs/2305.18806](http://arxiv.org/abs/2305.18806)

    本论文提出了一种新的增量学习分类方法——基于预测误差的分类方法（PEC）。对PEC的评估表明，在各种基准测试中，PEC可以与最先进的增量学习方法相竞争，并具有许多实际优势，例如样本效率高、易于调整。

    

    增量学习分类是连续学习中的一个挑战性问题，目标是学习来区分所有类别。现有的方法在处理大量分类时容易出现过度遗忘和分数不均衡。本研究提出了一种新方法，名为预测误差分类（PEC），它与传统的判别和生成分类范式有所不同。PEC通过测量模型在从该类别中学习的数据上复制随机神经网络输出的预测误差来计算类别得分。该方法可以解释为基于高斯过程后验方差的分类规则的近似。PEC具有几个实际优势，包括样本效率高、易于调整以及即使在逐个呈现数据时也很有效。本文的实证结果表明PEC在广泛的基准测试中表现出色，可以与最先进的增量学习方法相竞争。

    Class-incremental learning (CIL) is a particularly challenging variant of continual learning, where the goal is to learn to discriminate between all classes presented in an incremental fashion. Existing approaches often suffer from excessive forgetting and imbalance of the scores assigned to classes that have not been seen together during training. In this study, we introduce a novel approach, Prediction Error-based Classification (PEC), which differs from traditional discriminative and generative classification paradigms. PEC computes a class score by measuring the prediction error of a model trained to replicate the outputs of a frozen random neural network on data from that class. The method can be interpreted as approximating a classification rule based on Gaussian Process posterior variance. PEC offers several practical advantages, including sample efficiency, ease of tuning, and effectiveness even when data are presented one class at a time. Our empirical results show that PEC pe
    
[^406]: HiFA: 高保真度的文本到3D图像合成及其先进的扩散引导策略

    HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. (arXiv:2305.18766v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.18766](http://arxiv.org/abs/2305.18766)

    该论文提出了一种高保真度的文本到3D图像合成方法，并引入了先进的扩散引导策略。通过对NeRF渲染图像进行辅助深度监督和规范化密度场来提高3D几何表示。实验证明该方法优于以前的工作，产生了先进的照片真实感和改进的多视角一致性。

    

    通过优化3D模型，自动文本到3D合成在提升中已经取得了显著进展。现有方法通常依赖于预训练的文本到图像生成模型（如扩散模型），提供神经辐射场（NeRFs）的2D渲染得分并用于优化NeRFs。然而，由于其对3D几何的有限理解，这些方法经常遇到多个视角上的伪影和不一致现象。为了解决这些限制，我们提出了使用扩散先验重新制定优化损失的方法。此外，我们引入了一种新的训练方法，释放了扩散先验的潜力。为了提高3D几何表示，我们对NeRF渲染图像进行辅助深度监督，并规范化NeRF的密度场。大量实验证明了我们的方法优于以前的工作，产生了先进的照片真实感和改进的多视角一致性。

    Automatic text-to-3D synthesis has achieved remarkable advancements through the optimization of 3D models. Existing methods commonly rely on pre-trained text-to-image generative models, such as diffusion models, providing scores for 2D renderings of Neural Radiance Fields (NeRFs) and being utilized for optimizing NeRFs. However, these methods often encounter artifacts and inconsistencies across multiple views due to their limited understanding of 3D geometry. To address these limitations, we propose a reformulation of the optimization loss using the diffusion prior. Furthermore, we introduce a novel training approach that unlocks the potential of the diffusion prior. To improve 3D geometry representation, we apply auxiliary depth supervision for NeRF-rendered images and regularize the density field of NeRFs. Extensive experiments demonstrate the superiority of our method over prior works, resulting in advanced photo-realism and improved multi-view consistency.
    
[^407]: 具有信息预算的情境赌博机算法

    Contextual Bandits with Budgeted Information Reveal. (arXiv:2305.18511v1 [cs.LG])

    [http://arxiv.org/abs/2305.18511](http://arxiv.org/abs/2305.18511)

    本文介绍了一种针对医疗领域“亲治疗”操作的限制，且考虑到了操作预算的具有信息预算的情境赌博机算法，这种算法将在线原始-对偶算法和情境赌博机学习算法有机地结合在一起，取得了很好的效果。

    

    情境赌博机算法常用于推荐个性化的医疗处理方式，但在实际操作中，为保证治疗效果，医生通常需要要求患者采取没有直接好处的“亲治疗”操作，而且临床医生的操作预算有限。本文提出了一种新的优化学习算法，有效结合了两种算法方法之长：1）一个决定最佳时机与患者联系的在线原始-对偶（primal-dual）算法，2）用于向患者提供个性化治疗的情境赌博机学习算法。我们证明了该算法具有亚线性的回归界限。我们在合成和实际数据上展示了该算法的实用价值。

    Contextual bandit algorithms are commonly used in digital health to recommend personalized treatments. However, to ensure the effectiveness of the treatments, patients are often requested to take actions that have no immediate benefit to them, which we refer to as pro-treatment actions. In practice, clinicians have a limited budget to encourage patients to take these actions and collect additional information. We introduce a novel optimization and learning algorithm to address this problem. This algorithm effectively combines the strengths of two algorithmic approaches in a seamless manner, including 1) an online primal-dual algorithm for deciding the optimal timing to reach out to patients, and 2) a contextual bandit learning algorithm to deliver personalized treatment to the patient. We prove that this algorithm admits a sub-linear regret bound. We illustrate the usefulness of this algorithm on both synthetic and real-world data.
    
[^408]: 优化退火算法的误差界和局部搜索策略

    Sharpened Lazy Incremental Quasi-Newton Method. (arXiv:2305.17283v1 [math.OC])

    [http://arxiv.org/abs/2305.17283](http://arxiv.org/abs/2305.17283)

    本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。

    

    本文考虑了具有$Lipschitz$连续Hessian矩阵在$d$维空间中，$n$个强凸光滑函数的有限和最小化问题。在许多应用中，$n$的数量很大，因此必须使用每迭代一次与$n$无关的增量式或随机算法。本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。

    We consider the finite sum minimization of $n$ strongly convex and smooth functions with Lipschitz continuous Hessians in $d$ dimensions. In many applications where such problems arise, including maximum likelihood estimation, empirical risk minimization, and unsupervised learning, the number of observations $n$ is large, and it becomes necessary to use incremental or stochastic algorithms whose per-iteration complexity is independent of $n$. Of these, the incremental/stochastic variants of the Newton method exhibit superlinear convergence, but incur a per-iteration complexity of $O(d^3)$, which may be prohibitive in large-scale settings. On the other hand, the incremental Quasi-Newton method incurs a per-iteration complexity of $O(d^2)$ but its superlinear convergence rate has only been characterized asymptotically. This work puts forth the Sharpened Lazy Incremental Quasi-Newton (SLIQN) method that achieves the best of both worlds: an explicit superlinear convergence rate with a per-
    
[^409]: 大型语言模型作为工具制造者

    Large Language Models as Tool Makers. (arXiv:2305.17126v1 [cs.LG])

    [http://arxiv.org/abs/2305.17126](http://arxiv.org/abs/2305.17126)

    本文提出了一个闭环框架，即LLMs作为工具制造者（LATM），使LLMs能够自主地创建用于解决问题的工具，而不需要依赖于现有的外部工具。

    

    最近的研究表明，通过使用外部工具，大型语言模型（LLMs）可以增强其问题解决能力的潜力。然而，在这方面的先前工作依赖于现有工具的可用性。在本文中，我们提出了一个闭环框架，称为LLMs As Tool Makers（LATM），以消除这种依赖性，其中LLMs创建自己的可重用工具来解决问题。我们的方法包括两个关键阶段：1）制造工具：LLM作为工具制造者，为给定任务制作工具，其中工具作为Python实用函数实现。2）使用工具：LLM作为工具用户，应用工具制造者构建的工具来解决问题。工具用户可以是与工具制造者相同或不同的LLM。工具制造使LLM能够不断生成可应用于不同请求的工具，以便将来请求在解决问题时能调用相应的API。

    Recent research shows the potential of enhancing the problem-solving ability of large language models (LLMs) through the use of external tools. However, prior work along this line depends on the availability of existing tools. In this work, we take an initial step towards removing this dependency by proposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two key phases: 1) tool making: an LLM acts as the tool maker that crafts tools for given tasks, where a tool is implemented as a Python utility function. 2) tool using: an LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the 
    
[^410]: 向量值变分空间和DNN的宽度界：关于权重衰减正则化的见解。

    Vector-Valued Variation Spaces and Width Bounds for DNNs: Insights on Weight Decay Regularization. (arXiv:2305.16534v1 [stat.ML])

    [http://arxiv.org/abs/2305.16534](http://arxiv.org/abs/2305.16534)

    该论文提供了关于通过加权衰减训练的多输出ReLU神经网络的函数类型和相应的解决方案的新见解。

    

    深度神经网络(DNNs)通过梯度下降最小化损失项和平方权重和相应，对应于训练加权衰减的常见方法。本文提供了有关这种常见学习框架的新见解。我们表征了训练加权衰减以获得多输出(向量值)ReLU神经网络学习的函数类型。这扩展了先前限于单输出(标量值)网络的表征。这种表征需要定义我们称之为向量值变分(VV)空间的新类神经函数空间。我们通过一种新的表征定理证明，神经网络(NNs)是通过VV空间中提出学习问题的最优解。这个新的表征定理表明，这些学习问题的解存在于宽度受训练数据数限制的向量值神经网络中。接下来，通过与多任务lasso问题的新联系，我们导出了

    Deep neural networks (DNNs) trained to minimize a loss term plus the sum of squared weights via gradient descent corresponds to the common approach of training with weight decay. This paper provides new insights into this common learning framework. We characterize the kinds of functions learned by training with weight decay for multi-output (vector-valued) ReLU neural networks. This extends previous characterizations that were limited to single-output (scalar-valued) networks. This characterization requires the definition of a new class of neural function spaces that we call vector-valued variation (VV) spaces. We prove that neural networks (NNs) are optimal solutions to learning problems posed over VV spaces via a novel representer theorem. This new representer theorem shows that solutions to these learning problems exist as vector-valued neural networks with widths bounded in terms of the number of training data. Next, via a novel connection to the multi-task lasso problem, we derive
    
[^411]: 增加亚群模型性能的集成合成电子病历生成

    Ensemble Synthetic EHR Generation for Increasing Subpopulation Model's Performance. (arXiv:2305.16363v1 [cs.LG])

    [http://arxiv.org/abs/2305.16363](http://arxiv.org/abs/2305.16363)

    本文提出了一种利用生成模型集成合成数据的方法，以提高训练机器学习模型在代表不足的亚群体的性能。

    

    电子病历（EHR）通常包含某些亚群体（SP）的不同比例表示。患者的人口统计学、临床情况的流行程度和医疗中心类型等因素导致这种不充分的代表性。因此，当在这种数据集上训练机器学习模型时，模型很难进行良好的概括并在代表不足的SP上表现不佳。为了解决这个问题，我们提出了一种利用生成模型的新型集成框架。具体来说，我们为每个SP训练一个基于GAN的合成数据生成器，并将合成样本纳入每个SP的训练集中。最终，我们训练SP特定的预测模型。为了正确评估该方法，我们设计了一个评估流程，并使用从MIMIC数据库查询的两个真实用例数据集。我们的方法显示出在代表不足的SP上增加了模型的性能。我们的代码和模型将作为补充材料提供，并将在公共存储库上提供。

    Electronic health records (EHR) often contain different rates of representation of certain subpopulations (SP). Factors like patient demographics, clinical condition prevalence, and medical center type contribute to this underrepresentation. Consequently, when training machine learning models on such datasets, the models struggle to generalize well and perform poorly on underrepresented SPs. To address this issue, we propose a novel ensemble framework that utilizes generative models. Specifically, we train a GAN-based synthetic data generator for each SP and incorporate synthetic samples into each SP training set. Ultimately, we train SP-specific prediction models. To properly evaluate this method, we design an evaluation pipeline with 2 real-world use case datasets, queried from the MIMIC database. Our approach shows increased model performance over underrepresented SPs. Our code and models are given as supplementary and will be made available on a public repository.
    
[^412]: 条件分布之间的经验最优输运

    Empirical Optimal Transport between Conditional Distributions. (arXiv:2305.15901v1 [cs.LG])

    [http://arxiv.org/abs/2305.15901](http://arxiv.org/abs/2305.15901)

    本文考虑在一个公共变量的条件下，相应分布之间的最优输运问题。通过采用基于 MMD 的核正则化器，克服了条件变量是连续的和两个分布中该变量的边缘是不同的挑战。

    

    给定两个联合分布的样本，考虑在一个公共变量的条件下，相应分布之间的最优输运问题。本文的目标是估计伴随条件值的输运成本（Wasserstein 距离），以及条件分布间的输运计划。由于匹配条件分布是监督训练判别模型和（隐式）条件生成模型的核心，条件分布之间的最优输运具有在不同的机器学习应用中被应用的潜力。然而，由于涉及到隐式特定于联合（样本）的条件分布，因此制定这个问题是具有挑战性的，特别是在（i）条件变量是连续的和（ii）两个分布中该变量的边缘是不同的情况下。我们通过采用特定的基于 MMD（最大均值差异）的核正则化器来克服这些挑战。

    Given samples from two joint distributions, we consider the problem of Optimal Transportation (OT) between the corresponding distributions conditioned on a common variable. The objective of this work is to estimate the associated transport cost (Wasserstein distance) as well as the transport plan between the conditionals as a function of the conditioned value. Since matching conditional distributions is at the core of supervised training of discriminative models and (implicit) conditional-generative models, OT between conditionals has the potential to be employed in diverse machine learning applications. However, since the conditionals involved in OT are implicitly specified via the joint samples, it is challenging to formulate this problem, especially when (i) the variable conditioned on is continuous and (ii) the marginal of this variable in the two distributions is different. We overcome these challenges by employing a specific kernel MMD (Maximum Mean Discrepancy) based regularizer
    
[^413]: 本地贝叶斯优化的行为和收敛性

    The Behavior and Convergence of Local Bayesian Optimization. (arXiv:2305.15572v1 [cs.LG])

    [http://arxiv.org/abs/2305.15572](http://arxiv.org/abs/2305.15572)

    本文研究了贝叶斯本地优化策略的行为和收敛性，并在高维问题上提供了强大的实证性能。统计数据表明，单个高斯过程样本路径的本地解比全局方法恢复的预期值更好。Müller等人提出的贝叶斯本地优化算法的收敛速率在有噪音和无噪音的情况下都有推导。

    

    贝叶斯优化中一项最新的发展是使用本地优化策略，与传统的全局策略相比，可以在高维问题上提供强大的实证性能。文献中的“传统智慧”是，专注于本地优化规避了维度诅咒。然而，对于贝叶斯本地优化例程的预期行为或收敛性了解甚少。我们首先研究了本地方法的行为，并发现高斯过程样本路径单个本地解的统计数据与从全局方法恢复的预期值相比非常好。然后，我们展示了最近由Müller等人提出的基于贝叶斯本地优化算法的第一次严格分析，并在有噪音和无噪音的情况下推导出收敛速率。

    A recent development in Bayesian optimization is the use of local optimization strategies, which can deliver strong empirical performance on high-dimensional problems compared to traditional global strategies. The "folk wisdom" in the literature is that the focus on local optimization sidesteps the curse of dimensionality; however, little is known concretely about the expected behavior or convergence of Bayesian local optimization routines. We first study the behavior of the local approach, and find that the statistics of individual local solutions of Gaussian process sample paths are surprisingly good compared to what we would expect to recover from global methods. We then present the first rigorous analysis of such a Bayesian local optimization algorithm recently proposed by M\"uller et al. (2021), and derive convergence rates in both the noisy and noiseless settings.
    
[^414]: 基于分数的算子 Newton 方法用于测量运输

    A score-based operator Newton method for measure transport. (arXiv:2305.09792v1 [math.ST])

    [http://arxiv.org/abs/2305.09792](http://arxiv.org/abs/2305.09792)

    本文提出一种新的基于分数的算子Newton方法，可以迭代构造一个易处理的原概率测度，该方法可以在满足目标分数光滑性假设下，实现快速收敛性。

    

    概率测度的运输是统计学和机器学习中许多核心任务的基础，从变分推理到生成建模。一个典型的目标是将一个感兴趣的目标概率测度表示为通过学习的映射将一个易处理的原概率测度推向前面。我们提出了一种新的构建这样一个运输映射的方法，给出了评估目标分布分数的能力。具体而言，我们将该映射特征化为一个无穷维的分数残差算子的零，并推导出一种迭代构造这样一个零的牛顿类型方法。通过调用偏微分方程的经典椭圆正则性理论，我们证明了这些迭代的收敛性，并表明在目标分数光滑性假设下，这种构造具有快速收敛性。我们方法的一个关键元素是将基本的牛顿方法推广到无穷维算子，其他形式的无穷维算子已经出现在非线性 PDE 中。

    Transportation of probability measures underlies many core tasks in statistics and machine learning, from variational inference to generative modeling. A typical goal is to represent a target probability measure of interest as the push-forward of a tractable source measure through a learned map. We present a new construction of such a transport map, given the ability to evaluate the score of the target distribution. Specifically, we characterize the map as a zero of an infinite-dimensional score-residual operator and derive a Newton-type method for iteratively constructing such a zero. We prove convergence of these iterations by invoking classical elliptic regularity theory for partial differential equations (PDE) and show that this construction enjoys rapid convergence, under smoothness assumptions on the target score. A key element of our approach is a generalization of the elementary Newton method to infinite-dimensional operators, other forms of which have appeared in nonlinear PDE
    
[^415]: 评估用于液体氩探测器低能物理的Few-Hits机器学习分类算法

    Assessment of few-hits machine learning classification algorithms for low energy physics in liquid argon detectors. (arXiv:2305.09744v1 [physics.ins-det])

    [http://arxiv.org/abs/2305.09744](http://arxiv.org/abs/2305.09744)

    本文评估了在液体氩探测器低能物理中使用Few-Hits机器学习分类算法的效果，证明在单比特与双比特事件的分类问题上，卷积神经网络和Transformer-Encoder方法优于传统算法，并针对DUNE Phase II探测器优化了探测器参数。

    

    在低能区域，大型液体氩TPCs的物理潜力仍未充分利用，因为Few-Hits事件所编码的信息很难被传统分类算法利用。机器学习技术在这些类型的分类问题中表现最佳。本文评估了它们在传统（确定性）算法中的表现。我们证明，卷积神经网络（CNN）和Transformer-Encoder方法在低能物理中最具挑战性的分类问题（单比特与双比特事件）中优于确定性算法。我们讨论了Transformer-Encoder方法相对于CNN的优缺点，并利用这些方法优化了探测器参数，重点关注DUNE Phase II探测器（"机会模块"）。

    The physics potential of massive liquid argon TPCs in the low-energy regime is still to be fully reaped because few-hits events encode information that can hardly be exploited by conventional classification algorithms. Machine learning (ML) techniques give their best in these types of classification problems. In this paper, we evaluate their performance against conventional (deterministic) algorithms. We demonstrate that both Convolutional Neural Networks (CNN) and Transformer-Encoder methods outperform deterministic algorithms in one of the most challenging classification problems of low-energy physics (single- versus double-beta events). We discuss the advantages and pitfalls of Transformer-Encoder methods versus CNN and employ these methods to optimize the detector parameters, with an emphasis on the DUNE Phase II detectors ("Module of Opportunity").
    
[^416]: DeepTextMark：基于深度学习的文本水印技术用于检测大语言模型生成的文本

    DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text. (arXiv:2305.05773v1 [cs.MM])

    [http://arxiv.org/abs/2305.05773](http://arxiv.org/abs/2305.05773)

    本文提出了一种基于深度学习的文本水印技术DeepTextMark，可用于检测大语言模型生成的文本。该技术实现了盲目性、鲁棒性、隐蔽性和可靠性，并在水印检测精度和抵抗攻击方面优于现有方法。

    

    随着大型语言模型（LLM）的迅速发展，文本生成器的能力得到了提升。为了防止潜在的滥用，检测文本是否由LLM生成变得越来越重要。一些相关的工作试图使用将输入文本分类为人类编写的或LLM生成的二元分类器来解决这个问题。然而，这些分类器已被证明是不可靠的。由于分类结果可能对具有影响力的决策产生影响，文本源的检测需要具有高质量。为此，本文提出了DeepTextMark，一种基于深度学习的文本水印方法，用于文本源检测。DeepTextMark通过应用Word2Vec和句子编码进行水印插入，并使用基于Transformer的分类器进行水印检测，同时实现了盲目性、鲁棒性、隐蔽性和可靠性。正如本文所进一步讨论的那样，这些特性对于通用文本源检测是不可或缺的，并且DeepTextMark在水印检测精度和抵抗攻击方面优于现有方法。

    The capabilities of text generators have grown with the rapid development of Large Language Models (LLM). To prevent potential misuse, the ability to detect whether texts are produced by LLM has become increasingly important. Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously. As discussed further in the paper, these traits are indispensable for generic text source detec
    
[^417]: 探究大型语言模型在生成单元测试方面的有效性

    Exploring the Effectiveness of Large Language Models in Generating Unit Tests. (arXiv:2305.00418v1 [cs.SE])

    [http://arxiv.org/abs/2305.00418](http://arxiv.org/abs/2305.00418)

    本文研究了三种生成模型在单元测试生成方面的效果，并发现在不经过微调的情况下，它们的覆盖率较低且存在测试味道问题。

    

    代码生成模型可以通过使用代码注释、现有代码或两者的组合来生成代码。本文调查了三个生成模型（CodeGen、Codex和GPT-3.5）在没有微调的情况下是否能够成功用于生成单元测试的效果。研究中使用了两个基准（HumanEval和Evosuite SF110）来调查环境生成对单元测试生成过程的影响。我们根据编译率、测试正确性、覆盖率和测试味道来评估模型。我们发现，Codex模型在HumanEval数据集上取得了超过80%的覆盖率，但在EvoSuite SF110基准中没有一个模型超过2%的覆盖率。生成的测试还存在测试味道问题，比如重复的断言和空测试。

    A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning. To fill this gap, we investigated how well three generative models (CodeGen, Codex, and GPT-3.5) can generate test cases. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the context generation's effect in the unit test generation process. We evaluated the models based on compilation rates, test correctness, coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.
    
[^418]: 用基于随机梯度下降的内点算法求解光滑有界约束优化问题

    A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems. (arXiv:2304.14907v1 [math.OC])

    [http://arxiv.org/abs/2304.14907](http://arxiv.org/abs/2304.14907)

    本文提出了一种用于求解光滑有界约束优化问题的内点算法。它使用基于随机梯度估计的搜索方向和内部邻域，能够在确定性和随机性设置下具有收敛保证，并且在实验中表现出优于传统方法的性能。

    

    本文提出并分析了一种基于随机梯度估计的内点算法，用于求解存在约束的连续可微非凸目标函数最小化问题，并通过实验结果进行了演示。该算法在解决光滑（非凸）优化问题时与其他内点方法不同之处在于搜索方向是通过随机梯度估计计算得到的。它在使用可行域的内部邻域（由正且消失的邻域参数序列定义）的过程中也很独特，通过将迭代强制保留在该邻域内。实验结果表明，通过精心平衡屏障、步长和邻域序列，该算法能够满足确定性和随机性设置下的收敛保证。在两种设置下，数值实验的结果表明，该算法可以优于投影-（随机）梯度方法。

    A stochastic-gradient-based interior-point algorithm for minimizing a continuously differentiable objective function (that may be nonconvex) subject to bound constraints is presented, analyzed, and demonstrated through experimental results. The algorithm is unique from other interior-point methods for solving smooth (nonconvex) optimization problems since the search directions are computed using stochastic gradient estimates. It is also unique in its use of inner neighborhoods of the feasible region -- defined by a positive and vanishing neighborhood-parameter sequence -- in which the iterates are forced to remain. It is shown that with a careful balance between the barrier, step-size, and neighborhood sequences, the proposed algorithm satisfies convergence guarantees in both deterministic and stochastic settings. The results of numerical experiments show that in both settings the algorithm can outperform a projected-(stochastic)-gradient method.
    
[^419]: 张量神经网络在百慕大掉期定价中的应用

    Application of Tensor Neural Networks to Pricing Bermudan Swaptions. (arXiv:2304.09750v1 [q-fin.CP])

    [http://arxiv.org/abs/2304.09750](http://arxiv.org/abs/2304.09750)

    本论文用张量神经网络(TNN)对百慕大掉期进行定价，相比于传统方法，TNN具有更快的收敛速度和减少参数敏感度的优点。

    

    Cheyette模型是一种准高斯波动率利率模型，广泛用于定价利率衍生品，例如欧式掉期和百慕大掉期，而蒙特卡罗模拟已成为行业标准。在低维度下，这些方法为欧式掉期提供了准确而稳健的价格，但即使在这种计算简单的情况下，当使用状态变量作为回归器时，它们也会低估百慕大掉期的价值。这主要是由于所用回归器中预先确定的基函数数量有限。此外，在高维环境中，这些方法也面临着维度灾难的问题。为了解决这些问题，研究者提出利用张量神经网络(TNN)来进行百慕大掉期的定价。研究结果表明，与传统方法相比，TNN具有更快的收敛速度，对于回归器中所用基函数的数量等参数，减少了敏感度。数值实验证实TNN能够在高维度情况下准确地定价欧式掉期和百慕大掉期。

    The Cheyette model is a quasi-Gaussian volatility interest rate model widely used to price interest rate derivatives such as European and Bermudan Swaptions for which Monte Carlo simulation has become the industry standard. In low dimensions, these approaches provide accurate and robust prices for European Swaptions but, even in this computationally simple setting, they are known to underestimate the value of Bermudan Swaptions when using the state variables as regressors. This is mainly due to the use of a finite number of predetermined basis functions in the regression. Moreover, in high-dimensional settings, these approaches succumb to the Curse of Dimensionality. To address these issues, Deep-learning techniques have been used to solve the backward Stochastic Differential Equation associated with the value process for European and Bermudan Swaptions; however, these methods are constrained by training time and memory. To overcome these limitations, we propose leveraging Tensor Neura
    
[^420]: 使用线性规划在马尔可夫决策过程上进行在线强化学习

    Online Reinforcement Learning in Markov Decision Process Using Linear Programming. (arXiv:2304.00155v1 [cs.LG])

    [http://arxiv.org/abs/2304.00155](http://arxiv.org/abs/2304.00155)

    本论文提出了一种在未知转移矩阵和固定但未知分布的情况下进行在线MDP学习的简单而高效的方法，可以实现更紧的遗憾界，并通过置信区间框架改进了现有算法。

    

    本文考虑了具有未知转移矩阵和固定但未知分布的随机奖励的情况下，马尔可夫决策过程中的在线强化学习。学习者旨在通过与环境交互来学习最优策略并在有限的时间内最小化他们的遗憾。我们设计了一种简单而高效的模型算法，通过保持过渡和奖励函数的置信区间并使用占用度量将在线MDP与线性规划相连接，实现了$\tilde{O}(LX\sqrt{TA})$的高概率遗憾界。它比现有的使用类似置信区间框架的算法实现了更紧的遗憾界并改善了计算效率。

    We consider online reinforcement learning in episodic Markov decision process (MDP) with an unknown transition matrix and stochastic rewards drawn from a fixed but unknown distribution. The learner aims to learn the optimal policy and minimize their regret over a finite time horizon through interacting with the environment. We devise a simple and efficient model-based algorithm that achieves $\tilde{O}(LX\sqrt{TA})$ regret with high probability, where $L$ is the episode length, $T$ is the number of episodes, and $X$ and $A$ are the cardinalities of the state space and the action space, respectively. The proposed algorithm, which is based on the concept of "optimism in the face of uncertainty", maintains confidence sets of transition and reward functions and uses occupancy measures to connect the online MDP with linear programming. It achieves a tighter regret bound compared to the existing works that use a similar confidence sets framework and improves the computational effort compared
    
[^421]: 用于随机控制和博弈的机器学习方法的最新发展

    Recent Developments in Machine Learning Methods for Stochastic Control and Games. (arXiv:2303.10257v1 [math.OC])

    [http://arxiv.org/abs/2303.10257](http://arxiv.org/abs/2303.10257)

    本文回顾了基于机器学习的随机控制问题和博弈的计算方法，特别是着重介绍了使用深度学习算法解决高维度和非常复杂结构情况下问题的新方法。

    

    随机最优控制和博弈已经在金融、经济学、社会科学、机器人和能源管理等领域中找到了广泛的应用。许多真实世界的应用都涉及到复杂的模型，这推动了先进的数值方法的发展。最近，基于机器学习的计算方法已经发展用于随机控制问题和博弈。我们回顾这些方法，重点关注已经解锁了高维度和非常复杂结构情况下解决此类问题的可能性的深度学习算法，这是传统数值方法无法完成的。在这里，我们主要考虑连续时间和连续空间设置。许多新方法基于最近用于高维偏微分方程或反向随机微分方程的神经网络方法，或者基于无模型强化学习的马尔科夫决策过程，这导致了突破性的发展。

    Stochastic optimal control and games have found a wide range of applications, from finance and economics to social sciences, robotics and energy management. Many real-world applications involve complex models which have driven the development of sophisticated numerical methods. Recently, computational methods based on machine learning have been developed for stochastic control problems and games. We review such methods, with a focus on deep learning algorithms that have unlocked the possibility to solve such problems even when the dimension is high or when the structure is very complex, beyond what is feasible with traditional numerical methods. Here, we consider mostly the continuous time and continuous space setting. Many of the new approaches build on recent neural-network based methods for high-dimensional partial differential equations or backward stochastic differential equations, or on model-free reinforcement learning for Markov decision processes that have led to breakthrough 
    
[^422]: 通过最小批量优化传输改进和泛化基于流的生成模型

    Improving and generalizing flow-based generative models with minibatch optimal transport. (arXiv:2302.00482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00482](http://arxiv.org/abs/2302.00482)

    这篇论文提出了一种称为广义条件流匹配（CFM）的技术，在连续正则化流（CNFs）的生成模型中无需模拟训练，极大提高了效率和稳定性。此外，论文还引入了最优传输CFM（OT-CFM）的变体，可以以无模拟方式计算动态OT，加速了推断过程。

    

    连续正则化流（CNFs）是一种吸引人的生成建模技术，但由于其基于模拟的最大似然训练存在局限性而受到约束。我们介绍了广义条件流匹配（CFM）技术，这是一种针对CNFs的无模拟训练目标的集合。CFM具有类似于扩散模型中用于训练随机流的稳定回归目标，但同时享有确定性流模型的高效推断。与扩散模型和之前的CNF训练算法相比，CFM不需要源分布为高斯分布，也不需要对其密度进行评估。我们的目标的一种变体是最优传输CFM（OT-CFM），它创建了更简单的流，更容易训练，并且导致更快的推断，如我们的实验证明所示。此外，OT-CFM是第一种以无模拟方式计算动态OT的方法。使用CFM训练CNFs可以改进各种条件和...

    Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, OT-CFM is the first method to compute dynamic OT in a simulation-free way. Training CNFs with CFM improves results on a variety of conditional and u
    
[^423]: DiffSTG: 带有去噪扩散模型的概率时空图预测

    DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models. (arXiv:2301.13629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13629](http://arxiv.org/abs/2301.13629)

    本论文提出了一种新的方法DiffSTG，该方法结合了STGNN的时空学习能力和扩散模型的不确定性测量，可以有效减小STG预测中的排名概率分数和均方根误差。

    

    时空图神经网络（STGNN）已成为时空图（STG）预测的主要模型。然而，它们无法对STG数据中的内在不确定性进行建模，这使得它们在决策任务中的实用性受到限制。为此，本文关注概率STG预测，由于建模不确定性和复杂的ST依赖关系的困难，这是一个具有挑战性的问题。在本研究中，我们首次尝试将流行的去噪扩散概率模型推广到STG，提出了一种称为DiffSTG的新的非自回归框架，并在该框架中引入了第一个STG去噪网络UGnet。我们的方法将STGNN的时空学习能力与扩散模型的不确定性测量相结合。大量实验证实DiffSTG将持续排名概率分数（CRPS）降低了4%-14%，均方根误差（RMSE）降低了2%-7%。

    Spatio-temporal graph neural networks (STGNN) have emerged as the dominant model for spatio-temporal graph (STG) forecasting. Despite their success, they fail to model intrinsic uncertainties within STG data, which cripples their practicality in downstream tasks for decision-making. To this end, this paper focuses on probabilistic STG forecasting, which is challenging due to the difficulty in modeling uncertainties and complex ST dependencies. In this study, we present the first attempt to generalize the popular denoising diffusion probabilistic models to STGs, leading to a novel non-autoregressive framework called DiffSTG, along with the first denoising network UGnet for STG in the framework. Our approach combines the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models. Extensive experiments validate that DiffSTG reduces the Continuous Ranked Probability Score (CRPS) by 4%-14%, and Root Mean Squared Error (RMSE) by 2%-7% over existing 
    
[^424]: 基于语言驱动的锚点的零样本对抗鲁棒性

    Language-Driven Anchors for Zero-Shot Adversarial Robustness. (arXiv:2301.13096v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13096](http://arxiv.org/abs/2301.13096)

    本文提出了一种基于语言驱动、基于锚点的对抗训练策略LAAT，通过利用文本编码器的语义一致性，在零样本图像分类场景下增强图像模型的对抗鲁棒性。实验结果表明，该方法在零样本对抗性能上优于先前的最佳状态对抗性一次性方法，同时能为流行的图像分类模型带来实质性的零样本对抗性能提升。

    

    深度神经网络容易受到对抗性攻击。本文旨在改善具有挑战性的零样本图像分类场景下的对抗鲁棒性。为解决这一问题，我们提出了一种新的基于语言驱动、基于锚点的对抗训练策略LAAT。LAAT利用文本编码器为每个类别生成固定的锚点（归一化特征嵌入），并在对抗训练中使用这些锚点。通过利用文本编码器的语义一致性，LAAT可以增强图像模型在新类别上的对抗鲁棒性，而无需额外的样例。我们发现了最近文本编码器的余弦相似度问题，并设计了几种有效的技术来解决它。实验结果表明，LAAT显著提高了零样本对抗性能，优于先前的最佳状态对抗性一次性方法。此外，我们的方法在几个基准数据集上为流行的图像分类模型（如ResNet-50和DenseNet-121）产生了实质性的零样本对抗性能提升。

    Deep neural networks are known to be susceptible to adversarial attacks. In this work, we focus on improving adversarial robustness in the challenging zero-shot image classification setting. To address this issue, we propose LAAT, a novel Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes a text encoder to generate fixed anchors (normalized feature embeddings) for each category and then uses these anchors for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT can enhance the adversarial robustness of the image model on novel categories without additional examples. We identify the large cosine similarity problem of recent text encoders and design several effective techniques to address it. The experimental results demonstrate that LAAT significantly improves zero-shot adversarial performance, outperforming previous state-of-the-art adversarially robust one-shot methods. Moreover, our method produces substantial zero-shot adver
    
[^425]: PhAST：物理感知、可扩展、任务特定的GNN在加速催化剂设计中的应用

    PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design. (arXiv:2211.12020v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12020](http://arxiv.org/abs/2211.12020)

    提出了PhAST方法来快速发现更有效的催化剂来驱动电化学反应, 该方法适用于大多数体系结构, 可以增加计算效率和精度

    

    缓解气候危机需要快速向低碳能源转变。催化剂材料在许多工业过程中的电化学反应中起着至关重要的作用，如可再生能源储存和电荷合成。为了减少在这些过程中消耗的能量，我们必须快速发现更有效的催化剂来驱动电化学反应。机器学习（ML）有潜力从大量数据中高效地模拟材料的性质，从而加速电催化剂的设计。为此，Open Catalyst Project OC20数据集已经被构建。然而，大多数已经在OC20上训练的现有ML模型仍然无法满足实际应用的可扩展性和准确性要求。在这里，我们提出了几个任务特定的创新，适用于大多数体系结构，可以增加计算效率和精度。特别是我们在图翻译层、图注意力层和池化层中提出了改进。我们将这种方法称为Physical Attribute Scaling Transformer (PhAST)。我们证明了PhAST在生成准确数据的同时，具有低计算成本，适用于几个相关应用，包括电催化剂的发现和设计。

    Mitigating the climate crisis requires a rapid transition towards lower carbon energy. Catalyst materials play a crucial role in the electrochemical reactions involved in a great number of industrial processes key to this transition, such as renewable energy storage and electrofuel synthesis. To reduce the amount of energy spent on such processes, we must quickly discover more efficient catalysts to drive the electrochemical reactions. Machine learning (ML) holds the potential to efficiently model the properties of materials from large amounts of data, and thus to accelerate electrocatalyst design. The Open Catalyst Project OC20 data set was constructed to that end. However, most existing ML models trained on OC20 are still neither scalable nor accurate enough for practical applications. Here, we propose several task-specific innovations, applicable to most architectures, which increase both computational efficiency and accuracy. In particular, we propose improvements in (1) the graph 
    
[^426]: CAPE: 使用大型语言模型从前置错误中纠正行动

    CAPE: Corrective Actions from Precondition Errors using Large Language Models. (arXiv:2211.09935v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.09935](http://arxiv.org/abs/2211.09935)

    CAPE是一种利用大型语言模型从前置错误中纠正行动的方法，提高了生成计划的质量，使具身代理能够执行更多任务，并改善了计划的正确性。

    

    从大型语言模型中提取常识知识为设计智能机器人提供了一种途径。现有的利用语言模型进行规划的方法在行动失败时无法恢复，并且通常只能尝试重新执行失败的行动，而无法解决错误的根本原因。我们提出了一种新颖的方法（CAPE），试图在规划过程中提出纠正前置条件错误的行动。CAPE通过利用少样本推理从行动前置条件中提高了生成计划的质量。我们的方法使得具身代理能够执行比基线方法更多的任务，同时确保语义正确性和最小化重新提示。在VirtualHome中，CAPE生成可执行的计划，并且相比SayCan，将人工标注的计划正确度指标从28.89%提高到49.63%。我们的改进也适用于一台配置了一组以语言为指定的技能和相关前置条件的波士顿动力公司的Spot机器人，其中CAPE提高了正确性。

    Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause.  We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctne
    
[^427]: 防御联邦背后攻击的不变聚合器

    Invariant Aggregator for Defending against Federated Backdoor Attacks. (arXiv:2210.01834v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01834](http://arxiv.org/abs/2210.01834)

    该论文针对联邦学习中的背后攻击提出了一种不变聚合器，防御背后攻击并保持模型的整体效用。研究发现在扁平损失空间中，恶意客户端可以通过提供背后样本来误导联邦学习模型，而不需要与良性客户端有明显的差异。

    

    联邦学习因其能够在不直接共享私密数据的情况下训练高效模型而日益受到关注。然而，这种联邦设置使得模型在存在恶意客户端的情况下容易受到各种敌对攻击。尽管对于旨在降低模型效用的攻击的防御已经取得了理论和实证上的成功，但防御仅提高背后样本上模型准确性而不损害其他样本效用的背后攻击仍然具有挑战性。为此，我们首先分析了联邦学习在扁平损失空间上对背后攻击的脆弱性，这种扁平损失空间常见于设计良好的神经网络，如Resnet [He et al., 2015]，但往往被先前的工作所忽视。在扁平损失空间上，误导联邦学习模型以仅对恶意客户端的背后样本有利，并不需要恶意和良性客户端之间存在显著差异。

    Federated learning is gaining popularity as it enables training high-utility models across several clients without directly sharing their private data. As a downside, the federated setting makes the model vulnerable to various adversarial attacks in the presence of malicious clients. Despite the theoretical and empirical success in defending against attacks that aim to degrade models' utility, defense against backdoor attacks that increase model accuracy on backdoor samples exclusively without hurting the utility on other samples remains challenging. To this end, we first analyze the vulnerability of federated learning to backdoor attacks over a flat loss landscape which is common for well-designed neural networks such as Resnet [He et al., 2015] but is often overlooked by previous works. Over a flat loss landscape, misleading federated learning models to exclusively benefit malicious clients with backdoor samples do not require a significant difference between malicious and benign cli
    
[^428]: 基于大规模注释数据库的深度伪造检测 AI 偏差的全面分析

    A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases. (arXiv:2208.05845v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.05845](http://arxiv.org/abs/2208.05845)

    本研究通过提供包含47种不同属性注释的大规模数据集，并对三种最先进的Deepfake检测模型进行全面分析，旨在研究公共Deepfake数据集可能带来的AI偏差问题

    

    近年来，Deepfake 对图像和视频的篡改已经成为安全和社会的严重关注点。许多检测模型和数据集已经被提出，可靠地检测 Deepfake 数据。然而，人们越来越担心这些模型和训练数据集可能存在偏差，从而导致 Deepfake 检测器失效。本研究通过提供五个流行的 Deepfake 数据集中 47 种不同属性的大规模人口统计和非人口统计属性注释，并全面分析三种最先进的 Deepfake 检测模型对这些数据集的 AI 偏差问题，调查研究了超过 6500 万个标签的许多不同属性（包括人口统计学（年龄、性别、种族）和非人口统计学（头发、皮肤、配饰等）信息对检测性能的影响。结果表明，调查的数据库缺乏多样性，可能导致 AI 偏差。

    In recent years, image and video manipulations with Deepfake have become a severe concern for security and society. Many detection models and datasets have been proposed to detect Deepfake data reliably. However, there is an increased concern that these models and training databases might be biased and, thus, cause Deepfake detectors to fail. In this work, we investigate the bias issue caused by public Deepfake datasets by (a) providing large-scale demographic and non-demographic attribute annotations of 47 different attributes for five popular Deepfake datasets and (b) comprehensively analysing AI-bias of three state-of-the-art Deepfake detection backbone models on these datasets. The investigation analyses the influence of a large variety of distinctive attributes (from over 65M labels) on the detection performance, including demographic (age, gender, ethnicity) and non-demographic (hair, skin, accessories, etc.) information. The results indicate that investigated databases lack dive
    
[^429]: 高斯混合模型中的局部极小结构

    Local Minima Structures in Gaussian Mixture Models. (arXiv:2009.13040v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2009.13040](http://arxiv.org/abs/2009.13040)

    研究了高斯混合模型中的负对数似然函数的局部极小值结构，发现它们都共享一种常见结构而部分确定了真正的位置混合物的簇中心。这些结果适用于真实混合组分满足某种分离条件的情况，也适用于成分数量过多或过少的情况。

    

    我们在人口极限的情况下调查了混合成分模型（GMM）的负对数似然函数的情况，并探讨了具有一般成分数量的GMM的负对数似然函数的局部极小值结构。由于目标函数是非凸的，即使对于分离良好的混合模型，也可能存在不是全局最优的多个局部极小值。我们的研究揭示了所有局部极小值都共享一种常见结构，该结构部分确定了真正的位置混合物（即高斯成分的均值）的簇中心。具体而言，每个局部极小值可以表示为两种类型子配置的非重叠组合：将单个均值估计与多个高斯分量拟合或将多个估计拟合到单个真实分量。这些结果适用于真实混合组分满足某种分离条件的情况，并且在成分数量过多或过少的情况下也是有效的。我们还针对一维高斯混合物的设置提供了更精细的分析，通过结构计数论证导出了这些非全局最小值的精确数量和它们对应的配置。

    We investigate the landscape of the negative log-likelihood function of Gaussian Mixture Models (GMMs) with a general number of components in the population limit. As the objective function is non-convex, there can be multiple local minima that are not globally optimal, even for well-separated mixture models. Our study reveals that all local minima share a common structure that partially identifies the cluster centers (i.e., means of the Gaussian components) of the true location mixture. Specifically, each local minimum can be represented as a non-overlapping combination of two types of sub-configurations: fitting a single mean estimate to multiple Gaussian components or fitting multiple estimates to a single true component. These results apply to settings where the true mixture components satisfy a certain separation condition, and are valid even when the number of components is overor under-specified. We also present a more fine-grained analysis for the setting of one-dimensional G
    

