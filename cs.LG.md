# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning to Transform for Generalizable Instance-wise Invariance.](http://arxiv.org/abs/2309.16672) | 该论文提出了一种学习转换以实现通用的实例不变性的方法。通过使用归一化流来预测图像的变换分布，并对预测结果进行平均，可以实现对不同实例之间的对齐，从而推广不变性的类别间的应用。这种方法还可以适应超出分布范围的姿势，并且可以学习更广泛的变换范围。 |
| [^2] | [RealFill: Reference-Driven Generation for Authentic Image Completion.](http://arxiv.org/abs/2309.16668) | RealFill是一种个性化的生成修填模型，通过使用少量目标场景的参考图像，能够以真实、高质量、逼真的内容完成目标图像的修复。 |
| [^3] | [HyperPPO: A scalable method for finding small policies for robotic control.](http://arxiv.org/abs/2309.16663) | HyperPPO是一种可扩展的方法，用于寻找适用于机器人控制的小策略。它利用图状超网络同时估计多个神经网络架构的权重，可以获得性能优秀的策略，并能够满足用户的计算约束条件。 |
| [^4] | [Geodesic Regression Characterizes 3D Shape Changes in the Female Brain During Menstruation.](http://arxiv.org/abs/2309.16662) | 本研究首次探究了女性大脑在月经期间的3D形状变化，通过地理回归方法加速计算并提供精确度与速度之间的权衡。根据合成数据测试结果，可以在牺牲少部分精确度的情况下获得显著加速。 |
| [^5] | [Visual In-Context Learning for Few-Shot Eczema Segmentation.](http://arxiv.org/abs/2309.16656) | 这篇论文研究了视觉背景下的少样本湿疹分割学习的能力，提出了一种基于通用视觉模型SegGPT的策略，通过仅使用少量示例图像进行湿疹分割，而无需重新训练模型。 |
| [^6] | [Reusability report: Prostate cancer stratification with diverse biologically-informed neural architectures.](http://arxiv.org/abs/2309.16645) | 该研究通过验证和重新实现作者提出的生物信息化神经网络P-NET的方法，量化了使用Reactome生物通路进行网络稀疏化的贡献，并探索了其他神经架构和方法。研究结果表明，不同结构的深度神经网络对个体患者进行了错误的预测。 |
| [^7] | [Mixup Your Own Pairs.](http://arxiv.org/abs/2309.16633) | 本文提出了一种名为SupReMix的方法，通过混合样本，特别是混合负样本和混合正样本，来解决回归问题中表示学习的挑战。这种方法能够提供更好的性能和更准确的回归结果。 |
| [^8] | [Robust Offline Reinforcement Learning -- Certify the Confidence Interval.](http://arxiv.org/abs/2309.16631) | 本文提出了一种使用随机平滑算法认证给定策略在离线环境中的鲁棒性的方法，证明了该算法的高效性，并得到实验证实。 |
| [^9] | [On Learning with LAD.](http://arxiv.org/abs/2309.16630) | 该研究提出了关于LAD模型不存在过拟合的理论解释，并估计了LAD模型的VC维度。实验证实了这些观察结果。 |
| [^10] | [Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit.](http://arxiv.org/abs/2309.16620) | 这项研究通过残差分支尺度和$\mu$P参数化的残差网络，实现了深度学习中超参数的跨宽度和深度的转移。 |
| [^11] | [Exploiting Edge Features in Graphs with Fused Network Gromov-Wasserstein Distance.](http://arxiv.org/abs/2309.16604) | 该论文介绍了一种用于比较具有节点和边特征的图的扩展Gromov-Wasserstein距离的方法，并提出了新的算法来计算距离和重心。通过实验证明了该方法在图学习任务中的有效性。 |
| [^12] | [Deep Learning Based Uplink Multi-User SIMO Beamforming Design.](http://arxiv.org/abs/2309.16603) | 本论文提出了一种基于深度学习的无监督框架（NNBF）用于设计上行接收多用户单输入多输出（MU-SIMO）波束成形，旨在通过最大化总速率提高吞吐量，并提供高效的计算解决方案。 |
| [^13] | [Cross-Prediction-Powered Inference.](http://arxiv.org/abs/2309.16598) | 本文介绍了一种基于机器学习的交叉预测方法，可以有效地进行推理。该方法通过使用一个小型标记数据集和一个大型未标记数据集，通过机器学习填补缺失的标签，并采用去偏差方法纠正预测的不准确性。 |
| [^14] | [Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces.](http://arxiv.org/abs/2309.16597) | 本文提出了MPHD方法，通过模型预训练和神经网络在异质搜索空间上实现贝叶斯优化的迁移学习。实验证明了MPHD的有效性和在黑盒函数优化任务中的优越性能。 |
| [^15] | [Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why.](http://arxiv.org/abs/2309.16595) | 本文研究了大型语言模型（LLM）在图数据中的应用，发现LLM可以从结构信息中受益，尤其是在文本节点特征缺乏的情况下，而LLM的性能与数据泄露没有显著相关。 |
| [^16] | [Navigating Healthcare Insights: A Birds Eye View of Explainability with Knowledge Graphs.](http://arxiv.org/abs/2309.16593) | 本文总结了知识图谱在医疗领域的影响以及开发可解释性AI模型中的作用。强调了通过知识注入学习提高知识图谱的可解释性的重要性，并提供了未来方向的见解。 |
| [^17] | [Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection.](http://arxiv.org/abs/2309.16592) | 本文介绍了一种在数据受限红外目标检测中利用跨模态知识的方法，通过创新的张量分解方法，将RGB模态的目标检测器扩展到红外模态，同时保持性能。该方法首先在RGB模态上进行预训练，然后在IR模态上进行微调，以提高红外目标检测性能。 |
| [^18] | [A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems.](http://arxiv.org/abs/2309.16584) | 我们开发了一个CDML设计工具箱，可以指导开发者设计满足用例要求的协作分布式机器学习系统。 |
| [^19] | [M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning.](http://arxiv.org/abs/2309.16578) | M-OFDFT是一种利用深度学习模型解决分子系统问题的OFDFT方法，通过将非局域性建立在模型中并使用紧凑的密度表示，实现了与Kohn-Sham DFT相近的精确度，并且具有良好的外推能力。 |
| [^20] | [Compilation as a Defense: Enhancing DL Model Attack Robustness via Tensor Optimization.](http://arxiv.org/abs/2309.16577) | 本文通过使用张量优化的模型编译技术，提出了一种新的防御对抗机器学习侧信道攻击的方法，相对效果降低了43％。 |
| [^21] | [Review of Machine Learning Methods for Additive Manufacturing of Functionally Graded Materials.](http://arxiv.org/abs/2309.16571) | 该论文综述了基于机器学习方法的功能梯度材料增材制造。功能梯度材料是一类具有平滑性质过渡的高级复合材料，机器学习技术被应用于优化加工参数、提高产品质量和检测制造缺陷。这些方法有望提高零件性能和性质。 |
| [^22] | [Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings.](http://arxiv.org/abs/2309.16564) | 本文研究了无监督图表示学习，通过学习并利用保持语义的数据增强方法，创建了解释性嵌入，并解决了无监督表示学习可解释性研究领域的不足。 |
| [^23] | [CRIMED: Lower and Upper Bounds on Regret for Bandits with Unbounded Stochastic Corruption.](http://arxiv.org/abs/2309.16563) | 本文研究了具有任意破坏的多臂赌徒问题，并建立了一个与问题相关的遗憾下界。我们提出了CRIMED算法，该算法在具有已知方差的高斯分布赌徒问题上实现了遗憾下界。 |
| [^24] | [Voting Network for Contour Levee Farmland Segmentation and Classification.](http://arxiv.org/abs/2309.16561) | 本文提出了一个基于投票网络的端到端可训练模型，用于从高分辨率航空影像中分割和分类等高堤农田。通过使用投票机制来减少边界扭曲和类别混淆，实现了较高的准确性。 |
| [^25] | [Correcting for heterogeneity in real-time epidemiological indicators.](http://arxiv.org/abs/2309.16546) | 介绍了在流行病学监测中纠正数据源中的空间和时间异质性问题的方法，使用低秩矩阵来近似异质性并通过“引导”信号来纠正偏差，以提供更可靠的建模和预测信号。 |
| [^26] | [Unsupervised Fact Verification by Language Model Distillation.](http://arxiv.org/abs/2309.16540) | 本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。 |
| [^27] | [Uncertainty Quantification for Eosinophil Segmentation.](http://arxiv.org/abs/2309.16536) | 本研究提出了一种改进的方法，利用深度图像分割和蒙特卡洛辍学来量化嗜酸性粒细胞，为诊断嗜酸性食管炎提供不确定性评估和模型性能可视化。 |
| [^28] | [MotionLM: Multi-Agent Motion Forecasting as Language Modeling.](http://arxiv.org/abs/2309.16534) | MotionLM模型是一个用于多智能体运动预测的语言建模方法，不需要锚点或显式潜在变量优化，能够生成关于交互智能体未来的联合分布和实现时间因果条件展开。 |
| [^29] | [Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models.](http://arxiv.org/abs/2309.16521) | 本论文提出了一种使用深度生成时间序列模型和决策理论相结合的新框架，用于生成个性化的胰岛素治疗策略。通过学习生成逼真的个性化治疗和未来结果轨迹，可以为个性化患者历史匹配且针对最佳未来效果的新型多变量治疗策略。 |
| [^30] | [AtomSurf : Surface Representation for Learning on Protein Structures.](http://arxiv.org/abs/2309.16519) | 本文研究了将蛋白质作为3D网格的表面表示，并提出了一种结合图表面的协同方法，既有竞争优势，又有实际应用潜力。 |
| [^31] | [From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity.](http://arxiv.org/abs/2309.16512) | 本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。 |
| [^32] | [Deep Single Models vs. Ensembles: Insights for a Fast Deployment of Parking Monitoring Systems.](http://arxiv.org/abs/2309.16495) | 该研究揭示了在使用公开可用的标记停车场图像进行训练的全局框架的创建中的挑战，该框架能够在各种场景下准确执行停车空间监控任务，加速部署停车监控系统。 |
| [^33] | [Towards Poisoning Fair Representations.](http://arxiv.org/abs/2309.16487) | 这项研究提出了第一个针对公平表示学习（FRL）的数据中毒框架，该框架在对抗性场景下评估模型的健壮性，解决了FRL方法在面对数据中毒攻击时的脆弱性。 |
| [^34] | [High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality.](http://arxiv.org/abs/2309.16476) | 本文研究了在高维度和重尾干扰条件下的鲁棒回归估计器的性质，通过研究一类椭圆协变量和噪声数据分布的M-估计器，我们发现在存在重尾噪声的情况下，Huber损失需要进一步正则化才能达到最优性能。同时，我们还推导出了岭回归的超额风险的衰减速率。 |
| [^35] | [Compositional Program Generation for Systematic Generalization.](http://arxiv.org/abs/2309.16467) | 系统化泛化是人类的关键技能之一，组合式程序生成器（CPG）通过模块化、类型抽象和递归组合的特征，能够以少样本的方式对新概念进行系统化的泛化，在各种语言任务上具有生产力。 |
| [^36] | [A Metaheuristic for Amortized Search in High-Dimensional Parameter Spaces.](http://arxiv.org/abs/2309.16465) | 我们提出了一种新的元启发式算法，通过基于特征的变换实现高维参数空间中的无梯度参数搜索，从而解决参数推断中的挑战性问题。 |
| [^37] | [Augmenting LLMs with Knowledge: A survey on hallucination prevention.](http://arxiv.org/abs/2309.16459) | 本调研论文讨论了将预训练语言模型与外部知识源集成的方法，以解决模型对知识的访问和操作能力的限制问题。 |
| [^38] | [Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects.](http://arxiv.org/abs/2309.16457) | 该论文设计了一项新颖的实验并收集了52名参与者的全面脑电图数据集，从而解决了觉醒和睡眠状态下神经表示的差异问题。研究团队开发了通用睡眠解码器（USD），可以在不同个体间对齐觉醒和睡眠的神经模式，并取得了与使用个别睡眠数据进行解码相当的准确率。研究还发现，在测试个体上对USD进行微调可以进一步提高解码准确性。 |
| [^39] | [Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective.](http://arxiv.org/abs/2309.16456) | 本文提出了Snowball，一个通过个体视角上的双向选举方法来抵抗联邦学习中后门攻击的框架。它通过自下而上和自上而下的选举过程，逐步排除感染模型，以解决由于本地数据分布多样性导致模型更新混杂分散的问题。 |
| [^40] | [On the Trade-offs between Adversarial Robustness and Actionable Explanations.](http://arxiv.org/abs/2309.16452) | 本论文研究了对抗鲁棒模型对可操作解释的影响，并通过理论和实证分析比较了对抗性鲁棒和非鲁棒模型生成的追索结果的成本和有效性之间的差异。 |
| [^41] | [A parsimonious, computationally efficient machine learning method for spatial regression.](http://arxiv.org/abs/2309.16448) | MPRS是一种计算高效、具有物理启发的空间回归机器学习方法，通过使用距离相关的“相互作用”来引入空间或时间相关性，能够处理任意空间维度的分散数据。在各种合成和真实世界的测试中，MPRS展现了与标准插值方法相当的预测性能，特别适用于填补粗糙和非高斯数据的缺口。 |
| [^42] | [Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation.](http://arxiv.org/abs/2309.16429) | 通过轻量级适配器网络将音频的表示映射到文本到视频生成模型所期望的输入表示，实现了在全局和时间上与输入音频对齐的多样且逼真的视频生成。 |
| [^43] | [Nonlinear MPC design for incrementally ISS systems with application to GRU networks.](http://arxiv.org/abs/2309.16428) | 本研究提出了一种非线性模型预测控制（NMPC）策略，适用于增量ISS系统。通过简化计算终端成分，并明确定义最小预测范围，实现闭环稳定性。将该方法应用于GRU网络，并提供了一种量身定制状态观察器的设计方法。测试结果表明该控制架构具有良好的控制性能和高效的实用性。 |
| [^44] | [AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models.](http://arxiv.org/abs/2309.16414) | 本研究提出了一种名为AutoCLIP的方法，用于自动调谐视觉语言模型的零样本分类器。AutoCLIP通过为每个提示模板分配图像特定的权重，从而改进了从编码类别描述符推导零样本分类器的方式。 |
| [^45] | [Selective Nonparametric Regression via Testing.](http://arxiv.org/abs/2309.16412) | 通过检验给定条件方差的假设，我们开发了一种选择性非参数回归方法，允许考虑方差本身的值以及对应方差预测器的不确定性，并证明了估计器的风险的非渐近界。 |
| [^46] | [Constructing Synthetic Treatment Groups without the Mean Exchangeability Assumption.](http://arxiv.org/abs/2309.16409) | 本文提出了一种在没有均值互换性假设的情况下构建合成治疗组的方法，通过对源群体的治疗组加权混合来构建目标人群的合成治疗组，并通过最小化条件最大均值差异来估计权重。该方法在均值互换性假设被违反时可以作为一种新颖的补充方法。 |
| [^47] | [VAE-based latent-space classification of RNO-G data.](http://arxiv.org/abs/2309.16401) | 本研究提出了一种利用变分自编码器(VAE)的潜空间进行RNO-G数据分类的方法，在有噪声和静默观测站的情况下可以自动检测和区分多个噪声类别，并定量分析其中的物理信号。 |
| [^48] | [Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey.](http://arxiv.org/abs/2309.16398) | 本调查概述了差分隐私在集中式深度学习中的最新进展，包括审计和评估方法、隐私效用权衡的改进、对各种威胁和攻击的保护、差分隐私生成模型以及新兴应用领域。 |
| [^49] | [Uncertainty-Aware Decision Transformer for Stochastic Driving Environments.](http://arxiv.org/abs/2309.16397) | 本论文提出了一种针对随机驾驶环境的不确定性感知决策Transformer（UNREST），通过估计状态的不确定性并相应地分割序列，取代了全局回报。这项工作通过解决在不确定性环境中过于乐观的问题，为离线强化学习在自主驾驶任务中的应用提供了一种有效的解决方案。 |
| [^50] | [Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks.](http://arxiv.org/abs/2309.16391) | 本文介绍了一种通过Sobolev训练的2-Cats网络，它能够非参数地逼近任何二维Copula，并且在估计输出方面优于现有技术。 |
| [^51] | [Multi-Swap $k$-Means++.](http://arxiv.org/abs/2309.16384) | 本论文通过多交换$k$-Means++算法的改进和扩展，提出了一种在$k$-means聚类问题中能够获得$9 + \varepsilon$近似比的局部搜索算法，并证明了该算法在实际中取得了显著的质量改进。 |
| [^52] | [RLLTE: Long-Term Evolution Project of Reinforcement Learning.](http://arxiv.org/abs/2309.16382) | RLLTE是一种长期演进、极度模块化和开源的强化学习框架，提供了完整的生态系统，预计将为RL工程实践设定标准并刺激产业和学术界。 |
| [^53] | [MHG-GNN: Combination of Molecular Hypergraph Grammar with Graph Neural Network.](http://arxiv.org/abs/2309.16374) | MHG-GNN是一种将分子超图语法与图神经网络相结合的新型自编码器，在多样材料的物性预测任务中表现出巨大的潜力。 |
| [^54] | [Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification.](http://arxiv.org/abs/2309.16369) | 本研究探索了在音频场景分类任务中损失函数最小值的锐度与泛化之间的关联。研究发现锐度更高的最小值具有更好的泛化能力，尤其是对于从之前未见设备录制的领域外数据来说。研究还发现优化器的选择是最小值锐度的主要影响因素，并讨论了在可比性方面的相关限制。 |
| [^55] | [Leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs.](http://arxiv.org/abs/2309.16357) | 提出了一个新颖框架TEMT，利用预训练语言模型在文本增强的时态知识图中进行时间间隔预测和归纳推理。 |
| [^56] | [Transformer-VQ: Linear-Time Transformers via Vector Quantization.](http://arxiv.org/abs/2309.16354) | Transformer-VQ是一种基于向量量化实现线性时间的Transformer模型，能够高效计算自注意力，在大规模实验中表现出色。 |
| [^57] | [ShapeDBA: Generating Effective Time Series Prototypes using ShapeDTW Barycenter Averaging.](http://arxiv.org/abs/2309.16353) | 使用新型的ShapeDTW Barycenter Averaging方法生成逼真且有用的时间序列样本和原型，并解决了现有方法中生成不符合分布的人工制品的问题。 |
| [^58] | [Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks.](http://arxiv.org/abs/2309.16347) | 本文提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架，通过利用LLMs作为辅助内在奖励，解决了复杂长视程机器人操作任务中奖励稀疏问题，并在实验中展示了其较高的性能和模块化特性。 |
| [^59] | [LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite.](http://arxiv.org/abs/2309.16342) | LagrangeBench是第一个针对拉格朗日粒子问题的基准测试套件，提供了七个新的流体力学数据集（包括不同维度和物理特性），以及高效的API和已建立的图神经网络的JAX实现。 |
| [^60] | [EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect.](http://arxiv.org/abs/2309.16338) | EFFL是一种在联邦学习中实现均等公平性以减轻马太效应的方法，通过确保全局模型具有均等的准确性和决策偏见来减少客户端之间的资源差距。 |
| [^61] | [End-to-end Risk Prediction of Atrial Fibrillation from the 12-Lead ECG by Deep Neural Networks.](http://arxiv.org/abs/2309.16335) | 该研究提出了一种由深度神经网络进行的端到端方法，可以从12导联心电图中预测房颤的风险。研究结果显示，该模型可以在目前的心电图中识别出未来会发展房颤的患者，并可以根据患者的风险等级评估他们在一定时间内发展房颤的可能性。 |
| [^62] | [DeepPCR: Parallelizing Sequential Operations in Neural Networks.](http://arxiv.org/abs/2309.16318) | DeepPCR是一种新型算法，通过使用并行循环降解算法将常规的顺序操作在神经网络的推断和训练中并行化，从而实现了计算速度的提升。 |
| [^63] | [Astroconformer: The Prospects of Analyzing Stellar Light Curves with Transformer-Based Deep Learning Models.](http://arxiv.org/abs/2309.16316) | Astroconformer是一个基于Transformer的深度学习框架，旨在从恒星光曲线中捕捉长程依赖关系。通过应用于Kepler光曲线数据集，实现了对恒星表面重力的准确估计。 |
| [^64] | [A Primer on Bayesian Neural Networks: Review and Debates.](http://arxiv.org/abs/2309.16314) | 本论文综述介绍了贝叶斯神经网络（BNNs）的基本概念和其在解决神经网络局限性方面的重要性。目标读者包括具备贝叶斯方法背景但缺乏深度学习专业知识的统计学家，以及深度神经网络专业但对贝叶斯统计学有限了解的机器学习专家。 |
| [^65] | [CasIL: Cognizing and Imitating Skills via a Dual Cognition-Action Architecture.](http://arxiv.org/abs/2309.16299) | 本文介绍了一种名为CasIL的技能模仿学习框架，通过引入人类认知先验并将行动概念扩展为双重认知-行动架构，使机器人能够从原始视觉演示中有效地认知和模仿关键技能。该框架通过人机交互实现认知和行动的双重模仿，以提供整个过程的稳健性和可靠性。 |
| [^66] | [Efficiency Separation between RL Methods: Model-Free, Model-Based and Goal-Conditioned.](http://arxiv.org/abs/2309.16291) | 本论文证明了无模型和有模型强化学习方法在效率上存在根本的限制，但目标条件下的方法和构建逆动力学模型的算法不受该限制。 |
| [^67] | [LawBench: Benchmarking Legal Knowledge of Large Language Models.](http://arxiv.org/abs/2309.16289) | LawBench针对大型语言模型的法律知识进行了综合评估，从记忆，理解和应用三个层面评估了它们在法律任务上的能力。 |
| [^68] | [Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning.](http://arxiv.org/abs/2309.16286) | 本文提出了一种通用的异构联邦交叉相关和实例相似性学习的方法，利用非目标蒸馏来解决模型异质性和灾难性遗忘问题，提高了联邦学习的泛化能力和应用性能。 |
| [^69] | [A framework for paired-sample hypothesis testing for high-dimensional data.](http://arxiv.org/abs/2309.16274) | 本文提出了一个针对高维数据配对样本的假设检验框架，通过垂直平分线生成评分函数，并利用伪中位数求得最优评分函数。 |
| [^70] | [Hierarchical Network Data Analytics Framework for B5G Network Automation: Design and Implementation.](http://arxiv.org/abs/2309.16269) | 我们提出了一个层级网络数据分析框架(H-NDAF)，通过将推理任务分布在多个叶子NWDAF上，实现了分析结果的及时提供和更快的分析提供时间。 |
| [^71] | [Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints.](http://arxiv.org/abs/2309.16240) | 本论文提出了一种通过引入多样差异约束推广直接偏好优化（DPO）的方法，该方法消除了对估计方法的需要并简化了奖励和最优策略之间的复杂关系。 |
| [^72] | [Language models in molecular discovery.](http://arxiv.org/abs/2309.16235) | 语言模型在分子发现中的应用为加速药物发现和分子设计提供了有希望的方法，包括从头设计药物、性质预测和反应化学。同时，开源软件资源降低了科学语言建模领域的门槛，未来将结合聊天机器人和计算化学工具。 |
| [^73] | [GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations.](http://arxiv.org/abs/2309.16223) | 本文针对图神经网络解释的内分布评估问题，提出了GInX-Eval方法，克服了传统评估指标的局限性，为解释方法提供了新的见解。 |
| [^74] | [Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data.](http://arxiv.org/abs/2309.16220) | 该论文提出了一个基准来比较不同的方法在医学表格数据中进行ODD检测，为了实现在实际医疗系统中可靠地使用机器学习模型并避免对ODD数据进行不准确的预测，该基准利用了大规模的ICU患者数据集，考虑了多种方法和预测架构。 |
| [^75] | [Abdominal multi-organ segmentation in CT using Swinunter.](http://arxiv.org/abs/2309.16210) | 这项研究提出了使用Swinunter模型在CT中进行腹部多器官分割的方法。通过克服器官的模糊边界、复杂的背景和不同尺度的器官大小差异，该模型能够在公开验证集上取得令人满意的结果和较快的推理时间。 |
| [^76] | [Max-Sliced Mutual Information.](http://arxiv.org/abs/2309.16200) | 本论文提出了一种新的方法，最大切片互信息（mSMI），来量化高维随机变量之间的依赖关系。mSMI在捕捉复杂依赖关系的同时也适用于快速计算。 |
| [^77] | [Stackelberg Batch Policy Learning.](http://arxiv.org/abs/2309.16188) | Stackelberg批量策略学习是一种新颖的基于随机梯度的学习算法，采用博弈论的观点，对策略学习进行建模，并考虑了优化景观中的分层决策结构。 |
| [^78] | [Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models.](http://arxiv.org/abs/2309.16177) | 本论文研究了混合物理-机器学习气候模拟的挑战，并通过大规模在线建模错误采样和评估，在机器学习参数化设计中发现了改进性能的策略。 |
| [^79] | [Using Weak Supervision and Data Augmentation in Question Answering.](http://arxiv.org/abs/2309.16175) | 本文研究了在问答系统中使用弱监督和数据增强技术的作用，通过自动生成标签和使用信息检索技术来训练深度神经网络模型，并探索了数据增强技术的应用。 |
| [^80] | [Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation.](http://arxiv.org/abs/2309.16173) | 本论文提出了一种名为D2DGN的图遗忘方法，通过知识蒸馏的方式删除图神经网络中的信息。这种方法解决了传统方法在处理局部依赖和附加开销方面的局限性，并能够适应不断变化的数据分布和减少训练重复带来的能源消耗。 |
| [^81] | [The Trickle-down Impact of Reward (In-)consistency on RLHF.](http://arxiv.org/abs/2309.16155) | 本文研究了奖励模型（RM）的一致性对强化学习来自人类反馈（RLHF）模型训练所得的聊天机器人的影响，并提出了一种衡量RM一致性的对比提示的基准测试策略。 |
| [^82] | [Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples.](http://arxiv.org/abs/2309.16143) | 本文提出了一种基于生成基础模型生成的合成样本进行半监督学习的方法，旨在解决实际应用中无法获取大规模无标签数据集的问题。 |
| [^83] | [Two-Step Active Learning for Instance Segmentation with Uncertainty and Diversity Sampling.](http://arxiv.org/abs/2309.16139) | 这项研究提出了一种基于不确定性和多样性采样的两步主动学习算法，通过选择最具信息量和代表性的图像进行标注，提高实例分割模型的训练效率。 |
| [^84] | [A Spectral Approach for Learning Spatiotemporal Neural Differential Equations.](http://arxiv.org/abs/2309.16131) | 提出了一种使用谱展开学习时空微分方程的方法，不依赖于空间离散化，可以处理无界的时空方程。这个方法被证明与最新的机器学习方法一样准确，并可应用于更大的问题类别。 |
| [^85] | [ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers.](http://arxiv.org/abs/2309.16119) | ModuLoRA提出了一种内存高效、能够在消费级GPU上支持3比特LLMs微调的方法，并通过与模块化量化器的集成实现了竞争性能和更少的内存使用。 |
| [^86] | [D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation.](http://arxiv.org/abs/2309.16118) | D$^3$Fields是一个动态的三维描述符场，将底层三维环境的动态特性以及语义特征和实例掩模编码起来。它可以灵活地使用不同背景、风格和实例的二维图像指定目标，实现零样本机器人操作任务的可泛化。 |
| [^87] | [E2Net: Resource-Efficient Continual Learning with Elastic Expansion Network.](http://arxiv.org/abs/2309.16117) | E2Net是一种资源高效的持续学习方法，通过核心子网蒸馏和精确的回放样本选择，实现了卓越的准确性和较小的遗忘，在相同的计算和存储限制下最大程度地减少了处理时间。 |
| [^88] | [Compositional Sculpting of Iterative Generative Processes.](http://arxiv.org/abs/2309.16115) | 本文提出了一种称为组合塑造的通用方法，用于定义迭代生成过程的组合，通过分类器指导实现对这些组合的采样。展示了在GFlowNets和扩散模型中实现组合塑造的方法，并提出了两种二元运算的推广。 |
| [^89] | [Comparing Active Learning Performance Driven by Gaussian Processes or Bayesian Neural Networks for Constrained Trajectory Exploration.](http://arxiv.org/abs/2309.16114) | 本文比较了使用高斯过程或贝叶斯神经网络驱动的主动学习算法在受限场景中的性能差异。 |
| [^90] | [Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics.](http://arxiv.org/abs/2309.16109) | 本论文研究了非对比学习中的动力崩溃问题，发现特征归一化可以防止此问题的出现，为解决自监督表示学习的计算效率提供了新的思路。 |
| [^91] | [Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words.](http://arxiv.org/abs/2309.16108) | 本文提出了ChannelViT模型，通过对ViT架构的修改和引入分层通道采样技术，增强了对多通道图像的推理能力和鲁棒性，适用于显微镜和卫星成像等领域。 |
| [^92] | [Differentially Private Secure Multiplication: Hiding Information in the Rubble of Noise.](http://arxiv.org/abs/2309.16105) | 本文研究了在分布式计算中，允许信息泄漏和近似乘法的情况下，当诚实节点数量为少数时，差分隐私和准确性之间的权衡关系。 |
| [^93] | [Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness.](http://arxiv.org/abs/2309.16096) | 本研究论证了数据分布的集中程度对于决定鲁棒分类器的存在与否至关重要，并展示了在数据分布集中在低维线性子空间的并集时，利用数据结构可以获得具有良好鲁棒性保证的分类器的方法。 |
| [^94] | [Task-Oriented Koopman-Based Control with Contrastive Encoder.](http://arxiv.org/abs/2309.16077) | 该论文介绍了一种基于任务导向的Koopman控制方法，利用对比编码器和端到端强化学习来同时学习Koopman潜在嵌入、算子和相关线性控制器。通过优先考虑任务成本作为主要目标，减少了对于明确定义模型的控制器设计的依赖，将Koopman控制扩展到高维、复杂非线性系统，包括基于像素的场景。 |
| [^95] | [Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning.](http://arxiv.org/abs/2309.16074) | 本文通过反向强化学习方法解决复杂地形上的双足行走问题。我们提出了学习专家奖励函数的算法，并分析了学习到的函数。实验证明使用推理出的奖励函数可以提高双足行走策略的行走性能。 |
| [^96] | [Label Augmentation Method for Medical Landmark Detection in Hip Radiograph Images.](http://arxiv.org/abs/2309.16066) | 该论文提出了一种适用于医学髋关节X射线图像中的标记点检测任务的标签增强方法。通过使用仅标签增强方案进行训练，该方法超越了传统的数据增强方法，在样本利用效率上表现出色，可提高标记点检测的准确性。 |
| [^97] | [Masked autoencoders are scalable learners of cellular morphology.](http://arxiv.org/abs/2309.16064) | 本研究探索了弱监督和自监督深度学习方法在训练更大的模型和数据集时的可扩展性，并发现基于CNN和ViT的受屏蔽自动编码器在推断细胞形态学关系方面明显优于弱监督模型。 |
| [^98] | [Predicting Cardiovascular Complications in Post-COVID-19 Patients Using Data-Driven Machine Learning Models.](http://arxiv.org/abs/2309.16059) | 使用数据驱动的机器学习模型成功预测COVID-19后患者的心血管并发症，为及时干预和改善患者结果提供了希望。 |
| [^99] | [AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model.](http://arxiv.org/abs/2309.16058) | AnyMAL是一种高效可扩展的任意模态增强语言模型，能够处理多样化的输入模态信号，并在各种多模态任务上表现出最先进的性能。 |
| [^100] | [Identifying Risk Factors for Post-COVID-19 Mental Health Disorders: A Machine Learning Perspective.](http://arxiv.org/abs/2309.16055) | 本研究利用机器学习技术鉴别了COVID-19后精神健康障碍的风险因素，发现年龄、性别、居住地地理区域、合并症、COVID-19疾病的严重程度以及心理社会因素是发展精神健康障碍的重要因素。医疗服务提供者和政策制定者应考虑这些风险因素来设计有针对性的干预措施和支持系统。 |
| [^101] | [Improving Adaptive Online Learning Using Refined Discretization.](http://arxiv.org/abs/2309.16044) | 通过一种新颖的连续时间启发式算法，提高了自适应在线学习的效果，将梯度方差的依赖性从次优的$O(\sqrt{V_T\log V_T})$改进到最优速率$O(\sqrt{V_T})$，并可适用于未知Lipschitz常数的情况。 |
| [^102] | [Towards Best Practices of Activation Patching in Language Models: Metrics and Methods.](http://arxiv.org/abs/2309.16042) | 本研究系统地考察了激活路径修复中的方法细节对语言模型解释性结果的影响，并提出了最佳实践建议。 |
| [^103] | [Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization.](http://arxiv.org/abs/2309.16034) | 本论文研究了基于原始数据的体内纳米尺度定位的分析建模，分析了纳米设备的通信和能源约束对定位性能的影响。 |
| [^104] | [Learning Dissipative Neural Dynamical Systems.](http://arxiv.org/abs/2309.16032) | 本文提出了一种学习耗散神经动力系统模型的方法，该方法分为两个阶段。首先学习一个无约束的模型，然后导出条件来保证模型的耗散性质，并在保持逼近能力的同时扰动偏差。通过独立求解这两个扰动问题，得到一个保证为耗散的神经动力模型，同时紧密逼近非线性系统。 |
| [^105] | [Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies.](http://arxiv.org/abs/2309.16025) | 本文介绍了一种名为符号化模仿学习（SIL）的方法，通过引入归纳逻辑编程（ILP）来学习从现有数据集中获取透明、可解释和泛化的驾驶策略。与传统的基于深度神经网络的模仿学习方法相比，SIL不仅提高了驾驶策略的可解释性，还显著改进了它们在各种驾驶情况下的适用性。 |
| [^106] | [GNNHLS: Evaluating Graph Neural Network Inference via High-Level Synthesis.](http://arxiv.org/abs/2309.16022) | GNNHLS是一个通过高级综合评估FPGAs上的图神经网络推断的开源框架，能够实现高速加速和能量降低。 |
| [^107] | [GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization.](http://arxiv.org/abs/2309.16020) | GeoCLIP是一种受Clip启发的图像到GPS检索方法，用于全球地理定位。它通过对齐图像和其对应的GPS位置来提高定位精度，并克服了传统方法中固定分类的局限性。 |
| [^108] | [Graph-level Representation Learning with Joint-Embedding Predictive Architectures.](http://arxiv.org/abs/2309.16014) | 本文提出了一种用于图级表示学习的联合嵌入预测架构（JEPA），通过预测输入图的不同子图的潜在表示在2维单位双曲线上的坐标，实现了对图级表示的有效建模。 |
| [^109] | [Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems.](http://arxiv.org/abs/2309.15995) | 这篇论文提出了一种基于数字孪生的网络物理系统中的异常检测方法，通过引入课程学习优化学习范式，将训练数据进行难度得分采样。 |
| [^110] | [Machine Learning Based Analytics for the Significance of Gait Analysis in Monitoring and Managing Lower Extremity Injuries.](http://arxiv.org/abs/2309.15990) | 该研究使用机器学习模型分析步态数据，探讨了步态分析在评估下肢骨折患者并发症方面的潜力，并提出了一种处理类别不平衡的方法。最终结果表明，XGBoost是最佳模型。 |
| [^111] | [Open Source Infrastructure for Differentiable Density Functional Theory.](http://arxiv.org/abs/2309.15985) | 本研究构建了一个开源基础设施，用于训练可微分的交换相关泛函。通过借鉴多个团队的前沿技术，旨在规范化处理流程。已经在DeepChem库中开源了模型，为可微分量子化学方法的进一步研究提供了平台。 |
| [^112] | [TraCE: Trajectory Counterfactual Explanation Scores.](http://arxiv.org/abs/2309.15965) | TraCE是一个模型无关的模块化框架，用于评估顺序决策任务中的进展。它能够将高度复杂场景中的进展凝练为一个单一值，并在医疗保健和气候变化领域展示了其实用性。 |
| [^113] | [An Uncertainty-Aware Pseudo-Label Selection Framework using Regularized Conformal Prediction.](http://arxiv.org/abs/2309.15963) | 本文提出了一种使用正则化收缩预测的不确定性感知伪标签选择框架，通过修复校准不佳的神经网络，减少噪声训练数据。 |
| [^114] | [The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering.](http://arxiv.org/abs/2309.15954) | 本文介绍了一种针对数据过滤的新的基准，包括单模态过滤、跨模态过滤和数据分布对齐等策略。通过综合现有方法和提出新的解决方案，改善了基础模型的性能，并提供了深入的分析和讨论。 |
| [^115] | [Unified Long-Term Time-Series Forecasting Benchmark.](http://arxiv.org/abs/2309.15946) | 该论文介绍了一个专门用于长期时序预测的综合数据集，通过对多个经典和最先进的模型进行广泛基准分析，发现模型的有效性与数据集相关性有关。 |
| [^116] | [Exploring Self-Supervised Contrastive Learning of Spatial Sound Event Representation.](http://arxiv.org/abs/2309.15938) | 该研究提出了一个简单的多通道对比学习框架，用于编码空间音频的“什么”和“哪里”。通过多级数据增强和通道增强方法，该框架在事件分类和声音定位方面优于有监督模型。 |
| [^117] | [High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models.](http://arxiv.org/abs/2309.15889) | 本论文研究了通过深度学习的联合源-信道编码和去噪扩散模型在噪声无线信道上进行图像传输的问题。通过利用范围-零空间分解和逐步优化零空间内容，实现了在失真和感知质量方面的显著改进。 |
| [^118] | [Projection based fuzzy least squares twin support vector machine for class imbalance problems.](http://arxiv.org/abs/2309.15886) | 该论文提出了一种基于投影的模糊最小二乘双支持向量机方法来应对类不平衡问题和噪声数据集。通过引入直觉模糊成员关系和超平面模糊成员关系的概念，提出了两种不同的方法。这些方法能够在处理类不平衡问题时提供更准确的分类结果。 |
| [^119] | [Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training.](http://arxiv.org/abs/2309.15881) | 该论文提出了一种名为多层嵌入训练（MLET）的训练技术，通过跨类别学习产生优秀的嵌入。该方法通过嵌入层的分解训练嵌入，内部维度高于目标嵌入维度，并在推理时提高了效率。该技术的实验结果令人惊讶，并且通过理论解释了其有效性。 |
| [^120] | [Neuro-Inspired Hierarchical Multimodal Learning.](http://arxiv.org/abs/2309.15877) | 这项研究提出了一种神经启发的分层多模态学习方法，利用信息瓶颈理论构建了一种有效且紧凑的信息流，实现了对真实世界的全面和准确的感知。 |
| [^121] | [STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs.](http://arxiv.org/abs/2309.15875) | STAG是一个GNN服务框架，用于解决动态图中基于GNN的服务中的低延迟和低陈旧度问题。它采用协同服务机制和增量传播策略来优化节点表示的更新过程。 |
| [^122] | [Telescope: An Automated Hybrid Forecasting Approach on a Level-Playing Field.](http://arxiv.org/abs/2309.15871) | Telescope是一种基于机器学习的自动化混合预测方法，可以准确可靠地进行预测，无需参数化或训练和适配大量参数，并且与其他方法相比，Telescope能够快速提供预测结果。 |
| [^123] | [Identifying factors associated with fast visual field progression in patients with ocular hypertension based on unsupervised machine learning.](http://arxiv.org/abs/2309.15867) | 本研究使用无监督机器学习方法识别了眼压增高患者中具有不同视野退化趋势的亚型，并发现了快速视野退化的相关因素。 |
| [^124] | [Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data.](http://arxiv.org/abs/2309.15757) | 本文提出了一种基于潜在图的半监督学习方法，通过利用图的表示来捕捉数据之间的关系，并实现了全局和局部知识的有效融合。在生物医学数据集上的评估中，我们的方法表现出了最先进的结果。 |
| [^125] | [Enhancing Sharpness-Aware Optimization Through Variance Suppression.](http://arxiv.org/abs/2309.15639) | 本文通过方差抑制的方法（VaSSO）增强了锐度感知最小化（SAM）的优化算法，提高了深度神经网络的泛化能力，特别适用于模型无关任务和对高水平标签噪声具有鲁棒性的情况。 |
| [^126] | [Jointly Training Large Autoregressive Multimodal Models.](http://arxiv.org/abs/2309.15564) | 本研究提出了共同训练大型自回归多模态模型的方法，通过模块化的方式融合语言和图像生成模型，同时引入了数据高效的指令调优策略，使得该模型在生成高质量多模态输出方面表现出卓越的性能。 |
| [^127] | [Method and Validation for Optimal Lineup Creation for Daily Fantasy Football Using Machine Learning and Linear Programming.](http://arxiv.org/abs/2309.15253) | 本文提出了一种方法来预测NFL球员的表现，并使用线性规划找到最佳阵容来最大化奇幻分数。实验结果表明，这种方法可以有效提高阵容的性能。 |
| [^128] | [DPA-WNO: A gray box model for a class of stochastic mechanics problem.](http://arxiv.org/abs/2309.15128) | DPA-WNO是一种将可解释性的数据驱动模型与小波神经操作符相结合的新方法，用于纠正/识别缺失的物理，并解决了纯数据驱动模型的缺点。 |
| [^129] | [Uncovering Neural Scaling Laws in Molecular Representation Learning.](http://arxiv.org/abs/2309.15123) | 从数据中心的角度研究了分子表示学习的神经缩放行为，发现了数据量和性能之间的一致幂律关系，并提出了潜在的提高学习效率的方法。 |
| [^130] | [Improving Robustness of Deep Convolutional Neural Networks via Multiresolution Learning.](http://arxiv.org/abs/2309.13752) | 本研究通过多分辨率学习方式，显著提高了深度卷积神经网络对于1D信号和2D信号（图像）预测问题的鲁棒性，并发现在这种学习方式下，并不需要牺牲准确性来获得鲁棒性。 |
| [^131] | [Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition.](http://arxiv.org/abs/2309.13405) | 本论文通过桥块分解方法，提出了一种学习大规模MTP$_2$高斯图模型的策略，能够将大问题拆分为小问题进行优化，显著降低了计算复杂度并提高了算法速度。 |
| [^132] | [S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees.](http://arxiv.org/abs/2309.12041) | S-GBDT是一种节俭的差分隐私梯度提升决策树学习器，利用了四种技术来改善效用和隐私权之间的平衡，包括对隐私泄露的更紧密计算和整合个体Rényi滤波器以学习未充分利用的数据点。 |
| [^133] | [GLM Regression with Oblivious Corruptions.](http://arxiv.org/abs/2309.11657) | 这篇论文介绍了第一个在广义线性模型回归问题中处理加法无意识噪声的算法。算法的目标是通过样本访问来准确地恢复参数向量，使得模型的预测与真实值的误差尽可能小。 |
| [^134] | [Multiplying poles to avoid unwanted points in root finding and optimization.](http://arxiv.org/abs/2309.11475) | 通过增加极点来避免根查找和优化中不需要的点，方法是将代价函数除以到目标点的距离函数的适当幂。 |
| [^135] | [A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models.](http://arxiv.org/abs/2309.10003) | 本文提出了一种使用概率从语言模型中获得的自信息来测量专利权要求范围的新方法。该方法通过计算要求的发生概率和自信息来评估要求的信息量，进而反映出要求的范围。研究结果表明，不同类型的语言模型对范围测量的影响不同，最简单的模型可以将范围度量简化为单词或字符计数的倒数。此方法在九个系列的专利权要求上进行了验证，结果表明各系列的要求范围逐渐减小。 |
| [^136] | [General In-Hand Object Rotation with Vision and Touch.](http://arxiv.org/abs/2309.09979) | 本研究介绍了一个能够通过视觉和触觉感知实现手中物体通用旋转的系统，通过训练和模拟推断物体的形状和物理属性，并在实际部署中展示了显著的性能提升。 |
| [^137] | [Imbalanced Data Stream Classification using Dynamic Ensemble Selection.](http://arxiv.org/abs/2309.09175) | 本文提出了一个新的框架，通过对非稳态漂移的不平衡数据流进行分类框架设计，采用数据预处理和动态集成选择技术。该框架使用了六个人工生成的数据流，并且通过评估了七种预处理技术和两种动态集成选择方法的效果。 |
| [^138] | [Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection.](http://arxiv.org/abs/2309.07461) | 这项研究介绍了一个针对物联网环境定制的网络入侵检测系统的开放集分类器框架，利用图像表示和堆叠子聚类技术来识别未知攻击。 |
| [^139] | [Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices.](http://arxiv.org/abs/2309.06612) | 本文提出了基于硬件感知的资源受限设备上的多模态神经架构搜索框架Harmonic-NAS，通过两层优化实现了单模态骨干和多模态融合网络的联合优化。 |
| [^140] | [Instance-Agnostic Geometry and Contact Dynamics Learning.](http://arxiv.org/abs/2309.05832) | 本文提出了一个实例无关的学习框架，通过几何作为共享表示，将视觉与动力学相结合，从RGBD视频中学习物体的几何和动力学属性。实验结果表明，该框架能够学习刚性和凸物体的几何和动力学，并改进了跟踪框架。 |
| [^141] | [Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis.](http://arxiv.org/abs/2309.05525) | 该研究提出了一种在6G环境中支持联邦学习的可信架构，利用分布式分类账技术和图神经网络解决了隐私和安全问题，以实现安全聚合和异常检测。 |
| [^142] | [Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs.](http://arxiv.org/abs/2309.05516) | 本文提出一种名为SignRound的优化权重舍入的方法，通过使用有符号梯度进行轻量级分块调整，解决了大型语言模型(LLMs)的量化挑战。 |
| [^143] | [Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples.](http://arxiv.org/abs/2309.03847) | 通过多项式数量的样本和差分隐私约束，我们提出了一个可以估计高斯混合物的方法，并证明了这个方法的有效性，而无需对GMMs做任何结构性假设。 |
| [^144] | [Learning to Taste: A Multimodal Wine Dataset.](http://arxiv.org/abs/2308.16900) | 这个论文介绍了一个大型多模态葡萄酒数据集，用于研究视觉感知、语言和口感之间的关系，并提出了低维概念嵌入算法，将人类经验与自动机器相似度核相结合，改进了口味分类，并与人类口味知觉相一致。 |
| [^145] | [Understanding the Usage of QUBO-based Hamiltonian Function in Combinatorial Optimization over Graphs: A Discussion Using Max Cut (MC) Problem.](http://arxiv.org/abs/2308.13978) | 研究探讨了在图上基于QUBO公式的最大切割问题中，如何使用基于强化学习范式和哈密顿函数来解决组合优化问题。通过使用图神经网络作为信息传递架构，并通过三种不同的公式形式进行实验，发现... |
| [^146] | [Contrastive Learning of Temporal Distinctiveness for Survival Analysis in Electronic Health Records.](http://arxiv.org/abs/2308.13104) | 本文提出了一种新的基于对比学习的生存分析框架，充分利用截断和观察数据的生存时长来定义时序区别度，构建负样本对。 |
| [^147] | [Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting.](http://arxiv.org/abs/2308.10425) | 本研究提出了一种名为时空自适应嵌入的新组件，在普通的Transformer中实现了领先于其他方法的交通预测性能，通过捕捉交通时间序列中的时空关系和时间信息实现了优秀的结果。 |
| [^148] | [Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance.](http://arxiv.org/abs/2308.05034) | Kairos是第一个同时满足范围、攻击不可知性、时效性和攻击重建维度要求的溯源为基础的入侵检测和调查系统。 |
| [^149] | [Probabilistic Invariant Learning with Randomized Linear Classifiers.](http://arxiv.org/abs/2308.04412) | 本文介绍了一种使用随机线性分类器进行概率不变学习的方法，通过接受概率化的普遍逼近和不变性，设计了能同时具有表达能力和不变性的模型，并且使用更少的资源。通过实验证明了这种方法在分类任务中的有效性。 |
| [^150] | [Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness.](http://arxiv.org/abs/2308.03666) | 该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。 |
| [^151] | [Lossless Transformations and Excess Risk Bounds in Statistical Inference.](http://arxiv.org/abs/2307.16735) | 在统计推断中，我们研究了无损转换和过量风险的概念。我们提出了无损转换的特征，并构建了一个用于判断给定转换是否是无损的统计量。我们还引入了delta-无损转换的概念，并给出了充分条件。这些研究在分类、非参数回归和投资组合策略等领域具有应用价值。 |
| [^152] | [AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models.](http://arxiv.org/abs/2307.12499) | 本文提出了一种使用扩散模型生成无限制对抗样本的方法AdvDiff。通过设计两种新的对抗引导技术，在扩散模型的逆生成过程中进行对抗采样，从而有效地生成高质量、逼真的对抗样本。 |
| [^153] | [Flexible and efficient spatial extremes emulation via variational autoencoders.](http://arxiv.org/abs/2307.08079) | 本文提出了一种新的空间极端值模型，通过集成在变分自动编码器的结构中，可以灵活、高效地模拟具有非平稳相关性的极端事件。实验证明，在时间效率和性能上，相对于传统的贝叶斯推断和许多具有平稳相关性的空间极端值模型，我们的方法具有优势。 |
| [^154] | [TinyMetaFed: Efficient Federated Meta-Learning for TinyML.](http://arxiv.org/abs/2307.06822) | TinyMetaFed是一个适用于TinyML的高效联邦元学习框架，通过协同训练神经网络初始化，在小型设备上能够快速微调，同时实现通信节省和隐私保护。 |
| [^155] | [Onion Universe Algorithm: Applications in Weakly Supervised Learning.](http://arxiv.org/abs/2307.04870) | 洋葱宇宙算法是一种新颖的集成学习分类方法，可作为弱监督学习的标签模型。它在实现上简单、计算高效，适用于无完全标记数据的情况。经验证明，它在常见的基准数据集上表现出色。 |
| [^156] | [Set Learning for Accurate and Calibrated Models.](http://arxiv.org/abs/2307.02245) | 提出了一种集合学习方法(OKO)来解决机器学习中的模型过度自信和校准不良问题，通过最小化集合的交叉熵误差，从而提高准确性和校准效果，并在有限的训练数据和类别不平衡情况下表现出更好的结果。 |
| [^157] | [Temporal Graph Benchmark for Machine Learning on Temporal Graphs.](http://arxiv.org/abs/2307.01026) | TGB是一个用于在时态图上进行机器学习模型评估的基准测试数据集集合，具有挑战性和多样化，涵盖了节点和边级预测任务，对多种领域的时态图进行了广泛的基准测试，并发现常见模型的性能可能有巨大差异。在动态节点属性预测任务中，我们展示了简单方法可能比现有的时态图模型表现出更好的性能。 |
| [^158] | [Asymptotic-Preserving Convolutional DeepONets Capture the Diffusive Behavior of the Multiscale Linear Transport Equations.](http://arxiv.org/abs/2306.15891) | 本文介绍了两种新型的渐近保持的卷积Deep Operator网络（APCONs），用于解决多尺度时变线性输运问题。该方法通过采用多个局部卷积操作、池化和激活操作来捕捉线性输运问题的扩散行为，并验证了方法的有效性。 |
| [^159] | [DynaBench: A benchmark dataset for learning dynamical systems from low-resolution data.](http://arxiv.org/abs/2306.05805) | DynaBench是一个新的模拟基准数据集，用于直接从低分辨率的稀疏数据中学习动力系统，评估了多个机器学习模型的预测性能。 |
| [^160] | [Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed Assumptions.](http://arxiv.org/abs/2305.18471) | 本文提出了仅有互易噪声方差和有界平滑性假设的非凸目标的AdaGrad算法的简单收敛性证明，证明中基于辅助函数$\xi$，比现有结果更紧密，在超参数化的情况下，能够确保梯度范数小于$\varepsilon$的迭代次数为$\mathcal{O}(\frac{1}{\varepsilon^2})$，并考虑了一种实际平滑假设$(L_0,L_1)$-平滑条件。 |
| [^161] | [Disambiguated Attention Embedding for Multi-Instance Partial-Label Learning.](http://arxiv.org/abs/2305.16912) | 本文提出了一种用于多实例部分标签学习的消岐注意嵌入算法（DEMIPL），通过将多实例包嵌入到单个向量表示中，解决了现有方法在忽视全局包级信息和对负实例预测敏感的问题。 |
| [^162] | [Enhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset.](http://arxiv.org/abs/2305.10775) | 该论文提出了一种新的几何变换方法，将X-Ray Microbeam数据集中的解剖标记物的X-Y坐标沿中矢状面映射到多个相对测量中，进而改进了测量的准确性。 |
| [^163] | [Causal Policy Gradient for Whole-Body Mobile Manipulation.](http://arxiv.org/abs/2305.04866) | 本文提出了一种新框架——因果MoMa，可以训练适用于典型MoMa任务的策略，在此框架下，机动和交互自由度可以同时组合，并且不需要人类领域知识来划分动作空间或将动作部分与子目标匹配。 |
| [^164] | [Deep learning techniques for financial time series forecasting: A review of recent advancements: 2020-2022.](http://arxiv.org/abs/2305.04811) | 本研究综述了深度学习模型在金融时间序列预测上的最新研究进展，包括不同数据来源和神经网络结构以及实现细节，并提供未来研究方向的建议。 |
| [^165] | [Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation.](http://arxiv.org/abs/2305.03942) | 论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。 |
| [^166] | [Physics-Informed Representation Learning for Emergent Organization in Complex Dynamical Systems.](http://arxiv.org/abs/2304.12586) | 该论文提出了一个基于物理启示的自然组织表征学习框架，通过局部因果状态捕获复杂时空系统中的有序行为和相干结构。该方法在实际领域科学问题中具有很好的适用性。 |
| [^167] | [Synthesizing Stable Reduced-Order Visuomotor Policies for Nonlinear Systems via Sums-of-Squares Optimization.](http://arxiv.org/abs/2304.12405) | 该论文提出了一种通过平方和优化，综合动态的、降阶的输出反馈多项式控制器的方法，应用于控制仿射的非线性系统，并使用学习的感知模块和视觉观测来稳定地运行到目标状态。该方法提供了在观测噪声存在的情况下的稳定性保证。 |
| [^168] | [Model sparsification can simplify machine unlearning.](http://arxiv.org/abs/2304.04934) | 本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。 |
| [^169] | [Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels.](http://arxiv.org/abs/2303.16296) | 本文提出的Dice半度量损失函数可在软标签设置中使用，在医疗成像领域的分割方案中与使用软标签的研究相结合，可以获得更好的Dice分数和模型校准。 |
| [^170] | [Delay-Aware Hierarchical Federated Learning.](http://arxiv.org/abs/2303.12414) | 本论文提出了延迟感知的联邦学习(DFL)，通过解决边缘和云之间的通信延迟，提高了分布式机器学习模型训练的效率，并实现了一些政策以减少能量消耗和边缘到云端的通信。 |
| [^171] | [Tiny Classifier Circuits: Evolving Accelerators for Tabular Data.](http://arxiv.org/abs/2303.00031) | 本文提出了一种自动生成分类器电路的方法，用于对表格式数据进行分类，利用演化算法搜索逻辑门空间，生成具有最大化训练预测准确性的微小分类器电路，在使用更少的硬件资源和功耗的同时，具有与传统机器学习技术相当的预测性能。 |
| [^172] | [Discouraging posterior collapse in hierarchical Variational Autoencoders using context.](http://arxiv.org/abs/2302.09976) | 本研究提出了一种具有上下文的深层级变分自编码器，用于抑制后验坍塌问题。实验证明，该修改可以更好地利用潜在空间并且不会影响模型的生成能力。 |
| [^173] | [Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space.](http://arxiv.org/abs/2302.06807) | 本文提出了一种大间隔分类器，它使用浑拟圆决策边界可以优化测地凸优化问题，实验结果表明其竞争性能优越。 |
| [^174] | [Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification.](http://arxiv.org/abs/2301.11562) | 在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。 |
| [^175] | [Deep learning for bias-correcting CMIP6-class Earth system models.](http://arxiv.org/abs/2301.01253) | 本研究展示了一种基于物理约束生成对抗网络的后处理方法，可以同时校正CMIP6级地球系统模型的局部频率分布和空间模式中的偏差。 |
| [^176] | [HyperBO+: Pre-training a universal prior for Bayesian optimization with hierarchical Gaussian processes.](http://arxiv.org/abs/2212.10538) | 本文提出了一种名为HyperBO+的方法，通过使用分层高斯过程的预先训练，实现了在具有不同输入空间的函数上普适于贝叶斯优化。研究人员设计了一种两步预先训练方法，并分析了其吸引人的渐近性质。 |
| [^177] | [Attribute Graph Clustering via Learnable Augmentation.](http://arxiv.org/abs/2212.03559) | 本研究提出了一种通过可学习增强技术进行属性图聚类的方法，该方法利用可学习的增强器为深度图聚类提供高质量和适合的增强样本，通过改进矩阵提高了聚类性能的可靠性。 |
| [^178] | [Safe Imitation Learning of Nonlinear Model Predictive Control for Flexible Robots.](http://arxiv.org/abs/2212.02941) | 本文提出了一种使用模仿学习和预测安全过滤器进行非线性模型预测控制(NMPC)的安全近似的框架，以实现对柔性机器人的快速控制。与NMPC相比，在保证安全约束的情况下，我们的框架在计算时间上改善了8倍以上。 |
| [^179] | [Efficient Adversarial Input Generation via Neural Net Patching.](http://arxiv.org/abs/2211.16808) | 通过修补神经网络的方法生成对抗性输入，以解决建立深度神经网络的鲁棒性和可信度的问题。 |
| [^180] | [Vertical Federated Learning: Concepts, Advances and Challenges.](http://arxiv.org/abs/2211.12814) | 垂直联合学习（VFL）是一种联合学习设置，多个具有关于同一组用户不同特征的参与方共同训练机器学习模型，而不公开原始数据或模型参数。本文提供了对VFL概念、算法以及各个方面的进展和挑战的综合回顾，深入分析了隐私保护协议的分类、隐私攻击和防御策略，并提出了考虑多个约束条件的统一框架VFLow。此外，还回顾了工业应用中的最新进展和VFL面临的未来挑战和方向。 |
| [^181] | [Online Distribution Shift Detection via Recency Prediction.](http://arxiv.org/abs/2211.09916) | 本文提出了一种在线方法来有效检测机器人系统中的分布漂移，具有低误报率和高效率的特点。 |
| [^182] | [Private Stochastic Optimization With Large Worst-Case Lipschitz Parameter: Optimal Rates for (Non-Smooth) Convex Losses and Extension to Non-Convex Losses.](http://arxiv.org/abs/2209.07403) | 本论文研究了具有大的最坏情况Lipschitz参数的差分隐私随机优化问题，并提供了一种不依赖于统一Lipschitz参数的接近最优的过量风险界限方法。 |
| [^183] | [Just Noticeable Difference Modeling for Face Recognition System.](http://arxiv.org/abs/2209.05856) | 本研究探讨了人脸识别系统中的可察觉差异（JND），建立了JND数据集，并开发了一种新颖的JND预测模型，以提升自动人脸识别系统的性能。 |
| [^184] | [Classical-to-quantum convolutional neural network transfer learning.](http://arxiv.org/abs/2208.14708) | 本研究提出了经典到量子迁移学习的框架，通过利用预训练的经典卷积神经网络，实现了在噪声中间规模量子时代中较小的QCNN在复杂分类问题上的解决，充分利用了QCNN的优势。 |
| [^185] | [Developing a Philosophical Framework for Fair Machine Learning: Lessons From The Case of Algorithmic Collusion.](http://arxiv.org/abs/2208.06308) | 该论文提出了一个伦理框架，用于为公正机器学习开发和应用扩展到新领域的公正指标，通过提出特定范围的规范原则来使公正指标能够反映出各种领域中的公正要求。 |
| [^186] | [Quantum Self-Attention Neural Networks for Text Classification.](http://arxiv.org/abs/2205.05625) | 本论文提出了一种名为量子自注意力神经网络（QSANN）的简单网络架构，通过将自注意机制引入到量子神经网络中，并利用高斯投影的量子自注意力，弥补了现有量子自然语言处理方法的一些限制。QSANN在更大规模的数据集上具有有效和可扩展的性能，并且可以在近期量子设备上实现。 |
| [^187] | [DIRA: A Framework for Dynamic Domain Incremental Regularised Adaptation.](http://arxiv.org/abs/2205.00147) | DIRA是一个用于DNN分类器的动态领域自适应的框架，使用正则化技术来解决灾难性遗忘问题，并通过少量样本实现重新训练和适应性。 |
| [^188] | [Data Augmentation in the Underparameterized and Overparameterized Regimes.](http://arxiv.org/abs/2202.09134) | 这项研究提供了数据增强如何影响估计的方差和极限分布的确切量化结果，发现数据增强可能会增加估计的不确定性，并且其效果取决于多个因素。同时，该研究还通过随机转换的高维随机向量的函数的极限定理进行了证明。 |
| [^189] | [Group-Agent Reinforcement Learning.](http://arxiv.org/abs/2202.05135) | 群体代理强化学习是一种新型的强化学习方法，其利用多个代理之间的合作来提高每个代理的学习效果。我们提出了群体代理强化学习系统的概念，并设计了一种分布式强化学习框架DDAL来支持群体代理强化学习。 |
| [^190] | [Infinite Neural Network Quantum States: Entanglement and Training Dynamics.](http://arxiv.org/abs/2112.00723) | 本研究探索了无限神经网络量子态（∞-NNQS），其通过集合统计表现出高度的表示能力和可行的梯度下降动力学。研究发现，通过神经网络相关子和量子态神经切线核投入到训练动力学中，可以简化神经网络量子态的训练过程并恢复任意目标波函数。对有限和无限NNQS进行的数值实验验证了理论结果，并为进一步研究纠缠和训练动力学提供了新的领域。 |
| [^191] | [Axiomatic Aggregations of Abductive Explanations.](http://arxiv.org/abs/2109.03890) | 本论文提出了三种聚合方法，将各种可能的推断解释聚合成特征重要性分数，解决了推断解释中多个有效解释的问题。这些方法基于合作博弈理论的权力指数和已知的因果强度度量。 |
| [^192] | [Dynamic Selection in Algorithmic Decision-making.](http://arxiv.org/abs/2108.12547) | 本文研究了算法决策中的动态选择问题，针对在线学习算法中数据的内生性导致的偏差提出了一种基于工具变量的纠正算法，并证明了该算法可以获得真实参数值和较低遗憾水平。研究还提供了统计推断的中心极限定理。 |
| [^193] | [Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy.](http://arxiv.org/abs/1911.09307) | 这项工作提出了一种名为Pani的通用正则化器，它在深度神经网络中进行非局部表示，并将邻域补丁特征进行线性插值，从而构建了一种通用且有效的正则化策略。 |
| [^194] | [Learning Interpretable Characteristic Kernels via Decision Forests.](http://arxiv.org/abs/1812.00029) | 本论文介绍了一种通过决策森林构建可解释的特征核的方法，我们构建了基于叶节点相似性的核平均嵌入随机森林（KMERF），并证明其在离散和连续数据上都表现出渐进特征。实验证明KMERF在多种高维数据测试中优于目前的最先进的基于核的方法。 |

# 详细

[^1]: 学习转换以实现通用的实例不变性

    Learning to Transform for Generalizable Instance-wise Invariance. (arXiv:2309.16672v1 [cs.CV])

    [http://arxiv.org/abs/2309.16672](http://arxiv.org/abs/2309.16672)

    该论文提出了一种学习转换以实现通用的实例不变性的方法。通过使用归一化流来预测图像的变换分布，并对预测结果进行平均，可以实现对不同实例之间的对齐，从而推广不变性的类别间的应用。这种方法还可以适应超出分布范围的姿势，并且可以学习更广泛的变换范围。

    

    计算机视觉研究一直致力于构建对自然数据中的空间变换具有强鲁棒性的系统。传统上，可以通过数据增强或将不变性硬编码到架构中来实现这一点。然而，过多或过少的不变性都可能会影响结果，正确的不变性程度在先验中是未知的，并且依赖于实例。理想情况下，应该从数据中学习适当的不变性，并在测试时推断。我们将不变性视为一个预测问题。给定任何图像，我们使用一个归一化流来预测变换的分布，并对它们的预测进行平均。由于这个分布仅取决于实例，我们可以在分类之前对实例进行对齐，并在类别之间推广不变性。同样的分布也可以用于适应超出分布的姿势。这个归一化流是端到端训练的，并且可以学习比Augerino和InstaAug更多范围的变换。当用作数据增强时，我们的m

    Computer vision research has long aimed to build systems that are robust to spatial transformations found in natural data. Traditionally, this is done using data augmentation or hard-coding invariances into the architecture. However, too much or too little invariance can hurt, and the correct amount is unknown a priori and dependent on the instance. Ideally, the appropriate invariance would be learned from data and inferred at test-time.  We treat invariance as a prediction problem. Given any image, we use a normalizing flow to predict a distribution over transformations and average the predictions over them. Since this distribution only depends on the instance, we can align instances before classifying them and generalize invariance across classes. The same distribution can also be used to adapt to out-of-distribution poses. This normalizing flow is trained end-to-end and can learn a much larger range of transformations than Augerino and InstaAug. When used as data augmentation, our m
    
[^2]: RealFill：参考驱动的真实图像修复生成方法

    RealFill: Reference-Driven Generation for Authentic Image Completion. (arXiv:2309.16668v1 [cs.CV])

    [http://arxiv.org/abs/2309.16668](http://arxiv.org/abs/2309.16668)

    RealFill是一种个性化的生成修填模型，通过使用少量目标场景的参考图像，能够以真实、高质量、逼真的内容完成目标图像的修复。

    

    最近，生成图像的进展带来了能够在未知区域生成高质量、逼真图像内容的外拓和修填模型，但这些模型产生的内容是不真实的，因为模型缺乏关于真实场景的足够背景信息。在本文中，我们提出了一种新颖的真实图像修复生成方法RealFill，它通过填充图像中缺失区域使其内容真正应在的内容。RealFill是一种个性化的生成修填模型，仅使用几张目标场景的参考图像进行个性化。这些参考图像不需要与目标图像对齐，可以通过不同的视角、光照条件、摄像机光圈或图像风格拍摄。个性化后，RealFill能够以视觉上引人注目的内容完成目标图像，并且忠实于原始场景。我们在一个全面且具挑战性的图像修复基准上对RealFill进行评估。

    Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions, but the content these models hallucinate is necessarily inauthentic, since the models lack sufficient context about the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging
    
[^3]: HyperPPO:一种用于机器人控制寻找小策略的可扩展方法

    HyperPPO: A scalable method for finding small policies for robotic control. (arXiv:2309.16663v1 [cs.RO])

    [http://arxiv.org/abs/2309.16663](http://arxiv.org/abs/2309.16663)

    HyperPPO是一种可扩展的方法，用于寻找适用于机器人控制的小策略。它利用图状超网络同时估计多个神经网络架构的权重，可以获得性能优秀的策略，并能够满足用户的计算约束条件。

    

    针对记忆有限的高性能机器人的神经控制，需要具有较少参数的模型。寻找这些较小的神经网络架构可能耗费大量时间。我们提出了HyperPPO，一种基于策略上的强化学习算法，利用图状超网络同时估计多个神经网络架构的权重。我们的方法估计的网络权重要远小于常用网络的权重，但却能编码高性能策略。我们同时获得多个训练好的策略，并保持采样效率，使用户能够选择适合其计算约束条件的网络架构。我们展示了我们的方法具有良好的可扩展性-更多的训练资源会产生更快收敛到更高性能架构的结果。我们证明了由HyperPPO估计的神经策略能够分散控制Crazyflie2.1四旋翼飞行器。

    Models with fewer parameters are necessary for the neural control of memory-limited, performant robots. Finding these smaller neural network architectures can be time-consuming. We propose HyperPPO, an on-policy reinforcement learning algorithm that utilizes graph hypernetworks to estimate the weights of multiple neural architectures simultaneously. Our method estimates weights for networks that are much smaller than those in common-use networks yet encode highly performant policies. We obtain multiple trained policies at the same time while maintaining sample efficiency and provide the user the choice of picking a network architecture that satisfies their computational constraints. We show that our method scales well - more training resources produce faster convergence to higher-performing architectures. We demonstrate that the neural policies estimated by HyperPPO are capable of decentralized control of a Crazyflie2.1 quadrotor. Website: https://sites.google.com/usc.edu/hyperppo
    
[^4]: 首次研究女性大脑在月经期间的3D形状变化的地理回归

    Geodesic Regression Characterizes 3D Shape Changes in the Female Brain During Menstruation. (arXiv:2309.16662v1 [cs.CV])

    [http://arxiv.org/abs/2309.16662](http://arxiv.org/abs/2309.16662)

    本研究首次探究了女性大脑在月经期间的3D形状变化，通过地理回归方法加速计算并提供精确度与速度之间的权衡。根据合成数据测试结果，可以在牺牲少部分精确度的情况下获得显著加速。

    

    女性在绝经后更容易患上阿尔茨海默病和其他神经系统疾病，然而，将女性大脑健康与性激素波动联系起来的研究还有限。通过开发工具来量化大脑在性激素波动期间发生的3D形状变化，我们试图调查这种联系。地理回归在3D离散表面空间上提供了刻画大脑形状演变的原则性方法。然而，目前的方法在实际使用中计算复杂度过高。在本文中，我们提出了加速3D离散表面形状地理回归的近似方案。我们还提供了何时使用每个近似方法的经验规则。我们通过使用合成数据对这些近似方法进行了测试，量化了速度和精确度之间的权衡，并证明从从中获得非常显著的加速，并只稍微牺牲一些精确度。最后，我们将该方法应用于真实的大脑数据中。

    Women are at higher risk of Alzheimer's and other neurological diseases after menopause, and yet research connecting female brain health to sex hormone fluctuations is limited. We seek to investigate this connection by developing tools that quantify 3D shape changes that occur in the brain during sex hormone fluctuations. Geodesic regression on the space of 3D discrete surfaces offers a principled way to characterize the evolution of a brain's shape. However, in its current form, this approach is too computationally expensive for practical use. In this paper, we propose approximation schemes that accelerate geodesic regression on shape spaces of 3D discrete surfaces. We also provide rules of thumb for when each approximation can be used. We test our approach on synthetic data to quantify the speed-accuracy trade-off of these approximations and show that practitioners can expect very significant speed-up while only sacrificing little accuracy. Finally, we apply the method to real brain 
    
[^5]: 视觉背景下的少样本湿疹分割学习

    Visual In-Context Learning for Few-Shot Eczema Segmentation. (arXiv:2309.16656v1 [cs.CV])

    [http://arxiv.org/abs/2309.16656](http://arxiv.org/abs/2309.16656)

    这篇论文研究了视觉背景下的少样本湿疹分割学习的能力，提出了一种基于通用视觉模型SegGPT的策略，通过仅使用少量示例图像进行湿疹分割，而无需重新训练模型。

    

    从数字相机图像中自动诊断湿疹对于开发允许患者自我监测恢复的应用程序至关重要。其中一个重要的组成部分是从这些图像中分割湿疹区域。当前的湿疹分割方法依赖于深度神经网络，如基于卷积（CNN）的U-Net或基于转换器的Swin U-Net。虽然有效，但这些方法需要大量的注释数据，而这很难获得。在这里，我们研究了视觉背景下的少样本湿疹分割学习的能力，可以仅用少量示例进行湿疹分割，而无需对模型进行重新训练。具体而言，我们提出了一种应用于湿疹分割的视觉背景下学习策略，使用了一个名为SegGPT的通用视觉模型。在拥有注释湿疹图像的数据集上进行基准测试时，我们展示了SegGPT仅使用训练数据集中的2个代表性示例图像的性能更好（mIoU：36.69）。

    Automated diagnosis of eczema from digital camera images is crucial for developing applications that allow patients to self-monitor their recovery. An important component of this is the segmentation of eczema region from such images. Current methods for eczema segmentation rely on deep neural networks such as convolutional (CNN)-based U-Net or transformer-based Swin U-Net. While effective, these methods require high volume of annotated data, which can be difficult to obtain. Here, we investigate the capabilities of visual in-context learning that can perform few-shot eczema segmentation with just a handful of examples and without any need for retraining models. Specifically, we propose a strategy for applying in-context learning for eczema segmentation with a generalist vision model called SegGPT. When benchmarked on a dataset of annotated eczema images, we show that SegGPT with just 2 representative example images from the training dataset performs better (mIoU: 36.69) than a CNN U-Ne
    
[^6]: 可重复性报告：多样的生物信息化神经结构对前列腺癌分层的影响

    Reusability report: Prostate cancer stratification with diverse biologically-informed neural architectures. (arXiv:2309.16645v1 [cs.LG])

    [http://arxiv.org/abs/2309.16645](http://arxiv.org/abs/2309.16645)

    该研究通过验证和重新实现作者提出的生物信息化神经网络P-NET的方法，量化了使用Reactome生物通路进行网络稀疏化的贡献，并探索了其他神经架构和方法。研究结果表明，不同结构的深度神经网络对个体患者进行了错误的预测。

    

    在Elmarakeby等人的研究中，提出了一种具有生物信息化、稀疏连接的前馈神经网络（P-NET）来模拟前列腺癌的状态。我们使用原始代码和我们自己使用更现代化的库重新实现的方法来验证Elmarakeby等人研究的可复现性。我们量化了通过Reactome生物通路进行网络稀疏化的贡献，并确认其对P-NET的卓越性能的重要性。此外，我们还探索了将生物信息纳入网络的其他神经架构和方法。我们在相同的训练数据上尝试了三种类型的图神经网络，并研究了不同模型之间的临床预测一致性。我们的分析表明，具有不同结构的深度神经网络对个体患者进行错误的预测。

    In, Elmarakeby et al., "Biologically informed deep neural network for prostate cancer discovery", a feedforward neural network with biologically informed, sparse connections (P-NET) was presented to model the state of prostate cancer. We verified the reproducibility of the study conducted by Elmarakeby et al., using both their original codebase, and our own re-implementation using more up-to-date libraries. We quantified the contribution of network sparsification by Reactome biological pathways, and confirmed its importance to P-NET's superior performance. Furthermore, we explored alternative neural architectures and approaches to incorporating biological information into the networks. We experimented with three types of graph neural networks on the same training data, and investigated the clinical prediction agreement between different models. Our analyses demonstrated that deep neural networks with distinct architectures make incorrect predictions for individual patient that are pers
    
[^7]: 混合你自己的对比对

    Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])

    [http://arxiv.org/abs/2309.16633](http://arxiv.org/abs/2309.16633)

    本文提出了一种名为SupReMix的方法，通过混合样本，特别是混合负样本和混合正样本，来解决回归问题中表示学习的挑战。这种方法能够提供更好的性能和更准确的回归结果。

    

    在表示学习中，回归问题传统上比分类问题受到的关注较少。直接应用为分类设计的表示学习技术到回归问题往往会导致潜空间中碎片化的表示，从而产生次优的性能。本文认为，由于忽视了两个关键方面：序序感知和难度，对于回归问题而言，对比学习的潜能被忽视了。为了解决这些挑战，我们提倡“混合自己的对比对进行监督性对比回归”，而不仅仅依靠真实/增强样本。具体来说，我们提出了混合式监督对比回归学习（SupReMix）。它在嵌入级别上以锚点包含的混合（锚点和一个不同的负样本的混合）作为困难负对，以锚点排除的混合（两个不同的负样本的混合）作为困难正对。这一策略形成了困难样本对学习的方式。

    In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harde
    
[^8]: 鲁棒的离线强化学习 - 认证置信区间

    Robust Offline Reinforcement Learning -- Certify the Confidence Interval. (arXiv:2309.16631v1 [cs.LG])

    [http://arxiv.org/abs/2309.16631](http://arxiv.org/abs/2309.16631)

    本文提出了一种使用随机平滑算法认证给定策略在离线环境中的鲁棒性的方法，证明了该算法的高效性，并得到实验证实。

    

    目前，强化学习（RL），特别是深度RL，在研究领域中得到了越来越多的关注。然而，由于攻击方式变得成熟，RL的安全性成为一个明显的问题。为了抵御此类对抗性攻击，已经开发了几种实用的方法，如对抗性训练、数据过滤等。然而，这些方法大多基于经验算法和实验，缺乏对算法鲁棒性的严格理论分析。本文提出了一种利用随机平滑认证给定策略的鲁棒性的算法，该算法的效率可以证明和进行，与没有随机平滑的算法一样高效。不同环境的实验证实了我们算法的正确性。

    Currently, reinforcement learning (RL), especially deep RL, has received more and more attention in the research area. However, the security of RL has been an obvious problem due to the attack manners becoming mature. In order to defend against such adversarial attacks, several practical approaches are developed, such as adversarial training, data filtering, etc. However, these methods are mostly based on empirical algorithms and experiments, without rigorous theoretical analysis of the robustness of the algorithms. In this paper, we develop an algorithm to certify the robustness of a given policy offline with random smoothing, which could be proven and conducted as efficiently as ones without random smoothing. Experiments on different environments confirm the correctness of our algorithm.
    
[^9]: 关于使用LAD进行学习

    On Learning with LAD. (arXiv:2309.16630v1 [cs.LG])

    [http://arxiv.org/abs/2309.16630](http://arxiv.org/abs/2309.16630)

    该研究提出了关于LAD模型不存在过拟合的理论解释，并估计了LAD模型的VC维度。实验证实了这些观察结果。

    

    数据的逻辑分析（LAD）是一种基于具有析取范式（DNF）表示的布尔函数的技术，它产生的两类分类器。虽然LAD算法采用了优化技术，但得到的二进制分类器或二进制规则不会导致过拟合。我们提出了一个理论上的证明，解释了为什么LAD模型不存在过拟合，我们估计了由少量立方数单项式组成的DNF假设集的Vapnik-Chervonenkis维度（VC维度）。我们通过实验证实了我们的观察结果。

    The logical analysis of data, LAD, is a technique that yields two-class classifiers based on Boolean functions having disjunctive normal form (DNF) representation. Although LAD algorithms employ optimization techniques, the resulting binary classifiers or binary rules do not lead to overfitting. We propose a theoretical justification for the absence of overfitting by estimating the Vapnik-Chervonenkis dimension (VC dimension) for LAD models where hypothesis sets consist of DNFs with a small number of cubic monomials. We illustrate and confirm our observations empirically.
    
[^10]: 残差网络中的深度超参数转移：动态和缩放限制

    Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit. (arXiv:2309.16620v1 [stat.ML])

    [http://arxiv.org/abs/2309.16620](http://arxiv.org/abs/2309.16620)

    这项研究通过残差分支尺度和$\mu$P参数化的残差网络，实现了深度学习中超参数的跨宽度和深度的转移。

    

    随着模型大小的增加，深度学习中超参数调整的成本不断上升，促使从业者寻找使用较小网络的代理方法进行调整。其中一个建议使用$\mu$P参数化网络，其中小宽度网络的最佳超参数转移到任意宽度的网络中。然而，在这个方案中，超参数不会在不同深度之间转移。为了解决这个问题，我们研究了具有$1/\sqrt{\text{depth}}$的残差分支尺度和$\mu$P参数化的残差网络。我们通过实验证明，使用这种参数化训练的残差结构，包括卷积ResNet和Vision Transformer，在CIFAR-10和ImageNet上展示了跨宽度和深度的最佳超参数转移。此外，我们的经验发现得到了理论的支持和动机。利用神经网络学习动力学的动态均场理论（DMFT）描述的最新进展，我们展示了

    The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses $\mu$P parameterized networks, where the optimal hyperparameters for small width networks transfer to networks with arbitrarily large width. However, in this scheme, hyperparameters do not transfer across depths. As a remedy, we study residual networks with a residual branch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$P parameterization. We provide experiments demonstrating that residual architectures including convolutional ResNets and Vision Transformers trained with this parameterization exhibit transfer of optimal hyperparameters across width and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findings are supported and motivated by theory. Using recent developments in the dynamical mean field theory (DMFT) description of neural network learning dynamics, we show
    
[^11]: 利用融合网络Gromov-Wasserstein距离中的边特征对图进行挖掘

    Exploiting Edge Features in Graphs with Fused Network Gromov-Wasserstein Distance. (arXiv:2309.16604v1 [stat.ML])

    [http://arxiv.org/abs/2309.16604](http://arxiv.org/abs/2309.16604)

    该论文介绍了一种用于比较具有节点和边特征的图的扩展Gromov-Wasserstein距离的方法，并提出了新的算法来计算距离和重心。通过实验证明了该方法在图学习任务中的有效性。

    

    图的成对比较在机器学习中的许多应用中至关重要，包括聚类、基于核的分类/回归以及最近的监督图预测。图之间的距离通常依赖于这些结构化对象的信息表达，如子结构包或其他图嵌入。一种最近流行的解决方案是将图表示为度量测度空间，这样可以成功地利用最优输运，它提供了有意义的距离来比较它们：Gromov-Wasserstein距离。然而，这类距离忽略了边属性，而这对于许多结构化对象是至关重要的。在这项工作中，我们介绍了一种扩展Gromov-Wasserstein距离的方法，用于比较具有节点和边特征的图。我们提出了距离和重心计算的新算法。我们通过实验证明了新距离在图出现在输入和输出的学习任务中的有效性

    Pairwise comparison of graphs is key to many applications in Machine learning ranging from clustering, kernel-based classification/regression and more recently supervised graph prediction. Distances between graphs usually rely on informative representations of these structured objects such as bag of substructures or other graph embeddings. A recently popular solution consists in representing graphs as metric measure spaces, allowing to successfully leverage Optimal Transport, which provides meaningful distances allowing to compare them: the Gromov-Wasserstein distances. However, this family of distances overlooks edge attributes, which are essential for many structured objects. In this work, we introduce an extension of Gromov-Wasserstein distance for comparing graphs whose both nodes and edges have features. We propose novel algorithms for distance and barycenter computation. We empirically show the effectiveness of the novel distance in learning tasks where graphs occur in either inp
    
[^12]: 基于深度学习的上行多用户SIMO波束成形设计

    Deep Learning Based Uplink Multi-User SIMO Beamforming Design. (arXiv:2309.16603v1 [cs.IT])

    [http://arxiv.org/abs/2309.16603](http://arxiv.org/abs/2309.16603)

    本论文提出了一种基于深度学习的无监督框架（NNBF）用于设计上行接收多用户单输入多输出（MU-SIMO）波束成形，旨在通过最大化总速率提高吞吐量，并提供高效的计算解决方案。

    

    第五代（5G）无线通信网络的进展给无线资源管理解决方案提出了更高的要求，包括高数据速率、广泛覆盖、最小延迟和高效节能性能。然而，传统方法在计算复杂性和适应动态条件的能力方面存在缺点，导致理论分析与算法解决方案在无线资源管理方面的实际执行之间存在差距。基于深度学习的技术具有良好的表示能力，为弥合这一差距提供了有希望的解决方案。我们提出了一种新颖的无监督深度学习框架，称为NNBF，用于设计上行接收多用户单输入多输出（MU-SIMO）波束成形。主要目标是通过专注于最大化总速率来提高吞吐量，同时提供计算效率高的解决方案，与现有的协议相比具有较大优势。

    The advancement of fifth generation (5G) wireless communication networks has created a greater demand for wireless resource management solutions that offer high data rates, extensive coverage, minimal latency and energy-efficient performance. Nonetheless, traditional approaches have shortcomings when it comes to computational complexity and their ability to adapt to dynamic conditions, creating a gap between theoretical analysis and the practical execution of algorithmic solutions for managing wireless resources. Deep learning-based techniques offer promising solutions for bridging this gap with their substantial representation capabilities. We propose a novel unsupervised deep learning framework, which is called NNBF, for the design of uplink receive multi-user single input multiple output (MU-SIMO) beamforming. The primary objective is to enhance the throughput by focusing on maximizing the sum-rate while also offering computationally efficient solution, in contrast to established co
    
[^13]: 基于交叉预测的推理

    Cross-Prediction-Powered Inference. (arXiv:2309.16598v1 [stat.ML])

    [http://arxiv.org/abs/2309.16598](http://arxiv.org/abs/2309.16598)

    本文介绍了一种基于机器学习的交叉预测方法，可以有效地进行推理。该方法通过使用一个小型标记数据集和一个大型未标记数据集，通过机器学习填补缺失的标签，并采用去偏差方法纠正预测的不准确性。

    

    可靠的数据驱动决策依赖于高质量的标注数据，然而获取高质量的标注数据经常需要繁琐的人工标注或者缓慢昂贵的科学测量。机器学习作为一种替代方案正变得越来越有吸引力，因为精密的预测技术可以快速、廉价地产生大量预测标签；例如，预测的蛋白质结构被用来补充实验得到的结构，卫星图像预测的社会经济指标被用来补充准确的调查数据等。由于预测具有不完美和潜在偏差的特点，这种做法对下游推理的有效性产生了质疑。我们引入了基于机器学习的交叉预测方法，用于有效的推理。通过一个小的标记数据集和一个大的未标记数据集，交叉预测通过机器学习填补缺失的标签，并应用一种去偏差的方法来纠正预测不准确性。

    While reliable data-driven decision-making hinges on high-quality labeled data, the acquisition of quality labels often involves laborious human annotations or slow and expensive scientific measurements. Machine learning is becoming an appealing alternative as sophisticated predictive techniques are being used to quickly and cheaply produce large amounts of predicted labels; e.g., predicted protein structures are used to supplement experimentally derived structures, predictions of socioeconomic indicators from satellite imagery are used to supplement accurate survey data, and so on. Since predictions are imperfect and potentially biased, this practice brings into question the validity of downstream inferences. We introduce cross-prediction: a method for valid inference powered by machine learning. With a small labeled dataset and a large unlabeled dataset, cross-prediction imputes the missing labels via machine learning and applies a form of debiasing to remedy the prediction inaccurac
    
[^14]: 异质搜索空间上的贝叶斯优化的迁移学习

    Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces. (arXiv:2309.16597v1 [cs.LG])

    [http://arxiv.org/abs/2309.16597](http://arxiv.org/abs/2309.16597)

    本文提出了MPHD方法，通过模型预训练和神经网络在异质搜索空间上实现贝叶斯优化的迁移学习。实验证明了MPHD的有效性和在黑盒函数优化任务中的优越性能。

    

    贝叶斯优化是一种流行的黑盒函数优化方法，它基于贝叶斯模型（通常是高斯过程）进行顺序决策。为了确保模型的质量，我们开发了迁移学习方法，通过学习来自“训练”函数的观察结果来自动设计高斯过程先验。这些训练函数通常需要与“测试”函数（待优化的黑盒函数）具有相同的定义域。在本文中，我们介绍了一种名为MPHD的模型预训练方法，它使用神经网络将特定于领域的上下文映射到分层高斯过程的规范。MPHD可以与贝叶斯优化无缝集成，实现异质搜索空间的知识迁移。我们的理论和实证结果证明了MPHD的有效性，并展示了它在具有挑战性的黑盒函数优化任务中的优越性能。

    Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on "training" functions. These training functions are typically required to have the same domain as the "test" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.
    
[^15]: LLM能否有效利用结构信息进行图学习：何时何地。

    Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])

    [http://arxiv.org/abs/2309.16595](http://arxiv.org/abs/2309.16595)

    本文研究了大型语言模型（LLM）在图数据中的应用，发现LLM可以从结构信息中受益，尤其是在文本节点特征缺乏的情况下，而LLM的性能与数据泄露没有显著相关。

    

    本文研究了大型语言模型（LLM）在结构化数据（特别是图数据）上的应用，这是LLM文献中尚未充分探索的重要数据形态。我们旨在了解在节点分类任务中，何时何地引入图数据中的结构信息可以提高LLM的预测性能。为了解决“何时”问题，我们研究了多种编码结构信息的提示方法，设置中文本节点特征丰富或稀缺。对于“为什么”问题，我们探讨了LLM性能的两个潜在因素：数据泄露和同质性。我们的研究结果表明：（i）LLM可以从结构信息中受益，尤其是在文本节点特征缺乏的情况下；（ii）没有实质性的证据表明LLM性能与数据泄露有显著相关；（iii）LLM在目标节点上的性能与正向相关。

    This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
    
[^16]: 导航医疗洞见：知识图谱中可解释性的鸟瞰视角

    Navigating Healthcare Insights: A Birds Eye View of Explainability with Knowledge Graphs. (arXiv:2309.16593v1 [cs.AI])

    [http://arxiv.org/abs/2309.16593](http://arxiv.org/abs/2309.16593)

    本文总结了知识图谱在医疗领域的影响以及开发可解释性AI模型中的作用。强调了通过知识注入学习提高知识图谱的可解释性的重要性，并提供了未来方向的见解。

    

    知识图谱在医疗人工智能中日益受到重视，特别是在药物发现和制药研究中，因为它们提供了一种结构化的方式来整合多样化的信息源，增强了AI系统的可解释性。在医疗领域，这种可解释性非常重要，因为信任和透明性至关重要，可解释的AI（XAI）支持医疗专业人员的决策。本综述总结了关于知识图谱在医疗领域的影响及其在开发可解释性AI模型中的作用的最新文献。我们涵盖了知识图谱的工作流程，包括构建、关系提取、推理以及它们在药物-药物相互作用、药物靶点相互作用、药物开发、药物不良反应和生物信息学等领域的应用。我们强调通过在医疗中进行知识注入学习来提高知识图谱的可解释性的重要性。最后，我们突出研究挑战并提供未来方向的见解。

    Knowledge graphs (KGs) are gaining prominence in Healthcare AI, especially in drug discovery and pharmaceutical research as they provide a structured way to integrate diverse information sources, enhancing AI system interpretability. This interpretability is crucial in healthcare, where trust and transparency matter, and eXplainable AI (XAI) supports decision making for healthcare professionals. This overview summarizes recent literature on the impact of KGs in healthcare and their role in developing explainable AI models. We cover KG workflow, including construction, relationship extraction, reasoning, and their applications in areas like Drug-Drug Interactions (DDI), Drug Target Interactions (DTI), Drug Development (DD), Adverse Drug Reactions (ADR), and bioinformatics. We emphasize the importance of making KGs more interpretable through knowledge-infused learning in healthcare. Finally, we highlight research challenges and provide insights for future directions.
    
[^17]: 利用跨模态知识的张量分解在数据受限红外目标检测中的应用

    Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection. (arXiv:2309.16592v1 [cs.CV])

    [http://arxiv.org/abs/2309.16592](http://arxiv.org/abs/2309.16592)

    本文介绍了一种在数据受限红外目标检测中利用跨模态知识的方法，通过创新的张量分解方法，将RGB模态的目标检测器扩展到红外模态，同时保持性能。该方法首先在RGB模态上进行预训练，然后在IR模态上进行微调，以提高红外目标检测性能。

    

    红外图像识别性能较差的主要瓶颈是缺乏足够的标记训练数据，由于获取这些数据的成本高昂。在这项工作中，我们意识到RGB模态的目标检测方法相当稳健（至少对于一些常见类别，如人、车等），这要归功于现有的巨大训练集，因此我们尝试利用RGB模态的线索来扩展目标检测器到红外模态，同时保持RGB模态的模型性能。我们方法的核心是一种新颖的张量分解方法，称为TensorFact，它将卷积神经网络（CNN）的一层卷积核分解为具有比原始CNN更少参数的低秩因子矩阵。我们首先在RGB模态上预训练这些因子矩阵，假定存在大量的训练数据，然后仅在IR模态上增加少量可训练参数，以避免过拟合，同时进行训练。

    The primary bottleneck towards obtaining good recognition performance in IR images is the lack of sufficient labeled training data, owing to the cost of acquiring such data. Realizing that object detection methods for the RGB modality are quite robust (at least for some commonplace classes, like person, car, etc.), thanks to the giant training sets that exist, in this work we seek to leverage cues from the RGB modality to scale object detectors to the IR modality, while preserving model performance in the RGB modality. At the core of our method, is a novel tensor decomposition method called TensorFact which splits the convolution kernels of a layer of a Convolutional Neural Network (CNN) into low-rank factor matrices, with fewer parameters than the original CNN. We first pretrain these factor matrices on the RGB modality, for which plenty of training data are assumed to exist and then augment only a few trainable parameters for training on the IR modality to avoid over-fitting, while e
    
[^18]: 用于开发协作分布式机器学习系统的设计工具箱

    A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems. (arXiv:2309.16584v1 [cs.MA])

    [http://arxiv.org/abs/2309.16584](http://arxiv.org/abs/2309.16584)

    我们开发了一个CDML设计工具箱，可以指导开发者设计满足用例要求的协作分布式机器学习系统。

    

    为了在保护机器学习模型的机密性的同时利用来自多方的训练数据对模型进行充分训练，研究人员开发了各种协作分布式机器学习（CDML）系统设计，例如辅助学习、联邦学习和分裂学习。CDML系统设计展示了不同的特征，例如高度的代理人自治性、机器学习模型的机密性和容错性。面对不同特征的各种CDML系统设计，开发者很难有针对性地设计满足用例要求的CDML系统。然而，不合适的CDML系统设计可能导致CDML系统无法实现其预期目的。我们开发了一个CDML设计工具箱，可以指导CDML系统的开发。基于CDML设计工具箱，我们提出了具有不同关键特征的CDML系统典型，可以支持设计满足用例要求的CDML系统。

    To leverage training data for the sufficient training of ML models from multiple parties in a confidentiality-preserving way, various collaborative distributed machine learning (CDML) system designs have been developed, for example, to perform assisted learning, federated learning, and split learning. CDML system designs show different traits, for example, high agent autonomy, machine learning (ML) model confidentiality, and fault tolerance. Facing a wide variety of CDML system designs with different traits, it is difficult for developers to design CDML systems with traits that match use case requirements in a targeted way. However, inappropriate CDML system designs may result in CDML systems failing their envisioned purposes. We developed a CDML design toolbox that can guide the development of CDML systems. Based on the CDML design toolbox, we present CDML system archetypes with distinct key traits that can support the design of CDML systems to meet use case requirements.
    
[^19]: M-OFDFT：利用深度学习克服分子系统中的无轨道密度泛函理论的障碍

    M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning. (arXiv:2309.16578v1 [stat.ML])

    [http://arxiv.org/abs/2309.16578](http://arxiv.org/abs/2309.16578)

    M-OFDFT是一种利用深度学习模型解决分子系统问题的OFDFT方法，通过将非局域性建立在模型中并使用紧凑的密度表示，实现了与Kohn-Sham DFT相近的精确度，并且具有良好的外推能力。

    

    无轨道密度泛函理论（OFDFT）是一种具有较低运算成本的量子化学计算方法，比起常用的Kohn-Sham密度泛函理论更加适用于当代分子研究。然而，OFDFT的精确性受到了动能密度泛函的限制，对于非周期性分子系统的近似求解非常困难。本文提出了名为M-OFDFT的方法，利用深度学习的函数模型解决了分子系统的问题。我们将必要的非局域性建立在这个模型中，通过原子基下的展开系数作为紧凑的密度表示来降低成本。通过解决其中的非传统学习挑战的技术，M-OFDFT在一系列OFDFT无法触及的分子上实现了与Kohn-Sham DFT相当的精确度。更有吸引力的是，M-OFDFT在训练时属于更大的分子中有着良好的外推能力，为研究大分子提供了有吸引力的规模效应。

    Orbital-free density functional theory (OFDFT) is a quantum chemistry formulation that has a lower cost scaling than the prevailing Kohn-Sham DFT, which is increasingly desired for contemporary molecular research. However, its accuracy is limited by the kinetic energy density functional, which is notoriously hard to approximate for non-periodic molecular systems. In this work, we propose M-OFDFT, an OFDFT approach capable of solving molecular systems using a deep-learning functional model. We build the essential nonlocality into the model, which is made affordable by the concise density representation as expansion coefficients under an atomic basis. With techniques to address unconventional learning challenges therein, M-OFDFT achieves a comparable accuracy with Kohn-Sham DFT on a wide range of molecules untouched by OFDFT before. More attractively, M-OFDFT extrapolates well to molecules much larger than those in training, which unleashes the appealing scaling for studying large molecu
    
[^20]: 以编译为防御：通过张量优化增强深度学习模型的攻击鲁棒性

    Compilation as a Defense: Enhancing DL Model Attack Robustness via Tensor Optimization. (arXiv:2309.16577v1 [cs.LG])

    [http://arxiv.org/abs/2309.16577](http://arxiv.org/abs/2309.16577)

    本文通过使用张量优化的模型编译技术，提出了一种新的防御对抗机器学习侧信道攻击的方法，相对效果降低了43％。

    

    对抗机器学习(AML)是一个快速发展的安全研究领域，其中常被忽视的领域是通过侧信道进行模型攻击。此前的工作表明这种攻击是严重威胁，但在避免昂贵的模型重新设计的高效补救策略方面进展甚微。本文展示了一种新的对抗AML侧信道攻击的防御策略，利用模型编译技术，即张量优化。我们通过张量优化展示了攻击效果的相对降低达43％，讨论了其含义和未来研究方向。

    Adversarial Machine Learning (AML) is a rapidly growing field of security research, with an often overlooked area being model attacks through side-channels. Previous works show such attacks to be serious threats, though little progress has been made on efficient remediation strategies that avoid costly model re-engineering. This work demonstrates a new defense against AML side-channel attacks using model compilation techniques, namely tensor optimization. We show relative model attack effectiveness decreases of up to 43% using tensor optimization, discuss the implications, and direction of future work.
    
[^21]: 基于机器学习方法的功能梯度材料增材制造的综述

    Review of Machine Learning Methods for Additive Manufacturing of Functionally Graded Materials. (arXiv:2309.16571v1 [cs.LG])

    [http://arxiv.org/abs/2309.16571](http://arxiv.org/abs/2309.16571)

    该论文综述了基于机器学习方法的功能梯度材料增材制造。功能梯度材料是一类具有平滑性质过渡的高级复合材料，机器学习技术被应用于优化加工参数、提高产品质量和检测制造缺陷。这些方法有望提高零件性能和性质。

    

    增材制造通过实现直接材料连接，革新了复杂零件的制造，并提供了成本效益高的复杂零件制造、减少制造废料以及为制造自动化开启新的可能性等多个优势。其中，功能梯度材料（FGMs）作为一类材料，在提高零件性能和性质方面具有巨大潜力。FGMs是高级复合材料，其性质呈平滑过渡，因此被航空、汽车、生物医学和国防等行业广泛应用。与传统复合材料不同，FGMs中的成分会逐渐变化，从而提高了材料的性能。最近，机器学习技术已经成为制造FGMs的有希望的手段，可以优化加工参数、提高产品质量并检测制造缺陷。

    Additive manufacturing has revolutionized the manufacturing of complex parts by enabling direct material joining and offers several advantages such as cost-effective manufacturing of complex parts, reducing manufacturing waste, and opening new possibilities for manufacturing automation. One group of materials for which additive manufacturing holds great potential for enhancing component performance and properties is Functionally Graded Materials (FGMs). FGMs are advanced composite materials that exhibit smoothly varying properties making them desirable for applications in aerospace, automobile, biomedical, and defense industries. Such composition differs from traditional composite materials, since the location-dependent composition changes gradually in FGMs, leading to enhanced properties. Recently, machine learning techniques have emerged as a promising means for fabrication of FGMs through optimizing processing parameters, improving product quality, and detecting manufacturing defect
    
[^22]: 增强解释性: 无监督的和本质上可解释的图嵌入

    Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings. (arXiv:2309.16564v1 [cs.LG])

    [http://arxiv.org/abs/2309.16564](http://arxiv.org/abs/2309.16564)

    本文研究了无监督图表示学习，通过学习并利用保持语义的数据增强方法，创建了解释性嵌入，并解决了无监督表示学习可解释性研究领域的不足。

    

    无监督学习使我们能够利用大量可用的未标记数据，并创建可用于各种下游任务的嵌入。然而，无监督表示学习的典型缺乏解释性已成为最近透明人工智能法规的限制因素。在本文中，我们研究了图表示学习，并展示了学习保持语义的数据增强方法可以用于生成解释。我们的框架名为INGENIOUS，创建了本质上可解释的嵌入，并消除了昂贵的后续分析的需要。我们还引入了针对无监督表示学习可解释性研究领域缺乏形式化和度量的额外指标。我们的结果通过应用于图级和节点级任务的实验证明，可解释的嵌入在性能上达到了最先进水平。

    Unsupervised learning allows us to leverage unlabelled data, which has become abundantly available, and to create embeddings that are usable on a variety of downstream tasks. However, the typical lack of interpretability of unsupervised representation learning has become a limiting factor with regard to recent transparent-AI regulations. In this paper, we study graph representation learning and we show that data augmentation that preserves semantics can be learned and used to produce interpretations. Our framework, which we named INGENIOUS, creates inherently interpretable embeddings and eliminates the need for costly additional post-hoc analysis. We also introduce additional metrics addressing the lack of formalism and metrics in the understudied area of unsupervised-representation learning interpretability. Our results are supported by an experimental study applied to both graph-level and node-level tasks and show that interpretable embeddings provide state-of-the-art performance on 
    
[^23]: CRIMED：具有无界随机破坏的赌徒问题的遗憾下界和上界

    CRIMED: Lower and Upper Bounds on Regret for Bandits with Unbounded Stochastic Corruption. (arXiv:2309.16563v1 [stat.ML])

    [http://arxiv.org/abs/2309.16563](http://arxiv.org/abs/2309.16563)

    本文研究了具有任意破坏的多臂赌徒问题，并建立了一个与问题相关的遗憾下界。我们提出了CRIMED算法，该算法在具有已知方差的高斯分布赌徒问题上实现了遗憾下界。

    

    我们研究了在多臂赌徒问题中具有任意破坏的遗憾最小化问题。与经典设定类似，代理接收到的奖励是从每个时间点选择的臂的分布独立生成的。然而，这些奖励并不直接观察到。相反，对于固定的ε∈(0,12)，代理以概率1-ε从选择的臂的分布中观测一个样本，或以概率ε从任意破坏分布中观测。重要的是，我们对这些破坏分布不做任何假设，它们可以是无界的。在这种可能具有无界破坏的情况下，我们为给定的臂分布族建立了一个与问题相关的遗憾下界。我们引入了CRIMED，这是一个渐近最优的算法，它在具有已知方差的高斯分布赌徒问题上实现了遗憾下界。此外，我们还对有限样本进行了分析。

    We investigate the regret-minimisation problem in a multi-armed bandit setting with arbitrary corruptions. Similar to the classical setup, the agent receives rewards generated independently from the distribution of the arm chosen at each time. However, these rewards are not directly observed. Instead, with a fixed $\varepsilon\in (0,\frac{1}{2})$, the agent observes a sample from the chosen arm's distribution with probability $1-\varepsilon$, or from an arbitrary corruption distribution with probability $\varepsilon$. Importantly, we impose no assumptions on these corruption distributions, which can be unbounded. In this setting, accommodating potentially unbounded corruptions, we establish a problem-dependent lower bound on regret for a given family of arm distributions. We introduce CRIMED, an asymptotically-optimal algorithm that achieves the exact lower bound on regret for bandits with Gaussian distributions with known variance. Additionally, we provide a finite-sample analysis of 
    
[^24]: 基于投票网络的等高堤农田分割和分类

    Voting Network for Contour Levee Farmland Segmentation and Classification. (arXiv:2309.16561v1 [cs.CV])

    [http://arxiv.org/abs/2309.16561](http://arxiv.org/abs/2309.16561)

    本文提出了一个基于投票网络的端到端可训练模型，用于从高分辨率航空影像中分割和分类等高堤农田。通过使用投票机制来减少边界扭曲和类别混淆，实现了较高的准确性。

    

    高分辨率航空影像允许在农田分割中获取细节信息。然而，小物体和特征会导致物体边界的扭曲，需要更大的上下文视图来减少类别混淆。本文提出了一个端到端可训练的网络，用于从高分辨率航空影像中分割等高堤农田。我们设计了一个融合块，其中包括多个投票块，以实现图像分割和分类。我们将融合块与骨干网络结合，同时生成语义预测和分割片段。分割片段用于在预测上进行多数投票。网络被训练为将段落的最有可能类别标签分配给其像素，从而学习农田的概念，而不是单独分析像素。

    High-resolution aerial imagery allows fine details in the segmentation of farmlands. However, small objects and features introduce distortions to the delineation of object boundaries, and larger contextual views are needed to mitigate class confusion. In this work, we present an end-to-end trainable network for segmenting farmlands with contour levees from high-resolution aerial imagery. A fusion block is devised that includes multiple voting blocks to achieve image segmentation and classification. We integrate the fusion block with a backbone and produce both semantic predictions and segmentation slices. The segmentation slices are used to perform majority voting on the predictions. The network is trained to assign the most likely class label of a segment to its pixels, learning the concept of farmlands rather than analyzing constitutive pixels separately. We evaluate our method using images from the National Agriculture Imagery Program. Our method achieved an average accuracy of 94.3
    
[^25]: 在实时流行病学指标中纠正异质性

    Correcting for heterogeneity in real-time epidemiological indicators. (arXiv:2309.16546v1 [cs.LG])

    [http://arxiv.org/abs/2309.16546](http://arxiv.org/abs/2309.16546)

    介绍了在流行病学监测中纠正数据源中的空间和时间异质性问题的方法，使用低秩矩阵来近似异质性并通过“引导”信号来纠正偏差，以提供更可靠的建模和预测信号。

    

    辅助数据源在流行病学监测中变得越来越重要，因为它们通常比传统监测信号在空间和时间分辨率、覆盖范围和延迟方面更好。我们描述了从这些数据源导出的指标中存在的空间和时间异质性问题，其中存在空间和/或时间偏差。我们提出了一种使用“引导”信号来纠正这些偏差并产生更可靠的信号供建模和预测使用的方法。该方法假设异质性可以用低秩矩阵来近似，并且时间异质性在时间上是平滑的。我们还提出了一种超参数选择算法来选择表示矩阵秩和纠正的时间平滑度的参数。在缺乏基准事实的情况下，我们使用地图和图表来论证这种方法确实减少了异质性。

    Auxiliary data sources have become increasingly important in epidemiological surveillance, as they are often available at a finer spatial and temporal resolution, larger coverage, and lower latency than traditional surveillance signals. We describe the problem of spatial and temporal heterogeneity in these signals derived from these data sources, where spatial and/or temporal biases are present. We present a method to use a ``guiding'' signal to correct for these biases and produce a more reliable signal that can be used for modeling and forecasting. The method assumes that the heterogeneity can be approximated by a low-rank matrix and that the temporal heterogeneity is smooth over time. We also present a hyperparameter selection algorithm to choose the parameters representing the matrix rank and degree of temporal smoothness of the corrections. In the absence of ground truth, we use maps and plots to argue that this method does indeed reduce heterogeneity. Reducing heterogeneity from 
    
[^26]: 无监督语言模型蒸馏的事实验证

    Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])

    [http://arxiv.org/abs/2309.16540](http://arxiv.org/abs/2309.16540)

    本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。

    

    无监督事实验证旨在通过可靠知识库中的证据来验证主张，而无需任何形式的数据注释。为了解决这个挑战，算法必须为每个主张生成既语义明确又紧凑的特征，以便与源信息进行语义对齐。与之前的工作不同，前者通过学习包含主张及其相应标签的注释语料库来解决对齐问题。我们提出了SFAVEL（通过语言模型蒸馏的自监督事实验证），这是一个新颖的无监督框架，利用预训练的语言模型将自监督特征蒸馏为高质量的主张-事实对齐，而无需注释。这是通过一种新颖的对比损失函数实现的，该函数鼓励特征在保持语料库间的语义关系的同时实现高质量的主张和证据对齐。值得注意的是，我们展示了达到新颖的状态一.

    Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
    
[^27]: 对嗜酸性粒细胞分割的不确定性量化

    Uncertainty Quantification for Eosinophil Segmentation. (arXiv:2309.16536v1 [eess.IV])

    [http://arxiv.org/abs/2309.16536](http://arxiv.org/abs/2309.16536)

    本研究提出了一种改进的方法，利用深度图像分割和蒙特卡洛辍学来量化嗜酸性粒细胞，为诊断嗜酸性食管炎提供不确定性评估和模型性能可视化。

    

    嗜酸性食管炎（EoE）是一种日益普及的过敏性疾病。为了诊断EoE，病理学家必须在一个高倍视场（400倍放大率）内找到15个或更多的嗜酸性粒细胞。确定一个患者是否患有EoE可以是一个艰巨的过程，并且用于辅助诊断的任何医学成像方法都必须考虑效率和精确性。我们提出了一种改进Adorno等人对使用深度图像分割进行嗜酸性粒细胞定量的方法。我们的新方法利用了蒙特卡洛辍学（Monte Carlo Dropout），这是深度学习中常用的一种减少过拟合的方法，用于对当前深度学习模型进行不确定性量化。不确定性可以在输出图像中可视化，以评估模型性能，理解深度学习算法的工作原理，并帮助病理学家识别嗜酸性粒细胞。

    Eosinophilic Esophagitis (EoE) is an allergic condition increasing in prevalence. To diagnose EoE, pathologists must find 15 or more eosinophils within a single high-power field (400X magnification). Determining whether or not a patient has EoE can be an arduous process and any medical imaging approaches used to assist diagnosis must consider both efficiency and precision. We propose an improvement of Adorno et al's approach for quantifying eosinphils using deep image segmentation. Our new approach leverages Monte Carlo Dropout, a common approach in deep learning to reduce overfitting, to provide uncertainty quantification on current deep learning models. The uncertainty can be visualized in an output image to evaluate model performance, provide insight to how deep learning algorithms function, and assist pathologists in identifying eosinophils.
    
[^28]: MotionLM: 多智能体运动预测作为语言建模

    MotionLM: Multi-Agent Motion Forecasting as Language Modeling. (arXiv:2309.16534v1 [cs.CV])

    [http://arxiv.org/abs/2309.16534](http://arxiv.org/abs/2309.16534)

    MotionLM模型是一个用于多智能体运动预测的语言建模方法，不需要锚点或显式潜在变量优化，能够生成关于交互智能体未来的联合分布和实现时间因果条件展开。

    

    在自动驾驶车辆的安全规划中，可靠地预测道路上智能体的未来行为是至关重要的。在这里，我们将连续轨迹表示为离散运动令牌的序列，并将多智能体运动预测视为对该领域的语言建模任务。我们的模型MotionLM提供了几个优势：首先，它不需要锚点或显式潜在变量优化来学习多模态分布。相反，我们利用单个标准的语言建模目标，最大化序列令牌的平均对数概率。其次，我们的方法绕过事后交互启发式，其中在交互评分之前进行单个代理轨迹生成。相反，MotionLM在单个自回归解码过程中生成关于交互代理未来的联合分布。此外，模型的时序因子化使其能够实现时间因果条件展开。所提出的方法建立了新的最先进技术。

    Reliable forecasting of the future behavior of road agents is a critical component to safe planning in autonomous vehicles. Here, we represent continuous trajectories as sequences of discrete motion tokens and cast multi-agent motion prediction as a language modeling task over this domain. Our model, MotionLM, provides several advantages: First, it does not require anchors or explicit latent variable optimization to learn multimodal distributions. Instead, we leverage a single standard language modeling objective, maximizing the average log probability over sequence tokens. Second, our approach bypasses post-hoc interaction heuristics where individual agent trajectory generation is conducted prior to interactive scoring. Instead, MotionLM produces joint distributions over interactive agent futures in a single autoregressive decoding process. In addition, the model's sequential factorization enables temporally causal conditional rollouts. The proposed approach establishes new state-of-t
    
[^29]: 使用深度条件生成时间序列模型生成个性化的胰岛素治疗策略

    Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models. (arXiv:2309.16521v1 [stat.ML])

    [http://arxiv.org/abs/2309.16521](http://arxiv.org/abs/2309.16521)

    本论文提出了一种使用深度生成时间序列模型和决策理论相结合的新框架，用于生成个性化的胰岛素治疗策略。通过学习生成逼真的个性化治疗和未来结果轨迹，可以为个性化患者历史匹配且针对最佳未来效果的新型多变量治疗策略。

    

    我们提出了一种新的框架，将深度生成时间序列模型与决策理论相结合，用于生成个性化的治疗策略。它利用历史患者轨迹数据，通过深度生成时间序列模型共同学习生成逼真的个性化治疗和未来结果轨迹。特别地，我们的框架可以根据条件化期望效用最大化训练生成与个性化患者历史匹配且针对最佳未来效果的新型多变量治疗策略。我们通过为住院糖尿病患者生成个性化的胰岛素治疗策略和血糖预测来展示我们的框架，展示了我们的方法在生成改进的个性化治疗策略方面的潜力。

    We propose a novel framework that combines deep generative time series models with decision theory for generating personalized treatment strategies. It leverages historical patient trajectory data to jointly learn the generation of realistic personalized treatment and future outcome trajectories through deep generative time series models. In particular, our framework enables the generation of novel multivariate treatment strategies tailored to the personalized patient history and trained for optimal expected future outcomes based on conditional expected utility maximization. We demonstrate our framework by generating personalized insulin treatment strategies and blood glucose predictions for hospitalized diabetes patients, showcasing the potential of our approach for generating improved personalized treatment strategies. Keywords: deep generative model, probabilistic decision support, personalized treatment generation, insulin and blood glucose prediction
    
[^30]: AtomSurf：蛋白质结构上的学习的表面表示

    AtomSurf : Surface Representation for Learning on Protein Structures. (arXiv:2309.16519v1 [cs.LG])

    [http://arxiv.org/abs/2309.16519](http://arxiv.org/abs/2309.16519)

    本文研究了将蛋白质作为3D网格的表面表示，并提出了一种结合图表面的协同方法，既有竞争优势，又有实际应用潜力。

    

    近期Cryo-EM和蛋白质结构预测算法的进展使得大规模蛋白质结构可获得，为基于机器学习的功能注释铺平了道路。几何深度学习领域关注创建适用于几何数据的方法。从蛋白质结构中学习的一个重要方面是将这些结构表示为几何对象（如网格、图或表面）并应用适合这种表示形式的学习方法。给定方法的性能将取决于表示和相应的学习方法。在本文中，我们研究将蛋白质表示为$\textit{3D mesh surfaces}$并将其纳入已建立的表示基准中。我们的第一个发现是，尽管有着有希望的初步结果，但仅单独表面表示似乎无法与3D网格竞争。在此基础上，我们提出了一种协同方法，将表面表示与图表面结合起来。

    Recent advancements in Cryo-EM and protein structure prediction algorithms have made large-scale protein structures accessible, paving the way for machine learning-based functional annotations.The field of geometric deep learning focuses on creating methods working on geometric data. An essential aspect of learning from protein structures is representing these structures as a geometric object (be it a grid, graph, or surface) and applying a learning method tailored to this representation. The performance of a given approach will then depend on both the representation and its corresponding learning method.  In this paper, we investigate representing proteins as $\textit{3D mesh surfaces}$ and incorporate them into an established representation benchmark. Our first finding is that despite promising preliminary results, the surface representation alone does not seem competitive with 3D grids. Building on this, we introduce a synergistic approach, combining surface representations with gra
    
[^31]: 从复杂到清晰：通过Clifford的几何代数和凸优化的分析表达深度神经网络的权重

    From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])

    [http://arxiv.org/abs/2309.16512](http://arxiv.org/abs/2309.16512)

    本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。

    

    本文介绍了一种基于几何（Clifford）代数和凸优化的神经网络分析方法。我们展示了当使用标准正则化损失进行训练时，深度ReLU神经网络的最优权重由训练样本的楔积给出。此外，训练问题可简化为对楔积特征进行凸优化，在其中编码训练数据集的几何结构。该结构以数据向量生成的三角形和平行体的有符号体积表示。凸问题通过$\ell_1$正则化找到样本的一个小子集，以发现仅相关的楔积特征。我们的分析提供了对深度神经网络内部工作机制的新视角，并揭示了隐藏层的作用。

    In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
    
[^32]: 深度单一模型与集成：加速部署停车监控系统的见解

    Deep Single Models vs. Ensembles: Insights for a Fast Deployment of Parking Monitoring Systems. (arXiv:2309.16495v1 [cs.CV])

    [http://arxiv.org/abs/2309.16495](http://arxiv.org/abs/2309.16495)

    该研究揭示了在使用公开可用的标记停车场图像进行训练的全局框架的创建中的挑战，该框架能够在各种场景下准确执行停车空间监控任务，加速部署停车监控系统。

    

    在高密度城市中寻找可用停车位对驾车者来说是一项压力巨大的任务，可以通过提前了解最近可用的停车空间的系统来缓解。为此，基于图像的系统在安装和维护上相对其他传感器的选择（如超声波传感器）具有成本优势，需要更少的物理基础设施。尽管近年来深度学习取得了进展，但部署智能停车监控仍然是一个挑战，因为大多数方法涉及收集和标记大量数据，这是费时费力的。我们的研究旨在揭示在使用公开可用的标记停车场图像进行训练的全局框架的创建中所面临的挑战，该框架在各种场景下能够准确执行，从而实现停车空间监控作为一种可以立即投入新环境使用的系统。通过涉及不同数据集和深度学习体系结构的详尽实验，包括融合策略。

    Searching for available parking spots in high-density urban centers is a stressful task for drivers that can be mitigated by systems that know in advance the nearest parking space available.  To this end, image-based systems offer cost advantages over other sensor-based alternatives (e.g., ultrasonic sensors), requiring less physical infrastructure for installation and maintenance.  Despite recent deep learning advances, deploying intelligent parking monitoring is still a challenge since most approaches involve collecting and labeling large amounts of data, which is laborious and time-consuming. Our study aims to uncover the challenges in creating a global framework, trained using publicly available labeled parking lot images, that performs accurately across diverse scenarios, enabling the parking space monitoring as a ready-to-use system to deploy in a new environment. Through exhaustive experiments involving different datasets and deep learning architectures, including fusion strateg
    
[^33]: 对中毒公平表示的研究

    Towards Poisoning Fair Representations. (arXiv:2309.16487v1 [cs.LG])

    [http://arxiv.org/abs/2309.16487](http://arxiv.org/abs/2309.16487)

    这项研究提出了第一个针对公平表示学习（FRL）的数据中毒框架，该框架在对抗性场景下评估模型的健壮性，解决了FRL方法在面对数据中毒攻击时的脆弱性。

    

    公平机器学习旨在减少对某些人口子群体（如老年人和女性）的模型预测偏见。最近，通过深度神经网络训练的公平表示学习（FRL）表现出优越的性能，从数据中推断出不包含人口统计信息的表示，然后将其用作分类或其他下游任务的输入。尽管FRL方法得到了发展，但它们在面对数据中毒攻击时的脆弱性，即在对抗性场景下评估模型的健壮性的流行协议中，尚未得到充分研究。数据中毒攻击已经针对将公平性约束纳入浅层模型分类器的经典公平机器学习方法进行了开发。然而，由于公平目标和模型架构有明显的差异，这些攻击在FRL中不够有效。本文提出了第一个针对FRL的数据中毒框架。我们诱使模型输出包含尽可能多公平表示的不公平表示。

    Fair machine learning seeks to mitigate model prediction bias against certain demographic subgroups such as elder and female. Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic information are inferred from the data and then used as the input to classification or other downstream tasks. Despite the development of FRL methods, their vulnerability under data poisoning attack, a popular protocol to benchmark model robustness under adversarial scenarios, is under-explored. Data poisoning attacks have been developed for classical fair machine learning methods which incorporate fairness constraints into shallow-model classifiers. Nonetheless, these attacks fall short in FRL due to notably different fairness goals and model architectures. This work proposes the first data poisoning framework attacking FRL. We induce the model to output unfair representations that contain as much 
    
[^34]: 高维度下重尾数据下的鲁棒回归: 渐近性和普适性

    High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality. (arXiv:2309.16476v1 [math.ST])

    [http://arxiv.org/abs/2309.16476](http://arxiv.org/abs/2309.16476)

    本文研究了在高维度和重尾干扰条件下的鲁棒回归估计器的性质，通过研究一类椭圆协变量和噪声数据分布的M-估计器，我们发现在存在重尾噪声的情况下，Huber损失需要进一步正则化才能达到最优性能。同时，我们还推导出了岭回归的超额风险的衰减速率。

    

    我们研究了在协变量和响应函数都受重尾干扰的情况下，鲁棒回归估计量的高维性质。特别地，我们针对一类包含椭圆协变量和噪声数据分布的M-估计器提供了锐利的渐近特征化，包括二阶及以上矩不存在的情况。我们发现，在存在重尾噪声的高维情况下，尽管Huber损失通过最优调整的位置参数$\delta$是一致的，但其在性能上是次优的，突显了进一步正则化以达到最优性能的必要性。这个结果还揭示了$\delta$作为样本复杂度和污染的函数存在的一个有趣的转变。此外，我们还推导出岭回归中超额风险的衰减速率。我们发现，对于具有有限二阶矩的噪声分布，岭回归不仅是最优的，而且是普适的，但其衰减速率可以是...

    We investigate the high-dimensional properties of robust regression estimators in the presence of heavy-tailed contamination of both the covariates and response functions. In particular, we provide a sharp asymptotic characterisation of M-estimators trained on a family of elliptical covariate and noise data distributions including cases where second and higher moments do not exist. We show that, despite being consistent, the Huber loss with optimally tuned location parameter $\delta$ is suboptimal in the high-dimensional regime in the presence of heavy-tailed noise, highlighting the necessity of further regularisation to achieve optimal performance. This result also uncovers the existence of a curious transition in $\delta$ as a function of the sample complexity and contamination. Moreover, we derive the decay rates for the excess risk of ridge regression. We show that, while it is both optimal and universal for noise distributions with finite second moment, its decay rate can be consi
    
[^35]: 系统化泛化的组合式程序生成

    Compositional Program Generation for Systematic Generalization. (arXiv:2309.16467v1 [cs.LG])

    [http://arxiv.org/abs/2309.16467](http://arxiv.org/abs/2309.16467)

    系统化泛化是人类的关键技能之一，组合式程序生成器（CPG）通过模块化、类型抽象和递归组合的特征，能够以少样本的方式对新概念进行系统化的泛化，在各种语言任务上具有生产力。

    

    组合式泛化是人类的关键技能之一，它使我们能够从少数例子中学习新概念。机器学习模型，包括如今无处不在的transformers，在这方面很难进行泛化，并且通常需要在训练过程中提供数千个概念示例才能进行有意义的泛化。人类与人工神经网络结构在能力上的差异，促使了对一种称为组合式程序生成器（CPG）的神经符号体系结构进行研究。CPG具有三个关键特征：模块化、类型抽象和递归组合，它使其能够以少样本的方式对新概念进行系统化的泛化，同时在各种序列到序列的语言任务上具有生产力。对于每个输入，CPG使用输入领域的语法和解析器生成一个类型层次结构，在这个结构中，每个语法规则都被分配了一个独特的语义模块——一个概率性的复制或替换程序。

    Compositional generalization is a key ability of humans that enables us to learn new concepts from only a handful examples. Machine learning models, including the now ubiquitous transformers, struggle to generalize in this way, and typically require thousands of examples of a concept during training in order to generalize meaningfully. This difference in ability between humans and artificial neural architectures, motivates this study on a neuro-symbolic architecture called the Compositional Program Generator (CPG). CPG has three key features: modularity, type abstraction, and recursive composition, that enable it to generalize both systematically to new concepts in a few-shot manner, as well as productively by length on various sequence-to-sequence language tasks. For each input, CPG uses a grammar of the input domain and a parser to generate a type hierarchy in which each grammar rule is assigned its own unique semantic module, a probabilistic copy or substitution program. Instances w
    
[^36]: 一种在高维参数空间中进行摊销搜索的元启发式算法

    A Metaheuristic for Amortized Search in High-Dimensional Parameter Spaces. (arXiv:2309.16465v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.16465](http://arxiv.org/abs/2309.16465)

    我们提出了一种新的元启发式算法，通过基于特征的变换实现高维参数空间中的无梯度参数搜索，从而解决参数推断中的挑战性问题。

    

    对（生物）物理系统的动力学模型进行参数推断仍然是一个具有挑战性的问题。难以处理的梯度、高维空间和非线性模型函数通常在没有大量计算预算的情况下成为问题。最近在这个领域的一系列工作集中在贝叶斯推断方法上，这些方法考虑参数在其统计分布下，并且因此不得出最优参数值的点估计。在这里，我们提出了一种新的元启发式算法，通过基于特征的变换（DR-FFIT）来降低维度，以解决这些瓶颈问题。DR-FFIT实现了一种有效的抽样策略，可以在高维空间中进行无梯度的参数搜索。我们使用人工神经网络来获取模型感兴趣特征的可微代理。由此产生的梯度使得在定义的抽样区域内估计模型的本地活跃子空间成为可能。这种方法实现了高效的维度约简。

    Parameter inference for dynamical models of (bio)physical systems remains a challenging problem. Intractable gradients, high-dimensional spaces, and non-linear model functions are typically problematic without large computational budgets. A recent body of work in that area has focused on Bayesian inference methods, which consider parameters under their statistical distributions and therefore, do not derive point estimates of optimal parameter values. Here we propose a new metaheuristic that drives dimensionality reductions from feature-informed transformations (DR-FFIT) to address these bottlenecks. DR-FFIT implements an efficient sampling strategy that facilitates a gradient-free parameter search in high-dimensional spaces. We use artificial neural networks to obtain differentiable proxies for the model's features of interest. The resulting gradients enable the estimation of a local active subspace of the model within a defined sampling region. This approach enables efficient dimensio
    
[^37]: 使用知识增强LLM：关于幻觉预防的调研

    Augmenting LLMs with Knowledge: A survey on hallucination prevention. (arXiv:2309.16459v1 [cs.CL])

    [http://arxiv.org/abs/2309.16459](http://arxiv.org/abs/2309.16459)

    本调研论文讨论了将预训练语言模型与外部知识源集成的方法，以解决模型对知识的访问和操作能力的限制问题。

    

    大规模预训练语言模型在存储事实知识和在下游自然语言处理任务中取得显著结果方面表现出了熟练程度。然而，它们对于准确访问和操作知识的能力仍然受限，导致在知识密集型任务上的表现与特定任务的架构相比存在差距。此外，提供模型决策的来源和保持最新世界知识的挑战仍然是开放的研究前沿。为了解决这些限制，将预训练模型与可微分访问机制集成到显式的非参数记忆中成为一种有希望的解决方案。本调研探讨了增强了与外部知识来源（包括外部知识库和搜索引擎）相连接能力的语言模型（LMs）。

    Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missin
    
[^38]: 通用睡眠解码器：将觉醒和睡眠神经表示对齐于不同个体间

    Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects. (arXiv:2309.16457v1 [cs.LG])

    [http://arxiv.org/abs/2309.16457](http://arxiv.org/abs/2309.16457)

    该论文设计了一项新颖的实验并收集了52名参与者的全面脑电图数据集，从而解决了觉醒和睡眠状态下神经表示的差异问题。研究团队开发了通用睡眠解码器（USD），可以在不同个体间对齐觉醒和睡眠的神经模式，并取得了与使用个别睡眠数据进行解码相当的准确率。研究还发现，在测试个体上对USD进行微调可以进一步提高解码准确性。

    

    通过脑活动解码睡眠中的记忆内容长期以来一直是神经科学的目标。虽然已知啮齿类动物在睡眠中自发地重新激活记忆以支持记忆巩固和离线学习，但由于缺乏经过完整注释的睡眠数据集以及清醒状态和睡眠状态之间神经模式的巨大差异，捕捉人类的记忆再现是具有挑战性的。为了解决这些挑战，我们设计了一项新颖的认知神经科学实验，并从52名参与者收集了一份全面、完整注释的脑电图（EEG）数据集，涵盖了觉醒和睡眠两种状态。利用这个基准数据集，我们开发了通用睡眠解码器（USD），用于在不同个体间对齐觉醒与睡眠的神经表示。我们的模型在未见过的个体上实现了高达16.6%的top-1零样本准确率，与使用个别睡眠数据进行解码的性能相当。此外，对测试个体的USD进行微调可以提高解码准确性。

    Decoding memory content from brain activity during sleep has long been a goal in neuroscience. While spontaneous reactivation of memories during sleep in rodents is known to support memory consolidation and offline learning, capturing memory replay in humans is challenging due to the absence of well-annotated sleep datasets and the substantial differences in neural patterns between wakefulness and sleep. To address these challenges, we designed a novel cognitive neuroscience experiment and collected a comprehensive, well-annotated electroencephalography (EEG) dataset from 52 subjects during both wakefulness and sleep. Leveraging this benchmark dataset, we developed the Universal Sleep Decoder (USD) to align neural representations between wakefulness and sleep across subjects. Our model achieves up to 16.6% top-1 zero-shot accuracy on unseen subjects, comparable to decoding performances using individual sleep data. Furthermore, fine-tuning USD on test subjects enhances decoding accuracy
    
[^39]: 抵抗联邦学习中后门攻击的双向选举和个体视角方法

    Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective. (arXiv:2309.16456v1 [cs.LG])

    [http://arxiv.org/abs/2309.16456](http://arxiv.org/abs/2309.16456)

    本文提出了Snowball，一个通过个体视角上的双向选举方法来抵抗联邦学习中后门攻击的框架。它通过自下而上和自上而下的选举过程，逐步排除感染模型，以解决由于本地数据分布多样性导致模型更新混杂分散的问题。

    

    现有的抵御联邦学习中后门攻击的方法主要通过减轻感染模型的影响或排除感染模型来实现。前者会对模型准确性产生负面影响，而后者通常依赖于对良性和感染模型更新之间的全局清晰边界的判定。然而，由于本地数据分布的多样性，模型更新在现实中容易混杂并分散。本文关注在联邦学习中排除感染模型的问题。与以往从全局视角出发的观点不同，我们提出了Snowball，一种新颖的反后门联邦学习框架，通过个体视角上的双向选举，受到我们推导出的一个原则和联邦学习和深度学习中的两个原则的启发。它具有以下特点：a）自下而上的选举，每个候选模型更新对多个对等候选模型更新进行投票，以选出一些模型更新作为聚合的被选项；b）自上而下的选举，被选项逐步增加。

    Existing approaches defend against backdoor attacks in federated learning (FL) mainly through a) mitigating the impact of infected models, or b) excluding infected models. The former negatively impacts model accuracy, while the latter usually relies on globally clear boundaries between benign and infected model updates. However, model updates are easy to be mixed and scattered throughout in reality due to the diverse distributions of local data. This work focuses on excluding infected models in FL. Unlike previous perspectives from a global view, we propose Snowball, a novel anti-backdoor FL framework through bidirectional elections from an individual perspective inspired by one principle deduced by us and two principles in FL and deep learning. It is characterized by a) bottom-up election, where each candidate model update votes to several peer ones such that a few model updates are elected as selectees for aggregation; and b) top-down election, where selectees progressively enlarge t
    
[^40]: 关于对抗鲁棒性和可操作解释之间的权衡

    On the Trade-offs between Adversarial Robustness and Actionable Explanations. (arXiv:2309.16452v1 [cs.LG])

    [http://arxiv.org/abs/2309.16452](http://arxiv.org/abs/2309.16452)

    本论文研究了对抗鲁棒模型对可操作解释的影响，并通过理论和实证分析比较了对抗性鲁棒和非鲁棒模型生成的追索结果的成本和有效性之间的差异。

    

    随着机器学习模型在各种高风险环境中的应用越来越广泛，确保这些模型的预测不仅具有对抗性鲁棒性，而且还能向相关利益相关者提供可解释性变得越来越重要。然而，目前尚不清楚是否可以同时实现这两个概念，或者它们之间是否存在权衡。在这项工作中，我们首次尝试研究对抗性鲁棒模型对可操作解释的影响，这些解释为最终用户提供了追索权利。我们在理论和实证层面上分析了先进算法生成的追索结果的成本（实施的容易程度）和有效性（获得正向模型预测的概率），并比较了对抗性鲁棒和非鲁棒模型生成的追索结果之间的差异。具体而言，我们推导出对抗性鲁棒和非鲁棒模型生成的追索结果的成本和有效性之间的理论界限。

    As machine learning models are increasingly being employed in various high-stakes settings, it becomes important to ensure that predictions of these models are not only adversarially robust, but also readily explainable to relevant stakeholders. However, it is unclear if these two notions can be simultaneously achieved or if there exist trade-offs between them. In this work, we make one of the first attempts at studying the impact of adversarially robust models on actionable explanations which provide end users with a means for recourse. We theoretically and empirically analyze the cost (ease of implementation) and validity (probability of obtaining a positive model prediction) of recourses output by state-of-the-art algorithms when the underlying models are adversarially robust vs. non-robust. More specifically, we derive theoretical bounds on the differences between the cost and the validity of the recourses generated by state-of-the-art algorithms for adversarially robust vs. non-ro
    
[^41]: 一种简约、计算高效的空间回归机器学习方法

    A parsimonious, computationally efficient machine learning method for spatial regression. (arXiv:2309.16448v1 [stat.ML])

    [http://arxiv.org/abs/2309.16448](http://arxiv.org/abs/2309.16448)

    MPRS是一种计算高效、具有物理启发的空间回归机器学习方法，通过使用距离相关的“相互作用”来引入空间或时间相关性，能够处理任意空间维度的分散数据。在各种合成和真实世界的测试中，MPRS展现了与标准插值方法相当的预测性能，特别适用于填补粗糙和非高斯数据的缺口。

    

    我们介绍了改进的平面旋转器方法（MPRS），这是一种受物理启发的用于空间/时间回归的机器学习方法。MPRS是一个非参数模型，通过短程、距离相关的“相互作用”来引入空间或时间相关性，而不需要假设底层概率分布的特定形式。预测是通过完全自主的学习算法进行的，该算法使用平衡条件的蒙特卡罗模拟。MPRS能够处理分散的数据和任意的空间维度。我们在一维、二维和三维上对各种合成和真实世界的数据进行了测试，测试结果表明MPRS的预测性能（无需参数调整）与普通克里金法和逆距离加权法等标准插值方法相当。特别是，MPRS是一种特别有效的填补缺口方法，适用于粗糙和非高斯数据（如每日降水时间序列）。

    We introduce the modified planar rotator method (MPRS), a physically inspired machine learning method for spatial/temporal regression. MPRS is a non-parametric model which incorporates spatial or temporal correlations via short-range, distance-dependent ``interactions'' without assuming a specific form for the underlying probability distribution. Predictions are obtained by means of a fully autonomous learning algorithm which employs equilibrium conditional Monte Carlo simulations. MPRS is able to handle scattered data and arbitrary spatial dimensions. We report tests on various synthetic and real-word data in one, two and three dimensions which demonstrate that the MPRS prediction performance (without parameter tuning) is competitive with standard interpolation methods such as ordinary kriging and inverse distance weighting. In particular, MPRS is a particularly effective gap-filling method for rough and non-Gaussian data (e.g., daily precipitation time series). MPRS shows superior co
    
[^42]: 通过文本到视频模型自适应实现多样且对齐的音频到视频生成

    Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation. (arXiv:2309.16429v1 [cs.LG])

    [http://arxiv.org/abs/2309.16429](http://arxiv.org/abs/2309.16429)

    通过轻量级适配器网络将音频的表示映射到文本到视频生成模型所期望的输入表示，实现了在全局和时间上与输入音频对齐的多样且逼真的视频生成。

    

    我们考虑使用来自各种语义类别的自然音频样本来引导生成多样且逼真的视频的任务。对于这个任务，视频需要在全局和时间上与输入音频进行对齐：全局上，输入音频与整个输出视频有语义关联；时间上，输入音频的每个片段都与相应的视频片段关联。我们利用现有的文本驱动视频生成模型和预训练的音频编码器模型。所提出的方法基于一个轻量级的适配器网络，该网络学习将基于音频的表示映射到文本到视频生成模型所期望的输入表示。因此，它也可以实现基于文本、音频以及文本和音频的视频生成，据我们所知，这是首次实现。我们在三个数据集上对我们的方法进行了大量验证，展示了音频-视频样本的显著语义多样性，并进一步验证了我们的方法的性能。

    We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further
    
[^43]: 非线性MPC设计应用于增量ISS系统，以GRU网络为例

    Nonlinear MPC design for incrementally ISS systems with application to GRU networks. (arXiv:2309.16428v1 [eess.SY])

    [http://arxiv.org/abs/2309.16428](http://arxiv.org/abs/2309.16428)

    本研究提出了一种非线性模型预测控制（NMPC）策略，适用于增量ISS系统。通过简化计算终端成分，并明确定义最小预测范围，实现闭环稳定性。将该方法应用于GRU网络，并提供了一种量身定制状态观察器的设计方法。测试结果表明该控制架构具有良好的控制性能和高效的实用性。

    

    本研究针对指数增量输入-状态稳定（ISS）系统设计了一种非线性模型预测控制（NMPC）策略。具体而言，提出了一种新的公式，无需计算终端成分，而是依赖于明确定义的最小预测范围以确保闭环稳定性。该设计方法特别适用于由循环神经网络（RNNs）学习的系统，RNNs以其增强建模能力而闻名，而增量ISS属性可以通过简单的代数条件进行研究。该方法应用于门控循环单元（GRU）网络，并提供了一种具有收敛性保证的量身定制状态观察器的设计方法。所得到的控制架构在基准系统上进行了测试，证明了其良好的控制性能和高效的实用性。

    This brief addresses the design of a Nonlinear Model Predictive Control (NMPC) strategy for exponentially incremental Input-to-State Stable (ISS) systems. In particular, a novel formulation is devised, which does not necessitate the onerous computation of terminal ingredients, but rather relies on the explicit definition of a minimum prediction horizon ensuring closed-loop stability. The designed methodology is particularly suited for the control of systems learned by Recurrent Neural Networks (RNNs), which are known for their enhanced modeling capabilities and for which the incremental ISS properties can be studied thanks to simple algebraic conditions. The approach is applied to Gated Recurrent Unit (GRU) networks, providing also a method for the design of a tailored state observer with convergence guarantees. The resulting control architecture is tested on a benchmark system, demonstrating its good control performances and efficient applicability.
    
[^44]: AutoCLIP: 自动调谐视觉语言模型的零样本分类器

    AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])

    [http://arxiv.org/abs/2309.16414](http://arxiv.org/abs/2309.16414)

    本研究提出了一种名为AutoCLIP的方法，用于自动调谐视觉语言模型的零样本分类器。AutoCLIP通过为每个提示模板分配图像特定的权重，从而改进了从编码类别描述符推导零样本分类器的方式。

    

    基于视觉语言模型（如CLIP）构建的分类器在广泛的图像分类任务中展现了出色的零样本性能。先前的工作研究了根据提示模板自动创建每个类别的描述符集的不同方式，包括手工设计的模板、从大型语言模型获取的模板以及从随机单词和字符构建的模板。然而，从相应的编码类别描述符导出零样本分类器几乎没有改变：将图像的平均编码类别描述符与编码图像之间的余弦相似度最大化以进行分类。然而，当某些描述符比其他描述符更好地匹配给定图像上的视觉线索时，将所有类别描述符等权重可能不是最优的。在这项工作中，我们提出了一种自动调谐零样本分类器的方法AutoCLIP。AutoCLIP为每个提示模板分配了图像特定的权重，这些权重是从s

    Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. In contrast, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, that is: classify to the class that maximizes the cosine similarity between its averaged encoded class descriptors and the encoded image. However, weighting all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP assigns to each prompt template per-image weights, which are derived from s
    
[^45]: 通过检验进行选择性非参数回归

    Selective Nonparametric Regression via Testing. (arXiv:2309.16412v1 [stat.ML])

    [http://arxiv.org/abs/2309.16412](http://arxiv.org/abs/2309.16412)

    通过检验给定条件方差的假设，我们开发了一种选择性非参数回归方法，允许考虑方差本身的值以及对应方差预测器的不确定性，并证明了估计器的风险的非渐近界。

    

    在具有误差敏感的机器学习应用中，预测中的放弃可能性（或选择性预测）是一个重要问题。虽然分类设置中得到了广泛研究，但对于回归问题的选择性方法发展较少。在这项工作中，我们考虑非参数异方差回归问题，并通过检验给定点处条件方差的假设来开发一个放弃程序。与现有方法不同，提出的方法不仅允许考虑方差本身的值，还允许考虑对应方差预测器的不确定性。我们证明了所得估计器的风险的非渐近界，并展示了几种不同收敛模式的存在。理论分析与一系列在模拟和真实数据上的实验一起进行。

    Prediction with the possibility of abstention (or selective prediction) is an important problem for error-critical machine learning applications. While well-studied in the classification setup, selective approaches to regression are much less developed. In this work, we consider the nonparametric heteroskedastic regression problem and develop an abstention procedure via testing the hypothesis on the value of the conditional variance at a given point. Unlike existing methods, the proposed one allows to account not only for the value of the variance itself but also for the uncertainty of the corresponding variance predictor. We prove non-asymptotic bounds on the risk of the resulting estimator and show the existence of several different convergence regimes. Theoretical analysis is illustrated with a series of experiments on simulated and real-world data.
    
[^46]: 在没有均值互换性假设的情况下构建合成治疗组

    Constructing Synthetic Treatment Groups without the Mean Exchangeability Assumption. (arXiv:2309.16409v1 [stat.ML])

    [http://arxiv.org/abs/2309.16409](http://arxiv.org/abs/2309.16409)

    本文提出了一种在没有均值互换性假设的情况下构建合成治疗组的方法，通过对源群体的治疗组加权混合来构建目标人群的合成治疗组，并通过最小化条件最大均值差异来估计权重。该方法在均值互换性假设被违反时可以作为一种新颖的补充方法。

    

    本文的目的是将多个随机对照试验的信息传递给我们仅有控制组数据的目标人群。以前的工作在很大程度上依赖于均值互换性的假设。然而，正如许多现有研究所指出的，均值互换性假设可能被违反。受合成控制方法的启发，我们通过对源群体的治疗组加权混合构建了目标人群的合成治疗组。我们通过最小化源群体的加权对照组与目标人群之间的条件最大均值差异来估计权重。我们基于筛选半参数理论建立了合成治疗组估计量的渐近正态性。当均值互换性假设被违反时，我们的方法可以作为一种新颖的补充方法。在合成和现实世界数据集上进行实验证明了我们方法的有效性。

    The purpose of this work is to transport the information from multiple randomized controlled trials to the target population where we only have the control group data. Previous works rely critically on the mean exchangeability assumption. However, as pointed out by many current studies, the mean exchangeability assumption might be violated. Motivated by the synthetic control method, we construct a synthetic treatment group for the target population by a weighted mixture of treatment groups of source populations. We estimate the weights by minimizing the conditional maximum mean discrepancy between the weighted control groups of source populations and the target population. We establish the asymptotic normality of the synthetic treatment group estimator based on the sieve semiparametric theory. Our method can serve as a novel complementary approach when the mean exchangeability assumption is violated. Experiments are conducted on synthetic and real-world datasets to demonstrate the effe
    
[^47]: 基于VAE的RNO-G数据潜空间分类

    VAE-based latent-space classification of RNO-G data. (arXiv:2309.16401v1 [astro-ph.HE])

    [http://arxiv.org/abs/2309.16401](http://arxiv.org/abs/2309.16401)

    本研究提出了一种利用变分自编码器(VAE)的潜空间进行RNO-G数据分类的方法，在有噪声和静默观测站的情况下可以自动检测和区分多个噪声类别，并定量分析其中的物理信号。

    

    格陵兰岛的射电中微子观测站(RNO-G)是一种用于探测超高能中微子的射电天线阵列，位于格陵兰峰站。目前仍在建设中，目前已有7个观测站运作。中微子探测通过测量由中微子与核子相互作用产生的阿斯卡林辐射来实现。中微子候选事件必须从其他背景中找到，这些背景的记录率要高得多，包括宇宙射线和人为噪声等，有时其来源是未知的。这里，我们描述了一种利用变分自编码器的潜空间来对不同噪声类别进行分类的方法。潜空间形成了一种紧凑的表示，使得分类变得可行。我们分析了一个有噪声和一个静默观测站的数据。该方法能够自动检测并允许我们定性地区分多个事件类别，包括风引起的物理信号，同时适用于有噪声和静默观测站。

    The Radio Neutrino Observatory in Greenland (RNO-G) is a radio-based ultra-high energy neutrino detector located at Summit Station, Greenland. It is still being constructed, with 7 stations currently operational. Neutrino detection works by measuring Askaryan radiation produced by neutrino-nucleon interactions. A neutrino candidate must be found amidst other backgrounds which are recorded at much higher rates -- including cosmic-rays and anthropogenic noise -- the origins of which are sometimes unknown. Here we describe a method to classify different noise classes using the latent space of a variational autoencoder. The latent space forms a compact representation that makes classification tractable. We analyze data from a noisy and a silent station. The method automatically detects and allows us to qualitatively separate multiple event classes, including physical wind-induced signals, for both the noisy and the quiet station.
    
[^48]: 集中式深度学习中差分隐私的最新进展：系统性概述

    Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey. (arXiv:2309.16398v1 [cs.LG])

    [http://arxiv.org/abs/2309.16398](http://arxiv.org/abs/2309.16398)

    本调查概述了差分隐私在集中式深度学习中的最新进展，包括审计和评估方法、隐私效用权衡的改进、对各种威胁和攻击的保护、差分隐私生成模型以及新兴应用领域。

    

    差分隐私已成为机器学习中广受欢迎的数据保护方法，特别是因为它允许制定严格的数学隐私保证。本调查概述了差分隐私集中式深度学习的现状，对最新进展和开放问题进行了全面分析，并讨论了领域潜在的未来发展。根据系统性文献综述所示，讨论了以下主题：私有模型的审计和评估方法，隐私效用权衡的改进，对广泛威胁和攻击的保护，差分隐私生成模型以及新兴应用领域。

    Differential Privacy has become a widely popular method for data protection in machine learning, especially since it allows formulating strict mathematical privacy guarantees. This survey provides an overview of the state-of-the-art of differentially private centralized deep learning, thorough analyses of recent advances and open problems, as well as a discussion of potential future developments in the field. Based on a systematic literature review, the following topics are addressed: auditing and evaluation methods for private models, improvements of privacy-utility trade-offs, protection against a broad range of threats and attacks, differentially private generative models, and emerging application domains.
    
[^49]: 针对随机驾驶环境的不确定性感知决策Transformer

    Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v1 [cs.LG])

    [http://arxiv.org/abs/2309.16397](http://arxiv.org/abs/2309.16397)

    本论文提出了一种针对随机驾驶环境的不确定性感知决策Transformer（UNREST），通过估计状态的不确定性并相应地分割序列，取代了全局回报。这项工作通过解决在不确定性环境中过于乐观的问题，为离线强化学习在自主驾驶任务中的应用提供了一种有效的解决方案。

    

    离线强化学习（RL）已经成为一种无需主动交互的学习策略的有希望框架，因此在自主驾驶任务中尤其吸引人。最近Transformers的成功启发了将离线RL视为序列建模，这在长期任务中表现出色。然而，在具有不确定性的环境中，它们过于乐观，错误地假设相同的目标可以通过相同的动作一致实现。在本文中，我们引入了一种针对随机驾驶环境的不确定性感知决策Transformer（UNREST），不引入额外的转换模型或复杂的生成模型来进行规划。具体而言，UNREST通过转换与回报之间的条件互信息来估计状态的不确定性，并相应地分割序列。通过发现驾驶环境的“不确定性累积”和“时间局部性”特性，UNREST将决策Transformer中的全局回报替换为较少的部分回报。

    Offline Reinforcement Learning (RL) has emerged as a promising framework for learning policies without active interactions, making it especially appealing for autonomous driving tasks. Recent successes of Transformers inspire casting offline RL as sequence modeling, which performs well in long-horizon tasks. However, they are overly optimistic in stochastic environments with incorrect assumptions that the same goal can be consistently achieved by identical actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in stochastic driving environments without introducing additional transition or complex generative models. Specifically, UNREST estimates state uncertainties by the conditional mutual information between transitions and returns, and segments sequences accordingly. Discovering the `uncertainty accumulation' and `temporal locality' properties of driving environments, UNREST replaces the global returns in decision transformers with less 
    
[^50]: 通过Sobolev训练的二维Copula逼近变换：2-Cats网络

    Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])

    [http://arxiv.org/abs/2309.16391](http://arxiv.org/abs/2309.16391)

    本文介绍了一种通过Sobolev训练的2-Cats网络，它能够非参数地逼近任何二维Copula，并且在估计输出方面优于现有技术。

    

    Copula是一种强大的统计工具，用于捕捉数据维度之间的依赖关系。在应用Copula时，我们可以通过首先估计独立的边际分布（一个简单任务），然后估计连接边际的单个Copula函数C（一个困难任务）来估计多元分布函数。对于二维数据，Copula是一个形如C：(u，v)∈\mathbf{I}^2\rightarrow \mathbf{I}的二次增函数，其中\mathbf{I}=[0，1]。在本文中，我们展示了神经网络（NNs）如何能够非参数地逼近任何二维Copula。我们的方法被称为2-Cats，受到物理启发的神经网络和Sobolev训练文献的启发。我们不仅证明了我们能够比现有技术更好地估计2D Copula的输出，而且我们的方法是非参数的，并且符合Copula C的数学性质。

    Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
    
[^51]: 多交换$k$-Means++算法

    Multi-Swap $k$-Means++. (arXiv:2309.16384v1 [cs.CG])

    [http://arxiv.org/abs/2309.16384](http://arxiv.org/abs/2309.16384)

    本论文通过多交换$k$-Means++算法的改进和扩展，提出了一种在$k$-means聚类问题中能够获得$9 + \varepsilon$近似比的局部搜索算法，并证明了该算法在实际中取得了显著的质量改进。

    

    Arthur和Vassilvitskii提出的$k$-means++算法通常被实践者选择用于优化流行的$k$-means聚类目标，并在期望中获得$O(\log k)$的近似度。为了获得更高质量的解，Lattanzi和Sohler提出了通过$k$-means++采样分布获得的$O(k \log \log k)$个局部搜索步骤的增强$k$-means++算法，从而得到$k$-means聚类问题的$c$近似解，其中$c$是一个较大的常数。在这里，我们通过考虑更大更复杂的局部搜索邻域来推广和扩展他们的局部搜索算法，从而可以同时交换多个中心。我们的算法实现了$9 + \varepsilon$的近似比，这是局部搜索可能的最佳结果。重要的是，我们证明了我们的方法在实际中取得了实质性的改进，我们显示出与Lattanzi和Sohler的方法相比的显著质量改进。

    The $k$-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often the practitioners' choice algorithm for optimizing the popular $k$-means clustering objective and is known to give an $O(\log k)$-approximation in expectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML 2019) proposed augmenting $k$-means++ with $O(k \log \log k)$ local search steps obtained through the $k$-means++ sampling distribution to yield a $c$-approximation to the $k$-means clustering problem, where $c$ is a large absolute constant. Here we generalize and extend their local search algorithm by considering larger and more sophisticated local search neighborhoods hence allowing to swap multiple centers at the same time. Our algorithm achieves a $9 + \varepsilon$ approximation ratio, which is the best possible for local search. Importantly we show that our approach yields substantial practical improvements, we show significant quality improvements over the approach of Lattanzi and Sohler 
    
[^52]: RLLTE：强化学习的长期演进项目

    RLLTE: Long-Term Evolution Project of Reinforcement Learning. (arXiv:2309.16382v1 [cs.AI])

    [http://arxiv.org/abs/2309.16382](http://arxiv.org/abs/2309.16382)

    RLLTE是一种长期演进、极度模块化和开源的强化学习框架，提供了完整的生态系统，预计将为RL工程实践设定标准并刺激产业和学术界。

    

    我们提出了RLLTE：一种长期演进、极度模块化和开源的强化学习（RL）研究与应用框架。除了提供一流的算法实现之外，RLLTE还作为一个算法开发工具包。具体来说，RLLTE完全解耦了RL算法与开发算法的实践角度，提供了大量组件来加速算法的发展和演进。特别地，RLLTE是第一个构建了完整丰富的生态系统的RL框架，其中包括模型训练、评估、部署、基准测试中心和大语言模型（LLM）增强的副驾驶。预期RLLTE将为RL工程实践设定标准，并对产业和学术界具有高度刺激作用。

    We present RLLTE: a long-term evolution, extremely modular, and open-source framework for reinforcement learning (RL) research and application. Beyond delivering top-notch algorithm implementations, RLLTE also serves as a toolkit for developing algorithms. More specifically, RLLTE decouples the RL algorithms completely from the exploitation-exploration perspective, providing a large number of components to accelerate algorithm development and evolution. In particular, RLLTE is the first RL framework to build a complete and luxuriant ecosystem, which includes model training, evaluation, deployment, benchmark hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set standards for RL engineering practice and be highly stimulative for industry and academia.
    
[^53]: MHG-GNN: 分子超图语法与图神经网络的结合

    MHG-GNN: Combination of Molecular Hypergraph Grammar with Graph Neural Network. (arXiv:2309.16374v1 [cs.LG])

    [http://arxiv.org/abs/2309.16374](http://arxiv.org/abs/2309.16374)

    MHG-GNN是一种将分子超图语法与图神经网络相结合的新型自编码器，在多样材料的物性预测任务中表现出巨大的潜力。

    

    物性预测在材料发现中扮演重要角色。作为最终为材料科学开发基础模型的第一步，我们引入了一种新的自编码器，称为MHG-GNN，它将图神经网络（GNN）与分子超图语法（MHG）结合起来。在多样材料的各种物性预测任务中的结果表明，MHG-GNN具有巨大的潜力。

    Property prediction plays an important role in material discovery. As an initial step to eventually develop a foundation model for material science, we introduce a new autoencoder called the MHG-GNN, which combines graph neural network (GNN) with Molecular Hypergraph Grammar (MHG). Results on a variety of property prediction tasks with diverse materials show that MHG-GNN is promising.
    
[^54]: 把最小值的锐度的讨论带入到音频领域:一个用于声学场景分类的滤波器归一化评估

    Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification. (arXiv:2309.16369v1 [cs.SD])

    [http://arxiv.org/abs/2309.16369](http://arxiv.org/abs/2309.16369)

    本研究探索了在音频场景分类任务中损失函数最小值的锐度与泛化之间的关联。研究发现锐度更高的最小值具有更好的泛化能力，尤其是对于从之前未见设备录制的领域外数据来说。研究还发现优化器的选择是最小值锐度的主要影响因素，并讨论了在可比性方面的相关限制。

    

    在深度神经网络的背景下，损失函数最小值的锐度和泛化之间的相关性一直存在争议。虽然大多数研究都集中在计算机视觉领域的选定基准数据集上，但我们在DCASE2020挑战数据的音频场景分类任务中探索了这一方面。我们的分析基于二维滤波器归一化可视化和派生的锐度度量。我们的初步分析表明，锐度更高的最小值通常比平坦的最小值具有更好的泛化能力，尤其是对于从之前未见设备录制的领域外数据来说。这进一步加深了关于平坦最小值泛化能力的争议。我们还发现，特别是优化器的选择是最小值锐度的主要驱动因素，并讨论了在可比性方面的相关限制。我们的代码、训练模型状态和损失图景可公开获取。

    The correlation between the sharpness of loss minima and generalisation in the context of deep neural networks has been subject to discussion for a long time. Whilst mostly investigated in the context of selected benchmark data sets in the area of computer vision, we explore this aspect for the audio scene classification task of the DCASE2020 challenge data. Our analysis is based on twodimensional filter-normalised visualisations and a derived sharpness measure. Our exploratory analysis shows that sharper minima tend to show better generalisation than flat minima -even more so for out-of-domain data, recorded from previously unseen devices-, thus adding to the dispute about better generalisation capabilities of flat minima. We further find that, in particular, the choice of optimisers is a main driver of the sharpness of minima and we discuss resulting limitations with respect to comparability. Our code, trained model states and loss landscape visualisations are publicly available.
    
[^55]: 利用预训练语言模型进行文本增强的时间间隔预测在时态知识图中的应用

    Leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs. (arXiv:2309.16357v1 [cs.LG])

    [http://arxiv.org/abs/2309.16357](http://arxiv.org/abs/2309.16357)

    提出了一个新颖框架TEMT，利用预训练语言模型在文本增强的时态知识图中进行时间间隔预测和归纳推理。

    

    大多数知识图完成方法通过将给定图中的实体和关系映射到向量空间来学习潜在表示。虽然大多数这些方法专注于静态的知识图，但是许多公开可用的知识图包含描述某个事实为真的时间点/时间段的时间信息。这样的图通常被称为时态知识图。此外，知识图还可能包含实体和关系的文本描述。静态的知识图完成方法在表示学习过程中不考虑时间信息和文本描述，而只利用图的结构信息。最近一些研究利用时间信息提高了链接预测的性能，但它们没有利用文本描述，并不支持归纳推理（在训练中未被看到的实体上进行预测）。我们提出了一个新颖的框架TEM和TEM来利用预训练的语言模型和时态知识图中的文本描述，从而进行时间间隔的预测和归纳推理。

    Most knowledge graph completion (KGC) methods learn latent representations of entities and relations of a given graph by mapping them into a vector space. Although the majority of these methods focus on static knowledge graphs, a large number of publicly available KGs contain temporal information stating the time instant/period over which a certain fact has been true. Such graphs are often known as temporal knowledge graphs. Furthermore, knowledge graphs may also contain textual descriptions of entities and relations. Both temporal information and textual descriptions are not taken into account during representation learning by static KGC methods, and only structural information of the graph is leveraged. Recently, some studies have used temporal information to improve link prediction, yet they do not exploit textual descriptions and do not support inductive inference (prediction on entities that have not been seen in training).  We propose a novel framework called TEMT that exploits t
    
[^56]: Transformer-VQ: 基于向量量化实现线性时间的Transformer

    Transformer-VQ: Linear-Time Transformers via Vector Quantization. (arXiv:2309.16354v1 [cs.LG])

    [http://arxiv.org/abs/2309.16354](http://arxiv.org/abs/2309.16354)

    Transformer-VQ是一种基于向量量化实现线性时间的Transformer模型，能够高效计算自注意力，在大规模实验中表现出色。

    

    我们引入了Transformer-VQ，一种仅编码器的Transformer，能够在线性时间内计算基于softmax的密集自注意力。Transformer-VQ的高效注意力是通过向量量化键和一种新颖的缓存机制实现的。在大规模实验中，Transformer-VQ在质量上表现出色，Enwik8(0.99 bpb)，PG-19(26.6 ppl)和ImageNet64(3.16 bpb)都取得了很好的结果。

    We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
    
[^57]: ShapeDBA: 使用ShapeDTW Barycenter Averaging生成有效的时间序列原型

    ShapeDBA: Generating Effective Time Series Prototypes using ShapeDTW Barycenter Averaging. (arXiv:2309.16353v1 [cs.LG])

    [http://arxiv.org/abs/2309.16353](http://arxiv.org/abs/2309.16353)

    使用新型的ShapeDTW Barycenter Averaging方法生成逼真且有用的时间序列样本和原型，并解决了现有方法中生成不符合分布的人工制品的问题。

    

    时间序列数据几乎在各个领域中都有所应用，从医学领域到制造业和无线通信。生成逼真且有用的样本和原型是一项基本的数据分析任务。本文中，我们研究了一种新的方法来为时间序列数据生成逼真且有用的样本和原型。我们的方法使用了一种新型的时间序列平均形式，即ShapeDTW Barycentric Average。因此，我们专注于使用一种新的方法准确生成时间序列原型。现有的时间序列原型化方法依赖于动态时间规整(DTW)相似性度量，如DTW Barycentering Average(DBA)和SoftDBA。这些方法在其原型中存在一个常见问题，即生成了不符合分布的人工制品。这主要是由于所使用的DTW变体及其无法检测邻域相似性，而只能检测绝对相似性所致。我们提出的方法ShapeDBA通过改进DTW变体解决了这个问题，

    Time series data can be found in almost every domain, ranging from the medical field to manufacturing and wireless communication. Generating realistic and useful exemplars and prototypes is a fundamental data analysis task. In this paper, we investigate a novel approach to generating realistic and useful exemplars and prototypes for time series data. Our approach uses a new form of time series average, the ShapeDTW Barycentric Average. We therefore turn our attention to accurately generating time series prototypes with a novel approach. The existing time series prototyping approaches rely on the Dynamic Time Warping (DTW) similarity measure such as DTW Barycentering Average (DBA) and SoftDBA. These last approaches suffer from a common problem of generating out-of-distribution artifacts in their prototypes. This is mostly caused by the DTW variant used and its incapability of detecting neighborhood similarities, instead it detects absolute similarities. Our proposed method, ShapeDBA, us
    
[^58]: 复杂长视程机器人操作任务的内在语言引导探索

    Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks. (arXiv:2309.16347v1 [cs.RO])

    [http://arxiv.org/abs/2309.16347](http://arxiv.org/abs/2309.16347)

    本文提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架，通过利用LLMs作为辅助内在奖励，解决了复杂长视程机器人操作任务中奖励稀疏问题，并在实验中展示了其较高的性能和模块化特性。

    

    当前的强化学习算法在稀疏和复杂的环境中面临困境，尤其是在涉及众多不同序列的长视程操作任务中。在本研究中，我们提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架。通过利用LLMs作为辅助内在奖励，IGE-LLMs引导强化学习中的探索过程，以解决复杂的长视程操作任务中奖励稀疏问题。我们在一个具有探索挑战的环境和一个同时面临探索和长视程挑战的复杂机器人操作任务中评估了我们的框架和相关的内在学习方法。结果显示，IGE-LLMs(i)在相关的内在方法和直接使用LLMs进行决策的性能上表现出明显的较高水平，(ii)可以与现有的学习方法相结合和互补，突出其模块化性能，(iii)对于不同的内在缩放参数比较不敏感。

    Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and 
    
[^59]: LagrangeBench: 一种拉格朗日流体力学基准测试套件

    LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite. (arXiv:2309.16342v1 [cs.LG])

    [http://arxiv.org/abs/2309.16342](http://arxiv.org/abs/2309.16342)

    LagrangeBench是第一个针对拉格朗日粒子问题的基准测试套件，提供了七个新的流体力学数据集（包括不同维度和物理特性），以及高效的API和已建立的图神经网络的JAX实现。

    

    机器学习在各种科学应用中成功应用于基于网格的偏微分方程建模。然而，基于拉格朗日粒子离散化的学习PDE求解器，在涉及自由表面或复杂物理问题时仍然很少被探索。我们提出了LagrangeBench，这是针对拉格朗日粒子问题的第一个基准测试套件，重点是时间粗粒化。特别地，我们的贡献是：(a)使用平滑粒子流体动力学（SPH）方法生成的七个新的流体力学数据集（其中四个是2D的，三个是3D的），包括了Taylor-Green涡旋、驱动上盖、反Poiseuille流动和断坝等不同物理特性，如固体壁相互作用或自由表面，(b)高效的基于JAX的API，配备不同的近期训练策略和邻居搜索例程，以及(c)已建立的图神经网络（GNNs）如GNS和SEGNN的JAX实现与基准结果。最后，为了度量...

    Machine learning has been successfully applied to grid-based PDE modeling in various scientific applications. However, learned PDE solvers based on Lagrangian particle discretizations, which are the preferred approach to problems with free surfaces or complex physics, remain largely unexplored. We present LagrangeBench, the first benchmarking suite for Lagrangian particle problems, focusing on temporal coarse-graining. In particular, our contribution is: (a) seven new fluid mechanics datasets (four in 2D and three in 3D) generated with the Smoothed Particle Hydrodynamics (SPH) method including the Taylor-Green vortex, lid-driven cavity, reverse Poiseuille flow, and dam break, each of which includes different physics like solid wall interactions or free surface, (b) efficient JAX-based API with various recent training strategies and neighbors search routine, and (c) JAX implementation of established Graph Neural Networks (GNNs) like GNS and SEGNN with baseline results. Finally, to measu
    
[^60]: EFFL: 在联邦学习中实现均等公平性以减轻马太效应

    EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect. (arXiv:2309.16338v1 [cs.LG])

    [http://arxiv.org/abs/2309.16338](http://arxiv.org/abs/2309.16338)

    EFFL是一种在联邦学习中实现均等公平性以减轻马太效应的方法，通过确保全局模型具有均等的准确性和决策偏见来减少客户端之间的资源差距。

    

    最近在联邦学习（FL）方面取得的进展使得可以保护用户隐私的同时，从大规模和广泛分散的客户端进行机器学习（ML）模型的协作训练成为可能。然而，当不同客户端的数据集异构时，传统的FL机制产生的全局模型不能充分代表拥有有限数据资源的较贫困客户端，在其本地数据上导致较低的准确性和更高的偏见。根据马太效应的描述，即优势者获得更多优势，劣势者随着时间的推移失去更多，将这样的全局模型部署在客户端应用中可能加剧客户之间的资源差距，并损害社会福利和公平的原则。为了减轻马太效应，我们提出了均等公平联邦学习（EFFL），其中均等公平性指的是从FL学习到的全局模型具有以下特点：（1）客户端之间的准确性相等；（2）客户端之间的决策偏见相等。

    Recent advances in federated learning (FL) enable collaborative training of machine learning (ML) models from large-scale and widely dispersed clients while protecting their privacy. However, when different clients' datasets are heterogeneous, traditional FL mechanisms produce a global model that does not adequately represent the poorer clients with limited data resources, resulting in lower accuracy and higher bias on their local data. According to the Matthew effect, which describes how the advantaged gain more advantage and the disadvantaged lose more over time, deploying such a global model in client applications may worsen the resource disparity among the clients and harm the principles of social welfare and fairness. To mitigate the Matthew effect, we propose Egalitarian Fairness Federated Learning (EFFL), where egalitarian fairness refers to the global model learned from FL has: (1) equal accuracy among clients; (2) equal decision bias among clients. Besides achieving egalitaria
    
[^61]: 由深度神经网络从12导联心电图中进行的房颤风险预测的端到端方法

    End-to-end Risk Prediction of Atrial Fibrillation from the 12-Lead ECG by Deep Neural Networks. (arXiv:2309.16335v1 [cs.LG])

    [http://arxiv.org/abs/2309.16335](http://arxiv.org/abs/2309.16335)

    该研究提出了一种由深度神经网络进行的端到端方法，可以从12导联心电图中预测房颤的风险。研究结果显示，该模型可以在目前的心电图中识别出未来会发展房颤的患者，并可以根据患者的风险等级评估他们在一定时间内发展房颤的可能性。

    

    背景：房颤是最常见的心律失常之一，每年影响全球数百万人，与中风和心力衰竭等心血管疾病的风险密切相关。机器学习方法在评估从心电图中发展房颤的风险方面取得了有希望的结果。我们的目标是在巴西收集的大型CODE数据集上开发和评估这样的算法。结果：深度神经网络模型识别出在目前的心电图中没有房颤迹象，但将来会发展房颤的患者，AUC得分为0.845。从我们的生存模型中得知，高风险组（即未来房颤发病概率大于0.7）的患者在40周内发展房颤的可能性要高出50%，而属于最低风险组（即未来房颤发病概率小于或等于0.1）的患者有超过85%的机会。

    Background: Atrial fibrillation (AF) is one of the most common cardiac arrhythmias that affects millions of people each year worldwide and it is closely linked to increased risk of cardiovascular diseases such as stroke and heart failure. Machine learning methods have shown promising results in evaluating the risk of developing atrial fibrillation from the electrocardiogram. We aim to develop and evaluate one such algorithm on a large CODE dataset collected in Brazil.  Results: The deep neural network model identified patients without indication of AF in the presented ECG but who will develop AF in the future with an AUC score of 0.845. From our survival model, we obtain that patients in the high-risk group (i.e. with the probability of a future AF case being greater than 0.7) are 50% more likely to develop AF within 40 weeks, while patients belonging to the minimal-risk group (i.e. with the probability of a future AF case being less than or equal to 0.1) have more than 85% chance of r
    
[^62]: DeepPCR：神经网络中的并行化序列操作

    DeepPCR: Parallelizing Sequential Operations in Neural Networks. (arXiv:2309.16318v1 [cs.LG])

    [http://arxiv.org/abs/2309.16318](http://arxiv.org/abs/2309.16318)

    DeepPCR是一种新型算法，通过使用并行循环降解算法将常规的顺序操作在神经网络的推断和训练中并行化，从而实现了计算速度的提升。

    

    并行化技术已经在加速深度神经网络的推断和训练中变得普遍。尽管如此，仍然有一些操作是按顺序进行的。例如，前向传递和反向传递是逐层执行的，并且扩散模型的输出是通过应用一系列去噪步骤产生的。这种顺序方法导致计算成本与所涉及步骤的数量成正比，随着步骤数量的增加，可能出现潜在瓶颈。在这项工作中，我们介绍了一种新颖的算法DeepPCR，它并行化了神经网络推断和训练中通常是顺序操作的步骤。DeepPCR基于将$L$步骤的序列解释为特定方程组的解，我们使用并行循环降解算法恢复这些方程。这将顺序操作的计算复杂度从$\mathcal{O}(L)$降低到$\mathcal{O}(\log_2L)$，从而实现了加速。

    Parallelization techniques have become ubiquitous for accelerating inference and training of deep neural networks. Despite this, several operations are still performed in a sequential manner. For instance, the forward and backward passes are executed layer-by-layer, and the output of diffusion models is produced by applying a sequence of denoising steps. This sequential approach results in a computational cost proportional to the number of steps involved, presenting a potential bottleneck as the number of steps increases. In this work, we introduce DeepPCR, a novel algorithm which parallelizes typically sequential operations used in inference and training of neural networks. DeepPCR is based on interpreting a sequence of $L$ steps as the solution of a specific system of equations, which we recover using the Parallel Cyclic Reduction algorithm. This reduces the complexity of computing the sequential operations from $\mathcal{O}(L)$ to $\mathcal{O}(\log_2L)$, thus yielding a speedup for 
    
[^63]: Astroconformer：使用基于Transformer的深度学习模型分析恒星光曲线的前景

    Astroconformer: The Prospects of Analyzing Stellar Light Curves with Transformer-Based Deep Learning Models. (arXiv:2309.16316v1 [astro-ph.SR])

    [http://arxiv.org/abs/2309.16316](http://arxiv.org/abs/2309.16316)

    Astroconformer是一个基于Transformer的深度学习框架，旨在从恒星光曲线中捕捉长程依赖关系。通过应用于Kepler光曲线数据集，实现了对恒星表面重力的准确估计。

    

    恒星的光变曲线包含了丰富的关于恒星振荡和颗粒运动的信息，从而为我们提供了关于恒星内部结构和演化状态的重要洞见。传统的星震学技术主要依赖于功率谱分析，忽略了光变曲线中包含的宝贵的相位信息。虽然最近的机器学习应用在星震学中利用卷积神经网络（CNNs）从光变曲线中成功地推断出恒星属性，但往往受限于卷积操作中固有的局部特征提取。为了解决这些限制，我们提出了Astroconformer，这是一个基于Transformer的深度学习框架，旨在捕捉恒星光曲线中的长程依赖关系。我们的实证分析主要集中在估计表面重力（log g），并基于从Kepler光曲线中精心筛选得到的数据集进行。这些光曲线包含了大量恒星的观测数据。

    Light curves of stars encapsulate a wealth of information about stellar oscillations and granulation, thereby offering key insights into the internal structure and evolutionary state of stars. Conventional asteroseismic techniques have been largely confined to power spectral analysis, neglecting the valuable phase information contained within light curves. While recent machine learning applications in asteroseismology utilizing Convolutional Neural Networks (CNNs) have successfully inferred stellar attributes from light curves, they are often limited by the local feature extraction inherent in convolutional operations. To circumvent these constraints, we present $\textit{Astroconformer}$, a Transformer-based deep learning framework designed to capture long-range dependencies in stellar light curves. Our empirical analysis, which focuses on estimating surface gravity ($\log g$), is grounded in a carefully curated dataset derived from $\textit{Kepler}$ light curves. These light curves fe
    
[^64]: 贝叶斯神经网络：综述和讨论的入门指南

    A Primer on Bayesian Neural Networks: Review and Debates. (arXiv:2309.16314v1 [stat.ML])

    [http://arxiv.org/abs/2309.16314](http://arxiv.org/abs/2309.16314)

    本论文综述介绍了贝叶斯神经网络（BNNs）的基本概念和其在解决神经网络局限性方面的重要性。目标读者包括具备贝叶斯方法背景但缺乏深度学习专业知识的统计学家，以及深度神经网络专业但对贝叶斯统计学有限了解的机器学习专家。

    

    神经网络在各个问题领域取得了显著的性能，但其广泛应用受到了固有局限的限制，如过于自信的预测、缺乏可解释性以及容易受到对抗攻击。为了解决这些挑战，贝叶斯神经网络（BNNs）作为传统神经网络的一个引人注目的扩展，将不确定性估计整合到其预测能力中。本篇综述性入门指南系统介绍了神经网络和贝叶斯推断的基本概念，阐明了它们在BNNs开发中的协同整合。目标读者包括具备贝叶斯方法背景但缺乏深度学习专业知识的统计学家，以及深度神经网络专业但对贝叶斯统计学有限了解的机器学习专家。我们概述了常用的先验知识，考察了它们对模型行为的影响。

    Neural networks have achieved remarkable performance across various problem domains, but their widespread applicability is hindered by inherent limitations such as overconfidence in predictions, lack of interpretability, and vulnerability to adversarial attacks. To address these challenges, Bayesian neural networks (BNNs) have emerged as a compelling extension of conventional neural networks, integrating uncertainty estimation into their predictive capabilities.  This comprehensive primer presents a systematic introduction to the fundamental concepts of neural networks and Bayesian inference, elucidating their synergistic integration for the development of BNNs. The target audience comprises statisticians with a potential background in Bayesian methods but lacking deep learning expertise, as well as machine learners proficient in deep neural networks but with limited exposure to Bayesian statistics. We provide an overview of commonly employed priors, examining their impact on model beh
    
[^65]: CasIL: 通过双重认知-行动架构认知和模仿技能

    CasIL: Cognizing and Imitating Skills via a Dual Cognition-Action Architecture. (arXiv:2309.16299v1 [cs.RO])

    [http://arxiv.org/abs/2309.16299](http://arxiv.org/abs/2309.16299)

    本文介绍了一种名为CasIL的技能模仿学习框架，通过引入人类认知先验并将行动概念扩展为双重认知-行动架构，使机器人能够从原始视觉演示中有效地认知和模仿关键技能。该框架通过人机交互实现认知和行动的双重模仿，以提供整个过程的稳健性和可靠性。

    

    在长时间任务（如运动、操作等）中，使机器人能够有效地模仿专家技能是一个长期存在的挑战。现有的机器人模仿学习方法在复杂任务中仍面临次优性能的问题。本文考虑如何在人类认知先验的范围内解决这一挑战。我们通过引入直观的人类认知先验，启发性地将行动的概念扩展为双重认知（高层）-行动（低层）架构，并提出了一种新颖的基于认知和行动的技能模仿学习框架（CasIL），通过人机交互来使机器人有效地认知和模仿原始视觉演示中的关键技能。CasIL实现了认知和行动的双重模仿，高级技能认知明确地指导低级基本动作，为整个技能模仿学习过程提供了稳健性和可靠性。

    Enabling robots to effectively imitate expert skills in longhorizon tasks such as locomotion, manipulation, and more, poses a long-standing challenge. Existing imitation learning (IL) approaches for robots still grapple with sub-optimal performance in complex tasks. In this paper, we consider how this challenge can be addressed within the human cognitive priors. Heuristically, we extend the usual notion of action to a dual Cognition (high-level)-Action (low-level) architecture by introducing intuitive human cognitive priors, and propose a novel skill IL framework through human-robot interaction, called Cognition-Action-based Skill Imitation Learning (CasIL), for the robotic agent to effectively cognize and imitate the critical skills from raw visual demonstrations. CasIL enables both cognition and action imitation, while high-level skill cognition explicitly guides low-level primitive actions, providing robustness and reliability to the entire skill IL process. We evaluated our method 
    
[^66]: RL方法的效率分离：无模型、有模型和目标条件下的效率分析

    Efficiency Separation between RL Methods: Model-Free, Model-Based and Goal-Conditioned. (arXiv:2309.16291v1 [cs.LG])

    [http://arxiv.org/abs/2309.16291](http://arxiv.org/abs/2309.16291)

    本论文证明了无模型和有模型强化学习方法在效率上存在根本的限制，但目标条件下的方法和构建逆动力学模型的算法不受该限制。

    

    我们证明了一类广泛的强化学习（RL）算法的效率的基本限制。这个限制适用于无模型的RL方法，以及一系列的有模型方法，如树搜索的规划。在对这类问题的抽象定义下，我们提供了一系列RL问题，对于这些方法来说，它们与环境的交互寻找最优行为的时间复杂度下界是指数级的。然而，存在一种方法，不针对这个特定的问题家族，可以高效地解决这个问题家族中的问题。相比之下，我们的限制不适用于文献中提出的几种方法，例如目标条件下的方法或构建逆动力学模型的其他算法。

    We prove a fundamental limitation on the efficiency of a wide class of Reinforcement Learning (RL) algorithms. This limitation applies to model-free RL methods as well as a broad range of model-based methods, such as planning with tree search.  Under an abstract definition of this class, we provide a family of RL problems for which these methods suffer a lower bound exponential in the horizon for their interactions with the environment to find an optimal behavior. However, there exists a method, not tailored to this specific family of problems, which can efficiently solve the problems in the family.  In contrast, our limitation does not apply to several types of methods proposed in the literature, for instance, goal-conditioned methods or other algorithms that construct an inverse dynamics model.
    
[^67]: LawBench: 对大型语言模型的法律知识进行基准测试

    LawBench: Benchmarking Legal Knowledge of Large Language Models. (arXiv:2309.16289v1 [cs.CL])

    [http://arxiv.org/abs/2309.16289](http://arxiv.org/abs/2309.16289)

    LawBench针对大型语言模型的法律知识进行了综合评估，从记忆，理解和应用三个层面评估了它们在法律任务上的能力。

    

    大型语言模型（LLMs）在各方面表现出了强大的能力。然而，当将它们应用于高度专业化、安全关键的法律领域时，尚不清楚它们所具备的法律知识量以及它们是否能可靠地执行与法律相关的任务。为了填补这一空白，我们提出了一个全面评估基准LawBench。LawBench经过精心设计，从三个认知层面对LLMs的法律能力进行精确评估：（1）法律知识记忆：LLMs是否能够记住所需的法律概念、条款和事实；（2）法律知识理解：LLMs是否能够理解法律文本中的实体、事件和关系；（3）法律知识应用：LLMs是否能够正确运用自己的法律知识并进行必要的推理步骤来解决现实的法律任务。LawBench包含20个多样化的任务，涵盖了5种任务类型：单标签分类（SLC）、多标签分类（MLC）、回归等。

    Large language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark LawBench. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, e
    
[^68]: 通用的异构联邦交叉相关和实例相似性学习

    Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning. (arXiv:2309.16286v1 [cs.LG])

    [http://arxiv.org/abs/2309.16286](http://arxiv.org/abs/2309.16286)

    本文提出了一种通用的异构联邦交叉相关和实例相似性学习的方法，利用非目标蒸馏来解决模型异质性和灾难性遗忘问题，提高了联邦学习的泛化能力和应用性能。

    

    联邦学习是一种重要的保护隐私的多方学习范式，涉及与他人的合作学习和对私有数据的本地更新。模型异质性和灾难性遗忘是两个重要的挑战，极大地限制了应用和泛化性能。本文提出了一种新颖的FCCL+方法，即联邦相关性和相似性学习与非目标蒸馏，促进了域内区分能力和域间泛化能力。对于异质性问题，我们利用无关的未标记公共数据来进行异构参与者之间的通信。我们构建交叉相关矩阵，并在标志和特征水平上对实例相似性分布进行对齐，有效地克服了通信障碍并提高了广泛的能力。对于本地更新阶段的灾难性遗忘，FCCL+引入了联邦非目标蒸馏，既保留了域间知识又避免了优化问题。

    Federated learning is an important privacy-preserving multi-party learning paradigm, involving collaborative learning with others and local updating on private data. Model heterogeneity and catastrophic forgetting are two crucial challenges, which greatly limit the applicability and generalizability. This paper presents a novel FCCL+, federated correlation and similarity learning with non-target distillation, facilitating the both intra-domain discriminability and inter-domain generalization. For heterogeneity issue, we leverage irrelevant unlabeled public data for communication between the heterogeneous participants. We construct cross-correlation matrix and align instance similarity distribution on both logits and feature levels, which effectively overcomes the communication barrier and improves the generalizable ability. For catastrophic forgetting in local updating stage, FCCL+ introduces Federated Non Target Distillation, which retains inter-domain knowledge while avoiding the opt
    
[^69]: 高维数据配对样本假设检验的框架

    A framework for paired-sample hypothesis testing for high-dimensional data. (arXiv:2309.16274v1 [stat.ML])

    [http://arxiv.org/abs/2309.16274](http://arxiv.org/abs/2309.16274)

    本文提出了一个针对高维数据配对样本的假设检验框架，通过垂直平分线生成评分函数，并利用伪中位数求得最优评分函数。

    

    在多维数据的配对样本检验中，标准的方法是对每个特征应用多个单变量检验，然后进行p值调整。然而，当数据含有大量特征时，这种方法存在问题。已有一些研究表明，分类准确率可以作为双样本检验的代理。然而，迄今为止尚未提出理论基础或实际方法来将这种策略扩展到多维配对样本检验。在本研究中，我们提出了一种想法，即通过每对实例连接线段的垂直平分线定义的决策规则来生成评分函数。然后，通过这些规则的伪中位数来估计最优评分函数，我们通过自然地扩展Hodges-Lehmann估计量来进行估计。因此，我们提出了一个两步检验过程的框架。首先，我们估计每个特征的平分线。接下来，我们根据这些平分线定义评分函数，并通过伪中位数求得最优评分函数。

    The standard paired-sample testing approach in the multidimensional setting applies multiple univariate tests on the individual features, followed by p-value adjustments. Such an approach suffers when the data carry numerous features. A number of studies have shown that classification accuracy can be seen as a proxy for two-sample testing. However, neither theoretical foundations nor practical recipes have been proposed so far on how this strategy could be extended to multidimensional paired-sample testing. In this work, we put forward the idea that scoring functions can be produced by the decision rules defined by the perpendicular bisecting hyperplanes of the line segments connecting each pair of instances. Then, the optimal scoring function can be obtained by the pseudomedian of those rules, which we estimate by extending naturally the Hodges-Lehmann estimator. We accordingly propose a framework of a two-step testing procedure. First, we estimate the bisecting hyperplanes for each p
    
[^70]: B5G网络自动化的层级网络数据分析框架: 设计与实现

    Hierarchical Network Data Analytics Framework for B5G Network Automation: Design and Implementation. (arXiv:2309.16269v1 [cs.NI])

    [http://arxiv.org/abs/2309.16269](http://arxiv.org/abs/2309.16269)

    我们提出了一个层级网络数据分析框架(H-NDAF)，通过将推理任务分布在多个叶子NWDAF上，实现了分析结果的及时提供和更快的分析提供时间。

    

    5G引入了模块化的网络功能(NFs)，以更灵活、弹性地支持新兴服务。为了减少这种模块化NF管理中的复杂性，自动化的网络操作和管理是必不可少的，因此第三代合作伙伴计划(3GPP)引入了网络数据分析功能(NWDAF)。然而，传统的NWDAF需要进行推理和训练任务，因此很难及时地将分析结果提供给NF，尤其是在增加的分析请求数量下。在本文中，我们提出了一个层级网络数据分析框架(H-NDAF)，其中推理任务分布在多个叶子NWDAF上，训练任务在根NWDAF上进行。使用开源软件(free5GC)进行的大量仿真结果表明，与传统的NWDAF相比，H-NDAF可以提供足够准确的分析结果，并且分析提供时间更快。

    5G introduced modularized network functions (NFs) to support emerging services in a more flexible and elastic manner. To mitigate the complexity in such modularized NF management, automated network operation and management are indispensable, and thus the 3rd generation partnership project (3GPP) has introduced a network data analytics function (NWDAF). However, a conventional NWDAF needs to conduct both inference and training tasks, and thus it is difficult to provide the analytics results to NFs in a timely manner for an increased number of analytics requests. In this article, we propose a hierarchical network data analytics framework (H-NDAF) where inference tasks are distributed to multiple leaf NWDAFs and training tasks are conducted at the root NWDAF. Extensive simulation results using open-source software (i.e., free5GC) demonstrate that H-NDAF can provide sufficiently accurate analytics and faster analytics provision time compared to the conventional NWDAF.
    
[^71]: 超越逆KL：通过多样的差异约束推广直接偏好优化

    Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints. (arXiv:2309.16240v1 [cs.LG])

    [http://arxiv.org/abs/2309.16240](http://arxiv.org/abs/2309.16240)

    本论文提出了一种通过引入多样差异约束推广直接偏好优化（DPO）的方法，该方法消除了对估计方法的需要并简化了奖励和最优策略之间的复杂关系。

    

    大型语言模型（LLM）的不断增强能力为人工智能提供了机会，但同时也放大了安全问题，如AI系统的潜在滥用，这需要有效的AI对齐。基于人类反馈的强化学习（RLHF）已经成为AI对齐的一条有希望的路径，但由于其复杂性和对独立奖励模型的依赖性而带来了挑战。直接偏好优化（DPO）被提出作为一种替代方法，在逆KL正则化约束下等同于RLHF。本文提出了f-DPO，一种通过整合多样的差异约束来推广DPO的方法。我们证明，在某些f-散度下，包括Jensen-Shannon散度、正向KL散度和α-散度，奖励和最优策略之间的复杂关系也可以通过解决Karush-Kuhn-Tucker条件来简化。这消除了对估计方法的需要。

    The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimat
    
[^72]: 语言模型在分子发现中的应用

    Language models in molecular discovery. (arXiv:2309.16235v1 [physics.chem-ph])

    [http://arxiv.org/abs/2309.16235](http://arxiv.org/abs/2309.16235)

    语言模型在分子发现中的应用为加速药物发现和分子设计提供了有希望的方法，包括从头设计药物、性质预测和反应化学。同时，开源软件资源降低了科学语言建模领域的门槛，未来将结合聊天机器人和计算化学工具。

    

    语言模型的成功，特别是基于Transformer的架构，已经渗透到其他领域，出现了在小分子、蛋白质或聚合物上运作的“科学语言模型”。在化学领域，语言模型在加速分子发现周期方面发挥了作用，最近在早期药物发现方面有了有希望的发现。在这里，我们回顾了语言模型在分子发现中的角色，强调了从头设计药物、性质预测和反应化学方面的优势。我们还强调了有价值的开源软件资源，从而降低了科学语言建模领域的门槛。最后，我们勾画了未来分子设计的愿景，将聊天机器人界面与计算化学工具结合起来。我们的贡献为研究人员、化学家和对了解语言模型如何加速化学发现感兴趣的人提供了宝贵的资源。

    The success of language models, especially transformer-based architectures, has trickled into other domains giving rise to "scientific language models" that operate on small molecules, proteins or polymers. In chemistry, language models contribute to accelerating the molecule discovery cycle as evidenced by promising recent findings in early-stage drug discovery. Here, we review the role of language models in molecular discovery, underlining their strength in de novo drug design, property prediction and reaction chemistry. We highlight valuable open-source software assets thus lowering the entry barrier to the field of scientific language modeling. Last, we sketch a vision for future molecular design that combines a chatbot interface with access to computational chemistry tools. Our contribution serves as a valuable resource for researchers, chemists, and AI enthusiasts interested in understanding how language models can and will be used to accelerate chemical discovery.
    
[^73]: GInX-Eval: 面向图神经网络解释的内分布评估

    GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations. (arXiv:2309.16223v1 [cs.AI])

    [http://arxiv.org/abs/2309.16223](http://arxiv.org/abs/2309.16223)

    本文针对图神经网络解释的内分布评估问题，提出了GInX-Eval方法，克服了传统评估指标的局限性，为解释方法提供了新的见解。

    

    最近，为了突出图中对模型预测最有贡献的边和节点，人们开发了各种解释图神经网络（GNN）的方法。然而，目前尚不清楚如何从人类或模型的角度评估这些解释的正确性。当前评估过程中一个未解决的瓶颈问题是解释的分布与训练数据的分布不同的情况。这个重要问题会影响到现有的评估指标，如流行的忠实度或保真度得分。在本文中，我们展示了忠实度指标的局限性。我们提出了GInX-Eval（图内分布解释评估），这是一种用于评估图解释的过程，克服了忠实度的缺陷，并提供了对解释方法的新见解。使用重新训练策略，GInX得分可衡量已移除边对模型的信息量以及边的重要性。

    Diverse explainability methods of graph neural networks (GNN) have recently been developed to highlight the edges and nodes in the graph that contribute the most to the model predictions. However, it is not clear yet how to evaluate the correctness of those explanations, whether it is from a human or a model perspective. One unaddressed bottleneck in the current evaluation procedure is the problem of out-of-distribution explanations, whose distribution differs from those of the training data. This important issue affects existing evaluation metrics such as the popular faithfulness or fidelity score. In this paper, we show the limitations of faithfulness metrics. We propose GInX-Eval (Graph In-distribution eXplanation Evaluation), an evaluation procedure of graph explanations that overcomes the pitfalls of faithfulness and offers new insights on explainability methods. Using a retraining strategy, the GInX score measures how informative removed edges are for the model and the EdgeRank s
    
[^74]: 揭示变色龙：医学表格数据中的ODD检测的基准。 (arXiv:2309.16220v1 [cs.LG])

    Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data. (arXiv:2309.16220v1 [cs.LG])

    [http://arxiv.org/abs/2309.16220](http://arxiv.org/abs/2309.16220)

    该论文提出了一个基准来比较不同的方法在医学表格数据中进行ODD检测，为了实现在实际医疗系统中可靠地使用机器学习模型并避免对ODD数据进行不准确的预测，该基准利用了大规模的ICU患者数据集，考虑了多种方法和预测架构。

    

    尽管机器学习（ML）模型取得了成功，但它们在来自训练分布之外的数据上没有有效的泛化能力。为了可靠地在现实世界的医疗系统中使用ML模型，并避免对ODD数据进行不准确的预测，检测ODD样本至关重要。虽然在其他领域，特别是在计算机视觉领域，提出了许多ODD检测方法，但当处理医学表格数据时，是否解决了这一挑战仍不清楚。为了回答这一迫切需求，我们提出了一个广泛可复制的基准，通过多个测试来比较不同方法，包括近和远ODDs。我们的基准利用了最新版本的eICU和MIMIC-IV，这是两个公共数据集，涵盖了数万名ICU患者在多家医院。我们考虑了各种基于密度的方法和SOTA后置检测器，涵盖了多种预测架构，包括MLP、ResNet和Transformer。我们的研究结果显示。

    Despite their success, Machine Learning (ML) models do not generalize effectively to data not originating from the training distribution. To reliably employ ML models in real-world healthcare systems and avoid inaccurate predictions on out-of-distribution (OOD) data, it is crucial to detect OOD samples. Numerous OOD detection approaches have been suggested in other fields - especially in computer vision - but it remains unclear whether the challenge is resolved when dealing with medical tabular data. To answer this pressing need, we propose an extensive reproducible benchmark to compare different methods across a suite of tests including both near and far OODs. Our benchmark leverages the latest versions of eICU and MIMIC-IV, two public datasets encompassing tens of thousands of ICU patients in several hospitals. We consider a wide array of density-based methods and SOTA post-hoc detectors across diverse predictive architectures, including MLP, ResNet, and Transformer. Our findings sho
    
[^75]: 使用Swinunter在CT中进行腹部多器官分割

    Abdominal multi-organ segmentation in CT using Swinunter. (arXiv:2309.16210v1 [eess.IV])

    [http://arxiv.org/abs/2309.16210](http://arxiv.org/abs/2309.16210)

    这项研究提出了使用Swinunter模型在CT中进行腹部多器官分割的方法。通过克服器官的模糊边界、复杂的背景和不同尺度的器官大小差异，该模型能够在公开验证集上取得令人满意的结果和较快的推理时间。

    

    在计算机断层扫描（CT）中，腹部多器官分割对于许多临床应用非常重要，包括疾病检测和治疗计划。深度学习方法在这方面表现出了前所未有的性能。然而，由于器官的模糊边界、复杂的背景和不同尺度的器官大小差异，利用单一网络准确地分割不同器官仍然具有挑战性。在这项工作中，我们使用了基于transformer的模型进行训练。通过前几年的比赛发现，基本上所有前五名的方法都使用了基于CNN的方法，这可能是因为数据量不足，阻碍了transformer-based方法发挥其全部优势。本次比赛中的数千个样本可能使transformer-based模型获得更好的结果。公开验证集上的结果还表明，transformer-based模型可以达到可接受的结果和推理时间。

    Abdominal multi-organ segmentation in computed tomography (CT) is crucial for many clinical applications including disease detection and treatment planning. Deep learning methods have shown unprecedented performance in this perspective. However, it is still quite challenging to accurately segment different organs utilizing a single network due to the vague boundaries of organs, the complex background, and the substantially different organ size scales. In this work we used make transformer-based model for training. It was found through previous years' competitions that basically all of the top 5 methods used CNN-based methods, which is likely due to the lack of data volume that prevents transformer-based methods from taking full advantage. The thousands of samples in this competition may enable the transformer-based model to have more excellent results. The results on the public validation set also show that the transformer-based model can achieve an acceptable result and inference time
    
[^76]: 最大切片互信息

    Max-Sliced Mutual Information. (arXiv:2309.16200v1 [cs.LG])

    [http://arxiv.org/abs/2309.16200](http://arxiv.org/abs/2309.16200)

    本论文提出了一种新的方法，最大切片互信息（mSMI），来量化高维随机变量之间的依赖关系。mSMI在捕捉复杂依赖关系的同时也适用于快速计算。

    

    量化高维随机变量之间的依赖关系对于统计学习和推断至关重要。两种传统方法是标准相关分析（CCA），其识别出原始变量的最大相关投影版本，以及香农的互信息，它是一种通用的依赖度量，也能捕捉高阶依赖关系。然而，CCA仅考虑线性依赖关系，这在某些应用中可能不足够，而互信息在高维情况下通常难以计算/估计。本研究提出了一种折衷方案，即基于信息论的可扩展方式，称为最大切片互信息（mSMI）。mSMI等于高维变量的低维投影之间的最大互信息，而在高斯情况下减少到CCA。它兼具两者的优点：捕捉数据中的复杂依赖关系，同时适用于快速计算。

    Quantifying the dependence between high-dimensional random variables is central to statistical learning and inference. Two classical methods are canonical correlation analysis (CCA), which identifies maximally correlated projected versions of the original variables, and Shannon's mutual information, which is a universal dependence measure that also captures high-order dependencies. However, CCA only accounts for linear dependence, which may be insufficient for certain applications, while mutual information is often infeasible to compute/estimate in high dimensions. This work proposes a middle ground in the form of a scalable information-theoretic generalization of CCA, termed max-sliced mutual information (mSMI). mSMI equals the maximal mutual information between low-dimensional projections of the high-dimensional variables, which reduces back to CCA in the Gaussian case. It enjoys the best of both worlds: capturing intricate dependencies in the data while being amenable to fast comput
    
[^77]: Stackelberg批量策略学习

    Stackelberg Batch Policy Learning. (arXiv:2309.16188v1 [stat.ML])

    [http://arxiv.org/abs/2309.16188](http://arxiv.org/abs/2309.16188)

    Stackelberg批量策略学习是一种新颖的基于随机梯度的学习算法，采用博弈论的观点，对策略学习进行建模，并考虑了优化景观中的分层决策结构。

    

    批量强化学习定义了从固定的数据批次中进行学习，缺乏详尽的探索。最坏情况下的最优算法使用经验数据来校准价值函数模型，并在学习模型下执行某种悲观评估，已经成为批量强化学习中一种有前景的范式。然而，对于这个流派的现代研究通常忽视了优化景观中隐藏的分层决策结构。在本文中，我们采用博弈论的观点，将策略学习图表建模为具有领导者-跟随者结构的两人零和博弈。我们提出了一种新颖的基于随机梯度的学习算法：StackelbergLearner，领导者根据其目标的全导数进行更新，而不是通常的个体梯度，而跟随者进行个体更新并确保过渡一致的悲观推理。推导出的学习动力

    Batch reinforcement learning (RL) defines the task of learning from a fixed batch of data lacking exhaustive exploration. Worst-case optimality algorithms, which calibrate a value-function model class from logged experience and perform some type of pessimistic evaluation under the learned model, have emerged as a promising paradigm for batch RL. However, contemporary works on this stream have commonly overlooked the hierarchical decision-making structure hidden in the optimization landscape. In this paper, we adopt a game-theoretical viewpoint and model the policy learning diagram as a two-player general-sum game with a leader-follower structure. We propose a novel stochastic gradient-based learning algorithm: StackelbergLearner, in which the leader player updates according to the total derivative of its objective instead of the usual individual gradient, and the follower player makes individual updates and ensures transition-consistent pessimistic reasoning. The derived learning dynam
    
[^78]: 系统化采样和机器学习参数化在气候模型中的验证

    Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models. (arXiv:2309.16177v1 [physics.ao-ph])

    [http://arxiv.org/abs/2309.16177](http://arxiv.org/abs/2309.16177)

    本论文研究了混合物理-机器学习气候模拟的挑战，并通过大规模在线建模错误采样和评估，在机器学习参数化设计中发现了改进性能的策略。

    

    混合物理-机器学习气候模拟的进展受到获取高性能耦合（即在线）模拟的困难的限制。虽然在脱机环境中评估数百个机器学习参数化子网格闭合（如对流和辐射）是直接的，但在相同规模上的在线评估在技术上具有挑战性。我们的软件自动化实现了比以往任何时候都多一个数量级的在线建模错误采样。利用这一点，我们评估混合气候模型的性能，并制定改进策略。我们发现，在包含记忆、相对湿度输入特征转换和额外输入变量的情况下，模型的在线性能有所提高。我们还揭示了在线错误的显著差异以及脱机与在线错误统计之间的不一致性。这意味着需要在线评估数百个候选机器学习模型以检测参数化设计选择的影响。

    Progress in hybrid physics-machine learning (ML) climate simulations has been limited by the difficulty of obtaining performant coupled (i.e. online) simulations. While evaluating hundreds of ML parameterizations of subgrid closures (here of convection and radiation) offline is straightforward, online evaluation at the same scale is technically challenging. Our software automation achieves an order-of-magnitude larger sampling of online modeling errors than has previously been examined. Using this, we evaluate the hybrid climate model performance and define strategies to improve it. We show that model online performance improves when incorporating memory, a relative humidity input feature transformation, and additional input variables. We also reveal substantial variation in online error and inconsistencies between offline vs. online error statistics. The implication is that hundreds of candidate ML models should be evaluated online to detect the effects of parameterization design choi
    
[^79]: 在问答系统中使用弱监督和数据增强技术

    Using Weak Supervision and Data Augmentation in Question Answering. (arXiv:2309.16175v1 [cs.CL])

    [http://arxiv.org/abs/2309.16175](http://arxiv.org/abs/2309.16175)

    本文研究了在问答系统中使用弱监督和数据增强技术的作用，通过自动生成标签和使用信息检索技术来训练深度神经网络模型，并探索了数据增强技术的应用。

    

    COVID-19疫情的爆发强调了获取生物医学文献以回答及时和与疾病相关的问题的需求。在疫情初期，我们面临的最大挑战之一是缺乏用于训练问答模型的经过同行评审的COVID-19生物医学文章。本文中，我们探讨了弱监督和数据增强在训练深度神经网络问答模型中的作用。首先，我们使用信息检索算法BM25从学术论文的结构化摘要中自动生成标签，探究这些标签是否提供了弱监督信号来训练一个抽取式问答模型。在医学领域专家的注释数据不可用的情况下，我们还使用信息检索技术基于clinicaltrials.gov架构和文章的结构化摘要来策划新的问答对。此外，我们还探索了使用数据增强技术来扩充深度神经网络的训练数据。

    The onset of the COVID-19 pandemic accentuated the need for access to biomedical literature to answer timely and disease-specific questions. During the early days of the pandemic, one of the biggest challenges we faced was the lack of peer-reviewed biomedical articles on COVID-19 that could be used to train machine learning models for question answering (QA). In this paper, we explore the roles weak supervision and data augmentation play in training deep neural network QA models. First, we investigate whether labels generated automatically from the structured abstracts of scholarly papers using an information retrieval algorithm, BM25, provide a weak supervision signal to train an extractive QA model. We also curate new QA pairs using information retrieval techniques, guided by the clinicaltrials.gov schema and the structured abstracts of articles, in the absence of annotated data from biomedical domain experts. Furthermore, we explore augmenting the training data of a deep neural netw
    
[^80]: Distill to Delete: 使用知识蒸馏进行图网络中的遗忘

    Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation. (arXiv:2309.16173v1 [cs.LG])

    [http://arxiv.org/abs/2309.16173](http://arxiv.org/abs/2309.16173)

    本论文提出了一种名为D2DGN的图遗忘方法，通过知识蒸馏的方式删除图神经网络中的信息。这种方法解决了传统方法在处理局部依赖和附加开销方面的局限性，并能够适应不断变化的数据分布和减少训练重复带来的能源消耗。

    

    图遗忘已成为从预训练图神经网络（GNN）中删除信息的重要方法。可以删除节点、节点类、边或边类。遗忘方法使GNN模型符合数据保护法规（即被遗忘权），适应不断变化的数据分布，并通过避免重复训练来减少GPU小时的碳足迹。现有的基于分区和聚合的方法在处理局部图依赖和附加开销方面存在局限性。最近，GNNDelete提出了一种模型无关的方法，缓解了其中一些问题。我们的工作通过知识蒸馏采用了一种新的方法来解决图遗忘中的这些挑战，即GNN中的跨轴蒸馏进行删除（D2DGN）。这是一个模型无关的蒸馏框架，将完整的图知识划分并标记为保留和删除。它使用响应为基础进行蒸馏。

    Graph unlearning has emerged as a pivotal method to delete information from a pre-trained graph neural network (GNN). One may delete nodes, a class of nodes, edges, or a class of edges. An unlearning method enables the GNN model to comply with data protection regulations (i.e., the right to be forgotten), adapt to evolving data distributions, and reduce the GPU-hours carbon footprint by avoiding repetitive retraining. Existing partitioning and aggregation-based methods have limitations due to their poor handling of local graph dependencies and additional overhead costs. More recently, GNNDelete offered a model-agnostic approach that alleviates some of these issues. Our work takes a novel approach to address these challenges in graph unlearning through knowledge distillation, as it distills to delete in GNN (D2DGN). It is a model-agnostic distillation framework where the complete graph knowledge is divided and marked for retention and deletion. It performs distillation with response-bas
    
[^81]: 奖励（不）一致性对RLHF的涓滴效应

    The Trickle-down Impact of Reward (In-)consistency on RLHF. (arXiv:2309.16155v1 [cs.CL])

    [http://arxiv.org/abs/2309.16155](http://arxiv.org/abs/2309.16155)

    本文研究了奖励模型（RM）的一致性对强化学习来自人类反馈（RLHF）模型训练所得的聊天机器人的影响，并提出了一种衡量RM一致性的对比提示的基准测试策略。

    

    强化学习来自人类反馈（RLHF）的标准实践涉及优化奖励模型（RM），而RM本身是通过训练来反映人类对期望生成的偏好。一个值得研究的重要主题是RM的（不）一致性 - 即它们能否识别不同提示的语义变化并适当地调整奖励分配 - 以及它们对下游RLHF模型的影响。本文针对RM不一致性提出了一系列相关的研究问题：（1）我们如何衡量奖励模型的一致性？（2）现有的RM有多一致，我们如何改进它们？（3）奖励的不一致性以何种方式影响RLHF模型训练所得的聊天机器人？我们提出了对RM一致性的基准测试策略"对比提示"。每个对比提示示例都包含一对具有不同真实响应的词汇相似的指令。一致的RM是期望对这对指令给出相似奖励分配的。

    Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs -- whether they can recognize the semantic changes to different prompts and appropriately adapt their reward assignments -- and their impact on the downstream RLHF model.  In this paper, we visit a series of research questions relevant to RM inconsistency: (1) How can we measure the consistency of reward models? (2) How consistent are the existing RMs and how can we improve them? (3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training?  We propose Contrast Instructions -- a benchmarking strategy for the consistency of RM. Each example in Contrast Instructions features a pair of lexically similar instructions with different ground truth responses. A consistent RM is exp
    
[^82]: 基于元优化合成样本的生成式半监督学习

    Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples. (arXiv:2309.16143v1 [cs.LG])

    [http://arxiv.org/abs/2309.16143](http://arxiv.org/abs/2309.16143)

    本文提出了一种基于生成基础模型生成的合成样本进行半监督学习的方法，旨在解决实际应用中无法获取大规模无标签数据集的问题。

    

    半监督学习是使用有标签和无标签数据集来训练深度分类模型的一种有前景的方法。然而，现有的半监督学习方法依赖于大规模的无标签数据集，在许多实际应用中由于法律限制（例如，GDPR）可能无法获取。本文研究一个问题：我们能否在没有实际无标签数据集的情况下训练半监督学习模型？我们提出了一种使用从包含数百万样本的多样领域数据集（例如ImageNet）训练的生成基础模型生成的合成数据集的半监督学习方法。我们的主要思想是识别生成基础模型中仿真无标签样本的合成样本，并使用这些合成样本来训练分类器。为了实现这一点，我们的方法被构建为一个交替优化问题：（i）元学习生成基础模型和（ii）使用真实标记样本和合成样本进行半监督学习的分类器。

    Semi-supervised learning (SSL) is a promising approach for training deep classification models using labeled and unlabeled datasets. However, existing SSL methods rely on a large unlabeled dataset, which may not always be available in many real-world applications due to legal constraints (e.g., GDPR). In this paper, we investigate the research question: Can we train SSL models without real unlabeled datasets? Instead of using real unlabeled datasets, we propose an SSL method using synthetic datasets generated from generative foundation models trained on datasets containing millions of samples in diverse domains (e.g., ImageNet). Our main concepts are identifying synthetic samples that emulate unlabeled samples from generative foundation models and training classifiers using these synthetic samples. To achieve this, our method is formulated as an alternating optimization problem: (i) meta-learning of generative foundation models and (ii) SSL of classifiers using real labeled and synthet
    
[^83]: 基于不确定性和多样性采样的两步主动学习用于实例分割

    Two-Step Active Learning for Instance Segmentation with Uncertainty and Diversity Sampling. (arXiv:2309.16139v1 [cs.CV])

    [http://arxiv.org/abs/2309.16139](http://arxiv.org/abs/2309.16139)

    这项研究提出了一种基于不确定性和多样性采样的两步主动学习算法，通过选择最具信息量和代表性的图像进行标注，提高实例分割模型的训练效率。

    

    训练高质量的实例分割模型需要大量带有实例掩码和分类的标记图像，而这往往是昂贵的。主动学习通过选择最具信息量和代表性的图像进行标注，以在最小标注成本下追求最佳性能，从而解决了这一挑战。尽管主动学习在实例分割领域具有潜力，但与其他任务（如图像分类）相比，实例分割领域的研究较少，后者的标注需求较少。在本研究中，我们提出了一种后续主动学习算法，将基于不确定性的采样与基于多样性的采样结合起来。我们提出的算法不仅简单易实现，而且在各种数据集上均表现出卓越性能。在一个实际的遥感图像数据集上展示了它的实际应用，它使标注效率提高了五倍。

    Training high-quality instance segmentation models requires an abundance of labeled images with instance masks and classifications, which is often expensive to procure. Active learning addresses this challenge by striving for optimum performance with minimal labeling cost by selecting the most informative and representative images for labeling. Despite its potential, active learning has been less explored in instance segmentation compared to other tasks like image classification, which require less labeling. In this study, we propose a post-hoc active learning algorithm that integrates uncertainty-based sampling with diversity-based sampling. Our proposed algorithm is not only simple and easy to implement, but it also delivers superior performance on various datasets. Its practical application is demonstrated on a real-world overhead imagery dataset, where it increases the labeling efficiency fivefold.
    
[^84]: 学习时空神经微分方程的谱方法

    A Spectral Approach for Learning Spatiotemporal Neural Differential Equations. (arXiv:2309.16131v1 [cs.LG])

    [http://arxiv.org/abs/2309.16131](http://arxiv.org/abs/2309.16131)

    提出了一种使用谱展开学习时空微分方程的方法，不依赖于空间离散化，可以处理无界的时空方程。这个方法被证明与最新的机器学习方法一样准确，并可应用于更大的问题类别。

    

    快速发展的机器学习方法对从观测数据中计算重构微分方程（DE）的研究充满了兴趣，这可以提供关于潜在因果机制的额外洞察。本文提出了一种基于神经ODE的新方法，利用空间中的谱展开来学习时空DE。我们谱神经DE学习方法的主要优势是它不依赖于空间离散化，因此允许目标时空方程包含长范围、非局部的空间相互作用，并作用于无界的空间域。我们的谱方法被证明与一些最新的机器学习方法一样准确，可以用于学习作用于有界域的PDE。通过为学习PDE和积分微分方程开发一个谱框架，我们将机器学习方法扩展到适用于无界DE和更大类别的问题。

    Rapidly developing machine learning methods has stimulated research interest in computationally reconstructing differential equations (DEs) from observational data which may provide additional insight into underlying causative mechanisms. In this paper, we propose a novel neural-ODE based method that uses spectral expansions in space to learn spatiotemporal DEs. The major advantage of our spectral neural DE learning approach is that it does not rely on spatial discretization, thus allowing the target spatiotemporal equations to contain long range, nonlocal spatial interactions that act on unbounded spatial domains. Our spectral approach is shown to be as accurate as some of the latest machine learning approaches for learning PDEs operating on bounded domains. By developing a spectral framework for learning both PDEs and integro-differential equations, we extend machine learning methods to apply to unbounded DEs and a larger class of problems.
    
[^85]: ModuLoRA:通过与模块化量化器集成在消费级GPU上对3 Bit LLMs进行微调

    ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers. (arXiv:2309.16119v1 [cs.LG])

    [http://arxiv.org/abs/2309.16119](http://arxiv.org/abs/2309.16119)

    ModuLoRA提出了一种内存高效、能够在消费级GPU上支持3比特LLMs微调的方法，并通过与模块化量化器的集成实现了竞争性能和更少的内存使用。

    

    我们提出了一种内存高效的大型语言模型（LLMs）微调算法，可支持在仅使用1个48GB GPU上以3比特或4比特精度微调具有65B参数的LLMs。我们的方法——模块化低秩自适应（ModuLoRA），通过低秩适配器（LoRA）将任何用户指定的权重量化器与微调集成。我们的方法依赖于一个简单的量化无关的反向传播，通过自定义的黑盒量化模块从低精度LLM权重中自适应地生成权重。这种方法使得首次能够进行3比特LLMs的微调，利用先进的3比特OPTQ量化往往优于依赖于较不复杂的4比特和8比特方法的微调。在我们的实验中，ModuLoRA在文本分类、自然语言推理和指令跟随任务中取得了有竞争力的性能，使用的内存比现有方法少很多，并且在一个流行的摘要任务上超过了最先进的ROUGE分数。

    We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit precision on as little as one 48GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 3-bit LLMs for the first time--leveraging state-of-the-art 3-bit OPTQ quantization often outperforms finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. W
    
[^86]: D$^3$Fields: 动态三维描述符场用于零样本可泛化机器人操作

    D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation. (arXiv:2309.16118v1 [cs.RO])

    [http://arxiv.org/abs/2309.16118](http://arxiv.org/abs/2309.16118)

    D$^3$Fields是一个动态的三维描述符场，将底层三维环境的动态特性以及语义特征和实例掩模编码起来。它可以灵活地使用不同背景、风格和实例的二维图像指定目标，实现零样本机器人操作任务的可泛化。

    

    场景表示是机器人操作系统中一个关键的设计选择。一个理想的表示应该是三维的、动态的和语义化的，以满足不同操作任务的需求。然而，先前的工作往往同时缺乏这三个属性。在这项工作中，我们介绍了D$^3$Fields动态三维描述符场。这些场捕捉了底层三维环境的动态特性，编码了语义特征和实例掩模。具体而言，我们将工作区域中的任意三维点投影到多视角的二维视觉观察中，并插值从基本模型中得到的特征。由此得到的融合描述符场可以使用具有不同背景、风格和实例的二维图像灵活地指定目标。为了评估这些描述符场的有效性，我们以零样本方式将我们的表示应用于各种机器人操作任务。通过在真实场景和模拟中的广泛评估，我们展示了该方法的有效性。

    Scene representation has been a crucial design choice in robotic manipulation systems. An ideal representation should be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D$^3$Fields dynamic 3D descriptor fields. These fields capture the dynamics of the underlying 3D environment and encode both semantic features and instance masks. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to a wide range of robotic manipulation tasks in a zero-shot manner. Through extensive evaluation in both real-world scenarios and simulations, we demonst
    
[^87]: E2Net: 弹性扩展网络实现资源高效的持续学习

    E2Net: Resource-Efficient Continual Learning with Elastic Expansion Network. (arXiv:2309.16117v1 [cs.LG])

    [http://arxiv.org/abs/2309.16117](http://arxiv.org/abs/2309.16117)

    E2Net是一种资源高效的持续学习方法，通过核心子网蒸馏和精确的回放样本选择，实现了卓越的准确性和较小的遗忘，在相同的计算和存储限制下最大程度地减少了处理时间。

    

    持续学习方法旨在学习新任务而不消除以前的知识。然而，持续学习通常需要大量的计算能力和存储容量才能达到令人满意的性能。在本文中，我们提出了一种资源高效的持续学习方法，称为弹性扩展网络（E2Net）。通过核心子网蒸馏和精确的回放样本选择，E2Net在相同的计算和存储限制下实现了卓越的平均准确性和较小的遗忘，并最大程度地减少了处理时间。在E2Net中，我们提出了代表性网络蒸馏，通过评估参数数量和与工作网络的输出相似性来识别代表性的核心子网，蒸馏工作网络内的类似子网以减轻对重演缓冲区的依赖，并促进跨先前任务的知识转移。为了提高存储资源利用率，我们还提出了子网约束经验回放方法。

    Continual Learning methods are designed to learn new tasks without erasing previous knowledge. However, Continual Learning often requires massive computational power and storage capacity for satisfactory performance. In this paper, we propose a resource-efficient continual learning method called the Elastic Expansion Network (E2Net). Leveraging core subnet distillation and precise replay sample selection, E2Net achieves superior average accuracy and diminished forgetting within the same computational and storage constraints, all while minimizing processing time. In E2Net, we propose Representative Network Distillation to identify the representative core subnet by assessing parameter quantity and output similarity with the working network, distilling analogous subnets within the working network to mitigate reliance on rehearsal buffers and facilitating knowledge transfer across previous tasks. To enhance storage resource utilization, we then propose Subnet Constraint Experience Replay t
    
[^88]: 迭代生成过程的组合塑造

    Compositional Sculpting of Iterative Generative Processes. (arXiv:2309.16115v1 [cs.LG])

    [http://arxiv.org/abs/2309.16115](http://arxiv.org/abs/2309.16115)

    本文提出了一种称为组合塑造的通用方法，用于定义迭代生成过程的组合，通过分类器指导实现对这些组合的采样。展示了在GFlowNets和扩散模型中实现组合塑造的方法，并提出了两种二元运算的推广。

    

    生成模型的高训练成本和为特定任务进行微调的需求，引发了对模型重用和组合的浓厚兴趣。组合迭代生成过程(如GFlowNets和扩散模型)的关键挑战在于，为了实现所需的目标分布，生成过程的所有步骤都需要协调，并满足微妙的平衡条件。本文提出了一种称为组合塑造的通用方法，用于定义迭代生成过程的组合。然后，我们介绍了一种基于分类器指导的从这些组合中进行采样的方法。我们展示了如何在GFlowNets和扩散模型中实现组合塑造。我们强调了两种二元运算——调和平均($p_1 \otimes p_2$)和对比度($p_1 \unicode{x25D1}\,p_2$)之间的运算，以及将这些运算推广到多个组分分布的方法。我们提供了关于图像和...

    High training costs of generative models and the need to fine-tune them for specific tasks have created a strong interest in model reuse and composition. A key challenge in composing iterative generative processes, such as GFlowNets and diffusion models, is that to realize the desired target distribution, all steps of the generative process need to be coordinated, and satisfy delicate balance conditions. In this work, we propose Compositional Sculpting: a general approach for defining compositions of iterative generative processes. We then introduce a method for sampling from these compositions built on classifier guidance. We showcase ways to accomplish compositional sculpting in both GFlowNets and diffusion models. We highlight two binary operations $\unicode{x2014}$ the harmonic mean ($p_1 \otimes p_2$) and the contrast ($p_1 \unicode{x25D1}\,p_2$) between pairs, and the generalization of these operations to multiple component distributions. We offer empirical results on image and m
    
[^89]: 使用高斯过程或贝叶斯神经网络驱动的主动学习性能比较用于受限轨迹探索的论文翻译

    Comparing Active Learning Performance Driven by Gaussian Processes or Bayesian Neural Networks for Constrained Trajectory Exploration. (arXiv:2309.16114v1 [cs.RO])

    [http://arxiv.org/abs/2309.16114](http://arxiv.org/abs/2309.16114)

    本文比较了使用高斯过程或贝叶斯神经网络驱动的主动学习算法在受限场景中的性能差异。

    

    自主机器人通过增加自主性来推动我们的太空探测能力，尤其是用于代替人类探险者进行现场探索和取样。目前，人类驱动机器人以满足科学目标，但根据机器人的位置，人类操作员和机器人之间的信息交流和驱动命令可能会导致任务完成的不必要延迟。编码有科学目标和探索策略的自主机器人不会出现通信延迟，可以更快地完成任务。主动学习算法提供了智能探索的能力，但底层的模型结构会影响主动学习算法在准确形成对环境的理解方面的性能。本文研究了使用高斯过程或贝叶斯神经网络驱动的主动学习算法在探索策略上的性能差异，此探索策略编码在受限场景中的代理上。

    Robots with increasing autonomy progress our space exploration capabilities, particularly for in-situ exploration and sampling to stand in for human explorers. Currently, humans drive robots to meet scientific objectives, but depending on the robot's location, the exchange of information and driving commands between the human operator and robot may cause undue delays in mission fulfillment. An autonomous robot encoded with a scientific objective and an exploration strategy incurs no communication delays and can fulfill missions more quickly. Active learning algorithms offer this capability of intelligent exploration, but the underlying model structure varies the performance of the active learning algorithm in accurately forming an understanding of the environment. In this paper, we investigate the performance differences between active learning algorithms driven by Gaussian processes or Bayesian neural networks for exploration strategies encoded on agents that are constrained in their 
    
[^90]: 特征归一化防止非对比学习动力的崩溃

    Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics. (arXiv:2309.16109v1 [cs.LG])

    [http://arxiv.org/abs/2309.16109](http://arxiv.org/abs/2309.16109)

    本论文研究了非对比学习中的动力崩溃问题，发现特征归一化可以防止此问题的出现，为解决自监督表示学习的计算效率提供了新的思路。

    

    对比学习是一种自监督表示学习框架，通过数据增强生成的两个正视图在数据表示空间中通过吸引力使它们相似，而通过排斥力使它们远离负样本。非对比学习通过BYOL和SimSiam等手段去除了负样本，并提高了计算效率。虽然由于缺乏排斥力，学到的表示可能会崩溃成一个单点，但田等人（2021）通过学习动力分析揭示，如果数据增强足够强于正则化，则表示可以避免崩溃。然而，他们的分析没有考虑常用的特征归一化，即在衡量表示相似性之前进行的归一化操作，因此过强的正则化可能会导致动力崩溃，这在特征归一化存在的情况下是不自然的行为。

    Contrastive learning is a self-supervised representation learning framework, where two positive views generated through data augmentation are made similar by an attraction force in a data representation space, while a repulsive force makes them far from negative examples. Non-contrastive learning, represented by BYOL and SimSiam, further gets rid of negative examples and improves computational efficiency. While learned representations may collapse into a single point due to the lack of the repulsive force at first sight, Tian et al. (2021) revealed through the learning dynamics analysis that the representations can avoid collapse if data augmentation is sufficiently stronger than regularization. However, their analysis does not take into account commonly-used feature normalization, a normalizer before measuring the similarity of representations, and hence excessively strong regularization may collapse the dynamics, which is an unnatural behavior under the presence of feature normalizat
    
[^91]: 频道视觉Transformer：一张图值C x 16 x 16个词

    Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])

    [http://arxiv.org/abs/2309.16108](http://arxiv.org/abs/2309.16108)

    本文提出了ChannelViT模型，通过对ViT架构的修改和引入分层通道采样技术，增强了对多通道图像的推理能力和鲁棒性，适用于显微镜和卫星成像等领域。

    

    视觉Transformer在现代计算机视觉领域中已经成为一种强大的架构。然而，它在某些图像领域的应用，如显微镜和卫星成像，面临着独特的挑战。在这些领域中，图像通常包含多个通道，每个通道都携带着语义上不同和独立的信息。此外，模型必须对输入通道的稀疏性表现出鲁棒性，在训练或测试过程中可能没有密集可用的通道。在本文中，我们提出了对ViT架构的修改，增强了对输入通道之间的推理，并引入了分层通道采样(HCS)作为一种附加的正则化技术，以确保在测试过程中仅出现部分通道时的鲁棒性。我们提出的模型ChannelViT独立地构建补丁令牌并利用可学习的通道嵌入将其添加到补丁令牌中，类似于位置嵌入。我们进行了评估

    Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
    
[^92]: 差分隐私安全乘法：在噪声中隐藏信息

    Differentially Private Secure Multiplication: Hiding Information in the Rubble of Noise. (arXiv:2309.16105v1 [cs.IT])

    [http://arxiv.org/abs/2309.16105](http://arxiv.org/abs/2309.16105)

    本文研究了在分布式计算中，允许信息泄漏和近似乘法的情况下，当诚实节点数量为少数时，差分隐私和准确性之间的权衡关系。

    

    我们考虑私密分布式多方乘法的问题。已经确认，Shamir秘密共享编码策略可以通过Ben Or，Goldwasser，Wigderson算法（“BGW算法”）在分布式计算中实现完美的信息理论隐私。然而，完美的隐私和准确性需要一个诚实的多数，即需要$N \geq 2t+1$个计算节点以确保对抗性节点的隐私。我们通过允许一定量的信息泄漏和近似乘法来研究在诚实节点数量为少数时的编码方案，即$N< 2t+1$。我们通过使用差分隐私而不是完美隐私来测量信息泄漏，并使用均方误差度量准确性，对$N < 2t+1$的情况下的隐私-准确性权衡进行了紧密的刻画。一个新颖的技术方面是复杂地控制信息泄漏的细节。

    We consider the problem of private distributed multi-party multiplication. It is well-established that Shamir secret-sharing coding strategies can enable perfect information-theoretic privacy in distributed computation via the celebrated algorithm of Ben Or, Goldwasser and Wigderson (the "BGW algorithm"). However, perfect privacy and accuracy require an honest majority, that is, $N \geq 2t+1$ compute nodes are required to ensure privacy against any $t$ colluding adversarial nodes. By allowing for some controlled amount of information leakage and approximate multiplication instead of exact multiplication, we study coding schemes for the setting where the number of honest nodes can be a minority, that is $N< 2t+1.$ We develop a tight characterization privacy-accuracy trade-off for cases where $N < 2t+1$ by measuring information leakage using {differential} privacy instead of perfect privacy, and using the mean squared error metric for accuracy. A novel technical aspect is an intricately 
    
[^93]: 对抗样本可能是可以避免的：数据集中性在对抗鲁棒性中的作用

    Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness. (arXiv:2309.16096v1 [cs.LG])

    [http://arxiv.org/abs/2309.16096](http://arxiv.org/abs/2309.16096)

    本研究论证了数据分布的集中程度对于决定鲁棒分类器的存在与否至关重要，并展示了在数据分布集中在低维线性子空间的并集时，利用数据结构可以获得具有良好鲁棒性保证的分类器的方法。

    

    现代机器学习分类器对于对抗样本的敏感性引发了理论结果，暗示这些对抗样本可能是不可避免的。然而，这些结果可能过于一般化，无法应用于自然数据分布。事实上，人类在涉及视觉的任务中表现出相当的鲁棒性。这种明显的矛盾推动我们更深入地探索一个问题：对抗样本是否真的是不可避免的？在这项工作中，我们理论上证明了数据分布的一个关键属性——对输入空间的小容积子集的集中程度，决定了是否存在一个鲁棒分类器。我们进一步证明，在数据分布集中在低维线性子空间的并集时，利用数据结构自然地得到享有良好鲁棒性保证的分类器，改进了在特定范围内可证明认证方法。

    The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution -- concentration on small-volume subsets of the input space -- determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees, improving upon methods for provable certification in certain regimes.
    
[^94]: 基于任务导向的Koopman控制和对比编码器

    Task-Oriented Koopman-Based Control with Contrastive Encoder. (arXiv:2309.16077v1 [cs.RO])

    [http://arxiv.org/abs/2309.16077](http://arxiv.org/abs/2309.16077)

    该论文介绍了一种基于任务导向的Koopman控制方法，利用对比编码器和端到端强化学习来同时学习Koopman潜在嵌入、算子和相关线性控制器。通过优先考虑任务成本作为主要目标，减少了对于明确定义模型的控制器设计的依赖，将Koopman控制扩展到高维、复杂非线性系统，包括基于像素的场景。

    

    我们提出了一种利用端到端强化学习和对比编码器同时学习Koopman潜在嵌入，算子和相关线性控制器的任务导向Koopman控制方法。通过将任务成本作为主要目标进行控制器学习，我们减少了对于一个明确定义的模型的控制器设计的依赖，将Koopman控制扩展到包括基于像素的场景在内的高维、复杂非线性系统中。

    We present task-oriented Koopman-based control that utilizes end-to-end reinforcement learning and contrastive encoder to simultaneously learn the Koopman latent embedding, operator and associated linear controller within an iterative loop. By prioritizing the task cost as main objective for controller learning, we reduce the reliance of controller design on a well-identified model, which extends Koopman control beyond low-dimensional systems to high-dimensional, complex nonlinear systems, including pixel-based scenarios.
    
[^95]: 通过反向强化学习从示范中推理和调整：双足动作奖励学习

    Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning. (arXiv:2309.16074v1 [cs.RO])

    [http://arxiv.org/abs/2309.16074](http://arxiv.org/abs/2309.16074)

    本文通过反向强化学习方法解决复杂地形上的双足行走问题。我们提出了学习专家奖励函数的算法，并分析了学习到的函数。实验证明使用推理出的奖励函数可以提高双足行走策略的行走性能。

    

    使双足行走机器人学习如何在高度不平坦、动态变化的地形上行进是具有挑战性的，这是由于机器人动力学和相互作用环境的复杂性。最近示范学习的进展在复杂环境中展示了机器人学习的良好结果。虽然模仿学习专家策略的研究已经得到了很好的探索，但在腿部运动中学习专家奖励函数的研究在很大程度上仍未得到充分探索。本文将先进的反向强化学习(IRL)技术引入解决双足动作问题。我们提出了学习专家奖励函数的算法，并对学习到的函数进行了分析。通过非线性函数逼近，我们发现了专家的动作策略中有意义的见解。此外，我们通过实验证明，使用推理出的奖励函数训练双足行走策略可以提高其行走性能。

    Enabling bipedal walking robots to learn how to maneuver over highly uneven, dynamically changing terrains is challenging due to the complexity of robot dynamics and interacted environments. Recent advancements in learning from demonstrations have shown promising results for robot learning in complex environments. While imitation learning of expert policies has been well-explored, the study of learning expert reward functions is largely under-explored in legged locomotion. This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems over complex terrains. We propose algorithms for learning expert reward functions, and we subsequently analyze the learned functions. Through nonlinear function approximation, we uncover meaningful insights into the expert's locomotion strategies. Furthermore, we empirically demonstrate that training a bipedal locomotion policy with the inferred reward functions enhances its walking performance on
    
[^96]: 医学髋关节X射线图像中的标记点检测的标签增强方法

    Label Augmentation Method for Medical Landmark Detection in Hip Radiograph Images. (arXiv:2309.16066v1 [cs.LG])

    [http://arxiv.org/abs/2309.16066](http://arxiv.org/abs/2309.16066)

    该论文提出了一种适用于医学髋关节X射线图像中的标记点检测任务的标签增强方法。通过使用仅标签增强方案进行训练，该方法超越了传统的数据增强方法，在样本利用效率上表现出色，可提高标记点检测的准确性。

    

    本文报道了一种用于预测髋关节X射线图像中临床标记点的自动化医学标记点检测方法的实证性能。值得注意的是，该检测方法是使用仅标签增强方案进行训练的；我们的结果表明，这种增强形式优于传统的数据增强，并且产生高效的样本估计器。我们使用基于通用U-Net架构的课程训练，该训练包括两个阶段：首先通过将标记点扩大到区域来放松标记任务，然后逐渐将这些标签区域回归到基本任务。我们在含有黄金标准专家注释的六个放射图像数据集上评估了这种方法的优势。

    This work reports the empirical performance of an automated medical landmark detection method for predict clinical markers in hip radiograph images. Notably, the detection method was trained using a label-only augmentation scheme; our results indicate that this form of augmentation outperforms traditional data augmentation and produces highly sample efficient estimators. We train a generic U-Net-based architecture under a curriculum consisting of two phases: initially relaxing the landmarking task by enlarging the label points to regions, then gradually eroding these label regions back to the base task. We measure the benefits of this approach on six datasets of radiographs with gold-standard expert annotations.
    
[^97]: 受屏蔽自动编码器学习细胞形态的可扩展性

    Masked autoencoders are scalable learners of cellular morphology. (arXiv:2309.16064v1 [cs.CV])

    [http://arxiv.org/abs/2309.16064](http://arxiv.org/abs/2309.16064)

    本研究探索了弱监督和自监督深度学习方法在训练更大的模型和数据集时的可扩展性，并发现基于CNN和ViT的受屏蔽自动编码器在推断细胞形态学关系方面明显优于弱监督模型。

    

    在高内容显微镜检查中从细胞表型中推断生物关系在生物研究中提供了重要的机会和挑战。之前的研究结果表明，深度视觉模型比手工设计的特征更能捕捉生物信号。本研究探讨了弱监督和自监督深度学习方法在训练更大的模型和更大的数据集时的可扩展性。我们的结果显示，基于CNN和ViT的受屏蔽自动编码器在性能上显著优于弱监督模型。在我们研究的最高尺度上，一个在公共数据库中构建的细胞形态学关系数据集上训练的覆盖超过35亿个唯一剪裁图像的ViT-L/8模型，在推断已知生物关系时相对改进高达28%。

    Inferring biological relationships from cellular phenotypes in high-content microscopy screens provides significant opportunity and challenge in biological research. Prior results have shown that deep vision models can capture biological signal better than hand-crafted features. This work explores how weakly supervised and self-supervised deep learning approaches scale when training larger models on larger datasets. Our results show that both CNN- and ViT-based masked autoencoders significantly outperform weakly supervised models. At the high-end of our scale, a ViT-L/8 trained on over 3.5-billion unique crops sampled from 95-million microscopy images achieves relative improvements as high as 28% over our best weakly supervised models at inferring known biological relationships curated from public databases.
    
[^98]: 使用数据驱动的机器学习模型预测COVID-19后患者的心血管并发症

    Predicting Cardiovascular Complications in Post-COVID-19 Patients Using Data-Driven Machine Learning Models. (arXiv:2309.16059v1 [cs.LG])

    [http://arxiv.org/abs/2309.16059](http://arxiv.org/abs/2309.16059)

    使用数据驱动的机器学习模型成功预测COVID-19后患者的心血管并发症，为及时干预和改善患者结果提供了希望。

    

    COVID-19流行病全球性地带来了许多健康挑战，其中最重要的是COVID-19后出现的心血管并发症。本研究利用数据驱动的机器学习模型来预测伊拉克352名COVID-19后患者的这些并发症。收集了包括人口统计学数据、共病症、实验室结果和影像学在内的临床数据，并用于构建预测模型。这些模型利用各种机器学习算法，展示了在识别处于风险中的患者方面可观的性能。通过这些模型的早期检测，可以实现及时干预和改善结果。总之，本研究强调了数据驱动的机器学习在预测COVID-19后心血管并发症方面的潜力，并强调有必要在不同的临床环境中继续验证和研究。

    The COVID-19 pandemic has globally posed numerous health challenges, notably the emergence of post-COVID-19 cardiovascular complications. This study addresses this by utilizing data-driven machine learning models to predict such complications in 352 post-COVID-19 patients from Iraq. Clinical data, including demographics, comorbidities, lab results, and imaging, were collected and used to construct predictive models. These models, leveraging various machine learning algorithms, demonstrated commendable performance in identifying patients at risk. Early detection through these models promises timely interventions and improved outcomes. In conclusion, this research underscores the potential of data-driven machine learning for predicting post-COVID-19 cardiovascular complications, emphasizing the need for continued validation and research in diverse clinical settings.
    
[^99]: AnyMAL:一种高效可扩展的任意模态增强语言模型

    AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model. (arXiv:2309.16058v1 [cs.LG])

    [http://arxiv.org/abs/2309.16058](http://arxiv.org/abs/2309.16058)

    AnyMAL是一种高效可扩展的任意模态增强语言模型，能够处理多样化的输入模态信号，并在各种多模态任务上表现出最先进的性能。

    

    我们提出了Any-Modality Augmented Language Model (AnyMAL)，它是一个统一的模型，可以处理多样化的输入模态信号（包括文本、图像、视频、音频、IMU运动传感器），并生成文本响应。AnyMAL继承了最先进的LLMs（包括LLaMA-2（70B））的强大文本推理能力，并通过预训练的对齐模块将模态特定信号转换为联合文本空间。为进一步增强多模态LLM的能力，我们使用手动收集的多模态指令集对模型进行微调，以涵盖简单问答以外的各种主题和任务。我们进行了全面的实证分析，包括人工和自动评估，并展示了在各种多模态任务上的最先进性能。

    We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM's capabilities, we fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs. We conduct comprehensive empirical analysis comprising both human and automatic evaluations, and demonstrate state-of-the-art performance on various multimodal tasks.
    
[^100]: 鉴别COVID-19后精神健康障碍的风险因素：机器学习视角

    Identifying Risk Factors for Post-COVID-19 Mental Health Disorders: A Machine Learning Perspective. (arXiv:2309.16055v1 [cs.LG])

    [http://arxiv.org/abs/2309.16055](http://arxiv.org/abs/2309.16055)

    本研究利用机器学习技术鉴别了COVID-19后精神健康障碍的风险因素，发现年龄、性别、居住地地理区域、合并症、COVID-19疾病的严重程度以及心理社会因素是发展精神健康障碍的重要因素。医疗服务提供者和政策制定者应考虑这些风险因素来设计有针对性的干预措施和支持系统。

    

    在本研究中，我们利用机器学习技术识别了与COVID-19后精神健康障碍相关的风险因素。通过对伊拉克各省669名患者的数据分析，我们获得了有价值的洞见。我们发现年龄、性别和居住地地理区域是影响COVID-19后患者发展精神健康障碍可能性的重要人口统计因素。此外，合并症和COVID-19疾病的严重程度也是重要的临床预测因子。心理社会因素，如社会支持、应对策略和主观压力水平，也起到了重要作用。我们的发现强调了COVID-19康复后精神健康障碍发展中多个因素的复杂相互作用。医疗服务提供者和政策制定者在制定针对患者的有针对性干预和支持系统时应考虑这些风险因素。基于机器学习的方法

    In this study, we leveraged machine learning techniques to identify risk factors associated with post-COVID-19 mental health disorders. Our analysis, based on data collected from 669 patients across various provinces in Iraq, yielded valuable insights. We found that age, gender, and geographical region of residence were significant demographic factors influencing the likelihood of developing mental health disorders in post-COVID-19 patients. Additionally, comorbidities and the severity of COVID-19 illness were important clinical predictors. Psychosocial factors, such as social support, coping strategies, and perceived stress levels, also played a substantial role. Our findings emphasize the complex interplay of multiple factors in the development of mental health disorders following COVID-19 recovery. Healthcare providers and policymakers should consider these risk factors when designing targeted interventions and support systems for individuals at risk. Machine learning-based approach
    
[^101]: 改进的精细离散化方法提高自适应在线学习

    Improving Adaptive Online Learning Using Refined Discretization. (arXiv:2309.16044v1 [cs.LG])

    [http://arxiv.org/abs/2309.16044](http://arxiv.org/abs/2309.16044)

    通过一种新颖的连续时间启发式算法，提高了自适应在线学习的效果，将梯度方差的依赖性从次优的$O(\sqrt{V_T\log V_T})$改进到最优速率$O(\sqrt{V_T})$，并可适用于未知Lipschitz常数的情况。

    

    我们研究了具有Lipschitz损失的非约束在线线性优化问题。目标是同时达到（i）二阶梯度自适应性；和（ii）比较器范数自适应性，也被称为文献中的“参数自由性”。现有的遗憾界（Cutkosky和Orabona，2018；Mhammedi和Koolen，2020；Jacobsen和Cutkosky，2022）对于梯度方差$V_T$有次优的$O(\sqrt{V_T\log V_T})$依赖性，而本工作利用一种新颖的连续时间启发式算法将其改进为最优速率$O(\sqrt{V_T})$，而无需任何不切实际的加倍技巧。这一结果可以推广到未知Lipschitz常数的情况，消除了先前工作中的范围比率问题（Mhammedi和Koolen，2020）。具体来说，我们首先展示了在问题的连续时间类比中可以相当容易地实现目标的同时适应性，其中环境由任意连续半鞘式建模。然后，我们的关键创新是

    We study unconstrained Online Linear Optimization with Lipschitz losses. The goal is to simultaneously achieve ($i$) second order gradient adaptivity; and ($ii$) comparator norm adaptivity also known as "parameter freeness" in the literature. Existing regret bounds (Cutkosky and Orabona, 2018; Mhammedi and Koolen, 2020; Jacobsen and Cutkosky, 2022) have the suboptimal $O(\sqrt{V_T\log V_T})$ dependence on the gradient variance $V_T$, while the present work improves it to the optimal rate $O(\sqrt{V_T})$ using a novel continuous-time-inspired algorithm, without any impractical doubling trick. This result can be extended to the setting with unknown Lipschitz constant, eliminating the range ratio problem from prior works (Mhammedi and Koolen, 2020).  Concretely, we first show that the aimed simultaneous adaptivity can be achieved fairly easily in a continuous time analogue of the problem, where the environment is modeled by an arbitrary continuous semimartingale. Then, our key innovation 
    
[^102]: 《语言模型中激活路径修复的最佳实践：度量和方法》的论文翻译

    Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])

    [http://arxiv.org/abs/2309.16042](http://arxiv.org/abs/2309.16042)

    本研究系统地考察了激活路径修复中的方法细节对语言模型解释性结果的影响，并提出了最佳实践建议。

    

    机械解释性旨在理解机器学习模型的内部机制，其中定位-识别重要的模型组件是关键步骤。激活路径修复，也称为因果追踪或交换干预，是完成这一任务的标准技术，但文献中存在许多变体，对超参数或方法选择缺乏一致性。在这项工作中，我们系统地考察了激活路径修复中的方法细节对结果的影响，包括评估指标和损坏方法。在语言模型的定位和电路发现的几种设置中，我们发现不同的超参数可能导致不同的解释结果。通过经验观察支持，我们提出了为什么某些指标或方法可能更受欢迎的概念性论证。最后，我们提出了激活路径修复的最佳实践建议。

    Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.
    
[^103]: 基于原始数据的体内纳米尺度定位的分析建模

    Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization. (arXiv:2309.16034v1 [cs.ET])

    [http://arxiv.org/abs/2309.16034](http://arxiv.org/abs/2309.16034)

    本论文研究了基于原始数据的体内纳米尺度定位的分析建模，分析了纳米设备的通信和能源约束对定位性能的影响。

    

    纳米技术和材料科学的进展为纳米尺度设备的发展铺平了道路，这些设备结合了传感、计算、数据和能源储存以及无线通信。在精密医学中，这些纳米设备对于疾病诊断、治疗和监测呈现出巨大的潜力，而体内纳米尺度定位的流引导定位，即将所感知的生物事件与事件本身的位置关联起来，从精密医学的角度看将具有极大的益处。纳米设备的纳米尺度特性以及血液流动环境的挑战性导致目前的流引导定位方法在通信和能源相关能力方面受到限制。纳米设备的通信和能源约束导致流引导定位的原始数据具有不同的特征，从而影响其性能。本研究通过分析建模研究了这些不完美的影响。

    Advancements in nanotechnology and material science are paving the way toward nanoscale devices that combine sensing, computing, data and energy storage, and wireless communication. In precision medicine, these nanodevices show promise for disease diagnostics, treatment, and monitoring from within the patients' bloodstreams. Assigning the location of a sensed biological event with the event itself, which is the main proposition of flow-guided in-body nanoscale localization, would be immensely beneficial from the perspective of precision medicine. The nanoscale nature of the nanodevices and the challenging environment that the bloodstream represents, result in current flow-guided localization approaches being constrained in their communication and energy-related capabilities. The communication and energy constraints of the nanodevices result in different features of raw data for flow-guided localization, in turn affecting its performance. An analytical modeling of the effects of imperfe
    
[^104]: 学习耗散神经动力系统

    Learning Dissipative Neural Dynamical Systems. (arXiv:2309.16032v1 [cs.LG])

    [http://arxiv.org/abs/2309.16032](http://arxiv.org/abs/2309.16032)

    本文提出了一种学习耗散神经动力系统模型的方法，该方法分为两个阶段。首先学习一个无约束的模型，然后导出条件来保证模型的耗散性质，并在保持逼近能力的同时扰动偏差。通过独立求解这两个扰动问题，得到一个保证为耗散的神经动力模型，同时紧密逼近非线性系统。

    

    本文考虑一个未知的非线性动力系统，其已知是耗散的。本文的目标是学习一个神经动力模型，以逼近这个系统，并在模型中保持耗散性质。一般来说，在神经网络训练过程中施加耗散性约束是一个困难的问题，目前没有已知的技术可以解决。本文分为两个阶段解决了学习耗散神经动力系统模型的问题。首先，我们学习一个无约束的神经动力模型，该模型能够紧密逼近系统的动力学行为。然后，我们导出足够的条件来扰动神经动力模型的权重，以确保耗散性质，并在保持模型与非线性系统轨迹拟合的同时扰动偏差。我们证明了这两个扰动问题可以独立求解，从而获得一个保证为耗散的神经动力模型，同时紧密逼近非线性系统。

    Consider an unknown nonlinear dynamical system that is known to be dissipative. The objective of this paper is to learn a neural dynamical model that approximates this system, while preserving the dissipativity property in the model. In general, imposing dissipativity constraints during neural network training is a hard problem for which no known techniques exist. In this work, we address the problem of learning a dissipative neural dynamical system model in two stages. First, we learn an unconstrained neural dynamical model that closely approximates the system dynamics. Next, we derive sufficient conditions to perturb the weights of the neural dynamical model to ensure dissipativity, followed by perturbation of the biases to retain the fit of the model to the trajectories of the nonlinear system. We show that these two perturbation problems can be solved independently to obtain a neural dynamical model that is guaranteed to be dissipative while closely approximating the nonlinear syst
    
[^105]: 符号化模仿学习：从黑盒到可解释的驾驶策略

    Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies. (arXiv:2309.16025v1 [cs.LG])

    [http://arxiv.org/abs/2309.16025](http://arxiv.org/abs/2309.16025)

    本文介绍了一种名为符号化模仿学习（SIL）的方法，通过引入归纳逻辑编程（ILP）来学习从现有数据集中获取透明、可解释和泛化的驾驶策略。与传统的基于深度神经网络的模仿学习方法相比，SIL不仅提高了驾驶策略的可解释性，还显著改进了它们在各种驾驶情况下的适用性。

    

    当前的模仿学习方法主要基于深度神经网络，提供了从现实世界数据中获取驾驶策略的有效手段，但在可解释性和泛化性方面存在显著局限性。这些缺点在自动驾驶等安全关键应用中尤为令人担忧。本文通过引入符号化模仿学习（SIL），一种使用归纳逻辑编程（ILP）学习从可用数据集中获取透明、可解释和泛化的驾驶策略的创新方法，来解决这些局限性。利用真实世界的highD数据集，我们对我们的方法进行了严格的比较分析，与当前的基于神经网络的模仿学习方法进行了对比。我们的结果表明，SIL不仅提高了驾驶策略的可解释性，还显著提高了它们在各种驾驶情况下的适用性。因此，这项工作为实现更可靠和可解释的驾驶策略打开了一条新的途径。

    Current methods of imitation learning (IL), primarily based on deep neural networks, offer efficient means for obtaining driving policies from real-world data but suffer from significant limitations in interpretability and generalizability. These shortcomings are particularly concerning in safety-critical applications like autonomous driving. In this paper, we address these limitations by introducing Symbolic Imitation Learning (SIL), a groundbreaking method that employs Inductive Logic Programming (ILP) to learn driving policies which are transparent, explainable and generalisable from available datasets. Utilizing the real-world highD dataset, we subject our method to a rigorous comparative analysis against prevailing neural-network-based IL methods. Our results demonstrate that SIL not only enhances the interpretability of driving policies but also significantly improves their applicability across varied driving situations. Hence, this work offers a novel pathway to more reliable an
    
[^106]: GNNHLS: 通过高级综合评估图神经网络推断

    GNNHLS: Evaluating Graph Neural Network Inference via High-Level Synthesis. (arXiv:2309.16022v1 [cs.LG])

    [http://arxiv.org/abs/2309.16022](http://arxiv.org/abs/2309.16022)

    GNNHLS是一个通过高级综合评估FPGAs上的图神经网络推断的开源框架，能够实现高速加速和能量降低。

    

    随着图神经网络（GNNs）的逐渐流行，高效的GNN推断引起了极大关注。由于其精细级并行性、低功耗、可重构性和并发执行的特点，Field-Programming Gate Arrays (FPGAs) 是一个有前途的执行平台。更好的是，高级综合（HLS）工具弥补了非常规的FPGA开发工作和新GNN模型的快速涌现之间的差距。在本文中，我们提出了GNNHLS，一个开源框架，通过HLS全面评估FPGAs上的GNN推断加速，包括用于数据生成和基准部署的软件栈以及6个经过良好调优的GNN HLS内核的FPGA实现。我们使用4个具有不同拓扑结构和规模的图数据集对GNNHLS进行评估。结果表明，与CPU基准相比，GNNHLS实现了高达50.8倍的加速和423倍的能量降低。与GPU基准相比，GNNHLS实现了高达5.16倍的加速和74.5倍的能量降低。

    With the ever-growing popularity of Graph Neural Networks (GNNs), efficient GNN inference is gaining tremendous attention. Field-Programming Gate Arrays (FPGAs) are a promising execution platform due to their fine-grained parallelism, low-power consumption, reconfigurability, and concurrent execution. Even better, High-Level Synthesis (HLS) tools bridge the gap between the non-trivial FPGA development efforts and rapid emergence of new GNN models. In this paper, we propose GNNHLS, an open-source framework to comprehensively evaluate GNN inference acceleration on FPGAs via HLS, containing a software stack for data generation and baseline deployment, and FPGA implementations of 6 well-tuned GNN HLS kernels. We evaluate GNNHLS on 4 graph datasets with distinct topologies and scales. The results show that GNNHLS achieves up to 50.8x speedup and 423x energy reduction relative to the CPU baselines. Compared with the GPU baselines, GNNHLS achieves up to 5.16x speedup and 74.5x energy reductio
    
[^107]: GeoCLIP：受Clip启发的地点和图像对齐方法，用于有效的全球地理定位

    GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization. (arXiv:2309.16020v1 [cs.CV])

    [http://arxiv.org/abs/2309.16020](http://arxiv.org/abs/2309.16020)

    GeoCLIP是一种受Clip启发的图像到GPS检索方法，用于全球地理定位。它通过对齐图像和其对应的GPS位置来提高定位精度，并克服了传统方法中固定分类的局限性。

    

    全球地理定位旨在确定遥感图像的精确位置。由于地理景观的巨大变化，这项任务面临着巨大的挑战。 基于图像检索的方法无法在全球范围内解决这个问题，因为构建涵盖整个世界的大型图像库是不可行的。 相反，现有方法将全球划分为离散的地理单元，将问题转化为分类任务。 然而，这些方法的性能受到预定义类别的限制，并且当图像的位置与其类别中心显著偏离时，往往导致不准确的定位。 为了克服这些限制，我们提出了GeoCLIP，一种新颖的受Clip启发的图像到GPS检索方法，强制进行图像与其对应的GPS位置之间的对齐。 GeoCLIP的位置编码器通过使用随机傅里叶特征进行位置编码，将地球建模为连续函数。

    Worldwide Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth. This task has considerable challenges due to immense variation in geographic landscapes. The image-to-image retrieval-based approaches fail to solve this problem on a global scale as it is not feasible to construct a large gallery of images covering the entire world. Instead, existing approaches divide the globe into discrete geographic cells, transforming the problem into a classification task. However, their performance is limited by the predefined classes and often results in inaccurate localizations when an image's location significantly deviates from its class center. To overcome these limitations, we propose GeoCLIP, a novel CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between the image and its corresponding GPS locations. GeoCLIP's location encoder models the Earth as a continuous function by employing positional encoding through random Fourier features an
    
[^108]: 用联合嵌入预测架构进行图级表示学习

    Graph-level Representation Learning with Joint-Embedding Predictive Architectures. (arXiv:2309.16014v1 [cs.LG])

    [http://arxiv.org/abs/2309.16014](http://arxiv.org/abs/2309.16014)

    本文提出了一种用于图级表示学习的联合嵌入预测架构（JEPA），通过预测输入图的不同子图的潜在表示在2维单位双曲线上的坐标，实现了对图级表示的有效建模。

    

    联合嵌入预测架构（JEPAs）作为一种新颖而强大的自监督表示学习技术最近出现。它们旨在通过从上下文信号x中预测目标信号y的潜在表示来学习基于能量的模型。JEPAs绕过了对数据增强和负样本的需求，这通常是对比学习所要求的，同时避免了与生成式预训练相关的过拟合问题。在本文中，我们展示了该范式可以有效地对图级表示进行建模，并提出了Graph-JEPA，这是图领域的第一个JEPA。特别是，我们采用掩码建模的方式来学习输入图的不同子图的嵌入。为了赋予表示隐含的层次结构，我们设计了一种替代性的训练目标，该目标是预测编码子图在2维单位双曲线上的坐标。

    Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal $y$ from a context signal $x$. JEPAs bypass the need for data augmentation and negative samples, which are typically required by contrastive learning, while avoiding the overfitting issues associated with generative-based pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm and propose Graph-JEPA, the first JEPA for the graph domain. In particular, we employ masked modeling to learn embeddings for different subgraphs of the input graph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative training objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2
    
[^109]: 基于数字孪生的课程学习在网络物理系统中的异常检测

    Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems. (arXiv:2309.15995v1 [cs.LG])

    [http://arxiv.org/abs/2309.15995](http://arxiv.org/abs/2309.15995)

    这篇论文提出了一种基于数字孪生的网络物理系统中的异常检测方法，通过引入课程学习优化学习范式，将训练数据进行难度得分采样。

    

    异常检测对于保证网络物理系统（CPS）的安全至关重要。然而，由于攻击和CPS本身的复杂性不断增加，CPS中的异常检测变得越来越具有挑战性。在我们之前的工作中，我们提出了一种基于数字孪生的异常检测方法，称为ATTAIN，它利用了CPS的历史和实时数据。然而，这些数据在难度上有显著的差异。因此，类似于人类学习过程，深度学习模型（如ATTAIN）可以从简单到困难的课程中受益。为此，在本文中，我们提出了一种名为数字孪生课程学习的新方法，即LATTICE，它通过引入课程学习来优化ATTAIN的学习范式。LATTICE为每个样本赋予一个难度得分，然后将其输入到训练调度程序中进行训练数据的批处理采样。

    Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores s
    
[^110]: 基于机器学习的步态分析在监测和管理下肢损伤中的应用分析

    Machine Learning Based Analytics for the Significance of Gait Analysis in Monitoring and Managing Lower Extremity Injuries. (arXiv:2309.15990v1 [cs.LG])

    [http://arxiv.org/abs/2309.15990](http://arxiv.org/abs/2309.15990)

    该研究使用机器学习模型分析步态数据，探讨了步态分析在评估下肢骨折患者并发症方面的潜力，并提出了一种处理类别不平衡的方法。最终结果表明，XGBoost是最佳模型。

    

    本研究探讨了步态分析作为一种评估下肢骨折患者术后并发症（如感染、错位或硬件刺激）的工具的潜力。研究重点是使用连续步态数据预测并发症的监督式机器学习模型。我们在一家学术中心确定了下肢骨折患者。患者使用胸部装置进行步态分析。使用软件对原始步态数据进行预处理，并强调12个重要的步态变量。训练、测试和评估了包括XGBoost、逻辑回归、支持向量机、LightGBM和随机森林在内的机器学习模型。注意到了类别不平衡的问题，并采用了SMOTE方法进行处理。我们引入了一种在步态分析之间的时间差异独立计算步态变量变化率（ROC）的方法。在应用SMOTE之前和之后，XGBoost都是最优的模型。

    This study explored the potential of gait analysis as a tool for assessing post-injury complications, e.g., infection, malunion, or hardware irritation, in patients with lower extremity fractures. The research focused on the proficiency of supervised machine learning models predicting complications using consecutive gait datasets. We identified patients with lower extremity fractures at an academic center. Patients underwent gait analysis with a chest-mounted IMU device. Using software, raw gait data was preprocessed, emphasizing 12 essential gait variables. Machine learning models including XGBoost, Logistic Regression, SVM, LightGBM, and Random Forest were trained, tested, and evaluated. Attention was given to class imbalance, addressed using SMOTE. We introduced a methodology to compute the Rate of Change (ROC) for gait variables, independent of the time difference between gait analyses. XGBoost was the optimal model both before and after applying SMOTE. Prior to SMOTE, the model ac
    
[^111]: 可微分密度泛函理论的开源基础设施

    Open Source Infrastructure for Differentiable Density Functional Theory. (arXiv:2309.15985v1 [cs.LG])

    [http://arxiv.org/abs/2309.15985](http://arxiv.org/abs/2309.15985)

    本研究构建了一个开源基础设施，用于训练可微分的交换相关泛函。通过借鉴多个团队的前沿技术，旨在规范化处理流程。已经在DeepChem库中开源了模型，为可微分量子化学方法的进一步研究提供了平台。

    

    近年来，从数据中学习量子化学计算中使用的交换相关泛函变得越来越重要，但训练这样的泛函需要复杂的软件基础设施。因此，我们构建了开源基础设施来训练神经网络交换相关泛函。我们旨在通过借鉴多个团队的前沿技术，规范化处理流程。我们已经在DeepChem库中开源了模型，为可微分量子化学方法的其他研究提供了一个平台。

    Learning exchange correlation functionals, used in quantum chemistry calculations, from data has become increasingly important in recent years, but training such a functional requires sophisticated software infrastructure. For this reason, we build open source infrastructure to train neural exchange correlation functionals. We aim to standardize the processing pipeline by adapting state-of-the-art techniques from work done by multiple groups. We have open sourced the model in the DeepChem library to provide a platform for additional research on differentiable quantum chemistry methods.
    
[^112]: TraCE: 轨迹反事实解释分数

    TraCE: Trajectory Counterfactual Explanation Scores. (arXiv:2309.15965v1 [cs.LG])

    [http://arxiv.org/abs/2309.15965](http://arxiv.org/abs/2309.15965)

    TraCE是一个模型无关的模块化框架，用于评估顺序决策任务中的进展。它能够将高度复杂场景中的进展凝练为一个单一值，并在医疗保健和气候变化领域展示了其实用性。

    

    反事实解释和相关算法补救通常被用于理解、解释和可能改变来自黑盒分类器的预测。在本文中，我们提出将反事实扩展应用于评估顺序决策任务中的进展。为此，我们引入了一个模型无关的模块化框架TraCE（轨迹反事实解释）分数，能够将高度复杂场景中的进展凝练为一个单一值。我们通过展示在涵盖医疗保健和气候变化两个案例研究中TraCE的实用性来证明其主要特点。

    Counterfactual explanations, and their associated algorithmic recourse, are typically leveraged to understand, explain, and potentially alter a prediction coming from a black-box classifier. In this paper, we propose to extend the use of counterfactuals to evaluate progress in sequential decision making tasks. To this end, we introduce a model-agnostic modular framework, TraCE (Trajectory Counterfactual Explanation) scores, which is able to distill and condense progress in highly complex scenarios into a single value. We demonstrate TraCE's utility across domains by showcasing its main properties in two case studies spanning healthcare and climate change.
    
[^113]: 一种基于正则化收缩预测的不确定性感知伪标签选择框架

    An Uncertainty-Aware Pseudo-Label Selection Framework using Regularized Conformal Prediction. (arXiv:2309.15963v1 [cs.LG])

    [http://arxiv.org/abs/2309.15963](http://arxiv.org/abs/2309.15963)

    本文提出了一种使用正则化收缩预测的不确定性感知伪标签选择框架，通过修复校准不佳的神经网络，减少噪声训练数据。

    

    一种基于一致性正则化方法的半监督学习算法在性能方面表现出色。然而，这些方法主要依赖于特定领域的数据增强，这在数据增强不实用的领域中不可用。另一方面，伪标签是一种通用的、不依赖于领域的半监督学习方法，与一致性正则化方法不同，它不依赖于领域。由于模型校准不准确而导致的高置信度错误预测使伪标签效果不佳。本文提出了一种基于不确定性感知的伪标签选择框架，利用一致性正则化算法产生的不确定性集合来修复校准不佳的神经网络，减少噪声训练数据。

    Consistency regularization-based methods are prevalent in semi-supervised learning (SSL) algorithms due to their exceptional performance. However, they mainly depend on domain-specific data augmentations, which are not usable in domains where data augmentations are less practicable. On the other hand, Pseudo-labeling (PL) is a general and domain-agnostic SSL approach that, unlike consistency regularization-based methods, does not rely on the domain. PL underperforms due to the erroneous high-confidence predictions from poorly calibrated models. This paper proposes an uncertainty-aware pseudo-label selection framework that employs uncertainty sets yielded by the conformal regularization algorithm to fix the poor calibration neural networks, reducing noisy training data. The codes of this work are available at: https://github.com/matinmoezzi/ups conformal classification
    
[^114]: 细节决定成败：深入探索数据过滤的兔子洞

    The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering. (arXiv:2309.15954v1 [cs.CV])

    [http://arxiv.org/abs/2309.15954](http://arxiv.org/abs/2309.15954)

    本文介绍了一种针对数据过滤的新的基准，包括单模态过滤、跨模态过滤和数据分布对齐等策略。通过综合现有方法和提出新的解决方案，改善了基础模型的性能，并提供了深入的分析和讨论。

    

    预训练数据的质量对基础模型的性能起着关键作用。流行的基础模型通常设计自己的数据过滤方法，这使得分析和比较不同的数据过滤方法变得困难。DataComp是一个新的基准，专门用于评估不同的数据过滤方法。本文描述了我们参与DataComp挑战时的学习和解决方案。我们的过滤策略包括三个阶段：单模态过滤、跨模态过滤和数据分布对齐。我们整合了现有的方法并提出了新的解决方案，例如在水平翻转图像上计算CLIP分数以减少场景文字的干扰，使用视觉和语言模型检索目标下游任务的训练样本，重新平衡数据分布以提高分配计算预算的效率等。我们对设计选择进行了详尽的分析和讨论。

    The quality of pre-training data plays a critical role in the performance of foundation models. Popular foundation models often design their own recipe for data filtering, which makes it hard to analyze and compare different data filtering approaches. DataComp is a new benchmark dedicated to evaluating different methods for data filtering. This paper describes our learning and solution when participating in the DataComp challenge. Our filtering strategy includes three stages: single-modality filtering, cross-modality filtering, and data distribution alignment. We integrate existing methods and propose new solutions, such as computing CLIP score on horizontally flipped images to mitigate the interference of scene text, using vision and language models to retrieve training samples for target downstream tasks, rebalancing the data distribution to improve the efficiency of allocating the computational budget, etc. We slice and dice our design choices, provide in-depth analysis, and discuss
    
[^115]: 统一的长期时序预测基准

    Unified Long-Term Time-Series Forecasting Benchmark. (arXiv:2309.15946v1 [cs.LG])

    [http://arxiv.org/abs/2309.15946](http://arxiv.org/abs/2309.15946)

    该论文介绍了一个专门用于长期时序预测的综合数据集，通过对多个经典和最先进的模型进行广泛基准分析，发现模型的有效性与数据集相关性有关。

    

    为了支持机器学习方法在预测时间序列数据方面的进展，我们提出了一个专门用于长期时序预测的综合数据集。我们采集了来自各种不同、动态系统和真实记录的数据集。每个数据集都经过标准化处理，分为训练和测试轨迹，并预先确定了回溯长度。我们包括了长度为2000的轨迹，以确保可靠地评估长期预测能力。为了确定在不同场景中最有效的模型，我们使用经典和最先进的模型（包括LSTM、DeepAR、NLinear、N-Hits、PatchTST和LatentODE）进行了广泛的基准分析。我们的研究结果显示了这些模型之间有趣的性能比较，突出了模型有效性与数据集相关性的特点。值得注意的是，我们引入了一种定制的潜在NLinear模型，并在DeepAR中增加了一个课程学习阶段。

    In order to support the advancement of machine learning methods for predicting time-series data, we present a comprehensive dataset designed explicitly for long-term time-series forecasting. We incorporate a collection of datasets obtained from diverse, dynamic systems and real-life records. Each dataset is standardized by dividing it into training and test trajectories with predetermined lookback lengths. We include trajectories of length up to $2000$ to ensure a reliable evaluation of long-term forecasting capabilities. To determine the most effective model in diverse scenarios, we conduct an extensive benchmarking analysis using classical and state-of-the-art models, namely LSTM, DeepAR, NLinear, N-Hits, PatchTST, and LatentODE. Our findings reveal intriguing performance comparisons among these models, highlighting the dataset-dependent nature of model effectiveness. Notably, we introduce a custom latent NLinear model and enhance DeepAR with a curriculum learning phase. Both consist
    
[^116]: 探索自监督对比学习空间声音事件表示的研究

    Exploring Self-Supervised Contrastive Learning of Spatial Sound Event Representation. (arXiv:2309.15938v1 [eess.AS])

    [http://arxiv.org/abs/2309.15938](http://arxiv.org/abs/2309.15938)

    该研究提出了一个简单的多通道对比学习框架，用于编码空间音频的“什么”和“哪里”。通过多级数据增强和通道增强方法，该框架在事件分类和声音定位方面优于有监督模型。

    

    在这项研究中，我们提出了一个简单的多通道框架，用于对比学习(MC-SimCLR)，以编码空间音频的“什么”和“哪里”。MC-SimCLR从未标记的空间音频中学习联合频谱和空间表示，从而提高下游任务中的事件分类和声音定位能力。核心思想是，我们提出了一个多级数据增强流水线，对不同级别的音频特征进行增强，包括波形、Mel频谱图和广义互相关(GCC)特征。此外，我们引入了简单但有效的通道增强方法，随机交换麦克风顺序和屏蔽Mel和GCC通道。通过使用这些增强方法，我们发现在学习表示之上的线性层在事件分类准确性和定位误差方面明显优于有监督模型。我们还对每种增强方法的效果进行了全面分析，并进行了比较。

    In this study, we present a simple multi-channel framework for contrastive learning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR learns joint spectral and spatial representations from unlabeled spatial audios, thereby enhancing both event classification and sound localization in downstream tasks. At its core, we propose a multi-level data augmentation pipeline that augments different levels of audio features, including waveforms, Mel spectrograms, and generalized cross-correlation (GCC) features. In addition, we introduce simple yet effective channel-wise augmentation methods to randomly swap the order of the microphones and mask Mel and GCC channels. By using these augmentations, we find that linear layers on top of the learned representation significantly outperform supervised models in terms of both event classification accuracy and localization error. We also perform a comprehensive analysis of the effect of each augmentation method and a comparison of the 
    
[^117]: 使用去噪扩散模型实现高感知质量的无线图像传输

    High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models. (arXiv:2309.15889v1 [eess.IV])

    [http://arxiv.org/abs/2309.15889](http://arxiv.org/abs/2309.15889)

    本论文研究了通过深度学习的联合源-信道编码和去噪扩散模型在噪声无线信道上进行图像传输的问题。通过利用范围-零空间分解和逐步优化零空间内容，实现了在失真和感知质量方面的显著改进。

    

    我们考虑通过基于深度学习的联合源-信道编码（DeepJSCC）以及接收端的去噪扩散概率模型（DDPM）在噪声无线信道上进行图像传输。我们特别关注在实际有限块长度的情况下的感知失真权衡问题，这种情况下，分离的源编码和信道编码可能会高度不理想。我们引入了一种利用目标图像的范围-零空间分解的新方案。我们在编码后传输图像的范围空间，并使用DDPM逐步优化其零空间内容。通过广泛的实验证明，与标准的DeepJSCC和最先进的生成式学习方法相比，我们在重构图像的失真和感知质量方面实现了显著改进。为了促进进一步的研究和可重现性，我们将公开分享我们的源代码。

    We consider the image transmission problem over a noisy wireless channel via deep learning-based joint source-channel coding (DeepJSCC) along with a denoising diffusion probabilistic model (DDPM) at the receiver. Specifically, we are interested in the perception-distortion trade-off in the practical finite block length regime, in which separate source and channel coding can be highly suboptimal. We introduce a novel scheme that utilizes the range-null space decomposition of the target image. We transmit the range-space of the image after encoding and employ DDPM to progressively refine its null space contents. Through extensive experiments, we demonstrate significant improvements in distortion and perceptual quality of reconstructed images compared to standard DeepJSCC and the state-of-the-art generative learning-based method. We will publicly share our source code to facilitate further research and reproducibility.
    
[^118]: 基于投影的模糊最小二乘双支持向量机用于类不平衡问题

    Projection based fuzzy least squares twin support vector machine for class imbalance problems. (arXiv:2309.15886v1 [cs.LG])

    [http://arxiv.org/abs/2309.15886](http://arxiv.org/abs/2309.15886)

    该论文提出了一种基于投影的模糊最小二乘双支持向量机方法来应对类不平衡问题和噪声数据集。通过引入直觉模糊成员关系和超平面模糊成员关系的概念，提出了两种不同的方法。这些方法能够在处理类不平衡问题时提供更准确的分类结果。

    

    类不平衡是许多实际分类任务中的一个主要问题。由于样本数量不平衡，支持向量机（SVM）分类器对多数类别有偏差。此外，这些样本通常存在一定程度的噪声。因此，为了解决这些问题，我们提出了一种新颖的模糊方法来处理类不平衡和具有噪声的数据集。我们提出了两种方法来解决这些问题。第一种方法基于直觉模糊成员关系，称为鲁棒能量基模糊最小二乘双支持向量机（IF-RELSTSVM）。此外，在第二种方法中，我们引入了基于超平面的模糊成员关系的概念，其中最终分类器称为鲁棒能量基模糊最小二乘双支持向量机（F-RELSTSVM）。通过使用这种技术，成员关系值基于基于投影的方法，将数据点投影到超平面上。

    Class imbalance is a major problem in many real world classification tasks. Due to the imbalance in the number of samples, the support vector machine (SVM) classifier gets biased toward the majority class. Furthermore, these samples are often observed with a certain degree of noise. Therefore, to remove these problems we propose a novel fuzzy based approach to deal with class imbalanced as well noisy datasets. We propose two approaches to address these problems. The first approach is based on the intuitionistic fuzzy membership, termed as robust energy-based intuitionistic fuzzy least squares twin support vector machine (IF-RELSTSVM). Furthermore, we introduce the concept of hyperplane-based fuzzy membership in our second approach, where the final classifier is termed as robust energy-based fuzzy least square twin support vector machine (F-RELSTSVM). By using this technique, the membership values are based on a projection based approach, where the data points are projected on the hyper
    
[^119]: 利用多层嵌入训练增强推荐系统中的跨类别学习

    Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training. (arXiv:2309.15881v1 [cs.LG])

    [http://arxiv.org/abs/2309.15881](http://arxiv.org/abs/2309.15881)

    该论文提出了一种名为多层嵌入训练（MLET）的训练技术，通过跨类别学习产生优秀的嵌入。该方法通过嵌入层的分解训练嵌入，内部维度高于目标嵌入维度，并在推理时提高了效率。该技术的实验结果令人惊讶，并且通过理论解释了其有效性。

    

    现代基于DNN的推荐系统依赖于对稀疏特征进行训练得到的嵌入。输入稀疏性使得很难获得少出现类别的高质量嵌入，因为它们的表示很少更新。我们展示了一种训练时的技术，通过有效的跨类别学习产生优秀的嵌入，并从理论上解释了其令人惊讶的有效性。该方案被称为多层嵌入训练（MLET），通过嵌入层的分解训练嵌入，内部维度高于目标嵌入维度。为了提高推理效率，MLET将训练得到的双层嵌入转换为单层嵌入，从而保持了推理时的模型大小不变。MLET的实验优越性令人困惑，因为其搜索空间并不比单层嵌入更大。MLET对内部维度的强依赖甚至更令人惊讶。我们发展了一个理论来解释这两种行为。

    Modern DNN-based recommendation systems rely on training-derived embeddings of sparse features. Input sparsity makes obtaining high-quality embeddings for rarely-occurring categories harder as their representations are updated infrequently. We demonstrate a training-time technique to produce superior embeddings via effective cross-category learning and theoretically explain its surprising effectiveness. The scheme, termed the multi-layer embeddings training (MLET), trains embeddings using factorization of the embedding layer, with an inner dimension higher than the target embedding dimension. For inference efficiency, MLET converts the trained two-layer embedding into a single-layer one thus keeping inference-time model size unchanged.  Empirical superiority of MLET is puzzling as its search space is not larger than that of the single-layer embedding. The strong dependence of MLET on the inner dimension is even more surprising. We develop a theory that explains both of these behaviors 
    
[^120]: 神经启发的分层多模态学习

    Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v1 [cs.LG])

    [http://arxiv.org/abs/2309.15877](http://arxiv.org/abs/2309.15877)

    这项研究提出了一种神经启发的分层多模态学习方法，利用信息瓶颈理论构建了一种有效且紧凑的信息流，实现了对真实世界的全面和准确的感知。

    

    整合和处理来自多种信息源或模态对于获得对真实世界的全面和准确的感知至关重要。受到神经科学的启发，我们开发了信息论分层感知(ITHP)模型，该模型利用了信息瓶颈的概念。与大多数旨在将所有模态纳入输入的传统融合模型不同，我们的模型将主要模态指定为输入，而其余模态则作为信息路径中的检测器。我们提出的感知模型的重点是通过在潜在状态和输入模态状态之间最小化相互信息并在潜在状态和其余模态之间最大化相互信息的平衡，构建一种有效且紧凑的信息流。这种方法导致了保留相关信息并最小化冗余的紧凑潜在状态表示，从而实现更好的感知。

    Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Distinct from most traditional fusion models that aim to incorporate all modalities as input, our model designates the prime modality as input, while the remaining modalities act as detectors in the information pathway. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby sub
    
[^121]: STAG: 实现动态图中基于GNN的服务低延迟和低陈旧度

    STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs. (arXiv:2309.15875v1 [cs.LG])

    [http://arxiv.org/abs/2309.15875](http://arxiv.org/abs/2309.15875)

    STAG是一个GNN服务框架，用于解决动态图中基于GNN的服务中的低延迟和低陈旧度问题。它采用协同服务机制和增量传播策略来优化节点表示的更新过程。

    

    许多新兴的用户面向服务采用图神经网络（GNN）来提高服务准确性。当GNN模型使用的图发生变化时，图中节点的表示（嵌入）应相应更新。然而，节点表示的更新速度过慢，导致用户查询的响应延迟较长（更新完成后进行推理）或存在较高的陈旧度问题（基于陈旧数据进行推理）。我们的深入分析表明，更新过慢主要是由于图中的邻居爆炸问题和重复计算。基于这些发现，我们提出了STAG，这是一个能够实现基于GNN的服务低延迟和低陈旧度的GNN服务框架。它包括协同服务机制和基于可加性的增量传播策略。通过协同服务机制，只有部分节点表示在更新阶段进行更新，最终的表示是在增量传播策略中计算得到的。

    Many emerging user-facing services adopt Graph Neural Networks (GNNs) to improve serving accuracy. When the graph used by a GNN model changes, representations (embedding) of nodes in the graph should be updated accordingly. However, the node representation update is too slow, resulting in either long response latency of user queries (the inference is performed after the update completes) or high staleness problem (the inference is performed based on stale data). Our in-depth analysis shows that the slow update is mainly due to neighbor explosion problem in graphs and duplicated computation. Based on such findings, we propose STAG, a GNN serving framework that enables low latency and low staleness of GNN-based services. It comprises a collaborative serving mechanism and an additivity-based incremental propagation strategy. With the collaborative serving mechanism, only part of node representations are updated during the update phase, and the final representations are calculated in the i
    
[^122]: Telescope:一种基于机器学习的自动化混合预测方法

    Telescope: An Automated Hybrid Forecasting Approach on a Level-Playing Field. (arXiv:2309.15871v1 [cs.LG])

    [http://arxiv.org/abs/2309.15871](http://arxiv.org/abs/2309.15871)

    Telescope是一种基于机器学习的自动化混合预测方法，可以准确可靠地进行预测，无需参数化或训练和适配大量参数，并且与其他方法相比，Telescope能够快速提供预测结果。

    

    在决策的许多领域，预测是一个重要的支柱。因此，提出了许多不同的预测方法。根据我们的经验，最近提出的预测方法计算密集，自动化程度低，针对特定数据集定制，或者缺乏可预测的结果时间。为此，我们引入了Telescope，一种新颖的基于机器学习的预测方法，它可以自动从给定的时间序列中检索相关信息，并将其分割成部分，分别处理每个部分。与深度学习方法不同，我们的方法不需要参数化或训练和适配大量参数。它只使用一个时间序列，在几秒钟内提供预测，无需额外设置。我们的实验证明，Telescope通过提供准确可靠的预测，同时对分析的时间序列不做任何假设，胜过了最近的方法。

    In many areas of decision-making, forecasting is an essential pillar. Consequently, many different forecasting methods have been proposed. From our experience, recently presented forecasting methods are computationally intensive, poorly automated, tailored to a particular data set, or they lack a predictable time-to-result. To this end, we introduce Telescope, a novel machine learning-based forecasting approach that automatically retrieves relevant information from a given time series and splits it into parts, handling each of them separately. In contrast to deep learning methods, our approach doesn't require parameterization or the need to train and fit a multitude of parameters. It operates with just one time series and provides forecasts within seconds without any additional setup. Our experiments show that Telescope outperforms recent methods by providing accurate and reliable forecasts while making no assumptions about the analyzed time series.
    
[^123]: 基于无监督机器学习识别眼压增高患者快速视野退化的相关因素

    Identifying factors associated with fast visual field progression in patients with ocular hypertension based on unsupervised machine learning. (arXiv:2309.15867v1 [cs.LG])

    [http://arxiv.org/abs/2309.15867](http://arxiv.org/abs/2309.15867)

    本研究使用无监督机器学习方法识别了眼压增高患者中具有不同视野退化趋势的亚型，并发现了快速视野退化的相关因素。

    

    目的：基于无监督机器学习识别具有不同视野退化趋势的眼压增高 (OHT) 亚型，并发现与快速视野退化相关的因素。参与者：研究纳入了1568名眼压增高治疗研究 (OHTS）参与者的3133只眼，这些参与者至少有五次随访视野测试。方法：我们使用潜在类混合模型 (LCMM) 根据标准自动化视野检测 (SAP) 平均偏差 (MD) 轨迹识别眼压增高亚型。我们根据基线的人口统计学、临床、眼部和视野指标描述亚型。然后，我们使用广义估计方程 (GEE) 识别推进快速视野退化的因素，并通过定性和定量方式验证结果。结果：LCMM模型发现了四个眼睛群集（亚型），具有不同的MD恶化轨迹。其中，聚类中的眼睛数分别为794只 (25%)、1675只 (54%)、531只 (17%) 和133只 (4%)。

    Purpose: To identify ocular hypertension (OHT) subtypes with different trends of visual field (VF) progression based on unsupervised machine learning and to discover factors associated with fast VF progression. Participants: A total of 3133 eyes of 1568 ocular hypertension treatment study (OHTS) participants with at least five follow-up VF tests were included in the study. Methods: We used a latent class mixed model (LCMM) to identify OHT subtypes using standard automated perimetry (SAP) mean deviation (MD) trajectories. We characterized the subtypes based on demographic, clinical, ocular, and VF factors at the baseline. We then identified factors driving fast VF progression using generalized estimating equation (GEE) and justified findings qualitatively and quantitatively. Results: The LCMM model discovered four clusters (subtypes) of eyes with different trajectories of MD worsening. The number of eyes in clusters were 794 (25%), 1675 (54%), 531 (17%) and 133 (4%). We labelled the clu
    
[^124]: 基于潜在图的生物医学表格数据半监督学习

    Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v1 [cs.LG])

    [http://arxiv.org/abs/2309.15757](http://arxiv.org/abs/2309.15757)

    本文提出了一种基于潜在图的半监督学习方法，通过利用图的表示来捕捉数据之间的关系，并实现了全局和局部知识的有效融合。在生物医学数据集上的评估中，我们的方法表现出了最先进的结果。

    

    在半监督学习领域中，现有方法未充分利用（有）标记数据之间的实例间关系的潜力。本文通过提供一种推断捕捉内在数据关系的潜在图的方法来解决这个限制。通过利用基于图的表示，我们的方法促进了信息在整个图中的无缝传播，能够有效地融合全局和局部知识。通过在生物医学表格数据集上的评估，我们比较了我们的方法与其他当代方法的能力。我们的工作证明了发现实例间关系作为构建强化半监督学习技术的鲁棒潜在图的实际手段的重要性。我们的方法在三个生物医学数据集上取得了最先进的结果。

    In the domain of semi-supervised learning, the current approaches insufficiently exploit the potential of considering inter-instance relationships among (un)labeled data. In this work, we address this limitation by providing an approach for inferring latent graphs that capture the intrinsic data relationships. By leveraging graph-based representations, our approach facilitates the seamless propagation of information throughout the graph, enabling the effective incorporation of global and local knowledge. Through evaluations on biomedical tabular datasets, we compare the capabilities of our approach to other contemporary methods. Our work demonstrates the significance of inter-instance relationship discovery as practical means for constructing robust latent graphs to enhance semi-supervised learning techniques. Our method achieves state-of-the-art results on three biomedical datasets.
    
[^125]: 通过方差抑制增强锐度感知优化

    Enhancing Sharpness-Aware Optimization Through Variance Suppression. (arXiv:2309.15639v1 [cs.LG])

    [http://arxiv.org/abs/2309.15639](http://arxiv.org/abs/2309.15639)

    本文通过方差抑制的方法（VaSSO）增强了锐度感知最小化（SAM）的优化算法，提高了深度神经网络的泛化能力，特别适用于模型无关任务和对高水平标签噪声具有鲁棒性的情况。

    

    锐度感知最小化（SAM）在增强深度神经网络的泛化能力方面有着良好的记录，即使没有大规模的数据增强。SAM借助损失函数的几何特性，通过最小化在邻域内参数对敌对扰动引起的最大损失，寻找“平坦最小值”所在的“平坦山谷”，提高泛化能力。尽管考虑了损失函数的锐度是至关重要的，但这种“过于友好的敌对者”可能会限制泛化的最高水平。本文的新方法通过方差抑制（VaSSO）来稳定敌对者，避免这种友好性。 VaSSO的稳定性可证明，并在模型无关任务中（包括图像分类和机器翻译）相对于SAM有着数值上的改进。此外，实验证实VaSSO赋予SAM对高水平标签噪声的鲁棒性。

    Sharpness-aware minimization (SAM) has well documented merits in enhancing generalization of deep neural networks, even without sizable data augmentation. Embracing the geometry of the loss function, where neighborhoods of 'flat minima' heighten generalization ability, SAM seeks 'flat valleys' by minimizing the maximum loss caused by an adversary perturbing parameters within the neighborhood. Although critical to account for sharpness of the loss function, such an 'over-friendly adversary' can curtail the outmost level of generalization. The novel approach of this contribution fosters stabilization of adversaries through variance suppression (VaSSO) to avoid such friendliness. VaSSO's provable stability safeguards its numerical improvement over SAM in model-agnostic tasks, including image classification and machine translation. In addition, experiments confirm that VaSSO endows SAM with robustness against high levels of label noise.
    
[^126]: 共同训练大型自回归多模态模型

    Jointly Training Large Autoregressive Multimodal Models. (arXiv:2309.15564v1 [cs.LG])

    [http://arxiv.org/abs/2309.15564](http://arxiv.org/abs/2309.15564)

    本研究提出了共同训练大型自回归多模态模型的方法，通过模块化的方式融合语言和图像生成模型，同时引入了数据高效的指令调优策略，使得该模型在生成高质量多模态输出方面表现出卓越的性能。

    

    最近几年，语言和文本到图像模型的大规模预训练取得了重大突破，彻底改变了机器学习领域。然而，将这两种模态集成到一个能够生成无缝多模态输出的单一强大模型仍然是一个重大挑战。为了解决这一问题，我们提出了联合自回归混合（JAM）框架，一种系统融合现有文本和图像生成模型的模块化方法。我们还引入了一种专门的、数据高效的指令调优策略，针对混合模态生成任务进行了优化。我们最终的调优模型在生成高质量多模态输出方面表现出无与伦比的性能，并代表了第一个明确为此目的而设计的模型。

    In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.
    
[^127]: 使用机器学习和线性规划进行每日奇幻足球最佳阵容的方法和验证

    Method and Validation for Optimal Lineup Creation for Daily Fantasy Football Using Machine Learning and Linear Programming. (arXiv:2309.15253v1 [cs.LG])

    [http://arxiv.org/abs/2309.15253](http://arxiv.org/abs/2309.15253)

    本文提出了一种方法来预测NFL球员的表现，并使用线性规划找到最佳阵容来最大化奇幻分数。实验结果表明，这种方法可以有效提高阵容的性能。

    

    每日奇幻体育是每周或每日的在线比赛，其中个人运动员的真实比赛表现被转换为奇幻分数（FPTS）。用户根据设定的球员工资上限选择阵容以最大化他们的FPTS。本文针对两个重点进行研究：（1）在不确定性下预测NFL球员表现的方法的开发，（2）在设定的工资限制下确定最佳阵容以最大化FPTS。创建了一个监督学习神经网络，根据过去的球员表现（本工作使用2018NFL常规赛季）预测FPTS，并将这些预测的FPTS用于混合整数线性规划中寻找最佳阵容。将生成的阵容的性能与随机生成的阵容进行了比较。平均而言，最佳阵容的性能优于随机阵容。然后将生成的阵容与DraftKings用户的真实阵容进行了比较。生成的阵容通常接近31%的真实阵容。

    Daily fantasy sports (DFS) are weekly or daily online contests where real-game performances of individual players are converted to fantasy points (FPTS). Users select players for their lineup to maximize their FPTS within a set player salary cap. This paper focuses on (1) the development of a method to forecast NFL player performance under uncertainty and (2) determining an optimal lineup to maximize FPTS under a set salary limit. A supervised learning neural network was created and used to project FPTS based on past player performance (2018 NFL regular season for this work) prior to the upcoming week. These projected FPTS were used in a mixed integer linear program to find the optimal lineup. The performance of resultant lineups was compared to randomly-created lineups. On average, the optimal lineups outperformed the random lineups. The generated lineups were then compared to real-world lineups from users on DraftKings. The generated lineups generally fell in approximately the 31st p
    
[^128]: DPA-WNO：一类随机力学问题的灰箱模型

    DPA-WNO: A gray box model for a class of stochastic mechanics problem. (arXiv:2309.15128v1 [cs.LG])

    [http://arxiv.org/abs/2309.15128](http://arxiv.org/abs/2309.15128)

    DPA-WNO是一种将可解释性的数据驱动模型与小波神经操作符相结合的新方法，用于纠正/识别缺失的物理，并解决了纯数据驱动模型的缺点。

    

    在科学和工程中，众所周知的物理定律常常基于某些假设和近似。因此，基于这些方程进行的分析和设计也是近似的。数据驱动模型的出现在一定程度上解决了这个挑战；然而，纯数据驱动模型往往存在以下问题：(a)缺乏可解释性，(b)需要大量数据，(c)无法超越训练范围的泛化能力。最近，操作符学习被提出作为一个潜在的替代方案来解决上述挑战；然而，这些挑战仍然存在。我们在这里认为，可能的解决方案之一存在于数据物理融合中，其中数据驱动模型用于纠正/识别缺失的物理。为此，我们提出了一种新颖的可微分物理增强小波神经操作符(DPA-WNO)。该提出的DPA-WNO将可微分物理求解器与小波神经操作符(WNO)融合在一起，其中WNO的作用是模拟

    The well-known governing physics in science and engineering is often based on certain assumptions and approximations. Therefore, analyses and designs carried out based on these equations are also approximate. The emergence of data-driven models has, to a certain degree, addressed this challenge; however, the purely data-driven models often (a) lack interpretability, (b) are data-hungry, and (c) do not generalize beyond the training window. Operator learning has recently been proposed as a potential alternative to address the aforementioned challenges; however, the challenges are still persistent. We here argue that one of the possible solutions resides in data-physics fusion, where the data-driven model is used to correct/identify the missing physics. To that end, we propose a novel Differentiable Physics Augmented Wavelet Neural Operator (DPA-WNO). The proposed DPA-WNO blends a differentiable physics solver with the Wavelet Neural Operator (WNO), where the role of WNO is to model the 
    
[^129]: 揭示分子表示学习中的神经缩放定律

    Uncovering Neural Scaling Laws in Molecular Representation Learning. (arXiv:2309.15123v1 [physics.chem-ph])

    [http://arxiv.org/abs/2309.15123](http://arxiv.org/abs/2309.15123)

    从数据中心的角度研究了分子表示学习的神经缩放行为，发现了数据量和性能之间的一致幂律关系，并提出了潜在的提高学习效率的方法。

    

    分子表示学习（MRL）已经成为药物和材料发现的强大工具，在虚拟筛选和反向设计等各种任务中发挥着重要作用。虽然对于分子表示的数据数量和质量对MRL的影响的模型中心技术的推进引起了大量关注，但在这个领域内，对于这两个因素的影响还不完全清楚。在本文中，我们从数据中心的角度深入研究了MRL的神经缩放行为，考察了四个关键维度：（1）数据模态，（2）数据集划分，（3）预训练的作用，和（4）模型容量。我们的实证研究证实了数据量和MRL性能之间的一致的幂律关系。此外，通过详细分析，我们确定了提高学习效率的潜在途径。为了挑战这些缩放定律，我们将七种常见的数据修剪策略应用于分子数据并进行了性能评估。我们的研究结果强调了改善学习效率的潜在途径。

    Molecular Representation Learning (MRL) has emerged as a powerful tool for drug and materials discovery in a variety of tasks such as virtual screening and inverse design. While there has been a surge of interest in advancing model-centric techniques, the influence of both data quantity and quality on molecular representations is not yet clearly understood within this field. In this paper, we delve into the neural scaling behaviors of MRL from a data-centric viewpoint, examining four key dimensions: (1) data modalities, (2) dataset splitting, (3) the role of pre-training, and (4) model capacity. Our empirical studies confirm a consistent power-law relationship between data volume and MRL performance across these dimensions. Additionally, through detailed analysis, we identify potential avenues for improving learning efficiency. To challenge these scaling laws, we adapt seven popular data pruning strategies to molecular data and benchmark their performance. Our findings underline the im
    
[^130]: 通过多分辨率学习提高深度卷积神经网络的鲁棒性

    Improving Robustness of Deep Convolutional Neural Networks via Multiresolution Learning. (arXiv:2309.13752v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13752](http://arxiv.org/abs/2309.13752)

    本研究通过多分辨率学习方式，显著提高了深度卷积神经网络对于1D信号和2D信号（图像）预测问题的鲁棒性，并发现在这种学习方式下，并不需要牺牲准确性来获得鲁棒性。

    

    当前的深度学习过程，无论使用哪种深度神经网络（DNN）架构和/或学习算法，本质上都是单分辨率训练。我们探索了多分辨率学习，并表明多分辨率学习可以显著提高1D信号和2D信号（图像）预测问题的DNN模型的鲁棒性。我们通过噪声和对抗鲁棒性以及小训练数据集大小来展示这种改进。我们的结果还表明，通过多分辨率学习，不需要牺牲标准准确性来获得鲁棒性，这与传统的单分辨率学习设置的观察结果恰恰相反。

    The current learning process of deep learning, regardless of any deep neural network (DNN) architecture and/or learning algorithm used, is essentially a single resolution training. We explore multiresolution learning and show that multiresolution learning can significantly improve robustness of DNN models for both 1D signal and 2D signal (image) prediction problems. We demonstrate this improvement in terms of both noise and adversarial robustness as well as with small training dataset size. Our results also suggest that it may not be necessary to trade standard accuracy for robustness with multiresolution learning, which is, interestingly, contrary to the observation obtained from the traditional single resolution learning setting.
    
[^131]: 通过桥块分解学习大规模MTP$_2$高斯图模型

    Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition. (arXiv:2309.13405v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13405](http://arxiv.org/abs/2309.13405)

    本论文通过桥块分解方法，提出了一种学习大规模MTP$_2$高斯图模型的策略，能够将大问题拆分为小问题进行优化，显著降低了计算复杂度并提高了算法速度。

    

    本文研究了学习大规模的MTP$_2$高斯图模型的问题。通过引入桥的概念，我们展示了整个问题可以通过对阈值样本协方差图上的几个较小规模的子问题进行桥块分解来等效优化，以及对应于桥的条目的一组明确解决方案。从实践角度来看，这个简单而可证明的方法可以将一个大问题拆分为小的可处理的问题，大大减少了计算复杂度并显著改进了所有现有算法。合成和真实世界的实验证明，我们提议的方法与现有技术基准相比，速度显著提升。

    This paper studies the problem of learning the large-scale Gaussian graphical models that are multivariate totally positive of order two ($\text{MTP}_2$). By introducing the concept of bridge, which commonly exists in large-scale sparse graphs, we show that the entire problem can be equivalently optimized through (1) several smaller-scaled sub-problems induced by a \emph{bridge-block decomposition} on the thresholded sample covariance graph and (2) a set of explicit solutions on entries corresponding to \emph{bridges}. From practical aspect, this simple and provable discipline can be applied to break down a large problem into small tractable ones, leading to enormous reduction on the computational complexity and substantial improvements for all existing algorithms. The synthetic and real-world experiments demonstrate that our proposed method presents a significant speed-up compared to the state-of-the-art benchmarks.
    
[^132]: S-GBDT: 节俭的差分隐私梯度提升决策树

    S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees. (arXiv:2309.12041v1 [cs.CR])

    [http://arxiv.org/abs/2309.12041](http://arxiv.org/abs/2309.12041)

    S-GBDT是一种节俭的差分隐私梯度提升决策树学习器，利用了四种技术来改善效用和隐私权之间的平衡，包括对隐私泄露的更紧密计算和整合个体Rényi滤波器以学习未充分利用的数据点。

    

    差分隐私学习梯度提升决策树(GBDT)在表格数据(如人口普查数据或医疗元数据)中具有很强的效用和隐私权之间的平衡潜力：经典的GBDT学习器可以从小规模数据集中提取非线性模式。可证明具有隐私性质的当前方法是差分隐私，该方法要求单个数据点的影响有限且可否认。我们引入了一种新的差分隐私GBDT学习器，并利用四种主要技术来改善效用和隐私权之间的平衡。(1)我们使用了一种改进的噪声缩放方法，更紧密地计算了与先前工作相比决策树叶子的隐私泄露，从而导致噪声的期望与数据点数量n的比例为$O(1/n)$，其中n为数据点数量。(2)我们将个体Rényi滤波器整合到我们的方法中，以从在迭代训练过程中未充分利用的数据点中学习，这可能是独立于兴趣的结果。

    Privacy-preserving learning of gradient boosting decision trees (GBDT) has the potential for strong utility-privacy tradeoffs for tabular data, such as census data or medical meta data: classical GBDT learners can extract non-linear patterns from small sized datasets. The state-of-the-art notion for provable privacy-properties is differential privacy, which requires that the impact of single data points is limited and deniable. We introduce a novel differentially private GBDT learner and utilize four main techniques to improve the utility-privacy tradeoff. (1) We use an improved noise scaling approach with tighter accounting of privacy leakage of a decision tree leaf compared to prior work, resulting in noise that in expectation scales with $O(1/n)$, for $n$ data points. (2) We integrate individual R\'enyi filters to our method to learn from data points that have been underutilized during an iterative training process, which -- potentially of independent interest -- results in a natura
    
[^133]: GLM回归与无意识数据损坏

    GLM Regression with Oblivious Corruptions. (arXiv:2309.11657v1 [cs.DS])

    [http://arxiv.org/abs/2309.11657](http://arxiv.org/abs/2309.11657)

    这篇论文介绍了第一个在广义线性模型回归问题中处理加法无意识噪声的算法。算法的目标是通过样本访问来准确地恢复参数向量，使得模型的预测与真实值的误差尽可能小。

    

    我们展示了在广义线性模型（GLMs）的回归问题中，存在加法无意识噪声的第一个算法。我们假设我们有样本访问到的例子$(x, y)$，其中$y$是$g(w^* \cdot x)$的带噪声测量值。特别地，噪声标签的形式为$y = g(w^* \cdot x) + \xi + \epsilon$，其中$\xi$是与$x$独立抽取的无意识噪声满足$\Pr[\xi = 0] \geq o(1)$，而$\epsilon \sim \mathcal N(0, \sigma^2)$。我们的目标是准确地恢复一个参数向量$w$，使得函数$g(w \cdot x)$与真实值$g(w^* \cdot x)$相比具有任意小的误差，而不是与噪声测量$y$相比。我们提出了一个算法，解决了最一般的与分布无关的情况，其中解可能甚至不可识别。我们的算法返回一个准确的估计，如果它是可识别的，否则

    We demonstrate the first algorithms for the problem of regression for generalized linear models (GLMs) in the presence of additive oblivious noise. We assume we have sample access to examples $(x, y)$ where $y$ is a noisy measurement of $g(w^* \cdot x)$. In particular, \new{the noisy labels are of the form} $y = g(w^* \cdot x) + \xi + \epsilon$, where $\xi$ is the oblivious noise drawn independently of $x$ \new{and satisfies} $\Pr[\xi = 0] \geq o(1)$, and $\epsilon \sim \mathcal N(0, \sigma^2)$. Our goal is to accurately recover a \new{parameter vector $w$ such that the} function $g(w \cdot x)$ \new{has} arbitrarily small error when compared to the true values $g(w^* \cdot x)$, rather than the noisy measurements $y$.  We present an algorithm that tackles \new{this} problem in its most general distribution-independent setting, where the solution may not \new{even} be identifiable. \new{Our} algorithm returns \new{an accurate estimate of} the solution if it is identifiable, and otherwise
    
[^134]: 避免在根查找和优化中出现不需要的点的方法：通过增加极点

    Multiplying poles to avoid unwanted points in root finding and optimization. (arXiv:2309.11475v1 [math.OC])

    [http://arxiv.org/abs/2309.11475](http://arxiv.org/abs/2309.11475)

    通过增加极点来避免根查找和优化中不需要的点，方法是将代价函数除以到目标点的距离函数的适当幂。

    

    在根查找和优化中，存在许多情况下，我们可能无法保证自己选择的方法构造的序列收敛于一个闭集合A（在这里，我们并不假设A有其他附加属性，如凸性或连通性）。在这种情况下，我们希望有一个机制来避免在算法的下一次运行中再次遇到这个点z*。在本文中，我们提出了一种新的方法来实现这一目标：我们将代价函数除以到A的距离函数的适当幂。这个想法受到了在一维函数中尝试找到所有根的启发。我们首先解释了在代价函数的最小值恰好为0的情况下这种方法的启发式方法，然后解释了如果最小值不为零该如何进行（同时允许正的最小值）。

    In root finding and optimization, there are many cases where there is a closed set $A$ one does not the sequence constructed by one's favourite method will converge to A (here, we do not assume extra properties on $A$ such as being convex or connected). For example, if one wants to find roots, and one chooses initial points in the basin of attraction for 1 root $x^*$ (a fact which one may not know before hand), then one will always end up in that root. In this case, one would like to have a mechanism to avoid this point $z^*$ in the next runs of one's algorithm.  In this paper, we propose a new method aiming to achieve this: we divide the cost function by an appropriate power of the distance function to $A$. This idea is inspired by how one would try to find all roots of a function in 1 variable. We first explain the heuristic for this method in the case where the minimum of the cost function is exactly 0, and then explain how to proceed if the minimum is non-zero (allowing both positi
    
[^135]: 基于语言模型的概率测量专利权要求范围的新方法

    A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])

    [http://arxiv.org/abs/2309.10003](http://arxiv.org/abs/2309.10003)

    本文提出了一种使用概率从语言模型中获得的自信息来测量专利权要求范围的新方法。该方法通过计算要求的发生概率和自信息来评估要求的信息量，进而反映出要求的范围。研究结果表明，不同类型的语言模型对范围测量的影响不同，最简单的模型可以将范围度量简化为单词或字符计数的倒数。此方法在九个系列的专利权要求上进行了验证，结果表明各系列的要求范围逐渐减小。

    

    本文提出了一种将专利权要求的范围测量为该要求所包含的自信息的倒数的方法。这种方法基于信息论，基于一个假设，即罕见的概念比平常的概念更具信息量，因为它更令人惊讶。自信息是从该要求的发生概率计算得出的，其中概率是根据语言模型计算的。本文考虑了五个语言模型，从最简单的模型（每个单词或字符均从均匀分布中抽取）到中等模型（使用平均词或字符频率），再到一个大型语言模型（GPT2）。有趣的是，最简单的语言模型将范围度量减少为单词或字符计数的倒数，这是先前作品中已经使用的度量标准。该方法应用于九个系列的针对不同发明的专利权要求，其中每个系列的要求范围逐渐减小。

    This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
    
[^136]: 具备视觉和触觉的手中物体通用旋转

    General In-Hand Object Rotation with Vision and Touch. (arXiv:2309.09979v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.09979](http://arxiv.org/abs/2309.09979)

    本研究介绍了一个能够通过视觉和触觉感知实现手中物体通用旋转的系统，通过训练和模拟推断物体的形状和物理属性，并在实际部署中展示了显著的性能提升。

    

    我们介绍了一个名为RotateIt的系统，通过利用多模态感知输入，使指尖能够在多个轴上进行物体旋转。我们的系统在仿真环境中进行训练，其中可以获取物体的真实形状和物理属性。然后我们将其简化为在真实但噪声干扰下的模拟触觉和本体感知输入。这些多模态输入通过视觉触觉变换器进行融合，使得在部署过程中可以进行物体形状和物理属性的在线推断。我们展示了相比以前的方法的显著性能改进，并且证明了视觉和触觉感知的重要性。

    We introduce RotateIt, a system that enables fingertip-based object rotation along multiple axes by leveraging multimodal sensory inputs. Our system is trained in simulation, where it has access to ground-truth object shapes and physical properties. Then we distill it to operate on realistic yet noisy simulated visuotactile and proprioceptive sensory inputs. These multimodal inputs are fused via a visuotactile transformer, enabling online inference of object shapes and physical properties during deployment. We show significant performance improvements over prior methods and the importance of visual and tactile sensing.
    
[^137]: 通过动态集成选择进行不平衡数据流分类

    Imbalanced Data Stream Classification using Dynamic Ensemble Selection. (arXiv:2309.09175v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09175](http://arxiv.org/abs/2309.09175)

    本文提出了一个新的框架，通过对非稳态漂移的不平衡数据流进行分类框架设计，采用数据预处理和动态集成选择技术。该框架使用了六个人工生成的数据流，并且通过评估了七种预处理技术和两种动态集成选择方法的效果。

    

    现代流数据分类面临着概念漂移和类别不平衡数据的重大挑战。这对分类器的输出产生了负面影响，导致分类不正确。此外，多个类别的重叠等其他因素限制了输出的正确性程度。本文提出了一个新的框架，通过对非稳态漂移的不平衡数据流进行分类框架设计，采用数据预处理和动态集成选择技术。该框架使用了六个人工生成的数据流，这些数据流具有不同的不平衡比例，并且包含两种不同类型的概念漂移。每个数据流由200个500个对象的块组成，每个对象由八个特征描述，并且包含五个概念漂移。考虑了七种预处理技术和两种动态集成选择方法进行评估。

    Modern streaming data categorization faces significant challenges from concept drift and class imbalanced data. This negatively impacts the output of the classifier, leading to improper classification. Furthermore, other factors such as the overlapping of multiple classes limit the extent of the correctness of the output. This work proposes a novel framework for integrating data pre-processing and dynamic ensemble selection, by formulating the classification framework for the nonstationary drifting imbalanced data stream, which employs the data pre-processing and dynamic ensemble selection techniques. The proposed framework was evaluated using six artificially generated data streams with differing imbalance ratios in combination with two different types of concept drifts. Each stream is composed of 200 chunks of 500 objects described by eight features and contains five concept drifts. Seven pre-processing techniques and two dynamic ensemble selection methods were considered. According 
    
[^138]: 在物联网环境中检测未知攻击: 一种用于增强网络入侵检测的开放集分类器

    Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])

    [http://arxiv.org/abs/2309.07461](http://arxiv.org/abs/2309.07461)

    这项研究介绍了一个针对物联网环境定制的网络入侵检测系统的开放集分类器框架，利用图像表示和堆叠子聚类技术来识别未知攻击。

    

    物联网设备在生活的各个方面都得到了广泛的应用，这引入了互联的时代，创造了新的网络安全挑战，并强调了强大的入侵检测系统的需求。然而，传统的安全系统是基于封闭世界视角设计的，往往面临与不断发展的威胁环境中新的、陌生的攻击相处理的挑战。在本文中，我们介绍了一个框架，旨在解决物联网环境下网络入侵检测系统（NIDS）中的开放集识别（OSR）问题。我们的框架利用基于图像的数据表示，从网络流量中提取空间和时间模式。此外，我们还集成了堆叠和子聚类技术，通过有效地建模复杂和多样化的良性行为，实现对未知攻击的识别。

    The widespread integration of Internet of Things (IoT) devices across all facets of life has ushered in an era of interconnectedness, creating new avenues for cybersecurity challenges and underscoring the need for robust intrusion detection systems. However, traditional security systems are designed with a closed-world perspective and often face challenges in dealing with the ever-evolving threat landscape, where new and unfamiliar attacks are constantly emerging. In this paper, we introduce a framework aimed at mitigating the open set recognition (OSR) problem in the realm of Network Intrusion Detection Systems (NIDS) tailored for IoT environments. Our framework capitalizes on image-based representations of packet-level data, extracting spatial and temporal patterns from network traffic. Additionally, we integrate stacking and sub-clustering techniques, enabling the identification of unknown attacks by effectively modeling the complex and diverse nature of benign behavior. The empiric
    
[^139]: 基于硬件感知的资源受限设备上的多模态神经架构搜索(Harmonic-NAS)

    Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices. (arXiv:2309.06612v1 [cs.LG])

    [http://arxiv.org/abs/2309.06612](http://arxiv.org/abs/2309.06612)

    本文提出了基于硬件感知的资源受限设备上的多模态神经架构搜索框架Harmonic-NAS，通过两层优化实现了单模态骨干和多模态融合网络的联合优化。

    

    最近，对多模态神经网络（MM-NN）的兴趣激增，这归功于它们有效处理和整合来自不同数据源的信息的能力。在MM-NN中，使用适当的单模态骨干和特定的融合网络从多个模态提取和融合特征。尽管这有助于增强多模态信息表达，但设计此类网络是劳动密集型的。它需要调整单模态骨干的架构参数，选择融合点，并选择融合的操作。此外，多模性人工智能正在成为物联网系统中的一种尖端选择，其中推断延迟和能量消耗是除准确性外的关键指标。在本文中，我们提出了一种名为Harmonic-NAS的框架，用于在资源受限设备上具有硬件感知的单模态骨干和多模态融合网络的联合优化。

    The recent surge of interest surrounding Multimodal Neural Networks (MM-NN) is attributed to their ability to effectively process and integrate information from diverse data sources. In MM-NN, features are extracted and fused from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this helps strengthen the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Furthermore, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy. In this paper, we propose Harmonic-NAS, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. Harmonic-NAS involves a two-tier optimi
    
[^140]: 实例无关的几何与接触动力学学习

    Instance-Agnostic Geometry and Contact Dynamics Learning. (arXiv:2309.05832v1 [cs.CV])

    [http://arxiv.org/abs/2309.05832](http://arxiv.org/abs/2309.05832)

    本文提出了一个实例无关的学习框架，通过几何作为共享表示，将视觉与动力学相结合，从RGBD视频中学习物体的几何和动力学属性。实验结果表明，该框架能够学习刚性和凸物体的几何和动力学，并改进了跟踪框架。

    

    本文提出了一个实例无关的学习框架，通过几何作为共享表示，将视觉与动力学相结合，同时学习形状、姿态轨迹和物理性质。与许多接触学习方法不同，我们的框架从RGBD视频中学习物体的几何和动力学属性，而不需要类别级别或实例级别的形状先验知识。我们将视觉系统BundleSDF与动力学系统ContactNets集成，并提出了一个循环训练管道，使用动力学模块的输出通过透视重投影来改进视觉模块的姿态和几何。实验证明了我们的框架学习刚性和凸物体的几何和动力学，并改进了当前的跟踪框架。

    This work presents an instance-agnostic learning framework that fuses vision with dynamics to simultaneously learn shape, pose trajectories and physical properties via the use of geometry as a shared representation. Unlike many contact learning approaches that assume motion capture input and a known shape prior for the collision model, our proposed framework learns an object's geometric and dynamic properties from RGBD video, without requiring either category-level or instance-level shape priors. We integrate a vision system, BundleSDF, with a dynamics system, ContactNets and propose a cyclic training pipeline to use the output from the dynamics module to refine the poses and the geometry from the vision module, using perspective reprojection. Experiments demonstrate our framework's ability to learn the geometry and dynamics of rigid and convex objects and improve upon the current tracking framework.
    
[^141]: 6G中的联邦学习发展：基于图分析的可信架构

    Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis. (arXiv:2309.05525v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2309.05525](http://arxiv.org/abs/2309.05525)

    该研究提出了一种在6G环境中支持联邦学习的可信架构，利用分布式分类账技术和图神经网络解决了隐私和安全问题，以实现安全聚合和异常检测。

    

    将原生AI支持集成到网络架构中是6G的一个重要目标。联邦学习（FL）作为一种潜在的范式出现，可以在中央服务器的协调下，促进分散的AI模型训练跨越多种设备。然而，在6G环境下，有几个挑战阻碍了其广泛应用，例如恶意攻击和对本地模型更新的隐私监视，以及集中化的缺点。本研究提出了一种支持FL的可信架构，该架构利用分布式分类账技术（DLT）和图神经网络（GNN），包括三个关键特性。首先，引入同态加密的预处理层用于安全地聚合本地模型，保护个体模型的隐私。其次，考虑到预处理层中客户端和节点之间的分布式性质和图结构，利用GNN来识别异常本地模型，增强系统安全性。第三，利用DLT来维护网络中节点之间的信任关系，并提供安全的共识机制。

    Integrating native AI support into the network architecture is an essential objective of 6G. Federated Learning (FL) emerges as a potential paradigm, facilitating decentralized AI model training across a diverse range of devices under the coordination of a central server. However, several challenges hinder its wide application in the 6G context, such as malicious attacks and privacy snooping on local model updates, and centralization pitfalls. This work proposes a trusted architecture for supporting FL, which utilizes Distributed Ledger Technology (DLT) and Graph Neural Network (GNN), including three key features. First, a pre-processing layer employing homomorphic encryption is incorporated to securely aggregate local models, preserving the privacy of individual models. Second, given the distributed nature and graph structure between clients and nodes in the pre-processing layer, GNN is leveraged to identify abnormal local models, enhancing system security. Third, DLT is utilized to d
    
[^142]: 通过有符号梯度下降优化LLMs量化中的权重舍入

    Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05516](http://arxiv.org/abs/2309.05516)

    本文提出一种名为SignRound的优化权重舍入的方法，通过使用有符号梯度进行轻量级分块调整，解决了大型语言模型(LLMs)的量化挑战。

    

    大型语言模型(LLMs)在执行语言相关任务方面表现出了非凡的能力。然而，由于其巨大的内存和存储需求，它们的部署面临着重大挑战。为了解决这个问题，仅针对权重的量化，特别是3位和4位仅针对权重的量化，已经成为最可行的解决方案之一。随着位数的减少，量化网格变得更加宽泛，从而强调了上下舍入的重要性。尽管先前的研究表明，在某些情况下，通过添加扰动细调上下舍入可以提高准确性，但我们的研究受制于这些扰动的精确且有限的边界，只有改变舍入值的阈值才具有重要性。因此，我们提出了一种简洁高效的优化权重舍入任务的方法。我们的方法名为SignRound，它涉及使用有符号梯度的轻量级分块调整。

    Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
    
[^143]: 高斯混合物可以通过多项式数量的样本进行差分隐私学习

    Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples. (arXiv:2309.03847v1 [stat.ML])

    [http://arxiv.org/abs/2309.03847](http://arxiv.org/abs/2309.03847)

    通过多项式数量的样本和差分隐私约束，我们提出了一个可以估计高斯混合物的方法，并证明了这个方法的有效性，而无需对GMMs做任何结构性假设。

    

    我们研究了在差分隐私(DP)约束下估计高斯混合物的问题。我们的主要结果是，使用$\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$个样本即可在满足$(\varepsilon, \delta)$-DP的条件下估计$k$个高斯混合物，使其达到总变差距离$\alpha$。这是该问题的第一个有限样本复杂性上限，而无需对GMMs做任何结构性假设。为了解决这个问题，我们构建了一个新的框架，该框架对于其他任务可能也有用。在高层次上，我们展示了如果一个分布类（比如高斯分布）是（1）可列表译码的并且（2）在总变差距离方面具有“局部小”覆盖[ BKSW19]，则其混合物类是私密可学习的。证明绕过了一个已知障碍，表明与高斯分布不同，GMMs不具有局部小的覆盖[AAL21]。

    We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate a mixture of $k$ Gaussians up to total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.  To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a "locally small'' cover [BKSW19] with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover [AAL21].
    
[^144]: 学习品味：一个多模态葡萄酒数据集

    Learning to Taste: A Multimodal Wine Dataset. (arXiv:2308.16900v1 [cs.LG])

    [http://arxiv.org/abs/2308.16900](http://arxiv.org/abs/2308.16900)

    这个论文介绍了一个大型多模态葡萄酒数据集，用于研究视觉感知、语言和口感之间的关系，并提出了低维概念嵌入算法，将人类经验与自动机器相似度核相结合，改进了口味分类，并与人类口味知觉相一致。

    

    我们提出了一个大型的多模态葡萄酒数据集WineSensed，用于研究视觉感知、语言和口感之间的关系。该数据集包含89.7万张葡萄酒标签图片和82.4万条来自Vivino平台的葡萄酒评论。该数据集具有超过35万个独特的年份，附带了年份、产地、评分、酒精含量、价格和葡萄组成的注释。我们通过一项品酒实验对部分数据进行了细粒度的口味注释，共有256名参与者被要求根据口味的相似性对葡萄酒进行排序，得到了超过5千个配对的口味距离。我们提出了一种低维概念嵌入算法，将人类经验与自动机器相似度核相结合。我们证明，这个共享的概念嵌入空间在粗粒度口味分类（酒精含量，国家，葡萄，价格，评分）上改进，并且与复杂的人类口味知觉相一致。

    We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k images of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique vintages, annotated with year, region, rating, alcohol percentage, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algorithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification (alcohol percentage, country, grape, price, rating) and aligns with the intricate human perception of flavor.
    
[^145]: 理解基于QUBO的哈密顿函数在图上的组合优化中的使用：以最大切割问题为例讨论

    Understanding the Usage of QUBO-based Hamiltonian Function in Combinatorial Optimization over Graphs: A Discussion Using Max Cut (MC) Problem. (arXiv:2308.13978v1 [cs.AI])

    [http://arxiv.org/abs/2308.13978](http://arxiv.org/abs/2308.13978)

    研究探讨了在图上基于QUBO公式的最大切割问题中，如何使用基于强化学习范式和哈密顿函数来解决组合优化问题。通过使用图神经网络作为信息传递架构，并通过三种不同的公式形式进行实验，发现...

    

    二次无约束二进制优化（QUBO）是一种广义技术，用于将各种NP困难组合优化问题建模为二进制变量的形式。哈密顿函数经常用于形成QUBO问题，其中它在优化的上下文中被用作目标函数。在本研究中，我们研究了如何使用基于强化学习（RL）范式和哈密顿函数解决QUBO公式中的图上组合优化问题。我们使用图神经网络（GNN）作为信息传递架构在节点之间传递信息。我们主要研究了三种公式，Monty-Carlo Tree Search with GNN-based RL（MCTS-GNN）、DQN with GNN-based RL和带有注意力的通用GNN（GRL）。我们的研究结果表明，...

    Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to model various NP-hard combinatorial optimization problems in the form of binary variables. The Hamiltonian function is often used to formulate QUBO problems where it is used as the objective function in the context of optimization. In this study, we investigate how reinforcement learning-based (RL) paradigms with the presence of the Hamiltonian function can address combinatorial optimization problems over graphs in QUBO formulations. We use Graph Neural Network (GNN) as the message-passing architecture to convey the information among the nodes. We have centered our discussion on QUBO formulated Max-Cut problem but the intuitions can be extended to any QUBO supported canonical NP-Hard combinatorial optimization problems. We mainly investigate three formulations, Monty-Carlo Tree Search with GNN-based RL (MCTS-GNN), DQN with GNN-based RL, and a generic GNN with attention-based RL (GRL). Our findings state that i
    
[^146]: 电子健康记录中的生存分析的对比学习：构建时序区别度

    Contrastive Learning of Temporal Distinctiveness for Survival Analysis in Electronic Health Records. (arXiv:2308.13104v1 [cs.LG])

    [http://arxiv.org/abs/2308.13104](http://arxiv.org/abs/2308.13104)

    本文提出了一种新的基于对比学习的生存分析框架，充分利用截断和观察数据的生存时长来定义时序区别度，构建负样本对。

    

    生存分析在许多医疗决策中起着至关重要的作用，可以支持对患者医疗过程中感兴趣事件的风险预测。鉴于数据截断的存在，一种有效的生存分析方法是强制保持截断和观察数据之间的时序一致性，旨在利用截断前的时间间隔作为部分观测的事件发生时间标签用于监督学习。尽管现有研究大多采用排序方法来追求排序目标，但尚未对对比方法进行深入探索，这些方法通过数据之间的对比来学习有区别性的嵌入。因此，在本文中，我们提出了一种新颖的基于本体感知的时序对比生存分析（OTCSurv）框架，利用截断和观察数据的生存时长定义时序区别度，并构建负样本对。

    Survival analysis plays a crucial role in many healthcare decisions, where the risk prediction for the events of interest can support an informative outlook for a patient's medical journey. Given the existence of data censoring, an effective way of survival analysis is to enforce the pairwise temporal concordance between censored and observed data, aiming to utilize the time interval before censoring as partially observed time-to-event labels for supervised learning. Although existing studies mostly employed ranking methods to pursue an ordering objective, contrastive methods which learn a discriminative embedding by having data contrast against each other, have not been explored thoroughly for survival analysis. Therefore, in this paper, we propose a novel Ontology-aware Temporality-based Contrastive Survival (OTCSurv) analysis framework that utilizes survival durations from both censored and observed data to define temporal distinctiveness and construct negative sample pairs with adj
    
[^147]: 基于时空自适应嵌入，普通Transformer在交通预测中领先于其他方法

    Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting. (arXiv:2308.10425v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10425](http://arxiv.org/abs/2308.10425)

    本研究提出了一种名为时空自适应嵌入的新组件，在普通的Transformer中实现了领先于其他方法的交通预测性能，通过捕捉交通时间序列中的时空关系和时间信息实现了优秀的结果。

    

    随着智能交通系统（ITS）的快速发展，准确的交通预测已成为一个关键挑战。其核心瓶颈在于捕捉复杂的时空交通模式。近年来，已经提出了许多具有复杂架构的神经网络来解决这个问题。然而，网络架构的进步遇到了性能收益降低的问题。在本研究中，我们提出了一种称为时空自适应嵌入的新组件，可以在普通的Transformer中获得出色的结果。我们的提出的Spatio-Temporal Adaptive Embedding transformer（STAEformer）在五个真实世界的交通预测数据集上实现了最先进的性能。进一步的实验表明，时空自适应嵌入通过有效捕捉交通时间序列中的内在时空关系和时间信息，在交通预测中起到了关键作用。

    With the rapid development of the Intelligent Transportation System (ITS), accurate traffic forecasting has emerged as a critical challenge. The key bottleneck lies in capturing the intricate spatio-temporal traffic patterns. In recent years, numerous neural networks with complicated architectures have been proposed to address this issue. However, the advancements in network architectures have encountered diminishing performance gains. In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer) achieves state-of-the-art performance on five real-world traffic forecasting datasets. Further experiments demonstrate that spatio-temporal adaptive embedding plays a crucial role in traffic forecasting by effectively capturing intrinsic spatio-temporal relations and chronological information in traffic time series.
    
[^148]: Kairos: 使用整体系统溯源进行实用的入侵检测和调查

    Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance. (arXiv:2308.05034v1 [cs.CR])

    [http://arxiv.org/abs/2308.05034](http://arxiv.org/abs/2308.05034)

    Kairos是第一个同时满足范围、攻击不可知性、时效性和攻击重建维度要求的溯源为基础的入侵检测和调查系统。

    

    溯源图是描述系统执行历史的结构化审计日志。最近的研究探索了各种技术来分析溯源图，以实现自动化主机入侵检测，特别关注高级持久性威胁。通过研究其设计文档，我们确定了四个常见维度，推动溯源为基础的入侵检测系统（PIDS）的发展：范围（PIDS能否检测跨应用边界渗透的现代攻击？）、攻击不可知性（PIDS能否在没有攻击特征先验知识的情况下检测新型攻击？）、时效性（PIDS能否高效监视主机系统运行？）和攻击重建（PIDS能否从大型溯源图中提炼攻击活动，以便系统管理员能够轻松理解并迅速应对系统入侵？）。我们提出了KAIROS，这是第一个同时满足所有四个维度要求的PIDS，而现有的方法不能做到。

    Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approac
    
[^149]: 使用随机线性分类器进行概率不变学习

    Probabilistic Invariant Learning with Randomized Linear Classifiers. (arXiv:2308.04412v1 [cs.LG])

    [http://arxiv.org/abs/2308.04412](http://arxiv.org/abs/2308.04412)

    本文介绍了一种使用随机线性分类器进行概率不变学习的方法，通过接受概率化的普遍逼近和不变性，设计了能同时具有表达能力和不变性的模型，并且使用更少的资源。通过实验证明了这种方法在分类任务中的有效性。

    

    设计既具有表达能力又能保持任务已知不变性的模型是一个越来越困难的问题。现有解决方案在不变性和计算或内存资源之间进行权衡。在这项工作中，我们展示了如何利用随机性设计既具表达能力又具不变性但使用更少资源的模型。受随机算法的启发，我们的关键洞察是接受概率化的普遍逼近和不变性可以减少资源需求。具体而言，我们提出了一类称为随机线性分类器 (RLCs) 的二分类模型。我们给出了参数和样本大小的条件，在这些条件下，RLCs 可以以高概率逼近任何（平滑）函数，并保持对紧致群变换的不变性。利用这一结果，我们设计了三种可验证地概率不变的 RLCs，用于集合、图和球形数据的分类任务。我们展示了这些模型如何实现概率不变性。

    Designing models that are both expressive and preserve known invariances of tasks is an increasingly hard problem. Existing solutions tradeoff invariance for computational or memory resources. In this work, we show how to leverage randomness and design models that are both expressive and invariant but use less resources. Inspired by randomized algorithms, our key insight is that accepting probabilistic notions of universal approximation and invariance can reduce our resource requirements. More specifically, we propose a class of binary classification models called Randomized Linear Classifiers (RLCs). We give parameter and sample size conditions in which RLCs can, with high probability, approximate any (smooth) function while preserving invariance to compact group transformations. Leveraging this result, we design three RLCs that are provably probabilistic invariant for classification tasks over sets, graphs, and spherical data. We show how these models can achieve probabilistic invari
    
[^150]: 架起可信度与开放世界学习的桥梁：一种探索性神经方法，用于增强可解释性、泛化性和鲁棒性

    Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness. (arXiv:2308.03666v1 [stat.ML])

    [http://arxiv.org/abs/2308.03666](http://arxiv.org/abs/2308.03666)

    该论文探索了一种神经方法，用于解决当前人工智能系统存在的可信度问题，包括预测结果解释不足、学习模型泛化性不足和不适应不确定环境的问题，以提高可信度网络的设计级可解释性和泛化性能。

    

    随着研究人员努力缩小机器智能与人类之间的差距，通过发展人工智能技术，我们必须认识到可信度在开放世界中的关键重要性，在日常生活的各个方面对每个人都已经无处不在。然而，目前的人工智能系统存在几个挑战，可能会导致信任危机：1）对预测结果的解释不足；2）学习模型的泛化性不足；3）对不确定环境的适应能力差。因此，我们探索了一种神经程序，用于架起可信度与开放世界学习之间的桥梁，从单模态扩展到多模态场景，以供读者使用。1）为了增强设计级可解释性，我们首先定制了具有特定物理含义的可信网络；2）然后，通过灵活的学习正则化器设计环境福祉任务接口，以改善可信网络的泛化性能。

    As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy
    
[^151]: 无损转换和统计推断中的过量风险界限研究

    Lossless Transformations and Excess Risk Bounds in Statistical Inference. (arXiv:2307.16735v1 [cs.IT])

    [http://arxiv.org/abs/2307.16735](http://arxiv.org/abs/2307.16735)

    在统计推断中，我们研究了无损转换和过量风险的概念。我们提出了无损转换的特征，并构建了一个用于判断给定转换是否是无损的统计量。我们还引入了delta-无损转换的概念，并给出了充分条件。这些研究在分类、非参数回归和投资组合策略等领域具有应用价值。

    

    我们研究了统计推断中的过量最小风险，定义为从观测到的特征向量中估计随机变量的最小期望损失与从特征向量的转换（统计量）中估计相同随机变量的最小期望损失之间的差异。在描述了无损转换（即对于所有损失函数，过量风险为零的转换）之后，我们构建了一个对假设进行分区检验的统计量，用于判断给定转换是否为无损转换，并证明对于i.i.d.数据，该检验是强一致的。更一般地，我们根据信息理论给出了过量风险的上界，该上界在相当一般的损失函数类上都是一致的。基于这些界限，我们引入了“delta-无损转换”的概念，并给出了给定转换普遍是delta-无损的充分条件。该研究在分类、非参数回归、投资组合策略等方面具有应用价值。

    We study the excess minimum risk in statistical inference, defined as the difference between the minimum expected loss in estimating a random variable from an observed feature vector and the minimum expected loss in estimating the same random variable from a transformation (statistic) of the feature vector. After characterizing lossless transformations, i.e., transformations for which the excess risk is zero for all loss functions, we construct a partitioning test statistic for the hypothesis that a given transformation is lossless and show that for i.i.d. data the test is strongly consistent. More generally, we develop information-theoretic upper bounds on the excess risk that uniformly hold over fairly general classes of loss functions. Based on these bounds, we introduce the notion of a delta-lossless transformation and give sufficient conditions for a given transformation to be universally delta-lossless. Applications to classification, nonparametric regression, portfolio strategie
    
[^152]: AdvDiff:使用扩散模型生成无限制的对抗样本

    AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models. (arXiv:2307.12499v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12499](http://arxiv.org/abs/2307.12499)

    本文提出了一种使用扩散模型生成无限制对抗样本的方法AdvDiff。通过设计两种新的对抗引导技术，在扩散模型的逆生成过程中进行对抗采样，从而有效地生成高质量、逼真的对抗样本。

    

    无限制的对抗攻击对深度学习模型和对抗防御技术构成严重威胁。它们对深度学习应用造成严重的安全问题，因为它们可以有效地绕过防御机制。然而，先前的攻击方法通常利用生成对抗网络（GAN），这些网络在理论上无法证明，因此在大规模数据集（如ImageNet）上通过引入对抗目标生成的例子是不现实的。在本文中，我们提出了一种新的方法，称为AdvDiff，使用扩散模型生成无限制的对抗样本。我们设计了两种新的对抗引导技术，在扩散模型的逆生成过程中进行对抗采样。这两种技术通过可解释的目标分类器梯度集成生成高质量、逼真的对抗样本非常有效和稳定。在MNIST和ImageNet数据集上的实验结果表明，AdvDiff能够生成高质量、逼真的对抗样本。

    Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvD
    
[^153]: 通过变分自动 编码器实现灵活高效的空间极端值模拟

    Flexible and efficient spatial extremes emulation via variational autoencoders. (arXiv:2307.08079v1 [stat.ML])

    [http://arxiv.org/abs/2307.08079](http://arxiv.org/abs/2307.08079)

    本文提出了一种新的空间极端值模型，通过集成在变分自动编码器的结构中，可以灵活、高效地模拟具有非平稳相关性的极端事件。实验证明，在时间效率和性能上，相对于传统的贝叶斯推断和许多具有平稳相关性的空间极端值模型，我们的方法具有优势。

    

    许多现实世界的过程具有复杂的尾依赖结构，这种结构无法使用传统的高斯过程来描述。更灵活的空间极端值模型， 如高斯尺度混合模型和单站点调节模型，具有吸引人的极端依赖性质，但往往难以拟合和模拟。本文中，我们提出了一种新的空间极端值模型，具有灵活和非平稳的相关性属性，并将其集成到变分自动编码器 (extVAE) 的编码-解码结构中。 extVAE 可以作为一个时空模拟器，对潜在的机制模型输出状态的分布进行建模，并产生具有与输入相同属性的输出，尤其是在尾部区域。通过广泛的模拟研究，我们证明我们的extVAE比传统的贝叶斯推断更高效，并且在具有 平稳相关性结构的许多空间极端值模型中表现 更好。

    Many real-world processes have complex tail dependence structures that cannot be characterized using classical Gaussian processes. More flexible spatial extremes models such as Gaussian scale mixtures and single-station conditioning models exhibit appealing extremal dependence properties but are often exceedingly prohibitive to fit and simulate from. In this paper, we develop a new spatial extremes model that has flexible and non-stationary dependence properties, and we integrate it in the encoding-decoding structure of a variational autoencoder (extVAE). The extVAE can be used as a spatio-temporal emulator that characterizes the distribution of potential mechanistic model output states and produces outputs that have the same properties as the inputs, especially in the tail. Through extensive simulation studies, we show that our extVAE is vastly more time-efficient than traditional Bayesian inference while also outperforming many spatial extremes models with a stationary dependence str
    
[^154]: TinyMetaFed: 高效的用于TinyML的联邦元学习

    TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v1 [cs.LG])

    [http://arxiv.org/abs/2307.06822](http://arxiv.org/abs/2307.06822)

    TinyMetaFed是一个适用于TinyML的高效联邦元学习框架，通过协同训练神经网络初始化，在小型设备上能够快速微调，同时实现通信节省和隐私保护。

    

    Tiny Machine Learning (TinyML)领域在使得机器学习在低功耗设备（如微控制器）上实现方面取得了重大进展。这些微型设备的普及引发了一个问题，即聚合它们的知识是否能够使TinyML应用受益。联邦元学习是这个问题的一个有前景的答案，因为它解决了现实世界中标记数据的稀缺性和设备之间的异构数据分布。然而，部署TinyML硬件面临着独特的资源限制，现有方法由于能源、隐私和通信限制而不实用。我们引入了TinyMetaFed，一个适用于TinyML的模型无关的元学习框架。TinyMetaFed促进了神经网络初始化的协同训练，可以在新设备上快速微调。它通过部分本地重构和Top-P%选择性通信提供通信节省和隐私保护，具有计算效果好。

    The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational eff
    
[^155]: 洋葱宇宙算法：在弱监督学习中的应用

    Onion Universe Algorithm: Applications in Weakly Supervised Learning. (arXiv:2307.04870v1 [cs.LG])

    [http://arxiv.org/abs/2307.04870](http://arxiv.org/abs/2307.04870)

    洋葱宇宙算法是一种新颖的集成学习分类方法，可作为弱监督学习的标签模型。它在实现上简单、计算高效，适用于无完全标记数据的情况。经验证明，它在常见的基准数据集上表现出色。

    

    本文介绍了洋葱宇宙算法(OUA)，一种新颖的集成学习分类方法。特别地，我们展示了它作为弱监督学习标签模型的适用性。OUA在实现上简单，计算效率高，并且不依赖于数据或弱信号的任何假设。该模型非常适用于没有完全标记数据的情况。我们的方法基于对由弱信号所构成的空间的几何解释。经验证实，OUA在一般的弱信号集合下具有潜在的几何结构，并且在实践中表现良好。我们还通过实验证据展示，OUA在常见的基准数据集上相比现有的弱监督学习标签模型表现出色。

    We introduce Onion Universe Algorithm (OUA), a novel classification method in ensemble learning. In particular, we show its applicability as a label model for weakly supervised learning. OUA offers simplicity in implementation, computational efficiency, and does not rely on any assumptions regarding the data or weak signals. The model is well suited for scenarios where fully labeled data is not available. Our method is built upon geometrical interpretation of the space spanned by weak signals. Empirical results support our analysis of the hidden geometric structure underlying general set of weak signals and also illustrates that OUA works well in practice. We show empirical evidence that OUA performs favorably on common benchmark datasets compared to existing label models for weakly supervised learning.
    
[^156]: 准确和校准模型的集合学习

    Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02245](http://arxiv.org/abs/2307.02245)

    提出了一种集合学习方法(OKO)来解决机器学习中的模型过度自信和校准不良问题，通过最小化集合的交叉熵误差，从而提高准确性和校准效果，并在有限的训练数据和类别不平衡情况下表现出更好的结果。

    

    模型过度自信和校准不良在机器学习中很常见，并且在应用标准经验风险最小化时很难解决。在这项工作中，我们提出了一种新的方法来缓解这些问题，我们称之为奇数-$k$-去除学习（OKO），它通过最小化集合的交叉熵误差而不是单个示例的误差来实现。这自然地使模型能够捕捉数据示例之间的相关性，并在有限的训练数据和类别不平衡的情况下实现更好的准确性和校准。令人惊讶的是，即使使用硬标签进行训练并且不进行任何额外的校准参数调整，如温度缩放，OKO通常也能获得更好的校准效果。我们提供了理论上的证明，证明OKO自然地能够获得更好的校准效果，并进行了广泛的实验分析以验证我们的理论发现。我们强调，OKO是一个通用的框架，可以很容易地适应许多不同的情境。

    Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the
    
[^157]: 用于机器学习的时态图基准测试

    Temporal Graph Benchmark for Machine Learning on Temporal Graphs. (arXiv:2307.01026v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.01026](http://arxiv.org/abs/2307.01026)

    TGB是一个用于在时态图上进行机器学习模型评估的基准测试数据集集合，具有挑战性和多样化，涵盖了节点和边级预测任务，对多种领域的时态图进行了广泛的基准测试，并发现常见模型的性能可能有巨大差异。在动态节点属性预测任务中，我们展示了简单方法可能比现有的时态图模型表现出更好的性能。

    

    我们提出了Temporal Graph Benchmark（TGB），这是一个用于在时态图上进行真实、可重现和强大的机器学习模型评估的挑战性和多样化的基准测试数据集集合。TGB数据集具有大规模、跨年的时长，包括节点和边级预测任务，并涵盖了社交、贸易、交易和交通网络等多种领域。针对这两个任务，我们设计了基于实际使用案例的评估协议。我们对每个数据集进行了广泛的基准测试，并发现常见模型的性能在不同的数据集上可能存在巨大差异。此外，在动态节点属性预测任务中，我们展示了简单方法常常比现有的时态图模型表现出更好的性能。我们相信这些发现为未来的时态图研究开辟了机会。最后，TGB提供了一个自动化的机器学习流程，用于可重现和可访问的时态图研究，包括...

    We present the Temporal Graph Benchmark (TGB), a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation of machine learning models on temporal graphs. TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks. For both tasks, we design evaluation protocols based on realistic use-cases. We extensively benchmark each dataset and find that the performance of common models can vary drastically across datasets. In addition, on dynamic node property prediction tasks, we show that simple methods often achieve superior performance compared to existing temporal graph models. We believe that these findings open up opportunities for future research on temporal graphs. Finally, TGB provides an automated machine learning pipeline for reproducible and accessible temporal graph research, inclu
    
[^158]: 渐近保持的卷积Deep Operator网络捕捉多尺度线性输运方程的扩散行为

    Asymptotic-Preserving Convolutional DeepONets Capture the Diffusive Behavior of the Multiscale Linear Transport Equations. (arXiv:2306.15891v1 [cs.LG])

    [http://arxiv.org/abs/2306.15891](http://arxiv.org/abs/2306.15891)

    本文介绍了两种新型的渐近保持的卷积Deep Operator网络（APCONs），用于解决多尺度时变线性输运问题。该方法通过采用多个局部卷积操作、池化和激活操作来捕捉线性输运问题的扩散行为，并验证了方法的有效性。

    

    本文介绍了两种新型的渐近保持的卷积Deep Operator网络（APCONs），旨在解决多尺度时变线性输运问题。我们观察到，使用修改过的多层感知器（MLP）的基本物理约束DeepONets可能在保持期望的宏观行为上表现出不稳定性。因此，需要使用渐近保持的损失函数。受扩散方程中的热核的启发，我们提出了一种新的架构，称为卷积Deep Operator网络，它在每个滤波器层中采用多个局部卷积操作而不是全局热核，并结合池化和激活操作。我们的APCON方法的参数数量与网格大小无关，并能够捕捉线性输运问题的扩散行为。最后，通过几个数值实例验证了我们方法的有效性。

    In this paper, we introduce two types of novel Asymptotic-Preserving Convolutional Deep Operator Networks (APCONs) designed to address the multiscale time-dependent linear transport problem. We observe that the vanilla physics-informed DeepONets with modified MLP may exhibit instability in maintaining the desired limiting macroscopic behavior. Therefore, this necessitates the utilization of an asymptotic-preserving loss function. Drawing inspiration from the heat kernel in the diffusion equation, we propose a new architecture called Convolutional Deep Operator Networks, which employ multiple local convolution operations instead of a global heat kernel, along with pooling and activation operations in each filter layer. Our APCON methods possess a parameter count that is independent of the grid size and are capable of capturing the diffusive behavior of the linear transport problem. Finally, we validate the effectiveness of our methods through several numerical examples.
    
[^159]: DynaBench: 从低分辨率数据中学习动力系统的基准数据集。

    DynaBench: A benchmark dataset for learning dynamical systems from low-resolution data. (arXiv:2306.05805v1 [cs.LG])

    [http://arxiv.org/abs/2306.05805](http://arxiv.org/abs/2306.05805)

    DynaBench是一个新的模拟基准数据集，用于直接从低分辨率的稀疏数据中学习动力系统，评估了多个机器学习模型的预测性能。

    

    先前从数据中学习物理系统的工作侧重于高分辨率网格结构测量。但是，现实世界中这种系统的知识（例如天气数据）依赖于稀疏分布的测量站。在本文中，我们介绍了一个新颖的模拟基准数据集DynaBench，用于直接从稀疏散布的数据中学习动力系统，不需要先前了解该方程式。该数据集专注于预测动力系统的演变，使用低分辨率、非结构化的测量方式。我们模拟了六个不同的偏微分方程，涵盖了文献中常用的各种物理系统，并评估了多个机器学习模型，包括传统的图神经网络和点云处理模型，以预测系统的演变。该基准数据集可以期望成为一种开箱即用的工具，用于评估模型性能。

    Previous work on learning physical systems from data has focused on high-resolution grid-structured measurements. However, real-world knowledge of such systems (e.g. weather data) relies on sparsely scattered measuring stations. In this paper, we introduce a novel simulated benchmark dataset, DynaBench, for learning dynamical systems directly from sparsely scattered data without prior knowledge of the equations. The dataset focuses on predicting the evolution of a dynamical system from low-resolution, unstructured measurements. We simulate six different partial differential equations covering a variety of physical systems commonly used in the literature and evaluate several machine learning models, including traditional graph neural networks and point cloud processing models, with the task of predicting the evolution of the system. The proposed benchmark dataset is expected to advance the state of art as an out-of-the-box easy-to-use tool for evaluating models in a setting where only u
    
[^160]: AdaGrad算法在非凸目标优化中的收敛性: 简明证明和宽松假设

    Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed Assumptions. (arXiv:2305.18471v1 [cs.LG])

    [http://arxiv.org/abs/2305.18471](http://arxiv.org/abs/2305.18471)

    本文提出了仅有互易噪声方差和有界平滑性假设的非凸目标的AdaGrad算法的简单收敛性证明，证明中基于辅助函数$\xi$，比现有结果更紧密，在超参数化的情况下，能够确保梯度范数小于$\varepsilon$的迭代次数为$\mathcal{O}(\frac{1}{\varepsilon^2})$，并考虑了一种实际平滑假设$(L_0,L_1)$-平滑条件。

    

    我们提供了针对仅有互易噪声方差和有界平滑性假设的非凸目标的AdaGrad算法的简单收敛性证明。该证明基本上基于一个新颖的辅助函数$\xi$，有助于消除处理AdaGrad更新的分子和分母之间相关性的复杂性。通过简单的证明，我们能够获得比现有结果\citep{faw2022power}更紧密的结果，并将分析扩展到几种新的重要情况。具体而言，在超参数化的情况下，我们表明AdaGrad只需要$\mathcal{O}(\frac{1}{\varepsilon^2})$次迭代，就可以确保梯度范数小于$\varepsilon$，这与SGD的速率相匹配，并且比AdaGrad的现有速率$\mathcal{O}(\frac{1}{\varepsilon^4})$明显更紧密。然后，我们放弃有界平滑假设，并考虑一种称为$(L_0,L_1)$-平滑条件的实际平滑假设，该假设允许本地平滑性增长

    We provide a simple convergence proof for AdaGrad optimizing non-convex objectives under only affine noise variance and bounded smoothness assumptions. The proof is essentially based on a novel auxiliary function $\xi$ that helps eliminate the complexity of handling the correlation between the numerator and denominator of AdaGrad's update. Leveraging simple proofs, we are able to obtain tighter results than existing results \citep{faw2022power} and extend the analysis to several new and important cases. Specifically, for the over-parameterized regime, we show that AdaGrad needs only $\mathcal{O}(\frac{1}{\varepsilon^2})$ iterations to ensure the gradient norm smaller than $\varepsilon$, which matches the rate of SGD and significantly tighter than existing rates $\mathcal{O}(\frac{1}{\varepsilon^4})$ for AdaGrad. We then discard the bounded smoothness assumption and consider a realistic assumption on smoothness called $(L_0,L_1)$-smooth condition, which allows local smoothness to grow w
    
[^161]: 多实例部分标签学习的消岐注意嵌入

    Disambiguated Attention Embedding for Multi-Instance Partial-Label Learning. (arXiv:2305.16912v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16912](http://arxiv.org/abs/2305.16912)

    本文提出了一种用于多实例部分标签学习的消岐注意嵌入算法（DEMIPL），通过将多实例包嵌入到单个向量表示中，解决了现有方法在忽视全局包级信息和对负实例预测敏感的问题。

    

    在许多实际任务中，相关对象可以表示为与候选标签集相关联的多实例包，该集合包括一个真实的标签和多个误报标签。多实例部分标签学习（MIPL）是一种应对此类任务的学习范式，并已取得良好表现。现有的MIPL方法遵循实例空间范式，通过为每个实例分配袋子的扩充候选标签集并从实例级标签汇总包级标签。然而，该方案可能是次优的，因为忽视了全局的包级信息，并且包的预测标签对于负实例的预测非常敏感。在本文中，我们研究了一种将多实例包嵌入到单个向量表示的替代方案。因此，我们提出了一种直观的算法DEMIPL，即用于多实例部分标签学习的消岐注意嵌入。DEMIPL利用了一种消岐注意机制。

    In many real-world tasks, the concerned objects can be represented as a multi-instance bag associated with a candidate label set, which consists of one ground-truth label and several false positive labels. Multi-instance partial-label learning (MIPL) is a learning paradigm to deal with such tasks and has achieved favorable performances. Existing MIPL approach follows the instance-space paradigm by assigning augmented candidate label sets of bags to each instance and aggregating bag-level labels from instance-level labels. However, this scheme may be suboptimal as global bag-level information is ignored and the predicted labels of bags are sensitive to predictions of negative instances. In this paper, we study an alternative scheme where a multi-instance bag is embedded into a single vector representation. Accordingly, an intuitive algorithm named DEMIPL, i.e., Disambiguated attention Embedding for Multi-Instance Partial-Label learning, is proposed. DEMIPL employs a disambiguation atten
    
[^162]: 采用X射线微束数据几何变换增强语音发音分析

    Enhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset. (arXiv:2305.10775v1 [eess.AS])

    [http://arxiv.org/abs/2305.10775](http://arxiv.org/abs/2305.10775)

    该论文提出了一种新的几何变换方法，将X-Ray Microbeam数据集中的解剖标记物的X-Y坐标沿中矢状面映射到多个相对测量中，进而改进了测量的准确性。

    

    准确分析语音发音对于语音分析至关重要。然而，声门的X-Y坐标严重依赖于发言者的解剖结构和颗粒位置的可变性，现有的X射线微束数据集（XRMB）中的解剖标志物映射方法无法捕捉到发音道的整个解剖学。在本文中，我们提出了一种新的几何变换，改进了这些测量的准确性。我们的变换将解剖标记物的X-Y坐标沿中矢状面映射到6个相对测量中：唇缝张度（LA）、唇部突出（LP）、舌体收缩位置（TTCL）、度数（TBCD）、舌尖收缩位置（TTCL）和度数（TTCD）。我们的创新贡献是将腭板追踪延伸到推测的咽喉前线，从而改善了舌体收缩的测量。

    Accurate analysis of speech articulation is crucial for speech analysis. However, X-Y coordinates of articulators strongly depend on the anatomy of the speakers and the variability of pellet placements, and existing methods for mapping anatomical landmarks in the X-ray Microbeam Dataset (XRMB) fail to capture the entire anatomy of the vocal tract. In this paper, we propose a new geometric transformation that improves the accuracy of these measurements. Our transformation maps anatomical landmarks' X-Y coordinates along the midsagittal plane onto six relative measures: Lip Aperture (LA), Lip Protusion (LP), Tongue Body Constriction Location (TTCL), Degree (TBCD), Tongue Tip Constriction Location (TTCL) and Degree (TTCD). Our novel contribution is the extension of the palate trace towards the inferred anterior pharyngeal line, which improves measurements of tongue body constriction.
    
[^163]: 移动机器人全身操作的因果策略梯度

    Causal Policy Gradient for Whole-Body Mobile Manipulation. (arXiv:2305.04866v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.04866](http://arxiv.org/abs/2305.04866)

    本文提出了一种新框架——因果MoMa，可以训练适用于典型MoMa任务的策略，在此框架下，机动和交互自由度可以同时组合，并且不需要人类领域知识来划分动作空间或将动作部分与子目标匹配。

    

    开发下一代家庭机器人助手需要结合机动和交互能力，即通常所说的移动操作。由于机器人的大动作空间和任务常见的多目标性质，例如能够有效地达到目标且避免障碍，移动操作任务很难。目前的方法通常根据人工匹配动作空间的部分到移动操作子目标（例如用于移动目标的基础动作和用于操作的手臂动作）将任务分为不带操作的导航和不带机动的固定操作。此解决方案防止了机动和交互自由度的同时组合，并且需要人类领域知识来划分动作空间并将动作部分与子目标匹配。在本文中，我们介绍了一种新的框架——因果MoMa，该框架用于训练典型MoMa任务的策略。

    Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subsp
    
[^164]: 金融时间序列预测的深度学习技术: 2020-2022年的最新进展综述

    Deep learning techniques for financial time series forecasting: A review of recent advancements: 2020-2022. (arXiv:2305.04811v1 [q-fin.ST])

    [http://arxiv.org/abs/2305.04811](http://arxiv.org/abs/2305.04811)

    本研究综述了深度学习模型在金融时间序列预测上的最新研究进展，包括不同数据来源和神经网络结构以及实现细节，并提供未来研究方向的建议。

    

    预测金融时间序列一直是一个具有挑战性的问题，吸引了研究人员和从业者的关注。在过去几十年中，统计学和机器学习技术都被探索用于开发有效的预测模型。随着深度学习模型的最新发展，金融时间序列预测模型得到了显著的发展，这些新进展往往难以跟上。因此，我们进行了这篇文献综述，对2020年至2022年间基于深度学习模型用于预测金融价格的最新研究进行了全面评估。我们的综述介绍了不同的数据来源和神经网络结构，以及它们的实现细节。我们的目标是确保感兴趣的研究人员了解该领域的最新发展，并便于根据先前研究中使用的模型选择基准线。此外，我们提供了未来研究方向的建议。

    Forecasting financial time series has long been a challenging problem that has attracted attention from both researchers and practitioners. Statistical and machine learning techniques have both been explored to develop effective forecasting models in the past few decades. With recent developments in deep learning models, financial time series forecasting models have advanced significantly, and these developments are often difficult to keep up with. Hence, we have conducted this literature review to provide a comprehensive assessment of recent research from 2020 to 2022 on deep learning models used to predict prices based on financial time series. Our review presents different data sources and neural network structures, as well as their implementation details. Our goals are to ensure that interested researchers remain up-to-date on recent developments in the field and facilitate the selection of baselines based on models used in prior studies. Additionally, we provide suggestions for fu
    
[^165]: 学习6D非抓取式操作的混合演员-评论员地图

    Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])

    [http://arxiv.org/abs/2305.03942](http://arxiv.org/abs/2305.03942)

    论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。

    

    在人类的灵巧性中，非抓取式操作是操作物体的重要组成部分。非抓取式操纵可以使与物体的交互更加复杂，但也在推理交互方面提出了挑战。在本文中，我们引入了一个名为HACMan的混合演员评论员地图，这是一种使用点云观察的6D非抓取式物体操作的强化学习方法。HACMan提出了一种时间抽象和空间基础的物体中心动作表示，该表示包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。我们修改了一个现有的离线策略RL算法，以在这种混合的离散-连续动作表示学习。我们在仿真和现实世界中对HACMan进行了6D物体姿态对齐任务的评估。在最难的任务版本中，通过随机初始化物体和机器人配置，HACMan的表现优于现有的基线方法。

    Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
    
[^166]: 复杂动力系统中物理启示下的表征学习用于自然组织

    Physics-Informed Representation Learning for Emergent Organization in Complex Dynamical Systems. (arXiv:2304.12586v1 [physics.comp-ph])

    [http://arxiv.org/abs/2304.12586](http://arxiv.org/abs/2304.12586)

    该论文提出了一个基于物理启示的自然组织表征学习框架，通过局部因果状态捕获复杂时空系统中的有序行为和相干结构。该方法在实际领域科学问题中具有很好的适用性。

    

    非线性相互作用的系统组件经常引入不稳定性，产生具有新属性和不同时空尺度的现象。这被称为自发自组织，是在远离热力学平衡的系统中普遍存在的。我们引入了一个理论基础的框架，用于通过数据驱动的算法实现自然组织。它的基本构件是通过局部交互捕获信息在系统中传播方式的时空光锥。我们展示了预测光锥等价类——局部因果状态——捕获复杂时空系统中的有序行为和相干结构。通过我们的无监督物理启示的机器学习算法和高性能计算实现，我们展示了局部因果状态在实际领域科学问题中的适用性。我们展示了局部因果状态捕获旋涡及其功率的情况，这些是自然和工程领域中普遍存在的现象。

    Nonlinearly interacting system components often introduce instabilities that generate phenomena with new properties and at different space-time scales than the components. This is known as spontaneous self-organization and is ubiquitous in systems far from thermodynamic equilibrium. We introduce a theoretically-grounded framework for emergent organization that, via data-driven algorithms, is constructive in practice. Its building blocks are spacetime lightcones that capture how information propagates across a system through local interactions. We show that predictive equivalence classes of lightcones, local causal states, capture organized behaviors and coherent structures in complex spatiotemporal systems. Using our unsupervised physics-informed machine learning algorithm and a high-performance computing implementation, we demonstrate the applicability of the local causal states for real-world domain science problems. We show that the local causal states capture vortices and their pow
    
[^167]: 通过平方和优化综合稳定的降阶视动态控制器，应用于非线性系统

    Synthesizing Stable Reduced-Order Visuomotor Policies for Nonlinear Systems via Sums-of-Squares Optimization. (arXiv:2304.12405v1 [cs.RO])

    [http://arxiv.org/abs/2304.12405](http://arxiv.org/abs/2304.12405)

    该论文提出了一种通过平方和优化，综合动态的、降阶的输出反馈多项式控制器的方法，应用于控制仿射的非线性系统，并使用学习的感知模块和视觉观测来稳定地运行到目标状态。该方法提供了在观测噪声存在的情况下的稳定性保证。

    

    我们提出了一种方法，用于综合动态的、降阶的输出反馈多项式控制器，应用于控制仿射的非线性系统，并使用学习的感知模块和视觉观测来保证在反馈控制循环中稳定地运行到目标状态。我们利用Lyapunov分析来制定综合这种策略的问题。该问题在策略参数和用于证明策略稳定性的Lyapunov函数方面是非凸的。为了近似地解决这个问题，我们提出了两种方法：第一种解决了一系列平方和优化问题，以迭代改进一个通过构造可证明的稳定策略，而第二种则直接对多项式策略的参数进行基于梯度的优化，并在后验上验证其闭环稳定性。我们扩展了我们的方法，以在观测噪声存在的情况下提供稳定性保证，该情况实际上是由于观测误差而引起的。

    We present a method for synthesizing dynamic, reduced-order output-feedback polynomial control policies for control-affine nonlinear systems which guarantees runtime stability to a goal state, when using visual observations and a learned perception module in the feedback control loop. We leverage Lyapunov analysis to formulate the problem of synthesizing such policies. This problem is nonconvex in the policy parameters and the Lyapunov function that is used to prove the stability of the policy. To solve this problem approximately, we propose two approaches: the first solves a sequence of sum-of-squares optimization problems to iteratively improve a policy which is provably-stable by construction, while the second directly performs gradient-based optimization on the parameters of the polynomial policy, and its closed-loop stability is verified a posteriori. We extend our approach to provide stability guarantees in the presence of observation noise, which realistically arises due to erro
    
[^168]: 模型稀疏化可以简化机器反学习

    Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])

    [http://arxiv.org/abs/2304.04934](http://arxiv.org/abs/2304.04934)

    本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。

    

    最近的数据管制要求机器反学习（MU）：从模型中移除指定样例的影响。虽然可以通过使用剩余数据从头开始进行模型重新训练来进行精确反学习，但是其计算成本导致了近似但高效的反学习方案的开发。除了数据中心的MU解决方案，我们通过一种新颖的基于模型的视角推进MU：通过权值修剪进行稀疏化。我们的理论和实践结果表明，模型稀疏性可以提高近似反学习器的多标准反学习性能，缩小近似间隙，同时保持高效。有了这个认识，我们制定了两个新的稀疏感知反学习元方案，称为“先修剪，然后反学习”和“稀疏感知反学习”。广泛的实验表明，我们的发现和提议在各种场景下始终有益于MU，包括按类数据擦除、随机数据擦除和后门数据伪造等。

    Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
    
[^169]: Dice半度量损失函数：用软标签优化Dice分数

    Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])

    [http://arxiv.org/abs/2303.16296](http://arxiv.org/abs/2303.16296)

    本文提出的Dice半度量损失函数可在软标签设置中使用，在医疗成像领域的分割方案中与使用软标签的研究相结合，可以获得更好的Dice分数和模型校准。

    

    在医学成像领域的许多自动分割方案中，软Dice损失（SDL）发挥了关键作用。在过去几年中，人们已经揭示了其优越性能背后的一些原因并进一步探索了其优化。然而，目前还没有实现支持直接在软标签设置中使用它的方案。因此，在使用SDL和研究利用软标签的同时进行模型校准的协同作用仍然缺失。在本文中，我们介绍了Dice半度量损失函数（DMLs），它们（i）在硬标签的标准设置下与SDL相同，但（ii）也可在软标签设置中使用。我们在公共的QUBIQ、LiTS和KiTS基准测试上的实验证实了DMLs与软标签（如平均、标签平滑和知识蒸馏）的潜在协同作用，而DMLs与硬标签（如大多数投票和随机选择）相比，产生了更优秀的Dice分数和模型校准。

    The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
    
[^170]: 延迟感知的分层联邦学习

    Delay-Aware Hierarchical Federated Learning. (arXiv:2303.12414v1 [cs.LG])

    [http://arxiv.org/abs/2303.12414](http://arxiv.org/abs/2303.12414)

    本论文提出了延迟感知的联邦学习(DFL)，通过解决边缘和云之间的通信延迟，提高了分布式机器学习模型训练的效率，并实现了一些政策以减少能量消耗和边缘到云端的通信。

    

    联邦学习作为一种在分布式环境下训练模型的方法，已经越来越受到关注。本文引入了延迟感知的联邦学习(DFL)，通过解决边缘和云之间的通信延迟，提高了分布式机器学习模型训练的效率。DFL在每个全局聚合间隔期间对设备数据集执行多个随机梯度下降迭代，并通过边缘服务器在本地子网络中间断地聚合模型参数。云服务器通过局部-全局合并器将本地模型与全局部署模型同步。DFL的收敛行为在广义数据异质性度量下进行了理论研究。得出了一组条件，以实现O(1/k)的次线性收敛率。基于这些发现，开发了一个自适应控制算法来实现DFL，并实现了一些政策以减少能量消耗和边缘到云端的通信。

    Federated learning has gained popularity as a means of training models distributed across the wireless edge. The paper introduces delay-aware federated learning (DFL) to improve the efficiency of distributed machine learning (ML) model training by addressing communication delays between edge and cloud. DFL employs multiple stochastic gradient descent iterations on device datasets during each global aggregation interval and intermittently aggregates model parameters through edge servers in local subnetworks. The cloud server synchronizes the local models with the global deployed model computed via a local-global combiner at global synchronization. The convergence behavior of DFL is theoretically investigated under a generalized data heterogeneity metric. A set of conditions is obtained to achieve the sub-linear convergence rate of O(1/k). Based on these findings, an adaptive control algorithm is developed for DFL, implementing policies to mitigate energy consumption and edge-to-cloud co
    
[^171]: 微小分类器电路：用于表格式数据的演化加速器

    Tiny Classifier Circuits: Evolving Accelerators for Tabular Data. (arXiv:2303.00031v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2303.00031](http://arxiv.org/abs/2303.00031)

    本文提出了一种自动生成分类器电路的方法，用于对表格式数据进行分类，利用演化算法搜索逻辑门空间，生成具有最大化训练预测准确性的微小分类器电路，在使用更少的硬件资源和功耗的同时，具有与传统机器学习技术相当的预测性能。

    

    边缘计算中典型的机器学习(ML)开发循环是在模型训练期间最大化性能，然后在部署在针对CPU、GPU、微控制器或定制硬件加速器的边缘设备上时最小化经过训练的模型的内存/面积占用。本文提出了一种自动生成分类器电路的方法，用于对表格式数据进行分类，具有与传统ML技术相当的预测性能，同时使用更少的硬件资源和功耗。该提出的方法利用演化算法在逻辑门空间上进行搜索，并自动生成具有最大化训练预测准确性的分类器电路。分类器电路非常微小(即由不超过300个逻辑门组成)，被称为"微小分类器"电路，并可以在ASIC上或者FPGA上高效地实现。我们通过实验证明了自动生成微小分类器电路的方法的有效性。

    A typical machine learning (ML) development cycle for edge computing is to maximise the performance during model training and then minimise the memory/area footprint of the trained model for deployment on edge devices targeting CPUs, GPUs, microcontrollers, or custom hardware accelerators. This paper proposes a methodology for automatically generating predictor circuits for classification of tabular data with comparable prediction performance to conventional ML techniques while using substantially fewer hardware resources and power. The proposed methodology uses an evolutionary algorithm to search over the space of logic gates and automatically generates a classifier circuit with maximised training prediction accuracy. Classifier circuits are so tiny (i.e., consisting of no more than 300 logic gates) that they are called "Tiny Classifier" circuits, and can efficiently be implemented in ASIC or on an FPGA. We empirically evaluate the automatic Tiny Classifier circuit generation methodol
    
[^172]: 使用上下文抑制层级变分自编码器中的后验坍塌问题

    Discouraging posterior collapse in hierarchical Variational Autoencoders using context. (arXiv:2302.09976v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09976](http://arxiv.org/abs/2302.09976)

    本研究提出了一种具有上下文的深层级变分自编码器，用于抑制后验坍塌问题。实验证明，该修改可以更好地利用潜在空间并且不会影响模型的生成能力。

    

    层级变分自编码器(VAEs)是最为流行的基于似然的生成模型之一。人们普遍认为自顶向下的层级VAEs可以有效地学习深层潜在结构并避免后验坍塌等问题。然而，我们发现这并不一定成立，后验坍塌问题仍然存在。为了避免这个问题，我们提出了一种具有上下文的深层级VAE。具体而言，我们使用离散余弦变换来获取最后一个潜在变量。在一系列实验中，我们观察到所提出的修改可以更好地利用潜在空间，并且不会损害模型的生成能力。

    Hierarchical Variational Autoencoders (VAEs) are among the most popular likelihood-based generative models. There is a consensus that the top-down hierarchical VAEs allow effective learning of deep latent structures and avoid problems like posterior collapse. Here, we show that this is not necessarily the case, and the problem of collapsing posteriors remains. To discourage this issue, we propose a deep hierarchical VAE with a context on top. Specifically, we use a Discrete Cosine Transform to obtain the last latent variable. In a series of experiments, we observe that the proposed modification allows us to achieve better utilization of the latent space and does not harm the model's generative abilities.
    
[^173]: 超似曲空间的大间隔分类的浑拟圆决策边界

    Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space. (arXiv:2302.06807v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06807](http://arxiv.org/abs/2302.06807)

    本文提出了一种大间隔分类器，它使用浑拟圆决策边界可以优化测地凸优化问题，实验结果表明其竞争性能优越。

    

    近年来，用超似曲空间表示层次结构化数据已经越来越流行，同时，文献中也提出了几个针对这些空间中数据分类的算法。这些算法主要使用超平面或测地线作为决策边界，使用大间隔分类器设置，从而导致一个非凸优化问题。在本文中，我们提出了一种基于浑拟圆决策边界的新型大间隔分类器，它可以导致一个测地凸优化问题，可以使用任何黎曼梯度下降技术来优化，保证全局最优解。我们展示了几个实验，展示了我们的分类器相比于 SOTA 的竞争性能。

    Hyperbolic spaces have been quite popular in the recent past for representing hierarchically organized data. Further, several classification algorithms for data in these spaces have been proposed in the literature. These algorithms mainly use either hyperplanes or geodesics for decision boundaries in a large margin classifiers setting leading to a non-convex optimization problem. In this paper, we propose a novel large margin classifier based on horospherical decision boundaries that leads to a geodesically convex optimization problem that can be optimized using any Riemannian gradient descent technique guaranteeing a globally optimal solution. We present several experiments depicting the competitive performance of our classifier in comparison to SOTA.
    
[^174]: 预测是否随意？在公平分类中评估自洽性

    Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11562](http://arxiv.org/abs/2301.11562)

    在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。

    

    在公平分类中，不同经过训练的模型之间的预测方差是一个重要但鲜为人知的误差来源问题。 实证表明，某些情况下，预测的方差差异非常大，以至于决策实际上是随意的。 为了研究这个问题，我们进行了大规模的实证研究，并做出了四个总体贡献：我们1）定义了一种基于方差的度量标准，称为自洽性，在测量和减少随意性时使用； 2）开发了一种合理的算法，当预测无法做出决策时，可以放弃分类； 3）进行了迄今为止有关公平分类中方差（相对于自洽性和随意性）作用的最大规模实证研究； 4）推出了一个工具包，使美国住房抵押贷款披露法案（HMDA）数据集易于用于未来研究。 总的来说，我们的实证结果揭示了关于可重复性的令人震惊的见解。当考虑到方差和随意预测的可能性时，大多数公平分类基准接近公平。 但是，一小部分实例显示出极大的随意性水平，这表明当前的模型可能无法处理某些类型的数据。

    Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
    
[^175]: 深度学习用于纠正CMIP6级地球系统模型中的偏差

    Deep learning for bias-correcting CMIP6-class Earth system models. (arXiv:2301.01253v2 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2301.01253](http://arxiv.org/abs/2301.01253)

    本研究展示了一种基于物理约束生成对抗网络的后处理方法，可以同时校正CMIP6级地球系统模型的局部频率分布和空间模式中的偏差。

    

    精确地模拟降水对于可靠预测人为全球变暖对生态和社会经济影响的关键。然而，产生降水的复杂跨尺度过程相互作用很难模拟，导致地球系统模型（ESMs）的字段可能存在强烈的偏差，特别是在极端情况下。目前最先进的偏差校正方法仅在每个单独的格点局部地纠正模拟频率分布的误差。到目前为止，改善ESM输出的不真实空间模式，即需要空间上下文的问题一直无法解决。本文中，我们展示了一种基于物理约束生成对抗网络（cGANs）的后处理方法，可以同时校正最先进的CMIP6级ESM的局部频率分布和空间模式中的偏差。虽然我们的方法在改善局部频率分布方面与金标准的偏差校正方法一样出色。

    The accurate representation of precipitation in Earth system models (ESMs) is crucial for reliable projections of the ecological and socioeconomic impacts in response to anthropogenic global warming. The complex cross-scale interactions of processes that produce precipitation are challenging to model, however, inducing potentially strong biases in ESM fields, especially regarding extremes. State-of-the-art bias correction methods only address errors in the simulated frequency distributions locally at every individual grid cell. Improving unrealistic spatial patterns of the ESM output, which would require spatial context, has not been possible so far. Here, we show that a post-processing method based on physically constrained generative adversarial networks (cGANs) can correct biases of a state-of-the-art, CMIP6-class ESM both in local frequency distributions and in the spatial patterns at once. While our method improves local frequency distributions equally well as gold-standard bias-a
    
[^176]: HyperBO+：使用分层高斯过程为贝叶斯优化预先训练通用先验

    HyperBO+: Pre-training a universal prior for Bayesian optimization with hierarchical Gaussian processes. (arXiv:2212.10538v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10538](http://arxiv.org/abs/2212.10538)

    本文提出了一种名为HyperBO+的方法，通过使用分层高斯过程的预先训练，实现了在具有不同输入空间的函数上普适于贝叶斯优化。研究人员设计了一种两步预先训练方法，并分析了其吸引人的渐近性质。

    

    贝叶斯优化在许多黑盒函数优化任务中被证明非常有效，但需要实践者精心选择适合其感兴趣函数的先验概率模型。研究人员已经探索了基于迁移学习的方法，如多任务BO、少样本BO和HyperBO，来自动学习先验概率。然而，这些方法通常假设所有任务的输入空间相同，限制了它们在不同输入空间上使用观测结果或推广学习到的先验概率模型的能力。本文提出了HyperBO+，这是一种基于分层高斯过程的预先训练方法，能够在具有不同输入空间的函数上普遍适用于贝叶斯优化。我们提出了一种两步预先训练方法，并分析了其吸引人的渐近性质及特点。

    Bayesian optimization (BO), while proved highly effective for many black-box function optimization tasks, requires practitioners to carefully select priors that well model their functions of interest. Rather than specifying by hand, researchers have investigated transfer learning based methods to automatically learn the priors, e.g. multi-task BO (Swersky et al., 2013), few-shot BO (Wistuba and Grabocka, 2021) and HyperBO (Wang et al., 2022). However, those prior learning methods typically assume that the input domains are the same for all tasks, weakening their ability to use observations on functions with different domains or generalize the learned priors to BO on different search spaces. In this work, we present HyperBO+: a pre-training approach for hierarchical Gaussian processes that enables the same prior to work universally for Bayesian optimization on functions with different domains. We propose a two-step pre-training method and analyze its appealing asymptotic properties and 
    
[^177]: 通过可学习的增强技术进行属性图聚类

    Attribute Graph Clustering via Learnable Augmentation. (arXiv:2212.03559v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03559](http://arxiv.org/abs/2212.03559)

    本研究提出了一种通过可学习增强技术进行属性图聚类的方法，该方法利用可学习的增强器为深度图聚类提供高质量和适合的增强样本，通过改进矩阵提高了聚类性能的可靠性。

    

    对比深度图聚类（CDGC）利用对比学习将节点分组到不同的簇中。更好的增强技术有助于提高对比样本的质量，因此成为改善性能的关键因素之一。然而，现有方法中的增强样本始终由人类经验预定义，并且与下游任务聚类无关，从而导致人力资源成本高和性能差。为此，我们提出了一种通过可学习增强技术进行属性图聚类的方法（AGCLA），为CDGC引入可学习的增强器，以获得高质量且适合的增强样本。具体而言，我们设计了两种可学习的增强器分别用于属性和结构信息。此外，生成了两个改进矩阵，包括高置信度伪标签矩阵和跨视图样本相似度矩阵，以提高学习的亲和矩阵的可靠性。在训练过程中，我们不断调整这些增强器和改进矩阵，以最大程度地提高聚类性能。

    Contrastive deep graph clustering (CDGC) utilizes contrastive learning to group nodes into different clusters. Better augmentation techniques benefit the quality of the contrastive samples, thus being one of key factors to improve performance. However, the augmentation samples in existing methods are always predefined by human experiences, and agnostic from the downstream task clustering, thus leading to high human resource costs and poor performance. To this end, we propose an Attribute Graph Clustering method via Learnable Augmentation (\textbf{AGCLA}), which introduces learnable augmentors for high-quality and suitable augmented samples for CDGC. Specifically, we design two learnable augmentors for attribute and structure information, respectively. Besides, two refinement matrices, including the high-confidence pseudo-label matrix and the cross-view sample similarity matrix, are generated to improve the reliability of the learned affinity matrix. During the training procedure, we no
    
[^178]: 非线性模型预测控制在柔性机器人中的安全模仿学习

    Safe Imitation Learning of Nonlinear Model Predictive Control for Flexible Robots. (arXiv:2212.02941v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.02941](http://arxiv.org/abs/2212.02941)

    本文提出了一种使用模仿学习和预测安全过滤器进行非线性模型预测控制(NMPC)的安全近似的框架，以实现对柔性机器人的快速控制。与NMPC相比，在保证安全约束的情况下，我们的框架在计算时间上改善了8倍以上。

    

    柔性机器人可以解决一些工业领域的主要挑战，如实现固有安全的人机协作和实现更高的负载重量比。然而，由于其复杂的动力学特性，包括振荡行为和高维状态空间，控制柔性机器人非常复杂。非线性模型预测控制(NMPC)能够有效控制此类机器人，但其计算需求较高常常限制其在实时场景中的应用。为了实现对柔性机器人的快速控制，我们提出了一种使用模仿学习和预测安全过滤器进行NMPC的安全近似的框架。我们的框架显著减少了计算时间，同时在性能上略有损失。与NMPC相比，在模拟中控制一个三维柔性机械臂时，我们的框架在计算时间上改善了8倍以上，同时保证了安全约束。值得注意的是，我们的方法优于传统的强化学习方法。

    Flexible robots may overcome some of the industry's major challenges, such as enabling intrinsically safe human-robot collaboration and achieving a higher load-to-mass ratio. However, controlling flexible robots is complicated due to their complex dynamics, which include oscillatory behavior and a high-dimensional state space. NMPC offers an effective means to control such robots, but its extensive computational demands often limit its application in real-time scenarios. To enable fast control of flexible robots, we propose a framework for a safe approximation of NMPC using imitation learning and a predictive safety filter. Our framework significantly reduces computation time while incurring a slight loss in performance. Compared to NMPC, our framework shows more than a eightfold improvement in computation time when controlling a three-dimensional flexible robot arm in simulation, all while guaranteeing safety constraints. Notably, our approach outperforms conventional reinforcement le
    
[^179]: 通过神经网络修补厉害的对抗性输入生成的高效方法

    Efficient Adversarial Input Generation via Neural Net Patching. (arXiv:2211.16808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16808](http://arxiv.org/abs/2211.16808)

    通过修补神经网络的方法生成对抗性输入，以解决建立深度神经网络的鲁棒性和可信度的问题。

    

    对抗性输入的生成已成为建立深度神经网络的鲁棒性和可信度的关键问题，尤其是当它们在自动驾驶车辆和精确医学等安全关键应用领域使用时。然而，这个问题面临多个实际挑战，包括由于网络规模大而导致的可扩展性问题，以及生成缺乏自然性和输出中立性等重要特征的对抗性输入。这个问题与修补神经网络的任务共享相同的最终目标，其中需要发现一些网络权重的微小变化，以便在应用这些变化后，修改后的网络对于给定的输入集产生理想的输出。我们通过提出从一个修补补丁中获取对抗性输入来利用这种联系，其中的观察是通过改变输入也可以实现改变权重的效果。因此，本文提出了一种新颖的方法来解决这个问题。

    The generation of adversarial inputs has become a crucial issue in establishing the robustness and trustworthiness of deep neural nets, especially when they are used in safety-critical application domains such as autonomous vehicles and precision medicine. However, the problem poses multiple practical challenges, including scalability issues owing to large-sized networks, and the generation of adversarial inputs that lack important qualities such as naturalness and output-impartiality. This problem shares its end goal with the task of patching neural nets where small changes in some of the network's weights need to be discovered so that upon applying these changes, the modified net produces the desirable output for a given set of inputs. We exploit this connection by proposing to obtain an adversarial input from a patch, with the underlying observation that the effect of changing the weights can also be brought about by changing the inputs instead. Thus, this paper presents a novel way
    
[^180]: 垂直联合学习：概念、进展和挑战

    Vertical Federated Learning: Concepts, Advances and Challenges. (arXiv:2211.12814v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12814](http://arxiv.org/abs/2211.12814)

    垂直联合学习（VFL）是一种联合学习设置，多个具有关于同一组用户不同特征的参与方共同训练机器学习模型，而不公开原始数据或模型参数。本文提供了对VFL概念、算法以及各个方面的进展和挑战的综合回顾，深入分析了隐私保护协议的分类、隐私攻击和防御策略，并提出了考虑多个约束条件的统一框架VFLow。此外，还回顾了工业应用中的最新进展和VFL面临的未来挑战和方向。

    

    垂直联合学习（VFL）是一种联合学习设置，多个具有关于同一组用户不同特征的参与方共同训练机器学习模型，而不公开原始数据或模型参数。受VFL研究和实际应用的快速增长的推动，我们全面回顾了VFL的概念和算法，以及各个方面的当前进展和挑战，包括有效性、效率和隐私。我们提供了对VFL设置和隐私保护协议的详尽分类，并对每个协议的隐私攻击和防御策略进行了全面分析。最后，我们提出了一个统一的框架，称为VFLow，它考虑了VFL问题在通信、计算、隐私以及有效性和公平性约束下的情况。最后，我们回顾了工业应用中最新的进展，突出了VFL面临的开放性挑战和未来方向。

    Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.
    
[^181]: 在线分布漂移检测方法基于最近性预测

    Online Distribution Shift Detection via Recency Prediction. (arXiv:2211.09916v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.09916](http://arxiv.org/abs/2211.09916)

    本文提出了一种在线方法来有效检测机器人系统中的分布漂移，具有低误报率和高效率的特点。

    

    在高风险应用中部署现代机器学习驱动的机器人系统时，检测分布漂移至关重要。然而，大多数现有的分布漂移检测方法不适用于机器人环境，因为数据通常以流式方式到达并且可能具有非常高的维度。在本文中，我们提出了一种在线方法来检测分布漂移，并对误报率提供了保证 - 即当没有分布漂移时，我们的系统非常不可能（概率小于 epsilon）发出错误的警报；因此，任何发出的警报应该被重视。我们的方法专为高维数据的高效检测而设计，并且在实际情况下与以前的方法相比，实现了高达11倍的快速检测，同时保持低的误报率（在我们的实验中，当存在分布漂移时，我们的方法确实发出了警报）。

    When deploying modern machine learning-enabled robotic systems in high-stakes applications, detecting distribution shift is critical. However, most existing methods for detecting distribution shift are not well-suited to robotics settings, where data often arrives in a streaming fashion and may be very high-dimensional. In this work, we present an online method for detecting distribution shift with guarantees on the false positive rate - i.e., when there is no distribution shift, our system is very unlikely (with probability $< \epsilon$) to falsely issue an alert; any alerts that are issued should therefore be heeded. Our method is specifically designed for efficient detection even with high dimensional data, and it empirically achieves up to 11x faster detection on realistic robotics settings compared to prior work while maintaining a low false negative rate in practice (whenever there is a distribution shift in our experiments, our method indeed emits an alert). We demonstrate our a
    
[^182]: 具有大的最坏情况Lipschitz参数的私有随机优化：（非光滑）凸损失的最优速率及其对非凸损失的扩展

    Private Stochastic Optimization With Large Worst-Case Lipschitz Parameter: Optimal Rates for (Non-Smooth) Convex Losses and Extension to Non-Convex Losses. (arXiv:2209.07403v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07403](http://arxiv.org/abs/2209.07403)

    本论文研究了具有大的最坏情况Lipschitz参数的差分隐私随机优化问题，并提供了一种不依赖于统一Lipschitz参数的接近最优的过量风险界限方法。

    

    我们研究了具有最坏情况Lipschitz参数可能非常大的损失函数的差分隐私（DP）随机优化（SO）。迄今为止，大部分关于DP SO的工作都假设损失在所有数据点上是均匀Lipschitz连续的（即随机梯度在所有数据点上都有界）。虽然这种假设很方便，但通常会导致悲观的过量风险界限。在许多实际问题中，由于异常值，损失在所有数据点上的最坏情况（统一）Lipschitz参数可能非常大。在这种情况下，DP SO的误差界限与损失的最坏情况Lipschitz参数成比例，将会是空洞的。为了解决这些限制，本工作提供了一种接近最优的过量风险界限，不依赖于损失的统一Lipschitz参数。在最近的工作（Wang等人，2020; Kamath等人，2022）的基础上，我们假设随机梯度具有有界的k阶矩

    We study differentially private (DP) stochastic optimization (SO) with loss functions whose worst-case Lipschitz parameter over all data points may be extremely large. To date, the vast majority of work on DP SO assumes that the loss is uniformly Lipschitz continuous over data (i.e. stochastic gradients are uniformly bounded over all data points). While this assumption is convenient, it often leads to pessimistic excess risk bounds. In many practical problems, the worst-case (uniform) Lipschitz parameter of the loss over all data points may be extremely large due to outliers. In such cases, the error bounds for DP SO, which scale with the worst-case Lipschitz parameter of the loss, are vacuous. To address these limitations, this work provides near-optimal excess risk bounds that do not depend on the uniform Lipschitz parameter of the loss. Building on a recent line of work (Wang et al., 2020; Kamath et al., 2022), we assume that stochastic gradients have bounded $k$-th order moments fo
    
[^183]: 人脸识别系统中的可察觉差异建模

    Just Noticeable Difference Modeling for Face Recognition System. (arXiv:2209.05856v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.05856](http://arxiv.org/abs/2209.05856)

    本研究探讨了人脸识别系统中的可察觉差异（JND），建立了JND数据集，并开发了一种新颖的JND预测模型，以提升自动人脸识别系统的性能。

    

    监控和安全场景中，为确保自动人脸识别（FR）系统的稳定性和可靠性，需要高质量的人脸图像。然而，由于传输或存储的限制，通常会对大量人脸数据进行压缩分析。压缩后的图像可能会丢失重要的身份信息，导致FR系统性能下降。本文首次尝试研究FR系统的可察觉差异（JND），即FR系统无法注意到的最大失真。具体而言，我们建立了一个包括3530个原始图像和137,670个由先进的参考编解码软件基于Versatile Video Coding（VVC）标准（VTM-15.0）生成的压缩图像的JND数据集。随后，我们开发了一种新颖的JND预测模型，用于直接推断FR系统的JND图像。特别地，我们旨在最大限度地去除冗余而不损害 FR 系统。

    High-quality face images are required to guarantee the stability and reliability of automatic face recognition (FR) systems in surveillance and security scenarios. However, a massive amount of face data is usually compressed before being analyzed due to limitations on transmission or storage. The compressed images may lose the powerful identity information, resulting in the performance degradation of the FR system. Herein, we make the first attempt to study just noticeable difference (JND) for the FR system, which can be defined as the maximum distortion that the FR system cannot notice. More specifically, we establish a JND dataset including 3530 original images and 137,670 compressed images generated by advanced reference encoding/decoding software based on the Versatile Video Coding (VVC) standard (VTM-15.0). Subsequently, we develop a novel JND prediction model to directly infer JND images for the FR system. In particular, in order to maximum redundancy removal without impairment o
    
[^184]: 经典到量子卷积神经网络迁移学习

    Classical-to-quantum convolutional neural network transfer learning. (arXiv:2208.14708v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2208.14708](http://arxiv.org/abs/2208.14708)

    本研究提出了经典到量子迁移学习的框架，通过利用预训练的经典卷积神经网络，实现了在噪声中间规模量子时代中较小的QCNN在复杂分类问题上的解决，充分利用了QCNN的优势。

    

    使用量子卷积神经网络（QCNN）的机器学习在量子和经典数据分类中取得了成功。在之前的研究中，QCNN在少参数情况下的分类准确率比其经典对应物更高。然而，由于可靠实现的量子电路规模有限，在大规模量子模型的整体性能上很难进行检验。我们提出了迁移学习作为一种有效的策略，以充分利用在噪声中间规模量子时代中较小的QCNN。在经典到量子迁移学习框架中，QCNN可以通过利用预训练的经典卷积神经网络（CNN）解决复杂的分类问题，而无需大规模量子电路。我们对具有不同量子卷积和池化操作集的QCNN模型进行了数值模拟，以处理MNIST数据集。

    Machine learning using quantum convolutional neural networks (QCNNs) has demonstrated success in both quantum and classical data classification. In previous studies, QCNNs attained a higher classification accuracy than their classical counterparts under the same training conditions in the few-parameter regime. However, the general performance of large-scale quantum models is difficult to examine because of the limited size of quantum circuits, which can be reliably implemented in the near future. We propose transfer learning as an effective strategy for utilizing small QCNNs in the noisy intermediate-scale quantum era to the full extent. In the classical-to-quantum transfer learning framework, a QCNN can solve complex classification problems without requiring a large-scale quantum circuit by utilizing a pre-trained classical convolutional neural network (CNN). We perform numerical simulations of QCNN models with various sets of quantum convolution and pooling operations for MNIST data 
    
[^185]: 为公正机器学习开发哲学框架：从算法串通案例中的教训

    Developing a Philosophical Framework for Fair Machine Learning: Lessons From The Case of Algorithmic Collusion. (arXiv:2208.06308v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06308](http://arxiv.org/abs/2208.06308)

    该论文提出了一个伦理框架，用于为公正机器学习开发和应用扩展到新领域的公正指标，通过提出特定范围的规范原则来使公正指标能够反映出各种领域中的公正要求。

    

    公正机器学习研究主要关注导致歧视的分类任务。然而，随着机器学习算法在新的背景下的应用，产生的伤害和不公正与目前研究的本质上不同。机器学习中现有的研究范式无法解释这些本质上不同类型的不公正。算法串通和市场公平性问题是其中一个例子。算法串通的负面后果影响所有消费者，而不仅仅是某个受保护类别的成员。借鉴这个案例研究，我提出了一个伦理框架，供机器学习研究人员和实践者在开发和应用公正指标时使用，以扩展到新的领域。这个贡献将公正的形式指标的发展与特定范围的规范原则联系起来。这使得公正指标能够反映出各种领域中的公正要求。

    Fair machine learning research has been primarily concerned with classification tasks that result in discrimination. However, as machine learning algorithms are applied in new contexts the harms and injustices that result are qualitatively different than those presently studied. The existing research paradigm in machine learning which develops metrics and definitions of fairness cannot account for these qualitatively different types of injustice. One example of this is the problem of algorithmic collusion and market fairness. The negative consequences of algorithmic collusion affect all consumers, not only particular members of a protected class. Drawing on this case study, I propose an ethical framework for researchers and practitioners in machine learning seeking to develop and apply fairness metrics that extends to new domains. This contribution ties the development of formal metrics of fairness to specifically scoped normative principles. This enables fairness metrics to reflect di
    
[^186]: 量子自注意力神经网络用于文本分类

    Quantum Self-Attention Neural Networks for Text Classification. (arXiv:2205.05625v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2205.05625](http://arxiv.org/abs/2205.05625)

    本论文提出了一种名为量子自注意力神经网络（QSANN）的简单网络架构，通过将自注意机制引入到量子神经网络中，并利用高斯投影的量子自注意力，弥补了现有量子自然语言处理方法的一些限制。QSANN在更大规模的数据集上具有有效和可扩展的性能，并且可以在近期量子设备上实现。

    

    量子计算的一个新兴方向是在包括自然语言处理在内的人工智能各个领域建立有意义的量子应用。尽管基于句法分析的一些工作为量子自然语言处理研究打开了大门，但是诸如繁重的句法预处理和句法相关的网络结构等限制使得它们在更大规模和实际数据集上不可行。在本文中，我们提出了一种新的简单网络架构，称为量子自注意力神经网络（QSANN），可以弥补这些限制。具体而言，我们将自注意机制引入到量子神经网络中，然后利用高斯投影的量子自注意力作为自注意力的量子版本。结果表明，QSANN在更大规模的数据集上具有有效和可扩展的性能，并且具有在近期量子设备上可实现的理想性质。特别地，我们的QSANN优于最佳的现有模型。

    An emerging direction of quantum computing is to establish meaningful quantum applications in various fields of artificial intelligence, including natural language processing (NLP). Although some efforts based on syntactic analysis have opened the door to research in Quantum NLP (QNLP), limitations such as heavy syntactic preprocessing and syntax-dependent network architecture make them impracticable on larger and real-world data sets. In this paper, we propose a new simple network architecture, called the quantum self-attention neural network (QSANN), which can compensate for these limitations. Specifically, we introduce the self-attention mechanism into quantum neural networks and then utilize a Gaussian projected quantum self-attention serving as a sensible quantum version of self-attention. As a result, QSANN is effective and scalable on larger data sets and has the desirable property of being implementable on near-term quantum devices. In particular, our QSANN outperforms the best
    
[^187]: DIRA: 一种用于动态领域增量正则化自适应的框架

    DIRA: A Framework for Dynamic Domain Incremental Regularised Adaptation. (arXiv:2205.00147v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.00147](http://arxiv.org/abs/2205.00147)

    DIRA是一个用于DNN分类器的动态领域自适应的框架，使用正则化技术来解决灾难性遗忘问题，并通过少量样本实现重新训练和适应性。

    

    自主系统（AS）经常使用深度神经网络（DNN）分类器，使它们能够在复杂、高维、非线性和动态变化的环境中运行。由于这些环境的复杂性，当DNN分类器面对开发过程中未识别的领域时，可能会在操作过程中输出错误分类。随着AS的数量增加，将系统从运行中移除进行重新训练变得不切实际。为增加AS的可靠性并克服这一限制，DNN分类器需要在操作过程中适应不同的操作领域，并能够使用少量样本（例如100个样本）进行重新训练。然而，已知在少量样本上重新训练DNN会导致灾难性遗忘。在本文中，我们介绍了一种名为动态增量正则化自适应（DIRA）的框架，用于使用正则化技术来实现DNN分类器的操作领域适应，从而克服灾难性遗忘并实现适应性。

    Autonomous systems (AS) often use Deep Neural Network (DNN) classifiers to allow them to operate in complex, high dimensional, non-linear, and dynamically changing environments. Due to the complexity of these environments, DNN classifiers may output misclassifications during operation when they face domains not identified during development. Removing a system from operation for retraining becomes impractical as the number of such AS increase. To increase AS reliability and overcome this limitation, DNN classifiers need to have the ability to adapt during operation when faced with different operational domains using a few samples (e.g. 100 samples). However, retraining DNNs on a few samples is known to cause catastrophic forgetting. In this paper, we introduce Dynamic Incremental Regularised Adaptation (DIRA), a framework for operational domain adaption of DNN classifiers using regularisation techniques to overcome catastrophic forgetting and achieve adaptation when retraining using few
    
[^188]: 在欠参数化和过参数化的模式中的数据增强

    Data Augmentation in the Underparameterized and Overparameterized Regimes. (arXiv:2202.09134v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.09134](http://arxiv.org/abs/2202.09134)

    这项研究提供了数据增强如何影响估计的方差和极限分布的确切量化结果，发现数据增强可能会增加估计的不确定性，并且其效果取决于多个因素。同时，该研究还通过随机转换的高维随机向量的函数的极限定理进行了证明。

    

    我们提供了确切量化数据增强如何影响估计的方差和极限分布的结果，并详细分析了几个具体模型。结果证实了机器学习实践中的一些观察，但也得出了意外的发现：数据增强可能会增加而不是减少估计的不确定性，比如经验预测风险。它可以充当正则化器，但在某些高维问题中却无法实现，并且可能会改变经验风险的双重下降峰值。总的来说，分析表明数据增强被赋予的几个属性要么是真的，要么是假的，而是取决于多个因素的组合-特别是数据分布，估计器的属性以及样本大小，增强数量和维数的相互作用。我们的主要理论工具是随机转换的高维随机向量的函数的极限定理。

    We provide results that exactly quantify how data augmentation affects the variance and limiting distribution of estimates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. Our main theoretical tool is a limit theorem for functions of randomly transformed, high-dimensional random vectors. The proof draws on 
    
[^189]: 群体代理强化学习

    Group-Agent Reinforcement Learning. (arXiv:2202.05135v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05135](http://arxiv.org/abs/2202.05135)

    群体代理强化学习是一种新型的强化学习方法，其利用多个代理之间的合作来提高每个代理的学习效果。我们提出了群体代理强化学习系统的概念，并设计了一种分布式强化学习框架DDAL来支持群体代理强化学习。

    

    如果多个地理分布的代理进行合作性的个别强化学习任务，可以为每个代理的强化学习过程带来很大的好处。与多智能体强化学习（MARL）不同，MARL中多个代理共同存在于一个环境中，并且需要学习如何合作或竞争。在群体代理强化学习中，每个代理都有自己的环境，并且只与其他代理进行通信以分享知识，没有合作或竞争行为作为学习结果。事实上，这种情景在现实生活中普遍存在，其概念可以应用于许多应用领域，但尚未很好理解和表述。作为首次尝试，我们提出了群体代理强化学习系统作为对单个代理和多个代理系统的第三类强化学习系统的表述。然后，我们提出了一种分布式强化学习框架DDAL（分散式分布式异步学习），专为群体代理强化学习而设计。

    It can largely benefit the reinforcement learning (RL) process of each agent if multiple geographically distributed agents perform their separate RL tasks cooperatively. Different from multi-agent reinforcement learning (MARL) where multiple agents are in a common environment and should learn to cooperate or compete with each other, in this case each agent has its separate environment and only communicates with others to share knowledge without any cooperative or competitive behaviour as a learning outcome. In fact, this scenario exists widely in real life whose concept can be utilised in many applications, but is not well understood yet and not well formulated. As the first effort, we propose group-agent system for RL as a formulation of this scenario and the third type of RL system with respect to single-agent and multi-agent systems. We then propose a distributed RL framework called DDAL (Decentralised Distributed Asynchronous Learning) designed for group-agent reinforcement learnin
    
[^190]: 无限神经网络量子态：纠缠和训练动力学

    Infinite Neural Network Quantum States: Entanglement and Training Dynamics. (arXiv:2112.00723v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2112.00723](http://arxiv.org/abs/2112.00723)

    本研究探索了无限神经网络量子态（∞-NNQS），其通过集合统计表现出高度的表示能力和可行的梯度下降动力学。研究发现，通过神经网络相关子和量子态神经切线核投入到训练动力学中，可以简化神经网络量子态的训练过程并恢复任意目标波函数。对有限和无限NNQS进行的数值实验验证了理论结果，并为进一步研究纠缠和训练动力学提供了新的领域。

    

    本文研究了神经网络量子态（∞-NNQS）的无限极限，通过集合统计表现出其表示能力，并且具有可行的梯度下降动力学。以神经网络相关子表达拉尼熵的集合平均值，呈现了体积定律纠缠的结构。使用量子态神经切线核（QS-NTK）开发了一个用于研究神经网络量子态（NNQS）梯度下降动力学的通用框架。对于∞-NNQS，训练动力学被简化，因为QS-NTK变得确定和恒定。对于量子态监督学习导出了一个解析解，允许∞-NNQS恢复任何目标波函数。在横向场伊辛模型和费米哈伯模型中对有限和无限NNQS进行了数值实验，与理论结果具有良好的一致性。∞-NNQS为研究纠缠和训练动力学提供了新的机会。

    We study infinite limits of neural network quantum states ($\infty$-NNQS), which exhibit representation power through ensemble statistics, and also tractable gradient descent dynamics. Ensemble averages of Renyi entropies are expressed in terms of neural network correlators, and architectures that exhibit volume-law entanglement are presented. A general framework is developed for studying the gradient descent dynamics of neural network quantum states (NNQS), using a quantum state neural tangent kernel (QS-NTK). For $\infty$-NNQS the training dynamics is simplified, since the QS-NTK becomes deterministic and constant. An analytic solution is derived for quantum state supervised learning, which allows an $\infty$-NNQS to recover any target wavefunction. Numerical experiments on finite and infinite NNQS in the transverse field Ising model and Fermi Hubbard model demonstrate excellent agreement with theory. $\infty$-NNQS opens up new opportunities for studying entanglement and training dyn
    
[^191]: 《推断解释的公理聚合》

    Axiomatic Aggregations of Abductive Explanations. (arXiv:2109.03890v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.03890](http://arxiv.org/abs/2109.03890)

    本论文提出了三种聚合方法，将各种可能的推断解释聚合成特征重要性分数，解决了推断解释中多个有效解释的问题。这些方法基于合作博弈理论的权力指数和已知的因果强度度量。

    

    对后续模型逼近解释方法（如LIME和SHAP）的鲁棒性的近期批评导致了模型精确的推断解释的兴起。对于每个数据点，推断解释提供了一个足以生成结果的最小子集特征。尽管在理论上是严格和可靠的，但推断解释存在一个主要问题：同一数据点可以有多个有效的推断解释。在这种情况下，提供一个单一的推断解释可能是不足够的；另一方面，提供所有有效的推断解释可能由于其规模而难以理解。在这项工作中，我们通过将各种可能的推断解释聚合成特征重要性分数来解决这个问题。我们提出了三种聚合方法：两种基于合作博弈理论的权力指数方法和一种基于著名的因果强度度量的方法。我们从公理上对这三种方法进行了表征，证明每个方法都是良定义的且符合公理。

    The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
    
[^192]: 算法决策中的动态选择问题研究

    Dynamic Selection in Algorithmic Decision-making. (arXiv:2108.12547v3 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2108.12547](http://arxiv.org/abs/2108.12547)

    本文研究了算法决策中的动态选择问题，针对在线学习算法中数据的内生性导致的偏差提出了一种基于工具变量的纠正算法，并证明了该算法可以获得真实参数值和较低遗憾水平。研究还提供了统计推断的中心极限定理。

    

    本文识别和解决了在线学习算法中的动态选择问题，这些问题与内生数据有关。在上下文多臂赌博模型中，由于数据的内生性影响决策的选择，会产生一种新的偏差（自我实现偏差），从而影响到未来待收集和分析的数据的分布。我们提出了一种基于工具变量的算法，以纠正这种偏差。该算法可以获得真实参数值，并获得较低（类似对数的）遗憾水平。我们还证明了统计推断的中心极限定理。为了建立理论性质，我们开发了一个通用技术，以解开数据和行动之间的相互依赖关系。

    This paper identifies and addresses dynamic selection problems in online learning algorithms with endogenous data. In a contextual multi-armed bandit model, a novel bias (self-fulfilling bias) arises because the endogeneity of the data influences the choices of decisions, affecting the distribution of future data to be collected and analyzed. We propose an instrumental-variable-based algorithm to correct for the bias. It obtains true parameter values and attains low (logarithmic-like) regret levels. We also prove a central limit theorem for statistical inference. To establish the theoretical properties, we develop a general technique that untangles the interdependence between data and actions.
    
[^193]: Patch-level Neighborhood Interpolation: 一种通用且有效的基于图的正则化策略

    Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy. (arXiv:1911.09307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.09307](http://arxiv.org/abs/1911.09307)

    这项工作提出了一种名为Pani的通用正则化器，它在深度神经网络中进行非局部表示，并将邻域补丁特征进行线性插值，从而构建了一种通用且有效的正则化策略。

    

    正则化对于机器学习模型尤其是深度神经网络非常重要。现有的正则化技术主要依赖于独立同分布假设，并且仅考虑当前样本的知识，没有利用样本之间的邻居关系。在这项工作中，我们提出了一种称为“Patch-level Neighborhood Interpolation（Pani）”的通用正则化器，在网络计算中进行非局部表示。我们的提议明确地构建了不同层次的补丁级图，然后线性插值邻域补丁特征，作为一种通用且有效的正则化策略。此外，我们将我们的方法定制为两种流行的正则化方法，即虚拟对抗训练（VAT）和MixUp以及其变体。首先派生的“Pani VAT”通过使用补丁级插值扰动构建非局部对抗平滑度，提出了一种新颖的方法。

    Regularization plays a crucial role in machine learning models, especially for deep neural networks. The existing regularization techniques mainly rely on the i.i.d. assumption and only consider the knowledge from the current sample, without the leverage of the neighboring relationship between samples. In this work, we propose a general regularizer called \textbf{Patch-level Neighborhood Interpolation~(Pani)} that conducts a non-local representation in the computation of networks. Our proposal explicitly constructs patch-level graphs in different layers and then linearly interpolates neighborhood patch features, serving as a general and effective regularization strategy. Further, we customize our approach into two kinds of popular regularization methods, namely Virtual Adversarial Training (VAT) and MixUp as well as its variants. The first derived \textbf{Pani VAT} presents a novel way to construct non-local adversarial smoothness by employing patch-level interpolated perturbations. Th
    
[^194]: 通过决策森林学习可解释的特征核

    Learning Interpretable Characteristic Kernels via Decision Forests. (arXiv:1812.00029v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1812.00029](http://arxiv.org/abs/1812.00029)

    本论文介绍了一种通过决策森林构建可解释的特征核的方法，我们构建了基于叶节点相似性的核平均嵌入随机森林（KMERF），并证明其在离散和连续数据上都表现出渐进特征。实验证明KMERF在多种高维数据测试中优于目前的最先进的基于核的方法。

    

    决策森林被广泛用于分类和回归任务。树方法的一个较少被知晓的特性是可以从树构建相似性矩阵，并且这些相似性矩阵是由核诱导的。尽管对于核的应用和性质进行了广泛研究，但对于由决策森林诱导的核的研究相对较少。我们构建了基于叶节点相似性的核平均嵌入随机森林（KMERF），它可以从随机树或森林中诱导核。我们引入了渐进特征核的概念，并证明KMERF核对于离散和连续数据都是渐进特征的。由于KMERF是数据自适应的，我们怀疑它将在有限样本数据上胜过预先选择的核。我们展示了KMERF在各种高维两样本和独立性测试场景中几乎占据了目前的最先进的基于核的测试方法。

    Decision forests are widely used for classification and regression tasks. A lesser known property of tree-based methods is that one can construct a proximity matrix from the tree(s), and these proximity matrices are induced kernels. While there has been extensive research on the applications and properties of kernels, there is relatively little research on kernels induced by decision forests. We construct Kernel Mean Embedding Random Forests (KMERF), which induce kernels from random trees and/or forests using leaf-node proximity. We introduce the notion of an asymptotically characteristic kernel, and prove that KMERF kernels are asymptotically characteristic for both discrete and continuous data. Because KMERF is data-adaptive, we suspected it would outperform kernels selected a priori on finite sample data. We illustrate that KMERF nearly dominates current state-of-the-art kernel-based tests across a diverse range of high-dimensional two-sample and independence testing settings. Furth
    

