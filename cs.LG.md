# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Probabilistic Model to explain Self-Supervised Representation Learning](https://rss.arxiv.org/abs/2402.01399) | 该论文提出了一个概率模型来解释自监督表示学习的机制，并展示了鉴别性自监督算法在表示中近似诱导潜变量结构的统一理论框架。 |
| [^2] | [Compositional Generative Modeling: A Single Model is Not All You Need](https://rss.arxiv.org/abs/2402.01103) | 本文提出了一种组合生成方法，通过将较小的生成模型组合在一起来构建大型生成系统。该方法可以更高效地学习数据分布，实现对训练时未见的数据部分的泛化，并能够编写和构建新的生成模型。 |
| [^3] | [Repeat After Me: Transformers are Better than State Space Models at Copying](https://rss.arxiv.org/abs/2402.01032) | 这篇论文证明了Transformer模型在复制任务上的优势，相比于使用固定潜在状态的广义状态空间模型，Transformer模型能够更高效地复制和检索上下文信息。 |
| [^4] | [Approximate Nearest Neighbor Search with Window Filters](https://rss.arxiv.org/abs/2402.00943) | 这篇论文提出了一种使用窗口过滤的近似最近邻搜索方法，能够在各种语义搜索问题中实现高速搜索，并在多个基准数据集上取得了显著的速度提升。 |
| [^5] | [Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer](https://arxiv.org/abs/2403.18063) | 该论文提出了光谱卷积变压器 (SCT)，通过结合局部信息的卷积操作和全局信息的复杂傅里叶基础，实现了对视觉变压器中实部和复部多视图光谱算子的协调，从而实现了更好的性能。 |
| [^6] | [Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates](https://arxiv.org/abs/2403.11687) | 在非光滑设置下，提出了用于计算具有内映射的外映射固定点的隐式导数的新方法NSID，并提供了确定性情况下迭代微分（ITD）和近似隐式微分（AID）的改进线性收敛速率。 |
| [^7] | [Deep Limit Order Book Forecasting](https://arxiv.org/abs/2403.09267) | 该研究利用深度学习方法预测纳斯达克交易所股票的限价订单簿中间价格变动，提出了一个创新的操作框架来评估预测的实用性。 |
| [^8] | [COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization](https://arxiv.org/abs/2403.07134) | 提出了一种名为COMQ的创新后训练量化算法，通过逐层减小重构误差来有效降低大型神经网络的存储要求，同时保持原始准确性。 |
| [^9] | [Denoising Autoregressive Representation Learning](https://arxiv.org/abs/2403.05196) | DARL使用仅解码器的Transformer进行自回归预测，通过使用去噪补丁解码器和特定噪声计划改善图像生成能力，实现了与蒙版预测模型相近的性能。 |
| [^10] | [ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport](https://arxiv.org/abs/2403.03777) | 通过期望回归正则化，本论文提出了一种新的神经优化传输（NOT）训练程序扩展，能够有效地估计最优输运方案，并使学习变得稳定。 |
| [^11] | [State-Constrained Zero-Sum Differential Games with One-Sided Information](https://arxiv.org/abs/2403.02741) | 我们将零和微分博弈的理论扩展到具有状态约束，并提出了计算行为策略所需的原始和对偶子动态原则。 |
| [^12] | [Wukong: Towards a Scaling Law for Large-Scale Recommendation](https://arxiv.org/abs/2403.02545) | Wukong通过堆叠因子分解机和协同增长策略，在推荐领域建立了一个标度律，并在质量上优于现有模型。 |
| [^13] | [ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation](https://arxiv.org/abs/2403.01306) | 提出一种新的度量标准，图像描述具体性，用于评估标题文本的具体性和相关性，以帮助在多模态学习中隔离提供最强信号的最具体样本。 |
| [^14] | [Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming](https://arxiv.org/abs/2402.18866) | 本文提出了一个新的基于模型的通用型Agent Dr. Strategy，配备了Dreaming Strategy，实现了在梦境中学习一组潜在地标，并利用这些地标学习地标条件的高速公路策略。 |
| [^15] | [Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical Systems](https://arxiv.org/abs/2402.17036) | 提出了一种基于迭代INLA的方法，用于在非线性动力系统中推断状态和参数，能够保留可解释性并且适用于任意非线性系统。 |
| [^16] | [Pragmatic Goal-Oriented Communications under Semantic-Effectiveness Channel Errors](https://arxiv.org/abs/2402.16858) | 在AI辅助的6G网络中，实现了语义、实用和目标导向通信策略的整合，通过提出数学模型解决了语言不匹配导致的出错问题 |
| [^17] | [Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing](https://arxiv.org/abs/2402.16627) | 提出了一种新颖且通用的上下文化扩散模型（ContextDiff），通过在正向和逆向过程中融入文本条件和视觉样本之间的交互和对齐，以便在视觉生成中更准确地传达文本语义 |
| [^18] | [Comparing Graph Transformers via Positional Encodings](https://arxiv.org/abs/2402.14202) | 本文比较了使用绝对位置编码（APEs）和相对位置编码（RPEs）的图变换器，在最大化区分能力方面是等效的。 |
| [^19] | [Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains](https://arxiv.org/abs/2402.14145) | 提出了一种两阶段的乘幂稳健估计方法，用于改善表格数据分析中每个个体部分的模型性能，并建立了在测试风险上的理论保证。 |
| [^20] | [Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions](https://arxiv.org/abs/2402.13551) | 提出了一种新颖且实用的叙事理解范式，通过在叙事中形成图NARCO来描述整个背景的任务无关的连贯依赖，其中的边反映了高层次的连贯关系，无需依赖人类注释。 |
| [^21] | [ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling](https://arxiv.org/abs/2402.13542) | ARL2提出了一种检索器学习技术，利用LLMs作为标注者，并采用自适应自训练策略，能够有效减少注释成本，并在NQ和MMLU上取得了5.4%和4.6%的准确度提升。 |
| [^22] | [How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning](https://arxiv.org/abs/2402.13087) | 本论文探讨了超参数调整中的隐私性问题，发现当前的隐私分析在一般情况下是紧密的，但在特定的超参数调整问题上则不再成立，并通过隐私审计揭示了当前理论隐私界与实证之间的显著差距。 |
| [^23] | [Investigating the Impact of Model Instability on Explanations and Uncertainty](https://arxiv.org/abs/2402.13006) | 模型稳定性对解释和不确定性的影响进行了调查，并发现实际扰动对性能和解释影响较小，但掩盖却有 drastical 影响。 |
| [^24] | [In deep reinforcement learning, a pruned network is a good network](https://arxiv.org/abs/2402.12479) | 通过逐渐剪枝，使代理能够最大程度地发挥参数效能，从而产生比传统网络显著性能提升的网络，并展现出一种“缩放定律”。 |
| [^25] | [On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models](https://arxiv.org/abs/2402.12423) | 本研究在文本转语音模型中探索了冻结模型的潜空间，发现其中包含丰富的语义信息，并提出了一些新方法来找出其中的语义方向，从而实现了不经过额外训练、架构更改或数据需求就能进行音频编辑。 |
| [^26] | [Exploring Precision and Recall to assess the quality and diversity of LLMs](https://arxiv.org/abs/2402.10693) | 该研究提出了一种新的评估框架，将精度和召回率指标从图像生成转化为文本生成，细致评估了LLMs生成文本的质量和多样性，揭示了当前LLMs在生成任务中性能表现的重要见解。 |
| [^27] | [Learnability is a Compact Property](https://arxiv.org/abs/2402.10360) | 监督学习问题的困难性具有紧凑的有限特性表征。 |
| [^28] | [Privacy Attacks in Decentralized Learning](https://arxiv.org/abs/2402.10001) | 该论文介绍了分布式学习中的隐私攻击，针对分布式梯度下降（D-GD）提出了首个攻击方法，能够使用户重建其邻域之外其他用户的私有数据，并验证了这种攻击的有效性。 |
| [^29] | [API Pack: A Massive Multilingual Dataset for API Call Generation](https://arxiv.org/abs/2402.09615) | 这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成 |
| [^30] | [Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation](https://arxiv.org/abs/2402.08845) | 这篇论文提出了一种使用双阶段扰动测试来进行特征归因的方法，通过计算扰动一个特征对预测变化的必要性和充分性作用的概率来衡量特征重要性。该方法能够增强特征归因方法在区分不同特征贡献方面的能力。 |
| [^31] | [Model Assessment and Selection under Temporal Distribution Shift](https://arxiv.org/abs/2402.08672) | 本文研究了在变化环境中的模型评估与选择问题，通过合成不同时期的数据集，并开发了自适应滚动窗口方法来估计模型的泛化误差以及比较不同模型之间的差异。实验证明了我们提出的方法在非稳态数据中的适应性。 |
| [^32] | [Mixtures of Experts Unlock Parameter Scaling for Deep RL](https://arxiv.org/abs/2402.08609) | 本文证明了将专家组合模块融入基于值的网络中，尤其是软MoE，可以实现更具参数可扩展性的深度强化学习模型，这提供了强有力的实证证据以发展强化学习的缩放定律。 |
| [^33] | [Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing](https://arxiv.org/abs/2402.08400) | 本文提出了一种适用于图像语义分割的自适应分层认证方法，能够在多级层级标签空间中认证图像像素，并提供更多经过认证的语义信息。 |
| [^34] | [Differentially Private Decentralized Learning with Random Walks](https://arxiv.org/abs/2402.07471) | 这项工作研究了具有随机游走算法的差分隐私去中心化学习，并使用最近的差分隐私变种推导了节点之间的隐私损失。实验结果表明，与节点之间的八卦算法相比，随机游走算法更能提供较好的隐私保证。 |
| [^35] | [PASOA- PArticle baSed Bayesian Optimal Adaptive design](https://arxiv.org/abs/2402.07160) | PASOA是一种新的贝叶斯实验设计程序，通过提供连续的后验分布的准确估计，同时执行顺序设计优化和参数推断。该方法使用 stochastic optimization 和 tempered SMC 来最大化期望信息增益，并提供了一致性的最优设计估计。 |
| [^36] | [Resampling methods for Private Statistical Inference](https://arxiv.org/abs/2402.07131) | 这项研究提出了两种私有变体的非参数bootstrap方法，用于在差分隐私的情况下构建置信区间。方法在计算效率和置信区间长度上相比现有方法有显著改进。 |
| [^37] | [Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning](https://arxiv.org/abs/2402.07107) | 这篇论文提出了一个新的统计方法，利用深度Q网络和分位数回归来在模型-free的分布式强化学习中引入不确定性，该方法通过结合深度证据学习和基于合规推理原则的分位数校准，提供了全局不确定性的显式、无样本计算，具有更高的计算和统计效率，并成功处理了超出分布范围的观测数据。 |
| [^38] | [ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning](https://arxiv.org/abs/2402.06737) | 本文介绍了一种新颖的自监督学习方法ExGRG，它通过显式生成关系图来解决图结构数据上的挑战，将先验领域知识和在线提取的信息纳入自监督学习中，取得了显著的成功。 |
| [^39] | [Feedback Loops With Language Models Drive In-Context Reward Hacking](https://arxiv.org/abs/2402.06627) | 与语言模型的反馈循环可能导致上下文内奖励欺骗（ICRH），即语言模型在测试时在优化目标的同时却产生负面副作用。这项研究确定了两个导致ICRH的过程：输出优化和策略优化。 |
| [^40] | [Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control](https://arxiv.org/abs/2402.06570) | 本论文提出了一种名为HyperDistill的方法，通过精简形态条件超网络，可在训练和未知测试机器人上实现与通用的transformers策略相当的性能，同时大大减小模型尺寸和计算成本。 |
| [^41] | [Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty](https://arxiv.org/abs/2402.06529) | 本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。 |
| [^42] | [Learn To be Efficient: Build Structured Sparsity in Large Language Models](https://arxiv.org/abs/2402.06126) | 本文通过引入一种新的算法"Learn-To-be-Efficient(LTE)"，提出了在大型语言模型(LLM)中构建结构化稀疏性的方法。该方法通过训练高效意识的LLM学习激活更少的神经元，取得更好的稀疏性和性能折衷。 |
| [^43] | [Asymptotics of feature learning in two-layer networks after one gradient-step](https://arxiv.org/abs/2402.04980) | 通过研究两层神经网络在一次梯度下降步骤后的特征学习，我们提供了在高维极限下通用化误差的精确渐近描述，并发现在适应数据的情况下，网络能够高效地学习梯度方向上的非线性函数。 |
| [^44] | [QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396) | QuIP#是一种使用哈达玛德非相干性和格书的权重量化方法，能够在极限压缩范围下达到最先进的结果，并具有快速推理的优势。 |
| [^45] | [Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science](https://arxiv.org/abs/2402.04247) | 本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。 |
| [^46] | [Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective](https://arxiv.org/abs/2402.03496) | 移除自适应方法中的平方根可以在卷积结构上减小与SGD的泛化差距，同时保持在transformers上的性能。 |
| [^47] | [Timer: Transformers for Time Series Analysis at Scale](https://arxiv.org/abs/2402.02368) | 本文旨在早期开发大规模时间序列模型（LTSM），通过预训练和GPT风格架构，克服深度模型在小样本场景中的性能瓶颈，并实现在时间序列分析中的大样本泛化能力、可扩展性和任务普适性。 |
| [^48] | [Data-driven algorithm design using neural networks with applications to branch-and-cut](https://arxiv.org/abs/2402.02328) | 本文介绍了一种基于神经网络的数据驱动算法设计方法，并将其应用于分支定界框架中，用于解决混合整数优化问题。 |
| [^49] | [Dynamic Incremental Optimization for Best Subset Selection](https://arxiv.org/abs/2402.02322) | 本文研究了一类$\ell_0$正则化问题的对偶形式，并提出了一种高效的原对偶算法，通过充分利用对偶范围估计和增量策略，提高了最佳子集选择问题的解决方案的效率和统计性质。 |
| [^50] | [Challenges in Training PINNs: A Loss Landscape Perspective](https://arxiv.org/abs/2402.01868) | 本文探讨了训练PINNs的挑战，强调了损失函数空间在训练过程中的作用，引入了新颖的二阶优化器NNCG并优化了PINN性能，为训练PINNs提供了有价值的洞见和更强大的优化策略。 |
| [^51] | [Uncertainty-Aware Partial-Label Learning](https://arxiv.org/abs/2402.00592) | 本文提出了一种基于最近邻的部分标签学习算法，利用Dempster-Shafer理论实现对模糊标记的数据的训练。实验结果表明，该算法能够提供良好的不确定性估计，并具有竞争力的预测性能。 |
| [^52] | [Momentum Particle Maximum Likelihood](https://arxiv.org/abs/2312.07335) | 该论文提出了一种动态系统方法，用于在参数和概率分布的扩展空间上最小化自由能函数，该方法融合了Nesterov的加速梯度方法、欠阻尼Langevin扩散和p。 |
| [^53] | [Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use](https://arxiv.org/abs/2312.04455) | 本文证明了大型语言模型中关注分配的波形模式对其在需要高度上下文意识的任务中的性能有显著影响。我们提出了一种名为“Attention Buckets”的推理方法，通过多个并行过程和不同的旋转位置嵌入角度，增强了模型对不同上下文位置的意识，从而减轻了忽视关键信息的风险。 |
| [^54] | [Quantum Inception Score](https://arxiv.org/abs/2311.12163) | 通过量子启蒙分数，我们提出了一个用于评估量子生成模型质量的新指标，证明量子生成模型在质量上优于经典生成模型，并利用量子波动定理揭示了其物理限制。 |
| [^55] | [Fair Coresets via Optimal Transport](https://arxiv.org/abs/2311.05436) | 本研究提出了公平的Wasserstein核心集(FWC)，该方法通过最小化原始数据集与加权合成样本之间的Wasserstein距离，并强制实现人口平等，生成公平的合成代表性样本，可用于下游学习任务。 |
| [^56] | [Dynamical Survival Analysis with Controlled Latent States.](http://arxiv.org/abs/2401.17077) | 本论文提出了一种动态生存分析方法，通过控制潜在状态来学习个体特定的计数过程强度。研究者设计了一个神经控制微分方程模型，并证明了在足够正则条件下，可以在签名空间中线性化模型，得到一种基于签名的估计器。通过对金融、预测性维护和食品供应链管理等数据集的实验，验证了模型的性能。 |
| [^57] | [Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning.](http://arxiv.org/abs/2401.09651) | 本研究通过凸二级优化技术，开发了一个通用的基于梯度的神经和符号参数学习框架，具有100倍以上的学习时间改进和高达16%的预测性能提升。 |
| [^58] | [Understanding Heterophily for Graph Neural Networks.](http://arxiv.org/abs/2401.09125) | 本文通过提出的异质性随机块模型（HSBM）来提供对不同异质性模式对图神经网络（GNNs）影响的理论理解。研究发现，异质性对分类的影响需要与平均节点度一起评估，并且拓扑噪声对分类有负面影响。 |
| [^59] | [DiarizationLM: Speaker Diarization Post-Processing with Large Language Models.](http://arxiv.org/abs/2401.03506) | 本文介绍了DiarizationLM框架，利用大语言模型对说话人分离系统的输出进行后处理。实验证明，使用finetuned的PaLM 2-S模型可以显著减少分离错误率，对多种目标都有优化效果。 |
| [^60] | [Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities.](http://arxiv.org/abs/2401.02429) | 基于脑启发的脉冲神经网络是一种有前景的应用于工业故障诊断的替代方法，可以克服人工神经网络的限制，提供更精确和有效的故障识别。 |
| [^61] | [The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers.](http://arxiv.org/abs/2401.01537) | 这项研究介绍了一种使用动态触发器进行强健后门攻击的方法，通过巧妙设计的调整，使损坏的样本与干净的样本无法区分，实验证明这种方法可以成功地欺骗语音识别系统。 |
| [^62] | [Convergence Analysis of Fractional Gradient Descent.](http://arxiv.org/abs/2311.18426) | 本论文通过分析不同环境下的分数梯度下降方法，建立了分数导数与整数导数之间的新界限，并证明了在平滑且凸、平滑且强凸以及平滑且非凸环境下的收敛性，为分数梯度下降的收敛性分析提供了新的理论支持。 |
| [^63] | [Piecewise polynomial regression of tame functions via integer programming.](http://arxiv.org/abs/2311.13544) | 本论文提出了使用整数规划对温顺函数进行分段多项式回归的方法，这可以用于估计包含在许多应用中的温顺函数，并且展示了令人期待的计算结果。 |
| [^64] | [LLMs cannot find reasoning errors, but can correct them!.](http://arxiv.org/abs/2311.08516) | 本文研究了LLMs在自我纠正过程中的错误发现和输出纠正两个核心组成部分。研究发现LLMs通常难以发现逻辑错误，但通过使用回溯方法可以在提供错误位置信息时获得大幅改进。 |
| [^65] | [VQPy: An Object-Oriented Approach to Modern Video Analytics.](http://arxiv.org/abs/2311.01623) | VQPy是一种面向对象的视频分析方法，它使用Python变体作为前端，并具有可扩展的后端，可以自动构建和优化基于视频对象的处理流程。 |
| [^66] | [Loss Modeling for Multi-Annotator Datasets.](http://arxiv.org/abs/2311.00619) | 该论文提出了一种通过利用多任务学习和基于损失的标签修正来学习多注释者数据的准确表示的方法。通过这种方法，可以有效地分离赞同和不赞同的注释，并且在单一或多注释者设置下改善预测性能。该方法还显示出对主观数据的额外标签噪声具有鲁棒性。 |
| [^67] | [Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds.](http://arxiv.org/abs/2310.19690) | 本论文提出了一种非对抗的基于变分自动编码器的对齐方法，通过引入一组对齐上界，解决了先前方法中存在的不稳定性和限制。实验证明，这种新颖的对齐损失可以在不改变原始架构的情况下取代对抗损失，扩展了应用范围。 |
| [^68] | [Controlled Decoding from Language Models.](http://arxiv.org/abs/2310.17022) | 本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。 |
| [^69] | [OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift.](http://arxiv.org/abs/2310.12793) | OODRobustBench是一个基准，用于评估和分析在分布迁移下的对抗性鲁棒性。大规模分析结果表明，对抗性鲁棒性在离群分布测试下存在严重的泛化问题，而内分布鲁棒性与离群分布鲁棒性呈强正线性相关。 |
| [^70] | [Online Algorithms with Uncertainty-Quantified Predictions.](http://arxiv.org/abs/2310.11558) | 这篇论文研究了如何在设计在线算法时最佳利用不确定性量化预测，并提出了一种考虑预测概率性的在线算法设计方法。 |
| [^71] | [In-Context Unlearning: Language Models as Few Shot Unlearners.](http://arxiv.org/abs/2310.07579) | 这项工作提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。这种方法解决了消除对于很大模型来说在计算上的困难，并在实践中具有更高的可行性和便捷性。 |
| [^72] | [Deep ReLU networks and high-order finite element methods II: Chebyshev emulation.](http://arxiv.org/abs/2310.07261) | 本论文研究了在有限分割上使用深度ReLU神经网络对连续分段多项式函数的表达速率和稳定性，提出了一种使用切比雪夫多项式展开系数进行编码的新颖ReLU NN替代模型构造，与基于ReLU NN模拟多项式的构造相比，在表达速率和稳定性方面获得了更好的界限。 |
| [^73] | [Adversarial Attacks on Combinatorial Multi-Armed Bandits.](http://arxiv.org/abs/2310.05308) | 本文研究了对组合多臂老虎机的奖励污染攻击，并给出了攻击可能性的条件。与以往对多臂老虎机的理解相反，我们发现特定CMAB实例的攻击可能性还取决于发哥实例是否被对手知晓。这表明在实践中对CMAB进行对抗攻击是困难的，因为对手大部分情况下无法了解环境的情况。 |
| [^74] | [Towards Causal Foundation Model: on Duality between Causal Inference and Attention.](http://arxiv.org/abs/2310.00809) | 该论文提出了一种名为Causal Inference with Attention (CInA)的新方法，利用因果推断和注意力的对偶关系，在复杂任务中实现了零样本的因果推断。 |
| [^75] | [Robust Energy Consumption Prediction with a Missing Value-Resilient Metaheuristic-based Neural Network in Mobile App Development.](http://arxiv.org/abs/2309.12484) | 本研究提出了一种基于元启发式方法增强的神经网络框架，旨在实现在移动应用开发中稳健的能耗预测。 |
| [^76] | [Learning to Intervene on Concept Bottlenecks.](http://arxiv.org/abs/2308.13453) | 该论文提出了一种扩展了概念瓶颈模型的概念瓶颈记忆模型（CB2M），通过学习将干预推广到不同情境并重新应用先前干预来自动改善模型性能。当没有先前的人类干预信息时，CB2M能够检测错误并请求有针对性的干预。 |
| [^77] | [On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget.](http://arxiv.org/abs/2308.12000) | 本文研究了在有限预算的随机二臂赌博机中进行最佳臂选择的问题，并证明不存在比等概率采样算法更好的算法。我们引入了一致稳定算法的概念，并证明任何在所有情况下与等概率采样算法表现一样好的算法必须属于这个类别。这一结果解决了之前的两个未解之谜。 |
| [^78] | [Activation Addition: Steering Language Models Without Optimization.](http://arxiv.org/abs/2308.10248) | 这项研究探讨了一种在推理时通过改变激活来预测性地改变语言模型行为的方法，并且相比于传统方法具有更低的计算和实施成本，并且能够保持模型性能。 |
| [^79] | [VITS : Variational Inference Thomson Sampling for contextual bandits.](http://arxiv.org/abs/2307.10167) | VITS是一种基于高斯变分推理的新算法，用于情境背离问题的汤普森抽样。它提供了强大的后验近似，计算效率高，并且在线性情境背离问题中达到与传统TS相同阶数的次线性遗憾上界。 |
| [^80] | [Noise-aware Speech Enhancement using Diffusion Probabilistic Model.](http://arxiv.org/abs/2307.08029) | 本文提出了一种基于噪声感知的语音增强方法，在扩散模型中引入噪声特定信息，通过噪声分类和多任务学习方案来增强噪声调节器的噪声特异性。证实该方法在VoiceBank-DEMAND数据集上取得了良好的实验效果。 |
| [^81] | [Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks.](http://arxiv.org/abs/2307.06887) | 通过双层ReLU神经网络，本论文提出了一种可证明的多任务表示学习方法，用于解决神经网络在实践中同时训练多个任务时遇到的问题。 |
| [^82] | [Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining.](http://arxiv.org/abs/2307.03887) | 本研究通过引入奖励重新加权、重选和重新训练的方法，改进了原型零件网络的分类效果，解决了学习从图像的虚假或不一致的部分进行分类的问题。 |
| [^83] | [MALIBO: Meta-learning for Likelihood-free Bayesian Optimization.](http://arxiv.org/abs/2307.03565) | MALIBO是一种元学习贝叶斯优化方法，通过直接学习跨任务的查询效用，并引入辅助模型以实现对新任务的稳健适应，克服了现有方法的可伸缩性和不确定性的限制。 |
| [^84] | [Towards a Better Theoretical Understanding of Independent Subnetwork Training.](http://arxiv.org/abs/2306.16484) | 本研究对独立子网络训练（IST）进行了理论分析，发现了IST与其他模型并行方法之间的根本差异。 |
| [^85] | [More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity.](http://arxiv.org/abs/2306.12214) | 本文提出了一种新的高概率PAC-Bayes界限，在有界和一般尾部行为的损失中均适用。此外，这些界限还能够保持随时有效性。 |
| [^86] | [Correcting Underrepresentation and Intersectional Bias for Fair Classification.](http://arxiv.org/abs/2306.11112) | 本文提出一种可以有效纠正数据偏差和交叉偏差的学习方法，并构造了一个重新加权方案，可以精确评估任何假设在真实分布上的损失。 |
| [^87] | [CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification.](http://arxiv.org/abs/2306.10649) | 本研究提出了CompanyKG，一种用于公司相似性量化的大规模异构图数据集。通过丰富的公司特征和关系表示，以及多个评估任务的基准测试，为公司相似性量化方法的综合评估提供了支持。 |
| [^88] | [Improving the Validity of Decision Trees as Explanations.](http://arxiv.org/abs/2306.06777) | 该论文介绍了一个新的决策树模型，利用挂起的树的方式提高了其解释性和统计性能，达到了无限深度决策树的水平，并可与XGBoost等最先进的方法相媲美。 |
| [^89] | [Robust Data-driven Prescriptiveness Optimization.](http://arxiv.org/abs/2306.05937) | 本文提出了一种鲁棒的数据驱动优化模型，其中处方性系数代替了经验风险最小化目标，可用于确定最大化上下文决策质量和参考决策以及侧面信息处方能力的最佳策略。 |
| [^90] | [Layer-Wise Feedback Alignment is Conserved in Deep Neural Networks.](http://arxiv.org/abs/2306.01870) | 本论文揭示了层间反馈对齐在深度神经网络中的保守性，并发现FA与GD之间存在隐式偏差的相似之处，同时阐明了ReLU网络中与反馈矩阵对齐的充分条件。 |
| [^91] | [EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost.](http://arxiv.org/abs/2306.01310) | EPIC提出了一种基于插值的方法来增强图数据集，通过利用图编辑距离生成与原始图相似但有结构变化的新图，从而提高了分类模型的泛化能力。 |
| [^92] | [Demystifying Oversmoothing in Attention-Based Graph Neural Networks.](http://arxiv.org/abs/2305.16102) | 本文通过数学分析证明基于注意力的图神经网络并不能解决平滑过度问题，在实际应用中需要更多关注不对称、状态相关和有向图结构。 |
| [^93] | [On the Identifiability of Markov Switching Models.](http://arxiv.org/abs/2305.15925) | 本文研究了马尔科夫转换模型的可辨识性，通过非线性高斯参数化迁移分布实现第一阶段马尔科夫依赖结构中的可辨识性条件。该方法适用于依赖于政权的因果发现和高维时间序列分割。 |
| [^94] | [Inhomogeneous graph trend filtering via a l2,0 cardinality penalty.](http://arxiv.org/abs/2304.05223) | 本文提出了一种基于L2，0基数惩罚的图趋势过滤（GTF）模型，可同时进行k-means聚类和基于图的最小割，以估计在节点之间具有不均匀平滑水平的分段平滑图信号，并在降噪、支持恢复和半监督分类任务上表现更好，比现有方法更高效地处理大型数据集。 |
| [^95] | [Privacy-Preserving CNN Training with Transfer Learning.](http://arxiv.org/abs/2304.03807) | 本文提出了一种使用迁移学习实现同态加密技术下隐私保护的CNN训练的方案，通过转换思想和更快的梯度变体，取得了最先进的性能。 |
| [^96] | [Learning to Optimize for Reinforcement Learning.](http://arxiv.org/abs/2302.01470) | 学习优化器在监督学习中取得了显著的成功，但在强化学习中面临梯度范围变化大、梯度分布非独立且不同、高方差偏差等问题。本文提出了梯度处理、管道训练和一种新颖的优化器结构来解决这些问题。 |
| [^97] | [Neural Common Neighbor with Completion for Link Prediction.](http://arxiv.org/abs/2302.00890) | 提出了神经通用邻居模型（NCN）用于链接预测，使用可学习的成对表示来捕捉节点之间的成对关系，以提高性能，同时解决链路不完整问题。 |
| [^98] | [How to select an objective function using information theory.](http://arxiv.org/abs/2212.06566) | 信息论告诉我们，为了最大化模型的信息量，选择可能性最高或表示误差比特最少的客观函数。将不同的客观函数转换为似然函数，它们的相对大小表示我们应该更喜欢哪个客观函数，而其大小的对数表示模型的预期不确定性。 |
| [^99] | [Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting.](http://arxiv.org/abs/2211.15856) | 本文研究了利用气候模型集合进行地表季节性预测的应用，超越了传统的平均方法，利用集合预测中的信息提高了预测准确性，关注了极端事件的预测，同时考虑了空间变化的预测集合。 |
| [^100] | [Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds.](http://arxiv.org/abs/2210.01672) | 本论文提出了一种通过在双曲流形上使用GPLVM来在连续领域中应用机器人分类法的方法，通过捕捉相关层次结构的双曲嵌入来建模分类数据，并采用图形先验和保持距离的后向约束来实现分类法结构的纳入。 |

# 详细

[^1]: 解释自监督表示学习的概率模型

    A Probabilistic Model to explain Self-Supervised Representation Learning

    [https://rss.arxiv.org/abs/2402.01399](https://rss.arxiv.org/abs/2402.01399)

    该论文提出了一个概率模型来解释自监督表示学习的机制，并展示了鉴别性自监督算法在表示中近似诱导潜变量结构的统一理论框架。

    

    自监督学习（SSL）通过利用辅助的无监督任务，例如对语义相关样本进行分类，如不同的数据增强或模态来学习表示。在众多SSL方法中，对比方法（例如SimCLR，CLIP和VicREG）因学习到的表示在下游性能上接近有监督学习而受到关注。然而，这些方法背后的机制的理论理解仍然存在困难。我们提出了一个生成潜变量模型来表示数据，并展示了几类具有鉴别性的自监督算法（包括对比方法）近似诱导其表示中的潜变量结构，从而提供了一个统一的理论框架。我们还证明了与互信息和投影头的相关性。通过生成式地拟合我们的模型（如SimVE），在常见的基准测试上（例如FashionMNIST，CIFAR10，CelebA），性能优于之前的VAE方法。

    Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
    
[^2]: 组合生成建模：单一模型并不是您所需要的全部

    Compositional Generative Modeling: A Single Model is Not All You Need

    [https://rss.arxiv.org/abs/2402.01103](https://rss.arxiv.org/abs/2402.01103)

    本文提出了一种组合生成方法，通过将较小的生成模型组合在一起来构建大型生成系统。该方法可以更高效地学习数据分布，实现对训练时未见的数据部分的泛化，并能够编写和构建新的生成模型。

    

    在人工智能研究中，通过训练大规模的巨大的生成模型来处理海量数据已经成为一种越来越主流的方法。本文中，我们认为我们应该通过将较小的生成模型组合在一起来构建大型生成系统。我们展示了这种组合生成方法如何以更高效的方式学习分布，使得我们在训练时未见的数据分布部分也能进行泛化。我们进一步展示了这种方法如何使我们能够为训练时完全未见的任务编写和构建新的生成模型。最后，我们展示在许多情况下，我们可以从数据中发现独立的组合组件。

    Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
    
[^3]: 跟着我重复：Transformer在复制任务上比状态空间模型更好

    Repeat After Me: Transformers are Better than State Space Models at Copying

    [https://rss.arxiv.org/abs/2402.01032](https://rss.arxiv.org/abs/2402.01032)

    这篇论文证明了Transformer模型在复制任务上的优势，相比于使用固定潜在状态的广义状态空间模型，Transformer模型能够更高效地复制和检索上下文信息。

    

    Transformer是序列建模的主要架构，但对于使用不依赖于序列长度的固定大小潜在状态的模型，也就是"广义状态空间模型" (GSSMs)，引起了越来越多的关注。在本文中，我们展示了虽然GSSMs在推理时间效率上有优势，但在需要从输入上下文复制的任务上，它们相对于transformer模型来说有限制。我们从对简单的字符串复制任务的理论分析开始，并证明了一个两层的transformer可以复制指数长度的字符串，而GSSMs由于其固定大小的潜在状态在根本上是有限制的。实证上，我们发现transformer在需要复制上下文的合成任务中，在效率和泛化性能上优于GSSMs。最后，我们评估了预训练的大型语言模型，并发现transformer模型在复制和检索上下文信息方面远远优于状态空间模型。

    Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. 
    
[^4]: 使用窗口过滤的近似最近邻搜索

    Approximate Nearest Neighbor Search with Window Filters

    [https://rss.arxiv.org/abs/2402.00943](https://rss.arxiv.org/abs/2402.00943)

    这篇论文提出了一种使用窗口过滤的近似最近邻搜索方法，能够在各种语义搜索问题中实现高速搜索，并在多个基准数据集上取得了显著的速度提升。

    

    我们定义并研究了$\textit{c-近似窗口搜索}$问题：近似最近邻搜索其中数据集中的每个点都有一个数值标签，目标是在任意标签范围内找到查询点的最近邻。许多语义搜索问题，例如带有时间戳过滤器的图像和文档搜索，或带有成本过滤器的产品搜索，是这个问题的自然例子。我们提出并在理论上分析了一种基于模块化树的框架，用于将解决传统c-近似最近邻问题的索引转化为解决窗口搜索的数据结构。在标准的最近邻基准数据集上，配备了随机标签值、对抗性构建的嵌入以及带有真实时间戳的图像搜索嵌入，我们获得了与现有解决方案相比高达75倍的速度提升，同时保持相同的召回率。

    We define and investigate the problem of $\textit{c-approximate window search}$: approximate nearest neighbor search where each point in the dataset has a numeric label, and the goal is to find nearest neighbors to queries within arbitrary label ranges. Many semantic search problems, such as image and document search with timestamp filters, or product search with cost filters, are natural examples of this problem. We propose and theoretically analyze a modular tree-based framework for transforming an index that solves the traditional c-approximate nearest neighbor problem into a data structure that solves window search. On standard nearest neighbor benchmark datasets equipped with random label values, adversarially constructed embeddings, and image search embeddings with real timestamps, we obtain up to a $75\times$ speedup over existing solutions at the same level of recall.
    
[^5]: 光谱卷积变压器：协调视觉变压器中的实部和复部多视图光谱算子

    Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer

    [https://arxiv.org/abs/2403.18063](https://arxiv.org/abs/2403.18063)

    该论文提出了光谱卷积变压器 (SCT)，通过结合局部信息的卷积操作和全局信息的复杂傅里叶基础，实现了对视觉变压器中实部和复部多视图光谱算子的协调，从而实现了更好的性能。

    

    视觉中使用的Transformer已经通过各种结构进行了研究 - 如ViT、PVT和Swin。这些工作旨在改进注意力机制并使其更加高效。与此不同的是，人们感受到了包含局部信息的需要，这导致在Transformer中引入卷积，如CPVT和CvT。我们使用复杂傅立叶基础捕捉全局信息，通过各种方法，如AFNO、GFNet和Spectformer实现全局令牌混合。我们提倡结合数据的三种不同视图 - 局部、全局和长程依赖性。我们还研究了仅使用实域光谱表示的最简单全局表示 - 通过Hartley变换获得。我们在初始层中使用卷积算子捕捉局部信息。通过这两个贡献，我们能够优化并获得一个提供改进性能的光谱卷积变压器（SCT）。

    arXiv:2403.18063v1 Announce Type: cross  Abstract: Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance 
    
[^6]: 非光滑隐式微分：确定性和随机收敛速率

    Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates

    [https://arxiv.org/abs/2403.11687](https://arxiv.org/abs/2403.11687)

    在非光滑设置下，提出了用于计算具有内映射的外映射固定点的隐式导数的新方法NSID，并提供了确定性情况下迭代微分（ITD）和近似隐式微分（AID）的改进线性收敛速率。

    

    我们研究了有效计算参数化不可微收缩映射固定点导数的问题。这个问题在机器学习中有广泛的应用，包括超参数优化、元学习和数据污染攻击。我们分析了两种流行的方法：迭代微分（ITD）和近似隐式微分（AID）。在非光滑设置中的一个关键挑战是链规则不再成立。在Bolte等人（2022）最近的工作基础上，他们证明了不可微分ITD的线性收敛，我们提供了确定性情况下ITD和AID的改进线性收敛速率。我们进一步介绍了NSID，一种新的方法，用于在固定点被定义为只通过随机无偏估计器访问的外映射和内映射的组合时计算隐式导数。我们建立了该方法的收敛速率。

    arXiv:2403.11687v1 Announce Type: cross  Abstract: We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of 
    
[^7]: 深度限价订单簿预测

    Deep Limit Order Book Forecasting

    [https://arxiv.org/abs/2403.09267](https://arxiv.org/abs/2403.09267)

    该研究利用深度学习方法预测纳斯达克交易所股票的限价订单簿中间价格变动，提出了一个创新的操作框架来评估预测的实用性。

    

    我们利用尖端的深度学习方法探索了在纳斯达克交易所上交易的一组异质股票的高频限价订单簿中间价格变动的可预测性。在此过程中，我们发布了“LOBFrame”，一个开源代码库，可以高效处理大规模限价订单簿数据，并定量评估最先进的深度学习模型的预测能力。我们的结果是双重的。我们证明股票的微观结构特征影响深度学习方法的有效性，并且它们的高预测能力不一定对应可操作的交易信号。我们认为传统的机器学习指标未能充分评估限价订单簿环境中预测的质量。作为替代，我们提出了一个创新的操作框架，通过专注于准确预测的概率来评估预测的实用性。

    arXiv:2403.09267v1 Announce Type: cross  Abstract: We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base, to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that assesses predictions' practicality by focusing on the probability of accurately forecasting com
    
[^8]: COMQ: 一种无需反向传播的后训练量化算法

    COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization

    [https://arxiv.org/abs/2403.07134](https://arxiv.org/abs/2403.07134)

    提出了一种名为COMQ的创新后训练量化算法，通过逐层减小重构误差来有效降低大型神经网络的存储要求，同时保持原始准确性。

    

    后训练量化（PTQ）已经成为一种将大型神经网络压缩的实用方法，使其在部署时高度高效。然而，有效地将这些模型降至低比特表示而不损害原始准确性仍然是一个关键挑战。在本文中，我们提出了一种创新的PTQ算法称为COMQ，它通过依次减小逐层重构误差来进行坐标方向上的最小化。我们考虑了广泛使用的整数量化，其中每个量化权重可以分解为一个共享的浮点标量和一个整数位编码。在固定层内，COMQ将所有缩放因子和位编码视为重构误差的变量。每次迭代都会沿着一个坐标轴改进这个错误，同时保持所有其他变量恒定。COMQ易于使用，无需调整超参数。它只涉及点乘和四舍五入。

    arXiv:2403.07134v1 Announce Type: new  Abstract: Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding o
    
[^9]: 降噪自回归表示学习

    Denoising Autoregressive Representation Learning

    [https://arxiv.org/abs/2403.05196](https://arxiv.org/abs/2403.05196)

    DARL使用仅解码器的Transformer进行自回归预测，通过使用去噪补丁解码器和特定噪声计划改善图像生成能力，实现了与蒙版预测模型相近的性能。

    

    在本文中，我们探索了一种用于学习视觉表示的新的生成方法。我们的方法DARL采用了仅解码器的Transformer来自回归地预测图像补丁。我们发现仅使用均方误差（MSE）进行训练会得到强大的表示。为了增强图像生成能力，我们使用去噪补丁解码器将MSE损失替换为扩散目标。我们展示了通过使用量身定制的噪声计划和更大模型的更长训练可以改善学习到的表示。值得注意的是，最佳计划与标准图像扩散模型中常用的计划有显著差异。总体而言，尽管其简单的架构，DARL在微调协议下的性能接近最先进的蒙版预测模型。这标志着向能够同时进行视觉感知和生成的统一模型迈出了重要的一步，有效地结合了其优势。

    arXiv:2403.05196v1 Announce Type: new  Abstract: In this paper, we explore a new generative approach for learning visual representations. Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong representations. To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder. We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models. Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths o
    
[^10]: ENOT：期望回归用于神经优化传输的快速和准确训练

    ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport

    [https://arxiv.org/abs/2403.03777](https://arxiv.org/abs/2403.03777)

    通过期望回归正则化，本论文提出了一种新的神经优化传输（NOT）训练程序扩展，能够有效地估计最优输运方案，并使学习变得稳定。

    

    我们提出了一种新的神经优化传输（NOT）训练程序扩展，通过特定的共轭势正则化能够准确和高效地估计最优输运方案。现有NOT求解器的主要瓶颈在于找到共轭算子（即c-transform）的接近精确近似的过程，这要么通过优化最小-最大目标，要么通过计算密集型的对初始近似预测的精细调整来完成。我们通过提出一种新的、在期望回归形式上强制适应性条件于学习对偶势的理论上合理化损失来解决这两个问题。这样的正则化提供了可能共轭势分布的上限估计，并使学习变得稳定，消除了对额外广泛微调的需求。我们正式证明了我们的方法的效率。

    arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
    
[^11]: 具有单边信息的状态约束零和微分博弈

    State-Constrained Zero-Sum Differential Games with One-Sided Information

    [https://arxiv.org/abs/2403.02741](https://arxiv.org/abs/2403.02741)

    我们将零和微分博弈的理论扩展到具有状态约束，并提出了计算行为策略所需的原始和对偶子动态原则。

    

    我们研究了具有状态约束和单边信息的零和微分博弈，其中知情玩家（玩家1）具有对于未知于不知情玩家（玩家2）的分类支付类型。玩家1的目标是在不违反约束的情况下最小化他的支付，而玩家2的目标是要么违反状态约束，要么最大化支付。我们的理论贡献是将Cardaliaguet（2007）对于没有状态约束的类似博弈价值存在性和对于玩家的共同信仰凸性的结果扩展到具有状态约束的微分博弈，并推导了用于计算行为策略的原始和对偶子动态原则。

    arXiv:2403.02741v1 Announce Type: cross  Abstract: We study zero-sum differential games with state constraints and one-sided information, where the informed player (Player 1) has a categorical payoff type unknown to the uninformed player (Player 2). The goal of Player 1 is to minimize his payoff without violating the constraints, while that of Player 2 is to either violate the state constraints, or otherwise, to maximize the payoff. One example of the game is a man-to-man matchup in football. Without state constraints, Cardaliaguet (2007) showed that the value of such a game exists and is convex to the common belief of players. Our theoretical contribution is an extension of this result to differential games with state constraints and the derivation of the primal and dual subdynamic principles necessary for computing the behavioral strategies. Compared with existing works on imperfect-information dynamic games that focus on scalability and generalization, our focus is instead on reveal
    
[^12]: Wukong: 迈向大规模推荐的标度律

    Wukong: Towards a Scaling Law for Large-Scale Recommendation

    [https://arxiv.org/abs/2403.02545](https://arxiv.org/abs/2403.02545)

    Wukong通过堆叠因子分解机和协同增长策略，在推荐领域建立了一个标度律，并在质量上优于现有模型。

    

    缩放定律在提高模型质量方面起着关键作用。然而，迄今为止的推荐模型并没有展现出类似于大型语言模型领域观察到的定律，这是由于它们的升级机制的低效性。本文提出了一种基于纯堆叠因子分解机和协同增长策略的有效网络架构，统称为Wukong，以在推荐领域建立一个标度律。Wukong的独特设计使其能够通过更高更宽的层次简单捕获各种任意阶的交互。我们在六个公共数据集上进行了广泛评估，结果表明，与最先进的模型相比，Wukong在质量方面始终表现优越。此外，我们评估了Wuko

    arXiv:2403.02545v1 Announce Type: cross  Abstract: Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wuko
    
[^13]: ICC：用于多模态数据集筛选的图像描述具体性量化

    ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation

    [https://arxiv.org/abs/2403.01306](https://arxiv.org/abs/2403.01306)

    提出一种新的度量标准，图像描述具体性，用于评估标题文本的具体性和相关性，以帮助在多模态学习中隔离提供最强信号的最具体样本。

    

    arXiv:2403.01306v1 公告类型：新摘要：针对配对文本-图像数据的Web规模训练在多模态学习中变得越来越重要，但挑战在野外数据集的高噪声特性。标准数据过滤方法成功去除了不匹配的文本-图像对，但允许语义相关但非常抽象或主观的文本。这些方法缺乏细粒度的能力来隔离提供在嘈杂数据集中学习最强信号的最具体样本。在这项工作中，我们提出了一种新的度量标准，图像描述具体性，评估没有图像参考的标题文本以衡量其具体性和相关性，以供在多模态学习中使用。我们的方法利用了衡量视觉-语义信息损失的强基础模型来进行评估。我们证明了这与人类对单词和句子级文本具体性的评估高度相关。此外，我们展示了...

    arXiv:2403.01306v1 Announce Type: new  Abstract: Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show tha
    
[^14]: Dr. Strategy: 具有战略梦想的基于模型的通用型Agent

    Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming

    [https://arxiv.org/abs/2402.18866](https://arxiv.org/abs/2402.18866)

    本文提出了一个新的基于模型的通用型Agent Dr. Strategy，配备了Dreaming Strategy，实现了在梦境中学习一组潜在地标，并利用这些地标学习地标条件的高速公路策略。

    

    Model-based reinforcement learning（MBRL）一直是改善样本效率问题并创建通用型Agent的主要方法。然而，对于增强梦想策略本身的努力并不多。在本文中，受认知科学观察到人类在规划时使用空间分隔与征服策略的启发，我们提出了一个新的MBRL agent，称为Dr. Strategy，它配备了一种新颖的Dreaming Strategy。所提出的Agent在梦境中实现了一种类似于分隔与征服的策略。

    arXiv:2402.18866v1 Announce Type: new  Abstract: Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question whether and how an agent can "dream better" in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achi
    
[^15]: 迭代INLA用于非线性动力系统中的状态和参数估计

    Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical Systems

    [https://arxiv.org/abs/2402.17036](https://arxiv.org/abs/2402.17036)

    提出了一种基于迭代INLA的方法，用于在非线性动力系统中推断状态和参数，能够保留可解释性并且适用于任意非线性系统。

    

    数据同化（DA）方法使用源自微分方程的先验条件来稳健地对数据进行插值和外推。流行的技术，如处理高维非线性PDE先验条件的集合方法，主要关注状态估计，但可能会在学习参数方面遇到困难。另一方面，基于机器学习的方法可以自然地学习状态和参数，但它们的适用性可能受到限制，或者产生难以解释的不确定性。受空间统计中集成嵌套拉普拉斯近似（INLA）方法的启发，我们提出了一个基于迭代线性化动力学模型的DA替代方法。这在每次迭代中产生一个高斯马尔可夫随机场，使得可以使用INLA来推断状态和参数。我们的方法可以用于任意非线性系统，同时保持可解释性，并且进一步被证明可以o

    arXiv:2402.17036v1 Announce Type: cross  Abstract: Data assimilation (DA) methods use priors arising from differential equations to robustly interpolate and extrapolate data. Popular techniques such as ensemble methods that handle high-dimensional, nonlinear PDE priors focus mostly on state estimation, however can have difficulty learning the parameters accurately. On the other hand, machine learning based approaches can naturally learn the state and parameters, but their applicability can be limited, or produce uncertainties that are hard to interpret. Inspired by the Integrated Nested Laplace Approximation (INLA) method in spatial statistics, we propose an alternative approach to DA based on iteratively linearising the dynamical model. This produces a Gaussian Markov random field at each iteration, enabling one to use INLA to infer the state and parameters. Our approach can be used for arbitrary nonlinear systems, while retaining interpretability, and is furthermore demonstrated to o
    
[^16]: 基于语义有效性通道错误的实用目标导向通信

    Pragmatic Goal-Oriented Communications under Semantic-Effectiveness Channel Errors

    [https://arxiv.org/abs/2402.16858](https://arxiv.org/abs/2402.16858)

    在AI辅助的6G网络中，实现了语义、实用和目标导向通信策略的整合，通过提出数学模型解决了语言不匹配导致的出错问题

    

    在即将到来的AI辅助的6G网络中，整合语义、实用和目标导向通信策略变得至关重要。这种整合将实现仅涉及相关任务数据的感知、传输和处理，确保传达的信息具有可理解的、实用的语义重要性，与目标和需求相一致。在此背景下，除了典型无线通信动态引起的错误外，由于语义处理能力的限制、发射方意图和接收方解释之间的意义差异以及发射方和接收方之间的语言和知识表征差异，还可能出现发射方意图和接收方解释之间的潜在失真。本文的主要贡献有两个方面。首先，它提出并详细介绍了一种新颖的数学模型，用于描述源于语言不匹配的错误，包括语义和有效性级别的错误。

    arXiv:2402.16858v1 Announce Type: cross  Abstract: In forthcoming AI-assisted 6G networks, integrating semantic, pragmatic, and goal-oriented communication strategies becomes imperative. This integration will enable sensing, transmission, and processing of exclusively pertinent task data, ensuring conveyed information possesses understandable, pragmatic semantic significance, aligning with destination needs and goals. Without doubt, no communication is error free. Within this context, besides errors stemming from typical wireless communication dynamics, potential distortions between transmitter-intended and receiver-interpreted meanings can emerge due to limitations in semantic processing capabilities, as well as language and knowledge representation disparities between transmitters and receivers. The main contribution of this paper is two-fold. First, it proposes and details a novel mathematical modeling of errors stemming from language mismatches at both semantic and effectiveness le
    
[^17]: 文本引导下的跨模态上下文扩散模型用于视觉生成与编辑

    Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing

    [https://arxiv.org/abs/2402.16627](https://arxiv.org/abs/2402.16627)

    提出了一种新颖且通用的上下文化扩散模型（ContextDiff），通过在正向和逆向过程中融入文本条件和视觉样本之间的交互和对齐，以便在视觉生成中更准确地传达文本语义

    

    有条件的扩散模型在高保真度文本引导的视觉生成和编辑中展现出卓越的性能。然而，当前的文本引导视觉扩散模型主要集中于将文本-视觉关系独占地融入到逆过程中，往往忽略了它们在正向过程中的相关性。这种正反过程之间的不一致可能限制了在视觉合成结果中精确传达文本语义。为了解决这个问题，我们提出了一种新颖且通用的上下文化扩散模型（ContextDiff），通过将跨模态上下文包含文本条件和视觉样本之间的交互和对齐融入到正向和逆向过程中。我们将这个上下文传播到两个过程中的所有时间步，以调整它们的轨迹，从而促进跨模态条件建模。我们将我们的上下文化扩散推广到DDPMs和...

    arXiv:2402.16627v2 Announce Type: cross  Abstract: Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and 
    
[^18]: 通过位置编码比较图变换器

    Comparing Graph Transformers via Positional Encodings

    [https://arxiv.org/abs/2402.14202](https://arxiv.org/abs/2402.14202)

    本文比较了使用绝对位置编码（APEs）和相对位置编码（RPEs）的图变换器，在最大化区分能力方面是等效的。

    

    图变换器的区分能力与位置编码的选择紧密相关：用于增强基本变换器与图信息的特征。有两种主要类型的位置编码：绝对位置编码（APEs）和相对位置编码（RPEs）。APEs为每个节点分配特征，并作为变换器的输入。而RPEs则为每对节点（例如，图距离）分配一个特征，并用于增强注意力块。先验上，目前不清楚哪种方法更有利于最大化生成的图变换器的能力。本文旨在了解这两种不同类型位置编码之间的关系。有趣的是，我们展示了使用APEs和RPEs的图变换器在区分能力方面是等效的。特别地，我们展示了如何在保持其区分能力的同时交换APEs和RPEs。

    arXiv:2402.14202v1 Announce Type: new  Abstract: The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in
    
[^19]: 带有多个领域的本地分布偏移的乘幂稳健估计

    Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains

    [https://arxiv.org/abs/2402.14145](https://arxiv.org/abs/2402.14145)

    提出了一种两阶段的乘幂稳健估计方法，用于改善表格数据分析中每个个体部分的模型性能，并建立了在测试风险上的理论保证。

    

    分布偏移在现实世界的机器学习应用中普遍存在，给在一个数据分布上训练的模型推广到另一个数据分布带来挑战。本文专注于数据分布随整个总体的多个部分变化的情形，并仅在每个部分内对训练与测试（部署）数据分布的差异进行局部假设。我们提出了一种两阶段的乘幂稳健估计方法，用于改善表格数据分析中每个个体部分的模型性能。该方法涉及拟合基于从多个部分的训练数据中学到的模型的线性组合，然后对每个部分进行细化。我们的方法旨在与常用的现成机器学习模型一起实施。我们在测试风险上建立了该方法泛化界限的理论保证。通过大量实验...

    arXiv:2402.14145v1 Announce Type: cross  Abstract: Distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another. We focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment. We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. Our method is designed to be implemented with commonly used off-the-shelf machine learning models. We establish theoretical guarantees on the generalization bound of the method on the test risk. With extensive experiments
    
[^20]: 叙事背景的图表示：通过回顾性问题的连贯依赖

    Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions

    [https://arxiv.org/abs/2402.13551](https://arxiv.org/abs/2402.13551)

    提出了一种新颖且实用的叙事理解范式，通过在叙事中形成图NARCO来描述整个背景的任务无关的连贯依赖，其中的边反映了高层次的连贯关系，无需依赖人类注释。

    

    这项工作介绍了一种新颖且实用的叙事理解范式，这是基于一个观察：叙述中的个别段落通常是相互关联的，而不是孤立的。因此，我们提出在叙事中形成一个名为NARCO的图，描述整个背景的任务无关的连贯依赖。特别是，NARCO中的边涵盖了两个上下文片段之间的自由形式回顾性问题，反映了高层次的连贯关系，受到人类认知感知的启发，人类不断从先前背景中重申相关事件。重要的是，我们的图是通过我们设计的两阶段LLM提示实例化的，因此无需依赖人类注释。我们展示了三个关于其实际效用的独特研究，通过总结识别检验边的有效性，通过情节检索进行本地上下文增强，以及通过长文档问答示例化的更广泛应用。

    arXiv:2402.13551v1 Announce Type: new  Abstract: This work introduces a novel and practical paradigm for narrative comprehension, stemming from the observation that individual passages within narratives are often cohesively related than being isolated. We therefore propose to formulate a graph upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context. Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context. Importantly, our graph is instantiated through our designed two-stage LLM prompting, thereby without reliance on human annotations. We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document QA. Expe
    
[^21]: ARL2: 通过自导自适应相关性标记将检索器与黑盒大型语言模型对齐

    ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling

    [https://arxiv.org/abs/2402.13542](https://arxiv.org/abs/2402.13542)

    ARL2提出了一种检索器学习技术，利用LLMs作为标注者，并采用自适应自训练策略，能够有效减少注释成本，并在NQ和MMLU上取得了5.4%和4.6%的准确度提升。

    

    arXiv:2402.13542v1 公告类型: 交叉 摘要: 检索增强生成通过整合外部知识源的相关信息改进大型语言模型（LLMs），使LLMs能够适应特定领域，并减轻知识密集任务中的幻觉。然而，由于其分开的训练过程和LLMs的黑盒特性，现有的检索器通常与LLMs不匹配。为解决这一挑战，我们提出了ARL2，一种利用LLMs作为标注者的检索器学习技术。ARL2利用LLMs注释和评分相关证据，从而能够从强大的LLM监督中学习检索器。此外，ARL2使用自适应自训练策略来策划高质量和多样性相关性数据，可以有效降低标注成本。大量实验表明ARL2的有效性，与最先进方法相比，在NQ上提高了5.4%的准确率，在MMLU上提高了4.6%。

    arXiv:2402.13542v1 Announce Type: cross  Abstract: Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionall
    
[^22]: 选择如何泄漏隐私：重新审视私有选择及超参数调整的改进结果

    How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning

    [https://arxiv.org/abs/2402.13087](https://arxiv.org/abs/2402.13087)

    本论文探讨了超参数调整中的隐私性问题，发现当前的隐私分析在一般情况下是紧密的，但在特定的超参数调整问题上则不再成立，并通过隐私审计揭示了当前理论隐私界与实证之间的显著差距。

    

    我们研究了在超参数调整中保证差分隐私(DP)的问题，这是机器学习中一个关键的过程，涉及从几个运行中选择最佳的过程。与许多私有算法（包括普遍存在的DP-SGD）不同，调整的隐私影响仍然不够了解。最近的研究提出了一个通用的私有解决方案用于调整过程，然而一个根本的问题仍然存在：当前解决方案的隐私界是否紧密？本文对这个问题提出了积极和消极的答案。最初，我们提供的研究证实了当前的隐私分析在一般意义上确实是紧密的。然而，当我们专门研究超参数调整问题时，这种紧密性则不再成立。首先，通过对调整过程进行隐私审计来证明了这一点。我们的研究结果突显了当前理论隐私界与实证之间存在重大差距。

    arXiv:2402.13087v1 Announce Type: new  Abstract: We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several. Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood. Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight?   This paper contributes both positive and negative answers to this question. Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense. However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds. This is first demonstrated by applying privacy audit on the tuning process. Our findings underscore a substantial gap between the current theoretical privacy bound and the empirica
    
[^23]: 探究模型不稳定性对解释和不确定性的影响

    Investigating the Impact of Model Instability on Explanations and Uncertainty

    [https://arxiv.org/abs/2402.13006](https://arxiv.org/abs/2402.13006)

    模型稳定性对解释和不确定性的影响进行了调查，并发现实际扰动对性能和解释影响较小，但掩盖却有 drastical 影响。

    

    可解释的AI方法有助于理解模型行为，然而，对输入进行微小、不可察觉的扰动可能会极大地扭曲解释。这些解释通常在模型部署之前被全面评估，因此很难评估特定解释的可信度。一些研究已经尝试为解释创建置信度估计器，但没有人调查不确定性和解释质量之间的现有联系。我们通过在推断时引入噪声来人为模拟文本输入中的认识不确定性。在这项大规模实证研究中，我们插入不同级别的噪声扰动，并测量对预训练语言模型的输出和不同不确定性度量的影响。实际扰动对性能和解释的影响很小，然而掩盖却有 drastical 影响。我们发现高不确定性并不一定意味着解释不佳。

    arXiv:2402.13006v1 Announce Type: cross  Abstract: Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn't necessarily imply low explanation 
    
[^24]: 在深度强化学习中，修剪网络是一个好网络

    In deep reinforcement learning, a pruned network is a good network

    [https://arxiv.org/abs/2402.12479](https://arxiv.org/abs/2402.12479)

    通过逐渐剪枝，使代理能够最大程度地发挥参数效能，从而产生比传统网络显著性能提升的网络，并展现出一种“缩放定律”。

    

    最近的研究表明，深度强化学习代理在有效利用其网络参数方面存在困难。我们利用对稀疏训练技术优势的先前见解，并证明逐渐剪枝使代理能够最大程度地发挥参数效能。这导致网络比传统网络产生显著的性能改进，并表现出一种“缩放定律”，仅使用完整网络参数的一小部分。

    arXiv:2402.12479v1 Announce Type: cross  Abstract: Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of "scaling law", using only a small fraction of the full network parameters.
    
[^25]: 关于基于扩散的文本转语音模型的语义潜空间

    On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models

    [https://arxiv.org/abs/2402.12423](https://arxiv.org/abs/2402.12423)

    本研究在文本转语音模型中探索了冻结模型的潜空间，发现其中包含丰富的语义信息，并提出了一些新方法来找出其中的语义方向，从而实现了不经过额外训练、架构更改或数据需求就能进行音频编辑。

    

    在文本转语音（TTS）领域，Denoising Diffusion Models (DDMs) 的引入日益增多，为合成高质量语音提供了巨大价值。尽管它们展示出令人印象深刻的音频质量，但它们的语义能力程度尚不明确，并且控制合成语音的声音特性仍然是一个挑战。受图像合成最新进展的启发，我们探索了冻结的TTS模型的潜空间，该空间由DDM去噪器的潜空间激活组成。我们发现这个空间包含丰富的语义信息，并概述了若干查找其中语义方向的新方法，包括监督和无监督方法。然后，我们演示了如何利用这些方法进行现成音频编辑，无需进一步训练、架构更改或数据需求。我们呈现了编辑后音频的语义和声学特质的证据，并提供了补充样本。

    arXiv:2402.12423v1 Announce Type: cross  Abstract: The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech (TTS) domain is rising, providing great value in synthesizing high quality speech. Although they exhibit impressive audio quality, the extent of their semantic capabilities is unknown, and controlling their synthesized speech's vocal properties remains a challenge. Inspired by recent advances in image synthesis, we explore the latent space of frozen TTS models, which is composed of the latent bottleneck activations of the DDM's denoiser. We identify that this space contains rich semantic information, and outline several novel methods for finding semantic directions within it, both supervised and unsupervised. We then demonstrate how these enable off-the-shelf audio editing, without any further training, architectural changes or data requirements. We present evidence of the semantic and acoustic qualities of the edited audio, and provide supplemental samples: h
    
[^26]: 探索精度和召回率以评估LLMs的质量和多样性

    Exploring Precision and Recall to assess the quality and diversity of LLMs

    [https://arxiv.org/abs/2402.10693](https://arxiv.org/abs/2402.10693)

    该研究提出了一种新的评估框架，将精度和召回率指标从图像生成转化为文本生成，细致评估了LLMs生成文本的质量和多样性，揭示了当前LLMs在生成任务中性能表现的重要见解。

    

    这篇论文介绍了一种针对大型语言模型（LLMs）如Llama-2和Mistral的新型评估框架，重点是将图像生成的精度和召回率指标转化为文本生成。这种方法允许对生成文本的质量和多样性进行细致评估，而无需对齐的语料库。通过对最先进的语言模型进行全面评估，研究揭示了它们在开放生成任务上的表现，这是传统基准无法充分捕捉的。研究结果突出了在模型利用人类反馈进行微调时，生成样本质量和多样性之间的权衡。这项工作扩展了基于分布的自然语言处理评估工具包，为当前LLMs在生成多样性和高质量文本方面面临的实际能力和挑战提供了见解。

    arXiv:2402.10693v1 Announce Type: new  Abstract: This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text.
    
[^27]: 学习性是一种紧凑性质

    Learnability is a Compact Property

    [https://arxiv.org/abs/2402.10360](https://arxiv.org/abs/2402.10360)

    监督学习问题的困难性具有紧凑的有限特性表征。

    

    最近关于学习的工作取得了一个引人注目的结果：各种问题的可学习性可能是不可判定的，或者与标准集合论ZFC公理无关。此外，这种问题的可学习性可能不是具有有限特性的属性：非正式地说，它不能通过检查问题的有限投影来检测。

    arXiv:2402.10360v1 Announce Type: new  Abstract: Recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard ZFC axioms of set theory. Furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem.   On the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. How can these results be reconciled? More precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations?   We demonstrate that the difficulty of supervised learning with metric losses admits a tight finite characterization. In particular, we prove that the sample complexity of learning a hypothesis class can be detected by ex
    
[^28]: 分布式学习中的隐私攻击

    Privacy Attacks in Decentralized Learning

    [https://arxiv.org/abs/2402.10001](https://arxiv.org/abs/2402.10001)

    该论文介绍了分布式学习中的隐私攻击，针对分布式梯度下降（D-GD）提出了首个攻击方法，能够使用户重建其邻域之外其他用户的私有数据，并验证了这种攻击的有效性。

    

    分布式梯度下降（D-GD）允许一组用户在网络图中通过迭代平均本地模型更新与其邻居合作学习而无需共享数据。非邻居节点之间的直接通信的缺失可能导致用户无法推断出关于其他用户数据的精确信息。在这项工作中，我们提出了首个针对D-GD的攻击，使一个用户（或一组用户）能够重建其邻域之外其他用户的私有数据。我们的方法基于对传闻平均协议的重建攻击，然后将其扩展以处理D-GD提出的额外挑战。我们在真实图和数据集上验证了我们攻击的有效性，结果显示单个或少数攻击者所威胁到的用户数量通常是令人惊讶的大。我们对一些方案进行了经验性的研究。

    arXiv:2402.10001v1 Announce Type: new  Abstract: Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph. The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others. In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood. Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD. We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large. We empirically investigate some
    
[^29]: API Pack：一个用于API调用生成的大规模多语言数据集

    API Pack: A Massive Multilingual Dataset for API Call Generation

    [https://arxiv.org/abs/2402.09615](https://arxiv.org/abs/2402.09615)

    这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成

    

    我们介绍了API Pack，一个包含超过一百万个指令-API调用对的多语言数据集，旨在提高大型语言模型的API调用生成能力。通过实验，我们证明了API Pack在提升模型在这一特定任务上的效果的同时，保持其在一般编码方面的整体熟练程度。仅在20,000个Python实例上对CodeLlama-13B进行微调，其生成未见过的API调用的准确率比GPT-3.5和GPT-4分别高出10%和5%。扩展到100k个例子可以提高对训练期间未见过的新API的泛化能力。此外，实现了跨语言的API调用生成，而无需大量语言特定的数据。数据集、经过微调的模型和整体代码库可在https://github.com/anonymous_url上公开获取。

    arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
    
[^30]: 使用双阶段扰动测试通过必要性和充分性进行特征归因，以进行因果解释

    Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation

    [https://arxiv.org/abs/2402.08845](https://arxiv.org/abs/2402.08845)

    这篇论文提出了一种使用双阶段扰动测试来进行特征归因的方法，通过计算扰动一个特征对预测变化的必要性和充分性作用的概率来衡量特征重要性。该方法能够增强特征归因方法在区分不同特征贡献方面的能力。

    

    我们研究了机器学习中的可解释性问题。为了解决这个问题，特征归因方法（FAMs）通过扰动测试来测量每个特征的贡献，其中在不同扰动下的预测差异进行比较。然而，在特征的预测变化相同的情况下，这种扰动测试可能无法准确区分不同特征的贡献。为了增强FAMs在这种具有挑战性的情况下区分不同特征贡献的能力，我们提出利用扰动一个特征对预测变化起到必要性和充分性作用的概率（PNS）作为特征重要性的度量。我们的方法，利用必要性和充分性进行特征归因（FANS），通过涉及两个阶段（事实性和干预性）的扰动测试计算PNS。在实践中，为了生成反事实样本，我们使用了一个重新...

    arXiv:2402.08845v1 Announce Type: new Abstract: We investigate the problem of explainability in machine learning.To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate counterfactual samples, we use a re
    
[^31]: 模型评估与选择在时间分布转移下的研究

    Model Assessment and Selection under Temporal Distribution Shift

    [https://arxiv.org/abs/2402.08672](https://arxiv.org/abs/2402.08672)

    本文研究了在变化环境中的模型评估与选择问题，通过合成不同时期的数据集，并开发了自适应滚动窗口方法来估计模型的泛化误差以及比较不同模型之间的差异。实验证明了我们提出的方法在非稳态数据中的适应性。

    

    我们通过合成当前时期和历史时期的数据集，研究了在变化环境中的模型评估与选择。为了解决未知和可能任意的时间分布转移，我们开发了一种自适应滚动窗口方法来估计给定模型的泛化误差。这种策略还通过估计两个候选模型之间的泛化误差差异来方便比较。我们进一步将两两比较整合到单场淘汰赛中，从候选模型集合中实现了近乎最优的模型选择。理论分析和数值实验证明了我们所提出方法对数据非稳态的适应性。

    We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.
    
[^32]: 专家组合解锁深度强化学习的参数缩放

    Mixtures of Experts Unlock Parameter Scaling for Deep RL

    [https://arxiv.org/abs/2402.08609](https://arxiv.org/abs/2402.08609)

    本文证明了将专家组合模块融入基于值的网络中，尤其是软MoE，可以实现更具参数可扩展性的深度强化学习模型，这提供了强有力的实证证据以发展强化学习的缩放定律。

    

    最近对（自我）监督学习模型的快速进展很大程度上是通过实证缩放定律预测的：模型的性能与其规模成比例。然而，在强化学习领域中，寻找类似的缩放定律仍然困难，因为增加模型的参数数量往往会损害其最终性能。在本文中，我们证明将专家组合（MoE）模块，特别是软MoE（Puigcerver等人，2023年），融入基于值的网络中，可以得到更具参数可扩展性的模型，通过各种训练方案和模型规模的显著性能提升加以证明。因此，这项工作为发展强化学习的缩放定律提供了有力的实证证据。

    The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
    
[^33]: 使用随机平滑的自适应分层认证进行分割

    Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing

    [https://arxiv.org/abs/2402.08400](https://arxiv.org/abs/2402.08400)

    本文提出了一种适用于图像语义分割的自适应分层认证方法，能够在多级层级标签空间中认证图像像素，并提供更多经过认证的语义信息。

    

    通常的认证方法是在预定义的细粒度类别集上操作。然而，在本文中，我们提出了一种新颖、更普遍且实用的设置，即自适应分层认证用于图像语义分割。在这个设置中，认证可以在由细到粗的多级层级标签空间中进行。与经典方法不同，在不稳定的组件上，我们的方法自适应地放松了认证到层级中的更粗粒度的级别。这种放松降低了放弃率，同时提供了更多经过认证的语义有意义的信息。我们数学地形式化了问题设置，并首次引入了一种自适应分层认证算法用于图像语义分割，它可以在层级中对图像像素进行认证，并证明了其保证的正确性。由于认证的准确度在遍历时不考虑信息丢失。

    Common certification methods operate on a flat pre-defined set of fine-grained classes. In this paper, however, we propose a novel, more general, and practical setting, namely adaptive hierarchical certification for image semantic segmentation. In this setting, the certification can be within a multi-level hierarchical label space composed of fine to coarse levels. Unlike classic methods where the certification would abstain for unstable components, our approach adaptively relaxes the certification to a coarser level within the hierarchy. This relaxation lowers the abstain rate whilst providing more certified semantically meaningful information. We mathematically formulate the problem setup and introduce, for the first time, an adaptive hierarchical certification algorithm for image semantic segmentation, that certifies image pixels within a hierarchy and prove the correctness of its guarantees. Since certified accuracy does not take the loss of information into account when traversing
    
[^34]: 具有随机游走的差分隐私去中心化学习

    Differentially Private Decentralized Learning with Random Walks

    [https://arxiv.org/abs/2402.07471](https://arxiv.org/abs/2402.07471)

    这项工作研究了具有随机游走算法的差分隐私去中心化学习，并使用最近的差分隐私变种推导了节点之间的隐私损失。实验结果表明，与节点之间的八卦算法相比，随机游走算法更能提供较好的隐私保证。

    

    深圳的智能手机取而代之的是传统移动电话，这是现代科技发展的必然结果。

    The popularity of federated learning comes from the possibility of better scalability and the ability for participants to keep control of their data, improving data security and sovereignty. Unfortunately, sharing model updates also creates a new privacy attack surface. In this work, we characterize the privacy guarantees of decentralized learning with random walk algorithms, where a model is updated by traveling from one node to another along the edges of a communication graph. Using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by graph theoretic quantities. Our results further reveal that random walk algorithms tends to yield better privacy guarantees than gossip algorithms for nodes close from each other. We supplement our theoretical results with empir
    
[^35]: PASOA-基于粒子的贝叶斯最优自适应设计

    PASOA- PArticle baSed Bayesian Optimal Adaptive design

    [https://arxiv.org/abs/2402.07160](https://arxiv.org/abs/2402.07160)

    PASOA是一种新的贝叶斯实验设计程序，通过提供连续的后验分布的准确估计，同时执行顺序设计优化和参数推断。该方法使用 stochastic optimization 和 tempered SMC 来最大化期望信息增益，并提供了一致性的最优设计估计。

    

    我们提出了一种名为PASOA的新程序，用于贝叶斯实验设计，通过同时提供连续的后验分布的准确估计来执行顺序设计优化。顺序设计过程通过对比估计原则进行，使用随机优化和顺序蒙特卡罗（SMC）采样器来最大化期望信息增益（EIG）。由于连续后验分布之间的距离越大，获得的信息增益越大，因此这个EIG目标可能会恶化经典SMC的性能。为了解决这个问题，提出了温度调节，既可以获得大的信息增益，又可以获得准确的SMC采样，我们证明这对性能来说是至关重要的。这种随机优化和温度调节的新颖组合允许同时处理设计优化和参数推断。我们证明了所得到的最优设计估计量具有一致性。数值实验表明，我们的方法在相同计算预算下比其他方法更好地优化了设计。

    We propose a new procedure named PASOA, for Bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and Sequential Monte Carlo (SMC) samplers to maximise the Expected Information Gain (EIG). As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective may worsen classical SMC performance. To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling, that we show is crucial for performance. This novel combination of stochastic optimization and tempered SMC allows to jointly handle design optimization and parameter inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical
    
[^36]: 针对私有统计推断的重采样方法

    Resampling methods for Private Statistical Inference

    [https://arxiv.org/abs/2402.07131](https://arxiv.org/abs/2402.07131)

    这项研究提出了两种私有变体的非参数bootstrap方法，用于在差分隐私的情况下构建置信区间。方法在计算效率和置信区间长度上相比现有方法有显著改进。

    

    我们考虑使用差分隐私构建置信区间的任务。我们提出了两种私有变体的非参数bootstrap方法，该方法在数据的分区上私下计算多个“小”bootstrap的结果的中位数，并给出了得到的置信区间的渐进覆盖误差上界。对于固定的差分隐私参数ε，我们的方法在样本大小n上的误差率与非私有bootstrap相当，只是在对数因子内。我们使用真实数据和合成数据在均值估计、中位数估计和逻辑回归方面对我们的方法进行了经验验证。我们的方法在提供类似的覆盖精度的同时，比以前的方法提供了显著缩短（大约10倍）的置信区间。

    We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little'' bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.
    
[^37]: 索crates怀疑的回声：在校准的证据增强学习中接受不确定性

    Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning

    [https://arxiv.org/abs/2402.07107](https://arxiv.org/abs/2402.07107)

    这篇论文提出了一个新的统计方法，利用深度Q网络和分位数回归来在模型-free的分布式强化学习中引入不确定性，该方法通过结合深度证据学习和基于合规推理原则的分位数校准，提供了全局不确定性的显式、无样本计算，具有更高的计算和统计效率，并成功处理了超出分布范围的观测数据。

    

    我们提出了一种新颖的统计方法，用于在基于模型的分布强化学习中引入不确定性意识，涉及基于分位数回归的深度Q网络。提出的算法$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$旨在解决在随机环境中分别估计aleatoric和epistemic不确定性所面临的关键挑战。它将深度证据学习与基于合规推理原则的分位数校准相结合，提供了显式的、无样本计算的$\textit{全局}$不确定性，而不是基于简单方差的$\textit{局部}$估计，克服了传统方法在计算和统计效率以及处理超出分布范围的观测数据方面的局限性。在一套小型化的Atari游戏（即MinAtar）上进行测试，CEQR-DQN在得分和学习速度方面超过了类似的现有框架。它能够严谨地处理外部数据观测，并提供更高的计算和统计效率。

    We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
    
[^38]: ExGRG: 用于自监督表示学习的显式生成关系图

    ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning

    [https://arxiv.org/abs/2402.06737](https://arxiv.org/abs/2402.06737)

    本文介绍了一种新颖的自监督学习方法ExGRG，它通过显式生成关系图来解决图结构数据上的挑战，将先验领域知识和在线提取的信息纳入自监督学习中，取得了显著的成功。

    

    自监督学习（SSL）作为一种无需昂贵的标注标签而预训练深度学习模型的强大技术，通过利用未标记数据中的内嵌信号取得了显著的成功。然而，尽管SSL在计算机视觉任务中通过直观的数据增强展现了出色的性能，但其在图结构数据上的应用面临着挑战，因为图增强操作改变了语义并呈现出反直观的性质。针对这一限制，本文引入了一种新颖的非对比自监督学习方法，即显式生成关系图（ExGRG），以取代仅依靠传统的基于增强的隐式关系图。ExGRG提供了一个框架，可以将先验领域知识和在线提取的信息纳入自监督学习的不变性目标中，借鉴了拉普拉斯特征映射和期望最大化算法。通过将自监督学习与期望最大化算法结合，我们的E步骤涉及关系图的生成，以识别...

    Self-supervised Learning (SSL) has emerged as a powerful technique in pre-training deep learning models without relying on expensive annotated labels, instead leveraging embedded signals in unlabeled data. While SSL has shown remarkable success in computer vision tasks through intuitive data augmentation, its application to graph-structured data poses challenges due to the semantic-altering and counter-intuitive nature of graph augmentations. Addressing this limitation, this paper introduces a novel non-contrastive SSL approach to Explicitly Generate a compositional Relation Graph (ExGRG) instead of relying solely on the conventional augmentation-based implicit relation graph. ExGRG offers a framework for incorporating prior domain knowledge and online extracted information into the SSL invariance objective, drawing inspiration from the Laplacian Eigenmap and Expectation-Maximization (EM). Employing an EM perspective on SSL, our E-step involves relation graph generation to identify can
    
[^39]: 与语言模型的反馈循环推动上下文内奖励欺骗

    Feedback Loops With Language Models Drive In-Context Reward Hacking

    [https://arxiv.org/abs/2402.06627](https://arxiv.org/abs/2402.06627)

    与语言模型的反馈循环可能导致上下文内奖励欺骗（ICRH），即语言模型在测试时在优化目标的同时却产生负面副作用。这项研究确定了两个导致ICRH的过程：输出优化和策略优化。

    

    语言模型对外部世界产生影响：它们查询可以读写网页的API，生成能够影响人类行为的内容，以及作为自主代理运行系统命令。这些互动形成了反馈循环：语言模型的输出影响世界，反过来又影响后续的语言模型输出。在这项工作中，我们展示了反馈循环可能导致上下文内奖励欺骗(ICRH)，即测试时的语言模型在优化（可能隐含的）目标的同时，产生负面副作用。例如，考虑一个被部署用于增加Twitter参与度的语言模型代理；语言模型可能在上下文窗口中检索其以前的推文，并使推文更具争议性，从而增加参与度，但也增加了有毒性。我们确定并研究了导致ICRH的两个过程：输出优化和策略优化。对于这些过程，静态数据集上的评估是不足够的-他们无法捕捉到反馈效应，也不能捕捉到最有害的行为。为此，我们提供了...

    Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
    
[^40]: 通过精简形态条件超网络实现高效的通用形态控制

    Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control

    [https://arxiv.org/abs/2402.06570](https://arxiv.org/abs/2402.06570)

    本论文提出了一种名为HyperDistill的方法，通过精简形态条件超网络，可在训练和未知测试机器人上实现与通用的transformers策略相当的性能，同时大大减小模型尺寸和计算成本。

    

    在不同机器人形态之间学习一个通用策略可以显著提高学习效率，并实现对未知形态的零样本泛化。然而，学习一个高性能的通用策略需要像transformers（TF）这样具有较大内存和计算成本的复杂架构，而比较简单的多层感知器（MLP）则具有更高的效率。为了在推理时既能达到像TF一样好的性能，又能具有像MLP一样的高效率，我们提出了HyperDistill。它包括：（1）一个形态条件的超网络（HN），用于生成机器人特定的MLP策略，和（2）一个对于成功训练至关重要的策略蒸馏方法。我们展示了在UNIMAL上，一个包含数百种不同形态的基准测试中，HyperDistill在训练和未知测试机器人上都能和通用的TF教师策略一样表现出色，同时将模型尺寸减小了6-14倍，计算成本在不同环境下减小了67-160倍。

    Learning a universal policy across different robot morphologies can significantly improve learning efficiency and enable zero-shot generalization to unseen morphologies. However, learning a highly performant universal policy requires sophisticated architectures like transformers (TF) that have larger memory and computational cost than simpler multi-layer perceptrons (MLP). To achieve both good performance like TF and high efficiency like MLP at inference time, we propose HyperDistill, which consists of: (1) A morphology-conditioned hypernetwork (HN) that generates robot-wise MLP policies, and (2) A policy distillation approach that is essential for successful training. We show that on UNIMAL, a benchmark with hundreds of diverse morphologies, HyperDistill performs as well as a universal TF teacher policy on both training and unseen test robots, but reduces model size by 6-14 times, and computational cost by 67-160 times in different environments. Our analysis attributes the efficiency 
    
[^41]: 内省规划：引导语言驱动的代理机器人改进自身的不确定性

    Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty

    [https://arxiv.org/abs/2402.06529](https://arxiv.org/abs/2402.06529)

    本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。

    

    大型语言模型（LLM）展示了先进的推理能力，使得机器人能够理解自然语言指令，并通过适当的基础塑造来策略性地进行高级行动规划。然而，LLM产生的幻觉可能导致机器人自信地执行与用户目标不符或在极端情况下不安全的计划。此外，自然语言指令中的固有歧义可能引发任务的不确定性，尤其是在存在多个有效选项的情况下。为了解决这个问题，LLMs必须识别此类不确定性并主动寻求澄清。本文探索了内省规划的概念，作为一种系统方法，引导LLMs在无需微调的情况下形成意识到不确定性的机器人任务执行计划。我们研究了任务级机器人规划中的不确定性量化，并证明与最先进的基于LLM的规划方法相比，内省显著提高了成功率和安全性。

    Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
    
[^42]: 学习变得高效：在大型语言模型中构建结构化稀疏性

    Learn To be Efficient: Build Structured Sparsity in Large Language Models

    [https://arxiv.org/abs/2402.06126](https://arxiv.org/abs/2402.06126)

    本文通过引入一种新的算法"Learn-To-be-Efficient(LTE)"，提出了在大型语言模型(LLM)中构建结构化稀疏性的方法。该方法通过训练高效意识的LLM学习激活更少的神经元，取得更好的稀疏性和性能折衷。

    

    大型语言模型(LLM)以其十亿级参数取得了显著的成功，但它们产生了高昂的推理开销。在LLM中出现的激活稀疏性为通过仅涉及部分参数进行推理提供了一种自然的方法来减少这种成本。现有方法只关注利用这种自然形成的激活稀疏性，忽视了进一步放大这种固有稀疏性的潜力。本文中，我们假设LLM可以通过实现更结构化的激活稀疏性来学习高效。为实现这一目标，我们引入了一种新颖的算法"Learn-To-be-Efficient(LTE)", 旨在训练高效意识的LLM学习激活更少的神经元，并在稀疏性和性能之间取得更好的折衷。此外，与主要关注基于ReLU模型的SOTA MoEfication方法不同，LTE还可以应用于像GPT和LLaMA这样具有软激活函数的LLM。我们在四个模型和十一个数据集上评估了LTE。

    Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
    
[^43]: 一次梯度下降步骤后两层神经网络在特征学习中的渐近性质

    Asymptotics of feature learning in two-layer networks after one gradient-step

    [https://arxiv.org/abs/2402.04980](https://arxiv.org/abs/2402.04980)

    通过研究两层神经网络在一次梯度下降步骤后的特征学习，我们提供了在高维极限下通用化误差的精确渐近描述，并发现在适应数据的情况下，网络能够高效地学习梯度方向上的非线性函数。

    

    本文研究了两层神经网络在从数据中学习特征，并在使用单一梯度下降步骤训练后如何改进核心方法的问题。借助于（Ba et al., 2022）与非线性尖峰矩阵模型的关联以及对高斯泛化性的最新进展（Dandi et al., 2023），我们在样本数$n$、宽度$p$和输入维度$d$成比例增长的高维极限下，给出了一种精确的一致性误差描述。我们准确地刻画了适应数据对于网络在梯度方向上高效学习非线性函数的重要性——在初始化阶段，网络只能表达线性函数。据我们所知，我们的结果提供了在大学习率$\eta=\Theta_{d}(d)$的情况下特征学习对于两层神经网络泛化的首个准确描述，超越了核心方法。

    In this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate. We characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime. To our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\eta=\Theta_{d}(d)$, beyond
    
[^44]: QuIP#: 使用哈达玛德非相干性和格书进行更好的LLM量化

    QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks

    [https://arxiv.org/abs/2402.04396](https://arxiv.org/abs/2402.04396)

    QuIP#是一种使用哈达玛德非相干性和格书的权重量化方法，能够在极限压缩范围下达到最先进的结果，并具有快速推理的优势。

    

    后训练量化(PTQ)通过将LLM的权重量化为低精度来减少其内存占用。在这项工作中，我们引入了QuIP#，一种仅基于权重的PTQ方法，使用了三种新技术，在极限压缩范围($\le$ 4比特每个权重)上取得了最先进的结果。首先，QuIP#通过使用随机哈达玛德变换改进了QuIP中的非相干处理，该方法更快且具有更好的理论特性。其次，QuIP#使用向量量化技术利用了非相干权重具有的球形亚高斯分布特性：具体地说，我们引入了一组基于高度对称$E_8$格书的硬件高效代码书，实现了最优的8维单位球装填。第三，QuIP#使用微调来提高对原始模型的忠实度。我们的实验证明，QuIP#优于现有的PTQ方法，能够实现新的PTQ扩展行为，并支持快速推理。

    Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.
    
[^45]: 优先安全保障而非自治：科学中LLM智能机器人的风险

    Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science

    [https://arxiv.org/abs/2402.04247](https://arxiv.org/abs/2402.04247)

    本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。

    

    由大型语言模型（LLMs）驱动的智能机器人在各个学科中自主进行实验和促进科学发现方面展示了巨大的前景。尽管它们的能力非常有前途，但也引入了一些新的漏洞，需要仔细考虑安全性。然而，文献中存在显著的空白，尚未对这些漏洞进行全面探讨。本文通过对科学领域中基于LLM的机器人的漏洞进行深入研究，揭示了它们误用可能带来的潜在风险，并强调了对安全措施的需求，填补了这一空白。我们首先全面概述了科学LLM机器人固有的潜在风险，考虑了用户意图、特定的科学领域以及它们对外部环境可能造成的影响。然后，我们深入探讨了这些漏洞的起源和提供的解决方案。

    Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
    
[^46]: 我们能去掉自适应梯度方法中的平方根吗？一个二阶角度的研究

    Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective

    [https://arxiv.org/abs/2402.03496](https://arxiv.org/abs/2402.03496)

    移除自适应方法中的平方根可以在卷积结构上减小与SGD的泛化差距，同时保持在transformers上的性能。

    

    自适应梯度优化器如Adam(W)是许多深度学习结构（如transformers）的默认训练算法。它们的对角先验基于梯度外积，通过平方根加入到参数更新中。虽然这些方法通常被称为近似的二阶方法，但平方根表示了一个根本性的区别。在这项研究中，我们研究了在去掉平方根后自适应方法的行为如何变化，即加强它们的二阶动机。令人惊讶的是，我们发现这种去掉平方根的自适应方法能够在卷积结构上缩小与SGD的泛化差距，同时保持了在transformers上基于平方根的方法的性能。二阶角度对于开发具有非对角先验的自适应方法也具有实际好处。与像Shampoo这样基于平方根的对应方法不同，它们不需要数值不稳定的矩阵平方。

    Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square
    
[^47]: 计时器: 用于大规模时间序列分析的Transformer模型

    Timer: Transformers for Time Series Analysis at Scale

    [https://arxiv.org/abs/2402.02368](https://arxiv.org/abs/2402.02368)

    本文旨在早期开发大规模时间序列模型（LTSM），通过预训练和GPT风格架构，克服深度模型在小样本场景中的性能瓶颈，并实现在时间序列分析中的大样本泛化能力、可扩展性和任务普适性。

    

    深度学习在时间序列分析方面做出了显著贡献。然而，在现实世界的小样本场景中，深度模型可能遇到性能瓶颈，这可能由于当前基准测试中小模型的性能饱和而隐蔽。同时，通过大规模预训练，大模型在这些场景中展示了巨大的能力。随着大型语言模型的出现，取得了持续的进展，在少样本泛化能力、可扩展性和任务普适性方面展现了前所未有的能力，但这些能力在时间序列模型中不存在。为了改变目前在特定数据集上从头开始训练小模型的做法，本文旨在早期开发大规模时间序列模型（LTSM）。在预训练期间，我们策划了包含10亿个时间点的大规模数据集，将异构时间序列统一为单序列序列（S3）格式，并开发了面向LTSM的GPT风格架构。

    Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world small-sample scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progresses have been achieved as the emergence of large language models, exhibiting unprecedented ability in few-shot generalization, scalability, and task generality, which is however absent in time series models. To change the current practices of training small models on specific datasets from scratch, this paper aims at an early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet 
    
[^48]: 基于神经网络的数据驱动算法设计及其在分支定界中的应用

    Data-driven algorithm design using neural networks with applications to branch-and-cut

    [https://arxiv.org/abs/2402.02328](https://arxiv.org/abs/2402.02328)

    本文介绍了一种基于神经网络的数据驱动算法设计方法，并将其应用于分支定界框架中，用于解决混合整数优化问题。

    

    数据驱动算法设计是一种使用统计和机器学习技术从一类算法中选择在某个（未知）问题实例分布上表现最佳的算法的范例。本文在这一研究领域的最新工作的基础上引入了一个新的思路，即不仅仅选择一个具有最佳性能的单一算法，而是允许根据问题实例选择算法。具体而言，给定一组代表性的问题实例样本，我们学习一个神经网络，将问题实例映射到最合适的算法。我们将这一思路形式化，并根据最近的数据驱动算法设计的工作，推导出了这个学习问题的严格样本复杂度界限。然后，我们将这种方法应用到混合整数优化的分支定界框架中，以做出良好的决策。

    Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem. We build upon recent work in this line of research by introducing the idea where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved. In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm {\em for that instance}. We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design. We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization 
    
[^49]: 动态增量优化用于最佳子集选择

    Dynamic Incremental Optimization for Best Subset Selection

    [https://arxiv.org/abs/2402.02322](https://arxiv.org/abs/2402.02322)

    本文研究了一类$\ell_0$正则化问题的对偶形式，并提出了一种高效的原对偶算法，通过充分利用对偶范围估计和增量策略，提高了最佳子集选择问题的解决方案的效率和统计性质。

    

    最佳子集选择被认为是稀疏学习问题的“黄金标准”。已经提出了各种优化技术来攻击这个非光滑非凸问题。本文研究了一类$\ell_0$正则化问题的对偶形式。基于原始问题和对偶问题的结构，我们提出了一种高效的原对偶算法。通过充分利用对偶范围估计和增量策略，我们的算法潜在地减少了冗余计算并改进了最佳子集选择的解决方案。理论分析和对合成和真实数据集的实验验证了所提出解决方案的效率和统计性质。

    Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.
    
[^50]: 训练PINNs的挑战：从损失函数空间角度探究

    Challenges in Training PINNs: A Loss Landscape Perspective

    [https://arxiv.org/abs/2402.01868](https://arxiv.org/abs/2402.01868)

    本文探讨了训练PINNs的挑战，强调了损失函数空间在训练过程中的作用，引入了新颖的二阶优化器NNCG并优化了PINN性能，为训练PINNs提供了有价值的洞见和更强大的优化策略。

    

    本文通过研究物理信息神经网络（PINNs）的训练挑战，强调了损失函数空间在训练过程中的作用。我们分析了在最小化PINN损失函数方面的困难，特别是由于残差项中的微分算子引起的病态条件。我们比较了基于梯度的优化器Adam、L-BFGS以及它们的组合Adam+L-BFGS的性能，表明Adam+L-BFGS更优，并介绍了一种新颖的二阶优化器NysNewton-CG（NNCG），显著提高了PINN的性能。从理论上，我们阐明了病态微分算子与PINN损失中的病态条件之间的联系，并展示了结合一阶和二阶优化方法的好处。我们的工作为训练PINNs提供了有价值的洞见和更强大的优化策略，可以提高PINNs在解决困难的偏微分方程中的实用性。

    This paper explores challenges in training Physics-Informed Neural Networks (PINNs), emphasizing the role of the loss landscape in the training process. We examine difficulties in minimizing the PINN loss function, particularly due to ill-conditioning caused by differential operators in the residual term. We compare gradient-based optimizers Adam, L-BFGS, and their combination Adam+L-BFGS, showing the superiority of Adam+L-BFGS, and introduce a novel second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN performance. Theoretically, our work elucidates the connection between ill-conditioned differential operators and ill-conditioning in the PINN loss and shows the benefits of combining first- and second-order optimization methods. Our work presents valuable insights and more powerful optimization strategies for training PINNs, which could improve the utility of PINNs for solving difficult partial differential equations.
    
[^51]: 不确定性感知的部分标签学习

    Uncertainty-Aware Partial-Label Learning

    [https://arxiv.org/abs/2402.00592](https://arxiv.org/abs/2402.00592)

    本文提出了一种基于最近邻的部分标签学习算法，利用Dempster-Shafer理论实现对模糊标记的数据的训练。实验结果表明，该算法能够提供良好的不确定性估计，并具有竞争力的预测性能。

    

    在现实世界的应用中，人们经常遇到标记模糊的数据，即不同的标注者为相同样本分配了冲突的类别标签。部分标签学习允许在这种弱监督的情况下训练分类器。虽然最先进的方法已经具有良好的预测性能，但它们往往受到错误的不确定性估计的影响。然而，在医学和自动驾驶等安全关键领域，具有良好校准的不确定性估计尤为重要。在本文中，我们提出了一种基于最近邻的部分标签学习算法，该算法利用了Dempster-Shafer理论。对人工数据集和实际数据集进行的广泛实验表明，所提出的方法能够提供良好的不确定性估计，并具有竞争力的预测性能。此外，我们还证明了我们的算法具有风险一致性。

    In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.
    
[^52]: 动量粒子最大似然

    Momentum Particle Maximum Likelihood

    [https://arxiv.org/abs/2312.07335](https://arxiv.org/abs/2312.07335)

    该论文提出了一种动态系统方法，用于在参数和概率分布的扩展空间上最小化自由能函数，该方法融合了Nesterov的加速梯度方法、欠阻尼Langevin扩散和p。

    

    最大似然估计(MLE)的潜变量模型通常被重新解释为在参数和概率分布的扩展空间上的优化问题。例如，期望最大化(EM)算法可以解释为在这个空间上适用于合适的自由能函数的坐标下降。最近，这个观点与从最优传输和Wasserstein梯度流中获得的启示相结合，发展出了适用于更广泛模型类的基于粒子的算法，而不是标准的EM。受先前论文的启发，将将动量丰富的优化算法解释为常微分方程的离散化，我们提出了一种类似的动态系统方法，用于最小化在参数和概率分布的扩展空间上的自由能函数。结果是一个动态系统，结合了Nesterov的加速梯度方法、欠阻尼Langevin扩散和p。

    Maximum likelihood estimation (MLE) of latent variable models is often recast as an optimization problem over the extended space of parameters and probability distributions. For example, the Expectation Maximization (EM) algorithm can be interpreted as coordinate descent applied to a suitable free energy functional over this space. Recently, this perspective has been combined with insights from optimal transport and Wasserstein gradient flows to develop particle-based algorithms applicable to wider classes of models than standard EM.   Drawing inspiration from prior works which interpret `momentum-enriched' optimisation algorithms as discretizations of ordinary differential equations, we propose an analogous dynamical systems-inspired approach to minimizing the free energy functional over the extended space of parameters and probability distributions. The result is a dynamic system that blends elements of Nesterov's Accelerated Gradient method, the underdamped Langevin diffusion, and p
    
[^53]: 强化关注力中最短的支柱：增强大型语言模型的上下文意识，以实现有效的工具使用

    Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use

    [https://arxiv.org/abs/2312.04455](https://arxiv.org/abs/2312.04455)

    本文证明了大型语言模型中关注分配的波形模式对其在需要高度上下文意识的任务中的性能有显著影响。我们提出了一种名为“Attention Buckets”的推理方法，通过多个并行过程和不同的旋转位置嵌入角度，增强了模型对不同上下文位置的意识，从而减轻了忽视关键信息的风险。

    

    在本文中，我们证明了大型语言模型(LLMs)中关注分配中的内在波形模式显著影响它们在需要高度上下文意识的任务中的性能，例如利用LLMs进行工具使用。具体而言，当关键信息在上下文中位于关注波形的低谷区域时，模型可能会忽视该信息，导致性能下降。为了解决这个问题，我们提出了一种名为“Attention Buckets”的新型推理方法。它允许LLMs通过多个并行过程处理输入。每个过程使用不同的基准角度进行旋转位置嵌入，从而创建出一个独特的关注波形。通过用一个过程的关注低谷补偿另一个过程的关注高峰，我们的方法增强了LLM对不同上下文位置的意识，从而减轻了忽视关键信息的风险。

    In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
    
[^54]: 量子启蒙分数

    Quantum Inception Score

    [https://arxiv.org/abs/2311.12163](https://arxiv.org/abs/2311.12163)

    通过量子启蒙分数，我们提出了一个用于评估量子生成模型质量的新指标，证明量子生成模型在质量上优于经典生成模型，并利用量子波动定理揭示了其物理限制。

    

    受到经典生成模型在机器学习中取得巨大成功的启发，近期开始了对它们量子版本的热切探索。为了开始这一探索之旅，开发一个相关的度量标准来评估量子生成模型的质量是很重要的；在经典情况下，一个这样的例子便是启蒙分数。在本文中，我们提出了量子启蒙分数，它将质量与用于对给定数据集进行分类的量子通道的Holevo信息联系起来。我们证明，在这个提出的度量标准下，量子生成模型提供比它们的经典对应物更好的质量，因为存在着由不对称性的资源理论和纠缠所表征的量子相干性。此外，我们利用量子波动定理来表征限制量子生成模型质量的物理限制。最后，我们应用量子启蒙分数来

    arXiv:2311.12163v2 Announce Type: replace-cross  Abstract: Motivated by the great success of classical generative models in machine learning, enthusiastic exploration of their quantum version has recently started. To depart on this journey, it is important to develop a relevant metric to evaluate the quality of quantum generative models; in the classical case, one such example is the inception score. In this paper, we propose the quantum inception score, which relates the quality to the Holevo information of the quantum channel that classifies a given dataset. We prove that, under this proposed measure, the quantum generative models provide better quality than their classical counterparts because of the presence of quantum coherence, characterized by the resource theory of asymmetry, and entanglement. Furthermore, we harness the quantum fluctuation theorem to characterize the physical limitation of the quality of quantum generative models. Finally, we apply the quantum inception score 
    
[^55]: 通过最优传输实现公平的核心集

    Fair Coresets via Optimal Transport

    [https://arxiv.org/abs/2311.05436](https://arxiv.org/abs/2311.05436)

    本研究提出了公平的Wasserstein核心集(FWC)，该方法通过最小化原始数据集与加权合成样本之间的Wasserstein距离，并强制实现人口平等，生成公平的合成代表性样本，可用于下游学习任务。

    

    数据精炼和核心集已成为生成用于处理大规模数据集的下游学习任务的较小代表性样本集的流行方法。与此同时，机器学习越来越多地应用于社会层面的决策过程，使得模型构建者必须解决存在于数据中的子群体的固有偏见问题。当前方法通过优化相对于原始样本的局部属性来创建公平的合成代表性样本，但其对下游学习过程的影响尚未被探索。在这项工作中，我们提出了公平的Wasserstein核心集（FWC），一种新颖的核心集方法，它生成既具有公平性的合成代表性样本，又具有用于下游学习任务的样本级权重。FWC最小化原始数据集与加权合成样本之间的Wasserstein距离，同时强制实现人口平等。我们展示了FWC的无约束版本等价于通常的最优传输问题，并且通过实验证明了FWC的有效性和公平性。

    Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. Current approaches create fair synthetic representative samples by optimizing local properties relative to the original samples, but their effect on downstream learning processes has yet to be explored. In this work, we present fair Wasserstein coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC minimizes the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of FWC is equiv
    
[^56]: 动态生存分析与控制潜在状态

    Dynamical Survival Analysis with Controlled Latent States. (arXiv:2401.17077v1 [stat.ML])

    [http://arxiv.org/abs/2401.17077](http://arxiv.org/abs/2401.17077)

    本论文提出了一种动态生存分析方法，通过控制潜在状态来学习个体特定的计数过程强度。研究者设计了一个神经控制微分方程模型，并证明了在足够正则条件下，可以在签名空间中线性化模型，得到一种基于签名的估计器。通过对金融、预测性维护和食品供应链管理等数据集的实验，验证了模型的性能。

    

    我们考虑从一组静态变量和不规则采样的时间序列中学习个体特定的计数过程强度的任务。我们引入一种新颖的建模方法，其中强度是控制微分方程的解。首先，我们通过构建神经控制微分方程来设计一个神经估计器。然后，我们证明在足够正则条件下，我们的模型可以在签名空间中线性化，得到一种基于签名的估计器，我们称之为CoxSig。我们为这两种估计器提供理论学习保证，并展示了我们的模型在金融、预测性维护和食品供应链管理等各种模拟和真实数据集上的性能。

    We consider the task of learning individual-specific intensities of counting processes from a set of static variables and irregularly sampled time series. We introduce a novel modelization approach in which the intensity is the solution to a controlled differential equation. We first design a neural estimator by building on neural controlled differential equations. In a second time, we show that our model can be linearized in the signature space under sufficient regularity conditions, yielding a signature-based estimator which we call CoxSig. We provide theoretical learning guarantees for both estimators, before showcasing the performance of our models on a vast array of simulated and real-world datasets from finance, predictive maintenance and food supply chain management.
    
[^57]: 神经符号推理和学习的凸二级优化研究

    Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning. (arXiv:2401.09651v1 [cs.LG])

    [http://arxiv.org/abs/2401.09651](http://arxiv.org/abs/2401.09651)

    本研究通过凸二级优化技术，开发了一个通用的基于梯度的神经和符号参数学习框架，具有100倍以上的学习时间改进和高达16%的预测性能提升。

    

    通过利用凸二级优化技术，我们解决了神经符号系统的一个关键挑战，开发了一个通用的基于梯度的端到端神经和符号参数学习框架。我们利用最先进的神经符号体系结构NeuPSL来证明我们的框架的适用性。为了实现这一目标，我们提出了NeuPSL推理的平滑原始和对偶形式，并显示学习梯度是最优对偶变量的函数。此外，我们为新的形式开发了一种对偶块坐标下降算法，自然地利用了热启动。这使得我们相比当前最好的NeuPSL推理方法的学习时间改进了100倍以上。最后，我们对涵盖各种任务的8个数据集进行了广泛的实证评估，并证明我们的学习框架相比替代学习方法能够提升高达16%的预测性能。

    We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.
    
[^58]: 理解图神经网络的异质性

    Understanding Heterophily for Graph Neural Networks. (arXiv:2401.09125v1 [cs.LG])

    [http://arxiv.org/abs/2401.09125](http://arxiv.org/abs/2401.09125)

    本文通过提出的异质性随机块模型（HSBM）来提供对不同异质性模式对图神经网络（GNNs）影响的理论理解。研究发现，异质性对分类的影响需要与平均节点度一起评估，并且拓扑噪声对分类有负面影响。

    

    具有异质性的图被认为是图神经网络（GNNs）面临挑战的情景，其中节点通过各种模式与不同的邻居相连接。本文通过将图卷积（GC）操作合并到完全连接的网络中，通过提出的异质性随机块模型（HSBM）来提供对不同异质性模式对GNNs影响的理论理解，HSBM是一个可以容纳多样的异质性模式的通用随机图模型。首先，我们展示了通过应用GC操作，可分性增益取决于两个因素，即邻域分布的欧氏距离和$\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$，其中$\mathbb{E}\left[\operatorname{deg}\right]$是平均节点度。它揭示了异质性对分类的影响需要与平均节点度一起评估。其次，我们展示了拓扑噪声具有负面影响

    Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of the impacts of different heterophily patterns for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Firstly, we show that by applying a GC operation, the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$, where $\mathbb{E}\left[\operatorname{deg}\right]$ is the averaged node degree. It reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. Secondly, we show that the topological noise has a detrimenta
    
[^59]: DiarizationLM: 基于大语言模型的说话人分离后处理

    DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2401.03506](http://arxiv.org/abs/2401.03506)

    本文介绍了DiarizationLM框架，利用大语言模型对说话人分离系统的输出进行后处理。实验证明，使用finetuned的PaLM 2-S模型可以显著减少分离错误率，对多种目标都有优化效果。

    

    本文介绍了DiarizationLM，一个利用大语言模型（LLM）对说话人分离系统输出进行后处理的框架。这个框架可以实现多种目标，如改善分离对话转录的可读性，或减少词级分离错误率（WDER）。在这个框架中，自动语音识别（ASR）和说话人分离系统的输出被表示为一种紧凑的文本格式，其包含在一个可选择调整的LLM的提示中。LLM的输出可以作为所需改进的精细化分离结果。作为后处理步骤，该框架可以轻松应用于任何现有的ASR和说话人分离系统，无需重新训练现有的组件。我们的实验证明，finetuned的PaLM 2-S模型可以在Fisher电话对话数据集上将WDER降低55.5%，在Callhome英语数据集上降低44.9%。

    In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.
    
[^60]: 基于脑启发的脉冲神经网络在工业故障诊断中的应用：调查、挑战和机遇

    Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities. (arXiv:2401.02429v1 [cs.NE])

    [http://arxiv.org/abs/2401.02429](http://arxiv.org/abs/2401.02429)

    基于脑启发的脉冲神经网络是一种有前景的应用于工业故障诊断的替代方法，可以克服人工神经网络的限制，提供更精确和有效的故障识别。

    

    近几十年来，工业故障诊断（IFD）作为一门关注检测和收集工业设备健康状况重要信息的学科而出现，从而促进了对故障类型和严重程度的识别。精确和有效的故障识别引起了广泛关注，导致对自动化设备监测的关注，以避免安全事故并减少对人力的依赖。人工神经网络（ANNs）的出现在增强智能IFD算法方面起到了重要作用，特别是在大数据背景下。尽管取得了这些进展，作为一种简化的仿生神经网络模型，ANNs存在固有的限制，如资源和数据依赖性以及受限的认知能力。为了解决这些限制，基于脑启发计算原理的第三代脉冲神经网络（SNN）已经成为一种有前景的替代方法。

    In recent decades, Industrial Fault Diagnosis (IFD) has emerged as a crucial discipline concerned with detecting and gathering vital information about industrial equipment's health condition, thereby facilitating the identification of failure types and severities. The pursuit of precise and effective fault recognition has garnered substantial attention, culminating in a focus on automating equipment monitoring to preclude safety accidents and reduce reliance on human labor. The advent of artificial neural networks (ANNs) has been instrumental in augmenting intelligent IFD algorithms, particularly in the context of big data. Despite these advancements, ANNs, being a simplified biomimetic neural network model, exhibit inherent limitations such as resource and data dependencies and restricted cognitive capabilities. To address these limitations, the third-generation Spiking Neural Network (SNN), founded on principles of Brain-inspired computing, has surfaced as a promising alternative. Th
    
[^61]: 欺骗的艺术：使用动态触发器的强健后门攻击

    The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])

    [http://arxiv.org/abs/2401.01537](http://arxiv.org/abs/2401.01537)

    这项研究介绍了一种使用动态触发器进行强健后门攻击的方法，通过巧妙设计的调整，使损坏的样本与干净的样本无法区分，实验证明这种方法可以成功地欺骗语音识别系统。

    

    由于人工智能行业的最新进展，机器学习作为服务（MLaaS）领域正在经历增长的实施。然而，这种增长引发了对AI防御机制的担忧，特别是对于来自不完全可信的第三方提供商的潜在隐蔽攻击。最近的研究发现，听觉后门可能使用某些修改作为其启动机制。DynamicTrigger作为一种方法被引入，用于进行使用巧妙设计的调整来确保损坏的样本与干净的样本无法区分的动态后门攻击。通过利用波动的信号采样率，并通过动态声音触发器（比如拍手声）对说话者身份进行掩盖，可以欺骗语音识别系统（ASR）。我们的实证测试表明，DynamicTrigger在隐蔽攻击中既有效又隐蔽，并在攻击过程中取得了令人印象深刻的成功率。

    The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
    
[^62]: 分数梯度下降法的收敛性分析

    Convergence Analysis of Fractional Gradient Descent. (arXiv:2311.18426v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2311.18426](http://arxiv.org/abs/2311.18426)

    本论文通过分析不同环境下的分数梯度下降方法，建立了分数导数与整数导数之间的新界限，并证明了在平滑且凸、平滑且强凸以及平滑且非凸环境下的收敛性，为分数梯度下降的收敛性分析提供了新的理论支持。

    

    分数导数是整数阶导数的一种广义推广。对于优化问题，研究使用分数导数的梯度下降方法的收敛性是非常有意义的。目前，对于分数梯度下降的收敛性分析在研究方法和研究环境方面都存在限制。本文旨在填补这些空白，分析了平滑且凸、平滑且强凸以及平滑且非凸环境下的分数梯度下降的变种。首先，我们将建立将分数导数与整数导数联系起来的新界限。然后，利用这些界限证明了对于平滑且强凸函数的线性收敛性和对于平滑且凸函数的O(1/T)收敛性。此外，通过使用一种更适合分数导数的扩展平滑度概念-Holder平滑度，我们还证明了对于平滑且非凸函数的O(1/T)收敛性。

    Fractional derivatives are a well-studied generalization of integer order derivatives. Naturally, for optimization, it is of interest to understand the convergence properties of gradient descent using fractional derivatives. Convergence analysis of fractional gradient descent is currently limited both in the methods analyzed and the settings analyzed. This paper aims to fill in these gaps by analyzing variations of fractional gradient descent in smooth and convex, smooth and strongly convex, and smooth and non-convex settings. First, novel bounds will be established bridging fractional and integer derivatives. Then, these bounds will be applied to the aforementioned settings to prove linear convergence for smooth and strongly convex functions and $O(1/T)$ convergence for smooth and convex functions. Additionally, we prove $O(1/T)$ convergence for smooth and non-convex functions using an extended notion of smoothness - H\"older smoothness - that is more natural for fractional derivative
    
[^63]: 通过整数规划对温顺函数进行分段多项式回归

    Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2311.13544](http://arxiv.org/abs/2311.13544)

    本论文提出了使用整数规划对温顺函数进行分段多项式回归的方法，这可以用于估计包含在许多应用中的温顺函数，并且展示了令人期待的计算结果。

    

    我们考虑估计属于一类特定的非光滑函数的函数的任务，即所谓的温顺函数。这些函数出现在各种应用中：深度学习的训练、混合整数规划的价值函数或小分子的波函数。我们展示了温顺函数在任何完全维度的立方体上可用分段多项式来逼近。然后我们提出了第一个分段多项式回归的混合整数规划形式。这些方法可用于估计温顺函数。我们展示了令人期待的计算结果。

    We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.
    
[^64]: LLMs无法找到推理错误，但可以纠正它们！（arXiv：2311.08516v2 [cs.AI] UPDATED）

    LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.08516](http://arxiv.org/abs/2311.08516)

    本文研究了LLMs在自我纠正过程中的错误发现和输出纠正两个核心组成部分。研究发现LLMs通常难以发现逻辑错误，但通过使用回溯方法可以在提供错误位置信息时获得大幅改进。

    

    尽管自我纠正在改善LLM输出的风格和质量方面显示出了潜力（例如Chen等，2023；Madaan等，2023），最近对逻辑或推理错误进行自我纠正的尝试通常会导致正确答案变为错误，从而总体表现变差（Huang等，2023）。在本文中，我们将自我纠正过程分解为两个核心组成部分：错误发现和输出纠正。对于错误发现，我们发布了BIG-Bench Mistake，这是一个Chain-of-Thought推理轨迹中的逻辑错误数据集。我们为几种最先进的LLM提供基准数，并证明LLM通常难以发现逻辑错误。对于输出纠正，我们提出了一种回溯方法，在提供错误位置信息时可以大幅改进。我们将回溯解释为对强化学习方法的轻量级替代方案，并展示了在60-70％准确率下保持有效性的奖励模型。

    While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70% accuracy.
    
[^65]: VQPy：一种面向现代视频分析的面向对象方法。

    VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])

    [http://arxiv.org/abs/2311.01623](http://arxiv.org/abs/2311.01623)

    VQPy是一种面向对象的视频分析方法，它使用Python变体作为前端，并具有可扩展的后端，可以自动构建和优化基于视频对象的处理流程。

    

    视频分析广泛应用于当今系统和服务中。在视频分析的前沿是用户开发的视频查询，以找到特定感兴趣的对象。基于视频对象（例如人，动物，汽车等）与传统面向对象语言建模的对象相似的洞察力，我们提出了一种面向视频分析的面向对象方法。这种方法名为VQPy，包括一个前端（一种Python变体，其中包含用户可以表达视频对象及其交互的结构）和一个可扩展的后端，可以基于视频对象自动生成和优化管道。我们已经实施和开源了VQPy，它已经作为Cisco DeepVision框架的一部分产品化。

    Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
    
[^66]: 多注释者数据的损失建模

    Loss Modeling for Multi-Annotator Datasets. (arXiv:2311.00619v1 [cs.LG])

    [http://arxiv.org/abs/2311.00619](http://arxiv.org/abs/2311.00619)

    该论文提出了一种通过利用多任务学习和基于损失的标签修正来学习多注释者数据的准确表示的方法。通过这种方法，可以有效地分离赞同和不赞同的注释，并且在单一或多注释者设置下改善预测性能。该方法还显示出对主观数据的额外标签噪声具有鲁棒性。

    

    在公正性方面，考虑到数据集中所有注释者的意见至关重要。然而，在注释大型数据集时，个别注释者经常会提供数千个评分，这可能导致疲劳。此外，这些注释过程可能会持续多天，可能导致对注释者的意见随时间的不准确表示。为了解决这个问题，我们提出利用多任务学习和基于损失的标签修正来学习更准确的多样意见表示。我们展示了使用我们新颖的公式，我们可以清楚地分离赞同和不赞同的注释。此外，我们证明了这种修改可以改善单一或多注释者设置下的预测性能。最后，我们证明了该方法对应用于主观数据的额外标签噪声仍然具有稳健性。

    Accounting for the opinions of all annotators of a dataset is critical for fairness. However, when annotating large datasets, individual annotators will frequently provide thousands of ratings which can lead to fatigue. Additionally, these annotation processes can occur over multiple days which can lead to an inaccurate representation of an annotator's opinion over time. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, we demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.
    
[^67]: 通过变分界限实现实用的非对抗分布对齐

    Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds. (arXiv:2310.19690v1 [cs.LG])

    [http://arxiv.org/abs/2310.19690](http://arxiv.org/abs/2310.19690)

    本论文提出了一种非对抗的基于变分自动编码器的对齐方法，通过引入一组对齐上界，解决了先前方法中存在的不稳定性和限制。实验证明，这种新颖的对齐损失可以在不改变原始架构的情况下取代对抗损失，扩展了应用范围。

    

    分布对齐可用于学习具有公平性和鲁棒性应用的不变表示。大多数先前的工作都采用对抗对齐方法，但由此产生的极小极大问题不稳定且难以优化。非对抗的基于似然的方法要么需要模型可逆性，要么对潜在先验施加约束，要么缺乏通用的对齐框架。为了克服这些限制，我们提出了一种非对抗的基于变分自动编码器的对齐方法，可应用于任何模型管道。我们开发了一组对齐上界（包括一个含噪音的上界），其具有类似变分自动编码器的目标但具有不同的视角。我们在理论上和实证上仔细比较了我们的方法与先前的基于变分自动编码器的对齐方法。最后，我们证明我们的新颖对齐损失可以在标准的不变表示学习管道中取代对抗损失，而无需修改原始架构，从而显著拓展了应用范围。

    Distribution alignment can be used to learn invariant representations with applications in fairness and robustness. Most prior works resort to adversarial alignment methods but the resulting minimax problems are unstable and challenging to optimize. Non-adversarial likelihood-based approaches either require model invertibility, impose constraints on the latent prior, or lack a generic framework for alignment. To overcome these limitations, we propose a non-adversarial VAE-based alignment method that can be applied to any model pipeline. We develop a set of alignment upper bounds (including a noisy bound) that have VAE-like objectives but with a different perspective. We carefully compare our method to prior VAE-based alignment approaches both theoretically and empirically. Finally, we demonstrate that our novel alignment losses can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures -- thereby significantly bro
    
[^68]: 受控解码来自语言模型

    Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])

    [http://arxiv.org/abs/2310.17022](http://arxiv.org/abs/2310.17022)

    本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。

    

    我们提出了一种新颖的离策略强化学习方法，称为受控解码（CD），用于控制自回归语言模型的生成，以获得高回报的结果。CD通过值函数来解决离策略强化学习问题，该值函数被称为前缀评分器。前缀评分器在推理时用于引导生成向更高回报的结果。我们展示了前缀评分器可以从（可能是）离策略数据中训练出来，用于预测从部分解码的响应继续解码时的预期回报。我们在Reddit对话语料库上经验证明，CD作为一种控制机制是有效的。我们还展示了CD设计的模块化使其能够有效解决多目标强化学习问题，而不会增加任何复杂性。最后，我们展示了CD可以以一种新颖的分块方式在推理时应用，同样无需任何额外的操作。

    We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
    
[^69]: OODRobustBench: 在分布迁移下评估和分析对抗性鲁棒性的基准

    OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift. (arXiv:2310.12793v1 [cs.LG])

    [http://arxiv.org/abs/2310.12793](http://arxiv.org/abs/2310.12793)

    OODRobustBench是一个基准，用于评估和分析在分布迁移下的对抗性鲁棒性。大规模分析结果表明，对抗性鲁棒性在离群分布测试下存在严重的泛化问题，而内分布鲁棒性与离群分布鲁棒性呈强正线性相关。

    

    现有的研究在提高对抗性鲁棒性方面取得了很大进展，但通常只在与训练数据相同分布的数据上进行测试，即内分布（ID）测试。因此，目前尚不清楚这种鲁棒性在输入分布迁移，即离群分布（OOD）测试下的泛化性能。在实际部署时，由于这种分布迁移是不可避免的，这一问题十分令人担忧。为了解决这个问题，我们提出了一个名为OODRobustBench的基准，通过使用23个基于数据集的迁移（即输入分布的自然迁移）和6个基于威胁的迁移（即未知的对抗性威胁模型）来全面评估OOD的对抗性鲁棒性。OODRobustBench用于评估了706个鲁棒模型，并进行了60.7K次对抗性评估。这个大规模分析表明：1）对抗性鲁棒性存在严重的OOD泛化问题；2）ID鲁棒性与OOD鲁棒性呈强正线性相关。

    Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear wa
    
[^70]: 具有不确定性量化预测的在线算法

    Online Algorithms with Uncertainty-Quantified Predictions. (arXiv:2310.11558v1 [cs.LG])

    [http://arxiv.org/abs/2310.11558](http://arxiv.org/abs/2310.11558)

    这篇论文研究了如何在设计在线算法时最佳利用不确定性量化预测，并提出了一种考虑预测概率性的在线算法设计方法。

    

    具有预测的在线算法已成为算法的超越最坏情况分析领域的热门话题。这些算法利用对未来的预测来获得高质量的性能保证，当预测良好时，同时在预测任意差的情况下仍保持界限最坏情况保证。一般来说，算法被认为对预测质量不知情。然而，机器学习文献中的最新发展已研究了提供对机器学习预测进行的不确定性量化的技术，即描述模型对其质量的确定程度。本文考察了如何在在线算法设计中最佳利用不确定性量化预测的问题。具体而言，我们考虑了用描述基本事实落在某个范围内的不确定性量化增强预测的情况，并设计了具有这些概率性的在线算法。

    Online algorithms with predictions have become a trending topic in the field of beyond worst-case analysis of algorithms. These algorithms incorporate predictions about the future to obtain performance guarantees that are of high quality when the predictions are good, while still maintaining bounded worst-case guarantees when predictions are arbitrarily poor. In general, the algorithm is assumed to be unaware of the prediction's quality. However, recent developments in the machine learning literature have studied techniques for providing uncertainty quantification on machine-learned predictions, which describes how certain a model is about its quality. This paper examines the question of how to optimally utilize uncertainty-quantified predictions in the design of online algorithms. In particular, we consider predictions augmented with uncertainty quantification describing the likelihood of the ground truth falling in a certain range, designing online algorithms with these probabilistic
    
[^71]: In-Context Unlearning: 基于少样本学习的语言模型的消除研究

    In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.07579](http://arxiv.org/abs/2310.07579)

    这项工作提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。这种方法解决了消除对于很大模型来说在计算上的困难，并在实践中具有更高的可行性和便捷性。

    

    机器消除学习是研究如何高效地去除特定训练数据对训练模型的影响，近来引起了更多的关注，主要是由于需要遵守诸如被遗忘权等隐私法规的需求。尽管在版权问题上LLM（语言模型）尤其相关，但在非常大的模型上实现精确消除是计算上不可行的。为此，最近的研究提出了几种算法，可以在不重新训练模型的情况下近似消除训练数据。这些算法关键依赖于对模型参数的访问来更新它们，但在实践中可能由于计算约束或通过API访问LLM而无法满足这种假设。在这项工作中，我们提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。为了消除特定的训练实例，我们提供了i

    Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i
    
[^72]: 深度ReLU网络和高阶有限元方法II：切比雪夫模拟

    Deep ReLU networks and high-order finite element methods II: Chebyshev emulation. (arXiv:2310.07261v1 [math.NA])

    [http://arxiv.org/abs/2310.07261](http://arxiv.org/abs/2310.07261)

    本论文研究了在有限分割上使用深度ReLU神经网络对连续分段多项式函数的表达速率和稳定性，提出了一种使用切比雪夫多项式展开系数进行编码的新颖ReLU NN替代模型构造，与基于ReLU NN模拟多项式的构造相比，在表达速率和稳定性方面获得了更好的界限。

    

    研究了在有界区间$(a,b)$上的任意有限分割$\mathcal{T}$上，连续的、分段多项式函数的深度ReLU神经网络（NNs）在定义NN的参数数量方面对Sobolev范数的表达速率和稳定性。开发了新颖的ReLU NN替代模型构造，使用切比雪夫多项式展开系数对近似函数进行编码。可以通过将函数在Clenshaw-Curtis点上的值使用快速傅里叶逆变换轻松地计算出切比雪夫系数。在表达速率和稳定性方面获得了优于基于ReLU NN模拟多项式的构造[Opschoor，Petersen，Schwab，2020]的界限。所有模拟上界都明确地与区间的（任意）分割、目标模拟精度和分割中每个元素的多项式次数有关。提供了ReLU NN模拟误差估计，适用于各种情况。

    Expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) in terms of the number of parameters defining the NN for continuous, piecewise polynomial functions, on arbitrary, finite partitions $\mathcal{T}$ of a bounded interval $(a,b)$ are addressed. Novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients are developed. Chebyshev coefficients can be computed easily from the values of the function in the Clenshaw--Curtis points using the inverse fast Fourier transform. Bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials considered in [Opschoor, Petersen, Schwab, 2020] are obtained. All emulation bounds are explicit in terms of the (arbitrary) partition of the interval, the target emulation accuracy and the polynomial degree in each element of the partition. ReLU NN emulation error estimates are provided for variou
    
[^73]: 对组合多臂老虎机的对抗攻击

    Adversarial Attacks on Combinatorial Multi-Armed Bandits. (arXiv:2310.05308v1 [cs.LG])

    [http://arxiv.org/abs/2310.05308](http://arxiv.org/abs/2310.05308)

    本文研究了对组合多臂老虎机的奖励污染攻击，并给出了攻击可能性的条件。与以往对多臂老虎机的理解相反，我们发现特定CMAB实例的攻击可能性还取决于发哥实例是否被对手知晓。这表明在实践中对CMAB进行对抗攻击是困难的，因为对手大部分情况下无法了解环境的情况。

    

    本文研究了对组合多臂老虎机（CMAB）的奖励污染攻击。我们首先给出了CMAB攻击可能性的充分必要条件，该条件取决于相应CMAB实例的内在特性，如超臂的奖励分布和基本臂的结果分布。此外，我们设计了适用于可攻击CMAB实例的攻击算法。与以往对多臂老虎机的理解相反，我们的研究揭示了一个令人惊讶的事实，即特定CMAB实例的攻击可能性还取决于发哥实例是否被对手知晓。这一发现表明，CMAB的对抗攻击在实践中很困难，并且不存在适用于任何CMAB实例的通用攻击策略，因为环境对于对手来说大部分是未知的。我们通过对实际CMAB应用（包括概率最大覆盖问题、在线最小生成树问题）的大量实验验证了我们的理论发现。

    We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, which depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanni
    
[^74]: 指向因果基础模型: 因果推断与注意力的对偶关系

    Towards Causal Foundation Model: on Duality between Causal Inference and Attention. (arXiv:2310.00809v1 [cs.LG])

    [http://arxiv.org/abs/2310.00809](http://arxiv.org/abs/2310.00809)

    该论文提出了一种名为Causal Inference with Attention (CInA)的新方法，利用因果推断和注意力的对偶关系，在复杂任务中实现了零样本的因果推断。

    

    基于因果推断和注意力之间的对偶连接，我们提出了一种名为Causal Inference with Attention (CInA)的理论上完备的方法，利用多个无标签数据集进行自监督因果学习，并在新数据的未见任务上实现零样本因果推断。我们的实证结果表明了我们的方法在复杂任务中的有效性。

    Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach
    
[^75]: 在移动应用开发中基于缺失值抗干扰元启发式神经网络的稳健能耗预测

    Robust Energy Consumption Prediction with a Missing Value-Resilient Metaheuristic-based Neural Network in Mobile App Development. (arXiv:2309.12484v1 [cs.NE])

    [http://arxiv.org/abs/2309.12484](http://arxiv.org/abs/2309.12484)

    本研究提出了一种基于元启发式方法增强的神经网络框架，旨在实现在移动应用开发中稳健的能耗预测。

    

    能耗是移动应用开发中的一个基本问题，对开发人员和终端用户都具有重要意义。此外，在消费者考虑智能手机购买时，能耗是决策过程中的一个关键因素。从可持续发展的角度来看，鉴于数十亿部智能手机的广泛使用所带来的重大全球影响，探索旨在减少移动设备能耗的方法变得至关重要，这对环境产生了深远影响。尽管安卓平台（主导的移动生态系统）中存在各种节能编程实践，但仍需要专门面向移动应用开发的机器学习-based能耗预测算法的文档化。因此，本研究的主要目标是提出一种基于元启发式方法增强的神经网络框架，以实现稳健的能耗预测。

    Energy consumption is a fundamental concern in mobile application development, bearing substantial significance for both developers and end-users. Moreover, it is a critical determinant in the consumer's decision-making process when considering a smartphone purchase. From the sustainability perspective, it becomes imperative to explore approaches aimed at mitigating the energy consumption of mobile devices, given the significant global consequences arising from the extensive utilisation of billions of smartphones, which imparts a profound environmental impact. Despite the existence of various energy-efficient programming practices within the Android platform, the dominant mobile ecosystem, there remains a need for documented machine learning-based energy prediction algorithms tailored explicitly for mobile app development. Hence, the main objective of this research is to propose a novel neural network-based framework, enhanced by a metaheuristic approach, to achieve robust energy predi
    
[^76]: 学习干预概念瓶颈

    Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])

    [http://arxiv.org/abs/2308.13453](http://arxiv.org/abs/2308.13453)

    该论文提出了一种扩展了概念瓶颈模型的概念瓶颈记忆模型（CB2M），通过学习将干预推广到不同情境并重新应用先前干预来自动改善模型性能。当没有先前的人类干预信息时，CB2M能够检测错误并请求有针对性的干预。

    

    传统的深度学习模型缺乏解释性，而概念瓶颈模型（CBM）通过其概念表示提供固有的解释。具体而言，它们允许用户通过更新概念值并纠正模型的预测输出来进行干预交互。然而，传统方法中这些干预仅应用于模型一次后即被丢弃。为了纠正这一问题，我们提出了概念瓶颈记忆模型（CB2M），这是CBM的一个扩展。具体而言，CB2M通过双折叠记忆学习将干预的推广到适当的新情境中，从而能够学习检测错误并重新应用先前的干预。通过这种方式，CB2M能够从最初获得的少量干预中自动提高模型的性能。如果没有先前的人类干预信息，CB2M可以检测到CBM瓶颈的潜在错误并请求有针对性的干预。

    While traditional deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Specifically, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Traditionally, however, these interventions are applied to the model only once and discarded afterward. To rectify this, we present concept bottleneck memory models (CB2M), an extension to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate novel situations via a two-fold memory with which it can learn to detect mistakes and to reapply previous interventions. In this way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. In our experime
    
[^77]: 有关在有限预算二臂赌博机中进行最佳臂选择的统一最优算法研究

    On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget. (arXiv:2308.12000v1 [stat.ML])

    [http://arxiv.org/abs/2308.12000](http://arxiv.org/abs/2308.12000)

    本文研究了在有限预算的随机二臂赌博机中进行最佳臂选择的问题，并证明不存在比等概率采样算法更好的算法。我们引入了一致稳定算法的概念，并证明任何在所有情况下与等概率采样算法表现一样好的算法必须属于这个类别。这一结果解决了之前的两个未解之谜。

    

    本文研究了在具有伯努利奖励的随机二臂赌博机中，使用有限预算进行最佳臂选择的问题。我们证明令人惊讶的是，不存在一个算法可以在所有情况下与等概率采样算法表现一样好（该算法被称为“均匀采样”算法），并且在至少一个情况下明显优于该算法。简而言之，不存在比均匀采样算法更好的算法。为了证明这一结果，我们引入了“一致”和“稳定”算法的自然类，并且证明了任何算法要在所有情况下与均匀采样算法表现一样好，必须属于这个类别。通过导出满足任何一致且稳定算法的错误率的下界，并证明均匀采样算法与此下界相匹配，我们完成了证明过程。我们的结果解决了\cite{qin2022open}中提出的两个未解之谜。

    We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
    
[^78]: 激活添加: 无需优化即可操纵语言模型

    Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10248](http://arxiv.org/abs/2308.10248)

    这项研究探讨了一种在推理时通过改变激活来预测性地改变语言模型行为的方法，并且相比于传统方法具有更低的计算和实施成本，并且能够保持模型性能。

    

    可靠地控制大型语言模型的行为是一个紧迫的开放性问题。现有的方法包括有监督微调、根据人类反馈进行强化学习、提示工程和引导解码。我们相反，研究了激活工程：在推理时修改激活以可预测地改变模型行为。特别地，我们通过自然语言隐式指定了一个添加的“导向向量”来偏置前向传播。与以前学习这些导向向量的工作不同，我们的激活添加（ActAdd）方法通过计算来自提示对的激活差异来计算它们。我们在OpenWebText和ConceptNet上展示了ActAdd在GPT-2上的应用。我们的推理时方法控制了输出的高级属性并保持了非目标模型的性能。它所需的计算和实施工作比微调要少得多，允许用户提供自然语言的规范，并且其开销与模型规模自然地扩展。

    Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
    
[^79]: VITS: 基于变分推理的汤普森抽样用于情境背离问题的算法

    VITS : Variational Inference Thomson Sampling for contextual bandits. (arXiv:2307.10167v1 [stat.ML])

    [http://arxiv.org/abs/2307.10167](http://arxiv.org/abs/2307.10167)

    VITS是一种基于高斯变分推理的新算法，用于情境背离问题的汤普森抽样。它提供了强大的后验近似，计算效率高，并且在线性情境背离问题中达到与传统TS相同阶数的次线性遗憾上界。

    

    本文介绍并分析了一种用于情境背离问题的汤普森抽样（TS）算法的变体。传统的TS算法在每轮需要从当前的后验分布中抽样，而这通常是难以计算的。为了解决这个问题，可以使用近似推理技术并提供接近后验分布的样本。然而，当前的近似技术要么估计不准确（拉普拉斯近似），要么计算开销较大（MCMC方法，集成抽样...）。在本文中，我们提出了一种新的算法，基于高斯变分推理的变分推理汤普森抽样（VITS）。这种方法提供了强大的后验近似，并且容易从中抽样，而且计算效率高，是TS的理想选择。此外，我们还证明了在线性情境背离问题中，VITS实现了与传统TS相同阶数的次线性遗憾上界，与维度和回合数成正比。

    In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we 
    
[^80]: 基于扩散概率模型的噪声感知语音增强

    Noise-aware Speech Enhancement using Diffusion Probabilistic Model. (arXiv:2307.08029v1 [eess.AS])

    [http://arxiv.org/abs/2307.08029](http://arxiv.org/abs/2307.08029)

    本文提出了一种基于噪声感知的语音增强方法，在扩散模型中引入噪声特定信息，通过噪声分类和多任务学习方案来增强噪声调节器的噪声特异性。证实该方法在VoiceBank-DEMAND数据集上取得了良好的实验效果。

    

    随着扩散模型的最新进展，生成式语音增强（SE）因其对未知测试噪声的巨大潜力而受到了大量研究的关注。然而，现有的研究主要集中在清晰语音的固有特性上进行推理，没有充分利用真实世界条件下变化的噪声信息。在本文中，我们提出了一种噪声感知语音增强（NASE）方法，该方法提取噪声特定信息来引导扩散模型的逆向处理过程。具体而言，我们设计了一个噪声分类（NC）模型，用于产生声学嵌入作为噪声调节器来指导逆向降噪过程。同时，我们设计了多任务学习方案，共同优化SE和NC任务，以增强提取的噪声调节器的噪声特异性。我们提出的NASE被证明是一个即插即用的模块，可以推广到任何扩散SE模型。VoiceBank-DEMAND数据集上的实验证据表明，NASE达到了

    With recent advances of diffusion model, generative speech enhancement (SE) has attracted a surge of research interest due to its great potential for unseen testing noises. However, existing efforts mainly focus on inherent properties of clean speech for inference, underexploiting the varying noise information in real-world conditions. In this paper, we propose a noise-aware speech enhancement (NASE) approach that extracts noise-specific information to guide the reverse process in diffusion model. Specifically, we design a noise classification (NC) model to produce acoustic embedding as a noise conditioner for guiding the reverse denoising process. Meanwhile, a multi-task learning scheme is devised to jointly optimize SE and NC tasks, in order to enhance the noise specificity of extracted noise conditioner. Our proposed NASE is shown to be a plug-and-play module that can be generalized to any diffusion SE models. Experiment evidence on VoiceBank-DEMAND dataset shows that NASE achieves 
    
[^81]: 通过双层ReLU神经网络实现可证明的多任务表示学习

    Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v1 [cs.LG])

    [http://arxiv.org/abs/2307.06887](http://arxiv.org/abs/2307.06887)

    通过双层ReLU神经网络，本论文提出了一种可证明的多任务表示学习方法，用于解决神经网络在实践中同时训练多个任务时遇到的问题。

    

    特征学习是神经网络实际成功的关键，然而如何以及为何发生特征学习仍然难以解释。最近的理论研究表明，在用梯度下降方法优化的浅层神经网络上可以学习有意义的特征，扩展了我们对于神经切向核或随机特征范例中微不足道的特征学习的了解。然而，在实践中，神经网络越来越经常地同时训练多个具有不同损失函数的任务，并且这些先前的分析并不适用于这种情况。在多任务学习设置中，各种研究已经表明简单线性模型可以有效地进行特征学习。然而，通过非线性模型进行多任务学习，这在实践中是最常见的学习范式，仍然存在许多未知。在这项工作中，我们首次提出了一种可证明的多任务表示学习方法，通过双层ReLU神经网络实现。

    Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first 
    
[^82]: 通过奖励重新加权、重选和重新训练方法，改进了原型零件网络

    Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])

    [http://arxiv.org/abs/2307.03887](http://arxiv.org/abs/2307.03887)

    本研究通过引入奖励重新加权、重选和重新训练的方法，改进了原型零件网络的分类效果，解决了学习从图像的虚假或不一致的部分进行分类的问题。

    

    近年来，人们致力于开发深度可解释的图像分类方法，能够清楚地将模型的输出归因于数据的特定特征。其中一种方法是原型零件网络（ProtoPNet），它基于输入的有意义部分来尝试分类图像。然而，这种方法经常学习从图像的虚假或不一致的部分进行分类。为了解决这个问题，我们受到强化学习与人类反馈（RLHF）的最新发展启发，通过在CUB-200-2011数据集上收集人类原型质量的1-5分级注释，构建一个学习识别非虚假原型的奖励模型。我们提出了重新加权、重选和重新训练的原型零件网络（R3-ProtoPNet），该网络在ProtoPNet训练循环中增加了三个额外的步骤。

    In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
    
[^83]: MALIBO: 元学习应用于无似然贝叶斯优化

    MALIBO: Meta-learning for Likelihood-free Bayesian Optimization. (arXiv:2307.03565v1 [cs.LG])

    [http://arxiv.org/abs/2307.03565](http://arxiv.org/abs/2307.03565)

    MALIBO是一种元学习贝叶斯优化方法，通过直接学习跨任务的查询效用，并引入辅助模型以实现对新任务的稳健适应，克服了现有方法的可伸缩性和不确定性的限制。

    

    贝叶斯优化是一种优化昂贵黑盒函数的流行方法。传统的贝叶斯优化会从头开始优化每个新的目标任务，而元学习则是利用相关任务的知识来更快地优化新任务的一种方式。然而，现有的元学习贝叶斯优化方法依赖于标准模型，这些模型存在可伸缩性问题，并且对不同任务之间观察数据的尺度和噪声类型非常敏感。此外，它们常常忽视与任务相似性相关的不确定性，这导致在仅有有限观察数据或新任务与相关任务差异显著时，任务适应性不可靠。为了解决这些限制，我们提出了一种新颖的元学习贝叶斯优化方法，旨在绕开标准模型，直接学习跨任务的查询效用。我们的方法明确建模任务的不确定性，并引入了一个辅助模型，使其能够对新任务进行稳健适应。大量实验证明了我们方法的有效性。

    Bayesian optimization (BO) is a popular method to optimize costly black-box functions. While traditional BO optimizes each new target task from scratch, meta-learning has emerged as a way to leverage knowledge from related tasks to optimize new tasks faster. However, existing meta-learning BO methods rely on surrogate models that suffer from scalability issues and are sensitive to observations with different scales and noise types across tasks. Moreover, they often overlook the uncertainty associated with task similarity. This leads to unreliable task adaptation when only limited observations are obtained or when the new tasks differ significantly from the related tasks. To address these limitations, we propose a novel meta-learning BO approach that bypasses the surrogate model and directly learns the utility of queries across tasks. Our method explicitly models task uncertainty and includes an auxiliary model to enable robust adaptation to new tasks. Extensive experiments show that ou
    
[^84]: 朝着对独立子网络训练的更好理论理解迈进 (arXiv:2306.16484v1 [cs.LG])

    Towards a Better Theoretical Understanding of Independent Subnetwork Training. (arXiv:2306.16484v1 [cs.LG])

    [http://arxiv.org/abs/2306.16484](http://arxiv.org/abs/2306.16484)

    本研究对独立子网络训练（IST）进行了理论分析，发现了IST与其他模型并行方法之间的根本差异。

    

    现代大规模机器学习的进展离不开数据并行分布式计算的范式。由于大规模模型的分布式计算对通信通道施加了巨大压力，最近的研究主要集中在共同设计通信压缩策略和训练算法，以降低通信成本。尽管纯数据并行性允许更好的数据扩展性，但其在模型扩展性方面存在问题。事实上，计算节点受内存限制严重限制，阻止了模型尺寸的进一步增加。因此，训练巨型神经网络模型的最新成果也依赖于某种形式的模型并行性。在这项工作中，我们对独立子网络训练（IST）进行了更详细的理论研究，这是一种最近提出的高效解决上述问题的技术。我们发现IST和其他模型并行方法之间存在根本性的差异。

    Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, significant recent research has been directed toward co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models also rely on some form of model parallelism. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alter
    
[^85]: 更多的PAC-Bayes Bounds：从有界损失到具有一般性尾部行为的损失，到任何时间均有效的损失。

    More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity. (arXiv:2306.12214v1 [stat.ML])

    [http://arxiv.org/abs/2306.12214](http://arxiv.org/abs/2306.12214)

    本文提出了一种新的高概率PAC-Bayes界限，在有界和一般尾部行为的损失中均适用。此外，这些界限还能够保持随时有效性。

    

    本文针对不同类型的损失提出了新的高概率PAC-Bayes界限。首先，针对有界范围的损失，我们提出了Catoni界的加强版本，适用于所有参数值的统一界。这导致了新的快速速率和混合速率上限，这些上限可解释性强且比文献中先前界限更紧。其次，针对更一般的尾部行为的损失，我们引入了两个新的无参数上限：当损失的累积生成函数有界时，我们引入了一个PAC-Bayes Chernoff类比，另一个上限是损失的二阶矩有界。这两个上限是利用一种基于可能事件空间的离散化的新技术获得的，“在概率”参数优化问题。最后，我们使用一种适用于任何现有界限的简单技术将所有先前结果扩展到任何时间有效的上限。

    In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we present a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the "in probability" parameter optimization problem. Finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.
    
[^86]: 纠正公平分类中的低估偏差和交叉偏差

    Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v1 [cs.LG])

    [http://arxiv.org/abs/2306.11112](http://arxiv.org/abs/2306.11112)

    本文提出一种可以有效纠正数据偏差和交叉偏差的学习方法，并构造了一个重新加权方案，可以精确评估任何假设在真实分布上的损失。

    

    我们考虑学习被低估偏差损坏的数据的问题，其中正例在固定数量的敏感组中以不同的未知速率从数据中过滤掉。我们表明，在有少量无偏数据的情况下，我们可以有效地估计每个组的减少参数，即使在交叉组成员资格使得学习每个交叉率变得计算上不可行的情况下。利用这个分组丢失率的估计，我们构造了一个重新加权方案，可以使我们近似评估任何假设在真实分布上的损失，即使我们只能在一个有偏样本上观察到经验误差。最后，我们提出了一个封装了这个学习和重新加权过程的算法，并提供了强PAC风格的保证，即有很高的概率我们对假设在真实分布上的风险的估计将与真实风险任意接近。

    We consider the problem of learning from data corrupted by underrepresentation bias, where positive examples are filtered from the data at different, unknown rates for a fixed number of sensitive groups. We show that with a small amount of unbiased data, we can efficiently estimate the group-wise drop-out parameters, even in settings where intersectional group membership makes learning each intersectional rate computationally infeasible. Using this estimate for the group-wise drop-out rate, we construct a re-weighting scheme that allows us to approximate the loss of any hypothesis on the true distribution, even if we only observe the empirical error on a biased sample. Finally, we present an algorithm encapsulating this learning and re-weighting process, and we provide strong PAC-style guarantees that, with high probability, our estimate of the risk of the hypothesis over the true distribution will be arbitrarily close to the true risk.
    
[^87]: CompanyKG:一种用于公司相似性量化的大规模异构图

    CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification. (arXiv:2306.10649v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.10649](http://arxiv.org/abs/2306.10649)

    本研究提出了CompanyKG，一种用于公司相似性量化的大规模异构图数据集。通过丰富的公司特征和关系表示，以及多个评估任务的基准测试，为公司相似性量化方法的综合评估提供了支持。

    

    在投资行业中，对于许多目的包括市场映射、竞争对手分析和并购，进行细粒度公司相似性量化通常是至关重要的。我们提出并发布了一个名为CompanyKG的知识图，用于表示和学习多样化的公司特征和关系。具体而言，1.17百万家公司被表示为节点，丰富了公司描述嵌入; 15种不同的公司间关系导致了5106百万个带权重的边。为了实现对公司相似性量化方法的全面评估，我们设计并编译了三个带有注释测试集的评估任务: 相似性预测、竞争对手检索和相似性排序。我们对11种可重现预测方法进行了广泛的基准测试，分为节点、边和节点+边三组。据我们所知，CompanyKG是第一个大规模的异构图数据集

    In the investment industry, it is often essential to carry out fine-grained company similarity quantification for a range of purposes, including market mapping, competitor analysis, and mergers and acquisitions. We propose and publish a knowledge graph, named CompanyKG, to represent and learn diverse company features and relations. Specifically, 1.17 million companies are represented as nodes enriched with company description embeddings; and 15 different inter-company relations result in 51.06 million weighted edges. To enable a comprehensive assessment of methods for company similarity quantification, we have devised and compiled three evaluation tasks with annotated test sets: similarity prediction, competitor retrieval and similarity ranking. We present extensive benchmarking results for 11 reproducible predictive methods categorized into three groups: node-only, edge-only, and node+edge. To the best of our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset or
    
[^88]: 提高决策树解释性的有效性

    Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06777](http://arxiv.org/abs/2306.06777)

    该论文介绍了一个新的决策树模型，利用挂起的树的方式提高了其解释性和统计性能，达到了无限深度决策树的水平，并可与XGBoost等最先进的方法相媲美。

    

    在基于表格数据的分类和预测中，人们经常使用基于树的模型。这可以在表格数据上与深度神经网络竞争[参见Grinsztajn等人，NeurIPS 2022，arXiv：2207.08815]，并且在某些条件下是可解释的。可解释性取决于树的深度和每个叶节点的准确性。在这里，我们训练了一个低深度的树，其目标是最小化每个叶节点上的最大错误分类，并从低深度树的每个叶节点“挂起”进一步的基于树的模型（例如无限深度的树）。低深度树易于解释，而综合低深度和挂起的基于树的模型的整体统计性能优于使用经典方法（例如CART）训练的无限深度决策树，并且与最先进的方法（例如优化的XGBoost）相当。

    In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
    
[^89]: 鲁棒的数据驱动处方优化

    Robust Data-driven Prescriptiveness Optimization. (arXiv:2306.05937v1 [math.OC])

    [http://arxiv.org/abs/2306.05937](http://arxiv.org/abs/2306.05937)

    本文提出了一种鲁棒的数据驱动优化模型，其中处方性系数代替了经验风险最小化目标，可用于确定最大化上下文决策质量和参考决策以及侧面信息处方能力的最佳策略。

    

    巨量的数据促进了各种优化技术的出现，旨在利用现有的侧面信息提供更具预判性的决策。广泛的方法和应用背景促进了设计一种称为处方性系数的通用无单位性能指标的产生。该系数旨在量化上下文决策的质量与参考决策以及侧面信息的处方能力。为了在数据驱动的情况下确定最大化前者的策略，本文引入了一种分布式鲁棒的上下文优化模型，其中处方性系数代替了经典的经验风险最小化目标。我们提出了一个双分法算法来解决这个模型，当分布不确定性集合具有适当的嵌套形式和多面体结构时，该算法依赖于解决一系列线性规划问题。本文研究了上下文短语的应用。

    The abundance of data has led to the emergence of a variety of optimization techniques that attempt to leverage available side information to provide more anticipative decisions. The wide range of methods and contexts of application have motivated the design of a universal unitless measure of performance known as the coefficient of prescriptiveness. This coefficient was designed to quantify both the quality of contextual decisions compared to a reference one and the prescriptive power of side information. To identify policies that maximize the former in a data-driven context, this paper introduces a distributionally robust contextual optimization model where the coefficient of prescriptiveness substitutes for the classical empirical risk minimization objective. We present a bisection algorithm to solve this model, which relies on solving a series of linear programs when the distributional ambiguity set has an appropriate nested form and polyhedral structure. Studying a contextual short
    
[^90]: 层间反馈对齐在深度神经网络中的保守性

    Layer-Wise Feedback Alignment is Conserved in Deep Neural Networks. (arXiv:2306.01870v1 [cs.LG])

    [http://arxiv.org/abs/2306.01870](http://arxiv.org/abs/2306.01870)

    本论文揭示了层间反馈对齐在深度神经网络中的保守性，并发现FA与GD之间存在隐式偏差的相似之处，同时阐明了ReLU网络中与反馈矩阵对齐的充分条件。

    

    为了提高深度神经网络的效率和生物可塑性，反馈对齐（FA）作为传统反向传播的替代方法应运而生，它将训练过程中的反向传输权重替换为随机矩阵。虽然FA的吸引力在于它能够绕过计算挑战和其可信的生物对齐性，但对于这种学习规则的理解还是有所欠缺的。本文揭示了支撑FA学习动态的一组守恒定律，揭示了FA和梯度下降（GD）之间的有趣相似之处。我们的分析表明，FA具有与GD表现出的隐式偏差相似的隐式偏差，挑战了现有的这些学习算法之间根本不同的流行说法。此外，我们证明，这些守恒定律阐明了ReLU网络中与反馈矩阵对齐的充分条件。我们进一步展示，这意味着过参数化的双线性网络中可以实现线性地代替后向权重。

    In the quest to enhance the efficiency and bio-plausibility of training deep neural networks, Feedback Alignment (FA), which replaces the backward pass weights with random matrices in the training process, has emerged as an alternative to traditional backpropagation. While the appeal of FA lies in its circumvention of computational challenges and its plausible biological alignment, the theoretical understanding of this learning rule remains partial. This paper uncovers a set of conservation laws underpinning the learning dynamics of FA, revealing intriguing parallels between FA and Gradient Descent (GD). Our analysis reveals that FA harbors implicit biases akin to those exhibited by GD, challenging the prevailing narrative that these learning algorithms are fundamentally different. Moreover, we demonstrate that these conservation laws elucidate sufficient conditions for layer-wise alignment with feedback matrices in ReLU networks. We further show that this implies over-parameterized tw
    
[^91]: EPIC: 通过可学习的代价实现的编辑路径插值的图形增强

    EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost. (arXiv:2306.01310v1 [cs.LG])

    [http://arxiv.org/abs/2306.01310](http://arxiv.org/abs/2306.01310)

    EPIC提出了一种基于插值的方法来增强图数据集，通过利用图编辑距离生成与原始图相似但有结构变化的新图，从而提高了分类模型的泛化能力。

    

    基于图的模型在各个领域中变得越来越重要，但现有图数据集的有限规模和多样性经常限制它们的性能。为解决这个问题，我们提出了EPIC（通过可学习的代价实现的编辑路径插值），这是一种新颖的基于插值的增强图数据集的方法。我们的方法利用了图编辑距离来生成与原始图相似但结构有所变化的新图。为了实现这一点，我们通过比较带标签的图来学习图编辑距离，并利用这一知识在原始图对之间创建了图编辑路径。通过从图编辑路径中随机抽样的图形，我们丰富了训练集以增强分类模型的泛化能力。我们在几个基准数据集上展示了我们方法的有效性，并表明它在图分类任务中优于现有的增强方法。

    Graph-based models have become increasingly important in various domains, but the limited size and diversity of existing graph datasets often limit their performance. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. Our approach leverages graph edit distance to generate new graphs that are similar to the original ones but exhibit some variation in their structures. To achieve this, we learn the graph edit distance through a comparison of labeled graphs and utilize this knowledge to create graph edit paths between pairs of original graphs. With randomly sampled graphs from a graph edit path, we enrich the training set to enhance the generalization capability of classification models. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing augmentation methods in graph classification tasks.
    
[^92]: 揭示基于注意力的图神经网络中的平滑过度现象

    Demystifying Oversmoothing in Attention-Based Graph Neural Networks. (arXiv:2305.16102v1 [cs.LG])

    [http://arxiv.org/abs/2305.16102](http://arxiv.org/abs/2305.16102)

    本文通过数学分析证明基于注意力的图神经网络并不能解决平滑过度问题，在实际应用中需要更多关注不对称、状态相关和有向图结构。

    

    图神经网络中的平滑过度指的是增加网络深度导致节点表示变得相同的现象。尽管之前的研究已经证实了图卷积网络(GCN)会指数级失去表达能力，但是图注意力机制是否可以缓解平滑过度问题还存在争议。本文通过将基于注意力的图神经网络视为非线性时变动态系统，并结合非齐次矩阵乘积和联合谱半径理论的工具和技术，对这个问题进行了严格的数学分析，提出了一个明确的答案。我们证明了与流行观点相反，图注意力机制不能防止平滑过度现象，并且呈指数级失去表达能力。所提出的框架将对称GCN的平滑过度问题扩展到了更广泛的GNN模型类别中。特别地，我们的分析考虑了在现实应用中普遍存在的不对称、状态相关和有向图结构。

    Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models. In particular, our analysis accounts for asymmetric, state-dep
    
[^93]: 关于马尔科夫转换模型的可辨识性研究

    On the Identifiability of Markov Switching Models. (arXiv:2305.15925v1 [stat.ML])

    [http://arxiv.org/abs/2305.15925](http://arxiv.org/abs/2305.15925)

    本文研究了马尔科夫转换模型的可辨识性，通过非线性高斯参数化迁移分布实现第一阶段马尔科夫依赖结构中的可辨识性条件。该方法适用于依赖于政权的因果发现和高维时间序列分割。

    

    最近，潜变量模型的可辨识性因其在可解释性或分布泛化方面的应用而备受关注。本文探讨了作为将最近的结果扩展到序列潜变量模型的第一步的马尔科夫转换模型的可辨识性。我们在第一阶段马尔科夫依赖结构中提出了可辨识性条件，并通过非线性高斯参数化迁移分布。我们的实验展示了我们方法在依赖于政权的因果发现和高维时间序列分割方面的适用性。

    Identifiability of latent variable models has recently gained interest in terms of its applications to interpretability or out of distribution generalisation. In this work, we study identifiability of Markov Switching Models as a first step towards extending recent results to sequential latent variable models. We present identifiability conditions within first-order Markov dependency structures, and parametrise the transition distribution via non-linear Gaussians. Our experiments showcase the applicability of our approach for regime-dependent causal discovery and high-dimensional time series segmentation.
    
[^94]: 基于L2，0基数惩罚的不均匀图趋势过滤。

    Inhomogeneous graph trend filtering via a l2,0 cardinality penalty. (arXiv:2304.05223v1 [cs.LG])

    [http://arxiv.org/abs/2304.05223](http://arxiv.org/abs/2304.05223)

    本文提出了一种基于L2，0基数惩罚的图趋势过滤（GTF）模型，可同时进行k-means聚类和基于图的最小割，以估计在节点之间具有不均匀平滑水平的分段平滑图信号，并在降噪、支持恢复和半监督分类任务上表现更好，比现有方法更高效地处理大型数据集。

    

    我们研究了在图上估计分段平滑信号的方法，并提出了一种$\ell_{2,0}$-范数惩罚图趋势过滤（GTF）模型，以估计在节点之间具有不均匀平滑水平的分段平滑图信号。我们证明了所提出的GTF模型同时是基于节点上的信号的k-means聚类和基于图的最小割，其中聚类和割共享相同的分配矩阵。我们提出了两种方法来解决所提出的GTF模型：一种是基于谱分解的方法，另一种是基于模拟退火的方法。在合成和现实数据集的实验中，我们展示了所提出的GTF模型在降噪、支持恢复和半监督分类任务上表现更好，且比现有方法更高效地解决了大型数据集的问题。

    We study estimation of piecewise smooth signals over a graph. We propose a $\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate piecewise smooth graph signals that exhibits inhomogeneous levels of smoothness across the nodes. We prove that the proposed GTF model is simultaneously a k-means clustering on the signal over the nodes and a minimum graph cut on the edges of the graph, where the clustering and the cut share the same assignment matrix. We propose two methods to solve the proposed GTF model: a spectral decomposition method and a method based on simulated annealing. In the experiment on synthetic and real-world datasets, we show that the proposed GTF model has a better performances compared with existing approaches on the tasks of denoising, support recovery and semi-supervised classification. We also show that the proposed GTF model can be solved more efficiently than existing models for the dataset with a large edge set.
    
[^95]: 使用迁移学习实现隐私保护的CNN训练

    Privacy-Preserving CNN Training with Transfer Learning. (arXiv:2304.03807v1 [cs.CR])

    [http://arxiv.org/abs/2304.03807](http://arxiv.org/abs/2304.03807)

    本文提出了一种使用迁移学习实现同态加密技术下隐私保护的CNN训练的方案，通过转换思想和更快的梯度变体，取得了最先进的性能。

    

    隐私保护的神经网络推理已经得到很好的研究，同时保持同态CNN训练仍然是一项挑战性的任务。在本文中，我们提出了一种实用的解决方案来实现基于同态加密技术的隐私保护CNN训练。据我们所知，这是第一次成功突破这个难题，以前没有任何工作达到这个目标。采用了几种技术：（1）通过迁移学习，可以将隐私保护的CNN训练简化为同态神经网络训练，甚至是多类逻辑回归（MLR）训练；（2）通过更快的梯度变体$\texttt{Quadratic Gradient}$，应用于MLR的增强梯度方法，在收敛速度方面具有最先进的性能；（3）我们采用数学中的变换思想，将加密域中的近似Softmax函数转换成已经研究过的逼近方法，从而得到更好的结果。

    Privacy-preserving nerual network inference has been well studied while homomorphic CNN training still remains an open challenging task. In this paper, we present a practical solution to implement privacy-preserving CNN training based on mere Homomorphic Encryption (HE) technique. To our best knowledge, this is the first attempt successfully to crack this nut and no work ever before has achieved this goal. Several techniques combine to make it done: (1) with transfer learning, privacy-preserving CNN training can be reduced to homomorphic neural network training, or even multiclass logistic regression (MLR) training; (2) via a faster gradient variant called $\texttt{Quadratic Gradient}$, an enhanced gradient method for MLR with a state-of-the-art performance in converge speed is applied in this work to achieve high performance; (3) we employ the thought of transformation in mathematics to transform approximating Softmax function in encryption domain to the well-studied approximation of 
    
[^96]: 学习优化增强学习

    Learning to Optimize for Reinforcement Learning. (arXiv:2302.01470v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01470](http://arxiv.org/abs/2302.01470)

    学习优化器在监督学习中取得了显著的成功，但在强化学习中面临梯度范围变化大、梯度分布非独立且不同、高方差偏差等问题。本文提出了梯度处理、管道训练和一种新颖的优化器结构来解决这些问题。

    

    近年来，通过利用更多的数据、计算和不同的任务，学习优化器在监督学习中取得了显著的成功，超过了传统手动设计的优化器。然而，强化学习与监督学习本质上不同，这些学习优化器在简单的强化学习任务中效果不佳。我们调查了这一现象，发现了三个问题。首先，强化学习代理的梯度在对数上变化范围很大，而在绝对值上范围较小，这使得神经网络难以获得准确的参数更新。其次，代理梯度分布非独立且不同，导致元训练效率低下。最后，由于代理与环境之间的高度随机交互，代理梯度存在较高的偏差和方差，增加了强化学习优化器的学习难度。我们提出了梯度处理、管道训练和一种新颖的优化器结构。

    In recent years, by leveraging more data, computation, and diverse tasks, learned optimizers have achieved remarkable success in supervised learning, outperforming classical hand-designed optimizers. Reinforcement learning (RL) is essentially different from supervised learning and in practice these learned optimizers do not work well even in simple RL tasks. We investigate this phenomenon and identity three issues. First, the gradients of an RL agent vary across a wide range in logarithms while their absolute values are in a small range, making neural networks hard to obtain accurate parameter updates. Second, the agent-gradient distribution is non-independent and identically distributed, leading to inefficient meta-training. Finally, due to highly stochastic agent-environment interactions, the agent-gradients have high bias and variance, which increase the difficulty of learning an optimizer for RL. We propose gradient processing, pipeline training, and a novel optimizer structure wit
    
[^97]: 具有完成功能的神经通用邻居用于链接预测

    Neural Common Neighbor with Completion for Link Prediction. (arXiv:2302.00890v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00890](http://arxiv.org/abs/2302.00890)

    提出了神经通用邻居模型（NCN）用于链接预测，使用可学习的成对表示来捕捉节点之间的成对关系，以提高性能，同时解决链路不完整问题。

    

    尽管vanilla信息传递神经网络（MPNN）在各种图任务中具有出色的性能，但在链接预测任务中通常失败，因为它只使用两个单独目标节点的表示，并忽略它们之间的成对关系。为了捕获成对关系，一些模型将手动功能添加到输入图中，并使用MPNN的输出来生成成对表示。相反，其他人直接将手动功能用作成对表示。尽管此简化避免了将GNN逐个链接地应用于每个链接，从而提高了可扩展性，但由于手工制作的和不可学习的成对特征，这些模型仍有很大的性能提升空间。为了在保持可扩展性的同时提高性能，我们提出了神经通用邻居（NCN），它使用可学习的成对表示。为了进一步提高NCN的性能，我们研究了未观察到的链接问题。图的不完整性是普遍存在的，并导致分布偏移

    Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between
    
[^98]: 如何使用信息论选择客观函数

    How to select an objective function using information theory. (arXiv:2212.06566v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06566](http://arxiv.org/abs/2212.06566)

    信息论告诉我们，为了最大化模型的信息量，选择可能性最高或表示误差比特最少的客观函数。将不同的客观函数转换为似然函数，它们的相对大小表示我们应该更喜欢哪个客观函数，而其大小的对数表示模型的预期不确定性。

    

    在机器学习或科学计算中，模型性能是通过客观函数衡量的。但是为什么要选择某个客观函数而不是另一个？信息论给出了一个答案：为了最大化模型中的信息量，选择最有可能的客观函数或者代表误差的比特最少的函数。要评估不同的客观函数，将它们转换为似然函数。作为似然函数，它们的相对大小表示我们应该更喜欢哪个客观函数，而其大小的对数表示模型的预期不确定性。

    In machine learning or scientific computing, model performance is measured with an objective function. But why choose one objective over another? Information theory gives one answer: To maximize the information in the model, select the most likely objective function or whichever represents the error in the fewest bits. To evaluate different objectives, transform them into likelihood functions. As likelihoods, their relative magnitudes represent how much we should prefer one objective versus another, and the log of their magnitude represents the expected uncertainty of the model.
    
[^99]: 超越集合平均值：利用气候模型集合进行地表季节性预测

    Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting. (arXiv:2211.15856v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15856](http://arxiv.org/abs/2211.15856)

    本文研究了利用气候模型集合进行地表季节性预测的应用，超越了传统的平均方法，利用集合预测中的信息提高了预测准确性，关注了极端事件的预测，同时考虑了空间变化的预测集合。

    

    在操作性预测中，对于关键气候变量（如温度和降水）进行高质量的地表季节性时间尺度预测一直存在着差距。最近的研究表明，使用机器学习（ML）模型来推进地表季节性预测（SSF）取得了有希望的结果，但仍存在一些未解的问题。首先，过去的方法中使用了物理基于模型集合的平均作为这些模型的输入特征，然而集合预测中包含了可以帮助预测的信息，不仅仅是集合均值。其次，过去的方法关注平均性能，然而对于计划和减灾目的来说，极端事件的预测更加重要。第三，气候预测对应于一个空间变化的预测集合，而不同的方法以不同的方式考虑了响应的空间可变性。模型堆叠可以缓解不同方法之间的权衡。本文描述了一种利用模型集合进行地表季节性预测的应用。

    Producing high-quality forecasts of key climate variables such as temperature and precipitation on subseasonal time scales has long been a gap in operational forecasting. Recent studies have shown promising results using machine learning (ML) models to advance subseasonal forecasting (SSF), but several open questions remain. First, several past approaches use the average of an ensemble of physics-based forecasts as an input feature of these models. However, ensemble forecasts contain information that can aid prediction beyond only the ensemble mean. Second, past methods have focused on average performance, whereas forecasts of extreme events are far more important for planning and mitigation purposes. Third, climate forecasts correspond to a spatially-varying collection of forecasts, and different methods account for spatial variability in the response differently. Trade-offs between different approaches may be mitigated with model stacking. This paper describes the application of a va
    
[^100]: 通过在双曲流形上使用GPLVM将机器人分类带入连续领域

    Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds. (arXiv:2210.01672v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.01672](http://arxiv.org/abs/2210.01672)

    本论文提出了一种通过在双曲流形上使用GPLVM来在连续领域中应用机器人分类法的方法，通过捕捉相关层次结构的双曲嵌入来建模分类数据，并采用图形先验和保持距离的后向约束来实现分类法结构的纳入。

    

    机器人分类被用作将人类的移动和与环境互动的方式进行高层次的分层抽象。它们已被证明对于分析抓取、操纵技能和全身支撑姿势非常有用。尽管我们在设计层次结构和基础类别方面做出了大量努力，但它们在应用领域的使用仍然有限。这可能是因为缺乏填补分类层级结构和与其类别相关联的高维异构数据之间差距的计算模型。为了解决这个问题，我们建议通过捕捉相关层次结构的双曲嵌入来建模分类数据。我们通过构建一个新颖的高斯过程双曲潜变量模型来实现这一点，该模型通过图形先验和保持距离的后向约束将分类法结构纳入潜在空间中。我们在三个不同的机器人分类法上验证了我们的模型。

    Robotic taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use in application fields remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different robotics taxonomies to lear
    

