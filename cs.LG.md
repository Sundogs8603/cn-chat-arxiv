# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On the Actionability of Outcome Prediction.](http://arxiv.org/abs/2309.04470) | 提高结果预测准确性有助于确定最适合的干预措施，但在存在多种可能干预的情况下，仅仅依赖结果预测进行干预可能无法达到预期效果。 |
| [^2] | [Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models.](http://arxiv.org/abs/2309.04461) | 本研究探索了视觉-语言模型展示人类推理能力的能力，并提出了一种基于思维链的一致性度量。通过一个流水线和已有数据集，建立了一个基准来测量这种推理能力。 |
| [^3] | [Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning.](http://arxiv.org/abs/2309.04459) | 通过将行动空间离散化并采用分词技术，我们提出了一种在稀疏奖励强化学习中生成技巧的新方法。这种方法能够减少探索的难度，并在连续行动空间中达到良好的性能。 |
| [^4] | [Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks.](http://arxiv.org/abs/2309.04452) | 本研究使用置换不变神经网络对集合天气预测进行后处理，不同于之前的方法，该网络将预测集合视为一组无序的成员预测，并学习对成员顺序的排列置换具有不变性的链接函数。在地表温度和风速预测的案例研究中，我们展示了最先进的预测质量。 |
| [^5] | [Variations and Relaxations of Normalizing Flows.](http://arxiv.org/abs/2309.04433) | 正规化流是一种模型，采用一系列双射变换将复杂目标分布表示为简单基础分布。然而，由于对微分同胚的限制，其在有效表示复杂拓扑目标分布和处理先验分布与目标分布不同胚的情况方面存在局限性。相关研究通过结合其他生成模型的特点，放宽了正规化流的约束性，以平衡约束和灵活性之间的关系 |
| [^6] | [Soft Quantization using Entropic Regularization.](http://arxiv.org/abs/2309.04428) | 本研究通过使用熵正则化和软最小函数来解决量化问题，提出了一种新的近似技术，并使用随机梯度方法获得最优解。该方法具有调节优化问题难度的控制参数，可在处理具有挑战性问题时提供显著优势。 |
| [^7] | [Robust Representation Learning for Privacy-Preserving Machine Learning: A Multi-Objective Autoencoder Approach.](http://arxiv.org/abs/2309.04427) | 本论文介绍了一种多目标自编码器方法，利用鲁棒的表示学习来编码数据以进行隐私保护机器学习。该方法解决了传统技术中性能降低的问题，并在保护隐私的同时优化了模型的实用性。 |
| [^8] | [Parallel and Limited Data Voice Conversion Using Stochastic Variational Deep Kernel Learning.](http://arxiv.org/abs/2309.04420) | 该论文提出了一种基于随机变分深度核学习（SVDKL）的声音转换方法，能够适应有限数据，并利用深度神经网络和高斯过程的灵活性进行模型训练和复杂函数估计。 |
| [^9] | [Emergent learning in physical systems as feedback-based aging in a glassy landscape.](http://arxiv.org/abs/2309.04382) | 通过训练线性物理网络学习线性变换，我们发现其学习行为与无序和玻璃状系统中的老化和记忆形成过程相似。学习动态类似于老化过程，通过重复施加反馈边界力来放松并编码输入-输出关系的记忆。 |
| [^10] | [Generalization Bounds: Perspectives from Information Theory and PAC-Bayes.](http://arxiv.org/abs/2309.04381) | 该论文介绍了一般化界限的两个视角：信息论和PAC-Bayesian，并探讨了它们之间的联系和共同点。这对于理论机器学习的进一步发展和新算法的设计具有重要意义。 |
| [^11] | [Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control.](http://arxiv.org/abs/2309.04370) | 本研究提出了一种带有力响应式运动控制的导盲四足机器人导航系统，通过同时训练运动控制器和外力估计器，实现对外部拉扯力的稳健感知和响应。实验结果表明该系统能够准确导航并绕过障碍物，具有较强的鲁棒性。 |
| [^12] | [Active Learning for Classifying 2D Grid-Based Level Completability.](http://arxiv.org/abs/2309.04367) | 本文提出了使用主动学习方法来学习2D基于网格的关卡可完成性分类，通过对Super Mario Bros.、Kid Icarus和一个类似Zelda的游戏生成的关卡进行标记，获得了更好的分类器性能。 |
| [^13] | [Learning from Power Signals: An Automated Approach to Electrical Disturbance Identification Within a Power Transmission System.](http://arxiv.org/abs/2309.04361) | 本文提出了一种自动化方法，利用基于规则的分析来识别电力传输系统内记录的干扰事件。经过测试，在160个信号文件上，准确率达到了99％。 |
| [^14] | [Value-Compressed Sparse Column (VCSC): Sparse Matrix Storage for Redundant Data.](http://arxiv.org/abs/2309.04355) | 值压缩的稀疏列（VCSC）是一种新的稀疏矩阵存储格式，能够利用高冗余性将数据进一步压缩，并在性能上没有显著的负面影响。通过增量编码和字节打包压缩索引数组，IVCSC实现了更大的存储空间节省。 |
| [^15] | [Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts.](http://arxiv.org/abs/2309.04354) | 本论文研究通过使用稀疏MoEs来缩小视觉变换器（ViTs）的规模，以提高在资源受限的视觉应用中的性能和效率。 |
| [^16] | [Zero-Shot Robustification of Zero-Shot Models With Foundation Models.](http://arxiv.org/abs/2309.04344) | 提出了一种零样本强化方法RoboShot，通过使用零样本语言模型从任务描述中获取有用的见解，并应用于预训练模型嵌入中以去除有害成分并增强有用成分，从而改善预训练模型的鲁棒性。 |
| [^17] | [Online Submodular Maximization via Online Convex Optimization.](http://arxiv.org/abs/2309.04339) | 本论文研究了在线设置下的一般性子模最大化问题，并将一类大型子模函数归约到在线凸优化问题中。这种归约方式可在组合优化中实现次线性遗憾，并且适用于许多不同版本的在线学习问题。 |
| [^18] | [Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens.](http://arxiv.org/abs/2309.04333) | 本研究提出了Multi2SPE方法，通过使用多个CLS标记来编码多领域科学论文，能够更好地适应不同的科学领域，并在引文预测任务中减少了多达25％的误差。 |
| [^19] | [Graph Neural Networks Use Graphs When They Shouldn't.](http://arxiv.org/abs/2309.04332) | 在图形预测问题中，GNNs倾向于过拟合图结构，即使在忽略图结构的情况下可以获得更好的解决方案。常规图对于这种过拟合更具鲁棒性。 |
| [^20] | [Generating the Ground Truth: Synthetic Data for Label Noise Research.](http://arxiv.org/abs/2309.04318) | 本文提出了SYNLABEL框架，通过生成基于真实数据的无噪声数据集，并可以为每个数据点分配一个软标签或标签分布，用于改进标签噪声研究。 |
| [^21] | [Actor critic learning algorithms for mean-field control with moment neural networks.](http://arxiv.org/abs/2309.04317) | 我们开发了一种使用矩神经网络的演员-评论员算法，用于解决平均场控制问题。我们的方法利用基于梯度的价值函数表示，并通过直接采样分布的轨迹来实现学习。数值结果表明，我们的方法在多维和非线性二次控制问题等不同情境下具有良好的效果。 |
| [^22] | [Federated Learning for Early Dropout Prediction on Healthy Ageing Applications.](http://arxiv.org/abs/2309.04311) | 本文提出了一种基于联邦学习的方法，用于预测健康老龄应用中的早期退学情况。该方法通过联合个体和组织进行训练，并实现了隐私保护和分布式学习。 |
| [^23] | [Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility.](http://arxiv.org/abs/2309.04296) | 本研究提出了一种利用人类移动数据和持续学习技术的方法来解决COVID-19期间非分布期间的电力负荷预测问题，通过保留过去的见解并整合新的数据，提高了模型的准确性和鲁棒性。 |
| [^24] | [Viewing the process of generating counterfactuals as a source of knowledge -- Application to the Naive Bayes classifier.](http://arxiv.org/abs/2309.04284) | 将生成对立假设的过程视为知识来源，并应用于朴素贝叶斯分类器，展示其有趣属性。 |
| [^25] | [Optimal Rate of Kernel Regression in Large Dimensions.](http://arxiv.org/abs/2309.04268) | 该论文提出了一种针对大维度数据的核回归的最优比率，通过使用Mendelson复杂性和度量熵来刻画其上界和最小化下界。此外，研究还发现最优比率随着维度与样本大小关系的变化呈现出多次下降的行为。 |
| [^26] | [Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos.](http://arxiv.org/abs/2309.04236) | 本文提出了自适应分布式核岭回归（AdaDKRR）方法，该方法通过考虑自治性、隐私性和合作性解决了数据孤立的问题，理论上证明了其性能与整体数据运行最优学习算法类似。 |
| [^27] | [Offline Recommender System Evaluation under Unobserved Confounding.](http://arxiv.org/abs/2309.04222) | 本论文讨论了在存在潜在混淆因素的情况下进行离线推荐系统评估的问题，并特别关注推荐系统用例。通过对基于策略的估计器进行研究，我们描述了由混淆因素引起的统计偏差。 |
| [^28] | [Concomitant Group Testing.](http://arxiv.org/abs/2309.04221) | 本文介绍了一种新的组合测试问题，即同时进行的分组测试(ConcGT)。该问题需要通过尽可能少的测试来可靠地识别多个不相交的半缺陷集合。作者推导出了各种算法，并在两个半缺陷集合的情况下进行了重点讨论。 |
| [^29] | [Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse.](http://arxiv.org/abs/2309.04211) | 通过局部引导的顺序算法回溯，实现了使人工智能系统可解释的反事实解释方法。然而，如何有效定位一个反事实及其回溯仍然是一个未解决的挑战。 |
| [^30] | [Towards Mitigating Architecture Overfitting in Dataset Distillation.](http://arxiv.org/abs/2309.04195) | 本文解决了数据集蒸馏中的架构过度拟合问题，提出了一系列方法来提高不同网络架构在蒸馏训练数据上的泛化性能。 |
| [^31] | [Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity.](http://arxiv.org/abs/2309.04160) | 本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。 |
| [^32] | [A Deep Learning Method for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI).](http://arxiv.org/abs/2309.04100) | 本研究提出了一种用于增强DMI敏感性的深度学习方法，通过训练卷积神经网络来估计低信噪比和失真的DMI FID的代谢物浓度，并通过MRI的边缘保护正则化进一步提高估计精度。实验结果显示，该方法在提高参数质量方面具有潜在的改进效果。 |
| [^33] | [Sample-Efficient Co-Design of Robotic Agents Using Multi-fidelity Training on Universal Policy Network.](http://arxiv.org/abs/2309.04085) | 该论文提出了一种使用多保真度训练的机器人代理共设计的样本高效方法，通过在设计空间中共享学习到的控制器，以及通过特定方式遍历设计矩阵，可以提高设计评估的效率。 |
| [^34] | [Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning.](http://arxiv.org/abs/2309.04082) | 本文提出了一种混合曲率Transformer，用于图表示学习。通过将完全乘积立体变换器与标记化的图Transformer相结合，模型能够以端到端的方式学习适应输入图的曲率，从而更好地嵌入图中的分层或循环结构。 |
| [^35] | [UER: A Heuristic Bias Addressing Approach for Online Continual Learning.](http://arxiv.org/abs/2309.04081) | 本论文提出了一种针对在线连续学习的启发式偏差处理方法，通过将点积logits分解为角度因子和范数因子，解决了点积logits偏差问题。角度因子用于学习新的知识，而范数因子有助于记住历史知识。 |
| [^36] | [Enabling the Evaluation of Driver Physiology Via Vehicle Dynamics.](http://arxiv.org/abs/2309.04078) | 本文介绍了一种通过车辆动力学评估驾驶员生理状况的技术，该技术整合了商业传感器和驾驶员输入，可以提供驾驶行为和生理反应的关键参数，具有提升道路安全和早期检测健康相关并发症的潜力。 |
| [^37] | [Riemannian Langevin Monte Carlo schemes for sampling PSD matrices with fixed rank.](http://arxiv.org/abs/2309.04072) | 本文介绍了两种Riemannian Langevin Monte Carlo方案，用于从具有固定秩的PSD矩阵中采样。这些方案通过在流形上使用布朗运动的Riemannian Langevin方程的Euler-Maruyama离散化来实现采样，具有实际应用价值。 |
| [^38] | [3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation.](http://arxiv.org/abs/2309.04062) | 本论文提出了一种自监督的分子表示学习框架D&D，通过将3D去噪器的表示蒸馏到2D图形编码器中进行预训练，以解决从大规模无标签数据中预训练分子表示的问题。 |
| [^39] | [SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks.](http://arxiv.org/abs/2309.04037) | 提出了一种基于深度学习的科学数据错误有界损失压缩器SRN-SZ，用于改善难以压缩的数据集的压缩情况。 |
| [^40] | [Brief technical note on linearizing recurrent neural networks (RNNs) before vs after the pointwise nonlinearity.](http://arxiv.org/abs/2309.04030) | 这篇论文讨论了循环神经网络在激活与活动动力学线性化过程中的差异，以及线性化活动动力学下一些上下文相关效应的显现。 |
| [^41] | [TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models.](http://arxiv.org/abs/2309.04027) | 本文介绍了TIDE（Textual Identity Detection）方法来改善分类器和语言模型中的文本公平性。通过创建一个包含身份词汇和语境的数据集，以及开发一个身份注释和增强工具，可以提高机器学习公平性技术的效果。 |
| [^42] | [Optimal Transport with Tempered Exponential Measures.](http://arxiv.org/abs/2309.04015) | 本文推广了熵正则化最优输运方法，将其应用于温度指数测度中，实现了快速有效的算法和可控的稀疏性。 |
| [^43] | [An Element-wise RSAV Algorithm for Unconstrained Optimization Problems.](http://arxiv.org/abs/2309.04013) | 本文提出了一种能满足无条件能量耗散定律、在凸设置中证明线性收敛的新型优化算法E-RSAV，并且在单变量情况下改进了线性收敛速度为超线性，还提出了自适应版本的E-RSAV算法加以实现验证。 |
| [^44] | [Multimodal Transformer for Material Segmentation.](http://arxiv.org/abs/2309.04001) | 本文提出了一种新的多模态分割方法MMSFormer，该方法有效地融合四种不同模态的信息，并在MCubeS数据集上取得了显著的性能提升。 |
| [^45] | [Adapting Self-Supervised Representations to Multi-Domain Setups.](http://arxiv.org/abs/2309.03999) | 该论文提出了一种通用的、轻量级的领域解缠模块（DDM），可以在多个不同领域上进行有效的自监督表示学习，并显示出较高的线性探测准确率提升。 |
| [^46] | [ConDA: Contrastive Domain Adaptation for AI-generated Text Detection.](http://arxiv.org/abs/2309.03992) | 创新点：提出了一种基于对比域适应的框架 ConDA，用于检测由大型语言模型生成的新闻文本。这种方法解决了获取标记训练数据的困难，通过利用未标记的目标数据进行无监督域适应。 |
| [^47] | [Derivation of Coordinate Descent Algorithms from Optimal Control Theory.](http://arxiv.org/abs/2309.03990) | 本文从最优控制理论中导出了坐标下降算法，并证明了坐标下降算法的收敛性与李雅普诺夫函数的受控耗散有关。 |
| [^48] | [Noisy Computing of the $\mathsf{OR}$ and $\mathsf{MAX}$ Functions.](http://arxiv.org/abs/2309.03986) | 本研究考虑使用噪声查询计算$\mathsf{OR}$和$\mathsf{MAX}$函数，结果表明，在错误概率收敛时，计算这两个函数所需的期望查询数量与Kullback-Leibler差异之间有密切关系。 |
| [^49] | [LanSER: Language-Model Supported Speech Emotion Recognition.](http://arxiv.org/abs/2309.03978) | LanSER是一种基于语言模型的语音情绪识别方法，通过预先训练的大型语言模型进行弱监督学习，从而使得可以利用未标记数据。实验证明，使用这种弱监督训练的模型在标准SER数据集上表现优于其他基线模型，并且能够模拟语音的韵律内容。 |
| [^50] | [Automatic Concept Embedding Model (ACEM): No train-time concepts, No issue!.](http://arxiv.org/abs/2309.03970) | 这篇论文介绍了一种自动概念嵌入模型（ACEM），它可以解决概念注释对大型数据集的昂贵和不可行性的问题。 |
| [^51] | [Improving Resnet-9 Generalization Trained on Small Datasets.](http://arxiv.org/abs/2309.03965) | 本文提出了一种在小型数据集上改进ResNet-9的方法，通过应用一系列技术来提高其泛化性能，在不到10分钟的时间内，在CIFAR-10数据集的10%子集上达到了88%的准确率。 |
| [^52] | [REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation.](http://arxiv.org/abs/2309.03964) | 本文提出了一种改善单样本测试时自适应鲁棒性的方法，通过自适应学习和鲁棒损失函数的框架，解决了在线F-TTA中嘈杂样本导致的不稳定性问题。 |
| [^53] | [Large-Scale Automatic Audiobook Creation.](http://arxiv.org/abs/2309.03926) | 这项工作提出了一个可以自动生成高质量有声读物的系统，通过利用神经文本到语音技术，从在线电子书中创建数千本开放许可的有声读物。该系统允许用户自定义朗读速度和风格，并且提供了匹配所需声音的功能。该工作贡献了5000多本有声读物和一个交互式演示。 |
| [^54] | [Beyond attention: deriving biologically interpretable insights from weakly-supervised multiple-instance learning models.](http://arxiv.org/abs/2309.03925) | 本论文提出了一种后训练方法来分析多实例学习（MIL）模型，通过结合细化编码器产生的瓦片级别的注意力和预测得分，以及与细胞核分割掩模集成的预测-注意力加权（PAW）地图，实现了MIL模型的可解释性的提高。 |
| [^55] | [A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery.](http://arxiv.org/abs/2309.03919) | 提出了一种用于药物发现的混合量子-经典融合神经网络模型，通过优化的量子架构将3D和空间图卷积神经网络相互整合，提高了结合亲和力预测的准确性。 |
| [^56] | [A recommender for the management of chronic pain in patients undergoing spinal cord stimulation.](http://arxiv.org/abs/2309.03918) | 本文提出了一种推荐系统，用于管理进行SCS的慢性疼痛患者。通过使用上下文多臂赌博方法开发的系统，为患者推荐SCS设置，旨在改善其状况。这些推荐通过数字健康生态系统直接发送给患者，并与患者监测系统结合，为慢性疼痛患者的整个治疗过程提供闭环关怀。 |
| [^57] | [A Robust Adaptive Workload Orchestration in Pure Edge Computing.](http://arxiv.org/abs/2309.03913) | 本论文提出了一种稳健的纯边缘计算工作负载编排（R-AdWOrch）模型，通过使用优先级定义和重新分配策略，最小化截止时间的未达成以及低优先级任务的数据丢失。实验结果显示，R-AdWOrch能够在所有条件下尽量减少紧急任务的截止时间未达成和低优先级任务的数据丢失。 |
| [^58] | [DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs.](http://arxiv.org/abs/2309.03907) | 本研究开发了一个名为DrugChat的药物分子图聊天系统，该系统实现了类似ChatGPT的能力。通过上传化合物分子图并进行多轮互动，用户可以向DrugChat提问并获得回答，这可以加速药物发现、优化结构-活性关系、指导前导化合物优化等。系统由图神经网络、大型语言模型和适配器组成。 |
| [^59] | [A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes.](http://arxiv.org/abs/2309.03531) | 本文提出了一个鲁棒的部分领域适应（PDA）框架，通过整合鲁棒的目标监督策略，解决了负迁移问题，并在领域无关的方式下优化了类内紧凑性和类间分离性。通过预先推断源原型，确保了源数据的隐私性。实验证实了该框架的有效性。 |
| [^60] | [R2D2: Deep neural network series for near real-time high-dynamic range imaging in radio astronomy.](http://arxiv.org/abs/2309.03291) | R2D2是用于射电天文中高动态范围成像的深度神经网络系列，采用模型驱动方法和数据一致性更新，重建为一系列残差图像，可用于高分辨率强度成像。 |
| [^61] | [Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning.](http://arxiv.org/abs/2309.03157) | 本论文提出了一种通过深度强化学习学习充电来解决无人机覆盖路径规划问题的方法。该方法利用基于地图的观测信息，在整个任务周期内优化覆盖轨迹，并采用动作屏蔽和折扣因子调度等技术。实验结果表明，该方法优于基准启发式方法，在不同目标区域和地图上具有一定的泛化性能。 |
| [^62] | [Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach.](http://arxiv.org/abs/2309.02571) | 该论文提出了一种基于FFT的方法，用于恢复线性动态系统的因果结构。通过降低计算复杂度，可以有效地从时间序列中获取动态因果效应。 |
| [^63] | [TensorBank:Tensor Lakehouse for Foundation Model Training.](http://arxiv.org/abs/2309.02094) | TensorBank是一个基于Tensor的湖仓库，能够以高速从云对象存储流式传输张量到GPU内存，并通过使用分层统计指标进行查询加速。 |
| [^64] | [LoopTune: Optimizing Tensor Computations with Reinforcement Learning.](http://arxiv.org/abs/2309.01825) | LoopTune是一个使用强化学习优化张量计算的编译器，通过优化张量遍历顺序和使用代码生成器LoopNest执行硬件特定优化，LoopTune能够生成比其他编译器更快的代码。通过采用新的图形表示和动作空间，LoopTune比TVM快一个数量级，比MetaSchedule快2.8倍，比AutoTVM快1.08倍，并持续在与手工调优的库Numpy相当的水平上工作。此外，LoopTune优化代码的时间只需几秒钟。 |
| [^65] | [On the Robustness of Post-hoc GNN Explainers to Label Noise.](http://arxiv.org/abs/2309.01706) | 本文对后续GNN解释器在面对标签噪声时的鲁棒性进行了研究，并发现即使是轻微的标签噪声也会严重影响解释质量。 |
| [^66] | [Emergent Linear Representations in World Models of Self-Supervised Sequence Models.](http://arxiv.org/abs/2309.00941) | 本研究提供了自监督的序列模型中新型线性表示的证据，并表明通过探测棋盘的"我的颜色"与"对手的颜色"可以解释模型的内部状态。这种准确的内部表示理解使得我们能够通过简单的向量运算来控制模型的行为，并显著推进了可解释性的进展。 |
| [^67] | [A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications.](http://arxiv.org/abs/2308.16375) | 这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。 |
| [^68] | [Measurement Tampering Detection Benchmark.](http://arxiv.org/abs/2308.15605) | 本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。 |
| [^69] | [When Do Program-of-Thoughts Work for Reasoning?.](http://arxiv.org/abs/2308.15452) | 提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。 |
| [^70] | [Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation.](http://arxiv.org/abs/2308.15363) | 本文提出了一个大规模语言模型(LLMs)赋能的文本到SQL任务的基准评估，并基于实验结果提出了一种新的集成解决方案DAIL-SQL，刷新了Spider榜单并实现了86.6%的执行准确率。同时，强调了在提示工程中的词汇效率以实现高效经济的LLM-based文本到SQL解决方案，此外还对在上下文学习中应用开源LLMs进行了研究，并进行了任务特定的性能优化。 |
| [^71] | [A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data.](http://arxiv.org/abs/2308.13352) | 这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。 |
| [^72] | [Quantum-Inspired Machine Learning: a Survey.](http://arxiv.org/abs/2308.11269) | 量子启发式机器学习（QiML）是一个利用量子力学原理在经典计算框架中的新领域，在本调查中我们提供了对QiML的综合和全面的研究，展示了其多样化的研究领域、最新进展和实际应用，并为QiML建立了明确的定义。未来的研究将从量子力学、量子计算和经典机器学习中汲取经验，丰富QiML的领域。 |
| [^73] | [LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net.](http://arxiv.org/abs/2308.06603) | LadleNet是一种使用可扩展的两阶段U-Net将热红外图像转换为可见光图像的算法，通过引入跳跃连接和精细特征聚合技术，显著提高了模型性能。Handle模块构建抽象语义空间，Bowl模块解码该空间生成映射的VI图像。 |
| [^74] | [Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles.](http://arxiv.org/abs/2308.02603) | 这项研究提出了一种基于知识驱动的多智能体强化学习方法，用于解决物联网车辆中计算卸载的延迟问题。通过将计算任务卸载到边缘单元，可以减轻车载计算负担。该方法利用图神经网络结合领域知识，选择每辆车的最佳卸载选项。 |
| [^75] | [Accurate Neural Network Pruning Requires Rethinking Sparse Optimization.](http://arxiv.org/abs/2308.02060) | 这项工作研究了高稀疏对神经网络训练的影响，发现使用传统的密集训练策略进行稀疏训练效果不佳，提出了新的方法来解决这个问题，并在视觉和语言模型上都取得了最先进的结果。 |
| [^76] | [Kernelised Normalising Flows.](http://arxiv.org/abs/2307.14839) | 本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。 |
| [^77] | [An Empirical Evaluation of Temporal Graph Benchmark.](http://arxiv.org/abs/2307.12510) | 本文对Temporal Graph Benchmark进行了实证评估，通过扩展动态图库(DyGLib)到TGB，并使用十一种动态图学习方法进行全面比较。实验发现不同模型在不同数据集上表现出不同的性能，一些基线方法的性能可以通过使用DyGLib显著提高。 |
| [^78] | [Heterogeneous Federated Learning: State-of-the-art and Research Challenges.](http://arxiv.org/abs/2307.10616) | 异构联邦学习是联邦学习领域中的一个重要研究方向，涉及到数据分布、模型架构、网络环境和硬件设备的异质性挑战。本文对异构联邦学习的研究挑战和最新进展进行了综述和分类，为进一步的研究提供了参考。 |
| [^79] | [Identifying Interpretable Subspaces in Image Representations.](http://arxiv.org/abs/2307.10504) | FALCON是一个解释图像表示特征的框架，可以通过使用字幕数据集和视觉语言模型来生成人类可理解的概念，并通过对比解释消除虚假概念。在较大的空间中，特征通过研究组合可以更易解释和高阶评分概念的解释。 |
| [^80] | [Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization.](http://arxiv.org/abs/2307.05551) | 本研究提出了一种基于图神经网络的太赫兹流导向纳米尺度定位方法，可以提高定位精度和覆盖范围，解决了现有方法的定位精度低和无法全局定位的问题。 |
| [^81] | [$\mathbf{\mathbb{E}^{FWI}}$: Multi-parameter Benchmark Datasets for Elastic Full Waveform Inversion of Geophysical Properties.](http://arxiv.org/abs/2306.12386) | 本文介绍了一个专门为弹性全波形反演设计的全面基准数据集$\mathbf{\mathbb{E}^{FWI}}$，其包括8个不同的数据集，并提供了三种不同深度学习方法产生的基准结果。 |
| [^82] | [GPU-Accelerated Verification of Machine Learning Models for Power Systems.](http://arxiv.org/abs/2306.10617) | 本论文解决了GPU加速机器学习模型验证在电力系统问题上的应用障碍，通过同时验证多个问题，并引入精确转换方法，将一组潜在违规转化为增强原始神经网络能力的层。 |
| [^83] | [RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation.](http://arxiv.org/abs/2306.08947) | 本文提出了 RecFusion，一种特定针对1D和/或二进制设置的推荐模型方法，其利用了二项式扩散过程对二元用户-项目交互进行显式建模，并在核心推荐设置和最常见的数据集上接近复杂的VAE基线的表现。 |
| [^84] | [Off-policy Evaluation in Doubly Inhomogeneous Environments.](http://arxiv.org/abs/2306.08719) | 本研究在双非均匀环境下研究了离线策略评估(OPE)，提出了一种潜在因子模型用于奖励和观测转移函数，并开发了一个通用的OPE框架。该研究对标准RL假设不满足的环境中的OPE做出了理论贡献，并提供了几种实用的方法。 |
| [^85] | [Avoid Adversarial Adaption in Federated Learning by Multi-Metric Investigations.](http://arxiv.org/abs/2306.03600) | 本文提出了一个新概念——强适应对手，并通过实验表明，现有的防御方法不足以解决现实世界中的对手和数据分布问题。作者使用多度量调查来增强 FL 对这些对手的抵抗力。 |
| [^86] | [Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification.](http://arxiv.org/abs/2306.01726) | 本文提出了两种代数评估器来估计未标记数据中有噪声二元分类器的性能。其中，第二种评估器的正确性被保证。作者通过利用独立评估器无法返回合理估计的失败，缓解了委托/代理监控悖论，并通过搜索来寻找几乎无误差的三元组。 |
| [^87] | [Physics-informed machine learning of redox flow battery based on a two-dimensional unit cell model.](http://arxiv.org/abs/2306.01010) | 本文提出了一种利用二维数学模型进行物理启发式神经网络预测铁钒液流电池性能的方法，可正确预测单元电压。 |
| [^88] | [Improving Expressivity of Graph Neural Networks using Localization.](http://arxiv.org/abs/2305.19659) | 本文提出了Weisfeiler-Leman (WL)算法的局部版本，用于解决子图计数问题并提高图神经网络的表达能力，同时，也给出了一些时间和空间效率更高的$k-$WL变体和分裂技术。 |
| [^89] | [Photo-zSNthesis: Converting Type Ia Supernova Lightcurves to Redshift Estimates via Deep Learning.](http://arxiv.org/abs/2305.11869) | 本研究提出了一种基于卷积神经网络的方法"Photo-zSNthesis"，用于从多波段超新星光变曲线预测完整的红移概率分布，无需光谱信息。该方法在模拟和真实观测中都取得了重大提升，能极大地约束宇宙学。 |
| [^90] | [EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer.](http://arxiv.org/abs/2305.10502) | 本文提出了一种名为EENED的端到端神经元癫痫检测模型，结合了CNN和Transformer，能够同时捕捉EEG信号的全局依赖性和局部特征，并有望提高癫痫检测的准确性和可靠性。 |
| [^91] | [Verifiable Learning for Robust Tree Ensembles.](http://arxiv.org/abs/2305.03626) | 本论文提出了一种可验证学习的方法，即训练易于验证的限制模型类来解决决策树集成的 NP-hard 问题，并成功设计出一种新的训练算法，使得在多项式时间内可以进行安全验证，而且仍保持着该领域最好的鲁棒性能。 |
| [^92] | [Representation Learning via Manifold Flattening and Reconstruction.](http://arxiv.org/abs/2305.01777) | 本文提出了一种通过神经网络构建展平流形的算法FlatNet，具有可解释性和可扩展性，同时在测试数据上具有良好的泛化性能。 |
| [^93] | [Latent Fingerprint Recognition: Fusion of Local and Global Embeddings.](http://arxiv.org/abs/2304.13800) | 本文采用全局嵌入与局部嵌入相结合的方法，在提高识别准确率的同时，保证了较高的吞吐量和应用性。 |
| [^94] | [On the Prime Number Divisibility by Deep Learning.](http://arxiv.org/abs/2304.01333) | 本文提出了使用深度学习判断质数整除性的方法，并发现关键在于提供给深度学习模型的特征空间。此外，商业可用的自动化机器学习管道无法解决此问题，需要提供适当的特征工程来解决。研究者还提出了一个封闭式解决方案。 |
| [^95] | [Soft-Bellman Equilibrium in Affine Markov Games: Forward Solutions and Inverse Learning.](http://arxiv.org/abs/2304.00163) | 本文提出了一种新的解决方案概念——软Bellman平衡，解决了仿射马尔科夫博弈中的多个玩家交互问题，并提出了一种非线性最小二乘算法来计算此平衡，同时通过投影梯度算法解决推断玩家奖励参数的问题。 |
| [^96] | [Driver Profiling and Bayesian Workload Estimation Using Naturalistic Peripheral Detection Study Data.](http://arxiv.org/abs/2303.14720) | 本研究通过自然主义周边感知研究数据，提出了一种驾驶员特征识别和贝叶斯工作负荷预测的方法，通过对驾驶性能数据进行监测和分析，实现了驾驶员工作负荷的估计。 |
| [^97] | [Self-supervised based general laboratory progress pretrained model for cardiovascular event detection.](http://arxiv.org/abs/2303.06980) | 研究利用自监督学习和迁移学习，将心血管实验室指标的患者进展趋势从常见情况转移到罕见或特定心血管事件检测中，以协助检测经皮冠状动脉介入治疗患者的靶血管重建。 |
| [^98] | [Scalable precision wide-field imaging in radio interferometry: II. AIRI validated on ASKAP data.](http://arxiv.org/abs/2302.14149) | 本研究验证了AIRI算法在ASKAP数据上的有效性，通过使用训练好的深度神经网络进行正则化步骤中的去噪处理，进一步提高了图像动态范围。 |
| [^99] | [Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach.](http://arxiv.org/abs/2302.10798) | 本论文提出了一种轻量化参数裁剪策略来实现节能深度学习，可以发现最佳的静态子网络，包含二值门控模块和新颖的损失函数，适用于各种神经网络，同时在已有的基准测试中取得了竞争性结果。 |
| [^100] | [Two-step hyperparameter optimization method: Accelerating hyperparameter search by using a fraction of a training dataset.](http://arxiv.org/abs/2302.03845) | 这项研究提出了一种两步HPO方法，通过在训练数据集的一部分上进行预评估，然后在整个训练数据集上重新训练和评估，实现了加速超参数搜索的目的 |
| [^101] | [Node Injection for Class-specific Network Poisoning.](http://arxiv.org/abs/2301.12277) | 本文介绍了一种节点注入攻击的方法，针对图中的特定节点进行误分类，通过伪装成良性节点的方式进行注入。这种攻击方法可以破坏基于GNN的节点分类器的性能。 |
| [^102] | [Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids.](http://arxiv.org/abs/2209.12693) | 本文提出了两个基于宽带电力线通信测量的首个数据集FiN-1和FiN-2，共收集了130亿个数据点。这些数据可以应用于资产管理、电网状态可视化和预测，以解决电力网在向可再生能源过渡和复杂负载配置的情况下面临的新挑战。 |
| [^103] | [Information Processing Equalities and the Information-Risk Bridge.](http://arxiv.org/abs/2207.11987) | 本论文引入了两类新的信息度量方法，扩展和统一了不同分布之间的散度，通过信息度量和贝叶斯风险之间的几何关系以及信息处理等式，揭示了在经典风险最小化中选择假设类的重要性。 |
| [^104] | [First AI for deep super-resolution wide-field imaging in radio astronomy: unveiling structure in ESO 137--006.](http://arxiv.org/abs/2207.11336) | 这项研究介绍了首个基于人工智能的深度超分辨率广场射电干涉成像框架，并在ESO 137-006射电星系的观测中进行了演示。该框架通过“插入和播放”方案解决图像重建的反问题，实现了高分辨率图像的恢复和自适应去噪，可以应用于大数据和图像尺寸，并且实现了并行算法以提高效率。 |
| [^105] | [Network Revenue Management with Demand Learning and Fair Resource-Consumption Balancing.](http://arxiv.org/abs/2207.11159) | 本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。提出了一种基于UCB需求学习方法的原始对偶在线策略来最大化规范化收益。 |
| [^106] | [Generating and Detecting True Ambiguity: A Forgotten Danger in DNN Supervision Testing.](http://arxiv.org/abs/2207.10495) | 本文提出了一种新的方法来生成和检测真正歧义的测试输入，以解决现有测试生成器只能生成超出分布输入的问题。 |
| [^107] | [TREE-G: Decision Trees Contesting Graph Neural Networks.](http://arxiv.org/abs/2207.02760) | TREE-G引入了一种专门针对图数据的修改决策树方法，通过新颖的分裂函数和指针机制，使得决策树能够更好地应用于带有拓扑信息的图结构化数据。 |
| [^108] | [Dawn of the transformer era in speech emotion recognition: closing the valence gap.](http://arxiv.org/abs/2203.07378) | 本文通过对几种预训练变体的分析，发现在语音情绪识别领域，使用Transformer架构能够在没有使用显式语言信息的情况下获得最佳的价值预测性能，相关性系数为0.638。 |
| [^109] | [Multi-task UNet architecture for end-to-end autonomous driving.](http://arxiv.org/abs/2112.08967) | 我们提出了一种端到端驾驶模型，使用多任务UNet架构和控制算法，能够同时进行车道分割、路径预测和车辆控制。与增强学习模型相比，在弯曲道路上的性能相当，但具有端到端的优势。 |
| [^110] | [Frequentist Regret Bounds for Randomized Least-Squares Value Iteration.](http://arxiv.org/abs/1911.00567) | 本文引入了一种改进版本的随机最小二乘值迭代（RLSVI）算法，通过对行动值函数的最小二乘逼近进行扰动，诱导出了探索过程。在马尔可夫决策过程具有低秩转移动态的假设下，我们证明了RLSVI的频率后悔上界为$\widetilde O(d^2 H^2 \sqrt{T})$。这是对于带有函数逼近的随机探索的首个频率后悔分析。 |

# 详细

[^1]: 关于结果预测的可行性

    On the Actionability of Outcome Prediction. (arXiv:2309.04470v1 [cs.LG])

    [http://arxiv.org/abs/2309.04470](http://arxiv.org/abs/2309.04470)

    提高结果预测准确性有助于确定最适合的干预措施，但在存在多种可能干预的情况下，仅仅依赖结果预测进行干预可能无法达到预期效果。

    

    在社会影响领域，预测未来结果是机器学习的常见应用。例子从预测教育中学生的成功到预测医疗保健中的疾病风险。从实践者来看，最终目标不仅仅是预测，而是有效地行动。越来越多的证据表明，仅仅依靠结果预测进行下游干预可能不会产生预期结果。在大多数领域中，每个个体存在多种可能的干预措施，这使得采取有效行动的挑战更加严峻。即使连接个体潜在状态与结果的因果机制已经很好地理解，在任何给定的实例（特定的学生或患者）中，实践者仍然需要从预算测量的潜在状态中推断出哪种可能的干预措施对这个个体最有效。考虑到这一点，我们想问：准确的结果预测何时有助于识别最合适的干预措施？

    Predicting future outcomes is a prevalent application of machine learning in social impact domains. Examples range from predicting student success in education to predicting disease risk in healthcare. Practitioners recognize that the ultimate goal is not just to predict but to act effectively. Increasing evidence suggests that relying on outcome predictions for downstream interventions may not have desired results.  In most domains there exists a multitude of possible interventions for each individual, making the challenge of taking effective action more acute. Even when causal mechanisms connecting the individual's latent states to outcomes is well understood, in any given instance (a specific student or patient), practitioners still need to infer -- from budgeted measurements of latent states -- which of many possible interventions will be most effective for this individual. With this in mind, we ask: when are accurate predictors of outcomes helpful for identifying the most suitable
    
[^2]: 测量和改进视觉-语言模型中的思维链推理

    Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models. (arXiv:2309.04461v1 [cs.CL])

    [http://arxiv.org/abs/2309.04461](http://arxiv.org/abs/2309.04461)

    本研究探索了视觉-语言模型展示人类推理能力的能力，并提出了一种基于思维链的一致性度量。通过一个流水线和已有数据集，建立了一个基准来测量这种推理能力。

    

    最近，视觉-语言模型（VLMs）作为能解析关于视觉内容的自然查询并生成类人输出的视觉助手，展示出了强大的功效。在这项工作中，我们探索了这些模型展示基于所感知信息的类人推理的能力。为了解决关于它们的推理能力到底有多一致和有多基于实际的一个重要疑虑，我们还测量了这些模型的推理一致性。我们通过提出一种基于思维链（CoT）的一致性度量来实现这一目标。然而，这样的评估需要涵盖高层次推理和细节推理链的基准，这是一项昂贵的任务。我们通过提出LLM-Human-in-the-Loop流水线来应对这一挑战，该流水线显著降低了成本，同时确保生成高质量的数据集。基于这个流水线和现有的粗粒度注释数据集，我们构建了CURE基准来同时测量两者。

    Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both 
    
[^3]: 子词作为技巧：稀疏奖励强化学习的分词化

    Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning. (arXiv:2309.04459v1 [cs.LG])

    [http://arxiv.org/abs/2309.04459](http://arxiv.org/abs/2309.04459)

    通过将行动空间离散化并采用分词技术，我们提出了一种在稀疏奖励强化学习中生成技巧的新方法。这种方法能够减少探索的难度，并在连续行动空间中达到良好的性能。

    

    稀疏奖励强化学习中的探索具有困难，因为需要通过长期的、协调的行动序列才能获得任何奖励。而且，在连续的行动空间中，可能的行动数量是无穷多的，这只会增加探索的难度。为了解决这些问题，一类方法通过在同一领域收集的交互数据中形成时间上延伸的行动，通常称为技巧，并在这个新的行动空间上进行策略的优化。通常这样的方法在连续行动空间中需要一个漫长的预训练阶段，在强化学习开始之前形成技巧。鉴于先前的证据表明在这些任务中并不需要完整的连续行动空间，我们提出了一种新颖的技巧生成方法，包括两个组成部分。首先，我们通过聚类将行动空间离散化，然后我们利用从自然语言处理借鉴来的分词技术。

    Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language pro
    
[^4]: 使用置换不变神经网络对集合天气预测进行后处理

    Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks. (arXiv:2309.04452v1 [stat.ML])

    [http://arxiv.org/abs/2309.04452](http://arxiv.org/abs/2309.04452)

    本研究使用置换不变神经网络对集合天气预测进行后处理，不同于之前的方法，该网络将预测集合视为一组无序的成员预测，并学习对成员顺序的排列置换具有不变性的链接函数。在地表温度和风速预测的案例研究中，我们展示了最先进的预测质量。

    

    统计后处理用于将原始数值天气预报的集合转化为可靠的概率预测分布。本研究中，我们考察了使用置换不变神经网络进行这一任务的方法。与以往的方法不同，通常基于集合概要统计信息并忽略集合分布的细节，我们提出的网络将预测集合视为一组无序的成员预测，并学习对成员顺序的排列置换具有不变性的链接函数。我们通过校准度和锐度评估所获得的预测分布的质量，并将模型与经典的基准方法和基于神经网络的方法进行比较。通过处理地表温度和风速预测的案例研究，我们展示了最先进的预测质量。为了加深对学习推理过程的理解，我们进一步提出了基于置换的重要性评估方法。

    Statistical postprocessing is used to translate ensembles of raw numerical weather forecasts into reliable probabilistic forecast distributions. In this study, we examine the use of permutation-invariant neural networks for this task. In contrast to previous approaches, which often operate on ensemble summary statistics and dismiss details of the ensemble distribution, we propose networks which treat forecast ensembles as a set of unordered member forecasts and learn link functions that are by design invariant to permutations of the member ordering. We evaluate the quality of the obtained forecast distributions in terms of calibration and sharpness, and compare the models against classical and neural network-based benchmark methods. In case studies addressing the postprocessing of surface temperature and wind gust forecasts, we demonstrate state-of-the-art prediction quality. To deepen the understanding of the learned inference process, we further propose a permutation-based importance
    
[^5]: 正规化流的变化和放松

    Variations and Relaxations of Normalizing Flows. (arXiv:2309.04433v1 [cs.LG])

    [http://arxiv.org/abs/2309.04433](http://arxiv.org/abs/2309.04433)

    正规化流是一种模型，采用一系列双射变换将复杂目标分布表示为简单基础分布。然而，由于对微分同胚的限制，其在有效表示复杂拓扑目标分布和处理先验分布与目标分布不同胚的情况方面存在局限性。相关研究通过结合其他生成模型的特点，放宽了正规化流的约束性，以平衡约束和灵活性之间的关系

    

    正规化流（NFs）描述了一类将复杂目标分布表示为简单基础分布的一系列双射变换的模型。通过将候选变换空间限制为微分同胚，NFs可以高效地进行精确采样和密度评估，使其既能灵活地作为判别模型，又能作为生成模型。然而，它们对微分同胚的限制强制输入、输出和所有中间空间具有相同的维数，限制了它们有效表示具有复杂拓扑的目标分布的能力。此外，在先验分布和目标分布不同胚的情况下，正规化流可能会将质量泄漏到目标支持之外。本综述涵盖了一系列最近的工作，将其他生成模型类别（如VAEs和基于得分的扩散）的方面结合起来，从而放宽了NFs的严格双射约束，以实现平衡约束与灵活性之间的折中

    Normalizing Flows (NFs) describe a class of models that express a complex target distribution as the composition of a series of bijective transformations over a simpler base distribution. By limiting the space of candidate transformations to diffeomorphisms, NFs enjoy efficient, exact sampling and density evaluation, enabling NFs to flexibly behave as both discriminative and generative models. Their restriction to diffeomorphisms, however, enforces that input, output and all intermediary spaces share the same dimension, limiting their ability to effectively represent target distributions with complex topologies. Additionally, in cases where the prior and target distributions are not homeomorphic, Normalizing Flows can leak mass outside of the support of the target. This survey covers a selection of recent works that combine aspects of other generative model classes, such as VAEs and score-based diffusion, and in doing so loosen the strict bijectivity constraints of NFs to achieve a bal
    
[^6]: 使用熵正则化的软量化问题

    Soft Quantization using Entropic Regularization. (arXiv:2309.04428v1 [math.PR])

    [http://arxiv.org/abs/2309.04428](http://arxiv.org/abs/2309.04428)

    本研究通过使用熵正则化和软最小函数来解决量化问题，提出了一种新的近似技术，并使用随机梯度方法获得最优解。该方法具有调节优化问题难度的控制参数，可在处理具有挑战性问题时提供显著优势。

    

    量化问题旨在通过有限的离散测度找到在${\mathbb{R}}^d$上概率测度的最佳近似。Wasserstein距离是衡量近似质量的典型选择。本研究调查了熵正则化量化问题的性质和鲁棒性，该方法放松了标准的量化问题。提出的近似技术自然地采用了软最小函数，该函数因其在理论和实践的可靠性方面而闻名。此外，我们使用熵正则化的Wasserstein距离来评估软量化问题近似的质量，并实现了一种随机梯度方法来获得最优解。我们提出的方法中的控制参数可以调整优化问题的难度级别，在处理异常挑战性问题时具有显著优势。

    The quantization problem aims to find the best possible approximation of probability measures on ${\mathbb{R}}^d$ using finite, discrete measures. The Wasserstein distance is a typical choice to measure the quality of the approximation. This contribution investigates the properties and robustness of the entropy-regularized quantization problem, which relaxes the standard quantization problem. The proposed approximation technique naturally adopts the softmin function, which is well known for its robustness in terms of theoretical and practicability standpoints. Moreover, we use the entropy-regularized Wasserstein distance to evaluate the quality of the soft quantization problem's approximation, and we implement a stochastic gradient approach to achieve the optimal solutions. The control parameter in our proposed method allows for the adjustment of the optimization problem's difficulty level, providing significant advantages when dealing with exceptionally challenging problems of interes
    
[^7]: 鲁棒的表示学习用于隐私保护机器学习：一种多目标自编码器方法

    Robust Representation Learning for Privacy-Preserving Machine Learning: A Multi-Objective Autoencoder Approach. (arXiv:2309.04427v1 [cs.LG])

    [http://arxiv.org/abs/2309.04427](http://arxiv.org/abs/2309.04427)

    本论文介绍了一种多目标自编码器方法，利用鲁棒的表示学习来编码数据以进行隐私保护机器学习。该方法解决了传统技术中性能降低的问题，并在保护隐私的同时优化了模型的实用性。

    

    在各个领域中，机器学习在应用中越来越重要。对数据的重度依赖导致了各种数据伦理和隐私法规的出现，并且对隐私保护机器学习（ppML）的需求日益增长。当前的ppML技术主要采用基于密码学的方法，如同态加密，或者通过在输入中引入噪声，如差分隐私。对这些技术的主要批评是它们要么过于缓慢，要么在改善机器学习模型性能的同时降低了机密性。为了解决性能降低的问题，我们利用鲁棒的表示学习来编码数据，同时优化隐私效用的权衡。我们的方法主要是通过以多目标的方式训练自编码器，然后将编码部分的潜在特征和学习特征连接起来，作为我们数据的编码形式。

    Several domains increasingly rely on machine learning in their applications. The resulting heavy dependence on data has led to the emergence of various laws and regulations around data ethics and privacy and growing awareness of the need for privacy-preserving machine learning (ppML). Current ppML techniques utilize methods that are either purely based on cryptography, such as homomorphic encryption, or that introduce noise into the input, such as differential privacy. The main criticism given to those techniques is the fact that they either are too slow or they trade off a model s performance for improved confidentiality. To address this performance reduction, we aim to leverage robust representation learning as a way of encoding our data while optimizing the privacy-utility trade-off. Our method centers on training autoencoders in a multi-objective manner and then concatenating the latent and learned features from the encoding part as the encoded form of our data. Such a deep learnin
    
[^8]: 并行且有限数据的声音转换使用随机变分深度核学习

    Parallel and Limited Data Voice Conversion Using Stochastic Variational Deep Kernel Learning. (arXiv:2309.04420v1 [cs.SD])

    [http://arxiv.org/abs/2309.04420](http://arxiv.org/abs/2309.04420)

    该论文提出了一种基于随机变分深度核学习（SVDKL）的声音转换方法，能够适应有限数据，并利用深度神经网络和高斯过程的灵活性进行模型训练和复杂函数估计。

    

    通常，声音转换被视为一个拥有有限训练数据的工程问题。对于大规模数据的依赖限制了深度学习方法在实际应用上的可行性，而这些方法在最近几年得到了广泛研究。另一方面，统计方法在有限数据上是有效的，但在建模复杂映射函数时存在困难。本文提出一种基于随机变分深度核学习（SVDKL）的能够适应有限数据的声音转换方法。同时，SVDKL既能利用深度神经网络的表达能力，又能利用高灵活性的高斯过程作为贝叶斯和非参数方法。当将传统核函数与深度神经网络结合时，可以估计非光滑且更复杂的函数。此外，模型的稀疏变分高斯过程解决了可扩展性问题，并且与精确高斯过程不同，可以进行近似计算。

    Typically, voice conversion is regarded as an engineering problem with limited training data. The reliance on massive amounts of data hinders the practical applicability of deep learning approaches, which have been extensively researched in recent years. On the other hand, statistical methods are effective with limited data but have difficulties in modelling complex mapping functions. This paper proposes a voice conversion method that works with limited data and is based on stochastic variational deep kernel learning (SVDKL). At the same time, SVDKL enables the use of deep neural networks' expressive capability as well as the high flexibility of the Gaussian process as a Bayesian and non-parametric method. When the conventional kernel is combined with the deep neural network, it is possible to estimate non-smooth and more complex functions. Furthermore, the model's sparse variational Gaussian process solves the scalability problem and, unlike the exact Gaussian process, allows for the 
    
[^9]: 在物理系统中的紧急学习作为基于反馈的玻璃景观中的老化过程

    Emergent learning in physical systems as feedback-based aging in a glassy landscape. (arXiv:2309.04382v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2309.04382](http://arxiv.org/abs/2309.04382)

    通过训练线性物理网络学习线性变换，我们发现其学习行为与无序和玻璃状系统中的老化和记忆形成过程相似。学习动态类似于老化过程，通过重复施加反馈边界力来放松并编码输入-输出关系的记忆。

    

    通过训练线性物理网络来学习线性变换，我们发现它们的物理特性是由权重更新规则导致的。我们的研究结果突出了这样的网络学习行为与无序和玻璃状系统中老化和记忆形成过程之间的惊人相似之处。我们展示了学习动态类似于老化过程，系统在面对输入力的反馈边界力的重复施加时放松并编码了输入-输出关系的记忆。随着这种放松，相关长度增加，这可以通过网络的分量的两点相关函数来指示。我们还观察到误差均方根的平方根作为时间的函数呈非指数形式，这是玻璃系统的典型特征。这种物理解释表明通过将更详细的信息编码到输入和反馈边界中。

    By training linear physical networks to learn linear transformations, we discern how their physical properties evolve due to weight update rules. Our findings highlight a striking similarity between the learning behaviors of such networks and the processes of aging and memory formation in disordered and glassy systems. We show that the learning dynamics resembles an aging process, where the system relaxes in response to repeated application of the feedback boundary forces in presence of an input force, thus encoding a memory of the input-output relationship. With this relaxation comes an increase in the correlation length, which is indicated by the two-point correlation function for the components of the network. We also observe that the square root of the mean-squared error as a function of epoch takes on a non-exponential form, which is a typical feature of glassy systems. This physical interpretation suggests that by encoding more detailed information into input and feedback boundar
    
[^10]: 一般化界限：信息论和PAC-Bayesian的视角

    Generalization Bounds: Perspectives from Information Theory and PAC-Bayes. (arXiv:2309.04381v1 [cs.LG])

    [http://arxiv.org/abs/2309.04381](http://arxiv.org/abs/2309.04381)

    该论文介绍了一般化界限的两个视角：信息论和PAC-Bayesian，并探讨了它们之间的联系和共同点。这对于理论机器学习的进一步发展和新算法的设计具有重要意义。

    

    在理论机器学习中，一个基本问题是一般化。在过去的几十年里，PAC-Bayesian方法已经被确定为一个灵活的框架，用来解决机器学习算法的一般化能力，并设计新的算法。最近，由于其对多种学习算法（包括深度神经网络）的潜在适用性，它引起了越来越多的关注。与此同时，还发展了一种信息论的视角，其中建立了一般化与各种信息度量之间的关系。这个框架与PAC-Bayesian方法密切相关，并且在两个方面都有独立发现的很多结果。在本文中，我们强调这种强连接，并提出一种统一的一般化处理方法。我们介绍了两个视角共同拥有的技术和结果，并讨论了不同的方法和解释。特别是，我们展示了这种连接如何产生新的洞见和理论的发展，并展示了这两个领域的交叉应用和潜在的进一步研究方向。

    A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of generalization. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demons
    
[^11]: 带有力响应式运动控制的导盲四足机器人导航

    Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control. (arXiv:2309.04370v1 [cs.RO])

    [http://arxiv.org/abs/2309.04370](http://arxiv.org/abs/2309.04370)

    本研究提出了一种带有力响应式运动控制的导盲四足机器人导航系统，通过同时训练运动控制器和外力估计器，实现对外部拉扯力的稳健感知和响应。实验结果表明该系统能够准确导航并绕过障碍物，具有较强的鲁棒性。

    

    导盲机器人是为导引视障人士而设计的非常有用的工具，鉴于真正导盲犬的可获得性低且高成本，其可能产生巨大的社会影响。尽管已经演示了几个导盲机器人系统，但没有考虑到导盲犬实际环境中经常发生的来自人类的外部拉扯。本文通过强化学习（RL）同时训练了一个对外部拉扯力具有鲁棒性的运动控制器和一个通过监督学习的外力估计器。控制器确保了稳定的行走，外力估计器使机器人能够对人类施加的外部力做出响应。这些力用于将机器人引导到未知的全局目标，同时机器人通过局部规划器将人类引导绕过附近的障碍物。在模拟和硬件上的实验结果表明，我们的控制器对外部力具有鲁棒性，并且我们的导盲系统能够准确检测...

    Seeing-eye robots are very useful tools for guiding visually impaired people, potentially producing a huge societal impact given the low availability and high cost of real guide dogs. Although a few seeing-eye robot systems have already been demonstrated, none considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect
    
[^12]: 为分类2D基于网格的关卡可完成性提出的主动学习方法

    Active Learning for Classifying 2D Grid-Based Level Completability. (arXiv:2309.04367v1 [cs.LG])

    [http://arxiv.org/abs/2309.04367](http://arxiv.org/abs/2309.04367)

    本文提出了使用主动学习方法来学习2D基于网格的关卡可完成性分类，通过对Super Mario Bros.、Kid Icarus和一个类似Zelda的游戏生成的关卡进行标记，获得了更好的分类器性能。

    

    确定由程序生成器生成的关卡的可完成性是一项具有挑战性的任务，因为这可能涉及使用求解器代理来分析和解决关卡，而这通常需要大量的时间。尽管主动学习已经成功地应用于自然语言处理、图像和语音识别以及计算机视觉领域，但在游戏评估中，它还没有被广泛采用，尤其是在标记数据有限或昂贵的情况下。在本文中，我们提出了使用主动学习来学习关卡可完成性分类。通过主动学习的方法，我们训练深度学习模型来对Super Mario Bros.、Kid Icarus和一个类似Zelda的游戏中生成的关卡进行可完成性分类。我们将使用主动学习方法标记关卡与使用随机查询标记关卡进行了比较。我们的结果表明，使用主动学习方法来标记关卡可以获得更好的分类器性能，而查询方法相同。

    Determining the completability of levels generated by procedural generators such as machine learning models can be challenging, as it can involve the use of solver agents that often require a significant amount of time to analyze and solve levels. Active learning is not yet widely adopted in game evaluations, although it has been used successfully in natural language processing, image and speech recognition, and computer vision, where the availability of labeled data is limited or expensive. In this paper, we propose the use of active learning for learning level completability classification. Through an active learning approach, we train deep-learning models to classify the completability of generated levels for Super Mario Bros., Kid Icarus, and a Zelda-like game. We compare active learning for querying levels to label with completability against random queries. Our results show using an active learning approach to label levels results in better classifier performance with the same am
    
[^13]: 从功率信号中学习：电力传输系统中电力干扰识别的自动化方法

    Learning from Power Signals: An Automated Approach to Electrical Disturbance Identification Within a Power Transmission System. (arXiv:2309.04361v1 [cs.LG])

    [http://arxiv.org/abs/2309.04361](http://arxiv.org/abs/2309.04361)

    本文提出了一种自动化方法，利用基于规则的分析来识别电力传输系统内记录的干扰事件。经过测试，在160个信号文件上，准确率达到了99％。

    

    随着电力质量在电力工业中的重要性日益提高，干扰事件数据的数量不断增长。电力公司没有足够的人员手动分析每个事件。本文提出了一种自动化方法，用于分析在电力传输系统内由数字故障录波器和电力质量监测仪记录的电力质量事件。该自动化方法利用基于规则的分析方法来检查电压和电流信号的时域和频域特征。可定制的阈值被设置用来对每个干扰事件进行分类。本文分析的事件包括各种故障、电动机起动和仪器互感器潜在故障。已经开发了14种不同事件类型的分析方法。对160个信号文件进行测试，准确率达到99％。采用一种称为循环直方图的方法对连续的、正常的信号数据进行分析。

    As power quality becomes a higher priority in the electric utility industry, the amount of disturbance event data continues to grow. Utilities do not have the required personnel to analyze each event by hand. This work presents an automated approach for analyzing power quality events recorded by digital fault recorders and power quality monitors operating within a power transmission system. The automated approach leverages rule-based analytics to examine the time and frequency domain characteristics of the voltage and current signals. Customizable thresholds are set to categorize each disturbance event. The events analyzed within this work include various faults, motor starting, and incipient instrument transformer failure. Analytics for fourteen different event types have been developed. The analytics were tested on 160 signal files and yielded an accuracy of ninety-nine percent. Continuous, nominal signal data analysis is performed using an approach coined as the cyclic histogram. Th
    
[^14]: 值压缩的稀疏列（VCSC）：冗余数据的稀疏矩阵存储

    Value-Compressed Sparse Column (VCSC): Sparse Matrix Storage for Redundant Data. (arXiv:2309.04355v1 [cs.DS])

    [http://arxiv.org/abs/2309.04355](http://arxiv.org/abs/2309.04355)

    值压缩的稀疏列（VCSC）是一种新的稀疏矩阵存储格式，能够利用高冗余性将数据进一步压缩，并在性能上没有显著的负面影响。通过增量编码和字节打包压缩索引数组，IVCSC实现了更大的存储空间节省。

    

    压缩的稀疏列（CSC）和坐标（COO）是稀疏矩阵的常用压缩格式。然而，CSC和COO都是通用格式，不能利用除稀疏性以外的数据特性，如数据冗余性。高度冗余的稀疏数据在许多机器学习应用中很常见，例如基因组学，在传统的稀疏存储格式下，这些数据通常太大无法进行内存计算。本文中，我们提出了两个扩展的CSC格式：值压缩的稀疏列（VCSC）和索引和值压缩的稀疏列（IVCSC）。VCSC利用列内的高冗余性，将数据进一步压缩了3倍以上，相比COO压缩了2.25倍，而性能特征没有显著的负面影响。IVCSC通过增量编码和字节打包压缩索引数组，使内存使用量比COO减少了10倍，比CSC减少了7.5倍。

    Compressed Sparse Column (CSC) and Coordinate (COO) are popular compression formats for sparse matrices. However, both CSC and COO are general purpose and cannot take advantage of any of the properties of the data other than sparsity, such as data redundancy. Highly redundant sparse data is common in many machine learning applications, such as genomics, and is often too large for in-core computation using conventional sparse storage formats. In this paper, we present two extensions to CSC: (1) Value-Compressed Sparse Column (VCSC) and (2) Index- and Value-Compressed Sparse Column (IVCSC). VCSC takes advantage of high redundancy within a column to further compress data up to 3-fold over COO and 2.25-fold over CSC, without significant negative impact to performance characteristics. IVCSC extends VCSC by compressing index arrays through delta encoding and byte-packing, achieving a 10-fold decrease in memory usage over COO and 7.5-fold decrease over CSC. Our benchmarks on simulated and rea
    
[^15]: 移动V-MoEs：通过稀疏MoEs缩小视觉变换器的规模

    Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts. (arXiv:2309.04354v1 [cs.CV])

    [http://arxiv.org/abs/2309.04354](http://arxiv.org/abs/2309.04354)

    本论文研究通过使用稀疏MoEs来缩小视觉变换器（ViTs）的规模，以提高在资源受限的视觉应用中的性能和效率。

    

    最近，稀疏的专家混合模型（MoEs）因其能够通过仅激活给定输入令牌的模型参数的一小部分而将模型规模与推理效率分离而受到关注。因此，稀疏的MoEs在自然语言处理和计算机视觉等领域取得了巨大的成功。在这项工作中，我们研究了使用稀疏MoEs来缩小视觉变换器（ViTs）的规模，以使其在资源受限的视觉应用中更具吸引力。为此，我们提出了一种简化和适用于移动设备的MoE设计，其中整个图像而不是单个补丁被路由到专家。我们还提出了一种稳定的MoE训练过程，使用超类信息来引导路由器。实验证明，我们的稀疏移动视觉MoEs（V-MoEs）可以在性能和效率之间达到更好的折衷。例如，对于...

    Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due to their ability to decouple model size from inference efficiency by only activating a small subset of the model parameters for any given input token. As such, sparse MoEs have enabled unprecedented scalability, resulting in tremendous successes across domains such as natural language processing and computer vision. In this work, we instead explore the use of sparse MoEs to scale-down Vision Transformers (ViTs) to make them more attractive for resource-constrained vision applications. To this end, we propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the
    
[^16]: 使用基础模型对零样本模型进行零样本强化

    Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])

    [http://arxiv.org/abs/2309.04344](http://arxiv.org/abs/2309.04344)

    提出了一种零样本强化方法RoboShot，通过使用零样本语言模型从任务描述中获取有用的见解，并应用于预训练模型嵌入中以去除有害成分并增强有用成分，从而改善预训练模型的鲁棒性。

    

    零样本推断是一种强大的范式，可以在没有进一步训练的情况下使用预训练模型来进行下游分类任务。然而，这些模型容易受到继承的偏见的影响，从而影响它们的性能。传统的解决方案是微调，但这削弱了预训练模型的主要优势，即可以直接使用的能力。我们提出了RoboShot，一种完全零样本的方法，可以改善预训练模型嵌入的鲁棒性。首先，我们使用零样本语言模型（LMs）从任务描述中获取有用的见解。这些见解被嵌入并用于去除嵌入中的有害成分并增强有用成分--而无需任何监督。从理论上讲，我们提供了一个简单且可计算的模型，用于分析零样本嵌入中的偏见，并给出了在什么条件下我们的方法可以提高性能的结果。在实证上，我们在九个图像和NLP分类任务上评估了RoboShot。

    Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and s
    
[^17]: 通过在线凸优化实现在线子模最大化

    Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])

    [http://arxiv.org/abs/2309.04339](http://arxiv.org/abs/2309.04339)

    本论文研究了在线设置下的一般性子模最大化问题，并将一类大型子模函数归约到在线凸优化问题中。这种归约方式可在组合优化中实现次线性遗憾，并且适用于许多不同版本的在线学习问题。

    

    我们研究了在线设置下的一般性子模最大化问题在一般性模性约束下。我们证明了在线优化一类大型子模函数，即加权阈值势函数，可以归约到在线凸优化(OCO)问题。这是因为这个类别的函数可以进行凹松弛;因此，结合适当的舍入方案，OCO策略可以在组合设置中实现次线性遗憾。我们还展示了我们的简化方式可以应用在许多不同版本的在线学习问题中，包括动态遗憾、强盗和乐观学习等设置。

    We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
    
[^18]: 使用多个CLS标记集合对多领域科学论文进行编码

    Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens. (arXiv:2309.04333v1 [cs.CL])

    [http://arxiv.org/abs/2309.04333](http://arxiv.org/abs/2309.04333)

    本研究提出了Multi2SPE方法，通过使用多个CLS标记来编码多领域科学论文，能够更好地适应不同的科学领域，并在引文预测任务中减少了多达25％的误差。

    

    许多科学文档上的有用任务，如主题分类和引文预测，涉及跨越多个科学领域的语料库。通常，这些任务通过使用来自Transformer的单个CLS标记的向量嵌入来完成。在本文中，我们认为使用多个CLS标记可以使Transformer更好地专注于多个科学领域。我们提出了Multi2SPE：它鼓励多个CLS标记学习聚合标记嵌入的不同方式，然后将它们相加以创建单个向量表示。我们还提出了我们的新的多领域基准测试数据集Multi-SciDocs，以测试多领域设置下的科学论文向量编码器。我们证明Multi2SPE在多领域引文预测中减少了多达25％的误差，同时除了一次BERT前向传递之外，只需要极少量的计算量。

    Many useful tasks on scientific documents, such as topic classification and citation prediction, involve corpora that span multiple scientific domains. Typically, such tasks are accomplished by representing the text with a vector embedding obtained from a Transformer's single CLS token. In this paper, we argue that using multiple CLS tokens could make a Transformer better specialize to multiple scientific domains. We present Multi2SPE: it encourages each of multiple CLS tokens to learn diverse ways of aggregating token embeddings, then sums them up together to create a single vector representation. We also propose our new multi-domain benchmark, Multi-SciDocs, to test scientific paper vector encoders under multi-domain settings. We show that Multi2SPE reduces error by up to 25 percent in multi-domain citation prediction, while requiring only a negligible amount of computation in addition to one BERT forward pass.
    
[^19]: 图神经网络在不需要的时候仍然使用图形信息

    Graph Neural Networks Use Graphs When They Shouldn't. (arXiv:2309.04332v1 [cs.LG])

    [http://arxiv.org/abs/2309.04332](http://arxiv.org/abs/2309.04332)

    在图形预测问题中，GNNs倾向于过拟合图结构，即使在忽略图结构的情况下可以获得更好的解决方案。常规图对于这种过拟合更具鲁棒性。

    

    在各个领域中，包括社交网络、分子生物学、医学等，对图形进行预测起着至关重要的作用。图神经网络(GNNs)已成为学习图数据的主要方法。图形标注问题的实例包括图结构(即邻接矩阵)和节点特定的特征向量。在某些情况下，这种图结构对于预测任务来说并不具有信息量。例如，分子性质如摩尔质量仅依赖于组成原子(节点特征)，而与分子结构无关。尽管GNNs有能力在这种情况下忽略图结构，但不清楚它们是否会这样做。在这项工作中，我们展示了GNNs实际上倾向于在过拟合图结构，即在忽略它可以获得更好解决方案的情况下仍在使用。我们根据不同的图分布来研究这种现象，发现常规图对这种过拟合更加稳健。

    Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting
    
[^20]: 生成真实标签: 用于标签噪声研究的合成数据

    Generating the Ground Truth: Synthetic Data for Label Noise Research. (arXiv:2309.04318v1 [cs.LG])

    [http://arxiv.org/abs/2309.04318](http://arxiv.org/abs/2309.04318)

    本文提出了SYNLABEL框架，通过生成基于真实数据的无噪声数据集，并可以为每个数据点分配一个软标签或标签分布，用于改进标签噪声研究。

    

    大多数真实世界的分类任务都存在着一定程度的标签噪声。这种数据中的噪声对于学习模型的泛化误差产生不利影响，并且使得噪声处理方法的评估变得复杂，因为没有清晰的标签，无法准确衡量其性能。在标签噪声研究中，通常接受有噪声或简单的模拟数据作为基线，然后注入具有已知属性的额外噪声。在本文中，我们提出了SYNLABEL，这是一个旨在改进上述方法的框架。它允许通过预先指定或学习一个函数，并将其定义为生成标签的基本真值函数，从而创建一个无噪声的数据集。此外，通过在函数域中的选定特征上重新采样一些值，评估函数并汇总结果标签，可以为每个数据点分配一个软标签或标签分布。

    Most real-world classification tasks suffer from label noise to some extent. Such noise in the data adversely affects the generalization error of learned models and complicates the evaluation of noise-handling methods, as their performance cannot be accurately measured without clean labels. In label noise research, typically either noisy or incomplex simulated data are accepted as a baseline, into which additional noise with known properties is injected. In this paper, we propose SYNLABEL, a framework that aims to improve upon the aforementioned methodologies. It allows for creating a noiseless dataset informed by real data, by either pre-specifying or learning a function and defining it as the ground truth function from which labels are generated. Furthermore, by resampling a number of values for selected features in the function domain, evaluating the function and aggregating the resulting labels, each data point can be assigned a soft label or label distribution. Such distributions 
    
[^21]: 使用矩神经网络的平均场控制中的演员-评论员学习算法

    Actor critic learning algorithms for mean-field control with moment neural networks. (arXiv:2309.04317v1 [stat.ML])

    [http://arxiv.org/abs/2309.04317](http://arxiv.org/abs/2309.04317)

    我们开发了一种使用矩神经网络的演员-评论员算法，用于解决平均场控制问题。我们的方法利用基于梯度的价值函数表示，并通过直接采样分布的轨迹来实现学习。数值结果表明，我们的方法在多维和非线性二次控制问题等不同情境下具有良好的效果。

    

    我们在连续时间强化学习环境中开发了一种解决平均场控制问题的新的策略梯度和演员-评论员算法。我们的方法利用基于梯度的价值函数表示，采用参数化的随机策略。演员（策略）和评论员（价值函数）的学习是通过在概率测度的Wasserstein空间上的一类矩神经网络函数来实现的，其关键特征是直接采样分布的轨迹。这项研究中解决的一个核心挑战涉及到对于平均场框架特有的运算符的计算处理。为了说明我们方法的有效性，我们提供了一系列全面的数值结果。这些结果涵盖了多维设置和具有受控波动性的非线性二次平均场控制问题等不同的例子。

    We develop a new policy gradient and actor-critic algorithm for solving mean-field control problems within a continuous time reinforcement learning setting. Our approach leverages a gradient-based representation of the value function, employing parametrized randomized policies. The learning for both the actor (policy) and critic (value function) is facilitated by a class of moment neural network functions on the Wasserstein space of probability measures, and the key feature is to sample directly trajectories of distributions. A central challenge addressed in this study pertains to the computational treatment of an operator specific to the mean-field framework. To illustrate the effectiveness of our methods, we provide a comprehensive set of numerical results. These encompass diverse examples, including multi-dimensional settings and nonlinear quadratic mean-field control problems with controlled volatility.
    
[^22]: 基于联邦学习的健康老龄应用早期退学预测研究

    Federated Learning for Early Dropout Prediction on Healthy Ageing Applications. (arXiv:2309.04311v1 [cs.LG])

    [http://arxiv.org/abs/2309.04311](http://arxiv.org/abs/2309.04311)

    本文提出了一种基于联邦学习的方法，用于预测健康老龄应用中的早期退学情况。该方法通过联合个体和组织进行训练，并实现了隐私保护和分布式学习。

    

    社会护理应用的提供对于改善老年人的生活质量和为运营商提供早期干预是至关重要的。在健康老龄应用中准确预测用户退学是必要的，因为它们直接与个体健康状况相关。机器学习算法已经实现了高度准确的预测，超越了传统统计方法，在处理个体模式时非常困难。然而，机器学习需要大量的数据进行训练，这在存在个人可识别信息(PII)和受到法规碎片化的情况下具有挑战性。本文提出了一种联邦机器学习(FML)方法，该方法最大程度地减少了隐私问题，并实现了分布式训练，而不需要传输个体数据。我们通过考虑FML下的个体和组织来进行协作训练，这样可以对跨设备和跨平台学习场景进行建模。

    The provision of social care applications is crucial for elderly people to improve their quality of life and enables operators to provide early interventions. Accurate predictions of user dropouts in healthy ageing applications are essential since they are directly related to individual health statuses. Machine Learning (ML) algorithms have enabled highly accurate predictions, outperforming traditional statistical methods that struggle to cope with individual patterns. However, ML requires a substantial amount of data for training, which is challenging due to the presence of personal identifiable information (PII) and the fragmentation posed by regulations. In this paper, we present a federated machine learning (FML) approach that minimizes privacy concerns and enables distributed training, without transferring individual data. We employ collaborative training by considering individuals and organizations under FML, which models both cross-device and cross-silo learning scenarios. Our a
    
[^23]: 在COVID-19期间导航不在分布范围内的电力负荷预测：利用人类移动的持续学习方法

    Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])

    [http://arxiv.org/abs/2309.04296](http://arxiv.org/abs/2309.04296)

    本研究提出了一种利用人类移动数据和持续学习技术的方法来解决COVID-19期间非分布期间的电力负荷预测问题，通过保留过去的见解并整合新的数据，提高了模型的准确性和鲁棒性。

    

    在传统的深度学习算法中，一个关键假设是数据分布在训练和部署过程中保持不变。然而，在面对非分布期间时，如COVID-19的封锁期，数据分布与模型在训练过程中所见的明显偏离。本文采用双重策略：利用持续学习技术更新模型的新数据，并利用在建筑物外部的保护隐私的行人计数器收集的人类移动数据。与在线学习相比，后者常常会遭受“灾难性遗忘”的困扰，因为新获得的知识常常会抹去先前的信息，持续学习则通过保留过去的见解并整合新的数据，提供了一个整体的方法。本研究将FSNet，一种强大的持续学习算法，应用于墨尔本市13个建筑群的真实数据。

    In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the s
    
[^24]: 将生成对立假设的过程视为知识来源 - 应用于朴素贝叶斯分类器

    Viewing the process of generating counterfactuals as a source of knowledge -- Application to the Naive Bayes classifier. (arXiv:2309.04284v1 [cs.LG])

    [http://arxiv.org/abs/2309.04284](http://arxiv.org/abs/2309.04284)

    将生成对立假设的过程视为知识来源，并应用于朴素贝叶斯分类器，展示其有趣属性。

    

    现在有许多理解算法可以理解机器学习算法的决策，其中包括基于生成对立假设示例的算法。本文提出将这个生成过程视为一种创造一定量知识的方法，这些知识可以存储并在以后以不同的方式使用。本文在加法模型中进行了说明，具体而言，是在朴素贝叶斯分类器的情况下，展示了其在此目的上的有趣属性。

    There are now many comprehension algorithms for understanding the decisions of a machine learning algorithm. Among these are those based on the generation of counterfactual examples. This article proposes to view this generation process as a source of creating a certain amount of knowledge that can be stored to be used, later, in different ways. This process is illustrated in the additive model and, more specifically, in the case of the naive Bayes classifier, whose interesting properties for this purpose are shown.
    
[^25]: 大维度情况下核回归的最优比率

    Optimal Rate of Kernel Regression in Large Dimensions. (arXiv:2309.04268v1 [stat.ML])

    [http://arxiv.org/abs/2309.04268](http://arxiv.org/abs/2309.04268)

    该论文提出了一种针对大维度数据的核回归的最优比率，通过使用Mendelson复杂性和度量熵来刻画其上界和最小化下界。此外，研究还发现最优比率随着维度与样本大小关系的变化呈现出多次下降的行为。

    

    我们对大维度数据（样本大小$n$与样本维度$d$的关系为多项式，即$n\asymp d^{\gamma}$，其中$\gamma>0$）的核回归进行了研究。我们首先通过Mendelson复杂性$\varepsilon_{n}^{2}$和度量熵$\bar{\varepsilon}_{n}^{2}$来建立一个通用工具，用于刻画大维度数据的核回归的上界和最小化下界。当目标函数属于与$\mathbb{S}^{d}$上定义的（一般）内积模型相关联的RKHS时，我们利用这个新工具来展示核回归的过量风险的最小化率是$n^{-1/2}$，当$n\asymp d^{\gamma}$，其中$\gamma=2, 4, 6, 8, \cdots$。然后我们进一步确定了对于所有$\gamma>0$，核回归过量风险的最优比率，并发现随着$\gamma$的变化，最优比率的曲线展现出几个新现象，包括多次下降行为。

    We perform a study on kernel regression for large-dimensional data (where the sample size $n$ is polynomially depending on the dimension $d$ of the samples, i.e., $n\asymp d^{\gamma}$ for some $\gamma >0$ ). We first build a general tool to characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively. When the target function falls into the RKHS associated with a (general) inner product model defined on $\mathbb{S}^{d}$, we utilize the new tool to show that the minimax rate of the excess risk of kernel regression is $n^{-1/2}$ when $n\asymp d^{\gamma}$ for $\gamma =2, 4, 6, 8, \cdots$. We then further determine the optimal rate of the excess risk of kernel regression for all the $\gamma>0$ and find that the curve of optimal rate varying along $\gamma$ exhibits several new phenomena including the {\it multiple descent behavior
    
[^26]: 自适应分布式核岭回归：一种可行的解决数据孤立的分布式学习方案

    Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos. (arXiv:2309.04236v1 [cs.LG])

    [http://arxiv.org/abs/2309.04236](http://arxiv.org/abs/2309.04236)

    本文提出了自适应分布式核岭回归（AdaDKRR）方法，该方法通过考虑自治性、隐私性和合作性解决了数据孤立的问题，理论上证明了其性能与整体数据运行最优学习算法类似。

    

    数据孤立主要由隐私和互操作性引起，显著限制了不同组织在相同目的下具有相似数据的合作。基于分而治之的分布式学习为解决数据孤立提供了一种有前途的方法，但其面临着自治性、隐私保证和合作的必要性等诸多挑战。本文侧重于开发一种自适应分布式核岭回归（AdaDKRR）方法，考虑参数选择的自治性、传递非敏感信息的隐私性和性能改进的合作性。我们提供了对AdaDKRR的坚实理论验证和全面实验来证明其可行性和有效性。在理论上，我们证明在一些温和条件下，AdaDKRR在整个数据上运行最优学习算法的性能类似，验证了合作的必要性，并表明没有其他方法能够达到类似的性能。

    Data silos, mainly caused by privacy and interoperability, significantly constrain collaborations among different organizations with similar data for the same purpose. Distributed learning based on divide-and-conquer provides a promising way to settle the data silos, but it suffers from several challenges, including autonomy, privacy guarantees, and the necessity of collaborations. This paper focuses on developing an adaptive distributed kernel ridge regression (AdaDKRR) by taking autonomy in parameter selection, privacy in communicating non-sensitive information, and the necessity of collaborations in performance improvement into account. We provide both solid theoretical verification and comprehensive experiments for AdaDKRR to demonstrate its feasibility and effectiveness. Theoretically, we prove that under some mild conditions, AdaDKRR performs similarly to running the optimal learning algorithms on the whole data, verifying the necessity of collaborations and showing that no other
    
[^27]: 未观察到潜在混淆因素下的离线推荐系统评估

    Offline Recommender System Evaluation under Unobserved Confounding. (arXiv:2309.04222v1 [cs.LG])

    [http://arxiv.org/abs/2309.04222](http://arxiv.org/abs/2309.04222)

    本论文讨论了在存在潜在混淆因素的情况下进行离线推荐系统评估的问题，并特别关注推荐系统用例。通过对基于策略的估计器进行研究，我们描述了由混淆因素引起的统计偏差。

    

    离线政策估计方法(OPE)允许我们从记录的数据中学习和评估决策策略，使它们成为离线评估推荐系统的吸引人选择。最近的一些作品报道了成功采用OPE方法的情况。这项工作的一个重要假设是不存在未观察到的混淆因素：在数据收集时影响行动和奖励的随机变量。由于数据收集策略通常在从业者的控制之下，因此很少明确地提及无混淆假设，并且现有文献中很少处理其违规问题。这项工作旨在强调在存在未观察到的混淆因素的情况下进行离线策略估计时出现的问题，特别关注推荐系统的用例。我们专注于基于策略的估计器，其中日志倾向是从记录数据中学习的。我们对由于混淆因素引起的统计偏差进行了描述。

    Off-Policy Estimation (OPE) methods allow us to learn and evaluate decision-making policies from logged data. This makes them an attractive choice for the offline evaluation of recommender systems, and several recent works have reported successful adoption of OPE methods to this end. An important assumption that makes this work is the absence of unobserved confounders: random variables that influence both actions and rewards at data collection time. Because the data collection policy is typically under the practitioner's control, the unconfoundedness assumption is often left implicit, and its violations are rarely dealt with in the existing literature.  This work aims to highlight the problems that arise when performing off-policy estimation in the presence of unobserved confounders, specifically focusing on a recommendation use-case. We focus on policy-based estimators, where the logging propensities are learned from logged data. We characterise the statistical bias that arises due to
    
[^28]: 同时进行的分组测试

    Concomitant Group Testing. (arXiv:2309.04221v1 [cs.IT])

    [http://arxiv.org/abs/2309.04221](http://arxiv.org/abs/2309.04221)

    本文介绍了一种新的组合测试问题，即同时进行的分组测试(ConcGT)。该问题需要通过尽可能少的测试来可靠地识别多个不相交的半缺陷集合。作者推导出了各种算法，并在两个半缺陷集合的情况下进行了重点讨论。

    

    在本文中，我们引入了一种捕捉到了一个正测试需要多个“类型”项目的组合测试问题的变种。具体来说，我们假设存在多个不相交的“半缺陷集合”，并且只有包含每个集合中至少一个项目的测试结果才是正的。我们的目标是以尽可能少的测试来可靠地识别所有的半缺陷集合，我们将这个问题称为“同时进行的分组测试”(ConcGT)。我们针对这个任务推导出了各种算法，主要集中于存在两个半缺陷集合的情况。我们的算法有以下特点：(i)它们是确定性的(零错误)还是随机的(小错误)，(ii)它们是非自适应的、完全自适应的，还是有限自适应性的(例如，2或3个阶段)。我们的确定性自适应算法和我们的随机算法(非自适应或有限自适应性)在广泛的规模上都是最优的序列。

    In this paper, we introduce a variation of the group testing problem capturing the idea that a positive test requires a combination of multiple ``types'' of item. Specifically, we assume that there are multiple disjoint \emph{semi-defective sets}, and a test is positive if and only if it contains at least one item from each of these sets. The goal is to reliably identify all of the semi-defective sets using as few tests as possible, and we refer to this problem as \textit{Concomitant Group Testing} (ConcGT). We derive a variety of algorithms for this task, focusing primarily on the case that there are two semi-defective sets. Our algorithms are distinguished by (i) whether they are deterministic (zero-error) or randomized (small-error), and (ii) whether they are non-adaptive, fully adaptive, or have limited adaptivity (e.g., 2 or 3 stages). Both our deterministic adaptive algorithm and our randomized algorithms (non-adaptive or limited adaptivity) are order-optimal in broad scaling reg
    
[^29]: 通过局部引导的顺序算法回溯实现反事实解释

    Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse. (arXiv:2309.04211v1 [cs.LG])

    [http://arxiv.org/abs/2309.04211](http://arxiv.org/abs/2309.04211)

    通过局部引导的顺序算法回溯，实现了使人工智能系统可解释的反事实解释方法。然而，如何有效定位一个反事实及其回溯仍然是一个未解决的挑战。

    

    通过算法回溯实现的反事实解释已成为使人工智能系统可解释的强大工具。在概念上，给定一个被分类为y的个体（事实），我们寻找一些行动，使其预测成为所期望的类别y'（反事实）。这个过程提供了易于定制和解释的算法回溯，并与每个个体的目标直接对齐。然而，对一个“好”的反事实的性质仍然存在广泛的争议；如何有效地定位一个反事实及其相应的回溯仍然是一个未解决的挑战。一些策略使用梯度驱动的方法，但这些方法对回溯的可行性没有任何保证，并且容易受到对精心创建的曲面的对抗性攻击。这可能导致不公平和缺乏鲁棒性。其他方法是数据驱动的，这主要解决了可行性问题，但以隐私、安全和保密为代价。

    Counterfactuals operationalised through algorithmic recourse have become a powerful tool to make artificial intelligence systems explainable. Conceptually, given an individual classified as y -- the factual -- we seek actions such that their prediction becomes the desired class y' -- the counterfactual. This process offers algorithmic recourse that is (1) easy to customise and interpret, and (2) directly aligned with the goals of each individual. However, the properties of a "good" counterfactual are still largely debated; it remains an open challenge to effectively locate a counterfactual along with its corresponding recourse. Some strategies use gradient-driven methods, but these offer no guarantees on the feasibility of the recourse and are open to adversarial attacks on carefully created manifolds. This can lead to unfairness and lack of robustness. Other methods are data-driven, which mostly addresses the feasibility problem at the expense of privacy, security and secrecy as they 
    
[^30]: 在数据集蒸馏中缓解架构过度拟合的方法

    Towards Mitigating Architecture Overfitting in Dataset Distillation. (arXiv:2309.04195v1 [cs.LG])

    [http://arxiv.org/abs/2309.04195](http://arxiv.org/abs/2309.04195)

    本文解决了数据集蒸馏中的架构过度拟合问题，提出了一系列方法来提高不同网络架构在蒸馏训练数据上的泛化性能。

    

    数据集蒸馏方法在使用极少训练数据进行神经网络训练时表现出了显著的性能。然而，一个重要的挑战是架构过度拟合：由特定网络架构（即训练网络）合成的蒸馏训练数据在其他网络架构（即测试网络）训练时表现出较差的性能。本文解决了这个问题，提出了一系列架构设计和训练方案的方法，可以共同提高不同网络架构在蒸馏训练数据上的泛化性能。我们进行了大量实验证明了我们方法的有效性和普适性。特别地，在不同大小的蒸馏数据涉及的各种场景中，我们的方法在使用容量更大的网络对蒸馏数据进行训练时实现了与现有方法相当或更好的性能。

    Dataset distillation methods have demonstrated remarkable performance for neural networks trained with very limited training data. However, a significant challenge arises in the form of architecture overfitting: the distilled training data synthesized by a specific network architecture (i.e., training network) generates poor performance when trained by other network architectures (i.e., test networks). This paper addresses this issue and proposes a series of approaches in both architecture designs and training schemes which can be adopted together to boost the generalization performance across different network architectures on the distilled training data. We conduct extensive experiments to demonstrate the effectiveness and generality of our methods. Particularly, across various scenarios involving different sizes of distilled data, our approaches achieve comparable or superior performance to existing methods when training on the distilled data using networks with larger capacities.
    
[^31]: 利用特征缺失感知校准的原型患者表示来缓解电子健康记录数据稀疏性问题

    Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])

    [http://arxiv.org/abs/2309.04160](http://arxiv.org/abs/2309.04160)

    本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。

    

    电子健康记录（EHR）数据经常呈现稀疏特征，给预测建模带来挑战。当前的直接插补方法（如矩阵插补方法）依赖于参考类似行或列来完成原始缺失数据，不区分插补和实际值。因此，模型可能会无意中将与预测目标无关的或具有欺骗性的信息纳入其中，从而损害下游性能的效果。虽然一些方法尝试在直接插补后重新校准或增强EHR嵌入，但它们经常错误地优先考虑插补特征。这种优先错误可能会给模型引入偏见或不准确性。为了解决这些问题，我们的工作采用间接插补，利用类似患者的原型表示获取更密集的嵌入。认识到在衡量时通常将缺失特征与存在特征相同的限制时

    Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
    
[^32]: DMI (Deuterium Metabolic Imaging) 的敏感性增强的深度学习方法

    A Deep Learning Method for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI). (arXiv:2309.04100v1 [eess.IV])

    [http://arxiv.org/abs/2309.04100](http://arxiv.org/abs/2309.04100)

    本研究提出了一种用于增强DMI敏感性的深度学习方法，通过训练卷积神经网络来估计低信噪比和失真的DMI FID的代谢物浓度，并通过MRI的边缘保护正则化进一步提高估计精度。实验结果显示，该方法在提高参数质量方面具有潜在的改进效果。

    

    目的：DMI（Deuterium Metabolic Imaging）的空间分辨率和最小扫描时间受到可达到的信噪比的限制。本研究提出了一种用于增强DMI敏感性的深度学习方法。方法：设计了一个卷积神经网络（CNN）来估计低信噪比和失真的DMI FID的2H标记代谢物浓度。CNN使用合成数据进行训练，这些数据代表通常在体内遇到的一系列信噪比水平。通过对每个DMI数据集使用基于MRI的边缘保护正则化进行CNN的微调，进一步提高了估计精度。提出的处理方法，称为PRECISE-DMI（PReserved Edge ConvolutIonal neural network for Sensitivity Enhanced DMI），应用于模拟研究和体内实验，评估了在SNR上预期的改进，并研究了可能的不准确性。结果：PRECISE-DMI在低信噪比数据集的代谢图像上有着明显的改善，并提高了参数的质量。

    Purpose: Common to most MRSI techniques, the spatial resolution and the minimal scan duration of Deuterium Metabolic Imaging (DMI) are limited by the achievable SNR. This work presents a deep learning method for sensitivity enhancement of DMI.  Methods: A convolutional neural network (CNN) was designed to estimate the 2H-labeled metabolite concentrations from low SNR and distorted DMI FIDs. The CNN was trained with synthetic data that represent a range of SNR levels typically encountered in vivo. The estimation precision was further improved by fine-tuning the CNN with MRI-based edge-preserving regularization for each DMI dataset. The proposed processing method, PReserved Edge ConvolutIonal neural network for Sensitivity Enhanced DMI (PRECISE-DMI), was applied to simulation studies and in vivo experiments to evaluate the anticipated improvements in SNR and investigate the potential for inaccuracies.  Results: PRECISE-DMI visually improved the metabolic maps of low SNR datasets, and qua
    
[^33]: 使用多保真度训练在通用策略网络上的机器人代理共设计的样本高效方法

    Sample-Efficient Co-Design of Robotic Agents Using Multi-fidelity Training on Universal Policy Network. (arXiv:2309.04085v1 [cs.RO])

    [http://arxiv.org/abs/2309.04085](http://arxiv.org/abs/2309.04085)

    该论文提出了一种使用多保真度训练的机器人代理共设计的样本高效方法，通过在设计空间中共享学习到的控制器，以及通过特定方式遍历设计矩阵，可以提高设计评估的效率。

    

    共设计涉及同时优化控制器和代理物理设计。其固有的双层优化形式要求通过内层控制优化来驱动外层设计优化。当设计空间较大且每个设计评估都涉及数据密集型的强化学习过程时，这可能会带来挑战。为了提高样本效率，我们提出了一种基于Hyperband的多保真度设计探索策略，在设计空间中通过通用策略学习者将学习到的控制器进行关联，以启动后续控制器学习问题。此外，我们推荐一种特定的遍历Hyperband生成的设计矩阵的方式，以确保随着每个新的设计评估，通用策略学习者的增强效果越来越强，从而降低Hyperband的随机性。实验证明了该方法在广泛的年龄范围内的表现。

    Co-design involves simultaneously optimizing the controller and agents physical design. Its inherent bi-level optimization formulation necessitates an outer loop design optimization driven by an inner loop control optimization. This can be challenging when the design space is large and each design evaluation involves data-intensive reinforcement learning process for control optimization. To improve the sample-efficiency we propose a multi-fidelity-based design exploration strategy based on Hyperband where we tie the controllers learnt across the design spaces through a universal policy learner for warm-starting the subsequent controller learning problems. Further, we recommend a particular way of traversing the Hyperband generated design matrix that ensures that the stochasticity of the Hyperband is reduced the most with the increasing warm starting effect of the universal policy learner as it is strengthened with each new design evaluation. Experiments performed on a wide range of age
    
[^34]: 对于图表示学习的混合曲率Transformer: 拐弯你的注意力

    Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning. (arXiv:2309.04082v1 [cs.LG])

    [http://arxiv.org/abs/2309.04082](http://arxiv.org/abs/2309.04082)

    本文提出了一种混合曲率Transformer，用于图表示学习。通过将完全乘积立体变换器与标记化的图Transformer相结合，模型能够以端到端的方式学习适应输入图的曲率，从而更好地嵌入图中的分层或循环结构。

    

    现实世界中的图往往具有不适合典型欧几里得空间的分层或循环结构。虽然存在能够利用双曲或球面空间学习更准确嵌入这些结构的图神经网络，但这些方法局限于消息传递范式，使得模型容易受到过度平滑和过度压缩等副作用的影响。最近的研究提出了基于全局注意力的图Transformer，可以轻松建模长程交互，但对非欧几里得几何的拓展尚未得到探索。为了弥合这一差距，我们提出了完全乘积立体变换器，这是一种对常曲率空间的变换器的推广。当与标记化的图Transformer结合使用时，我们的模型可以以端到端的方式学习适合输入图的曲率，无需在不同曲率上进行额外调整。

    Real-world graphs naturally exhibit hierarchical or cyclical structures that are unfit for the typical Euclidean space. While there exist graph neural networks that leverage hyperbolic or spherical spaces to learn representations that embed such structures more accurately, these methods are confined under the message-passing paradigm, making the models vulnerable against side-effects such as oversmoothing and oversquashing. More recent work have proposed global attention-based graph Transformers that can easily model long-range interactions, but their extensions towards non-Euclidean geometry are yet unexplored. To bridge this gap, we propose Fully Product-Stereographic Transformer, a generalization of Transformers towards operating entirely on the product of constant curvature spaces. When combined with tokenized graph Transformers, our model can learn the curvature appropriate for the input graph in an end-to-end fashion, without the need of additional tuning on different curvature i
    
[^35]: UER: 一种针对在线连续学习的启发式偏差处理方法

    UER: A Heuristic Bias Addressing Approach for Online Continual Learning. (arXiv:2309.04081v1 [cs.LG])

    [http://arxiv.org/abs/2309.04081](http://arxiv.org/abs/2309.04081)

    本论文提出了一种针对在线连续学习的启发式偏差处理方法，通过将点积logits分解为角度因子和范数因子，解决了点积logits偏差问题。角度因子用于学习新的知识，而范数因子有助于记住历史知识。

    

    在线连续学习旨在通过单次遍历数据对神经网络进行连续训练。作为最有效的方法，基于回放的方法会重新播放部分先前的数据。然而，现有方法中使用的常见预测器倾向于生成偏向当前数据类别的有偏差的点积logits，这被称为偏差问题和遗忘现象。许多方法已经被提出来通过纠正偏差来克服遗忘问题，但是它们还需要在在线方式下改进。在本文中，我们尝试通过一种更直接和更高效的方法来解决偏差问题。通过将点积logits分解为角度因子和范数因子，我们经验性地发现偏差问题主要发生在角度因子中，可以用来学习新的知识作为余弦logits。相反，被现有方法抛弃的范数因子有助于记住历史知识。

    Online continual learning aims to continuously train neural networks from a continuous data stream with a single pass-through data. As the most effective approach, the rehearsal-based methods replay part of previous data. Commonly used predictors in existing methods tend to generate biased dot-product logits that prefer to the classes of current data, which is known as a bias issue and a phenomenon of forgetting. Many approaches have been proposed to overcome the forgetting problem by correcting the bias; however, they still need to be improved in online fashion. In this paper, we try to address the bias issue by a more straightforward and more efficient method. By decomposing the dot-product logits into an angle factor and a norm factor, we empirically find that the bias problem mainly occurs in the angle factor, which can be used to learn novel knowledge as cosine logits. On the contrary, the norm factor abandoned by existing methods helps remember historical knowledge. Based on this
    
[^36]: 通过车辆动力学评估驾驶员生理状况的技术

    Enabling the Evaluation of Driver Physiology Via Vehicle Dynamics. (arXiv:2309.04078v1 [cs.HC])

    [http://arxiv.org/abs/2309.04078](http://arxiv.org/abs/2309.04078)

    本文介绍了一种通过车辆动力学评估驾驶员生理状况的技术，该技术整合了商业传感器和驾驶员输入，可以提供驾驶行为和生理反应的关键参数，具有提升道路安全和早期检测健康相关并发症的潜力。

    

    驾驶对全球很多人来说是日常工作。本文介绍了将车辆转变为一个连接的生态系统的配置和方法，该系统能够评估驾驶员的生理状况。我们整合了来自汽车和数字健康领域的商业传感器以及车辆本身的驾驶员输入。这些传感器的组合能够精确记录外部条件和驾驶动作。这些数据流经过处理后提取关键参数，可以洞察驾驶员在外部环境中的行为，并揭示重要的生理反应。这种创新的驾驶员评估系统有可能提升道路安全性。此外，当与传统健康设置的数据配对使用时，它可能增强对与健康相关并发症的早期检测。

    Driving is a daily routine for many individuals across the globe. This paper presents the configuration and methodologies used to transform a vehicle into a connected ecosystem capable of assessing driver physiology. We integrated an array of commercial sensors from the automotive and digital health sectors along with driver inputs from the vehicle itself. This amalgamation of sensors allows for meticulous recording of the external conditions and driving maneuvers. These data streams are processed to extract key parameters, providing insights into driver behavior in relation to their external environment and illuminating vital physiological responses. This innovative driver evaluation system holds the potential to amplify road safety. Moreover, when paired with data from conventional health settings, it may enhance early detection of health-related complications.
    
[^37]: Riemannian Langevin Monte Carlo方案用于从具有固定秩的PSD矩阵中进行采样

    Riemannian Langevin Monte Carlo schemes for sampling PSD matrices with fixed rank. (arXiv:2309.04072v1 [math.NA])

    [http://arxiv.org/abs/2309.04072](http://arxiv.org/abs/2309.04072)

    本文介绍了两种Riemannian Langevin Monte Carlo方案，用于从具有固定秩的PSD矩阵中采样。这些方案通过在流形上使用布朗运动的Riemannian Langevin方程的Euler-Maruyama离散化来实现采样，具有实际应用价值。

    

    本文介绍了两种显式方案，用于从 $\mathcal S^{n,p}_+$ 中的Gibbs分布中采样矩阵，其中 $\mathcal S^{n,p}_+$ 是尺寸为$n\times n$，秩为$p$的实正半定（PSD）矩阵流形。给定一个能量函数 $\mathcal E:\mathcal S^{n,p}_+\to \mathbb{R}$ 和某些在 $\mathcal S^{n,p}_+$ 上的Riemannian度量 $g$，这些方案依赖于在流形上使用布朗运动的Riemannian Langevin方程（RLE）的Euler-Maruyama离散化。我们针对 $\mathcal S^{n,p}_+$ 上的两个基本度量（a）从嵌入 $\mathcal S^{n,p}_+ \subset \mathbb{R}^{n\times n} $ 获得的度量；以及（b）对应于商流形几何的Bures-Wasserstein度量，提供了RLE的数值方案。我们还提供了具有明确Gibbs分布的能量函数的示例，以便对这些方案进行数值验证。

    This paper introduces two explicit schemes to sample matrices from Gibbs distributions on $\mathcal S^{n,p}_+$, the manifold of real positive semi-definite (PSD) matrices of size $n\times n$ and rank $p$. Given an energy function $\mathcal E:\mathcal S^{n,p}_+\to \mathbb{R}$ and certain Riemannian metrics $g$ on $\mathcal S^{n,p}_+$, these schemes rely on an Euler-Maruyama discretization of the Riemannian Langevin equation (RLE) with Brownian motion on the manifold. We present numerical schemes for RLE under two fundamental metrics on $\mathcal S^{n,p}_+$: (a) the metric obtained from the embedding of $\mathcal S^{n,p}_+ \subset \mathbb{R}^{n\times n} $; and (b) the Bures-Wasserstein metric corresponding to quotient geometry. We also provide examples of energy functions with explicit Gibbs distributions that allow numerical validation of these schemes.
    
[^38]: 3D去噪器是好的2D教师：通过去噪和跨模态蒸馏进行分子预训练

    3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation. (arXiv:2309.04062v1 [cs.LG])

    [http://arxiv.org/abs/2309.04062](http://arxiv.org/abs/2309.04062)

    本论文提出了一种自监督的分子表示学习框架D&D，通过将3D去噪器的表示蒸馏到2D图形编码器中进行预训练，以解决从大规模无标签数据中预训练分子表示的问题。

    

    从大量无标签数据中预训练分子表示对于分子属性预测至关重要，因为获取地面真实标签的成本很高。虽然存在各种基于2D图形的分子预训练方法，但这些方法难以在预测性能上显示出统计学上的显著提升。因此，最近的工作提出了在去噪任务下基于3D构象的预训练方法，取得了有希望的结果。然而，在下游微调过程中，使用3D构象训练的模型需要准确的先前未见过的分子原子坐标，这在大规模情况下计算成本很高。基于这一限制，我们提出了D&D，一种自监督的分子表示学习框架，通过将3D去噪器的表示蒸馏到2D图形编码器中进行预训练。通过去噪和跨模态知识蒸馏，我们的方法不仅利用了从去噪中获得的知识，而且应用起来很容易。

    Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless applica
    
[^39]: SRN-SZ: 基于深度学习的科学数据错误有界损失压缩与超分辨率神经网络

    SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks. (arXiv:2309.04037v1 [cs.LG])

    [http://arxiv.org/abs/2309.04037](http://arxiv.org/abs/2309.04037)

    提出了一种基于深度学习的科学数据错误有界损失压缩器SRN-SZ，用于改善难以压缩的数据集的压缩情况。

    

    现代超级计算系统的计算能力和规模的快速增长给超级计算系统的管理带来了巨大挑战。为了保持科学数据的可用性，提出并发展了错误有界损失压缩作为科学数据尺寸缩减的重要技术，以限制数据失真。在各种科学模拟生成的多样数据集中，某些数据集无法通过现有的传统技术的错误有界损失压缩器有效压缩。人工智能的最近成功启发了多位研究人员将神经网络集成到错误有界损失压缩器中。然而，这些工作仍然面临着有限的压缩比和/或极低的效率。为了解决这些问题并改善难以压缩的数据集的压缩情况，本文提出了SRN-SZ，它是一种基于深度学习的科学数据错误有界损失压缩器。

    The fast growth of computational power and scales of modern super-computing systems have raised great challenges for the management of exascale scientific data. To maintain the usability of scientific data, error-bound lossy compression is proposed and developed as an essential technique for the size reduction of scientific data with constrained data distortion. Among the diverse datasets generated by various scientific simulations, certain datasets cannot be effectively compressed by existing error-bounded lossy compressors with traditional techniques. The recent success of Artificial Intelligence has inspired several researchers to integrate neural networks into error-bounded lossy compressors. However, those works still suffer from limited compression ratios and/or extremely low efficiencies. To address those issues and improve the compression on the hard-to-compress datasets, in this paper, we propose SRN-SZ, which is a deep learning-based scientific error-bounded lossy compressor 
    
[^40]: 在逐点非线性之前与之后线性化循环神经网络 (RNNs) 的简要技术说明

    Brief technical note on linearizing recurrent neural networks (RNNs) before vs after the pointwise nonlinearity. (arXiv:2309.04030v1 [cs.LG])

    [http://arxiv.org/abs/2309.04030](http://arxiv.org/abs/2309.04030)

    这篇论文讨论了循环神经网络在激活与活动动力学线性化过程中的差异，以及线性化活动动力学下一些上下文相关效应的显现。

    

    经常使用线性化研究循环神经网络 (RNNs) 的动力学特性。相同的 RNN 动力学可以用“激活”（每个单元的净输入，在逐点非线性之前）或“活动”（每个单元的输出，在逐点非线性之后）来表示；两种对应的线性化方法彼此不同。这篇简短的非正式技术说明描述了两种线性化方法之间的关系，它们动力学矩阵的左右特征向量之间的关系，并表明一些上下文相关效应在活动动力学的线性化下显而易见，而在激活动力学的线性化下则不明显。

    Linearization of the dynamics of recurrent neural networks (RNNs) is often used to study their properties. The same RNN dynamics can be written in terms of the ``activations" (the net inputs to each unit, before its pointwise nonlinearity) or in terms of the ``activities" (the output of each unit, after its pointwise nonlinearity); the two corresponding linearizations are different from each other. This brief and informal technical note describes the relationship between the two linearizations, between the left and right eigenvectors of their dynamics matrices, and shows that some context-dependent effects are readily apparent under linearization of activity dynamics but not linearization of activation dynamics.
    
[^41]: TIDE: 用于评估和增强分类和语言模型的文本身份检测

    TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models. (arXiv:2309.04027v1 [cs.CL])

    [http://arxiv.org/abs/2309.04027](http://arxiv.org/abs/2309.04027)

    本文介绍了TIDE（Textual Identity Detection）方法来改善分类器和语言模型中的文本公平性。通过创建一个包含身份词汇和语境的数据集，以及开发一个身份注释和增强工具，可以提高机器学习公平性技术的效果。

    

    机器学习模型可以继承不公正和不平衡数据集中的意外偏见。在文本数据集中，评估和去偏这些数据集和模型尤其困难，因为种族、性别和性取向等敏感属性可能不可用。当这些模型投放到社会中时，它们可能对历史上弱势群体产生不公平的结果。本文提出了一个与方法相结合的数据集，以改善分类器和语言模型中的文本公平性。我们创建了一个更全面的身份词汇表TIDAL，包括15,123个身份术语和相关的语境，涵盖了三个人口统计类别。我们利用TIDAL开发了一个身份注释和增强工具，可以用于改善身份语境的可用性和机器学习公平性技术的效果。我们使用人类贡献者对我们的方法进行了评估，并进行了重点关注数据集和模型去偏的实验。

    Machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. Evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. When these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. In this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. We create a new, more comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ML fairness techniques. We evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. Resul
    
[^42]: 使用温度指数测度的最优输运

    Optimal Transport with Tempered Exponential Measures. (arXiv:2309.04015v1 [cs.LG])

    [http://arxiv.org/abs/2309.04015](http://arxiv.org/abs/2309.04015)

    本文推广了熵正则化最优输运方法，将其应用于温度指数测度中，实现了快速有效的算法和可控的稀疏性。

    

    在最优输运领域中，两个重要的子领域相互对立：（i）非正则化最优输运，“卡托诺维奇方式”，导致了非常稀疏的规划，但算法效率较低；（ii）熵正则化最优输运，“辛克霍恩-库都里方式”，获得了近似线性算法，但最大程度上无法稀疏规划。本文中，我们将后一种方法推广到温度指数测度，即具有间接测度归一化的指数族泛化，取得了非常方便的折中效果，具有非常快的近似算法和可控的稀疏性，同时也适用于不平衡最优输运问题。

    In the field of optimal transport, two prominent subfields face each other: (i) unregularized optimal transport, ``\`a-la-Kantorovich'', which leads to extremely sparse plans but with algorithms that scale poorly, and (ii) entropic-regularized optimal transport, ``\`a-la-Sinkhorn-Cuturi'', which gets near-linear approximation algorithms but leads to maximally un-sparse plans. In this paper, we show that a generalization of the latter to tempered exponential measures, a generalization of exponential families with indirect measure normalization, gets to a very convenient middle ground, with both very fast approximation algorithms and sparsity which is under control up to sparsity patterns. In addition, it fits naturally in the unbalanced optimal transport problem setting as well.
    
[^43]: 一种逐元平方和放松的辅助变量算法用于无约束优化问题

    An Element-wise RSAV Algorithm for Unconstrained Optimization Problems. (arXiv:2309.04013v1 [math.OC])

    [http://arxiv.org/abs/2309.04013](http://arxiv.org/abs/2309.04013)

    本文提出了一种能满足无条件能量耗散定律、在凸设置中证明线性收敛的新型优化算法E-RSAV，并且在单变量情况下改进了线性收敛速度为超线性，还提出了自适应版本的E-RSAV算法加以实现验证。

    

    我们提出了一种新颖的优化算法，即逐元放松的辅助标量变量（E-RSAV），它满足无条件的能量耗散定律，并且改进了修改后与原能量的对齐。我们的算法在凸设置中证明了线性收敛的严格证明。此外，我们提出了一种简单的加速算法，可以改善单变量情况下的线性收敛速度为超线性。我们还提出了一种自适应版本的E-RSAV算法，采用Steffensen步长。通过大量的数值实验，我们验证了我们算法的鲁棒性和快速收敛性。

    We present a novel optimization algorithm, element-wise relaxed scalar auxiliary variable (E-RSAV), that satisfies an unconditional energy dissipation law and exhibits improved alignment between the modified and the original energy. Our algorithm features rigorous proofs of linear convergence in the convex setting. Furthermore, we present a simple accelerated algorithm that improves the linear convergence rate to super-linear in the univariate case. We also propose an adaptive version of E-RSAV with Steffensen step size. We validate the robustness and fast convergence of our algorithm through ample numerical experiments.
    
[^44]: 多模态变换器用于材料分割

    Multimodal Transformer for Material Segmentation. (arXiv:2309.04001v1 [cs.CV])

    [http://arxiv.org/abs/2309.04001](http://arxiv.org/abs/2309.04001)

    本文提出了一种新的多模态分割方法MMSFormer，该方法有效地融合四种不同模态的信息，并在MCubeS数据集上取得了显著的性能提升。

    

    利用不同模态的信息可以提高多模态分割任务的性能。然而，由于每个模态的独特特性，有效地融合不同模态的信息仍然具有挑战性。在本文中，我们提出了一种新的融合策略，可以有效地融合四种不同模态的信息：RGB、线性偏振角（AoLP）、线性偏振度（DoLP）和近红外（NIR）。我们还提出了一种名为多模态分割变换器（MMSFormer）的新模型，该模型将所提出的融合策略结合起来进行多模态材料分割。MMSFormer在多模态材料分割（MCubeS）数据集上取得了52.05％的mIoU，超过了当前最先进的方法。例如，我们的方法在检测砾石（+10.4％）和人类（+9.1％）类上提供了显着的改进。消融研究表明融合块中的不同模块对结果至关重要。

    Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for
    
[^45]: 自适应自监督表示到多领域设置中

    Adapting Self-Supervised Representations to Multi-Domain Setups. (arXiv:2309.03999v1 [cs.CV])

    [http://arxiv.org/abs/2309.03999](http://arxiv.org/abs/2309.03999)

    该论文提出了一种通用的、轻量级的领域解缠模块（DDM），可以在多个不同领域上进行有效的自监督表示学习，并显示出较高的线性探测准确率提升。

    

    当在单个领域上训练时，当前最先进的自监督方法非常有效，但在未知领域上的泛化能力有限。我们观察到，即使在混合领域上进行训练，这些模型的泛化能力也很差，因此不适合在多样化的真实世界环境中部署。因此，我们提出了一个通用、轻量级的领域解缠模块（DDM），可以插入到任何自监督编码器中，在具有或不具有共享类的多个不同领域上有效地进行表示学习。在自监督损失的预训练过程中，DDM通过分割表示空间为领域变体和领域不变部分来强制实现表示空间中的解缠。当没有领域标签可用时，DDM使用强健的聚类方法来发现伪领域。我们展示了使用DDM进行预训练可以在最先进的自监督模型上显示出高达3.5%的线性探测准确率的提升。

    Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including 
    
[^46]: ConDA: 基于对比域适应的AI生成文本检测

    ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])

    [http://arxiv.org/abs/2309.03992](http://arxiv.org/abs/2309.03992)

    创新点：提出了一种基于对比域适应的框架 ConDA，用于检测由大型语言模型生成的新闻文本。这种方法解决了获取标记训练数据的困难，通过利用未标记的目标数据进行无监督域适应。

    

    大型语言模型（LLMs）越来越多地被用于各种用途的文本生成，包括新闻报道。鉴于这些LLMs可能被恶意使用来大规模生成虚假信息，构建有效的检测AI生成文本的工具显得尤为重要。由于新的LLMs不断被开发，获取用于监督式检测器的标记训练数据成为一个瓶颈。然而，可能存在大量未标记的文本数据，没有关于其生成器的信息。在这项工作中，我们解决了此数据问题，即检测AI生成的新闻文本，并将问题框架化为无监督域适应任务。这里的域是不同的文本生成器，即LLMs，我们假设只能访问标记的源数据和未标记的目标数据。我们开发了一个名为ConDA的对比域适应框架，将标准的域适应技术与表示能力相结合。

    Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power 
    
[^47]: 从最优控制理论导出坐标下降算法

    Derivation of Coordinate Descent Algorithms from Optimal Control Theory. (arXiv:2309.03990v1 [math.OC])

    [http://arxiv.org/abs/2309.03990](http://arxiv.org/abs/2309.03990)

    本文从最优控制理论中导出了坐标下降算法，并证明了坐标下降算法的收敛性与李雅普诺夫函数的受控耗散有关。

    

    最近有人提出，可以用最优控制理论的一个核心源来统一不同的优化算法。本文进一步证明了这一命题，展示了如何从这一新兴原理中导出坐标下降算法。具体而言，我们展示了基本的坐标下降算法可以通过最大原理和一系列最大函数作为“控制”李雅普诺夫函数来导出。因此，得到的坐标下降算法的收敛性与相应的李雅普诺夫函数的受控耗散有关。在所有情况下，搜索向量的操作度量由凸目标函数的Hessian矩阵给出。

    Recently, it was posited that disparate optimization algorithms may be coalesced in terms of a central source emanating from optimal control theory. Here we further this proposition by showing how coordinate descent algorithms may be derived from this emerging new principle. In particular, we show that basic coordinate descent algorithms can be derived using a maximum principle and a collection of max functions as "control" Lyapunov functions. The convergence of the resulting coordinate descent algorithms is thus connected to the controlled dissipation of their corresponding Lyapunov functions. The operational metric for the search vector in all cases is given by the Hessian of the convex objective function.
    
[^48]: 噪声计算$\mathsf{OR}$和$\mathsf{MAX}$函数

    Noisy Computing of the $\mathsf{OR}$ and $\mathsf{MAX}$ Functions. (arXiv:2309.03986v1 [cs.DS])

    [http://arxiv.org/abs/2309.03986](http://arxiv.org/abs/2309.03986)

    本研究考虑使用噪声查询计算$\mathsf{OR}$和$\mathsf{MAX}$函数，结果表明，在错误概率收敛时，计算这两个函数所需的期望查询数量与Kullback-Leibler差异之间有密切关系。

    

    我们考虑使用含有噪声的查询来计算一个包含$n$个变量的函数的问题，其中每个查询在某个固定的已知概率$p \in (0,1/2)$下是不正确的。具体来说，我们考虑计算$n$个位的$\mathsf{OR}$函数（其中查询对应于位的噪声读数）和$n$个实数的$\mathsf{MAX}$函数（其中查询对应于有噪声的两两比较）。我们证明，在误差概率$\delta = o(1)$迅速收敛的情况下，计算这两个函数所需的期望查询数量为\[ (1 \pm o(1)) \frac{n\log \frac{1}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)} \] ，其中 $D_{\mathsf{KL}}(p \| 1-p)$表示$\mathsf{Bern}(p)$和$\mathsf{Bern}(1-p)$分布之间的Kullback-Leibler差异。与以前的工作相比，我们的结果在两个函数的上下界中都加强了对$p$的依赖关系。

    We consider the problem of computing a function of $n$ variables using noisy queries, where each query is incorrect with some fixed and known probability $p \in (0,1/2)$. Specifically, we consider the computation of the $\mathsf{OR}$ function of $n$ bits (where queries correspond to noisy readings of the bits) and the $\mathsf{MAX}$ function of $n$ real numbers (where queries correspond to noisy pairwise comparisons). We show that an expected number of queries of \[ (1 \pm o(1)) \frac{n\log \frac{1}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)} \] is both sufficient and necessary to compute both functions with a vanishing error probability $\delta = o(1)$, where $D_{\mathsf{KL}}(p \| 1-p)$ denotes the Kullback-Leibler divergence between $\mathsf{Bern}(p)$ and $\mathsf{Bern}(1-p)$ distributions. Compared to previous work, our results tighten the dependence on $p$ in both the upper and lower bounds for the two functions.
    
[^49]: LanSER: 基于语言模型的语音情绪识别

    LanSER: Language-Model Supported Speech Emotion Recognition. (arXiv:2309.03978v1 [cs.CL])

    [http://arxiv.org/abs/2309.03978](http://arxiv.org/abs/2309.03978)

    LanSER是一种基于语言模型的语音情绪识别方法，通过预先训练的大型语言模型进行弱监督学习，从而使得可以利用未标记数据。实验证明，使用这种弱监督训练的模型在标准SER数据集上表现优于其他基线模型，并且能够模拟语音的韵律内容。

    

    语音情绪识别（SER）模型通常依赖于昂贵的人工标注数据进行训练，使得对大型语音数据集和微妙情绪分类的扩展方法困难重重。我们提出了LanSER，一种通过预训练的大型语言模型通过弱监督学习来推测弱情绪标签，从而实现对未标记数据的使用。对于受到分类约束的弱标签推测，我们采用了一种文本蕴涵方法，通过自动语音识别提取的语音转录中选择一个具有最高蕴涵得分的情绪标签。我们的实验结果表明，使用这种弱监督训练的大型数据集预训练的模型在标准SER数据集上的性能超过其他基线模型，并显示出改进的标签效率。尽管只在文本上推测出的标签进行预训练，我们展示了所得到的表示似乎能够模拟语音的韵律内容。

    Speech emotion recognition (SER) models typically rely on costly human-labeled data for training, making scaling methods to large speech datasets and nuanced emotion taxonomies difficult. We present LanSER, a method that enables the use of unlabeled data by inferring weak emotion labels via pre-trained large language models through weakly-supervised learning. For inferring weak labels constrained to a taxonomy, we use a textual entailment approach that selects an emotion label with the highest entailment score for a speech transcript extracted via automatic speech recognition. Our experimental results show that models pre-trained on large datasets with this weak supervision outperform other baseline models on standard SER datasets when fine-tuned, and show improved label efficiency. Despite being pre-trained on labels derived only from text, we show that the resulting representations appear to model the prosodic content of speech.
    
[^50]: 自动概念嵌入模型（ACEM）：无需训练时的概念，无需担心!

    Automatic Concept Embedding Model (ACEM): No train-time concepts, No issue!. (arXiv:2309.03970v1 [cs.LG])

    [http://arxiv.org/abs/2309.03970](http://arxiv.org/abs/2309.03970)

    这篇论文介绍了一种自动概念嵌入模型（ACEM），它可以解决概念注释对大型数据集的昂贵和不可行性的问题。

    

    神经网络的可解释性和可解释性在安全关键领域和提供社会解释权方面的重要性不断增加。基于概念的解释与人类推理相吻合，证明是一种很好的解释模型的方法。概念嵌入模型（CEM）是一种基于概念的解释架构。它们已经证明可以克服可解释性和性能之间的权衡。然而，它们存在一个关键限制 - 它们需要为所有训练数据提供概念注释。对于大型数据集，这可能是昂贵而不可行的。出于这个动机，我们提出了自动概念嵌入模型（ACEM），它可以自动学习概念注释。

    Interpretability and explainability of neural networks is continuously increasing in importance, especially within safety-critical domains and to provide the social right to explanation. Concept based explanations align well with how humans reason, proving to be a good way to explain models. Concept Embedding Models (CEMs) are one such concept based explanation architectures. These have shown to overcome the trade-off between explainability and performance. However, they have a key limitation -- they require concept annotations for all their training data. For large datasets, this can be expensive and infeasible. Motivated by this, we propose Automatic Concept Embedding Models (ACEMs), which learn the concept annotations automatically.
    
[^51]: 在小型数据集上改进ResNet-9的泛化性能

    Improving Resnet-9 Generalization Trained on Small Datasets. (arXiv:2309.03965v1 [cs.LG])

    [http://arxiv.org/abs/2309.03965](http://arxiv.org/abs/2309.03965)

    本文提出了一种在小型数据集上改进ResNet-9的方法，通过应用一系列技术来提高其泛化性能，在不到10分钟的时间内，在CIFAR-10数据集的10%子集上达到了88%的准确率。

    

    本文提出了我们提出的方法，该方法在ICLR硬件感知高效训练竞赛中获得了第一名。挑战是在不到10分钟的时间内，在一个小数据集上实现尽可能高的图像分类准确率。训练使用的小数据集是从CIFAR-10数据集中随机挑选的5000幅图像。评估由竞赛组织者在一个包含1000幅相同大小图像的秘密数据集上进行。我们的方法包括应用一系列技术来提高ResNet-9的泛化性能，包括：锐度感知优化、标签平滑、梯度居中化、输入图像补丁白化以及基于元学习的训练。我们的实验结果表明，在不到10分钟的时间内，ResNet-9可以在仅训练CIFAR-10数据集的10%子集上达到88%的准确率。

    This paper presents our proposed approach that won the first prize at the ICLR competition on Hardware Aware Efficient Training. The challenge is to achieve the highest possible accuracy in an image classification task in less than 10 minutes. The training is done on a small dataset of 5000 images picked randomly from CIFAR-10 dataset. The evaluation is performed by the competition organizers on a secret dataset with 1000 images of the same size. Our approach includes applying a series of technique for improving the generalization of ResNet-9 including: sharpness aware optimization, label smoothing, gradient centralization, input patch whitening as well as metalearning based training. Our experiments show that the ResNet-9 can achieve the accuracy of 88% while trained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets
    
[^52]: REALM: 鲁棒的熵自适应损失最小化以提高单样本测试时适应性

    REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation. (arXiv:2309.03964v1 [cs.LG])

    [http://arxiv.org/abs/2309.03964](http://arxiv.org/abs/2309.03964)

    本文提出了一种改善单样本测试时自适应鲁棒性的方法，通过自适应学习和鲁棒损失函数的框架，解决了在线F-TTA中嘈杂样本导致的不稳定性问题。

    

    充分测试时适应（F-TTA）可以减轻由于训练和测试数据之间的分布偏移而导致的性能损失（1）无需访问训练数据，（2）无需了解模型训练过程。在在线F-TTA中，通过最小化自我监督目标（例如熵最小化）来适应使用测试样本流的预训练模型。然而，使用熵最小化在线进行适应的模型在单样本设置中不稳定，导致退化解，并限制了TTA推理策略的采用。之前的工作确定了嘈杂或不可靠的样本是在线F-TTA失败的原因之一。一种解决方案是忽略这些样本，这可能导致更新过程中的偏差，适应缓慢和泛化差。在这项工作中，我们提出了一个改善F-TTA对这些嘈杂样本鲁棒性的通用框架，受到自适应学习和鲁棒损失函数的启发。

    Fully-test-time adaptation (F-TTA) can mitigate performance loss due to distribution shifts between train and test data (1) without access to the training data, and (2) without knowledge of the model training procedure. In online F-TTA, a pre-trained model is adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization. However, models adapted with online using entropy minimization, are unstable especially in single sample settings, leading to degenerate solutions, and limiting the adoption of TTA inference strategies. Prior works identify noisy, or unreliable, samples as a cause of failure in online F-TTA. One solution is to ignore these samples, which can lead to bias in the update procedure, slow adaptation, and poor generalization. In this work, we present a general framework for improving robustness of F-TTA to these noisy samples, inspired by self-paced learning and robust loss functions. Our proposed approach, Robust Entropy Adap
    
[^53]: 大规模自动创建有声读物

    Large-Scale Automatic Audiobook Creation. (arXiv:2309.03926v1 [cs.SD])

    [http://arxiv.org/abs/2309.03926](http://arxiv.org/abs/2309.03926)

    这项工作提出了一个可以自动生成高质量有声读物的系统，通过利用神经文本到语音技术，从在线电子书中创建数千本开放许可的有声读物。该系统允许用户自定义朗读速度和风格，并且提供了匹配所需声音的功能。该工作贡献了5000多本有声读物和一个交互式演示。

    

    有声读物可以显著提高文学作品的可访问性和读者参与度。然而，制作、编辑和发布有声读物可能需要数百小时的人力。在这项工作中，我们提出了一个可以从在线电子书自动生成高质量有声读物的系统。特别是，我们利用最近在神经文本到语音方面的进展，从项目古腾堡电子书收藏中创建并发布了数千本高质量、开放许可的有声读物。我们的方法可以针对各种结构多样的图书，识别出适合朗读的电子书内容的合适子集，并可以并行处理数百本书籍。我们的系统允许用户自定义有声读物的朗读速度和风格、情感语调，甚至可以使用少量样本音频来匹配所需的声音。这项工作贡献了5000多本开放许可的有声读物以及一个交互式演示，使用户能够快速创建自己定制的有声读物。

    An audiobook can dramatically improve a work of literature's accessibility and improve reader engagement. However, audiobooks can take hundreds of hours of human effort to create, edit, and publish. In this work, we present a system that can automatically generate high-quality audiobooks from online e-books. In particular, we leverage recent advances in neural text-to-speech to create and release thousands of human-quality, open-license audiobooks from the Project Gutenberg e-book collection. Our method can identify the proper subset of e-book content to read for a wide collection of diversely structured books and can operate on hundreds of books in parallel. Our system allows users to customize an audiobook's speaking speed and style, emotional intonation, and can even match a desired voice using a small amount of sample audio. This work contributed over five thousand open-license audiobooks and an interactive demo that allows users to quickly create their own customized audiobooks. T
    
[^54]: 超越注意力：从弱监督的多实例学习模型中得出可解释的生物洞察

    Beyond attention: deriving biologically interpretable insights from weakly-supervised multiple-instance learning models. (arXiv:2309.03925v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.03925](http://arxiv.org/abs/2309.03925)

    本论文提出了一种后训练方法来分析多实例学习（MIL）模型，通过结合细化编码器产生的瓦片级别的注意力和预测得分，以及与细胞核分割掩模集成的预测-注意力加权（PAW）地图，实现了MIL模型的可解释性的提高。

    

    最近注意力机制的多实例学习（MIL）的进展提高了我们对数字病理学中模型依赖的组织区域进行预测的洞察力。然而，这些方法的可解释性仍然有限。特别是，它们没有报告高注意力区域与类标签的正向或负向关联，以及这些区域与先前建立的临床和生物学知识的对应程度。我们通过引入一种后训练方法来分析MIL模型来解决这个问题。首先，我们通过结合细化编码器产生的瓦片级别的注意力和预测得分，引入了预测-注意力加权（PAW）地图，从而能够量化高注意力区域的预测贡献。其次，我们通过将PAW地图与细胞核分割掩模集成起来，引入了一种生物特征实例化技术。这进一步提高了可解释性，提供了与生物意义相关的特征。

    Recent advances in attention-based multiple instance learning (MIL) have improved our insights into the tissue regions that models rely on to make predictions in digital pathology. However, the interpretability of these approaches is still limited. In particular, they do not report whether high-attention regions are positively or negatively associated with the class labels or how well these regions correspond to previously established clinical and biological knowledge. We address this by introducing a post-training methodology to analyse MIL models. Firstly, we introduce prediction-attention-weighted (PAW) maps by combining tile-level attention and prediction scores produced by a refined encoder, allowing us to quantify the predictive contribution of high-attention regions. Secondly, we introduce a biological feature instantiation technique by integrating PAW maps with nuclei segmentation masks. This further improves interpretability by providing biologically meaningful features relate
    
[^55]: 一种用于药物发现的混合量子-经典融合神经网络以提高蛋白质-配体结合亲和力预测的准确性

    A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery. (arXiv:2309.03919v1 [quant-ph])

    [http://arxiv.org/abs/2309.03919](http://arxiv.org/abs/2309.03919)

    提出了一种用于药物发现的混合量子-经典融合神经网络模型，通过优化的量子架构将3D和空间图卷积神经网络相互整合，提高了结合亲和力预测的准确性。

    

    药物发现领域关键在于准确预测潜在药物分子与靶蛋白之间的结合亲和力，特别是当这些蛋白直接影响疾病的进展时。然而，估计结合亲和力需要显著的财务和计算资源。虽然最先进的方法使用经典机器学习技术，但新兴的混合量子机器学习模型显示出更好的性能，这归功于它们固有的并行性和管理数据维度指数级增加的能力。尽管有这些进展，现有模型在收敛稳定性和预测准确性方面存在问题。本文介绍了一种新颖的混合量子-经典深度学习模型，用于药物发现中的结合亲和力预测。

    The field of drug discovery hinges on the accurate prediction of binding affinity between prospective drug molecules and target proteins, especially when such proteins directly influence disease progression. However, estimating binding affinity demands significant financial and computational resources. While state-of-the-art methodologies employ classical machine learning (ML) techniques, emerging hybrid quantum machine learning (QML) models have shown promise for enhanced performance, owing to their inherent parallelism and capacity to manage exponential increases in data dimensionality. Despite these advances, existing models encounter issues related to convergence stability and prediction accuracy. This paper introduces a novel hybrid quantum-classical deep learning model tailored for binding affinity prediction in drug discovery. Specifically, the proposed model synergistically integrates 3D and spatial graph convolutional neural networks within an optimized quantum architecture. S
    
[^56]: 管理脊髓刺激术慢性疼痛患者的推荐系统

    A recommender for the management of chronic pain in patients undergoing spinal cord stimulation. (arXiv:2309.03918v1 [cs.AI])

    [http://arxiv.org/abs/2309.03918](http://arxiv.org/abs/2309.03918)

    本文提出了一种推荐系统，用于管理进行SCS的慢性疼痛患者。通过使用上下文多臂赌博方法开发的系统，为患者推荐SCS设置，旨在改善其状况。这些推荐通过数字健康生态系统直接发送给患者，并与患者监测系统结合，为慢性疼痛患者的整个治疗过程提供闭环关怀。

    

    脊髓刺激术（Spinal cord stimulation，SCS）是一种用于治疗慢性疼痛的治疗方法。其通过植入装置向脊髓传递电脉冲，在给予适当的刺激参数时，可以掩盖或阻断疼痛信号。优化刺激参数的选择通常在临床由医生负责，而在家庭中的SCS优化则由患者自己管理。在本文中，我们提出了一种用于管理进行SCS的慢性疼痛患者的推荐系统。具体而言，我们使用了一种上下文多臂赌博（CMAB）方法来开发一个系统，为患者推荐SCS设置，旨在改善其状况。这些推荐通过数字健康生态系统直接发送给患者，并与患者监测系统结合，为慢性疼痛患者的整个治疗过程提供闭环关怀。我们在一组接受SCS植入的ENVISION患者中进行了系统评估。

    Spinal cord stimulation (SCS) is a therapeutic approach used for the management of chronic pain. It involves the delivery of electrical impulses to the spinal cord via an implanted device, which when given suitable stimulus parameters can mask or block pain signals. Selection of optimal stimulation parameters usually happens in the clinic under the care of a provider whereas at-home SCS optimization is managed by the patient. In this paper, we propose a recommender system for the management of pain in chronic pain patients undergoing SCS. In particular, we use a contextual multi-armed bandit (CMAB) approach to develop a system that recommends SCS settings to patients with the aim of improving their condition. These recommendations, sent directly to patients though a digital health ecosystem, combined with a patient monitoring system closes the therapeutic loop around a chronic pain patient over their entire patient journey. We evaluated the system in a cohort of SCS-implanted ENVISION 
    
[^57]: 一种稳健的纯边缘计算工作负载编排

    A Robust Adaptive Workload Orchestration in Pure Edge Computing. (arXiv:2309.03913v1 [cs.DC])

    [http://arxiv.org/abs/2309.03913](http://arxiv.org/abs/2309.03913)

    本论文提出了一种稳健的纯边缘计算工作负载编排（R-AdWOrch）模型，通过使用优先级定义和重新分配策略，最小化截止时间的未达成以及低优先级任务的数据丢失。实验结果显示，R-AdWOrch能够在所有条件下尽量减少紧急任务的截止时间未达成和低优先级任务的数据丢失。

    

    纯边缘计算旨在将云应用和服务带到网络边缘，以支持不断增长的用户对时态应用和数据驱动计算的需求。然而，边缘设备的移动性和有限的计算能力对于支持一些紧急和计算密集型任务且具有严格响应时间要求的任务带来挑战。如果这些任务的执行结果超过了截止时间，它们就变得毫无价值，并且可能导致严重的安全问题。因此，确保边缘节点尽可能完成尽可能多的低延迟任务是非常重要的。

    Pure Edge computing (PEC) aims to bring cloud applications and services to the edge of the network to support the growing user demand for time-sensitive applications and data-driven computing. However, mobility and limited computational capacity of edge devices pose challenges in supporting some urgent and computationally intensive tasks with strict response time demands. If the execution results of these tasks exceed the deadline, they become worthless and can cause severe safety issues. Therefore, it is essential to ensure that edge nodes complete as many latency-sensitive tasks as possible. \\In this paper, we propose a Robust Adaptive Workload Orchestration (R-AdWOrch) model to minimize deadline misses and data loss by using priority definition and a reallocation strategy. The results show that R-AdWOrch can minimize deadline misses of urgent tasks while minimizing the data loss of lower priority tasks under all conditions.
    
[^58]: DrugChat：实现聊天GPT样能力于药物分子图上的探索

    DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs. (arXiv:2309.03907v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.03907](http://arxiv.org/abs/2309.03907)

    本研究开发了一个名为DrugChat的药物分子图聊天系统，该系统实现了类似ChatGPT的能力。通过上传化合物分子图并进行多轮互动，用户可以向DrugChat提问并获得回答，这可以加速药物发现、优化结构-活性关系、指导前导化合物优化等。系统由图神经网络、大型语言模型和适配器组成。

    

    在药物研究中，一种类似ChatGPT的药物化合物系统可能是一个改变游戏规则的因素，加快药物发现，增强我们对结构-活性关系的理解，指导前导化合物优化，帮助药物再利用，减少失败率并简化临床试验。本文通过开发一个名为DrugChat的原型系统，初步尝试在药物分子图上实现类似ChatGPT的能力。DrugChat的工作方式类似于ChatGPT。用户上传一个化合物分子图并提出各种关于该化合物的问题。DrugChat将以多轮互动的方式回答这些问题。DrugChat系统由一个图神经网络（GNN）、一个大型语言模型（LLM）和一个适配器组成。GNN将化合物分子图作为输入，学习该图的表示。适配器将GNN生成的图表示转换为LLM可接受的另一个表示。LLM接收该表示并回答用户的问题。

    A ChatGPT-like system for drug compounds could be a game-changer in pharmaceutical research, accelerating drug discovery, enhancing our understanding of structure-activity relationships, guiding lead optimization, aiding drug repurposing, reducing the failure rate, and streamlining clinical trials. In this work, we make an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat works in a similar way as ChatGPT. Users upload a compound molecule graph and ask various questions about this compound. DrugChat will answer these questions in a multi-turn, interactive manner. The DrugChat system consists of a graph neural network (GNN), a large language model (LLM), and an adaptor. The GNN takes a compound molecule graph as input and learns a representation for this graph. The adaptor transforms the graph representation produced by the GNN into another representation that is acceptable to the LLM. The LLM takes th
    
[^59]: 使用源原型的强大负学习方法来进行部分领域适应的鲁棒性

    A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes. (arXiv:2309.03531v1 [cs.CV])

    [http://arxiv.org/abs/2309.03531](http://arxiv.org/abs/2309.03531)

    本文提出了一个鲁棒的部分领域适应（PDA）框架，通过整合鲁棒的目标监督策略，解决了负迁移问题，并在领域无关的方式下优化了类内紧凑性和类间分离性。通过预先推断源原型，确保了源数据的隐私性。实验证实了该框架的有效性。

    

    本文提出了一个鲁棒的部分领域适应（PDA）框架，通过整合鲁棒的目标监督策略，缓解了负迁移问题。该框架利用集成学习和包含多样化、互补的标签反馈，减轻了错误反馈的影响，并促进伪标签的改进。与仅依赖分布对齐的一阶矩不同，我们的方法通过推断源原型和高可信的目标样本，在领域无关的方式下，提供了优化类内紧凑性和类间分离性的明确目标。值得注意的是，我们通过预先推断源原型，确保了源数据的隐私性，在适应阶段无需访问源数据。我们进行了一系列全面的实验，包括消融分析，涵盖了各种部分领域适应任务。在基准数据集上的全面评估证实了我们的框架的有效性。

    This work proposes a robust Partial Domain Adaptation (PDA) framework that mitigates the negative transfer problem by incorporating a robust target-supervision strategy. It leverages ensemble learning and includes diverse, complementary label feedback, alleviating the effect of incorrect feedback and promoting pseudo-label refinement. Rather than relying exclusively on first-order moments for distribution alignment, our approach offers explicit objectives to optimize intra-class compactness and inter-class separation with the inferred source prototypes and highly-confident target samples in a domain-invariant fashion. Notably, we ensure source data privacy by eliminating the need to access the source data during the adaptation phase through a priori inference of source prototypes. We conducted a series of comprehensive experiments, including an ablation analysis, covering a range of partial domain adaptation tasks. Comprehensive evaluations on benchmark datasets corroborate our framewo
    
[^60]: R2D2: 用于射电天文学中近实时高动态范围成像的深度神经网络系列

    R2D2: Deep neural network series for near real-time high-dynamic range imaging in radio astronomy. (arXiv:2309.03291v1 [astro-ph.IM] CROSS LISTED)

    [http://arxiv.org/abs/2309.03291](http://arxiv.org/abs/2309.03291)

    R2D2是用于射电天文中高动态范围成像的深度神经网络系列，采用模型驱动方法和数据一致性更新，重建为一系列残差图像，可用于高分辨率强度成像。

    

    我们提出了一种新颖的人工智能方法，通过射电干涉测量（RI）在天文学中实现高分辨率高动态范围合成成像。R2D2代表“高动态范围成像的残差到残差DNN系列”，是一种基于模型的数据驱动方法，依赖于混合深度神经网络（DNN）和数据一致性更新。它的重建是由一系列残差图像组成的，这些残差图像被估计为DNN的输出，每个DNN都以上一次迭代的残差脏图片作为输入。该方法可以解释为匹配追踪方法的学习版本，其中模型组件从残差脏图片中迭代地识别出来，CLEAN就是一个众所周知的例子。我们提出了R2D2模型的两个变体，分别基于两种不同的DNN架构：标准的U-Net和一种新颖的展开架构。我们展示了它们在S波段对射电星系Cygnus~A的高灵敏度观测中用于单色强度成像的应用。

    We present a novel AI approach for high-resolution high-dynamic range synthesis imaging by radio interferometry (RI) in astronomy. R2D2, standing for "{R}esidual-to-{R}esidual {D}NN series for high-{D}ynamic range imaging", is a model-based data-driven approach relying on hybrid deep neural networks (DNNs) and data-consistency updates. Its reconstruction is built as a series of residual images estimated as the outputs of DNNs, each taking the residual dirty image of the previous iteration as an input. The approach can be interpreted as a learned version of a matching pursuit approach, whereby model components are iteratively identified from residual dirty images, and of which CLEAN is a well-known example. We propose two variants of the R2D2 model, built upon two distinctive DNN architectures: a standard U-Net, and a novel unrolled architecture. We demonstrate their use for monochromatic intensity imaging on highly-sensitive observations of the radio galaxy Cygnus~A at S band, from the
    
[^61]: 通过深度强化学习学习充电：无人机覆盖路径规划

    Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning. (arXiv:2309.03157v1 [cs.RO])

    [http://arxiv.org/abs/2309.03157](http://arxiv.org/abs/2309.03157)

    本论文提出了一种通过深度强化学习学习充电来解决无人机覆盖路径规划问题的方法。该方法利用基于地图的观测信息，在整个任务周期内优化覆盖轨迹，并采用动作屏蔽和折扣因子调度等技术。实验结果表明，该方法优于基准启发式方法，在不同目标区域和地图上具有一定的泛化性能。

    

    覆盖路径规划（CPP）是机器人学中一个关键问题，其目标是找到一个有效的路径，覆盖兴趣区域中的每一个点。本文解决了充电有限的无人机（UAV）的电力限制CPP问题。在这个问题中，将充电旅程整合到整体覆盖策略中带来了一个显著的挑战，突出了制定战略性、长期性决策的复杂任务。我们提出了一种基于近端策略优化（PPO）的深度强化学习（DRL）方法，利用基于地图的观测信息，运用动作屏蔽和折扣因子调度来优化整个任务周期内的覆盖轨迹。我们还提供了一个位置历史记录给智能体，以处理充电能力引起的新出现的状态循环。我们的方法优于基准启发式方法，在不同目标区域和地图上具有泛化性能，但对未知地图的泛化性能有限。

    Coverage path planning (CPP) is a critical problem in robotics, where the goal is to find an efficient path that covers every point in an area of interest. This work addresses the power-constrained CPP problem with recharge for battery-limited unmanned aerial vehicles (UAVs). In this problem, a notable challenge emerges from integrating recharge journeys into the overall coverage strategy, highlighting the intricate task of making strategic, long-term decisions. We propose a novel proximal policy optimization (PPO)-based deep reinforcement learning (DRL) approach with map-based observations, utilizing action masking and discount factor scheduling to optimize coverage trajectories over the entire mission horizon. We further provide the agent with a position history to handle emergent state loops caused by the recharge capability. Our approach outperforms a baseline heuristic, generalizes to different target zones and maps, with limited generalization to unseen maps. We offer valuable in
    
[^62]: 线性动态系统的因果结构恢复：基于FFT的方法

    Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach. (arXiv:2309.02571v1 [cs.LG])

    [http://arxiv.org/abs/2309.02571](http://arxiv.org/abs/2309.02571)

    该论文提出了一种基于FFT的方法，用于恢复线性动态系统的因果结构。通过降低计算复杂度，可以有效地从时间序列中获取动态因果效应。

    

    从数据中学习因果效应是科学中一个基础且广泛研究的问题，尤其是当因果关系是静态的时候。然而，在存在跨时间点实体之间依赖的情况下，对动态因果效应的识别较少被探索。与静态情况相比，从时间序列观测中获取动态因果效应的计算复杂度较高。我们展示了对向量自回归 (VAR) 模型恢复因果结构的计算复杂度为 $O(Tn^3N^2)$，其中 $n$ 是节点数，$T$ 是样本数，$N$ 是实体之间的最大时间滞后。我们提出了一种复杂度为 $O(Tn^3\log N)$ 的方法，用于恢复因果结构以获得频域 (FD) 表示的时间序列。由于FFT将所有时间依赖关系积累在每个频率上，可以高效地进行因果推断。

    Learning causal effects from data is a fundamental and well-studied problem across science, especially when the cause-effect relationship is static in nature. However, causal effect is less explored when there are dynamical dependencies, i.e., when dependencies exist between entities across time. Identifying dynamic causal effects from time-series observations is computationally expensive when compared to the static scenario. We demonstrate that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. Since FFT accumulates all the time dependencies on every frequency, causal inference can be performed efficiently by consider
    
[^63]: TensorBank: 基于Tensor的湖仓库用于基础模型训练

    TensorBank:Tensor Lakehouse for Foundation Model Training. (arXiv:2309.02094v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.02094](http://arxiv.org/abs/2309.02094)

    TensorBank是一个基于Tensor的湖仓库，能够以高速从云对象存储流式传输张量到GPU内存，并通过使用分层统计指标进行查询加速。

    

    随着基础模型在自然语言之外的领域的兴起，存储和流式处理高维数据成为基础模型训练的关键需求。在本文中，我们介绍了TensorBank，一个能够基于复杂关系查询从云对象存储（COS）流式传输张量到GPU内存的百亿级张量湖仓库。我们使用分层统计指标（HSI）来加速查询。我们的架构允许使用HTTP范围读取来直接访问块级别的张量。一旦在GPU内存中，数据可以使用PyTorch转换进行转换。我们提供了一个通用的PyTorch数据集类型，配有相应的数据集工厂，用于将关系查询和请求的转换作为一个实例进行翻译。通过使用HSI，可以跳过不相关的块，而无需读取它们，因为这些索引包含不同层次分辨率级别上内容的统计信息。这是一个基于开放标准的有主观观点的架构。

    Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making h
    
[^64]: LoopTune: 使用强化学习优化张量计算

    LoopTune: Optimizing Tensor Computations with Reinforcement Learning. (arXiv:2309.01825v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01825](http://arxiv.org/abs/2309.01825)

    LoopTune是一个使用强化学习优化张量计算的编译器，通过优化张量遍历顺序和使用代码生成器LoopNest执行硬件特定优化，LoopTune能够生成比其他编译器更快的代码。通过采用新的图形表示和动作空间，LoopTune比TVM快一个数量级，比MetaSchedule快2.8倍，比AutoTVM快1.08倍，并持续在与手工调优的库Numpy相当的水平上工作。此外，LoopTune优化代码的时间只需几秒钟。

    

    先进的编译器技术对于使机器学习应用在新型硬件上运行至关重要，但传统编译器无法提供性能，普通的自动调节器搜索时间长，经专家优化的库导致不可持续的成本。为了解决这个问题，我们开发了LoopTune，这是一个深度强化学习编译器，用于优化CPU中深度学习模型中的张量计算。LoopTune在使用超快轻量级代码生成器LoopNest执行硬件特定优化的同时，优化张量遍历顺序。通过采用新颖的基于图的表示和动作空间，LoopTune将LoopNest的速度提高了3.2倍，生成的代码比TVM快一个数量级，比MetaSchedule快2.8倍，比AutoTVM快1.08倍，并持续在手工调整的库Numpy的水平上运行。此外，LoopTune可以在几秒钟内优化代码。

    Advanced compiler technology is crucial for enabling machine learning applications to run on novel hardware, but traditional compilers fail to deliver performance, popular auto-tuners have long search times and expert-optimized libraries introduce unsustainable costs. To address this, we developed LoopTune, a deep reinforcement learning compiler that optimizes tensor computations in deep learning models for the CPU. LoopTune optimizes tensor traversal order while using the ultra-fast lightweight code generator LoopNest to perform hardware-specific optimizations. With a novel graph-based representation and action space, LoopTune speeds up LoopNest by 3.2x, generating an order of magnitude faster code than TVM, 2.8x faster than MetaSchedule, and 1.08x faster than AutoTVM, consistently performing at the level of the hand-tuned library Numpy. Moreover, LoopTune tunes code in order of seconds.
    
[^65]: 关于后续GNN解释器对标签噪声的鲁棒性的研究

    On the Robustness of Post-hoc GNN Explainers to Label Noise. (arXiv:2309.01706v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01706](http://arxiv.org/abs/2309.01706)

    本文对后续GNN解释器在面对标签噪声时的鲁棒性进行了研究，并发现即使是轻微的标签噪声也会严重影响解释质量。

    

    后续GNN解释器被提出作为解决图神经网络(GNNs)固有黑盒限制的方案，旨在提供对训练后的GNN表现行为的精确和深刻的解释。尽管在学术和工业环境中最近有显著进展，但后续GNN解释器在面对标签噪声时的鲁棒性尚未被探索。为了填补这一空白，我们进行了系统的实证研究，评估了不同程度标签噪声下各种后续GNN解释器的功效。我们的结果揭示了几个关键见解：首先，后续GNN解释器容易受到标签扰动的影响。其次，即使是对GNN表现无关紧要的轻微标签噪声，也会严重损害生成的解释质量。最后，我们就随着噪声水平的升高逐渐恢复解释效果展开讨论。

    Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.
    
[^66]: 自监督的序列模型中的新型线性表示

    Emergent Linear Representations in World Models of Self-Supervised Sequence Models. (arXiv:2309.00941v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.00941](http://arxiv.org/abs/2309.00941)

    本研究提供了自监督的序列模型中新型线性表示的证据，并表明通过探测棋盘的"我的颜色"与"对手的颜色"可以解释模型的内部状态。这种准确的内部表示理解使得我们能够通过简单的向量运算来控制模型的行为，并显著推进了可解释性的进展。

    

    序列模型如何表示其决策过程？之前的研究表明，黑白棋博弈神经网络学到了非线性的棋盘状态模型（Li等，2023）。在本研究中，我们提供了相关的证据，表明存在一个密切相关的线性棋盘表示。特别是，我们展示了通过探测"我的颜色"与"对手的颜色"这种简单而强大的方式来解释模型的内部状态。对内部表示的准确理解使我们能够通过简单的向量运算来控制模型的行为。线性表示使得重要的可解释性进展成为可能，我们通过进一步探索世界模型的计算来证明这一点。

    How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.
    
[^67]: 图神经网络中的隐私调查：攻击、保护和应用

    A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])

    [http://arxiv.org/abs/2308.16375](http://arxiv.org/abs/2308.16375)

    这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。

    

    随着处理图结构数据的能力和实际应用的改善，图神经网络（GNNs）引起了人们的极大关注。然而，许多这些模型优先考虑高效能表现，如准确性，而缺乏隐私考虑，这是现代社会隐私攻击盛行的重要问题。为了解决这个问题，研究人员开始开发保护隐私的GNNs。尽管取得了进展，但在图领域缺乏对攻击和隐私保护技术的综合概述。在本调查中，我们旨在通过总结针对图数据的攻击、对GNNs中的隐私保护技术进行分类以及审查可用于分析/解决GNNs中隐私问题的数据集和应用程序，填补这一空白。我们还概述了未来研究的潜在方向，以建立更好的隐私保护GNNs。

    Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
    
[^68]: 测量篡改检测基准

    Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])

    [http://arxiv.org/abs/2308.15605](http://arxiv.org/abs/2308.15605)

    本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。

    

    在训练强大的人工智能系统来执行复杂任务时，提供对优化具有稳健性的训练信号可能是具有挑战性的。一个问题是测量篡改，即人工智能系统操纵多个测量结果，以营造良好结果的假象，而不是实现期望的结果。在这项工作中，我们构建了四个新的基于文本的数据集，用于评估大规模语言模型上的测量篡改检测技术。具体来说，给定一组文本输入和测量结果，旨在确定某个结果是否发生，以及一个能够准确预测测量结果的基础模型，目标是确定所有测量结果都表明结果发生的示例是否确实发生了结果，或者这是由于测量篡改引起的。我们展示了在大多数数据集上优于简单基准的技术，但没有达到最佳性能。我们相信在技术和数据集方面都有很大的改进空间，我们感到兴奋。

    When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited 
    
[^69]: 什么时候编程思维对推理起作用?

    When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])

    [http://arxiv.org/abs/2308.15452](http://arxiv.org/abs/2308.15452)

    提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。

    

    大型语言模型（LLM）的推理能力在体现出人工智能领域中起着关键作用。尽管像编程思维提示这样的方法对于使用编程语言来解决复杂推理任务的LLM非常有效，但代码数据对推理能力的具体影响仍未充分探索。为了填补这一空白，我们提出了复杂性影响推理分数（CIRS），它结合了结构和逻辑属性，以衡量代码和推理能力之间的相关性。具体而言，我们使用抽象语法树来编码结构信息，并通过考虑难度和圈复杂度来计算逻辑复杂性。通过实证分析，我们发现并非所有复杂性的代码数据都可以被LLM学习或理解。最佳复杂性水平对于通过编程辅助提示改善推理能力至关重要。然后我们设计了一个自动合成的方法...

    The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
    
[^70]: 大语言模型赋能文本到SQL的研究：一个基准评估

    Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])

    [http://arxiv.org/abs/2308.15363](http://arxiv.org/abs/2308.15363)

    本文提出了一个大规模语言模型(LLMs)赋能的文本到SQL任务的基准评估，并基于实验结果提出了一种新的集成解决方案DAIL-SQL，刷新了Spider榜单并实现了86.6%的执行准确率。同时，强调了在提示工程中的词汇效率以实现高效经济的LLM-based文本到SQL解决方案，此外还对在上下文学习中应用开源LLMs进行了研究，并进行了任务特定的性能优化。

    

    大语言模型(LLMs)已经成为文本到SQL任务的一种新范式。然而，缺乏一个系统性的基准阻碍了设计有效、高效和经济的LLM-based文本到SQL解决方案的发展。为了解决这一挑战，本文首先对现有的提示工程方法进行了系统性和广泛的比较，包括问题表示、示例选择和示例组织，并根据实验结果详细阐述了它们的优缺点。基于这些发现，我们提出了一种新的集成解决方案，名为DAIL-SQL，刷新了Spider榜单，达到了86.6%的执行准确率，建立了一个新的标杆。为了实现高效经济的LLM-based文本到SQL解决方案，我们强调提示工程中的词汇效率，并在此度量下比较了之前的研究。此外，我们还研究了上下文学习中的开源LLMs，并用任务特定的监督进行了进一步的性能优化。

    Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
    
[^71]: 一个用于完全无监督异常检测的通用机器学习框架，适用于污染数据

    A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])

    [http://arxiv.org/abs/2308.13352](http://arxiv.org/abs/2308.13352)

    这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。

    

    在各个领域和应用中，机器学习算法已经解决了异常检测（AD）任务。这些算法中大多数使用正常数据对一个基于残差的模型进行训练，并根据未见样本与学习到的正常范围的不相似性来分配异常分数。这些方法的基本假设是可以用无异常的数据进行训练。然而，在真实世界中的操作环境中，训练数据通常会与一定比例的异常样本混合。而利用污染数据进行训练必然会导致基于残差的算法的AD性能下降。本文介绍了一个完全无监督的用于AD任务的污染训练数据的改进框架。该框架是通用的，可应用于任何基于残差的机器学习模型。我们展示了该框架在两个多元时间数据集上的应用。

    Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
    
[^72]: 量子启发式机器学习：一项调查

    Quantum-Inspired Machine Learning: a Survey. (arXiv:2308.11269v1 [cs.LG])

    [http://arxiv.org/abs/2308.11269](http://arxiv.org/abs/2308.11269)

    量子启发式机器学习（QiML）是一个利用量子力学原理在经典计算框架中的新领域，在本调查中我们提供了对QiML的综合和全面的研究，展示了其多样化的研究领域、最新进展和实际应用，并为QiML建立了明确的定义。未来的研究将从量子力学、量子计算和经典机器学习中汲取经验，丰富QiML的领域。

    

    量子启发式机器学习（QiML）是一个不断发展的领域，因其在经典计算框架中利用量子力学原理的潜力而受到全球研究人员的关注。然而，目前的综述文献经常只对QiML进行表面探索，而更多关注广义的量子机器学习（QML）领域。为了弥补这一空白，本调查提供了对QiML的综合和全面的研究，探讨了QiML的多样化研究领域，包括张量网络模拟、非量子化算法等，展示了最新的进展、实际应用，并揭示了潜在的未来研究方向。此外，通过分析先前对QiML的各种解释及其内在的模糊性，建立了一个具体的QiML定义。随着QiML的不断发展，我们预计将有大量的未来发展从量子力学、量子计算和经典机器学习中汲取经验，丰富QiML的领域。

    Quantum-inspired Machine Learning (QiML) is a burgeoning field, receiving global attention from researchers for its potential to leverage principles of quantum mechanics within classical computational frameworks. However, current review literature often presents a superficial exploration of QiML, focusing instead on the broader Quantum Machine Learning (QML) field. In response to this gap, this survey provides an integrated and comprehensive examination of QiML, exploring QiML's diverse research domains including tensor network simulations, dequantized algorithms, and others, showcasing recent advancements, practical applications, and illuminating potential future research avenues. Further, a concrete definition of QiML is established by analyzing various prior interpretations of the term and their inherent ambiguities. As QiML continues to evolve, we anticipate a wealth of future developments drawing from quantum mechanics, quantum computing, and classical machine learning, enriching 
    
[^73]: LadleNet: 使用可扩展的两阶段U-Net将热红外图像转换为可见光图像

    LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net. (arXiv:2308.06603v1 [cs.CV])

    [http://arxiv.org/abs/2308.06603](http://arxiv.org/abs/2308.06603)

    LadleNet是一种使用可扩展的两阶段U-Net将热红外图像转换为可见光图像的算法，通过引入跳跃连接和精细特征聚合技术，显著提高了模型性能。Handle模块构建抽象语义空间，Bowl模块解码该空间生成映射的VI图像。

    

    将热红外（TIR）图像转换为可见光（VI）图像是一项具有挑战性的任务，具有潜在的应用领域，如TIR-VI图像配准和融合。利用从TIR图像转换中得到的补充信息可以显着提高模型性能和应用程序的泛化能力。然而，该领域存在的主要问题包括图像保真度不高和模型可扩展性有限。本文介绍了一种基于U-Net架构的算法LadleNet。 LadleNet采用了两阶段U-Net串联结构，增加了跳跃连接和精细特征聚合技术，从而显著提高了模型性能。LadleNet由“Handle”和“Bowl”模块组成，Handle模块用于构建抽象语义空间，而Bowl模块则解码这个抽象语义空间，生成映射的VI图像。

    The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibilit
    
[^74]: 知识驱动的多智能体强化学习用于物联网车辆中的计算卸载

    Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles. (arXiv:2308.02603v1 [cs.LG])

    [http://arxiv.org/abs/2308.02603](http://arxiv.org/abs/2308.02603)

    这项研究提出了一种基于知识驱动的多智能体强化学习方法，用于解决物联网车辆中计算卸载的延迟问题。通过将计算任务卸载到边缘单元，可以减轻车载计算负担。该方法利用图神经网络结合领域知识，选择每辆车的最佳卸载选项。

    

    通过将车辆的计算密集型任务卸载到道路边的单元(RSU)，移动边缘计算(MEC)在物联网车辆(IoV)中可以减轻车载计算负担。

    By offloading computation-intensive tasks of vehicles to roadside units (RSUs), mobile edge computing (MEC) in the Internet of Vehicles (IoV) can relieve the onboard computation burden. However, existing model-based task offloading methods suffer from heavy computational complexity with the increase of vehicles and data-driven methods lack interpretability. To address these challenges, in this paper, we propose a knowledge-driven multi-agent reinforcement learning (KMARL) approach to reduce the latency of task offloading in cybertwin-enabled IoV. Specifically, in the considered scenario, the cybertwin serves as a communication agent for each vehicle to exchange information and make offloading decisions in the virtual space. To reduce the latency of task offloading, a KMARL approach is proposed to select the optimal offloading option for each vehicle, where graph neural networks are employed by leveraging domain knowledge concerning graph-structure communication topology and permutation
    
[^75]: 准确的神经网络剪枝需要重新思考稀疏优化

    Accurate Neural Network Pruning Requires Rethinking Sparse Optimization. (arXiv:2308.02060v1 [cs.LG])

    [http://arxiv.org/abs/2308.02060](http://arxiv.org/abs/2308.02060)

    这项工作研究了高稀疏对神经网络训练的影响，发现使用传统的密集训练策略进行稀疏训练效果不佳，提出了新的方法来解决这个问题，并在视觉和语言模型上都取得了最先进的结果。

    

    在模型压缩领域，获得既高精确又高稀疏的深度神经网络版本是一个主要挑战，社区已经对几种高性能的剪枝技术进行了研究。然而，我们对稀疏性和用于训练稀疏网络的标准随机优化技术的交互了解较少，大多数现有工作使用标准的密集训练计划和超参数来训练稀疏网络。在这项工作中，我们通过使用标准的计算机视觉和自然语言处理稀疏基准来研究高稀疏对模型训练的影响。我们首先展示了使用标准的密集训练策略进行稀疏训练是次优的，导致欠训练。我们提供了新的方法来解决这个问题，既可以用于视觉模型（如ResNet50/ImageNet）的稀疏预训练，也可以用于语言模型（如BERT/GLUE）的稀疏微调，实现了最先进的结果。

    Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art resul
    
[^76]: 核化归一化流

    Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])

    [http://arxiv.org/abs/2307.14839](http://arxiv.org/abs/2307.14839)

    本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。

    

    归一化流是以其可逆的架构而被描述的生成模型。然而，可逆性要求对其表达能力施加限制，需要大量的参数和创新的架构设计来达到满意的结果。虽然基于流的模型主要依赖于基于神经网络的转换来实现表达能力，但替代的转换方法却受到了有限的关注。在这项工作中，我们提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到框架中。我们的结果表明，相比于基于神经网络的流，核化流可以产生有竞争力或优越的结果，同时保持参数效率。核化流在低数据环境中表现出色，可以在数据稀缺的应用中进行灵活的非参数密度估计。

    Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
    
[^77]: Temporal Graph Benchmark的实证评估

    An Empirical Evaluation of Temporal Graph Benchmark. (arXiv:2307.12510v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12510](http://arxiv.org/abs/2307.12510)

    本文对Temporal Graph Benchmark进行了实证评估，通过扩展动态图库(DyGLib)到TGB，并使用十一种动态图学习方法进行全面比较。实验发现不同模型在不同数据集上表现出不同的性能，一些基线方法的性能可以通过使用DyGLib显著提高。

    

    本文通过将我们的动态图库(DyGLib)扩展到Temporal Graph Benchmark (TGB)，对TGB进行了实证评估。与TGB相比，我们包括了十一种流行的动态图学习方法进行更全面的比较。通过实验，我们发现：（1）不同模型在不同数据集上表现出不同的性能，这与之前的观察一致；（2）使用DyGLib时，一些基线方法的性能可以显著提高。本工作旨在方便研究人员在TGB上评估各种动态图学习方法，并试图提供可直接参考的结果供后续研究使用。本项目中使用的所有资源均可在https://github.com/yule-BUAA/DyGLib_TGB上公开获取。本工作正在进行中，欢迎社区提供反馈以进行改进。

    In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
    
[^78]: 异构联邦学习：现状与研究挑战的技术综述

    Heterogeneous Federated Learning: State-of-the-art and Research Challenges. (arXiv:2307.10616v1 [cs.LG])

    [http://arxiv.org/abs/2307.10616](http://arxiv.org/abs/2307.10616)

    异构联邦学习是联邦学习领域中的一个重要研究方向，涉及到数据分布、模型架构、网络环境和硬件设备的异质性挑战。本文对异构联邦学习的研究挑战和最新进展进行了综述和分类，为进一步的研究提供了参考。

    

    联邦学习 (FL) 由于在大规模工业应用中的潜在用途而受到越来越多的关注。现有的联邦学习研究主要针对模型同质的情况。然而，实际的联邦学习通常面临参与方之间的数据分布、模型架构、网络环境和硬件设备的异质性。异构联邦学习 (HFL) 更具挑战性，相应的解决方案多样且复杂。因此，对这个主题进行关于研究挑战和最新进展的系统调查至关重要。在这项调查中，我们首先总结了 HFL 中来自五个方面的各种研究挑战：统计异质性、模型异质性、通信异质性、设备异质性和额外挑战。此外，我们回顾了 HFL 中的最新进展，并提出了对现有 HFL 方法的新分类法，并对其优缺点进行了深入分析。

    Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify
    
[^79]: 在图像表示中识别可解释子空间

    Identifying Interpretable Subspaces in Image Representations. (arXiv:2307.10504v1 [cs.CV])

    [http://arxiv.org/abs/2307.10504](http://arxiv.org/abs/2307.10504)

    FALCON是一个解释图像表示特征的框架，可以通过使用字幕数据集和视觉语言模型来生成人类可理解的概念，并通过对比解释消除虚假概念。在较大的空间中，特征通过研究组合可以更易解释和高阶评分概念的解释。

    

    我们提出了一种解释图像表示特征的可解释性框架——FALCON（Automatic Feature Explanation using Contrasting Concepts）。对于目标特征，FALCON使用一个大型的字幕数据集（如LAION-400m）和一个预训练的视觉语言模型（如CLIP）对其高度激活的裁剪图像进行字幕生成。每个字幕中的单词都经过评分和排序，从而得到了与目标特征密切相关的少数共享的、人类可理解的概念。FALCON还使用低激活的（对立的）图像应用对比解释，以消除虚假概念。尽管许多现有方法独立解释特征，但我们观察到在最先进的自监督和监督模型中，不到20%的表示空间可以通过单个特征进行解释。我们展示了当一组特征一起研究时，更大的空间中的特征变得更易解释，并可以通过FALCON得到高阶评分概念的解释。

    We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON.
    
[^80]: 基于图神经网络的太赫兹流导向纳米尺度定位

    Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization. (arXiv:2307.05551v1 [cs.LG])

    [http://arxiv.org/abs/2307.05551](http://arxiv.org/abs/2307.05551)

    本研究提出了一种基于图神经网络的太赫兹流导向纳米尺度定位方法，可以提高定位精度和覆盖范围，解决了现有方法的定位精度低和无法全局定位的问题。

    

    纳米技术和先进材料的科学进展为体内精准医学的纳米尺度装置铺平了道路，其中包括集成感应、计算、通信、数据和能量存储能力。在人体心血管系统中，这些装置被设想为被动流动并持续感知以便检测诊断感兴趣的事件。通过将这些事件的物理位置（如身体区域）分配给它们，可以提高检测到这些事件的诊断价值，这是流导向定位的主要命题。当前的流导向定位方法存在定位精度低和无法在整个心血管系统内本地化事件的问题。为了解决这个问题，我们提出利用图神经网络（GNNs）来进行定位，并证明我们的方法在定位精度和覆盖范围上优于现有的最先进方法。

    Scientific advancements in nanotechnology and advanced materials are paving the way toward nanoscale devices for in-body precision medicine; comprising integrated sensing, computing, communication, data and energy storage capabilities. In the human cardiovascular system, such devices are envisioned to be passively flowing and continuously sensing for detecting events of diagnostic interest. The diagnostic value of detecting such events can be enhanced by assigning to them their physical locations (e.g., body region), which is the main proposition of flow-guided localization. Current flow-guided localization approaches suffer from low localization accuracy and they are by-design unable to localize events within the entire cardiovascular system. Toward addressing this issue, we propose the utilization of Graph Neural Networks (GNNs) for this purpose, and demonstrate localization accuracy and coverage enhancements of our proposal over the existing State of the Art (SotA) approaches. Based
    
[^81]: $\mathbf{\mathbb{E}^{FWI}}$:多参数地球物理属性弹性全波形反演的基准数据集

    $\mathbf{\mathbb{E}^{FWI}}$: Multi-parameter Benchmark Datasets for Elastic Full Waveform Inversion of Geophysical Properties. (arXiv:2306.12386v1 [physics.geo-ph])

    [http://arxiv.org/abs/2306.12386](http://arxiv.org/abs/2306.12386)

    本文介绍了一个专门为弹性全波形反演设计的全面基准数据集$\mathbf{\mathbb{E}^{FWI}}$，其包括8个不同的数据集，并提供了三种不同深度学习方法产生的基准结果。

    

    弹性地球物理属性（如P和S波速度）在CO$_2$封存和能源勘探（如氢和地热能）等各种地下应用中非常重要。弹性全波形反演(FWI)广泛应用于表征储层属性。本文介绍了$\mathbf{\mathbb{E}^{FWI}}$，一个专门为弹性FWI设计的全面基准数据集。$\mathbf{\mathbb{E}^{FWI}}$包括8个不同的数据集，涵盖各种不同的地下地质结构（平坦、曲线、断层等）。提供了三种不同深度学习方法产生的基准结果。与我们先前提供的声学FWI压力记录数据集（称为OpenFWI）相比，$\mathbf{\mathbb{E}^{FWI}}$的地震数据集具有垂直和水平分量。此外，$\mathbf{\mathbb{E}^{FWI}}$中的速度图包含了P和S波速度。虽然多分量FWI一直以来都是一个热门话题，但如何利用深度学习技术进行弹性FWI的多参数反演仍然是一个具有挑战性的课题。

    Elastic geophysical properties (such as P- and S-wave velocities) are of great importance to various subsurface applications like CO$_2$ sequestration and energy exploration (e.g., hydrogen and geothermal). Elastic full waveform inversion (FWI) is widely applied for characterizing reservoir properties. In this paper, we introduce $\mathbf{\mathbb{E}^{FWI}}$, a comprehensive benchmark dataset that is specifically designed for elastic FWI. $\mathbf{\mathbb{E}^{FWI}}$ encompasses 8 distinct datasets that cover diverse subsurface geologic structures (flat, curve, faults, etc). The benchmark results produced by three different deep learning methods are provided. In contrast to our previously presented dataset (pressure recordings) for acoustic FWI (referred to as OpenFWI), the seismic dataset in $\mathbf{\mathbb{E}^{FWI}}$ has both vertical and horizontal components. Moreover, the velocity maps in $\mathbf{\mathbb{E}^{FWI}}$ incorporate both Pand S-wave velocities. While the multicomponen
    
[^82]: 用于电力系统的GPU加速机器学习模型验证

    GPU-Accelerated Verification of Machine Learning Models for Power Systems. (arXiv:2306.10617v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10617](http://arxiv.org/abs/2306.10617)

    本论文解决了GPU加速机器学习模型验证在电力系统问题上的应用障碍，通过同时验证多个问题，并引入精确转换方法，将一组潜在违规转化为增强原始神经网络能力的层。

    

    最近几年，用于严格验证大规模机器学习模型性能的计算工具取得了显著进展。最成功的求解器采用了高度专业化的、GPU加速的分支和界限算法。这样的工具对于在安全关键系统中成功部署机器学习应用至关重要，例如电力系统。然而，尽管取得了成功，仍然存在障碍阻止了将这些求解器直接应用于电力系统问题。本文通过两种关键方式解决了这个问题。首先，我们首次启用了多个验证问题的同时验证（例如，同时检查所有线路流量约束的违规，并非通过解决单个验证问题）。为此，我们引入了一种精确的转换，将一组潜在违规中的最坏情况违规转化为一系列基于ReLU的层，从而增加了原始神经网络的能力。

    Computational tools for rigorously verifying the performance of large-scale machine learning (ML) models have progressed significantly in recent years. The most successful solvers employ highly specialized, GPU-accelerated branch and bound routines. Such tools are crucial for the successful deployment of machine learning applications in safety-critical systems, such as power systems. Despite their successes, however, barriers prevent out-of-the-box application of these routines to power system problems. This paper addresses this issue in two key ways. First, for the first time to our knowledge, we enable the simultaneous verification of multiple verification problems (e.g., checking for the violation of all line flow constraints simultaneously and not by solving individual verification problems). For that, we introduce an exact transformation that converts the "worst-case" violation across a set of potential violations to a series of ReLU-based layers that augment the original neural n
    
[^83]: RecFusion：基于二项式扩散过程的1D数据推荐模型

    RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation. (arXiv:2306.08947v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2306.08947](http://arxiv.org/abs/2306.08947)

    本文提出了 RecFusion，一种特定针对1D和/或二进制设置的推荐模型方法，其利用了二项式扩散过程对二元用户-项目交互进行显式建模，并在核心推荐设置和最常见的数据集上接近复杂的VAE基线的表现。

    

    本文提出了RecFusion，这是一组用于推荐的扩散模型。不同于包含空间相关性的图像数据，常用于推荐的用户-项目交互矩阵缺乏用户和项目之间的空间关系。我们在一个一维向量上制定了扩散方法，并提出了二项式扩散，这个方法利用了伯努利过程显式地对二元用户-项目交互进行建模。我们展示了RecFusion在核心推荐设置（针对二进制非顺序反馈的前n项推荐）和最常见的数据集（MovieLens和Netflix）上接近于复杂的VAE基线的表现。我们提出的专门针对1D和/或二进制设置的扩散模型的意义超出了推荐系统，例如在医学领域中使用MRI和CT扫描。

    In this paper we propose RecFusion, which comprise a set of diffusion models for recommendation. Unlike image data which contain spatial correlations, a user-item interaction matrix, commonly utilized in recommendation, lacks spatial relationships between users and items. We formulate diffusion on a 1D vector and propose binomial diffusion, which explicitly models binary user-item interactions with a Bernoulli process. We show that RecFusion approaches the performance of complex VAE baselines on the core recommendation setting (top-n recommendation for binary non-sequential feedback) and the most common datasets (MovieLens and Netflix). Our proposed diffusion models that are specialized for 1D and/or binary setups have implications beyond recommendation systems, such as in the medical domain with MRI and CT scans.
    
[^84]: 双非均匀环境下的离线策略评估

    Off-policy Evaluation in Doubly Inhomogeneous Environments. (arXiv:2306.08719v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2306.08719](http://arxiv.org/abs/2306.08719)

    本研究在双非均匀环境下研究了离线策略评估(OPE)，提出了一种潜在因子模型用于奖励和观测转移函数，并开发了一个通用的OPE框架。该研究对标准RL假设不满足的环境中的OPE做出了理论贡献，并提供了几种实用的方法。

    

    本研究旨在研究离线策略评估（OPE）在两个关键的强化学习（RL）假设——时间稳定性和个体均匀性均被破坏的情况下的应用。为了处理“双非均匀性”，我们提出了一类潜在因子模型用于奖励和观测转移函数，并在此基础上开发了一个包含模型驱动和模型自由方法的通用OPE框架。据我们所知，这是第一篇在离线RL中开发统计上可靠的OPE方法的论文，并且涉及了标准RL假设不满足的环境。该研究深入理解了标准RL假设不满足的环境中的OPE，并在这些设置中提供了几种实用的方法。我们确定了所提出的价值估计器的理论性质，并通过实证研究表明我们的方法优于忽视时间非稳定性或个体异质性的竞争方法。最后，我们在一个数据集上说明了我们的方法。

    This work aims to study off-policy evaluation (OPE) under scenarios where two key reinforcement learning (RL) assumptions -- temporal stationarity and individual homogeneity are both violated. To handle the ``double inhomogeneities", we propose a class of latent factor models for the reward and observation transition functions, under which we develop a general OPE framework that consists of both model-based and model-free approaches. To our knowledge, this is the first paper that develops statistically sound OPE methods in offline RL with double inhomogeneities. It contributes to a deeper understanding of OPE in environments, where standard RL assumptions are not met, and provides several practical approaches in these settings. We establish the theoretical properties of the proposed value estimators and empirically show that our approach outperforms competing methods that ignore either temporal nonstationarity or individual heterogeneity. Finally, we illustrate our method on a data set
    
[^85]: 通过多度量调查避免联邦学习中的对抗适应

    Avoid Adversarial Adaption in Federated Learning by Multi-Metric Investigations. (arXiv:2306.03600v1 [cs.LG])

    [http://arxiv.org/abs/2306.03600](http://arxiv.org/abs/2306.03600)

    本文提出了一个新概念——强适应对手，并通过实验表明，现有的防御方法不足以解决现实世界中的对手和数据分布问题。作者使用多度量调查来增强 FL 对这些对手的抵抗力。

    

    联邦学习 (FL) 可以在分布在多个设备上的数据上训练机器学习模型，避免将数据传输到中央位置。这提高了隐私保护，减少了通信成本，并增强了模型性能。然而，FL 容易受到污染攻击，可以是非定向的，旨在降低模型性能，也可以是有目的的，即所谓的后门攻击，这种攻击会添加对抗性行为，可以通过适当制作的输入触发。为了追求隐蔽性，后门攻击更难应对。对抗性攻击缓解技术依赖于监视某些度量标准和过滤恶意模型更新。然而，以前的工作没有考虑到现实世界的对手和数据分布。为了支持我们的论点，我们定义了一个新概念——强适应对手，它可以同时适应多个目标，并通过广泛的测试证明，现有的防御方法在这种对手模型中可以被绕过。我们还证明可以使用多度量标准调查来显著提高 FL 对强适应对手的韧性，而无需额外的数据或模型假设。

    Federated Learning (FL) trains machine learning models on data distributed across multiple devices, avoiding data transfer to a central location. This improves privacy, reduces communication costs, and enhances model performance. However, FL is prone to poisoning attacks, which can be untargeted aiming to reduce the model performance, or targeted, so-called backdoors, which add adversarial behavior that can be triggered with appropriately crafted inputs. Striving for stealthiness, backdoor attacks are harder to deal with.  Mitigation techniques against poisoning attacks rely on monitoring certain metrics and filtering malicious model updates. However, previous works didn't consider real-world adversaries and data distributions. To support our statement, we define a new notion of strong adaptive adversaries that can simultaneously adapt to multiple objectives and demonstrate through extensive tests, that existing defense methods can be circumvented in this adversary model. We also demon
    
[^86]: 评估有噪声判别器对未标记数据的流式算法 -- 二元分类

    Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification. (arXiv:2306.01726v1 [stat.ML])

    [http://arxiv.org/abs/2306.01726](http://arxiv.org/abs/2306.01726)

    本文提出了两种代数评估器来估计未标记数据中有噪声二元分类器的性能。其中，第二种评估器的正确性被保证。作者通过利用独立评估器无法返回合理估计的失败，缓解了委托/代理监控悖论，并通过搜索来寻找几乎无误差的三元组。

    

    本文将对未标记数据中的有噪声二元分类器的评估作为流式任务进行研究: 给定一个分类器决策的数据草图，估计标签的真实流行度以及每个分类器对它们的准确度。本文构建了两种完全代数化的评估器来实现这一目标。两种评估器都基于分类器产生独立错误的假设。第一种是基于多数投票的。而第二种则是本文的主要贡献，并被保证是正确的。但是如何确保分类器在任何给定的测试中是独立的呢？本文通过利用独立评估器无法返回合理估计的失败来缓解这个委托/代理监控悖论。通过利用代数故障模式来拒绝太相关的评估集合，使用 \texttt{adult}，\texttt{mushroom} 和 \texttt{two-norm} 数据集对一组几乎无误差三元组进行了实证搜索。这些搜索通过构建评估空间中的表面来进行精细化。

    The evaluation of noisy binary classifiers on unlabeled data is treated as a streaming task: given a data sketch of the decisions by an ensemble, estimate the true prevalence of the labels as well as each classifier's accuracy on them. Two fully algebraic evaluators are constructed to do this. Both are based on the assumption that the classifiers make independent errors. The first is based on majority voting. The second, the main contribution of the paper, is guaranteed to be correct. But how do we know the classifiers are independent on any given test? This principal/agent monitoring paradox is ameliorated by exploiting the failures of the independent evaluator to return sensible estimates. A search for nearly error independent trios is empirically carried out on the \texttt{adult}, \texttt{mushroom}, and \texttt{two-norm} datasets by using the algebraic failure modes to reject evaluation ensembles as too correlated. The searches are refined by constructing a surface in evaluation spa
    
[^87]: 基于二维单元模型的物理启发式机器学习预测铁钒液流电池性能

    Physics-informed machine learning of redox flow battery based on a two-dimensional unit cell model. (arXiv:2306.01010v1 [cs.LG])

    [http://arxiv.org/abs/2306.01010](http://arxiv.org/abs/2306.01010)

    本文提出了一种利用二维数学模型进行物理启发式神经网络预测铁钒液流电池性能的方法，可正确预测单元电压。

    

    本文提出了一种物理启发式神经网络(PINN)方法，通过一个二维数学模型来预测全钒液流电池的性能，并强制执行其物理约束条件。该二维模型包括6个控制方程和24个边界条件，详细描述了铁钒液流电池内发生的电化学反应、质量传输和流体力学过程。为了使用PINN方法解决2D模型，采用复合神经网络来逼近物种浓度和电位；根据电池系统的先验知识对输入和输出进行规范化处理；将处理后的控制方程和边界条件先缩放到1的数量级，然后采用自重法进一步平衡。数值结果表明，PINN能够正确地预测单元电压，但电位的预测具有类似于常数的偏移。

    In this paper, we present a physics-informed neural network (PINN) approach for predicting the performance of an all-vanadium redox flow battery, with its physics constraints enforced by a two-dimensional (2D) mathematical model. The 2D model, which includes 6 governing equations and 24 boundary conditions, provides a detailed representation of the electrochemical reactions, mass transport and hydrodynamics occurring inside the redox flow battery. To solve the 2D model with the PINN approach, a composite neural network is employed to approximate species concentration and potentials; the input and output are normalized according to prior knowledge of the battery system; the governing equations and boundary conditions are first scaled to an order of magnitude around 1, and then further balanced with a self-weighting method. Our numerical results show that the PINN is able to predict cell voltage correctly, but the prediction of potentials shows a constant-like shift. To fix the shift, th
    
[^88]: 利用局部化提高图神经网络的表达能力

    Improving Expressivity of Graph Neural Networks using Localization. (arXiv:2305.19659v1 [cs.LG])

    [http://arxiv.org/abs/2305.19659](http://arxiv.org/abs/2305.19659)

    本文提出了Weisfeiler-Leman (WL)算法的局部版本，用于解决子图计数问题并提高图神经网络的表达能力，同时，也给出了一些时间和空间效率更高的$k-$WL变体和分裂技术。

    

    本文提出了Weisfeiler-Leman (WL)算法的局部版本，旨在增加表达能力并减少计算负担。我们专注于子图计数问题，并为任意$k$给出$k-$WL的局部版本。我们分析了Local $k-$WL的作用，并证明其比$k-$WL更具表现力，并且至多与$(k+1)-$WL一样具有表现力。我们给出了一些模式的特征，如果两个图是Local $k-$WL等价的，则它们的子图和诱导子图的计数是不变的。我们还介绍了$k-$WL的两个变体：层$k-$WL和递归$k-$WL。这些方法的时间和空间效率比在整个图上应用$k-$WL更高。我们还提出了一种分裂技术，使用$1-$WL即可保证所有大小不超过4的诱导子图的准确计数。相同的方法可以使用$k>1$进一步扩展到更大的模式。我们还将Local $k-$WL的表现力与其他GNN层次结构进行了比较。

    In this paper, we propose localized versions of Weisfeiler-Leman (WL) algorithms in an effort to both increase the expressivity, as well as decrease the computational overhead. We focus on the specific problem of subgraph counting and give localized versions of $k-$WL for any $k$. We analyze the power of Local $k-$WL and prove that it is more expressive than $k-$WL and at most as expressive as $(k+1)-$WL. We give a characterization of patterns whose count as a subgraph and induced subgraph are invariant if two graphs are Local $k-$WL equivalent. We also introduce two variants of $k-$WL: Layer $k-$WL and recursive $k-$WL. These methods are more time and space efficient than applying $k-$WL on the whole graph. We also propose a fragmentation technique that guarantees the exact count of all induced subgraphs of size at most 4 using just $1-$WL. The same idea can be extended further for larger patterns using $k>1$. We also compare the expressive power of Local $k-$WL with other GNN hierarc
    
[^89]: Photo-zSNthesis: 通过深度学习将Ia型超新星光变曲线转化为红移估计

    Photo-zSNthesis: Converting Type Ia Supernova Lightcurves to Redshift Estimates via Deep Learning. (arXiv:2305.11869v1 [astro-ph.CO])

    [http://arxiv.org/abs/2305.11869](http://arxiv.org/abs/2305.11869)

    本研究提出了一种基于卷积神经网络的方法"Photo-zSNthesis"，用于从多波段超新星光变曲线预测完整的红移概率分布，无需光谱信息。该方法在模拟和真实观测中都取得了重大提升，能极大地约束宇宙学。

    

    即将到来的光度巡天将会发现数以万计的Ia型超新星(SNe Ia)，远远超过了我们的光谱资源容量。为了在没有光谱信息的情况下最大化观测数据的科学回报，我们必须只利用光度信息准确地提取关键参数，例如SN红移。我们提出了一种基于卷积神经网络的方法Photo-zSNthesis，用于从多波段超新星光变曲线预测完整的红移概率分布，并在模拟的SDSS和LSST数据以及观测到的SDSS SNe上进行了测试。我们展示了对现有方法进行的预测的显著改进，无论是在模拟还是实际观测中，且存在极少的红移相关偏差，这是由于选择效应(例如Malmquist偏差)而带来的挑战。该方法产生的概率密度函数能够得到很好的限制，并将最大程度地约束宇宙学。

    Upcoming photometric surveys will discover tens of thousands of Type Ia supernovae (SNe Ia), vastly outpacing the capacity of our spectroscopic resources. In order to maximize the science return of these observations in the absence of spectroscopic information, we must accurately extract key parameters, such as SN redshifts, with photometric information alone. We present Photo-zSNthesis, a convolutional neural network-based method for predicting full redshift probability distributions from multi-band supernova lightcurves, tested on both simulated Sloan Digital Sky Survey (SDSS) and Vera C. Rubin Legacy Survey of Space and Time (LSST) data as well as observed SDSS SNe. We show major improvements over predictions from existing methods on both simulations and real observations as well as minimal redshift-dependent bias, which is a challenge due to selection effects, e.g. Malmquist bias. The PDFs produced by this method are well-constrained and will maximize the cosmological constraining 
    
[^90]: EENED：基于卷积变压器的端到端神经元癫痫检测

    EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer. (arXiv:2305.10502v1 [eess.SP])

    [http://arxiv.org/abs/2305.10502](http://arxiv.org/abs/2305.10502)

    本文提出了一种名为EENED的端到端神经元癫痫检测模型，结合了CNN和Transformer，能够同时捕捉EEG信号的全局依赖性和局部特征，并有望提高癫痫检测的准确性和可靠性。

    

    最近，在EEG信号处理中，基于变压器（Transformer）和卷积神经网络（CNN）的模型表现出了很好的结果。变压器模型可以通过自我注意机制捕捉EEG信号的全局依赖性，而CNN模型可以捕捉如锯齿波之类的局部特征。在本文中，我们提出了一种名为EENED的端到端神经元癫痫检测模型，它结合了CNN和Transformer。具体地，通过在Transformer编码器中引入卷积模块，EENED可以学习患者EEG信号特征的时间依赖关系，并注意到与癫痫密切相关的局部EEG异常突变，如尖锐波的出现和缓慢波的散布。我们提出的框架结合了Transformer和CNN捕捉EEG信号不同尺度的特征的能力，有望提高癫痫检测的准确性和可靠性。我们的源代码将很快在GitHub上发布。

    Recently Transformer and Convolution neural network (CNN) based models have shown promising results in EEG signal processing. Transformer models can capture the global dependencies in EEG signals through a self-attention mechanism, while CNN models can capture local features such as sawtooth waves. In this work, we propose an end-to-end neural epilepsy detection model, EENED, that combines CNN and Transformer. Specifically, by introducing the convolution module into the Transformer encoder, EENED can learn the time-dependent relationship of the patient's EEG signal features and notice local EEG abnormal mutations closely related to epilepsy, such as the appearance of spikes and the sprinkling of sharp and slow waves. Our proposed framework combines the ability of Transformer and CNN to capture different scale features of EEG signals and holds promise for improving the accuracy and reliability of epilepsy detection. Our source code will be released soon on GitHub.
    
[^91]: 鲁棒决策树集成的可验证学习

    Verifiable Learning for Robust Tree Ensembles. (arXiv:2305.03626v1 [cs.LG])

    [http://arxiv.org/abs/2305.03626](http://arxiv.org/abs/2305.03626)

    本论文提出了一种可验证学习的方法，即训练易于验证的限制模型类来解决决策树集成的 NP-hard 问题，并成功设计出一种新的训练算法，使得在多项式时间内可以进行安全验证，而且仍保持着该领域最好的鲁棒性能。

    

    在测试时间内验证机器学习模型对抗攻击的鲁棒性是一个重要的研究问题。不幸的是，先前的研究确定，对于决策树集成，这个问题是 NP-hard ，因此对于特定的输入来说是不可解的。在本文中，我们确定了一类受限决策树集成，称为 large-spread 集成，其允许在多项式时间内运行安全验证算法。然后，我们提出了一种新方法，称为可验证学习，该方法倡导训练这种易于验证的受限模型类。我们通过设计一种新的训练算法，从标记数据中自动学习 large-spread 决策树集成来展示这种方法的益处，从而使其能够在多项式时间内进行安全验证。公开可用数据集上的实验结果证实，使用我们的算法训练的 large-spread 集成可以在几秒钟内使用标准半定编程求解器进行验证，同时对抗当前最先进的攻击具有竞争力的性能。

    Verifying the robustness of machine learning models against evasion attacks at test time is an important research problem. Unfortunately, prior work established that this problem is NP-hard for decision tree ensembles, hence bound to be intractable for specific inputs. In this paper, we identify a restricted class of decision tree ensembles, called large-spread ensembles, which admit a security verification algorithm running in polynomial time. We then propose a new approach called verifiable learning, which advocates the training of such restricted model classes which are amenable for efficient verification. We show the benefits of this idea by designing a new training algorithm that automatically learns a large-spread decision tree ensemble from labelled data, thus enabling its security verification in polynomial time. Experimental results on publicly available datasets confirm that large-spread ensembles trained using our algorithm can be verified in a matter of seconds, using stand
    
[^92]: 通过流形展平和重构进行表示学习

    Representation Learning via Manifold Flattening and Reconstruction. (arXiv:2305.01777v1 [cs.LG])

    [http://arxiv.org/abs/2305.01777](http://arxiv.org/abs/2305.01777)

    本文提出了一种通过神经网络构建展平流形的算法FlatNet，具有可解释性和可扩展性，同时在测试数据上具有良好的泛化性能。

    

    本文提出了一种算法，可以从流形的有限样本中显式构建一对神经网络，用于线性化和重构嵌入子流形。我们所生成的神经网络称为展平网络（FlatNet），在理论上具有可解释性，在计算上可扩展性强，并且在测试数据上具有良好的泛化性能，这种平衡通常在基于流形的学习方法中难以实现。我们基于合成的高维流形数据和2D图像数据进行了实证实验，并与其他模型进行了比较。我们的代码是公开的。

    This work proposes an algorithm for explicitly constructing a pair of neural networks that linearize and reconstruct an embedded submanifold, from finite samples of this manifold. Our such-generated neural networks, called flattening networks (FlatNet), are theoretically interpretable, computationally feasible at scale, and generalize well to test data, a balance not typically found in manifold-based learning methods. We present empirical results and comparisons to other models on synthetic high-dimensional manifold data and 2D image data. Our code is publicly available.
    
[^93]: 潜指纹识别：局部和全局嵌入的融合

    Latent Fingerprint Recognition: Fusion of Local and Global Embeddings. (arXiv:2304.13800v1 [cs.CV])

    [http://arxiv.org/abs/2304.13800](http://arxiv.org/abs/2304.13800)

    本文采用全局嵌入与局部嵌入相结合的方法，在提高识别准确率的同时，保证了较高的吞吐量和应用性。

    

    在指纹识别中，一个最具挑战性的问题是确定与犯罪现场留下的部分和模糊的指纹（即潜指纹或指纹痕迹）相关联的嫌疑人身份。本文将全局嵌入与局部嵌入相结合，为潜在卷积和拍打指纹匹配提供了最先进的准确率和高吞吐量。这种局部和全局表示的组合使识别准确率在NIST SD 27、NIST SD 302、MSP、MOLF DB1/DB4和MOLF DB2/DB4潜指纹数据集上都得到了显着提高。

    One of the most challenging problems in fingerprint recognition continues to be establishing the identity of a suspect associated with partial and smudgy fingerprints left at a crime scene (i.e., latent prints or fingermarks). Despite the success of fixed-length embeddings for rolled and slap fingerprint recognition, the features learned for latent fingerprint matching have mostly been limited to local minutiae-based embeddings and have not directly leveraged global representations for matching. In this paper, we combine global embeddings with local embeddings for state-of-the-art latent to rolled matching accuracy with high throughput. The combination of both local and global representations leads to improved recognition accuracy across NIST SD 27, NIST SD 302, MSP, MOLF DB1/DB4, and MOLF DB2/DB4 latent fingerprint datasets for both closed-set (84.11%, 54.36%, 84.35%, 70.43%, 62.86% rank-1 retrieval rate, respectively) and open-set (0.50, 0.74, 0.44, 0.60, 0.68 FNIR at FPIR=0.02, resp
    
[^94]: 关于使用深度学习判断质数整除性

    On the Prime Number Divisibility by Deep Learning. (arXiv:2304.01333v1 [cs.LG])

    [http://arxiv.org/abs/2304.01333](http://arxiv.org/abs/2304.01333)

    本文提出了使用深度学习判断质数整除性的方法，并发现关键在于提供给深度学习模型的特征空间。此外，商业可用的自动化机器学习管道无法解决此问题，需要提供适当的特征工程来解决。研究者还提出了一个封闭式解决方案。

    

    对于确定一个整数是否能够被2、3或其他质数整除这样的任务，对于人类来说可能很简单，但在没有预先指定算法的情况下，这对于计算机来说可能并不容易。本文测试了多个深度学习体系结构和特征工程方法，并评估了在确定大有限整数（高达$2^{32}$）是否能够被小质数整除的情况下，各种框架和网络结构（CNN、RNN、Transformer等）的能力。结果表明，预测质数整除性的能力极大地取决于提供给深度学习模型的特征空间，而不是网络框架或网络结构的复杂性（CNN、RNN、Transformer等）。我们还评估了来自亚马逊、谷歌和微软的商业可用的自动化机器学习（AutoML）管道，并证明除非提供适当的特征工程，否则它们无法解决此问题。我们进一步提出了一个封闭式解决方案来解决这个问题。

    Certain tasks such as determining whether a given integer can be divided by 2, 3, or other prime numbers may be trivial for human beings, but can be less straightforward for computers in the absence of pre-specified algorithms. In this paper, we tested multiple deep learning architectures and feature engineering approaches, and evaluated the scenario of determining divisibility of large finite integers (up to $2^{32}$) by small prime numbers. It turns out that, regardless of the network frameworks or the complexity of the network structures (CNN, RNN, Transformer, etc.), the ability to predict the prime number divisibility critically depends on the feature space fed into the deep learning models. We also evaluated commercially available Automated Machine Learning (AutoML) pipelines from Amazon, Google and Microsoft, and demonstrated that they failed to address this issue unless appropriately engineered features were provided. We further proposed a closed form solution to the problem us
    
[^95]: 仿射马尔科夫博弈中的软Bellman平衡：前向解与逆向学习

    Soft-Bellman Equilibrium in Affine Markov Games: Forward Solutions and Inverse Learning. (arXiv:2304.00163v1 [cs.GT])

    [http://arxiv.org/abs/2304.00163](http://arxiv.org/abs/2304.00163)

    本文提出了一种新的解决方案概念——软Bellman平衡，解决了仿射马尔科夫博弈中的多个玩家交互问题，并提出了一种非线性最小二乘算法来计算此平衡，同时通过投影梯度算法解决推断玩家奖励参数的问题。

    

    马尔科夫博弈在随机动态环境中模拟多个玩家之间的交互。每个玩家在马尔科夫博弈中最大化其期望的总折现奖励，该奖励取决于其他玩家的策略。我们提出了一类马尔科夫博弈，称为仿射马尔科夫博弈，在其中，仿射奖励函数耦合了玩家的行动。我们引入了一种新的解决方案概念，即软Bellman平衡，在其中，每个玩家都是有限理性的，并选择软Bellman策略，而不是像著名的Nash平衡概念中那样选择纯理性策略。我们提供了软Bellman平衡存在和唯一性的条件，并提出了一个非线性最小二乘算法来计算前向问题中的这种平衡。然后，我们通过投影梯度算法解决了推断玩家奖励参数的逆向博弈问题。在掠食者-猎物OpenAI Gym环境中的实验表明，使用软Bellman策略可以更有效地控制掠食者和猎物之间的交互。

    Markov games model interactions among multiple players in a stochastic, dynamic environment. Each player in a Markov game maximizes its expected total discounted reward, which depends upon the policies of the other players. We formulate a class of Markov games, termed affine Markov games, where an affine reward function couples the players' actions. We introduce a novel solution concept, the soft-Bellman equilibrium, where each player is boundedly rational and chooses a soft-Bellman policy rather than a purely rational policy as in the well-known Nash equilibrium concept. We provide conditions for the existence and uniqueness of the soft-Bellman equilibrium and propose a nonlinear least squares algorithm to compute such an equilibrium in the forward problem. We then solve the inverse game problem of inferring the players' reward parameters from observed state-action trajectories via a projected gradient algorithm. Experiments in a predator-prey OpenAI Gym environment show that the rewa
    
[^96]: 使用自然主义周边感知研究数据的驾驶员特征识别与贝叶斯工作负荷预测

    Driver Profiling and Bayesian Workload Estimation Using Naturalistic Peripheral Detection Study Data. (arXiv:2303.14720v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2303.14720](http://arxiv.org/abs/2303.14720)

    本研究通过自然主义周边感知研究数据，提出了一种驾驶员特征识别和贝叶斯工作负荷预测的方法，通过对驾驶性能数据进行监测和分析，实现了驾驶员工作负荷的估计。

    

    监测驾驶员的心理工作负荷有助于与车内信息系统进行安全互动，从而实现自适应的人机交互，并减少对驾驶主任务的影响。本文通过驾驶性能数据实现了工作负荷估计的问题。首先，我们提出了一种在自然环境中通过改进的周边感知任务收集主观工作负荷数据的新型实地研究。通过视频分析确定诱发高心理工作负荷的关键环境因素，如交叉路口和前方车辆行为。其次，引入了一个基于最先进的时间序列分类器（如卷积神经网络和变换技术）的监督学习框架，用于根据驾驶员在行程中平均经历的工作负荷特征来进行驾驶员特征识别。然后，提出了一种基于贝叶斯滤波的方法，用于实时或接近实时地顺序估计驾驶员的瞬时工作负荷。

    Monitoring drivers' mental workload facilitates initiating and maintaining safe interactions with in-vehicle information systems, and thus delivers adaptive human machine interaction with reduced impact on the primary task of driving. In this paper, we tackle the problem of workload estimation from driving performance data. First, we present a novel on-road study for collecting subjective workload data via a modified peripheral detection task in naturalistic settings. Key environmental factors that induce a high mental workload are identified via video analysis, e.g. junctions and behaviour of vehicle in front. Second, a supervised learning framework using state-of-the-art time series classifiers (e.g. convolutional neural network and transform techniques) is introduced to profile drivers based on the average workload they experience during a journey. A Bayesian filtering approach is then proposed for sequentially estimating, in (near) real-time, the driver's instantaneous workload. Th
    
[^97]: 一种基于自监督的心血管事件检测通用实验室进展预训练模型

    Self-supervised based general laboratory progress pretrained model for cardiovascular event detection. (arXiv:2303.06980v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06980](http://arxiv.org/abs/2303.06980)

    研究利用自监督学习和迁移学习，将心血管实验室指标的患者进展趋势从常见情况转移到罕见或特定心血管事件检测中，以协助检测经皮冠状动脉介入治疗患者的靶血管重建。

    

    定期监测是管理心血管疾病的必要方面。由于罕见或特定疾病的患者规模较小，观察也是间歇性的，因此其招募常常受到限制，而常见情况由于定期随访而更容易累积纵向数据。然而，这些数据以其无规律性、时间性、缺席性和稀疏性而闻名。本研究利用自监督学习和迁移学习来克服上述障碍，将心血管实验室指标的患者进展趋势从常见情况转移到罕见或特定心血管事件检测中。我们使用高血压患者（尚未患糖尿病）进行了一般实验室进展（GLP）预训练模型的预训练，并将其实验室进展趋势转移，以协助检测经皮冠状动脉介入治疗患者的靶血管重建（TVR）。GLP采用了两个阶段的训练过程，包括预训练和微调。

    Regular surveillance is an indispensable aspect of managing cardiovascular disorders. Patient recruitment for rare or specific diseases is often limited due to their small patient size and episodic observations, whereas prevalent cases accumulate longitudinal data easily due to regular follow-ups. These data, however, are notorious for their irregularity, temporality, absenteeism, and sparsity. In this study, we leveraged self-supervised learning (SSL) and transfer learning to overcome the above-mentioned barriers, transferring patient progress trends in cardiovascular laboratory parameters from prevalent cases to rare or specific cardiovascular events detection. We pretrained a general laboratory progress (GLP) pretrain model using hypertension patients (who were yet to be diabetic), and transferred their laboratory progress trend to assist in detecting target vessel revascularization (TVR) in percutaneous coronary intervention patients. GLP adopted a two-stage training process that u
    
[^98]: 可扩展精密宽场射电干涉成像：II. 在ASKAP数据上对AIRI进行验证

    Scalable precision wide-field imaging in radio interferometry: II. AIRI validated on ASKAP data. (arXiv:2302.14149v2 [astro-ph.IM] CROSS LISTED)

    [http://arxiv.org/abs/2302.14149](http://arxiv.org/abs/2302.14149)

    本研究验证了AIRI算法在ASKAP数据上的有效性，通过使用训练好的深度神经网络进行正则化步骤中的去噪处理，进一步提高了图像动态范围。

    

    本文是第一部分的续篇，对最近提出的AI用于射电干涉成像（AIRI）算法在澳大利亚方阵千平方千米阵列（ASKAP）观测数据上进行了验证。本文展示了使用相同并行化和自动化成像框架生成的单色AIRI-ASKAP图像，该框架在第一部分中描述为“在ASKAP数据上经过验证的uSARA”。AIRI采用“即插即用”方法，通过在去卷积的正向反向算法中，用经过训练的去噪深度神经网络（DNN）替代正则化步骤中的近似算子。我们构建了一组经过训练的DNN去噪器，针对所选数据的预估图像动态范围。此外，我们量化了在选择最接近的DNN与使用具有最高动态范围的通用DNN时，AIRI重建的变化，为一个更完整的框架打开了大门，该框架不仅可以提供图像精确性，而且可以提供较高的动态范围。

    Accompanying Part I, this sequel delineates a validation of the recently proposed AI for Regularisation in radio-interferometric Imaging (AIRI) algorithm on observations from the Australian Square Kilometre Array Pathfinder (ASKAP). The monochromatic AIRI-ASKAP images showcased in this work are formed using the same parallelised and automated imaging framework described in Part I: ``uSARA validated on ASKAP data''. Using a Plug-and-Play approach, AIRI differs from uSARA by substituting a trained denoising deep neural network (DNN) for the proximal operator in the regularisation step of the forward-backward algorithm during deconvolution. We build a trained shelf of DNN denoisers which target the estimated image-dynamic-ranges of our selected data. Furthermore, we quantify variations of AIRI reconstructions when selecting the nearest DNN on the shelf versus using a universal DNN with the highest dynamic range, opening the door to a more complete framework that not only delivers image es
    
[^99]: 轻量化参数裁剪以实现节能深度学习: 一种二值门控模块方法

    Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach. (arXiv:2302.10798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10798](http://arxiv.org/abs/2302.10798)

    本论文提出了一种轻量化参数裁剪策略来实现节能深度学习，可以发现最佳的静态子网络，包含二值门控模块和新颖的损失函数，适用于各种神经网络，同时在已有的基准测试中取得了竞争性结果。

    

    随着神经网络越来越大且更加复杂，绿色AI已经引起了深度学习社区的关注。现有的解决方案通常采用对网络参数进行裁剪，以减少训练推断时的计算负荷。然而，裁剪方案通常会导致额外的开销，因为需要进行迭代训练和微调或重复计算动态裁剪图。我们提出了一种新的参数裁剪策略，以学习轻量级子网络，既能最小化能耗，又能在给定的下游任务上与完全参数化的网络保持相似的性能。我们的裁剪方案以绿色为导向，因为它仅需要动态裁剪方法进行一次训练来发现最佳的静态子网络。该方案由一个二进制门控模块和一个新颖的损失函数组成，以发现具有用户定义稀疏度的子网络。我们的方法可以对卷积神经网络（CNN）和循环神经网络（RNN）等通用神经网络进行裁剪和转换，大大减少了计算复杂度和能量消耗，同时在已有的基准测试上取得了竞争性的结果。

    The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and tr
    
[^100]: 使用训练数据集的一部分加速超参数搜索的两步超参数优化方法

    Two-step hyperparameter optimization method: Accelerating hyperparameter search by using a fraction of a training dataset. (arXiv:2302.03845v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03845](http://arxiv.org/abs/2302.03845)

    这项研究提出了一种两步HPO方法，通过在训练数据集的一部分上进行预评估，然后在整个训练数据集上重新训练和评估，实现了加速超参数搜索的目的

    

    超参数优化(HPO)是机器学习(ML)模型开发中的重要步骤，但常见做法过时，主要依赖于手动或网格搜索。这部分是因为采用先进的HPO算法会引入额外的复杂性，导致计算时间更长。这对ML应用构成了显著挑战，因为次优的超参数选择限制了ML模型性能的潜力，最终阻碍了对ML技术的充分利用。在本文中，我们提出了一种两步HPO方法作为解决计算需求和等待时间的战略性方案，这是从应用ML参数化工作的实践经验中得到的。初始阶段涉及对训练数据集的一个小子集上的超参数进行初步评估，然后在整个训练数据集上重新训练并重新评估最佳候选模型。这种两步HPO方法是普适的

    Hyperparameter optimization (HPO) is an important step in machine learning (ML) model development, but common practices are archaic -- primarily relying on manual or grid searches. This is partly because adopting advanced HPO algorithms introduces added complexity to the workflow, leading to longer computation times. This poses a notable challenge to ML applications, as suboptimal hyperparameter selections curtail the potential of ML model performance, ultimately obstructing the full exploitation of ML techniques. In this article, we present a two-step HPO method as a strategic solution to curbing computational demands and wait times, gleaned from practical experiences in applied ML parameterization work. The initial phase involves a preliminary evaluation of hyperparameters on a small subset of the training dataset, followed by a re-evaluation of the top-performing candidate models post-retraining with the entire training dataset. This two-step HPO method is universally applicable acr
    
[^101]: 类特定网络中节点注入的研究

    Node Injection for Class-specific Network Poisoning. (arXiv:2301.12277v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12277](http://arxiv.org/abs/2301.12277)

    本文介绍了一种节点注入攻击的方法，针对图中的特定节点进行误分类，通过伪装成良性节点的方式进行注入。这种攻击方法可以破坏基于GNN的节点分类器的性能。

    

    图神经网络（GNNs）在学习丰富的网络表示方面具有强大的能力，有助于下游任务的性能。然而，最近的研究表明，GNNs容易受到节点注入和网络扰动等对抗性攻击的影响。其中，节点注入攻击更加实际，因为它们不需要对现有网络进行操纵，可以更真实地执行。在本文中，我们提出了一个新的问题陈述 - 在图中进行类特定的毒素攻击，攻击者旨在将目标类中的特定节点误分类为不同的类，使用节点注入。此外，节点以一种伪装成良性节点的方式注入。我们提出了一种新的攻击策略NICKI，它利用基于优化的方法破坏基于GNN的节点分类器的性能。NICKI分为两个阶段工作，首先学习节点表示，然后生成注入节点的特征和边。

    Graph Neural Networks (GNNs) are powerful in learning rich network representations that aid the performance of downstream tasks. However, recent studies showed that GNNs are vulnerable to adversarial attacks involving node injection and network perturbation. Among these, node injection attacks are more practical as they don't require manipulation in the existing network and can be performed more realistically. In this paper, we propose a novel problem statement - a class-specific poison attack on graphs in which the attacker aims to misclassify specific nodes in the target class into a different class using node injection. Additionally, nodes are injected in such a way that they camouflage as benign nodes. We propose NICKI, a novel attacking strategy that utilizes an optimization-based approach to sabotage the performance of GNN-based node classifiers. NICKI works in two phases - it first learns the node representation and then generates the features and edges of the injected nodes. Ex
    
[^102]: 利用新数据在电力网通讯中的潜力

    Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids. (arXiv:2209.12693v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12693](http://arxiv.org/abs/2209.12693)

    本文提出了两个基于宽带电力线通信测量的首个数据集FiN-1和FiN-2，共收集了130亿个数据点。这些数据可以应用于资产管理、电网状态可视化和预测，以解决电力网在向可再生能源过渡和复杂负载配置的情况下面临的新挑战。

    

    电力网已成为日常生活中不可或缺的一部分，尽管在日常生活中往往不会注意到。只有当电力网不再可用时，我们通常才会特别意识到这种依赖。然而，重大变化，如向可再生能源（光伏、风力涡轮机等）的过渡以及复杂负载配置（电动汽车、家庭电池系统等）的能源消费者数量的增加，给电力网带来了新的挑战。为了解决这些挑战，我们提出了两个基于宽带电力线通信（PLC）基础设施测量的首个数据集。这两个数据集 FiN-1 和 FiN-2 在德国低压电网的一部分实际使用中收集，向大约440万人提供服务，并显示5100多个传感器收集的超过130亿个数据点。此外，我们提出了不同的用例，用于资产管理、电网状态可视化和预测。

    Electricity grids have become an essential part of daily life, even if they are often not noticed in everyday life. We usually only become particularly aware of this dependence by the time the electricity grid is no longer available. However, significant changes, such as the transition to renewable energy (photovoltaic, wind turbines, etc.) and an increasing number of energy consumers with complex load profiles (electric vehicles, home battery systems, etc.), pose new challenges for the electricity grid. To address these challenges, we propose two first-of-its-kind datasets based on measurements in a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1 and FiN-2, were collected during real practical use in a part of the German low-voltage grid that supplies around 4.4 million people and show more than 13 billion datapoints collected by more than 5100 sensors. In addition, we present different use cases in asset management, grid state visualization, forecasting, 
    
[^103]: 信息处理相等性和信息风险桥梁

    Information Processing Equalities and the Information-Risk Bridge. (arXiv:2207.11987v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11987](http://arxiv.org/abs/2207.11987)

    本论文引入了两类新的信息度量方法，扩展和统一了不同分布之间的散度，通过信息度量和贝叶斯风险之间的几何关系以及信息处理等式，揭示了在经典风险最小化中选择假设类的重要性。

    

    我们引入了两类新的信息度量方法，用于统计实验，推广了和包含了$\phi$-散度、积分概率度量、$\mathfrak{N}$-距离（MMD）和两个或多个分布之间的$(f,\Gamma)$-散度。这使得我们能够推导出信息度量和统计决策问题的贝叶斯风险之间的简单几何关系，从而以完全对称的方式将变分$\phi$-散度表示扩展到多个分布中。新的散度族在马尔可夫算子的作用下保持不变，产生了一个信息处理等式，它是经典数据处理不等式的细化和推广。这个等式揭示了在经典风险最小化中选择假设类的重要性。

    We introduce two new classes of measures of information for statistical experiments which generalise and subsume $\phi$-divergences, integral probability metrics, $\mathfrak{N}$-distances (MMD), and $(f,\Gamma)$ divergences between two or more distributions. This enables us to derive a simple geometrical relationship between measures of information and the Bayes risk of a statistical decision problem, thus extending the variational $\phi$-divergence representation to multiple distributions in an entirely symmetric manner. The new families of divergence are closed under the action of Markov operators which yields an information processing equality which is a refinement and generalisation of the classical data processing inequality. This equality gives insight into the significance of the choice of the hypothesis class in classical risk minimization.
    
[^104]: 首个用于深度超分辨率广场电波天文学成像的人工智能：揭示ESO 137--006的结构

    First AI for deep super-resolution wide-field imaging in radio astronomy: unveiling structure in ESO 137--006. (arXiv:2207.11336v2 [astro-ph.IM] CROSS LISTED)

    [http://arxiv.org/abs/2207.11336](http://arxiv.org/abs/2207.11336)

    这项研究介绍了首个基于人工智能的深度超分辨率广场射电干涉成像框架，并在ESO 137-006射电星系的观测中进行了演示。该框架通过“插入和播放”方案解决图像重建的反问题，实现了高分辨率图像的恢复和自适应去噪，可以应用于大数据和图像尺寸，并且实现了并行算法以提高效率。

    

    我们引入了第一个基于人工智能的深度超分辨率广场射电干涉成像框架，并在ESO 137-006射电星系的观测中进行了演示。该算法框架用于解决图像重建的反问题，基于最近的“插入和播放”方案，通过在优化算法中将去噪操作器注入为图像正则化器，实现去噪步骤和梯度下降数据保真度步骤交替进行，直至收敛。我们研究了高分辨率高动态范围去噪器的手工设计和学习改进版本。我们提出了一种并行算法实现，该算法依赖于对图像进行自动分解为面和测量算子为稀疏低维块的方法，实现对大数据和图像尺寸的可扩展性。我们使用1053和1399 MHz的19 GB MeerKAT数据验证了我们的图像形成框架，在包含ESO 137-006的广域视野上进行了重建。

    We introduce the first AI-based framework for deep, super-resolution, wide-field radio-interferometric imaging, and demonstrate it on observations of the ESO~137-006 radio galaxy. The algorithmic framework to solve the inverse problem for image reconstruction builds on a recent ``plug-and-play'' scheme whereby a denoising operator is injected as an image regulariser in an optimisation algorithm, which alternates until convergence between denoising steps and gradient-descent data-fidelity steps. We investigate handcrafted and learned variants of high-resolution high-dynamic range denoisers. We propose a parallel algorithm implementation relying on automated decompositions of the image into facets and the measurement operator into sparse low-dimensional blocks, enabling scalability to large data and image dimensions. We validate our framework for image formation at a wide field of view containing ESO~137-006, from 19 gigabytes of MeerKAT data at 1053 and 1399 MHz. The recovered maps exhi
    
[^105]: 基于需求学习和公平资源消耗平衡的网络收益管理

    Network Revenue Management with Demand Learning and Fair Resource-Consumption Balancing. (arXiv:2207.11159v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.11159](http://arxiv.org/abs/2207.11159)

    本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。提出了一种基于UCB需求学习方法的原始对偶在线策略来最大化规范化收益。

    

    除了最大化总收益外，很多行业的决策者还希望确保不同资源之间消耗的平衡。例如，在零售行业中，确保来自不同供应商的资源平衡消耗有助于提高公平性并维持良好的渠道关系；在云计算行业中，资源消耗的平衡有助于提高客户满意度并降低运营成本。针对这些实际需求，本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。我们引入了规范化收益的概念，即通过平衡正则化将公平的资源消耗平衡纳入到收益最大化的目标中。我们提出了一种基于上置信界限（UCB）需求学习方法的原始对偶在线策略来最大化规范化收益。我们采用了几个创新方法来应对需求学习和资源消耗平衡的挑战。

    In addition to maximizing the total revenue, decision-makers in lots of industries would like to guarantee balanced consumption across different resources. For instance, in the retailing industry, ensuring a balanced consumption of resources from different suppliers enhances fairness and helps main a good channel relationship; in the cloud computing industry, resource-consumption balance helps increase customer satisfaction and reduce operational costs. Motivated by these practical needs, this paper studies the price-based network revenue management (NRM) problem with both demand learning and fair resource-consumption balancing. We introduce the regularized revenue, i.e., the total revenue with a balancing regularization, as our objective to incorporate fair resource-consumption balancing into the revenue maximization goal. We propose a primal-dual-type online policy with the Upper-Confidence-Bound (UCB) demand learning method to maximize the regularized revenue. We adopt several innov
    
[^106]: 生成和检测真正的歧义：DNN监督测试中的一个被忽视的危险

    Generating and Detecting True Ambiguity: A Forgotten Danger in DNN Supervision Testing. (arXiv:2207.10495v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2207.10495](http://arxiv.org/abs/2207.10495)

    本文提出了一种新的方法来生成和检测真正歧义的测试输入，以解决现有测试生成器只能生成超出分布输入的问题。

    

    深度神经网络(DNNs)正在成为现代软件系统的关键组成部分，但在与训练期间观察到的条件不同的情况下（超出分布的输入）或在真正模糊的输入上（即，在标签中存在多个类别且其概率不为零的输入）可能会失败。最近的工作提出了DNN监督器，以在可能导致任何损害之前检测到高不确定性的输入。为了测试和比较DNN监督器的能力，研究者们提出了测试生成技术，以将测试工作重点放在那些监督器应该将其识别为异常的高不确定性输入上。然而，现有的测试生成器旨在生成超出分布的输入。没有现有的模型和监督器无关的技术针对生成真正模糊的测试输入，即，根据专家人员判断，输入可以对应多个类别。在本文中，我们提出了一种新的方法来生成和检测真正歧义的测试输入。

    Deep Neural Networks (DNNs) are becoming a crucial component of modern software systems, but they are prone to fail under conditions that are different from the ones observed during training (out-of-distribution inputs) or on inputs that are truly ambiguous, i.e., inputs that admit multiple classes with nonzero probability in their labels. Recent work proposed DNN supervisors to detect high-uncertainty inputs before their possible misclassification leads to any harm. To test and compare the capabilities of DNN supervisors, researchers proposed test generation techniques, to focus the testing effort on high-uncertainty inputs that should be recognized as anomalous by supervisors. However, existing test generators aim to produce out-of-distribution inputs. No existing model- and supervisor independent technique targets the generation of truly ambiguous test inputs, i.e., inputs that admit multiple classes according to expert human judgment.  In this paper, we propose a novel way to gener
    
[^107]: TREE-G:决策树对抗图神经网络

    TREE-G: Decision Trees Contesting Graph Neural Networks. (arXiv:2207.02760v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02760](http://arxiv.org/abs/2207.02760)

    TREE-G引入了一种专门针对图数据的修改决策树方法，通过新颖的分裂函数和指针机制，使得决策树能够更好地应用于带有拓扑信息的图结构化数据。

    

    在处理表格数据时，基于决策树的模型是一个流行的选择，因为它们在这些数据类型上具有高准确性、易于应用和可解释性的特点。然而，在处理图结构化数据时，如何有效地应用决策树并将拓扑信息与图的顶点上的表格数据相结合仍不清楚。为了解决这个挑战，我们引入了TREE-G。TREE-G修改了标准决策树，引入了一种专门针对图数据的新型分裂函数。这个分裂函数不仅包括节点特征和拓扑信息，还使用了一种新颖的指针机制，允许分裂节点使用在先前分裂中计算得到的信息。因此，分裂函数能够适应预测任务和当前的图。我们对TREE-G的理论性质进行了分析，并在多个图和顶点预测基准测试上从经验上证明了它的好处。

    When dealing with tabular data, models based on decision trees are a popular choice due to their high accuracy on these data types, their ease of application, and explainability properties. However, when it comes to graph-structured data, it is not clear how to apply them effectively, in a way that incorporates the topological information with the tabular data available on the vertices of the graph. To address this challenge, we introduce TREE-G. TREE-G modifies standard decision trees, by introducing a novel split function that is specialized for graph data. Not only does this split function incorporate the node features and the topological information, but it also uses a novel pointer mechanism that allows split nodes to use information computed in previous splits. Therefore, the split function adapts to the predictive task and the graph at hand. We analyze the theoretical properties of TREE-G and demonstrate its benefits empirically on multiple graph and vertex prediction benchmarks
    
[^108]: 语音情绪识别中的Transformer时代的黎明：弥合情感价值差距

    Dawn of the transformer era in speech emotion recognition: closing the valence gap. (arXiv:2203.07378v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2203.07378](http://arxiv.org/abs/2203.07378)

    本文通过对几种预训练变体的分析，发现在语音情绪识别领域，使用Transformer架构能够在没有使用显式语言信息的情况下获得最佳的价值预测性能，相关性系数为0.638。

    

    最近的Transformer架构在自监督预训练方面取得了重大突破，并在多个机器学习任务中表现出了巨大的潜力。在音频领域，这种架构也已成功应用于语音情绪识别(SER)领域。然而，现有的研究还没有评估模型大小和预训练数据对下游性能的影响，并且对泛化能力、稳健性、公平性和效率方面的关注有限。本文在几种预训练变体的wav2vec 2.0和HuBERT上进行了详细分析，并在MSP-Podcast的唤起、控制和价值维度上进行了微调，同时使用IEMOCAP和MOSI进行跨语料库泛化测试。据我们所知，在不使用显式语言信息的情况下，我们在MSP-Podcast上获得了最佳的价值预测性能，相关性系数为0.638。

    Recent advances in transformer-based architectures which are pre-trained in self-supervised manner have shown great promise in several machine learning tasks. In the audio domain, such architectures have also been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of .638 on MSP-Podcast. Further
    
[^109]: 多任务UNet架构用于端到端自动驾驶

    Multi-task UNet architecture for end-to-end autonomous driving. (arXiv:2112.08967v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08967](http://arxiv.org/abs/2112.08967)

    我们提出了一种端到端驾驶模型，使用多任务UNet架构和控制算法，能够同时进行车道分割、路径预测和车辆控制。与增强学习模型相比，在弯曲道路上的性能相当，但具有端到端的优势。

    

    我们提出了一种端到端驾驶模型，该模型将多任务UNet（MTUNet）架构和控制算法整合在从前置摄像头到驾驶决策的数据流管道中。它提供了定量指标来评估端到端驾驶系统的整体、动态和实时性能，从而评估MTUNet的安全性和可解释性。该架构包括一个分割任务、一个回归任务和两个分类任务，分别用于车道分割、路径预测和车辆控制。我们提出了三个不同复杂度的架构变体，并在四个静态度量的不同任务上进行比较，然后通过两个附加的动态度量在实时仿真中确定最佳架构。我们的结果表明，所提出的监督学习模型在相同任务的弯曲道路上的性能与增强学习模型相当，而后者不是端到端的而是多任务的。

    We propose an end-to-end driving model that integrates a multi-task UNet (MTUNet) architecture and control algorithms in a pipeline of data flow from a front camera through this model to driving decisions. It provides quantitative measures to evaluate the holistic, dynamic, and real-time performance of end-to-end driving systems and thus the safety and interpretability of MTUNet. The architecture consists of one segmentation, one regression, and two classification tasks for lane segmentation, path prediction, and vehicle controls. We present three variants of the architecture having different complexities, compare them on different tasks in four static measures for both single and multiple tasks, and then identify the best one by two additional dynamic measures in real-time simulation. Our results show that the performance of the proposed supervised learning model is comparable to that of a reinforcement learning model on curvy roads for the same task, which is not end-to-end but multi
    
[^110]: 随机最小二乘值迭代的频率后悔界限

    Frequentist Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:1911.00567v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.00567](http://arxiv.org/abs/1911.00567)

    本文引入了一种改进版本的随机最小二乘值迭代（RLSVI）算法，通过对行动值函数的最小二乘逼近进行扰动，诱导出了探索过程。在马尔可夫决策过程具有低秩转移动态的假设下，我们证明了RLSVI的频率后悔上界为$\widetilde O(d^2 H^2 \sqrt{T})$。这是对于带有函数逼近的随机探索的首个频率后悔分析。

    

    我们考虑有限时间域强化学习中的探索-利用困境。当状态空间很大或连续时，传统的表格方法不可行，必须采用函数逼近的形式。在本文中，我们引入了一种乐观初始化的改进版本的随机最小二乘值迭代（RLSVI）算法，该算法是一种无模型算法，其中探索是通过扰动行动值函数的最小二乘逼近来诱导的。在假设马尔可夫决策过程具有低秩转移动态的情况下，我们证明了RLSVI的频率后悔将上界为$\widetilde O(d^2 H^2 \sqrt{T})$，其中$ d $是特征维度，$ H $是时间限制，$ T $是总步数。据我们所知，这是对于带有函数逼近的随机探索的第一个频率后悔分析。

    We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by $\widetilde O(d^2 H^2 \sqrt{T})$ where $ d $ are the feature dimension, $ H $ is the horizon, and $ T $ is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation.
    

