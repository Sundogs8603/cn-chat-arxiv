# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Synthesizing Artistic Cinemagraphs from Text.](http://arxiv.org/abs/2307.03190) | 本论文介绍了一种通过文本描述来创建艺术性影动图的自动化方法。通过合成图像双胞胎，即一对艺术图像和与之对齐的真实图像，可以同时满足艺术风格和外观的要求并简化动作分析。同时，利用现有数据集可以准确地分割真实图像并预测合理的运动。 |
| [^2] | [TGRL: An Algorithm for Teacher Guided Reinforcement Learning.](http://arxiv.org/abs/2307.03186) | TGRL是一种用于教师引导强化学习的算法，通过动态和自动平衡何时遵循教师指导和何时使用奖励，教师监督的重要性会根据代理的表现调整。 |
| [^3] | [Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles.](http://arxiv.org/abs/2307.03176) | 通过引入异构特征集成，本文研究了在线性回归设置中构建的岭回归集成的学习曲线，结果表明异构特征集成具有较好的性能。 |
| [^4] | [Push Past Green: Learning to Look Behind Plant Foliage by Moving It.](http://arxiv.org/abs/2307.03175) | 本文提出了一种通过移动植物来查看叶片背后内容的方法，通过使用自我监督训练了一个神经网络SRPNet，可以预测有效的显露出植物叶片下空间的动作，进一步可以通过执行一系列动作逐步显露出更多空间，实验结果表明该方法在合成和真实植物上都取得了良好的效果。 |
| [^5] | [Focused Transformer: Contrastive Training for Context Scaling.](http://arxiv.org/abs/2307.03170) | Focused Transformer通过反差训练优化了上下文缩放问题，允许语言模型处理更长的上下文信息。 |
| [^6] | [Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?.](http://arxiv.org/abs/2307.03157) | 本研究利用多个皮肤病变数据集，研究了域适应方法在皮肤病变分类中的应用。结果表明，域适应在减少不平衡的情况下对于二分类任务有效，但在多分类任务中性能较差，需要解决不平衡问题以提高准确性。 |
| [^7] | [Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images.](http://arxiv.org/abs/2307.03137) | 本文介绍了一种新的拓扑感知损失函数，通过持久同调来惩罚计算机断层扫描图像中主动脉和大血管分割结果与真实值之间的拓扑差异。这种方法能够改善分割任务的性能，尤其是针对具有固有几何特征的对象。 |
| [^8] | [Multiplicative Updates for Online Convex Optimization over Symmetric Cones.](http://arxiv.org/abs/2307.03136) | 本文研究了在线凸优化问题，使用了对称锥乘法权重更新算法(SCMWU)，该算法在任意对称锥的迹为一处进行在线优化，并且在实验证明了其是无悔算法。 |
| [^9] | [Distilling Large Vision-Language Model with Out-of-Distribution Generalizability.](http://arxiv.org/abs/2307.03135) | 本文研究了针对大型视觉语言模型的模型压缩方法，将教师模型的视觉表示压缩到学生模型中。研究重点在于超出分布可泛化的问题，并提出了两个原则来增强学生模型的性能。 |
| [^10] | [Benchmarking Test-Time Adaptation against Distribution Shifts in Image Classification.](http://arxiv.org/abs/2307.03133) | 该论文提出一个基准，用于系统地评估图像分类中对抗分布偏移的测试时间自适应方法的有效性。研究作者使用13个著名的TTA方法及其变体在五个广泛使用的图像分类数据集上进行了评估，并讨论了不同方法在适应性场景中的兼容性。 |
| [^11] | [T-MARS: Improving Visual Representations by Circumventing Text Feature Learning.](http://arxiv.org/abs/2307.03132) | T-MARS提出一种新的数据筛选方法，通过规避文本特征学习，改善了视觉表示的学习，解决了大型多模态数据集中存在的文本与图像重叠的问题。 |
| [^12] | [Principal subbundles for dimension reduction.](http://arxiv.org/abs/2307.03128) | 本文展示了如何利用次微分几何进行流形学习和曲面重建。通过将局部线性逼近的点云集合成低维子束，根据次微分度量下的次微分测地线解决了多个重要问题，同时在处理噪声数据时具有良好的稳健性。 |
| [^13] | [Context-Aware Configuration and Management of WiFi Direct Groups for Real Opportunistic Networks.](http://arxiv.org/abs/2307.03126) | 本文提出了一种WiFi Direct组管理协议（WFD-GM），用于实现在实时机会网络中自主连接和组间通信。 |
| [^14] | [Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance.](http://arxiv.org/abs/2307.03119) | 本文提出了一种多智能体强化学习方法，考虑实际约束下的多订单执行问题。通过智能体之间的通信与协作，最大化整体利润。现有的方法忽视了同时执行多个订单的情况，导致次优性和偏差。 |
| [^15] | [Quantum Solutions to the Privacy vs. Utility Tradeoff.](http://arxiv.org/abs/2307.03118) | 提出了一种基于量子密码学原理的新型架构，用于保护生成模型免受成员推断攻击，具有可证明的隐私和安全保证。 |
| [^16] | [How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models.](http://arxiv.org/abs/2307.03108) | 本文提出了一种方法，通过在训练的文本到图像扩散模型中植入注入的记忆化内容，来检测未授权数据使用。该方法修改了受保护的图像数据集，添加了对人眼不可察觉但模型可以捕捉和记忆的内容，通过分析模型对注入内容的记忆来判断模型是否存在生成类似图像的能力。 |
| [^17] | [Beyond Intuition, a Framework for Applying GPs to Real-World Data.](http://arxiv.org/abs/2307.03093) | 提出了一个框架，用于确定高斯过程在实际问题中的适用性，并建立一个稳健且明确的模型。通过对核函数设计和计算可扩展性选项的指导，该框架在冰川高程变化的案例研究中实现了更准确的结果。 |
| [^18] | [OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models.](http://arxiv.org/abs/2307.03084) | OpenDelta是一个开源库，提供了各种delta调整方法的即插即用实现。它能够以高效的方式调整大型预训练模型的参数，而无需修改模型的代码，具有实用性和灵活性。 |
| [^19] | [Learning Disentangled Representations in Signed Directed Graphs without Social Assumptions.](http://arxiv.org/abs/2307.03077) | 本文提出了一种新方法DINES，用于在无社交假设的有向带符号图中学习解缠节点表示。该方法采用解缠框架，可以捕捉多个潜在因素，并使用轻量级图卷积和解码器进行符号关系分类。 |
| [^20] | [A Hybrid End-to-End Spatio-Temporal Attention Neural Network with Graph-Smooth Signals for EEG Emotion Recognition.](http://arxiv.org/abs/2307.03068) | 一种融合时空注意力神经网络和图信号平滑的EEG情绪识别模型，通过深度架构和图信号处理技术，实现了在情感计算中的出色表现。 |
| [^21] | [DeepOnto: A Python Package for Ontology Engineering with Deep Learning.](http://arxiv.org/abs/2307.03067) | DeepOnto是一个Python包，用于深度学习本体工程。它通过集成深度学习框架和本体API，提供了丰富的工具和算法，支持本体工程任务，如本体对齐和完成。 |
| [^22] | [Generalizing Backpropagation for Gradient-Based Interpretability.](http://arxiv.org/abs/2307.03056) | 本论文在深度神经网络的特征解释中，泛化了反向传播算法，以便更好地理解梯度图的可解释统计数据，如最高加权路径和熵。作者通过在合成数据集上的评估和应用于BERT的实验中验证了该方法的有效性。 |
| [^23] | [Origin-Destination Travel Time Oracle for Map-based Services.](http://arxiv.org/abs/2307.03048) | 本论文提出了一种基于地图服务的起点-终点出行时间预测系统，通过利用历史轨迹估计OD对的时间变化，解决了多个历史轨迹与异常轨迹之间的复杂性问题。 |
| [^24] | [Track Mix Generation on Music Streaming Services using Transformers.](http://arxiv.org/abs/2307.03045) | 本文介绍了2022年在Deezer音乐流媒体服务上推出的Track Mix个性化歌单生成系统，通过使用Transformer模型分析用户播放列表的曲目序列来生成以初始音乐曲目为灵感的“混合”播放列表，提升用户在Deezer上的音乐发现体验。 |
| [^25] | [A Near-Linear Time Algorithm for the Chamfer Distance.](http://arxiv.org/abs/2307.03043) | 本文提出了一个适用于Chamfer距离的近线性时间算法，可以估计Chamfer距离的$(1+\epsilon)$-近似值，解决了处理大数据集时的运行时间问题。 |
| [^26] | [Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain.](http://arxiv.org/abs/2307.03042) | 本研究提出了一种参数高效微调（PEFT）方法，在临床领域使用临床记录训练了一个专门适配临床领域的LLaMA-LoRA模型，同时提出了一个两步PEFT框架，用于将其与Downstream LLaMA-LoRA适配器进行融合，以实现领域适应。 |
| [^27] | [PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models.](http://arxiv.org/abs/2307.03034) | 本文研究了一种一般观测模型下的不安定多臂赌博机问题，提出了PCL-可索引性和Whittle索引的分析方法，并通过近似过程将问题转化为有限状态问题。数值实验表明算法表现优秀。 |
| [^28] | [Improving Retrieval-Augmented Large Language Models via Data Importance Learning.](http://arxiv.org/abs/2307.03027) | 本文通过多线性扩展算法评估检索增强模型中检索到的数据点的数据重要性，并提出了一个多项式时间算法来计算其数据重要性。实验结果表明，修剪或增强大型语言模型可以提高性能。 |
| [^29] | [Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial to Human Experts.](http://arxiv.org/abs/2307.03003) | 本研究提出了一个混合系统，在人类专家中添加人工智能，从之前由人类专家审查过的未知类别的数据实例中学习分类，以提高人机协同系统的效率。 |
| [^30] | [ContainerGym: A Real-World Reinforcement Learning Benchmark for Resource Allocation.](http://arxiv.org/abs/2307.02991) | ContainerGym是一个受到真实世界工业资源分配任务启发的强化学习基准，将一系列常见的真实世界挑战编码进去，从而在任何真实世界问题上评估强化学习算法。 |
| [^31] | [A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications.](http://arxiv.org/abs/2307.02984) | 这项工作提出了一种潜在空间导航策略，通过使用辅助身份分类器作为导向，在潜在空间中生成多样化的合成样本，以支持深度模型的训练，并解决了由于使用生成对抗网络而导致的隐私问题。 |
| [^32] | [Transfer Learning for the Efficient Detection of COVID-19 from Smartphone Audio Data.](http://arxiv.org/abs/2307.02975) | 本文介绍了三种不同的深度学习模型和迁移学习方法，以提高从智能手机音频数据中进行COVID-19的高效检测。通过在多个数据集上进行实验评估，结果显示这些方法可以有效应对移动健康系统中的开放性研究挑战。 |
| [^33] | [Pruning vs Quantization: Which is Better?.](http://arxiv.org/abs/2307.02973) | 本文比较了神经网络量化和修剪这两种压缩深度神经网络的技术，结果表明在大多数情况下，量化优于修剪。 |
| [^34] | [DPM: Clustering Sensitive Data through Separation.](http://arxiv.org/abs/2307.02969) | 本文提出了差分隐私聚类算法DPM，通过搜索准确的数据点分离器来进行隐私保护的聚类。关键贡献是识别大间隔分离器并合理分配隐私预算。 |
| [^35] | [SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks.](http://arxiv.org/abs/2307.02953) | 这个论文提出了一种轻量级的医学图像分割网络SegNetr，通过引入SegNetr块和信息保留跳跃连接实现了动态的局部-全局交互和编码器-解码器特征的精确融合。 |
| [^36] | [When No-Rejection Learning is Optimal for Regression with Rejection.](http://arxiv.org/abs/2307.02932) | 本文研究了具有拒绝的回归问题，并调查了将其视为标准回归任务来学习预测器的无拒绝学习策略。 |
| [^37] | [A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition.](http://arxiv.org/abs/2307.02906) | 本文提出了一种实时人体姿势估计方法，通过使用视频录制的2D姿势估计来确定最佳传感器布局，在传感器基础的人体活动识别中取得了重要的突破。 |
| [^38] | [PUFFIN: A Path-Unifying Feed-Forward Interfaced Network for Vapor Pressure Prediction.](http://arxiv.org/abs/2307.02903) | PUFFIN是一种结合迁移学习和基于领域知识的归纳偏差节点的机器学习框架，用于改进蒸汽压力预测。通过利用归纳偏差和图嵌入的迁移学习，PUFFIN在预测中胜过不使用归纳偏差或使用通用描述符的替代策略。 |
| [^39] | [Free Bits: Latency Optimization of Mixed-Precision Quantized Neural Networks on the Edge.](http://arxiv.org/abs/2307.02894) | 本论文提出了一种混合搜索方法，通过硬件不可知的可微分搜索算法和硬件感知的启发式优化，可以优化混合精度配置对特定硬件目标的延迟。在MobileNetV1和MobileNetV2上的实验结果表明，在1000类ImageNet数据集上相比于8位模型，在保证几乎没有准确性下降的情况下，能够实现高达28.6％的端到端延迟降低 |
| [^40] | [BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables.](http://arxiv.org/abs/2307.02891) | 本文提出了一种名为BaBE的方法，通过估计潜在解释变量来提高公平性。该方法通过结合贝叶斯推断和期望最大化方法，估计给定Z的每个群体E的最可能值。 |
| [^41] | [Learning to Solve Tasks with Exploring Prior Behaviours.](http://arxiv.org/abs/2307.02889) | 本文提出了一种基于示例的控制方法（IRDEC），通过内在奖励驱动以及探索获得先前行为，并与示范中的任务特定行为连接，从而解决具有稀疏奖励的任务。这种方法在三个导航任务上的性能优于其他基线方法。 |
| [^42] | [Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight.](http://arxiv.org/abs/2307.02884) | 本文研究了在部分可观察的马尔可夫决策过程（POMDPs）中学习的样本高效性，提出了一个增强的反馈模型，利用事后多观察数据实现了对两种新的POMDP子类的样本高效学习。 |
| [^43] | [Towards a safe MLOps Process for the Continuous Development and Safety Assurance of ML-based Systems in the Railway Domain.](http://arxiv.org/abs/2307.02867) | 本文提出了一个安全的MLOps流程，用于在铁路领域中持续开发和安全保证基于机器学习系统的系统。该流程整合了系统工程、安全保证和机器学习生命周期，解决了再现性、可追溯性、协作性和持续适应性的问题。 |
| [^44] | [PLIERS: a Popularity-Based Recommender System for Content Dissemination in Online Social Networks.](http://arxiv.org/abs/2307.02865) | PLIERS是一种基于流行度的在线社交网络内容传播推荐系统，通过在算法复杂性和推荐物品的个性化水平之间取得平衡，提供了更好的个性化、相关性和推荐的新颖性。 |
| [^45] | [Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation.](http://arxiv.org/abs/2307.02842) | 这篇论文研究了一种新颖的风险敏感强化学习方法，通过迭代条件风险价值目标以及线性和一般函数逼近方法，实现了安全性保证，并提出了高效的算法。通过对于不同逼近方法的实验结果，验证了算法的有效性和优越性。 |
| [^46] | [Policy Contrastive Imitation Learning.](http://arxiv.org/abs/2307.02829) | 政策对比仿真学习(PCIL)是一种解决敌对仿真学习(AIL)性能不佳问题的新方法。PCIL通过学习对比表示空间，并生成平滑的余弦相似度奖励，提供更有意义的代理与政策之间的比较。 |
| [^47] | [Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks.](http://arxiv.org/abs/2307.02828) | 本文提出了一种名为采样-based快速梯度重标定方法（S-FGRM）的对抗攻击算法。通过使用数据重标定来替代梯度更新中的符号函数，我们解决了梯度更新估计不准确和对抗转移次优解的问题。 |
| [^48] | [Trends in Machine Learning and Electroencephalogram (EEG): A Review for Undergraduate Researchers.](http://arxiv.org/abs/2307.02819) | 这篇论文是一篇关于脑机接口（BCIs）在机器学习背景下的系统文献综述，重点介绍了脑电图（EEG）研究的最新趋势，并旨在为本科研究生提供了解BCI领域的概述，包括任务、算法和数据集，通过综合最新研究发现，提供对BCI研究的基本理解，并找出未来研究的有前途的方向。 |
| [^49] | [CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks.](http://arxiv.org/abs/2307.02813) | 这篇论文提出了一种对比式预训练方法（CPDG）用于动态图神经网络（DGNNs），通过灵活的结构-时序子图采样和结构-时序对比式预训练方案，解决了DGNNs预训练中的泛化和长短期建模能力等挑战，实验证明CPDG在各种下游任务中的动态图预训练方面优于现有方法。 |
| [^50] | [OLR-WA Online Regression with Weighted Average.](http://arxiv.org/abs/2307.02804) | 在线回归方法OLR-WA通过将新到达的数据与先前模型相结合，使用用户定义的权重进行加权平均，在遇到数据变化时提供灵活性，既能适应一致数据又能适应变化数据。 |
| [^51] | [Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information.](http://arxiv.org/abs/2307.02799) | 本文提出了一种使用张量回归进行少样本个性化显著性预测的方法，以保留个性化显著性图的结构全局信息。 |
| [^52] | [VerifAI: Verified Generative AI.](http://arxiv.org/abs/2307.02796) | 验证生成式人工智能的输出是一个新兴问题，我们提出了通过分析多模态数据湖的底层数据，评估其质量和一致性，来建立评估生成式人工智能模型输出的更坚实基础，并解决错误信息传播的挑战。 |
| [^53] | [The Role of Subgroup Separability in Group-Fair Medical Image Classification.](http://arxiv.org/abs/2307.02791) | 本研究研究了深度分类器中的表现差异，发现分类器将个体分为子群的能力在医学成像模态和受保护特征方面存在显著差异，并证明了这个属性对算法偏见具有预测能力。通过理论分析和实证评估，我们发现子群可分性、子群差异和模型在存在系统偏见数据时的性能降级之间存在关系，这为公平医学成像人工智能的发展提供了重要的见解。 |
| [^54] | [Large Language Models Empowered Autonomous Edge AI for Connected Intelligence.](http://arxiv.org/abs/2307.02779) | 大型语言模型赋能连接智能的自主边缘AI系统，通过云-边缘-客户端的分层架构和强大的GPT模型能力，实现高质量、低延迟、隐私保护的AI服务，满足用户个人需求并实现自动化。 |
| [^55] | [Temporal Difference Learning for High-Dimensional PIDEs with Jumps.](http://arxiv.org/abs/2307.02766) | 本文提出了一种用于解决高维度跳跃偏微分方程的时差学习深度学习框架，该方法具有低计算成本和稳健性的优势，可以有效地处理具有不同形式和强度跳跃的问题。 |
| [^56] | [When Does Confidence-Based Cascade Deferral Suffice?.](http://arxiv.org/abs/2307.02764) | 本研究旨在探讨何时基于置信度的级联延迟可能失败，以及何时备选的延迟策略可能表现更好。通过理论分析和实验证明事后延迟机制能够显著提高性能。 |
| [^57] | [Offline Reinforcement Learning with Imbalanced Datasets.](http://arxiv.org/abs/2307.02752) | 本文提出了一种在不平衡数据集中的新型离线强化学习方法，通过将CQL与回溯过程相结合来提取策略，从而有效地解决了不平衡数据集带来的挑战。 |
| [^58] | [Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?.](http://arxiv.org/abs/2307.02732) | 本文首次对任务级别的评估进行了研究，发现现有的少样本学习基准并不能以可靠的方式设计，无法获取关于如何评估和选择模型的可靠情况的完整画面。 |
| [^59] | [Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning.](http://arxiv.org/abs/2307.02728) | 分层授权提出了一种可以计算授权的新框架，通过引入变分下界和分层架构，实现了在短期和长期时间尺度上的授权计算，并在模拟机器人任务中得到了验证。 |
| [^60] | [Through the Fairness Lens: Experimental Analysis and Evaluation of Entity Matching.](http://arxiv.org/abs/2307.02726) | 本文通过实验对多种实体匹配技术进行了广泛评估，发现实体匹配存在潜在的不公平性。研究结果表明，当某些人口群体过度代表或某些群体中的姓名更相似时，实体匹配可能存在公平性问题。 |
| [^61] | [Understanding Uncertainty Sampling.](http://arxiv.org/abs/2307.02719) | 本研究通过系统研究流式和池式主动学习下的不确定性采样算法，提出了一个等效损失的概念，并证明不确定性采样算法实质上是针对该等效损失进行优化。 |
| [^62] | [Multi-Similarity Contrastive Learning.](http://arxiv.org/abs/2307.02712) | 本文提出了一种多相似度对比损失方法（MSCon），通过联合利用多个相似度度量的监督来学习可泛化的嵌入，从而实现对新任务的更好领域外泛化。 |
| [^63] | [Towards Symmetry-Aware Generation of Periodic Materials.](http://arxiv.org/abs/2307.02707) | 提出了一种名为SyMat的新的材料生成方法，能够对周期性材料的物理对称性进行感知，并能在生成和优化任务中取得有希望的性能。 |
| [^64] | [Loss Functions and Metrics in Deep Learning. A Review.](http://arxiv.org/abs/2307.02694) | 本文回顾了深度学习中最常见的损失函数和性能测量方法，旨在帮助从业者选择最适合其特定任务的方法。 |
| [^65] | [Kernels, Data & Physics.](http://arxiv.org/abs/2307.02693) | 该论文主要介绍了NTK方法在机器学习问题中的应用，通过找到可处理的内核表达形式来解决一般无法解决的问题，重点讨论了数据精炼和对抗鲁棒性等实际应用。 |
| [^66] | [Scaling In-Context Demonstrations with Structured Attention.](http://arxiv.org/abs/2307.02690) | 本研究提出了一种用于上下文学习的结构化注意力机制，解决了大规模语言模型在使用演示进行上下文学习时遇到的限制与挑战。 |
| [^67] | [GIT: Detecting Uncertainty, Out-Of-Distribution and Adversarial Samples using Gradients and Invariance Transformations.](http://arxiv.org/abs/2307.02672) | 该论文提出了GIT，一种综合方法来检测深度神经网络的泛化错误，该方法结合了梯度信息和不变性变换。通过将错分样本转回神经网络的泛化区域，并测量初始预测与使用转换样本的神经网络的相应计算之间的矛盾，GIT相对于现有技术表现出更好的性能。 |
| [^68] | [Active Class Selection for Few-Shot Class-Incremental Learning.](http://arxiv.org/abs/2307.02641) | 本文结合少样本类增量学习和主动类别选择的思想，提出了一个新的框架(FIASco)，使自主代理通过要求用户仅对最具信息量的少数对象进行标注来不断学习新对象，并将其与基于潜力场的导航技术集成，形成一个完整的框架。 |
| [^69] | [Hybrid Ground-State Quantum Algorithms based on Neural Schr\"odinger Forging.](http://arxiv.org/abs/2307.02633) | 提出了一种基于神经网络的纠缠锻造方法来解决基态问题，通过识别最相关的基态位串，消除了指数级求和的需求，并展示了该方法在不同系统上可以达到相当或更优的性能。 |
| [^70] | [Stability of Q-Learning Through Design and Optimism.](http://arxiv.org/abs/2307.02632) | 本文主要介绍了Q-learning在强化学习中的重要性，以及使用乐观性训练和修改后的策略解决Q-learning的稳定性问题和算法收敛加速问题的方法。 |
| [^71] | [An explainable model to support the decision about the therapy protocol for AML.](http://arxiv.org/abs/2307.02631) | 本文提出了一种可解释的机器学习模型，用于支持AML患者治疗方案的决策，解决了当前风险分类存在的问题和专家需求额外测试和分析的困扰。 |
| [^72] | [FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout.](http://arxiv.org/abs/2307.02623) | FLuID提出了一种使用不变性丢失的方法来减轻联邦学习中性能较低设备导致的训练时间问题，并开发了一个自适应训练框架。通过动态平衡训练负载，FLuID能有效地减轻阻塞设备的工作负载，同时不影响模型质量。 |
| [^73] | [Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning.](http://arxiv.org/abs/2307.02620) | 本文研究了在观测代价敏感强化学习中，强化学习代理在每个时间步不需要昂贵的测量，提出了一种新的方法DMSOA，并在多个环境中进行了评估，结果表明DMSOA能够以更少的决策步骤和测量次数学到更好的策略。 |
| [^74] | [Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition.](http://arxiv.org/abs/2307.02615) | 本研究通过比较学习和渐进对齐的方式，借鉴人类语言习得的过程，探索了一种用于基于经验的词汇获取的计算过程。该方法不涉及固定的词汇量大小，也不涉及有区分性的目标，能够高效地持续学习更多的概念。 |
| [^75] | [Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation.](http://arxiv.org/abs/2307.02598) | 这篇论文解决了表示学习中的潜变量识别和"支持外"图像生成问题，展示了加法解码器能够对潜变量进行识别，并提供了理论依据支持这种方法的有效性。 |
| [^76] | [TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers.](http://arxiv.org/abs/2307.02588) | TransformerG2G是一种使用Transformer进行自适应时间步长的图嵌入模型，通过学习历史上的长程依赖关系，准确地捕捉时态图的动态特征。 |
| [^77] | [Multimodal Temporal Fusion Transformers Are Good Product Demand Forecasters.](http://arxiv.org/abs/2307.02578) | 本文提出了一种使用多模态信息进行产品需求预测的方法，通过融合图像和文本描述等多种信息，有效地解决了传统方法的缺点，并在大型实际数据集上取得了较好的预测效果。 |
| [^78] | [How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?.](http://arxiv.org/abs/2307.02575) | 本研究定量评估和比较了11个公开可用的土地覆盖地图，以确定它们在非洲农田分类和基于卫星地球观测的农业监测中的适用性。研究结果可帮助用户找到最适合其需求的地图，并鼓励未来工作改进地图的一致性和低精度区域的准确性。 |
| [^79] | [Semi-supervised Learning from Street-View Images and OpenStreetMap for Automatic Building Height Estimation.](http://arxiv.org/abs/2307.02574) | 这项工作提出了一种基于街景图像和OpenStreetMap的半监督学习方法，用于自动估计建筑物的高度，并生成低成本的3D城市模型。 |
| [^80] | [Conditional Korhunen-Lo\'{e}ve regression model with Basis Adaptation for high-dimensional problems: uncertainty quantification and inverse modeling.](http://arxiv.org/abs/2307.02572) | 本研究提出了一种改进高维问题中代理模型准确性的方法，通过使用条件Korhunen-Lo\'{e}ve展开来表示参数场，并利用高斯过程回归实现对直接测量的建模。 |
| [^81] | [Conditional independence testing under model misspecification.](http://arxiv.org/abs/2307.02520) | 该论文研究了模型错误下的条件独立性检验，在这种情况下提出了新的近似或上界来衡量基于回归的测试的测试误差，并引入了一种新颖的基于回归的CI检验方法RBPT，对模型错误具有鲁棒性。 |
| [^82] | [Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency.](http://arxiv.org/abs/2307.02516) | 本文提出了一种新方法，利用表征差异性来降低模型的相关性和常见失败模式。通过使架构之间不同深度的中间表示具有差异性，以学习具有不同失败模式的强大集合，结果表明，这种方法可以提高集合的准确性。 |
| [^83] | [Diffusion Models for Computational Design at the Example of Floor Plans.](http://arxiv.org/abs/2307.02511) | 该论文探索了基于扩散模型的AI生成器在计算设计中的能力，并提出了具有改进的语义编码的新扩散模型。利用这些模型，可以提高生成楼层平面的有效性，并改进不同示例的查询性能。该研究还探讨了将扩散模型与建筑信息模型相结合的方法。 |
| [^84] | [Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams).](http://arxiv.org/abs/2307.02509) | 本文提出了一种计算框架，用于将传统的自编码神经网络扩展到合并树的Wasserstein度量空间。算法在准确性和可解释性方面表现出 |
| [^85] | [STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting.](http://arxiv.org/abs/2307.02507) | 本研究通过引入先进的对比学习方法，提出了一种新颖的时空同步上下文对比学习（STS-CCL）模型，用于高效地捕捉大规模无标签交通数据的复杂时空表示。该模型通过使用动态图视图生成器和语义上下文对比方法，实现了节点级和图级的对比学习。 |
| [^86] | [Generalization Guarantees via Algorithm-dependent Rademacher Complexity.](http://arxiv.org/abs/2307.02501) | 提出了一种通过算法和数据相关的假设类的经验Rademacher复杂度来控制泛化错误的方法，基于有限分形维度获得了新的界限，并简化了对随机梯度下降的无维度泛化界限的证明。 |
| [^87] | [Multi-gauge Hydrological Variational Data Assimilation: Regionalization Learning with Spatial Gradients using Multilayer Perceptron and Bayesian-Guided Multivariate Regression.](http://arxiv.org/abs/2307.02497) | 本研究提出了一种新的水文数据同化方法，通过使用多层感知器和贝叶斯引导的多元回归，实现了无缝区域化学习。这种方法可以准确估计空间分布的水文参数，并且解决了可行解的等似性问题。 |
| [^88] | [Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion.](http://arxiv.org/abs/2307.02496) | 本研究利用可逆神经网络和误差扩散方法，通过测量气泡引起的磁场波动，重建电解过程中的气泡分布和电导率图，并实现了比传统方法更优异的性能。 |
| [^89] | [FREEDOM: Target Label & Source Data & Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization.](http://arxiv.org/abs/2307.02493) | 本文提出了一种新的问题场景TFDA，即三无领域自适应，解决了多源领域自适应中目标标签、源数据和领域信息不可用的问题。这种方法更加实用，避免了对先前领域信息的依赖和数据隐私问题。 |
| [^90] | [TablEye: Seeing small Tables through the Lens of Images.](http://arxiv.org/abs/2307.02491) | 本文提出了一种创新的框架TablEye，通过生成表格图像来实现领域转换，克服了形成表格数据先验知识的限制，从而实现了少样本表格学习。 |
| [^91] | [Dynamical Isometry based Rigorous Fair Neural Architecture Search.](http://arxiv.org/abs/2307.02263) | 本论文提出了一种基于动态等色性的神经架构搜索算法，通过分析稳态随机神经网络的动力学行为和使用良好条件的Jacobian来保证权重共享的公平性，实验证明所提出的算法搜索到的架构具有更好的性能。 |
| [^92] | [Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework.](http://arxiv.org/abs/2307.01715) | 本文提出了一个通用的插入式框架，用于优化CTC模型中的所需属性。该框架通过补充额外的损失项来优先考虑符合所需属性的对齐，并不需要修改CTC损失函数。 |
| [^93] | [Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network.](http://arxiv.org/abs/2307.01622) | 本研究提出了一种基于循环趋势预测神经网络的嵌入式调度预测算法，在智能家居环境中实现了高效的住宅需求控制，并同时预测可再生能源发电。该算法具有鲁棒性，能够应对预测误差。 |
| [^94] | [Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly.](http://arxiv.org/abs/2307.01317) | 本文提出了一种基于密度的可行性学习方法，使用归一化流进行自省式机器人装配。该方法只需要可行的示例来训练，可以更好地检测不可行的装配方案。 |
| [^95] | [REAL: A Representative Error-Driven Approach for Active Learning.](http://arxiv.org/abs/2307.00968) | 本研究提出了一种名为REAL的新方法，该方法通过选择具有代表性错误的数据实例来改进主动学习。通过考虑错误实例及其邻域中的错误密度，REAL在准确率和F1-macro得分方面优于其他算法。 |
| [^96] | [UTRNet: High-Resolution Urdu Text Recognition In Printed Documents.](http://arxiv.org/abs/2306.15782) | 本文提出了一种解决印刷乌尔都文本识别挑战的新方法，并引入了大规模实际标记数据集和合成数据集，提供了乌尔都文本行检测的基准数据集，同时开发了一个在线工具，实现了印刷文档中乌尔都OCR的端到端识别。 |
| [^97] | [Balanced Filtering via Non-Disclosive Proxies.](http://arxiv.org/abs/2306.15083) | 本文研究了在群组成员资格不可用或被禁止使用时，如何以非泄露方式收集平衡的数据样本。通过使用代理函数和抽样概率，实现了对个体样本的分类和选择，同时保护个体样本的敏感群组成员资格不被泄露。 |
| [^98] | [Co-design Hardware and Algorithm for Vector Search.](http://arxiv.org/abs/2306.11182) | 本论文提出了一个在FPGA上的向量搜索框架FANNS，实现了硬件和算法的共同设计，可以根据用户需求和硬件预算生成相应的加速器。与FPGA和CPU基准相比，FANNS实现了显著的加速，并展现了卓越的可扩展性。 |
| [^99] | [Active Policy Improvement from Multiple Black-box Oracles.](http://arxiv.org/abs/2306.10259) | 本研究提出了MAPS和MAPS-SE两个算法，可在多黑盒预言情况下，采用模仿学习并主动选择和改进最优预言，显著提升了性能。 |
| [^100] | [Fairness in Multi-Task Learning via Wasserstein Barycenters.](http://arxiv.org/abs/2306.10155) | 本文提出了一种方法，通过多元Wasserstein barycenters扩展`Strong Demographic Parity`的定义，实现多任务学习中的公平性，包括回归和二元分类任务。在实验中表现出良好的效果。 |
| [^101] | [Stacking of Hyperparameter Tuned Models for Tagging Coding Problems.](http://arxiv.org/abs/2306.10077) | 这项工作使用超参数调优的增强模型堆叠方法，在Codeforces和Leetcode的数据集上取得了77.8%的准确率和0.815的PR-AUC分数。 |
| [^102] | [The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases.](http://arxiv.org/abs/2306.06767) | 本研究调查了ChatGPT和LLMs在医学影像领域的变革潜力，它们正在增强放射科医生的解释能力、提升患者与医生之间的沟通，以及简化临床工作流程。 |
| [^103] | [Deep Learning for Day Forecasts from Sparse Observations.](http://arxiv.org/abs/2306.06079) | 本文提出了一种基于稀疏观测数据的MetNet-3深度学习模型，可对20小时内的天气进行准确的预测。MetNet-3的技术创新包括可学习卷积、特征学习和多任务训练优化。此外，使用持续性启发法来外推初始条件或进行短期预测来填补缺失的观测数据更进一步提高了预测性能。 |
| [^104] | [Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers.](http://arxiv.org/abs/2306.04504) | 本文评估了ChatGPT在生物医学任务上的表现，发现在生物数据集训练样本较小时，零样例ChatGPT甚至优于精调生成式变压器模型。由此表明ChatGPT具有在生物医学领域成为有价值工具的潜力。 |
| [^105] | [Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL.](http://arxiv.org/abs/2306.04220) | 本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。 |
| [^106] | [A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets.](http://arxiv.org/abs/2305.18486) | 本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。 |
| [^107] | [Duality in Multi-View Restricted Kernel Machines.](http://arxiv.org/abs/2305.17251) | 该论文提出了一个统一的框架，将现有的受限制内核机器方法结合成一个单一的原始-对偶多角度框架，可用于核主成分分析，实现了原始和对偶公式的完全等价性，并最终在时间序列数据集上验证了其等价性和提供的洞察。 |
| [^108] | [Layer-wise Adaptive Step-Sizes for Stochastic First-Order Methods for Deep Learning.](http://arxiv.org/abs/2305.13664) | 本文提出了一种针对深度学习的随机一阶优化方法的分层自适应步长策略，通过利用深度神经网络中浅层的随机曲率信息为每一层计算自适应步长，消除了用户调整学习率的需求。实验结果显示，结合该策略的算法在DNN任务的训练中优于精细调整学习率版本以及流行的一阶和二阶算法。 |
| [^109] | [Time Series Clustering With Random Convolutional Kernels.](http://arxiv.org/abs/2305.10457) | 该论文介绍了一种新方法用于时间序列聚类，该方法利用随机卷积结构将数据转换为增强的特征表示，再进行聚类，以识别异常值，该方法在时间序列聚类基准上实现了更好的结果。 |
| [^110] | [ZeroFlow: Fast Zero Label Scene Flow via Distillation.](http://arxiv.org/abs/2305.10424) | ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。 |
| [^111] | [Intercomparison of Brown Dwarf Model Grids and Atmospheric Retrieval Using Machine Learning.](http://arxiv.org/abs/2305.07719) | 本文通过机器学习方法分析了14个棕矮星模型网格的预测能力，发现棕矮星的有效温度可以被预测，但推断表面重力加速度和金属丰度与模型网格有关。 |
| [^112] | [Effects of data time lag in a decision-making system using machine learning for pork price prediction.](http://arxiv.org/abs/2305.05677) | 本文研究了数据采集延迟对机器学习预测模型在猪肉价格预测中的影响，并提出了解决方案，通过利用同日获取的数据，可以有效缓解数据时间滞后的影响。 |
| [^113] | [FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models.](http://arxiv.org/abs/2304.13407) | 该论文提出FedVS，一种同时解决垂直联邦学习中滞后客户端和数据泄露问题的方法，通过设计本地数据和模型的秘密共享方案，以保证信息理论隐私，并通过解密计算股份，无损重构所有客户端的嵌入的汇总。 |
| [^114] | [Learning to Predict Navigational Patterns from Partial Observations.](http://arxiv.org/abs/2304.13242) | 本文提出了一种仅通过局部观测学习预测真实环境中导航模式的自监督学习方法，能够胜过两个有监督模型，并且可以在无限数据的情况下预测无偏本地方向软车道概率场。 |
| [^115] | [An Automatic Guidance and Quality Assessment System for Doppler Imaging of Umbilical Artery.](http://arxiv.org/abs/2304.05463) | 提出了一个自动导向和质量评估系统，使用改进的 Faster R-CNN 算法来建议脐动脉多普勒流门的位置，并评估多普勒波形质量，有效地填补了经验不足的超声医生的缺陷。 |
| [^116] | [SLPerf: a Unified Framework for Benchmarking Split Learning.](http://arxiv.org/abs/2304.01502) | SLPerf是一个统一的研究和开放式研究库，用于共享学习，通过对不同情况下不同共享学习范式的基准比较，提供了改进共享学习范式的见解。 |
| [^117] | [Sandwiched Video Compression: Efficiently Extending the Reach of Standard Codecs with Neural Wrappers.](http://arxiv.org/abs/2303.11473) | 本文提出了夹心视频压缩方法，通过包装标准编解码器来使神经网络优化压缩性能，在高清视频传输和语音识别视频压缩等场景中表现显著。 |
| [^118] | [Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning.](http://arxiv.org/abs/2303.10135) | 本文提出了一种基于图表示学习的装配序列规划方法，通过GRACE模型可以从装配图中提取信息并预测可行的装配序列。 |
| [^119] | [Fast and Multi-aspect Mining of Complex Time-stamped Event Streams.](http://arxiv.org/abs/2303.03789) | 这篇论文提出了一种名为CubeScope的方法，用于快速、多方面地挖掘复杂的时间戳事件流。该方法能够识别突然的不连续性和不同的动态模式，并对所有属性进行多方面摘要，并发现隐藏的群体和其关系。CubeScope还能检测到异常的突然出现。 |
| [^120] | [Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization.](http://arxiv.org/abs/2303.03108) | 研究提出了一阶平坦度的概念，使用梯度范数感知最小化算法寻找在所有方向上具有均匀小曲率的极小值，提高了广义化能力和测试损失。 |
| [^121] | [Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials.](http://arxiv.org/abs/2303.02216) | 该论文提出了一种用于准确和可迁移神经势的非平衡分子去噪预训练方法，并通过实验证明了该方法能显著提高势能模型的准确性和可迁移性。 |
| [^122] | [Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression.](http://arxiv.org/abs/2303.02118) | 本研究研究了混合稀疏线性回归问题，在实际应用中发现了统计和计算之间的权衡关系，并确定了样本复杂度和运行时间之间的平滑信息-计算权衡关系。 |
| [^123] | [Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings.](http://arxiv.org/abs/2302.07729) | 该论文提出了一种使用指针生成网络和SciBERT嵌入来自动生成研究论文亮点的方法。在多个基准数据集上的实验证明，该模型在研究亮点生成方面具有最佳性能。 |
| [^124] | [Unleashing the Power of Electrocardiograms: A novel approach for Patient Identification in Healthcare Systems with ECG Signals.](http://arxiv.org/abs/2302.06529) | 本研究提出了一种利用心电图信号进行患者识别的新方法，通过卷积神经网络对提取的心电图图像进行分类。该方法综合考虑了心血管疾病对用户识别的影响，并通过实验验证了其准确性和可靠性。 |
| [^125] | [Approximating the Shapley Value without Marginal Contributions.](http://arxiv.org/abs/2302.00736) | 本文提出了两种无参数、领域无关的Shapley值近似算法SVARM和Stratified SVARM，它们基于一种与边际贡献概念脱钩的Shapley值表示。我们证明了它们在近似质量方面具有无与伦比的理论保证，并通过实证结果将其与合成游戏和常见可解释性用例进行了比较。 |
| [^126] | [ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation.](http://arxiv.org/abs/2301.13166) | 本文提出了一种新颖的零样本物体导航方法 ESC，它从预先训练的视觉和自然语言处理模型中转移常识知识，可在未知环境中进行导航，具有广阔的应用前景。 |
| [^127] | [Adapting Neural Link Predictors for Complex Query Answering.](http://arxiv.org/abs/2301.12313) | 本文提出通过训练一个参数高效的分数适应模型来重新校准神经链接预测分数以解决神经链接预测器在复杂查询回答中的问题。 |
| [^128] | [Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise.](http://arxiv.org/abs/2301.01054) | 本文对常用的组织病理学图像分类的不确定性估计方法进行了基准测试，比较了不同方法在领域转移和标签噪声下的性能，并得出方法的集合通常会有更好的结果。 |
| [^129] | [Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model.](http://arxiv.org/abs/2212.09811) | 本研究提出了一种节约内存的NLLB-200模型修剪方法，可在保持翻译质量的同时移除多达80％的专家，使得在单个32GB的GPU上运行模型成为可能。这对于大规模多语言机器翻译具有重要的意义。 |
| [^130] | [A Recursively Recurrent Neural Network (R2N2) Architecture for Learning Iterative Algorithms.](http://arxiv.org/abs/2211.12386) | 本文提出了一个名为 R2N2 的递归循环神经网络结构，用于学习定制的迭代算法。与传统的深度学习方法不同的是，R2N2 将生成信息和组装信息的过程划分为不同的模块，通过在每次迭代中评估函数来生成局部信息，并将这些评估的线性组合用于更新下一次迭代的结果，从而降低算法的复杂性。 |
| [^131] | [A Finite-Particle Convergence Rate for Stein Variational Gradient Descent.](http://arxiv.org/abs/2211.09721) | 本文提供了Stein变分梯度下降算法的有限粒子收敛速度，证明了当目标分布为次高斯且具有Lipschitz积分核时，使用适当的步长序列和粒子数量，可以以1/√(log log n)的速度将核Stein差异逼近零。 |
| [^132] | [DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models.](http://arxiv.org/abs/2210.14896) | 介绍了DiffusionDB数据集，这是一个规模庞大的文本到图像提示数据集，总计包含1400万张图像和180万个唯一提示。该数据集被用来帮助研究人员解决文本提示生成图像时所需的适当提示的问题，并指出了一些特定的提示样式和超参数值可能导致模型错误，甚至生成误导信息。 |
| [^133] | [Chaos Theory and Adversarial Robustness.](http://arxiv.org/abs/2210.13235) | 本文利用混沌理论解释、分析和量化了神经网络对抗性攻击的易受性和鲁棒性，提出了一种新的度量指标“易受性比”，结果表明模型深度增加会显著增加对抗攻击的易受性，这对于生产环境中的神经网络设计具有安全影响。 |
| [^134] | [Meta Learning of Interface Conditions for Multi-Domain Physics-Informed Neural Networks.](http://arxiv.org/abs/2210.12669) | 提出了一种元学习方法来动态确定解决一类参数化PDEs所需的适当接口条件，以进一步改善多领域物理信息神经网络（PINNs）的性能。 |
| [^135] | [A Detailed Study of Interpretability of Deep Neural Network based Top Taggers.](http://arxiv.org/abs/2210.04371) | 本文详细研究了深度神经网络基于Top Tagger的可解释性。通过回顾现有模型并探索不同定量方法，我们确定了在识别顶夸克的喷注中起关键作用的特征。我们的研究揭示了现有XAI方法的问题，并提出了克服这些问题的方法。 |
| [^136] | [PathProx: A Proximal Gradient Algorithm for Weight Decay Regularized Deep Neural Networks.](http://arxiv.org/abs/2210.03069) | 本文提出一种用于权值衰减正则化深度神经网络的近端梯度算法 PathProx，它可以更快地收敛到标准权值衰减训练所共享的稀疏解。 |
| [^137] | [Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks.](http://arxiv.org/abs/2209.05251) | 本论文通过使用卫星图像来预测西尼罗河病毒的循环，提出了一种空间感知的方法，利用图神经网络(GNN)来聚合邻居的特征。 |
| [^138] | [A methodology for identifying resiliency in renewable electrical distribution system using complex network.](http://arxiv.org/abs/2208.11543) | 本文提出一种使用复杂网络理论来识别可再生电力分布系统弹性的方法，可以识别系统中太阳能电池板的托管能力，从而有助于提高系统的韧性。 |
| [^139] | [DataPerf: Benchmarks for Data-Centric AI Development.](http://arxiv.org/abs/2207.10062) | DataPerf是一个由社区主导的基准测试套件，旨在通过竞争、可比性和可重复性促进数据中心人工智能的创新。 |
| [^140] | [Modeling Content Creator Incentives on Algorithm-Curated Platforms.](http://arxiv.org/abs/2206.13102) | 该论文讨论了在线平台上内容创作者激励机制的建模，通过分析算法选择对曝光游戏（包括现代分解和两塔架构）中（纳什）均衡的影响，提出了使用曝光游戏模型进行预部署审计的方法，以识别期望和激励内容之间的不匹配。 |
| [^141] | [An AI tool for automated analysis of large-scale unstructured clinical cine CMR databases.](http://arxiv.org/abs/2206.08137) | 本研究提出了一种用于自动分析大规模非结构化临床心脏磁共振数据库的人工智能工具，并验证了其稳健性和准确性。该工具能够从短轴心脏磁共振影像中自动量化心脏功能，并具备自动化的质量控制功能，可检测和纠正错误。该算法在大量数据集上进行了训练和验证，具备良好的适应性和推广性。 |
| [^142] | [Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations.](http://arxiv.org/abs/2206.04779) | 该论文研究了离线强化学习从视觉观察中的挑战和机遇，针对这一复杂领域建立了视觉领域中连续控制的简单基准，并设计了一系列基准任务，以更好地表示现实世界离线RL问题中的数据分布，并通过对两种基于视觉的在线强化学习算法的简单修改进行评估。 |
| [^143] | [Trainable Weight Averaging: A General Approach for Subspace Training.](http://arxiv.org/abs/2205.13104) | 可训练的权重平均值是一种通用的子空间训练方法，通过连接子空间训练和权重平均值，提供高效的训练和易于使用的方法。这种方法可以用于改进神经网络训练效果和降低计算负担。 |
| [^144] | [Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching.](http://arxiv.org/abs/2205.03447) | 本文介绍了五个新的生物医学本体匹配任务，通过引入机器学习技术并解决现有评估方法的限制，提供了综合评估框架来衡量本体匹配系统的性能。 |
| [^145] | [A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond.](http://arxiv.org/abs/2204.09269) | 这项调查研究了非自回归生成在神经机器翻译以及其他领域的应用。研究发现，尽管非自回归生成可以加快推理速度，但与自回归生成相比存在翻译准确性的损失。然而，通过各种方法和算法的改进，可以缩小这一准确性差距。 |
| [^146] | [Learning Low-Dimensional Nonlinear Structures from High-Dimensional Noisy Data: An Integral Operator Approach.](http://arxiv.org/abs/2203.00126) | 本文提出了一种用于从高维噪声数据中学习低维非线性结构的算法，该算法使用自适应带宽选择过程，并获得了理论上的收敛性证明。算法的低维嵌入结果可用于数据可视化、聚类和预测等任务。 |
| [^147] | [Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms.](http://arxiv.org/abs/2201.07856) | 这项研究发现，机器学习算法在胸部X射线数据集上训练时会在未得到足够服务的人群中产生高虚报率，可能放大了系统的未诊断问题。然而，研究的实验设置不足以全面研究算法的未诊断问题，而且使用与训练数据相同偏倚的测试数据进一步加剧了结果的解释难度。 |
| [^148] | [Expert Aggregation for Financial Forecasting.](http://arxiv.org/abs/2111.15365) | 本文应用伯恩斯坦在线聚合（BOA）方法，将不同机器学习模型的个股收益预测结合成多空策略的构建。专家在线聚合在非平稳环境下表现出有吸引力的投资组合表现，胜过单独的算法，并提出了扩展方法以改善整体混合的性能。 |
| [^149] | [Convex-Concave Min-Max Stackelberg Games.](http://arxiv.org/abs/2110.05192) | 本文介绍了两种一阶方法来求解一类大规模凸凹极小极大 Stackelberg 博弈，并证明了方法的多项式时间收敛性。 |
| [^150] | [Convolutional Filtering and Neural Networks with Non Commutative Algebras.](http://arxiv.org/abs/2108.09923) | 本文介绍了非交换卷积神经网络的代数推广，利用代数信号处理理论建模非交换卷积架构，并推导出稳定性界限。研究表明非交换卷积架构可以在算子空间上保持稳定性，且非交换滤波器独立地处理傅里叶分量。稳定性和选择性之间存在权衡，由低维矩阵空间中的矩阵多项式函数所控制。 |
| [^151] | [Airfoil GAN: Encoding and Synthesizing Airfoils for Aerodynamic Shape Optimization.](http://arxiv.org/abs/2101.04757) | 本文提出了一种基于数据驱动的方法，通过编码和生成现有翼型，进而优化合成翼型的气动性能。该方法利用VAEGAN模型将翼型编码为潜在向量，并通过映射潜在向量到翼型坐标空间生成新的翼型。 |
| [^152] | [ShadowNet: A Secure and Efficient On-device Model Inference System for Convolutional Neural Networks.](http://arxiv.org/abs/2011.05905) | ShadowNet是一种安全高效的设备内推断系统，通过使用可信执行环境（TEE）保护模型隐私，同时将模型的重型线性层外包给不受信任的硬件加速器，实现了对模型隐私的保护。 |

# 详细

[^1]: 从文本生成艺术性的影动图

    Synthesizing Artistic Cinemagraphs from Text. (arXiv:2307.03190v1 [cs.CV])

    [http://arxiv.org/abs/2307.03190](http://arxiv.org/abs/2307.03190)

    本论文介绍了一种通过文本描述来创建艺术性影动图的自动化方法。通过合成图像双胞胎，即一对艺术图像和与之对齐的真实图像，可以同时满足艺术风格和外观的要求并简化动作分析。同时，利用现有数据集可以准确地分割真实图像并预测合理的运动。

    

    我们介绍了一种全自动的方法，通过文本描述来创建艺术性的影动图。在处理虚构元素和艺术风格的提示时，这是一项特别具有挑战性的任务，因为需要解释这些图像的语义和动作的复杂性。现有的单图动画方法在艺术性输入方面存在不足，而最近的基于文本的视频方法常常引入时间不一致性，难以使某些区域保持静态。为了应对这些挑战，我们提出了一种通过单个文本提示合成图像双胞胎的思想，即艺术图像和其与像素对齐的自然外观配对。虽然艺术图像描绘了我们在文本提示中详细描述的风格和外观，但真实的对应图像大大简化了布局和动作分析。利用现有的自然图像和视频数据集，我们可以准确地分割出真实图像并根据语义信息预测出合理的运动。

    We introduce Artistic Cinemagraph, a fully automated method for creating cinemagraphs from text descriptions - an especially challenging task when prompts feature imaginary elements and artistic styles, given the complexity of interpreting the semantics and motions of these images. Existing single-image animation methods fall short on artistic inputs, and recent text-based video methods frequently introduce temporal inconsistencies, struggling to keep certain regions static. To address these challenges, we propose an idea of synthesizing image twins from a single text prompt - a pair of an artistic image and its pixel-aligned corresponding natural-looking twin. While the artistic image depicts the style and appearance detailed in our text prompt, the realistic counterpart greatly simplifies layout and motion analysis. Leveraging existing natural image and video datasets, we can accurately segment the realistic image and predict plausible motion given the semantic information. The predi
    
[^2]: TGRL:一种用于教师引导强化学习的算法

    TGRL: An Algorithm for Teacher Guided Reinforcement Learning. (arXiv:2307.03186v1 [cs.LG])

    [http://arxiv.org/abs/2307.03186](http://arxiv.org/abs/2307.03186)

    TGRL是一种用于教师引导强化学习的算法，通过动态和自动平衡何时遵循教师指导和何时使用奖励，教师监督的重要性会根据代理的表现调整。

    

    学习奖励(即强化学习或RL)和学习模仿教师(即教师-学生学习)是解决顺序决策问题的两种成熟方法。为了结合这些不同形式学习的优点，通常会训练一个策略来最大化强化学习和教师-学生学习目标的组合。然而，如果没有一个有原则的方法来平衡这些目标，之前的工作使用启发式方法和问题特定的超参数搜索来平衡两个目标。我们提出了一种"有原则"的方法，并提出了一种近似实现"动态"和"自动"平衡何时遵循教师和何时使用奖励。主要思想是通过比较代理的性能与没有教师监督并只从奖励中学习的对照情景来调整教师监督的重要性。如果使用教师监督改善了代理的性能，那么教师监督的重要性就会增加。

    Learning from rewards (i.e., reinforcement learning or RL) and learning to imitate a teacher (i.e., teacher-student learning) are two established approaches for solving sequential decision-making problems. To combine the benefits of these different forms of learning, it is common to train a policy to maximize a combination of reinforcement and teacher-student learning objectives. However, without a principled method to balance these objectives, prior work used heuristics and problem-specific hyperparameter searches to balance the two objectives. We present a $\textit{principled}$ approach, along with an approximate implementation for $\textit{dynamically}$ and $\textit{automatically}$ balancing when to follow the teacher and when to use rewards. The main idea is to adjust the importance of teacher supervision by comparing the agent's performance to the counterfactual scenario of the agent learning without teacher supervision and only from rewards. If using teacher supervision improves 
    
[^3]: 异构特征子采样的Ridge Ensemble的学习曲线

    Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles. (arXiv:2307.03176v1 [stat.ML])

    [http://arxiv.org/abs/2307.03176](http://arxiv.org/abs/2307.03176)

    通过引入异构特征集成，本文研究了在线性回归设置中构建的岭回归集成的学习曲线，结果表明异构特征集成具有较好的性能。

    

    特征包装是一种旨在通过在随机子样本或特征投影上训练估计器来减少预测方差的成熟集成方法。通常，集成选择是同质的，即估计器可用的特征维数在整个集成中是均匀的。在这里，我们介绍了异构特征集成方法，其中的估计器基于变动的特征维数，并研究其在线性回归设置中的性能。我们研究了一个线性预测器的集成，每个预测器使用部分可用特征进行岭回归拟合。我们允许这些子集中包含的特征数量有所变化。利用统计物理中的复制技巧，我们推导了具有确定性线性掩模的岭回归集成的学习曲线。对于具有各向同性特征噪声的等相相关数据，我们得到了学习曲线的显式表达式。利用这些推导表达式，我们研究了集成在不同特征维数下的性能。

    Feature bagging is a well-established ensembling method which aims to reduce prediction variance by training estimators in an ensemble on random subsamples or projections of features. Typically, ensembles are chosen to be homogeneous, in the sense the the number of feature dimensions available to an estimator is uniform across the ensemble. Here, we introduce heterogeneous feature ensembling, with estimators built on varying number of feature dimensions, and consider its performance in a linear regression setting. We study an ensemble of linear predictors, each fit using ridge regression on a subset of the available features. We allow the number of features included in these subsets to vary. Using the replica trick from statistical physics, we derive learning curves for ridge ensembles with deterministic linear masks. We obtain explicit expressions for the learning curves in the case of equicorrelated data with an isotropic feature noise. Using the derived expressions, we investigate t
    
[^4]: 推开绿色：通过移动植物来查看植物叶片背后的内容的学习

    Push Past Green: Learning to Look Behind Plant Foliage by Moving It. (arXiv:2307.03175v1 [cs.RO])

    [http://arxiv.org/abs/2307.03175](http://arxiv.org/abs/2307.03175)

    本文提出了一种通过移动植物来查看叶片背后内容的方法，通过使用自我监督训练了一个神经网络SRPNet，可以预测有效的显露出植物叶片下空间的动作，进一步可以通过执行一系列动作逐步显露出更多空间，实验结果表明该方法在合成和真实植物上都取得了良好的效果。

    

    自主农业应用（例如检查、表型分析、采摘水果）需要操作植物叶片以查看叶子和枝干的背后。部分可见性、极端杂乱、薄结构以及植物的未知几何和动力学都使得这种操作具有挑战性。我们通过数据驱动的方法解决了这些挑战。我们使用自我监督来训练SRPNet，一个神经网络，该网络预测在给定植物上执行候选动作时会显露出多少空间。我们使用带有交叉熵方法的SRPNet来预测有效地显露出植物叶片下的空间的动作。此外，由于SRPNet不仅预测显露出多少空间，还预测显露出空间的位置，因此我们可以执行一系列动作，逐步显露出更多的植物叶片下的空间。在物理测试平台上，我们对合成的藤蔓和真实植物（龙血树）进行了实验，涵盖了5个设置，包括2个测试泛化性能的设置。

    Autonomous agriculture applications (e.g., inspection, phenotyping, plucking fruits) require manipulating the plant foliage to look behind the leaves and the branches. Partial visibility, extreme clutter, thin structures, and unknown geometry and dynamics for plants make such manipulation challenging. We tackle these challenges through data-driven methods. We use self-supervision to train SRPNet, a neural network that predicts what space is revealed on execution of a candidate action on a given plant. We use SRPNet with the cross-entropy method to predict actions that are effective at revealing space beneath plant foliage. Furthermore, as SRPNet does not just predict how much space is revealed but also where it is revealed, we can execute a sequence of actions that incrementally reveal more and more space beneath the plant foliage. We experiment with a synthetic (vines) and a real plant (Dracaena) on a physical test-bed across 5 settings including 2 settings that test generalization to
    
[^5]: Focused Transformer: 反差训练对上下文缩放进行优化

    Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v1 [cs.CL])

    [http://arxiv.org/abs/2307.03170](http://arxiv.org/abs/2307.03170)

    Focused Transformer通过反差训练优化了上下文缩放问题，允许语言模型处理更长的上下文信息。

    

    大规模语言模型能够以上下文化的方式吸纳新的信息，但由于有效上下文长度的限制，这种方法的潜力通常受到限制。解决这个问题的一种方法是为注意力层提供访问外部存储器的能力，该存储器由（键，值）对组成。然而，随着文档数量的增加，相关键与无关键的比例减少，使模型更加关注无关键。我们发现了一个名为分心问题的重要挑战，即与不同语义值相关联的键可能重叠，使它们难以区分。为了解决这个问题，我们引入了Focused Transformer（FoT），一种受对比学习启发的训练方法。这种新颖的方法增强了（键，值）空间的结构，使上下文长度得以扩展。我们的方法允许对现有大型模型进行微调，以更好地处理长上下文。

    Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-s
    
[^6]: 域适应能提高皮肤病变分类的准确性和公平性吗？

    Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?. (arXiv:2307.03157v1 [cs.CV])

    [http://arxiv.org/abs/2307.03157](http://arxiv.org/abs/2307.03157)

    本研究利用多个皮肤病变数据集，研究了域适应方法在皮肤病变分类中的应用。结果表明，域适应在减少不平衡的情况下对于二分类任务有效，但在多分类任务中性能较差，需要解决不平衡问题以提高准确性。

    

    深度学习诊断系统在分类皮肤癌症病变时表现出潜力，但缺乏标记数据会影响准确可靠的诊断系统的发展。在本研究中，我们利用多个皮肤病变数据集，研究了各种无监督域适应方法在二分类和多分类皮肤病变分类中的可行性。尤其是，我们评估了三种域适应训练方案：单源、综合和多源。实验结果表明，域适应在二分类中是有效的，在减少不平衡的情况下还能进一步提高。在多分类任务中，其性能不太明显，需要解决不平衡问题才能达到基准准确性。通过量化分析，我们发现多分类任务的测试错误与标签偏移强烈相关。

    Deep learning-based diagnostic system has demonstrated potential in classifying skin cancer conditions when labeled training example are abundant. However, skin lesion analysis often suffers from a scarcity of labeled data, hindering the development of an accurate and reliable diagnostic system. In this work, we leverage multiple skin lesion datasets and investigate the feasibility of various unsupervised domain adaptation (UDA) methods in binary and multi-class skin lesion classification. In particular, we assess three UDA training schemes: single-, combined-, and multi-source. Our experiment results show that UDA is effective in binary classification, with further improvement being observed when imbalance is mitigated. In multi-class task, its performance is less prominent, and imbalance problem again needs to be addressed to achieve above-baseline accuracy. Through our quantitative analysis, we find that the test error of multi-class tasks is strongly correlated with label shift, an
    
[^7]: 计算机断层扫描图像中主动脉和大血管分割的拓扑感知损失

    Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images. (arXiv:2307.03137v1 [eess.IV])

    [http://arxiv.org/abs/2307.03137](http://arxiv.org/abs/2307.03137)

    本文介绍了一种新的拓扑感知损失函数，通过持久同调来惩罚计算机断层扫描图像中主动脉和大血管分割结果与真实值之间的拓扑差异。这种方法能够改善分割任务的性能，尤其是针对具有固有几何特征的对象。

    

    当使用标准损失函数训练分割网络时，网络并没有明确被要求学习图像的全局不变性，如对象的形状和多个对象之间的几何关系。然而，将这些不变性纳入网络训练中可能有助于改善各种分割任务的性能，尤其是当它们是需要分割的对象的固有特性时。本文以计算机断层扫描（CT）图像中主动脉和大血管的分割为例，这些血管由于人体解剖学，通常在身体中以特定的几何形状出现，并在2D CT图像上主要呈现为圆形对象。本文通过引入一种新的拓扑感知损失函数，通过持久同调惩罚地面真实值和预测之间的拓扑差异来解决这个问题。这与先前提出的分割网络设计不同，先前的设计是将阈值滤波应用于预测图像的似然函数。

    Segmentation networks are not explicitly imposed to learn global invariants of an image, such as the shape of an object and the geometry between multiple objects, when they are trained with a standard loss function. On the other hand, incorporating such invariants into network training may help improve performance for various segmentation tasks when they are the intrinsic characteristics of the objects to be segmented. One example is segmentation of aorta and great vessels in computed tomography (CT) images where vessels are found in a particular geometry in the body due to the human anatomy and they mostly seem as round objects on a 2D CT image. This paper addresses this issue by introducing a new topology-aware loss function that penalizes topology dissimilarities between the ground truth and prediction through persistent homology. Different from the previously suggested segmentation network designs, which apply the threshold filtration on a likelihood function of the prediction map 
    
[^8]: 在对称锥上进行的在线凸优化的乘法更新

    Multiplicative Updates for Online Convex Optimization over Symmetric Cones. (arXiv:2307.03136v1 [math.OC])

    [http://arxiv.org/abs/2307.03136](http://arxiv.org/abs/2307.03136)

    本文研究了在线凸优化问题，使用了对称锥乘法权重更新算法(SCMWU)，该算法在任意对称锥的迹为一处进行在线优化，并且在实验证明了其是无悔算法。

    

    我们研究了在线凸优化问题，其中可能的操作是对称锥中的迹为一的元素，这扩展了广泛研究的专家设置及其量子对应物。对称锥为一些最重要的优化模型提供了统一的框架，包括线性、二阶锥和半定优化。使用欧几里德约旦代数领域的工具，我们引入了对称锥乘法权重更新(SCMWU)，这是一个在任意对称锥的迹为一处进行在线优化的无投影算法。我们证明了SCMWU等价于Follow-the-Regularized-Leader和Online Mirror Descent，其正则化器为对称锥负熵。通过这个结构结果，我们证明了SCMWU是无悔算法，并通过大量实验验证了我们的理论结果。

    We study online convex optimization where the possible actions are trace-one elements in a symmetric cone, generalizing the extensively-studied experts setup and its quantum counterpart. Symmetric cones provide a unifying framework for some of the most important optimization models, including linear, second-order cone, and semidefinite optimization. Using tools from the field of Euclidean Jordan Algebras, we introduce the Symmetric-Cone Multiplicative Weights Update (SCMWU), a projection-free algorithm for online optimization over the trace-one slice of an arbitrary symmetric cone. We show that SCMWU is equivalent to Follow-the-Regularized-Leader and Online Mirror Descent with symmetric-cone negative entropy as regularizer. Using this structural result we show that SCMWU is a no-regret algorithm, and verify our theoretical results with extensive experiments. Our results unify and generalize the analysis for the Multiplicative Weights Update method over the probability simplex and the M
    
[^9]: 用于超出分布可泛化性的大型视觉语言模型压缩

    Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])

    [http://arxiv.org/abs/2307.03135](http://arxiv.org/abs/2307.03135)

    本文研究了针对大型视觉语言模型的模型压缩方法，将教师模型的视觉表示压缩到学生模型中。研究重点在于超出分布可泛化的问题，并提出了两个原则来增强学生模型的性能。

    

    大型视觉语言模型取得了出色的性能，但其规模和计算要求使它们在资源受限设备和时间敏感任务上的部署变得不切实际。模型压缩是创建更小、更快的模型以保持较大模型性能的有希望的方法。本文研究了将大型视觉语言模型中的视觉表示压缩到轻量级学生模型中的过程，使用小型或中型数据集。值得注意的是，本研究关注的是超出分布（OOD）可泛化的开放词汇问题，这在以往的模型压缩研究中被忽视了。我们从视觉和语言的角度提出了两个原则来增强学生模型的OOD可泛化性：（1）更好地模仿教师的视觉表示空间，并在视觉语言对齐方面谨慎地促进更好的一致性；（2）通过丰富学生模型的自举学习和数据扩充来提高OOD可泛化性。

    Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
    
[^10]: 在图像分类中对抗分布偏移的测试时间自适应性能评估

    Benchmarking Test-Time Adaptation against Distribution Shifts in Image Classification. (arXiv:2307.03133v1 [cs.LG])

    [http://arxiv.org/abs/2307.03133](http://arxiv.org/abs/2307.03133)

    该论文提出一个基准，用于系统地评估图像分类中对抗分布偏移的测试时间自适应方法的有效性。研究作者使用13个著名的TTA方法及其变体在五个广泛使用的图像分类数据集上进行了评估，并讨论了不同方法在适应性场景中的兼容性。

    

    测试时间自适应（TTA）是一种通过仅在预测过程中利用未标记样本来增强模型泛化性能的技术。针对神经网络系统在面临分布偏移时需要具备鲁棒性的需求，最近提出了许多TTA方法。然而，评估这些方法常常在不同的设置下进行，如不同的分布偏移、主干网络和设计场景，导致缺乏一致和公平的基准来验证它们的有效性。为了解决这个问题，我们提出了一个基准，对五个广泛使用的图像分类数据集（CIFAR-10-C、CIFAR-100-C、ImageNet-C、DomainNet和Office-Home）系统地评估了13个著名的TTA方法及其变体。这些方法涵盖了广泛的适应性场景（例如在线适应与离线适应、实例适应与批量适应与领域适应）。此外，我们还探讨了不同TTA方法在测试时间自适应方面的兼容性。

    Test-time adaptation (TTA) is a technique aimed at enhancing the generalization performance of models by leveraging unlabeled samples solely during prediction. Given the need for robustness in neural network systems when faced with distribution shifts, numerous TTA methods have recently been proposed. However, evaluating these methods is often done under different settings, such as varying distribution shifts, backbones, and designing scenarios, leading to a lack of consistent and fair benchmarks to validate their effectiveness. To address this issue, we present a benchmark that systematically evaluates 13 prominent TTA methods and their variants on five widely used image classification datasets: CIFAR-10-C, CIFAR-100-C, ImageNet-C, DomainNet, and Office-Home. These methods encompass a wide range of adaptation scenarios (e.g. online adaptation v.s. offline adaptation, instance adaptation v.s. batch adaptation v.s. domain adaptation). Furthermore, we explore the compatibility of differe
    
[^11]: T-MARS：通过规避文本特征学习来改善视觉表示

    T-MARS: Improving Visual Representations by Circumventing Text Feature Learning. (arXiv:2307.03132v1 [cs.CV])

    [http://arxiv.org/abs/2307.03132](http://arxiv.org/abs/2307.03132)

    T-MARS提出一种新的数据筛选方法，通过规避文本特征学习，改善了视觉表示的学习，解决了大型多模态数据集中存在的文本与图像重叠的问题。

    

    大型网络来源的多模态数据集为学习通用视觉表示的新方法提供了动力，推动了计算机视觉的最新发展，并彻底改变了零样本和少样本识别。一个关键的决策问题是如何筛选这些日益庞大的数据集。本文提出了一种新的最先进的数据筛选方法，其动机是我们观察到近40%的LAION数据集的图像与说明存在重叠的文本。直觉上，这样的数据可能会浪费资源，因为它鼓励模型进行光学字符识别而不是学习视觉特征。然而，简单地将所有这些数据去除也可能浪费，因为这会丢弃包含视觉特征的图像（除了重叠的文本）。我们提出了一种简单而可扩展的方法来解决这个问题。

    Large web-sourced multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a designated threshold. In this paper, we propose a new state-of-the-art data filtering approach motivated by our observation that nearly 40% of LAION's images contain text that overlaps significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models to perform optical character recognition rather than learning visual features. However, naively removing all such data could also be wasteful, as it throws away images that contain visual features (in addition to overlapping text). Our simple and scalable app
    
[^12]: 主要子束用于降维

    Principal subbundles for dimension reduction. (arXiv:2307.03128v1 [stat.ME])

    [http://arxiv.org/abs/2307.03128](http://arxiv.org/abs/2307.03128)

    本文展示了如何利用次微分几何进行流形学习和曲面重建。通过将局部线性逼近的点云集合成低维子束，根据次微分度量下的次微分测地线解决了多个重要问题，同时在处理噪声数据时具有良好的稳健性。

    

    本文展示了如何利用次微分几何进行流形学习和曲面重建，通过将局部线性逼近的点云集合成低维子束。通过将局部主成分分析得到的局部逼近收集到一个秩为$k$的切觉子束上，其中$k<d$，我们称之为主要子束。这确定了$\mathbb{R}^d$上的次微分度量。我们展示了在这个度量下的次微分测地线可以成功地应用于许多重要问题，例如：明确构造一个近似子流形$M$，在$\mathbb{R}^k$中构造点云的表示，并计算考虑学习到的几何的观测值之间的距离。在切空间被准确估计的极限情况下，重建保证与真实子流形相等。通过模拟实验，我们展示了该框架在处理噪声数据时的稳健性。

    In this paper we demonstrate how sub-Riemannian geometry can be used for manifold learning and surface reconstruction by combining local linear approximations of a point cloud to obtain lower dimensional bundles. Local approximations obtained by local PCAs are collected into a rank $k$ tangent subbundle on $\mathbb{R}^d$, $k<d$, which we call a principal subbundle. This determines a sub-Riemannian metric on $\mathbb{R}^d$. We show that sub-Riemannian geodesics with respect to this metric can successfully be applied to a number of important problems, such as: explicit construction of an approximating submanifold $M$, construction of a representation of the point-cloud in $\mathbb{R}^k$, and computation of distances between observations, taking the learned geometry into account. The reconstruction is guaranteed to equal the true submanifold in the limit case where tangent spaces are estimated exactly. Via simulations, we show that the framework is robust when applied to noisy data. Furth
    
[^13]: 实时机会网络的上下文感知WiFi Direct组配置和管理

    Context-Aware Configuration and Management of WiFi Direct Groups for Real Opportunistic Networks. (arXiv:2307.03126v1 [cs.NI])

    [http://arxiv.org/abs/2307.03126](http://arxiv.org/abs/2307.03126)

    本文提出了一种WiFi Direct组管理协议（WFD-GM），用于实现在实时机会网络中自主连接和组间通信。

    

    WiFi Direct是一种在商用移动设备上支持设备对设备通信（D2D）的有前途的技术。然而，现有的标准不足以完全支持基于D2D的机会网络等网络解决方案的实际部署。实际上，WiFi Direct具有一些特征，可能限制用户个人设备之间自主创建D2D连接。具体而言，标准明确要求用户授权才能建立两个或多个设备之间的连接，并且对组间通信提供有限支持。在某些情况下，这可能导致孤立的节点组的创建，节点组之间无法相互通信。本文提出了一种新的中间件层协议，用于高效配置和管理WiFi Direct组（WiFi Direct Group Manager，WFD-GM），以实现自主连接和组间通信。这使得机会网络得以实现。

    Wi-Fi Direct is a promising technology for the support of device-to-device communications (D2D) on commercial mobile devices. However, the standard as-it-is is not sufficient to support the real deployment of networking solutions entirely based on D2D such as opportunistic networks. In fact, WiFi Direct presents some characteristics that could limit the autonomous creation of D2D connections among users' personal devices. Specifically, the standard explicitly requires the user's authorization to establish a connection between two or more devices, and it provides a limited support for inter-group communication. In some cases, this might lead to the creation of isolated groups of nodes which cannot communicate among each other. In this paper, we propose a novel middleware-layer protocol for the efficient configuration and management of WiFi Direct groups (WiFi Direct Group Manager, WFD-GM) to enable autonomous connections and inter-group communication. This enables opportunistic networks
    
[^14]: 学习多智能体意图感知通信以实现金融中的最优多订单执行

    Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance. (arXiv:2307.03119v1 [cs.AI])

    [http://arxiv.org/abs/2307.03119](http://arxiv.org/abs/2307.03119)

    本文提出了一种多智能体强化学习方法，考虑实际约束下的多订单执行问题。通过智能体之间的通信与协作，最大化整体利润。现有的方法忽视了同时执行多个订单的情况，导致次优性和偏差。

    

    订单执行是量化金融中的一个基本任务，旨在完成特定资产的一系列交易订单的收购或清算。最近无模型强化学习（RL）的进展为订单执行问题提供了一种数据驱动的解决方案。然而，现有的工作总是针对单个订单进行优化，忽视了同时执行多个订单的实践，导致次优性和偏差。在本文中，我们首先提出了一种考虑实际约束的多智能体强化学习（MARL）方法来执行多订单。具体而言，我们将每个智能体视为一个独立的操作员来交易一个特定的订单，同时保持彼此通信并协作以最大化总体利润。然而，现有的MARL算法通常通过仅交换部分观测信息来在智能体之间进行通信，这在复杂的金融环境中是低效的。

    Order execution is a fundamental task in quantitative finance, aiming at finishing acquisition or liquidation for a number of trading orders of the specific assets. Recent advance in model-free reinforcement learning (RL) provides a data-driven solution to the order execution problem. However, the existing works always optimize execution for an individual order, overlooking the practice that multiple orders are specified to execute simultaneously, resulting in suboptimality and bias. In this paper, we first present a multi-agent RL (MARL) method for multi-order execution considering practical constraints. Specifically, we treat every agent as an individual operator to trade one specific order, while keeping communicating with each other and collaborating for maximizing the overall profits. Nevertheless, the existing MARL algorithms often incorporate communication among agents by exchanging only the information of their partial observations, which is inefficient in complicated financial
    
[^15]: 量子解决隐私与效用权衡问题

    Quantum Solutions to the Privacy vs. Utility Tradeoff. (arXiv:2307.03118v1 [quant-ph])

    [http://arxiv.org/abs/2307.03118](http://arxiv.org/abs/2307.03118)

    提出了一种基于量子密码学原理的新型架构，用于保护生成模型免受成员推断攻击，具有可证明的隐私和安全保证。

    

    在这项工作中，我们提出了一种基于量子密码学原理的新型架构（以及其几个变体），能够针对生成模型的成员推断攻击提供可证明的隐私和安全保证。我们的架构可以在任何现有的经典或量子生成模型之上使用。我们认为，与基于标准差分隐私技术的方法相比，使用与单位算子相关的量子门提供了与所有多项式时间对手的有保证的安全性的固有优势。

    In this work, we propose a novel architecture (and several variants thereof) based on quantum cryptographic primitives with provable privacy and security guarantees regarding membership inference attacks on generative models. Our architecture can be used on top of any existing classical or quantum generative models. We argue that the use of quantum gates associated with unitary operators provides inherent advantages compared to standard Differential Privacy based techniques for establishing guaranteed security from all polynomial-time adversaries.
    
[^16]: 如何检测文本到图像扩散模型中的未授权数据使用

    How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models. (arXiv:2307.03108v1 [cs.CV])

    [http://arxiv.org/abs/2307.03108](http://arxiv.org/abs/2307.03108)

    本文提出了一种方法，通过在训练的文本到图像扩散模型中植入注入的记忆化内容，来检测未授权数据使用。该方法修改了受保护的图像数据集，添加了对人眼不可察觉但模型可以捕捉和记忆的内容，通过分析模型对注入内容的记忆来判断模型是否存在生成类似图像的能力。

    

    最近的文本到图像扩散模型在生成高质量图像方面表现出令人惊讶的性能。然而，对于训练过程中的未授权数据使用引起了关注。一个例子是当模型训练者收集了一个特定艺术家创建的一系列图像，并试图训练一个能够生成类似图像的模型，而没有获得艺术家的许可。为了解决这个问题，我们提出了一种方法，通过将注入的记忆化内容植入保护数据集上训练的文本到图像扩散模型中，来检测此类未授权数据使用。具体地，我们通过在图像上添加独特的内容，例如对人类视觉不可察觉但能够被扩散模型捕捉和记忆的隐秘图像包装函数，来修改受保护的图像数据集。通过分析模型是否对注入的内容进行记忆化，我们可以判断模型是否存在这一记忆（即是否存在生成类似图像的能力）。

    Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized usage of data during the training process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission from the artist. To address this issue, it becomes crucial to detect unauthorized data usage. In this paper, we propose a method for detecting such unauthorized data usage by planting injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected image dataset by adding unique contents on the images such as stealthy image wrapping functions that are imperceptible to human vision but can be captured and memorized by diffusion models. By analyzing whether the model has memorization for the injected content (i.e., whether the gener
    
[^17]: 超越直觉，将高斯过程应用于实际数据的框架

    Beyond Intuition, a Framework for Applying GPs to Real-World Data. (arXiv:2307.03093v1 [cs.LG])

    [http://arxiv.org/abs/2307.03093](http://arxiv.org/abs/2307.03093)

    提出了一个框架，用于确定高斯过程在实际问题中的适用性，并建立一个稳健且明确的模型。通过对核函数设计和计算可扩展性选项的指导，该框架在冰川高程变化的案例研究中实现了更准确的结果。

    

    高斯过程（GPs）提供了一种用于小型、结构化和相关数据集的回归的吸引人的方法。然而，它们的应用受到计算成本的限制，并且对于如何将GPs应用于复杂的高维数据集的指导有限。我们提出了一个框架，用于确定GPs在给定问题中的适用性以及如何建立一个强大且明确的GP模型。指导方针形式化了经验丰富的GP实践者的决策，特别强调了核函数设计和计算可扩展性选项。然后，我们将该框架应用于冰川高程变化的案例研究中，在测试时产生了更准确的结果。

    Gaussian Processes (GPs) offer an attractive method for regression over small, structured and correlated datasets. However, their deployment is hindered by computational costs and limited guidelines on how to apply GPs beyond simple low-dimensional datasets. We propose a framework to identify the suitability of GPs to a given problem and how to set up a robust and well-specified GP model. The guidelines formalise the decisions of experienced GP practitioners, with an emphasis on kernel design and options for computational scalability. The framework is then applied to a case study of glacier elevation change yielding more accurate results at test time.
    
[^18]: OpenDelta: 一种用于参数高效调整预训练模型的即插即用库

    OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models. (arXiv:2307.03084v1 [cs.LG])

    [http://arxiv.org/abs/2307.03084](http://arxiv.org/abs/2307.03084)

    OpenDelta是一个开源库，提供了各种delta调整方法的即插即用实现。它能够以高效的方式调整大型预训练模型的参数，而无需修改模型的代码，具有实用性和灵活性。

    

    大型预训练模型 (PTMs) 的规模给调整下游任务带来了重大挑战，原因是全参数微调涉及高昂的优化开销和存储成本。为了解决这个问题，许多研究探索了参数高效调整方法，也称为 "delta 调整"，即仅更新一小部分参数，称为 "delta 模块"，同时保持主干模型的参数固定。然而，由于现有实现直接修改主干 PTMs 的代码，并为每个 PTM 硬编码特定的 delta 调整方法，delta 调整的实用性和灵活性受到了限制。在本文中，我们提出了 OpenDelta，这是一个开源库，通过提供各种 delta 调整方法的即插即用实现来克服这些限制。我们的新技术消除了修改主干 PTMs 代码的需求，使 OpenDelta 可以与不同的、甚至是新的 PTMs 兼容。OpenDelta 的设计简单、可扩展，并且易于使用。

    The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as "delta tuning", which updates only a small subset of parameters, known as "delta modules", while keeping the backbone model's parameters fixed. However, the practicality and flexibility of delta tuning have been limited due to existing implementations that directly modify the code of the backbone PTMs and hard-code specific delta tuning methods for each PTM. In this paper, we present OpenDelta, an open-source library that overcomes these limitations by providing a plug-and-play implementation of various delta tuning methods. Our novel techniques eliminate the need to modify the backbone PTMs' code, making OpenDelta compatible with different, even novel PTMs. OpenDelta is designed to be simple, mo
    
[^19]: 无社交假设下学习有向带符号图中的解缠表示

    Learning Disentangled Representations in Signed Directed Graphs without Social Assumptions. (arXiv:2307.03077v1 [cs.LG])

    [http://arxiv.org/abs/2307.03077](http://arxiv.org/abs/2307.03077)

    本文提出了一种新方法DINES，用于在无社交假设的有向带符号图中学习解缠节点表示。该方法采用解缠框架，可以捕捉多个潜在因素，并使用轻量级图卷积和解码器进行符号关系分类。

    

    带符号图是表示信任关系或各个领域中的偏好关系的复杂系统。在这样的图中学习节点表示对于许多挖掘任务至关重要。虽然现实世界中的符号关系可能受多个潜在因素的影响，但大多数现有方法往往过于简化符号关系的建模，依赖社交理论并将其视为简单的因素。这限制了它们的表达能力和捕捉塑造这些关系的多样因素的能力。在本文中，我们提出了DINES，一种在无社交假设的有向带符号图中学习解缠节点表示的新方法。我们采用了一种解缠框架，将每个嵌入分离成不同的因素，从而可以捕捉多个潜在因素。我们还探索了轻量级的图卷积，仅关注符号和方向，而不依赖社交理论。此外，我们提出了一个有效的解码器，能够有效地对符号关系进行分类。

    Signed graphs are complex systems that represent trust relationships or preferences in various domains. Learning node representations in such graphs is crucial for many mining tasks. Although real-world signed relationships can be influenced by multiple latent factors, most existing methods often oversimplify the modeling of signed relationships by relying on social theories and treating them as simplistic factors. This limits their expressiveness and their ability to capture the diverse factors that shape these relationships. In this paper, we propose DINES, a novel method for learning disentangled node representations in signed directed graphs without social assumptions. We adopt a disentangled framework that separates each embedding into distinct factors, allowing for capturing multiple latent factors. We also explore lightweight graph convolutions that focus solely on sign and direction, without depending on social theories. Additionally, we propose a decoder that effectively class
    
[^20]: 一种融合时空注意力神经网络和图信号平滑的EEG情绪识别模型

    A Hybrid End-to-End Spatio-Temporal Attention Neural Network with Graph-Smooth Signals for EEG Emotion Recognition. (arXiv:2307.03068v1 [cs.LG])

    [http://arxiv.org/abs/2307.03068](http://arxiv.org/abs/2307.03068)

    一种融合时空注意力神经网络和图信号平滑的EEG情绪识别模型，通过深度架构和图信号处理技术，实现了在情感计算中的出色表现。

    

    最近，生理数据如脑电图（EEG）信号在情感计算中引起了极大关注。在这个背景下，主要目标是设计一个可以评估情绪状态的自动化模型。近年来，深度神经网络在情绪识别任务中表现出了很好的性能。然而，设计一个可以从原始数据中提取实际信息的深度架构仍然是一个挑战。在这里，我们引入了一种深度神经网络，通过时空编码和循环注意力网络块的混合结构获得可解释的生理表示。此外，还使用图信号处理工具对原始数据进行预处理，以在空间域中进行图信号平滑处理。我们证明了我们提出的架构在公开可用的DEAP数据集上超过了现有技术的情绪分类结果。为了探索学习模型的普遍性，我们还评估了在其他数据集上的性能。

    Recently, physiological data such as electroencephalography (EEG) signals have attracted significant attention in affective computing. In this context, the main goal is to design an automated model that can assess emotional states. Lately, deep neural networks have shown promising performance in emotion recognition tasks. However, designing a deep architecture that can extract practical information from raw data is still a challenge. Here, we introduce a deep neural network that acquires interpretable physiological representations by a hybrid structure of spatio-temporal encoding and recurrent attention network blocks. Furthermore, a preprocessing step is applied to the raw data using graph signal processing tools to perform graph smoothing in the spatial domain. We demonstrate that our proposed architecture exceeds state-of-the-art results for emotion classification on the publicly available DEAP dataset. To explore the generality of the learned model, we also evaluate the performance
    
[^21]: DeepOnto: 一个用于深度学习本体工程的Python包

    DeepOnto: A Python Package for Ontology Engineering with Deep Learning. (arXiv:2307.03067v1 [cs.AI])

    [http://arxiv.org/abs/2307.03067](http://arxiv.org/abs/2307.03067)

    DeepOnto是一个Python包，用于深度学习本体工程。它通过集成深度学习框架和本体API，提供了丰富的工具和算法，支持本体工程任务，如本体对齐和完成。

    

    应用深度学习技术，特别是语言模型（LMs），在本体工程中已经引起了广泛关注。然而，深度学习框架如PyTorch和Tensorflow主要是为Python开发的，而广泛使用的本体API（如OWL API和Jena）主要是基于Java的。为了方便无缝集成这些框架和API，我们提出了Deeponto，一个专为本体工程设计的Python包。该包包括一个基于广泛认可和可靠的OWL API的核心本体处理模块，以更“Pythonic”的方式封装其基本特性，并扩展其功能以包括其他重要组成部分，包括推理、语言化、规范化、投影等。基于这个模块，Deeponto提供了一套工具、资源和算法，支持各种本体工程任务，例如本体对齐和完成，利用深度学习方法实现。

    Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more "Pythonic" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning meth
    
[^22]: 泛化反向传播用于基于梯度的可解释性

    Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG])

    [http://arxiv.org/abs/2307.03056](http://arxiv.org/abs/2307.03056)

    本论文在深度神经网络的特征解释中，泛化了反向传播算法，以便更好地理解梯度图的可解释统计数据，如最高加权路径和熵。作者通过在合成数据集上的评估和应用于BERT的实验中验证了该方法的有效性。

    

    许多用于解释深度神经网络的流行特征归因方法依赖于计算模型输出对输入的梯度。虽然这些方法可以指示哪些输入特征可能对模型的预测很重要，但它们对模型本身的内部工作了解甚少。在本文中，我们观察到模型的梯度计算是使用半环的更一般形式的特例。这种观察使我们能够将反向传播算法泛化，以高效地计算关于神经网络梯度图的其他可解释统计数据，例如最高加权路径和熵。我们实现了这个泛化算法，在合成数据集上进行评估以更好地理解它计算的统计数据，并将其应用于研究BERT在主谓数一致性任务（SVA）上的行为。使用这种方法，我们验证了模型组件上通过的梯度流量反映了其重要性。

    Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject-verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its impor
    
[^23]: 基于地图服务的起点-终点出行时间预测系统

    Origin-Destination Travel Time Oracle for Map-based Services. (arXiv:2307.03048v1 [cs.LG])

    [http://arxiv.org/abs/2307.03048](http://arxiv.org/abs/2307.03048)

    本论文提出了一种基于地图服务的起点-终点出行时间预测系统，通过利用历史轨迹估计OD对的时间变化，解决了多个历史轨迹与异常轨迹之间的复杂性问题。

    

    给定一个起点(O)，一个终点(D)和一个出发时间(T)，起点-终点出行时间预测系统(ODT-Oracle)会返回在T时间出发从O到D所需的时间估计。ODT-Oracle在基于地图的服务中起着重要作用。为了构建这样的预测系统，我们提供了一种利用历史轨迹估计OD对的时间变化的出行时间估计(TTE)方法。由于连接OD对的多个历史轨迹可能有不同的旅行时间，而且轨迹可能互不相同，因此解决这个问题时去除异常轨迹至关重要。我们提出了一种新颖的两阶段框架Diffusion-based Origin-destination Travel Time Estimation (DOT)来解决这个问题。首先，DOT使用Pixelated Trajectories (PiT)去噪器来建立基于扩散的轨迹估计模型来预测未来查询的出行时间。

    Given an origin (O), a destination (D), and a departure time (T), an Origin-Destination (OD) travel time oracle~(ODT-Oracle) returns an estimate of the time it takes to travel from O to D when departing at T. ODT-Oracles serve important purposes in map-based services. To enable the construction of such oracles, we provide a travel-time estimation (TTE) solution that leverages historical trajectories to estimate time-varying travel times for OD pairs.  The problem is complicated by the fact that multiple historical trajectories with different travel times may connect an OD pair, while trajectories may vary from one another. To solve the problem, it is crucial to remove outlier trajectories when doing travel time estimation for future queries.  We propose a novel, two-stage framework called Diffusion-based Origin-destination Travel Time Estimation (DOT), that solves the problem. First, DOT employs a conditioned Pixelated Trajectories (PiT) denoiser that enables building a diffusion-based
    
[^24]: 在音乐流媒体服务中使用Transformer生成歌单混合

    Track Mix Generation on Music Streaming Services using Transformers. (arXiv:2307.03045v1 [cs.IR])

    [http://arxiv.org/abs/2307.03045](http://arxiv.org/abs/2307.03045)

    本文介绍了2022年在Deezer音乐流媒体服务上推出的Track Mix个性化歌单生成系统，通过使用Transformer模型分析用户播放列表的曲目序列来生成以初始音乐曲目为灵感的“混合”播放列表，提升用户在Deezer上的音乐发现体验。

    

    本文介绍了Track Mix，这是一个于2022年在音乐流媒体服务Deezer上推出的个性化歌单生成系统。Track Mix通过自动为用户生成以初始音乐曲目为灵感的“混合”播放列表，让用户可以发现与他们喜爱的内容相似的音乐。为了生成这些混合歌单，我们考虑了使用Transformer模型在用户播放列表的数百万个曲目序列上进行训练。鉴于近年来Transformers的日益流行，我们分析了与传统合作过滤方法相比，在服务中使用这种模型进行混合生成所带来的优势、不足和技术挑战。自推出以来，Track Mix每天为数百万用户生成歌单，在Deezer上提升了他们的音乐发现体验。

    This paper introduces Track Mix, a personalized playlist generation system released in 2022 on the music streaming service Deezer. Track Mix automatically generates "mix" playlists inspired by initial music tracks, allowing users to discover music similar to their favorite content. To generate these mixes, we consider a Transformer model trained on millions of track sequences from user playlists. In light of the growing popularity of Transformers in recent years, we analyze the advantages, drawbacks, and technical challenges of using such a model for mix generation on the service, compared to a more traditional collaborative filtering approach. Since its release, Track Mix has been generating playlists for millions of users daily, enhancing their music discovery experience on Deezer.
    
[^25]: 一个用于Chamfer距离的近线性时间算法

    A Near-Linear Time Algorithm for the Chamfer Distance. (arXiv:2307.03043v1 [cs.DS])

    [http://arxiv.org/abs/2307.03043](http://arxiv.org/abs/2307.03043)

    本文提出了一个适用于Chamfer距离的近线性时间算法，可以估计Chamfer距离的$(1+\epsilon)$-近似值，解决了处理大数据集时的运行时间问题。

    

    对于任意两个大小不超过n的点集A，B，从A到B的Chamfer距离定义为CH(A,B)=∑(a∈A)min(b∈B)dX(a,b)，其中dX是底层距离度量（例如欧几里得距离或曼哈顿距离）。Chamfer距离是衡量点云之间差异的一种常用方法，被广泛应用于机器学习、计算机视觉和图形学等领域，并且有一个直观的O(dn^2)时间复杂度的暴力算法。然而，运行时间对n的二次依赖使得直接算法在处理大数据集时难以承受。我们克服了这个瓶颈，并提出了第一个具有近线性运行时间的$(1+\epsilon)$-近似算法来估计Chamfer距离。具体而言，我们的算法的运行时间为O(ndlog(n)/ε^2)，可以实现。

    For any two point sets $A,B \subset \mathbb{R}^d$ of size up to $n$, the Chamfer distance from $A$ to $B$ is defined as $\text{CH}(A,B)=\sum_{a \in A} \min_{b \in B} d_X(a,b)$, where $d_X$ is the underlying distance measure (e.g., the Euclidean or Manhattan distance). The Chamfer distance is a popular measure of dissimilarity between point clouds, used in many machine learning, computer vision, and graphics applications, and admits a straightforward $O(d n^2)$-time brute force algorithm. Further, the Chamfer distance is often used as a proxy for the more computationally demanding Earth-Mover (Optimal Transport) Distance. However, the \emph{quadratic} dependence on $n$ in the running time makes the naive approach intractable for large datasets.  We overcome this bottleneck and present the first $(1+\epsilon)$-approximate algorithm for estimating the Chamfer distance with a near-linear running time. Specifically, our algorithm runs in time $O(nd \log (n)/\varepsilon^2)$ and is implementa
    
[^26]: LLaMA在临床领域的参数高效微调

    Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])

    [http://arxiv.org/abs/2307.03042](http://arxiv.org/abs/2307.03042)

    本研究提出了一种参数高效微调（PEFT）方法，在临床领域使用临床记录训练了一个专门适配临床领域的LLaMA-LoRA模型，同时提出了一个两步PEFT框架，用于将其与Downstream LLaMA-LoRA适配器进行融合，以实现领域适应。

    

    传统上，将预训练的语言模型适应到新领域，如临床应用，需要重新训练所有参数。然而，由于训练这些大型语言模型所需的计算资源巨大，这种方法的实践性越来越被证明是不切实际的。为了解决这个问题，参数高效微调（PEFT）技术提供了一种可行的解决方案，通过选择性地微调一个小的附加参数集，显著减少了领域适应所需的计算资源。在本研究中，我们提出了临床LLaMA-LoRA，这是一个构建在开源LLaMA模型上的PEFT适配器层。临床LLaMA-LoRA使用从MIMIC-IV数据库中获取的临床记录进行训练，从而创建了一个专为临床领域设计的专用适配器。此外，我们提出了一个两步PEFT框架，将临床LLaMA-LoRA与Downstream LLaMA-LoRA进行融合，后者是另一个专为下游任务设计的PEFT适配器。

    Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. W
    
[^27]: 带有一般观测模型的不安定赌博机问题的PCL-可索引性和Whittle索引

    PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models. (arXiv:2307.03034v1 [stat.ML])

    [http://arxiv.org/abs/2307.03034](http://arxiv.org/abs/2307.03034)

    本文研究了一种一般观测模型下的不安定多臂赌博机问题，提出了PCL-可索引性和Whittle索引的分析方法，并通过近似过程将问题转化为有限状态问题。数值实验表明算法表现优秀。

    

    本文考虑了一种一般观测模型，用于不安定多臂赌博机问题。由于资源约束或环境或固有噪声，玩家操作需要基于某种有误差的反馈机制。通过建立反馈/观测动力学的一般概率模型，我们将问题表述为一个从任意初始信念（先验信息）开始的具有可数信念状态空间的不安定赌博机问题。我们利用具有部分守恒定律（PCL）的可实现区域方法，分析了无限状态问题的可索引性和优先级索引（Whittle索引）。最后，我们提出了一个近似过程，将问题转化为可以应用Niño-Mora和Bertsimas针对有限状态问题的AG算法的问题。数值实验表明，我们的算法具有出色的性能。

    In this paper, we consider a general observation model for restless multi-armed bandit problems. The operation of the player needs to be based on certain feedback mechanism that is error-prone due to resource constraints or environmental or intrinsic noises. By establishing a general probabilistic model for dynamics of feedback/observation, we formulate the problem as a restless bandit with a countable belief state space starting from an arbitrary initial belief (a priori information). We apply the achievable region method with partial conservation law (PCL) to the infinite-state problem and analyze its indexability and priority index (Whittle index). Finally, we propose an approximation process to transform the problem into which the AG algorithm of Ni\~no-Mora and Bertsimas for finite-state problems can be applied to. Numerical experiments show that our algorithm has an excellent performance.
    
[^28]: 通过数据重要性学习改善检索增强的大型语言模型

    Improving Retrieval-Augmented Large Language Models via Data Importance Learning. (arXiv:2307.03027v1 [cs.LG])

    [http://arxiv.org/abs/2307.03027](http://arxiv.org/abs/2307.03027)

    本文通过多线性扩展算法评估检索增强模型中检索到的数据点的数据重要性，并提出了一个多项式时间算法来计算其数据重要性。实验结果表明，修剪或增强大型语言模型可以提高性能。

    

    检索增强使得大型语言模型能够利用外部知识，例如在问题回答和数据补全等任务中。然而，这种检索增强模型的性能受到其基础检索语料的数据质量的限制。本文提出了一种基于多线性扩展的算法，用于评估检索到的数据点的数据重要性。多线性扩展中存在指数级的项，本文的一个关键贡献是提出了一个多项式时间算法，能够精确计算具有加法效用函数和验证集的检索增强模型中的数据点在检索语料中的数据重要性。我们还提出了一种更高效的（ε，δ）-近似算法。实验结果表明，我们可以通过仅修剪或增强大型语言模型来提高其性能。

    Retrieval augmentation enables large language models to take advantage of external knowledge, for example on tasks like question answering and data imputation. However, the performance of such retrieval-augmented models is limited by the data quality of their underlying retrieval corpus. In this paper, we propose an algorithm based on multilinear extension for evaluating the data importance of retrieved data points. There are exponentially many terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the data importance of data points in the retrieval corpus using the multilinear extension of the model's utility function. We further proposed an even more efficient ({\epsilon}, {\delta})-approximation algorithm. Our experimental results illustrate that we can enhance the performance of large language models by only pruning or r
    
[^29]: 提高人机协同系统的效率: 在人类专家中添加人工智能

    Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial to Human Experts. (arXiv:2307.03003v1 [cs.LG])

    [http://arxiv.org/abs/2307.03003](http://arxiv.org/abs/2307.03003)

    本研究提出了一个混合系统，在人类专家中添加人工智能，从之前由人类专家审查过的未知类别的数据实例中学习分类，以提高人机协同系统的效率。

    

    信息系统越来越多地利用人工智能（AI）和机器学习（ML）从大量数据中生成价值。然而，ML模型并不完美，可能会产生错误的分类。因此，人机协同（HITL）扩展了ML模型，为难以分类的实例添加了人工审核。本研究认为，持续依赖人类专家处理困难的模型分类会导致人力投入的大幅增加，增加了有限资源的压力。为解决这个问题，我们提出了一个混合系统，创建了人工专家，从之前由人类专家审查过的未知类别的数据实例中学习分类。我们的混合系统评估哪个人工专家适合分类来自未知类别的实例，并自动分配。随着时间的推移，这减少了人力投入，提高了系统的效率。我们的实验表明，我们的方法优于传统的HITL系统。

    Information systems increasingly leverage artificial intelligence (AI) and machine learning (ML) to generate value from vast amounts of data. However, ML models are imperfect and can generate incorrect classifications. Hence, human-in-the-loop (HITL) extensions to ML models add a human review for instances that are difficult to classify. This study argues that continuously relying on human experts to handle difficult model classifications leads to a strong increase in human effort, which strains limited resources. To address this issue, we propose a hybrid system that creates artificial experts that learn to classify data instances from unknown classes previously reviewed by human experts. Our hybrid system assesses which artificial expert is suitable for classifying an instance from an unknown class and automatically assigns it. Over time, this reduces human effort and increases the efficiency of the system. Our experiments demonstrate that our approach outperforms traditional HITL sy
    
[^30]: ContainerGym: 一种针对资源分配的现实强化学习基准

    ContainerGym: A Real-World Reinforcement Learning Benchmark for Resource Allocation. (arXiv:2307.02991v1 [cs.LG])

    [http://arxiv.org/abs/2307.02991](http://arxiv.org/abs/2307.02991)

    ContainerGym是一个受到真实世界工业资源分配任务启发的强化学习基准，将一系列常见的真实世界挑战编码进去，从而在任何真实世界问题上评估强化学习算法。

    

    我们提出了ContainerGym，一种受到真实世界工业资源分配任务启发的强化学习基准。所提出的基准编码了一系列常见的真实世界顺序决策问题中遇到的挑战，如不确定性。它可以配置成不同难度的问题，例如变量维度。与其他针对编码真实世界困难的强化学习基准不同的是，我们的基准直接来源于一个经过最小化简和精简的真实世界工业问题。它足够灵活，可以在适用于我们资源分配框架的任何真实世界问题上评估强化学习算法。我们提供了标准基准方法的结果。除了通常的训练奖励曲线，我们的结果和用于解释它们的统计工具允许突出显示我们的所提算法的一些有趣局限性。

    We present ContainerGym, a benchmark for reinforcement learning inspired by a real-world industrial resource allocation task. The proposed benchmark encodes a range of challenges commonly encountered in real-world sequential decision making problems, such as uncertainty. It can be configured to instantiate problems of varying degrees of difficulty, e.g., in terms of variable dimensionality. Our benchmark differs from other reinforcement learning benchmarks, including the ones aiming to encode real-world difficulties, in that it is directly derived from a real-world industrial problem, which underwent minimal simplification and streamlining. It is sufficiently versatile to evaluate reinforcement learning algorithms on any real-world problem that fits our resource allocation framework. We provide results of standard baseline methods. Going beyond the usual training reward curves, our results and the statistical tools used to interpret them allow to highlight interesting limitations of we
    
[^31]: 生成模型潜在空间中的隐私保护行走在医学应用中

    A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications. (arXiv:2307.02984v1 [cs.LG])

    [http://arxiv.org/abs/2307.02984](http://arxiv.org/abs/2307.02984)

    这项工作提出了一种潜在空间导航策略，通过使用辅助身份分类器作为导向，在潜在空间中生成多样化的合成样本，以支持深度模型的训练，并解决了由于使用生成对抗网络而导致的隐私问题。

    

    生成对抗网络（GANs）展示了它们生成与目标分布匹配的合成样本的能力。然而，从隐私角度来看，使用GAN作为数据共享的代理不是一个安全的解决方案，因为它们往往在潜在空间中嵌入接近真实样本的副本。最近的研究受k-匿名原则的启发，通过在潜在空间中对样本进行聚合来解决这个问题，但会减少数据集的大小。我们的工作旨在通过提出一种潜在空间导航策略来减轻这个问题，该策略能够生成多样化的合成样本，以支持深度模型的有效训练，并以原则性的方式解决隐私问题。我们的方法利用辅助身份分类器作为导向，在潜在空间中非线性地在点之间移动，最小化与接近真实样本的副本发生冲突的风险。我们通过实验证明，对于潜在空间中的任意随机点对，我们的方法能够生成多样化的合成样本，达到了同时解决隐私问题和有效训练的目的。

    Generative Adversarial Networks (GANs) have demonstrated their ability to generate synthetic samples that match a target distribution. However, from a privacy perspective, using GANs as a proxy for data sharing is not a safe solution, as they tend to embed near-duplicates of real samples in the latent space. Recent works, inspired by k-anonymity principles, address this issue through sample aggregation in the latent space, with the drawback of reducing the dataset by a factor of k. Our work aims to mitigate this problem by proposing a latent space navigation strategy able to generate diverse synthetic samples that may support effective training of deep models, while addressing privacy concerns in a principled way. Our approach leverages an auxiliary identity classifier as a guide to non-linearly walk between points in the latent space, minimizing the risk of collision with near-duplicates of real samples. We empirically demonstrate that, given any random pair of points in the latent sp
    
[^32]: 使用手机音频数据进行COVID-19的高效检测的迁移学习

    Transfer Learning for the Efficient Detection of COVID-19 from Smartphone Audio Data. (arXiv:2307.02975v1 [cs.LG])

    [http://arxiv.org/abs/2307.02975](http://arxiv.org/abs/2307.02975)

    本文介绍了三种不同的深度学习模型和迁移学习方法，以提高从智能手机音频数据中进行COVID-19的高效检测。通过在多个数据集上进行实验评估，结果显示这些方法可以有效应对移动健康系统中的开放性研究挑战。

    

    从智能手机数据中检测疾病是移动健康（m-health）系统中的一个开放性研究挑战。COVID-19及其呼吸症状是该领域的一个重要案例研究，在这个领域的早期检测是对抗大流行病的一个潜在有效手段。该解决方案的有效性主要取决于应用于收集数据的AI算法的性能以及其可能直接在用户手机上实施。考虑到这些问题和有限的可用数据量，本文通过与手工制作的特征进行比较，展示了三种不同的深度学习模型（VGGish，YAMNET和L3-Net）的实验评估以及在所考虑的场景中的两种主要迁移学习方法：特征提取和微调。具体而言，我们通过在4个不同数据集（总共13,447个样本）上进行独立用户实验来评估12个不同配置的模型。

    Disease detection from smartphone data represents an open research challenge in mobile health (m-health) systems. COVID-19 and its respiratory symptoms are an important case study in this area and their early detection is a potential real instrument to counteract the pandemic situation. The efficacy of this solution mainly depends on the performances of AI algorithms applied to the collected data and their possible implementation directly on the users' mobile devices. Considering these issues, and the limited amount of available data, in this paper we present the experimental evaluation of 3 different deep learning models, compared also with hand-crafted features, and of two main approaches of transfer learning in the considered scenario: both feature extraction and fine-tuning. Specifically, we considered VGGish, YAMNET, and L\textsuperscript{3}-Net (including 12 different configurations) evaluated through user-independent experiments on 4 different datasets (13,447 samples in total).
    
[^33]: 修剪与量化：哪个更好？

    Pruning vs Quantization: Which is Better?. (arXiv:2307.02973v1 [cs.LG])

    [http://arxiv.org/abs/2307.02973](http://arxiv.org/abs/2307.02973)

    本文比较了神经网络量化和修剪这两种压缩深度神经网络的技术，结果表明在大多数情况下，量化优于修剪。

    

    神经网络修剪和量化技术几乎和神经网络本身一样古老。然而，迄今为止只有两者之间的临时比较发表过。本文旨在回答哪个更好：神经网络量化还是修剪？通过回答这个问题，我们希望为神经网络硬件的设计决策提供信息。我们对压缩深度神经网络的这两种技术进行了全面比较。首先，我们对一般数据分布的期望量化和修剪误差进行了分析比较。然后，我们提供了在训练好的网络中每层修剪和量化误差的下界，并将其与优化后的经验误差进行了比较。最后，我们对3个任务上的8个大规模模型进行了广泛的实验比较。我们的结果表明，在大多数情况下，量化优于修剪。只有在一些极高压缩比的情况下，修剪表现出更好的效果。

    Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question on which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower bounds for the per-layer pruning and quantization error in trained networks, and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models on 3 tasks. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with very high compression ratio, p
    
[^34]: DPM: 通过分离聚类敏感数据

    DPM: Clustering Sensitive Data through Separation. (arXiv:2307.02969v1 [cs.CR])

    [http://arxiv.org/abs/2307.02969](http://arxiv.org/abs/2307.02969)

    本文提出了差分隐私聚类算法DPM，通过搜索准确的数据点分离器来进行隐私保护的聚类。关键贡献是识别大间隔分离器并合理分配隐私预算。

    

    隐私保护聚类以无监督方式对数据点进行分组，同时确保敏感信息得以保护。先前的隐私保护聚类关注点在于识别点云的聚集。本文则采取另一种方法，关注于识别适当的分离器以分离数据集。我们引入了新颖的差分隐私聚类算法DPM，以差分隐私的方式搜索准确的数据点分离器。DPM解决了寻找准确分离器的两个关键挑战：识别聚类间的大间隔分离器而不是聚类内的小间隔分离器，以及在开销隐私预算时，优先考虑将数据划分为较大子部分的分离器。利用差分隐私指数机制，DPM通过随机选择具有高效用性的聚类分离器：对于数据集D，如果中心的60%分位数中存在宽的低密度分离器，DPM会发现它。

    Privacy-preserving clustering groups data points in an unsupervised manner whilst ensuring that sensitive information remains protected. Previous privacy-preserving clustering focused on identifying concentration of point clouds. In this paper, we take another path and focus on identifying appropriate separators that split a data set. We introduce the novel differentially private clustering algorithm DPM that searches for accurate data point separators in a differentially private manner. DPM addresses two key challenges for finding accurate separators: identifying separators that are large gaps between clusters instead of small gaps within a cluster and, to efficiently spend the privacy budget, prioritising separators that split the data into large subparts. Using the differentially private Exponential Mechanism, DPM randomly chooses cluster separators with provably high utility: For a data set $D$, if there is a wide low-density separator in the central $60\%$ quantile, DPM finds that
    
[^35]: SegNetr：重新思考U型网络中的局部-全局交互和跳跃连接

    SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks. (arXiv:2307.02953v1 [eess.IV])

    [http://arxiv.org/abs/2307.02953](http://arxiv.org/abs/2307.02953)

    这个论文提出了一种轻量级的医学图像分割网络SegNetr，通过引入SegNetr块和信息保留跳跃连接实现了动态的局部-全局交互和编码器-解码器特征的精确融合。

    

    最近，由于其简单且易于调整的结构，U型网络在医学图像分割领域占据主导地位。然而，现有的U型分割网络存在以下问题：1）主要关注设计复杂的自注意力模块，以弥补基于卷积操作的长期依赖性的缺失，从而增加了网络的总参数和计算复杂性；2）简单地融合编码器和解码器的特征，忽略了它们空间位置之间的连接。本文中，我们重新思考了上述问题，并构建了一种轻量级的医学图像分割网络，称为SegNetr。具体地，我们引入了一种新颖的SegNetr块，可以在任何阶段动态地进行局部-全局交互，并且具有线性复杂度。同时，我们设计了一个通用的信息保留跳跃连接（IRSC），以保留编码器特征的空间位置信息，并与解码器特征实现精确融合。

    Recently, U-shaped networks have dominated the field of medical image segmentation due to their simple and easily tuned structure. However, existing U-shaped segmentation networks: 1) mostly focus on designing complex self-attention modules to compensate for the lack of long-term dependence based on convolution operation, which increases the overall number of parameters and computational complexity of the network; 2) simply fuse the features of encoder and decoder, ignoring the connection between their spatial locations. In this paper, we rethink the above problem and build a lightweight medical image segmentation network, called SegNetr. Specifically, we introduce a novel SegNetr block that can perform local-global interactions dynamically at any stage and with only linear complexity. At the same time, we design a general information retention skip connection (IRSC) to preserve the spatial location information of encoder features and achieve accurate fusion with the decoder features. 
    
[^36]: 当拒绝学习对具有拒绝的回归问题最优时

    When No-Rejection Learning is Optimal for Regression with Rejection. (arXiv:2307.02932v1 [cs.LG])

    [http://arxiv.org/abs/2307.02932](http://arxiv.org/abs/2307.02932)

    本文研究了具有拒绝的回归问题，并调查了将其视为标准回归任务来学习预测器的无拒绝学习策略。

    

    拒绝学习是研究人类和人工智能在预测任务上相互作用的典型模型。该模型包括一个预测器和一个拒绝器。在样本到达时，拒绝器首先决定是否接受它；如果接受，预测器完成预测任务；如果被拒绝，则将预测推迟给人类。学习问题需要同时学习预测器和拒绝器。这改变了传统损失函数的结构，通常导致非凸性和一致性问题。对于带有拒绝的分类问题，一些研究开发了代理损失函数，同时具有可验证的一致性保证；与此同时，关于回归问题的研究较少。我们研究了带有拒绝的回归问题并研究了将其视为标准回归任务来学习预测器的无拒绝学习策略。

    Learning with rejection is a prototypical model for studying the interaction between humans and AI on prediction tasks. The model has two components, a predictor and a rejector. Upon the arrival of a sample, the rejector first decides whether to accept it; if accepted, the predictor fulfills the prediction task, and if rejected, the prediction will be deferred to humans. The learning problem requires learning a predictor and a rejector simultaneously. This changes the structure of the conventional loss function and often results in non-convexity and inconsistency issues. For the classification with rejection problem, several works develop surrogate losses for the jointly learning with provable consistency guarantees; in parallel, there has been less work for the regression counterpart. We study the regression with rejection (RwR) problem and investigate the no-rejection learning strategy which treats the RwR problem as a standard regression task to learn the predictor. We establish tha
    
[^37]: 一个用于传感器基础的人体活动识别中传感器优化布局的实时人体姿势估计方法

    A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition. (arXiv:2307.02906v1 [cs.LG])

    [http://arxiv.org/abs/2307.02906](http://arxiv.org/abs/2307.02906)

    本文提出了一种实时人体姿势估计方法，通过使用视频录制的2D姿势估计来确定最佳传感器布局，在传感器基础的人体活动识别中取得了重要的突破。

    

    传感器基础的人体活动识别可以方便地监测人体动作，但确定最佳传感器布局以实现最优的分类性能仍然具有挑战性。本文提出了一种新的方法来解决这个问题，使用从目标活动的视频录制中得到的实时2D姿势估计。通过得到的骨骼数据，可以确定最佳传感器位置的独特策略。我们通过一个可行性研究验证了我们的方法，在十个被试者上应用惯性传感器监测了13种不同的活动。我们的研究结果表明，基于视觉的传感器布局方法与传统的深度学习方法具有可比较的结果，证明了其有效性。这项研究通过提供一种轻量级、设备上的解决方案来确定最佳传感器布局，从而提高了数据匿名化的能力，并支持多用户环境。

    Sensor-based Human Activity Recognition facilitates unobtrusive monitoring of human movements. However, determining the most effective sensor placement for optimal classification performance remains challenging. This paper introduces a novel methodology to resolve this issue, using real-time 2D pose estimations derived from video recordings of target activities. The derived skeleton data provides a unique strategy for identifying the optimal sensor location. We validate our approach through a feasibility study, applying inertial sensors to monitor 13 different activities across ten subjects. Our findings indicate that the vision-based method for sensor placement offers comparable results to the conventional deep learning approach, demonstrating its efficacy. This research significantly advances the field of Human Activity Recognition by providing a lightweight, on-device solution for determining the optimal sensor placement, thereby enhancing data anonymization and supporting a multimo
    
[^38]: PUFFIN: 用于蒸汽压力预测的路径统一前向接口网络

    PUFFIN: A Path-Unifying Feed-Forward Interfaced Network for Vapor Pressure Prediction. (arXiv:2307.02903v1 [physics.chem-ph])

    [http://arxiv.org/abs/2307.02903](http://arxiv.org/abs/2307.02903)

    PUFFIN是一种结合迁移学习和基于领域知识的归纳偏差节点的机器学习框架，用于改进蒸汽压力预测。通过利用归纳偏差和图嵌入的迁移学习，PUFFIN在预测中胜过不使用归纳偏差或使用通用描述符的替代策略。

    

    准确预测蒸汽压力对于工业和环境应用至关重要。然而，由于实验的资源和劳动强度，无法获得所有有兴趣的化合物的准确测量。当希望预测蒸汽压力的温度相关关系时，资源和劳动的需求进一步增加。在本文中，我们提出了PUFFIN（路径统一前向接口网络），这是一个将迁移学习与启发于领域知识（安托万方程）的新归纳偏差节点结合起来的机器学习框架，以改善蒸汽压力预测。通过利用归纳偏差和使用图嵌入的迁移学习，PUFFIN优于不使用归纳偏差或使用通用描述符的替代策略。该框架将领域特定知识的融入克服了数据可用性不足的限制，展示出了其在更广泛应用中的潜力。

    Accurately predicting vapor pressure is vital for various industrial and environmental applications. However, obtaining accurate measurements for all compounds of interest is not possible due to the resource and labor intensity of experiments. The demand for resources and labor further multiplies when a temperature-dependent relationship for predicting vapor pressure is desired. In this paper, we propose PUFFIN (Path-Unifying Feed-Forward Interfaced Network), a machine learning framework that combines transfer learning with a new inductive bias node inspired by domain knowledge (the Antoine equation) to improve vapor pressure prediction. By leveraging inductive bias and transfer learning using graph embeddings, PUFFIN outperforms alternative strategies that do not use inductive bias or that use generic descriptors of compounds. The framework's incorporation of domain-specific knowledge to overcome the limitation of poor data availability shows its potential for broader applications in 
    
[^39]: 自由位：在边缘上优化混合精度量化的神经网络延迟

    Free Bits: Latency Optimization of Mixed-Precision Quantized Neural Networks on the Edge. (arXiv:2307.02894v1 [cs.LG])

    [http://arxiv.org/abs/2307.02894](http://arxiv.org/abs/2307.02894)

    本论文提出了一种混合搜索方法，通过硬件不可知的可微分搜索算法和硬件感知的启发式优化，可以优化混合精度配置对特定硬件目标的延迟。在MobileNetV1和MobileNetV2上的实验结果表明，在1000类ImageNet数据集上相比于8位模型，在保证几乎没有准确性下降的情况下，能够实现高达28.6％的端到端延迟降低

    

    混合精度量化，即将深度神经网络的层量化为不同的精度，为优化模型大小、延迟和统计准确性之间的权衡提供了机会，超越了同质位宽量化所能实现的。为了在给定网络的混合精度配置的难以处理的搜索空间中进行导航，本文提出了一种混合搜索方法。它由一种硬件不可知的可微分搜索算法和一种硬件感知的启发式优化组成，以找到针对特定硬件目标优化延迟的混合精度配置。我们在MobileNetV1和MobileNetV2上评估了我们的算法，并将结果网络部署在不同硬件特性的多核RISC-V微控制器平台系列上。与8位模型相比，在1000类ImageNet数据集上，与全精度基准相比，我们实现了高达28.6％的端到端延迟降低，准确性几乎没有下降

    Mixed-precision quantization, where a deep neural network's layers are quantized to different precisions, offers the opportunity to optimize the trade-offs between model size, latency, and statistical accuracy beyond what can be achieved with homogeneous-bit-width quantization. To navigate the intractable search space of mixed-precision configurations for a given network, this paper proposes a hybrid search methodology. It consists of a hardware-agnostic differentiable search algorithm followed by a hardware-aware heuristic optimization to find mixed-precision configurations latency-optimized for a specific hardware target. We evaluate our algorithm on MobileNetV1 and MobileNetV2 and deploy the resulting networks on a family of multi-core RISC-V microcontroller platforms with different hardware characteristics. We achieve up to 28.6% reduction of end-to-end latency compared to an 8-bit model at a negligible accuracy drop from a full-precision baseline on the 1000-class ImageNet dataset
    
[^40]: BaBE:通过估计潜在解释变量增强公平性

    BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables. (arXiv:2307.02891v1 [cs.LG])

    [http://arxiv.org/abs/2307.02891](http://arxiv.org/abs/2307.02891)

    本文提出了一种名为BaBE的方法，通过估计潜在解释变量来提高公平性。该方法通过结合贝叶斯推断和期望最大化方法，估计给定Z的每个群体E的最可能值。

    

    本文考虑了两个群体之间不公平歧视的问题，并提出了一种预处理方法来实现公平性。我们提出了一种基于贝叶斯推断和期望最大化方法的BaBE (Bayesian Bias Elimination)方法，用于估计给定Z的每个群体的E的最可能值。

    We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data, i.e., it is a latent variable. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each grou
    
[^41]: 学习探索先前行为来解决任务

    Learning to Solve Tasks with Exploring Prior Behaviours. (arXiv:2307.02889v1 [cs.RO])

    [http://arxiv.org/abs/2307.02889](http://arxiv.org/abs/2307.02889)

    本文提出了一种基于示例的控制方法（IRDEC），通过内在奖励驱动以及探索获得先前行为，并与示范中的任务特定行为连接，从而解决具有稀疏奖励的任务。这种方法在三个导航任务上的性能优于其他基线方法。

    

    在深度强化学习中，示范常被广泛用于解决具有稀疏奖励的任务。然而，现实世界场景中的任务往往具有与示范不同的初始条件，这就需要额外的先前行为。例如，假设我们得到了“从打开抽屉中拿取物体”的任务的示范，但在训练时抽屉是关闭的。如果没有掌握打开抽屉的先前行为，机器人很难解决这个任务。为了解决这个问题，我们提出了一种内在奖励驱动的基于示例的控制方法（IRDEC）。我们的方法可以赋予智能体探索和获取所需的先前行为的能力，并与示范中的任务特定行为连接，从而解决稀疏奖励任务，而无需额外展示先前行为示范。我们的方法在三个导航任务上的性能优于其他基线方法。

    Demonstrations are widely used in Deep Reinforcement Learning (DRL) for facilitating solving tasks with sparse rewards. However, the tasks in real-world scenarios can often have varied initial conditions from the demonstration, which would require additional prior behaviours. For example, consider we are given the demonstration for the task of \emph{picking up an object from an open drawer}, but the drawer is closed in the training. Without acquiring the prior behaviours of opening the drawer, the robot is unlikely to solve the task. To address this, in this paper we propose an Intrinsic Rewards Driven Example-based Control \textbf{(IRDEC)}. Our method can endow agents with the ability to explore and acquire the required prior behaviours and then connect to the task-specific behaviours in the demonstration to solve sparse-reward tasks without requiring additional demonstration of the prior behaviours. The performance of our method outperforms other baselines on three navigation tasks a
    
[^42]: 使用事后多观察数据的POMDP样本高效学习

    Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight. (arXiv:2307.02884v1 [cs.LG])

    [http://arxiv.org/abs/2307.02884](http://arxiv.org/abs/2307.02884)

    本文研究了在部分可观察的马尔可夫决策过程（POMDPs）中学习的样本高效性，提出了一个增强的反馈模型，利用事后多观察数据实现了对两种新的POMDP子类的样本高效学习。

    

    本文研究了在部分可观察的马尔可夫决策过程（POMDPs）中学习的样本高效性，这是强化学习中一个在最坏情况下被证明是指数级困难的问题。受到现实世界中游戏中的加载等情景的启发，我们提出了一个增强的反馈模型，称为“事后多观察数据”，其中在与POMDP进行交互的每个周期之后，学习者可以收集到从遇到的潜在状态发出的多个附加观测数据，但不能直接观测到潜在状态本身。我们证明了在这个反馈模型下，对于两种新的POMDP子类（多观测展示POMDP和可区分POMDP），可以实现样本高效的学习。这两个子类相对于广泛研究的展示POMDP子类来说更加普遍和放松，而在标准轨迹反馈下可以实现样本高效学习。值得注意的是，可区分POMDP只需使用最少的观测数据和反馈进行学习。

    This paper studies the sample-efficiency of learning in Partially Observable Markov Decision Processes (POMDPs), a challenging problem in reinforcement learning that is known to be exponentially hard in the worst-case. Motivated by real-world settings such as loading in game playing, we propose an enhanced feedback model called ``multiple observations in hindsight'', where after each episode of interaction with the POMDP, the learner may collect multiple additional observations emitted from the encountered latent states, but may not observe the latent states themselves. We show that sample-efficient learning under this feedback model is possible for two new subclasses of POMDPs: \emph{multi-observation revealing POMDPs} and \emph{distinguishable POMDPs}. Both subclasses generalize and substantially relax \emph{revealing POMDPs} -- a widely studied subclass for which sample-efficient learning is possible under standard trajectory feedback. Notably, distinguishable POMDPs only require th
    
[^43]: 在铁路领域中持续开发和安全保证基于机器学习系统的安全MLOps流程

    Towards a safe MLOps Process for the Continuous Development and Safety Assurance of ML-based Systems in the Railway Domain. (arXiv:2307.02867v1 [cs.SE])

    [http://arxiv.org/abs/2307.02867](http://arxiv.org/abs/2307.02867)

    本文提出了一个安全的MLOps流程，用于在铁路领域中持续开发和安全保证基于机器学习系统的系统。该流程整合了系统工程、安全保证和机器学习生命周期，解决了再现性、可追溯性、协作性和持续适应性的问题。

    

    传统的自动化技术单独并不足以实现非受限基础设施上的无人驾驶列车运行（称为GoA 4）。现今，所需的感知任务通过机器学习（ML）实现，因此需要可靠高效地开发和部署。达到这个目标的一个重要方面是使用一个安全的MLOps流程，用于解决改进再现性、可追溯性、协作性和无人驾驶运营对不断变化的条件的持续适应性。MLOps结合了机器学习应用开发和操作（Ops），基于运营反馈实现高频软件发布和持续创新。本文概述了在铁路领域中持续开发和安全保证基于机器学习系统的安全MLOps流程。它将系统工程、安全保证和机器学习生命周期融入一个全面的工作流程中。我们介绍了流程的各个阶段和它们之间的相互关系。

    Traditional automation technologies alone are not sufficient to enable driverless operation of trains (called Grade of Automation (GoA) 4) on non-restricted infrastructure. The required perception tasks are nowadays realized using Machine Learning (ML) and thus need to be developed and deployed reliably and efficiently. One important aspect to achieve this is to use an MLOps process for tackling improved reproducibility, traceability, collaboration, and continuous adaptation of a driverless operation to changing conditions. MLOps mixes ML application development and operation (Ops) and enables high frequency software releases and continuous innovation based on the feedback from operations. In this paper, we outline a safe MLOps process for the continuous development and safety assurance of ML-based systems in the railway domain. It integrates system engineering, safety assurance, and the ML life-cycle in a comprehensive workflow. We present the individual stages of the process and thei
    
[^44]: PLIERS: 基于流行度的在线社交网络内容传播推荐系统

    PLIERS: a Popularity-Based Recommender System for Content Dissemination in Online Social Networks. (arXiv:2307.02865v1 [cs.IR])

    [http://arxiv.org/abs/2307.02865](http://arxiv.org/abs/2307.02865)

    PLIERS是一种基于流行度的在线社交网络内容传播推荐系统，通过在算法复杂性和推荐物品的个性化水平之间取得平衡，提供了更好的个性化、相关性和推荐的新颖性。

    

    本文提出了一种新颖的基于标签的推荐系统PLIERS，该系统假设用户主要对与他们已拥有的物品和标签具有相似流行度的物品和标签感兴趣。PLIERS旨在在算法复杂性和推荐物品的个性化水平之间取得良好的平衡。通过在真实的在线社交网络数据集上进行一系列实验，我们验证了PLIERS在个性化、相关性和推荐的新颖性方面优于现有解决方案。

    In this paper, we propose a novel tag-based recommender system called PLIERS, which relies on the assumption that users are mainly interested in items and tags with similar popularity to those they already own. PLIERS is aimed at reaching a good tradeoff between algorithmic complexity and the level of personalization of recommended items. To evaluate PLIERS, we performed a set of experiments on real OSN datasets, demonstrating that it outperforms state-of-the-art solutions in terms of personalization, relevance, and novelty of recommendations.
    
[^45]: 可证明高效的迭代CVaR强化学习与函数逼近

    Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation. (arXiv:2307.02842v1 [cs.LG])

    [http://arxiv.org/abs/2307.02842](http://arxiv.org/abs/2307.02842)

    这篇论文研究了一种新颖的风险敏感强化学习方法，通过迭代条件风险价值目标以及线性和一般函数逼近方法，实现了安全性保证，并提出了高效的算法。通过对于不同逼近方法的实验结果，验证了算法的有效性和优越性。

    

    风险敏感的强化学习旨在优化平衡期望奖励和风险的策略。本文研究了一种新颖的风险敏感强化学习形式，采用迭代条件风险价值（CVaR）目标以线性和一般的函数逼近方法。这种名为带有函数逼近的ICVaR-RL的新形式，为每个决策步骤提供了一种可靠的安全保证方式。对于采用线性函数逼近的ICVaR-RL，我们提出了一个计算效率高的算法ICVaR-L，该算法的后悔度为$\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$，其中$\alpha$是风险水平，$d$是状态行动特征的维度，$H$是每个episode的长度，$K$是episode的数量。我们还建立了一个相匹配的下界$\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$，以验证ICVaR-L在$d$和$K$方面的最优性。对于采用一般函数逼近的ICVaR-RL，我们提出了算法ICVaR-G，它实现了...

    Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we investigate a novel risk-sensitive RL formulation with an Iterated Conditional Value-at-Risk (CVaR) objective under linear and general function approximations. This new formulation, named ICVaR-RL with function approximation, provides a principled way to guarantee safety at each decision step. For ICVaR-RL with linear function approximation, we propose a computationally efficient algorithm ICVaR-L, which achieves an $\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$ regret, where $\alpha$ is the risk level, $d$ is the dimension of state-action features, $H$ is the length of each episode, and $K$ is the number of episodes. We also establish a matching lower bound $\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$ to validate the optimality of ICVaR-L with respect to $d$ and $K$. For ICVaR-RL with general function approximation, we propose algorithm ICVaR-G, which achiev
    
[^46]: 政策对比仿真学习

    Policy Contrastive Imitation Learning. (arXiv:2307.02829v1 [cs.LG])

    [http://arxiv.org/abs/2307.02829](http://arxiv.org/abs/2307.02829)

    政策对比仿真学习(PCIL)是一种解决敌对仿真学习(AIL)性能不佳问题的新方法。PCIL通过学习对比表示空间，并生成平滑的余弦相似度奖励，提供更有意义的代理与政策之间的比较。

    

    敌对仿真学习（AIL）是一种最近取得了很大成功的流行方法。然而，AIL在更具挑战性的任务上的表现仍然不令人满意。我们发现其中一个主要原因是由于AIL鉴别器表示的质量较低。由于AIL鉴别器通过二元分类进行训练，并不一定以有意义的方式区分政策和专家，因此得到的奖励可能也是没有意义的。为了解决这个问题，我们提出了一种新的方法，称为政策对比仿真学习（PCIL）。PCIL通过锚定不同的策略来学习对比表示空间，并生成基于余弦相似度的平滑奖励。我们提出的表示学习目标可以被看作是AIL目标的更强版本，并提供了更有意义的代理与政策之间的比较。从理论的角度出发，我们使用学徒技艺证明了我们方法的有效性。

    Adversarial imitation learning (AIL) is a popular method that has recently achieved much success. However, the performance of AIL is still unsatisfactory on the more challenging tasks. We find that one of the major reasons is due to the low quality of AIL discriminator representation. Since the AIL discriminator is trained via binary classification that does not necessarily discriminate the policy from the expert in a meaningful way, the resulting reward might not be meaningful either. We propose a new method called Policy Contrastive Imitation Learning (PCIL) to resolve this issue. PCIL learns a contrastive representation space by anchoring on different policies and generates a smooth cosine-similarity-based reward. Our proposed representation learning objective can be viewed as a stronger version of the AIL objective and provide a more meaningful comparison between the agent and the policy. From a theoretical perspective, we show the validity of our method using the apprenticeship le
    
[^47]: 基于采样的快速梯度重标定方法用于高度可转移的对抗攻击

    Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (arXiv:2307.02828v1 [cs.CV])

    [http://arxiv.org/abs/2307.02828](http://arxiv.org/abs/2307.02828)

    本文提出了一种名为采样-based快速梯度重标定方法（S-FGRM）的对抗攻击算法。通过使用数据重标定来替代梯度更新中的符号函数，我们解决了梯度更新估计不准确和对抗转移次优解的问题。

    

    深度神经网络对通过向无害输入添加人类无法察觉的扰动来制造的对抗样本是脆弱的。在白盒环境中几乎实现了100％的攻击成功率后，更多的关注点转向了黑盒攻击，其中对抗样本的可转移性引起了重要关注。无论哪种情况，常见的基于梯度的方法通常使用符号函数来生成梯度更新的扰动，这提供了大致正确的方向并取得了巨大的成功。但是很少有研究关注它可能存在的局限性。在这项工作中，我们观察到原始梯度和生成噪声之间的偏差可能导致梯度更新估计不准确和对抗转移的次优解。为此，我们提出了一种基于采样的快速梯度重标定方法（S-FGRM）。具体而言，我们使用数据重标定来替代符号函数，而不需要额外的计算成本。

    Deep neural networks are known to be vulnerable to adversarial examples crafted by adding human-imperceptible perturbations to the benign input. After achieving nearly 100% attack success rates in white-box setting, more focus is shifted to black-box attacks, of which the transferability of adversarial examples has gained significant attention. In either case, the common gradient-based methods generally use the sign function to generate perturbations on the gradient update, that offers a roughly correct direction and has gained great success. But little work pays attention to its possible limitation. In this work, we observe that the deviation between the original gradient and the generated noise may lead to inaccurate gradient update estimation and suboptimal solutions for adversarial transferability. To this end, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM). Specifically, we use data rescaling to substitute the sign function without extra computational cost. We
    
[^48]: 机器学习和脑电图（EEG）的发展趋势：本科研究生的综述

    Trends in Machine Learning and Electroencephalogram (EEG): A Review for Undergraduate Researchers. (arXiv:2307.02819v1 [cs.HC])

    [http://arxiv.org/abs/2307.02819](http://arxiv.org/abs/2307.02819)

    这篇论文是一篇关于脑机接口（BCIs）在机器学习背景下的系统文献综述，重点介绍了脑电图（EEG）研究的最新趋势，并旨在为本科研究生提供了解BCI领域的概述，包括任务、算法和数据集，通过综合最新研究发现，提供对BCI研究的基本理解，并找出未来研究的有前途的方向。

    

    本文对机器学习背景下的脑机接口（BCIs）进行了系统的文献综述。我们关注脑电图（EEG）研究，强调最新的趋势（截至2023年）。目标是为本科研究生提供对BCI领域的可获取概述，涵盖任务、算法和数据集。通过综合最新研究发现，我们旨在提供对BCI研究的基本理解，识别未来研究的有前途的方向。

    This paper presents a systematic literature review on Brain-Computer Interfaces (BCIs) in the context of Machine Learning. Our focus is on Electroencephalography (EEG) research, highlighting the latest trends as of 2023. The objective is to provide undergraduate researchers with an accessible overview of the BCI field, covering tasks, algorithms, and datasets. By synthesizing recent findings, our aim is to offer a fundamental understanding of BCI research, identifying promising avenues for future investigations.
    
[^49]: CPDG: 一种用于动态图神经网络的对比式预训练方法

    CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks. (arXiv:2307.02813v1 [cs.LG])

    [http://arxiv.org/abs/2307.02813](http://arxiv.org/abs/2307.02813)

    这篇论文提出了一种对比式预训练方法（CPDG）用于动态图神经网络（DGNNs），通过灵活的结构-时序子图采样和结构-时序对比式预训练方案，解决了DGNNs预训练中的泛化和长短期建模能力等挑战，实验证明CPDG在各种下游任务中的动态图预训练方面优于现有方法。

    

    近年来，由于动态图中蕴含丰富信息并在实际场景中得到广泛应用，动态图数据挖掘变得越来越流行。尽管动态图神经网络（DGNNs）取得了一定的进展，但其丰富的信息和多样的下游任务给在工业情景中实际应用带来了显著困难。因此，在本文中，我们提出了对比式预训练方法（CPDG）来解决这些问题。CPDG通过一种灵活的结构-时序子图采样器和结构-时序对比式预训练方案，解决了DGNNs预训练中的泛化和长短期建模能力等挑战。在大规模的研究和工业动态图数据集上进行的大量实验表明，CPDG在各种下游任务中的动态图预训练方面优于现有方法。

    Dynamic graph data mining has gained popularity in recent years due to the rich information contained in dynamic graphs and their widespread use in the real world. Despite the advances in dynamic graph neural networks (DGNNs), the rich information and diverse downstream tasks have posed significant difficulties for the practical application of DGNNs in industrial scenarios. To this end, in this paper, we propose to address them by pre-training and present the Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG). CPDG tackles the challenges of pre-training for DGNNs, including generalization and long-short term modeling capability, through a flexible structural-temporal subgraph sampler along with structural-temporal contrastive pre-training schemes. Extensive experiments conducted on both large-scale research and industrial dynamic graph datasets show that CPDG outperforms existing methods in dynamic graph pre-training for various downstream tasks under three transf
    
[^50]: OLR-WA 带加权平均的在线回归

    OLR-WA Online Regression with Weighted Average. (arXiv:2307.02804v1 [cs.LG])

    [http://arxiv.org/abs/2307.02804](http://arxiv.org/abs/2307.02804)

    在线回归方法OLR-WA通过将新到达的数据与先前模型相结合，使用用户定义的权重进行加权平均，在遇到数据变化时提供灵活性，既能适应一致数据又能适应变化数据。

    

    为了构建准确的模型，机器学习需要大量的训练数据。有时候数据是逐步到达的，需要大量的存储空间，并且需要重新计算模型以适应新的数据。在线学习通过在遇到数据时逐步修改模型，然后丢弃数据来解决这些问题。在本研究中，我们介绍了一种新的在线线性回归方法。我们的方法将新到达的数据与先前存在的模型相结合，创建一个新模型。引入的模型名为 OLR-WA（带加权平均的在线回归），使用用户定义的权重来在数据变化时提供灵活性，以偏向旧数据或新数据的结果。我们进行了2D和3D实验，将 OLR-WA 与使用整个数据集的静态模型进行了比较。结果显示，在数据一致的情况下，OLR-WA 和静态批处理模型的表现类似，而在数据变化的情况下，用户可以设置 OLR-WA 更快地适应新数据或旧数据的需求。

    Machine Learning requires a large amount of training data in order to build accurate models. Sometimes the data arrives over time, requiring significant storage space and recalculating the model to account for the new data. On-line learning addresses these issues by incrementally modifying the model as data is encountered, and then discarding the data. In this study we introduce a new online linear regression approach. Our approach combines newly arriving data with a previously existing model to create a new model. The introduced model, named OLR-WA (OnLine Regression with Weighted Average) uses user-defined weights to provide flexibility in the face of changing data to bias the results in favor of old or new data. We have conducted 2-D and 3-D experiments comparing OLR-WA to a static model using the entire data set. The results show that for consistent data, OLR-WA and the static batch model perform similarly and for varying data, the user can set the OLR-WA to adapt more quickly or t
    
[^51]: 用张量回归进行少样本个性化显著性预测，保留结构全局信息。

    Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information. (arXiv:2307.02799v1 [eess.IV])

    [http://arxiv.org/abs/2307.02799](http://arxiv.org/abs/2307.02799)

    本文提出了一种使用张量回归进行少样本个性化显著性预测的方法，以保留个性化显著性图的结构全局信息。

    

    本文提出了一种使用张量到矩阵回归进行少样本个性化显著性预测的方法，以保留个性化显著性图（PSM）的结构全局信息。与一般的显著性图相比，PSM具有巨大的潜力，因为它的映射指示了个体特定的视觉注意力，对于从凝视区域的异质性中获取个体视觉偏好非常有用。PSM的预测是为了获取未见图像的PSM，但由于个体凝视模式的复杂性，其预测仍然是一项具有挑战性的任务。为了从有限的眼动数据中识别个体凝视模式，先前的方法采用个体之间凝视趋势的相似性。然而，在先前的方法中，PSMs被向量化以适应预测模型，从而忽视了与图像对应的PSMs的结构全局信息。为了自动揭示PSMs之间的关系，我们聚焦于...

    This paper presents a few-shot personalized saliency prediction using tensor-to-matrix regression for preserving the structural global information of personalized saliency maps (PSMs). In contrast to a general saliency map, a PSM has been great potential since its map indicates the person-specific visual attention that is useful for obtaining individual visual preferences from heterogeneity of gazed areas. The PSM prediction is needed for acquiring the PSM for the unseen image, but its prediction is still a challenging task due to the complexity of individual gaze patterns. For recognizing individual gaze patterns from the limited amount of eye-tracking data, the previous methods adopt the similarity of gaze tendency between persons. However, in the previous methods, the PSMs are vectorized for the prediction model. In this way, the structural global information of the PSMs corresponding to the image is ignored. For automatically revealing the relationship between PSMs, we focus on the
    
[^52]: VerifAI：验证生成式人工智能

    VerifAI: Verified Generative AI. (arXiv:2307.02796v1 [cs.DB])

    [http://arxiv.org/abs/2307.02796](http://arxiv.org/abs/2307.02796)

    验证生成式人工智能的输出是一个新兴问题，我们提出了通过分析多模态数据湖的底层数据，评估其质量和一致性，来建立评估生成式人工智能模型输出的更坚实基础，并解决错误信息传播的挑战。

    

    生成式人工智能已经取得了重要的进展，但是对于其输出的准确性和可靠性的担忧仍在增长。这种不准确性可能产生严重后果，如错误决策，传播虚假信息，侵犯隐私，法律责任等。虽然已经在进行应对这些风险的努力，包括可解释的人工智能和负责任的人工智能实践，如透明度，隐私保护，偏见缓解以及社会和环境责任等，但由生成式人工智能引起的错误信息仍然是一个重大挑战。我们提出，从数据管理的角度验证生成式人工智能的输出是生成式人工智能的一个新兴问题。这包括分析来自多模态数据湖的底层数据，包括文本文件，表格和知识图谱，并评估其质量和一致性。通过这样做，我们可以为评估生成式人工智能模型的输出奠定更坚实的基础。这种方法能够帮助解决生成式人工智能的输出验证问题。

    Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. Although efforts to address these risks are underway, including explainable AI and responsible AI practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge. We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI. This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models. Such an approach ca
    
[^53]: 子群可分性在组公平医学图像分类中的作用

    The Role of Subgroup Separability in Group-Fair Medical Image Classification. (arXiv:2307.02791v1 [cs.CV])

    [http://arxiv.org/abs/2307.02791](http://arxiv.org/abs/2307.02791)

    本研究研究了深度分类器中的表现差异，发现分类器将个体分为子群的能力在医学成像模态和受保护特征方面存在显著差异，并证明了这个属性对算法偏见具有预测能力。通过理论分析和实证评估，我们发现子群可分性、子群差异和模型在存在系统偏见数据时的性能降级之间存在关系，这为公平医学成像人工智能的发展提供了重要的见解。

    

    我们研究了深度分类器中的表现差异。我们发现分类器将个体分为子群的能力在医学成像模态和受保护特征方面存在显著差异；关键是，我们证明了这个属性对算法偏见具有预测能力。通过理论分析和大量实证评估，我们发现子群可分性、子群差异和模型在存在系统偏见数据（如欠诊断）时的性能降级之间存在关系。我们的发现为模型如何产生偏见提供了新的视角，为公平医学成像人工智能的发展提供了重要的见解。

    We investigate performance disparities in deep classifiers. We find that the ability of classifiers to separate individuals into subgroups varies substantially across medical imaging modalities and protected characteristics; crucially, we show that this property is predictive of algorithmic bias. Through theoretical analysis and extensive empirical evaluation, we find a relationship between subgroup separability, subgroup disparities, and performance degradation when models are trained on data with systematic bias such as underdiagnosis. Our findings shed new light on the question of how models become biased, providing important insights for the development of fair medical imaging AI.
    
[^54]: 大型语言模型赋能连接智能的自主边缘AI

    Large Language Models Empowered Autonomous Edge AI for Connected Intelligence. (arXiv:2307.02779v1 [cs.IT])

    [http://arxiv.org/abs/2307.02779](http://arxiv.org/abs/2307.02779)

    大型语言模型赋能连接智能的自主边缘AI系统，通过云-边缘-客户端的分层架构和强大的GPT模型能力，实现高质量、低延迟、隐私保护的AI服务，满足用户个人需求并实现自动化。

    

    无线网络的发展朝着连接智能的方向发展，这一概念设想了在超连接的网络物理世界中，人类、物体和智能之间实现无缝互联。边缘AI作为实现连接智能的有希望的解决方案，通过在网络边缘提供高质量、低延迟和隐私保护的AI服务。在本文中，我们介绍了一种自主边缘AI系统，该系统自动组织、适应和优化自己以满足用户的各种需求。该系统采用云-边缘-客户端的分层架构，其中大型语言模型——生成式预训练变换器（GPT）存放在云端，其他AI模型被共同部署在设备和边缘服务器上。通过利用GPT在语言理解、规划和代码生成方面的强大能力，我们提出了一个多功能的框架，有效协调边缘AI模型以满足用户的个人需求，同时实现自动化。

    The evolution of wireless networks gravitates towards connected intelligence, a concept that envisions seamless interconnectivity among humans, objects, and intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a promising solution to achieve connected intelligence by delivering high-quality, low-latency, and privacy-preserving AI services at the network edge. In this article, we introduce an autonomous edge AI system that automatically organizes, adapts, and optimizes itself to meet users' diverse requirements. The system employs a cloud-edge-client hierarchical architecture, where the large language model, i.e., Generative Pretrained Transformer (GPT), resides in the cloud, and other AI models are co-deployed on devices and edge servers. By leveraging the powerful abilities of GPT in language understanding, planning, and code generation, we present a versatile framework that efficiently coordinates edge AI models to cater to users' personal demands while automati
    
[^55]: 高维度跳跃偏微分方程的时差学习

    Temporal Difference Learning for High-Dimensional PIDEs with Jumps. (arXiv:2307.02766v1 [math.NA])

    [http://arxiv.org/abs/2307.02766](http://arxiv.org/abs/2307.02766)

    本文提出了一种用于解决高维度跳跃偏微分方程的时差学习深度学习框架，该方法具有低计算成本和稳健性的优势，可以有效地处理具有不同形式和强度跳跃的问题。

    

    在本文中，我们提出了基于时差学习的深度学习框架，用于解决高维度的偏积分微分方程（PIDEs）。我们引入了一组Levy过程，并构建了一个相应的强化学习模型。为了模拟整个过程，我们使用深度神经网络来表示方程的解和非局部项。随后，我们使用时差误差、终止条件和非局部项的属性作为损失函数来训练网络。该方法在100维的实验中相对误差达到了O(10^{-3})，在一维纯跳跃问题中达到了O(10^{-4})。此外，我们的方法具有低计算成本和稳健性的优势，适用于解决具有不同形式和强度跳跃的问题。

    In this paper, we propose a deep learning framework for solving high-dimensional partial integro-differential equations (PIDEs) based on the temporal difference learning. We introduce a set of Levy processes and construct a corresponding reinforcement learning model. To simulate the entire process, we use deep neural networks to represent the solutions and non-local terms of the equations. Subsequently, we train the networks using the temporal difference error, termination condition, and properties of the non-local terms as the loss function. The relative error of the method reaches O(10^{-3}) in 100-dimensional experiments and O(10^{-4}) in one-dimensional pure jump problems. Additionally, our method demonstrates the advantages of low computational cost and robustness, making it well-suited for addressing problems with different forms and intensities of jumps.
    
[^56]: 何时使用基于置信度的级联延迟足够？

    When Does Confidence-Based Cascade Deferral Suffice?. (arXiv:2307.02764v1 [cs.LG])

    [http://arxiv.org/abs/2307.02764](http://arxiv.org/abs/2307.02764)

    本研究旨在探讨何时基于置信度的级联延迟可能失败，以及何时备选的延迟策略可能表现更好。通过理论分析和实验证明事后延迟机制能够显著提高性能。

    

    级联是一种经典的策略，可以实现适应性地在样本之间变化的推理成本，其中按顺序调用一系列分类器。延迟规则确定是否调用序列中的下一个分类器，或者终止预测。一种简单的延迟规则利用当前分类器的置信度，例如基于最大预测的softmax概率。尽管对级联结构不敏感——例如不建模下游模型的错误——但这种基于置信度的延迟经常在实践中表现出色。在本文中，我们旨在更好地理解基于置信度的延迟可能失败的条件，以及何时备选的延迟策略可能更好。我们首先对最优延迟规则进行了理论表征，精确地描述了基于置信度的延迟可能受到影响的设置。然后我们研究了事后延迟机制，并验证它们可以显著提高性能。

    Cascades are a classical strategy to enable inference cost to vary adaptively across samples, wherein a sequence of classifiers are invoked in turn. A deferral rule determines whether to invoke the next classifier in the sequence, or to terminate prediction. One simple deferral rule employs the confidence of the current classifier, e.g., based on the maximum predicted softmax probability. Despite being oblivious to the structure of the cascade -- e.g., not modelling the errors of downstream models -- such confidence-based deferral often works remarkably well in practice. In this paper, we seek to better understand the conditions under which confidence-based deferral may fail, and when alternate deferral strategies can perform better. We first present a theoretical characterisation of the optimal deferral rule, which precisely characterises settings under which confidence-based deferral may suffer. We then study post-hoc deferral mechanisms, and demonstrate they can significantly improv
    
[^57]: 在不平衡数据集中的离线强化学习

    Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2307.02752v1 [cs.LG])

    [http://arxiv.org/abs/2307.02752](http://arxiv.org/abs/2307.02752)

    本文提出了一种在不平衡数据集中的新型离线强化学习方法，通过将CQL与回溯过程相结合来提取策略，从而有效地解决了不平衡数据集带来的挑战。

    

    当前离线强化学习（RL）研究中对基准的普遍使用导致了对实际数据集分布不平衡的忽视。由于探索或安全考虑的挑战，实际离线RL数据集在状态空间上通常是不平衡的。我们在本文中具体说明了离线RL中不平衡数据集的特性，其中状态覆盖率遵循一个由偏态策略所特征化的幂律分布。理论上和实证上，我们证明了基于分布约束的典型离线RL方法，如保守Q学习（CQL），在不平衡数据集下提取策略是无效的。受自然智能的启发，我们提出了一种新的离线RL方法，该方法利用CQL的增强与回溯过程相结合，以回忆以往相关经验，有效地缓解不平衡数据集带来的挑战。我们在多个任务上评估了我们的方法。

    The prevalent use of benchmarks in current offline reinforcement learning (RL) research has led to a neglect of the imbalance of real-world dataset distributions in the development of models. The real-world offline RL dataset is often imbalanced over the state space due to the challenge of exploration or safety considerations. In this paper, we specify properties of imbalanced datasets in offline RL, where the state coverage follows a power law distribution characterized by skewed policies. Theoretically and empirically, we show that typically offline RL methods based on distributional constraints, such as conservative Q-learning (CQL), are ineffective in extracting policies under the imbalanced dataset. Inspired by natural intelligence, we propose a novel offline RL method that utilizes the augmentation of CQL with a retrieval process to recall past related experiences, effectively alleviating the challenges posed by imbalanced datasets. We evaluate our method on several tasks in the 
    
[^58]: 评估评估器：当前的少样本学习基准适用吗？

    Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?. (arXiv:2307.02732v1 [cs.LG])

    [http://arxiv.org/abs/2307.02732](http://arxiv.org/abs/2307.02732)

    本文首次对任务级别的评估进行了研究，发现现有的少样本学习基准并不能以可靠的方式设计，无法获取关于如何评估和选择模型的可靠情况的完整画面。

    

    在过去的十年中，提出了许多少样本学习的基准。然而，所有这些基准都集中在对许多任务平均性能的评估上，但在这种情况下如何可靠地评估和调整针对个别任务训练的模型的问题尚未解决。本文首次对任务级别的评估进行了研究，这在部署模型时是一个基本步骤。我们测量了少样本场景中性能估计器的准确性，考虑了模型选择的策略，并检查了通常被认为是鲁棒的评估器失败的原因。我们得出结论，用较少的折叠进行交叉验证是直接估计模型性能的最佳选择，而用自助法或大量折叠进行交叉验证更适合于模型选择的目的。总的来说，我们发现现有的少样本学习基准并不能以可靠的方式设计，无法获取关于如何评估和选择模型的可靠情况的完整画面。

    Numerous benchmarks for Few-Shot Learning have been proposed in the last decade. However all of these benchmarks focus on performance averaged over many tasks, and the question of how to reliably evaluate and tune models trained for individual tasks in this regime has not been addressed. This paper presents the first investigation into task-level evaluation -- a fundamental step when deploying a model. We measure the accuracy of performance estimators in the few-shot setting, consider strategies for model selection, and examine the reasons for the failure of evaluators usually thought of as being robust. We conclude that cross-validation with a low number of folds is the best choice for directly estimating the performance of a model, whereas using bootstrapping or cross validation with a large number of folds is better for model selection purposes. Overall, we find that existing benchmarks for few-shot learning are not designed in such a way that one can get a reliable picture of how e
    
[^59]: 分层授权：朝着可行的基于授权的技能学习迈进

    Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning. (arXiv:2307.02728v1 [cs.LG])

    [http://arxiv.org/abs/2307.02728](http://arxiv.org/abs/2307.02728)

    分层授权提出了一种可以计算授权的新框架，通过引入变分下界和分层架构，实现了在短期和长期时间尺度上的授权计算，并在模拟机器人任务中得到了验证。

    

    通用智能体需要大量的技能。 授权 - 技能和状态之间的最大互信息 - 为学习大量不同技能提供了一条路径，但互信息很难优化。我们介绍了一种新的框架，分层授权，通过集成目标条件层次强化学习的概念，使得计算授权更加可行。我们的框架提供了两个具体的贡献。首先，我们介绍了一种新的变分下界，可用于计算短期视角下的授权。其次，我们引入了一个分层架构，用于计算指数时间尺度下的授权。我们在一系列模拟机器人任务中验证了该框架的贡献。在一个流行的蚂蚁导航领域，我们的四级智能体能够学习覆盖面积比之前的工作大两个数量级的技能。

    General purpose agents will require large repertoires of skills. Empowerment -- the maximum mutual information between skills and the states -- provides a pathway for learning large collections of distinct skills, but mutual information is difficult to optimize. We introduce a new framework, Hierarchical Empowerment, that makes computing empowerment more tractable by integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning. Our framework makes two specific contributions. First, we introduce a new variational lower bound on mutual information that can be used to compute empowerment over short horizons. Second, we introduce a hierarchical architecture for computing empowerment over exponentially longer time scales. We verify the contributions of the framework in a series of simulated robotics tasks. In a popular ant navigation domain, our four level agents are able to learn skills that cover a surface area over two orders of magnitude larger than prior work.
    
[^60]: 经过公平性角度的实验分析和评估实体匹配问题

    Through the Fairness Lens: Experimental Analysis and Evaluation of Entity Matching. (arXiv:2307.02726v1 [cs.DB])

    [http://arxiv.org/abs/2307.02726](http://arxiv.org/abs/2307.02726)

    本文通过实验对多种实体匹配技术进行了广泛评估，发现实体匹配存在潜在的不公平性。研究结果表明，当某些人口群体过度代表或某些群体中的姓名更相似时，实体匹配可能存在公平性问题。

    

    实体匹配是一个具有挑战性的问题，多个社区已经研究了半个多世纪。算法公平性也已经成为解决机器偏见及其社会影响的一个及时话题。尽管对这两个问题进行了广泛的研究，但对实体匹配的公平性关注甚少。为了解决这一差距，本文对多种实体匹配技术进行了广泛的实验评估。我们从公开可得的数据集中生成了两个社交数据集，以通过公平性的视角审查实体匹配。我们的研究结果强调了现实社会中两种常见情况下的潜在不公平性：（i）某些人口群体的过度代表和（ii）某些群体中的姓名相比其他群体更相似。在我们众多的研究发现中，值得一提的是，虽然各种公平性定义在不同环境中都是有价值的，但由于实体匹配具有类别不平衡的特点，例如正预测率等指标在这种情况下更有价值。

    Entity matching (EM) is a challenging problem studied by different communities for over half a century. Algorithmic fairness has also become a timely topic to address machine bias and its societal impacts. Despite extensive research on these two topics, little attention has been paid to the fairness of entity matching.  Towards addressing this gap, we perform an extensive experimental evaluation of a variety of EM techniques in this paper. We generated two social datasets from publicly available datasets for the purpose of auditing EM through the lens of fairness. Our findings underscore potential unfairness under two common conditions in real-world societies: (i) when some demographic groups are overrepresented, and (ii) when names are more similar in some groups compared to others. Among our many findings, it is noteworthy to mention that while various fairness definitions are valuable for different settings, due to EM's class imbalance nature, measures such as positive predictive va
    
[^61]: 理解不确定性采样

    Understanding Uncertainty Sampling. (arXiv:2307.02719v1 [cs.LG])

    [http://arxiv.org/abs/2307.02719](http://arxiv.org/abs/2307.02719)

    本研究通过系统研究流式和池式主动学习下的不确定性采样算法，提出了一个等效损失的概念，并证明不确定性采样算法实质上是针对该等效损失进行优化。

    

    不确定性采样是一种常见的主动学习算法，它顺序地查询当前预测模型对数据样本的不确定性。然而，不确定性采样的使用往往是启发式的：（i）关于在特定任务和特定损失函数下对“不确定性”的准确定义没有共识；（ii）没有理论保证能够给出一个标准协议来实施该算法，例如，在随机梯度下降等优化算法框架下如何处理顺序到达的注释数据。在本研究中，我们系统地研究了流式和池式主动学习下的不确定性采样算法。我们提出了一个等效损失的概念，该概念取决于使用的不确定性度量和原始损失函数，并确立了不确定性采样算法本质上是针对这种等效损失进行优化。这一观点验证了算法的适当性。

    Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of "uncertainty" for a specific task under a specific loss; (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properne
    
[^62]: 多相似度对比学习

    Multi-Similarity Contrastive Learning. (arXiv:2307.02712v1 [cs.LG])

    [http://arxiv.org/abs/2307.02712](http://arxiv.org/abs/2307.02712)

    本文提出了一种多相似度对比损失方法（MSCon），通过联合利用多个相似度度量的监督来学习可泛化的嵌入，从而实现对新任务的更好领域外泛化。

    

    在给定相似度度量的情况下，对比学习方法学习一种表示，其中相似的样本被推到一起，不相似的样本被拉开。对比学习技术已被广泛应用于学习用于图像分类到字幕生成等任务的表示。然而，现有的对比学习方法可能在泛化方面存在问题，因为它们没有考虑到不同相似性关系的可能性。在本文中，我们提出了一种新颖的多相似度对比损失（MSCon），通过联合利用多个相似度度量的监督来学习可泛化的嵌入。我们的方法根据相应相似度的不确定性自动学习对比相似度的权重，降低不确定任务的权重，从而实现对新任务的更好领域外泛化。我们的实验证明，使用MSCon训练的网络在领域内优于最先进的基线模型。

    Given a similarity metric, contrastive methods learn a representation in which examples that are similar are pushed together and examples that are dissimilar are pulled apart. Contrastive learning techniques have been utilized extensively to learn representations for tasks ranging from image classification to caption generation. However, existing contrastive learning approaches can fail to generalize because they do not take into account the possibility of different similarity relations. In this paper, we propose a novel multi-similarity contrastive loss (MSCon), that learns generalizable embeddings by jointly utilizing supervision from multiple metrics of similarity. Our method automatically learns contrastive similarity weightings based on the uncertainty in the corresponding similarity, down-weighting uncertain tasks and leading to better out-of-domain generalization to new tasks. We show empirically that networks trained with MSCon outperform state-of-the-art baselines on in-domain
    
[^63]: 实现对周期性材料的对称感知生成

    Towards Symmetry-Aware Generation of Periodic Materials. (arXiv:2307.02707v1 [cs.LG])

    [http://arxiv.org/abs/2307.02707](http://arxiv.org/abs/2307.02707)

    提出了一种名为SyMat的新的材料生成方法，能够对周期性材料的物理对称性进行感知，并能在生成和优化任务中取得有希望的性能。

    

    我们考虑使用深度模型生成周期性材料的问题。虽然对于对称感知的分子生成已经得到了广泛研究，但是周期性材料具有不同的对称性，现有的方法并不能完全捕捉到这些对称性。在这项工作中，我们提出了一种新的材料生成方法SyMat，能够捕捉周期性材料结构的物理对称性。SyMat利用变分自动编码器模型生成材料的原子类型和晶格，通过生成原子类型集、晶格长度和晶格角度。此外，SyMat采用基于评分的扩散模型生成材料的原子坐标，在坐标扩散过程中使用了一种新的对称感知概率模型。我们证明SyMat在所有材料的对称变换上理论上是不变的，并且证明了SyMat在随机生成和性能优化任务上取得了有希望的性能。

    We consider the problem of generating periodic materials with deep models. While symmetry-aware molecule generation has been studied extensively, periodic materials possess different symmetries, which have not been completely captured by existing methods. In this work, we propose SyMat, a novel material generation approach that can capture physical symmetries of periodic material structures. SyMat generates atom types and lattices of materials through generating atom type sets, lattice lengths and lattice angles with a variational auto-encoder model. In addition, SyMat employs a score-based diffusion model to generate atom coordinates of materials, in which a novel symmetry-aware probabilistic model is used in the coordinate diffusion process. We show that SyMat is theoretically invariant to all symmetry transformations on materials and demonstrate that SyMat achieves promising performance on random generation and property optimization tasks.
    
[^64]: 深度学习中的损失函数和度量方法：一项评论

    Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])

    [http://arxiv.org/abs/2307.02694](http://arxiv.org/abs/2307.02694)

    本文回顾了深度学习中最常见的损失函数和性能测量方法，旨在帮助从业者选择最适合其特定任务的方法。

    

    深度学习的一个重要组成部分是选择用于训练和评估模型的损失函数和性能度量。本文回顾了深度学习中最常见的损失函数和性能测量方法。我们探讨了每种技术的优势和局限性，并举例说明它们在各种深度学习问题上的应用。我们的评论旨在全面了解最常见的深度学习任务中使用的不同损失函数和性能指标，并帮助从业者选择最适合其特定任务的方法。

    One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
    
[^65]: 内核，数据和物理。

    Kernels, Data & Physics. (arXiv:2307.02693v1 [cs.LG])

    [http://arxiv.org/abs/2307.02693](http://arxiv.org/abs/2307.02693)

    该论文主要介绍了NTK方法在机器学习问题中的应用，通过找到可处理的内核表达形式来解决一般无法解决的问题，重点讨论了数据精炼和对抗鲁棒性等实际应用。

    

    这是Julia Kempe教授在Les Houches举办的夏季学校“机器学习的统计物理”中所讲授的课程笔记。笔记讨论了所谓的NTK方法，该方法通过找到可处理的内核表达形式来理解一般无法解决的问题。笔记主要关注实际应用，如数据精炼和对抗鲁棒性，也讨论了归纳偏差的示例。

    Lecture notes from the course given by Professor Julia Kempe at the summer school "Statistical physics of Machine Learning" in Les Houches. The notes discuss the so-called NTK approach to problems in machine learning, which consists of gaining an understanding of generally unsolvable problems by finding a tractable kernel formulation. The notes are mainly focused on practical applications such as data distillation and adversarial robustness, examples of inductive bias are also discussed.
    
[^66]: 使用结构化注意力扩展上下文演示

    Scaling In-Context Demonstrations with Structured Attention. (arXiv:2307.02690v1 [cs.CL])

    [http://arxiv.org/abs/2307.02690](http://arxiv.org/abs/2307.02690)

    本研究提出了一种用于上下文学习的结构化注意力机制，解决了大规模语言模型在使用演示进行上下文学习时遇到的限制与挑战。

    

    最近大规模语言模型的兴起突出了它们在上下文学习方面的能力，即在上下文中从少数演示中“学习”执行任务而无需进行参数更新。然而，它们在上下文学习方面的能力受到模型架构的限制：1）由于位置嵌入，演示的使用受到最大句子长度的限制；2）注意力的二次复杂度阻碍用户有效使用更多的演示；3）研究表明，LLM对演示的顺序敏感。在这项工作中，我们通过提出更好的架构设计来解决这些挑战。我们提出了SAICL（用于上下文学习的结构化注意力），它通过为上下文学习设计了一种结构化注意力机制来替换全注意力，并消除了个别演示之间不必要的依赖关系，同时使模型对演示的排列不变。

    The recent surge of large language models (LLMs) highlights their ability to perform in-context learning, i.e., "learning" to perform a task from a few demonstrations in the context without any parameter updates. However, their capabilities of in-context learning are limited by the model architecture: 1) the use of demonstrations is constrained by a maximum sentence length due to positional embeddings; 2) the quadratic complexity of attention hinders users from using more demonstrations efficiently; 3) LLMs are shown to be sensitive to the order of the demonstrations. In this work, we tackle these challenges by proposing a better architectural design for in-context learning. We propose SAICL (Structured Attention for In-Context Learning), which replaces the full-attention by a structured attention mechanism designed for in-context learning, and removes unnecessary dependencies between individual demonstrations, while making the model invariant to the permutation of demonstrations. We e
    
[^67]: GIT: 使用梯度和不变性变换检测不确定性、超出分布和对抗样本

    GIT: Detecting Uncertainty, Out-Of-Distribution and Adversarial Samples using Gradients and Invariance Transformations. (arXiv:2307.02672v1 [cs.LG])

    [http://arxiv.org/abs/2307.02672](http://arxiv.org/abs/2307.02672)

    该论文提出了GIT，一种综合方法来检测深度神经网络的泛化错误，该方法结合了梯度信息和不变性变换。通过将错分样本转回神经网络的泛化区域，并测量初始预测与使用转换样本的神经网络的相应计算之间的矛盾，GIT相对于现有技术表现出更好的性能。

    

    深度神经网络往往会做出过于自信的预测，并且通常需要额外的检测器来应对错误分类，尤其是对于安全关键的应用。现有的检测方法通常只关注对抗攻击或超出分布样本作为错误预测的原因。然而，通常出现的泛化误差往往与学习相关的不变性有关。因此，我们提出了GIT，一种检测泛化错误的整体方法，该方法结合了梯度信息和不变性变换的使用。不变性变换的设计是将错误分类的样本转回神经网络的泛化区域，而梯度信息则通过测量初始预测与使用转换样本的神经网络的相应固有计算之间的矛盾来衡量。我们的实验结果表明，GIT相对于现有技术具有更优异的性能。

    Deep neural networks tend to make overconfident predictions and often require additional detectors for misclassifications, particularly for safety-critical applications. Existing detection methods usually only focus on adversarial attacks or out-of-distribution samples as reasons for false predictions. However, generalization errors occur due to diverse reasons often related to poorly learning relevant invariances. We therefore propose GIT, a holistic approach for the detection of generalization errors that combines the usage of gradient information and invariance transformations. The invariance transformations are designed to shift misclassified samples back into the generalization area of the neural network, while the gradient information measures the contradiction between the initial prediction and the corresponding inherent computations of the neural network using the transformed sample. Our experiments demonstrate the superior performance of GIT compared to the state-of-the-art on
    
[^68]: 适用于少样本类增量学习的主动类别选择

    Active Class Selection for Few-Shot Class-Incremental Learning. (arXiv:2307.02641v1 [cs.RO])

    [http://arxiv.org/abs/2307.02641](http://arxiv.org/abs/2307.02641)

    本文结合少样本类增量学习和主动类别选择的思想，提出了一个新的框架(FIASco)，使自主代理通过要求用户仅对最具信息量的少数对象进行标注来不断学习新对象，并将其与基于潜力场的导航技术集成，形成一个完整的框架。

    

    对于真实世界的应用，机器人需要通过与用户有限的交互不断地在环境中学习。在这方面，之前的研究在少样本类增量学习（FSCIL）和主动类别选择（ACS）方面取得了有希望的结果，但是测试都在约束的设置中进行。因此，在本文中，我们结合了FSCIL和ACS的思想，提出了一个新的框架，可以让自主代理通过要求用户仅对环境中最具信息量的少数对象进行标注来不断学习新对象。为此，我们在基于最新技术的FSCIL模型基础上，加入了ACS文献中的一些技术进行扩展。我们把这个模型命名为Few-shot Incremental Active class SeleCtiOn（FIASco）。我们进一步将基于潜力场的导航技术与我们的模型集成在一起，开发了一个完整的框架，使代理能够通过FIASco模型对其感知数据进行处理和推理，并朝最具信息量的对象导航。

    For real-world applications, robots will need to continually learn in their environments through limited interactions with their users. Toward this, previous works in few-shot class incremental learning (FSCIL) and active class selection (ACS) have achieved promising results but were tested in constrained setups. Therefore, in this paper, we combine ideas from FSCIL and ACS to develop a novel framework that can allow an autonomous agent to continually learn new objects by asking its users to label only a few of the most informative objects in the environment. To this end, we build on a state-of-the-art (SOTA) FSCIL model and extend it with techniques from ACS literature. We term this model Few-shot Incremental Active class SeleCtiOn (FIASco). We further integrate a potential field-based navigation technique with our model to develop a complete framework that can allow an agent to process and reason on its sensory data through the FIASco model, navigate towards the most informative obje
    
[^69]: 基于神经薛定谔锻造的混合基态量子算法

    Hybrid Ground-State Quantum Algorithms based on Neural Schr\"odinger Forging. (arXiv:2307.02633v1 [quant-ph])

    [http://arxiv.org/abs/2307.02633](http://arxiv.org/abs/2307.02633)

    提出了一种基于神经网络的纠缠锻造方法来解决基态问题，通过识别最相关的基态位串，消除了指数级求和的需求，并展示了该方法在不同系统上可以达到相当或更优的性能。

    

    基于纠缠锻造的变分算法利用量子系统的双分割来解决基态问题。这些方法的主要限制在于对整个系统的Schmidt分解时需要对无数潜在基态进行指数级求和。为了克服这个挑战，我们提出了一种新的纠缠锻造方法，利用生成性神经网络来识别最相关的基态位串，消除了指数级求和的需求。通过在复杂度递增的系统上进行实证演示，我们展示了所提出的算法与现有的纠缠锻造标准实现相比可达到相当或更优的性能。此外，通过控制所需资源的数量，该方案可以应用于更大的非置换不变系统，后者限制与海森伯锻造过程相关。

    Entanglement forging based variational algorithms leverage the bi-partition of quantum systems for addressing ground state problems. The primary limitation of these approaches lies in the exponential summation required over the numerous potential basis states, or bitstrings, when performing the Schmidt decomposition of the whole system. To overcome this challenge, we propose a new method for entanglement forging employing generative neural networks to identify the most pertinent bitstrings, eliminating the need for the exponential sum. Through empirical demonstrations on systems of increasing complexity, we show that the proposed algorithm achieves comparable or superior performance compared to the existing standard implementation of entanglement forging. Moreover, by controlling the amount of required resources, this scheme can be applied to larger, as well as non permutation invariant systems, where the latter constraint is associated with the Heisenberg forging procedure. We substan
    
[^70]: Q-Learning 的稳定性通过设计和乐观性

    Stability of Q-Learning Through Design and Optimism. (arXiv:2307.02632v1 [cs.LG])

    [http://arxiv.org/abs/2307.02632](http://arxiv.org/abs/2307.02632)

    本文主要介绍了Q-learning在强化学习中的重要性，以及使用乐观性训练和修改后的策略解决Q-learning的稳定性问题和算法收敛加速问题的方法。

    

    自从20世纪80年代Chris Watkins的论文中介绍以来，Q-learning已成为强化学习工具包中的重要组成部分。本文部分是关于随机逼近和Q-learning的教程，提供了关于INFORMS APS发布的第一届应用概率信托全体大会的详细信息。该论文还提出了确保这些算法的稳定性和可能加速收敛的新方法，以及其他设置中的随机逼近。两个贡献是全新的：1. Q-learning在线性函数逼近下的稳定性一直是一个有待研究的话题。结果表明，通过适当的乐观训练和修改后的Gibbs策略，可以存在满足投影Bellman方程的解，并且该算法是稳定的（参数估计有界）。收敛性仍然是众多待研究的问题之一。2. 新的优化方法在小批量执行中改善了逼近算法的迭代速度。

    Q-learning has become an important part of the reinforcement learning toolkit since its introduction in the dissertation of Chris Watkins in the 1980s. The purpose of this paper is in part a tutorial on stochastic approximation and Q-learning, providing details regarding the INFORMS APS inaugural Applied Probability Trust Plenary Lecture, presented in Nancy France, June 2023.  The paper also presents new approaches to ensure stability and potentially accelerated convergence for these algorithms, and stochastic approximation in other settings. Two contributions are entirely new:  1. Stability of Q-learning with linear function approximation has been an open topic for research for over three decades. It is shown that with appropriate optimistic training in the form of a modified Gibbs policy, there exists a solution to the projected Bellman equation, and the algorithm is stable (in terms of bounded parameter estimates). Convergence remains one of many open topics for research.  2. The ne
    
[^71]: 一种可解释的模型以支持AML治疗方案的决策

    An explainable model to support the decision about the therapy protocol for AML. (arXiv:2307.02631v1 [cs.LG])

    [http://arxiv.org/abs/2307.02631](http://arxiv.org/abs/2307.02631)

    本文提出了一种可解释的机器学习模型，用于支持AML患者治疗方案的决策，解决了当前风险分类存在的问题和专家需求额外测试和分析的困扰。

    

    急性髓细胞白血病（AML）是一种最具侵略性的血液肿瘤。为了支持专家关于合适治疗的决策，AML患者根据其细胞遗传和分子特征获得预后信息，通常分为有利、中等和不利三个风险类别。然而，当前的风险分类存在已知问题，如同一风险组中患者之间的异质性和中风险类别的清晰定义缺失。此外，由于大多数AML患者被归为中风险分类，专家常需进行其他测试和分析，导致治疗延迟和患者临床状况恶化。本文提出了数据分析和一种可解释的机器学习模型，以支持根据患者生存预测确定最合适的治疗方案的决策。

    Acute Myeloid Leukemia (AML) is one of the most aggressive types of hematological neoplasm. To support the specialists' decision about the appropriate therapy, patients with AML receive a prognostic of outcomes according to their cytogenetic and molecular characteristics, often divided into three risk categories: favorable, intermediate, and adverse. However, the current risk classification has known problems, such as the heterogeneity between patients of the same risk group and no clear definition of the intermediate risk category. Moreover, as most patients with AML receive an intermediate-risk classification, specialists often demand other tests and analyses, leading to delayed treatment and worsening of the patient's clinical condition. This paper presents the data analysis and an explainable machine-learning model to support the decision about the most appropriate therapy protocol according to the patient's survival prediction. In addition to the prediction model being explainable
    
[^72]: FLuID: 使用不变性丢失减轻联邦学习中的阻塞问题

    FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout. (arXiv:2307.02623v1 [cs.LG])

    [http://arxiv.org/abs/2307.02623](http://arxiv.org/abs/2307.02623)

    FLuID提出了一种使用不变性丢失的方法来减轻联邦学习中性能较低设备导致的训练时间问题，并开发了一个自适应训练框架。通过动态平衡训练负载，FLuID能有效地减轻阻塞设备的工作负载，同时不影响模型质量。

    

    联邦学习（FL）允许机器学习模型在个体移动设备上进行本地训练，并通过共享服务器同步模型更新。这种方法保护用户隐私，但也由于不同设备的性能差异而产生了一个异构的训练环境。因此，在FL中，性能较低的阻塞设备经常决定整体训练时间。在这项工作中，我们旨在通过系统动态平衡训练负载来减轻由于阻塞器产生的性能瓶颈。我们引入了不变性丢失，一种基于权重更新阈值提取子模型的方法，从而最小化对准确性的潜在影响。在此丢失技术的基础上，我们开发了一种自适应训练框架FLuID。FLuID提供了一种轻量级的子模型提取方法来调节计算强度，从而减少阻塞设备的负载而不影响模型质量。

    Federated Learning (FL) allows machine learning models to train locally on individual mobile devices, synchronizing model updates via a shared server. This approach safeguards user privacy; however, it also generates a heterogeneous training environment due to the varying performance capabilities across devices. As a result, straggler devices with lower performance often dictate the overall training time in FL. In this work, we aim to alleviate this performance bottleneck due to stragglers by dynamically balancing the training load across the system. We introduce Invariant Dropout, a method that extracts a sub-model based on the weight update threshold, thereby minimizing potential impacts on accuracy. Building on this dropout technique, we develop an adaptive training framework, Federated Learning using Invariant Dropout (FLuID). FLuID offers a lightweight sub-model extraction to regulate computational intensity, thereby reducing the load on straggler devices without affecting model q
    
[^73]: 在观测代价敏感强化学习中的动态观测策略

    Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning. (arXiv:2307.02620v1 [cs.LG])

    [http://arxiv.org/abs/2307.02620](http://arxiv.org/abs/2307.02620)

    本文研究了在观测代价敏感强化学习中，强化学习代理在每个时间步不需要昂贵的测量，提出了一种新的方法DMSOA，并在多个环境中进行了评估，结果表明DMSOA能够以更少的决策步骤和测量次数学到更好的策略。

    

    强化学习已被证明可以学习复杂任务的高级控制策略，包括游戏、机器人、供暖与制冷系统和文本生成。然而，强化学习中的动作-感知循环通常假设在每个时间步都可以获得对环境状态的测量，且不产生成本。然而，在深海和行星机器人探索、材料设计和医学等应用中，测量或者近似环境状态可能会产生高昂的成本。本文调查了近来不断增长的文献，采取了RL代理可能不需要或者不想在每个时间步进行昂贵测量的观点。在这个背景下，我们提出了Deep Dynamic Multi-Step Observationless Agent (DMSOA)，并将其与文献进行对比，并在OpenAI gym和Atari Pong环境中进行了实证评估。我们的结果显示，DMSOA能够以更少的决策步骤和测量次数学到更好的策略。

    Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as deep-sea and planetary robot exploration, materials design and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and meas
    
[^74]: 基于人类启发的渐进对齐和比较学习用于基于经验的词汇获取

    Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition. (arXiv:2307.02615v1 [cs.CL])

    [http://arxiv.org/abs/2307.02615](http://arxiv.org/abs/2307.02615)

    本研究通过比较学习和渐进对齐的方式，借鉴人类语言习得的过程，探索了一种用于基于经验的词汇获取的计算过程。该方法不涉及固定的词汇量大小，也不涉及有区分性的目标，能够高效地持续学习更多的概念。

    

    人类语言习得是一种高效、受监督和持续的过程。本研究借鉴了人类婴儿习得第一门语言的方式，通过比较学习开发了一种用于词汇获取的计算过程。受认知发现的启发，我们生成了一个小型数据集，使计算模型能够比较各种属性的相似性和差异性，学习过滤出并提取共同的信息用于每个共享的语言标签。我们将词汇获取框架定义为既包括信息过滤过程，也包括表征-符号映射过程。该过程不涉及固定的词汇量大小，也不涉及有区分性的目标，使模型能够持续高效地学习更多的概念。我们在控制实验中的结果显示了这种方法在高效地持续学习基于经验的词汇方面的潜力。

    Human language acquisition is an efficient, supervised, and continual process. In this work, we took inspiration from how human babies acquire their first language, and developed a computational process for word acquisition through comparative learning. Motivated by cognitive findings, we generated a small dataset that enables the computation models to compare the similarities and differences of various attributes, learn to filter out and extract the common information for each shared linguistic label. We frame the acquisition of words as not only the information filtration process, but also as representation-symbol mapping. This procedure does not involve a fixed vocabulary size, nor a discriminative objective, and allows the models to continually learn more concepts efficiently. Our results in controlled experiments have shown the potential of this approach for efficient continual learning of grounded words.
    
[^75]: 添加解码器用于潜变量识别和笛卡尔积推算

    Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation. (arXiv:2307.02598v1 [cs.LG])

    [http://arxiv.org/abs/2307.02598](http://arxiv.org/abs/2307.02598)

    这篇论文解决了表示学习中的潜变量识别和"支持外"图像生成问题，展示了加法解码器能够对潜变量进行识别，并提供了理论依据支持这种方法的有效性。

    

    我们解决了表示学习中的潜变量识别和“支持外”图像生成问题。我们展示了在一类我们称为“加法”的解码器中，这两者是可能的，这些解码器类似于用于面向对象表示学习（OCRL）的解码器，并且非常适用于可以分解为多个特定对象图像的图像。我们提供了在使用加法解码器完全解决重构问题时，对潜变量块进行了置换和块状逆变换的识别的条件。这个保证仅基于关于潜因子分布的非常弱的假设，潜因子可能存在统计依赖并且具有几乎任意形状的支持。我们的结果提供了非线性独立成分分析（ICA）可能性的新设置，并且增加了我们对OCRL方法的理论理解。我们还从理论上证明了加法解码器可以

    We tackle the problems of latent variables identification and "out-of-support" image generation in representation learning. We show that both are possible for a class of decoders that we call additive, which are reminiscent of decoders used for object-centric representation learning (OCRL) and well suited for images that can be decomposed as a sum of object-specific images. We provide conditions under which exactly solving the reconstruction problem using an additive decoder is guaranteed to identify the blocks of latent variables up to permutation and block-wise invertible transformations. This guarantee relies only on very weak assumptions about the distribution of the latent factors, which might present statistical dependencies and have an almost arbitrarily shaped support. Our result provides a new setting where nonlinear independent component analysis (ICA) is possible and adds to our theoretical understanding of OCRL methods. We also show theoretically that additive decoders can 
    
[^76]: TransformerG2G：使用Transformer进行自适应时间步长的学习时态图嵌入

    TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v1 [cs.LG])

    [http://arxiv.org/abs/2307.02588](http://arxiv.org/abs/2307.02588)

    TransformerG2G是一种使用Transformer进行自适应时间步长的图嵌入模型，通过学习历史上的长程依赖关系，准确地捕捉时态图的动态特征。

    

    动态图嵌入已成为处理不同时间图分析任务（如链路预测、节点分类、推荐系统、异常检测和图生成）的一种非常有效的技术，广泛应用于各种应用领域。这些时态图展现了异质的瞬时动态、不同的时间间隔以及在演化过程中高度变化的节点特征。因此，将历史图上的长程依赖融入到学习时态动态的过程中至关重要。本文提出了一个带有不确定性量化的图嵌入模型TransformerG2G，通过利用先进的Transformer编码器从当前状态（$t$）和之前的上下文（时间戳[$t-1, t-l$]，$l$是上下文的长度）中首先学习中间节点表示。此外，我们采用两个投影层来生成每个节点的低维多元高斯分布，作为其潜在嵌入。

    Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node's latent embedding at ti
    
[^77]: 多模态时间融合变压器是良好的产品需求预测器

    Multimodal Temporal Fusion Transformers Are Good Product Demand Forecasters. (arXiv:2307.02578v1 [cs.LG])

    [http://arxiv.org/abs/2307.02578](http://arxiv.org/abs/2307.02578)

    本文提出了一种使用多模态信息进行产品需求预测的方法，通过融合图像和文本描述等多种信息，有效地解决了传统方法的缺点，并在大型实际数据集上取得了较好的预测效果。

    

    多模态需求预测旨在利用视觉、文本和上下文信息来预测产品需求。本文提出了一种使用卷积、基于图和基于变压器的体系结构进行多模态产品需求预测的方法。传统的需求预测方法依赖于历史需求、产品类别和其他上下文信息，如季节性和事件。然而，这些方法存在一些缺点，例如冷启动问题，使得在为特定产品提供足够的历史数据之前很难预测产品需求，以及它们无法妥善处理类别动态性。通过融合产品图像和文本描述等多模态信息，我们的体系结构旨在解决传统方法的缺点并超越它们。在一个大型实际数据集上进行的实验表明，所提出的方法有效地预测了各种产品的需求。

    Multimodal demand forecasting aims at predicting product demand utilizing visual, textual, and contextual information. This paper proposes a method for multimodal product demand forecasting using convolutional, graph-based, and transformer-based architectures. Traditional approaches to demand forecasting rely on historical demand, product categories, and additional contextual information such as seasonality and events. However, these approaches have several shortcomings, such as the cold start problem making it difficult to predict product demand until sufficient historical data is available for a particular product, and their inability to properly deal with category dynamics. By incorporating multimodal information, such as product images and textual descriptions, our architecture aims to address the shortcomings of traditional approaches and outperform them. The experiments conducted on a large real-world dataset show that the proposed approach effectively predicts demand for a wide 
    
[^78]: 现有的撒哈拉以南非洲农业土地覆盖地图的准确性有多高？

    How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?. (arXiv:2307.02575v1 [cs.LG])

    [http://arxiv.org/abs/2307.02575](http://arxiv.org/abs/2307.02575)

    本研究定量评估和比较了11个公开可用的土地覆盖地图，以确定它们在非洲农田分类和基于卫星地球观测的农业监测中的适用性。研究结果可帮助用户找到最适合其需求的地图，并鼓励未来工作改进地图的一致性和低精度区域的准确性。

    

    卫星地球观测（EO）可以提供经济实惠和及时的信息来评估作物状况和粮食生产。在非洲，这样的监测系统至关重要，因为那里存在粮食不安全和缺乏农业统计数据的问题。基于EO的监测系统需要准确的农田地图来提供有关农田的信息，但是缺乏数据来确定哪些可用的土地覆盖地图最准确地识别非洲国家的农田。本研究通过使用来自8个国家的统计严谨的参考数据，对11个公开可用的土地覆盖地图进行定量评估和比较，以评估它们在农田分类和基于EO的非洲农业监测中的适用性。我们希望本研究的结果可以帮助用户确定最适合他们需求的地图，并鼓励未来的工作集中解决地图之间的不一致性并提高低精度地区的准确性。

    Satellite Earth observations (EO) can provide affordable and timely information for assessing crop conditions and food production. Such monitoring systems are essential in Africa, where there is high food insecurity and sparse agricultural statistics. EO-based monitoring systems require accurate cropland maps to provide information about croplands, but there is a lack of data to determine which of the many available land cover maps most accurately identify cropland in African countries. This study provides a quantitative evaluation and intercomparison of 11 publicly available land cover maps to assess their suitability for cropland classification and EO-based agriculture monitoring in Africa using statistically rigorous reference datasets from 8 countries. We hope the results of this study will help users determine the most suitable map for their needs and encourage future work to focus on resolving inconsistencies between maps and improving accuracy in low-accuracy regions.
    
[^79]: 基于街景图像和OpenStreetMap的半监督学习方法用于自动建筑高度估计

    Semi-supervised Learning from Street-View Images and OpenStreetMap for Automatic Building Height Estimation. (arXiv:2307.02574v1 [cs.CV])

    [http://arxiv.org/abs/2307.02574](http://arxiv.org/abs/2307.02574)

    这项工作提出了一种基于街景图像和OpenStreetMap的半监督学习方法，用于自动估计建筑物的高度，并生成低成本的3D城市模型。

    

    准确的建筑高度估计是从大规模地理空间数据中自动推导出3D城市模型的关键，包括志愿地理信息（VGI）。然而，目前还缺乏基于低成本VGI数据的大规模建筑高度自动估计的解决方案。VGI数据平台的快速发展，特别是OpenStreetMap（OSM）和众包街景图像（SVI），为填补这一研究空白提供了有力的机会。在这项工作中，我们提出了一种基于Mapillary SVI和OSM数据的半监督学习（SSL）方法，用于生成低成本和开源的LoD1 3D城市模型。所提方法包括三个部分：首先，我们提出了一种SSL模式，可以在监督回归过程中设置不同比例的“伪标签”；其次，我们从OSM数据（即建筑物和街道）中提取多级形态学特征，用于推断建筑物高度；

    Accurate building height estimation is key to the automatic derivation of 3D city models from emerging big geospatial data, including Volunteered Geographical Information (VGI). However, an automatic solution for large-scale building height estimation based on low-cost VGI data is currently missing. The fast development of VGI data platforms, especially OpenStreetMap (OSM) and crowdsourced street-view images (SVI), offers a stimulating opportunity to fill this research gap. In this work, we propose a semi-supervised learning (SSL) method of automatically estimating building height from Mapillary SVI and OSM data to generate low-cost and open-source 3D city modeling in LoD1. The proposed method consists of three parts: first, we propose an SSL schema with the option of setting a different ratio of "pseudo label" during the supervised regression; second, we extract multi-level morphometric features from OSM data (i.e., buildings and streets) for the purposed of inferring building height;
    
[^80]: 基于条件Korhunen-Lo\'{e}ve回归模型的基准适应方法用于高维问题：不确定性量化和逆建模

    Conditional Korhunen-Lo\'{e}ve regression model with Basis Adaptation for high-dimensional problems: uncertainty quantification and inverse modeling. (arXiv:2307.02572v1 [cs.LG])

    [http://arxiv.org/abs/2307.02572](http://arxiv.org/abs/2307.02572)

    本研究提出了一种改进高维问题中代理模型准确性的方法，通过使用条件Korhunen-Lo\'{e}ve展开来表示参数场，并利用高斯过程回归实现对直接测量的建模。

    

    我们提出了一种方法，用于改进物理系统的可观察响应的代理模型的准确性，这些物理系统的响应是其空间异质参数场的函数，应用于高维问题中的不确定性量化和参数估计。当参数场的直接测量可用时，我们提出通过使用条件Korhunen-Lo\'{e}ve展开（CKLEs）来表示参数场，从而提高这些代理模型的准确性。CKLEs通过使用高斯过程回归来通过条件展开的协方差核对直接测量进行建模。

    We propose a methodology for improving the accuracy of surrogate models of the observable response of physical systems as a function of the systems' spatially heterogeneous parameter fields with applications to uncertainty quantification and parameter estimation in high-dimensional problems. Practitioners often formulate finite-dimensional representations of spatially heterogeneous parameter fields using truncated unconditional Karhunen-Lo\'{e}ve expansions (KLEs) for a certain choice of unconditional covariance kernel and construct surrogate models of the observable response with respect to the random variables in the KLE. When direct measurements of the parameter fields are available, we propose improving the accuracy of these surrogate models by representing the parameter fields via conditional Karhunen-Lo\'{e}ve expansions (CKLEs). CKLEs are constructed by conditioning the covariance kernel of the unconditional expansion on the direct measurements via Gaussian process regression an
    
[^81]: 模型错误下的条件独立性检验

    Conditional independence testing under model misspecification. (arXiv:2307.02520v1 [stat.ML])

    [http://arxiv.org/abs/2307.02520](http://arxiv.org/abs/2307.02520)

    该论文研究了模型错误下的条件独立性检验，在这种情况下提出了新的近似或上界来衡量基于回归的测试的测试误差，并引入了一种新颖的基于回归的CI检验方法RBPT，对模型错误具有鲁棒性。

    

    条件独立性（CI）检验是现代统计学和机器学习中基础且具有挑战性的问题。许多现代的CI检验方法依赖于强大的监督学习方法来学习回归函数或贝叶斯预测器作为中间步骤。尽管这些方法在监督学习方法准确估计回归函数或贝叶斯预测器时保证了控制第一类错误，但它们在模型错误导致失败时的行为尚不清楚。从更广泛的意义上讲，即使使用了通用逼近器（例如深度神经网络），模型错误也可能出现。因此，我们研究了在模型错误下的基于回归的CI检验的性能。具体地，我们提出了新的近似或上界来衡量依赖于错误的三个基于回归的测试的测试误差。此外，我们引入了Rao-Blackwellized Predictor Test（RBPT），这是一种新颖的基于回归的CI检验，对模型错误具有鲁棒性。

    Conditional independence (CI) testing is fundamental and challenging in modern statistics and machine learning. Many modern methods for CI testing rely on powerful supervised learning methods to learn regression functions or Bayes predictors as an intermediate step. Although the methods are guaranteed to control Type-I error when the supervised learning methods accurately estimate the regression functions or Bayes predictors, their behavior is less understood when they fail due to model misspecification. In a broader sense, model misspecification can arise even when universal approximators (e.g., deep neural nets) are employed. Then, we study the performance of regression-based CI tests under model misspecification. Namely, we propose new approximations or upper bounds for the testing errors of three regression-based tests that depend on misspecification errors. Moreover, we introduce the Rao-Blackwellized Predictor Test (RBPT), a novel regression-based CI test robust against model mis
    
[^82]: 探索新的方法：强化表征差异以学习新特征并降低错误一致性

    Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency. (arXiv:2307.02516v1 [cs.LG])

    [http://arxiv.org/abs/2307.02516](http://arxiv.org/abs/2307.02516)

    本文提出了一种新方法，利用表征差异性来降低模型的相关性和常见失败模式。通过使架构之间不同深度的中间表示具有差异性，以学习具有不同失败模式的强大集合，结果表明，这种方法可以提高集合的准确性。

    

    独立训练的机器学习模型往往学习相似的特征。在一组独立训练的模型中，这导致预测相关性和常见的失败模式。以往的尝试着重于减小输出预测或logits的相关性，结果产生了不一致的优化目标，从而降低了模型准确性。在本文中，我们提出了一种新颖的思想，即利用表征相似性领域的方法，在训练过程中促进差异性，而不是衡量训练模型的相似性。为此，我们促进了架构之间不同深度的中间表示的差异性，并旨在学习具有不同失败模式的强大集合。我们表明，高度差异的中间表示导致更少相关的输出预测和稍微较低的错误一致性，从而提高了整体准确性。通过这样做，我们首次揭示了连接的新可能性。

    Independently trained machine learning models tend to learn similar features. Given an ensemble of independently trained models, this results in correlated predictions and common failure modes. Previous attempts focusing on decorrelation of output predictions or logits yielded mixed results, particularly due to their reduction in model accuracy caused by conflicting optimization objectives. In this paper, we propose the novel idea of utilizing methods of the representational similarity field to promote dissimilarity during training instead of measuring similarity of trained models. To this end, we promote intermediate representations to be dissimilar at different depths between architectures, with the goal of learning robust ensembles with disjoint failure modes. We show that highly dissimilar intermediate representations result in less correlated output predictions and slightly lower error consistency, resulting in higher ensemble accuracy. With this, we shine first light on the conne
    
[^83]: 用于计算设计的扩散模型在楼层平面示例中的应用

    Diffusion Models for Computational Design at the Example of Floor Plans. (arXiv:2307.02511v1 [cs.LG])

    [http://arxiv.org/abs/2307.02511](http://arxiv.org/abs/2307.02511)

    该论文探索了基于扩散模型的AI生成器在计算设计中的能力，并提出了具有改进的语义编码的新扩散模型。利用这些模型，可以提高生成楼层平面的有效性，并改进不同示例的查询性能。该研究还探讨了将扩散模型与建筑信息模型相结合的方法。

    

    最近，基于扩散模型的AI图像生成器因其能够根据简单的文本提示创建图像而受到广泛讨论。但是，在土木工程的实际应用中，它们需要能够根据给定的约束条件创建特定的建筑设计方案。在本文中，我们以楼层平面作为示例，探索基于扩散的AI生成器在计算设计中的能力，并确定它们目前的限制。我们解释了扩散模型的工作原理，并提出了具有改进的语义编码的新扩散模型。通过多次实验，我们展示了我们可以将生成的楼层平面的有效性从6%提高到90%，并改进了不同示例的查询性能。我们发现了一些问题，并针对这些模型提出了未来的研究挑战，并讨论了将扩散模型与建筑信息模型相结合的需要。通过这些，我们为土木工程中扩散模型的当前状态和未来方向提供了关键见解。

    AI Image generators based on diffusion models are widely discussed recently for their capability to create images from simple text prompts. But, for practical use in civil engineering they need to be able to create specific construction plans for given constraints. Within this paper we explore the capabilities of those diffusion-based AI generators for computational design at the example of floor plans and identify their current limitation. We explain how the diffusion-models work and propose new diffusion models with improved semantic encoding. In several experiments we show that we can improve validity of generated floor plans from 6% to 90% and query performance for different examples. We identify short comings and derive future research challenges of those models and discuss the need to combine diffusion models with building information modelling. With this we provide key insights into the current state and future directions for diffusion models in civil engineering.
    
[^84]: Wasserstein自编码合并树（和持续图）

    Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams). (arXiv:2307.02509v1 [cs.LG])

    [http://arxiv.org/abs/2307.02509](http://arxiv.org/abs/2307.02509)

    本文提出了一种计算框架，用于将传统的自编码神经网络扩展到合并树的Wasserstein度量空间。算法在准确性和可解释性方面表现出

    

    本文提出了一种计算框架，用于合并树的Wasserstein自编码(MT-WAE)，这是将传统的自编码神经网络架构扩展到合并树的Wasserstein度量空间的一种新颖方法。与操作向量化数据的传统自编码器不同，我们的公式在网络的每一层明确地操作合并树的关联度量空间，从而获得更高的准确性和可解释性。我们的新颖神经网络方法可以解释为前期线性尝试[65]的非线性推广。它也很容易扩展到持续图。对公共连锁反应的广泛实验表明我们算法的效率，MT-WAE的计算平均时间仅为几分钟。我们将我们的贡献的实用性通过两个应用来展示，这些应用是基于以前关于合并树编码的工作[65]进行调整得到的。首先，我们将MT-WAE应用于数据压缩，并可靠地压缩合并树。

    This paper presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts [65] at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding [65]. First, we apply MT-WAE to data reduction and reliably compress merge t
    
[^85]: STS-CCL：用于城市交通预测的时空同步上下文对比学习

    STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting. (arXiv:2307.02507v1 [cs.LG])

    [http://arxiv.org/abs/2307.02507](http://arxiv.org/abs/2307.02507)

    本研究通过引入先进的对比学习方法，提出了一种新颖的时空同步上下文对比学习（STS-CCL）模型，用于高效地捕捉大规模无标签交通数据的复杂时空表示。该模型通过使用动态图视图生成器和语义上下文对比方法，实现了节点级和图级的对比学习。

    

    高效地捕捉大规模无标签交通数据中复杂的时空表示仍然是一个具有挑战性的任务。鉴于这个困境，本文采用先进的对比学习方法，并提出了一种新颖的时空同步上下文对比学习（STS-CCL）模型。首先，我们详细阐述了用于时空图数据的基本和强大的增强方法，这些方法不仅扰动了图结构和时间特征的数据，而且还利用了基于学习的动态图视图生成器进行自适应增强。其次，我们引入了一种时空同步对比模块（STS-CM），以同时捕捉良好的时空依赖关系并实现图级对比。为了进一步区分负筛选中的节点个体，设计了一种基于语义特征和空间异质性的语义上下文对比方法，实现了节点级的对比学习以及…

    Efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains to be a challenging task. In considering of the dilemma, this work employs the advanced contrastive learning and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. Second, we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. To further discriminate node individuals in negative filtering, a Semantic Contextual Contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with
    
[^86]: 通过算法相关的Rademacher复杂度实现泛化保证

    Generalization Guarantees via Algorithm-dependent Rademacher Complexity. (arXiv:2307.02501v1 [stat.ML])

    [http://arxiv.org/abs/2307.02501](http://arxiv.org/abs/2307.02501)

    提出了一种通过算法和数据相关的假设类的经验Rademacher复杂度来控制泛化错误的方法，基于有限分形维度获得了新的界限，并简化了对随机梯度下降的无维度泛化界限的证明。

    

    现代机器学习算法的泛化行为需要算法和数据相关的泛化界限来解释。在这个背景下，存在着涉及(各种形式的)互信息的信息论泛化界限，以及基于假设集稳定性的界限。我们提出了一个在技术上不同但在概念上相关的复杂度度量，来控制泛化错误，即算法和数据相关的假设类的经验Rademacher复杂度。结合Rademacher复杂度的标准属性和该类的便捷结构，我们能够：(i)基于有限分形维度获得新的界限，这些界限将前人工作中的分形维度界限从连续的假设类推广到有限假设类，并且避免了之前工作中需要的互信息项；(ii)大大简化了最近提出的针对随机梯度下降的无维度泛化界限的证明。

    Algorithm- and data-dependent generalization bounds are required to explain the generalization behavior of modern machine learning algorithms. In this context, there exists information theoretic generalization bounds that involve (various forms of) mutual information, as well as bounds based on hypothesis set stability. We propose a conceptually related, but technically distinct complexity measure to control generalization error, which is the empirical Rademacher complexity of an algorithm- and data-dependent hypothesis class. Combining standard properties of Rademacher complexity with the convenient structure of this class, we are able to (i) obtain novel bounds based on the finite fractal dimension, which (a) extend previous fractal dimension-type bounds from continuous to finite hypothesis classes, and (b) avoid a mutual information term that was required in prior work; (ii) we greatly simplify the proof of a recent dimension-independent generalization bound for stochastic gradient 
    
[^87]: 多尺度水文变分数据同化：使用多层感知器和贝叶斯引导的多元回归进行区域化学习

    Multi-gauge Hydrological Variational Data Assimilation: Regionalization Learning with Spatial Gradients using Multilayer Perceptron and Bayesian-Guided Multivariate Regression. (arXiv:2307.02497v1 [cs.LG])

    [http://arxiv.org/abs/2307.02497](http://arxiv.org/abs/2307.02497)

    本研究提出了一种新的水文数据同化方法，通过使用多层感知器和贝叶斯引导的多元回归，实现了无缝区域化学习。这种方法可以准确估计空间分布的水文参数，并且解决了可行解的等似性问题。

    

    本研究针对估计空间分布的水文参数，特别是针对未测量水道上的洪水问题，提出了一个新颖的无缝区域化技术，用于学习高分辨率水文模型的复杂区域转移函数。该转移函数依赖于：（i）一个多层感知器，通过无缝的梯度计算流动来使用机器学习优化算法，或者（ii）一个多元回归，由变分数据同化算法优化并由贝叶斯估计引导，解决可行解的等似性问题。该方法涉及将可推断的区域化映射纳入到可微分的水文模型中，并利用精确的基于伴随的空间分布梯度计算的多测量数据上计算的代价函数进行优化。

    Tackling the difficult problem of estimating spatially distributed hydrological parameters, especially for floods on ungauged watercourses, this contribution presents a novel seamless regionalization technique for learning complex regional transfer functions designed for high-resolution hydrological models. The transfer functions rely on: (i) a multilayer perceptron enabling a seamless flow of gradient computation to employ machine learning optimization algorithms, or (ii) a multivariate regression mapping optimized by variational data assimilation algorithms and guided by Bayesian estimation, addressing the equifinality issue of feasible solutions. The approach involves incorporating the inferable regionalization mappings into a differentiable hydrological model and optimizing a cost function computed on multi-gauge data with accurate adjoint-based spatially distributed gradients.
    
[^88]: 使用可逆神经网络和误差扩散学习利用导电图重建气泡分布

    Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion. (arXiv:2307.02496v1 [eess.IV])

    [http://arxiv.org/abs/2307.02496](http://arxiv.org/abs/2307.02496)

    本研究利用可逆神经网络和误差扩散方法，通过测量气泡引起的磁场波动，重建电解过程中的气泡分布和电导率图，并实现了比传统方法更优异的性能。

    

    电解是环保的氢气生产过程中的关键，但是过程中产生的气泡会阻碍反应，降低电池效率，增加能量消耗。此外，这些气泡会导致电池内部的电导率发生变化，从而导致周围产生感应磁场的变化。因此，利用外部磁传感器测量这些气泡引起的磁场波动，并求解Biot-Savart定律的反问题，可以估计电池内的电导率，从而得到气泡的大小和位置。然而，仅凭几个感应磁场测量值确定高分辨率电导率图是一个病态反问题。为了克服这个问题，我们利用可逆神经网络（INN）重建电导率场。我们的定性结果和使用随机误差扩散的定量评估表明，与Tikhonov正则化相比，INN具有更优异的性能。

    Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles generated during the process hinder reactions, reduce cell efficiency, and increase energy consumption. Additionally, these gas bubbles cause changes in the conductivity inside the cell, resulting in corresponding variations in the induced magnetic field around the cell. Therefore, measuring these gas bubble-induced magnetic field fluctuations using external magnetic sensors and solving the inverse problem of Biot-Savart Law allows for estimating the conductivity in the cell and, thus, bubble size and location. However, determining high-resolution conductivity maps from only a few induced magnetic field measurements is an ill-posed inverse problem. To overcome this, we exploit Invertible Neural Networks (INNs) to reconstruct the conductivity field. Our qualitative results and quantitative evaluation using random error diffusion show that INN achieves far superior performance compared to Tikhonov regularizatio
    
[^89]: FREEDOM: 无目标标签、无源数据和无领域信息的多源领域自适应无监督个性化方法

    FREEDOM: Target Label & Source Data & Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization. (arXiv:2307.02493v1 [cs.LG])

    [http://arxiv.org/abs/2307.02493](http://arxiv.org/abs/2307.02493)

    本文提出了一种新的问题场景TFDA，即三无领域自适应，解决了多源领域自适应中目标标签、源数据和领域信息不可用的问题。这种方法更加实用，避免了对先前领域信息的依赖和数据隐私问题。

    

    从服务角度来看，多源领域自适应（MSDA）是一种适应部署模型到客户数据集的有希望的场景。它可以在没有目标标签的情况下提供适应，并支持多个领域构建的源数据集的情况。然而，这种方法依赖于先前的多源数据集的领域信息，即有多少个领域存在以及每个数据样本的领域标签。此外，MSDA需要同时拥有源数据集和目标数据集（物理上），这会导致客户设备存储限制或通过将客户数据传输到服务器引起数据隐私问题。为了更实际的模型自适应场景，我们放宽了这些限制，提出了一种新的问题场景，即三无领域自适应（TFDA），其中1）目标标签，2）源数据集，以及大部分3）源领域信息（领域标签+领域数量）都不可用。

    From a service perspective, Multi-Source Domain Adaptation (MSDA) is a promising scenario to adapt a deployed model to a client's dataset. It can provide adaptation without a target label and support the case where a source dataset is constructed from multiple domains. However, it is impractical, wherein its training heavily relies on prior domain information of the multi-source dataset -- how many domains exist and the domain label of each data sample. Moreover, MSDA requires both source and target datasets simultaneously (physically), causing storage limitations on the client device or data privacy issues by transferring client data to a server. For a more practical scenario of model adaptation from a service provider's point of view, we relax these constraints and present a novel problem scenario of Three-Free Domain Adaptation, namely TFDA, where 1) target labels, 2) source dataset, and mostly 3) source domain information (domain labels + the number of domains) are unavailable. Und
    
[^90]: TablEye: 通过图像视角看小表格

    TablEye: Seeing small Tables through the Lens of Images. (arXiv:2307.02491v1 [cs.LG])

    [http://arxiv.org/abs/2307.02491](http://arxiv.org/abs/2307.02491)

    本文提出了一种创新的框架TablEye，通过生成表格图像来实现领域转换，克服了形成表格数据先验知识的限制，从而实现了少样本表格学习。

    

    少样本表格学习的探索变得迫切。表格数据是一种多功能的表示形式，可以捕捉多样的信息，但也存在数据和模型大小的限制。标记大量的表格数据可能具有挑战性，并且可能不可行捕捉每一个重要特征。然而，少样本表格学习相对未被充分探索，主要是由于独立数据集中共享信息的稀缺性以及定义表格数据边界的内在模糊性。据我们所知，目前尚未开发出无限制条件的有意义且可行的少样本表格学习技术。在本文中，我们提出了一种创新的框架TablEye，通过采用领域转换来克服形成表格数据先验知识的限制。它通过生成表格图像来实现领域转换，从而有效地保留了内在的语义。

    The exploration of few-shot tabular learning becomes imperative. Tabular data is a versatile representation that captures diverse information, yet it is not exempt from limitations, property of data and model size. Labeling extensive tabular data can be challenging, and it may not be feasible to capture every important feature. Few-shot tabular learning, however, remains relatively unexplored, primarily due to scarcity of shared information among independent datasets and the inherent ambiguity in defining boundaries within tabular data. To the best of our knowledge, no meaningful and unrestricted few-shot tabular learning techniques have been developed without imposing constraints on the dataset. In this paper, we propose an innovative framework called TablEye, which aims to overcome the limit of forming prior knowledge for tabular data by adopting domain transformation. It facilitates domain transformation by generating tabular images, which effectively conserve the intrinsic semantic
    
[^91]: 基于动态等色性的严格公平神经架构搜索

    Dynamical Isometry based Rigorous Fair Neural Architecture Search. (arXiv:2307.02263v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02263](http://arxiv.org/abs/2307.02263)

    本论文提出了一种基于动态等色性的神经架构搜索算法，通过分析稳态随机神经网络的动力学行为和使用良好条件的Jacobian来保证权重共享的公平性，实验证明所提出的算法搜索到的架构具有更好的性能。

    

    最近，权重共享技术显著加快了神经架构搜索的训练和评估过程。然而，大多数现有的权重共享策略仅基于经验或观察，导致搜索结果缺乏可解释性和合理性。此外，由于对公平性的忽视，当前方法在模块评估中容易做出错误判断。为了解决这些问题，我们提出了一种基于动态等色性的新型神经架构搜索算法。我们使用均场理论中的固定点分析方法来分析稳态随机神经网络的动力学行为，以及动态等色性如何保证基于权重共享的NAS的公平性。同时，我们通过估计所有模块的良好条件的Jacobian的泛化误差证明了我们的模块选择策略是严格公平的。大量实验表明，与相同大小的架构相比，我们提出的算法搜索到的架构具有更好的性能。

    Recently, the weight-sharing technique has significantly speeded up the training and evaluation procedure of neural architecture search. However, most existing weight-sharing strategies are solely based on experience or observation, which makes the searching results lack interpretability and rationality. In addition, due to the negligence of fairness, current methods are prone to make misjudgments in module evaluation. To address these problems, we propose a novel neural architecture search algorithm based on dynamical isometry. We use the fix point analysis method in the mean field theory to analyze the dynamics behavior in the steady state random neural network, and how dynamic isometry guarantees the fairness of weight-sharing based NAS. Meanwhile, we prove that our module selection strategy is rigorous fair by estimating the generalization error of all modules with well-conditioned Jacobian. Extensive experiments show that, with the same size, the architecture searched by the propo
    
[^92]: 符合目标：使用通用的插入式框架在CTC模型中优化所需属性

    Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])

    [http://arxiv.org/abs/2307.01715](http://arxiv.org/abs/2307.01715)

    本文提出了一个通用的插入式框架，用于优化CTC模型中的所需属性。该框架通过补充额外的损失项来优先考虑符合所需属性的对齐，并不需要修改CTC损失函数。

    

    连接主义时间分类（CTC）是训练监督序列到序列模型广泛使用的准则。它通过将完美对齐（产生基本事实）的边际化来学习输入和输出序列之间的关系，称为对其，以代价不完美对齐。这种对完美和不完美对齐的二元区分无法捕捉到在其他实际应用中具有重要意义的其他关键对齐属性。在这里，我们提出了$\textit{Align With Purpose}$，这是一个用于增强CTC条件下训练模型中所需属性的$\textbf{通用插入式框架}$。我们通过使用额外的损失项来补充CTC来优先考虑符合所需属性的对齐。我们的方法不需要干预CTC损失函数，能够轻松优化各种属性，并且可以区分完美和不完美的对齐。

    Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
    
[^93]: 在智能家居环境中基于循环趋势预测神经网络的可再生能源管理

    Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network. (arXiv:2307.01622v1 [cs.LG])

    [http://arxiv.org/abs/2307.01622](http://arxiv.org/abs/2307.01622)

    本研究提出了一种基于循环趋势预测神经网络的嵌入式调度预测算法，在智能家居环境中实现了高效的住宅需求控制，并同时预测可再生能源发电。该算法具有鲁棒性，能够应对预测误差。

    

    智能家居能源管理系统能够帮助配电网络更加高效和可靠地运行，并有效地整合分布式可再生能源。这些系统依赖于强大的预测、优化和控制/调度算法，能够处理需求和可再生能源发电的不确定性。本文提出了一种先进的机器学习算法，称为基于循环趋势预测神经网络的嵌入式调度预测（rTPNN-FES），用于提供高效的住宅需求控制。rTPNN-FES是一种新颖的神经网络架构，可以同时预测可再生能源发电并安排家电的使用时间。通过其嵌入式结构，rTPNN-FES消除了使用单独算法进行预测和调度的需要，并生成对预测误差具有鲁棒性的调度安排。本文还评估了该算法在物联网智能家居中的性能。评估结果显示，rTPNN-FES能够有效应对预测误差。

    Smart home energy management systems help the distribution grid operate more efficiently and reliably, and enable effective penetration of distributed renewable energy sources. These systems rely on robust forecasting, optimization, and control/scheduling algorithms that can handle the uncertain nature of demand and renewable generation. This paper proposes an advanced ML algorithm, called Recurrent Trend Predictive Neural Network based Forecast Embedded Scheduling (rTPNN-FES), to provide efficient residential demand control. rTPNN-FES is a novel neural network architecture that simultaneously forecasts renewable energy generation and schedules household appliances. By its embedded structure, rTPNN-FES eliminates the utilization of separate algorithms for forecasting and scheduling and generates a schedule that is robust against forecasting errors. This paper also evaluates the performance of the proposed algorithm for an IoT-enabled smart home. The evaluation results reveal that rTPNN
    
[^94]: 基于密度的可行性学习与归一化流在自省式机器人装配中的应用

    Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly. (arXiv:2307.01317v1 [cs.RO])

    [http://arxiv.org/abs/2307.01317](http://arxiv.org/abs/2307.01317)

    本文提出了一种基于密度的可行性学习方法，使用归一化流进行自省式机器人装配。该方法只需要可行的示例来训练，可以更好地检测不可行的装配方案。

    

    机器学习模型在机器人装配序列规划中需要对预测的解决方案进行自省，即判断其可行性，以避免潜在的效率降低。之前的研究在训练时需要可行和不可行的示例。然而，当需要重新训练以适应新的产品变体时，收集足够的不可行示例是困难的。在这项工作中，我们提出了一种只需要可行示例的基于密度的可行性学习方法。具体地，我们将可行性学习问题形式化为使用归一化流进行分布区分，归一化流是用于估计复杂概率分布的强大生成模型。实验结果表明，所提出的方法在机器人装配案例中表现优于其他单分类基线模型，能够检测出不可行的装配方案。我们进一步研究了该方法的内部工作机理，并展示了一个大规模的记忆机制。

    Machine Learning (ML) models in Robotic Assembly Sequence Planning (RASP) need to be introspective on the predicted solutions, i.e. whether they are feasible or not, to circumvent potential efficiency degradation. Previous works need both feasible and infeasible examples during training. However, the infeasible ones are hard to collect sufficiently when re-training is required for swift adaptation to new product variants. In this work, we propose a density-based feasibility learning method that requires only feasible examples. Concretely, we formulate the feasibility learning problem as Out-of-Distribution (OOD) detection with Normalizing Flows (NF), which are powerful generative models for estimating complex probability distributions. Empirically, the proposed method is demonstrated on robotic assembly use cases and outperforms other single-class baselines in detecting infeasible assemblies. We further investigate the internal working mechanism of our method and show that a large memo
    
[^95]: REAL:一种基于代表性错误驱动的主动学习方法

    REAL: A Representative Error-Driven Approach for Active Learning. (arXiv:2307.00968v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00968](http://arxiv.org/abs/2307.00968)

    本研究提出了一种名为REAL的新方法，该方法通过选择具有代表性错误的数据实例来改进主动学习。通过考虑错误实例及其邻域中的错误密度，REAL在准确率和F1-macro得分方面优于其他算法。

    

    在拥有有限标记预算的情况下，主动学习旨在从未标记的样本中选择最具信息量的实例，以获取标签用于模型训练。为了实现这一目标，主动学习通常根据不确定性和多样性来衡量未标记实例的信息量。然而，它并不考虑具有邻域错误密度的错误实例，这些实例在提高模型性能方面具有巨大潜力。为了解决这个局限性，我们提出了一种名为REAL的新方法，该方法选择具有代表性错误的数据实例用于主动学习。它将少数预测作为聚类中的“伪错误”进行识别，并根据估计的错误密度为该聚类分配自适应的采样预算。在五个文本分类数据集上进行的大量实验证明，REAL在准确率和F1-macro得分方面始终优于所有表现最佳的基准算法。

    Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training. To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity. However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance. To address this limitation, we propose $REAL$, a novel approach to select data instances with $\underline{R}$epresentative $\underline{E}$rrors for $\underline{A}$ctive $\underline{L}$earning. It identifies minority predictions as \emph{pseudo errors} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets demonstrate that $REAL$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of
    
[^96]: UTRNet: 印刷文档中高分辨率乌尔都文本识别

    UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v1 [cs.CV])

    [http://arxiv.org/abs/2306.15782](http://arxiv.org/abs/2306.15782)

    本文提出了一种解决印刷乌尔都文本识别挑战的新方法，并引入了大规模实际标记数据集和合成数据集，提供了乌尔都文本行检测的基准数据集，同时开发了一个在线工具，实现了印刷文档中乌尔都OCR的端到端识别。

    

    本文提出了一种新颖方法来解决印刷乌尔都文本识别的挑战，使用高分辨率、多尺度的语义特征提取。我们提出的UTRNet架构，一个混合CNN-RNN模型，在基准数据集上展示了最先进的性能。为了解决以前工作的局限性，这些工作很难推广到乌尔都文本的复杂性和缺乏足够的实际标记数据，我们引入了UTRSet-Real，一个包含超过11,000行的大规模实际标记数据集和UTRSet-Synth，一个与实际世界非常相似的含有20,000行的合成数据集，并对现有的IIITH数据集的基准真实性进行了修正，使其成为未来研究的更可靠的资源。我们还提供了UrduDoc，一种用于扫描文档中乌尔都文本行检测的基准数据集。此外，我们还开发了一种在线工具，通过将UTRNet与文本的端到端乌尔都OCR集成在印刷文档中。

    In this paper, we propose a novel approach to address the challenges of printed Urdu text recognition using high-resolution, multi-scale semantic feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model, demonstrates state-of-the-art performance on benchmark datasets. To address the limitations of previous works, which struggle to generalize to the intricacies of the Urdu script and the lack of sufficient annotated real-world data, we have introduced the UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000 lines closely resembling real-world and made corrections to the ground truth of the existing IIITH dataset, making it a more reliable resource for future research. We also provide UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. Additionally, we have developed an online tool for end-to-end Urdu OCR from printed documents by integrating UTRNet with a tex
    
[^97]: 平衡过滤使用非泄露代理

    Balanced Filtering via Non-Disclosive Proxies. (arXiv:2306.15083v1 [cs.LG])

    [http://arxiv.org/abs/2306.15083](http://arxiv.org/abs/2306.15083)

    本文研究了在群组成员资格不可用或被禁止使用时，如何以非泄露方式收集平衡的数据样本。通过使用代理函数和抽样概率，实现了对个体样本的分类和选择，同时保护个体样本的敏感群组成员资格不被泄露。

    

    当群组成员资格在收集时不可用或被禁止使用时，我们研究了非泄露方式收集与敏感群组平衡的数据样本的问题。具体而言，我们的收集机制不会比基本比率能够确定的任何个体样本的群组成员资格更多地透露相关信息。为了做到这一点，我们采用了公平流程的观点，即学习者可以使用少量的标记数据训练代理函数，这个代理函数以后可以用于这个过滤任务。然后，我们将代理函数的范围与抽样概率相关联；给定一个新的候选样本，我们使用代理函数对其进行分类，然后根据与其代理分类对应的抽样概率选择它作为我们的样本。重要的是，我们要求代理分类本身不会透露任何个体样本的敏感群组成员资格的重要信息（即，它应该是非泄露的）。

    We study the problem of non-disclosively collecting a sample of data that is balanced with respect to sensitive groups when group membership is unavailable or prohibited from use at collection time. Specifically, our collection mechanism does not reveal significantly more about group membership of any individual sample than can be ascertained from base rates alone. To do this, we adopt a fairness pipeline perspective, in which a learner can use a small set of labeled data to train a proxy function that can later be used for this filtering task. We then associate the range of the proxy function with sampling probabilities; given a new candidate, we classify it using our proxy function, and then select it for our sample with probability proportional to the sampling probability corresponding to its proxy classification. Importantly, we require that the proxy classification itself not reveal significant information about the sensitive group membership of any individual sample (i.e., it sho
    
[^98]: 为向量搜索进行硬件和算法的共同设计

    Co-design Hardware and Algorithm for Vector Search. (arXiv:2306.11182v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11182](http://arxiv.org/abs/2306.11182)

    本论文提出了一个在FPGA上的向量搜索框架FANNS，实现了硬件和算法的共同设计，可以根据用户需求和硬件预算生成相应的加速器。与FPGA和CPU基准相比，FANNS实现了显著的加速，并展现了卓越的可扩展性。

    

    向量搜索已成为大规模信息检索和机器学习系统的基础，像Google和Bing这样的搜索引擎通过评估编码查询文本和网络文档之间的向量相似度，每秒处理数万个查询，在拥有PB级文档数据集的情况下。随着对向量搜索系统性能的需求激增，在摩尔定律时代后，加速硬件成为了一个有前景的解决方案。我们介绍了一个在FPGA上的端到端可扩展向量搜索框架FANNS。给定用户提供的对数据集的召回要求和硬件资源预算，FANNS自动进行硬件和算法的共同设计，随后生成相应的加速器。该框架还通过在加速器中引入硬件TCP/IP堆栈来支持规模扩展。与FPGA和CPU基准相比，FANNS分别实现了23.0倍和37.2倍的加速，并展现了卓越的可扩展性。

    Vector search has emerged as the foundation for large-scale information retrieval and machine learning systems, with search engines like Google and Bing processing tens of thousands of queries per second on petabyte-scale document datasets by evaluating vector similarities between encoded query texts and web documents. As performance demands for vector search systems surge, accelerated hardware offers a promising solution in the post-Moore's Law era. We introduce \textit{FANNS}, an end-to-end and scalable vector search framework on FPGAs. Given a user-provided recall requirement on a dataset and a hardware resource budget, \textit{FANNS} automatically co-designs hardware and algorithm, subsequently generating the corresponding accelerator. The framework also supports scale-out by incorporating a hardware TCP/IP stack in the accelerator. \textit{FANNS} attains up to 23.0$\times$ and 37.2$\times$ speedup compared to FPGA and CPU baselines, respectively, and demonstrates superior scalabil
    
[^99]: 多黑盒预言下的主动策略改进

    Active Policy Improvement from Multiple Black-box Oracles. (arXiv:2306.10259v1 [cs.LG])

    [http://arxiv.org/abs/2306.10259](http://arxiv.org/abs/2306.10259)

    本研究提出了MAPS和MAPS-SE两个算法，可在多黑盒预言情况下，采用模仿学习并主动选择和改进最优预言，显著提升了性能。

    

    强化学习在各种复杂领域中取得了重大进展，但是通过强化学习确定有效策略往往需要进行广泛的探索，而模仿学习旨在通过使用专家演示来指导探索，缓解这个问题。在真实世界情境下，人们通常只能接触到多个次优的黑盒预言，而不是单个最优的预言，这些预言不能在所有状态下普遍优于彼此，这给主动决定在哪种状态下使用哪种预言以及如何改进各自估计值函数提出了挑战。本文介绍了一个可行的解决方案，即MAPS和MAPS-SE算法。

    Reinforcement learning (RL) has made significant strides in various complex domains. However, identifying an effective policy via RL often necessitates extensive exploration. Imitation learning aims to mitigate this issue by using expert demonstrations to guide exploration. In real-world scenarios, one often has access to multiple suboptimal black-box experts, rather than a single optimal oracle. These experts do not universally outperform each other across all states, presenting a challenge in actively deciding which oracle to use and in which state. We introduce MAPS and MAPS-SE, a class of policy improvement algorithms that perform imitation learning from multiple suboptimal oracles. In particular, MAPS actively selects which of the oracles to imitate and improve their value function estimates, and MAPS-SE additionally leverages an active state exploration criterion to determine which states one should explore. We provide a comprehensive theoretical analysis and demonstrate that MAP
    
[^100]: 通过Wasserstein Barycenters实现多任务学习中的公平性

    Fairness in Multi-Task Learning via Wasserstein Barycenters. (arXiv:2306.10155v1 [stat.ML])

    [http://arxiv.org/abs/2306.10155](http://arxiv.org/abs/2306.10155)

    本文提出了一种方法，通过多元Wasserstein barycenters扩展`Strong Demographic Parity`的定义，实现多任务学习中的公平性，包括回归和二元分类任务。在实验中表现出良好的效果。

    

    算法公平性是机器学习中的一个已经成熟的领域，旨在减少数据中的偏差。最近的进展提出了各种方法来确保单变量环境下的公平性，即目标是去除单个任务的偏差。然而，将公平性扩展到多任务环境，其中使用共享表示来优化多个目标，仍未得到充分开发。为了填补这一差距，我们利用多元Wasserstein barycenters将\textit{Strong Demographic Parity}的定义扩展到多任务学习中。我们的方法为最优的公平多任务预测器提供了封闭式解，包括回归和二元分类任务。我们开发了一种数据驱动的估计过程，以寻找解决方案，并在合成和实际数据集上运行数字实验。经验结果突显了我们的后处理方法在促进公平决策方面的实际价值。

    Algorithmic Fairness is an established field in machine learning that aims to reduce biases in data. Recent advances have proposed various methods to ensure fairness in a univariate environment, where the goal is to de-bias a single task. However, extending fairness to a multi-task setting, where more than one objective is optimised using a shared representation, remains underexplored. To bridge this gap, we develop a method that extends the definition of \textit{Strong Demographic Parity} to multi-task learning using multi-marginal Wasserstein barycenters. Our approach provides a closed form solution for the optimal fair multi-task predictor including both regression and binary classification tasks. We develop a data-driven estimation procedure for the solution and run numerical experiments on both synthetic and real datasets. The empirical results highlight the practical value of our post-processing methodology in promoting fair decision-making.
    
[^101]: 使用超参数调优模型的堆叠方法解决编码问题的翻译

    Stacking of Hyperparameter Tuned Models for Tagging Coding Problems. (arXiv:2306.10077v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10077](http://arxiv.org/abs/2306.10077)

    这项工作使用超参数调优的增强模型堆叠方法，在Codeforces和Leetcode的数据集上取得了77.8%的准确率和0.815的PR-AUC分数。

    

    编码问题是需要以计算机程序形式提供解决方案的问题。编码问题在学生和专业人士中非常受欢迎，因为它可以提升他们的技能和职业机会。一个能够帮助练习编码问题的AI系统将非常有用，并且存在巨大的潜力。在这项工作中，我们提出了一种采用超参数调优的增强模型堆叠方法，以在从Codeforces和Leetcode获取的数据集上达到77.8％的准确率和0.815的PR-AUC分数。我们开源了为这项工作开发的数据集和模型。

    Coding problems are problems that require a solution in the form of a computer program. Coding problems are popular among students and professionals as it enhances their skills and career opportunities. An AI system that would help those who practice coding problems would be highly useful and there is a huge potential for such a system. In this work, we propose a model which uses stacking of hyperparameter tuned boosting models to achieve impressive metric scores of 77.8% accuracy and 0.815 PR-AUC on the dataset that was scraped from Codeforces and Leetcode. We open source the dataset and the models developed for this work.
    
[^102]: ChatGPT和LLMs对医学影像利益相关者的影响：观点和应用案例

    The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases. (arXiv:2306.06767v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2306.06767](http://arxiv.org/abs/2306.06767)

    本研究调查了ChatGPT和LLMs在医学影像领域的变革潜力，它们正在增强放射科医生的解释能力、提升患者与医生之间的沟通，以及简化临床工作流程。

    

    本研究调查了大型语言模型（LLMs）如OpenAI ChatGPT在医学影像领域的变革潜力。借助公共数据，这些模型具有卓越的语言理解和生成能力，正在增强放射科医生的解释能力，提升患者与医生之间的沟通，并简化临床工作流程。本文介绍了一个分析框架，用于展示LLMs与医学影像利益相关者之间的复杂互动，包括企业、保险机构、政府、研究机构和医院（被称为BIGR-H）。通过详细分析、示例应用案例以及对广泛影响和未来方向的讨论，本文旨在提高在AI驱动医疗保健时代的战略规划和决策制定中的讨论水平。

    This study investigates the transformative potential of Large Language Models (LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public data, these models, which possess remarkable language understanding and generation capabilities, are augmenting the interpretive skills of radiologists, enhancing patient-physician communication, and streamlining clinical workflows. The paper introduces an analytic framework for presenting the complex interactions between LLMs and the broader ecosystem of medical imaging stakeholders, including businesses, insurance entities, governments, research institutions, and hospitals (nicknamed BIGR-H). Through detailed analyses, illustrative use cases, and discussions on the broader implications and future directions, this perspective seeks to raise discussion in strategic planning and decision-making in the era of AI-enabled healthcare.
    
[^103]: 从稀疏观测数据中进行日预测的深度学习

    Deep Learning for Day Forecasts from Sparse Observations. (arXiv:2306.06079v1 [physics.ao-ph])

    [http://arxiv.org/abs/2306.06079](http://arxiv.org/abs/2306.06079)

    本文提出了一种基于稀疏观测数据的MetNet-3深度学习模型，可对20小时内的天气进行准确的预测。MetNet-3的技术创新包括可学习卷积、特征学习和多任务训练优化。此外，使用持续性启发法来外推初始条件或进行短期预测来填补缺失的观测数据更进一步提高了预测性能。

    

    深度神经网络提供了一种建模天气条件的替代范例。神经模型在数据可用时以少于1秒的速度进行预测，并以非常高的时间和空间分辨率进行预测。它们可以直接从大气观测数据中进行学习，这是这些模型独特的优势之一。然而，迄今为止，仅仅预测降水这一唯一变量时，仅能使用大气观测数据来训练神经模型，才能达到与现有概率性数值天气预报模型相当的良好表现到12个小时的提前量。本文提出了MetNet-3，它显著扩展了基于观测数据的神经模型能够良好预测的引导时间范围和变量。MetNet-3从密集和稀疏的数据传感器中学习，并为降水、风、温度和露点进行24小时的预测。MetNet-3在体系结构层面引入了许多技术创新，这被证明对提高模型性能有所贡献，包括可学习的时空卷积、基于注意力的特征学习和多任务训练优化。此外，我们展示了将神经模型泛化为仅接受稀疏的气压计观测数据作为输入，并通过使用简单的持续性启发法来外推初始条件，或通过使用低分辨率数值模型进行短期预测来填补缺失的观测数据的方法是有益的。MetNet-3在降水、温度和露点预测方面比现有的大气模型在提前至24小时方面表现更好。

    Deep neural networks offer an alternative paradigm for modeling weather conditions. The ability of neural models to make a prediction in less than a second once the data is available and to do so with very high temporal and spatial resolution, and the ability to learn directly from atmospheric observations, are just some of these models' unique advantages. Neural models trained using atmospheric observations, the highest fidelity and lowest latency data, have to date achieved good performance only up to twelve hours of lead time when compared with state-of-the-art probabilistic Numerical Weather Prediction models and only for the sole variable of precipitation. In this paper, we present MetNet-3 that extends significantly both the lead time range and the variables that an observation based neural model can predict well. MetNet-3 learns from both dense and sparse data sensors and makes predictions up to 24 hours ahead for precipitation, wind, temperature and dew point. MetNet-3 introduc
    
[^104]: 在生物医学任务上评估ChatGPT：与精调生成式变压器的零样例比较。

    Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v1 [cs.CL])

    [http://arxiv.org/abs/2306.04504](http://arxiv.org/abs/2306.04504)

    本文评估了ChatGPT在生物医学任务上的表现，发现在生物数据集训练样本较小时，零样例ChatGPT甚至优于精调生成式变压器模型。由此表明ChatGPT具有在生物医学领域成为有价值工具的潜力。

    

    ChatGPT是OpenAI开发的大型语言模型。尽管其在各种任务上表现出色，但先前的工作尚未研究其在生物医学领域的能力。因此，本文旨在评估ChatGPT在各种基准生物医学任务上的性能，如关系提取、文档分类、问答和摘要。据我们所知，这是首次对ChatGPT在生物医学领域进行全面评估的工作。有趣的是，在训练集较小的生物医学数据集中，基于我们的评估结果，零样例ChatGPT甚至优于先进的精调生成式变压器模型，如BioGPT和BioBART。这表明ChatGPT在大型文本语料库上的预训练使其在生物医学领域具有相当的专业性。我们的发现表明，ChatGPT在生物医学领域具有成为各种任务的有价值工具的潜力。

    ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that la
    
[^105]: 在表面之下寻找：利用基本对称性实现高效离线强化学习

    Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])

    [http://arxiv.org/abs/2306.04220](http://arxiv.org/abs/2306.04220)

    本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。

    

    离线强化学习通过从预先收集的数据集中学习策略来解决与环境交互的实际问题。然而，现有的离线强化学习算法的性能严重依赖于数据集的规模和状态-动作空间覆盖范围。真实世界数据的收集通常是昂贵和难以控制的，导致数据集小且覆盖范围狭窄，从而对离线强化学习的实际部署提出了重大挑战。在本文中，我们提供了一个新的见解，即利用系统动力学的基本对称性可以在小数据集下显著提高离线强化学习的性能。具体来说，我们提出了一个时间反演对称(T-symmetry)强制的动力学模型(TDM)，建立了一对正向和反向潜在动力学之间的一致性。TDM为小数据集提供了良好的表示，并基于T-symmetry的符合性提供了一种新的OOD样本的可靠性度量。

    Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
    
[^106]: 基准数据集上 ChatGPT 的系统研究和全面评估

    A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])

    [http://arxiv.org/abs/2305.18486](http://arxiv.org/abs/2305.18486)

    本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。

    

    最近，如 ChatGPT 这样的大型语言模型（LLM）的开发引起了很多关注。然而，由于难以将该模型生成的产出与基本事实进行比较，因此其在基准学术数据集上的评估仍未充分探索。本文旨在对 ChatGPT 在包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务中的表现进行彻底评估。具体而言，我们在 140 个任务中评估了 ChatGPT，并分析了其在这些数据集中生成的 255K 次响应，这使我们的工作成为了在 NLP 基准测试中对 ChatGPT 进行的最大评估。简而言之，我们的研究旨在验证 ChatGPT 在各种任务中的优势和弱点，并为使用 LLM 的未来研究提供见解。我们还报告了一种新的迸发能力，即遵循多个查询指令。

    The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
    
[^107]: 多角度受限制内核机器的对偶性

    Duality in Multi-View Restricted Kernel Machines. (arXiv:2305.17251v1 [cs.LG])

    [http://arxiv.org/abs/2305.17251](http://arxiv.org/abs/2305.17251)

    该论文提出了一个统一的框架，将现有的受限制内核机器方法结合成一个单一的原始-对偶多角度框架，可用于核主成分分析，实现了原始和对偶公式的完全等价性，并最终在时间序列数据集上验证了其等价性和提供的洞察。

    

    我们提出了一个统一的框架，将现有的受限制内核机器方法结合成一个单一的原始-对偶多角度框架，用于核主成分分析，可以用于有监督和无监督学习。我们导出了该框架的原始和对偶表示，并从理论角度关联了不同的训练和推理算法。我们展示了如何通过重新调整原始变量来实现原始和对偶公式的完全等价性。最后，我们通过递归预测未看到测试数据并可视化所学特征的方式，在许多时间序列数据集上实验验证了等价性并提供了对不同方法之间关系的洞察。

    We propose a unifying setting that combines existing restricted kernel machine methods into a single primal-dual multi-view framework for kernel principal component analysis in both supervised and unsupervised settings. We derive the primal and dual representations of the framework and relate different training and inference algorithms from a theoretical perspective. We show how to achieve full equivalence in primal and dual formulations by rescaling primal variables. Finally, we experimentally validate the equivalence and provide insight into the relationships between different methods on a number of time series data sets by recursively forecasting unseen test data and visualizing the learned features.
    
[^108]: 针对深度学习的随机一阶优化方法的分层自适应步长策略

    Layer-wise Adaptive Step-Sizes for Stochastic First-Order Methods for Deep Learning. (arXiv:2305.13664v1 [cs.LG])

    [http://arxiv.org/abs/2305.13664](http://arxiv.org/abs/2305.13664)

    本文提出了一种针对深度学习的随机一阶优化方法的分层自适应步长策略，通过利用深度神经网络中浅层的随机曲率信息为每一层计算自适应步长，消除了用户调整学习率的需求。实验结果显示，结合该策略的算法在DNN任务的训练中优于精细调整学习率版本以及流行的一阶和二阶算法。

    

    我们提出了一种新的分层自适应步长策略，用于随机一阶优化方法来最小化深度学习中的经验损失函数，消除了用户调整学习率的需求。该方法利用深度神经网络（DNNs） 浅层中包含的对角线块的层随机曲率信息来计算每一层的自适应步长（即学习率）。该方法的内存需求与一阶方法相当，而其每次迭代的时间复杂度仅增加了约等于另一个梯度计算量的量。数值实验表明，结合所提出的分层步幅大小的SGD动量法和AdamW能够选择有效的学习率进度，并在Autoencoder、卷积神经网络（CNN）和循环神经网络（RNN）任务的DNN训练中优于这些方法的精细调整学习率版本以及流行的一阶和二阶算法。

    We propose a new per-layer adaptive step-size procedure for stochastic first-order optimization methods for minimizing empirical loss functions in deep learning, eliminating the need for the user to tune the learning rate (LR). The proposed approach exploits the layer-wise stochastic curvature information contained in the diagonal blocks of the Hessian in deep neural networks (DNNs) to compute adaptive step-sizes (i.e., LRs) for each layer. The method has memory requirements that are comparable to those of first-order methods, while its per-iteration time complexity is only increased by an amount that is roughly equivalent to an additional gradient computation. Numerical experiments show that SGD with momentum and AdamW combined with the proposed per-layer step-sizes are able to choose effective LR schedules and outperform fine-tuned LR versions of these methods as well as popular first-order and second-order algorithms for training DNNs on Autoencoder, Convolutional Neural Network (CN
    
[^109]: 随机卷积核的时间序列聚类

    Time Series Clustering With Random Convolutional Kernels. (arXiv:2305.10457v1 [cs.LG])

    [http://arxiv.org/abs/2305.10457](http://arxiv.org/abs/2305.10457)

    该论文介绍了一种新方法用于时间序列聚类，该方法利用随机卷积结构将数据转换为增强的特征表示，再进行聚类，以识别异常值，该方法在时间序列聚类基准上实现了更好的结果。

    

    时间序列可描述广泛的自然和社会现象，如气候、地震、股票价格或网站访问趋势。时间序列聚类有助于找到异常值，这些异常值可能代表温度异常、火山爆发、市场干扰或欺诈性网站流量。基于自动特征提取技术的成功，特别是采用随机核技术，我们开发了一种新的时间序列聚类方法，包括两个步骤。首先，一个随机卷积结构将数据转换为增强的特征表示。然后，聚类算法对转换后的数据进行分类。该方法在时间序列聚类基准上改善了最先进的结果。

    Time series can describe a wide range of natural and social phenomena. A few samples are climate and seismic measures trends, stock prices, or website visits. Time-series clustering helps to find outliers that, related to these instances, could represent temperature anomalies, imminent volcanic eruptions, market disturbances, or fraudulent web traffic. Founded on the success of automatic feature extraction techniques, specifically employing random kernels, we develop a new method for time series clustering consisting of two steps. First, a random convolutional structure transforms the data into an enhanced feature representation. Afterwards, a clustering algorithm classifies the transformed data. The method improves state-of-the-art results on time series clustering benchmarks.
    
[^110]: ZeroFlow: 通过蒸馏实现快速零标签场景流

    ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])

    [http://arxiv.org/abs/2305.10424](http://arxiv.org/abs/2305.10424)

    ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。

    

    场景流估计是描述连续点云之间的三维运动场的任务。最先进的方法使用强大的先验知识和测试时优化技术，但对于大规模点云需要数十秒的时间，使其无法作为实时应用程序（如开放世界目标检测）的计算机视觉基元使用。前向传递方法相对快速，对于大规模点云的运行时间在数十至数百毫秒之间，但需要昂贵的人力监督。为了解决这两个限制，我们提出了一种简单的蒸馏框架 Scene Flow via Distillation，使用无标签优化方法来生成伪标签以监督前向传递模型。我们实现了这个框架中的 ZeroFlow，使用零人工标签，在大规模点云上实时生成场景流估计结果，同时质量竞争状态下的最先进方法。值得注意的是，在测试时 ZeroFlow

    Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
    
[^111]: 棕矮星模型网格与机器学习大气反演的比较研究

    Intercomparison of Brown Dwarf Model Grids and Atmospheric Retrieval Using Machine Learning. (arXiv:2305.07719v1 [astro-ph.SR])

    [http://arxiv.org/abs/2305.07719](http://arxiv.org/abs/2305.07719)

    本文通过机器学习方法分析了14个棕矮星模型网格的预测能力，发现棕矮星的有效温度可以被预测，但推断表面重力加速度和金属丰度与模型网格有关。

    

    理解次恒星光谱数据和模型之间的差异一直是个挑战，尤其对于自洽模型网格的全面研究。本文采用随机森林监督机器学习方法，研究了14个以前发表的棕矮星模型网格（从1997到2021年）。随机森林方法让我们能够分析这些模型网格的预测能力，并在近似贝叶斯计算框架下解释数据。我们的数据集包括3个基准棕矮星（Gl 570D，ε Indi Ba和Bb）以及19个L型和T型矮星的样本；这个样本之前曾使用传统贝叶斯方法（嵌套取样）在Lueber等人（2022）中进行过分析。我们发现，可以独立于所选择的模型网格，强有力地预测棕矮星的有效温度。然而，推断棕矮星表面重力加速度和金属丰度因所选择的模型网格而异。

    Understanding differences between sub-stellar spectral data and models has proven to be a major challenge, especially for self-consistent model grids that are necessary for a thorough investigation of brown dwarf atmospheres. Using the supervised machine learning method of the random forest, we study the information content of 14 previously published model grids of brown dwarfs (from 1997 to 2021). The random forest method allows us to analyze the predictive power of these model grids, as well as interpret data within the framework of Approximate Bayesian Computation (ABC). Our curated dataset includes 3 benchmark brown dwarfs (Gl 570D, {\epsilon} Indi Ba and Bb) as well as a sample of 19 L and T dwarfs; this sample was previously analyzed in Lueber et al. (2022) using traditional Bayesian methods (nested sampling). We find that the effective temperature of a brown dwarf can be robustly predicted independent of the model grid chosen for the interpretation. However, inference of the sur
    
[^112]: 应用机器学习的决策系统中，数据时间滞后对猪肉价格预测的影响研究

    Effects of data time lag in a decision-making system using machine learning for pork price prediction. (arXiv:2305.05677v1 [cs.LG])

    [http://arxiv.org/abs/2305.05677](http://arxiv.org/abs/2305.05677)

    本文研究了数据采集延迟对机器学习预测模型在猪肉价格预测中的影响，并提出了解决方案，通过利用同日获取的数据，可以有效缓解数据时间滞后的影响。

    

    西班牙是世界上第三大猪肉生产国，许多农场依赖于这个市场。然而，当前的定价体系是不公平的，因为一些人比其他人有更好的市场信息。在这种情况下，历史价格是易于获取和经济实惠的数据来源，可以帮助所有代理商更好地了解市场。然而，数据采集延迟可能会影响他们的定价决策。在本文中，我们研究了多个预测算法在价格预测系统中的数据采集延迟对其影响。我们描述了最佳提案的集成到决策支持系统原型中，并在实际情况下进行了测试。具体而言，我们使用西班牙最重要的地区猪肉市场的公共数据，由农业部发布，延迟两周，和订阅方式获取的同一日的同一市场数据。结果表明，最佳预测算法和两周延迟的数据相比，订阅数据可以减少错误差异，这证明了集成多个算法可以有效缓解数据时间滞后的影响。

    Spain is the third-largest producer of pork meat in the world, and many farms in several regions depend on the evolution of this market. However, the current pricing system is unfair, as some actors have better market information than others. In this context, historical pricing is an easy-to-find and affordable data source that can help all agents to be better informed. However, the time lag in data acquisition can affect their pricing decisions. In this paper, we study the effect that data acquisition delay has on a price prediction system using multiple prediction algorithms. We describe the integration of the best proposal into a decision support system prototype and test it in a real-case scenario. Specifically, we use public data from the most important regional pork meat markets in Spain published by the Ministry of Agriculture with a two-week delay and subscription-based data of the same markets obtained on the same day. The results show that the error difference between the bes
    
[^113]: FedVS: 面向分割模型的容错和隐私保护垂直联邦学习

    FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models. (arXiv:2304.13407v1 [cs.LG])

    [http://arxiv.org/abs/2304.13407](http://arxiv.org/abs/2304.13407)

    该论文提出FedVS，一种同时解决垂直联邦学习中滞后客户端和数据泄露问题的方法，通过设计本地数据和模型的秘密共享方案，以保证信息理论隐私，并通过解密计算股份，无损重构所有客户端的嵌入的汇总。

    

    在一个由中央服务器和许多分布式客户端组成的垂直联邦学习系统中，训练数据被垂直分割，不同的特征存储在不同的客户端上。分割垂直联邦学习的问题是训练一个在服务器和客户端之间划分的模型。本文旨在解决分割垂直联邦学习中的两个主要挑战：1）由于训练过程中存在迟滞的客户端造成的性能下降；2）客户端上传数据嵌入导致的数据和模型隐私泄露。我们提出了FedVS来同时解决这两个挑战。FedVS的关键思想是设计本地数据和模型的秘密共享方案，从而保证针对勾结客户和好奇服务器的信息理论隐私，并且通过解密计算股份，无损重构所有客户端的嵌入的汇总。在各种类型的VFL数据集（包括表格，CV，图像，NLP）上进行了广泛的实验，证明了FedVS的有效性。

    In a vertical federated learning (VFL) system consisting of a central server and many distributed clients, the training data are vertically partitioned such that different features are privately stored on different clients. The problem of split VFL is to train a model split between the server and the clients. This paper aims to address two major challenges in split VFL: 1) performance degradation due to straggling clients during training; and 2) data and model privacy leakage from clients' uploaded data embeddings. We propose FedVS to simultaneously address these two challenges. The key idea of FedVS is to design secret sharing schemes for the local data and models, such that information-theoretical privacy against colluding clients and curious server is guaranteed, and the aggregation of all clients' embeddings is reconstructed losslessly, via decrypting computation shares from the non-straggling clients. Extensive experiments on various types of VFL datasets (including tabular, CV, a
    
[^114]: 从局部观测学习预测导航模式

    Learning to Predict Navigational Patterns from Partial Observations. (arXiv:2304.13242v1 [cs.CV])

    [http://arxiv.org/abs/2304.13242](http://arxiv.org/abs/2304.13242)

    本文提出了一种仅通过局部观测学习预测真实环境中导航模式的自监督学习方法，能够胜过两个有监督模型，并且可以在无限数据的情况下预测无偏本地方向软车道概率场。

    

    人类通过遵守相互知晓的导航模式在遵循规则的环境下进行合作导航，这些模式可以表示为方向路径或道路车道。从不完全观测到的环境中推断出这些导航模式是智能移动机器人在未映射位置操作所必需的。然而，算法定义这些导航模式是非常困难的。本文提出了第一个仅从局部观测中学习推断真实环境中导航模式的自监督学习（SSL）方法。我们解释了如何使用几何数据增强，预测世界建模和信息论正则化器实现了在无限数据情况下预测无偏本地方向软车道概率（DSLP）场。我们演示了如何通过将最大似然图拟合到DSLP场中来推断全局导航模式。实验证明，在模拟和移动机器人的真实世界实验中，我们的自监督模型在从局部观测中学习预测导航模式的任务中胜过了两个SOTA有监督模型。

    Human beings cooperatively navigate rule-constrained environments by adhering to mutually known navigational patterns, which may be represented as directional pathways or road lanes. Inferring these navigational patterns from incompletely observed environments is required for intelligent mobile robots operating in unmapped locations. However, algorithmically defining these navigational patterns is nontrivial. This paper presents the first self-supervised learning (SSL) method for learning to infer navigational patterns in real-world environments from partial observations only. We explain how geometric data augmentation, predictive world modeling, and an information-theoretic regularizer enables our model to predict an unbiased local directional soft lane probability (DSLP) field in the limit of infinite data. We demonstrate how to infer global navigational patterns by fitting a maximum likelihood graph to the DSLP field. Experiments show that our SSL model outperforms two SOTA supervis
    
[^115]: 自动导向和质量评估系统用于脐动脉多普勒成像

    An Automatic Guidance and Quality Assessment System for Doppler Imaging of Umbilical Artery. (arXiv:2304.05463v1 [eess.IV])

    [http://arxiv.org/abs/2304.05463](http://arxiv.org/abs/2304.05463)

    提出了一个自动导向和质量评估系统，使用改进的 Faster R-CNN 算法来建议脐动脉多普勒流门的位置，并评估多普勒波形质量，有效地填补了经验不足的超声医生的缺陷。

    

    在胎儿超声筛查中，通过脐带进行监测的脐动脉多普勒图像对于监测胎儿的血液供应非常重要。然而，为了捕捉脐动脉多普勒图像，需要正确地执行多个步骤：在超声图像中放置门，以获取血流波形，并判断多普勒波形质量。这些步骤都依赖于操作者的经验。经验不足的超声医生的短缺因此产生了机器辅助的需求。我们提出了一个自动系统来填补这个缺口。使用改进的 Faster R-CNN 方法，我们得到了一个算法，它建议多普勒流门的位置。我们随后评估多普勒波形质量。我们在国家超声筛查数据库上对所提出的系统进行了验证，涵盖了657个扫描结果。实验结果表明，我们的系统在指导操作者捕捉脐动脉多普勒图像和评估图像质量方面是有用的。

    In fetal ultrasound screening, Doppler images on the umbilical artery (UA) are important for monitoring blood supply through the umbilical cord. However, to capture UA Doppler images, a number of steps need to be done correctly: placing the gate at a proper location in the ultrasound image to obtain blood flow waveforms, and judging the Doppler waveform quality. Both of these rely on the operator's experience. The shortage of experienced sonographers thus creates a demand for machine assistance. We propose an automatic system to fill this gap. Using a modified Faster R-CNN we obtain an algorithm that suggests Doppler flow gate locations. We subsequently assess the Doppler waveform quality. We validate the proposed system on 657 scans from a national ultrasound screening database. The experimental results demonstrate that our system is useful in guiding operators for UA Doppler image capture and quality assessment.
    
[^116]: SLPerf：基准测试共享学习的统一框架

    SLPerf: a Unified Framework for Benchmarking Split Learning. (arXiv:2304.01502v1 [cs.LG])

    [http://arxiv.org/abs/2304.01502](http://arxiv.org/abs/2304.01502)

    SLPerf是一个统一的研究和开放式研究库，用于共享学习，通过对不同情况下不同共享学习范式的基准比较，提供了改进共享学习范式的见解。

    

    随着数据隐私问题的日益严重，分布式数据的中央化训练变得不可行，需要协作学习框架。为了解决这个问题，出现了两种主要的框架：联邦学习（FL）和分割学习（SL）。虽然FL已经建立了各种基准框架和研究库，但SL目前尚缺乏统一的库，尤其是在标签共享、模型聚合和切割层选择方面的多样性。这种标准化缺乏使得比较SL范式变得困难。为了解决这个问题，我们提出了SLPerf，这是一个统一的共享学习研究框架和开放式研究库，并在四个广泛使用的数据集上进行了大量实验，这些数据集涵盖了IID和非IID数据的情况。我们的贡献包括对最近提出的SL范式的全面调查，不同SL范式在不同情况下进行详细的基准比较，以及用于改进SL范式的丰富工程带走信息和研究见解。

    Data privacy concerns has made centralized training of data, which is scattered across silos, infeasible, leading to the need for collaborative learning frameworks. To address that, two prominent frameworks emerged, i.e., federated learning (FL) and split learning (SL). While FL has established various benchmark frameworks and research libraries, SL currently lacks a unified library despite its diversity in terms of label sharing, model aggregation, and cut layer choice. This lack of standardization makes comparing SL paradigms difficult. To address this, we propose SLPerf, a unified research framework and open research library for SL, and conduct extensive experiments on four widely-used datasets under both IID and Non-IID data settings. Our contributions include a comprehensive survey of recently proposed SL paradigms, a detailed benchmark comparison of different SL paradigms in different situations, and rich engineering take-away messages and research insights for improving SL parad
    
[^117]: 夹心视频压缩：通过神经网络封装来高效扩展标准编解码器的应用

    Sandwiched Video Compression: Efficiently Extending the Reach of Standard Codecs with Neural Wrappers. (arXiv:2303.11473v1 [eess.IV])

    [http://arxiv.org/abs/2303.11473](http://arxiv.org/abs/2303.11473)

    本文提出了夹心视频压缩方法，通过包装标准编解码器来使神经网络优化压缩性能，在高清视频传输和语音识别视频压缩等场景中表现显著。

    

    我们提出了夹心视频压缩--一种在标准视频编解码器周围包装神经网络的视频压缩系统。该夹心框架由神经前处理器、标准视频编解码器和神经后处理器组成。这些网络被联合训练以优化码率-失真损失函数，旨在在各种压缩场景中显着改善标准编解码器。在这个设置下的端到端训练需要一个可微的标准视频编解码器代理，它包括时间处理、运动补偿、内/间模式决策和循环滤波。我们提出了针对关键视频编解码器组件的可微逼近，并证明了夹心的神经编码相对于在两个重要场景中压缩输入视频的原始帧而言，具有显着更好的码率失真性能。在通过低分辨率HEVC传输高分辨率视频的情况下，夹心系统获得了6.5 dB的PSNR改善；在另一种场景中，压缩大词汇语音识别视频在0.02 bpp的情况下使用夹心系统，获得了30%的单词错误率降低。

    We propose sandwiched video compression -- a video compression system that wraps neural networks around a standard video codec. The sandwich framework consists of a neural pre- and post-processor with a standard video codec between them. The networks are trained jointly to optimize a rate-distortion loss function with the goal of significantly improving over the standard codec in various compression scenarios. End-to-end training in this setting requires a differentiable proxy for the standard video codec, which incorporates temporal processing with motion compensation, inter/intra mode decisions, and in-loop filtering. We propose differentiable approximations to key video codec components and demonstrate that the neural codes of the sandwich lead to significantly better rate-distortion performance compared to compressing the original frames of the input video in two important scenarios. When transporting high-resolution video via low-resolution HEVC, the sandwich system obtains 6.5 dB
    
[^118]: 基于图表示学习的高效可行的机器人装配序列规划

    Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning. (arXiv:2303.10135v1 [cs.RO])

    [http://arxiv.org/abs/2303.10135](http://arxiv.org/abs/2303.10135)

    本文提出了一种基于图表示学习的装配序列规划方法，通过GRACE模型可以从装配图中提取信息并预测可行的装配序列。

    

    自动机器人装配序列规划（RASP）可以显著提高现代制造业的生产力和适应力，随着对更大量化生产需求的不断增长。实现这种自动化的主要挑战之一在于从不断增加的潜在序列中高效地找到解决方案，进行越来越复杂的装配还需要成本昂贵的可行性检查。为了解决这个问题，我们提出了一种包括产品装配图的图形方法和一个名为GRACE的策略架构，用于装配序列生成。其次，我们使用GRACE从图形输入中提取有意义的信息，并逐步预测装配序列。在实验中，我们展示了我们的方法可以根据在模拟中收集的数据，预测铝型材产品变体的可行装配序列。

    Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve productivity and resilience in modern manufacturing along with the growing need for greater product customization. One of the main challenges in realizing such automation resides in efficiently finding solutions from a growing number of potential sequences for increasingly complex assemblies. Besides, costly feasibility checks are always required for the robotic system. To address this, we propose a holistic graphical approach including a graph representation called Assembly Graph for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE for assembly sequence generation. Secondly, we use GRACE to extract meaningful information from the graph input and predict assembly sequences in a step-by-step manner. In experiments, we show that our approach can predict feasible assembly sequences across product variants of aluminum profiles based on data collected in simulation of a
    
[^119]: 快速和多方面挖掘复杂的时间戳事件流

    Fast and Multi-aspect Mining of Complex Time-stamped Event Streams. (arXiv:2303.03789v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03789](http://arxiv.org/abs/2303.03789)

    这篇论文提出了一种名为CubeScope的方法，用于快速、多方面地挖掘复杂的时间戳事件流。该方法能够识别突然的不连续性和不同的动态模式，并对所有属性进行多方面摘要，并发现隐藏的群体和其关系。CubeScope还能检测到异常的突然出现。

    

    针对具有多个属性的大规模在线时间演变事件流（如在线购物日志：项目、价格、品牌、时间和地理位置活动：上车和下车地点、时间），我们如何对大规模、动态、高阶张量流进行摘要？我们的回答是专注于两种类型的模式，即“制度”和“组件”，我们提出了一种高效有效的方法CubeScope来处理高阶张量流。具体而言，它识别任何突然的不连续性，并识别出不同的动态模式“制度”（例如工作日/周末/假期模式）。在每个制度中，它还对所有属性（例如项目、价格、品牌和时间）进行多方面摘要，并发现表示潜在群体（例如项目/品牌群）及其关系的隐藏的“组件”。由于其简洁而有效的摘要，CubeScope还能检测到异常的突然出现。

    Given a huge, online stream of time-evolving events with multiple attributes, such as online shopping logs: (item, price, brand, time), and local mobility activities: (pick-up and drop-off locations, time), how can we summarize large, dynamic high-order tensor streams? How can we see any hidden patterns, rules, and anomalies? Our answer is to focus on two types of patterns, i.e., ''regimes'' and ''components'', for which we present CubeScope, an efficient and effective method over high-order tensor streams. Specifically, it identifies any sudden discontinuity and recognizes distinct dynamical patterns, ''regimes'' (e.g., weekday/weekend/holiday patterns). In each regime, it also performs multi-way summarization for all attributes (e.g., item, price, brand, and time) and discovers hidden ''components'' representing latent groups (e.g., item/brand groups) and their relationship. Thanks to its concise but effective summarization, CubeScope can also detect the sudden appearance of anomalie
    
[^120]: 梯度范数感知最小化在寻找一阶平坦度中改进了广义化能力

    Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization. (arXiv:2303.03108v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03108](http://arxiv.org/abs/2303.03108)

    研究提出了一阶平坦度的概念，使用梯度范数感知最小化算法寻找在所有方向上具有均匀小曲率的极小值，提高了广义化能力和测试损失。

    

    最近，已经证明了平坦的极小值有效地提高了泛化能力，而锐度感知最小化 (SAM) 实现了最先进的性能。然而，SAM 及其后续讨论中当前关于平坦性的定义仅限于零阶平坦性 (即扰动半径内最坏损失)。我们表明，当存在单一最小值或给定扰动半径内的多个最小值时，零阶平坦度可能不足以区分具有低泛化误差和高泛化误差的极小值。因此，我们提出了一阶平坦度，这是一种更强的平坦度测量，重点关注扰动半径内的最大梯度范数，其限制了局部极小值的 Hessian 的最大特征值和 SAM 的正则化函数。我们还提出了一种名为 Gradient norm Aware Minimization (GAM) 的新型训练过程，以寻找所有方向上曲率均匀小的最小值。实验结果表明，GAM 显着改进了广义化能力和测试损失

    Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results s
    
[^121]: 用于准确和可迁移神经势的非平衡分子去噪预训练

    Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials. (arXiv:2303.02216v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02216](http://arxiv.org/abs/2303.02216)

    该论文提出了一种用于准确和可迁移神经势的非平衡分子去噪预训练方法，并通过实验证明了该方法能显著提高势能模型的准确性和可迁移性。

    

    最近等变图神经网络（GNN）的进展使深度学习能够开发快速的模型替代昂贵的从头计算量子力学（QM）方法，以用于分子势能的预测。然而，使用GNN建立准确和可迁移的势能模型仍然具有挑战性，因为数据受到昂贵的计算成本和QM方法的理论层次的限制，特别是对于大型和复杂的分子系统。在这项工作中，我们提出了对非平衡分子构型进行去噪预训练，以实现更准确和可迁移的GNN势能预测。具体地，对采样的非平衡构型的原子坐标进行随机噪声扰动，并预训练GNN对扰动的分子构型进行去噪，从而恢复原始坐标。对多个基准测试进行严格实验证明，预训练显著提高了神经势能的准确性。此外，我们还展示了预训练可以提高可迁移性，使模型在不同系统上表现良好。

    Recent advances in equivariant graph neural networks (GNNs) have made deep learning amenable to developing fast surrogate models to expensive ab initio quantum mechanics (QM) approaches for molecular potential predictions. However, building accurate and transferable potential models using GNNs remains challenging, as the data is greatly limited by the expensive computational costs and level of theory of QM methods, especially for large and complex molecular systems. In this work, we propose denoise pretraining on nonequilibrium molecular conformations to achieve more accurate and transferable GNN potential predictions. Specifically, atomic coordinates of sampled nonequilibrium conformations are perturbed by random noises and GNNs are pretrained to denoise the perturbed molecular conformations which recovers the original coordinates. Rigorous experiments on multiple benchmarks reveal that pretraining significantly improves the accuracy of neural potentials. Furthermore, we show that the
    
[^122]: 混合稀疏线性回归中的统计与计算权衡

    Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression. (arXiv:2303.02118v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.02118](http://arxiv.org/abs/2303.02118)

    本研究研究了混合稀疏线性回归问题，在实际应用中发现了统计和计算之间的权衡关系，并确定了样本复杂度和运行时间之间的平滑信息-计算权衡关系。

    

    我们考虑具有两个部分的混合稀疏线性回归问题，其中需要从n个无标签的噪声线性测量中恢复两个实数k稀疏信号β1、β2。稀疏度允许在维度上是亚线性的，且假设添加的噪声是独立的高斯噪声，方差为σ²。之前的研究表明，该问题存在一个k/SNR²到k²/SNR²的统计到计算间隙，类似于其他具有计算挑战性的高维推断问题，如稀疏主成分分析和鲁棒稀疏均值估计；这里的SNR是信噪比。通过低次多项式方法，我们证明了这个问题存在更广泛的计算障碍，但只在非常狭窄的对称参数范围内才表现出计算困难。我们确定了样本复杂度n和运行时间之间的平滑信息-计算权衡关系，对于任何随机化算法。

    We consider the problem of mixed sparse linear regression with two components, where two real $k$-sparse signals $\beta_1, \beta_2$ are to be recovered from $n$ unlabelled noisy linear measurements. The sparsity is allowed to be sublinear in the dimension, and additive noise is assumed to be independent Gaussian with variance $\sigma^2$. Prior work has shown that the problem suffers from a $\frac{k}{SNR^2}$-to-$\frac{k^2}{SNR^2}$ statistical-to-computational gap, resembling other computationally challenging high-dimensional inference problems such as Sparse PCA and Robust Sparse Mean Estimation; here $SNR$ is the signal-to-noise ratio. We establish the existence of a more extensive computational barrier for this problem through the method of low-degree polynomials, but show that the problem is computationally hard only in a very narrow symmetric parameter regime. We identify a smooth information-computation tradeoff between the sample complexity $n$ and runtime for any randomized algor
    
[^123]: 使用指针生成网络和SciBERT嵌入生成研究论文的摘要

    Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07729](http://arxiv.org/abs/2302.07729)

    该论文提出了一种使用指针生成网络和SciBERT嵌入来自动生成研究论文亮点的方法。在多个基准数据集上的实验证明，该模型在研究亮点生成方面具有最佳性能。

    

    如今，许多研究文章都以研究亮点作为前言，以总结论文的主要发现。亮点不仅帮助研究人员准确快速地识别论文的贡献，还通过搜索引擎增加了文章的可发现性。我们的目标是在给定研究论文的特定段落的情况下自动构建研究亮点。我们使用了一个具有覆盖机制和上下文嵌入层的指针生成网络，将输入标记编码为SciBERT嵌入。我们在基准数据集CSPubSum上测试了我们的模型，并且还提出了MixSub，一个用于自动生成研究亮点的新的跨学科论文语料库。对于CSPubSum和MixSub，我们观察到所提出的模型相对于相关变体和文献中提出的其他模型来说具有最佳性能。在CSPubSum数据集上，我们的模型在只使用论文的摘要作为输入时表现最佳。

    Nowadays many research articles are prefaced with research highlights to summarize the main findings of the paper. Highlights not only help researchers precisely and quickly identify the contributions of a paper, they also enhance the discoverability of the article via search engines. We aim to automatically construct research highlights given certain segments of a research paper. We use a pointer-generator network with coverage mechanism and a contextual embedding layer at the input that encodes the input tokens into SciBERT embeddings. We test our model on a benchmark dataset, CSPubSum, and also present MixSub, a new multi-disciplinary corpus of papers for automatic research highlight generation. For both CSPubSum and MixSub, we have observed that the proposed model achieves the best performance compared to related variants and other models proposed in the literature. On the CSPubSum dataset, our model achieves the best performance when the input is only the abstract of a paper as op
    
[^124]: 充分发挥心电图的能力：一种在具有心电图信号的医疗系统中进行患者识别的新方法

    Unleashing the Power of Electrocardiograms: A novel approach for Patient Identification in Healthcare Systems with ECG Signals. (arXiv:2302.06529v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06529](http://arxiv.org/abs/2302.06529)

    本研究提出了一种利用心电图信号进行患者识别的新方法，通过卷积神经网络对提取的心电图图像进行分类。该方法综合考虑了心血管疾病对用户识别的影响，并通过实验验证了其准确性和可靠性。

    

    在过去的二十年中，大量的研究已经证实了利用心脏信号作为生物识别模式的可行性。本文提出了一种使用心电图信号进行患者识别的新方法。利用卷积神经网络基于从心电图信号中提取的图像对用户进行分类。该识别系统在多个数据库中进行了评估，全面了解其在实际场景中的潜力。以往的研究在普通用户识别中往往忽略了心血管疾病的影响。所提出的方法考虑了患者的心血管状况，确保所得结果不具有偏见或限制。此外，通过广泛的实验验证，所得结果具有一致性和可靠性，并具有较低的错误率和更高的准确度指标。

    Over the course of the past two decades, a substantial body of research has substantiated the viability of utilising cardiac signals as a biometric modality. This paper presents a novel approach for patient identification in healthcare systems using electrocardiogram signals. A convolutional neural network is used to classify users based on images extracted from ECG signals. The proposed identification system is evaluated in multiple databases, providing a comprehensive understanding of its potential in real-world scenarios. The impact of Cardiovascular Diseases on generic user identification has been largely overlooked in previous studies. The presented method takes into account the cardiovascular condition of the patients, ensuring that the results obtained are not biased or limited. Furthermore, the results obtained are consistent and reliable, with lower error rates and higher accuracy metrics, as demonstrated through extensive experimentation. All these features make the proposed 
    
[^125]: 不使用边际贡献近似计算Shapley值

    Approximating the Shapley Value without Marginal Contributions. (arXiv:2302.00736v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00736](http://arxiv.org/abs/2302.00736)

    本文提出了两种无参数、领域无关的Shapley值近似算法SVARM和Stratified SVARM，它们基于一种与边际贡献概念脱钩的Shapley值表示。我们证明了它们在近似质量方面具有无与伦比的理论保证，并通过实证结果将其与合成游戏和常见可解释性用例进行了比较。

    

    Shapley值是为合作博弈中的玩家分配有意义的贡献值的最流行方法，最近在可解释的人工智能中得到了广泛应用。Shapley值的有意义性源于仅有Shapley值满足的公理属性，然而，确切计算的代价是随着玩家数量指数级增长。因此，许多研究致力于高效近似Shapley值，其中大部分围绕着玩家的边际贡献的概念。在本文中，我们提出了两种基于与边际贡献概念脱钩的Shapley值表示的无参数、领域无关的近似算法SVARM和Stratified SVARM。我们证明了它们在近似质量方面的无与伦比的理论保证，并提供了包括合成游戏和常用可解释性用例的实证结果进行比较。

    The Shapley value is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, which has recently been used intensively in explainable artificial intelligence. The meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley values, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contributions. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparin
    
[^126]: ESC：具备软件常识约束的零样本物体导航探索

    ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation. (arXiv:2301.13166v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.13166](http://arxiv.org/abs/2301.13166)

    本文提出了一种新颖的零样本物体导航方法 ESC，它从预先训练的视觉和自然语言处理模型中转移常识知识，可在未知环境中进行导航，具有广阔的应用前景。

    

    准确地定位和导航到特定物体的能力对于在现实世界中操作并与物体交互以完成任务的实体代理来说至关重要。这种物体导航任务通常需要在具有标记物体的视觉环境中进行大规模训练，这种训练效果在未知环境中的新颖物体上泛化效果较差。本文提出了一种新颖的零样本物体导航方法——具备软件常识约束的探索（ESC），它将预先训练的模型中的常识知识转移到在视觉环境上进行开放世界物体导航时不需要进行导航或其他视觉环境训练。首先，ESC利用预先训练的视觉和语言模型进行开放世界基于提示的接地，利用预先训练的常识语言模型进行房间和物体推理。然后，ESC通过将常识知识建模为软逻辑谓词来使其转化为导航动作，从而进行有效的探索。在MP3D上进行了大量实验，......

    The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, 
    
[^127]: 用于复杂查询回答的神经链接预测器的调整

    Adapting Neural Link Predictors for Complex Query Answering. (arXiv:2301.12313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12313](http://arxiv.org/abs/2301.12313)

    本文提出通过训练一个参数高效的分数适应模型来重新校准神经链接预测分数以解决神经链接预测器在复杂查询回答中的问题。

    

    在不完整知识图谱上回答复杂查询是一项具有挑战性的任务，模型需要在缺失知识的情况下回答复杂逻辑查询。最近，Arakelyan等人（2021）；Minervini等人（2022）表明，神经链接预测器也可以用于回答复杂查询：他们的连续查询分解（CQD）方法通过将复杂查询分解为原子子查询，使用神经链接预测器回答并通过t-范数来聚合其分数，以对每个复杂查询的答案进行排序。然而，CQD不处理否定并且仅使用原子训练查询的训练信号：在回答复杂查询期间，神经链接预测分数没有通过模糊逻辑t-范数进行校准以相互作用。在这项工作中，我们提出通过训练一个参数高效的分数适应模型来重新校准神经链接预测分数以解决这个问题：这个新组件通过反向传播法在复杂查询上进行训练。

    Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022) showed that neural link predictors could also be used for answering complex queries: their Continuous Query Decomposition (CQD) method works by decomposing complex queries into atomic sub-queries, answers them using neural link predictors and aggregates their scores via t-norms for ranking the answers to each complex query. However, CQD does not handle negations and only uses the training signal from atomic training queries: neural link prediction scores are not calibrated to interact together via fuzzy logic t-norms during complex query answering. In this work, we propose to address this problem by training a parameter-efficient score adaptation model to re-calibrate neural link prediction scores: this new component is trained on complex queries by back-propa
    
[^128]: 用于组织病理学图像的常见不确定性估计方法在领域转移和标签噪声下的基准测试

    Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise. (arXiv:2301.01054v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.01054](http://arxiv.org/abs/2301.01054)

    本文对常用的组织病理学图像分类的不确定性估计方法进行了基准测试，比较了不同方法在领域转移和标签噪声下的性能，并得出方法的集合通常会有更好的结果。

    

    在过去几年中，深度学习在组织病理学应用领域的使用有所增加。然而，尽管这些方法展示了巨大的潜力，在高风险环境中，深度学习模型需要能够判断其不确定性，并在存在较大的错误分类风险时能够拒绝输入。在这项工作中，我们对最常用的用于分类整个切片图像的不确定性和鲁棒性方法进行了严格评价，重点关注选择性分类的任务，在这个任务中，模型应在不确定的情况下拒绝分类。我们在领域转移和标签噪声方面对切片级别和瓦片级别进行了实验。在实验中，我们比较了深度集成、蒙特卡洛丢弃、随机变分推断、测试时数据增强以及后两种方法的集合。我们观察到，方法的集合通常会产生更好的结果。

    In the past years, deep learning has seen an increase in usage in the domain of histopathological applications. However, while these approaches have shown great potential, in high-risk environments deep learning models need to be able to judge their uncertainty and be able to reject inputs when there is a significant chance of misclassification. In this work, we conduct a rigorous evaluation of the most commonly used uncertainty and robustness methods for the classification of Whole Slide Images, with a focus on the task of selective classification, where the model should reject the classification in situations in which it is uncertain. We conduct our experiments on tile-level under the aspects of domain shift and label noise, as well as on slide-level. In our experiments, we compare Deep Ensembles, Monte-Carlo Dropout, Stochastic Variational Inference, Test-Time Data Augmentation as well as ensembles of the latter approaches. We observe that ensembles of methods generally lead to bett
    
[^129]: 高效节约内存的NLLB-200：针对大规模多语言机器翻译模型的语言特定专家删减

    Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09811](http://arxiv.org/abs/2212.09811)

    本研究提出了一种节约内存的NLLB-200模型修剪方法，可在保持翻译质量的同时移除多达80％的专家，使得在单个32GB的GPU上运行模型成为可能。这对于大规模多语言机器翻译具有重要的意义。

    

    与传统的双语翻译系统相比，大规模多语言机器翻译具有吸引力，因为一个单一模型可以翻译成多种语言，并从知识转移中获益，尤其是对于低资源语言。然而，大规模多语言模型受到多语言性的限制，除非进行大规模扩展，否则会增加训练和推理成本。稀疏的专家混合模型是一种在不需要大量计算的情况下大幅增加模型容量的方法。最近发布的NLLB-200是这样一个模型的例子。它涵盖了202种语言，但仅推理就需要至少四个32GB的GPU。在这项工作中，我们提出了一种修剪方法，允许删除多达80％的专家，但翻译质量几乎没有损失，这使得在单个32GB的GPU上运行该模型成为可能。进一步分析表明，我们的修剪度量指标可以识别出语言特定的专家

    Compared to conventional bilingual translation systems, massively multilingual machine translation is appealing because a single model can translate into multiple languages and benefit from knowledge transfer for low resource languages. On the other hand, massively multilingual models suffer from the curse of multilinguality, unless scaling their size massively, which increases their training and inference costs. Sparse Mixture-of-Experts models are a way to drastically increase model capacity without the need for a proportional amount of computing. The recently released NLLB-200 is an example of such a model. It covers 202 languages but requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that allows the removal of up to 80\% of experts with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics allow to identify language-specific experts and p
    
[^130]: 用于学习迭代算法的递归循环神经网络 (R2N2) 架构

    A Recursively Recurrent Neural Network (R2N2) Architecture for Learning Iterative Algorithms. (arXiv:2211.12386v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12386](http://arxiv.org/abs/2211.12386)

    本文提出了一个名为 R2N2 的递归循环神经网络结构，用于学习定制的迭代算法。与传统的深度学习方法不同的是，R2N2 将生成信息和组装信息的过程划分为不同的模块，通过在每次迭代中评估函数来生成局部信息，并将这些评估的线性组合用于更新下一次迭代的结果，从而降低算法的复杂性。

    

    数值算法的元学习包括对算法结构和相关超参数的数据驱动识别和适应。为了限制元学习问题的复杂性，可以使用具有对有利算法结构的某种归纳偏差的神经网络架构。我们将之前介绍的龙格－库塔神经网络推广为一种递归循环神经网络 (R2N2) 超结构，用于设计定制的迭代算法。与现成的深度学习方法不同，它具有明确的模块划分，用于生成信息和将该信息组装成解决方案。通过从当前外部迭代开始进行从属内部迭代的循环函数评估，可以生成局部信息，以子空间的形式呈现。下一次外部迭代的更新是通过这些评估的线性组合计算得出的，从而减少算法的复杂性。

    Meta-learning of numerical algorithms for a given task consists of the data-driven identification and adaptation of an algorithmic structure and the associated hyperparameters. To limit the complexity of the meta-learning problem, neural architectures with a certain inductive bias towards favorable algorithmic structures can, and should, be used. We generalize our previously introduced Runge-Kutta neural network to a recursively recurrent neural network (R2N2) superstructure for the design of customized iterative algorithms. In contrast to off-the-shelf deep learning approaches, it features a distinct division into modules for generation of information and for the subsequent assembly of this information towards a solution. Local information in the form of a subspace is generated by subordinate, inner, iterations of recurrent function evaluations starting at the current outer iterate. The update to the next outer iterate is computed as a linear combination of these evaluations, reducing
    
[^131]: 《Stein变分梯度下降的有限粒子收敛速度》

    A Finite-Particle Convergence Rate for Stein Variational Gradient Descent. (arXiv:2211.09721v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09721](http://arxiv.org/abs/2211.09721)

    本文提供了Stein变分梯度下降算法的有限粒子收敛速度，证明了当目标分布为次高斯且具有Lipschitz积分核时，使用适当的步长序列和粒子数量，可以以1/√(log log n)的速度将核Stein差异逼近零。

    

    我们首次提供了Stein变分梯度下降（SVGD）的有限粒子收敛速度，这是一种用一组粒子逼近概率分布的流行算法。具体来说，只要目标分布是次高斯的，并且具有Lipschitz积分核，使用n个粒子和适当的步长序列进行SVGD，核Stein差异将以1/√(log log n)的速度趋于零。我们怀疑n的依赖性可以改进，希望我们的明确的非渐近证明策略能为未来的改进提供模板。

    We provide the first finite-particle convergence rate for Stein variational gradient descent (SVGD), a popular algorithm for approximating a probability distribution with a collection of particles. Specifically, whenever the target distribution is sub-Gaussian with a Lipschitz score, SVGD with n particles and an appropriate step size sequence drives the kernel Stein discrepancy to zero at an order 1/sqrt(log log n) rate. We suspect that the dependence on n can be improved, and we hope that our explicit, non-asymptotic proof strategy will serve as a template for future refinements.
    
[^132]: DiffusionDB: 文本到图像生成模型的大规模提示画廊数据集

    DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models. (arXiv:2210.14896v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.14896](http://arxiv.org/abs/2210.14896)

    介绍了DiffusionDB数据集，这是一个规模庞大的文本到图像提示数据集，总计包含1400万张图像和180万个唯一提示。该数据集被用来帮助研究人员解决文本提示生成图像时所需的适当提示的问题，并指出了一些特定的提示样式和超参数值可能导致模型错误，甚至生成误导信息。

    

    随着扩散模型的最新进展，用户可以通过编写自然语言提示生成高质量图像。然而，生成具有所需细节的图像需要适当的提示，而且往往不清楚模型对不同提示的反应或最佳提示是什么。为了帮助研究人员解决这些关键挑战，我们介绍了DiffusionDB，这是第一个大规模的文本到图像提示数据集，总计6.5TB，包含使用Stable Diffusion生成的1400万张图像，180万个唯一提示和由真实用户指定的超参数。我们分析了提示的语法和语义特征，并指出了可能导致模型错误的特定超参数值和提示样式，并提供了潜在有害模型使用的证据，如生成误导信息。这个人为驱动的数据集的空前规模和多样性为了解提示和生成图像之间相互作用提供了激动人心的研究机会。

    With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generat
    
[^133]: 混沌理论与对抗性鲁棒性

    Chaos Theory and Adversarial Robustness. (arXiv:2210.13235v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13235](http://arxiv.org/abs/2210.13235)

    本文利用混沌理论解释、分析和量化了神经网络对抗性攻击的易受性和鲁棒性，提出了一种新的度量指标“易受性比”，结果表明模型深度增加会显著增加对抗攻击的易受性，这对于生产环境中的神经网络设计具有安全影响。

    

    神经网络容易受到对抗性攻击，在关键领域或对抗性应用中使用前应进行严格的审查。本文利用混沌理论的思想，解释、分析和量化神经网络对抗性攻击的易受性和鲁棒性程度。为此，我们提出了一种新的度量指标“易受性比”，由$\hat \Psi(h, \theta)$给出，该指标衡量模型输出对于给定输入扰动的变化程度。我们的结果表明，随着模型深度的增加，对抗攻击的易受性显著增长，这对于设计用于生产环境的神经网络具有安全影响。我们提供了关于$\hat \Psi$与分类模型攻击后准确性的关系的实验证据，并讨论了其在缺乏硬决策边界任务中的应用。我们还演示了如何快速简便地近似计算认证的r。

    Neural networks, being susceptible to adversarial attacks, should face a strict level of scrutiny before being deployed in critical or adversarial applications. This paper uses ideas from Chaos Theory to explain, analyze, and quantify the degree to which neural networks are susceptible to or robust against adversarial attacks. To this end, we present a new metric, the "susceptibility ratio," given by $\hat \Psi(h, \theta)$, which captures how greatly a model's output will be changed by perturbations to a given input.  Our results show that susceptibility to attack grows significantly with the depth of the model, which has safety implications for the design of neural networks for production environments. We provide experimental evidence of the relationship between $\hat \Psi$ and the post-attack accuracy of classification models, as well as a discussion of its application to tasks lacking hard decision boundaries. We also demonstrate how to quickly and easily approximate the certified r
    
[^134]: 多领域物理信息神经网络接口条件的元学习

    Meta Learning of Interface Conditions for Multi-Domain Physics-Informed Neural Networks. (arXiv:2210.12669v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12669](http://arxiv.org/abs/2210.12669)

    提出了一种元学习方法来动态确定解决一类参数化PDEs所需的适当接口条件，以进一步改善多领域物理信息神经网络（PINNs）的性能。

    

    物理信息神经网络（PINNs）作为一种无网格求解器，越来越受欢迎，用于偏微分方程（PDEs）。最近的扩展将领域分解，并应用不同的PINNs来解决每个子域的问题，并在接口处拼接子域。这样可以进一步减轻问题的复杂性，降低计算成本，并允许并行化。然而，多领域PINNs的性能对接口条件的选择非常敏感。虽然已经提出了很多条件，但没有关于如何根据具体问题选择条件的建议。为了填补这一空白，我们提出了META Learning of Interface Conditions (METALIC)，这是一种简单、高效但强大的方法，用于动态确定解决一类参数化PDEs所需的适当接口条件。具体而言，我们开发了两种情境多臂赌博（MAB）模型。第一个模型适用于整个训练过程，并进行在线更新一个G+-

    Physics-informed neural networks (PINNs) are emerging as popular mesh-free solvers for partial differential equations (PDEs). Recent extensions decompose the domain, apply different PINNs to solve the problem in each subdomain, and stitch the subdomains at the interface. Thereby, they can further alleviate the problem complexity, reduce the computational cost, and allow parallelization. However, the performance of multi-domain PINNs is sensitive to the choice of the interface conditions. While quite a few conditions have been proposed, there is no suggestion about how to select the conditions according to specific problems. To address this gap, we propose META Learning of Interface Conditions (METALIC), a simple, efficient yet powerful approach to dynamically determine appropriate interface conditions for solving a family of parametric PDEs. Specifically, we develop two contextual multi-arm bandit (MAB) models. The first one applies to the entire training course, and online updates a G
    
[^135]: 深度神经网络基于Top Tagger的可解释性研究的详细研究

    A Detailed Study of Interpretability of Deep Neural Network based Top Taggers. (arXiv:2210.04371v4 [hep-ex] UPDATED)

    [http://arxiv.org/abs/2210.04371](http://arxiv.org/abs/2210.04371)

    本文详细研究了深度神经网络基于Top Tagger的可解释性。通过回顾现有模型并探索不同定量方法，我们确定了在识别顶夸克的喷注中起关键作用的特征。我们的研究揭示了现有XAI方法的问题，并提出了克服这些问题的方法。

    

    最近可解释性人工智能 (XAI) 方法的发展使研究人员能够探索深度神经网络 (DNNs) 的内部工作原理，揭示有关输入-输出关系的关键信息，并了解数据与机器学习模型的连接方式。本文中，我们探讨了设计用于在大型强子对撞机 (LHC) 上识别来自顶夸克衰变的喷注的DNN模型的可解释性。我们回顾了一部分现有的顶夸克标记模型，并探索了不同的定量方法，以确定哪些特征在识别顶夸克的喷注中起着最重要的作用。我们还研究了不同的XAI指标对特征重要性的变化方式及其解释能力的影响，特征之间的相关性如何影响它们的可解释性，以及潜在空间表示如何编码信息以及与物理上有意义的量之间的相关性。我们的研究揭示了现有XAI方法的一些主要问题，并说明了如何克服这些问题。

    Recent developments in the methods of explainable AI (XAI) allow researchers to explore the inner workings of deep neural networks (DNNs), revealing crucial information about input-output relationships and realizing how data connects with machine learning models. In this paper we explore interpretability of DNN models designed to identify jets coming from top quark decay in high energy proton-proton collisions at the Large Hadron Collider (LHC). We review a subset of existing top tagger models and explore different quantitative methods to identify which features play the most important roles in identifying the top jets. We also investigate how and why feature importance varies across different XAI metrics, how correlations among features impact their explainability, and how latent space representations encode information as well as correlate with physically meaningful quantities. Our studies uncover some major pitfalls of existing XAI methods and illustrate how they can be overcome to 
    
[^136]: PathProx: 一种用于权值衰减正则化深度神经网络的近端梯度算法

    PathProx: A Proximal Gradient Algorithm for Weight Decay Regularized Deep Neural Networks. (arXiv:2210.03069v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03069](http://arxiv.org/abs/2210.03069)

    本文提出一种用于权值衰减正则化深度神经网络的近端梯度算法 PathProx，它可以更快地收敛到标准权值衰减训练所共享的稀疏解。

    

    权值衰减是深度学习中最广泛使用的正则化方法之一，已被证明可以提高泛化能力和鲁棒性。驱动权值衰减的优化目标是损失之和加上与权值平方和成比例的项。本文认为，随机梯度下降（SGD）可能是这个目标的一种低效算法。对于带有ReLU激活函数的神经网络，权重衰减目标的解与另一个目标的解是等价的，其中正则化项改为与每个ReLU神经元关联的输入和输出权重的$\ell_2$（不是平方）范数乘积之和。这种替代（并且有效等价）的正则化方法提出了一种用于网络训练的新近端梯度算法。理论和实验证实了这种新的训练方法，显示它可以更快地收敛到标准权值衰减训练所共享的稀疏解。

    Weight decay is one of the most widely used forms of regularization in deep learning, and has been shown to improve generalization and robustness. The optimization objective driving weight decay is a sum of losses plus a term proportional to the sum of squared weights. This paper argues that stochastic gradient descent (SGD) may be an inefficient algorithm for this objective. For neural networks with ReLU activations, solutions to the weight decay objective are equivalent to those of a different objective in which the regularization term is instead a sum of products of $\ell_2$ (not squared) norms of the input and output weights associated with each ReLU neuron. This alternative (and effectively equivalent) regularization suggests a novel proximal gradient algorithm for network training. Theory and experiments support the new training approach, showing that it can converge much faster to the sparse solutions it shares with standard weight decay training.
    
[^137]: 从卫星中发现病毒：通过图神经网络对西尼罗河病毒的循环建模

    Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks. (arXiv:2209.05251v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.05251](http://arxiv.org/abs/2209.05251)

    本论文通过使用卫星图像来预测西尼罗河病毒的循环，提出了一种空间感知的方法，利用图神经网络(GNN)来聚合邻居的特征。

    

    西尼罗河病毒(WNV)的发生代表了最常见的蚊媒性动物传播病毒感染之一。它的循环通常与适合于媒介蚊子繁殖和病毒复制的气候和环境条件相关。此外，已开发了几种统计模型来塑造和预测WNV的循环：特别是，地球观测(EO)数据的大规模可用性，加上人工智能领域的不断进步，提供了宝贵的机会。在本文中，我们试图通过将卫星图像输入深度神经网络(DNNs)来预测WNV的循环，这些卫星图像已经广泛显示具有环境和气候特征。值得注意的是，虽然之前的方法独立地分析每个地理点，但我们提出了一种空间感知的方法，考虑了附近点的特征。具体而言，我们依靠图神经网络(GNN)来聚合邻居的特征。

    The occurrence of West Nile Virus (WNV) represents one of the most common mosquito-borne zoonosis viral infections. Its circulation is usually associated with climatic and environmental conditions suitable for vector proliferation and virus replication. On top of that, several statistical models have been developed to shape and forecast WNV circulation: in particular, the recent massive availability of Earth Observation (EO) data, coupled with the continuous advances in the field of Artificial Intelligence, offer valuable opportunities.  In this paper, we seek to predict WNV circulation by feeding Deep Neural Networks (DNNs) with satellite images, which have been extensively shown to hold environmental and climatic features. Notably, while previous approaches analyze each geographical site independently, we propose a spatial-aware approach that considers also the characteristics of close sites. Specifically, we build upon Graph Neural Networks (GNN) to aggregate features from neighbour
    
[^138]: 一种利用复杂网络识别可再生电力分布系统弹性的方法

    A methodology for identifying resiliency in renewable electrical distribution system using complex network. (arXiv:2208.11543v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2208.11543](http://arxiv.org/abs/2208.11543)

    本文提出一种使用复杂网络理论来识别可再生电力分布系统弹性的方法，可以识别系统中太阳能电池板的托管能力，从而有助于提高系统的韧性。

    

    近年来，电力配电系统广泛采用分布式能源资源（DER）以满足能源需求，普遍认为这可以提高系统的弹性。然而，由于各种因素（如间歇性可用性、天气条件的动态变化、非线性等）可能对电网运营产生不利影响。本文提出了一种使用复杂网络理论来识别带有太阳能光伏发电的配电系统弹性的方法。我们根据不同条件获得了不同条件下的复杂相关网络，并计算了各种网络参数，以识别网络的弹性。所提出的方法可以识别系统中太阳能电池板的托管能力，并在不同不良条件下保持系统的弹性，从而有助于提高系统的韧性。

    Recently, Electrical Distribution Systems are extensively penetrated with the Distributed Energy Resources (DERs) to cater the energy demands with general perception that it enhances the system resiliency. However, it may be adverse for the grid operation due to various factors like its intermittent availability, dynamics in weather condition, introduction of nonlinearity, complexity etc. This needs a detailed understanding of system resiliency that our method proposes here. We introduce a methodology using complex network theory to identify the resiliency of distribution system when incorporated with Solar PV generation under various undesirable configurations. Complex correlated networks for different conditions were obtained and various network parameters were computed for identifying the resiliency of those networks. The proposed methodology identifies the hosting capacity of solar panels in the system while maintaining the resiliency under different unwanted conditions hence helps
    
[^139]: DataPerf：数据中心人工智能开发的基准测试

    DataPerf: Benchmarks for Data-Centric AI Development. (arXiv:2207.10062v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10062](http://arxiv.org/abs/2207.10062)

    DataPerf是一个由社区主导的基准测试套件，旨在通过竞争、可比性和可重复性促进数据中心人工智能的创新。

    

    长期以来，机器学习研究一直注重模型而不是数据集，并且著名数据集被用于常见的机器学习任务，而忽视了底层问题的广度、难度和准确性。忽视数据的重要性导致了现实应用中的不准确性、偏见和脆弱性，并且现有的数据集基准测试已经达到了饱和状态，阻碍了研究的进展。为此，我们提出了DataPerf，这是一个由社区主导的评估机器学习数据集和数据中心算法的基准测试套件。我们旨在通过竞争、可比性和可重复性促进数据中心人工智能的创新。我们使机器学习社区能够迭代数据集，而不仅仅是架构，并提供一个开放的在线平台，以支持这种迭代开发。DataPerf的第一个迭代包含了五个基准测试，涵盖了视觉、语音、获取等多种数据中心技术、任务和模态。

    Machine learning research has long focused on models rather than datasets, and prominent datasets are used for common ML tasks without regard to the breadth, difficulty, and faithfulness of the underlying problems. Neglecting the fundamental importance of data has given rise to inaccuracy, bias, and fragility in real-world applications, and research is hindered by saturation across existing dataset benchmarks. In response, we present DataPerf, a community-led benchmark suite for evaluating ML datasets and data-centric algorithms. We aim to foster innovation in data-centric AI through competition, comparability, and reproducibility. We enable the ML community to iterate on datasets, instead of just architectures, and we provide an open, online platform with multiple rounds of challenges to support this iterative development. The first iteration of DataPerf contains five benchmarks covering a wide spectrum of data-centric techniques, tasks, and modalities in vision, speech, acquisition, 
    
[^140]: 在算法策划平台上建模内容创作者的激励机制

    Modeling Content Creator Incentives on Algorithm-Curated Platforms. (arXiv:2206.13102v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2206.13102](http://arxiv.org/abs/2206.13102)

    该论文讨论了在线平台上内容创作者激励机制的建模，通过分析算法选择对曝光游戏（包括现代分解和两塔架构）中（纳什）均衡的影响，提出了使用曝光游戏模型进行预部署审计的方法，以识别期望和激励内容之间的不匹配。

    

    内容创作者在争夺用户注意力。他们的影响力在很大程度上取决于在线平台开发者所做的算法选择。为了最大限度地提高曝光率，许多创作者采取战略性的调整，如搜索引擎优化行业的例子所证明。这导致了对有限用户注意力池的竞争。我们在所谓的曝光游戏中形式化了这些动态，这是一种由算法引起的激励模型，其中包括现代分解和（深层）两塔架构。我们证明了看似无害的算法选择，例如非负与无约束分解，在曝光游戏中显著影响（纳什）均衡的存在和特性。我们提出使用创作者行为模型，如曝光游戏，进行（ex-ante）预部署审计。这样的审计可以识别期望和激励内容之间的不匹配，并在内容过滤和管理等事后措施上进行补充。为此，我们提出了一些工具。

    Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by algorithms, including modern factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices, e.g., non-negative vs. unconstrained factorization, significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models, like exposure games, for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools 
    
[^141]: 一种用于自动分析大规模非结构化临床心脏磁共振数据库的人工智能工具

    An AI tool for automated analysis of large-scale unstructured clinical cine CMR databases. (arXiv:2206.08137v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.08137](http://arxiv.org/abs/2206.08137)

    本研究提出了一种用于自动分析大规模非结构化临床心脏磁共振数据库的人工智能工具，并验证了其稳健性和准确性。该工具能够从短轴心脏磁共振影像中自动量化心脏功能，并具备自动化的质量控制功能，可检测和纠正错误。该算法在大量数据集上进行了训练和验证，具备良好的适应性和推广性。

    

    提出了一种用于自动分析短轴心脏磁共振影像（CMR）的人工智能技术，但目前没有自动分析大规模非结构化临床CMR数据库的工具。我们开发并验证了一种稳健的人工智能工具，用于从大规模临床数据库的短轴心脏CMR中全自动量化心脏功能。我们的流程包括自动识别正确数据、稳健的图像预处理、短轴CMR的双心室分割和功能生物标志估计的人工智能算法，以及自动后分析质量控制来检测和纠正错误。分割算法是在两家英国国家医疗服务体系（NHS）医院的2793个CMR扫描数据上训练的，并在该数据集的其他案例（n=414）和五个外部数据集（n=6888）上进行了验证，包括来自12个不同中心使用CMR扫描仪获取的各种疾病患者的扫描。

    Artificial intelligence (AI) techniques have been proposed for automating analysis of short axis (SAX) cine cardiac magnetic resonance (CMR), but no CMR analysis tool exists to automatically analyse large (unstructured) clinical CMR datasets. We develop and validate a robust AI tool for start-to-end automatic quantification of cardiac function from SAX cine CMR in large clinical databases. Our pipeline for processing and analysing CMR databases includes automated steps to identify the correct data, robust image pre-processing, an AI algorithm for biventricular segmentation of SAX CMR and estimation of functional biomarkers, and automated post-analysis quality control to detect and correct errors. The segmentation algorithm was trained on 2793 CMR scans from two NHS hospitals and validated on additional cases from this dataset (n=414) and five external datasets (n=6888), including scans of patients with a range of diseases acquired at 12 different centres using CMR scanners from all maj
    
[^142]: 离线强化学习从视觉观察中的挑战和机遇

    Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations. (arXiv:2206.04779v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04779](http://arxiv.org/abs/2206.04779)

    该论文研究了离线强化学习从视觉观察中的挑战和机遇，针对这一复杂领域建立了视觉领域中连续控制的简单基准，并设计了一系列基准任务，以更好地表示现实世界离线RL问题中的数据分布，并通过对两种基于视觉的在线强化学习算法的简单修改进行评估。

    

    离线强化学习已经展现了在利用大规模预先收集的数据集进行策略学习方面的巨大潜力，使得Agent可以避免通常费时昂贵的在线数据收集。然而，在连续动作空间中，基于视觉观察的离线强化学习仍然未被充分探索，在这个复杂的领域中对关键挑战的理解有限。在本文中，我们为视觉领域中的连续控制建立简单的基准，并引入了一系列针对离线强化学习的基准任务，这些任务旨在更好地表示现实世界离线RL问题中存在的数据分布，并受离线RL从视觉观察中的一组期望所指导，包括对视觉干扰的稳健性和动力学中可视化变化的识别能力。通过使用这套基准任务，我们展示了对两种广泛使用的基于视觉的在线强化学习算法DreamerV2和DrQ-v2进行简单修改的可行性。

    Offline reinforcement learning has shown great promise in leveraging large pre-collected datasets for policy learning, allowing agents to forgo often-expensive online data collection. However, offline reinforcement learning from visual observations with continuous action spaces remains under-explored, with a limited understanding of the key challenges in this complex domain. In this paper, we establish simple baselines for continuous control in the visual domain and introduce a suite of benchmarking tasks for offline reinforcement learning from visual observations designed to better represent the data distributions present in real-world offline RL problems and guided by a set of desiderata for offline RL from visual observations, including robustness to visual distractions and visually identifiable changes in dynamics. Using this suite of benchmarking tasks, we show that simple modifications to two popular vision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2, suf
    
[^143]: 可训练的权重平均值：子空间训练的一般方法

    Trainable Weight Averaging: A General Approach for Subspace Training. (arXiv:2205.13104v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13104](http://arxiv.org/abs/2205.13104)

    可训练的权重平均值是一种通用的子空间训练方法，通过连接子空间训练和权重平均值，提供高效的训练和易于使用的方法。这种方法可以用于改进神经网络训练效果和降低计算负担。

    

    在低维子空间中训练深度神经网络(DNNs)是实现高效训练和更好的泛化性能的一个有前景的方向。以往的工作通过使用随机投影或在训练轨迹上执行降维方法来提取子空间，但这些方法在维度和数值运算方面可能效率低下或不稳定。在本文中，我们将子空间训练与权重平均值联系起来，并提出了可训练权重平均值(TWA)，这是一种泛化以前努力的子空间训练的一般方法。TWA在维度方面具有高效性，并且易于使用，使其成为一种有前景的子空间训练新方法。我们进一步设计了一个有效的方案来应对大规模问题的子空间训练，它允许多个节点上的并行训练，并将内存和计算负担均匀分配给每个节点。我们将TWA应用于高效的神经网络训练和改进

    Training deep neural networks (DNNs) in low-dimensional subspaces is a promising direction for achieving efficient training and better generalization performance. Previous works extract the subspaces by using random projection or performing dimensionality reduction method on the training trajectory, but these methods can be inefficient or unstable in terms of dimensionality and numerical operations. In this paper, we connect subspace training to weight averaging and propose Trainable Weight Averaging (TWA), a general approach for subspace training that generalizes the previous efforts. TWA is efficient in terms of dimensionality and also easy to use, making it a promising new method for subspace training. We further design an efficient scheme for subspace training to cope with large-scale problems, which allows parallel training across multiple nodes and evenly distributing the memory and computation burden to each node. We apply TWA to efficient neural network training and improving f
    
[^144]: 机器学习友好的生物医学数据集用于等价和包含关系本体匹配

    Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v7 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.03447](http://arxiv.org/abs/2205.03447)

    本文介绍了五个新的生物医学本体匹配任务，通过引入机器学习技术并解决现有评估方法的限制，提供了综合评估框架来衡量本体匹配系统的性能。

    

    本体匹配在生物信息学和语义网等许多领域中扮演着重要角色，随着机器学习技术的应用，其研究越来越受到关注。然而，现有的本体匹配评估方法仍存在一些限制，包括对包含关系映射的有限评估、参考映射的亚优解以及对基于机器学习的系统评估的有限支持。为了解决这些限制，我们提出了五个新的生物医学本体匹配任务，涉及从Mondo和UMLS中提取的本体。每个任务包括等价和包含关系匹配，并通过人工筛选、本体修剪等方式确保参考映射的质量，并提出了一个综合评估框架，来从不同的角度评估基于机器学习和非机器学习的本体匹配系统性能。我们报告了评估结果。

    Ontology Matching (OM) plays an important role in many domains such as bioinformatics and the Semantic Web, and its research is becoming increasingly popular, especially with the application of machine learning (ML) techniques. Although the Ontology Alignment Evaluation Initiative (OAEI) represents an impressive effort for the systematic evaluation of OM systems, it still suffers from several limitations including limited evaluation of subsumption mappings, suboptimal reference mappings, and limited support for the evaluation of ML-based systems. To tackle these limitations, we introduce five new biomedical OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes both equivalence and subsumption matching; the quality of reference mappings is ensured by human curation, ontology pruning, etc.; and a comprehensive evaluation framework is proposed to measure OM performance from various perspectives for both ML-based and non-ML-based OM systems. We report evaluation r
    
[^145]: 针对神经机器翻译及其扩展的非自回归生成的调查

    A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond. (arXiv:2204.09269v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.09269](http://arxiv.org/abs/2204.09269)

    这项调查研究了非自回归生成在神经机器翻译以及其他领域的应用。研究发现，尽管非自回归生成可以加快推理速度，但与自回归生成相比存在翻译准确性的损失。然而，通过各种方法和算法的改进，可以缩小这一准确性差距。

    

    非自回归（NAR）生成首次在神经机器翻译（NMT）中提出，旨在加速推理过程，并引起了机器学习和自然语言处理领域的广泛关注。尽管NAR生成可以显著加快机器翻译的推理速度，但与其对应的自回归（AR）生成相比，其翻译准确性有所降低。近年来，许多新模型和算法已被设计/提出以弥补NAR生成与AR生成之间的准确性差距。在本文中，我们对不同方面的各种非自回归翻译（NAT）模型进行了系统调查，并进行了比较和讨论。具体而言，我们将NAT的努力分成了几个方向，包括数据处理、建模方法、训练标准、解码算法以及来自预训练模型的益处。此外，我们还简要回顾了NAR模型的其他应用。

    Non-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, autoregressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models 
    
[^146]: 从高维噪声数据中学习低维非线性结构：一种积分算子方法的应用

    Learning Low-Dimensional Nonlinear Structures from High-Dimensional Noisy Data: An Integral Operator Approach. (arXiv:2203.00126v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.00126](http://arxiv.org/abs/2203.00126)

    本文提出了一种用于从高维噪声数据中学习低维非线性结构的算法，该算法使用自适应带宽选择过程，并获得了理论上的收敛性证明。算法的低维嵌入结果可用于数据可视化、聚类和预测等任务。

    

    我们提出了一种核谱嵌入算法，用于从高维噪声观测中学习低维非线性结构，其中假设数据集从本质上是一个低维流形，并受到高维噪声的污染。该算法采用了一种自适应带宽选择过程，不依赖于对底层流形的先验知识。所获得的低维嵌入还可以进一步用于数据可视化、聚类和预测等下游任务。我们的方法在理论上得到了证明，并且具有实际可解释性。具体而言，我们在样本的维度和大小相对较大时，建立了最终嵌入到无噪声对应的收敛性，并刻画了信噪比对收敛速度和相变的影响。我们还证明了嵌入到由核定义的积分算子的特征函数的收敛性。

    We propose a kernel-spectral embedding algorithm for learning low-dimensional nonlinear structures from high-dimensional and noisy observations, where the datasets are assumed to be sampled from an intrinsically low-dimensional manifold and corrupted by high-dimensional noise. The algorithm employs an adaptive bandwidth selection procedure which does not rely on prior knowledge of the underlying manifold. The obtained low-dimensional embeddings can be further utilized for downstream purposes such as data visualization, clustering and prediction. Our method is theoretically justified and practically interpretable. Specifically, we establish the convergence of the final embeddings to their noiseless counterparts when the dimension and size of the samples are comparably large, and characterize the effect of the signal-to-noise ratio on the rate of convergence and phase transition. We also prove convergence of the embeddings to the eigenfunctions of an integral operator defined by the kern
    
[^147]: 数据集偏倚的潜在来源使机器学习算法对未诊断问题的研究复杂化

    Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms. (arXiv:2201.07856v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2201.07856](http://arxiv.org/abs/2201.07856)

    这项研究发现，机器学习算法在胸部X射线数据集上训练时会在未得到足够服务的人群中产生高虚报率，可能放大了系统的未诊断问题。然而，研究的实验设置不足以全面研究算法的未诊断问题，而且使用与训练数据相同偏倚的测试数据进一步加剧了结果的解释难度。

    

    越来越多的报告引起了对机器学习算法可能放大由训练数据中嵌入的偏见导致的健康差异的担忧。 Seyyed-Kalantari等人发现，在三个胸部X射线数据集上训练的模型在“未发现”标签（表示没有疾病）的亚组之间产生了虚报率（FPR）的差异。这些模型在已知历史上未得到足够服务的亚组中一直产生更高的FPR，并且该研究得出结论，这些模型显示并且可能会放大系统的未诊断问题。我们认为该研究中的实验设置不足以研究算法未诊断问题。在缺乏关于数据集偏倚程度和性质的具体知识（或假设）的情况下，很难调查模型偏倚问题。重要的是，他们使用的测试数据展示了与训练数据相同的偏倚（由于随机分割），严重复杂化了所报道的差异的解释。

    An increasing number of reports raise concerns about the risk that machine learning algorithms could amplify health disparities due to biases embedded in the training data. Seyyed-Kalantari et al. find that models trained on three chest X-ray datasets yield disparities in false-positive rates (FPR) across subgroups on the 'no-finding' label (indicating the absence of disease). The models consistently yield higher FPR on subgroups known to be historically underserved, and the study concludes that the models exhibit and potentially even amplify systematic underdiagnosis. We argue that the experimental setup in the study is insufficient to study algorithmic underdiagnosis. In the absence of specific knowledge (or assumptions) about the extent and nature of the dataset bias, it is difficult to investigate model bias. Importantly, their use of test data exhibiting the same bias as the training data (due to random splitting) severely complicates the interpretation of the reported disparities
    
[^148]: 金融预测的专家聚合

    Expert Aggregation for Financial Forecasting. (arXiv:2111.15365v4 [q-fin.ST] UPDATED)

    [http://arxiv.org/abs/2111.15365](http://arxiv.org/abs/2111.15365)

    本文应用伯恩斯坦在线聚合（BOA）方法，将不同机器学习模型的个股收益预测结合成多空策略的构建。专家在线聚合在非平稳环境下表现出有吸引力的投资组合表现，胜过单独的算法，并提出了扩展方法以改善整体混合的性能。

    

    专注于金融时间序列预测的机器学习算法引起了很多关注。但是，在多个算法之间进行选择可能很具有挑战性，因为它们的估计准确性可能随时间而不稳定。专家在线聚合将有限集合模型的预测结合在一个单一的方法中，而不对模型做任何假设。本文将伯恩斯坦在线聚合（BOA）程序应用于由不同的机器学习模型生成的个股收益预测构建的多空策略的构建中。专家在线聚合在非平稳环境下也能产生有吸引力的投资组合表现。聚合方法胜过单独的算法，在投资组合夏普比率较高、亏损较低，并且换手率相似。还提出了专家和聚合特化的扩展，以改善整体混合在一族投资组合评估指标上的性能。

    Machine learning algorithms dedicated to financial time series forecasting have gained a lot of interest. But choosing between several algorithms can be challenging, as their estimation accuracy may be unstable over time. Online aggregation of experts combine the forecasts of a finite set of models in a single approach without making any assumption about the models. In this paper, a Bernstein Online Aggregation (BOA) procedure is applied to the construction of long-short strategies built from individual stock return forecasts coming from different machine learning models. The online mixture of experts leads to attractive portfolio performances even in environments characterised by non-stationarity. The aggregation outperforms individual algorithms, offering a higher portfolio Sharpe Ratio, lower shortfall, with a similar turnover. Extensions to expert and aggregation specialisations are also proposed to improve the overall mixture on a family of portfolio evaluation metrics.
    
[^149]: 凸凹 极小极大 Stackelberg 博弈

    Convex-Concave Min-Max Stackelberg Games. (arXiv:2110.05192v8 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2110.05192](http://arxiv.org/abs/2110.05192)

    本文介绍了两种一阶方法来求解一类大规模凸凹极小极大 Stackelberg 博弈，并证明了方法的多项式时间收敛性。

    

    极小极大优化问题（即极小极大博弈）因其在各种机器学习问题中的适用性而吸引了很多关注。尽管最近取得了重要进展，但迄今为止的文献主要关注具有独立策略集的博弈；对于具有依赖策略集的博弈求解知之甚少，这可以被描述为极小极大 Stackelberg 博弈。我们引入了两种一阶方法来求解一类大规模凸凹极小极大 Stackelberg 博弈，并证明了我们的方法在多项式时间内收敛。极小极大 Stackelberg 博弈首次由 Wald 研究，以 Wald 的极小极大模型的遗书名字进行命名，这一变种是鲁棒优化中主要范例，这意味着我们的方法也可以解决许多凸鲁棒优化问题。我们观察到在 Fisher 市场中计算竞争均衡也包含了一个极小极大 Stackelberg 博弈。

    Min-max optimization problems (i.e., min-max games) have been attracting a great deal of attention because of their applicability to a wide range of machine learning problems. Although significant progress has been made recently, the literature to date has focused on games with independent strategy sets; little is known about solving games with dependent strategy sets, which can be characterized as min-max Stackelberg games. We introduce two first-order methods that solve a large class of convex-concave min-max Stackelberg games, and show that our methods converge in polynomial time. Min-max Stackelberg games were first studied by Wald, under the posthumous name of Wald's maximin model, a variant of which is the main paradigm used in robust optimization, which means that our methods can likewise solve many convex robust optimization problems. We observe that the computation of competitive equilibria in Fisher markets also comprises a min-max Stackelberg game. Further, we demonstrate th
    
[^150]: 具有非交换代数的卷积滤波和神经网络

    Convolutional Filtering and Neural Networks with Non Commutative Algebras. (arXiv:2108.09923v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.09923](http://arxiv.org/abs/2108.09923)

    本文介绍了非交换卷积神经网络的代数推广，利用代数信号处理理论建模非交换卷积架构，并推导出稳定性界限。研究表明非交换卷积架构可以在算子空间上保持稳定性，且非交换滤波器独立地处理傅里叶分量。稳定性和选择性之间存在权衡，由低维矩阵空间中的矩阵多项式函数所控制。

    

    本文介绍和研究了非交换卷积神经网络的代数推广。我们利用代数信号处理理论来建模非交换卷积架构，并推导出具体的稳定性界限，这些界限扩展了文献中用于交换卷积神经网络的界限。我们展示了非交换卷积架构可以对算子空间上的变形保持稳定性。我们发展了非交换信号模型的谱表示，以展示非交换滤波器独立地处理傅里叶分量。特别地，我们证明了虽然非交换模型中信号的谱分解与维度大于1的特征空间相关联，但存在着稳定性和选择性之间的权衡，这由低维矩阵空间中的矩阵多项式函数所控制。这一权衡展示了当代数的滤波器相互作用在非交换代数结构上发生时的情况。

    In this paper we introduce and study the algebraic generalization of non commutative convolutional neural networks. We leverage the theory of algebraic signal processing to model convolutional non commutative architectures, and we derive concrete stability bounds that extend those obtained in the literature for commutative convolutional neural networks. We show that non commutative convolutional architectures can be stable to deformations on the space of operators. We develop the spectral representation of non commutative signal models to show that non commutative filters process Fourier components independently of each other. In particular we prove that although the spectral decompositions of signals in non commutative models are associated to eigenspaces of dimension larger than one, there exists a trade-off between stability and selectivity, which is controlled by matrix polynomial functions in spaces of matrices of low dimension. This tradeoff shows how when the filters in the alge
    
[^151]: Airfoil GAN: 针对气动外形优化进行空气动力学超声波成像编码与合成

    Airfoil GAN: Encoding and Synthesizing Airfoils for Aerodynamic Shape Optimization. (arXiv:2101.04757v2 [cs.CE] UPDATED)

    [http://arxiv.org/abs/2101.04757](http://arxiv.org/abs/2101.04757)

    本文提出了一种基于数据驱动的方法，通过编码和生成现有翼型，进而优化合成翼型的气动性能。该方法利用VAEGAN模型将翼型编码为潜在向量，并通过映射潜在向量到翼型坐标空间生成新的翼型。

    

    目前气动外形设计，如翼型，需要进行计算密集型的模拟，以探索可能的设计空间。通常，这样的设计依赖于预先定义的设计参数，并对合成新形状施加限制。在这项工作中，我们提出了一种基于数据驱动的形状编码和生成方法，该方法可以自动从现有的翼型中学习表示，并利用学习到的表示来生成新的翼型。然后，利用这些表示对合成翼型的气动性能进行优化。我们的模型基于VAEGAN，这是一种将变分自动编码器与生成对抗网络相结合的神经网络，并通过梯度技术进行训练。我们的模型能够（1）将现有的翼型编码为潜在向量，并从中重构出翼型，（2）通过随机采样潜在向量并将其映射到翼型坐标空间，生成新的翼型。

    The current design of aerodynamic shapes, like airfoils, involves computationally intensive simulations to explore the possible design space. Usually, such design relies on the prior definition of design parameters and places restrictions on synthesizing novel shapes. In this work, we propose a data-driven shape encoding and generating method, which automatically learns representations from existing airfoils and uses the learned representations to generate new airfoils. The representations are then used in the optimization of synthesized airfoil shapes based on their aerodynamic performance. Our model is built upon VAEGAN, a neural network that combines Variational Autoencoder with Generative Adversarial Network and is trained by the gradient-based technique. Our model can (1) encode the existing airfoil into a latent vector and reconstruct the airfoil from that, (2) generate novel airfoils by randomly sampling the latent vectors and mapping the vectors to the airfoil coordinate domain
    
[^152]: ShadowNet：一种用于卷积神经网络的安全高效的设备内推断系统

    ShadowNet: A Secure and Efficient On-device Model Inference System for Convolutional Neural Networks. (arXiv:2011.05905v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2011.05905](http://arxiv.org/abs/2011.05905)

    ShadowNet是一种安全高效的设备内推断系统，通过使用可信执行环境（TEE）保护模型隐私，同时将模型的重型线性层外包给不受信任的硬件加速器，实现了对模型隐私的保护。

    

    随着在移动设备和边缘设备上使用AI加速器的增加，设备内机器学习（ML）变得越来越受欢迎。今天，在数十亿个不受信任的设备上部署了数千个专有的ML模型。这引起了对模型隐私的严重关注。然而，如何在不丢失对不受信任的AI加速器的访问权的情况下保护模型隐私是一个具有挑战性的问题。在本文中，我们提出了一种新颖的设备内模型推断系统ShadowNet。ShadowNet通过使用可信执行环境（TEE）保护模型隐私，同时安全地将模型的重型线性层外包给不受信任的硬件加速器。ShadowNet通过在外包之前转换线性层的权重并在TEE内恢复结果来实现这一目标。非线性层也被安全地保留在TEE内。ShadowNet的设计确保了权重的高效转换和结果的后续恢复。我们构建了一个ShadowNet原型

    With the increased usage of AI accelerators on mobile and edge devices, on-device machine learning (ML) is gaining popularity. Thousands of proprietary ML models are being deployed today on billions of untrusted devices. This raises serious security concerns about model privacy. However, protecting model privacy without losing access to the untrusted AI accelerators is a challenging problem. In this paper, we present a novel on-device model inference system, ShadowNet. ShadowNet protects the model privacy with Trusted Execution Environment (TEE) while securely outsourcing the heavy linear layers of the model to the untrusted hardware accelerators. ShadowNet achieves this by transforming the weights of the linear layers before outsourcing them and restoring the results inside the TEE. The non-linear layers are also kept secure inside the TEE. ShadowNet's design ensures efficient transformation of the weights and the subsequent restoration of the results. We build a ShadowNet prototype b
    

